{"title": "SR-Reward: Taking The Path More Traveled", "authors": ["Seyed Mahdi B. Azad", "Zahra Padar", "Gabriel Kalweit", "Joschka Boedecker"], "abstract": "In this paper, we propose a novel method for learning reward functions directly from offline demonstrations. Unlike traditional inverse reinforcement learning (IRL), our approach decouples the reward function from the learner's policy, eliminating the adversarial interaction typically required between the two. This results in a more stable and efficient training process. Our reward function, called SR-Reward, leverages successor representation (SR) to encode a state based on expected future states' visitation under the demonstration policy and transition dynamics. By utilizing the Bellman equation, SR-Reward can be learned concurrently with most reinforcement learning (RL) algorithms without altering the existing training pipeline. We also introduce a negative sampling strategy to mitigate overestimation errors by reducing rewards for out-of-distribution data, thereby enhancing robustness. This strategy inherently introduces a conservative bias into RL algorithms that employ the learned reward. We evaluate our method on the D4RL benchmark, achieving competitive results compared to offline RL algorithms with access to true rewards and imitation learning (IL) techniques like behavioral cloning. Moreover, our ablation studies on data size and quality reveal the advantages and limitations of SR-Reward as a proxy for true rewards.", "sections": [{"title": "Introduction", "content": "Imitation learning (IL) from expert demonstrations is one of the most popular avenues for tackling sequential decision-making tasks. There are two categories of methods that make use of expert demonstrations. The first focuses on learning a policy that resembles the expert behavior, e.g. using Behavioral Cloning Pomerleau (1991). Another set of methods, known as inverse reinforcement learning (IRL) (Ng & Russell, 2000), first infer a reward function that explains the expert behavior and then learn a policy derived from that reward function. In popular application domains of IL, such as robotics or medicine, interacting with the environment during training is not always possible due to risk and safety concerns, making it crucial to be able to learn from only the limited, previously collected expert demonstrations.\nIn this work, we focus on the offline inverse reinforcement learning setting, where the agent neither has access to the reward function nor can query the expert for any feedback. Furthermore, the transition dynamics of the environment are unknown and the agent is provided with limited data in the form of expert demonstrations. Our first contribution, SR-Reward, is a reward function based on Successor Representations (SR), that is learned offline from expert demonstrations. Unlike adversarial schemes popular with IRL methods, our"}, {"title": "Related Work", "content": "Learning to perform a task from offline data has been extensively studied under the IL and IRL umbrella (Abbeel & Ng, 2004; Ho & Ermon, 2016; Fu et al., 2018; Garg et al., 2021; Kostrikov et al., 2020; Kalweit et al., 2020; Pomerleau, 1991). One common approach is methods based on behavioral cloning (Pomerleau, 1991) which reduce imitation learning to a supervised learning problem, i.e., learning a mapping from environment states to expert actions. They aim to increase the probability of expert actions for the states seen in the demonstrations. Although this approach can work in simple environments with large amounts of data, it is inherently myopic and fails to reason about the consequences of its selected actions. Consequently, such greedy approaches suffer from compounding errors due to covariant shift (Ross et al., 2010) when the agent deviates from the demonstrated states."}, {"title": "Background", "content": "We first introduce the notation and provide a more detailed review of concepts from imitation learning and successor representation."}, {"title": "Notation", "content": "We consider settings where the environment is represented by a Markov Decision Process (MDP) and is defined as a tuple M = (S, A, T, r, \u03b3, \u03bc\u03bf). S and A represent the continuous state and continuous action spaces respectively. T(s'|s, a) represents the state transition dynamics, r(s, a) represents the reward function, \u03b3\u2208 (0,1] is the discount factor and \u00b5o represents the starting state distribution. In the offline inverse reinforcement learning setting, we only have access to a limited set of expert demonstrations of the form D = {(so,A0,81,01,...ST)}0. In this paper, we are focusing on a minimal setting where neither the transition dynamics T(s's, a) nor the reward function r(s, a) are known. The goal is to learn a reward function re(s, a) from expert demonstrations such that its corresponding policy \u03c0\u03c6(as) performs similarly to that of the expert."}, {"title": "Imitation Learning via Distribution Matching", "content": "Methods like behavioral cloning (BC), which directly learn a policy \u03c0(as) mapping states to actions, are straightforward and effective when ample data is available. However, they are prone to distribution shift because they only match the observed action distribution. During inference, as the distribution of encountered states deviates from those seen during training, the accuracy of action predictions diminishes. This leads to accumulating errors that the policy cannot correct.\nDistribution matching methods, and related approaches (Ke et al., 2020; Kostrikov et al., 2020; Nachum et al., 2019; Ho & Ermon, 2016; Fu et al., 2018; Ghasemipour et al., 2019), are more robust to distribution shifts since they aim to match both the state and action distributions encountered during training. This helps keep the policy close to the states observed in demonstrations.\nFormally, the occupancy measure of a state-action pair under policy can be defined as\n$\\rho^{\\pi}(s, a)=E_{\\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} I(s_{t}=s, a_{t}=a)\\right]$\nwhere I is the indicator function, which equals one if the condition is met and zero otherwise. This is closely related to the state-action distribution d\" (s, a) = (1 \u2212 \u03b3)p\" (s, a). As shown by Puterman (1994), there is a one-to-one correspondence between the state-action distribution and the policy.\nDistribution matching methods aim to indirectly learn a policy by minimizing the divergence between dExpert and d\". A common choice is KL-Divergence, and minimizing DKL(d\" ||dExpert) can be viewed as maximizing the RL objective\n$E_{\\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} \\log \\frac{d_{\\text {Expert}}(s, a)}{d^{\\pi}(s, a)}\\right]$\nwhere the reward is given by the log ratio of the state-action distributions between the expert policy and the learned policy \u03c0. Since the state-action distribution is often unavailable, efforts are typically focused on estimating the ratio of the two distributions (Ho & Ermon, 2016; Nachum et al., 2019).\nIn this paper, we propose a method to directly estimate a proxy for the expert's state-action distribution from demonstrations and use it as the reward for downstream RL algorithms. We train our reward network using TD learning which can be integrated seamlessly into RL training pipeline and allows for fast inference in continuous high-dimensional spaces."}, {"title": "Successor Representations", "content": "Successor Representation (SR) was originally introduced as a method to generalize the value function across different rewards (Dayan, 1993). SR is defined as the cumulative discounted probability of visiting future states when following a specific policy, effectively representing the current state (and action) in terms of potential future states (and actions).\nFor any given pair of states s, s' and actions a, a', the SR is expressed as:\n$M(s, a, s', a')=\\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} I\\left(s_{t}=s', a_{t}=a'\\right) | s_{0}=s, a_{0}=a\\right]$\nwhere the expectation is taken over the policy \u03c0(\u03b1\u03c2) and the environment's transition dynamics T(s'|s,a). Similar to the Q-function, SR can be estimated using the recursive Bellman equation:\n$M(st, at, s', a') = I(st = s', at = a') + \u03b3E [M(st+1, at+1, s', a')]$.\nThis recursive formulation is particularly useful when learning SR alongside other temporal difference (TD) methods. Our SR-based reward function leverages this recursive approach, allowing the reward network to be trained in parallel with the actor and critic networks, with minimal changes to the existing training pipeline.\nHowever, directly estimating SR using these formulations becomes computationally intractable as the number of states and actions increases, or when transitioning from discrete to continuous domains. To address this, previous research (Kulkarni et al., 2016; Machado et al., 2020; Zhang et al., 2017) has extended SR to continuous state and action spaces using Successor Features Representation (SF). SF is expressed in terms of state and action features (s, a):\n$M(St, at) = \\phi(St, at) + \\mathbb{E} [M(st+1, at+1)].\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(1)$\nThe choice of feature extractor & is a design decision that depends on the environment. Most existing work focuses on extracting features only from the state, not the actions. In this scenario, $(s, a) can be represented as ($(8)), which is a concatenation of state features and actions.\nIn this work, we adopt this approach and use a feature extractor network to derive features from the state only."}, {"title": "Relationship to State-Action Visitation", "content": "SR implicitly captures the state-action visitation. Many density-based IL methods, such as GAIL (Ho & Ermon, 2016), use state-action distribution or occupancy measure for their distribution matching techniques."}, {"title": "Technical Approach", "content": "We use the architecture shown in Figure 1 to estimate the SR vector in continuous state and action settings. Our architecture is built upon the works of Machado et al. (2020), Kulkarni et al. (2016), and Borsa et al. (2019) with a few notable changes. First, our SR network extends the previous works to include the action when estimating the SR. This is important as our SR-based reward function r(s, a) is a function of both the state and the action and needs to distinguish the reward values of different actions. Second, it is common to use an auxiliary task when learning the encoder from scratch. Kulkarni et al. (2016) use the reconstruction of the state as the auxiliary task, while Machado et al. (2020) opt for a prediction task in which the next state is predicted from the encoded state and the action. Inspired by the results of Ni et al. (2024), we use the prediction of the next encoded state as our auxiliary task. Given the encoding of the current state $(s) and its corresponding action in the dataset a, we predict the encoded next state (s'). We use the 12 loss for this auxiliary task. Finally, our encoder consists of fully connected layers with ReLU activation layer as the final layer. We normalize the feature vector to ensure that all features are in the same range, such that ||$(s)||1 = 1 as suggested by Machado et al. (2020). If the environment dynamics are not fully Markovian one can use a history of states as s and replace the fully connected layers of the encoder with LSTM layers as proposed by Borsa et al. (2019)."}, {"title": "From SR Vector to Scalar Reward", "content": "Machado et al. (2020) shows that the norm of SR implicitly counts the state visitation. Motivated by this result, we use the 12-norm of the SR vector as our reward function. Intuitively, each element i of the SR vector, estimated using Equation 1, is the expected discounted sum of feature i of the state according to the policy that created the demonstration dataset. Hence aggregating all the elements of the SR vector in our offline setting can be seen as a visitation count of the state-action pairs when following the demonstration policy. If the demonstrations are created by an expert, ||SR(s, a) ||2 represents how often the expert has visited (s, a) while performing a task. Taken as the reward for offline RL, we set out to find a policy that maximizes the state-action visitation of the expert. We empirically show that we can learn competitive policies using this reward function."}, {"title": "Negative Sampling", "content": "Neural networks tend to overestimate the value of out-of-distribution data points (Thrun & Schwartz, 1999; Ball et al., 2023; Fujimoto et al., 2019; 2018). The overestimation error is especially concerning in our setup because an overestimated value of the reward for unseen states and actions will encourage the value"}, {"title": "Training", "content": "We employ several loss functions to train our SR network. As mentioned in Section 3.3, we can estimate the SR using the Bellman equation in a continuous state-action setting. The reward for the Bellman target in Equation 1 is replaced with $(s, a) = ($(8)) which is the concatenation of the encoded state and the action. Note that (s) and M(St+1, St+1) are calculated without the gradient. We use the 12-loss to minimize the Bellman error:\n$L_{Bellman} = \\mathbb{E}_{(s, a, s', a') \\sim D} [(M(s, a) - (\\phi(s, a) + \\gamma M(s', a')))^{2}]$\nTo help train the encoder we use an auxiliary prediction task that predicts the next encoded state (s') from the current encoded state $(s) and action a. We compute the 12-loss as\n$L_{Prediction} = \\mathbb{E}_{(s, a, s') \\sim D} [(\\phi(s') - \\text{Predictor}(\\phi(s), a))^{2}]$\nWe have added an extra loss to penalize the magnitude of the reward for values greater than 1. This loss was found to stabilize the training and create a soft upper bound for the reward. As explained in Section 4.2 we use the 12-norm of the SR vector as our reward.\n$L_{Magnitude} = \\mathbb{E}_{(s, a) \\sim D} [\\text{max}(Reward(s, a) - 1, 0)^{2}]$\nFinally, we add a negative sampling loss to improve the robustness of the reward function for out-of-distribution state-action pairs. Similar to Luo et al. (2020) we create negative samples \u0161 and a by perturbing states and actions from the demonstrations with noise. There might be concerns that the negative samples will fall into the same distribution as the demonstrations and so harm the estimation of the SR. However, as discussed by Luo et al. (2020), the demonstrations cover only a small subset of the space, hence the negative samples are with high probability orthogonal to the demonstrations, an effect that increases with the state and action dimensions of the environment. We use isotropic Gaussian noise N(0, \u03b2) to create the negative samples. The hyperparameter \u1e9e controls the standard deviation of the Gaussian noise. Intuitively, we want perturbed state-action pairs (\u0161, \u00e3) to have lower reward values proportional to the distance from their counterpart (s, a) from the dataset. Since SR estimates the visitation count based on $(s,a) = ($(s)), we measure the distance between the negative samples and their original counterparts in the space of features and actions ($(s)). We calculate the decay factor using an exponential kernel as\n$\\alpha_{\\text { decay }}=\\exp \\left(-\\frac{\\|\\phi(\\hat{s}, \\hat{a})-\\phi(s, a)\\|2}{\\sigma^{2}}\\right)\\qquad(2)$\n\u03c3can also be adjusted as a hyperparameter. Higher values of o will produce a softer decay for the reward of negative samples. The 12-loss is used to correct the estimation of SR for negative samples:\n$L_{Neg.Sample} = \\mathbb{E}_{(s, a) \\sim D} [(Reward(\\hat{s}, \\hat{a}) - \\alpha_{\\text { decay }} \\times Reward(s, a))^{2}]$\nWe train our SR network using the summation of all losses as our total loss:\n$L_{Total} = L_{Bellman} + L_{Prediction} + L_{Magnitude} + L_{Neg.Sample}$\nAlgorithm 1 shows the pseudocode for training the SR-Reward and the offline RL in the same loop. The sampled transitions used for training the SR networks have the form (s, a, s', a') which is different from the ones typically used for RL due to the addition of the next action a'. This form of transition, however, can be easily produced with access to a set of demonstrations D. We warm-start the training loop by pre-training the SR networks for 10,000 steps before using its SR-Reward to train the RL agent."}, {"title": "Experiments", "content": "To evaluate the proposed SR-Reward framework, we integrate it with two distinct offline RL algorithms: f-DVL (Sikchi et al., 2023) and SparseQL (Xu et al., 2023). Both algorithms, which build on foundational concepts from IQL (Kostrikov et al., 2022) and XQL (Garg et al., 2023), have demonstrated enhanced stability and strong performance in offline reinforcement learning settings. In our experiments, we replace the rewards in the offline dataset with those generated by SR-Reward, allowing the reward function to be learned in conjunction with the RL algorithms."}, {"title": "SR-Reward + RL v.s. BC", "content": "Behavioral Cloning (BC) is a straightforward and effective imitation learning algorithm, particularly when large volumes of expert data are available. The D4RL datasets provide over 1,000 expert demonstrations for"}, {"title": "SR-Reward v.s. True Reward", "content": "Since SR-Reward serves as a proxy for the true reward, we compare the performance of agents trained with SR-Reward to those trained with the environment's native reward. As shown in Table 1, in the MuJoCo environments, the performance of SR-Reward combined with RL closely matches that of offline RL agents trained with the true reward, indicating that the dense reward generated by SR-Reward is as informative as the environment-provided reward.\nThe Adroit hand environments present additional challenges for offline RL due to their narrower distribution of trajectories and the discontinuous, hand-engineered rewards assigned to each task. As with the MuJoCo experiments, we trained the offline RL agents using the hand-engineered rewards from the D4RL datasets, which include a combination of sub-rewards and thresholds. These discontinuous rewards highlight the complexities of manually designing reward functions and further emphasize the benefits of using SR-Reward.\nAs illustrated in Table 1, agents trained with SR-Reward perform on par with those trained with the environment's dense reward. This underscores SR-Reward's effectiveness, particularly in cases where only sparse rewards are available or where hand-engineered reward functions fail to provide a sufficiently informative reward signal."}, {"title": "Effect of Negative Sampling", "content": "The negative sampling strategy is designed to reduce reward values for out-of-distribution state-action pairs. To evaluate its effectiveness, we train SR-Reward both with and without the negative sampling strategy and compare the results in the PickCube, StackCube, and TurnFaucet environments (Figure 5). These"}, {"title": "Data Size Ablation", "content": "To investigate the impact of data size on SR-Reward, we trained each algorithm using varying numbers of expert trajectories from the D4RL dataset. We used sparseQL as our offline RL algorithm. Specifically, we evaluated performance using [10, 50, 100, 500, 1000] demonstrations for MuJoCo and [10, 50, 100, 500, 1000, 5000] demonstrations for Adroit hand environments. As shown in Figure 7, agents trained with true reward do not significantly outperform those trained with SR-Reward across different data sizes in all MuJoCo environments.\nIt's important to note that MuJoCo environments provide dense and continuous rewards based on the agents' velocity along the X-axis. The straightforward nature of these rewards presents a challenge for any reward-learning algorithm attempting to outperform them. Agents trained with SR-Reward achieve similar returns in most Adroit hand environments than those trained with the true reward, indicating that SR-Reward can also learn an informative reward function comparable to the hand-engineered rewards offered by the environment.\nAs the number of demonstrations decreases, performance declines for all agents, regardless of the reward function used. This trend suggests that informative rewards can still be learned even with limited data."}, {"title": "Data Quality Ablation", "content": "Depending on the environment, creating a set of high-quality expert demonstrations can quickly become a cumbersome task. Therefore, it is important to know the effect of sub-optimal demonstrations when used for training the SR-Reward. We conduct our experiments on MuJoCo environments using three datasets with different quality demonstrations from D4RL with the \"medium-expert\" dataset being a combination of both expert and medium demonstrations. Figure 8 shows that agents using SR-Reward have similar performance to the ones trained using true environment reward. The mixing of expert and medium datasets does not show a significant negative impact on agents trained with SR-Reward as compared to other agents. In fact including the sub-optimal trajectories results in higher returns, especially for the more difficult Walker2D environment which can benefit from larger datasets. Having low sensitivity to sub-optimal demonstrations is a desirable attribute of SR-Reward since collecting expert demonstrations can be tedious and error-prone, which increases the possibility of including sub-optimal demonstrations."}, {"title": "Conclusion", "content": "We introduced SR-Reward, a reward function based on successor representation, which is learned from offline expert demonstrations. This reward function assigns high rewards to state-action pairs frequently visited by expert demonstrators. SR-Reward is independent of both policy and value functions but can be trained concurrently with them, enabling easy integration with various RL algorithms without requiring significant modifications to the training pipeline.\nAdditionally, we implemented a negative sampling strategy to encourage a pessimistic estimation of rewards for out-of-distribution state-action pairs, thereby making the reward function more resistant to overestimation errors. Our empirical results demonstrate that SR-Reward can effectively serve as a proxy for the"}, {"title": "Limitations and Future Work", "content": "Our experiments focused exclusively on state-based demonstrations. Extending these methods to visual domains is possible by substituting the encoder in Figure 1 with one capable of extracting meaningful representations from image data. Designing an effective visual encoder introduces a new set of engineering challenges that must be addressed when expanding to visual domains. In addition, a theoretical investigation regarding the convergence properties of using SR as a reward can provide more support for our claim, however, in this paper, we have focused on supporting our claims using empirical results.\nWe focused our experiments on offline settings because the negative sampling strategy can only protect the SR-Reward from overestimation errors near the expert demonstrations, where meaningful negative samples are generated by perturbing expert trajectories. Since expert trajectories cover only a small portion of the state space, high extrapolation errors can be expected in regions far from these demonstrations. Consequently, in online RL, when the agent explores areas distant from the expert trajectories, it may be misled by inflated rewards, leading to the learning of suboptimal policies.\nOne limitation of SR-Reward is the assumed availability of a dataset of optimal trajectories. Although our empirical results indicate a degree of robustness when combining optimal and sub-optimal datasets (Figure 8), the presence of sub-optimal demonstrations can negatively impact SR-Reward since the training process treats optimal and sub-optimal demonstrations equally. Enhancing the ability to control the influence of demonstrations based on their quality could lead to higher-quality rewards and more data-efficient learning, offering a promising direction for future research.\nGiven that the successor representation is closely linked to occupancy measures and state-action distributions, the SR-Reward function proposed here can be employed to approximate the state-action distributions of both expert and non-expert actors. This paves the way for developing new algorithms in imitation learning (IL) and inverse reinforcement learning (IRL), enabling the direct matching of distributions using an approximate model of state-action distributions. We consider this an exciting direction for further exploration and future research."}, {"title": "Broader Impact Statement", "content": "The ability to learn from demonstrations enables users without technical knowledge to program agents such as industrial or household robots. This technology can have great potential for automating tasks where the industry is facing a labor shortage or where the safety of humans is of concern. On the other hand, such technology can accelerate the loss of jobs due to automation, a trend that raises concerns for many. So far automation in the physical world especially outside of repetitive motions of industrial robots has been limited, however, this can change through further development and deployment of systems that can easily and flexibly learn from a handful of demonstrations."}, {"title": "Occupancy Measure and Successor Representations", "content": "We will restrict ourselves to the occupancy measure of the state only (instead of state and action). The extension to state and action is trivial via a second summation over the actions.\nThe expectation is with respect to starting state distribution \u03bc\u03bf, the policy that is followed \u03c0, and the transition dynamics of the environment T.\nWe can write the definitions of occupancy measures p(s) and successor representations M(s, s') in terms of probabilities p.\n$M(s, s') = E[\\gamma^{t} I(st = s')|so = s]$\n$=\\sum_{t=0}^{\\infty} E[I(st = s')|so = s]$\n$=\\sum_{t=0}^{\\infty} \u03b3^{t} p(st = s'|so = s)$\nand similarly for the occupancy measure p(s):\n$p(s) = E[\\gamma^{t} I(st = s)]$\n$=\\sum_{t=0}^{\\infty} E[I(st = 8)]$\n$=\\sum_{t=0}^{\\infty} \u03b3^{t} p(st = s)$\nBelow we show that p(s') = \u2211p(s)M(s, s'):\n$p(s') = \\sum p(st = s')$"}]}