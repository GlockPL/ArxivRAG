{"title": "DISCRETE DIFFUSION SCHR\u00d6DINGER BRIDGE MATCHING FOR GRAPH TRANSFORMATION", "authors": ["Jun Hyeong Kim", "Seonghwan Kim", "Seokhyun Moon", "Hyeongwoo Kim", "Jeheon Woo", "Woo Youn Kim"], "abstract": "Transporting between arbitrary distributions is a fundamental goal in generative modeling. Recently proposed diffusion bridge models provide a potential solution, but they rely on a joint distribution that is difficult to obtain in practice. Furthermore, formulations based on continuous domains limit their applicability to discrete domains such as graphs. To overcome these limitations, we propose Discrete Diffusion Schr\u00f6dinger Bridge Matching (DDSBM), a novel framework that utilizes continuous-time Markov chains to solve the SB problem in a high-dimensional discrete state space. Our approach extends Iterative Markovian Fitting to discrete domains, and we have proved its convergence to the SB. Furthermore, we adapt our framework for the graph transformation and show that our design choice of underlying dynamics characterized by independent modifications of nodes and edges can be interpreted as the entropy-regularized version of optimal transport with a cost function described by the graph edit distance. To demonstrate the effectiveness of our framework, we have applied DDSBM to molecular optimization in the field of chemistry. Experimental results demonstrate that DDSBM effectively optimizes molecules' property-of-interest with minimal graph transformation, successfully retaining other features.", "sections": [{"title": "INTRODUCTION", "content": "Transporting an initial distribution to a target distribution is a foundational concept in modern generative modeling. Denoising diffusion models (DDMs) have been highly influential in this area, with a primary focus on generating data distributions from simple prior (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020; Song et al., 2020; Kim et al., 2024b). Despite their promising results, setting the initial distribution as a simple prior makes DDMs hard to work in tasks where the initial distribution becomes a data distribution, such as image-to-image translation. To tackle this, diffusion bridge models (DBMs) extend DDMs to transport data between arbitrary distributions (Liu & Wu, 2023; Liu et al., 2023; Zhou et al., 2023). However, training DBMs requires a coupling between the initial and target distributions, which is often difficult to obtain in practice. The Schr\u00f6dinger Bridge (SB) provides an attractive framework for constructing a joint distribution of two data distributions while aligning with the underlying stochastic dynamics (Pariset et al., 2023; Kim et al., 2023; Dong et al., 2024).\nFormally, the SB problem seeks the stochastic process that connects two distributions and is closest to a reference process, measured by the Kullback-Leibler (KL) divergence (Schr\u00f6dinger, 1932). The SB problem can be described as an entropy-regularized optimal transport (EOT) problem, which in-troduces an entropy term to the optimal transport (OT) objective, resulting in randomness in the transport process (L\u00e9onard, 2013). Here, the transportation cost is determined by the system's natural dynamics; for example, in the case of Brownian motion, the transportation cost becomes L2 (De Bortoli et al., 2021). The SB/EOT can be computed efficiently using the Sinkhorn algorithm, though high-dimensional or large-scale data applications remain challenging (Sinkhorn, 1967; Cuturi, 2013). In recent, many methods have been proposed to approximate SB via distribution learn-ing, utilizing techniques developed in DBM and DDM (De Bortoli et al., 2021; Liu et al., 2022; Somnath et al., 2023; Liu et al., 2023; Peluchetti, 2023a; Shi et al., 2024; Lee et al., 2024).\nDespite the progress, most of the methods focus on the continuous spaces, where diffusion processes are represented by Brownian motion, and SB problems in discrete domains are less explored. It is particularly significant in fields that handle discrete state data, such as graphs or natural language (Austin et al., 2021; Vignac et al., 2022). Directly applying frameworks for approximating SB formulated in continuous spaces to these domains limits its potential since it does not reflect the intrinsic properties of discrete data space. To bridge the gap, we propose a novel framework called Discrete Diffusion Schr\u00f6dinger Bridge Matching (DDSBM) utilizing the continuous-time Markov chains (CTMCs) to solve the SB problem in a high-dimensional discrete setting. Our approach leverages Iterative Markovian Fitting (IMF), which was originally proposed for the SB problem in a continuous domain (Peluchetti, 2023a; Shi et al., 2024).\nWe then extend our formulation to the graph domain, where the underlying process can be inter-preted as independent modifications to both the nodes and edges (Vignac et al., 2022). In this case, the cost function of the corresponding EOT can then be regarded as the graph edit distance (GED), which is especially suited for systems where preserving graph similarity is crucial (Sanfeliu & Fu, 1983). The molecular optimization in drug/material discovery is such a case in that molecules are represented as graphs. In addition, the goal is to obtain the molecules with desired molecular prop-erties while retaining favorable properties in acclaimed molecules. Since molecular structures are closely related to their properties, it is highly advantageous to minimize structural changes (or graph editing) during the optimization.\nTo validate our framework, we evaluated the performance of DDSBM on molecular optimization tasks, with criteria of demonstrating optimal structural modifications to achieve desired property. DDSBM shows promising results in molecular distribution shift with minimum structural change compared to the previous graph-to-graph translation models. As a direct result of this, DDSBM retains multiple properties of the initial molecules, along with desired property. Lastly, we applied DDSBM for a more challenging task, where proper joint pairing between two molecular spaces can not be defined properly.\nOur contributions are as follows:\n\u2022 We propose a novel framework, DDSBM, for the SB problem in discrete state spaces by exploiting the IMF procedure and prove its convergence to the SB solution.\n\u2022 We extend our framework to the graph domain, demonstrating a connection between the objective function and the GED."}, {"title": "THEORETICAL BACKGROUND", "content": "Notations. We consider the sample space \u03a9 = D([0, \u03c4], X), which is set of all left limited and right continuous (c\u00e0dl\u00e0g) paths over the finite metric space (X, dx). The space of path measures is denoted by P(\u03a9) = P(D([0, \u03c4], X)). The subset of Markov measures of which the transition probability are continuous and differentiable for time is denoted by M. The transition probability from state x at time s to state y at time t will be represented by Ps:t(x, y). Similarly, the transition rate or generator of a Markov measure as As(x, y). For any M\u2208 M, the reciprocal class of M is denoted by R(M) (see Definition 2.1). For any P\u2208 P(\u03a9), Pt denotes the marginal distribution at time t, Ps,t as the joint distribution at time s and t, Pt's the conditional distribution at time t given state at time s. X (Xt)t\u2208[0,7] denotes the canonical process given by Xt(w) = wt for \u03c9 = (\u03c9\u03c2)\u2208[0,1] \u2208 \u03a9. The reference measure Q \u2208 M is an irreducible Markov measure on \u03a9 with its associated canonical filtration. We will denote the transition rate (or probability) of any Markov measures M other than Q, using a superscript notation.\n2.1 SCHR\u00d6DINGER BRIDGE PROBLEM\nSchr\u00f6dinger Bridge (SB) problem is finding a stochastic process that most closely resembles a given reference process, with a condition that initial and final marginal distributions are fixed as \u0393 and \u039e. Specifically, the reference process with the law Q is given, the optimality is achieved by minimizing Kullback-Leibler (KL) divergence to the reference process, DKL(P||Q). Thus the definition of the SB solution is as below:\n$P^{SB} = arg \\min_{P} \\{D_{KL}(P||Q) : P_0 = \\Gamma, P_\\tau = \\Xi\\}.$  (1)\nIf we additionally fix the initial and terminal coupling P0,7, the optimality can be found easily as a mixture of bridges P0,7Q.10,7, which implies that finding the SB solution is equivalent to finding optimal coupling PS (L\u00e9onard, 2013). In particular, such optimal coupling is called static SB solution, which could be defined as follows:\n$D^{SB}_{JP^{SB}_{\u041e, \u0442}} = arg \\min_{P_{0, \\tau}} \\{D_{KL} (P_{0, \\tau}||Q_{0, \\tau}) : P_0 = \\Gamma, P_\\tau = \\Xi\\}.$ (2)\nNote that the KL-divergence is decomposed into the entropy term H(P0,7) and the cross-entropy term Ep. [-log q(x7|x0)], where q denotes the density function of Q. The cross-entropy term is represented as L\u00b2 distance when the Q is associated with the reversible Brownian motion. In general, the static SB problem is equivalent to the entropy-regularized optimal transport (EOT) problem with the cost function c(x, y) = log q7|0(y|x) (L\u00e9onard, 2013). While previous approaches have primarily focused on solving SB problems in continuous spaces, typically involving Brownian mo-tion, our work differs by addressing SB solutions in discrete spaces, specifically dealing with c\u00e1dl\u00e1g paths. Despite this distinction, the core idea remains rooted in the intrinsic properties of the SB solution (see Theorem A.1).\n2.2 SOLUTION METHOD\nTo solve the SB problem, we adopted the Iterative Markovian Fitting (IMF) method, a technique that has been proposed in various studies (Peluchetti, 2023a; Shi et al., 2024). These works primarily address diffusion processes in Euclidean space, whereas our approach differs slightly as we focus on a continuous-time Markov chain over finite state space. Applying IMF to c\u00e0dl\u00e0g paths in discrete spaces requires additional theoretical developments or modifications to ensure proper convergence and stability in this new context, which is described in Appendix A.\nThe SB solution is a mixture of pinned-down measures of Q(\u00b7|Xo = x0, X\u2081 = x+), where the pair (x0, x7) is drawn from the coupling PSB. Based on this, the projection method first constructs a reciprocal measure, which is mixture of pinned-down processes from a given initial coupling."}, {"title": "METHODS", "content": "Here, we propose the Discrete Diffusion Schr\u00f6dinger Bridge Matching (DDSBM) framework, which focuses on solving the SB problem in discrete state spaces. Approaches to the SB problem in con-tinuous spaces model probability flows through stochastic differential equations, while our method uses continuous-time Markov chains (CTMCs) in discrete state spaces, governed by the Kolmogorov equation Equation (9). We first provide the algorithm of DDSBM (Section 3.1) to apply Iterative Markovian Fitting (IMF) to c\u00e0dl\u00e0g paths in discrete spaces, of which the convergence is ensured theoretically by Theorem 2.3. We then discuss how DDSBM can be implemented in the graph transformation (Section 3.2) and introduce a graph permutation matching algorithm for efficiency (Section 3.3)."}, {"title": "ALGORITHM FOR SOLVING THE SCHR\u00d6DINGER BRIDGE PROBLEM", "content": "In this section, we present the framework for solving the SB problem in discrete state spaces based on the IMF algorithm proposed by Shi et al. (2024). We refer to it as the DDSBM framework. The framework iteratively applies Markov and reciprocal projections to update the sequence of measures, as described in Equation (3).\nIt begins with an initial random coupling \u3160 such that \u03c0\u03bf = \u0393 and \u03c0\u2081 = \u039e. Following the definition of the reciprocal class in Definition 2.1, we construct the initial reciprocal bridge to obtain the measure \u039b(0).\nGiven a reciprocal measure A(2n) \u2208 R(Q), the next step is to compute its Markov projection \u041c(2n+1) := \u041f\u043c(A(2n)). The exact form of the transition rate for M(2n+1) is provided in Proposi-tion A.2. In practice, the transition rate is approximated by a neural network.\nTo achieve this, it first samples pairs (x0,x7) from A(2n) and samples intermediate states xt by constructing the bridge Q(\u00b7|X0, X7). The sampled pairs (xt, x7) are distributed according to A.(2n)Using these realizations, the rate matrix of M(2n+1) is approximated by parameterized Markov measure M\u00ba, by minimizing the following loss function:\n$L(\\theta) = E^{A^{\\theta}} \\Bigg[ A^{Q, T}_{t|0} (A^{Q, T}_{t|0} - A^{M^{\\theta}}_{t|0}) (X_t, X_\\tau) + \\sum_{y \\neq X_t} \\int_{0}^{\\tau} A^{Q}_{t|0} \\cdot log M^{\\theta} (X_t, y) dt \\Bigg],$ (4)\nwhere A denotes the generator of pinned down process of Q(\u00b7|X-). From the approximated generator AM\u00ba, we can sample x\u012b given xo, where (x0,x+) ~ M8,- \u2248 M(2n+1). Note that, until the sequence of measures converges, the new joint coupling MO,T ~ MO,T will differ from the previous one A(2n)~\nOnce the Markov measure M(2n+1) is obtained, we proceed to compute the corresponding recip-rocal measure A(2n+1) through reciprocal projection, A(2n+1) := \u03a0r(Q)(M(2n+1)). In theory, as shown in Proposition A.2, the time marginal distributions are preserved under Markov projection, meaning that A(2n) = M(2n+1) for all t \u2208 [0,7]. However, in practice, since the Markov projec-tion is approximated using a neural network, repeatedly applying this approximation can lead to an accumulation of errors in the time marginals. Such accumulated errors may violate the marginal condition of the SB problem, particularly leading to a potential failure in satisfying the terminal condition P\u2081 = \u039e.\nTo compensate these errors, the next Markov measure M(2n+2) := \u041f\u043c(A(2n+1)) is approximated in a time-reversal way (see Proposition A.8). Based on the time-symmetric nature of Markov mea-sures, we can leverage the time-reversed generator \u00c2M(2n+2), which enables the sampling of xo conditioned on 27, where (x0,x\u2081) ~ M(2n+2). The approximation of \u0100M(2n+2) is achieved by minimizing the following loss function:\n$L(\\theta) = E^{A^{\\theta}} \\Bigg[ A^{Q, I} (A^{Q, I} - A^{M^{\\theta}}) (X_t, X_\\tau) + \\sum_{y \\neq X_t} \\int_{0}^{\\tau} A^{Q} \\cdot log M^{\\theta} (X_t, y) dt \\Bigg],$ (5)\nwhere A10 denotes the time-reversal generator of pinned down process of Q(\u00b7|Xo), and $ repre-sents the parameters of the neural network approximating the time-reversed generator \u00c3M.\nIn this manner, the iterative Markov projection following the reciprocal projection is performed alternately in a forward and backward (time-reversal) fashion. Finally, this process yields a sequence of measures (\u039b(n))n\u2208N and (M(n))n\u2208N+, which converges to PSB in theory."}, {"title": "PROCESS ON THE GRAPH", "content": "We present a method for applying the previously described solution to a graph transformation. A graph is represented as G = (V, E), where V = (v(i)); and E = (e(ij))ij denote node and edge features, respectively. In a molecular graph, for example, the node and edge features correspond to atomic types and edge features, respectively. Here, V and E are modeled as products of categorical random variables.\nAs the reference process, we define a jump process in which the nodes and edges of the graph change discretely, assuming that all nodes and edges are independent. Therefore, the transition probability P and the rate A of the reference process is described as follows:\n$P_G(G_1, G_2) = \\prod_{i} PV (v^{(i)}_1, v^{(i)}_2) \\prod_{i,j} PE (e^{(i,j)}_1, e^{(i,j)}_2),$ (6)\n$P(x,y) = \\sum_{z \\in X} P(x, z)A (z, y),$ (7)\n$A (x, y) = \\frac{\\partial}{\\partial t} P(x, y)|_{t=s},$ (8)\nwhere denotes V or E, G\u2081 denotes (V1, E1) = ((2)), ((3))), G2 denotes (V2, E2) = ((2)), (())), and XV and XE denote the state space of the nodes and edges, respectively. More specifically, we use a monotonically decreasing function for signal to noise ratio, \u1fb6 : [0, \u03c4] \u2192 (0, 1], in which the transition rate is defined as:\n$A_{t}^{V}(x,y) = \\delta_{xy}(ln\\alpha(t)) - m^{V}(y))$, (7)\nwhere dry denotes the Kronecker delta, and m(V) and m(E) denote the prior distribution of nodes and edges as proposed in the previous discrete diffusion work (Vignac et al., 2022). According to the Kolmogorov equation, we get the transition probability as,\n$P_{t}^{V}(x,y) = \\alpha(t) \\delta_{xy} + \\frac{(1-\\alpha(t))}{\\alpha(s)} m^{V}(y).$ (8)\nNote that the choice of m(\u00b7) as uniform is associated to the diffusion on the state space X, where the X is considered fully-connected graph. Moreover, the stationary distribution of the associated gener-ator always becomes m(\u00b7). Although non-uniform choice of m(\u00b7) breaks the diffusion formulation on X, it does not harm SB formulation."}, {"title": "RESULTS AND DISCUSSIONS", "content": "Here, we demonstrate the effectiveness of the Discrete Diffusion Schr\u00f6dinger Bridge Matching (DDSBM) framework on graph transformation tasks. Specifically, we apply DDSBM to a chemical domain, where the graph transformation task is nontrivial due to additional constraints imposed by molecular graphs and their associated properties. We conducted experiments on two different molecule datasets: ZINC250K (Kusner et al., 2017) and Polymer (St. John et al., 2019). Throughout this section, we first elaborate on the common experimental setups and metrics for evaluation. The second and third sections provide a detailed analysis of ZINC250K and Polymer experiments. Finally, we empirically show utilizing an initial coupling based on the Tanimoto similarity (Willett et al., 1998), the widely-used metric for assessing chemical similarity, is effective for training DDSBM.\n5.1 EXPERIMENTAL SETUP AND METRICS\nExperimental Setup. To train the models on graph transformation problems, an initial coupling between two distributions is necessary. We randomly coupled the data of initial and terminal distri-butions to obtain paired data. The molecular pairs are divided into training and test datasets in a ratio of 8:2. The effect of different initial couplings is discussed in Section 5.4. Detailed explanations about model architectures and hyperparameters can be found in Appendix C.\nMetrics. By definition of SB (see Equation (1)), both joint distribution and marginal distributions at each side must be examined to assess the degree to which the SB has been successfully achieved. Specifically, we evaluate these distributions from two perspectives: graph structure and molecular properties. While graph structure provides valuable insights, it alone may not be sufficient for molecular optimization tasks. Therefore, the assessment of molecular properties is essential for a more comprehensive validation of the models, as these properties might exhibit weak correlations with the graph structures.\nIn the context of graph structure, we calculate Fr\u00e9chet ChemNet Distance (FCD) score (Preuer et al., 2018) between the target molecule distribution and the generated molecules from each model, which focuses on the marginal distribution of the molecules. As the metric for evaluating the quality of the joint distribution, we report the negative log-likelihood (NLL), which measures the cost of transforming one graph into another based on the reference process, aligning with the graph edit distance (GED) (see Appendix B.6).\nFor molecular properties, we measure the Wasserstein-1 distance (W1) between the property dis-tributions of the target and generated molecules representing the properties to be modified, which corresponds to the marginal distribution. In terms of the joint distribution, we calculate the mean absolute difference (MAD) of other key properties between the initial and generated molecules that should be retained, where drug-likeness (QED) (Bickerton et al., 2012) and synthetic accessibility score (SAscore) (Ertl & Schuffenhauer, 2009) are typical examples in optimizing drug candidates."}, {"title": "DISCUSSION", "content": "In this paper, we propose Discrete Diffusion Schr\u00f6dinger Bridge Matching (DDSBM), a novel framework utilizing continuous-time Markov chains to solve the SB problem in a high-dimensional discrete state space. To this end, we extend Iterative Markovian Fitting (IMF), proving its conver-gence to SB. We successfully apply our framework to graph transformation, specifically for molec-ular optimization. Experimental results demonstrate that DDSBM effectively transforms molecules to achieve the desired property with minimal graph transformation, while retaining other features.\nHowever, our method has several limitations. The IMF requires iterative sampling from the learned Markov process, which can be more computationally intensive than simple bridge matching. Addi-tionally, the graph permutation matching may introduce challenges for wider applicability in general graph tasks."}, {"title": "PROPOSITIONS AND PROOF", "content": "A.1 NOTATIONS\nIn this section, we introduce the notations that will be used throughout the proofs of the propositions. \u03a9 = D([0, 1], X) denotes the space of all left-limited and right-continuous (c\u00e0dl\u00e0g) paths over a finite space X. We assume the state space X has connected finite graph structure (fully-connected graph), which imply it becomes metric space with graph distance metric dx(\u00b7,\u00b7). Accordingly, the sample space 2 is equipped with Skorokhod topology with Skorokhod metric d\u0272(\u00b7,\u00b7), and associated Borel \u03c3-algebra. X = (Xt)t\u2208[0,1] denotes the canonical process given by:\n$X_t(w) = w_t, \\qquad t\\in [0, \\tau], w = (w_s)_{s\\in[0,1]}.$ \nThe reference measure Q is an irreducible Markov measure on \u03a9 with its associated canonical filtration. Assume that the transition probability of Q, denoted by Ps:t(x, y), from (s, x) \u2208 [0, \u03c4] \u00d7 X to (t, y) \u2208 [0, \u03c4] \u00d7 X, is continuous and differentiable over time. The measure is generated by the transition rate function As(x, y), which gives the rate of transition from x \u2208 X to y \u2208 X at time s\u2208 (0,7), and satisfies the Kolmogorov forward equation:\n$\\frac{\\partial}{\\partial t} P_{s:t}(x, y) = \\sum_{z\\in X} P_{s:t}(x, z) A_t (z,y),$ (9)\n$A_s(x, y) = \\frac{\\partial}{\\partial t} P_{s:t}(x, y)]_{t=s}.$\nWe also assume that Q can construct a bridge Q(X\u2081 = x, X\u2081 = y) for all x, y \u2208 X. For any Markov measure M, the corresponding generator is denoted as A(M).\nA.2 THEOREM A.1\nTheorem A.1. (Uniqueness of the Schr\u00f6dinger Bridge Solution)\nIf the reference process Q is Markov, then under mild conditions the Schr\u00f6dinger Bridge solution [PSB exists and is unique. Furthermore, the solution is mixture with static Schr\u00f6dinger Bridge solution represented as ][PSB = PSB,Q.10,7 \u2208 R(Q), and also it is in M. Conversely, a process P = P0,7Q.10,7 is Markov if and only if P = [PSB.\nProof. This is direct consequence of Theorem 2.12 of (L\u00e9onard, 2013).\nA.3 MARKOV PROJECTION\nProposition A.2. (solution of Markov projection)\nLet M* = \u041f\u043c(\u0410), \u041b\u2208 R(Q) and the generator of Qbe At(x, y) with transition probability Ps:t(x, y). Under mild assumptions, the generator of the Markov measure M* becomes\n$A^{(M^*)} (X,y) = E_{A^{\\Lambda, \\tau}} [A_t(X_t, y; X_\\tau)|X_t]$\n$A_t(x, y; z) = A_t(x, y) \\frac{P_{s:\\tau} (Y, Z)}{P_{s:\\tau}(X,Z)} - \\delta_{xy} \\sum_u A_t(y, u) \\frac{P_{t:\\tau} (u, z)}{P_{t:\\tau}(x, Z)}$\nwhere z \u2208 X\nThe reverse KL-divergence is\n$D_{KL}(A||M^*) = \\int_0^T E_{A_{t,0}} \\Bigg[ (A^{(\\Lambda,0)} - A^{(M^*)) (X_t, X_\\tau) + \\sum_{y \\neq X_t} A_{t,0}^{A^{\\Lambda}} log \\frac{A_t}{A^{(M^*)} (X_t,y)} \\Bigg] dt,$\nwhere the A|0 is the generator for the conditioned measure A10 which is defined as\n$A_t(X,y) = E_{A_{t,0}} [A_t(X_t, y; X_\\tau)| X_t, X_0]$"}]}