{"title": "Whisper in Medusa's Ear: Multi-head Efficient Decoding for Transformer-based ASR", "authors": ["Yael Segal-Feldman", "Aviv Shamsian", "Aviv Navon", "Gill Hetz", "Joseph Keshet"], "abstract": "Large transformer-based models have significant potential for speech transcription and translation. Their self-attention mechanisms and parallel processing enable them to capture complex patterns and dependencies in audio sequences. However, this potential comes with challenges, as these large and computationally intensive models lead to slow inference speeds. Various optimization strategies have been proposed to improve performance, including efficient hardware utilization and algorithmic enhancements. In this paper, we introduce Whisper-Medusa, a novel approach designed to enhance processing speed with minimal impact on Word Error Rate (WER). The proposed model extends the OpenAI's Whisper architecture by predicting multiple tokens per iteration, resulting in a 50% reduction in latency. We showcase the effectiveness of Whisper-Medusa across different learning setups and datasets.", "sections": [{"title": "I. INTRODUCTION", "content": "Transformer-based supervised models are currently achieving state-of-the-art automatic speech recognition (ASR) results. Specifically, OpenAI's Whisper [1] is one of the most successful models and has approximately 1.5 billion parameters (in its largest version), which ensure high transcription accuracy. However, despite their impressive performance these large models tend to suffer from slow inference speeds.\nRecently, several optimization strategies have been developed to enhance inference efficiency in transformers. These strategies encompass efficient hardware utilization, algorithmic improvements, and sophisticated pre-processing and post-processing methodologies. One of the most well-known solutions is Faster-Whisper [2]. This approach involves re-implementing OpenAI's Whisper model using CTranslate2 [3], a high-performance inference engine optimized explicitly for transformer models. Another prevalent approach involves applying guided knowledge distillation and quantization to the Whisper model [4], [5], reducing the model's size and improving speed, at the cost of WER degradation.\nSpeculative decoding [6], [7] represents an emerging family of techniques in natural language processing (NLP) designed to speed up inference in large models. By performing multiple decoding steps in parallel, it reduces the number of computationally expensive operations required while decoding. Typically, a smaller and faster assistant model generates several candidate tokens, from which the most promising ones are selected, enhancing efficiency and effectiveness. The assistant model produces multiple candidate sequences, evaluated and scored by the counterpart large mode based on their likelihood or quality, with the best sequence chosen as the final output. Recently, various approaches for candidate evaluation have been proposed, each differing in methodology and implementation [8]\u2013[10].\nHowever, for practitioners, developing an effective assistant model and maintaining both the original and the assistant models is challenging. To address this, self-assistance methods have been proposed, where the original model serves as its own assistant model. Studies like [11], [12], and [13] train the original model to generate multiple predictions at each step, using different techniques to generate and evaluate candidate sequences. Other approaches [14] and [15] utilize only parts of the original model as the assistant model.\nInspired by Stern et al. [11] and Cai et al. [13] in the NLP domain, our study extends speculative decoding methods for the speech domain. Instead of sequentially generating a single token at a time, we propose modifying the architecture of encoder-decoder transformer-based ASR to predict multiple tokens per decoding step. ASR transformer-based models work differently than most large language models (LLMs), implemented as decoder-only. In ASR, the transformer decoder processes the entire speech audio from the encoder, which might offer even better efficiency than the decoder-only architectures. We exemplify our approach to OpenAI's Whisper large model.\nThe contributions of this paper are as follows: (1) We propose a novel ASR approach, Whisper-Medusa, utilizing Speculative Decoding; (2) We introduce two architectures to implement this approach; (3) We demonstrate the effectiveness and advantages of our method through a comprehensive evaluation of diverse multilingual benchmarks. Additionally, we release the open-source code for further research and development, available at: https://github.com/aiola-lab/whisper-medusa."}, {"title": "II. METHOD", "content": "Transformer-based ASR architectures typically consist of an encoder and a decoder. The encoder processes the input audio waveform and converts it into a sequence of high-dimensional embeddings. Then, the decoder takes these embeddings as input and generates a sequence of tokens (usually sub-word units). The decoder operates autoregressively, predicting one token at a time. At each step, it estimates the probability distribution over the entire set of possible tokens and selects the most likely one. This process continues until an end-of-sequence token is generated.\nOur focus is on modifying the decoder's behavior. Instead of predicting a single token at each iteration, we propose adjusting the decoder to simultaneously predict K+1 tokens. This approach aims to improve efficiency and potentially capture longer-range dependencies in the output sequence. In the following sections, we will rigorously describe the processes.\nLet $y \\in Y$ denote a token from the set of tokens Y. Denote by $y_{<t} = (y_0, y_1,..., y_{t-1})$ the sequence of tokens starting from the 0-th token $y_0$ until the t-1-th token, $y_{t-1}$.\nThe decoder estimates the probability distribution of the next token $y_t$ given the predicted token sequence $\\hat{y}_{<t}$ and the encoder's embeddings $z$. That is, $p(y_t | \\hat{y}_{<t}, z)$. Throughout the paper, we assume greedy decoding, where each token is predicted by selecting the one with the highest probability from the distribution:\n$\\hat{y}_t = \\arg \\max_{y_t} p(y_t | \\hat{y}_{<t}, z)$.\nWe assume there are K+1 prediction heads. The base head is the original decoder prediction head (before or after fine-tuning). This base head generates the probability distribution $p_0$ over the set of tokens for the t-th token, conditioned on the preceding already predicted sequence of tokens $\\hat{y}_{<t}$, as described earlier. The probability distribution associated with the k-th head corresponds to the $y_{t+k}$ token, conditioned on the previous tokens $\\hat{y}_{<t}$, and is given by $p_k(y_{t+k} | \\hat{y}_{<t}, z)$.\nWe describe the inference process using K+1 heads, assuming the model and its corresponding heads have already been trained. The inference includes two phases token prediction and verification. In the first phase, we estimate the distributions of all K+1 heads:\n$p_0(y_t | \\hat{y}_{<t}, z), p_k (y_{t+k} | \\hat{y}_{<t}, z)$ for $1 \\le k \\le K$,\nand identify the set of the subsequent K + 1 tokens $\\{\\hat{y}_t, \\hat{y}_{t+1},..., \\hat{y}_{t+K}\\}$ by selecting the tokens that maximize these probabilities: $\\{\\hat{y}_t, y_{t+1},..., \\hat{y}_{t+K}\\}$.\nIn the second phase, we pass all the predicted tokens from the first phase through the base head and select all heads $0 \\le k \\le K$ for which the resulted probabilities are above a threshold:\n$p_0(y_{t+k} | \\hat{y}_{<t}, \\hat{y}_t,..., \\hat{y}_{t+k-1}, z) > \\min{\\epsilon, \\alpha p_{max}}$,\nwhere $\\epsilon = 0.09$ and $\\alpha = 0.3$ (selected on a held-out set), and $p_{max}$ is the exponent of the entropy function\u00b9 of the distribution $p_0$.\nWe introduce two architectures. The first, Medusa-Linear, features K heads, each consisting of a single linear layer with a residual connection, followed by a shared vocabulary projection layer to reduce parameters. We only update the final decoder layer and the heads to simplify training. We train the base head along with the additional K heads by applying a cross-entropy (CE) loss to each one. The total loss is computed as the average of these individual losses, combined with a weighted KL-divergence loss between the probability distributions of the original ASR model (the weight was 0.01).\nThe second architecture, Medusa-Block, includes an additional decoder block shared across all K heads. This is followed by a single linear layer and a residual connection for each head, with the outputs then passed to a shared vocabulary projection layer. In this architecture, all the ASR model's weights are frozen, and only the Medusa weights are updated. Hence, we trained our model using K weighted CE loss functions, where the base head is not trained, and without the KL loss function. Our models are depicted in Fig. 1."}, {"title": "III. EXPERIMENTS", "content": "We train and evaluate our models using two datasets. First, we utilized LibriSpeech [17] dataset, a widely recognized resource in speech recognition research. LibriSpeech consists of approximately 1,000 hours of English read speech, sourced from public domain audiobooks, and is accompanied by corresponding transcriptions. Specifically, our model was trained on the LibriSpeech 100, 360, and 500 subsets. The original transcripts in LibriSpeech are in uppercase format, therefore we used the transcripts from [18], which restores punctuation and capitalization for better readability. We evaluate our models using LibriSpeech's Test-Clean and Test-Other sets. Second, we used the VoxPopuli dataset [19], a large-scale multilingual speech corpus derived from European Parliament event recordings between 2009 and 2020. This dataset includes 400K hours of unlabelled speech data in 23 languages and 1,800 hours of transcribed speech data in 16 languages. We employ VoxPopuli in two experimental setups: fully-supervised and self-supervised learning."}, {"title": "B. Experimental setup", "content": "We integrated our Medusa heads into the Whisper Large-v2 model. The model was trained with a batch size of 16 for the LibriSpeech and fully-supervised VoxPopuli setups, and a batch size of 8 for the self-supervised setup of VoxPopuli, using a learning rate of 1e-4 and the Adafactor optimizer [20] across all setups. We trained the model for 200K/400K/500K update steps in LibriSpeech, fully supervised VoxPopuli, and self-supervised VoxPopuli setups, respectively. In all setups, we utilized early stopping on the validation set to select the best-performing model.\nThe input to our model follows a similar process to Whisper [1], where audio is sampled at 16 kHz and converted into an 80-channel log-magnitude Mel spectrogram with a 25-millisecond window and a 10-millisecond stride. On inference, we applied an exponential decay length penalty re-scaling the model's logits. Specifically, we employ the regulation starting at the 140th token for the LibriSpeech dataset and at the 190th token for the VoxPopuli setups, using a regulation factor of 1.01 for all setups.\nWe compare our models to the original Whisper, a fine-tuned Whisper, and a speculative decoding method with an assistant model. The assistant model generates candidate tokens, which are validated by the main model in a single pass, retaining a token only if all previous tokens match the main model's output. Specifically, we used the model from [5], which is available only for English, therefore we tested it solely on the LibriSpeech dataset. We did not apply an exponential decay length penalty to these models, as it did not enhance their performance. All speed evaluations were conducted on a single NVIDIA A10G GPU."}, {"title": "IV. RESULTS", "content": "We evaluated the speed performance and accuracy of Medusa-Linear and Medusa-Block models on the Librispeech dataset, as shown in Table II. The WER and CER for Medusa-Block fall between those of Whisper vanilla and fine-tuned Whisper, leaning closer to Whisper vanilla due to its reliance on the un-tuned base Whisper head. The Medusa-Linear model offers better speedup than Medusa-Block but shows some WER and CER degradation due to fine-tuning. The Whisper with assistant model achieves the best speedup performance while maintaining similar results to Whisper vanilla, as dictated by its evaluation process. The Medusa-Linear adds 18M parameters, Medusa-Block adds 42M, and the assistant model adds the largest amount of parameters, 119M.\nNext, we proceeded to evaluate our model on the transcribed subset of the VoxPopuli dataset, focusing on English, German, French, and Spanish. We train our models simultaneously across all these languages, using 500 hours for English, 243 hours for German, 179 hours for French, and 132 hours for Spanish. Again, the Medusa-Block model demonstrates WER and CER results that lie between those of Whisper Vanilla and Whisper Fine-tuned, with speedup ranging from 1.40 for English, the highest, to 1.23 for German, the lowest. The Medusa-Linear model demonstrates better speedup results, with a peak of 1.48 for English and 1.31 for German, but achieves lower accuracy overall. This discrepancy may be due to the highly imbalanced training data, which makes the Medusa head training challenging. In this context, it is notable that the performance of the fine-tuned Whisper model for Spanish is lower than the performance of the vanilla Whisper model."}, {"title": "B. Self-supervised", "content": "We explored our model's performance in a self-supervised setup with the VoxPopuli dataset, which includes 400,000 hours of unlabeled speech. We selected three languages Czech, Finnish, and Dutch which transcribed them using Whisper-Large with a beam size of 5. We filtered out 5% of examples with significant discrepancies between expected and actual transcript lengths and removed examples that appear in the test set of the transcribed VoxPopuli subset. This process resulted in 730 hours of transcribed data for Czech, 733 hours for Finnish, and 720 hours for Dutch. We trained both Medusa-Block and Medusa-Linear on this data and evaluated the models on the corresponding languages from the transcribed VoxPopuli subset. Results are presented in Table III. It can be seen that our models perform best in the Czech language. Notably, both Medusa-Block and Medusa-Linear achieve higher accuracy than Whisper on Czech, despite being trained on Whisper's output. In terms of speedup, our models achieve the best results for Czech and Finnish, with strong performance for Dutch as well. As in previous setups, Medusa-Linear delivers the best speed results and, unlike other setups, in this case, surpasses the Whisper vanilla model for both Czech and Finnish. In this setup, both Medusa-Linear and Medusa-Block perform least effectively for Dutch, the language with the fewest examples, showing lower WER and CER than the Whisper vanilla model and their lowest speed results. This behavior highlights the importance of data quantity and dataset balance for effective multi-language Medusa training.\nFigure 2 presents the speedup results by target token sequence length for our self-supervised setup. The efficiency of the Medusa heads improves with longer target sequences, reaching a plateau of around 130 tokens. The Medusa heads are able to fully utilize their capabilities in longer sequences."}, {"title": "C. Ablation", "content": "We evaluate the impact of the number of Medusa heads on the speed and accuracy of our Medusa-Linear model on the LibriSpeech dataset, as shown in Table IV. 10 Medusa heads offer the best speed, while 5 heads achieve the best WER and CER. With 15 and 20 heads, both speed and accuracy decline."}, {"title": "D. Conclusions", "content": "In this paper, we introduce Whisper-Medusa, a novel ASR approach that leverages Speculative Decoding. To implement this approach, we present two architectures: Medusa-Linear and Medusa-Block. Our findings demonstrate that Medusa-Block achieves WER and CER results comparable to the original Whisper model, with latency reductions between 20% to 68%, depending on the dataset. On the other hand, Medusa-Linear delivers greater latency reductions, ranging from 30% to 80%, but with some performance degradation. For future work, we aim to enhance the performance of Medusa-Linear, which is affected by imbalance distribution of the dataset, and develop a specialized beam search designed for the Medusa method."}]}