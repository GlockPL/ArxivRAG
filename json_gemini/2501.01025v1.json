{"title": "Towards Adversarially Robust Deep Metric Learning", "authors": ["Xiaopeng Ke"], "abstract": "Deep Metric Learning (DML) has shown remarkable successes in many domains by taking advantage of powerful deep neural networks. Deep neural networks are prone to adversarial attacks and could be easily fooled by adversarial examples. The current progress on this robustness issue is mainly about deep classification models but pays little attention to DML models. Existing works fail to thoroughly inspect the robustness of DML and neglect an important DML scenario, the clustering-based inference.\nIn this work, we first point out the robustness issue of DML models in clustering-based inference scenarios. We find that, for the clustering-based inference, existing defenses designed DML are unable to be reused and the adaptions of defenses designed for deep classification models cannot achieve satisfactory robustness performance. To alleviate the hazard of adversarial examples, we propose a new defense, the Ensemble Adversarial Training (EAT), which exploits ensemble learning and adversarial training. EAT promotes the diversity of the ensemble, encouraging each model in the ensemble to have different robustness features, and employs a self-transferring mechanism to make full use of the robustness statistics of the whole ensemble in the update of every single model. We evaluate the EAT method on three widely-used datasets with two popular model architectures. The results show that the proposed EAT method greatly outperforms the adaptions of defenses designed for deep classification models.", "sections": [{"title": "Introduction", "content": "Deep Metric Learning (DML) aims to learn a distance metric over objects using deep neural networks. Powered by the great success of deep learning techniques, DML has demonstrated remarkable performance in many critical domains, including face verification (Hu, Lu, and Tan 2014; Schroff, Kalenichenko, and Philbin 2015; Lu, Hu, and Tan 2017; Liu et al. 2017), pedestrian re-identification (Shi et al. 2015; Xiao et al. 2017; Wojke and Bewley 2018; Huang et al. 2021), representation learning (Bengio, Courville, and Vincent 2013; Wang et al. 2015; Qiao et al. 2019; He et al. 2020), few-shot learning (Ravi and Larochelle 2016; Sung et al. 2018; Kim et al. 2019; Wang et al. 2020), etc.\nThe issue of adversarial examples is a thorn for deep neural networks. Szegedy et al. (2014) have first shown the robustness vulnerability of deep neural networks. They found that adding a specially crafted imperceptible perturbation on a natural image could greatly affect the prediction of a deep classification model. These perturbed images are called adversarial examples. Since then, there has been an arms race between attackers and defenders on this issue. However, this arms race is mainly about deep classification models, leaving DML models largely unexplored.\nThere are few existing works focus on the robustness of DML models (Bai et al. 2020; Panum et al. 2021), but those works failed to cover all the important applications of DML models. The use of trained DML models can be categorized into template matching and clustering. As to template matching, for a given input sample, the DML model extracts an embedding vector and then computes the distance (e.g, 12-norm) from that vector to each template embedding. The label of the template with the shortest distance is used as the prediction result (Shi et al. 2015; Xiao et al. 2017; Wojke and Bewley 2018; Huang et al. 2021). As for clustering, there is no prior-known template embedding. Clustering is performed over the embeddings of a set of unlabeled samples (Oh Song et al. 2017; Wang et al. 2017; Chen and Deng 2018; Duan et al. 2018; Chen and Deng 2019a,b; Dutta, Harandi, and Sekhar 2020; Huang12 et al. 2020; Hu et al. 2021). Existing works (Bai et al. 2020; Panum et al. 2021) all fall into the template matching category but neglect the clustering-based use. Defending against adversarial examples for the clustering-based scenarios is much harder than that of template matching scenarios. For template matching scenarios, in each distance computing, the attacker has the chance to manipulate one input term, the embedding vector, by adding perturbations on the corresponding sample, but does not have the chance to modify the other input term, the template embedding, which is pre-calculated and fixed during the inference. However, in each distance computing of the clustering scenario, the attacker has the chance to manipulate both the two input terms by adding perturbations on the two corresponding samples, and that means the attacker has a bigger space to exploit, making the defense much harder.\nAlso, existing defenses designed for the template matching scenarios are not suitable for the more challenging clustering scenarios. Existing defenses adapted adversarial training methods design for deep classification models to DML scenarios. Since those methods are designed without considering clustering scenarios, their adversarial training processes require labeled templates that are unavailable in the clustering scenario, making those methods unsuitable.\nThe design principles of defenses designed for deep classification models are unsuitable for the scenario considered by us either because DML models have unstable label vectors and their embedding spaces are high-dimensional (Details will be covered in the Problem Formulation Section). Those results suggest that adaptions cannot achieve strong robustness for the clustering scenarios.\nIn this paper, we propose a new method, the Ensemble Adversarial Training (EAT), to enhance the adversarial robustness of DML. EAT fuses ensemble learning and adversarial training considering the training characteristics of DML. Instead of exploiting the uninterpretable high-dimensional embedding space (Pang et al. 2019), to promote the diversity of the ensemble, EAT leverages the training data arrangement, which is not only lightweight but also friendly to the training of DML models. EAT also employs a self-transferring mechanism to distribute the robustness statistics of the whole ensemble to each model in the ensemble to further enhance the model robustness.\nWe evaluate the proposed method on three mostly used DML datasets (CUB200 (Welinder et al. 2010), CARS196 (Krause et al. 2013), and In-Shop (Liu et al. 2016)) with two popular models (MobileNet-V2 (Sandler et al. 2018) and BN-Inception (Ioffe and Szegedy 2015)), using the state-of-the-art metric learning design. And the results show that our method greatly outperforms the defenses which are adapted from methods designed for deep classifcation models.\nThe contributions of this paper are summarized below:\n\u2022 To the best of our knowledge, we are the first to point out the threat of adversarial examples in the clustering-based inference of DML models, and show that the adaptions of defenses originally designed for deep classification models cannot work well for this DML scenario.\n\u2022 We propose the EAT method as a defense for enhancing the robustness of DML models used in clustering scenarios. The EAT method promotes the model diversity of the ensemble by exploiting the training data arrangement, reducing the chance that all the models in the ensemble are simultaneously fooled by one sample. Additionally, the EAT method also uses a self-transferring mechanism to make full use of the ensemble's robustness statistics.\n\u2022 We evaluate the EAT method on three popular datasets (CUB200, CARS196, and In-Shop) with two models (MobileNetV2 and BN-Inception). Our results show that our EAT method greatly outperforms other methods adapted from defenses originally designed for deep classification tasks in terms of model robustness while introducing a negligible performance drop on the original DML task.\nThe rest of this paper is organized as follows. First, we introduce related works and some background knowledge in the Related Works Section. Then, we formalize our problem and discuss adapted forms of famous defenses originally designed for deep classification tasks (Problem Formulation Section). Next, we describe our proposed EAT method in the Proposed Approach Section. Finally, we show our experimental settings and evaluation results in the Experiments Section."}, {"title": "Related Works", "content": "DML aims to learn a distance representation for each input through deep neural networks (Parkhi, Vedaldi, and Zisserman 2015; Schroff, Kalenichenko, and Philbin 2015). Similar inputs should produce close representations under the same metric learning model. The key of DML is the loss function, which is evaluated using a tuple consisting of samples from the same class and samples from different classes (The formal definitions will be covered in the Problem Formulation Section). There are many famous loss designs for metric learning, including the contrastive loss (Bromley et al. 1994; Chopra, Hadsell, and LeCun 2005; Hadsell, Chopra, and LeCun 2006) and the triplet loss (Schroff, Kalenichenko, and Philbin 2015).\nThe contrastive loss and the triplet loss are pair-based losses, and those kinds of losses need to sample data and build tuples in the model training. The sampling operation in pair-based loss introduces high complexity. To reduce the complexity of the pair-based loss, the proxy-based methods (Aziere and Todorovic 2019) are proposed. The proxy-based methods always maintain proxies to represent each class, and the corresponding loss is computed using those proxies and real training samples. Also, Kim et al. (2020) propose a new proxy-based loss called PAL by making a combination of the SoftTriple Loss (Qian et al. 2020) and the Proxy-NCA Loss (Movshovitz-Attias et al. 2017). In this paper, we select the PAL which is the state-of-the-art as our deep metric loss.\nInitially, Szegedy et al. (2014) found that deep neural networks can be fooled by some specific perturbations on the input. Goodfellow, Shlens, and Szegedy (2015) then proposed Fast Gradient Sign Method (FGSM). FGSM exploits the sign of gradients resulted from the backpropagation to construct perturbations. Papernot et al. (2015) designed the Jacobian-based Saliency Map Attack (JSMA) through a saliency map based on the gradient. Carlini and Wagner (2017) developed the C&W attack. That attack alleviates the linear property in the optimization which is used for searching the perturbation. Madry et al. (2019) proposed Projected Gradient Method (PGD) to further improve the attack performance by the project operation.\nIn this work, we follow common practice (Pang et al. 2019; Pang, Xu, and Zhu 2020) and select the PGD attack to evaluate the robustness of deep learning models.\nThere are many defenses proposed to improve the adversarial robustness of the classification learning model against adversarial examples. Those defenses can be categorized as training stage defenses (Goodfellow, Shlens, and Szegedy 2015; Zhang et al. 2018; Verma et al. 2019; Madry et al. 2019) and inference stage defenses (Xie et al. 2017; Pang, Xu, and Zhu 2020).\nMadry et al. (2019) propose a defense, the Adversarial Training (AT), which dynamically generates adversarial examples during the model training and uses those generated samples as training samples. AT-based methods have shown superior performance over other kinds of defenses on defending against adversarial examples. Lamb et al. (2019) propose a defense called Interpolated Adversarial Training (IAT) which combines the Mix-Up methods with AT methods. Pang, Xu, and Zhu (2020) propose a defense based on the Mix-Up training paradigm. In the model training, they directly perform linear combinations to generate new training samples instead of launching adversarial attacks as AT does.\nInference stage defenses. Guo et al. (2017), Xie et al. (2017) and Raff et al. (2019) propose methods based on the similar design principle that performs a non-linear transformation on the input of deep learning models.\nThere are few works that are designed for defending DML models against adversarial examples (Bai et al. 2020; Panum et al. 2021). Those works only consider the template matching use of DML models (Recall the template matching based DML inference and clustering-based inference described in the Introduction Section). The defense methods of those works follow an adversarial training manner and require labeled templates for generating adversarial examples in the model training. However, there is no template in the clustering-based inference of DML models, and that suggests those defenses cannot be used for the clustering-based inference."}, {"title": "Problem Formulation", "content": "We denote the DML model as $F(x;\\theta)$, which takes an m-dimensional sample $x \\in R^m$ as the input and $\\theta$ as model parameters. The output of $F(.)$ is an embedding vector with d dimensions ($F(x;\\theta) = \\hat{y} \\in R^d$).\nWe denote the training dataset and the test dataset as $D_{train} = \\{(x_1,y_1), (x_2,y_2), ...,(x_N,y_N)\\}$ and $D_{test} = \\{(x_1,y_1), (x_2,y_2), ..., (x_M,y_M)\\}$, respectively. N is the number of training samples and M is the number of test samples. We represent the adversarial example of a specific input x as $x_{adv} = x + \\delta$, where $\\delta$ is the perturbation computed by the attacker and usually has small amplitude. Following the same choice in (Pang, Xu, and Zhu 2020; Zhang et al. 2018; Madry et al. 2019; Verma et al. 2019), in this paper, we use $l_p$-norm to bound $\\delta$ and consider the $\\delta$ s.t. $|\\delta|_p \\leq \\epsilon$ as valid perturbations.\nBefore introducing the formal definition of the problem, we first review the definition for defending against adversarial examples for traditional classification tasks, which can be written as:\n$\\min_{\\theta} E_{(x,y)\\sim D}[\\max_{x_{adv} \\in S(x,\\epsilon)} L(C(x_{adv}; \\theta),y)]  \\qquad (1)$\n$S(x, \\epsilon)$ is the set of possible adversarial examples generated using x as the starting point for attacking classification model $C(.)$. And $L(\u00b7)$ is the loss function (e.g. cross-entropy loss) that evaluates the distance between the one-hot label (y) of x and the output of classification model $C(\u00b7)$ when its input is $x_{adv}$. Given an input x, Equation 1 encourages $C(.)$ to output the correct label for all $x_{adv} \\in S(x, \\epsilon)$, and thus making sure that the attacker can hardly find the small perturbation $\\epsilon$ that can fool the classifier.\nHowever, Equation 1 cannot be used for DML because there is no explicit label (y) for the data samples in DML tasks. DML is designed to model the distance over data samples. Specifically, DML extracts embedding vectors from input data, and then treats data samples whose embedding vectors have small distances as the same class, while considers data samples whose embedding vectors have large distances as different classes. The label vector of a data sample is usually the centroid of the class it belongs to in the embedding space and is unclear untill the training is finished.\nThe loss function of DML always depends on several samples instead of one sample, and its general form can be well represented by the triplet loss (Schroff, Kalenichenko, and Philbin 2015):\n$L_{metric}(F(x), F(x^+), F(x^-)) = $|F(x) \u2013 F(x^+)| + max(0, m \u2212 |F(x) \u2013 F(x^-)|)  \\qquad (2)$\nwhere x, x+ are data samples of the same class while x\u00af is from another class, and m is a constant number. Similarly to Equation 1, we can write the problem of defending against adversarial examples for DML as:\n$\\min_{\\theta} E_{(x,x^+,x^-)\\sim D}[\\max_{x_{adv}, x^+_{adv}, x^-_{adv} \\in S(x,x^+,x^-,\\epsilon)}  L_{metric} (F(x_{adv}; \\theta), F(x^+_{adv}; \\theta), F(x^-_{adv}; \\theta))]  \\qquad (3)$\nwhere $S(x,x^+,x^-,\\epsilon)$ is the abbreviation of $\\{S(x, \\epsilon), S(x^+, \\epsilon), S(x^-,\\epsilon)\\}$."}, {"title": "Previous Defenses", "content": "In this section, we will introduce the definition of four kinds of representative defenses designed for classification models and analyse whether or not their design principles fit for DML. We give adapted forms of these defenses for DML. We also empirically show that those defenses are unsuitable for DML tasks in the Experiments Section.\n(Madry et al. 2019). AT improves the adversarial robustness by introducing adversarial examples into the training phase. At each training iteration, AT generates adversarial examples based on the current model and includes those examples into the training dataset. AT minimizes the empirical risk as:\n$\\min_{\\theta} \\frac{1}{|D|} \\sum_{(x,y) \\in D} L(C(x_{adv}), y) \\qquad (4)$\nwhere $x_{adv}$ is generated by launching adversarial attacks. Taking the most powerful and representative attack PGD (Madry et al. 2019) as an example, generating adversarial examples can be described as:\n$x'^{(t+1)}_{adv} = Proj_{x+S}(x^{(t)}_{adv} + \\alpha sgn(\\triangledown L(C(x^{(t)}_{adv}), y))) \\qquad (5)$\nwhere $Proj_{x+S}(z)$ is the projection function which can project z into the space x + S and S is the possible adversarial example space of x.\nFor DML, Equation 4 and 5 cannot be directly used because there is no fixed label vector y during the model training. To tackle this problem, we use $F(\\cdot)$ as the replacement of y and use $L_{metric}(\\cdot)$ and $F(\\cdot)$ in place of $L(\\cdot)$ and $C(\\cdot)$, respectively. It is worth noting that the dynamic \u201clabel\" F(x) could make the training hard to converge.\nMix-Up Training (Zhang et al. 2018). Mix-Up Training methods have shown great performance in improving the adversarial robustness of deep neural networks. These kinds of methods have lower complexity compared to the AT-based methods. The target of the Mix-Up Training methods can be written as:\n$\\min_{\\theta} \\frac{1}{m} \\sum_{i=1}^{m} L(C(C(x_i), y_i) \\qquad (6)$\nwhere $x_i = \\lambda x_{i0} + (1 \u2212 \\lambda)x_{i1}, y_i = \\lambda y_{i0} + (1 \u2013 \\lambda)y_{i1}, \\lambda ~ Beta(a, a)$ and m is the number of training samples. Mix-Up Training methods exploit the linear combination of training sample instead of generating adversarial examples in the model training, which obtains lower computational complexity.\nTo make Mix-up training available for DML, we propose to replace the \u1ef9i in Equation 6 as $F(x_{i0}) + (1 \u2212 \\lambda)F(x_1)$, and replace the $L(\\cdot)$ and $C(\\cdot)$ with $L_{metric}(\\cdot)$ and $F(\\cdot)$, respectively. Since the new label vector $\u1ef9_i = F(x_{i0}) + (1 \u2212\\lambda)F(x_1)$ changes as the update of $F(\\cdot)$ and is highly dynamic, the training could be greatly effected and be hard to converge.\nInterpolated Adversarial Training (IAT) (Lamb et al. 2019). IAT is a fusion of the Mix-Up Training and AT. The target of IAT can be written as:\n$\\min_{\\theta} \\frac{1}{m} \\sum_{i=1}^{m} L(C(C(x_i), y_i) + \\frac{1}{m} \\sum_{i=1}^{m} L(C(g(x_i)), y_i) \\qquad (7)$\nwhere $g(x_i) = Adv(x_{i0}) + (1 \u2212 \\lambda)Adv(x_{i1}), Adv(\u00b7)$ is an arbitrary adversarial example attack and $\\lambda ~ Beta(a, a)$. We design to adapt IAT to DML by using the same method used in adapting Mix-Up training. Due to the similar design, the IAT method may also have the convergence issue as Mix-up training does under DML scenarios.\n(Zhang et al. 2019). TRADES is a defensive method that exploits the trade-off between robustness and accuracy. The target of TRADES can be written as:\n$\\min_{\\theta} \\frac{1}{m} \\sum_{i=0}^{m}(L(C(x_i), y_i) +  \\cdot L(C(Adv(x_i)), C(x_i))) \\qquad (8)$\nwhere $Adv(\u00b7)$ is an arbitrary adversarial example attack and $\\lambda > 0$ is a hyper-parameter weighting the importance of the two terms. TRADES has demonstrated great performance on classification tasks, but it still relies on the fixed one-hot target vector, which may lead to convergence issues when it is adapted to DML scenarios. We can naturally change the first loss term to the metric loss and replace the C(.) with F(\u00b7) to enable this method for DML.\n(Pang et al. 2019) propose a defense by enlarging the diversity of the ensemble to improve the adversarial robustness. They define the ensemble diversity as $E_D = det(M^TM)$ where M = $(F^{(1)}(x), F^{(2)}(x), ..., F^{(K)}(x)) \\in R^{(L\u22121)\\times K}$ and $F^{(i)}$ is the order-preserving prediction of the i-th classifier on the input x. The training goal of their defense is to maximize the diversity term $E_D$ to 1. However, this training goal could be difficult to achieve when the output of $F^{(i)}$ is high-dimensional. Unfortunately, DML models usually have a large embedding space to obtain better performance, making that ensemble diversity design unsuitable.\nPrevious defenses cannot work well for DML. We conduct experiments (in the Experiments Section) to verify the effectiveness of those adapted defenses. The results show that defense principles designed for deep classification models cannot work well in DML scenarios which suggests that we need new defensive designs dedicated for DML tasks."}, {"title": "Proposed Approach", "content": "In this section, we will propose a new method, the Ensemble Adversarial Training (EAT), for defending against adversarial examples to improve the robustness of DML models.\nAs discussed in the Problem Formulation Section, defensive methods designed for classification models cannot be directly used for DML and their adapted forms also cannot work well because of unstable label vectors and the high-dimensional issue.\nTo avoid those issues, we propose a new method based on ensemble training and adversarial training. Different from the method proposed by Pang et al. (2019), we leverage the distribution difference of training data to promote the diversity of the ensemble instead of using the det(\u00b7) operation that takes model output vectors as inputs. Additionally, we construct a self-transferring mechanism that aims to further enhance the robustness of each model in the ensemble by considering the robustness statistics of the whole ensemble.\nAlgorithm 1 shows our overall design. We randomly split the training set into N parts and the combinations that consists of N-1 parts are used as candidate training sets. Then, we assign a unique candidate training set to each model in the ensemble as its training set. This design is to promote the diversity of the ensemble. Next, we start the model training that includes a normal metric learning part and an adversarial training part.\nAs to the normal metric learning part, each model evaluates its loss on its training set normally (Line 6 in Algorithm 1). As for the adversarial training part, we use a self-transferring mechanism. Specifically, for the training of each model in the ensemble, we generate adversarial examples on the whole ensemble instead of that single model (Line 7) and those generated examples are later used in the training of that model (Line 8). This design shares the robustness statistics of the whole ensemble with each model and could further enhance the model robustness.\ngen(\\cdot,\\cdot,\\cdot) in Line 7 of Algorithm 1 can be formulated as:\n$gen(D,F,\\epsilon) = \\{p(x, F, \\epsilon)|x \\in D\\} \\qquad (9)$\n$F = (F^1, F^2, ..., F^N)$\n$p(\\cdot,\\cdot,\\cdot)$ is a variant of the PGD attack and its iterative form can be written as:\n$x'^{(t+1)}_{adv} = clip (Proj_{x+S}(x^{(t)}_{adv} + sgn(SG)), \\epsilon) \\qquad (10)$\n$SG = \\sum_{i=1}^{N} \\triangledown_x L(F^{(i)}(x^{(t)}_{adv}), F^{(i)}(x))$\nwhere $x^{(0)}_{adv} = x, 0 < t < T$, T is the maximum number of attack iterations, and SG denotes the sum of all the gradients. Only the samples generated in the last iteration are kept ($p(x, F, \\epsilon) = x^{(T)}_{adv}$). Model inference. After the EAT training, we will obtain a bunch of models. For the model inference, we need a method that not only leverages the knowledge of the whole ensemble but also is friendly to model robustness. Here, we denote the output of the ensemble as (e1, e2, ..., en) for a given input x. A possible choice for model inference is to average all the output vector of the ensemble as the new output, i.e.$\\frac{1}{N}\\sum_{i=1}^{N} e_i$. This method is simple, though, the drawback is that the averaging operation is differentiable which might give the attacker an advantage for launching attacks. So we use an undifferentiable voting mechanism for the inference. Specifically, for a given input x, we count the predicted label of each model in the ensemble and select the label that appears most frequently as the prediction result."}, {"title": "Experiments", "content": "In this work, we consider the adversarial example attack in the DML tasks under the white-box setting (which means the attacker knows the structure and the parameters of the deep learning model). In this section, we first evaluate our EAT defense on three widely used datasets (CUB200, CARS196, and In-Shop) with two popular models (MobileNetV2 and BN-Inception). We then analyze the performance under various settings.\nWe evaluate our methods on CUB200 (Welinder et al. 2010), CARS196 (Krause et al. 2013) and In-Shop (Liu et al. 2016). CUB200 is an image dataset with photos of 200 bird species. For CUB200, we select its first 100 classes which contain 2944 images as our training set and other 3089 images of the other classes as our test set. CARS196 is an image dataset with 196 classes of cars. For the CARS196, we choose the 8054 images of its first 98 classes as our training set and the other 8131 images are used for testing. For the In-Shop, we use its first 3997 classes which contain 25882 images for training and the other 28760 images are used for testing. The test set of the In-Shop consists of three parts: the training part, the query part and the gallery part. The training part contains 25882 images. The query part contains 14218 images of 3986 classes and the gallery part has 12612 images of 3985 classes. In this work, we only select the query part as the test data to analyze the robustness like the test part of CUB200 and CARS196."}, {"title": "Models and Metric Learning Loss", "content": "In this work, we select the MobileNetV2 (Sandler et al. 2018) and the BN-Inception (Ioffe and Szegedy 2015) as the deep learning model to train and test. In every experiment, we all use the corresponding pre-trained model on the ImageNet (Deng et al. 2009) as the initial model before the training phase. For the metric learning loss, since the PAL loss (Kim et al. 2020) achieves the SOTA performance, we choose it as the metric learning loss, i.e. $L_{metric} = L_{pal}$ .\nIn the evaluation phase, we exploit three metrics (Recall, F1-Score, and NMI) to evaluate the performance of the deep learning model in DML tasks. The form of the recall value can be written as:\n$Recall@k = \\frac{TP}{TP + FN} \\qquad (11)$\nwhere TP is the true-positive counts and the FN is the false-negative counts in top-k predictions. The form of the F1-Score is:\n$F1-Score = \\frac{2TP}{2TP + FN + FP} \\qquad (12)$\nwhere FP is the false-positive counts. The F1-Score is the combination of recall and precision. The NMI value is the Normalized Mutual Information. The form of the NMI can be written as:\n$NMI(\\Omega, C) = \\frac{2I(\\Omega, C)}{H(\\Omega) + H(C)} \\qquad (13)$\nwhere $\\Omega$ is the class label, C is the cluster label, I(\u00b7, \u00b7) is the mutual information function and H(\u00b7) is the entropy function.\nDuring the training step, we exploit the AdamW (Loshchilov and Hutter 2017) optimizer which has the same update process of the Adam (Kingma and Ba 2014) yet decays the weights separately to perform the backpropagation. Our models are trained for 200 epochs with the initial learning rate $10^{-4}$ on three datasets. We apply the learning rate scheduler to adjust the learning rate during the training phase. During the training phase, we set the PGD attack with $\\epsilon = 16/255$ and the iteration number is set as 10. In the test phase, we set $\\epsilon = 8/255$ and the iteration number is also set as 10. As for the implementation, our code is based on the open-source code of Roth et al. (2020) and Pang, Xu, and Zhu (2020)."}, {"title": "Result Analysis", "content": "The adversarial robustness comparison of all defenses. As is shown in Table 1 to Table 4, five kinds of defenses all obtain great performance under the clean mode (i.e. without any attacks). These defenses all cause a little drop of all metrics after the training phase. The AT method presents minimum performance drop under the clean mode and the Mix-Up method shows maximum drop without any attacks.\nAs for the adversarial robustness of these defenses, the Mix-Up method and the IAT method fail under the PGD attack with 10 iterations with a huge drop of all the metrics. Our proposed EAT defense presents the SOTA performance under the PGD attack. Especially on the recall metric, our EAT defense significantly outperforms other defenses.\nThe adversarial robustness under different iteration numbers of the attack. As is shown in Figure 2, our proposed EAT defense presents the SOTA adversarial robustness than other defenses under the PGD attack with different iteration numbers. Under the PGD attack, the Mix-Up method is close to the No Defense. That means the effect of the Mix-Up method is small and it is not suitable for applying that kind of method in the DML tasks. The F1-Score and the Recall@1 decrease when we increasing the iteration number of the PGD attack but there are some fluctuations due to the randomness.\nThe robustness under different attack budgets $\\epsilon$ settings. We evaluate our EAT defense in comparison with the other defenses under different $\\epsilon$ settings (8/255, 12/255, 16/255, 20/255, 24/255, 28/255, and 32/255). As is shown in Figure 3, the EAT defense still outperforms other defenses. The F1-Score and the Recall@1 decrease when increasing the $\\epsilon$ of the PGD attack. There are also some fluctuations due to the randomness of the iteration step.\nThe adversarial robustness of the individual model in the whole ensemble. As is shown in Figure 4, every individual model of the ensemble is still more robust than other defenses and it is better to construct the ensemble and apply our voting mechanism for achieving higher robustness."}, {"title": "Ablation Experiments", "content": "The comparison of the Naive Ensemble Method As is shown in Table 3, we perform the naive ensemble method (i.e. Training three models individually and construct a simple ensemble through the voting mechanism) on CUB200. Our EAT method presents better robustness than the naive ensemble method.\nThe robustness gain of the split mechanism As is shown in the Table 3, we also analyze the gain of our split mechanism. Our split mechanism brings the 9.13% robustness improvement on CUB200."}, {"title": "Conclusion", "content": "In this work, we analyze the adversarial robustness of DML models when they are used in clustering scenarios. We find that the simple adaptions of defenses designed for deep classification models, including AT, Mix-Up, IAT, and TRADES, cannot work well for DML models. To defend against adversarial examples, we propose a new method, the Ensemble Adversarial Training (EAT), which takes advantage of ensemble learning and adversarial training. EAT promotes the diversity of the ensemble to make the adversarial training more powerful by exploiting dataset arrangement, and employs a self-transferring mechanism to further improve the adversarial robustness of the ensemble. We implement and evaluate our EAT defense on three popular datasets with two commonly-used datasets. The experimental results show that EAT greatly outperforms defenses that are simply adapted from defenses designed for deep classification models."}]}