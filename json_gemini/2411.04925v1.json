{"title": "STORYAGENT: Customized STORYTELLING VIDEO\nGENERATION VIA MULTI-AGENT COLLABORATION", "authors": ["Panwen Hu", "Jin Jiang", "Jianqi Chen", "Mingfei Han", "Shengcai Liao", "Xiaojun Chang", "Xiaodan Liang"], "abstract": "The advent of AI-Generated Content (AIGC) has spurred research into automated\nvideo generation to streamline conventional processes. However, automating\nstorytelling video production, particularly for customized narratives, remains chal-\nlenging due to the complexity of maintaining subject consistency across shots.\nWhile existing approaches like Mora and AesopAgent integrate multiple agents\nfor Story-to-Video (S2V) generation, they fall short in preserving protagonist\nconsistency and supporting Customized Storytelling Video Generation (CSVG).\nTo address these limitations, we propose StoryAgent, a multi-agent framework\ndesigned for CSVG. StoryAgent decomposes CSVG into distinct subtasks assigned\nto specialized agents, mirroring the professional production process. Notably, our\nframework includes agents for story design, storyboard generation, video creation,\nagent coordination, and result evaluation. Leveraging the strengths of different\nmodels, StoryAgent enhances control over the generation process, significantly\nimproving character consistency. Specifically, we introduce a customized Image-\nto-Video (I2V) method, LoRA-BE, to enhance intra-shot temporal consistency,\nwhile a novel storyboard generation pipeline is proposed to maintain subject con-\nsistency across shots. Extensive experiments demonstrate the effectiveness of our\napproach in synthesizing highly consistent storytelling videos, outperforming state-\nof-the-art methods. Our contributions include the introduction of StoryAgent, a\nversatile framework for video generation tasks, and novel techniques for preserving\nprotagonist consistency.", "sections": [{"title": "1 INTRODUCTION", "content": "Storytelling videos, typically multi-shot sequences depicting a consistent subject such as a human,\nanimal, or cartoon character, are extensively used in advertising, education, and entertainment.\nProducing these videos traditionally is both time-consuming and expensive, requiring significant\ntechnical expertise. However, with advancements in AI-Generated Content (AIGC), automated video\ngeneration is becoming an increasingly researched area, offering the potential to streamline and\nenhance traditional video production processes. Techniques such as Text-to-Video (T2V) generation\nmodels and Image-to-Video (I2V) methods enable users to generate corresponding video outputs simply\nby inputting text or images.\nWhile significant advancements have been made in video generation research, automating storytelling\nvideo production remains challenging. Current models struggle to preserve subject consistency\nthroughout the complex process of storytelling video generation. Recent agent-driven systems, such\nas Mora and AesopAgent, have been proposed to address\nStory-to-Video (S2V) generation by integrating multiple specialized agents, such as T2I and I2V\ngeneration agents. However, these methods fall short in allowing users to generate storytelling videos\nfeaturing their designated subjects, i.e., Customized Storytelling Video Generation (CSVG). The\nprotagonists generated from story descriptions often exhibit inconsistency across multiple shots.\nAnother line of research focusing on customized text-to-video generation like Dream Video and Magic-Me can also be employed to synthesize storytelling videos.\nThey first fine-tune the models using the data about the given reference protagonists, then generate the\nvideos from the story descriptions. Despite these efforts, maintaining fidelity to the reference subjects\nremains a significant challenge. As shown in Figure 1, the results of TI-AnimateDiff, DreamVideo,\nand Magic-Me fail to preserve the appearance of the reference subject in the video. In these methods,\nthe learned concept embeddings cannot fully capture and express the subject in different scenes.\nConsidering the limitations of existing storytelling video generation models, we explore the potential\nof multi-agent collaboration to synthesize customized storytelling videos. In this paper, we introduce\na multi-agent framework called StoryAgent, which consists of multiple agents with distinct roles that\nwork together to perform CSVG. Our framework decomposes CSVG into several subtasks, with each\nagent responsible for a specific role: 1) Story designer, writing detailed storylines and descriptions\nfor each scene.2) Storyboard generator, generating storyboards based on the story descriptions and\nthe reference subject. 3) Video creator, creating videos from the storyboard. 4) Agent manager,\ncoordinating the agents to ensure orderly workflow. 5) Observer, reviewing the results and providing\nfeedback to the corresponding agent to improve outcomes. By leveraging the generative capabilities of\ndifferent models, StoryAgent enhances control over the generation process, resulting in significantly\nimproved character consistency. The core functionality of the agents in our framework can be flexibly\nreplaced, enabling the framework to complete a wide range of video-generation tasks. This paper\nprimarily focuses on the accomplishment of CSVG.\nHowever, simply equipping the storyboard generator with existing T2I models, such as SDXL as used by Mora and AesopAgent, often fails to preserve inter-shot consistency, i.e.,\nmaintaining the same appearance of customized protagonists across different storyboard images.\nSimilarly, directly employing existing I2V methods such as SVD and Gen-2 leads to issues with intra-shot consistency, failing to keep the character's fidelity\nwithin a single shot. Inspired by the image customization method AnyDoor, we\ndevelop a new pipeline comprising three main steps-generation, removal, and redrawing-as the\ncore functionality of the storyboard generator agent to produce highly consistent storyboards. To\nfurther improve intra-shot consistency, we propose a customized I2V method. This involves integrat-\ning a background-agnostic data augmentation module and a Low-Rank Adaptation with Block-wise\nEmbeddings (LoRA-BE) into an existing I2V model to enhance the preservation\nof protagonist consistency. Extensive experiments on both customized and public datasets demon-\nstrate the superiority of our method in generating highly consistent customized storytelling videos\ncompared to state-of-the-art customized video generation approaches. Readers can view the dynamic"}, {"title": "2 RELATED WORK", "content": "Story Visulization. Our StoryAgent framework decomposes CSVG into three subtasks, including\ngenerating a storyboard from story descriptions, akin to story visualization. Recent advancements in\nDiffusion Models (DMs) have shifted focus from GAN-based and VAE-based frameworks to DM-based approaches.\nAR-LDM uses a DM framework to generate the current frame in an autoregressive\nmanner, conditioned on historical captions and generated images. However, these methods struggle\nwith diverse characters and scenes due to story-specific training on datasets like PororoSV and FlintstonesSV. For general story visualization, StoryGen iteratively synthesizes coherent image sequences using current captions and\nprevious visual-language contexts. AutoStory generates story images based\non layout conditions by combining large language models and DMs. StoryDiffusion introduces a training-free Consistent Self-Attention module to enhance consistency among\ngenerated images in a zero-shot manner.Additionally, methods like T2I-Adapter, IP-Adapter , and Mix-of-Show, designed to enhance customizable\nsubject generation, can also be used for storyboards. However, these often fail to maintain detail\nconsistency across sequences. To address this, our storyboard generator, inspired by AnyDoor, employs a pipeline of removal and redrawing to ensure high character consistency.\nImage Animation. Animating a single image, a crucial aspect of storyboard animation, has garnered\nconsiderable attention. Previous studies have endeavored to animate various scenarios, including\nhuman faces, bodies, and natural dynamics. Some methods have employed optical flow to model motion\nand utilized warping techniques to generate future frames. However, this approach often yields\ndistorted and unnatural results. Recent research in image animation has shifted towards diffusion\nmodels due to\ntheir potential to produce high-quality outcomes. Several approaches have been proposed to tackle\nopen-domain image animation challenges, achieving remarkable performance for in-domain subjects.\nHowever, animating out-domain customized subjects remains challenging, often resulting in distorted\nvideo subjects. To address this issue, we propose LoRA-BE, aimed at enhancing customization\ngeneration capabilities.\nAI Agent. Numerous sophisticated AI agents, rooted in large language models (LLMs), have emerged,\nshowcasing remarkable abilities in task planning and utility usage. For instance, Generative Agents\nintroduces an architecture that simulates believable human behavior, enabling\nagents to remember, retrieve, reflect, and interact. MetaGPT models a software\ncompany with a group of agents, incorporating an executive feedback mechanism to enhance code\ngeneration quality. AutoGPT and AutoGen focus on interaction\nand cooperation among multiple agents for complex decision-making tasks. Inspired by these agent\ntechniques, AesopAgent proposes an agent-driven evolutionary system for\nstory-to-video production, involving script generation, image generation, and video assembly. While"}, {"title": "3 STORYAGENT", "content": "As depicted in Figure 2, StoryAgent takes as inputs a prompt and a few videos of the reference\nsubjects, and employs the collaborative efforts of five agents: the agent manager, story designer,\nstoryboard generator, video creator, and observer, to create highly consistent multi-shot storytelling\nvideos. The workflow is segmented into three distinct steps: storyline generation, storyboard creation,\nand video generation.\nDuring storyline generation, the agent manager forwards the user-provided prompt to the story\ndesigner, who crafts a suitable storyline and detailed descriptions p = {p1,\u2026,pN} (where N\nrepresents the number of shots in the final storytelling video) outlining background scenes and\nprotagonist actions. These results are then reviewed by the observer or user via the agent manager,\nand the process advances to the next step once the observer signals approval or the maximum chat\nrounds are reached.\nThe second step focuses on generating the storyboard I = {I\u2081,\u00b7\u00b7\u00b7, IN}. Here, the agent manager\nprovides the story descriptions p and protagonist videos Vref to the storyboard generator, which\nproduces a series of images aligned with p and Vref. Similar to the previous step, the storyboard\nresults undergo user or observer evaluation until they meet the desired criteria. Finally, the story\ndescriptions p, storyboard Vref, and protagonist videos Vref are handed over to the video creator\nfor synthesizing multi-shot storytelling videos. Instead of directly employing existing models, as done\nby Mora, the storyboard generator and the video creator agents utilize a novel storyboard generation\npipeline and the proposed LoRA-BE customized generation method respectively to enhance both\ninter-shot and intra-shot consistency. In the subsequent section, we will delve into the definitions and\nimplementations of the agents within our framework."}, {"title": "3.1 LLM-BASED AGENTS", "content": "Agent Manager. Customized Storytelling Video Generation (CSVG) is a multifaceted task that\nnecessitates the orchestration of several subtasks, each requiring the cooperation of multiple agents to\nensure their successful completion in a predefined sequence. To facilitate this coordination, we intro-\nduce an agent manager tasked with overseeing the agents' activities and facilitating communication\nbetween them. Leveraging the capabilities of Large Language Models (LLM) such as GPT-4 and Llama , the agent manager selects the next agent in line. This\nprocess involves presenting a prompt to the LLM, requesting the selection of the subsequent agent\nfrom a predetermined list of available agents within the agent manager. The prompt, referred to as the\nrole message, is accompanied by contextual information detailing which agents have completed their\ntasks. Empowered by the LLM's decision-making prowess, the agent manager ensures the orderly\nexecution of tasks across various agents, thus streamlining the CSVG process.\nStory Designer. In order to craft captivating storyboards and storytelling videos, crafting detailed,\nimmersive, and narrative-rich story descriptions is crucial. To accomplish this, we introduce a story\ndesigner agent, which harnesses the capabilities of Large Language Models (LLM). This agent offers\nflexibility in LLM selection, accommodating models like GPT-4, Claude, and\nGemini. By prompting the LLM with a role message tailored to the story designer's\nspecifications, including parameters such as the number of shots (N), background descriptions, and\nprotagonist actions, the story designer generates a script comprising n shots with corresponding story\ndescriptions p = {p1,\u2026, pn}, ensuring the inclusion of desired narrative elements.\nObserver. The observer is an optional agent within the framework, and it acts as a critical evaluator,\ntasked with assessing the outputs of other agents, such as the storyboard generator, and signaling the\nagent manager to proceed or provide feedback for optimizing the results. At its core, this agent can\nutilize Aesthetic Quality Assessment (AQA) methods or the general Multimodal\nLarge Language Models (MLLMs), such as GPT-4 or LLaVA ,\ncapable of processing visual elements to score and determine their quality. However, existing MLLMs\nstill have limited capability in evaluating images or videos. As demonstrated in our experiments in\nAppendix A.5, these models cannot distinguish between ground-truth and generated storyboards.\nTherefore, we implemented the LAION aesthetic predictor as the core of\nthis agent, which can effectively assess the quality of storyboards in certain cases and filter out some\nlow-quality results. Nevertheless, current AQA methods remain unreliable. In practical applications,\nusers have the option to replace this agent's function with human evaluation or omit it altogether to\ngenerate storytelling videos. Since designing a robust quality assessment model is beyond the scope\nof this paper, we will leave it for future work."}, {"title": "3.2 VISUAL AGENTS", "content": "Storyboard Generator. Storyboard generation requires maintaining the subject's consistency across\nshots. It is still a challenging task despite advancements in coherent image generation for storytelling\nhave been made. To address this, inspired\nby AnyDoor, we propose a novel pipeline for storyboard generation that ensures\nsubject consistency through removal and redrawing, as shown in Fig. 3. Initially, given detailed\ndescriptions p = {p1,\u2026\u2026,pN}, we employ text-to-image diffusion models like StoryDiffusion\nto generate an initial storyboard sequence S = {$1,\u2026, SN}. During removal,\neach storyboard sn undergoes subject segmentation using algorithms like LangSAM, resulting in\nthe subject mask M = {m1,\u2026\u2026,mn}. For redrawing, a user-provided subject image with its"}, {"title": "Video Creator: LoRA-BE for Customized Image Animation.", "content": "Given the reference videos Vref, the\nstoryboard I, and the story descriptions p, the goal of the video creator is to animate the storyboard\nfollowing the story descriptions p to form the storytelling videos with consistent subjects of in Vref.\nTheoretically, existing I2V methods, such as SVD , and SparseCtrl , can equip the agent to perform this task. However, these methods still face significant\nchallenges in maintaining protagonist consistency, especially when the given subject is a cartoon\ncharacter like Miffy. Inspired by the customized generation concept in image domain, we propose a\nconcept learning method, named LoRA-BE, to achieve customized I2V generation.\nOur method is built upon a Latent Diffusion Model(LDM) -based I2V generation\nmodel, DynamiCrafter(DC). The modules in this method include a VAE encoder\nEi and decoder Di, a text encoder Er, an image condition encoder Ec, and a 3D U-Net architecture\nU with self-attention, temporal attention, and cross-attention blocks within. We first introduce the\ninference process of the valina DC. As shown in Figure 4, a noisy video zT \u2208 RF\u00d7C\u00d7h\u00d7w is\nsampled from Gaussian distribution N, where F is the number of frames, and C, h, w represent the\nchannel dimension, height, and width of the frame latent codes. Then the condition image In, i.e., the\nstoryboard in our task, is encoded by E and contacted with zy as the input of U-Net U. Additionally,\nthe condition image is also projected by the condition encoder E to extract image embedding. Similar\nto the text embedding extracted by the text encoder from the text prompt pn, the image embedding is\ninjected into the video through the cross-attention block inside the U-Net. The output er of U-Net\nwill be used to denoise the noisy video z\u012b following the backward process B of LDM. The denoising\nprocess for the n-th shot at step t can be written as:\nZ_{t-1} = B(U([z_t; E_i(I_n)], E_r(p_n), E_c(I_n)), z_t, t)  (1)\nwhere [;] means the concatenation operation along the channel dimension. We will drop off the\nsubscript n in the following content for simplicity.\nAlthough the reference image is encoded to provide the visual information of the reference protagonist,\nthe existing pre-trained DC model still fails to preserve the consistency of the out-domain subject.\nHence, we propose to enhance its customization ability of animating out-domain subjects by fine-\ntuning. Inspired by the conclusions of Mix-of-Show that fine-tuning the embedding\nof the new token, e.g., <Miffy>, helps to capture the in-domain subject, and fine-tuning to shift\nthe pre-trained model, i.e., LoRA , helps to capture out-domain identity, we enhance\nDC's customization ability from both aspects. Specifically, for each linear projection L(x) = Wx\nin the self-attention, cross-attention, and temporal attention module, we add a few extra trainable\nparameters A and B to adjust the original projection to L(x) = Wx+\u2206Wx=Wx+ BAx, thereby\nthe generation domain of DC is shifted to the corresponding new subject after training. Moreover, we\nalso train token embeddings for the new subject tokens. Unlike the Text Inversion (TI) method which trains an embedding and injects the same embedding in all the cross-attention"}, {"title": "4 EXPERIMENTS", "content": "Implementation Details. For storyboard generation, we employed AnyDoor as the redrawer and\nfine-tuned it to accommodate the new subject using the Adam optimizer with an initial learning rate\nof 1e-5. We selected 4-5 videos, each lasting 1-2 seconds, for every subject as reference videos,\nand conducted 20,000 fine-tuning steps. Regarding the training of the I2V model, we utilized\nDynamiCrafter (DC) as the foundational model. We trained only the parameters\nof LoRA and block-wise token embeddings (LoRA-BE) using the Adam optimizer with a learning\nrate of 1e-4 for 400 epochs. All experiments were executed on an NVIDIA V100 GPU.\nDatasets and Metrics. We employed two publicly available storytelling datasets, PororoSV and FlintstonesSV, which include both story scripts\nand corresponding videos, for evaluating our method. From PororoSV, we selected 5 characters,\nand from FlintstonesSV, we chose 4 characters as the customized subjects. For the training set,\nwe selected reference videos for each subject from one episode, simulating practical application\nscenarios. For the testing set, we curated 10 samples for each subject, each consisting of 4 shots\nhighly relevant to the subject. To evaluate our method on these datasets, we utilized reference-based\nmetrics such as FVD, PSNR, SSIM , and LPIPS . Additionally, to assess the generalization ability, we collected 8 other subjects from\nYouTube and open-source online websites to form an open-domain set. Story descriptions for this\nset were generated using ChatGPT. Since there is no ground truth for this set, we reported the\nresults on non-reference metrics as outlined in Liu et al. (2023), including Inception Score (IS),\ntext-video consistency (Clip-score), semantic consistency (Clip-temp), Warping error, and Average\nflow (Flow-score). Arrows next to the metric names indicate whether higher (\u2191) or lower (\u2193) values\nare better for that particular metric. For Flow-Score, the arrow is replaced with a rightwards arrow\n(\u2192) as it is a neutral metric."}, {"title": "4.1 EVALUATION ON PUBLIC DATASETS", "content": "Quantitative Results. The PororoSV and FlintstonesSV datasets comprise story descriptions and corresponding videos, serving as ground truth for\nevaluating storytelling video generation methods. During testing, we generate a storyboard with a\nconsistent background aligned with the ground-truth video. To achieve this, we use the first frame of\neach video with the subject removed as the initial storyboard. Subsequently, our storyboard generator\nredraws this initial storyboard to produce the final version. Finally, the generated storyboard is\nanimated by the video creator agent to create a video of the subject."}, {"title": "4.2 EVALUATION ON OPEN-DOMAIN SUBJECTS", "content": "Open-domain Dataset Results. In this experiment, we also qualitatively compare our method\nwith other CSVG methods, the video generation performance is shown in Figure 1. Due to the\nrecent work, StoryDiffusion , did not release the codes for video generation, we\ncompare its storyboard generation performance in Figure 6. For other T2V methods, TI-AnimateDiff"}, {"title": "4.3 USER STUDIES", "content": "We conducted a user study on the results of different methods on the open-domain dataset and\nthe Pororo dataset. We presented the results of different methods to the participants (They do not\nknow which method each video comes from) and asked them to rate five aspects on a scale of 1-5:\nInteR-shot subject Consistency (IRC), IntrA-shot subject Consistency (IAC), Subject-Background\nHarmony (SBH), Text Alignment (TA) and Overall Quality (OQ). More details of the user studes can\nbe seen in Appendix A.6."}, {"title": "4.4 ABLATION STUDIES", "content": "Effectiveness of RoLA-BE. One core contribution of this paper is the customized I2V generation.\nIn this section, we will assess the results with and without this component. We finetuned the image\ninjection module of DynamiCrafter (DC) with the reference videos to improve\nthe customization ability as the baseline. As shown in Table 5, without the proposed RoLA-BE, DC\nfails to preserve intra-shot consistency, and the score performance measuring the video quality and\nhuman perception decreases. The visualization results can be found in Appendix.A.4 In contrast, our\nmethod achieves better inter-shot and intra-shot consistency, while obtaining high-quality videos.\nThese results suggest that the proposed method is effective in animating customized subjects."}, {"title": "5 CONCLUSION", "content": "We introduce StoryAgent, a multi-agent framework tailored for customized storytelling video genera-\ntion. Recognizing the intricate nature of this task, we employ multiple agents to ensure the production\nof highly consistent video outputs. Unlike approaches that directly generate storytelling videos from\nstory descriptions, StoryAgent divides the task into three distinct subtasks: story description genera-\ntion, storyboard creation, and animation. Our storyboard generation method fortifies the inter-shot\nconsistency of the reference subject, while the RoLA-BE strategy enhances intra-shot consistency"}, {"title": "A APPENDIX", "content": "The outline of the Appendix is as follows:\n\u2022 More details of the agent scheduling process in Aagent Manager (AM).\n\u2022 More evaluations on public datasets;\nMore storytelling video generation results on public datasets;\n\u2022 More evaluations on open-domain subjects;\nMore storytelling video generation results on open-domain subjects;\n\u2022 More ablation studies;\nMore storytelling video generation ablation on public datasets;\n\u2022 The performance of Observer agent;\n\u2022 The details of user studies.\n\u2022 Social impact."}, {"title": "A.1 MORE DETAILS OF THE AGENT SCHEDULING PROCESS IN AM", "content": ""}, {"title": "A.2 MORE EVALUATIONS ON PUBLIC DATASETS", "content": "More Storytelling Video Generation Results on Public Datasets.\nAs mentioned before, existing I2V methods, such as SVD , and SparseC-trl , also can be used by our video creator to animate the storyboard I following the\nstory descriptions p to form the storytelling videos. To further indicate the benefits of the proposed\nStoryAgent, we also visualize the storytelling videos generation results on FlintstonesSV dataset. As\nshown in Figure 8, our StoryAgent with the proposed LoRA-BE can not only generate results closer\nto the ground truth but also maintain the temporal consistency of subjects better, compared with the\nresults generated by other methods."}, {"title": "A.3 MORE EVALUATIONS ON OPEN-DOMAIN SUBJECTS", "content": "More Storytelling Video Generation Results on Open-domain Subjects.\nComparing our method with SVD and TI-SparseCtrl ,\nwe also visualize more generated storytelling videos from story scripts on open-domain subjects,\nwhere the story descriptions are generated by our story designer agent. As shown in Figure 9 and\nFigure 10, TI- SparseCtrl fails to maintain consistency throughout all the shots where the subjects\nchange significantly in subsequent shots, such as the last shots on both of the two subjects. The\nproposed StoryAgent effectively maintain the temporal consistency between the referenced subjects\nthroughout the story sequences in details, such as the clothes of cartoon subjects like Kitty and the\nappearance of real-world subjects like the bird. Although SVD also performs well in maintaining\ntemporal consistency of the real-world bird in Figure 10, the movements of the bird are less able to\nfollow the text, while our method can produce more vivid videos of the subject."}, {"title": "A.4 MORE ABLATION STUDIES", "content": "More Storytelling Video Generation Ablation on Public Datasets."}, {"title": "A.7 SOCIAL IMPACT", "content": "Although storytelling video synthesis can be useful in applications such as education, and adver-\ntisement. Similar to general video synthesis techniques, these models are susceptible to misuse,\nexemplified by their potential for creating deep fakes. Besides, questions about ownership and copy-\nright infringement may also arise. Nevertheless, employing forensic analysis and other manipulation\ndetection methods could effectively alleviate such negative effects."}]}