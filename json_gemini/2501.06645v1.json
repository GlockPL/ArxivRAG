{"title": "FocalPO: Enhancing Preference Optimizing by Focusing on Correct Preference Rankings", "authors": ["Tong Liu", "Xiao Yu", "Wenxuan Zhou", "Jindong Gu", "Volker Tresp"], "abstract": "Efficient preference optimization algorithms such as Direct Preference Optimization (DPO) have become a popular approach in aligning large language models (LLMs) with human preferences. These algorithms implicitly treat the LLM as a reward model, and focus on training it to correct misranked preference pairs. However, recent work (Chen et al., 2024) empirically finds that DPO training rarely improves these misranked preference pairs, despite its gradient emphasizing on these cases. We introduce FocalPO, a DPO variant that instead down-weighs misranked preference pairs and prioritizes enhancing the model's understanding of pairs that it can already rank correctly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this by adding a modulating factor to dynamically scale DPO loss. Our experiment demonstrates that FocalPO surpasses DPO and its variants on popular benchmarks like Alpaca Eval 2.0 using Mistral-Base-7B and Llama-3-Instruct-8B. Additionally, we empirically reveals how FocalPO affects training on correct and incorrect sample groups, further underscoring its effectiveness.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning from human feedback (RLHF) played a crucial role in aligning large language models (LLMs) with human preferences. However, conducting RLHF with Proximal Policy Optimization (PPO) is computationally expensive. Therefore, recent works have studied more efficient approaches, such as Direct Preference Optimization (DPO) and its variants, which implicitly treat the language model itself as a reward model and directly optimizes it using preference datasets."}, {"title": "2 Background", "content": "Given an instruction-tuned language model \u03c0\u03b8, Direct Preference Optimization (DPO, Rafailov et al. (2023)) further optimizes it as an implicit reward model using a preference dataset. Let D = {(x(i), y(i)w,y(i)l} denote a preference dataset where x(i) \u2208 X is a prompt and (y(i)w,y(i)l) is a pair of answers, with the preference of y(i)w > y(i)l expressed by human or AI annotators. Under the assumption of Bradley-Terry model (Bradley and Terry, 1952), the human preference distribution p* can be approximated as:\np*(y(i)w > y(i)l | x(i)) = \u03c3(r*(x(i),y(i)w) \u2013 r*(x(i), y(i)l)),\nwhere \u03c3 and r*(x, y) are the sigmoid function and the latent reward model used to generate the ground-truth preference, respectively. To model this human preference, DPO uses a reparametrization trick to express it in terms of the optimal policy\n\u03c0* := argmax_\u03c0 Ex\u223cDEx, where\nr*(x, y) = Blog (\u03c0*(y|x)/\u03c0ref(y|x)) + Blog Z(x), where Z(x) is the partition function only based on x. Applying the above equations, the maximum likelihood estimation objective of DPO is:\nLDPO (\u03c0\u03b8; \u03c0ref) = -E(x,yw,yl)\u223cD [logp (yw > Yl | x)]\n= -E(x,yw,yl)\u223cD logo (Blog (\u03c0\u03b8(yw | x)/\u03c0ref(yw | x)) - Blog (\u03c0\u03b8(yl | x)/\u03c0ref(yl | x)))\n= -E(x,yw,yl)\u223cD logo (Blog (\u03c0\u03b8(yw | x)/\u03c0ref(yw | x)) - Blog (\u03c0\u03b8(yl | x)/\u03c0ref(yl | x))\nwhich emphasizes on learning responses pairs that the model struggle to rank (i.e., wrong reward estimates). However, recent work such as Chen et al. (2024) shows that DPO cannot learn to correct these response pairs, and a few other competitive variants such as SimPO (Meng et al., 2024) and ODPO (Amini et al., 2024) implicitly introduce a margin term that reduces such emphasis."}, {"title": "3 FocalPO: Focal loss inspired Preference Optimization", "content": ""}, {"title": "3.1 FocalPO loss", "content": "We propose FocalPO loss, which directly scales DPO's loss to reduce emphasis on learning response pairs the model struggles to rank and focuses on preference pairs with accurate implicit reward estimates. Inspired by Focal Loss (Lin et al., 2020) in vision tasks (i.e., adding a modulating factor (1 \u2013 p)^\u03b3 to the cross entropy loss), we replace the original cross-entropy loss in Eq. 2 with:\nLFocalPO (\u03c0\u03b8; \u03c0ref) = -E(x,yw,yl)\u223cD [(1 \u2212 p (yw > Yl | x))^-\u03b3-logp (yw > Yl | x)]\n\u2248 -E(x,yw,yl)\u223cD [p (yw > Yl | x)^\u03b3 log p (yw > Yl | x)] + constant,\nwhere \u03b3 \u2265 0 is a tunable focusing hyperparameter. Here we take negative index -\u03b3 to replace the original positive term \u03b3 in Focal Loss, in order to focus on the easier or correctly ranked preference pairs. To simplify the optimization process, we further approximate (1 \u2013 p)^-\u03b3 with the term p^\u03b3. Unless otherwise specified, we use Eq. 4 as FocalPO for the rest of the paper. Converse to (1-p)^\u03b3, a higher \u03b3 in p^\u03b3 leads to a less emphasis placed on hard negative samples."}, {"title": "3.2 Gradient analysis", "content": "To better understand the impact of modulating factor on the DPO loss, we perform a gradient analysis. The gradient of DPO with respect to \u03b8 is:\n\u2207\u03b8LDPO = \u2212E(x,yw,yl)\u223cD [\u03c3'(\u0176\u03b8(Yl) \u2013 \u0176\u03b8(Yw)) (\u2207\u03b8 log \u03c0(Yw|x) / Ve - \u2207\u03b8 log \u03c0(Yl|x))],\nwhere r\u03b8(y) = Blog (\u03c0\u03b8(y|x)/\u03c0ref(y|x)) is the reward implicitly defined by the language model \u03c0\u03b8 and reference model \u03c0ref. The term \u03c3(re(yl) \u2013 ro(Yw)) assigns higher weights when reward estimate is wrong. In comparison, the gradient of FocalPO is:\n\u2207\u03b8LFocalPO = -E(x,yw,yl)\u223cD [[\u03c3(re(yl) \u2013 ro(Yw))\u00b7 \u03c3'(re(Yw) \u2013 re(Yl)) + log \u03c3(re(Yw) \u2013 re(Yl))\u00b7\u03b3\u03c3'(re(Yw) \u2013 re(Yl))\u00b7 \u03c3(re(Yl) \u2013 re(Yw))] (\u2207\u03b8 log \u03c0(Yw|x)/ Ve - \u2207\u03b8 log \u03c0(Yl|x))\nWe illustrate the influence of each term in Fig. 3. The blue curve represents the gradient term from DPO, while the red and green curves correspond to the additional gradient terms introduced by our modulating factor. Compared to DPO, the new gradient terms from FocalPO assign lower weights to incorrect preference pairs. When combined, as shown in the black curve in Fig. 3, FocalPO forms"}, {"title": "4 Experimental setup", "content": "Model and training datasets We perform preference optimization on two representative models, Mistral-Base SFT (7B) and instruction-tuned Llama-3 (8B) . We perform perference learning on the UltraFeedback dataset for Mistral-Base, and on the Llama3-ultrafeedback-armorm dataset for Llama-3-Instruct. The former dataset is sampled from multiple LLMs and judged by GPT-4, and the later contains preference pairs generated by Llama-3-Instruct and judged by ArmoRM"}, {"title": "5 Results", "content": "Effectiveness of FocalPO Table 1 demonstrates that FocalPO surpasses DPO and its variants on Alpaca Eval 2.0. Notably, FocalPO improves upon SimPO, a highly performant alignment algorithm, without removing the reference model or addition-"}, {"title": "Focus learning correct/incorrect samples", "content": "FocalPO down-weighs incorrectly ranked instances that the model struggles to learn, and placing greater emphasis on correct ones compared to DPO. To examine the impact of this choice, we empirically compare two FocalPO settings against DPO: emphasize learning incorrect samples using \u03b3 = 1 in the factor (1 \u2013 p); and emphasize learning correct samples using \u03b3 = 0.05 in p\u03b3. We find that focusing on incorrect samples yields inferior performance compared to DPO, while focusing on correct samples outperforms DPO. This result underscores the effectiveness our method, which prioritizes on learning preference pairs with accurate (i.e., correct) implicit reward estimates."}, {"title": "6 Conclusion", "content": "In this paper, we propose FocalPO, a variant of DPO that dynamically scales the loss function to de-emphasize learning response pairs the implicit reward model struggles to rank, and to enhance model's understanding of pairs it can already rank correctly. We evaluate FocalPO on two popular chat benchmarks (Alpaca Eval 2.0 and MT-Bench) using two models (LLaMA-3 and Mistral) and demonstrate that FocalPO-trained models achieve the best performance in nearly all cases."}, {"title": "Limitations", "content": "More model family and hyperparameter settings As research line on preference optimization advances, a growing number of model-by-preference-dataset combination training settings and evaluation benchmarks have emerged. We did not conduct experiments on all of these settings. Besides, we only conduct experiments with hyperparameter search over a small range of our introduced hyperparameter \u03b3 in FocalPO, limited by compute resources."}, {"title": "Future Work", "content": "the integration of our proposed FocalPO into other types of offline preference optimization loss functions remains unexplored. For instance, SimPO demonstrates that removing reference policy and adding sequence length as a normalization factor significantly enhance the optimization performance. Our experimental results show that FocalPO outperforms SimPO even without these adjustments, leaving space for the investigation of the potential benefits of combining FocalPO with these factors."}, {"title": "Ethics statement", "content": "In this work, we focus on improving the effectiveness of preference learning algorithms to generate more helpful, coherent, and informative responses. Our experiments were based on publicly available models and alignment datasets. Despite our efforts to carefully choose our training data/model backbone, some malicious or harmful content may persist, leading to non-ideal model responses. To address this, we recognize the need to incorporate additional safety-focused datasets and commit to conducting more comprehensive evaluations on safety, harmfulness, and bias to improve the robustness and ethical alignment of future models. We do not condone the usage of our methods/models for any unlawful or morally unjust purposes."}, {"title": "A Training details", "content": "We perform preference optimization on two representative models, Mistral-Base (7B) 6 and instruction-tuned Llama-3 (8B) 7. For Mistral-Base (7B), we follow the official hyperparameters of zephyr for the implmentation of DPO and FocalPO: \u03b2 = 0.01, epoch as 1, batch size as 128 and learning rate as 5e-7. We perform the preference optimization on UltraFeedback dataset (Cui et al., 2023) 8. For instruction-tuned Llama-3 (8B), we follow the setting by SimPO. We perform the preference optimization on llama3-ultrafeedback-armorm dataset 9 using ArmoRM as the reward model. We use a batch size of 128 and a single training epoch and perform individual searches of the learning rates in the range of [3e-7, 5e-7, 6e-7, le-6] for each method, as suggested by SimPO. We tune the hyperparameter of \u03b3 in FocalPO in [0.05, 0.07] for both models to prevent possible overfitting of hyperparameters. Our code is implemented based on Transformer Reinforcement Learning (TRL) library"}, {"title": "B Related works", "content": "Several preference optimization loss functions have been proposed as alternatives to DPO. For instance, KTO proposes to directly maximize the utility of generations instead of maximize the likelihood of preferences. ORPO focuses on jointly optimizing SFT and preference optimization loss by adding the odds ratio term. SimPO removes the requirement of reference model, and introduces a target reward margin and a length-normalized factor. ODPO further modifies DPO to incorporates additional labeled reward values and also employs an instance-dependent margin term. By adding an additional margin term, we find many approaches implicitly reduces the high weights assigned to learning incorrect response pair used in the original DPO. In this work, we propose FocalPO inspired by Focal Loss from vision tasks, to explicitly reduces emphasis on learning response pairs that the model struggles to rank while prioritizing preference pairs with accurate implicit reward estimates."}, {"title": "C Qualitative analysis", "content": "We present a pair of answers generated by Llama-3-Instruct after training with DPO and FocalPO in Table 2 and 3, respectively. Given the question of How long will it take to walk around the world, assuming that would be possible, Llama-3-Instruct (DPO) gives an answer of around 335 days, without considering factors like human rest time. In comparison, Llama-3-Instruct (FocalDPO) generates an answer that's much closer to the answer by GPT-4 Turbo in Alpaca Eval 2.0, 1,169 days, and exactly aligned with the answer by 01, 1,002 days."}]}