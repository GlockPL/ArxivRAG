{"title": "Learned Ranking Function: From Short-term Behavior Predictions to Long-term User Satisfaction", "authors": ["Yi Wu", "Daryl Chang", "Jennifer She", "Zhe Zhao", "Li Wei", "Lukasz Heldt"], "abstract": "We present the Learned Ranking Function (LRF), a system that takes short-term user-item behavior predictions as input and outputs a slate of recommendations that directly optimizes for long-term user satisfaction. Most previous work is based on optimizing the hyperparameters of a heuristic function. We propose to model the problem directly as a slate optimization problem with the objective of maximizing long-term user satisfaction. We also develop a novel constraint optimization algorithm that stabilizes objective tradeoffs for multi-objective optimization. We evaluate our approach with live experiments and describe its deployment on YouTube.", "sections": [{"title": "1 INTRODUCTION AND RELATED WORK", "content": "Large video recommendation systems typically have the following stages:\n(1) Candidate Generation: The system first generates a short list of video candidates from a large corpus [6, 19].\n(2) Multitask Model Scoring: A Multitask model makes predictions about user behaviors (such as CTR, watch time after click) for all the candidates [8, 21].\n(3) Ranking: Multitask predictions are combined into a single ranking score to sort all candidates [1, 2, 4, 10, 17, 20].\n(4) Re-ranking: Additional logic is applied to ranking score to ensure other objectives, e.g. diversity [18], taking cross-item interaction into consideration.\nThis paper primarily focuses on the ranking stage, i.e. combining user behavior predictions to optimize long-term user satisfaction.\nMost existing deployed solutions (e.g. Meta [1, 2, 17], Pinter-est [10] and Kuaishou [4]) use a heuristic ranking function to combine multitask model predictions. As an example, given input user behavior predictions $s_1, s_2, ..., s_k$, the ranking formula can be\n$\\sum_{i=1}^{k} w_i s_i$\nwith $w_i$ being the hyperparameters. Then these systems apply hyperparameter search methods (e.g. Bayesian optimization, policy gradient) to optimize the ranking function. Typically the complexity of optimization grows with the number of hyperparameters, making it hard to change objectives, add input signals, or increase the expressiveness of the combination function.\nWe formulate the problem as a slate optimization instead. The goal of the optimization is learn a general ranking function to produce a slate that maximizes long-term user satisfaction.\nLet us take a look at related work in the area of slate optimiza-tion for long-term rewards. In [13], the authors propose the SlateQ method, which applies reinforcement learning to solve slate opti-mization. One limitation of the work is it assumes a simple user interaction model without considering the impact of slate posi-tion on the click probability. In [3], the authors give an efficient algorithm for the combinatorial optimization problem of reward maximization under the cascade click model [5, 9], assuming the dynamics of the system are given as input.\nExisting slate optimization work typically assumes that future rewards are zero when a user abandons a slate, which is unrealistic. Platforms like video streaming services have multiple recommen-dation systems (e.g., watch page, home page, search page), where users might abandon one and return later through engagement with another. Hence, it is important to model and optimize the lift value of a slate, i.e. its incremental value over the baseline value of the user abandoning the slate.\nAnother less studied but important issue when applying slate optimization at the ranking stage is the stability of multi-objective"}, {"title": "2 PROBLEM FORMATION", "content": "We model the problem of ranking videos using the following MDP:\n\u2022 state space $S = U \\times \\{V | V \\subset \\mathcal{V}, |V| = n\\}$. Here $U$ is some user state space and $V$ is a set of $n$ candidate videos nominated for ranking from $\\mathcal{V}$, the universe of all videos.\n\u2022 action space $A$ is all permutations of $n$. The system will rank $V = \\{V_1, V_2, ,V_n\\}$ by the order of $V_{\\sigma(1)}, ..., V_{\\sigma(n)}$ with action $\\sigma \\in A$.\n\u2022$\\mathcal{P}: S \\times A \\times S \\rightarrow [0, 1]$ is the state transition probability.\n\u2022 reward function $r(s, \\sigma) \\in \\mathbb{R}^m$ is the immediate reward vector by taking action $\\sigma$ on state $s$. We consider the general case that there are $m$ different type of rewards.\n\u2022 discounting factor $\\gamma \\in (0, 1)$ and initial state distribution $\\rho_0$.\nA policy $\\pi$ is a mapping from user state $S$ to a distribution on $A$. Applying policy $\\pi$ on $\\rho_0$ gives a distribution on user trajectory $\\mathcal{D}(\\rho_0, \\pi)$ defined as follows.\nDefinition 2.1. We define $\\mathcal{D}(\\rho_0, \\pi)$ as the distribution of user trajectories when applying policy $\\pi$ on initial state distribution $\\rho_0$. Here each user trajectory is a list of tuples $((s_0, \\sigma_0, c_0), (s_1, \\sigma_1, c_1), . . ., )$. Here $s_i = (u_i, V_i)$ is the user state; $\\sigma_i$ is a permutation action applied on $V$; $c_i$ is the user click position (with a value of 0 indicating no click). We define cumulative reward for $\\tau$ starting from timestamp $t$ as\n$\\text{Reward}(\\tau, t) = \\sum_{t'\\geq t} r(s_{t'}, \\sigma_{t'})$.\nand cumulative reward for policy $\\pi$ as\n$J(\\pi) = \\mathbb{E}_{\\tau \\sim \\mathcal{D}(\\rho_0, \\pi)} [\\text{Reward}(\\tau, 0)]$.\nThe optimization problem is to maximize cumulative reward for a primary objective subject to constraints on secondary objectives:\nPROBLEM 1.\n$\\max_\\pi J^1(\\pi)$\nsubject to $J^k(\\pi) \\geq \\beta_k$ for $k = 2,3, ..., m$. Here $J^i(\\pi)$ is the $i$-th element of $J(\\pi)$."}, {"title": "2.2 Lift Formulation with Cascade Click model", "content": "Let us follow the standard notation in reinforcement learning and define $Q^{\\pi}(s, \\sigma)$ as the expected cumulative reward taking action $\\sigma$ at state $s$ and applying policy $\\pi$ afterwards; i.e.,\n$Q^{\\pi}(s, \\sigma) = r(s, \\sigma) + \\gamma \\mathbb{E}_{\\tau \\sim \\mathcal{D}(\\mathcal{P}(s,\\sigma,\\cdot), \\pi)}[\\text{Reward}(\\tau, 0)]$.\nBelow we will factorize $Q^{\\pi}(s, \\sigma)$ into user-item-functions; i.e., functions that only depend on user and individual item.\nConditional on click position $c$, we can then rewrite $Q^{\\pi}(s, \\sigma)$ as\n$\\text{Pr}(c = 0) \\cdot \\mathbb{E}[Q^{\\pi}(s, \\sigma) | c = 0] + \\sum_{1 < i < n} \\text{Pr}(c = i) \\mathbb{E}[Q^{\\pi}(s, \\sigma) | c = i]$\nAs mentioned in Section 1, the reward associated with the user abandoning the slate ($c = 0$) can be nonzero.\nNotice that $\\sum_{0 \\leq i \\leq n} \\text{Pr}(c = i) = 1$, so we can further rewrite $Q^{\\pi}(s, \\sigma)$ as\n$\\mathbb{E}[Q^{\\pi}(s, \\sigma) | c = 0]$\n$+ \\sum_{1 < i < n} \\text{Pr}(c = i) \\cdot (\\mathbb{E}[Q^{\\pi}(s, \\sigma) | c = i] - \\mathbb{E}[Q^{\\pi}(s, \\sigma) | c = 0])$\nFirst, we simplify the term $\\mathbb{E}[Q^{\\pi}(s, \\sigma) | c = i]$ for $0 \\leq i \\leq n$ with user-item functions. In order to do so, we make the \"Reward/transition dependence on selection\" assumption from [13] which states that future reward only depends on the item the user clicks. In other words for $s = (u, V)$,\n(1) when $i > 0$, $\\mathbb{E}[Q^{\\pi}(s, \\sigma) | c = i]$ can can be written as $R^{\\text{clk}}_\\pi(u, V_{\\sigma(i)})$ for $R^{\\text{clk}}$ being a user-item function\n(2) $\\mathbb{E}[Q^{\\pi}(s, \\sigma) | c = 0]$ can be written as $R^{\\text{abd}}_\\pi(u)$ for $R^{\\text{abd}}$ being a user level function.\nWe further define $R^{\\text{lift}}$ as $R^{\\text{lift}}_\\pi(u, v) = R^{\\text{clk}}_\\pi(u, v) - R^{\\text{abd}}_\\pi(u)$, being the difference (i.e., lift) of future rewards associated with the user clicking item $v$ compared to user abandoning the slate.\nNext, we simplify the term $\\text{Pr}(c = i)$ with user-item functions by assuming the user interacts with the slate according to a cascade click model [9]. To model the behavior of the user abandoning a slate, we consider a variant [3, 5] which also allows the user to abandon the slate, in addition to skip and click, when inspecting an item, as illustrated in Figure 1:"}, {"title": "Markov Reward Process with Cascade Click Model", "content": "Definition 2.2. (Cascade Click Model) Given user state $s = (u, V)$, where $u$ represents the user and $V$ represents the set of items, and a ranking order $\\sigma$ on $V$, the Cascade model describes how a user interacts with a list of items sequentially. The user's interaction with the list when inspecting an item is characterized by the following user-item functions:\n\u2022 $P_{\\text{clk}}$: A user-item function where $P_{\\text{clk}}(u, v)$ represents the probability of user $u$ clicking on item $v$ when inspecting it.\n\u2022 $P_{\\text{abd}}$: A user-item function where $P_{\\text{abd}}(u, v)$ represents the probability of user $u$ abandoning the slate when inspecting item $v$.\nTaking $(u, V), \\sigma, P_{\\text{clk}}, P_{\\text{abd}}$ as input, the Cascade model defines function $p^{\\text{cascade}}_i$ that outputs the probability the user clicks on the item at the $i$-th position (for $1 \\leq i \\leq n$) with the form:\n$p^{\\text{cascade}}_i (P_{\\text{clk}}, P_{\\text{abd}}, (u, V), \\sigma) = (\\prod_{j=1}^{i-1} [(1 - P_{\\text{clk}}(u, V_{\\sigma(j)}) - P_{\\text{abd}}(u, V_{\\sigma(j)}))]) \\cdot P_{\\text{clk}}(u, v_{\\sigma(i)})$.\nThe probability that the user abandons the slate without clicking on any items is defined by the function $P^{\\text{cascade}}_0$ as:\n$p^{\\text{cascade}}_0 (P_{\\text{clk}}, P_{\\text{abd}}, (u, V), \\sigma) = 1 - \\sum_{i=1}^{n} p^{\\text{cascade}}_i (P_{\\text{clk}}, P_{\\text{abd}}, (u, V), \\sigma)$.\nPutting everything together, we can rewrite $Q^{\\pi}(s, \\sigma)$ as\n$Q^{\\pi}(s, \\sigma) = R^{\\text{abd}}_\\pi(u) + \\sum_{i=1}^{n} p^{\\text{cascade}}_i(P_{\\text{clk}}, P_{\\text{abd}}, s, \\sigma) \\cdot R^{\\text{lift}}_\\pi(u,v_{\\sigma(i)})$.\nWe call equation (3) the lift formulation with cascade click model. A natural question is how to order items to maximize $Q^{\\pi}(s, \\sigma)$ when there is only a single objective. Interestingly, despite the slate nature of this optimization, we prove that the problem can be solved by a user-item ranking function.\nTHEOREM 2.3. Given user-item functions $P_{\\text{clk}}, P_{\\text{abd}}, R^{\\text{abd}}_\\pi, R^{\\text{lift}}_\\pi$ as input, the optimal ranking for user $u$ on candidate $V$ maximizing $Q^{\\pi}((u, V), \\sigma)$ for a scalar reward function is to order all items $v \\in V$ by $\\frac{P_{\\text{clk}}(u,v)}{P_{\\text{clk}}(u,v) + P_{\\text{abd}}(u,v)} \\cdot R^{\\text{lift}}_\\pi(u, v)$."}, {"title": "3 OPTIMIZATION ALGORITHM", "content": "This section outlines the optimization algorithm for solving Prob-lem 1, initially for the special case of a single objective; i.e., $m = 1$ and subsequently extending to multi-objective constraint optimization.\nOur algorithm employs an on-policy Monte Carlo approach [16] which iteratively applies following two steps:\n(1) Training: Build a function approximation $Q(s, \\sigma; \\theta)$ for $Q^{\\pi}(s, \\sigma)$ by separately building function approximations for $R^{\\text{abd}}, R^{\\text{clk}}, P_{\\text{clk}}$ and $P_{\\text{abd}}$, using data collected by applying some initial policy $\\pi$.\n(2) Inference: Modify the policy $\\pi$ to be $\\text{arg} \\max_\\sigma Q(s, \\sigma; \\theta)$ (with exploration).\nWe outline the main steps in Algorithm 1 and discuss technical details in Section 3.1.1 and 3.1.2.\n3.1.1 Training. The training data is collection of user trajectories (see Definition 2.1) stored in $D$. Each user trajectory can be writ-ten as $(((u_0, V_0), \\sigma_0, c_0), ((u_1, V_1), \\sigma_1, c_1), . . ., )$. We apply gradient updates for $\\theta$ with the following loss functions, in sequential order.\nTraining the abandon reward network. $R^{\\text{abd}}(u; \\theta)$ on abandoned pages with MSE loss function\n$\\mathbb{E}_{\\tau \\sim D, t \\sim [\\tau]} [|R^{\\text{abd}}(u_t; \\theta) - \\text{Reward}(t, t)|^2 | c_t = 0]$\nTraining the lift reward network. $R^{\\text{lift}}(u, v; \\theta)$ on clicked videos with MSE loss function\n$\\mathbb{E}_{\\tau \\sim D, t \\sim [\\tau]} [|R^{\\text{lift}}(u_t, V_{\\sigma_t}(c_t); \\theta) + R^{\\text{abd}}(u_t; \\theta) - \\text{Reward}(t, t)|^2 | c_t > 0]$\nHere we apply the idea from uplift modeling [12] by directly esti-mating the difference between $R^{\\text{clk}}(u, v)$ and $R^{\\text{abd}}(u)$.\nTraining the click network. on every page with cross-entropy loss function\n$\\mathbb{E}_{\\tau \\sim D, t \\sim [\\tau]} [-log(p^{\\text{cascade}}(P_{\\text{clk}}(\\cdot, \\cdot; \\theta), P_{\\text{abd}}(\\cdot, \\cdot; \\theta), (u_t, V_t), \\sigma_t))]$,\nusing $p^{\\text{cascade}}$ in Definition 2.2.\n3.1.2 Inference. Here we simply apply Theorem 2.3 using the func-tion approximation for $P_{\\text{clk}}, P_{\\text{abd}}, R^{\\text{abd}}, R^{\\text{lift}}$. We randomly pro-mote a candidate to top with small probability as exploration."}, {"title": "3.2 Constraint optimization", "content": "When there are multiple objectives, we apply linear scalarization [15] to reduce the constraint optimization problem to a unconstrained optimization problem; i.e., we find weights $w_2, w_3, ..., w_m$ and de-fine the new reward function as $r^1 + \\sum_{i=2} w_i \\cdot r^i$. With fixed weight combination, we found it often necessary to search new weights"}, {"title": "4 DEPLOYMENT AND EVALUATION", "content": "The LRF was initially launched on YouTube's Watch Page, followed by the Home and Shorts pages. Below we discuss its deployment on Watch Page. Also see illustration in Figure 2.\nLightweight model with on-policy training. The LRF system ap-plies an on-policy RL algorithm. In order to enable evaluating many different LRF models with on-policy training, we make all LRF model training on a small slice (e.g. 1%) of overall traffic. By doing so, we can compare production and many experimental models that are all trained on-policy together.\nTraining and Serving. The LRF system is continuously trained with user trajectories from the past few days. Our primary reward function is defined as the user satisfaction on watches, similar to the metric described in Section 6 of [7]. The features include the user behavior predictions from multitask models, user context features"}, {"title": "4.1 Deployment of LRF System", "content": "Intuitively, we would like to minimize the change to the pri-mary objective while satisfying offline evaluation on secondary objectives.\nIn the case of a single constraint (i.e., $m = 2$), there is a closed-form solution for the problem. To see this, the optimal solution for $w_2$ must be either 0 or be a solution that makes the constraint tight; i.e.,\n$\\text{Corr} (r^1, R^{\\text{lift}} (u, v; \\theta) + w_2 \\cdot R^{\\text{lift}} (u, v; \\theta)) = \\alpha_2$.\nIt is not hard to verify that the solution for equation (4) is also the solution for a quadratic equation of $w_2$ that can be solved with closed form. Therefore, we can set $w_2$ be the best feasible solution from $\\{0\\} \\cup \\{\\text{solution for equation (4)}\\}$.\nWhen there is more than one constraint, we found that applying constraints sequentially works well in practice. One can also apply grid search as the offline evaluation can be done efficiently."}, {"title": "4.2 Evaluation", "content": "We conducted A/B experiments for (week) on YouTube to evalu-ate the effectiveness of the LRF. Metric trends are shown in Figure 3. Note that first three experiments describe sequential improvements to the production system; the last two experiments ablate certain components of the LRF.\nEvaluation Metric. Our primary objective is a metric measuring long-term cumulative user satisfaction; see Sec 6 of [7] for details.\nBaseline before LRF Launch: The previous system uses a heuristic ranking function optimized by Bayesian optimization [11].\nHyperparameters: We tuned two types of hyperparameters when deploying the LRF: training parameters, such as batch size, are tuned using offline loss; reward parameters, such as constraint weights, are tuned using live experiments."}, {"title": "4.2.1 Initial Deployment of LRF", "content": "We initially launched a simplified version of the LRF that uses the CTR prediction from the multitask model and ranks all candidates by CTR Rlift. It also uses a set of fixed weights to combine secondary objectives. The control was the previous production system, a heuristic ranking function tuned using Bayesian optimization. The LRF outperformed the production by 0.21% with 95% CI [0.04, 0.38] and was launched to production."}, {"title": "4.2.2 Launch Cascade Click Model", "content": "After initial deployment of the LRF, we ran an experiment to determine the efficacy of the cascade click model, i.e., replacing CTR with $\\frac{P_{\\text{clk}}}{P_{\\text{clk}} + P_{\\text{abd}}}$. Adding the cascade click model outperformed the control by 0.66% with 95% CI [0.60, 0.72] in the top-line metric and was launched to production."}, {"title": "4.2.3 Launch Constraint Optimization", "content": "We found metric trade-offs between primary and secondary objectives unstable when com-bining the rewards using fixed weights. To improve stability, we launched the constraint optimization. As an example of the improve-ment, for the same model architecture change, we saw a 13.15% change in the secondary objective pre-launch, compared to a 1.46% change post-launch. This post-launch fluctuation is considered small for that metric."}, {"title": "4.2.4 Ablating Lift Formulation", "content": "To determine the necessity of the lift formula, we ran an experiment that set Rabd to be 0. Such a change regresses top-line metrics by 0.46% with 95% CI [0.43, 0.49]. The metric contributed from watch page recommendations actually increases by 0.2% with 95% CI [0.14, 0.26]. This suggests the importance of lift formulation as it is sub-optimal to only maximize rewards from watch page suggestions."}, {"title": "4.2.5 Two Model Approach", "content": "We make separate predictions for Rabd and Rclk. This is also known as the two-model baseline in uplift mod-elling [12]. The ranking formula is then $\\frac{P_{\\text{clk}}}{P_{\\text{clk}} + P_{\\text{abd}}} (R_{\\text{click}} - R_{\\text{abd}})$. The experiment results show that our production LRF outperforms this baseline in the top-line metric by 0.12% with 95% CI [0.06, 0.18]."}, {"title": "5 CONCLUSION", "content": "We presented the Learned Ranking Function (LRF), a system that combines short-term user-item behavior predictions to optimizing slates for long-term user satisfaction. One future direction is to apply more ideas from Reinforcement Learning such as off-policy training and TD Learning [16]. Another future direction is to in-corporate re-ranking algorithm (e.g., [14, 18]) into the LRF system."}]}