{"title": "Learned Ranking Function: From Short-term Behavior Predictions to Long-term User Satisfaction", "authors": ["Yi Wu", "Daryl Chang", "Jennifer She", "Zhe Zhao", "Li Wei", "Lukasz Heldt"], "abstract": "We present the Learned Ranking Function (LRF), a system that takes short-term user-item behavior predictions as input and outputs a slate of recommendations that directly optimizes for long-term user satisfaction. Most previous work is based on optimizing the hyperparameters of a heuristic function. We propose to model the problem directly as a slate optimization problem with the objective of maximizing long-term user satisfaction. We also develop a novel constraint optimization algorithm that stabilizes objective tradeoffs for multi-objective optimization. We evaluate our approach with live experiments and describe its deployment on YouTube.", "sections": [{"title": "1 INTRODUCTION AND RELATED WORK", "content": "Large video recommendation systems typically have the following stages:\n(1) Candidate Generation: The system first generates a short list of video candidates from a large corpus [6, 19].\n(2) Multitask Model Scoring: A Multitask model makes predictions about user behaviors (such as CTR, watch time after click) for all the candidates [8, 21].\n(3) Ranking: Multitask predictions are combined into a single ranking score to sort all candidates [1, 2, 4, 10, 17, 20].\n(4) Re-ranking: Additional logic is applied to ranking score to ensure other objectives, e.g. diversity [18], taking cross-item interaction into consideration.\nThis paper primarily focuses on the ranking stage, i.e. combining user behavior predictions to optimize long-term user satisfaction.\nMost existing deployed solutions (e.g. Meta [1, 2, 17], Pinterest [10] and Kuaishou [4]) use a heuristic ranking function to combine multitask model predictions. As an example, given input user behavior predictions S1, S2, ..., sk, the ranking formula can be\n$\\sum_{i=1}^{k} w_i s_i$\nwith wi being the hyperparameters. Then these systems apply hyperparameter search methods (e.g. Bayesian optimization, policy gradient) to optimize the ranking function. Typically the complexity of optimization grows with the number of hyperparameters, making it hard to change objectives, add input signals, or increase the expressiveness of the combination function.\nWe formulate the problem as a slate optimization instead. The goal of the optimization is learn a general ranking function to produce a slate that maximizes long-term user satisfaction.\nLet us take a look at related work in the area of slate optimization for long-term rewards. In [13], the authors propose the SlateQ method, which applies reinforcement learning to solve slate optimization. One limitation of the work is it assumes a simple user interaction model without considering the impact of slate position on the click probability. In [3], the authors give an efficient algorithm for the combinatorial optimization problem of reward maximization under the cascade click model [5, 9], assuming the dynamics of the system are given as input.\nExisting slate optimization work typically assumes that future rewards are zero when a user abandons a slate, which is unrealistic. Platforms like video streaming services have multiple recommendation systems (e.g., watch page, home page, search page), where users might abandon one and return later through engagement with another. Hence, it is important to model and optimize the lift value of a slate, i.e. its incremental value over the baseline value of the user abandoning the slate.\nAnother less studied but important issue when applying slate optimization at the ranking stage is the stability of multi-objective"}, {"title": "2 PROBLEM FORMATION", "content": "optimization. Most recommendation systems need to balance trade-offs among multiple objectives. Stability here refers to maintaining consistent trade-offs among these objectives when orthogonal changes, such as adding a feature or modifying the model architecture, are made to the algorithm. The stability is crucial for system reliability and developer velocity.\nTo address these existing limitations, we present the Learned Ranking Function (LRF) system. Our main contributions are three-fold:\n(1) We model the user-slate interaction as a cascade click model [9] and propose an algorithm to optimize slate-wise long-term rewards. We explicitly model the value of abandonment and optimize for the long-term rewards for entire platform.\n(2) We propose a novel constrained optimization algorithm based on dynamic linear scalarization to ensure the stability of trade-offs for multi-objective optimization.\n(3) We show how the LRF is fully launched on YouTube and provide empirical evaluation results.\nThe rest of paper is organized as follows. In Section 2, we define the Markov Decision Process(MDP) for the problem of long-term rewards slate optimization. In Section 3, we propose an optimization algorithm to solve the MDP problem. We show how we deploy the LRF to YouTube with evaluation results in Section 4."}, {"title": "2.1 MDP Formulation", "content": "We model the problem of ranking videos using the following MDP:\n\u2022 state space S = U \u00d7 {V|V \u2282 V, |V| = n}. Here U is some user state space and V is a set of n candidate videos nominated for ranking from V, the universe of all videos.\n\u2022 action space A is all permutations of n. The system will rank V = {V1, V2, ,Vn} by the order of V\u03c3(1), ..., V\u03c3(n) with action \u03c3\u2208 A.\n\u2022P:S\u00d7A\u00d7S \u2192 [0, 1] is the state transition probability.\n\u2022 reward functionr(s, \u03c3) \u2208 Rm is the immediate reward vector by taking action \u03c3 on state s. We consider the general case that there are m different type of rewards.\n\u2022 discounting factor \u03b3 \u2208 (0, 1) and initial state distribution po.\nA policy \u03c0 is a mapping from user state S to a distribution on A. Applying policy \u03c0 on po gives a distribution on user trajectory D(\u03c1\u03bf, \u03c0) defined as follows.\nDefinition 2.1. We define D(\u03c1\u03bf, \u03c0) as the distribution of user trajectories when applying policy \u03c0 on initial state distribution po. Here each user trajectory is a list of tuples ((so, \u03c3\u03bf, Co), (51, 51, C1), . . ., ). Here si = (ui, Vi) is the user state; \u03c3\u00a1 is a permutation action applied on V; ci is the user click position (with a value of 0 indicating no click). We define cumulative reward for \u03c4 starting from timestamp tas\n$Reward(t, t) = \\sum_{t'\\geq t}rt-t r(s_t, \\sigma_t)$.\nand cumulative reward for policy \u03c0 as\n$J(\\pi) = \\mathbb{E}_{\\tau\\sim D(\\rho_0,\\pi)} [Reward(\\tau, 0)]$.\nThe optimization problem is to maximize cumulative reward for a primary objective subject to constraints on secondary objectives:"}, {"title": "PROBLEM 1.", "content": "max J\u00b9 (\u03c0)\n\u03c0\nsubject to Jk (\u03c0) \u2265 \u03b2k fork = 2,3, ..., m. Here J\u00b9 (\u03c0) is the i-th element of J(\u03c0)."}, {"title": "2.2 Lift Formulation with Cascade Click model", "content": "Let us follow the standard notation in reinforcement learning and define Q\u03c0(s, \u03c3) as the expected cumulative reward taking action \u03c3 at state s and applying policy \u03c0 afterwards; i.e.,\n$Q^{\\pi}(s, \\sigma) = r(s, \\sigma) + \\gamma \\mathbb{E}_{\\tau\\sim D(P(s,\\sigma,\\cdot),\\pi)} [Reward(\\tau, 0)]$.\nBelow we will factorize Q\u03c0(s, \u03c3) into user-item-functions; i.e., functions that only depend on user and individual item.\nConditional on click position c, we can then rewrite Q\u03c0(s, \u03c3) as\n$Pr(c = 0)\\cdot\\mathbb{E}[Q^{\\pi} (s, \\sigma)|c = 0]+ \\sum_{1<i<n} Pr(c = i) \\mathbb{E}[Q^{\\pi} (s, \\sigma)|c = i]$\nAs mentioned in Section 1, the reward associated with the user abandoning the slate (c = 0) can be nonzero.\nNotice that $\\sum_{0\\leq i\\leq n} Pr(c = i) = 1$, so we can further rewrite Q\u03c0(s, \u03c3) as\n$\\mathbb{E}[Q^{\\pi} (s, \\sigma)|c = 0]$\\n$+ \\sum_{1<i<n} Pr(c = i) \\cdot (\\mathbb{E}[Q^{\\pi} (s, \\sigma)|c = i] - \\mathbb{E}[Q^{\\pi} (s, \\sigma)|c = 0])$\nFirst, we simplify the term $\\mathbb{E}[Q^{\\pi} (s, \\sigma)|c = i]$ for 0 \u2264 i \u2264 n with user-item functions. In order to do so, we make the \"Reward/transition dependence on selection\" assumption from [13] which states that future reward only depends on the item the user clicks. In other words for s = (u, V),\n(1) when i > 0, $\\mathbb{E}[Q^{\\pi} (s, \\sigma)|c = i]$ can can be written as $R_{clk}^{\\pi}(u, V_{\\sigma(i)})$ for $R_{clk}^{\\pi}$ being a user-item function\n(2) $\\mathbb{E}[Q^{\\pi} (s, \\sigma)|c = 0]$ can be written as $R_{abd}^{\\pi}(u)$ for $R_{abd}^{\\pi}$ being a user level function.\nWe further define $R_{lift}^{\\pi}$ as $R_{lift}^{\\pi}(u, v) = R_{clk}^{\\pi}(u, v) - R_{abd}^{\\pi}(u)$, being the difference (i.e., lift) of future rewards associated with the user clicking item v compared to user abandoning the slate.\nNext, we simplify the term Pr(c = i) with user-item functions by assuming the user interacts with the slate according to a cascade click model [9]. To model the behavior of the user abandoning a slate, we consider a variant [3, 5] which also allows the user to abandon the slate, in addition to skip and click, when inspecting an item, as illustrated in Figure 1:"}, {"title": "Definition 2.2.", "content": "(Cascade Click Model) Given user state s = (u, V), where u represents the user and V represents the set of items, and a ranking order \u03c3 on V, the Cascade model describes how a user interacts with a list of items sequentially. The user's interaction with the list when inspecting an item is characterized by the following user-item functions:\n\u2022 Pclk: A user-item function where Pclk (u, v) represents the probability of user u clicking on item v when inspecting it.\n\u2022 Pabd: A user-item function where Pabd(u, v) represents the probability of user u abandoning the slate when inspecting item v.\nTaking (u, V), \u03c3, Pclk, Pabd as input, the Cascade model defines function $P_{cascade}^{\\pi}$ that outputs the probability the user clicks on the item at the i-th position (for 1 \u2264 i \u2264 n) with the form:\n$p_i^{cascade} P_{cascade}^{\\pi} (P_{clk}, P_{abd}, (u, V), \\sigma) =$\\n$\\prod_{j=1}^{i-1}[(1 - P_{clk}(u, V_{\\sigma(j)}) - P_{abd} (u, V_{\\sigma(j)}))] \\cdot P_{clk}(u, v_{\\sigma(i)}).$ (1)\nThe probability that the user abandons the slate without clicking on any items is defined by the function $P_0^{cascade}$ as:\n$p_0 P_{cascade}^{\\pi} (P_{clk}, P_{abd}, (u, v), \\sigma) = 1-\\sum_{i=1}^{n} P_{cascade}^{\\pi} (P_{clk}, P_{abd}, (u, V), \\sigma)$. (2)\nPutting everything together, we can rewrite Qt (s, \u03c3) as\n$Q^{\\pi} (s, \\sigma) = R_{abd}^{\\pi}() + \\sum_{i=1}^{n} P_{cascade}^{\\pi} (P_{clk}, P_{abd}, s, \\sigma)\\cdot R_{lift}^{\\pi} (u,V_{\\sigma(i)})$ (3)\nWe call equation (3) the lift formulation with cascade click model. A natural question is how to order items to maximize Q\u03c0(s, \u03c3) when there is only a single objective. Interestingly, despite the slate nature of this optimization, we prove that the problem can be solved by a user-item ranking function."}, {"title": "THEOREM 2.3.", "content": "Given user-item functions Pclk, Pabd, $R_{abd}^{\\pi}$, $R_{lift}^{\\pi}$ as input, the optimal ranking for user u on candidate V maximizing Q\u03c0 ((u, V), \u03c3) for a scalar reward function is to order all items\nv \u2208 V by $\\frac{P_{clk} (u,v)}{P_{clk} (u,v)+P_{abd} (u,v)} \\cdot R_{lift}^{\\pi} (u, v)$."}, {"title": "3 OPTIMIZATION ALGORITHM", "content": "This section outlines the optimization algorithm for solving Problem 1, initially for the special case of a single objective; i.e., m = 1 and subsequently extending to multi-objective constraint optimization."}, {"title": "3.1 Single Objective Optimization", "content": "Our algorithm employs an on-policy Monte Carlo approach [16] which iteratively applies following two steps:\n(1) Training: Build a function approximation Q(s, \u03c3; \u03b8) for Q\u03c0(s, \u03c3) by separately building function approximations for $R_{abd}^{\\pi}$, $R_{clk}^{\\pi}$, Pclk and Pabd, using data collected by applying some initial policy \u03c0.\n(2) Inference: Modify the policy \u03c0 to be arg maxo Q(s, \u03c3; \u03b8) (with exploration).\nWe outline the main steps in Algorithm 1 and discuss technical details in Section 3.1.1 and 3.1.2."}, {"title": "3.1.1 Training.", "content": "The training data is collection of user trajectories (see Definition 2.1) stored in D. Each user trajectory can be written as (((uo, Vo), \u03c3\u03bf, Co), ((u1, V\u2081), 51, c\u2081), . . ., .). We apply gradient updates for \u03b8 with the following loss functions, in sequential order.\nTraining the abandon reward network. Rabd(u; 0) on abandoned pages with MSE loss function\n$\\mathbb{E}_{\\tau\\sim D,t\\sim[\\tau]} [[R_{abd}^{\\pi}(u_t;\\theta) - Reward(t, t)|^2|c_t = 0]$\nTraining the lift reward network. Rlift (u, v; 0) on clicked videos with MSE loss function\n$\\mathbb{E}_{\\tau\\sim D,t\\sim[\\tau]} [|R_{Lift}^{\\pi} (u_t, V_{\\sigma V_t} (c_t); \\theta) + R_{abd}^{\\pi} (u_t; \\theta)-\\nReward(t, t)|^2|c_t > 0]$\nHere we apply the idea from uplift modeling [12] by directly estimating the difference between $R_{clk}^{\\pi}(u, v)$ and $R_{abd}^{\\pi}(u)$.\nTraining the click network. on every page with cross-entropy loss function\n$\\mathbb{E}_{\\tau\\sim D,t\\sim[\\tau]} [-log(P_{cascade}^{\\pi} (P_{clk} (\\cdot, \\cdot; \\theta), P_{abd} (\\cdot, \\cdot; \\theta), (u_t, V_t), \\sigma_t))]$,\nusing $P_{cascade}^{\\pi}$ in Definition 2.2."}, {"title": "3.1.2 Inference.", "content": "Here we simply apply Theorem 2.3 using the function approximation for Pclk, Pabd, $R_{abd}^{\\pi}$, $R_{lift}^{\\pi}$. We randomly promote a candidate to top with small probability as exploration."}, {"title": "3.2 Constraint optimization", "content": "When there are multiple objectives, we apply linear scalarization [15] to reduce the constraint optimization problem to a unconstrained optimization problem; i.e., we find weights w2, W3, ..., wm and define the new reward function as r\u00b9 + \u22112 wi \u00b7 r\u00b9. With fixed weight combination, we found it often necessary to search new weights"}, {"title": "PROBLEM 2.", "content": "min\n$w_2,w_3,..., w_m \\mathbb{E}_{(u,v,r_o) \\sim D_{eval}} \\sum_{i=2}^{m} [(\\mathbb{R}_{lift}(u,v;\\theta), w)]$\nsuch that Corr$\\mathbb{E}_{(u,v,r_o) \\sim D_{eval}}(r_o, (\\mathbb{R}_{lift}(u,v;\\theta), w)) \\geq \\alpha_i$ for i = 2,..., m and w = (1, W2,..., Wm)"}, {"title": "4 DEPLOYMENT AND EVALUATION", "content": "Intuitively, we would like to minimize the change to the pri-mary objective while satisfying offline evaluation on secondary objectives.\nIn the case of a single constraint (i.e., m = 2), there is a closed-form solution for the problem. To see this, the optimal solution for w2 must be either 0 or be a solution that makes the constraint tight; i.e.,\nCorr (r, Rlift (u, v; \u03b8) + w2. Rlift (u, v; \u03b8)) = \u03b12. (4)\nIt is not hard to verify that the solution for equation (4) is also the solution for a quadratic equation of w2 that can be solved with closed form. Therefore, we can set w2 be the best feasible solution from {0} \u222a {solution for equation (4)}.\nWhen there is more than one constraint, we found that applying constraints sequentially works well in practice. One can also apply grid search as the offline evaluation can be done efficiently."}, {"title": "4.1 Deployment of LRF System", "content": "The LRF was initially launched on YouTube's Watch Page, followed by the Home and Shorts pages. Below we discuss its deployment on Watch Page. Also see illustration in Figure 2.\nLightweight model with on-policy training. The LRF system applies an on-policy RL algorithm. In order to enable evaluating many different LRF models with on-policy training, we make all LRF model training on a small slice (e.g. 1%) of overall traffic. By doing so, we can compare production and many experimental models that are all trained on-policy together.\nTraining and Serving. The LRF system is continuously trained with user trajectories from the past few days. Our primary reward function is defined as the user satisfaction on watches, similar to the metric described in Section 6 of [7]. The features include the user behavior predictions from multitask models, user context features"}, {"title": "4.2 Evaluation", "content": "(e.g. demographics) and video features (e.g. video topic). We use both continuous features and sparse features with small cardinality. The LRF model comprises small deep neural networks, with roughly \u0398(104) parameters. The inference cost of the model is small due to the size of the model. We use the offline evaluation described in Section 3.2.1 to ensure model quality before pushing to production. At serving time, the LRF takes the aforementioned features as input and outputs a ranking score for all items."}, {"title": "4.2.1 Initial Deployment of LRF.", "content": "We initially launched a simplified version of the LRF that uses the CTR prediction from the multitask model and ranks all candidates by CTR Rlift. It also uses a set of fixed weights to combine secondary objectives. The control was the previous production system, a heuristic ranking function tuned using Bayesian optimization. The LRF outperformed the production by 0.21% with 95% CI [0.04, 0.38] and was launched to production."}, {"title": "4.2.2 Launch Cascade Click Model.", "content": "After initial deployment of the LRF, we ran an experiment to determine the efficacy of the cascade click model, i.e., replacing CTR with $\\frac{Pclk}{Pclk+Pabd}$. Adding the cascade click model outperformed the control by 0.66% with 95% CI [0.60, 0.72] in the top-line metric and was launched to production."}, {"title": "4.2.3 Launch Constraint Optimization.", "content": "We found metric trade-offs between primary and secondary objectives unstable when combining the rewards using fixed weights. To improve stability, we launched the constraint optimization. As an example of the improvement, for the same model architecture change, we saw a 13.15% change in the secondary objective pre-launch, compared to a 1.46% change post-launch. This post-launch fluctuation is considered small for that metric."}, {"title": "4.2.4 Ablating Lift Formulation.", "content": "To determine the necessity of the lift formula, we ran an experiment that set Rabd to be 0. Such a change regresses top-line metrics by 0.46% with 95% CI [0.43, 0.49]. The metric contributed from watch page recommendations actually increases by 0.2% with 95% CI [0.14, 0.26]. This suggests the importance of lift formulation as it is sub-optimal to only maximize rewards from watch page suggestions."}, {"title": "4.2.5 Two Model Approach.", "content": "We make separate predictions for Rabd and Rclk. This is also known as the two-model baseline in uplift modeling [12]. The ranking formula is then $\\frac{Pclk}{Pclk+Pabd}$ (Rclick - Rabd). The experiment results show that our production LRF outperforms this baseline in the top-line metric by 0.12% with 95% CI [0.06, 0.18]."}, {"title": "5 CONCLUSION", "content": "We presented the Learned Ranking Function (LRF), a system that combines short-term user-item behavior predictions to optimizing slates for long-term user satisfaction. One future direction is to apply more ideas from Reinforcement Learning such as off-policy training and TD Learning [16]. Another future direction is to incorporate re-ranking algorithm (e.g., [14, 18]) into the LRF system."}]}