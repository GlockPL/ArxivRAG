{"title": "TRACE-CS: Trustworthy Reasoning for Contrastive Explanations in Course Scheduling Problems", "authors": ["Stylianos Loukas Vasileiou", "William Yeoh"], "abstract": "We present TRACE-CS, a novel hybrid system that combines symbolic reasoning with large language models (LLMs) to address contrastive queries in scheduling problems. TRACE-CS leverages SAT solving techniques to encode scheduling constraints and generate explanations for user queries, while utilizing an LLM to process the user queries into logical clauses as well as refine the explanations generated by the symbolic solver to natural language sentences. By integrating these components, our approach demonstrates the potential of combining symbolic methods with LLMs to create explainable AI agents with correctness guarantees.", "sections": [{"title": "Introduction", "content": "Scheduling systems, which allocate finite resources to multiple agents over time, are ubiquitous in real-world systems, from personnel shift assignments (Van den Bergh et al. 2013) to Mars rover activities (Chi, Chien, and Agrawal 2020). Beyond generating valid and optimal schedules, it is crucial to ensure that both the schedule and the decision-making process are explainable to human users. Explainable scheduling, therefore, is essential for understanding scheduling decisions, rectifying issues, and providing explanations for specific decisions or schedule generation failures. Most of the work in this space have relied on symbolic, logical methods that generate valid and sound explanations.\nAt the other end of the spectrum, the emergence of large language models (LLMs) has marked a significant milestone in AI. While LLMs excel at generating coherent and contextually relevant text (Brown et al. 2020), their reliance on statistical inference leads to challenges in maintaining logical consistency and accuracy in reasoning and planning tasks (McCoy et al. 2023; Valmeekam et al. 2023). This limitation is particularly apparent when explanations need to be both linguistically coherent and logically sound. In contrast, symbolic, logical methods provide a robust medium for reasoning and planning due to their ability to perform valid and sound inference. This realization offers an opportunity to combine the strengths of both LLMs and symbolic methods, creating synergistic systems that ensure decisions are not only provably correct and robust, but also communicated in a user-friendly manner."}, {"title": "TRACE-CS Overview", "content": "We now provide an overview of the TRACE-CS system, illustrated in Figure 1.\nSymbolic Module. The Symbolic Module forms the core of TRACE-CS, handling the scheduling logic and explanation generation:\n\u2022 Encoder: Encodes specific scheduling constraints into logical formulae, creating a knowledge base KB that represents the scheduling problem. This includes encoding course prerequisites, credit requirements, semester constraints, and so on. Each formula has an associated label attached to it, describing in English the type of scheduling constraint it encodes.\n\u2022 Explainer: Utilizes the state-of-the-art symbolic explanation generation solver by Vasileiou, Previti, and Yeoh (2021). It takes as input the knowledge base KB from the Encoder and a user contrastive query 4 (processed by the LLM module), and generates contrastive explanations. The output is a set of logical formulae along with their corresponding labels.\nLLM Module. The LLM Module serves as the interface between the user and the Symbolic Module, handling natural language processing tasks:\n\u2022 Query Parser: Interprets a user's contrastive query in natural language and converts it into a symbolic representation consistent with the encoded knowledge base KB. This process employs in-context learning to ensure accurate interpretation.\n\u2022 Explanation Refiner: Takes the symbolic explanation \u025b from the Explainer and translates it into natural language sentences. This translation process also uses in-context learning, utilizing the labels attached to each formula in \u025b to ensure accurate and coherent explanations.\nFigure 1 shows the workflow of TRACE-CS: (1) The user submits a contrastive query in natural language; (2) The Query Parser extracts the information from the query and converts it into a symbolic representation \u03c6 consistent with the knowledge base KB created by the Encoder; (3) The user verifies if the extracted query information corresponds to the original query, and proceeds to the next step if it is; (4) The Explainer generates a symbolic explanation \u03b5 for \u03c6 with respect to KB; (5) The Explanation Refiner converts \u03b5 into natural language and outputs it to the user."}, {"title": "Proof-of-Concept: Academic Course Schedules", "content": "We implemented TRACE-cs in Python as a proof-of-concept for scheduling courses for an undergraduate computer science student across the eight academic semesters at Washington University in St. Louis. To create a comprehensive and realistic scheduling environment, we scraped the computer science course catalog and degree requirements from the university's official website. The Symbolic Module was implemented using PySAT (Ignatiev, Morgado, and Marques-Silva 2018) and the LLM Module was implemented using the GPT-4 model (OpenAI 2023). Figure 2 shows the user interface of our implementation.\nTo evaluate the effectiveness of TRACE-CS, we conducted a comparative experimental study against zero-shot and few-shot LLM-only approaches. Specifically, we generated 10 distinct schedules and created 10 queries for each schedule, totaling 100 schedule-query pairs. Our evaluation metrics were explanation correctness with respect to the degree and course constraints, and explanation verbosity measured by the average number of words per explanation.\nTable 1 shows the results, where TRACE-CS significantly outperformed both zero-shot and few-shot LLM approaches"}, {"title": "Conclusions", "content": "We introduced TRACE-CS, a hybrid system that synergistically combines the theoretical frameworks developed in this thesis with large language models (LLMs) to generate contrastive explanations for course scheduling problems. Our proof-of-concept implementation and experimental results demonstrate the system's ability to provide accurate, logically sound, and naturally expressed explanations, significantly outperforming LLM-only approaches in explanation correctness.\nThe success of TRACE-CS demonstrates the feasibility of creating AI systems that are both logically sound and user-friendly, a combination crucial for building trust in AI-driven decision-making processes. Looking ahead, the principles underlying TRACE-Cs could be extended to more complex decision-making processes, potentially enhancing how AI systems interact with and explain their decisions to human users. This work paves the way for a new generation of explainable AI systems that can reason logically, communicate naturally, and adapt to user needs, ultimately enhancing human-AI collaboration in tackling complex real-world problems."}, {"title": "Related Work", "content": "Explainable scheduling research has predominantly relied on logical symbolic methods (Cyras et al. 2019; Agrawal, Yelamanchili, and Chien 2020; Bertolucci et al. 2021; Pozanco et al. 2022; Powell and Riccardi 2022; Vasileiou et al. 2022; Vasileiou, Xu, and Yeoh 2023; Zehtabi et al. 2024). While grounded in sound inference procedures, these approaches often produce explanations that are difficult to communicate to users due to their logic-based nature. Attempts to mitigate this limitation have used templates mapping logical explanations to pre-specified natural language sentences (Pozanco et al. 2022; Vasileiou, Xu, and Yeoh 2023) or visualization interfaces (Cyras, Lee, and Letsios 2021; Kumar et al. 2022; Powell and Riccardi 2022).\nConcurrently, LLMs have revolutionized natural language processing and found applications across diverse domains, including planning (Kambhampati et al. 2024), code generation (Roziere et al. 2023), and medical applications (Zhou et al. 2023). However, the integration of LLMs with symbolic explainable scheduling systems remains largely unexplored. Our work, TRACE-CS, represents the first attempt to address this gap by presenting a novel hybrid system that synergistically combines a symbolic explainable scheduling module with an LLM module."}, {"title": "4 Conclusions", "content": "We introduced TRACE-CS, a hybrid system that synergistically combines the theoretical frameworks developed in this thesis with large language models (LLMs) to generate contrastive explanations for course scheduling problems. Our proof-of-concept implementation and experimental results demonstrate the system's ability to provide accurate, logically sound, and naturally expressed explanations, significantly outperforming LLM-only approaches in explanation correctness.\nThe success of TRACE-CS demonstrates the feasibility of creating AI systems that are both logically sound and user-friendly, a combination crucial for building trust in AI-driven decision-making processes. Looking ahead, the principles underlying TRACE-Cs could be extended to more complex decision-making processes, potentially enhancing how AI systems interact with and explain their decisions to human users. This work paves the way for a new generation of explainable AI systems that can reason logically, communicate naturally, and adapt to user needs, ultimately enhancing human-AI collaboration in tackling complex real-world problems."}]}