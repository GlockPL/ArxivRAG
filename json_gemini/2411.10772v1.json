{"title": "MRI Parameter Mapping via Gaussian Mixture VAE:\nBreaking the Assumption of Independent Pixels", "authors": ["Moucheng Xu", "Yukun Zhou", "Tobias Goodwin-Allcock", "Kimia Firoozabadi", "Joseph Jacob", "Daniel C. Alexander", "Paddy J. Slator"], "abstract": "We introduce and demonstrate a new paradigm for quantitative parameter mapping\nin MRI. Parameter mapping techniques, such as diffusion MRI and quantitative\nMRI, have the potential to robustly and repeatably measure biologically-relevant\ntissue maps that strongly relate to underlying microstructure. Quantitative maps\nare calculated by fitting a model to multiple images, e.g. with least-squares or\nmachine learning. However, the overwhelming majority of model fitting techniques\nassume that each voxel is independent, ignoring any co-dependencies in the data.\nThis makes model fitting sensitive to voxelwise measurement noise, hampering\nreliability and repeatability. We propose a self-supervised deep variational approach\nthat breaks the assumption of independent pixels, leveraging redundancies in the\ndata to effectively perform data-driven regularisation of quantitative maps. We\ndemonstrate that our approach outperforms current model fitting techniques in\ndMRI simulations and real data. Especially with a Gaussian mixture prior, our\nmodel enables sharper quantitative maps, revealing finer anatomical details that\nare not presented in the baselines. Our approach can hence support the clinical\nadoption of parameter mapping methods such as dMRI and qMRI. Our code is\navailable at https://github.com/moucheng2017/MRI-GMM-VAE.", "sections": [{"title": "1 Introduction", "content": "Multiple MRI techniques can produce quantitative maps of biophysical, chemical and physiological\ntissue properties. Such quantitative parameter mapping techniques include diffusion MRI (dMRI)\nand quantitative MRI (qMRI). dMRI and qMRI use an essentially identical approach; by acquiring\nmultiple images then fitting a model to the images, intrinsic values of the relevant tissue properties in\neach voxel can be estimated. In dMRI the images have different diffusion weightings and directions,\nand model fitting enables estimation of microstructural parameters, such as diffusivity and kurtosis."}, {"title": "2 Methods", "content": "Problem Setting We assume a dMRI dataset and model throughout the paper. We assume an\nobserved discrete series of, T, dMRI images, $S = {S^{(1)}, S^{(2)}, ..., S^{(T)}}$, where $S^{(i)}$ is a diffusion\nweighted image (DWI) with height, H, width, W, and depth, D. The DWIs are defined $S^{(i)} =$\n${s^{(i)}_{h,w,d};h \\in [1,2,.., H], w \\in [1,2,.., W], d \\in [1,2,.., D]}$, where $s^{(i)}_{h,w,d}$ is the signal at the\nvoxel (h, w, d) for the ith DWI. We aim to estimate the tissue properties, i.e. model parameters,\n$X = {x_{h,w,d}; h \\in [1, 2, .., H], w \\in [1, 2, .., W], d \\in [1, 2, .., D]}$ where the tissue properties for a\nvoxel located at (h, w, d) corresponds to the vector $x_{h,w,d} = (x^{(1)}_{h,w,d}, ..., x^{(M)}_{h,w,d})$ for the M tissue\nproperties. The model parameters are estimated with respect to the signal model. Traditionally, each\nvoxel would be estimated independently of all the other voxels, for example, the MLE at $(h_1, w_1, d_1)$\nis calculated by finding the parameters that maximise the probability of seeing the observed data; i.e.\nwhat maximises $p((s^{(1)}_{h_1,w_1,d_1}, ..., s^{(T)}_{h_1,w_1,d_1})|x_{h_1,w_1,d_1})$. This is performed independently to the\nMLE at $(h_2, w_2, d_2)$.\nLatent Variable Model In this work, we propose to jointly model the distribution of the voxels\ntogether as $p_{\\theta}(S)$ using a latent variable (z) model: $\\int p_{\\theta}(S|z)p_{\\theta}(z)dz$. Considering dMRI-specific\nformat, with independence assumption for expression clarity (we note that despite this assumption\nour proposed model does not treat voxels as independent due to the shared latent space): $p_{\\theta}(S) \\approx\n\\Pi_{t=1}^{T} \\Pi_{h=1}^{H} \\Pi_{w=1}^{W} \\Pi_{d=1}^{D} p_{\\theta}(s^{(i)}_{h,w,d}|z)p_{\\theta}(z)$. Our latent variable model enjoys two benefits.\nThe first benefit is, by conditioning the data from each DWI from each voxel $S^{(i)}_{h,w,d}$ on z, we\nabsorb all the arbitrary dependencies among voxels into z, a compact representation in a latent\nspace. In the latent space which has lower dimension than the data space, voxels are clustered with\ntheir close voxels, therefore inter-voxels information must be captured. The second benefit is, the\ncomplicated underlying distribution of $p_{\\theta}(S)$ can be learnt via learning a much simpler distribution\n$p_{\\theta}(z)$. However, the marginal distribution of $p_{\\theta}(z)$ is still intractable: $p_{\\theta}(z|S) = p_{\\theta}(S, z)/p_{\\theta}(S)$\nbecause the data density is unknown. To address this computational issue, we deploy a variational"}, {"title": "Univariate Gaussian prior", "content": "We start with a simple univariate Gaussian $N(0, 1)$ for prior $q(z)$ and\nour first implementation is called VAE-UniG. We first use the encoder to parameterize z from input\nobserved signals: $\\mu, Log(\\sigma^2) = \\theta_{Encoder}(S)$. Then, $p(z|S, \\theta_{Encoder}) \\approx N(\\mu, \\sigma)$. We then use a\ndecoder to map randomly drawn samples $z \\sim p(z)$ to physically-relevant dMRI model parameters\n(X) that are inputs to a closed-form dMRI model that reconstructs the MRI signal. We denote\nthe closed-form physics decoding process as $\\Phi(.)$. We emphasise that $\\Phi(.)$ can be any dMRI (or\nqMRI) model. The complete decoding process is: $p(S) = \\int p(\\Phi(X|z, \\theta_{Decoder}))$. The loss function is:\n$Log(p_{\\theta}(S)) \\geq E_{z \\sim p(z)} [Log(p(\\Phi(X|z, \\theta)))] - KL(p(z)||q(z))$. The likelihood of $Logp(\\Phi(x|z, \\theta)))$\nis measured as a mean squared error loss between estimated signals and raw input signals."}, {"title": "Gaussian Mixture Prior", "content": "Our second implementation is VAE-GMM with enhanced expressivity\nby considering a prior as a mixture of univariate Gaussians. We add an extra latent variable y\nfor controlling the index of the Gaussian component. The prior of y is chosen as a Categorical\ndistribution. The probabilistic model is: $p_{\\theta}(S) = \\int \\int_y p_{\\theta}(S|z)p(z|y)p(y)dydz$. In implementation,\nwe build our Gaussian mixture VAE (VAE-GMM) following a hierarchical order. We first need\nto parametrize the mixing coeffients of each Gaussian using Gumbel Softmax Maddison et al.\n[2016]: $c = Gumbel(\\theta_{Encoder1st}(S))$. Where c is a normalised vector indicating the weight for\neach Gaussian and the sum of the $c$ is 1. We then concatenate c with input to parameterize the\nparameters of Gaussians: $\\mu_\\kappa, Log(\\sigma^2) = \\theta_{Encoder2nd}(S, c)$. Also, $p(z|S, \\theta_{Encoder}) \\approx N(\\mu_{\\kappa}, \\sigma_{\\kappa})$.\nWhere $\\kappa$ means that the mean and the variance are for the K-th Gaussian. We apply the same\ndecoding process as in the last section. And the loss function now becomes: $Log(p_{\\theta}(S)) \\geq$\n$E_{z \\sim p(y/z)p(z|x)} [Log(p(\\Phi(X|z, \\theta)))] - KL(p(z, y)||q(z, y))$."}, {"title": "MRI physics models", "content": "We test our approach on two dMRI models, the mean signal diffusion kurtosis\nimaging (MS-DKI)Henriques et al. [2019] model and ball-stick model Behrens et al. [2003]. The\nnormalised signal for MS-DKI is given by: $\\Phi(b) = exp(-bD+ b^2D^2K/6)$, where b is the b-\nvalue, D is the diffusivity and K the kurtosis. For ball-stick the normalised signal is: $\\Phi(b, g) =$\n$f exp (-bD_{||}(g.n)) + (1 \u2212 f) exp (\u2212bD_{iso})$, where b is the b-value, g the gradient direction, f is\nthe stick volume fraction, $D_{||}$ is the parallel diffusivity of the stick, and $D_{iso}$ is the ball isotropic\ndiffusivity."}, {"title": "3 Results", "content": "Simulated Data We use least squares fitting and voxel-wise self-supervised fitting (a 3-layer fully\nconnected network Lim et al. [2022]) as baselines. We first test our method on a simulated MSDKI\ndataset with 3 clusters. We chose each cluster's diffusivity and kurtosis to mimic white matter,\ngrey matter, and CSF; the mean D, K values for each cluster were {1,1.5}, {1.5, 1}, and {3,0}\nrespectively, with diffusivity in units of \u00b5m\u00b2/ms. We simulated 10,000 voxels, with relative weightings\nof each cluster {0.5, 0.4, 1}. The specific ground truth parameter value was simulated from a Gaussian\nwith the relevant mean D and K for that cluster, and variance 0.1 for white matter and grey matter\nclusters, and 0.01 for the CSF cluster. As shown in the Fig. 1, across different settings of signal-noise\nratio (SNR), our model's predictions (row 2) are much better diagonally aligned, meaning that our\nlatent variable model consistently outperform the voxel-wise self-supervised baseline (row 1).\nReal Data We as well test our model on real data with ball-stick fit, with publicly-available dMRI\ndata from HCP WU-Minn ConsortiumVan Essen et al. [2012]. We used preprocessedGlasser et al.\n[2013] data from a single subject from the 1200 Subjects Data Release (Release Date: Mar 01,\n2017, available online at http://humanconnectome.org). As shown in the 1st row in Fig. 2, when\npredicting the diffusivity parameters, both of our models can drastically reduce the background\nnoises in comparison with the baselines. Interestingly, our model VAE-GMM (4th column) can even\ndiscover new anatomical structures, as seen in highlighted areas in row3 in Fig. 2, showing potential\nclinical application promises. Evidently, our model VAE-GMM also successfully captured at least\ntwo mixing components of the latent variable, as shown in Fig. 3."}, {"title": "4 Conclusion", "content": "We introduce a deep VAE model fitting method that exploits data redundancies to maximise informa-\ntion extraction in parameter mapping techniques like dMRI and qMRI. Our approach outperforms\nbaseline methods in both simulated and real data using the ball-stick model, improving ground truth\nestimations and revealing new anatomical details. It can enhance existing acquisition sequences\nfor better tissue maps or shorten scan times without loss of quality. However, limitations include\nvariability in parameter maps and trade-offs between parameter accuracy. Future work will focus on\ngeneralising the method to handle arbitrary acquisition schemes."}, {"title": "6 Impact Statement", "content": "Medical physics plays a pivotal role in advancing physical sciences for the betterment of humanity.\nAmong the many branches of this field, Magnetic Resonance Imaging (MRI) stands out as a critical\narea of study due to its wide-ranging applications in diagnostics, treatment planning, and medical\nresearch. MRI has revolutionized healthcare, enabling non-invasive imaging with detailed insights\ninto the human body, making it indispensable for modern medicine. Our proposed method aims\nto further enhance MRI parameter mapping, improving both accuracy and efficiency. This has\nthe potential to significantly impact the medical community by providing more reliable tools for\ndiagnosis and research. Moreover, the method is versatile and can be adapted for use in a wide variety\nof parameter mapping applications beyond MRI, broadening its utility across multiple domains in\nmedical physics and other scientific fields. By offering a solution that is both robust and adaptable, our"}, {"title": "A The architectures of the neural networks", "content": "Figure 4: Architectures of our model implementations. Row 1: with univariate prior. Row 2: with\nGaussian mixture model prior. The encoder is three fully connected layers. The decoder is one fully\nconnected layer."}, {"title": "B Results on MSDKI MRI model", "content": "Figure 5: Comparisons on MS-DKI fits on HCP dMRI subject. Our approach has less obvious\nimprovements when the MRI model is relatively simple, but doesn't hallucinate spurious anatomical\nfeatures."}, {"title": "C Results on the sensitivity of hyper parameters", "content": "Figure 6: Hyper-parameter sensitivities of our model with diffusivity results on simulated model\nusing MSDKI, comparisons between self-supervised baseline and ours with Gaussian prior. X axis:\nground truth of simulated diffusivity. Y axis: prediction of diffusivity. We observe that both latent\ndimension and kl loss strength have optimal values, but those values might be data dependant."}, {"title": "D Results on kurtosis estimation of MSDKI with VAE and simulated data", "content": "Figure 7: Kurtosis results on simulated model using MSDKI, comparisons between self-supervised\nbaseline and our VAE-UniG. X axis: ground truth of simulated diffusivity. Y axis: prediction of\ndiffusivity. SNR: signal-noise-ratio. Blue: cluster 1. Orange: cluster 2. Green: cluster 3. Ours vastly\noutperforms the baseline in recovery of the ground truth."}]}