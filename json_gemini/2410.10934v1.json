{"title": "Agent-as-a-Judge:\nEvaluate Agents with Agents", "authors": ["Mingchen Zhuge", "Changsheng Zhao", "Dylan Ashley", "Wenyi Wang", "Dmitrii Khizbullin", "Yunyang Xiong", "Zechun Liu", "Ernie Chang", "Raghuraman Krishnamoorthi", "Yuandong Tian", "Yangyang Shi", "Vikas Chandra", "J\u00fcrgen Schmidhuber"], "abstract": "Contemporary evaluation techniques are inadequate for agentic systems. These approaches either focus\nexclusively on final outcomes ignoring the step-by-step nature of agentic systems, or require excessive\nmanual labour. To address this, we introduce the Agent-as-a-Judge framework, wherein agentic\nsystems are used to evaluate agentic systems. This is an organic extension of the LLM-as-a-Judge\nframework, incorporating agentic features that enable intermediate feedback for the entire task-solving\nprocess. We apply the Agent-as-a-Judge to the task of code generation. To overcome issues with\nexisting benchmarks and provide a proof-of-concept testbed for Agent-as-a-Judge, we present DevAl,\na new benchmark of 55 realistic automated AI development tasks. It includes rich manual annotations,\nlike a total of 365 hierarchical user requirements. We benchmark three of the popular agentic systems\nusing Agent-as-a-Judge and find it dramatically outperforms LLM-as-a-Judge and is as reliable as\nour human evaluation baseline. Altogether, we believe that Agent-as-a-Judge marks a concrete step\nforward for modern agentic systems-by providing rich and reliable reward signals necessary for\ndynamic and scalable self-improvement.", "sections": [{"title": "1 Introduction", "content": "Recent years have seen multimodal agentic systems move from occasionally being able to solve small toy\nproblems to being regularly deployed for challenging real-world problems (the dream of most AI research).\nYet, the current evaluation methods and the available benchmarks for agentic systems are struggling to keep\nup with these rapid advances, dramatically slowing true progress.\nWe believe that the current issue with evaluating agentic systems stems from the lack of feedback during the\nintermediate task-solving stages for these nontraditional systems. Agentic systems think more like humans,\noften act step-by-step (Wooldridge, 1999) and often host very human-like symbolic communications internally\nto solve problems (Zhuge et al., 2023). And thus agentic systems should be evaluated like a human, with\nrich evaluative feedback which looks at the full thought and action trajectory; evaluating an agentic system\nin the traditional way is like evaluating a student using multiple-choice testing-a comparatively unreliable\nestimator (Park, 2010). For example, while SWE-Bench (Yang et al., 2024a) is widespread, its evaluation\nmethod, which relies solely on the final resolve rate for long-term automated repair tasks, does not effectively\npinpoint what is happening within agentic systems that affects the resolve rate. On the other hand, performing\na better evaluation with a human is prohibitively expensive. We instead propose that agentic systems should\nbe used to evaluate agentic systems. Inspired by LLM-as-a-Judge (Zheng et al., 2024; Fu et al., 2023; Chen\net al., 2024b), which uses LLMs to evaluate LLMs, we call this framework Agent-as-a-Judge, of which it is\n1"}, {"title": "2 DevAl: A Dataset for Automated Al Development", "content": "In this section, we introduce our new DevAI benchmark. We then evaluate three state-of-the-art code-\ngenerating agentic systems on this benchmark in Section 3 and present their basic statistics.\n2.1 Motivation\nBackground The code generation domain is an area where agentic systems have seen significant industrial\ndeployment over the past two years (e.g., Devin\u00b9 and Cursor\u00b2). However, in code generation, there isn't yet a\nbenchmark that accurately reflects realistic user queries for developing complete AI systems. We believe this\nis because of the difficulty to evaluate such complex, real-world tasks. For example, while many companies\nadvertise their systems based on its performance on benchmarks such as SWE-Bench (Yang et al., 2024a) (for\nautomated repair) or HumanEval (Chen et al., 2021) (for algorithmic problems), these benchmarks cover only\na small bit of an actual development process. Moreover, none of them accurately reflect the intermediate\nstages of development or provide sufficient reward signals for long-horizon development-similar issues are\npresent in OpenAI's recent MLE-Bench (Chan et al., 2024). A benchmark that can evaluate the entire\ndevelopment process ideally in a way that can help understand the degree to which current AI methods can\nreduce human labour is missing.\nTopic We chose automated AI development as our main topic. While AI and ML tasks are often more\ncomplex, they follow clear, standard procedures. For example, data processing typically comes first in an\nAI pipeline, and performance reporting goes at the end. We believe this topological nature can help better\nmonitor the development process and provide useful signals to the agentic systems.\nGoals An ideal benchmark should address critical issues in automated development by focusing on three key\nfactors. First, it should reflect practical software scenarios, where tasks are often too complex for a single\nLLM, requiring human or agentic systems. Second, it should emphasize the development process, not just\nfinal outcomes (e.g., pass@1 rates offer limited feedback and fail to highlight intermediate problems). Lastly,\nthe evaluation should be computationally cost-effective and efficient, avoiding long training times or excessive\nmanual oversight.\n3"}, {"title": "2.2 The DevAl Dataset", "content": "Motivated by the ideas outlined above, we propose the DevAI dataset. DevAI consists of a curated set of 55\ntasks, each defined by (1) a plain text user query that describes an AI development task; (2) a set of plain\ntext requirements (for a total of 365 requirements), each with a set of dependencies connecting them to other\nrequirements; and (3) a set of preferences (for a total of 125 preferences) which represent softer requirements.\nDevAI is structured so that an agentic system starts by receiving a user query to begin development. The\nsystem is then evaluated on how well it meets the requirements, with preferences serving as optional, softer\ncriteria. An example of one of the DevAI tasks can be seen in Figure 3.\nThe tasks in DevAI are relatively small-scale but cover commonly used key development techniques. As shown\nin Figure 2, our tasks are tagged and cover a variety of key areas in AI: supervised learning, reinforcement\nlearning, computer vision, natural language processing, generative models, and others. Each of the tasks\nis a real-world problem that could be given to a research engineer, while simultaneously being relatively\ninexpensive computationally to run so as to reduce the cost of evaluating a method on this benchmark. Details\nof the sample collection and human labeling process for DevAI are provided in Appendix E.\nThe requirements belonging to each task represent a milestone in the comprehensive development process and\nare arranged as a directed acyclic graph (similar to the work by He et al. (2021)), with requirements such\nas visualizing results depending on correct data loading and modeling. This allows for more comprehensive\nnon-sparse feedback than a binary success metric. Furthermore, the inclusion of hierarchical requirements\nmakes simple memorization an inadequate solution strategy, as completing the entire task requires agentic\ncapabilities rather than relying solely on symbolic memorization, as is typical in foundation models.\n2.3\nPreliminary Benchmark\nWe first conduct experiments to collect development outcomes from different frameworks, which serve as\nbaselines in the DevAI dataset. We test three of the most popular open-source frameworks (which we\n4"}, {"title": "3 Human-as-a-Judge: Manual Evaluation on DevAl", "content": "To determine the pragmatic validity of DevAI and to accurately estimate the actual code-generating abilities\nof current state-of-the-art agentic systems, in this section, we run and then manually evaluate the application\nof three AI developer baselines to DevAI. In Section 4, we show how this evaluation can be automated."}, {"title": "3.1 Benchmark Baselines by Human-as-a-Judge", "content": "Human Evaluation Setup After obtaining the baseline executions and conducting basic statistical analysis,\nwe have three expert human evaluators (referred to here by their anonymous names: 231a, 38bb, and cn90)\nreview the outputs of AI developer baselines to assess whether each requirement was satisfied. We have two\nrounds of human evaluations. To capture the bias inherent in typical human evaluation (this is desirable to\ncapture here as it represents a likely scenario in deployment), in the first round, our evaluators first discussed\nthe basic standards but were given minimal instructions. The templates the evaluators were given for the\nevaluation and their self-reported post-hoc descriptions of how they resolved ambiguities are reported in\nFigure 12 in Appendix H.\nAfter the initial round of human evaluations (which totaled an estimated total of 58 human hours), we asked\nour evaluators to discuss and reach a consensus on their assessments (which took an estimated total of 28.5\nadditional human hours). This consensus, achieved after long sessions of debate, was used as the final human\nevaluation result for each method.\nPerformance Analysis The results of this experiment are shown in Table 2. We found that the two best-\nperforming methods (GPT-Pilot and OpenHands) could satisfy about 29% of the requirements (or around\n44% if prerequisites are ignored) but only on one task could they meet all the requirements. This highlights\nthat DevAI offers a considerable but appropriate level of challenge for current and future methods. Moreover,\nthe fulfillment of intermediate requirements aligns with our expectations, as discussed in Section 2, that\nDevAI provides richer feedback by uncovering how agentic systems falter during the process instead of just\nfocusing on a single performance metric at the end.\n3.2 Judging Human-as-a-Judge"}, {"title": "4 Agent-as-a-Judge: Evaluating Agents with Agents", "content": "Human evaluation, while somewhat reliable, is time-consuming and requires substantial expertise. To address\nthis, we propose the Agent-as-a-Judge framework. If such an agentic system could evaluate like a human, it\nwould reduce the need for human involvement and eliminate the trade-off between thoroughness and effort.\n4.1 Proof-of-Concept\nBased on our prior experiences with agent design and by imitating the human evaluation process, we initially\ndesigned eight modular, interacting components that form the foundation of our Proof-of-Concept for the\nAgent-as-a-Judge.\n(1) The graph module constructs a graph that\ncaptures the entire structure of the project, includ-\ning files, modules, and dependencies. It can also\nbreak down chunks of code into code snippets. (2)\nThe locate module identifies the specific folder\nor file referred to by a requirement. (3) The read\nmodule goes beyond simple file parsing, support-\ning the reading and understanding of multimodal\ndata across 33 different formats, including code, im-\nages, videos and documents. This allows the agent\nto cross-reference various data streams and verify\ndifferent kinds of requirement. (4) The search\nmodule provides a contextual understanding of\ncode and can quickly retrieve highly relevant code\nsnippets, as well as the nuances behind them (e.g.,\nhidden dependencies). (5) The retrieve module"}, {"title": "4.2 Judging Agent-as-a-Judge and LLM-as-a-Judge", "content": "Judge Shift Judge Shift measures deviation from the Human-as-a-Judge consensus results, with lower values\nindicating a closer alignment. As shown in table 3, Agent-as-a-Judge consistently outperforms LLM-as-a-Judge\nacross tasks, particularly those with task dependencies. For example, in Requirement (I), Agent-as-a-Judge\nshows a Judge Shift as low as 0.27%, while LLM-as-a-Judge reaches 31.24% for OpenHands. This underscores\nAgent-as-a-Judge's stability and suitability for meeting task requirements. Furthermore, in the gray-box\nsetting, both Agent-as-a-Judge and LLM-as-a-Judge show even better results than their performance in the\nblack-box setting.\nAlignment Rate The Alignment Rate reflects how\nclosely the AI Judges' evaluations align with human\nconsensus across all 365 requirements. It is defined\nas the percentage of requirement evaluations that\nare the same as the Human-as-a-Judge consensus\nevaluation. Compared to LLM-as-a-Judge, Agent-\nas-a-Judge consistently achieves a higher Alignment\nRate, closely matching human judgments. For\nexample, when evaluating OpenHands, Agent-as-a-\nJudge reaches 92.07% and 90.44%, surpassing LLM-\nas-a-Judge's 70.76% and 60.38% in both gray-box\nand black-box settings. This shows that Agent-as-a-\nJudge produces more accurate and human-aligned\nevaluations, especially in complex scenarios.\nPR Curves Judging developer agents is a class-\nimbalanced task, where meeting requirements is\nmuch rarer than failing. Metrics like judge shift\nand alignment rate can be misleading. For example,\nsince MetaGPT rarely meets requirements, LLM-as-a-Judge easily identifies most cases as negative (achieving\n84.15% in the black-box setting). PR Curves offer a clearer performance measure by balancing precision and\nrecall. Agent-as-a-Judge even outperforms any single human evaluator on OpenHands and aligns closest with\nmajority voting. This shows that, in some cases, Agent-as-a-Judge can nearly replace human evaluators."}, {"title": "4.3 Ablations For Agent-as-a-Judge", "content": "We conduct ablations to evaluate the impact of adding different components on Agent-as-a-Judge's performance.\nThe components analyzed include ask, graph, read, locate, and retrieve. The component ablation study\nfor Agent-as-a-Judge reveals key insights into the performance gains from adding specific functionalities.\nWith only the ask component, the agent\nachieves a 65.03% alignment rate. Adding\nthe graph component increases performance\nto 75.95%, as the agent can better understand\nthe relationships between files.\nThe introduction of read further improves\nthe alignment rate to 82.24%, reflecting the\nvalue of direct access to the contents of the\nfile. Incorporating locate brings a substantial boost to 90.44%, as the agent can efficiently target files relevant"}, {"title": "4.4 Cost Analysis", "content": "The Human-as-a-Judge took the three evaluators a self-reported total of 86.5 hours. With a 15 USD minimum\nwage (assuming this would buy a subject expert in AI), a full evaluation under DevAI would cost around\n1297.50 USD. In comparison, Agent-as-a-Judge cost only 30.58 USD in API calls and took only 118.43\nminutes-2.29% of the cost and 2.36% of the time of Human-as-a-Judge. LLM-as-a-Judge was faster at 10.99\nminutes, but due to the absence of intelligent context selection by the Agent-as-a-Judge's modules, it still cost\n29.63 USD."}, {"title": "5 Related Work", "content": "Agentic systems and their applications are highly active research areas with numerous recent works having a\nrelation to this work. This section details those works most relevant to ours. We provide a treatment of the\nless relevant related works in Appendix D.\nAl Developers AI in software development is growing fast (Liu et al., 2024). AI-driven developers have been\napplied to directly imitate software companies (Hong et al., 2024b; Qian et al., 2024a), debug code (Yang\net al., 2024a), run data science methods (Guo et al., 2024; Hong et al., 2024a; Li et al., 2024; Qiao et al.,\n2023), and even write academic papers (Lu et al., 2024a).\nBenchmarks for Al developments Benchmarks like MLAgentBench (Huang et al., 2024), ML-Bench (Liu\net al., 2023c), SUPER (Bogin et al., 2024), DS-bench (Jing et al., 2024), and MLE-Bench (Chan et al., 2024)\nall focus on benchmarking agentic systems using AI tasks. However, DevAI distinguishes itself from all of\nthese by focusing on realistic user queries that target a complete development cycle. It further includes\na more comprehensive evaluation with multiple hierarchical requirements and preferences for each task.\nComparatively, MLAgentBench (Huang et al., 2024) for example, focuses on final performance for a limited\nset of well-known tasks, which risks overfitting and fails to assess a system's generalization or adaptability.\nAl Judges Several works have looked at using AI systems as judges\u00b3. The work by Chan et al. (2023); Zhao\net al. (2024), for example, extends LLM-as-a-Judge to have multiple LLMs in their evaluation process for\nconversational tasks. Unlike Agent-as-a-Judge, they employ a trivial agentic system and apply it only to\nevaluate LLMs under traditional evaluation setups. In contrast, (Lu et al., 2024b) uses a single LLM-based\nevaluator but, unlike LLM-as-a-Judge, applies this to multimodal tasks rather than just for evaluating LLMs.\nLess relevant are frameworks like those by Chen et al. (2024a); Arora et al. (2024); M\u00fcndler et al. (2024),\nwhere intermediate signals are used during coding development."}, {"title": "6 Discussion and Conclusion", "content": "Outlook 1: Intermediate Feedback for Agentic Self-Improvement A key power of the Agent-as-a-Judge,\nthough not fully exploited here but nonetheless clear, is that it provides intermediate feedback that is essential\nfor effective and efficient optimization (Zhuge et al., 2024). For example, Agarwal et al. (2019) proposes\nto solve the sparse reward problem in reinforcement learning, by learning auxiliary reward functions that\nprovide intermediate feedback. Perhaps the greatest strength of the Agent-as-a-Judge framework is that an\nagentic system can use it to identify and fix issues in its solutions to complex, multistage problems on the\nfly-something older, delayed-feedback methods did not permit.\nOutlook 2: Flywheel Effect Driven by Agent-as-a-Judge The cycle of mutual improvement between the\nAgent-as-a-Judge and the evaluated agents, where both evolve together through iterative feedback, presents a"}, {"title": "D Extend Related Work", "content": "Our main paper includes mostly related works of Al developers, Benchmarks for Al developments, and Al\njudges. However, the following works contribute significantly to the community and also relate to this work.\nWe record this work as additional related work.\nLLM-based Autonomous Agents Recent developments in LLM-based agents have expanded their capabilities\nbeyond simple task execution to more autonomous problem-solving and decision-making. AutoGPT (Gravitas,\n2023) and Lang Chain (Chase, 2022) provide frameworks for single-agent systems that leverage external tools\nfor more complex tasks. Similarly, research such as MetaGPT (Hong et al., 2024b), AutoGen (Wu et al., 2023)\nand CAMEL (Li et al., 2023) focus on role-based multi-agent communication, improving collaboration among\nagents. However, the challenge of maintaining coherence in agents' dialogue and preventing hallucination\nremains prominent (Du et al., 2024; Zhou et al., 2023). Most recently, using graphs to build agents has\ngained prominence. Earlier work like GPTSwarm (Zhuge et al., 2024) and LangGraph (langchain ai, 2024)\nproposed using nodes to represent operations and edges to represent the connections between them. In\nGPTSwarm, multiple agents represented as subgraphs in a graph are connected by optimizable edges, and\nreinforcement learning is employed to optimize the edges. Following this approach, several agent frameworks\nhave incorporated graphs into their designs (Hong et al., 2024a; Zhou et al., 2024; Qian et al., 2024b).\nAdditionally, various optimization methods have been developed to enhance agent performance further (Wu\net al., 2024; Song et al., 2024; Hu et al., 2024). In practical applications, many studies focus on understanding\nand interacting with GUIs (Wang et al., 2024a; Chen et al., 2024c; Yang et al., 2023; Xu et al., 2024; Tan\net al., 2024). For code generation agents (Jin et al., 2024), current research mainly emphasizes automated\nrepair (Yang et al., 2024a; Phan et al., 2024; Tao et al., 2024), computational modular design (Khattab et al.,\n2023; Cheng et al., 2024), and automated development (Tufano et al., 2024; Huang et al., 2023). Among these,\nopen-sourced frameworks like OpenHands (Wang et al., 2024d) have gained popularity due to their strong\nuser experience. Moreover, scientific discovery (Jansen et al., 2024; Lu et al., 2024a) and ML agents (Yang\net al., 2024b) are also receiving increased attention.\nLLM-as-a-Judge In the domain of AI evaluation and judgment, frameworks (Zheng et al., 2024; Fu et al., 2023;\nChen et al., 2024b) have pioneered the use of LLMs to assess conversational agents, demonstrating how LLMs\ncan evaluate dialogue quality and consistency. Expanding beyond dialogue, LLMs like CodeR (Chen et al.,\n2024a) and MASAI (Arora et al., 2024) apply similar judging principles to the code validation process, where\nAI systems autonomously evaluate and verify computer programs. Our work builds on these advancements\nby exploring how LLMs can perform more nuanced judgment tasks, further investigating their potential in\ndecision-making across various domains. Recent research also focuses on judging LLM-as-a-Judges (Chen\net al., 2024d; Bavaresco et al., 2024; Thakur et al., 2024; Dong et al., 2024; Shi et al., 2024; Raina et al., 2024).\nCoding Benchmarks Recent advances in code generation have led to the innovation of various benchmarks\nto evaluate model performance (Liu et al., 2024). Early benchmarks, such as MBPP (Austin et al., 2021),\nHumanEval (Chen et al., 2021), and MultiPL-E (Cassano et al., 2023), focus primarily on generating simple\nfunctions. While these benchmarks are useful for evaluating the correctness of generated code, they are limited\nin complexity and do not fully represent the challenges encountered in real-world software development.\nAs the field progressed, newer benchmarks began to focus on more complex and realistic tasks. APPS (Hendrycks\net al., 2021), CodeContests (Li et al., 2022), and LiveCodeBench (Jain et al., 2024) moved toward com-\npetitive programming challenges that involve advanced algorithms and data structures. These tasks are\nmore representative of problems encountered in coding competitions and help push models toward more\nsophisticated problem-solving. DS-1000 (Lai et al., 2023) was introduced to assess the skills of models with\ndata science libraries, evaluating their ability to use APIs and execute complex data analysis workflows.\nMeanwhile, AgentBench (Liu et al., 2023b) focuses on testing reasoning and decision-making abilities in\ninteractive environments, highlighting differences in performance between commercial and open-source models.\nTo address real-world programming needs beyond code generation, specialized benchmarks have been created\nto evaluate tasks such as debugging, refactoring, and code navigation. CANITEDIT (Cassano et al., 2024),\nDebugBench (Tian et al., 2024), and FixEval (Haque, 2023) evaluate the ability of a model to edit and"}, {"title": "K Ablations of Agent-as-a-Judge", "content": "K.1 Component Abalations\nAnalysis We designed 8 modular components for the Agent-as-a-Judge system. In the Table 5, components\nare added progressively from left to right. If the addition of a component led to a significant performance drop,\nwe removed it from further iterations. Our experiments showed that adding the components ask, graph, read,\nand locate resulted in significant performance gains. However, when the search component was introduced,\nthere was a noticeable decline in performance.\nWe hypothesize that the performance drop from search is due to its role in retrieving relevant code snippets\n(top-3) using BM25. The retrieval accuracy of BM25 (Robertson et al., 2009) might not have been high\nenough, potentially introducing noise. Moreover, as noted in Table 1, the DevAI tasks in our experiments\ndid not generate a large amount of code. In fact, even when all code was fed into an LLM, the total content\ntypically stayed within the maximum context length. Therefore, in simpler workspaces, search was less\ncritical. However, we believe this component will become more important as the complexity of the workspace\nincreases, making it more valuable in larger and more complex environments.\nWe also observed that the introduction of the planning mechanism did not bring a noticeable improvement\nin performance. This may be related to the nature of the Judge - it needs clean factual information. When\nplanning is unstable, the evidence collected from different actions can become inconsistent, leading to a\ndecline in performance. Finally, we experimented with a memory mechanism. Initially, we hypothesized that\nsince DevAI tasks often involve interconnected requirements, memory could help track whether requirements\nwere met. However, in practice, we saw no improvement. We suspect that the interconnected nature of the\nrequirements may have caused biases: specifically, once a prior requirement was fulfilled, it might have overly\ninfluenced positive judgments on subsequent requirements, even if they were not fully met.\nK.2\nSearch Algorithms in Search Module\nWe initially hypothesized that the performance drop was due to the low precision of the search component,\nparticularly with BM2.5. To explore this, we replaced BM2.5 with Sentence-BERT (Reimers, 2019) as a more\nadvanced alternative and tested Fuzzy Search (Levenshtein, 1966) as a less precise option. However, neither\nimproved the performance of the Agent-as-a-Judge."}, {"title": "K.3 Search Algorithms in Retrieve Module", "content": "In our experiments, we found that accurately locating relevant information within a trajectory is a challenging\ntask. Although the addition of the retrieve component (gray-box) did not lead to a significant improvement\nin performance in this specific case, its impact has been notable in other settings, such as in GPT-Pilot. As\nshown in Table 3, the integration of retrieve in GPT-Pilot brought substantial gains.\nWe conducted an ablation study on GPT-Pilot to optimize\nthe retrieval of useful information at each step. Our experi-\nments revealed that in large trajectories, truncating the final\nsections of the file often results in losing critical information,\nas the latter part of the trajectory typically contains dense\ninformation about the final development state. Truncating\nthe beginning of the trajectory proved to be the most effective\nin improving the retrieval efficiency.\nFor individual steps, truncating the middle section worked\nbest. This is because error messages usually appear early in\nthe output, while the corresponding file paths and specific\nerror locations are found towards the end. By focusing on\nthese retrieval strategies, we can significantly enhance the per-\nformance of the retrieve component, particularly in complex\nscenarios like GPT-Pilot."}, {"title": "L Prompt Demos of Agent-as-a-Judge", "content": "Here, we present some prompts used by the Agent-as-a-Judge system. Each of these prompt demos plays a\ncrucial role in guiding the agent's behavior.\nL.1 System Prompt for Agent-as-a-Judge"}]}