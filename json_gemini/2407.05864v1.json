{"title": "Neural Network-based Information Set Weighting for Playing Reconnaissance Blind Chess", "authors": ["Timo Bertram", "Johannes F\u00fcrnkranz", "Martin M\u00fcller"], "abstract": "In imperfect information games, the game state is generally not fully observable to players. Therefore, good gameplay requires policies that deal with the different information that is hidden from each player. To combat this, effective algorithms often reason about information sets; the sets of all possible game states that are consistent with a player's observations. While there is no way to distinguish between the states within an information set, this property does not imply that all states are equally likely to occur in play. We extend previous research on assigning weights to the states in an information set in order to facilitate better gameplay in the imperfect information game of Reconnaissance Blind Chess. For this, we train two different neural networks which estimate the likelihood of each state in an information set from historical game data. Experimentally, we find that a Siamese neural network is able to achieve higher accuracy and is more efficient than a classical convolutional neural network for the given domain. Finally, we evaluate an RBC-playing agent that is based on the generated weightings and compare different parameter settings that influence how strongly it should rely on them. The resulting best player is ranked 5th on the public leaderboard.", "sections": [{"title": "I. INTRODUCTION", "content": "GAME AI has achieved superhuman performance in many classic, fully observable games such as chess [12], Go [32], and backgammon [35], as well as in imperfect information games like Poker [10]. Imperfect information games, in contrast to fully observable ones, feature a game state that is only partly visible to a player, which increases their difficulty and makes them an attractive area for artificial intelligence research. For example, playing imperfect information games requires probabilistic mixed policies to avoid being predictable and therefore exploitable by other players. As a formal model of player's uncertainty, game theory for imperfect information games [38] clusters all states which one player cannot distinguish into an information set. However, while players can generally not identify the true game state, states within an information set are not equally likely in practice \u2013 sophisticated play by agents should reach some states more frequently than others. Furthermore, opponent modelling from observing past behaviour can give strong clues about the likelihood of their future actions.\nIn this work, we employ Siamese neural networks to learn a function that maps an information set I of game positions to a weight distribution, which translates to the probability of each state being the true game state. As a proof-of-concept of the utility of such a distribution, we create a simple player for the imperfect information game Reconnaissance Blind Chess, which plays the game based on the best perfect information responses on the boards of the information set, weighted by the obtained distribution.\nFirst, we briefly introduce Reconnaissance Blind Chess in Section II. We then formalise and motivate the approach of reducing the evaluation of an imperfect information state to a weighted evaluation of perfect information states in Section III. As a proposed method for this, we introduce our approach with Siamese neural networks and relate it to previous work in Section IV. The experimental setup including data preparation and the training process is described in Section V. In Section VI, we evaluate our Siamese network approach by comparing it to a conventional baseline; a convolutional neural network that directly estimates the probability of a board state given an information set. Lastly, we test the quality of our idea in a predictive setting (Section VII) and as an integral part of an actual agent (Sections VIII and IX).\nWhile the key ideas of information set weighting via Siamese neural networks have already been introduced in our prior work [4], this extended version of the IEEE Conference on Games 2023 conference paper elaborates on it in several aspects:\n\u2022 We add a more detailed motivation of the problem and discussion of the approach.\n\u2022 Section III-B offers a more elaborate review on related algorithms for imperfect information games.\n\u2022 Section VI introduces a new baseline to the paper. By comparing to a conventional neural network, we show that our contrastive model is more computationally efficient in testing and achieves higher overall performance.\n\u2022 Section VIII adds a more detailed account on how to use the weighting scheme in gameplay.\n\u2022 Section IX investigates a temperature parameter responsible for the smoothness of the weight distribution, i.e. setting the optimism of the agent."}, {"title": "II. RECONNAISSANCE BLIND CHESS", "content": "Reconnaissance Blind Chess (RBC) [18] is an imperfect information variant of classical chess. While the game pieces start in the same arrangement as in classical chess, players do not receive full information about their opponent's moves and are generally unaware of the exact configuration of their opponent's pieces. A turn of Reconnaissance Blind Chess consists of four parts:\n1) First, the player receives a limited amount of information about the latest opponent move: If the opponent captured one of the player's pieces, the player is only notified about the square where the capture occurred. For non-capture moves, no information is obtained.\n2) The player is allowed to sense a 3\u00d73 area of the board, which reveals all pieces in that region. Players never receive any information about the opponent's senses.\n3) With some minor differences, a player decides on a move as in conventional chess. Due to the missing information, players frequently cannot decide whether a move is legal.\n4) Finally, the player receives information about whether their chosen move succeeded.\nOne turn of RBC includes two separate actions \u2013 sensing and moving. The result of the sense is obtained immediately, and the move is chosen with that information. Due to the imperfect information nature of RBC, some rules differ from normal chess:\n\u2022 Players can attempt illegal moves. \u201cIf a player tries to move a sliding piece through an opponent's piece, the opponent's piece is captured and the moved piece is stopped where the capture occurred\u201d [36]. Other illegal moves are converted into a pass.\n\u2022 Players are not notified when their king is put in check. They are also allowed to move their king into check, and castle through and out of checks. There are no draws by stalemate, where a player is unable to move without putting their king into check.\n\u2022 Most games end by capturing the opponent's king. Rare draws result from 50 consecutive turns without game progress, as in classical chess.\nThese rules imply the following properties of RBC:\n\u2022 A player receives enough information to always perfectly know the placement of their own pieces.\n\u2022 Because of perfect recall, it is always possible to compute the current information set. However, the size of this set can become large, especially when sensing is ineffective or when players play erratically.\n\u2022 Speculative moves and aggressive strategies that directly launch a concealed attack towards the opposing king, which would be self-destructive in classical chess, become a major factor in RBC. Such strategies can win quickly if the opponent does not recognise and defend against them.\nWith these changes in mind, we now introduce the problem setting formally."}, {"title": "III. PROBLEM STATEMENT", "content": "A. Imperfect Information Games\nIn an extensive form imperfect information game, a game state e captures all information of the current situation, including the private information of all players. An information set I for a player is a set of states {e1,e2,...,e|I|} which are indistinguishable from that player's perspective. In the case of RBC, this is the set of boards, i.e. piece configurations of the opponent, that are consistent with the information a player has received throughout the game. Formally, the private observation history at player's turn t with Ot = (o0,..., ot) implicitly defines their information set It. In Figure 1, the observation history of the current player is depicted at the bottom, consisting of the current position of one's pieces, the name of the opponent, and the history of previous senses and moves, including information about whether the moves were successful, and whether pieces were captured by opponent's moves (see Table I). Of the boards in the information set, one is the correct board pt \u2208 It, whereas all other boards nt,i \u2208 It \\{pt} differ from the true hidden game state. In Figure 1, the correct board pt is shown on the left, whereas the boards nt,i are shown in the upper right corner.\nB. State-of-the-Art in Imperfect Information Games\nAs there are numerous algorithms used in imperfect information games, we can not give a complete overview of all of them. However, we briefly review a subset of the current state-of-the-art approaches for the most popular games.\nIn Poker, various forms of counterfactual regret minimization [41], such as CFR+ [6], Monte Carlo counterfactual regret minimization [20], [9], [10], or DeepStack [26] reign superior. Algorithms such as ReBel [8] use the same general framework but add reinforcement learning to CFR. Fictitious play-based algorithms [22] use deep reinforcement learning from self-play, but have not yet reached the same performance level as the previously mentioned algorithms. However, newer self-play reinforcement learning approaches [40] are more light-weight than CFR and have shown promising results.\nStratego has similar imperfect information components as Poker but requires long-term strategy. Here, the current state-of-the-art algorithm, DeepNash [27], uses deep reinforcement learning and self-play. It does not use search due to the high difficulty of estimating the opponent's hidden information.\nCommercial card games such as Hearthstone and Magic: The Gathering inherently feature a vast amount of imperfect information. So far, few complete agents for those games exist, and the majority of research has been focused on pre-gameplay tasks such as card balancing and deck building. Recently, one of the first human-level agents for Hearthstone [39] was constructed based on fictitious self-play and Monte Carlo Tree Search. Generally, MCTS can be applied to imperfect information games with some adaptions [16].\nFaster, commercial, non-sequential games feature imperfect information through obstruction of the visible game space. In games such as Starcraft and Dota [1], [37], large-scale deep reinforcement learning is typically used, and hidden information is regarded as an intrinsic part of the environment."}, {"title": "C. Weighting Information Sets", "content": "In several imperfect information games, remarkable performance has been achieved by basing the imperfect information gameplay, whether implicitly or explicitly, on perfect information evaluations of states in an information set. Bl\u00fcml et al. [5] show that proficient gameplay can be achieved in imperfect information games with an approach akin to AlphaZero [33], in which they fuse policies across different states. Bertram et al. show that one can directly formulate a policy using the information set as an input [3], and many Monte Carlo Tree Search-based methods individually evaluate states of an information set [11] before combining the results. Additionally, strong programs in RBC usually rely heavily on classical engines for evaluating chess positions [18], [29], [19], using the idea of approximating the public state evaluation with the expected value of the states in the information set.\nThe idea behind such approaches is to approximate the evaluation g(m, s) of a move m in an imperfect information state s with the weighted sum of the evaluations of the resulting perfect information game states in the information set Is:\n$$g(m, s) \\approx \\sum_{e\\in I_s} w_{e,s} \\cdot f(m, e)$$\nwhere f(...) is a perfect information evaluation of a move in a state of the information set and we,s is the weight assigned to that position. In RBC, f(.,.) is often the evaluation of a chess position using a conventional chess program such as Stockfish. A simple weighting can be achieved by considering all states e in the information set as equal (we,s = 1/|Is|), as has, e.g., been done in [5] with surprisingly strong performance. Other choices are to only use the most likely state and ignore all others (we,s = [e = arg max we,s]) [4] or to interpret the weights as a probability distribution (we,s = P(e|s)), in which case Equation (1) computes the expected value of the given move over all states in the information set.\nIt is important to acknowledge the limitations of basing imperfect information policy fully on perfect information evaluations. An example of such a case is shown in Figure 2: Assume that white knows that the Knight was previously on e5 and now moved to a different square, threatening to take the King from either d3 or f3. Thus, the information"}, {"title": "D. Learning Information Weightings", "content": "The goal of this work is to find useful weights wi for Equation (1) by learning a function F : I \u2192 [0,1] which maps each ei \u2208 I to a weight wi = F(ei) and captures the likelihood that ei is the true state in the current situation. F is trained from past game data including each player's observations and the full true game state information at each move. One approach to this is to train a binary classifier, which maps the currently known set of observations Ot and a given position eti in the information It to a classification signal, which indicates the probability that this position is the correct one (et,i = pt). We believe that such an approach has several disadvantages, due to the skewed distribution of the dataset and the high evaluation complexity, which our experiments in Section VI support. Instead, we tackle this problem by training a Siamese neural network."}, {"title": "IV. SIAMESE NETWORKS FOR INFORMATION SET WEIGHTING", "content": "From each observation Ot we can create a total of |It| - 1 triplets of the form\u3008Ot, Pt, nt,i), which indicate that in It, the positive example pt was the true game state, while each of the negative examples nt,i was not. These triplets are used to train a Siamese neural network (shown in the middle of Figure 1) with the triplet loss. The goal of training for the Siamese network is to embed the inputs such that the positive example pt is positioned closer to the observation history Ot in the embedding space than the negative examples nt,i.\nA. Siamese Neural Networks for Imperfect Information Games\nTraditionally, Siamese neural networks [7], [13] are used to compare the strength of a relationship of several options to an anchor. One famous application is one-shot learning in image recognition [23], [30], where a network is trained to model that the positive image p is more similar to an anchor image a than a negative image n.\nGiven triplets (a,p, n), comparisons are constructed by feeding all three items e \u2208 {a,p,n} to a neural network F, resulting in high-dimensional output embedding vectors F(e). The parameters of F are trained such that distances between these embeddings model the strength of the relationship of the inputs, with the goal that dp < dn, as illustrated in the centre of Figure 1.2 This is commonly achieved by minimising the triplet loss.\n$$L_{triplet}(a, p, n) = max (dp - dn + m, 0) .$$\nHere, the margin m controls the minimum distance difference between the positive and negative examples necessary to achieve zero loss. As shown at the top of Figure 1, we set m = 1. This parameter is based on heuristic results, although we found little difference between choices. For the distance metric, we chose the Euclidean distance.\nAs noted above, we apply a Siamese network to model the weights of each specific board state of an information set, using the observation history as the context, with training triplets of the form (Ot, Pt, nt,i). Critically, in contrast to previously discussed tasks, the imperfect information setting usually does not possess a single \u201ctrue answer\". Which board occurs in a game depends on the non-deterministic move choice of the opponent. However, our network attempts to learn from historical training data which board states are more likely, creating a probability distribution rather than deciding on a single state.\nAs a second key difference between the image recognition setting and our use case, all inputs to a Siamese image recog-nition network usually share a common vector representation."}, {"title": "V. EXPERIMENTAL SETUP", "content": "A. Preparation of Gameplay Data\nWe obtained 582 450 games of RBC as training data for our network [36]. Each recorded game includes a turn-by-turn list of all observations received from each player's perspective, and supplementary information such as the name of the opponent. From this information, we form the anchors Ot shown at the bottom of Figure 1. As mentioned, it is possible to fully reconstruct the information set for each player and action, but for some players, mainly naive or malfunctioning ones without a reasonable sensing strategy, the information set can grow too large, so we limit them to 5000 boards. In addition to the information set and the observations, we also extract the board that represents the true underlying game state. Together, the observations, the true (positive) board, and one wrong (negative) board form the triplets used for training the neural network (Figure 1). This results in a total of 27 million samples, which are split 90/10 into training and test data to compute an out-of-sample accuracy estimate for the trained neural network. In training, we take precautions to prevent oversampling decisions with large information sets (see Section V-D).\nB. Representation of Boards and Observations\nEach chess position is represented by a 12 \u00d7 8 \u00d7 8 bit tensor which encodes the occurrences of the 12 different chess pieces. This representation omits some details, such as castling rights and turn numbers, but captures the vast majority of information. We represent a truncated history of observations as follows: For each turn of the game, a 90 \u00d7 8 \u00d7 8 bit tensor (Table I) encodes all information received from one player's point of view [3]. The majority of this encoding is taken up by specifying the last-requested move by the player [33]. We truncate the observation history to the 20 most recent turns, padding the input if fewer turns were played and discarding any turns further in the past. Finally, we one-hot encode the 50 most prominent opponent's names in 50 8\u00d78 planes, since this information can have a large influence on the policy of an agent [14]. If the opponent's name is not in this list of players, all 50 planes are set to zero. Thus, the total observation representation is 20 \u00d7 90+50 = 1850 bit tensors of size 8\u00d78.\nC. Neural Network Architecture\n1) Encoding Networks: To translate the two different repre-sentations of boards and observation histories into a common input to the Siamese network, we use two small convolu-tional neural networks that differ in input but share the same output format. In training, the boards and observations are transformed using their respective encoding network (blue and green in Figure 1) before reaching the common Siamese neural network (orange). They are basic convolutional neural networks with 5 layers, 64 filters per layer, and the ELU activation function [15], only differing in the shape of the"}, {"title": "VI. COMPARISON TO CONVENTIONAL NEURAL NETWORK", "content": "Although using pairwise comparisons to train the model is intuitive based on the raw information in the data, one can also train a classical, non-Siamese neural network similarly. We thus provide a baseline reference of performance with a classical, non-Siamese convolutional neural network. We compare both approaches, which mostly differ in the loss function used, and highlight the strength of our Siamese architecture.\nA. Learning RBC with a Convolutional Neural Network\nTo provide a comparative baseline, we model the training of the convolutional neural network closely to the training of the Siamese network. W we use the same architecture, data, and training process as outlined in Section V-A with the following changes:\n\u2022 Instead of training with triplets, we use a concatenation of the observation history Ot and either the true board pt or an incorrect board nt, which forms a single input of shape 1862 \u00d7 8 \u00d7 8.\n\u2022 The network learns with binary classification and is trained to output 1 if the input contains pt and 0 if it contains nt.\nB. Limitations\nOne immediate problem with this approach is an increase in the computational complexity of weighting an entire in-formation set. Both the Siamese and the single convolutional network evaluate the information set via a forward pass of the observation history and every game state in the state. However, the observations have to be passed through the Siamese net-works only once and can then be used to compute all distances in the embedding space (Figure 3). With the conventional architecture, observations need to be paired with every single board in the information set, unnecessarily bloating the input and causing computational overhead.\nTo a lesser extent, the Siamese network architecture may also provide additional functionality than just generating weights. While we solely focus on the distances of the embeddings here, the embeddings themselves are informative"}, {"title": "VII. PREDICTIVE EVALUATION", "content": "As a first evaluation of our approach, we use the dataset from Section V-A to test the accuracy of the network's predictions on a held-out test set. In Section IX, we add an evaluation through real game-play.\nFor each game position in the test set, we embed all states in the information set and the observation history with the Siamese network (see Figure 1). In the embedding space, we rank all states in ascending order based on their distances to the observations and check the position of the true board in the ranking.\nWe report two metrics:\n\u2022 top-k-percentage accuracy: how often is the true board ranked in the top k-percent boards? We choose this over top-k accuracy, as the size of information sets varies greatly.\n\u2022 pick-distance: the position of the true board in the ranking of all boards in the information set.\nWe compare our results to four baseline rankings: random ranking, ranking by the evaluations of the boards given by Stockfish, ranking by using the internal evaluation of Strange-Fish2, and ranking using the probabilities of the classical CNN from Section VI. Using classical chess engines for ranking assumes that the opponent is more likely to play strong moves, thus leading to boards that have higher evaluations. While such evaluations cannot directly translate to RBC evaluations, and often give vastly differing results, most current strong RBC agents use a classical chess engine [18], [29]. Here, we especially regard StrangeFish2 [28], the currently strongest player on the public leaderboard, which uses Stockfish with several RBC-specific adaptations.\nFigure 4 shows the top-k-percent accuracy of the five meth-ods. Ranking boards by their Stockfish evaluation achieves higher accuracy than random ranking, and the additional adap-tations of StrangeFish2 result in slight further improvements. While training a CNN to explicitly model the likelihood of a given position gives a better accuracy, the Siamese network achieves the best performance overall. In more than half of the samples (52.91%), the true board is ranked first and in 98% of cases, the true board is ranked in the top half of all options. Note that while the Siamese network is able to achieve better weightings and board predictions than the algorithm in StrangeFish2 provides, this does not necessitate that our agent plays better overall. In this experiment, we solely compare the weighting scheme itself on a held-out test-set. This does not involve gameplay and how the weighting is used in practice. We provide a gameplay-based test in Section IX.\nFigure 5 provides more insights into the performance of"}, {"title": "VIII. A SIAMESE RBC AGENT", "content": "To see the potential gain of information set weighting, we implement and evaluate an RBC agent that strongly utilises the Siamese neural network to aid sense and move selection. An overview of the basic concept of the player is shown in Figure 6.\nBuilding an RBC agent requires three main components: (i) handling information received throughout the game, (ii) choosing a sensing action, and (iii) selecting the move to be played. Our agent is built around the idea of tracking the information set of board states, starting with the initial piece configuration and adding and removing states based on the received information. The trained Siamese network is heavily used for both sensing and moving. When sensing, the agent aims to minimise a weighted measure of the information set size, which is achieved by computing the weighted number of board conflicts per sensing square (see Section VIII-B). To choose a move, the agent uses Stockfish\u00b3 to evaluate the strength of a move on each board, and weights them to receive an overall evaluation of a move across the information set (see Section VIII-C).\nA. Handling Information Sets\nThe majority of strong RBC agents are based on tracking the information set of board states I [29]. For computing board probabilities, our agent also needs to track I. After each opponent's turn, the set of possible states is replaced by all possible states that could follow each of the boards in the previous set. If the opponent captured a piece, this set decreases in size, as only a limited number of previous states allow capturing pieces, while non-capture moves greatly increase the cardinality. Whenever the agent itself senses or moves, it removes boards from the information set that are inconsistent with the new observations. Crucially, efficient sensing is important, as the information set can quickly grow unmanageably if too few states are removed.\nB. Choosing a Sensing Action\nMost RBC agents mainly sense to reduce the information set size [29] and our agent follows this practice. To sense, a weight distribution over the information set is computed from the distances in the embedding space created by the Siamese network. The distances are then divided by a pre-set tempera-ture parameter, that scales the optimism of the agent towards the weights (see Section VIII-D), and fed into a Softmin to receive normalised weights. Based on this, the agent computes a score for each of the possible sensing locations.4 The score estimates the expected number of board state eliminations based on conflicts of possible sensing results, weighted by the probability of each of the top 100 likeliest boards (see Algorithm 1). Finally, the area with the highest score is chosen.\nC. Choosing a Move\nAfter receiving new information from the sense, the agent appends this observation to its history, which shifts the anchor in the embedding space. Then, the agent again computes"}, {"title": "IX. PLAYING RECONNAISSANCE BLIND CHESS", "content": "Finally, we evaluate the agent described in Section VIII on the publicly available RBC leaderboard [36]. The RBC leader-board is a freely available service which provides automated testing against all other currently active agents. While active players change, a small selection of agents is always con-nected, whose strength varies from rather basic to state-of-the-art. This ensures that players can be assigned a representative Elo rating, albeit with some variance.\nWe previously found that using the Siamese weighting performs significantly better than the baselines [4], and extend the comparison by exploring different temperature settings. The result of this test is shown in Figure 7, where Siamese {t} signals that this version uses a temperature of t.\nWith Figure 7 we can confirm that neither a very sharp nor a very flat weight distribution leads to optimal performance. Rather, a moderate setting of t = 10 performed best with t = 5 and t = 25 as the two next best settings. While sample sizes are low, the win rates against specific opponents show the influence of the temperature parameter. Against attacker, a confident defence is crucial to success, thus the flat distribution leads to a large drop in performance. All other tested agents are able to easily exploit this opponent, but a too-high temperature hinders the predictive ability of the model. When directly comparing our agents, t = 10 is the only one that achieves a positive win rate against the others Siamese agents, and also receives the overall best performance on the leaderboard. At the time of writing, our agent Siamese 10 is ranked #5 on the public leaderboard."}, {"title": "X. CONCLUSION AND FUTURE WORK", "content": "We trained a Siamese network to estimate situation-specific probabilities of states in an information set. Our experiments show that the network is able to perfectly identify the true state in many cases, even when the information set is large. In addi-tion, we see that this allows for a perfect information approach to the imperfect information game of Reconnaissance Blind Chess. We build an agent that evaluates a set of moves on all possible boards with Stockfish and combines the resulting evaluations with the weight distribution obtained from the Siamese network. This approach prunes unlikely situations, which leads to overall better play. Thus, we reduce RBC to evaluations of classical chess and show good performance, such that our agent is currently ranked among the strongest on the public leaderboard.\nDespite this, we consider the fact that moves are still selected using Stockfish to be the main weakness of our current approach. In particular, RBC-specific policies, such as strategies using speculative attacks on the opponent's king, can not be generated in a setting where moves are obtained from a classical chess engine. Changing the final move selection, for example, to an RBC-specific neural network, or conducting a search with our learned probability distributions, may further improve our agent.\nAlthough only tested in one domain so far, our underlying method is general and can be used in other applications for weighting information sets. We intend to further explore future use in other imperfect information decision-making tasks."}]}