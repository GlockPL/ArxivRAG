{"title": "Task-oriented Over-the-Air Computation for Edge-device Co-inference with Balanced Classification Accuracy", "authors": ["Xiang Jiao", "Dingzhu Wen", "Guangxu Zhu", "Wei Jiang", "Wu Luo", "Yuanming Shi"], "abstract": "Edge-device co-inference, which concerns the coop- eration between edge devices and an edge server for completing inference tasks over wireless networks, has been a promising technique for enabling various kinds of intelligent services at the network edge, e.g., auto-driving. In this paradigm, the concerned design objective of the network shifts from the traditional communication throughput to the effective and efficient execution of the inference task underpinned by the network, measured by, e.g., the inference accuracy and latency. In this paper, a task- oriented over-the-air computation scheme is proposed for a multi- device artificial intelligence system. Particularly, a novel tractable inference accuracy metric is proposed for classification tasks, which is called minimum pair-wise discriminant gain. Unlike prior work measuring the average of all class pairs in feature space, it measures the minimum distance of all class pairs. By maximizing the minimum pair-wise discriminant gain instead of its average counterpart, any pair of classes can be better separated in the feature space, and thus leading to a balanced and improved inference accuracy for all classes. Besides, this paper jointly optimizes the minimum discriminant gain of all feature elements instead of separately maximizing that of each element in the existing designs. As a result, the transmit power can be adaptively allocated to the feature elements according to their different contributions to the inference accuracy, opening an extra degree of freedom to improve inference performance. Extensive experiments are conducted using a concrete use case of human motion recognition to verify the superiority of the proposed design over the benchmarking scheme.", "sections": [{"title": "I. INTRODUCTION", "content": "Edge artificial intelligence (AI) has been an emerging tech- nique to provide various kinds of intelligent services to support many applications like auto-driving and remote health [1]\u2013[4]. However, the realization of these intelligent services demands the deployment of well-trained AI models at the network edge and utilizes their inference capability for making human-like intelligent decisions. This gives rise to a new research topic, called edge inference [5]-[12]. There are now three types of edge inference: on-device inference, which uses the device for the entire inference task, takes up a lot of computational resources on the device (see, e.g., [5]). The second one is known as on-server inference which uses a server to perform the entire inference task completely (see, e.g., [6]). It causes huge communication overhead and privacy leakage. Among others, the technique of edge-device co-inference, called edge split inference, has been the most popular architecture [7]\u2013 [12]. It divides an AI model into two parts. One part with light size is deployed at the device for extracting a low- dimensional feature vector from the high-dimensional raw data. The remaining computation-intensive part is deployed at the server and utilizes the received feature vector from the device to complete the downstream inference task. By avoiding high-dimensional raw data transmission and offloading the in- tensive computation to the server, the edge-device co-inference can enjoy the advantages of enhanced communication and computation efficiency as well as preserving data privacy and thus is considered in the current work.\nAs stated in [11]-[14], the design of edge-device co- inference calls for task-oriented communication techniques, since traditional techniques designed for throughput maxi- mization or distortion minimization, fail to distinguish the data samples of different contributions on the inference task. To tackle this problem, a task-oriented scheme of integrated sens- ing, computation, and communication (ISCC) was proposed in [11] for multi-device AI inference, where each device senses a target area from disjoint narrow views. Furthermore, for the case where different devices observe the same wide view of a target area with each providing a noisy observation, a task- oriented over-the-air computation (AirComp) was proposed in [12] to efficiently aggregate the local features for suppress- ing the sensing and channel noise by directly adopting the inference accuracy as the design goal instead of using the traditional minimum mean square error (MMSE) (see, e.g., [15], [16]). This task-oriented AirComp design was extended to the Cloud-RAN architectures [13]. As the instantaneous inference accuracy is unknown at the design stage, when the input data of the AI model is not obtained, authors in [11]-[13] adopted an approximate but tractable metric as a surrogate for classification tasks, called discriminant gain and originally proposed in [10] based on the well-known Kullback- Leibler divergence [17]. Specifically, as shown in Fig. 1, the discriminant gain of an arbitrary class pair (e.g., $G_{1,2}$, $G_{1,3}$ or $G_{2,3}$) is defined as the distance between their centroids in the"}, {"title": "II. SYSTEM MODEL AND PROBLEM FORMULATION", "content": "A. System Model\n1) Network Model: Consider an edge-device co-inference system with $K$ single-antenna devices and one edge server equipped with a single-antenna access point, as shown in Fig. 2. The devices sense the same wide view of a target and obtain real-time noise-corrupted sensory data for the inference task. Then, a principal component analysis (PCA) based feature extractor is utilized to extract a local low-dimensional feature vector from the raw sensory data at each device. In practice, the number of extracted feature dimensions is task-dependent and can be pre-determined during the training stage. Next, for suppressing the sensing noise to achieve higher inference performance, all local feature vectors are aggregated at the server using AirComp to derive a denoised global one, which is further input into an AI sub-model for completing the downstream inference task [12]. The number of extracted feature elements is denoted as $M$. As a reasonable example,\n2) Feature Distribution: The extracted local feature vector is a noise-corrupted version of the ground-true one, as\n$x_k = x + d_k, 1 \\leq k \\leq K,$\nwhere $x = \\{x_1, ..., x_m, ..., x_M\\}$ is the ground-true feature vector, $d_k = \\{d_{k,1},..., d_{k,m},...d_{k,M}\\}$ is the difference be- tween the noise-corrupt feature and ground-true one which is assumed as the Gaussian feature sensing noise. As PCA is applied, different elements of $x$ and $d_k$ are independent. Consider a classification task with $L$ classes, the $m$-th element of $x$ has a distribution of\n$x_m \\sim \\frac{1}{L} \\sum_{l=1}^{L} \\mathcal{N} \\left(\\mu_{l,m}, \\sigma_{l,m}^{2}\\right),$\nwhere $\\mathcal{N} \\left(\\mu_{l,m}, \\sigma_{l,m}^{2}\\right)$ represents the Gaussian distribution cor- responding to the $l$-th class, $\\mu_{l,m}$ is the mean of $m$-th dimen- sion in the $l$-th class, $\\sigma_{l,m}^{2}$ is the variance of $m$-th dimension. In practice, the values of these parameters are estimated at the server in the training stage using the training data samples. The $m$-th element of $d_k$ has a distribution of\n$d_{k,m} \\sim \\mathcal{N} \\left(0, \\delta_{k,m}^{2}\\right),$\nwhere $\\delta_{k,m}^{2}$ is the feature noise variance. It can be pre- estimated by first estimating the ground-true feature vector by averaging massive noisy ones and then analyzing the variance between each local feature vector and the estimated ground- true one. Thereby, the $m$-th element of $x_k$, given by\n$x_{k,m} = x_m + d_{k,m},$\nwhich has a distribution of\n$x_{k,m} \\sim \\frac{1}{L} \\sum_{l=1}^{L} \\mathcal{N} \\left(\\mu_{l,m}, \\sigma_{l,m}^{2}+\\delta_{k, m}^{2}\\right).$\n3) AirComp: To simultaneously access multiple devices to aggregate the local feature vectors for sensing and channel noise suppression, the technique of AirComp is adopted to reduce the communication overhead. For an arbitrary time slot $m$, all devices pre-code and transmit the $m$-th element of their local feature vectors, i.e., $x_{km}$ over the same resource block. Thereby, the received element at the server is given by\n$\\hat{x}_{m}=\\sum_{k=1}^{K} h_{k} b_{k, m} x_{k, m}+n, 1 \\leq m \\leq M,$ \nwhere $b_{k,m}$ is the precoding scalar at device $k$, $n$ is the channel noise with a distribution of $n \\sim \\mathcal{N} \\left(0, \\delta_{n}^{2}\\right)$, and $\\delta_{n}^{2}$ is the channel noise power.\nThen, by substituting the distribution of $x_{k,m}$ in (5) into $\\hat{x}_{m}$, its distribution can be derived as\n$\\hat{x}_{m} \\sim \\sum_{l=1}^{L} \\mathcal{N} \\left(\\hat{\\mu}_{l, m}, \\hat{\\sigma}_{m}^{2}\\right),$\nwhere $\\hat{\\sigma}_{m}^{2}=\\sum_{k=1}^{K}\\left(h_{k} b_{k, m}\\right)^{2}\\left(\\sigma_{l, m}^{2}+\\delta_{k, m}^{2}\\right)+\\delta_{n}^{2}$ and $\\hat{\\mu}_{l, m}=\\sum_{k=1}^{K} h_{k} b_{k, m} \\mu_{l, m}$."}, {"title": "4) Inference Accuracy Measured by Discriminant Gain:", "content": "In this work, inference accuracy is adopted as the design criterion instead of the traditional MSE in existing designs. Since the latter targets minimizing the distortion between the received feature vector and the ground-true one, it ignores that the same distortion level on different feature elements has different impacts on the inference accuracy [10]\u2013[12]. However, the instantaneous inference accuracy is unknown in the design stage before the feature vector is inputted into the AI model. To address this issue, discriminant gain is adopted as the surrogate. Specifically, consider a classification task with $L$ classes, whose feature distribution is in (7). For an arbitrary class pair, say classes $l$ and $l'$, the discriminant gain between the two classes is\n$G_{l l^{\\prime}}(\\mathbf{x})=\\sum_{m=1}^{M}\\left(\\hat{\\mu}_{l m}-\\hat{\\mu}_{l^{\\prime} m}\\right)^{2} / \\hat{\\sigma}_{m}^{2}.$ \nThe pair-wise discriminant gain in (8) represents the discerni- bility of the two classes in the feature space, as shown in Fig. 1. With larger discriminant gain, the two classes are better separated, leading to higher achievable inference accuracy.\nB. Problem Formulation\nExisting work adopts the metric of maximizing average pair-wise discriminant gain of all class pairs for enhancing the inference accuracy [10]\u2013[12]. This, however, causes an unbalanced and low accuracy as mentioned before and as shown in the left side of Fig. 1. To address this issue, we propose a novel design criterion, which maximizes the minimum pair-wise discriminant gain over all class pairs. As a result, the class pair with the smallest distance can be well separated, resulting in enhanced achievable inference accuracy for all classes, as shown in the right side of Fig. 1. Besides, there are two kinds of constraints. One is the transmit power constraint for each device in each time slot. The other is the overall energy constraint of each device to transmit the whole feature vector. In summary, the problem is given by\n$\\begin{aligned}&\\max _{\\left\\{b_{k}\\right\\}} \\min _{\\left(l, l^{\\prime}>l\\right)} G_{l l^{\\prime}}(\\mathbf{x})=\\sum_{m=1}^{M}\\left(\\hat{\\mu}_{l m}-\\hat{\\mu}_{l^{\\prime} m}\\right)^{2} / \\hat{\\sigma}_{m}^{2}, \\\\&\\text { s.t. } b_{k, m}^{2} \\leq P_{k}, \\forall(k, m), \\\\&\\sum_{m=1}^{M} b_{k, m}^{2} \\leq P_{k}, \\forall k,\\end{aligned}$\nwhere $P_{k}$ is transmit power threshold in each time slot and $P_{k}$ is total power constraint of all time slots.\nIII. MINIMUM DISCRIMINANT GAIN MAXIMIZATION\n(P1) is non-convex due to the complicated form of the objective function. To this end, the following variable trans- formation is applied to simplify (P1). For all class pairs $\\left(l, l^{\\prime}>l\\right)$, we introduce\n$\\Gamma_{m, l l^{\\prime}} \\leq \\frac{\\left(\\hat{\\mu}_{l m}-\\hat{\\mu}_{l^{\\prime} m}\\right)^{2}}{\\hat{\\sigma}_{m}^{2}}, \\forall m.$\nThen, denote $\\Gamma=\\sum_{m=1}^{M} \\Gamma_{m, l l^{\\prime}}, \\forall \\left(l, l^{\\prime}>l\\right)$. Accordingly, it is easy to derive that\n$\\Gamma \\leq \\sum_{m=1}^{M} \\frac{\\left(\\hat{\\mu}_{l m}-\\hat{\\mu}_{l^{\\prime} m}\\right)^{2}}{\\hat{\\sigma}_{m}^{2}} \\triangleq V_{\\left(l, l^{\\prime}>l\\right)}.$\nNext, by substituting $\\Gamma$ above into (P1), it can be equivalently derived as\n$\\begin{aligned}&\\max _{\\Gamma,\\left\\{b_{k}\\right\\}} \\Gamma \\\\&\\text { s.t. }(C 1) b_{k, m}^{2} \\leq P_{k}, \\forall(k, m), \\\\&(C 2) \\sum_{m=1}^{M} b_{k, m}^{2} \\leq P_{k}, \\forall k,\\\\&(C 3) \\Gamma \\leq \\sum_{m=1}^{M} \\Gamma_{m, l, l^{\\prime}}, \\forall\\left(l, l^{\\prime}>l\\right), \\\\&(C 4) \\Gamma_{m, l, l^{\\prime}} \\leq \\frac{\\left(\\hat{\\mu}_{l m}-\\hat{\\mu}_{l^{\\prime} m}\\right)^{2}}{\\hat{\\sigma}_{m}^{2}} , \\forall\\left(l, l^{\\prime}>l, m\\right).\\end{aligned}$\n(P2) is still a non-convex problem. In the sequel, an equivalent d.c. form is first derived and the problem is addressed by the typical method of SCA [19]. To begin with, by substituting $\\hat{\\mu}_{l,m}, \\hat{\\mu}_{l^{\\prime},m}$ and $\\hat{\\sigma}_{m}^{2}$ in (7) into the fourth constrain of (P2) and with some simple derivations, we can get\n$\\sum_{k=1}^{K}\\left(h_{k} b_{k, m}\\right)^{2} \\geq \\frac{\\left(\\mu_{l, m}-\\mu_{l^{\\prime}, m}\\right)^{2}}{\\Gamma_{m, l, l^{\\prime}}}-\\sum_{k=1}^{K} \\delta_{k, m}^{2} \\left|h_{k}\\right|^{2}-\\delta_{n}^{2}.$\nfor all (l, l' > l, m). In (11), it is observed that both sides of the inequality are convex. Define the left-side term as\n$Q_{m, l, l^{\\prime}}\\left(\\left\\{b_{k, m}\\right\\}, \\Gamma_{m, l, l^{\\prime}}\\right)=\\sum_{k=1}^{K}\\left(h_{k} b_{k, m}\\right)^{2}-\\frac{\\left(\\mu_{l, m}-\\mu_{l^{\\prime}, m}\\right)^{2}}{\\Gamma_{m, l, l^{\\prime}}}+\\sum_{k=1}^{K} \\delta_{k, m}^{2} \\left|h_{k}\\right|^{2}+\\delta_{n}^{2},$\nfor all $\\left(l, l^{\\prime}>l, m\\right)$. It is no less than its first- order Taylor expansion at $\\left(\\left\\{b_{k, m}^{[t]}\\right\\}, \\Gamma_{m, l, l^{\\prime}}^{[t]}\\right)$, which is denoted as $\\Theta_{m, l, l^{\\prime}}\\left(\\left\\{b_{k, m}\\right\\}, \\Gamma_{m, l, l^{\\prime}}\\right)$. That says, $Q_{m, l, l^{\\prime}}\\left(\\left\\{b_{k, m}\\right\\}, \\Gamma_{m, l, l^{\\prime}}\\right) \\geq \\Theta_{m, l, l^{\\prime}}\\left(\\left\\{b_{k, m}\\right\\}, \\Gamma_{m, l, l^{\\prime}}\\right)$.\nThereby, the method of SCA can be utilized to solve (P2) via iteratively solving the following approximated convex problem, say (P3), by using solution in the last iteration as the reference point of the first-order Taylor expansion.\n$\\begin{aligned}&\\max _{\\Gamma} \\Gamma \\\\&\\text { s.t. }(C 1),(C 2),(C 3), \\\\&(P 3) \\sum_{k=1}^{K}\\left(h_{k} b_{k, m}\\right)^{2}+\\sum_{k=1}^{K} \\delta_{k, m}^{2} \\left|h_{k}\\right|^{2}+\\delta_{n}^{2} \\\\&\\leq \\Theta_{m, l, l^{\\prime}}\\left(\\left\\{b_{k, m}^{[t]}\\right\\}, \\Gamma_{m, l, l^{\\prime}}^{[t]}\\right), \\forall\\left(l, l^{\\prime}>l, m\\right).\\end{aligned}$\nBesides, (P3) can be solved by the common convex algorithms using e.g., the cvx toolbox."}, {"title": "IV. PERFORMANCE EVALUATION", "content": "A. Experiment Setup\nConsider a single-cell network with a radius of 500 meters, where the server equipped with a single-antenna base station is located at the center and $K$ single-antenna devices are randomly distributed. The channel gain model for each device $k$ is $H_{k} = |\\phi_{k}h_{k}|^{2}$, where $\\phi_{k}$ and $h_{k}$ denote the large-scale and small-scale fading propagation coefficients, respectively. A human motion recognition inference task is considered, where there are four classes including child walking, child pac- ing, adult walking, and adult pacing. A high-fidelity wireless sensing simulator proposed in [20] is applied for generating sensory data samples. In the training phase, the model is trained using 6400 data samples to get well-trained AI models at the server. In the inference phase, a noise-corrupted raw sensory data sample is generated for each device, from which local feature vectors are extracted. At the server, all local feature vectors are aggregated for completing the downstream inference task. For testing the inference accuracy, 1600 inde- pendent inference experiments are conducted. Two AI models are adopted for the task. One is a support vector machine (SVM). The other is a multi-layer perceptron (MLP) neural network with two hidden layers, each with 80 and 40 neurons respectively. By default, the number of devices, the feature noise variance, the number of extracted feature dimensions, the transmit power, and the stepsize of SCA are set as $K = 3$, $\\delta_{k,m}^{2}= 0.4$, $M = 12$, $P_{k} = 12$dBm, and $\\alpha = 0.7$ respectively, unless specified otherwise.\nFor comparison, we consider three schemes as follows.\nBaseline: All the parameters are allocated following the task-oriented AirComp scheme in [12], which aims at maximizing average pair-wise discriminant gain.\nWeighted subspace centroid: All the parameters are allo- cated following the traditional AirComp scheme in [21], whose design criterion is MMSE.\nJoint optimization of all feature elements (our proposal): All parameters are set follow the proposed scheme.\nB. Experimental Results\nIn this part, the experimental results are shown. The relation between inference accuracy and the minimum discriminant gain is first presented, followed by the comparison among the three schemes."}, {"title": "V. CONCLUSION", "content": "This work proposed a task-oriented AirComp scheme for edge-device co-inference, which addresses the issue of un- balanced classification accuracy by maximizing the minimum pair-wise discriminant gain and jointly optimizing the long- term transmission over multiple time slots. It opens several directions like extending this scheme to the multiple-input multiple-output systems, using unlicensed spectrum [22], the scenario of the large language model, etc. Particularly, since AirComp has been shown a promising technique for imple- menting federated learning (see e.g., [23]), task-oriented Air- Comp for federated learning is an attractive research direction."}]}