{"title": "THE UNIQUENESS OF LLAMA3-70B WITH PER-CHANNEL QUANTIZATION: AN EMPIRICAL STUDY", "authors": ["Minghai Qin"], "abstract": "We have observed a distinctive quantization-related behavior in the LLaMA3/3.1-70B models that is absent in both the LLaMA2-70B and LLaMA3/3.1-8B/405B models. Quantization is a crucial technique for deploying large language models (LLMs) efficiently, as it reduces memory consumption, decreases memory transactions, and potentially accelerates inference using lower-precision compute cores. Among various bit widths and representations for weights and activations, the 8-bit integer weight and 8-bit integer activation (W8A8) configuration is particularly popular due to its widespread hardware support. However, the impact of W8A8 post-training quantization on model accuracy remains contentious. While several studies have suggested calibrating either weights or activations to mitigate accuracy degradation, a comprehensive solution has yet to be identified. In this paper, we empirically investigate multiple LLMs featured on an open LLM leaderboard, discovering that the LLaMA3-70B model series have a unique accuracy degradation behavior with W8A8 per-channel post-training quantization. In contrast, other model series such as LLaMA2, LLaMA3-8B, Qwen, Mixtral, Mistral, Phi-3, and Falcon demonstrate robust performance with W8A8, sometimes surpassing their FP16 counterparts. Contrary to previous assertions attributing degradation to the large dynamic range of activations, our findings indicate that the weight distribution of the LLaMA3-70B is the primary factor behind the vulnerability. By meticulously analyzing the distinct characteristics of weight distributions across Transformer blocks, we propose a mixed strategy with less than 3% of the layers enabling finer W8A8 quantization granularity, while the remaining 97% of layers retain the per-channel configuration. As a result, the average accuracy of LLaMA3-70B-W8A8 is increased from 45.5% to 73.4% (just 0.7% shy of LLaMA3-70B-FP16) across eight reasoning tasks. Notably, our method requires neither calibration nor fine-tuning.", "sections": [{"title": "1 Introduction", "content": "The rising popularity of large language models (LLMs) Vaswani et al. [2023], Brown et al. [2020], Devlin et al. [2019], Liu et al. [2019], Touvron et al. [2023] in diverse practical applications underscores their revolutionary impact on natural language processing, data analysis, and artificial intelligence. However, their immense size imposes a substantial burden on GPU memory Kaplan et al. [2020], complicating deployment in resource-constrained environments. Quantization is a pivotal technique to mitigate this issue, reducing the memory footprint of LLMs while maintaining accuracy Gholami et al. [2021], Zafrir et al. [2019], Wang et al. [2023]. By converting high-precision weights and activations to lower-precision formats, quantization facilitates more efficient model storage and accelerates inference. As the demand for LLMs grows, the advancement of quantization methods will be crucial for their scalable and cost-effective deployment in real-world applications.\nQuantization uses lower precision to represent weights and activations. In this study, we focus on post-training integer-bit quantization (PTQ). The 8-bit weights and 8-bit activations (W8A8) configuration is often regarded as a"}, {"title": "2 Quantization Basics", "content": "In this paper, we use symmetric integer-bit quantization due to its high efficiency and simpler hardware support. In symmetric n-bit integer quantization, we map floating-point (FP) values to integers in $[-2^{n-1}, 2^{n-1} \u2013 1]$. This process involves scaling the FP values based on a scale-factor and an offset. This offset is sometimes called \u201czero-point\" and is set to 0 for symmetric quantization.\nLet x be a FP value, and $x_q$ its quantized integer representation. The scale-factor s is determined by the range of the FP values and the desired range of the integers. The symmetric quantization formula is given by:\n$x_q = round(\\frac{x}{s})$\nwhere round() rounds the result to the nearest integer.\nThe scale-factor s is typically calculated based on the maximum absolute value of the FP values, max_abs, and the maximum representable integer value, $2^{n-1} \u2013 1$. For symmetric quantization, the scale-factor is defined as:\n$s = \\frac{max\\_abs}{2^{n-1}-1}$\nThus, the quantization and dequantization processes can be expressed as:\n$x_q = round(\\frac{x}{s})$\nand $x \u2248 x_q . s$\nThe scale-factor s plays a crucial role in the quantization process. The number of floating-point values used to determine max_abs is typically referred to as the group size. In LLMs, consider a weight matrix W of dimension N \u00d7 M and an input activation matrix A of dimension M \u00d7 P. The resultant output of W. A has dimension N \u00d7 P. For per-channel quantization of W, the group size is M, implying that each row of M values in W shares a common scale-factor. Consequently, all scale-factors form a vector of length N. Similarly, per-channel quantization of A also has a group size of M, with each column of M values in A sharing a scale-factor, resulting in a scale-factor vector of length P. This configuration enables the computation of floating-point matrix multiplication using integer-bit compute cores (e.g., INT8 tensor cores), followed by dequantization. The dequantization process involves an element-wise multiplication with the scale-factor matrix of dimension N \u00d7 P, derived as the outer product of the scale-factor vectors of W and A. Notably, this dequantization can be efficiently fused into the subsequent operation after W. A in LLMs, which is typically an element-wise operation such as rotary embedding, SiLU activation application, or normalization. In contrast, per-group quantization of W and A employs a finer granularity, where each row of W (and column of A) is partitioned into groups, each with its own scale-factor. Consequently, the output W. A cannot be computed in a single cycle within the INT tensor core, as the scale-factors differ during accumulation in the multiply-accumulation (MAC) unit. This leads to a degradation in the compute efficiency of the quantized model. Given these considerations, per-channel quantization is generally preferred unless the quantization-induced error degrades the model's accuracy to an unacceptable level."}, {"title": "3 The Uniqueness of LLaMA3-70B with per-channel W8A8", "content": "We evaluated LLMs across eight reasoning tasks: Hellaswag (HS), PIQA (PQ), OpenbookQA (OQ), ARC-Easy (AE), Winogrande (WG), ARC-Challenge (AC), BoolQ (BQ), and MMLUZellers et al. [2019], Bisk et al. [2019], Mihaylov et al. [2018], Clark et al. [2018], Sakaguchi et al. [2019], Clark et al. [2019], Hendrycks et al. [2021a,b]. We realize that many studies report the average accuracy over these reasoning tasks by simply computing the arithmetic mean of individual task accuracies (denoted by \u201cAVG\u201d in our figures). However, this approach may lack rigor due to the significant variance in the number of questions across tasks. For instance, HS and MMLU each contain over 10,000 questions, while OQ comprises only 500.\nTo address this disparity, we use an additional metric: the \u201cweighted average accuracy.\" This metric is calculated by dividing the total number of correct answers across all tasks by the total number of questions. Consequently, this method assigns greater weight to datasets with a larger number of questions, providing a more balanced representation of model performance across tasks of varying sizes. It is denoted by \u201cWT-AVG\" in our figures."}, {"title": "3.2 Tested Models", "content": "We test the accuracy of the following models: LLaMA3-70B, LLaMA3-70B-Instruct, LLaMA3.1-70B, Llama3-70B-Synthia, calme-2.2-llama3-70b, LLaMA3-8B, LLaMA2-70B, LLaMA2-70B-chat, Qwen2-72B, Mixtral-8x7B, Phi3-14B-Instruct, Mistral-L-Instruct-123B, Falcon-40B. HuggingFace links to them will be provided in the appendix. We also analyze the weight distribution of LLaMA3.1-405B without evaluating its accuracy with FP16 due to GPU memory limitations."}, {"title": "3.3 Test Environments", "content": "All tests are done within 8xA100 (80GB) GPUs with PyTorch framework using lm-evaluation-harness Gao et al. [2023]."}, {"title": "3.4 LLaMA3-70B's sensitivity to W8A8", "content": "Figure 1 and Figure 3 present a comparative analysis of accuracy for several models in the LLaMA3-70B series subjected to 8-bit per-channel post-training quantization. This analysis encompasses the recently released LLaMA3.1-70B and two additional models fine-tuned from LLaMA3-70B. These figures yield several notable observations:\n\u2022\tThe accuracy of models undergoes significant degradation with W8 quantization, even when activations are maintained at FP16 precision. This indicates that the observed accuracy deterioration is not attributable to quantization errors in activations, but rather originates from the 8-bit weight quantization process.\n\u2022\tModels fine-tuned from LLaMA3-70B exhibit the same vulnerability to W8 quantization as their base model. This persistence of vulnerability can be attributed to the fact that fine-tuning typically induces only minor alterations to weight values. In the subsequent section, we will demonstrate that this vulnerability is intrinsically linked to the underlying weight distributions of the model."}, {"title": "3.5 LLAMA3-70B is the ONLY ONE sensitive to W8A8", "content": "Figure 4 and Figure 5 compare the accuracy of other models with different architectures with W8A8 quantization. The analysis spans the LLaMA family, including LLaMA3-8B, LLaMA2-70B, and LLaMA2-70B-chat, as well as other prominent Transformer-based designs such as Qwen2-72B, Mixtral-8x7B, Mistral-123B, Phi3-13B, and Falcon-40B. These architectures represent the top-performing architectures in the LLM Open Leaderboard.\nThe data presented in these figures illustrates a contrast to the behavior observed in the LLaMA3-70B series. Specifically, the models examined here exhibit remarkable resilience to W8A8 per-channel quantization. In the majority of instances, the performance of W8A8 quantized versions closely approximates that of their FP16 counterparts. While minor variations exist, the performance decline rarely exceeds 1.5% (Mistral-123B)."}, {"title": "3.6 Why LLaMA3-70B is the only one?", "content": "In order to understand the uniqueness of LLaMA3-70B models, we explore the weight distributions."}, {"title": "3.6.1 A qualitative exploration", "content": "Figures 2, 6, 7, and 8, illustrate the absolute values of the V matrix weights in the first Transformer block for LLaMA3-70B, LLaMA3.1-70B, LLaMA3-8B, and LLaMA2-70B models, respectively. Other weight matrices for other models are shown in the Appendix. It is important to note that in per-channel quantization, each group comprises a length-M vector along the input dimension. Our analysis reveals several key observations:\n\u2022\tThe maximum absolute weight values (max_abs) in LLaMA3-70B (93) and LLaMA3.1-70B (92.5) surpass those in LLaMA3-8B (0.05) and LLaMA2-70B (0.07) by approximately three orders of magnitude. Furthermore, LLaMA3/3.1-70B exhibits a non-trivial number of weights with large magnitudes along the output dimension, which we term \u201cweight outliers\u201d. This extensive range of weight values results in large quantization"}, {"title": "3.6.2 A quantitative exploration", "content": "To quantitatively assess the robustness of weights under W8 quantization, we employ two key metrics. The first is the maximum absolute value (max_abs) of weights for each layer in the model, which determines the scale factor and, consequently, the quantization interval. The second metric is the quantization error, measured by the root-mean-square"}, {"title": "4 A Mixed Grouping Strategy", "content": "Based on our analysis of Figure 9, we have empirically identified that in the LLaMA3-70B model, the Q, K, V, Up, and Gate matrices of Block 0, 1, and 3 exhibit exceptionally high quantization errors and maximum absolute weight values. To mitigate these quantization errors, we propose implementing a finer grouping granularity specifically for these 15 layers.\nIt is noteworthy that the layers requiring per-group quantization, as opposed to per-channel quantization, constitute only 2.68% (15/560) of the total layers in the model. While many quantization studies employ group sizes of 128 or smaller"}, {"title": "5 Ablation on the group size", "content": "We examine the influence of varying group sizes on the accuracy of W8A8 quantization for the LLaMA3-70B model. Figure 15 illustrates this comparative analysis. Our results reveal that certain datasets, notably AE, AC, and OQ, exhibit particular sensitivity to increased group size applied exclusively to a total of 15 layers within Blocks 0, 1, and 3. From a holistic perspective, per-group quantization employing sizes of 512 or 1024 emerges as a good compromise between model accuracy and hardware efficiency."}, {"title": "6 Discussions", "content": "According to Meta's technical report Dubey et al. [2024], LLaMA2-70B and LLaMA3-70B share similar model architectures, while LLaMA3-8B/70B and LLaMA3.1-8B/70B/405B are reported to have similar training data and strategies. However, our analysis reveals significantly different weight distribution characteristics in LLaMA3/3.1-70B compared to other LLaMA models. This unexpected divergence in weight patterns, particularly the presence of numerous large outliers exclusively in LLaMA3/3.1-70B models, presents an intriguing peculiarity.\nThe origins of this phenomenon remain unclear, given that models sharing either architectural design or training methodologies do not exhibit similar characteristics. Unraveling the underlying mechanisms responsible for this unique weight distribution could potentially offer valuable insights into the learning processes of Large Language Models (LLMs), especially in the context of reasoning tasks."}, {"title": "7 Related Works", "content": "Quantization is useful in reducing model size and potentially increases model inference speed Han et al. [2016], Jacob et al. [2017], Nagel et al. [2019], Bondarenko et al. [2021], Wang et al. [2019], Bengio et al. [2013]. There two main categories of quantization, post-training quantization (PTQ) and quantization-aware training (QAT)."}, {"title": "7.1 Post-training quantization", "content": "PTQ does not use any back-propagation to finetune weights. For weight-only quantization, LLM.int8() Dettmers et al. [2022a] uses a mixed precision with 16-bit MAC for outliers. GPTQ Frantar et al. [2023] and AWQ Lin et al. [2024a] quantize the per-group weight-only tensor using as small as 3-4 bits. SqueezeLLM Kim et al. [2024] uses non-uniform quantization and store outliers efficiently. For weight-activation quantization that enables faster inference Dettmers et al. [2022b], Wei et al. [2023] SmoothQuant Xiao et al. [2024] balances the outliers from activation to weights to enable per-tensor W8A8. QServe Lin et al. [2024b] proposes W4A8K4 to serve LLM models. Some other studies enable even smaller number of bits Shao et al. [2024], Zhao et al. [2024], Ashkboos et al. [2024]."}, {"title": "7.2 Quantization aware training", "content": "Quantization-aware training (QAT) requires both calibration data and back-propagation to finetune the quantize weights Bengio et al. [2013], Gholami et al. [2021]. While full finetuning Taori et al. [2023] can recover the accuracy to the maximum extent, Q-LoRA Dettmers et al. [2023] reduces the memory requirement significantly."}, {"title": "8 Conclusions", "content": "Our investigation reveals a distinctive vulnerability in the LLaMA3-70B model series, including LLaMA3.1-70B, to per-channel quantization. We attribute this susceptibility to significant weight outliers in the initial layers. The contrast in behavior between the LLaMA3-70B and other LLaMA models (LLaMA2, LLaMA3/3.1-8B, LLaMA3.1-405B), as well as other tested LLMs with different architectures, warrants further exploration of the underlying training strategies. To address this issue, we propose a mixed-grouping method that substantially enhances the accuracy of LLaMA3-70B from 45.4% to 73.4%, effectively matching the performance of its FP16 counterpart."}]}