{"title": "THE UNIQUENESS OF LLAMA3-70B WITH PER-CHANNEL QUANTIZATION: AN EMPIRICAL STUDY", "authors": ["Minghai Qin"], "abstract": "We have observed a distinctive quantization-related behavior in the LLaMA3/3.1-70B models that is absent in both the LLaMA2-70B and LLaMA3/3.1-8B/405B models. Quantization is a crucial technique for deploying large language models (LLMs) efficiently, as it reduces memory consumption, decreases memory transactions, and potentially accelerates inference using lower-precision compute cores. Among various bit widths and representations for weights and activations, the 8-bit integer weight and 8-bit integer activation (W8A8) configuration is particularly popular due to its widespread hardware support. However, the impact of W8A8 post-training quantization on model accuracy remains contentious. While several studies have suggested calibrating either weights or activations to mitigate accuracy degradation, a comprehensive solution has yet to be identified. In this paper, we empirically investigate multiple LLMs featured on an open LLM leaderboard, discovering that the LLaMA3-70B model series have a unique accuracy degradation behavior with W8A8 per-channel post-training quantization. In contrast, other model series such as LLaMA2, LLaMA3-8B, Qwen, Mixtral, Mistral, Phi-3, and Falcon demonstrate robust performance with W8A8, sometimes surpassing their FP16 counterparts. Contrary to previous assertions attributing degradation to the large dynamic range of activations, our findings indicate that the weight distribution of the LLaMA3-70B is the primary factor behind the vulnerability. By meticulously analyzing the distinct characteristics of weight distributions across Transformer blocks, we propose a mixed strategy with less than 3% of the layers enabling finer W8A8 quantization granularity, while the remaining 97% of layers retain the per-channel configuration. As a result, the average accuracy of LLaMA3-70B-W8A8 is increased from 45.5% to 73.4% (just 0.7% shy of LLaMA3-70B-FP16) across eight reasoning tasks. Notably, our method requires neither calibration nor fine-tuning.", "sections": [{"title": "1 Introduction", "content": "The rising popularity of large language models (LLMs) in diverse practical applications underscores their revolutionary impact on natural language processing, data analysis, and artificial intelligence. However, their immense size imposes a substantial burden on GPU memory complicating deployment in resource-constrained environments. Quantization is a pivotal technique to mitigate this issue, reducing the memory footprint of LLMs while maintaining accuracy. By converting high-precision weights and activations to lower- precision formats, quantization facilitates more efficient model storage and accelerates inference. As the demand for LLMs grows, the advancement of quantization methods will be crucial for their scalable and cost-effective deployment in real-world applications.\nQuantization uses lower precision to represent weights and activations. In this study, we focus on post-training integer-bit quantization (PTQ). The 8-bit weights and 8-bit activations (W8A8) configuration is often regarded as a"}, {"title": "", "content": "good balance between efficiency and accuracy. Compared to the FP16 counterpart, W8 can save reduce the memory consumption and transaction by 50%. Although 4-bit weights and 16-bit activations (W4A16) require even less memory, they often lack broad hardware support for the INT4-FP16 matrix-multiplication compute cores. In contrast, existing INT8-INT8 compute cores offer superior acceleration for the inference of W8A8 models. On the other hand, while the W4A4 configuration may reduce memory usage and latency through INT4-INT4 compute cores, it typically suffers from significant accuracy degradation due to the aggressive quantization.\nThe granularity of a quantization group in large language models (LLMs) is another critical factor influencing the accuracy-acceleration trade-off. Within a quantization group, all values share the same scaling factor and zero point. When the group size matches the input dimension of a layer, this is termed \"per-channel\" quantization, which maximizes potential speedup. Conversely, \"per-group\u201d quantization occurs when an input channel is divided into multiple groups. This requires the intermediate partial sums of matrix-matrix multiplication to be transferred out of the compute cores, significantly reducing system throughput. Therefore, maintaining accuracy with per-channel quantization is essential for practical applications.\nIn this paper, we investigate the W8A8 quantization for the most popular architectures and models featured on the LLM open leaderboard. Our study reveals a noteworthy phenomenon: the LLaMA3-70B model series exhibits a pronounced vulnerability to W8A8 per-channel quantization, in stark contrast to other models, which demonstrate significantly greater robustness, typically experiencing less than 1% accuracy degradation compared to their FP16 counterparts under the same quantization scheme. Our observation on the \"LLaMA3-70B series\" also include the LLaMA3.1-70B and various fine-tuned versions, and other robust models include LLaMA2 series, LLaMA3-8B, Mistral-123B, Mixtral-8x7B, Qwen2-72B, Phi3-13B, and Falcon-40B . Figure 1 provides a comparative analysis of FP16 (labeled as W16A16) and W8A16 quantization for the officially released LLaMA3/3.1-70B and LLaMA3/3.1-70B-Instruct by Meta. This comparison reveals that significant accuracy degradation already occurs with W8A16, indicating that the issue primarily stems from weight quantization rather than activation quantization. Details will be presented in the experiment section. Released in April 2024, the LLaMA3-70B has not been extensively examined in recent research on W8A8 per-channel quantization. Given that this series is currently the most widely used base model on the open leaderboard, further exploration and understanding of its unique characteristics are imperative."}, {"title": "", "content": "To elucidate the unique vulnerability of the LLaMA3-70B series to W8A8 per-channel quantization, we conducted a comparative analysis of its weight distributions against those of other robust models. Our investigation revealed a significant discrepancy, particularly pronounced in the initial layers. These layers exhibit a distinctive characteristic wherein magnitudes of certain weights exceed others by several orders of magnitude. Figure 2 shows an example of the \"V\" matrix in the first Transformer block of the LLaMA3-70B model. Weights with large magnitudes are concentrated on specific input dimensions, with the largest values exceeding 90. These outlier weights substantially expand the quantization range, resulting in larger quantization intervals and consequently diminished precision for smaller weight values. Notably, these weight outliers are only prominent in the \u201cQ\u201d, \u201cK\u201d, \u201cV\u201d, \u201cUp\u201d, and \u201cGate\u201d weight matrices of the initial transformer blocks. To mitigate these quantization errors, we propose implementing per-group quantization specifically for these affected layers. Although these layers constitute merely 2-3% of the total layers in LLaMA3-70B, this targeted approach yields a dramatic enhancement in accuracy, elevating model performance to a level approaching that of FP16 models.\nThe contribution of this paper is as follows.\n\u2022 We show that the LLaMA3-70B model series has a unique characteristic that makes it the only model series on the open LLM leaderboard vulnerable to W8A8 per-channel post-training quantization (PTQ). In contrast, all other models can be safely quantized to per-channel-W8A8 with negligible accuracy loss. We believe this observation holds significant implications since LLaMA3-70B serves as one of the most popular base models for numerous models fine-tuned for domain-specific tasks.\n\u2022 We identify that LLaMA3-70B's sensitivity to per-channel PTQ stems from its significantly different weight distribution, particularly the presence of substantial weight outliers.\n\u2022 To address this, we propose a mixed per-channel/per-group quantization strategy. This approach applies per-group quantization to less than 3% of the layers, specifically those with significant weight outliers, while maintaining per-channel quantization for the remaining 97% layers. This mixed strategy effectively restores the accuracy of the LLaMA3-70B model series to levels comparable to its FP16 counterparts."}, {"title": "2 Quantization Basics", "content": "In this paper, we use symmetric integer-bit quantization due to its high efficiency and simpler hardware support. In symmetric n-bit integer quantization, we map floating-point (FP) values to integers in $[-2^{n-1}, 2^{n-1} \u2013 1]$. This process involves scaling the FP values based on a scale-factor and an offset. This offset is sometimes called \u201czero-point\" and is set to 0 for symmetric quantization.\nLet x be a FP value, and $x_q$ its quantized integer representation. The scale-factor s is determined by the range of the FP values and the desired range of the integers. The symmetric quantization formula is given by:\n$X_q = round(\\frac{X}{S})$\nwhere round() rounds the result to the nearest integer.\nThe scale-factor s is typically calculated based on the maximum absolute value of the FP values, max_abs, and the maximum representable integer value, $2^{n-1} - 1$. For symmetric quantization, the scale-factor is defined as:\n$S = \\frac{max\\_abs}{2^{n-1}-1}$\nThus, the quantization and dequantization processes can be expressed as:\n$X_q = round(\\frac{X}{S})$ and $X \\approx X_q. S$\nThe scale-factor s plays a crucial role in the quantization process. The number of floating-point values used to determine max_abs is typically referred to as the group size. In LLMs, consider a weight matrix W of dimension N \u00d7 M and an input activation matrix A of dimension M \u00d7 P. The resultant output of W. A has dimension N \u00d7 P. For per-channel quantization of W, the group size is M, implying that each row of M values in W shares a common scale-factor. Consequently, all scale-factors form a vector of length N. Similarly, per-channel quantization of A also has a group size of M, with each column of M values in A sharing a scale-factor, resulting in a scale-factor vector of length P. This configuration enables the computation of floating-point matrix multiplication using integer-bit compute cores (e.g., INT8 tensor cores), followed by dequantization. The dequantization process involves an element-wise multiplication with the scale-factor matrix of dimension N \u00d7 P, derived as the outer product of the scale-factor vectors of W and A. Notably, this dequantization can be efficiently fused into the subsequent operation after W. A in LLMs, which is typically an element-wise operation such as rotary embedding, SiLU activation application, or normalization. In contrast, per-group quantization of W and A employs a finer granularity, where each row of W (and column of A) is partitioned into groups, each with its own scale-factor. Consequently, the output W. A cannot be computed in a single cycle within the INT tensor core, as the scale-factors differ during accumulation in the multiply-accumulation (MAC) unit. This leads to a degradation in the compute efficiency of the quantized model. Given these considerations, per-channel quantization is generally preferred unless the quantization-induced error degrades the model's accuracy to an unacceptable level."}, {"title": "3 The Uniqueness of LLaMA3-70B with per-channel W8A8", "content": "We evaluated LLMs across eight reasoning tasks: Hellaswag (HS), PIQA (PQ), OpenbookQA (OQ), ARC-Easy (AE), Winogrande (WG), ARC-Challenge (AC), BoolQ (BQ), and MMLU. We realize that many studies report the average accuracy over these reasoning tasks by simply computing the arithmetic mean of individual task accuracies (denoted by \u201cAVG\u201d in our figures). However, this approach may lack rigor due to the significant variance in the number of questions across tasks. For instance, HS and MMLU each contain over 10,000 questions, while OQ comprises only 500.\nTo address this disparity, we use an additional metric: the \u201cweighted average accuracy.\" This metric is calculated by dividing the total number of correct answers across all tasks by the total number of questions. Consequently, this method assigns greater weight to datasets with a larger number of questions, providing a more balanced representation of model performance across tasks of varying sizes. It is denoted by \u201cWT-AVG\" in our figures."}, {"title": "3.1 Test Dataset", "content": null}, {"title": "3.2 Tested Models", "content": "We test the accuracy of the following models: LLaMA3-70B, LLaMA3-70B-Instruct, LLaMA3.1-70B, Llama3- 70B-Synthia, calme-2.2-llama3-70b, LLaMA3-8B, LLaMA2-70B, LLaMA2-70B-chat, Qwen2-72B, Mixtral-8x7B, Phi3-14B-Instruct, Mistral-L-Instruct-123B, Falcon-40B. HuggingFace links to them will be provided in the appendix. We also analyze the weight distribution of LLaMA3.1-405B without evaluating its accuracy with FP16 due to GPU memory limitations."}, {"title": "3.3 Test Environments", "content": "All tests are done within 8xA100 (80GB) GPUs with PyTorch framework using lm-evaluation-harness."}, {"title": "3.4 LLaMA3-70B's sensitivity to W8A8", "content": "Figure 1 and Figure 3 present a comparative analysis of accuracy for several models in the LLaMA3-70B series subjected to 8-bit per-channel post-training quantization. This analysis encompasses the recently released LLaMA3.1-70B and two additional models fine-tuned from LLaMA3-70B. These figures yield several notable observations:\n\u2022 The accuracy of models undergoes significant degradation with W8 quantization, even when activations are maintained at FP16 precision. This indicates that the observed accuracy deterioration is not attributable to quantization errors in activations, but rather originates from the 8-bit weight quantization process.\n\u2022 Models fine-tuned from LLaMA3-70B exhibit the same vulnerability to W8 quantization as their base model. This persistence of vulnerability can be attributed to the fact that fine-tuning typically induces only minor alterations to weight values. In the subsequent section, we will demonstrate that this vulnerability is intrinsically linked to the underlying weight distributions of the model."}, {"title": "3.5 LLAMA3-70B is the ONLY ONE sensitive to W8A8", "content": "Figure 4 and Figure 5 compare the accuracy of other models with different architectures with W8A8 quantization. The analysis spans the LLaMA family, including LLaMA3-8B, LLaMA2-70B, and LLaMA2-70B-chat, as well as other prominent Transformer-based designs such as Qwen2-72B, Mixtral-8x7B, Mistral-123B, Phi3-13B, and Falcon-40B. These architectures represent the top-performing architectures in the LLM Open Leaderboard.\nThe data presented in these figures illustrates a contrast to the behavior observed in the LLaMA3-70B series. Specifically, the models examined here exhibit remarkable resilience to W8A8 per-channel quantization. In the majority of instances, the performance of W8A8 quantized versions closely approximates that of their FP16 counterparts. While minor variations exist, the performance decline rarely exceeds 1.5% (Mistral-123B)."}, {"title": "3.6 Why LLaMA3-70B is the only one?", "content": "In order to understand the uniqueness of LLaMA3-70B models, we explore the weight distributions."}, {"title": "3.6.1 A qualitative exploration", "content": "Figures 2, 6, 7, and 8, illustrate the absolute values of the V matrix weights in the first Transformer block for LLaMA3- 70B, LLaMA3.1-70B, LLaMA3-8B, and LLaMA2-70B models, respectively. Other weight matrices for other models are shown in the Appendix. It is important to note that in per-channel quantization, each group comprises a length-M vector along the input dimension. Our analysis reveals several key observations:\n\u2022 The maximum absolute weight values (max_abs) in LLaMA3-70B (93) and LLaMA3.1-70B (92.5) surpass those in LLaMA3-8B (0.05) and LLaMA2-70B (0.07) by approximately three orders of magnitude. Further- more, LLaMA3/3.1-70B exhibits a non-trivial number of weights with large magnitudes along the output dimension, which we term \u201cweight outliers\u201d. This extensive range of weight values results in large quantization"}, {"title": "3.6.2 A quantitative exploration", "content": "To quantitatively assess the robustness of weights under W8 quantization, we employ two key metrics. The first is the maximum absolute value (max_abs) of weights for each layer in the model, which determines the scale factor and, consequently, the quantization interval. The second metric is the quantization error, measured by the root-mean-square"}, {"title": "", "content": "intervals for LLaMA3/3.1-70B, leading to substantial quantization errors in groups containing any weight outliers.\n\u2022 LLaMA3-70B displays a distinct pattern where weights with large max_abs values cluster at specific input indices, forming visible \"walls\" in Figure 2 and 6. In contrast, LLaMA2-70B and LLaMA3-8B lacks such obvious patterning."}, {"title": "4 A Mixed Grouping Strategy", "content": "Based on our analysis of Figure 9, we have empirically identified that in the LLaMA3-70B model, the Q, K, V, Up, and Gate matrices of Block 0, 1, and 3 exhibit exceptionally high quantization errors and maximum absolute weight values. To mitigate these quantization errors, we propose implementing a finer grouping granularity specifically for these 15 layers.\nIt is noteworthy that the layers requiring per-group quantization, as opposed to per-channel quantization, constitute only 2.68% (15/560) of the total layers in the model. While many quantization studies employ group sizes of 128 or smaller"}, {"title": "", "content": "(e.g., GPTQ, AWQ), our investigation reveals that a group size of 1024 is sufficient to prevent accuracy degradation while maintaining a larger group size for improved hardware efficiency.\nFigure 13 presents a comparative analysis of the accuracy achieved by our proposed method against that of FP16 models. The presented models includes several LLaMA3-70 and its variants and derivatives in the Open LLM Leaderboard. The results convincingly demonstrate that our mixed quantization approach-combining 2.68% per-group and 97.32% per-channel 8-bit quantization can match the accuracy of the LLaMA3-70B model with FP16 precision.\nFigure 14 illustrates the significant reduction in quantization errors achieved for the initially affected layers (specifically, the Q, K, V, Up, and Gate matrices of Blocks 0, 1, and 3) through the implementation of per-group quantization with a group size of 1024. While the maximum absolute value (max_abs) for each layer remains unchanged, the reduction in the number of quantized values per group leads to a marked decrease in quantization error.\nThis improvement can be attributed to the finer granularity of the quantization process in these critical layers. By reducing the group size from the entire channel to 1024 elements, we effectively create multiple quantization groups within each channel. This approach allows for more precise representation of weight values, particularly in the presence of outliers that previously dominated the quantization scale factors.\nIn summary, this hybrid quantization strategy effectively addresses the unique challenges posed by the LLaMA3-70B model's weight distribution, particularly in its initial layers, while minimizing the hardware complexity overhead associated with per-group quantization across the entire model."}, {"title": "5 Ablation on the group size", "content": "We examine the influence of varying group sizes on the accuracy of W8A8 quantization for the LLaMA3-70B model. Figure 15 illustrates this comparative analysis. Our results reveal that certain datasets, notably AE, AC, and OQ, exhibit particular sensitivity to increased group size applied exclusively to a total of 15 layers within Blocks 0, 1, and 3. From a holistic perspective, per-group quantization employing sizes of 512 or 1024 emerges as a good compromise between model accuracy and hardware efficiency."}, {"title": "6 Discussions", "content": "According to Meta's technical report , LLaMA2-70B and LLaMA3-70B share similar model architectures, while LLaMA3-8B/70B and LLaMA3.1-8B/70B/405B are reported to have similar training data and strategies. However, our analysis reveals significantly different weight distribution characteristics in LLaMA3/3.1-70B compared to other LLaMA models. This unexpected divergence in weight patterns, particularly the presence of numerous large outliers exclusively in LLaMA3/3.1-70B models, presents an intriguing peculiarity.\nThe origins of this phenomenon remain unclear, given that models sharing either architectural design or training methodologies do not exhibit similar characteristics. Unraveling the underlying mechanisms responsible for this unique weight distribution could potentially offer valuable insights into the learning processes of Large Language Models (LLMs), especially in the context of reasoning tasks."}, {"title": "7 Related Works", "content": "Quantization is useful in reducing model size and potentially increases model inference speed. There two main categories of quantization, post-training quantization (PTQ) and quantization-aware training (QAT)."}, {"title": "7.1 Post-training quantization", "content": "PTQ does not use any back-propagation to finetune weights. For weight-only quantization, LLM.int8() uses a mixed precision with 16-bit MAC for outliers. GPTQ and AWQ  quantize the per-group weight-only tensor using as small as 3-4 bits. SqueezeLLM uses non-uniform quantization and store outliers efficiently. For weight-activation quantization that enables faster inference SmoothQuant  balances the outliers from activation to weights to enable per-tensor W8A8. QServe proposes W4A8K4 to serve LLM models. Some other studies enable even smaller number of bits Shao et al. [2024], Zhao et al. [2024], Ashkboos et al. [2024]."}, {"title": "7.2 Quantization aware training", "content": "Quantization-aware training (QAT) requires both calibration data and back-propagation to finetune the quantize weights. While full finetuning can recover the accuracy to the maximum extent, Q-LoRA  reduces the memory requirement significantly."}, {"title": "8 Conclusions", "content": "Our investigation reveals a distinctive vulnerability in the LLaMA3-70B model series, including LLaMA3.1-70B, to per-channel quantization. We attribute this susceptibility to significant weight outliers in the initial layers. The contrast in behavior between the LLaMA3-70B and other LLaMA models (LLaMA2, LLaMA3/3.1-8B, LLaMA3.1-405B), as well as other tested LLMs with different architectures, warrants further exploration of the underlying training strategies. To address this issue, we propose a mixed-grouping method that substantially enhances the accuracy of LLaMA3-70B from 45.4% to 73.4%, effectively matching the performance of its FP16 counterpart."}, {"title": "9 Appendix", "content": null}, {"title": "9.1 Models", "content": "Table 1 shows all the tested models downloaded from Huggingface repository."}, {"title": "9.2 Weights of LLaMA3-70B", "content": null}, {"title": "9.2.1 The first block has more large outliers", "content": "We show the distribution of weights in the first block in LLaMA3-70B. Figures 16, 17, 18, 19, 20, 21, 22 shows seven weight matrices in the 0-th Transformer block. The z-axis shows the magnitudes of outliers in these matrices. We can see that the Q, K, V, Up, and Gate matrices have larger outliers and they form a \u201cwall\" in some input indices. The other two weights, namely O and Down, do not show such a pattern."}, {"title": "9.2.2 The latter block has less large outliers", "content": "Figure 23, 24, 25, 26, 27, and 28 show the Q and Up matrices in the 10th, 40th, and 79th (last) Transformer block in LLaMA3-70B. The magnitude of the weight outliers is significantly smaller than that in the first block, demonstrating more robustness to W8 per-channel quantization."}, {"title": "9.3 Weights of Other LLM models", "content": "Figure 29, 30, 31, 32 show the weights with maximum quantization error under W8 per-channel quantization in four LLMs, that is, LLaMA2-70B, LLaMA3-8B, LLaMA3.1-405B, and Qwen2-72B. It is worth noting that the maximum quantization error occurs all at the same layer, which is the \"K\" matrix in the first Transformer block. We can see that the weight outliers in the layer with maximum quantization error is less than 1.0, which is far smaller than that of LLaMA3-70B models, which is usually greater than 90. The difference of 2 orders of magnitude causes the quantization error of the LLaMA3-70Bs to be much more than the rest of the models."}]}