{"title": "Could ChatGPT get an Engineering Degree? Evaluating Higher Education Vulnerability to AI Assistants", "authors": ["BEATRIZ BORGES", "NEGAR FOROUTAN", "DENIZ BAYAZIT", "ANNA SOTNIKOVA", "SYRIELLE MONTARIOL", "TANYA NAZARETZKY", "MOHAMMADREZA BANAEI", "ALIREZA SAKHAEIRAD", "PHILIPPE SERVANT", "SEYED PARSA NESHAEI", "JIBRIL FREJ", "ANGELIKA ROMANOU", "GAIL WEISS", "SEPIDEH MAMOOLER", "ZEMING CHEN", "SIMIN FAN", "SILIN GAO", "METE ISMAYILZADA", "DEBJIT PAUL", "PHILIPPE SCHWALLER", "SACHA FRIEDLI", "PATRICK JERMANN", "TANJA KASER", "ANTOINE BOSSELUT"], "abstract": "AI assistants are being increasingly used by students enrolled in higher education institutions. While these tools provide opportunities for improved teaching and education, they also pose significant challenges for assessment and learning outcomes. We conceptualize these challenges through the lens of vulnerability, the potential for university assessments and learning outcomes to be impacted by student use of generative AI. We investigate the potential scale of this vulnerability by measuring the degree to which AI assistants can complete assessment questions in standard university-level STEM courses. Specifically, we compile a novel dataset of textual assessment questions from 50 courses at EPFL and evaluate whether two AI assistants, GPT-3.5 and GPT-4 can adequately answer these questions. We use eight prompting strategies to produce responses and find that GPT-4 answers an average of 65.8% of questions correctly, and can even produce the correct answer across at least one prompting strategy for 85.1% of questions. When grouping courses in our dataset by degree program, these systems already pass non-project assessments of large numbers of core courses in various degree programs, posing risks to higher education accreditation that will be amplified as these models improve. Our results call for revising program-level assessment design in higher education in light of advances in generative AI.", "sections": [{"title": "1 Introduction", "content": "ChatGPT, a system using a large language model (LLM), GPT-3.5, as its foundation, was released in November 2022 to broad adoption and fanfare, reaching 100M users in its first month of use and remaining in the public discourse to this day. As arguably the most hyped AI system to date, its release has prompted a continuing discussion of societal transformations likely to be induced by AI over the next years and decades. Potential changes in modern educational systems have remained a core topic in this discussion, with early reports highlighting the risk of these AI systems as tools that would allow students to succeed in university coursework without learning the relevant skills those courses are meant to teach. Despite this concern, there has yet to be a comprehensive empirical study of the potential impact of LLMs (and derivative tools) on the assessment methods that educational institutions use to evaluate students. A few studies have explored the interesting sub-task of how well models perform on problems related to topics typically taught in many university courses and aggregated relevant question sets for this purpose [4, 25, 27, 60, 71]. However, none of these works extrapolate these findings to assess the downstream impact of these tools on degree programs, where the risk of these technologies relative to their pedagogical benefits must actually be measured.\nIn this work, we conduct a large-scale study across 50 courses from EPFL to measure the current performance of LLMs on higher education course assessments. The selected courses are sampled from 9 Bachelor's, Master's, and Online programs, covering between 33% and 66% of the required courses in these programs. From these courses, we compile a bilingual dataset (English and French) of 5,579 textual open-answer and multiple-choice questions (MCQ). All questions were extracted from real exams, assignments, and practical exercise sessions used to evaluate students in previous years. The course distribution is presented in Figure 1, and the dataset statistics are shown in Table 1 (stratified by particular dataset attributes).\nUsing this dataset, we subsequently test two commonly-used mod-els, GPT-4 [40], the system widely considered to be the most performant [70] among public AI assistants,\u00b9 and GPT-3.5 [39], a highly performant freely available system. We generate responses from these systems using a range of prompting strategies [10, 36, 58, 61, 62, 64, 67], each of which varies in complexity, but all of which could easily be applied by a lay practitioner with minimal training in prompt engineering [49]. We evaluate these systems using both automatic and manual grading, where manual grading of open-answer questions is performed by the same faculty staff that designed these problems and who have experience grading student answers to them. Subsequently, we conduct a detailed analysis of the generated outputs and their assessment results, considering factors such as the number of courses that would be passed, their distribution across university programs, as well as the effects of the question difficulty and language.\nOur results show that AI systems are relatively capable of an-swering questions used in university assessments. GPT-4 responds"}, {"title": "2 Data Collection", "content": "We compile a new dataset of assessment questions from 50 courses offered at our institution from both on-campus and online classes. Following data pre-processing and filtering steps, this dataset consists of a total bank of 5,579 textual multiple-choice (MCQ) and open-answer questions in both English and French. These questions span various levels (e.g., Bachelor, Master), and cover a broad spectrum of STEM disciplines, including Computer Science, Mathematics, Biology, Chemistry, Physics, and Material Sciences. Table 1 and Figure 1 provide an overview of the dataset's main statistics and the distribution of questions across different topics. Additionally, we have collected course-specific attributes such as the year when the course is first offered in our institution's degree programs (e.g., Master's year 1), the program designation (e.g., Physics), the language of instruction (e.g., French), and the average student enrollment over recent years. Finally, certain questions have been labeled by the in-structor who designed the question with a subjective annotation of the question's difficulty."}, {"title": "3 Experimental Findings", "content": "In our study, we investigate the vulnerability of university programs to generative AI systems using our question bank of 5,579 evaluation questions from 50 courses. We consider two models, GPT-4 and GPT-3.5, selected due to their popularity and usage rates. GPT-4 is considered the most performant model among all publicly accessible LLMs but is only available through a premium subscription, impeding its use by many students. GPT-3.5 is a less performant alternative, but free to use. We generate responses to questions from these models using eight relatively easy-to-apply prompting methods (implementation details are described in Appendix B). For multiple-choice questions, we assess whether a response is correct by comparing the selected choice with the annotated correct answer option. For open-response questions, we use GPT-4 to rate the quality of the response on a four-point scale: Correct, Mostly Correct, Mostly Incorrect, Incorrect, which we map to scores of 1, 0.66, 0.33, and 0, respectively, for calculating performance.\u00b2"}, {"title": "3.1 Can LLM systems pass university-level courses?", "content": "We begin our analysis by assessing model performance in a setting where the user has zero knowledge about the question topic. In the simplest scenarios, where we use the most straightforward prompting strategies, such as directly asking a question (zero-shot) or asking the model to provide a reasoning chain before answering the question (chain-of-thought zero-shot), GPT-4 achieves average accuracies of 55.9% and 65.1%, respectively. However, if the user employed a slightly more complex zero-knowledge strategy, such as majority voting over the eight answers generated by the different prompting strategies, they would receive a correct answer to 65.8% (on average) of questions using GPT-4 (and 52.2% using GPT-3.5). We observe that this performance trend holds regardless of the language of the assessment, with English exhibiting slightly better results than French. Further experimental results for assessments in English and French are detailed in Appendix G.4.\nBeyond overall performance across the question bank, Figure 2 presents the proportion of passed courses for our sample of 50 courses based on varying passing thresholds. Alarmingly, GPT-4 can easily be used to reach a 50% performance threshold (which could be sufficient to pass many courses at various universities) for 89% of courses with MCQ-based evaluations and for 77% of courses for open-answer questions. While not performing quite as well, GPT-3.5, the freely available model, can reach a 50% threshold for 70% of courses with MCQ-based assessments and for 50% of courses with open-answer questions. As passing thresholds may not be set to 50% for all institutions, we vary this threshold and find that GPT-4 still passes 68% of courses at a 60% passing threshold, and 38% of courses at a 70% passing threshold for MCQ. Similar results are found for open-answer questions.\nOur results suggest that users with no knowledge of a particular subject could solve enough assessment questions to pass a majority of the courses in our dataset, making a compelling argument that AI assistants could potentially augment student learning as support tools. However, they simultaneously present a credible short-term risk to educational systems if these systems are not adapted to protect against the misuse of these technologies. Finally, we expect this problem to only grow worse over time, as continual model improvements in the years to come will make these tools even more performant in academic contexts."}, {"title": "3.2 How do these results affect university programs?", "content": "The average performance across courses demonstrates each course's potential vulnerability to generative AI tools, which is particularly important if considerable portions of degree programs can be com-pleted using these tools. To evaluate this program's vulnerability, we aggregate the questions in our datasets according to the study programs in which they are core courses. Specifically, we include four program types: 1st-year Bachelor, Full Bachelor, Full Master, and Online courses. We separate the first year of the Bachelor's degree because, at many institutions (including ours), the first year of the Bachelor's may have a fairly standardized curriculum that serves a special purpose (e.g., replacing or complementing entrance exams). Full Bachelor's and Master's correspond to regular Bachelor's and Master's programs. We also include online courses, as official certifications can often be awarded for completing a sequence of these courses. For each program, our dataset contains a sample of courses that cover from 33% to 66% of the required courses for that program. For more program statistics, see Appendix F.\nWe consider the same two aggregation strategies across the re-sponses provided by the eight prompting methods: majority vote and maximum performance. For the majority vote, given the eight prompting strategies we have, the final answer to the question is the one that is the most frequent across all strategies. In the maxi-mum performance aggregation, only a single prompting strategy is required to answer correctly for the model to be deemed correct in its response, approximating a pseudo-oracle setting that remains con-textually realistic, as a user might be able to distinguish the answer when presented with it, even if they could not find it on their own.\nIn Table 2, we present the number of courses that would be passed by GPT-4 across the 9 tested degree programs for various course passing thresholds t (i.e., the performance that must be achieved to consider the course passed). Our results show that the general course vulnerability observed in the previous section extends to program vulnerability. At the \u03c4 = 50% threshold for passing a course, at"}, {"title": "3.3 More challenging assessments are a half-solution.", "content": "One possible solution to mitigate assessment vulnerability would be to increase their difficulty beyond what generative AI systems are capable of solving, as we observe that the performance of these systems does decrease on more challenging assessment questions (Figure 3). We measure the difficulty using a sub-sample of our question bank that is annotated according two different categoriza-tions of their difficulty: (1) instructor-reported question difficulty, a five-point difficulty categorization for Bachelor courses and two-point for Master's courses provided by the course instructors, and (2) Bloom's taxonomy [9], a six-point scale that measures the cognitive complexity of a question."}, {"title": "4 Discussion", "content": "Summary. In this work, we tested the ability of LLMs to solve assessment questions for a large number of courses from technical and natural sciences academic programs at EPFL. We find that LLMs are generally capable of answering 50-70% (depending on the model) of questions correctly given no subject-related knowledge, and up to 85.1% of questions correctly when some subject-specific knowledge is assumed (i.e., the ability to recognize the correct answer). Most importantly, when considering performance across programs, GPT-4 can achieve greater than 50% performance for 83% to 100% of courses (depending on the program) with an average program pass rate of 91.7%. While a higher per-course passing threshold of 70% would only result in 23% to 50% of courses being passed across our programs (with an average of 37%), this would also lead to higher student fail rates as passing thresholds would similarly affect them.\nGiven that continuous advancements in LLM technology will likely further improve these LLM performance numbers in the future, we conclude that higher-education assessment schemes are immediately vulnerable to student exploitation of generative AI tools, specifically in the engineering and natural sciences.\nAssessment Vulnerability. Our results indicate that the broad avail-ability of generative AI tools should urgently trigger discussion on the design and implementation of assessments. Naturally, our results must be placed in the context where they would normally be observed. In many educational institutions, student assessments are frequently closed-book, thereby precluding the direct use of generative AI tools. Many course assessments (e.g., assignments), though, are completed at home without supervision. In the same vein, most online courses typically evaluate students without any supervised, in-person assessment. In these unsupervised settings, the availability of generative AI tools for aiding in the completion of assessments poses risks for many commonly used student evaluation methods.\nOne general trend that we observe from our data is that models perform well on basic learning tasks, such as memorizing factual knowledge. In courses where memorization of factual knowledge is a core evaluation objective, students should not be allowed to use these tools in non-proctored settings, and these courses should perhaps return to traditional in-person exam-ination [59]. Barring this possibility, in the case of non-proctored assessments, we recommend that their design should not only con-sider the possibility of assistance from generative AI but actively assume its use. At the very least, assessments should be designed with generative AI in the loop to design AI-adversarial evaluation that remain fair to students.\nAt the same time, these findings provide an opportunity to im-prove and diversify student learning through assessments. Students acquire relevant skills when assessments emphasize analytical and applied knowledge settings [69]. Rather than using proctored ex-ams, then, or limited practical works, such as assignments, students should be evaluated using assessments requiring a more composite application of course concepts, such as larger class projects. Project settings more closely assess students on problems resembling real-world challenges, would provide students with more opportunities to make problem-solving decisions, such as problem simplification, decomposition, and planning [37], and would mitigate the impact of generative AI tools .\nEducation Vulnerability. While our results point to an urgent need to mitigate assessment vulnerabilities in higher education, a longer-term view requires considering how education as a practice should evolve alongside the availability of generative AI tools. Since the release of ChatGPT, ongoing discussions have revolved around this topic with both negative and optimistic views. While numerous studies explore the ways AI can enhance learning, ethical concerns related to plagiarism, biases, and overreliance on technology have also been highlighted [3, 12, 16, 18, 30, 42, 66].\nAn important dimension of these discussions emphasizes the need to revisit evaluation procedures to ensure that students acquire nec-essary skills and critical thinking abilities in the face of AI integra-tion [7, 20, 38, 43]. For instance, observations from various works (and our study) show that models excel in generating code to solve software problems [26, 31, 32, 55, 65]. While this capability reduces the burden on professional (and hobbyist) software developers, it also poses a risk for learners by potentially offering shortcuts that impede the acquisition of fundamental coding and engineering skills [17]. Coding tools such as GitHub's Copilot or OpenAI's Codex may lead novices to over-rely on auto-suggested solutions. This overreliance may diminish their engagement with computational thinking [7, 43], which is arguably the most important skill that is learned in any computer science course or program.\nBeyond this example, many studies underscore the significance of adapting course materials and assessments to promote critical thinking, encourage student collaboration, develop practical skills, enhance soft skills, and promote interdisciplinary learning, all with the aim of cultivating graduates equipped with a diverse range of com-petencies [2, 11, 15, 38]. In particular, reinforcing our conclusions above, open-ended assessments are proposed to promote originality and creativity, potentially discouraging reliance on generative AI tools and fostering unique ideas and critical analysis [15, 33]. Ulti-mately, these studies suggest the greater risk of generative AI may be its potential to enable the unintentional circumvention of frameworks by which learners are taught the foundations to learn later skills, and that teaching and assessment should be adapted for this risk.\nFinally, assuming that students will use and become acquainted with the capabilities of these technologies, we recommend that stu-dents should not only be educated on the technical and ethical chal-lenges of generative AI systems, but also on the critical thinking required to successfully engage with such tools [57]. One such mea-sure could involve establishing committees for ethical oversight and adding classroom discussions on the use of AI tools. Such discus-sions would clarify what constitutes plagiarism and address potential ethical concerns, ensuring that students understand the importance of academic integrity and discern the boundaries between legitimate assistance and academic misconduct [2, 11, 15, 17, 20, 33]."}, {"title": "5 Limitations", "content": "While our study offers preliminary insights into the vulnerability of degree programs to student use of AI assistants for assessments, we acknowledge several limitations in our study.\nFirst, our study excluded any multimodal inputs, such as questions containing diagrams, figures, or graphs, which were omitted from our dataset. Approximately 57% of the initially collected data did not qualify for inclusion in the final dataset of 5,579 questions. Conse-quently, models were solely evaluated with text-only questions. This approach likely resulted in performance outcomes that are higher than what these models would attain when tested on question sets that include these other modalities, though we also note rapid growth in the multimodal capabilities of these models [68].\nSimultaneously, our findings might underestimate the performance potential that students could attain through collaboration with these systems. Although we conducted a thorough examination of prompt-ing strategies, our methods are limited by the fact that they (1) rely solely on published prompting strategies, (2) are generally non-interactive, and (3) are tailored for scalability across all questions to facilitate a comprehensive study. Students aiming to address individ-ual questions could devote more time and devise more interactive, less standardized prompting strategies to collaboratively guide the models toward improved solutions.\nFinally, we acknowledge certain gaps between our evaluation of AI systems in this study, and how students are normally evaluated in these courses. First, our study standardizes system evaluation across all course assessments, removing course-specific assessment policies for questions. For example, certain courses, beyond not giving points for correct answers to multiple-choice questions, might also penalize incorrect answers more than leaving a question unanswered, while our study simply gives zero points for incorrect answers. Second, our dataset of questions for each course is not necessarily balanced according to a course's grading rubric. As an example, our dataset may contain a balanced mixture of questions from assignments and exams for a particular course, while the overall evaluation of a student in this same course would compute their grade as a 10% mixture of assignments, and 90% mixture of exam questions. Similarly, many courses at our institution also include lab or project components as part of their final grade. Since these parts of the assessment do not have a correct answer that can be easily marked, they are not included in our dataset.\nAs we do not consider these course-specific assessment policies when computing the course pass rates of our tested AI assistants, these design decisions introduce a gap between our evaluation and the actual assessment rubrics by which students are graded in our institution's courses. Despite this divergence, however, we note that other institutions may implement course assessments and grading rubrics in different ways. As a result, while our study is not an exact simulation of our institution's diverse course assessment schemes, it remains a suitable testbed for providing insights into how course assessments are vulnerable to AI assistants, and how this vulnerability would extend to full university programs without mitigations."}, {"title": "6 Materials and Methods", "content": "Below, we outline the grading strategies used to evaluate the model's performance across two question types: multiple-choice (MCQ) and open-answer questions. For MCQ, grading is automated by comparing against the annotated answer. Answers receive a binary score of 0/1 if only one correct option exists, or a proportional score based on the number of correct choices made for multi-answer questions (with no penalty for wrong choices). Appendix C provides more details for grading MCQs. For open-answer questions, we constructed an evaluation pipeline using GPT-4 as a grader [70], which we describe below. For both types of results, we report error bars representing 95% confidence intervals (Figures 3, 4). These intervals were com-puted using the non-parametric bootstrap with 1000 resamples. We also tasked human experts with independently grading a subset of model responses to measure alignment between model and human grading, and establish a confidence level for model-based grading.\nEvaluating Open-Answer Questions. A significant portion of the questions we extracted are open-answer questions, which are chal-lenging to evaluate manually due to the sheer volume of responses (a total of 33,904 answers from 2,119 questions, answered by 2 models using 8 prompting strategies). As a result, we use a state-of-the-art LLM, GPT-4, as a grader. To automate the grading of open answers, we provide the model with the question, the correct solution from an answer sheet of the assessment, and the generated answer text, prompting it to assign a rating based on the quality of the response. We provide the model with a 4-point grading scale: Correct, Mostly Correct, Mostly Incorrect, Incorrect. The model is first tasked with assessing the accuracy and completeness of the answer before assigning the final grade. Although we do not use these interim accuracy and completeness scores, we manually observe that these stages enhance the quality of overall grading. The specific prompting strategy is detailed in Appendix C.2. As an answer was produced for each question using eight distinct prompting strate-gies, we obtained eight different answers and corresponding grades."}, {"title": "6.2 Evaluation", "content": "To present a cohesive performance score for both GPT-4 and GPT-3.5 for a given question, we employ two aggregation methods: (1) the maximum approach, which selects the answer with the highest grade for each question as a representation of model performance, and (2) the majority approach, which considers the grade that appears most frequently among the eight prompting strategies. As an example, for a hypothetical question whose generated answers for the eight prompting strategies received 2 Correct, 2 Mostly Correct and 4 Mostly Incorrect grades, the maximum grade would be Correct and the majority grade would be Mostly Incorrect.\nHuman Grading. To assess how well model grading aligned with hu-man grading on open-answer questions, we enlisted 28 expert annota-tors from the teaching faculty of 11 courses to evaluate 933 questions. The courses chosen for expert grading are listed in Appendix C.3. Specifically, we requested graders to assign scores to open-ended re-sponses generated by GPT-4 and GPT-3.5. Human-graded responses for both models were generated using two prompting strategies: zero-shot chain-of-thought prompting [62] (a simple prompting method at the disposal of any student) and metacognitive prompting [61] (one of the most effective strategies across all courses). We anonymized the outputs to prevent graders from knowing which model and prompting strategy they were evaluating. To maintain consistency, we instructed graders to use the same grading scale as GPT-4 direct grading. The number of graders per course varied from 1 to 10, and a total of 3732 answers were evaluated. On average, graders spent approximately 5 minutes assessing each answer.\nFigure 6 indicates a general alignment between human graders and GPT-4 when categorizing answers into a simplified correct/incorrect quadrant. Out of the examples identified as Correct by graders, the model assigned the same grade to 61% of them. Similarly, for examples graded as Almost Correct by graders, the model's grade matched in 36% of cases. Additionally, in instances where graders labeled examples as Mostly Incorrect, the model's"}, {"title": "B Prompting Strategies", "content": "Our study's goal is to identify the vulnerability of educational as-sessments to AI systems. As a result, we select prompting strategies that simulate realistic student use and assess prompting strategies that can be used with minimum effort, requiring only knowledge of the relevant literature and minimal adaptation. We exclude strategies involving training models. Our assessment encompasses three pri-mary categories of prompting strategies: direct prompting, wherein the model is directly prompted to provide an answer; rationalized prompting, which encourages the model to first verbalize reasoning steps before providing a response; and reflective prompting, which prompts the model to reflect on a previously generated response be-fore finalizing an answer. Each prompt is tailored for three scenarios: (1) MCQs with a single correct answer, (2) MCQs with multiple correct answers, and (3) open-answer questions. Below, we outline the strategies used to prompt models to answer questions:"}, {"title": "B.1 Direct Prompting", "content": "We explore three strategies for direct prompting: zero-shot, one-shot, and expert prompting. All these strategies ask the LLM directly for an answer without encouraging any particular strategy or rationale to arrive at the answer.\nZero-shot Prompting. We ask the model to solve questions without any demonstrations or system role prompts. The instructions vary depending on the type of question: open-answer or multiple-choice (MCQ). Additionally, we differentiate between multiple-choice cases where a single answer is correct and cases where multiple correct answers can be selected.\nMCQ single answer: You are given a question followed by the possible answers. Only one answer is correct. Output the correct answer.\nMCQ multi answer: You are given a question followed by the possible answers. The question can have multiple correct choices. Output all the correct answers.\nOpen answer: Solve the following question:\nFor MCQs, each answer option is associated with a letter, and the model is expected to provide the letter corresponding to its choice.\nOne-shot Prompting [10]. In this prompting strategy, we instruct the model to solve questions based on a provided example as a demon-stration, without any additional system role prompt. Each question is paired with a demonstration that is the most similar to the ques-tion being addressed. Specifically, we use the \"all-roberta-large-v1\" model4 [35] to embed all questions as vectors, and then retrieve the most similar question vector based on cosine similarity to the prompt question vector. We append the corresponding demonstrative example to this retrieved vector to the prompt. The prompt instruc-tions remain the same as for zero-shot prompting. The demonstration is provided to the model in a multi-message setting, mimicking an actual conversation between the user and the assistant.\nExpert Prompting [64]. In expert prompting, we use the LLM to to simulate the responses of three experts in the field. The model gener-ates answers as if written by these experts, and then we combine their responses using collaborative decision-making, typically through majority voting. This process is represented by using a generic expert defined as the system role, such as \"You are a professor of Machine Learning\" for questions from, e.g., Machine Learning and prompt-ing the model to give us the names of three experts in the field capable of solving the given question using the prompt:\nSystem: You are an expert in {course name}.\nGive an educated guess of the three experts most capable of solving the following question. Only output the name of these three experts as a json format with key as number and value as a name, without any explanation."}, {"title": "B.2 Rationalized Prompting", "content": "We explore three strategies for eliciting reasoned answers: zero-shot and four-shot chain-of-thought, and tree-of-thought prompting. Each strategy involves prompting the LLM to generate a rationale before providing a final answer.\nChain-of-Thought Prompting (CoT) [62]. In chain-of-thought prompting, we guide the model to generate a sequence of reasoning steps before providing an answer. This approach typically results in more coherent, structured, and accurate responses, as it requires the model to present arguments before delivering the final answer. This behavior is often initiated by an instruction such as \"Let's think step by step.\" For better performance, the model may be given demon-strations that illustrate how to break down a question into multiple reasoning steps. However, manually generating these demonstrations for each course is time-consuming. Therefore, we automatically gen-erate multiple example rationales using GPT-4 for questions from each course. Domain experts then manually select the best chain-of-thought reasoning trace for each question and correct or improve it if necessary.\nWe experiment with two settings: zero-shot (no demonstration) and few-shot (4 demonstrations). For the latter, for each question, we sample 4 demonstrations of the same course cluster (same topic) and the same question type (MCQ or open-answer), ensuring that these demonstrations were different from the question being asked to the model. Sometimes, the total length of the 4 demonstrations exceeds the model's maximum context length. In such cases, we reduced the number of demonstrations to fit within the context limit. Additionally, we provided a system prompt that included the course topic as an extra hint for the model. For each question type, the selected prompts are the following:\nSystem: You are an expert in {course name}.\nMCQ single answer: You are given a question followed by the possible answers. Only one answer is correct. Give a step-by-step reasoning, and then output the correct answer.\nMCQ multi answer: You are given a question followed by the possible answers. The question can have multiple correct choices. Give a step-by-step reasoning, and then output all the correct answers.\nOpen answer: Solve the following question, by first giving the step-by-step reasoning and then outputting the answer:\nThe demonstration pairs, which include the question and its reasoning explanation, are provided to the OpenAI API using a multi-message setting similar to the few-shot strategy.\nTree-of-Thought Prompting [67]. While chain-of-thought prompt-ing has led to performance improvements in many NLP tasks, it is sensitive to incorrect reasoning steps, as there is no mechanism to as-sess and fix a reasoning error after it has been made. Tree-of-thought prompting extends chain-of-thought by having the model emulate"}, {"title": "B.3 Reflective Prompting", "content": "We explore two strategies for reflective prompting: self-critique and metacognitive prompting. Both strategies involve the model reflecting on an answer it previously provided. Based on this reflection, the model then generates a final, improved answer.\nSelf-Reflect Prompting [36, 58]. This strategy is performed on in conjunction to CoT to refine the reasoning traces generated by the model. Focusing on MCQ questions, first, we provide the model with a question and its zero-shot CoT response. Then, we prompt the model to revise its reasoning and produce a refined answer. Notably, this refinement process is carried out without any demonstrations.\n{CoT prompt and model output}\nMCQ single answer: Please consider that there is a single correct choice. Is the provided reasoning accurate? If there isn't any inaccu-racy, please output \"Reasoning is fine.\" Otherwise, please revise your reasoning and then choose the single correct choice.\nMCQ multi answer: Please consider that multiple choices can be correct. Is the provided reasoning accurate? If there isn't any inaccu-racy, please output \"Reasoning is fine.\" Otherwise, please revise your reasoning and then and then output all the correct choices.\nOpen answer: Assume you got the above answer from a student and you're looking for inaccuracies in either the reasoning or the final response. Try to refine any inaccuracy and answer the question from scratch. Please don't mention in your answer that you're refining a previous answer and write a new answer from scratch. Answer:\nMetacognitive Prompting [61]. Motivated by the concept of meta-cognition, this prompt is designed to emulate the human process of introspection and regulation of thinking. To achieve this, the lan-guage model is tasked with following a specific procedure akin to human cognitive processes. This involves sequentially: (1) deeply understanding the problem, akin to human comprehension; (2) iden-tifying relevant concepts and formulating a preliminary answer; (3)"}, {"title": "C Evaluation", "content": "In this section we describe the methods used for grading MCQs and open answer questions with GPT-4."}, {"title": "C.1 Multiple Choice Scoring", "content": "Regardless of the prompting strategy used for MCQs, the model is provided with the list of answer choices, each associated with a letter, and is asked to generate the letter(s) corresponding to the correct answer(s). Therefore, grading MCQs involves extracting the letter(s) indicated in the model's response and comparing them with the correct answer(s) (i.e., ground truth).\nThis process is straightforward for direct prompting strategies, but more challenging for strategies involving reasoning, such as chain-of-thought, where the model's response may include long explanations that discuss incorrect answers. To ensure consistency in answer extraction across different types of responses, we use an LLM (GPT-3.5) with the following prompt to extract the model's final answer:\n{Question prompt}\n{Model output}\nIf the above answer does not provide an option, or gives an answer which is not in the options list, you should give the following:\n{\"selection\": [None]}\nOtherwise, please return the answer in a dictionary format, with the key being \"selection\", and the value is a list that contains the index of letters of all the correct choices, with A being 0, B being 1, and so on:"}, {"title": "C.2 Open Answer Direct Grading", "content": "For open-answer grading", "below": "nSystem prompt: You are a teacher of {course name"}, ".", "You must grade exam questions.\nUser Prompt: You must rigorously grade an exam question. Please be strict and precise in your assessment, providing reasoning for your assigned grade. Here's the process I'd like you to follow:\nCarefully read and understand the question.\nThoroughly compare the student's answer with the correct golden answer.\nEvaluate the student's response based"]}