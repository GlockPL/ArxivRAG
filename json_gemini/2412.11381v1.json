{"title": "Adapting Segment Anything Model (SAM) to\nExperimental Datasets via Fine-Tuning on GAN-based\nSimulation: A Case Study in Additive Manufacturing", "authors": ["Anika Tabassum", "Amirkoushyar Ziabari"], "abstract": "Industrial X-ray computed tomography (XCT) is a powerful tool for non-destructive char-\nacterization of materials and manufactured components. XCT commonly accompanied\nby advanced image analysis and computer vision algorithms to extract relevant informa-\ntion from the images. Traditional computer vision models often struggle due to noise,\nresolution variability, and complex internal structures, particularly in scientific imaging\napplications. State-of-the-art foundational models, like the Segment Anything Model\n(SAM)-designed for general-purpose image segmentation-have revolutionized image\nsegmentation across various domains, yet their application in specialized fields like mate-\nrials science remains under-explored. In this work, we explore the application and limita-\ntions of SAM for industrial X-ray CT inspection of additive manufacturing components.\nWe demonstrate that while SAM shows promise, it struggles with out-of-distribution data,\nmulticlass segmentation, and computational efficiency during fine-tuning. To address\nthese issues, we propose a fine-tuning strategy utilizing parameter-efficient techniques,\nspecifically Conv-LoRa, to adapt SAM for material-specific datasets. Additionally, we\nleverage generative adversarial network (GAN)-generated data to enhance the training\nprocess and improve the model's segmentation performance on complex X-ray CT data.\nOur experimental results highlight the importance of tailored segmentation models for\naccurate inspection, showing that fine-tuning SAM on domain-specific scientific imaging\ndata significantly improves performance. However, despite improvements, the model's\nability to generalize across diverse datasets remains limited, highlighting the need for\nfurther research into robust, scalable solutions for domain-specific segmentation tasks.\nCode and data are available for research purposes 1.", "sections": [{"title": "1 Introduction", "content": "X-ray computed tomography (XCT) has become an indispensable tool for three-dimensional (3D)\nnon-destructive evaluation and characterization (NDE/NDC) in scientific imaging applications. XCT\nenables detailed 3D visualization and analysis of the internal structure of materials and components,\nproviding valuable insights into microstructures, defect detection, and material behavior over time.\nIndustries such as aerospace, automotive, and energy rely heavily on XCT for quality control, process\noptimization, and research purposes, as it allows for precise inspection of complex components\nwithout physical alteration. In the field of additive manufacturing (AM), XCT is widely used for\nNDE to detect flaws that may form during the build process. Additive manufacturing (also known as\n3D printing) involves building parts layer by layer, enabling the creation of complex and customized\nstructures. However, this process can introduce defects, such as voids or cracks, which affect the\nfinal part's strength and reliability. XCT allows for the non-destructive identification and detection\nof these flaws, enabling qualification of parts for use and optimizing manufacturing processes by\nadjusting parameters such as laser power or speed to minimize defects [10, 19, 29, 12].\nTraditional segmentation algorithms developed for defect detection in XCT images often require\nextensive manual work and rely on specific parameters that are highly dependent on data quality [4,\n21]. These methods are often inconsistent across different materials and defect types, limiting their\nscalability for broader applications. Additionally, their lack of adaptability reduces their effectiveness\nin diverse manufacturing scenarios [17].\nSupervised deep learning (DL) methods have been developed to improve characterization and\nsegmentation quality [3, 25, 7]. However, these methods require significant amounts of labeled data\nand often lack generalization capability across datasets. To enhance generalizability and reduce\nreliance on labeled data, unsupervised and weakly-supervised DL-based algorithms have been\ndeveloped [6, 14, 26, 23]. These techniques, typically trained on large, diverse image sets, can be\nfine-tuned to segment new images not present in the training data, with few/zero-shot learning.\nRecently, foundational segmentation models have emerged, offering greater flexibility and per-\nformance. These models are designed to automatically and robustly delineate different material\nphases within microscopic images, regardless of material type, image resolution, or imaging source.\nLeveraging such advanced models significantly enhances the precision and efficiency of analysis,\nenabling advancements in materials design and quality control.\nOne such foundational model is the Segment Anything Model (SAM), pre-trained on multiple\npublicly available ImageNet datasets [11]. Fine-tuning SAM has shown promising results on medical\nimages [13] and industrial CT images [24]. However, these datasets typically involve single-class\nsemantic segmentation, or SAM was fine-tuned with different prompts that augment the training\ndataset. Additionally, early experiments with SAM on segmenting small flaws and anomalies in\nXCT data from materials have shown limitations. As shown in Fig. 1(c), the pre-trained SAM model\nstruggles to differentiate between pore and inclusion structures (dark and bright spot anomalies) in\nmaterials XCT images.\nIn this work, we aim to address these limitations by proposing a fine-tuning strategy utilizing\nparameter-efficient techniques, such as Conv-LoRa [27], to adapt SAM for material-specific datasets.\nAdditionally, we leverage GAN-generated data to enhance the training process and improve the\nmodel's segmentation performance on challenging XCT data used for inspecting additive manufac-\nturing parts. Our approach demonstrates significant improvements in the segmentation of X-ray\ncomputed tomography (XCT) images, particularly in handling out-of-distribution data and fine-tuning\nfor specialized applications. In addition, we address some of the challenges encountered during\nfine-tuning and inference of SAM on this data. It is worth noting that we selected a case study in\nadditive manufacturing, as the anomalies and flaws in AM parts' XCT images result in an extremely\nunbalanced segmentation problem. Additionally, automated high-throughput inspection of AM parts\nhas significant implications for the next generation of manufacturing processes, addressing supply\nchain shortages and enabling Industry 4.0."}, {"title": "2 Data Generation", "content": ""}, {"title": "2.1 CycleGAN for Unpaired Data Generation and Domain Adaptation", "content": "CycleGAN is a well-established method for unpaired image-to-image translation and is widely used\nfor domain adaptation tasks [28]. CycleGAN learns two mappings: one from domain A to domain B,\nand a reverse mapping from B to A. Its cycle consistency loss ensures that, when an image is mapped\nfrom one domain to another and then back, it closely resembles the original image:\n$\\mathcal{L}_{cyc}(G, F) = E_{x \\sim P_{data}(x)} [||F(G(x)) - X||_1] + E_{y \\sim P_{data}(y)} [||G(F(y)) \u2013 y||_1 $  (1)\nwhere G and F are the generators for mapping between the two domains. This cycle consistency\nproperty allows the model to effectively learn mappings between unpaired datasets without requiring\none-to-one correspondence. In [30], authors utilized computer-aided design (CAD) models of metal\nadditive manufacturing (AM) parts to simulate realistic X-ray CT data. A library of flaws-typically\nappearing as dark spots within the material regions\u2014was embedded into the CAD models to generate\ndatasets incorporating realistic defect distributions. These flaws were simulated using a physics-based\nX-ray CT simulator, modeling beam hardening and noise characteristics inherent to the XCT process.\nDue to significant differences between simulated and real XCT data, including noise and artifacts,\nthe models faced challenges in accurately mimicking real-world data conditions. To address this\ndomain shift, a CycleGAN-based domain adaptation approach was employed to generate realistic\ndatasets. These enhanced datasets were then used to train deep learning models for improved XCT\nreconstruction by suppressing noise and artifacts.\nIn comparison to [30], our work expands the data generation process to include not only material\nflaws but also additional anomalies known as inclusions, which typically consist of denser materials\nand appear as bright spots in industrial XCT images. A known challenge with CycleGANs is\nthe difficulty in preserving small features such as flaws. In our experiments, pores generated by\nCycleGAN tended to have higher contrast and less blurriness compared to real data, affecting their\nrealism. To address this, we ensured that the attenuation coefficient distributions for flaws in our\nsimulations closely matched those obtained from real data, while other attenuation coefficients were\nadjusted based on the density of the materials.\nAnother important factor addressed in our methodology was the alignment of simulated and\nreal data. Given that unpaired datasets do not guarantee perfect registration, we ensured that the\norientations of the simulated data roughly matched those of the real samples on the XCT stage. This\nalignment helped mitigate some limitations of CycleGAN in maintaining the global structure of\nobjects during domain adaptation, improving the overall quality of the generated data without needing\nperfect registration. It is worth noting that we also experimented with constraining the loss function\nof CycleGAN (similar to SP-CycleGAN [8]), as well as Constrained Unpaired Translation GAN\n(CUT-GAN) [15] to further improve feature preservation, but the improvement was marginal, if any.\nWhile the core CycleGAN architecture was sufficient for much of the domain adaptation task, further\nfine-tuning may be needed for handling the nuances of complex material flaws.\nFor training, we used 1746 images with a resolution of 1200x1200 pixels, with a pixel size of\n17.3 \u00d7 17.3\u00b5m\u00b2. We utilized full-resolution image patches of 900x900 to maintain feature fidelity\nduring training (the largest crop size we could keep with GPU constraints that was divisible by\n4). The model was initialized with a learning rate of 0.0002, which was reduced after 100 epochs.\nTraining was conducted for 200 epochs in total, with the first 100 epochs using the fixed learning rate\nand the remaining 100 epochs applying a linear decay. The batch size was set to 4, and a pool size of\n200 was used to store generated images for reuse in the discriminator to prevent overfitting. The pool\nsize, set empirically, helps generalize by exposing the discriminator to a broader range of images,\nthus stabilizing training.\nTraining was performed on four A100 Nvidia GPUs, each with 40GB of memory, and model\nweights were saved every two epochs. We evaluated the performance of the models using the Fr\u00e9chet"}, {"title": "2.2 Real Data Preparation", "content": "X-ray Computed Tomography (XCT) is a technique where an object is scanned from multiple\nangles, with each angle producing a projection recorded on a detector. These projections are then\nprocessed using a reconstruction algorithm to generate a 3D representation of the object. In industrial\napplications, especially when dealing with large detectors and the need to capture fine features,\nthousands of views are often required to achieve high-quality reconstructions. However, this process\ncan be time-consuming, labor-intensive, and costly [22]. The material's complexity also plays a\nsignificant role in the quality of the reconstruction. For instance, dense metallic parts, such as those\nproduced in additive manufacturing, can significantly degrade the quality of the reconstruction as\nmaterial density increases or the geometry becomes more complex [1].\nAdditionally, fabrication and manufacturing processes, combined with material properties, may\nintroduce anomalies and artifacts in the XCT data, such as pores, inclusions, and noise. To create\nboth in-distribution (InD) and out-of-distribution (OoD) datasets for testing deep learning and SAM\nalgorithms, we scanned five separate parts from four distinct materials. We selected a few cases that\nare diverse enough through difference in noise texture (due to materials and scan setting variation),\nand type of flaws and anomalies present in the data, so that we can evaluate the performance of the\nalgorithm. Hence, the resulting reconstructions were intentionally varied, producing both noisy and\nclean datasets, with different types of anomalies and varying numbers of pores. For each part, we\nalso conducted a high-resolution reference scan, using an increased number of views and a deep\nlearning-based model-based iterative reconstruction algorithm [16, 20]. This yielded high-quality\nreconstructions, allowing us to segment pores and inclusions at a higher resolution. These high-quality\nsegmentation were then used as ground truth for validating the lower-quality scans employed for\ntesting our models.\nIn practical industrial settings, performing these high-resolution reference scans is often impracti-\ncal or cost-prohibitive, particularly in environments like manufacturing facilities where thousands\nof samples may need to be scanned for process optimization or part qualification. The high cost\nof detailed scans results in a trade-off between cost and quality. Consequently, many scans are\nconducted using sub-optimal, faster, and sparser acquisition settings, which leads to lower-quality\nreconstructions. This reduction in quality complicates the inspection process, especially in industrial\napplications. Artifacts arising from sparse data, complex geometries, and high-density materials can\nfurther degrade even high-quality scans. Therefore, automated deep learning techniques, particularly\nthose that adapt to parameter variations, become essential for improving the inspection process. By\ngenerating both high-quality and low-quality scans, these datasets help develop more robust models\ncapable of handling the variability and complexities inherent in industrial XCT data. In the following,\nwe describe these real test dataset."}, {"title": "2.3 Training and Test Dataset", "content": "CycleGAN was used to generate several datasets based on different materials and X-ray CT scan\nsettings. Two of these datasets were used for training (referred to as Tr-1 and Tr-2), and one was\nused as an OoD test dataset (referred to as Te-5). In addition, six real experimental test datasets\nwere prepared. These datasets are from various metallic alloys (Al, Ti, Steel, and Ni alloys), each\nwith different properties, and were scanned using different acquisition settings. For each dataset,\nwe acquired a high-quality reference scan to generate ground truth segmentation for performance\nevaluation. By sub-sampling the raw high-quality reference data significantly (by as much as 12x), we\ngenerated reconstructions of lower quality that included artifacts and noise. The real test datasets are\nreferred to as Te-1, Te-2, Te-3, Te-4, Te-6, and are described in Figure 2. While most reconstructions\nwere performed using the standard Feldkamp-Davis-Kress (FDK) algorithm [5], for Te-4 we utilized\na deep learning-based reconstruction method [29], which significantly reduced noise and provided"}, {"title": "3 Methodology", "content": "For efficient training, we leverage a parameter-efficient SAM Conv-LoRa, due to its adaptability for\nmulticlass prediction and utilization of a Mixture-of-Expert (MoE) based low-rank adaptation for\nfine-tuning [27].\nParameter Efficient Segment Anything Model (SAM) (PEFT-SAM)\nSAM consists of three core components: (i) a prompt encoder to enable segmentation for a given\nbox, point, or shape in an image, (ii) an image encoder trained on various masked image datasets,\nand (iii) a mask decoder that captures segmentation from the embeddings obtained from the image\nencoder and prompt embeddings.\nThe image encoder contains a large weight module. Mixture-of-Expert (MoE) is specifically de-\nsigned to adapt a large model's fine-tuning capacity, introducing minimal computational overhead [18].\nInstead of fine-tuning the image encoder, Conv-LoRa adopts a parallel encoder-decoder structure\nalongside the frozen pre-trained weights of the image encoder. Inside this structure, Conv-LoRa\nuses a lightweight convolution operations on the encoder-decoder module as a gating mechanism,\nmanaged by MoE (expert size n = 8). For each expert E\u2081, the embedding H of the parallel structure\nconsists of:\n$\\mathcal{H} = W_{ox} + W_D \\sum_i Conv((W_E x)_i; N_i W_E(x)),$\nwhere Ni is the ith MoE expert module and Conv is the gating mechanism for expert selection.\nWE, WD are the trainable parameters of the encoder-decoder. The aggregated embedding H, com-\nbined with the embedding from the pre-trained image encoder, is then used to train the mask decoder.\nThe purpose behind leveraging Conv-LoRa is determining scale of feature maps as for introducing\ndifferent local priors to improve on performance over OoD data. As MoE consists of multiple N ex-\npert networks a gating module to automatically select which expert to activate in forward propagation.\nAs our goal is multi-class segmentation, the target is the entire input image, where each pixel's class\nmust be identified. To automate SAM without requiring explicit prompts, the prompt encoder is also\nfrozen during training. Full fine-tuning is applied to the mask decoder, as it is a lightweight module.\nFor fine-tuning, the model uses a structure loss (a combination of frequency-weighted IoU loss and\nbinary cross-entropy loss) for 15 epochs until convergence. The validation metric is IoU.\nTraining: We utilized synthetic GAN-generated data for fine-tuning. As each class (material, pore,\nand inclusion) is heavily imbalanced, we found that fine-tuning PEFT-SAM for each class separately\nled to faster convergence and better performance for OoD. To handle the class imbalance, we trained\nthree PEFT-SAM models for three types of binary semantic masks (one for each class). Similar to the\noriginal work of PEFT-SAM Conv-LoRa [27], for this experiment we choose number of experts 8 and\nnumber of trainable parameters 4.02M (0.63% of the pretrained SAM parameters). The number of\ntrainable A post-processing step was then used to aggregate the predicted binary masks, resulting in a\nfinal multiclass predicted mask."}, {"title": "4 Experiments", "content": "We aim to address the following research questions regarding the fine-tuning performance of founda-\ntional models for material microstructure segmentation:\n(R1) How well does fine-tuning SAM compare to the state-of-the-art supervised models?\n(R2) Does including generative model synthesized data improve performance?\n(R3) Can the fine-tuned SAM generalize for out-of-distribution (OoD) data?\n(R4) Does catastrophic forgetting due to fine-tuning impact performance on OoD data?"}, {"title": "4.1 Baseline", "content": "As the baseline, we used the 2.5D U-Net Model proposed in [2]. The model accepts and processes\nmultiple input channels to capture 3D spatial information without the high computational cost of\nfull 3D segmentation. Specifically, the 2.5D U-Net model processes 5 consecutive image slices as\ninput channels to segment one image slice at a time, leveraging the context of adjacent slices in the\n3D volume. For training, we used a single pair of CycleGAN-generated data that included both\npores and inclusions. This data was augmented and preprocessed into 20,000 image patches of size\n5 \u00d7 256 x 256, with each patch containing five channels corresponding to five consecutive slices\nfrom the 3D volume. 80% of the data was used for training, and 20% for validation. These patches\nwere fed into the 2.5D U-Net model to capture the 3D structure of the defects while maintaining the\ncomputational simplicity of a 2D segmentation framework.\nWeighted Dice Loss Function: One of the key challenges during training was the extreme class\nimbalance between the background material, pores, and inclusions. In fact, in Tr-1, 3% and 0.06% of\nthe volume contained pores and inclusions, while in Tr-2, only 0.8% of the volume contained pores,\nwith no inclusions. In the initial stages, we employed a standard Dice loss function, commonly used\nin segmentation tasks due to its effectiveness in handling imbalanced data. However, this approach did\nnot yield satisfactory performance on the validation set, likely due to the severe imbalance between\nthe large background and the much smaller regions containing pores and inclusions.\nTo address this, we implemented a weighted Dice loss function, which applies different weights\nto each class based on their frequency in the training data. The Dice loss function is defined as:\n$L_{Dice} = 1 \u2013 \\frac{2 \\sum_i W_i P_i g_i}{\\sum_i W_i (P_i + g_i)}$, where pi and gi represent the predicted and ground truth binary masks\nfor class i, and wi is the weight for class i. Initially, the weights wi were calculated based on the\nratio of voxels belonging to each class in the training data. However, this approach did not result in\noptimal performance, so we empirically fine-tuned the weights using a small subset of the data. This\nempirical adjustment of the weights improved the model's performance on the validation set.\nTraining Process: The 2.5D U-Net model was trained using the Adam optimizer with an initial\nlearning rate of 2 \u00d7 10-4. The learning rate was reduced whenever the validation loss stagnated\nfor 15 consecutive epochs, with the model's parameters reset to the best-performing state to avoid\noverfitting. The model was trained for 150 epochs, and the best model, selected based on the highest\nvalidation Dice score, was used for inference and evaluation in the subsequent stages."}, {"title": "4.2 Training Data", "content": "To analyze SAM's robustness with Out-of-distribution (OoD) tasks, we fine-tuned SAM using three\nsets of training data. (i) SAM-GAN : fine-tuned with synthetic GAN-based data from Tr-1 and Tr-2\n(detailed in Sec. 2). (ii) SAM-FineReal : SAM-GAN was re-fine-tuned with 15 experimental XCT\nimages. For the two highest FID OoD real test datasets (Te-4 and Te-6), we selected images to add to\nthe training data for further fine-tuning. We selected 6 images from a sample with the same material\nas Te-4, scanned under the same settings and reconstructed using the same algorithm. Additionally,\nwe selected 9 images from the Te-6 volume and removed them from the test set for use in training.\nThese 15 images were used to re-fine-tune the trained SAM-GAN model. (iii) SAM-FineReal-Sub :\nSAM-GAN was re-fine-tuned with only the last 9 experimental XCT images. For (ii) and (iii), we aim\nto analyze SAM's performance after further fine-tuning on a few samples of experimental data. The\nhypothesis is that we can improve the performance of the already trained SAM model by adding just\na few real data images in multiple pairs (case ii) or a single pair (case iii)."}, {"title": "4.3 Layer-by-Layer Precision, Recall, and F1-Score Calculation", "content": "To evaluate segmentation performance, we calculated precision, recall, and F1-score on a layer-by-\nlayer basis for each 3D volume. We used this approach to align with the SAM model, as it operates\non 2D slices of the 3D volume, one at a time. For each layer z, the true positives (TP) represent the\ncorrectly predicted defects that overlap with the ground truth, while false positives (FP) are defects\npredicted but not present in the ground truth, and false negatives (FN) are the ground truth defects\nmissed by the predictions. The precision for each layer is defined as the ratio of true positives to the\nsum of true and false positives: Precision = $\\frac{TP_z}{TP_z + FP_z}$ . Recall is the ratio of true positives to the sum\nof true positives and false negatives: Recallz = $\\frac{TP_z}{TP_z + FN_z}$ . The F1-score, which balances precision\nand recall, is the harmonic mean of the two: F1\u2082 = $\\frac{2 \\cdot \\text{Precision}_z \\cdot \\text{Recall}_z}{\\text{Precision}_z + \\text{Recall}_z}$ .\nWe computed these metrics for each layer and averaged them to obtain the mean precision,\nrecall, and F1-scores across all layers: Mean Precision = $\\frac{1}{n} \\sum_{z=1}^n \\text{Precision}_z$, Mean Recall = $\\frac{1}{n} \\sum_{z=1}^n \\text{Recall}_z$, and Mean F1 = $\\frac{1}{n} \\sum_{z=1}^n \\text{F1}_z$, where n is the total number of layers. We also\ncalculated the standard deviation of these metrics across layers to assess performance variability. To\naddress the research questions posed in Sec. 1, we evaluate and discuss the impact of GAN-generated\ndata (Tr-1, Tr-2) and improvements in segmentation and defect detection tasks on different fine-tuning\nvariants of SAM and U-Net. For the rest of the paper, we use \"experimental/real\" and \"synthetic/GAN\"\ninterchangeably."}, {"title": "4.4 Out-of-distribution (OoD) Performance", "content": "For research questions, (R1) - (R2), we analyzed the performance of the baseline and fine-tuned\nvariants of SAM on the Out-of-distribution (OoD) datasets Te-1-Te-5 (detailed in Fig. 2). Fig. 4(a)-(c)\nshows the IoU performance of SAM on different training sets and the baseline for InD and OoD\ntest datasets (Te-1-Te-5) for the material, pore, and inclusion classes. We observed that SAM-GAN\nachieved higher IoU for InD (Te-3) for the material and pore classes, but consistently shows lower\nIoU for OoD compared to the baseline. This can be explained by comparing the FID score of test\nOoD data as mentioned in Fig. 2(c). SAM-GAN tends to show lower IoU for OoD data with high FID\nscores relative to the training data. For inclusions, comparing with Fig. 2(d), SAM-GAN performs\nbetter on OoD datasets with a high volume of inclusions (Te-1) and shows lower IoU for OoD datasets\nwith a low volume of inclusions. Fig. 4(d)-(e) shows the performance of SAM-GAN in terms of mean\nF1-score on test datasets for the pore and inclusion classes. Here, SAM-GAN performs comparably\nto the baseline for InD (Te-3) and OoD datasets with no inclusions (Te-4, Te-5), but yields lower\nF1-scores for OoD datasets with high FID scores (Te-1, Te-2). Similarly, SAM-GAN achieves better\nF1-scores for inclusions on OoD datasets with a high volume of inclusions and lower F1-scores for\nOoD datasets with low inclusion volumes. Fig. 5 shows an example of SAM-GAN predictions across\nall test datasets (Te-1-Te-5). SAM-GAN fails to capture the material in smoother regions of test data\nwith high FID (Te-1 and Te-4).\n(R3) Robustness to Noise: To analyze how finetuning variants of SAM responds to noise, we choose\nto analyze the performance on a very different training noise distribution, Te-6 (high FID w.r.t\nboth training Tr-1 and Tr-2). Figure 6a shows IoU of the models for material, pore, and inclusion.\nWe observe, SAM-GAN under-performs for all three classes. However, SAM-FineReal-Sub after\nrefinetuning on a small real data increase IoU to almost 85% for material, 14% for pore."}, {"title": "5 Discussion", "content": "To summarize our observations, as per research questions, for (R1), we observe SAM-GAN has\nsignificantly improved 11.97% IoU and 33.45% F1 compared to baseline (2.5D U-Net trained with\nsame GAN-generated data) on all test OoD data across all three classes. For (R2) and (R3), we\nsee training on GAN generated data can significantly improve the performance for in-distribution.\nHowever, SAM-GAN yields high performance (in terms of popular metrics) on OoD with low FID\nscore and low performance on OoD with high FID (Fr\u00e9chet inception distance) w.r.t training data.\nFor noisy OoD (Te-6), SAM-GAN struggles to distinguish material class even on smoother region.\nWhereas, SAM-FineReal-Sub and SAM-FineReal with re-finetuning can improve the performance on\nOoD where SAM-GAN struggles. For (R3), although SAM-FineReal-Sub improves performance for\nOoD than SAM-GAN, it again decrease on in-distribution, where SAM-GAN performance are high.\nHence, results in catastrophic forgetting.\nChallenges: Finetuning SAM on material XCT also pose some challenges. (i) Without a parameter\nefficient technique, training is computationally expensive and takes longer iteration to converge. (ii)\nThe lightweight mask-decoder for PEFT-SAM can adapt for multiclass segmentation by adding a\nclassification layer. However, our observation shows, this cannot still adapt for manufacturing data\nto distinguish different classes in one image. Hence, we finetune as binary segmentation for each\nclass separately. (iv) Refinetuning fails to predict many layers on 3D slices for OoD, although it can\ndistinguish the materials well on 2D images. We further need to investigate the reason behind missing\nslices for SAM-FineReal and SAM-FineReal-Sub.\nConclusion: In this work, we investigated the application of the Segment Anything Model (SAM) for\nthe segmentation of flaws and inclusions in industrial X-ray computed tomography (XCT) data. By\nleveraging GAN-generated data and parameter-efficient fine-tuning techniques like Conv-LoRa, we\ndemonstrated that SAM, when fine-tuned, significantly improves segmentation performance compared\nto the 2.5D U-Net baseline on in-distribution (InD) data. While SAM-GAN excelled in detecting\ninclusions and flaws in lower-noise out-of-distribution (OoD) data, it struggles with higher-noise\nOoD datasets. However, re-fine-tuning SAM (SAM-FineReal-Sub and SAM-FineReal ) improved\nperformance in more challenging scenarios, although at the cost of some accuracy on InD data due to\ncatastrophic forgetting. Future work will focus on addressing the challenge of catastrophic forgetting\nduring refinetuning, especially for tasks involving both InD and OoD data. We plan to explore more\nadvanced finetuning strategies and loss functions that improve generalization to noisy data and allows\nfor few/zero-shot learning. Moreover, future efforts will involve enhancing SAM's ability to handle\nmulti-class segmentation of defects, such as flaws and inclusions, in complex XCT data, considering\n3D nature of the data."}, {"title": "7 Appendix", "content": "SAM-GAN Finetuning Settings: We finetune on 4 GPUs of 80GB memory for finetuning. Learning\nrate set to .0003 and batch size was set to 4. Each training ran for 20 epochs. Most finetuning\nconverge by 15 iterations. Each training finishes within 4 hours."}, {"title": "7.1 Additional Results of OoD", "content": ""}]}