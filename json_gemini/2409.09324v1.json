{"title": "Efficient Fine-Tuning of Large Language Models for Automated Medical Documentation", "authors": ["Hui Yi, Leong", "Yi Fan, Gao", "Shuai, Ji", "Uktu Pamuksuz"], "abstract": "Scientific research indicates that for every hour spent in direct patient care, physicians spend nearly two additional hours on administrative tasks, particularly on electronic health records (EHRs) and desk work. This excessive administrative burden not only reduces the time available for patient care but also contributes to physician burnout and inefficiencies in healthcare delivery. To address these challenges, this study introduces MediGen, a fine-tuned large language model (LLM) designed to automate the generation of medical reports from medical dialogues. By leveraging state-of-the-art methodologies for fine-tuning open-source pretrained models, including LLaMA3-8B, MediGen achieves high accuracy in transcribing and summarizing clinical interactions. The fine-tuned LLaMA3-8B model demonstrated promising results, achieving a ROUGE score of 58% and a BERTScore-F1 of 72%, indicating its effectiveness in generating accurate and clinically relevant medical reports. These findings suggest that MediGen has the potential to significantly reduce the administrative workload on physicians, improving both healthcare efficiency and physician well-being.", "sections": [{"title": "I. INTRODUCTION", "content": "The integration of Artificial Intelligence (AI) into healthcare has led to significant advancements in areas such as disease diagnosis, treatment planning, and medical reporting [1]. In particular, Large Language Models (LLMs) have demonstrated the potential to transform medical documentation by automating the generation of clinical notes. However, despite the progress made in electronic health records (EHRs) systems, clinicians still face the challenge of excessive administrative burdens, which can reduce the time available for patient care and contribute to physician burnout. Studies suggest that for every hour of clinical work, physicians spend nearly two additional hours managing EHRs and other administrative tasks. This clerical burden contributes to an increased risk of professional burnout [2]. This imbalance hampers the efficiency of healthcare delivery and impacts both patient outcomes and physician well-being.\nTo address these challenges, this study proposes a novel approach to automating the generation of clinical reports from medical dialogues. By leveraging a fine-tuned Large Language Model, the goal is to reduce the time physicians spend on documentation, allowing them to focus more on patient care."}, {"title": "II. RELATED WORK", "content": "The increasing administrative burden associated with medical documentation has been a growing concern in healthcare. Several studies have explored the use of Artificial Intelligence (AI) and Natural Language Processing (NLP) to automate clinical note generation, aiming to alleviate this workload. The emergence of Large Language Models (LLMs) has significantly advanced this field, providing new methods for summarizing medical dialogues and automating the documentation process.\nOne of the earliest approaches to automating clinical note generation involved the use of Recurrent Neural Networks (RNNs), which were effective in handling sequential data such as conversations. These models predict subsequent words based on the preceding context, making them suitable for speech recognition tasks. However, RNNs had significant limitations, particularly their inability to handle long-range dependencies effectively. This shortcoming often led to incomplete or inaccurate summaries in longer dialogues, reducing their utility in real-world medical settings.\nThe development of transformer-based models, particularly those utilizing attention mechanisms, revolutionized NLP by allowing models to weigh the importance of words regardless of their position in a sequence. Transformers, such as BERT and GPT, dramatically improved the accuracy of medical report generation. However, high computational cost and memory requirements present significant barriers to widespread implementation, especially in resource-limited healthcare environments [3].\nSeveral works have focused on leveraging LLMs for medical report generation. For instance, Yim et al. developed the ACI-BENCH dataset to benchmark systems for clinical note generation from doctor-patient dialogues [4]. While this dataset has provided a valuable resource for evaluating model performance, it is limited in scope due to its relatively small size and lack of diversity in clinical scenarios. This limitation restricts models' ability to generalize across various medical contexts and patient populations, potentially leading to inaccuracies in less represented cases."}, {"title": "III. METHODOLOGY", "content": "The Figure 1 describes the training pipeline for the MediGen model, including dataset preparation, data preprocessing, model selection, and fine-tuning techniques. The evaluation results will be discussed later."}, {"title": "A. Dataset", "content": "To train and fine-tune our model, we used a combination of publicly available medical dialogue datasets, including the ACI-BENCH dataset [4]. ACI-BENCH contains 207 doctor-patient role-played dialogue-note pairs, with each dialogue averaging 1,302 tokens and corresponding SOAP notes averaging 490 tokens. The dataset encompasses diverse clinical scenarios, including virtual assistant calls and real-life doctor-patient interactions. This variety in data allows for a broader training base and enhances the model's ability to generalize.\nThe dataset was split into three subsets: 67 dialogues for training, 20 for validation, and 120 for testing (divided into three test sets). This structured split allows us to assess the model's performance on unseen data and measure its generalizability."}, {"title": "B. Data Preprocessing", "content": "Preprocessing was a crucial step to ensure the quality and consistency of the input data. The dialogues were cleaned by removing irrelevant or repeated words, correcting punctuation, and tokenizing the text using an instruction-tuning format. Each dialogue was paired with its corresponding SOAP note and segmented into input-output pairs for training.\nWe employed the following steps during preprocessing:\nText normalization: All text data was standardized by removing special characters and ensuring consistency in medical terminology.\nTokenization: The dialogues and notes were tokenized using a sentencepiece tokenizer pre-trained on medical texts.\nSpeaker identification: We identified speaker roles (doctor or patient) to ensure that the model could correctly distinguish between medical advice and patient symptoms or queries.."}, {"title": "C. Model Selection", "content": "Given the limitations of RNNs in handling long-range dependencies, we opted for transformer-based models that utilize attention mechanisms to retain critical information across lengthy medical dialogues. While transformer models like BERT and GPT are effective, they come with significant computational and memory requirements, making them less practical for resource-constrained healthcare settings [3]. This is especially important in medical dialogue summarization, where the model must efficiently process long conversations and scale effectively.\nWe selected LLaMA3-8B [5] as it balances performance and resource efficiency. It is more resource-friendly and easier to fine-tune using advanced techniques like QLoRA [6] and PEFT [7], which reduce memory and computational overhead without sacrificing performance. These features make LLaMA3-8B ideal for healthcare, especially in handling long medical conversations. Additionally, we included GEMMA-7B [8] and Mistral-7B [9] for ablation studies to assess how different architectures impact summarization performance, providing valuable insights for optimizing models for healthcare tasks."}, {"title": "D. Fine-Tuning Techniques", "content": "To optimize MediGen's performance while addressing computational constraints, we employed three advanced fine-tuning strategies: Quantized Low-Rank Adaptation (QLORA) [5], Parameter-Efficient Fine-Tuning (PEFT) [6], and Instruction Tuning. These techniques were selected to enhance the model's ability to generate accurate and detailed SOAP notes from medical dialogues while reducing memory usage and computational load.\na) Quantized Low-Rank Adaptation (QLoRA): We selected QLORA to reduce memory consumption by quantizing model parameters to 4 bits and applying low-rank adaptations. This allows efficient fine-tuning with fewer resources, crucial for real-world healthcare settings where computational power may be limited, without sacrificing the performance of the original model.\nb) Parameter Efficient Fine-Tuning (PEFT): PEFT was chosen because it allows the fine-tuning of only a small subset of the model's parameters, particularly those most critical for the task. This significantly reduces the number of trainable parameters, making the process more computationally efficient while maintaining accuracy an essential consideration for scalable healthcare applications.\nc) Instruction Tuning Method: Transformer models often struggle with maintaining nuanced details in medical conversations, which can lead to overly generic summaries that miss vital information. To address this, we employed instruction tuning, a technique that explicitly directs the model to organize information into specific SOAP note sections (e.g., Subjective, Objective, Assessment, Plan). This method ensures that the model can generate coherent, accurate, and actionable medical summaries by following structured input-output pairs.\nd) Adjust hyperparameters: Hyperparameters were adjusted based on iterative evaluations with healthcare providers. By involving real users in the evaluation process, we continuously refined the model's performance to ensure accuracy and prevent overfitting."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "We used a combination of quantitative metrics (ROUGE, BERTScore, and BLEURT) and qualitative assessments to evaluate the performance of MediGen. The quantitative metrics focused on the accuracy, relevance, and coherence of the generated medical reports, while the qualitative assessments involved expert reviews by medical professionals. The fine-tuned MediGen model, based on LLaMA3-8B, was evaluated using the ACI-BENCH dataset through three test sets, and the results were averaged to ensure consistency across diverse doctor-patient dialogues."}, {"title": "B. Ablation Studies", "content": "To gain a deeper understanding of the contributions of model architecture and fine-tuning techniques, an ablation study was conducted to assess the impact of model selection and instruction tuning. Specifically, we fine-tuned similar models like Mistral-7B [8], Gemma-7B [7], and Phi-3-mini-4k-instru-ft using consistent evaluation settings to compare their performance against LLaMA3-8b-ft, aiming to determine the reason behind LLaMA3's superior performance."}, {"title": "V. CONCLUSIONS", "content": "In this study, we introduced MediGen, a fine-tuned large language model designed to automate the generation of medical reports from doctor-patient dialogues, thereby addressing the administrative burden on healthcare professionals. By utilizing state-of-the-art models like LLaMA3-8B, GEMMA-7B, and Mistral-7B, combined with advanced fine-tuning techniques such as Quantized Low-Rank Adaptation (QLORA) and Parameter-Efficient Fine-Tuning (PEFT), we demonstrated significant improvements in the accuracy, structural coherence, and semantic relevance of generated medical reports.\nOur results, validated through metrics like ROUGE and BERTScore, confirmed the superior performance of fine-tuned models compared to baseline approaches. Furthermore, the ablation study underscored the importance of instruction tuning in enhancing both the accuracy and completeness of the generated reports.\nWhile our model showed promising results in reducing the time healthcare professionals spend on documentation, challenges remain. Future work will explore integrating more advanced retrieval techniques like Retrieval-Augmented Generation (RAG) to further enhance the model's performance in diverse clinical settings. The application of MediGen has the potential to significantly improve healthcare workflows, reduce physician burnout, and enhance overall efficiency in clinical environments."}]}