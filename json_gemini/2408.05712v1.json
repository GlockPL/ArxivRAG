{"title": "DeepAir: A Multi-Agent Deep Reinforcement Learning Based Scheme for an Unknown User Location Problem", "authors": ["Baris Yamansavascilar", "Atay Ozgovde", "Cem Ersoy"], "abstract": "The deployment of unmanned aerial vehicles (UAVs) in many different settings has provided various solutions and strategies for networking paradigms. Therefore, it reduces the complexity of the developments for the existing problems, which otherwise require more sophisticated approaches. One of those existing problems is the unknown user locations in an infrastructure-less environment in which users cannot connect to any communication device or computation-providing server, which is essential to task offloading in order to achieve the required quality of service (QoS). Therefore, in this study, we investigate this problem thoroughly and propose a novel deep reinforcement learning (DRL) based scheme, DeepAir. DeepAir considers all of the necessary steps including sensing, localization, resource allocation, and multi-access edge computing (MEC) to achieve QoS requirements for the offloaded tasks without violating the maximum tolerable delay. To this end, we use two types of UAVs including detector UAVs, and serving UAVs. We utilize detector UAVs as DRL agents which ensure sensing, localization, and resource allocation. On the other hand, we utilize serving UAVs to provide MEC features. Our experiments show that DeepAir provides a high task success rate by deploying fewer detector UAVs in the environment, which includes different numbers of users and user attraction points, compared to benchmark methods.", "sections": [{"title": "I. INTRODUCTION", "content": "The widespread utilization of cloud computing after nearly two decades has brought about many opportunities for both companies and end-users that benefit from task offloading, content caching, and resource allocation. Especially through- out its computational advantages, cloud computing has pro- vided computing capacity, reliability, and robustness for the offloaded tasks which otherwise may not be solved on the user devices [1], [2]. However, even though it provides important opportunities, many other computing paradigms including Cloudlet [3], Edge Computing [4], [5], and Fog Computing [6] emerged in the last decade since cloud computing cannot meet the low latency requirements of novel user applications due to the wide area network delay (WAN) [7].\nAmong those emerged edge solutions, multi-access edge computing (MEC) [8] has become an extensively used tech- nology since it provides low latency and computation power for intensive tasks. Therefore, it is deployed for different application types including healthcare, video analytics, smart home, and virtual reality [9], [10]. Nevertheless, the fixed infrastructure of MEC prevents its utilization for dynamic scenarios in which the number of users/requests increases suddenly because of an event or a disaster. For example, a sport or concert event, in which there are thousands of new users, can exceed the capacity of the existing MEC infrastructure and therefore the quality of service (QoS) of users can be heavily affected as the corresponding tasks cannot be executed properly.\nTo meet the dynamic capacity requirements, UAVs have re- cently been deployed along with other air vehicles in different air platforms under varied names such as aerial radio access network (ARAN), and air computing [1], [11]. An air comput- ing environment is depicted in Figure 1. The communication between those different air platforms throughout the corre- sponding air vehicles brings about new research opportunities to meet those requirements considering the application QoS and user Quality of Experience (QoE) [12]. Thus, different application profiles can benefit from various advantages of this new 3D dynamic computing paradigm.\nAmong those different air vehicles, UAVs are the most studied units since their deployment is easier considering their energy consumption, flying altitude, and configuration [13]. To this end, they are used for dynamic capacity enhancement in environments in which the fixed capacity would not be sufficient to meet the application requirements of an increasing number of users. Therefore, this feature solves variety of prob- lems such as communication in a disaster site, and enhancing services in infrastructure-less environments [14], [15]. More- over, their deployment provides significant vertical networking opportunities such as high mobility support, coverage, latency, and two-way task offloading [16]. As a result, the requirements of users living in urban, suburban, and rural areas can be met efficiently through these vertical networking opportunities.\nThere are many studies in which UAVs are used as flying computational units to assist either deployed edge servers or are solely deployed to enhance network capacity for task offloading [15], [17]. Since the battery and CPU capacity of the end users would not be sufficient to process the corresponding tasks, UAVs therefore can be used as a flying edge server. However, since there are many different scenarios, various methods and algorithms are developed to meet the service requirements. Deep Reinforcement Learning (DRL) is one of those methods that is applied in the literature since the traditional heuristic methods and convex optimization cannot solve the corresponding dynamic problems [18]. To this end, DRL solutions would be used for trajectory optimization, energy-efficient offloading, UAV placement, and generic task offloading."}, {"title": "A. Motivations", "content": "User connectivity is a primary issue in accessing the related resources for task offloading and service differentiation. How- ever, in order to provide a required service, the corresponding technology such as edge or UAV should first detect the user, and then the connection should be established. However, in an environment which is infrastructure-less and user locations are unknown, providing those services is a crucial technical challange.\nEven though UAVs are used in many different cases, their utilization in an infrastructure-less environment in which users cannot connect to any cellular operator, edge/cloud server, or satellite has not been investigated properly. That environment can be a disaster site, wilderness, or a natural area that is open to visitors. In such an environment, the detection of user locations, localization, and then the measurement of required capacity for user tasks are major issues to ensure the necessary service. Therefore, in this study, we focus on an environment in which there are users at unknown locations where we locate them through a novel method using DRL. Afterwards, as the second step, we estimate the corresponding requirements of the connected users at detected locations and decide how many UAVs are needed in those corresponding areas.\nOn the other hand, as detailed by Bai et al. [19], DRL- based UAV studies have four main categories, and most of the studies in the literature focus on a subset of those cate- gories. Therefore, in addition to the challenging environment, providing a solution for each of those categories is our overall goal."}, {"title": "B. Contributions", "content": "In this study, we develop a DRL-based scheme, DeepAir, which takes unconnected users' emitted RSSI signals into account as a reward and then finds the corresponding user attraction points in the environment. Since the number of user attraction points could not be known, DeepAir iteratively detects those points. Afterwards, based on the quality of those detected locations, users that are in the coverage can connect to agents, which we also call detector UAVs. After these phases, the corresponding user task profiles are extracted through the requests of connected users. Next, the required capacity based on those task profiles is computed, and then the necessary number of serving UAVs, which have MEC features, is measured for the detected user-concentrated areas. Thus, we increase the task success rate based on their Service Level Agreement (SLA) requirements. The main contributions of our paper are as follows.\n\u2022 We investigate the case of providing service to end-users in an infrastructure-less environment considering their QoS requirements which is not thoroughly investigated before. In this environment, users cannot connect to any operator or server, therefore their locations are not known.\n\u2022 In DRL-based UAV studies, a subset of four categories, including (1) sensing, (2) localization, (3) resource allo- cation, and (4) UAV-assisted MEC, are mainly considered in most of the cases [19]. In this study, we take all of these categories into account using our novel approach.\n\u2022 In order to provide such a system, we develop a DRL- based multi-agent scheme, DeepAir, which is a novel approach that iteratively detects user locations in the environment using unconnected users' additive RSSI as the reward. DeepAir performs this operation iteratively, sending a single agent (detector UAV) to the environment for each iteration, since the number of user concentrated areas cannot be known.\nThe rest of this paper is organized as follows. In Section II, we elaborate on the related works including task offloading, UAVs, and DRL. We provide the system model and prob- lem formulation in Section III. In Section IV, we introduce DeepAir providing technical and theoretic discussions. We show the experimental results in Section V. We discussed our observations through experiments in Section VI. Finally, we conclude our study in Section VII."}, {"title": "II. RELATED WORKS", "content": "We surveyed the related research papers considering our DeepAir implementation and scenario in which user locations are not known and UAVs are primarily used for sensing, and localization. We conducted our research considering various high-impact journals and conferences."}, {"title": "III. SYSTEM MODEL AND PROBLEM FORMULATION", "content": "We consider an environment as a set of users denoted as $M = \\{1,2,..., M\\}$, a set of serving UAVs represented as $N_s = \\{1,2,..., N_s\\}$, and a set of detector UAVs specified as $N_d = \\{1,2, ..., N_d\\}$. Detector UAVs are used for sensing and localization of users in the environment whose locations are unknown. Serving UAVs, on the other hand, are used as a computational resource for offloaded tasks of users. In other words, serving UAVs are flying edge servers in the environment. Each UAV type has a horizontal radius, $r$, for communicational or computational coverage. Each user $m \\in M$ randomly produces a computation-intensive task $W_m = (D_m,C_m, A_m, T_m^{max})$, where $D_m$ is the size of the task as bits, $C_m$ is the required CPU cycle for processing as cycle/bit, $A_m$ is the arrival rate as task/sec, and $T_m^{max}$ is the maximum tolerable latency as seconds. For convenience to read, the list of main notations used in formulations is given in Table III.\nIn the environment, the location of a UAV $n \\in N$, or $n\\in N_a$ can be denoted as $u_n(t) = [x_n(t), y_n(t), z_n(t)]$, where $x_n(t)$, $y_n(t)$, and $z_n(t)$ are the X, Y, Z coordinates at time time t. Therefore, the position of UAV n at the next time step for a horizontal flight can be expressed as\n$x_n(t + 1) = x_n(t) + d_n(t) \\times cos(v_n(t))$ (1)\n$y_n(t + 1) = y_n(t) + d_n(t) \\times sin(v_n(t))$ (2)\nwhere $d_n(t)$ is the flight distance, and $v_n(t) \\in [0,2\\pi]$ is the flight angle. Moreover, the following movement constraints should be satisfied during the horizontal flight considering the area of the environment\n$0 \\leq x_n(t) \\leq X_{max}$ (3)\n$0 \\leq y_n(t) < Y_{max}$ (4)\nwhere $X_{max}$ and $Y_{max}$ are the maximum lengths of the environment. Similarly, to avoid collision between any two UAVs, including serving and detector UAVs, minimum dis- tance should be satisfied as follows\n$||u_i(t) \u2013 u_j(t)|| \\geq d_{min} \\forall i, j, i \\neq j$ (5)\nwhere $d_{min}$ denotes the minimum distance. On the other hand, location of a user $m \\in M$ at time t is represented as $L_m(t) = [x_m(t), y_m(t), 0]$ where $x_n(t)$, and $y_n(t)$ are the X, and Y coordinates, respectively. Since users are in the ground, the Z coordinates are zero.\nIf a user $m \\in M$ processes its task locally, the correspond- ing local computation delay is measured as follows\n$T_m^{loc} = \\frac{D_m \\times C_m}{f_m}$ (6)\nwhere $f_m$ is the computational capacity of user m as CPU cycle per second. On the other hand, if the task $W_m$ is offloaded to a serving UAV $n_s \\in N_s$, the computation delay at $n_s$ is measured as\n$T_{n_s}^{UAV} = \\frac{D_m \\times C_m}{f_{n_s}}$ (7)\nwhere $f_{n_s}$ is the computational capacity of UAV $n_s$ as CPU cycles per second. Since multiple users can be connected and therefore offload their tasks to a serving UAV, M/M/1 queueing model is used for the overall delay measurement at the corresponding serving UAV as\n$T_t = \\frac{\\sum_{m \\in M_{sub}} (D_m \\times C_m)}{f_{n_s} - \\sum_{m \\in M_{sub}} (A_m \\times D_m \\times C_m)}$ (8)\nwhere $M_{sub}$ denotes a subset of users concentrated in the corresponding area. Therefore, queueing delay at the serving UAV is measured as\n$T_{n_s}^q = T_t - T_{n_s}^{UAV}$ (9)\nConsidering the task offloading case for task $W_m$, trans- mission delay between a user $m \\in M$ and a serving UAV $n_s \\in N_s$ is measured as\n$T_{m,n_s} = \\frac{D_m}{U_{m,n_s}}$ (10)\nwhere $U_{m,n_s}$ is the data rate between m and ns as bit/sec. It is important to note that users connect and communicate multi- ple serving UAVs via orthogonal frequency-division multiple access (OFDMA). Therefore, the transmission interference between different users can be ignored.\nChannel gain, which indicates to the measurement of the strength of the signal between the transmitter and receiver in wireless communication, is computed between a user m and a detector UAV $n_a \\in N_a$ using free-space path loss model as\n$h_{m,n_a}(t) = \\frac{g_{m,n_a}}{|d_{m,n_a}(t)|^2}$ (11)\nwhere $g_{m,n_a}$ denotes the channel power gain between the user m and detector UAV na, and $d_{m,n_a}(t)$ is the distance at"}, {"title": "A. Problem Formulation", "content": "In the environment, the total delay for a task of user $m$ is computed as\n$t_{w_m} = t_{network} + t_{service}$ (13)\nwhere $t_{network}$ is the network delay, and $t_{service}$ is the service delay. The network delay is computed as\n$t_{network} = \\begin{cases} T_{m,n_s}, & \\text{if } \\beta_{m,n_s} = 1\\\\ 0, & \\text{if } \\beta_{m,n_s} = 0 \\end{cases}$ (14)\nwhere $\\beta_{m,n_s}$ is a binary variable that indicates whether the task is offloaded to a serving UAV or not. While calculating the network delay, the propagation delay is ignored. The service delay on the other hand is computed as\n$t_{service} = \\begin{cases} T_{n_s}^{UAV}+T_{n_s}^q, & \\text{if } \\beta_{m,n_s} = 1\\\\ T_m^{loc}, & \\text{if } \\beta_{m,n_s} = 0 \\end{cases}$ (15)\nIn our environment, a task of user m, $W_m$, is successfully completed if the total delay is lower than or equal to the maximum tolerable delay of the task. Hence, we define $\\alpha_{\\omega}$ as a success variable for task completion as\n$\\alpha_{\\omega} = \\begin{cases} 1, & \\text{if } t_{w_m} < T_m^{max}\\\\ 0, & \\text{otherwise} \\end{cases}$ (16)\nThus, in this study, our main goal is to maximize the overall task success in the environment. To this end, our objective function is defined as\n$max \\sum_m^M \\sum_n^N \\alpha_{\\omega}$ (17)\nsubject to\n$\\sum_{n_s}^{N_s} \\beta_{m,n_s} \\leq 1 \\forall m \\in M$ (Constraint 1)\n$d_{m,n_s} \\leq r_{n_s} \\forall m \\in M, \\forall n_s \\in N_s$ (Constraint 2)\n$f_{n_s} - (A_m \\times D_m \\times C_m) > 0 \\forall m,n_s$ (Constraint 3)\nEquations (3) - (5) (Constraint 4)\nwhere $d_{m,n_s}$ is the distance between user m and serving UAV $n_s$. Constraint 1 represents that a task can be offloaded to only a single serving UAV even though the user can connect to multiple serving UAVs. Constraint 2 denotes that a user should be in the coverage of a serving UAV in order to connect it and then offload a task to it. Constraint 3 ensures that offloaded tasks cannot exceed the capacity of a serving UAV. Finally, Constraint 4 describes the movement constraints."}, {"title": "IV. DEEPAIR", "content": "Our DeepAir operation has four main phases that should be considered to provide the required QoS for user tasks. As formulated in Section III, a user task is successfully completed if the total delay in the system is smaller than or equal to its maximum tolerable delay. Therefore, each phase, including sensing, localization, resource allocation, and MEC, is significant for the efficient operation in the environment. To this end, we use detector UAVs for the sensing, localization, and resource allocation phases. Note that a detector UAV is also used as a DRL agent in the environment to find the user- concentrated areas. On the other hand, based on the reporting of detector UAVs, we deploy serving UAVs for the offloading and processing of tasks as part of the MEC resources. The whole operation including four phases is depicted in Figure 2. Each type of UAV in the environment can communicate with each other via a separate channel. Thus, they know their existing location. Moreover, they can also communicate with the base, which is at (0,0) coordinates, so that they can notify the existing situation in the corresponding areas in their horizontal coverage. As a result, the system can react to the events in those corresponding areas in real-time."}, {"title": "A. Sensing", "content": "Due to the nature of the infrastructure-less deployment, initially, users in the environment are not connected to any system component, emitting only signals for a possible con- nection. Therefore, as shown in Figure 3a, a detector UAV can sense signal strength at some point in the environment at time t. Based on the location of users and the detector UAV, that signal strength can change in different areas of the environment considering the cumulative signal strength measurements in Equations 11 and 12.\nIn this study, we assume that there are different user attraction points in the environment whose locations are also unknown. Based on those user attraction points as shown in Figure 3a, users are gathered in certain areas. Therefore, the corresponding additive signal strength would be higher when the detector UAV is close to that area. However, considering the fact that there would be many user attraction points whose locations could be in different parts of the environment, and some of those parts may have similar user densities, sensing levels can turn out to have identical or similar values for different points in the environment. Thus, this fact should be taken into account in the localization phase in which we use DRL."}, {"title": "B. Localization", "content": "In the localization phase, information gathered in sensing is initially used for the movement of detector UAVs (agents) and then to locate the corresponding user attraction points. One of the crucial factors here is that the number of user attraction points in the environment is also unknown along with the number of users, and user locations. Therefore, we cannot apply multiple agents simultaneously in the environment since if the number of deployed agents is less than the number of user attraction points then users at some of the attraction points cannot be detected and served. Thus, we apply an iterative approach using DRL as shown in Algorithm 1.\nIn Algorithm 1, we find the user attraction points in the environment. To perform this, we initially set a threshold for new connected users in an iteration. For each iteration we send a DRL agent to the environment flying from the base at (0,0) coordinates, and after the convergence it returns the corresponding location information along with how many new users are connected to it. If the number of connections is smaller than the threshold, the execution of the algorithm is terminated. Otherwise, we continue to send an agent to the environment. Note that when a user device connects to a detector UAV, it stops emitting the connection signal. As a result, the additive signal power would be less in a particular place in the next iteration for the agent."}, {"title": "1) MDP Definition:", "content": "The DRL is based on MDP which is formally defined as a 4-tuple $ < S, A, P, R > $ where\n\u2022 S is the set of states where s \u2208 S\n\u2022 A is the set of actions where a \u2208 A\n\u2022 $P:S\\timesA \\rightarrow P(S)$ is the state transition probability function that provides the probability of $P(s_t, s_{t+1}) = P(s_{t+1} = s' | s_t = s, a_t = a)$. This function denotes that the current state $s_t$ at time t changes to the state"}, {"title": "2) State Space S:", "content": "Based on the Algorithm 1, there is only a single agent in the environment for each iteration. Therefore, the state at time t consists of the current location of the agent as\n$s(t) = u_n(t) = [x_n(t), y_n (t), z_n(t)]$ (18)"}, {"title": "3) Action Space A:", "content": "In our environment, the action space A consists of five discrete actions considering the horizontal movements. Therefore, it is defined as\n$a(t) = [Left, Right, Up, Down, NoMove]$ (19)\nBased on this definition, the horizontal speed of each agent is fixed during their flight. Considering the selected action at time t, they can stay fixed at their horizontal coordinates by NoMove action. Otherwise, they can move into four different areas of the environment."}, {"title": "4) Reward Function R:", "content": "Based on the policy \u3160, the agent takes the corresponding actions in the environment to maxi- mize its cumulative reward. To this end, for a given state st, the agent maximizes the expected sum of future reward by applying policy \u03c0(st) as follows\n$R_t = \\sum_{i=t}^{00} \\gamma * R(S_i, S_{i+1})$ (20)\nwhere $\\gamma \\in [0,1]$ is the discount factor that denotes the importance of the long-term rewards if its value is close to one. Otherwise, its value would be close to zero. Thus, the reward function is defined as follows based on the consideration above\n$R(t) = \\begin{cases} H(t), & \\text{if satisfying constraints}\\\\ 1-1, & \\text{if otherwise} \\end{cases}$ (21)"}, {"title": "5) Application of DRL:", "content": "Since our action space is discrete, applying a value-based DRL algorithm is more convenient in our environment. Therefore, for each DRL agent, we implement Deep Q-Learning (DQN) algorithm [30], which manifests a high success in many different environments with different state spaces.\nIn value-based DRL algorithms such as DQN, the agent should select the best state-action pair among different options through its policy by a given state st by maximizing Q-function, $Q_{\\pi}(S_t, a_t)$. Therefore, under the policy \u3160, the Q- function is defined as\n$Q_{\\pi}(S_t, a_t) = E [R_t | S_i = s, a_i = a]$ (22)\nwhich denotes the value of an action $a_t$ in a state $S_t$. Thus, an optimal policy is defined as selecting the highest valued action in each state\n$\\pi(s_t) = arg \\underset{a'}{max}(Q(s_t, a'))$ (23)\nwhere a' indicates the set of all possible actions.As a result, the value of a state-action pair is computed as\n$Z_t = R_t (S_t, S_{t+1}) + \\gamma * Q(s_{t+1}, arg \\underset{a'}{max} Q(s_{t+1}, a'; \\theta_t); \\theta_t)$ (24)\nwhere $\\theta$ represents the Q-network parameters. On the other hand, considering the convergence through the Q-network, the agent minimize the temporal difference error, $\\delta_t$, of $Q_{\\pi}(S_t, a_t)$ regarding $z_t$:\n$\\delta_t = |Q(S_t, a_t) - Z_t|$ (25)"}, {"title": "C. Resource Allocation", "content": "After the completion of sensing and localization phases through detector UAVs, the next step is the resource allocation for serving UAVs which provide computational serving capa- bilities. Since a serving UAV has a limited capacity, measuring how many of them should be deployed is the main problem in this phase.\nAs users have already connected to the corresponding detector UAVs, and have started to send their task of- floading requests, the system can create a task profile in those user-connected areas by conducting several capacity calculations. To this end, we perform a capacity calculation that includes the task profile of each user as we defined $W_m = (D_m,C_m, A_m, T_m^{max})$. The measurements are essen- tially based on Equation 8 as the delay at serving UAVs is based on M/M/1 queueing model.\nAfter the measurement of the required number of serving UAVs for each detected area, the other important issue is the deployment of available serving UAVs into those areas, each of which may have different task profiles. Note that the available serving UAVs may not meet the total required serving UAVs in the environment. In this case, available serving UAVs are first deployed to the areas which require the lowest $T_m^{max}$. In other words, if an area has a higher need for serving UAVs, that area has a priority. On the other hand, if required numbers of serving UAVs are equal for the corresponding areas, then a round-robin approach is applied for the deployment."}, {"title": "D. MEC", "content": "After the resource allocation, the next and final phase is providing the MEC features to users. To this end, users offload their tasks to serving UAVs and expect a valid service without violating the task's maximum tolerable delay, $T_m^{max}$.\nNote that a user in the corresponding area can connect to multiple serving UAVs simultaneously. To this end, we assume that a user is informed by those serving UAVs via separate channel about the current queueing condition. Hence, a user connected to multiple serving UAVs can select the serving UAV for task offloading regarding minimum delay."}, {"title": "V. PERFORMANCE EVALUATION", "content": "We conducted experiments using a discrete event simulation for the performance evaluation. In these experiments, we have an environment whose size is 500 x 500 m\u00b2. In this environ- ment, there are various number of user attraction points around which the user densities are higher. Note that the location of those users and attraction points are initially not known by the system. The corresponding simulation parameters are given in Table IV. Throughout the experiments, we used Python 3.10. Moreover, we used PyTorch version 2.2.0 for the training of DRL agents.\nIn our experiments, we assumed that each user in the environment produces a task with the parameters $D_m$, $C_m$, $A_m$, and $T_m^{max}$. Moreover, each produced task should be offloaded to one of the serving UAVs since we assumed that the computational capabilities of user devices are not sufficient to meet the task requirements. Similarly, each detector and serving UAV is identical in terms of radius, and altitude. Considering the offloading, we ignored the propagation delay for simplicity. We repeated our experiments 50 times with different seeds. The duration of each experiment in simulator time was 1000 seconds."}, {"title": "A. Training Step", "content": "Regarding Algorithm 1, we train a DRL agent through detector UAVs for each iteration as long as it provides new connections. To this end, we tested several hyperparameters throughout the training step in order to achieve convergence for different user distributions in the environment considering the final model. Generally, we used random search based on our experience in the domain [31].\nThe neural network in our final model consists of three layers each of which includes 128 neurons. Rectified Linear Unit (ReLU) is applied for each neuron as the activation"}, {"title": "B. Competitors", "content": "We used two competitors as benchmark methods namely the Community Flying (CF) and Random methods similar to [25]. In CF, we divided the environment into equal communities, and the center of each community was evaluated as a possible user attraction point. Accordingly, detector UAVs are sent to those centers for possible connections and corresponding QoS measurements. Afterwards, the required number of serving UAVs is deployed based on the needs of those areas. On the other hand, in the random method, the possible attraction centers are selected randomly in the environment based on the available number of detector UAVs. We named the random methods based on the available number of detector UAVs as in the case of CF. To this end, for example, if we use 8 detector UAVs, then the method is named as Random-8."}, {"title": "C. Performance Experiments", "content": "We first evaluate the performance of DeepAir considering the effect of varying numbers of users, and different serving UAVs. As shown in Figure 5, the success rate of DeepAir is quite high based on the successfully placed detector UAVs through DRL. Note that based on the configuration in the experiments, at most six detector UAVs are used when we apply DeepAir. On the other hand, when the number of users increases based on the same number of attraction points, the task success rate for each serving UAV case decreases. This is an expected result since the computational capacity of those serving UAVs would not be sufficient to meet the higher number of tasks produced by each user in the environment. Similarly, a higher number of serving UAVs results in a higher task success rate since they provide more computational capacity.\nPrior to the evaluation of the performance of benchmark methods, we first conducted experiments to observe their success rate using different numbers of detector UAVs. To this end, as shown in Figure 6, we compared 4, 6, 8, and 16 detector UAV cases using 80 users. As expected, using an increased number of detector UAVs provided a higher task success rate since the probability of covered users in the environment is higher in that case. Therefore, we used CF-16 and Random-16 as the benchmark methods in the experiments. The performance of Random-16 and CF-16 methods based on different numbers of users and serving UAVs are shown in"}, {"title": "VI. DISCUSSION", "content": "Considering the fact that the unknown user location problem has not been deeply investigated in the literature, we think that several points should be discussed based on our obser- vations and experiments throughout this study. We first noted about the discrete action space for each type of UAV. Since continuous horizontal actions would complicate the already complex problem regarding DRL, we applied a discrete action space. Moreover, and more importantly, applying a discrete action space alleviates the problem since it turns that into a maze problem in which there are several different prizes (RSSI power) in different sections of the environment. The agent learns to follow those small prizes to reach a bigger prize through episodes. Therefore, the convergence of the agents would be more quickly compared to the case of a continuous action space. As a result we could apply the DQN algorithm, which is a less complex regarding other DRL algorithms such as PPO, and DDPG.\nThroughout our experiments, we also observed that higher RSSI due to a bigger number of users provides more accurate location prediction in DRL. As a result of this, more users can connect to the corresponding detector UAVs. Therefore, if the capacity of serving UAVs is so high, it is not so affected by the number of users, then the task success rate would be larger even though the load is increased. This is an important observation since, otherwise, the corresponding results would be evaluated incorrectly. For this reason, the selection of the capacity of serving UAVs and the required CPU cycles for user tasks are significant for manifesting the accuracy of the experiments."}, {"title": "VII. CONCLUSION", "content": "In this study, we investigated the unknown user location problem in a UAV-assisted environment. The corresponding environment can be a disaster site, wilderness, or a rural area in which user devices cannot connect to any communication device and edge servers because of the lack of infrastructure. Moreover, each user device produces tasks that should be completed regarding their maximum tolerable delay which is not met by the computational capabilities of user devices. Therefore, those tasks should be offloaded to the related computational units. In order to achieve this in such an environment, sensing, localization, resource allocation, and MEC capabilities should be provided together, sequentially. Therefore, we proposed DeepAir, a novel approach which uses DRL iteratively via detector UAVs that are responsible for sensing, localization, and resource allocation. Afterwards, MEC features are provided to those connected users by serving UAVs. Conducted experiments show that DeepAir provides a high task success rate by using a small number of detector UAVs in the environment regarding the benchmark methods. In the future, we plan to take energy consumption into account since energy efficiency is crucial for the movements of the UAVs which would affect the performance. Therefore, we plan to optimize the trade-off between energy consumption and task success rate efficiently."}]}