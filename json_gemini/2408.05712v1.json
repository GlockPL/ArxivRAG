{"title": "DeepAir: A Multi-Agent Deep Reinforcement Learning Based Scheme for an Unknown User Location Problem", "authors": ["Baris Yamansavascilar", "Atay Ozgovde", "Cem Ersoy"], "abstract": "The deployment of unmanned aerial vehicles (UAVs) in many different settings has provided various solutions and strategies for networking paradigms. Therefore, it reduces the complexity of the developments for the existing problems, which otherwise require more sophisticated approaches. One of those existing problems is the unknown user locations in an infrastructure-less environment in which users cannot connect to any communication device or computation-providing server, which is essential to task offloading in order to achieve the required quality of service (QoS). Therefore, in this study, we investigate this problem thoroughly and propose a novel deep reinforcement learning (DRL) based scheme, DeepAir. DeepAir considers all of the necessary steps including sensing, localization, resource allocation, and multi-access edge computing (MEC) to achieve QoS requirements for the offloaded tasks without violating the maximum tolerable delay. To this end, we use two types of UAVs including detector UAVs, and serving UAVs. We utilize detector UAVs as DRL agents which ensure sensing, localization, and resource allocation. On the other hand, we utilize serving UAVs to provide MEC features. Our experiments show that DeepAir provides a high task success rate by deploying fewer detector UAVs in the environment, which includes different numbers of users and user attraction points, compared to benchmark methods.", "sections": [{"title": "I. INTRODUCTION", "content": "The widespread utilization of cloud computing after nearly two decades has brought about many opportunities for both companies and end-users that benefit from task offloading, content caching, and resource allocation. Especially through-out its computational advantages, cloud computing has pro-vided computing capacity, reliability, and robustness for the offloaded tasks which otherwise may not be solved on the user devices [1], [2]. However, even though it provides important opportunities, many other computing paradigms including Cloudlet [3], Edge Computing [4], [5], and Fog Computing [6] emerged in the last decade since cloud computing cannot meet the low latency requirements of novel user applications due to the wide area network delay (WAN) [7]. Among those emerged edge solutions, multi-access edge computing (MEC) [8] has become an extensively used tech-nology since it provides low latency and computation power for intensive tasks. Therefore, it is deployed for different application types including healthcare, video analytics, smart home, and virtual reality [9], [10]. Nevertheless, the fixed infrastructure of MEC prevents its utilization for dynamic scenarios in which the number of users/requests increases suddenly because of an event or a disaster. For example, a sport or concert event, in which there are thousands of new users, can exceed the capacity of the existing MEC infrastructure and therefore the quality of service (QoS) of users can be heavily affected as the corresponding tasks cannot be executed properly. To meet the dynamic capacity requirements, UAVs have re-cently been deployed along with other air vehicles in different air platforms under varied names such as aerial radio access network (ARAN), and air computing [1], [11]. An air comput-ing environment is depicted in Figure 1. The communication between those different air platforms throughout the corre-sponding air vehicles brings about new research opportunities to meet those requirements considering the application QoS and user Quality of Experience (QoE) [12]. Thus, different application profiles can benefit from various advantages of this new 3D dynamic computing paradigm. Among those different air vehicles, UAVs are the most studied units since their deployment is easier considering their energy consumption, flying altitude, and configuration [13]. To this end, they are used for dynamic capacity enhancement in environments in which the fixed capacity would not be sufficient to meet the application requirements of an increasing number of users. Therefore, this feature solves variety of prob-lems such as communication in a disaster site, and enhancing services in infrastructure-less environments [14], [15]. More-over, their deployment provides significant vertical networking opportunities such as high mobility support, coverage, latency, and two-way task offloading [16]. As a result, the requirements of users living in urban, suburban, and rural areas can be met efficiently through these vertical networking opportunities. There are many studies in which UAVs are used as flying computational units to assist either deployed edge servers or are solely deployed to enhance network capacity for task offloading [15], [17]. Since the battery and CPU capacity of the end users would not be sufficient to process the corresponding tasks, UAVs therefore can be used as a flying edge server. However, since there are many different scenarios, various methods and algorithms are developed to meet the service requirements. Deep Reinforcement Learning (DRL) is one of those methods that is applied in the literature since the traditional heuristic methods and convex optimization cannot solve the corresponding dynamic problems [18]. To this end, DRL solutions would be used for trajectory optimization, energy-efficient offloading, UAV placement, and generic task offloading."}, {"title": "II. RELATED WORKS", "content": "We surveyed the related research papers considering our DeepAir implementation and scenario in which user locations are not known and UAVs are primarily used for sensing, and localization. We conducted our research considering various high-impact journals and conferences. In [20], Wang et al. focused on the fairness-related opti-mization of user equipments considering geographical fairness, load, and overall energy consumption. To perform this, they developed a multi-agent deep reinforcement learning based trajectory control algorithm based on the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm. They used average the fairness index and overall energy consump-tion as their success metric. Chang et al. proposed a trajectory design and resource allocation scheme based on multi-UAVS [21]. To this end, they considered the joint user associa-tion, power allocation, and trajectory design to maximize the system utility. They used a multi-agent DRL method which performs centralized learning and decentralized execution. In [22], Zhang et al. investigated user localization through UAV swarms in the case of a disaster-affected ground that has no base station. Their goals were to increase the efficiency of the task and to minimize the energy consumption of the UAVs. They proposed a multi-Agent DRL approach whose initial route is based on the probability distribution map of the users. In [23], Zhang et al. investigated a multi-UAV cooperative reconnaissance and search (MCRS) scheme for the localization of static targets. Therefore, they designed a belief probability map model based on Dempster-Shafer (DS) evidence theory and then proposed a new DRL algorithm called Double Critic Deep Deterministic Policy Gradient (DCDDPG). DCDDPG takes the belief probability map into account and uses the MADDPG approach by utilizing two critic networks. They evaluated their system performance based on decreasing un-certainty, and increasing number of targets found. Chent et al. focused on energy-efficient and dynamic multi-UAV coverage control for disaster areas [24]. They developed a trace pheromone-based mechanism through the MADDPG algorithm in order to provide non-overlapping coverage. Based on reduced overlapping UAVs, they could achieve energy efficiency. They evaluated the performance of their system using average coverage rate, normalized average energy con-sumption, and coverage efficiency. In [25], Ning et al. pro-posed a UAV trajectory optimization scheme considering user-differentiated services. Their goal was to minimize both the computational costs of users and UAVs. They solved the prob-lems, including unknown information about the corresponding services, and UAV trajectory, by using a multi-agent DRL approach. In the end, they measured the overall cost of UAVs and users, respectively, as the performance metric. In [26], Hao et al. developed a DRL-based multi-UAV solu-tion for task offloading problem. Therefore, they investigated a UAV-assisted MEC system considering energy consumption and task delay. Shi et al. focused on the optimization of the UAV trajectories and offloading strategies of users jointly [27]. Hence, they investigated multi-UAV collaboration considering UAV-assisted MEC. To this end, they used a multi-agent DRL approach since the corresponding joint optimization requires non-convex operation. In [28], Zhang et al. investigated UAV-assisted communications using a single UAV and multiple users. To this end, they proposed a proximal policy opti-mization (PPO) based DRL algorithm in order to adjust the direction, altitude, and speed of the UAV. In addition to these adjustments, they took the QoS requirements of the users into account regarding service time minimization. Oubbati et al. proposed a DRL-based multi-UAV optimization method called DISCOUNT in order to utilize UAVs as relays in vehicular ad hoc networks (VANETs) [29]. Accordingly, they considered energy consumption, coverage, and routing performance to cover sparse areas in the network. To the best of our knowledge, the unknown user location case is not deeply investigated by the related studies that utilize UAVs to provide required services in an environment. Thus, we provide a novel DRL-based approach, DeepAir, which locates the users through their RSSI using UAVs iteratively. Afterwards, we compute the QoS requirements of connected users in the detected areas and provide the corresponding serving UAVs. On the other hand, our novel scheme provides sensing, resource allocation, localization, and UAV-assisted MEC features that the related studies ensure only a subset of them. Our main differences between the related studies are given in Table II."}, {"title": "III. SYSTEM MODEL AND PROBLEM FORMULATION", "content": "We consider an environment as a set of users denoted as $M = \\{1,2,..., M\\}$, a set of serving UAVs represented as $N_s = \\{1,2,..., N_s\\}$, and a set of detector UAVs specified as $N_d = \\{1,2, ..., N_d\\}$. Detector UAVs are used for sensing and localization of users in the environment whose locations are unknown. Serving UAVs, on the other hand, are used as a computational resource for offloaded tasks of users. In other words, serving UAVs are flying edge servers in the environment. Each UAV type has a horizontal radius, $r$, for communicational or computational coverage. Each user $m \\in M$ randomly produces a computation-intensive task $W_m = (D_m,C_m, A_m, T_m^{max})$, where $D_m$ is the size of the task as bits, $C_m$ is the required CPU cycle for processing as cycle/bit, $A_m$ is the arrival rate as task/sec, and $T_m^{max}$ is the maximum tolerable latency as seconds. For convenience to read, the list of main notations used in formulations is given in Table III. In the environment, the location of a UAV $n \\in N$, or $n \\in N_a$ can be denoted as $u_n(t) = [x_n(t), y_n(t), z_n(t)]$, where $x_n(t), y_n(t)$, and $z_n(t)$ are the X, Y, Z coordinates at time time t. Therefore, the position of UAV n at the next time step for a horizontal flight can be expressed as\n$x_n(t + 1) = x_n(t) + d_n(t) \\times cos(\\vartheta_n(t))$\n(1)\n$y_n(t + 1) = y_n(t) + d_n(t) \\times sin(\\vartheta_n(t))$\n(2)\nwhere $d_n(t)$ is the flight distance, and $\\vartheta_n(t) \\in [0,2\\pi]$ is the flight angle. Moreover, the following movement constraints should be satisfied during the horizontal flight considering the area of the environment\n$0\\leq x_n(t) \\leq X_{max}$\n(3)\n$0 \\leq y_n(t) < Y_{max}$\n(4)\nwhere $X_{max}$ and $Y_{max}$ are the maximum lengths of the environment. Similarly, to avoid collision between any two UAVs, including serving and detector UAVs, minimum dis-tance should be satisfied as follows\n$||U_i(t) \u2013 u_j(t)|| \u2265 d_{min} \u2200i, j, i \u2260 j$\n(5)\nwhere $d_{min}$ denotes the minimum distance. On the other hand, location of a user $m \u2208 M$ at time t is represented as $L_m(t) = [x_m(t), y_m(t), 0]$ where $x_n(t)$, and $y_n(t)$ are the X, and Y coordinates, respectively. Since users are in the ground, the Z coordinates are zero. If a user $m \u2208 M$ processes its task locally, the correspond-ing local computation delay is measured as follows\n$T_m^{loc} = \\frac{D_m \\times C_m}{f_m}$\n(6)\nwhere $f_m$ is the computational capacity of user $m$ as CPU cycle per second. On the other hand, if the task $W_m$ is offloaded to a serving UAV $n_s \u2208 N_s$, the computation delay at $n_s$ is measured as\n$T_{n_s}^{UAV} = \\frac{D_m \\times C_m}{f_{n_s}}$\n(7)\nwhere $f_{n_s}$ is the computational capacity of UAV $n_s$ as CPU cycles per second. Since multiple users can be connected and therefore offload their tasks to a serving UAV, M/M/1 queueing model is used for the overall delay measurement at the corresponding serving UAV as\n$T_t = \\frac{\\sum_{M_{sub}} (\\lambda_m \\times D_m \\times C_m)}{f_{n_s} - \\sum_{M_{sub}} (A_m \\times D_m \\times C_m)}$\n(8)\nwhere $M_{sub}$ denotes a subset of users concentrated in the corresponding area. Therefore, queueing delay at the serving UAV is measured as\n$T_{q}^{n_s} = T - T_{n_s}^{UAV}$\n(9)\nConsidering the task offloading case for task $W_m$, trans-mission delay between a user $m \u2208 M$ and a serving UAV $n_s \u2208 N_s$ is measured as\n$T_{m,n_s} = \\frac{D_m}{U_{m,n_s}}$\n(10)\nwhere $U_{m,n_s}$ is the data rate between $m$ and $n_s$ as bit/sec. It is important to note that users connect and communicate multi-ple serving UAVs via orthogonal frequency-division multiple access (OFDMA). Therefore, the transmission interference between different users can be ignored. Channel gain, which indicates to the measurement of the strength of the signal between the transmitter and receiver in wireless communication, is computed between a user $m$ and a detector UAV $n_a \u2208 N_a$ using free-space path loss model as\n$h_{m,n_a}(t) = \\frac{g_{m,n_a}}{|d_{m,n_a}(t)|^2}$\n(11)\nwhere $g_{m,n_a}$ denotes the channel power gain between the user m and detector UAV $n_a$, and $d_{m,n_a}(t)$ is the distance at"}, {"title": "A. Problem Formulation", "content": "In the environment, the total delay for a task of user m is computed as\n$t_{w_m} = t_{network} + t_{service}$\n(13)\nwhere $t_{network}$ is the network delay, and $t_{service}$ is the service delay. The network delay is computed as\n$t_{network} = \\begin{cases} T_{m,n_s}, & \\text{if } \\beta_{m,n_s} = 1 \\\\ 0, & \\text{if } \\beta_{m,n_s} = 0 \\end{cases}$\n(14)\nwhere $\\beta_{m,n_s}$ is a binary variable that indicates whether the task is offloaded to a serving UAV or not. While calculating the network delay, the propagation delay is ignored. The service delay on the other hand is computed as\n$t_{service} = \\begin{cases} T_{n_s}^{UAV} + T_q^{n_s}, & \\text{if } \\beta_{m,n_s} = 1 \\\\ T_m^{loc}, & \\text{if } \\beta_{m,n_s} = 0 \\end{cases}$\n(15)\nIn our environment, a task of user m, $W_m$, is successfully completed if the total delay is lower than or equal to the maximum tolerable delay of the task. Hence, we define $\u03b1_\u03c9$ as a success variable for task completion as\n$\\alpha_\u03c9 = \\begin{cases} 1, & \\text{if } t_{w_m} < T_{max} \\\\ 0, & \\text{otherwise} \\end{cases}$\n(16)\nThus, in this study, our main goal is to maximize the overall task success in the environment. To this end, our objective function is defined as\n$\\max_{r} \\sum_{W_m} \\sum_{n_s} \\alpha_\u03c9$\n(17)\nsubject to\n$\\sum_{n_s} \\beta_{m,n_s} \\leq 1 \u2200m \u2208 M$\n(Constraint 1)\n$d_{m,n_s} \u2264 r_{n_s} \u2200m \u2208 M, \u2200n_s \u2208 N_s$\n(Constraint 2)\n$f_{n_s} - (A_m \u00d7 D_m \u00d7 C_m) > 0 \u2200m, n_s$\n(Constraint 3)\nEquations (3) - (5)\n(Constraint 4)"}, {"title": "IV. DEEPAIR", "content": "Our DeepAir operation has four main phases that should be considered to provide the required QoS for user tasks. As formulated in Section III, a user task is successfully completed if the total delay in the system is smaller than or equal to its maximum tolerable delay. Therefore, each phase, including sensing, localization, resource allocation, and MEC, is significant for the efficient operation in the environment. To this end, we use detector UAVs for the sensing, localization, and resource allocation phases. Note that a detector UAV is also used as a DRL agent in the environment to find the user-concentrated areas. On the other hand, based on the reporting of detector UAVs, we deploy serving UAVs for the offloading and processing of tasks as part of the MEC resources. The whole operation including four phases is depicted in Figure 2. Each type of UAV in the environment can communicate with each other via a separate channel. Thus, they know their existing location. Moreover, they can also communicate with the base, which is at (0,0) coordinates, so that they can notify the existing situation in the corresponding areas in their horizontal coverage. As a result, the system can react to the events in those corresponding areas in real-time."}, {"title": "A. Sensing", "content": "Due to the nature of the infrastructure-less deployment, initially, users in the environment are not connected to any system component, emitting only signals for a possible con-nection. Therefore, as shown in Figure 3a, a detector UAV can sense signal strength at some point in the environment at time t. Based on the location of users and the detector UAV, that signal strength can change in different areas of the environment considering the cumulative signal strength measurements in Equations 11 and 12. In this study, we assume that there are different user attraction points in the environment whose locations are also unknown. Based on those user attraction points as shown in Figure 3a, users are gathered in certain areas. Therefore, the corresponding additive signal strength would be higher when the detector UAV is close to that area. However, considering the fact that there would be many user attraction points whose locations could be in different parts of the environment, and some of those parts may have similar user densities, sensing levels can turn out to have identical or similar values for different points in the environment. Thus, this fact should be taken into account in the localization phase in which we use DRL."}, {"title": "B. Localization", "content": "In the localization phase, information gathered in sensing is initially used for the movement of detector UAVs (agents) and then to locate the corresponding user attraction points. One of the crucial factors here is that the number of user attraction points in the environment is also unknown along with the number of users, and user locations. Therefore, we cannot apply multiple agents simultaneously in the environment since if the number of deployed agents is less than the number of user attraction points then users at some of the attraction points cannot be detected and served. Thus, we apply an iterative approach using DRL as shown in Algorithm 1. In Algorithm 1, we find the user attraction points in the environment. To perform this, we initially set a threshold for new connected users in an iteration. For each iteration we send a DRL agent to the environment flying from the base at (0,0) coordinates, and after the convergence it returns the corresponding location information along with how many new users are connected to it. If the number of connections is smaller than the threshold, the execution of the algorithm is terminated. Otherwise, we continue to send an agent to the environment. Note that when a user device connects to a detector UAV, it stops emitting the connection signal. As a result, the additive signal power would be less in a particular place in the next iteration for the agent. This situation is depicted in Figure 3. 1) MDP Definition: The DRL is based on MDP which is formally defined as a 4-tuple < S, A, P, R > where\n\u2022\nS is the set of states where s \u2208 S\n\u2022\nA is the set of actions where a \u2208 A\n\u2022\nP:S\u00d7A \u2192 P(S) is the state transition probability function that provides the probability of P(st, st+1) =\nP(st+1 = s' | st = s, at = a). This function denotes\nthat the current state st at time t changes to the state \u2022\nst+1 by taking the action a.\n\u2022\nR: S\u00d7A\u00d7 S \u2192 R is the reward function. It defines the corresponding reward R at time t by taking an action a\nin a state st. Therefore, it can be also defined as Rt =\nR(st, at, st+1). Our environment provides the corresponding MDP defi-nition through the state representation, action space, reward mechanism, and state transition. Therefore, an applied DRL algorithm in the environment using a detector UAV can create a policy \u03c0 based on a given state as \u03c0(a|s) =\nP (at = a | st = s). Through this policy, the agent can learn the dynamics in the environment for a given state and therefore takes the most effective action.\n2) State Space S: Based on the Algorithm 1, there is only a single agent in the environment for each iteration. Therefore, the state at time t consists of the current location of the agent as\ns(t) = un(t) = [xn(t), yn (t), zn(t)]\n(18)\n3) Action Space A: In our environment, the action space A consists of five discrete actions considering the horizontal movements. Therefore, it is defined as\na(t) = [Left, Right, Up, Down, NoMove]\n(19)\nBased on this definition, the horizontal speed of each agent is fixed during their flight. Considering the selected action at time t, they can stay fixed at their horizontal coordinates by NoMove action. Otherwise, they can move into four different areas of the environment.\n4) Reward Function R: Based on the policy \u03c0, the agent takes the corresponding actions in the environment to maxi-mize its cumulative reward. To this end, for a given state st, the agent maximizes the expected sum of future reward by applying policy \u03c0(st) as follows\nRt = \u2211\u03b3 * R(si, si+1)\ni=t\n(20)\nwhere \u03b3\u2208 [0,1] is the discount factor that denotes the importance of the long-term rewards if its value is close to one. Otherwise, its value would be close to zero. Thus, the reward function is defined as follows based on the consideration above\nR(t) = \\begin{cases} H(t), & \\text{if satisfying constraints} \\\\ 1-l, & \\text{if otherwise} \\end{cases}\n(21)\n5) Application of DRL: Since our action space is discrete, applying a value-based DRL algorithm is more convenient in our environment. Therefore, for each DRL agent, we implement Deep Q-Learning (DQN) algorithm [30], which manifests a high success in many different environments with different state spaces. In value-based DRL algorithms such as DQN, the agent should select the best state-action pair among different options through its policy by a given state st by maximizing Q-function, Q\u03c0(st, at). Therefore, under the policy \u03c0, the Q-function is defined as\nQ\u03c0(st, at) = E [Rt | si = s, a\u1d62 = a]\n(22)"}, {"title": "C. Resource Allocation", "content": "After the completion of sensing and localization phases through detector UAVs, the next step is the resource allocation for serving UAVs which provide computational serving capa-bilities. Since a serving UAV has a limited capacity, measuring how many of them should be deployed is the main problem in this phase. As users have already connected to the corresponding detector UAVs, and have started to send their task of-floading requests, the system can create a task profile in those user-connected areas by conducting several capacity calculations. To this end, we perform a capacity calculation that includes the task profile of each user as we defined Wm = (Dm,Cm, Am, Tmax). The measurements are essen-tially based on Equation 8 as the delay at serving UAVs is based on M/M/1 queueing model. After the measurement of the required number of serving UAVs for each detected area, the other important issue is the deployment of available serving UAVs into those areas, each of which may have different task profiles. Note that the available serving UAVs may not meet the total required serving UAVs in the environment. In this case, available serving UAVs are first deployed to the areas which require the lowest Tmax. In other words, if an area has a higher need for serving UAVs, that area has a priority. On the other hand, if required numbers of serving UAVs are equal for the corresponding areas, then a round-robin approach is applied for the deployment."}, {"title": "D. MEC", "content": "After the resource allocation, the next and final phase is providing the MEC features to users. To this end, users offload their tasks to serving UAVs and expect a valid service without violating the task's maximum tolerable delay, Tmax. Note that a user in the corresponding area can connect to multiple serving UAVs simultaneously. To this end, we assume that a user is informed by those serving UAVs via separate channel about the current queueing condition. Hence, a user connected to multiple serving UAVs can select the serving UAV for task offloading regarding minimum delay."}, {"title": "V. PERFORMANCE EVALUATION", "content": "We conducted experiments using a discrete event simulation for the performance evaluation. In these experiments, we have an environment whose size is 500 x 500 m\u00b2. In this environ-ment, there are various number of user attraction points around which the user densities are higher. Note that the location of those users and attraction points are initially not known by the system. The corresponding simulation parameters are given in Table IV. Throughout the experiments, we used Python 3.10. Moreover, we used PyTorch version 2.2.0 for the training of DRL agents. In our experiments, we assumed that each user in the environment produces a task with the parameters Dm, Cm, Am, and Tmax. Moreover, each produced task should be offloaded to one of the serving UAVs since we assumed that the computational capabilities of user devices are not sufficient to meet the task requirements. Similarly, each detector and serving UAV is identical in terms of radius, and altitude. Considering the offloading, we ignored the propagation delay for simplicity. We repeated our experiments 50 times with different seeds. The duration of each experiment in simulator time was 1000 seconds."}, {"title": "A. Training Step", "content": "Regarding Algorithm 1, we train a DRL agent through detector UAVs for each iteration as long as it provides new connections. To this end, we tested several hyperparameters throughout the training step in order to achieve convergence for different user distributions in the environment considering the final model. Generally, we used random search based on our experience in the domain [31]. The neural network in our final model consists of three layers each of which includes 128 neurons. Rectified Linear Unit (ReLU) is applied for each neuron as the activation function. We used mean squared error (MSE) for the loss, and stochastic gradient descent (SGD) as the optimizer. On the other hand, for the initial exploration, exploration factor, final exploration, and replay memory size, we used well-known values from the literature as given in Table V. Since the learning rate is the most crucial hyperparameter for the performance of the model, we exclusively tested different settings. Thus, we determined 0.005 as the final value for the neural network of the agents. Figure 4 shows the effect of two different learning rates over episodes. As shown in the figure, a lower value of learning rate, such as 0.0005, causes a late convergence in the environment. On the other hand, if we use 0.005 as the learning rate, the model converges earlier which provides time efficiency."}, {"title": "B. Competitors", "content": "We used two competitors as benchmark methods namely the Community Flying (CF) and Random methods similar to [25]. In CF, we divided the environment into equal communities, and the center of each community was evaluated as a possible user attraction point. Accordingly, detector UAVs are sent to those centers for possible connections and corresponding QoS measurements. Afterwards, the required number of serving UAVs is deployed based on the needs of those areas. On the other hand, in the random method, the possible attraction centers are selected randomly in the environment based on the available number of detector UAVs. We named the random methods based on the available number of detector UAVs as in the case of CF. To this end, for example, if we use 8 detector UAVs, then the method is named as Random-8."}, {"title": "C. Performance Experiments", "content": "We first evaluate the performance of DeepAir considering the effect of varying numbers of users, and different serving UAVs. As shown in Figure 5, the success rate of DeepAir is quite high based on the successfully placed detector UAVs through DRL. Note that based on the configuration in the experiments, at most six detector UAVs are used when we apply DeepAir. On the other hand, when the number of users increases based on the same number of attraction points, the task success rate for each serving UAV case decreases. This is an expected result since the computational capacity of those serving UAVs would not be sufficient to meet the higher number of tasks produced by each user in the environment. Similarly, a higher number of serving UAVs results in a higher task success rate since they provide more computational capacity. Prior to the evaluation of the performance of benchmark methods, we first conducted experiments to observe their success rate using different numbers of detector UAVs. To this end, as shown in Figure 6, we compared 4, 6, 8, and 16 detector UAV cases using 80 users. As expected, using an increased number of detector UAVs provided a higher task success rate since the probability of covered users in the environment is higher in that case. Therefore, we used CF-16 and Random-16 as the benchmark methods in the experiments. The performance of Random-16 and CF-16 methods based on different numbers of users and serving UAVs are shown in"}, {"title": "VI. DISCUSSION", "content": "Considering the fact that the unknown user location problem has not been deeply investigated in the literature, we think that several points should be discussed based on our obser-vations and experiments throughout this study. We first noted about the discrete action space for each type of UAV. Since continuous horizontal actions would complicate the already complex problem regarding DRL, we applied a discrete action space. Moreover, and more importantly, applying a discrete action space alleviates the problem since it turns that into a maze problem in which there are several different prizes (RSSI power) in different sections of the environment. The agent learns to follow those small prizes to reach a bigger prize through episodes. Therefore, the convergence of the agents would be more quickly compared to the case of a continuous action space. As a result we could apply the DQN algorithm, which is a less complex regarding other DRL algorithms such as PPO, and DDPG. Throughout our experiments, we also observed that higher RSSI due to a bigger number of users provides more accurate location prediction in DRL. As a result of this, more users can connect to the corresponding detector UAVs. Therefore, if the capacity of serving UAVs is so high, it is not so affected by the number of users, then the task success rate would be larger even though the load is increased. This is an important observation since, otherwise, the corresponding results would be evaluated incorrectly. For this reason, the selection of the capacity of serving UAVs and the required CPU cycles for user tasks are significant for manifesting the accuracy of the experiments."}, {"title": "VII. CONCLUSION", "content": "In this study, we investigated the unknown user location problem in a UAV-assisted environment. The corresponding environment can be a disaster site, wilderness, or a rural area in which user devices cannot connect to any communication device and edge servers because of the lack of infrastructure. Moreover, each user device produces tasks that should be completed regarding their maximum tolerable delay which is not met by the computational capabilities of user devices. Therefore, those tasks should be offloaded to the related computational units. In order to achieve this in such an environment, sensing, localization, resource allocation, and MEC capabilities should be provided together, sequentially. Therefore, we proposed DeepAir, a novel approach which uses DRL iteratively via detector UAVs that are responsible for sensing, localization, and resource allocation. Afterwards, MEC features are provided to those connected users by serving UAVs. Conducted experiments show that DeepAir provides a high task success rate by using a small number of detector UAVs in the environment regarding the benchmark methods. In the future, we plan to take energy consumption into account since energy efficiency is crucial for the movements of the UAVs which would affect the performance. Therefore, we plan to optimize the trade-off between energy consumption and task success rate efficiently."}]}