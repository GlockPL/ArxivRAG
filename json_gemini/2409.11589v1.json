{"title": "ProSLM : A Prolog Synergized Language Model\nfor explainable Domain Specific Knowledge Based\nQuestion Answering", "authors": ["Priyesh Vakharia", "Abigail Kufeldt", "Max Meyers", "Ian Lane", "Leilani H. Gilpin"], "abstract": "Neurosymbolic approaches can add robustness to opaque\nneural systems by incorporating explainable symbolic representations.\nHowever, previous approaches have not used formal logic to contextu-\nalize queries to and validate outputs of large language models (LLMs).\nWe propose ProSLM, a novel neurosymbolic framework, to improve the\nrobustness and reliability of LLMs in question-answering tasks. We pro-\nvide ProSLM with a domain-specific knowledge base, a logical reasoning\nsystem, and an integration to an existing LLM. This framework has two\ncapabilities (1) context gathering: generating explainable and relevant\ncontext for a given query, and (2) validation: confirming and validating\nthe factual accuracy of a statement in accordance with a knowledge base\n(KB). Our work opens a new area of neurosymbolic generative AI text\nvalidation and user personalization.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) are changing the way in which data is used\nand information is disseminated [1]. For example, LLM-driven recommendation\nsystems decide what content we view [5]. Interactive question answering (QA)\nsystems, e.g., OpenAI's ChatGPT, answers questions, or prompts, in natural\nlanguage text [3], leading to the development of \u201cprompt engineering\" [25] to\nguide the LLM on a variety of tasks from questions in medical diagnosis [12] to\ncustomer service [6]. Although LLMs are more than question-answering or QA\nsystems, they exhibit compelling emergent abilities: few-shot prompting capa-\nbilities \u2013 not observed in smaller models [23]. Unsurprisingly, recent state of the\nart LLMs like OpenAI's GPT or Meta's LLaMA models have exhibited incred-\nible performance and generalization across a multitude of different tasks and\ndomains, from telecommunications [22] to clean energy [14] to healthcare [26].\nDespite their demonstrated power and social influence, these models are not\ntrustworthy [21]. LLMs, like many deep neural network, are considered \"black\nbox\" systems; their large number of parameters makes their reasoning uninter-\npretable to humans [18]. Although LLMs can be prompted to self-explain [2],"}, {"title": "Background", "content": "Our work takes inspiration from previous neural and neurosymbolic methods\naimed at improving model explainability."}, {"title": "Hallucination", "content": "Hallucinations in a language model refer to outputs that in some way diverge\nfrom factual truth [15]. A recent example was when two attorneys used Chat-\nGPT to draft a court filing and faced punishment after the model cited entirely\nfictitious court cases. This demonstrates that any trustworthy KBQA system\nneeds validation and sanity checks.\nA new taxonomy of these errors proposed in [7] divides hallucinations into\nfactuality and faithfulness hallucinations. The former encompasses both minor\nfactual inconsistencies and complete fabrications. Faithfulness hallucinations re-\nfer to divergence from the prompt instruction, the context of the query, or logic\nitself prompt, context, and logic inconsistencies, respectively.\nAs for their cause, hallucinations can originate in any part of the language\nmodeling process, from data to the inference step [7] [4]. Still, they can be de-\ntected and sometimes mitigated with the right strategies [19]. When seeking to\nimprove model explainability, then, it is prudent to strive to detect and mitigate\nhallucinations; in our case, ProSLM's symbolic component mitigates hallucina-\ntions by grounding the model with logical, relevant context, and has the poten-\ntial to detect them by determining the truth value of a claim. This is discussed\nfurther in Section 4.3."}, {"title": "Neural Approaches", "content": "To alleviate the inherently uninterpretable nature of LLMs, many noteworthy\nneural approaches have been proposed.\nThe PredictAndExplain method, introduced in [2], illustrates that models\ncan be trained to generate explanations of their outputs without sacrificing per-\nformance, but this approach requires access to a reliable explanation-augmented\ndataset for training.\nInstead of training from scratch, chain of thought (CoT) prompting lever-\nages large pre-trained models by instructing the model to describe the \"chain of\nthought\" it utilized to come to its answer during generation [24]. This method\noutperforms baselines in arithmetic, commonsense, and symbolic reasoning tasks,\nbut requires sufficiently large LLMs in order to be successful.\nRetrieval augmented generation (RAG) architectures, first introduced in [10],\nallows one to retrieve relevant documents from a database and input those texts\nas additional context to a language model. As opposed to CoT, this architecture\ndirectly feeds useful information to the QA model, thus grounding the response,\nreducing factuality hallucinations, and improving explainability of results. How-\never, this approach becomes insufficient when given questions whose answers\ncannot be found in existing documents but rather must be deducted through\nlogical inference and world knowledge."}, {"title": "Neurosymbolic Approaches", "content": "Now we turn to neurosymbolic AI, a re-emergent field of techniques whose pri-\nmary strengths include improved model transparency and explainability.\nOne recent example of this in the visual question-answering (VQA) domain is\nthe Neuro-Symbolic Concept Learner, a framework which jointly learns images,\nwords, and semantic relationships from natural supervision using three modules\na neural perception module, a semantic parser, and a symbolic program executor\n[11]. As they demonstrate in their paper, the use of a DSL specifically designed\nfor VQA to translate natural language queries into executable programs via\nformal semantics is powerful in its ability to generalize to new DSLs, queries,\nand images. In a similar fashion, [13] proposed a framework to incorporate First-\nOrder Logic with LLMs to solve logical reasoning tasks.\nNeither [11] nor [13] pose a viable approach in this current work, however, due\nto the complexity of questions expected for a KBQA system, and our subsequent\ninterest in leveraging the generative power of an LLM to answer inputted queries.\nHowever, we can take inspiration from both approaches' use of an interpretable\nsymbolic translator to generate useful context for the LLM to reason upon."}, {"title": "Preliminaries", "content": "First-Order Logic allows for a language representation using predicates, con-\nstants and functions. Predicate symbols represent relations on certain objects.\nObjects are represented by constant symbols. A term refers to an object and can\neither use a function or not. Given that now we can represent objects and rela-\ntions for those objects, we can now create sentences using logical connectives. We\nuse atomic sentences, made of a predicate and objects, to state facts. An atom\nis simply a fact. friends (priyesh, abigail) represents an atomic sentence\nthat is interpreted as 'priyesh and abigail are friends'. A literal is an atom or its\nnegation. A positive literal is simply an atom. A definite clause is a clause with\nonly one positive literal. For example, if human (priyesh), human(abigail), and\nfriends (priyesh, abigail) are atoms, then:\nfriends (priyesh, abigail) \u2228 \u00ac human(priyesh) V \u00ac human(abigail)\nis a definite clause and it is represented as\nfriends (priyesh, abigail) :- human(priyesh), human(abigail).\nBackward Chaining is used by logical inference algorithms over definite clauses.\nThese algorithms work backwards from the goal, chaining through rules to find\nknown facts that support the proof [16]. A goal is proved, and returns True, if\nthe KB has a rule lhs => goal where the lhs is made of conjuncts, with each\nconjunct an operation on two logical values, and if the lhs can be proved True.\nThe algorithm then makes every conjunct the next goal, essentially running a\nrecursive depth-first search for the first possible rule that returns True."}, {"title": "Method", "content": "As previously discussed, LLMs lack trustworthy explainability [21]. To allevi-\nate this, we introduced ProSLM, a novel neurosymbolic framework. ProSLM is\nproposed as a domain-specific grounded KBQA system that combines the gen-\neralization and reasoning capabilities of an LLM with an explainable context-\ngathering symbolic component. We demonstrate the utility of ProSLM for two\nsituations - explainable context gathering and explainable fact validation."}, {"title": "ProSLM Components", "content": "This framework expands the problem formulation-and-reasoning paradigm by\njoining a neural generator to a symbolic component via a translator to improve\nthe response's explainability, and consequently trustworthiness.\nSymbolic Component Consisting of a domain-specific knowledge base and\nan inference agent, this component's role is to assemble explainable context,\ngiven an input query. Information stored in the knowledge base is represented in\nFirst-Order Logic and written in the form of Prolog rules and facts. This KB is\nalso able to hold real-time information and user-specific preferences, and can be\nupdated independently of the rest of the framework. The inference agent uses\nSWI-Prolog backward chaining to return the truth value conditioned on a Prolog\nquery.\nNeural Generator The neural generator is an LLM. The LLM serves to create\na natural language response conditioned on a user query and context. In this\nwork, we use GPT 3.5.\nNeural Translator We use this translator to facilitate the conversion from\nnatural language (English) prose to Prolog and from Prolog to natural language\nprose. This is used to allow communication between our Neural Generator and\nour Symbolic Component. We use an LLM \u2013 GPT 3.5 \u2013 as our Neural Translator."}, {"title": "Explainable Context Gathering", "content": "In this configuration of ProSLM (Fig 2), the symbolic component determin-\nistically gathers and provides relevant context to the LLM for an explainable\nresponse generation. This setting is most useful where the reasoning behind the\ncontext grounds the LLM response, making it trustworthy. Scenarios such as pro-\nviding answers that could be based on the user's individual preferences, real-time\ninformation about the domain, or even information that does not exist officially\nbut is built from experience are examples. For instance, in a community setting,\na new member might want to use the swimming pool, but does not have much\ninformation about timings, busy hours, etc. A standalone LLM system, such as\nChatGPT, would not have access to the relevant domain context and at best\nmay provide a response such as this:"}, {"title": "Fact Validation", "content": "We can also use the Symbolic Component as a tool to validate LLM-generated\nresponses to fact gathering questions. As seen in Figure 4, an LLM response\ncan be asserted true if each fact in that response is individually asserted true.\nA goal tree complements every fact validation, making the response and its\nvalidation explainable and trustworthy. Note that since backward chaining can\nonly deterministically assert a sentence, a False value should be interpreted as\nmeaning either the sentence is factually false or the KB or the rule base is\nincomplete."}, {"title": "Results", "content": "We demonstrate the capability of ProSLM in a specific domain. Here we focus on\nthe use case of a KBQA system for a university campus, specifically UC Santa\nCruz."}, {"title": "Explainable Context Gathering", "content": "We continue showing scenarios where explainable context gathering helps ground\nthe response and improve its trustworthiness. We compare the output from Chat-\nGPT 3.5 and the output from ProSLM, complete with the reasoning steps. We\nalso report the goal tree from the symbolic component. We return the context\nfrom the Symbolic Component in a variable Y."}, {"title": "Fact Validation", "content": "We also demonstrate ProSLM's flexibility by employing it to improve explain-\nability in another way - fact checking. In these scenarios, one can use the frame-\nwork to determine the truth value of statements or model outputs. We show an\nexample of our system validating factual claims in a GPT-generated response\nagainst our UCSC Knowledge Base."}, {"title": "Discussion and Conclusion", "content": "In this work we introduced ProSLM, a novel neurosymbolic framework for domain-\nspecific KBQA systems, and demonstrated the importance of explainable context\ngathering and fact validation for a variety of scenarios in Section 5 above.\nProSLM is able to provide correct and reasonably sound 'opinions' and 'ver-\nified factual responses' due to either the curated context or the fact validation\nrespectively. The symbolic component in ProSLM produces an explainable chain\nof reasoning to contextualize a query, as is the case in the context gathering task,\nor to fact check a statement, as is the case in fact validation, while leveraging the\ninherently uninterpretable, powerful generative abilities of an LLM. This makes\nthe LLM response, that is conditioned on this context, trustworthy and explain-\nable. As seen in Query 3, by providing context built on the users preferences,\nthis also allows us to personalize the response for a user. This approach holds\nsignificant promise for the field of explainable AI; especially given the fact our\nframework does not need any LLM retraining\nOne clear limitation is having an incomplete KB, which then will make\nProSLM return an inaccurate answer. One way to mitigate this is to manually\nupdate the KB every time, but that's not feasible. Hence, we plan to implement\nuser-interactive updates to the KB. In other words, if the user notices discrep-\nancies in the goal tree created, they will be able to write a natural language text\nthat ProSLM will utilize to update the KB. ProSLM can then be personalized to\nindividual users each person having a local knowledge base that is continually\nupdated with user preferences, new facts, etc. With this added functionality, the\nsystem can be adapted to new domains with ease, and scaled up to handle more\nuse cases within a given domain.\nAnother limitation of ProSLM is that while the goal trees are accurate to the\ninternal workings of the inference engine, they are not currently returned by the"}]}