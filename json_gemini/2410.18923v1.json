{"title": "SEGLLM: MULTI-ROUND REASONING SEGMENTATION", "authors": ["XuDong Wang", "Shaolun Zhang", "Kehan Li", "Yusuke Kato", "Shufan Li", "Konstantinos Kallidromitis", "Kazuki Kozuka", "Trevor Darrell"], "abstract": "We present SegLLM, a novel multi-round interactive reasoning segmentation model that enhances LLM-based segmentation by exploiting conversational memory of both visual and textual outputs. By leveraging a mask-aware multimodal LLM, SegLLM re-integrates previous segmentation results into its input stream, enabling it to reason about complex user intentions and segment objects in relation to previously identified entities, including positional, interactional, and hierarchical relationships, across multiple interactions. This capability allows SegLLM to respond to visual and text queries in a chat-like manner. Evaluated on the newly curated MRSeg benchmark, SegLLM outperforms existing methods in multi-round interactive reasoning segmentation by over 20%. Additionally, we observed that training on multi-round reasoning segmentation data enhances performance on standard single-round referring segmentation and localization tasks, resulting in a 5.5% increase in cIoU for referring expression segmentation and a 4.5% improvement in Acc@0.5 for referring expression localization.", "sections": [{"title": "1 INTRODUCTION", "content": "Image segmentation plays a crucial role in numerous computer vision tasks, while traditional methods have been limited to providing segmentation results for close-set categories (Cheng et al., 2022; He et al., 2017) or simple text queries (Ding et al., 2023; Wang et al., 2024b) using CLIP (Ding et al., 2023; Radford et al., 2021) or BERT (Wang et al., 2024b; Devlin et al., 2018) text embeddings as classifiers. Recent advancements in Large Vision-Language Models (LVMs) (Pi et al., 2023a; Zhang et al., 2023a; Lai et al., 2024; Wu et al., 2024; Liu et al., 2024; Touvron et al., 2023; Alayrac et al., 2022; Awadalla et al., 2023; Dai et al., 2024) have reformulated image segmentation as a next token prediction task, enabling segmentation models to engage in natural language conversations with users and reason about the presence, location, and relationships of objects in complex visual scenes. For instance, LISA (Lai et al., 2024), a Language Instructed Segmentation Assistant, produces segmentation masks by incorporating a [SEG] token into its vocabulary, which, when generated, is decoded into the corresponding segmentation mask.\nThese LLM segmentation models (Lai et al., 2024; Wu et al., 2024; Pi et al., 2023a; Zhang et al., 2023a) typically achieve their localization capabilities by incorporating a decoder that converts the output [SEG] tokens of LLMs into localization results. They are trained on numerous visual queries such as \"please find the heart healthy food in the image\u201d, where responses include both text outputs and segmentation masks. Essentially, these models are advanced versions of early open-vocabulary segmentation models, with their text encoders upgraded from smaller language models, such as BERT (Devlin et al., 2018), to smarter LLMs, such as Llama (Touvron et al., 2023). Consequently, LLM segmentation models are often evaluated on traditional referring expression segmentation (RES) datasets, such as RefCOCO, which provide a single text query corresponding to each mask. These single-round referring expression segmentation (RES) datasets overlook one of the most remarkable properties of LLMs (Achiam et al., 2023; Team et al., 2023; Touvron et al., 2023; Jiang et al., 2023): generating multi-round responses in a conversational manner. In this paper, we intend to answer the question: can segmentation models reason about previously segmented objects and conversations, responding to multiple visual and text queries in a chat-like manner?\nCurrent LLM segmentation or detection models (Lai et al., 2024; Zhang et al., 2023a; Wu et al., 2024), despite their impressive single-round performance, fall short as multi-modal conversation"}, {"title": "2 RELATED WORKS", "content": "2.1 MULTI-MODAL LARGE LANGUAGE MODELS\nTo leverage the advancements in language models (Brown et al., 2020; Touvron et al., 2023; Chowdhery et al., 2023; Le Scao et al., 2023; Hoffmann et al., 2022) across various modalities, Multi-modal Large Language Models (MLLMs) have been developed to combine language and vision (Yin et al., 2023; Liu et al., 2024; Zhu et al., 2023; Alayrac et al., 2022). Flamingo was one of the first unified architectures to align image and text pairs in context learning through gated cross-attention blocks (Alayrac et al., 2022). End-to-end MLLMs typically require a finetuning process where an intermediate network (Lai et al., 2024; Zhang et al., 2023a) and/or sampler module (You et al., 2023) is used to map the vision features into the language space. BLIP-2 bridges the modality gap with a querying transformer and a two-stage training process, which involves pretraining on a trainable LLM and instruction tuning on a frozen one (Li et al., 2023b). Models like MiniGPT-4 (Zhu et al., 2023) and LLava (Liu et al., 2024) follow a similar training paradigm, with Vicuna 18 as a language decoder and GPT-4 designed prompts. Other notable models in instruction tuning include Otter (Li et al., 2023a) that is based on (Awadalla et al., 2023), mPLUG-Owl (Ye et al., 2023) with a novel modular architecture, and InstructBLIP (Dai et al., 2024) which features an instruction aware Q-former.\n2.2 MULTI-ROUND CONVERSATIONAL MLLMS\nRecent advancements in MLLMs have focused on enhancing interactive capabilities. Models like Kosmos-2 (Peng et al., 2023) and Shikra (Chen et al., 2023) use visual grounding and referring to provide the LLM with detailed location information of the objects, which enables the user to point out specific areas in the image. Various works aim to improve local information, such as Ferret (You et al., 2023) and PerceptionGPT (Pi et al., 2023b) which employ flexible continuous representations to handle different shapes. Other approaches (Yang et al., 2023a;b; Zeng et al., 2022) utilize prompt engineering and APIs to facilitate interaction, instead of relying on end-to-end models.\nMore recent approaches introduce the concept of reasoning, leveraging LLMs to provide a visual answer based on implied information. DetGPT (Pi et al., 2023a) performs object detection using high-level instructions rather than distinct classes. GPT4R0I (Zhang et al., 2023b) receives spatial boxes as input to focus on specific regions and better align vision and text. LISA (Lai et al., 2024) adds a new embedding prompt to the mask decoder of the SAM (Kirillov et al., 2023) guiding segmentation, which is then processed by LLaVA (Liu et al., 2024) to perform high-level reasoning. NEXT-Chat (Zhang et al., 2023a) expands on LISA by using embeddings instead of tokens for location information and adding a decoder with a joint loss to facilitate object detection.\nWhile some methods support multi-round conversations, they often lack mechanisms to maintain localization performance over successive rounds, leading to degradation and information loss. SegLLM improves the multi-round interactive segmentation by leveraging the text and segmentation results from previous rounds, thereby generating refined masks and supporting hierarchical representations to enhance performance in multi-round interactions."}, {"title": "3 BACKGROUND: REASONING SEGMENTATION", "content": "Task definition. The reasoning segmentation task (Lai et al., 2024) involves generating binary segmentation masks based on an image and descriptive, free-form text prompts. This task requires the model to possess cross-modality comprehension, understanding both the complex visual scenes, as well as the natural-language signals in the text prompt. Specifically, the model must interpret complex user text prompts that go beyond simple class names to include implicit descriptions that require general world knowledge, such as \u201cthe device that can illuminate a dark room\".\nOverall pipeline. To achieve such capabilities, reasoning segmentation model typically first employs a pre-trained large multimodal models (VLMs), Fmm, which is capable of comprehending both visual and textual information simultaneously (Lai et al., 2024). A new [SEG] token is then added to the"}, {"title": "4 MULTI-ROUND REASONING SEGMENTATION", "content": "The success of our SegLLM method relies on two essential components: a comprehensive dataset MRSeg that has an extensive collection of Multi-Round interactive Segmentation instructions, and a mask-aware VLMs specifically designed to reason about the conversational history, with a particular focus on the segmentation masks generated in previous interactions.\n4.1 DATA PIPELINE\nData sources. We constructed our multi-round image reasoning segmentation dataset (MRSeg) based on several widely utilized datasets, and include data from the following sources: RefCOCO(+/g) (Yu et al., 2016; Kazemzadeh et al., 2014), Visual Genome (Krishna et al., 2017), PACO-LVIS (Ramanathan et al., 2023), LVIS (Gupta et al., 2019), Pascal Panoptic Part (de Geus et al., 2021), ADE20K(Zhou et al., 2017), COCO-Stuff (Caesar et al., 2016) and MSCOCO(Lin et al., 2014b). We used bounding box or segmentation annotations from these datasets to generate natural language conversations, applying a template-based approach as detailed in subsequent sections. The overall pipeline can be seen in Fig. 2 and we provide the statistics and some sample data for MRSeg in Fig. 3.\nMulti-round conversation generation. We design various pipelines for generating multi-round conversations, tailored to the types of data and inter-instance relationships they support:\n\u2022 Hierarchical Relationships (PACO-LVIS, Pascal Panoptic Part): In these queries, the model is tasked with segmenting objects that are sub-parts of previously segmented instances. The queries start by asking about the instance, followed by questions about its parts. Example query: \u201cCan you segment the  of the ?\"\n\u2022 Positional Relationships (RefCOCO(+/g), LVIS): These queries require the model to segment objects based on their positional relationships to previous outputs. An example query is: \u201cCan you segment the  that is  the output from round <i>?\u201d We refine these conversations using GPT-4 (our full prompt to GPT-4 can be found in Table A3) to ensure natural language fluency. Details on the RefCOCO(+/g) pipeline are in Fig. A2. Additionally, we introduce a challenging variant called MRSeg (hard), where understanding previous round information is necessary to correctly segment the current instance (details in Appendix A.1).\n\u2022 Interactional Relationships (Visual Genome): Utilizing Visual Genome (VG) relationship annotations, we construct conversations that focus on interactional dynamics, rather than just positional relationships. Each conversation has two rounds: the first round segments the subject, and the second round segments an object based on its relationship to the subject.\n\u2022 Attribute-oriented Queries (MSCOCO): These queries ask the model to segment objects based on their attributes or usage rather than class names. An example query is: Q: Outline and extract the object that has a tall, slender neck covered with a distinct pattern of patches. A: Yes, the figure you specified for segmentation is a giraffe. We generate captions by cropping MSCOCO instances and using GPT-4V prompts (details in Table A2).\n\u2022 Single-Round Semantic Segmentation is based on ADE20K and COCO-Stuff datasets. We construct single-round conversations by fitting class labels into various query templates.\nAdditional details on the multi-round data pre-processing for MRSeg are provided in Appendix A.1.\nConversation templates. We observed that current state-of-the-art chat-based image segmentation models, such as LISA (Lai et al., 2024), tend to rely heavily on a fixed set of question templates. This leads to fluctuations and instability in segmentation quality when user prompts are phrased differently, suggesting potential overfitting to specific language prompts. To address this, we leveraged the web-version of GPT-4 (Achiam et al., 2023) to generate diverse templates, creating more natural language conversations from dataset annotations. We generated templates for direct referring segmentation queries, relational queries, and hierarchical queries. For each query type, we created 100~200 templates for training and 50~100 different templates for validation.\n4.2 SEGLLM FOR MULTI-ROUND IMAGE REASONING SEGMENTATION\nOverall Pipeline. We introduce SegLLM to ensure that the VLMs's next token predictions can incorporate the conversational memory from previous interactions, including the visual outputs, i.e., segmented masks, and the text conversations. The architecture of our model is illustrated in Fig. 4. SegLLM consists of two key components: 1) Mask-Encoding Module: This module feeds the output masks back into the input stream of the LLM, enabling it to reason about segmented masks from previous rounds. 2) Mask-Aware Decoding Module: This module allows the mask decoder to generate new masks based on both the visual and textual conversational history, enhancing its contextual understanding. For example, when a user requests segmentation of a part of an object identified in a previous round (e.g., the ear of a man), the model's ability to access prior mask data enables the decoder to more precisely localize and segment the specified object."}, {"title": "5 EXPERIMENTS", "content": "5.1 IMPLEMENTATION DETAILS\nWe use a pretrained CLIP-ViT-Large (Radford et al., 2021) with a patch size of 14 as the image encoder, HIPIE-R50 (Wang et al., 2024b) as the mask encoder and LLaVA-v1.5-7B (Liu et al., 2024) as the base language model. Compared with LISA, which has exactly one mask per training sample, SegLLM's setup contains multiple masks per conversation. Hence, we replaced the SAM ViT-H mask decoder (Kirillov et al., 2023) with a smaller HIPIE-R50 (Wang et al., 2024b) to reduce the computation overhead during the training, We then fine-tune the LLM model and the projector weights fv2L using the training set of our own multi-round instruction-segmentation dataset MRSeg, while keeping the weights of the CLIP image encoder and the HIPIE mask decoder frozen.\nWe use NVIDIA A100 GPUs for model training. We fine-tune our model with a total batch size of 16 (a per-device batch size of 2) using the AdamW optimizer (Loshchilov & Hutter, 2017) with a learning rate of 2e\u00af5. Furthermore, we utilize stage-2 DeepSpeed accelerator (Rasley et al., 2020) and bf16 floating point precision to enhance training efficiency and reduce memory consumption.\n5.2 EVALUATION\nEvaluation benchmarks. For standard single-round image reasoning segmentation and detection tasks, we evaluate our model on the widely used referring segmentation and comprehension benchmarks, RefCOCO/+/g (Yu et al., 2016). We also conduct qualitative and quantitative comparisons with previous SOTA models on our multi-round referring segmentation benchmarks, based on MSCOCO (Lin et al., 2014a), PACO (Ramanathan et al., 2023) and LVIS (Gupta et al., 2019), which assess performance based on positional, interactional or hierarchical relationship queries.\nEvaluation metrics. We use mean Intersection-Over-Union (mIoU) and cumulative Intersection-Over-Union (cIoU) as our main evaluation metrics. To assess the model's performance across multiple rounds of conversation, we track the mIoU and cIoU scores for each round's segmentation outputs.\nBaseline. Since some baseline models, e.g., LISA, do not natively support multi-round interactive segmentation, for comparisons, we adapt our multi-round validation data into their supported single-turn format by converting the N-turn data into N single-turn instruction segmentation tasks."}, {"title": "6 CONCLUSIONS", "content": "We introduce SegLLM, a novel multi-round interactive reasoning segmentation model that enhances traditional segmentation models by retaining conversational memory of visual, not just textual, results. Utilizing a mask-aware multimodal large language model, SegLLM integrates previous segmentation outputs back into its input stream, allowing it to handle complex queries about relationships between objects across multiple interactions. Tested on the newly curated MRSeg, SegLLM significantly outperforms existing benchmarks in multi-round interactive segmentation by over 20% and shows a 4.7% improvement in single-round referring segmentation. These results demonstrate SegLLM's capability as a versatile model for a broad range of instruction-following segmentation tasks."}, {"title": "D LIMITATIONS", "content": "Although we have shown some promising quantitative results in the novel multi-round reasoning segmentation task, our method exhibits several limitations upon qualitative examination, as shown in Fig. A4 and discussed in detail below. In addition to revealing potential weaknesses within our proposed model components, perhaps the existence of failure cases in qualitative evaluation despite the impressive performance in quantitative evaluation indicates that our dataset construction methodology also requires improvement such as including harder test samples. It is our hope that these findings will encourage further research along the direction of multi-round segmentation, aiming to improve the incorporation of conversation and segmentation history, address some of these limitations and extend upon our initial proposals and approaches.\nSensitivity to conversation history order. Given fixed input queries for subsequent rounds, when the order of the previous rounds in the conversation history are permuted, the model's output is not consistent. As shown by conversation 1 in Fig. A4, in rounds 1 and 2, version A first asks for the left paddle board and then the right paddle board, whereas version B first asks for the right paddle board and then the left paddle board. However, in rounds 3 and 4, both version have the same queries, first asking for the person standing on top of the paddle board from round 1 followed by the person standing on top of the paddle board from round 2. Despite having the same input queries for rounds 3 and 4, the model only succeeds in segmenting the correct instance in version A and fails to do so in version B. From the user's standpoint, the two versions are equivalent, since the user is equally likely to start the conversation by asking for the left paddle board, as for the right paddle board. However, the model's behavior is not invariant under the permutation of the conversation history, suggesting that the robustness of our model can be improved.\nSensitivity to input query order. Symmetric to the previous case, given a fixed conversation history, if the order of the queries in subsequent rounds are permuted, then the model's output is also no consistent. As shown by conversation 2 in Fig. A4, in rounds 1 and 2, both version A and version B first asks for the person wearing red followed by the person wearing blue. Then, in rounds 3 and 4, version A first asks for the chair that the person from round 1 is sitting on (query 1), followed by the chair that the person from round 2 is sitting on (query 2), and version B asks - the same queries but in the opposite order. However, despite the queries being the exact same, simply switching the order of these two queries causes the model to only succeed in segmenting the correct instance in version A and fails to do so in version B. Again, from the user's perspective, the two versions are equivalent, since the user is equally likely to first request an instance related to the output from round 1, as to first request an instance related the output from round 2. However, the model's behavior is not invariant under the permutation of the input queries in subsequent rounds, suggesting that the robustness of our model can be improved.\nIndependence of encoding information. Although we quantitatively showed that using our proposed mask encoding component to re-introduce past segmentation outputs into the model's input stream can surpass the performance of other models without this component on our multi-round segmentation benchmarks in Sec. 5.3, as well as verified its effectiveness through conducting ablation study in"}]}