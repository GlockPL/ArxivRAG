{"title": "Consistent Flow Distillation for Text-to-3D Generation", "authors": ["Runjie Yan", "Yinbo Chen", "Xiaolong Wang"], "abstract": "Score Distillation Sampling (SDS) has made significant strides in distilling image-generative models for 3D generation. However, its maximum-likelihood-seeking behavior often leads to degraded visual quality and diversity, limiting its effectiveness in 3D applications. In this work, we propose Consistent Flow Distillation (CFD), which addresses these limitations. We begin by leveraging the gradient of the diffusion ODE or SDE sampling process to guide the 3D generation. From the gradient-based sampling perspective, we find that the consistency of 2D image flows across different viewpoints is important for high-quality 3D generation. To achieve this, we introduce multi-view consistent Gaussian noise on the 3D object, which can be rendered from various viewpoints to compute the flow gradient. Our experiments demonstrate that CFD, through consistent flows, significantly outperforms previous methods in text-to-3D generation. Project page: https://runjie-yan.github.io/cfd/.", "sections": [{"title": "1 INTRODUCTION", "content": "3D content generation has been gaining increasing attention in recent years for its wide range of applications. However, it is expensive to create high-quality 3D assets or scan objects in the real world. The scarcity of 3D data has been a primary challenge in 3D generation. On the other hand, image synthesis has witnessed great progress, particularly with diffusion models trained on large-scale datasets with massive high-quality and diverse images. Leveraging the 2D generative knowledge for 3D generation by model distillation has become a research direction of key importance.\nScore Distillation Sampling (Poole et al., 2023) (SDS) pioneered the paradigm. It uses a pretrained text-to-image diffusion model to optimize a single 3D representation such that the rendered views seek a maximum likelihood objective. Several subsequent efforts (Zhu et al., 2024; Liang et al., 2023; Katzir et al., 2024; Huang et al., 2024; Tang et al., 2023; Wang et al., 2023b; Armandpour et al., 2023) have been made to improve SDS, while the maximum-likelihood-seeking behavior remains, which has a detrimental effect on the visual quality and diversity. Variational Score Distillation (Wang et al., 2024a) (VSD) tackles this issue by treating the 3D representation as a random variable instead of a single point as in SDS. However, the random variable is simulated by particles in VSD. Single-particle VSD is theoretically equivalent to SDS (Wang et al., 2023b), assuming the LORA network in VSD is always trained to optimal. While the optimization-based sampling of VSD is k times slower with k particles.\nIn this work, we propose Consistent Flow Distillation (CFD), which distills 3D representations through gradient-based diffusion sampling of consistent 2D image probability flows across different views. We provide theoretical analysis of this process and extend it to a wide range of deterministic and stochastic diffusion sampling processes. In the distillation process, we identify that a key is to apply consistent flows to the 3D representation. Intuitively, in 2D image generation, the same region is always associated with the same fixed noise for the correct flow sampling. Analogously, in 3D generation, the 2D image flows from different camera views should also use the noise patterns that are consistent on the object surface with correct correspondence. To achieve this, we design a multi-view consistent Gaussian noise based on Noise Transport Equation (Chang et al., 2024), which can compute the multi-view consistent noise with negligible cost. During the distillation process, the"}, {"title": "2 PRELIMINARIES", "content": null}, {"title": "2.1 DIFFUSION MODELS AND PROBABILITY FLOW ORDINARY DIFFERENTIAL EQUATION\n(PF-ODE)", "content": "A forward diffusion process (Sohl-Dickstein et al., 2015; Ho et al., 2020) gradually adds noise to a data point $x_0 \\sim p_0(x_0)$, such that the intermediate distribution $p_{t0}(x_t|x_0)$ conditioned on initial sample $x_0$ at diffusion timestep $t$ is $N(a_tx_0, \\sigma_t^2 I)$, which can be equivalently written as\n$Xt = AtX0 + \\sigma t\\epsilon, \\epsilon \\sim N(0,I)$ (1)\nwhere $\u03b1_0 = 1,\u03c3_0 = 0$ at the beginning, and $\u03b1_T \u2248 0,\u03c3_T \u2248 1$ in the end, such that $p_T (x_T )$ is approximately the standard Gaussian $N(0,\u03c3_T^2 I)$. A diffusion model $\u03f5_\u03c6$ is learned to reverse such process, typically with the following denoising training objective (Ho et al., 2020):\n$L_{DM}(\u03c6) = E_{x0,\u03f5,t}[Wt||\u03f5_\u03c6(Xt, t) \u2212 \u03f5||^2 ].$ (2)\nAfter training, $\u03f5_\u03c6(x_t, t) \u2248 \u2212\u03c3_t\u2207x_t log p_t (x_t )$, where $\u2207x_t log p_t (x_t )$ is termed score function.\nA Probability Flow Ordinary Differential Equation (PF-ODE) has the same marginal distribution as the forward diffusion process at any time $t$ (Song et al., 2021b). The PF-ODE can be written as:\n$d(x_t/\u03b1_t) \\over dt = (\u2212\u03c3_t\u2207x_t log p_t (x_t )) {d(\u03c3_t/\u03b1_t ) \\over dt}$ (3)\n$d(x_t/\u03b1_t ) \\over dt = {d(\u03c3_t/\u03b1_t ) \\over dt} \u03f5_\u03c6(x_t, t), x_T \u223c p_T (x_T ).$ (4)\nA data point $x_0$ can be sampled by starting from a Gaussian noise $x_T \u223c N(0,\u03c3_T^2 I)$ and following the PF-ODE trajectory from $t = T$ to $t = 0$, typically with discretized timesteps and an ODE solver."}, {"title": "2.2 DIFFERENTIABLE 3D REPRESENTATIONS", "content": "Differentiable 3D representations are typically parameterized by the learnable parameters $\u03b8$ and a differentiable rendering function $g_\u03b8(c)$ to render images corresponding to the camera views $c$. In many tasks, the gradient is first obtained on the rendered images $g_\u03b8(c)$ and then backpropagated through the Jacobian matrix $\u2202g_\u03b8(c) \\over \u2202\u03b8$ of the renderer to the learnable parameters $\u03b8$.\nCommon 3D neural representations include Neural Radiance Field (NeRF) (Mildenhall et al., 2021; M\u00fcller et al., 2022; Wang et al., 2021; Barron et al., 2021; Xu et al., 2022), 3D Gaussian Splatting"}, {"title": "3 CONSISTENT FLOW DISTILLATION", "content": "We present Consistent Flow Distillation (CFD), which takes a pretrained and frozen text-to-image diffusion model and distills a 3D representation by the gradient from the probability flow of the 2D image diffusion model. We propose to guide 3D generation with 2D clean flow gradients operating jointly on a 3D object. We identify that a key in this process is to make the flow guidance consistent across different camera views (see Sec. 3.1). We further propose an SDE, a generalization of the clean flow ODE, that incorporates noise injection during optimization to enhance generation quality (see Sec. 3.2). To achieve the consistent flow, we propose an algorithm to compute a multi-view consistent Gaussian noise, which provides noise for different views with noise texture exactly aligned on the surface of the 3D object (see Sec. 3.3). Finally, we draw connections between CFD and other score distillation methods (see Sec. 3.4)."}, {"title": "3.1 3D GENERATION WITH 2D CLEAN FLOW GRADIENT", "content": "Given a pretrained text-to-image diffusion model $\u03f5_\u03c6(x_t, t, y)$, let $y$ denote the condition (text prompt), the conditional distribution $p(x_0|y)$ can be sampled from the PF-ODE (Song et al., 2021b) trajectory from $t = T$ to $t = 0$, which takes the form\n$d(\\hat{x}_t) \\over dt = {d(\u03c3_t/\u03b1_t ) \\over dt} \u03f5_\u03c6(\u03b1_t\\hat{x}_t, t, y) \\over {\u03b1_t \u2202L}  \u2212 lr  \\nabla_\u03b8$ (5)\nBy following the diffusion PF-ODE, pure Gaussian noise is transformed to an image in the target distribution $p(x_0|y)$. Thus PF-ODE can be interpreted as guiding the refinement of a noisy image to a realistic image. Can we use image PF-ODE to directly guide the generation of a differentiable 3D representation $\u03b8$ through the refining process, with $\u03b8$ as its learnable parameters and $g_\u03b8$ as its differentiable rendering function?\nA direct implementation can be substituting the noisy images in Eq. 5 with the rendered images $g_\u03b8(c)$ at the camera view $c$ by letting $\\hat{x}_t = g_\u03b8(c)$. By viewing $d(\\hat{x}_t) \\over dt$ as the learning rate $lr$ of an optimizer and $\u03f5_\u03c6(x_t, t, y)$ as the loss gradient to the gradient can be backpropagated through the Jacobian matrix of the renderer $g_\u03b8(c)$ to update $\u03b8$ according to\n$\\Delta \u03b8 = \u2212 lr \u03f5_\u03c6(\u03b1_tg_\u03b8(c), t, y) {\u2202g_\u03b8(c) \\over \u2202\u03b8}$ (6)\nHowever, such a direct attempt may not work (see Fig. 5 (a)), since the image $x_t$ at diffusion timestep $t$ contains Gaussian noise. It is hard for the images rendered by a 3D representation to match the noisy images $\\hat{x}_t$ in an image PF-ODE, particularly around the beginning $t = T$, where $x_T$ is per-pixel independent Gaussian noise. It is generally impossible for a continuous 3D representation to be rendered as per-pixel independent Gaussian noise from all camera views simultaneously. As a result, the rendered views may be out-of-distribution (OOD) as the input to the pretrained image diffusion model, and therefore cannot get meaningful gradient as guidance.\nTo resolve the OOD issue, we use a change-of-variable (Gu et al., 2023; Yan et al., 2024) to transform the original noisy variable $x_t$ in PF-ODE (Eq. 5) to a new variable that is free of Gaussian noise at any time $t \u2208 [0, T]$. For each trajectory ${x_t}_{t\u2208[0,T]}$ of the $x_t$ in the original PF-ODE, the new variable $\\hat{x}_t$ is defined as\n$\\hat{x}_t = {x_t \\over \u03b1_t } - {\u03c3_t \\over \u03b1_t }\\tilde{\u03f5}$, (7)\nwhere $\\tilde{\u03f5}$ is set as the initial noise $\\tilde{\u03f5} = \u03f5_T$ and is a constant for each ODE trajectory ${x_t}_{t\u2208[0,T]}$. By Eq. 5 and Eq. 7, the evolution of the new variable $\\hat{x}_t$ is derived as follows:\n$d\\hat{x}_t \\over dt = d({x_t \\over \u03b1_t }) \\over dt = {d(\u03c3_t/\u03b1_t ) \\over dt} \u03b1_t (\u03f5_\u03c6(\u03b1_t\\hat{x}_t + \u03c3_t\\tilde{\u03f5}, t, y) \u2212 \\tilde{\u03f5}).$\n-lr \\nabla_\u03b8$ (8)"}, {"title": "3.2 GUIDING 3D GENERATION WITH DIFFUSION SDE", "content": "Despite that PF-ODE and diffusion SDE can recover the same marginal distributions in theory, SDE-based stochastic sampling may result in better generation quality as reported in prior works (Song"}, {"title": "3.3 MULTI-VIEW CONSISTENT GAUSSIAN NOISE E", "content": "To get consistent flow, a multi-view consistent Gaussian noise function $\\tilde{\u03f5}(\u03b8, c)$ is required, which (i) is a per-pixel independent Gaussian noise for all camera views $c$; (ii) the noise patterns from different views have the correct correspondence according to the 3D object surface. It is non-trivial to satisfy all these properties with common warping and interpolation methods. The query rays from camera views $c$ take continuous coordinates, simply using common interpolation methods such as bilinear may break the per-pixel independent property and result in bad quality (see Fig. 5 (b))."}, {"title": "3.4 COMPARISON WITH OTHER SCORE DISTILLATION METHODS", "content": "Comparison with SDS. Both SDS and our CFD share a similar gradient form $(\u03f5_\u03c6(x_t, t, y) {\u2202g_\u03b8(c) \\over \u2202\u03b8})$ to update the 3D representation $\u03b8$ from a sampled rendered view. In SDS, $t$ is typically ran-domly sampled from a range $[t_{min}, t_{max}]$, and $\u03f5$ is a noise randomly sampled at each step. In contrast to SDS, our CFD uses an annealing timestep $t(\u03c4)$ that decreases from $t_{max}$ to $t_{min}$, the deterministic noise $\\tilde{\u03f5}(\u03b8, c)$ depends on both the object surface and the camera view, it is designed to let the noise from different views have correct correspondence according to the object surface. Notably, SDS with annealing timestep schedule can be viewed as setting $\u03b3 = 1$ in CFD, where significant stochasticity is injected in the optimization. As a comparison, for typical diffusion sampling processes, $\u03b3 \u2248 0.00024$ in DDPM, and $\u03b3 = 0$ in DDIM (see Appx. G.4.2). In our CFD, the definition of $\u03b3$ requires that $\u03b3 < 1$ (Appx. Eq. 37), which implies a difference between CFD and SDS.\nTheoretically, when restricted to 2D image generation where $x = g_\u03b8(c)$, SDS is equivalent to seeking the maximum likelihood point in the noisy distribution $p_t$ with a Gaussian distribution $N(\u03b1_t x, \u03c3_t^2 I)$ centered at the image $x$. When the optimization of SDS loss is near optimal, their generation results are centered around a few modes (Poole et al.,"}, {"title": "4 EXPERIMENTS", "content": "In comparisons to prior methods, we distill Stable Diffusion (Rombach et al., 2022) and use the same codebase threestudio (Guo et al., 2023). We compare CFD with various prior state-of-the-art methods, including SDS (Poole et al., 2023; Wang et al., 2023a), VSD (Wang et al., 2024a) and ISM (Liang et al., 2023). Specifically, VSD incorporates LoRA network training in their score distillation, ISM incorporates DDIM inversion in their score distillation. Since timestep annealing (Zhu et al., 2024; Wang et al., 2024a; Huang et al., 2024) has been shown to help improve generation quality (Wang et al., 2024a; Zhu et al., 2024; Huang et al., 2024), we also apply timestep annealing to all baseline methods. We use results from the official implementation of other baselines in qualitative comparisons if not specified. In addition, we show results of a 2-stage pipeline in Fig. 1(a), 1(b), where we first distill MVDream (Shi et al., 2024), then distill Stable Diffusion, which alleviates the multi-face issue (Poole et al., 2023; Armandpour et al., 2023; Hong et al., 2023a). We provide implementation details in Appx. A and details of experiment metrics in Appx. B."}, {"title": "4.1 COMPARISON WITH BASELINES", "content": "We compute 3D-FID following VSD (Wang et al., 2024a) to evaluate the quality and diversity of different score distillation methods, and compute 3D-CLIP to evaluate prompt alignment for different methods. We provide qualitative comparison in Fig. 4 and quantitative results in Tab. 2, 3, and Appx. Tab. 5. We also provide additional comparisons with VSD in Appx. Fig. 9, ISM in Appx. Fig. 10, and SDS in Appx. Fig. 11. As shown in both quantitative and qualitative results, CFD outperforms all baseline methods and has better generation quality (Fig. 4 and Appx. Fig. 9, 10, 11) and diversity (Appx. Fig. 9, 10, 11). Our method produces rich details and the results are more photorealistic. Addition results and comparisons are in Appx. C."}, {"title": "4.2 ABLATION STUDIES", "content": "Ablation on the flow space. As shown in Fig. 5: (a) When directly training $\u03b8$ with original PF-ODE using Eq. 6 with noisy variable, the training fails after several iterations. (b) Simply using bilinear interpolation instead of Noise Transport Equation leads to correlated pixel noise and generates blurry results. (c) When using the random noise as in SDS, the results are over-smoothed. (d) Our consistent flow distillation with multi-view consistent Gaussian noise generates high-quality results. By using a multi-view consistent Gaussian noise, the flow for a fixed camera is more aligned with a diffusion sampling process, and the quality improves. We also provide additional ablations on our design choices in Appx. E.\nAblation on noise injection rate \u03b3. Noise injection rate $\u03b3$ in Eq. 11 determines the rate at which new noise will be injected into the noise function. When $\u03b3 = 0$, no noise will be injected, $\u03f5$ will be fixed constant if the geometry and camera view is fixed and CFD corresponds to using ODE guidance. When $\u03b3 > 0$, new noise will be injected, and $\u03f5(\u03b8, c)$ will gradually change. In this case, CFD corresponds to using SDE guidance. Using SDE-based stochastic samplers may help to improve image generation quality as reported in prior works (Song et al., 2021b;a; Karras et al., 2022). In Tab. 4. We also observe that use a small nonzero $\u03b3$ helps to improve the performance of CFD. In practice, we found that using a $\u03b3$ larger than 0.0001 could result in over-smoothed texture, therefore we set $\u03b3 = 0.0001$ by default in our experiments for CFD. As a reference, we calculated a typical equivalent $\u03b3$ value of DDPM to be $\u03b3 \u2248 0.00024$ (see Appx. G.4.2)."}, {"title": "5 RELATED WORK", "content": "Diffusion models Diffusion models (Sohl-Dickstein et al., 2015; Sharma et al., 2018; Ho et al., 2020; Song et al., 2021b; Changpinyo et al., 2021; Schuhmann et al., 2022) are generative models that are learned to reverse a diffusion process. A diffusion process gradually adds noise to a data distribution, and the diffusion model is trained to reverse such an iterative process based on the score function. Denoise Diffusion Implicit Models (DDIM) (Song et al., 2021a) proposed a determinis-"}, {"title": "6 CONCLUSION", "content": "In this paper, we proposed Consistent Flow Distillation. We begin by leveraging the gradient of the diffusion ODE or SDE sampling process to guide the 3D generation. From a sampling perspective, we identified that using consistent flow to guide the 3D generation is the key to this process. We developed a multi-view consistent Gaussian noise with correct correspondence on the object surface and used it to implement the consistent flow. Our method can generate high-quality 3D representations by distilling 2D image diffusion models and shows improvement in quality and diversity compared with prior score distillation methods.\nLimitations and broader impact. Although CFD can generate 3D assets of high fidelity and diversity, similar to prior works SDS, ISM, and VSD, the generation can take one to a few hours, and when distilling a text-to-image diffusion model, due to the properties of the teacher models, the distilled 3D representation sometimes may have multi-face Janus problem and may not be good for complex prompt. Besides, due to 3D representation flexibility and interference from other views, it is very hard to guarantee that the sampling process from a rendered view of the 3D object is exactly the same as sampling for 2D images given text in practice. While our 3D consistent noise can reduce the interference and achieve better results, the flow for 3D rendered views may not be exactly the same as 2D flows of the initial noise. Also, like other generative models, it needs to pay attention to avoid generating fake and malicious content."}, {"title": "APPENDIX", "content": null}, {"title": "A IMPLEMENTATION DETAILS", "content": "In this paper, we conduct experiments primarily on a single NVIDIA-GeForce-RTX-3090 or NVIDIA-L40 GPU (the latter only for soft shading rendering). In the quantitative experiments, we adopt similar pipelines (including the choice of 3D representation, training steps, shape initialization, teacher diffusion model, etc.) across methods. We apply timestep annealing for all methods and use the same negative prompts in the quantitative experiments. The main differences between methods lie in the loss functions used.\nWe use CFG (Ho & Salimans, 2022) scale of 75 for CFD in quantitative experiments. In practice, We found CFD works the best with CFG scale of 50-75. We apply the same fixed negative prompts (Shi et al., 2024; Katzir et al., 2024; McAllister et al., 2024) for different text prompts.\nFor simple prompts, we directly use CFD to distill Stable Diffusion v2.1 (Fig. 4, 12 and 13).\nFor mesh generation, we first use CFD to generate coarse shapes with MVDream (Shi et al., 2024). Then we use CFD and follow the geometry and mesh refinement stages in VSD (Wang et al., 2024a) with Stable Diffusion v2.1 to generate the mesh results in Fig. 1(b).\nFor complex prompts, we adopt a 2 stage pipeline (Fig. 1(a), 1(c), 6, 7, 8, 9, 10 and 11). We first generate coarse shape by distilling MVDream to avoid multi-face problems. Then we distill Stable Diffusion v2.1 to refine the details and colors (stage 2). We randomly replace the rendered image with normal map with 0.2 probability to regularize the geometry in stage 2. The total training time is approximately 3 hours on A100 GPU."}, {"title": "B EXPERIMENT DETAILS", "content": "3D-FID We compute the FID score between the rendered images for the generated 3D samples and the images generated by the teacher diffusion models following the evaluation setting of VSD (Wang et al., 2024a). For the experiments with 10 prompts in Tab. 2, we sampled 5,000 images for each prompt from Stable Diffusion, creating a real image set with a total of 50,000 images. We generated 3D objects using different score distillation methods, with 10 different seeds per prompt for each method. We rendered 60 views for each 3D object, resulting in a fake image set of 6,000 images. We use FID implementation from torchmetrics package with feature=2048.\n3D-IS We compute the Inception Score (IS) for the front-view images to measure the quality and diversity. We set split=2 to compute the standard variance of the IS metric. Due to limited compute budget, we use 16 random seeds for each parameter setting of $\u03b3$ and then use the rendered front view to compute IS metric. The IS implementation used in our experiments is from the torchmetrics package.\n3D-CLIP We compute the CLIP cosine similarity between the rendered images of the 3D samples and the corresponding text prompt. For one sample, we render 120 views and take the maximum CLIP score. Then we average the CLIP score across different seeds and prompts (and CLIP models). We use CLIP socre implementation from torchmetrics package.\nAesthetic evaluation Following Diffusion-DPO (Wallace et al., 2024), we conduct an automated win rate comparison under reward models in Tab. 3. The performance of our CFD method is evaluated against baseline models using Aesthetics Scores (Schuhmann, 2022) and PickScores (Kirstain et al., 2023). We calculate the scores on rendered images generated from 50 samples, each corresponding to a randomly selected prompt."}, {"title": "CADDITIONAL QUALITATIVE COMPARISON", "content": "We present more comparison between baseline methods and CFD in Fig. 9, Fig. 10, and Fig. 11. We present additional generation results of CFD in Fig. 6, Fig. 7, Fig. 8, and Fig. 12."}, {"title": "D ALGORITHMS", "content": "We provide pseudo algorithms for CFD in Algorithm 1. Algorithm 2 presents how to compute the multi-view consistent Gaussian noise $\u03f5(\u03b8, c)$.\nChoices of warping function $T^{-1}$ and reference space $E_{ref}$ Generally speaking, correct correspondence of noise map between different camera views can be achieved with any choice of continuous warping function $T^{-1}$ and reference space $E_{ref}$. In this work, we choose $E_{ref}$ to be a 2D square space $E_{ref} = [-1, 1]^2$ to utilize existing fast rasterization algorithms, so that Algorithm 2 can be efficiently computed. We design a warping function $T^{-1}$ to map points in 3D world space $E_{world}$ to 2D reference space $E_{ref}$. Specifically, to compute the warping $T^{-1}$ we first convert the"}, {"title": "E ADDITIONAL ABLATIONS", "content": "E.1 ABLATION ON THE DESIGN SPACE\nWe ablate our proposed improvement step by step in this section. Timestep annealing (Wang et al., 2024a; Zhu et al., 2024; Huang et al., 2024) is helpful for forming finer details. Adding negative prompts (Shi et al., 2024; Katzir et al., 2024; McAllister et al., 2024) helps to improve generation styles. We also find that adding negative prompts is crucial when timestep $t(\u03c4)$ is small. Without negative prompts, the color of samples will become unnatural during the optimization at small timesteps. In this work, we apply negative prompts by directly replacing the unconditional prediction of the diffusion model with prediction conditioned on negative prompts. Finally, by changing the random sampled noise in SDS with our multi-view consistent Gaussian noise, the generated samples can form much richer details and are more diverse. We visualize this ablation in Fig. 15.\nWe propose utilizing CFD to distill the multi-view diffusion model, MVDream, in Stage 1 as shape initialization for complex prompts. This decision is based on our observation that both baseline methods and our CFD can experience multi-face issues when solely distilling SDv2.1 (Fig. 14(a) and Fig. 14(b)). However, distilling only MVDream produces low-quality results (Fig. 14(c)). To address these issues, we adopt a two-stage pipeline in our complete method, where Stage 1 initializes the shape using MVDream, and Stage 2 refines it by distilling SDv2.1. This approach effectively mitigates the challenges identified above, as illustrated in Fig. 14(d).\nE.2 COMPARE THE PIPELINE OF DIFFERENT METHODS\nWe list the differences between the pipelines of different baseline methods in Tab. 7."}, {"title": "E.3 COMPARISON ON NOISE METHODS", "content": "We list the differences between the noising methods of different baseline methods in Tab. 6. Concurrent work FSD (Yan et al., 2024) also employs a deterministic, view-dependent noising function and can therefore be considered a special case of our CFD with $\u03b3 = 0$. The noise of FSD is aligned on a shpere independent of the 3D object surface. However, this noise design can still lead to over-smoothed textures, and the misalignment of noise with the 3D object surface can sometimes result in suboptimal geometry (see Fig. 13). The noise design of FSD is inferior to ours when the 3D object shape is nearly formed. Gradient consistency is essential for accurately constructing geometry in differentiable 3D representations like NeRF. Aligning noise in 3D space independently of the object surface can lead to deviations from the original geometry, even when a relatively good shape"}, {"title": "F GRADIENT VARIANCE", "content": "We compare the gradient variance of different methods during training. We compute the scaled gradient variance by taking Exponential Moving Average parameters $\\hat{v}_t$, $m_t$ from Adam optimizer for convenience. We report the scaled gradient variance $\u03c3$ on the parameters of nerf hash encoding with 10 seeds for each of the noising methods. $\u03c3$ was calculated according to (where $g_t$ is the gradient):\n$m_t \u2248 E[g_t ],$\n$\\hat{v}_t \u2248 E[g_t^2 ],$\n$\u03c3 = {\\sqrt{sum(\\hat{v}_t - m_t^2 )} \\over \\sqrt{sum(\\hat{v}_t )} } \u2248 {\\sqrt{sum(Var(g_t))} \\over \\sqrt{sum(\\hat{v}_t )}}$. (15)\nWe report the gradient variance in training for VSD (Wang et al., 2024a), SDS (Poole et al., 2023; Wang et al., 2023a), FSD (Yan et al., 2024) and our methods in Tab. 8."}, {"title": "G CLEAN FLOW SDE", "content": "G.1 BACKGROUND\nSong et al. (Song et al., 2021b) presented a SDE that has the same marginal distribution $p_t (x_t )$ as the forward diffusion process (Eq. 1). EDM (Karras et al., 2022) presented a more general form of this SDE, and the SDE corresponds to forward process defined in Eq. 1 takes the following form:\n$d(\\hat{x_t}) = \u2212\u03c3_t\\nabla_{x_t} log p_t(x_t) d({\u03c3_t \\over \u03b1_t}) + \u03b2_t(\u03b1_t)\u03c3_t\\nabla_{x_t} log p_t(x_t)dt + \\sqrt{2\u03b2_t(\u03b1_t)}dw_t$ (16)\n$d(\\hat{x_t}) = (d({\u03c3_t \\over \u03b1_t}) = \u03b2_t dt) \u03f5_\u03c6(x_t, t, y) + \\sqrt{2\u03b2_t(t)}dw_t,$ (17)\n\u03b1t\n\u03b1t\n\u03b1t\n\u03b1t\n\u2212\u03c3t\u2207xt log pt(xt) + \u221a2\u03b2t(t)dwt,\nwhere $dw_t$ is the standard Wiener process. If we set \u03b1t = 1 for all t \u2208 [0, T], Eq. 17 will become the same SDE in EDM (Karras et al., 2022). The initial condition for the forward process is $x^+ \u223c p_t(x^+)$ at t = ts (ts is small enough but ts > 0 to avoid numerical issues), and for the reverse process, it is $x^\u2212 \u223c N(0, \u03c3_T^2 I)$ at t = T (Note that we also let $\u03b1_T$ be a small number but \u03b1T > 0 to avoid numerical issues)."}, {"title": "G.2 CLEAN FLOW SDE", "content": "The clean flow SDE takes the following form:\n$d\\hat{x_t} = (d({\u03c3_t \\over \u03b1_t}) + \u03b2_t dt) (\u03f5_\u03c6(\u03b1_t \\hat{x}_t + \u03c3_t \u03f5_\u00b1, t, y) \u2212 \u03f5_\u00b1)$,\n$d\u03f5^\u00b1 = \u2213\u03f5_\u00b1\u03b2_tdt + \\sqrt{2\u03b2_t}dw_t,$\n(18)\nwhere $dw_t$ is the standard Wiener process. For the forward process, the initial condition at t = ts is  \u0303+ ~ pt(x+), \u03f5+ ~ N(0, I), and x+ and \u03f5+ are independent. For the reverse process, the initial condition at t = T is x\u2212 = 0 and \u03f5\u2212 ~ N(0, I)."}, {"title": "G.3 PROPERTIES OF", "content": "G.3.1  ARE CLEAN IMAGES FOR ALL t \u2208 [ts, T]\nLemma 1 (Sample predictions are non-noisy images). The sample prediction of the diffusion model\ngt = xt \u2212 \u03c3t\u03f5\u03c6 (xt, t, y)\nxis a weighted average of images in the target distribution po(x0):\nThen, X - are non-noisy images. Furthermore,\n\u03f5\u03c6(xt, t, y) =Xt \u2212 \u03b1tE [XoXt]\u03c3\u03c4 .\ngt = E[xoxt]gt gtAt.\n(21)\n(22)\n(23)"}, {"title": "G.4 PROPERTIES OF \u20ac\u00b1", "content": "+ can be seen as the \"pure noise\" part in the clean flow SDE (Eq. 18). Notably, the evolution of + does not depend on and has a closed-form solution. The dynamic of + is given by\nd  = ++ dt +  \u221a2 dt.\n(27)\nThe initial condition for in both the forward and reverse process are  ~ N(0, I)."}, {"title": "H DISCUSSION ON THE CHOICE OF THE VARIABLE SPACE", "content": "H.1 GROUND-TRUTH VARIABLE\nApart from the clean variable X , FSD (Yan et al., 2024) also defined another variable space that is visually clean, which is the ground-truth variable  \u0303"}]}