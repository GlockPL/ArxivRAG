{"title": "BrewCLIP: A Bifurcated Representation Learning Framework for Audio-Visual Retrieval", "authors": ["Zhenyu Lu", "Lakshay Sethi"], "abstract": "Previous methods for audio-image matching generally fall into one of two categories: pipeline models or End-to-End models. Pipeline models first transcribe speech and then encode the resulting text; End-to-End models encode speech directly. Generally, pipeline models outperform end-to-end models, but the intermediate transcription necessarily discards some potentially useful non-textual information. In addition to textual information, speech can convey details such as accent, mood, and and emphasis, which should be effectively captured in the encoded representation. In this paper, we investigate whether non-textual information, which is overlooked by pipeline-based models, can be leveraged to improve speech-image matching performance. We thoroughly analyze and compare End-to-End models, pipeline models, and our proposed dual-channel model for robust audio-image retrieval on a variety of datasets. Our approach achieves a substantial performance gain over the previous state-of-the-art by leveraging strong pre-trained models, a prompting mechanism and a bifurcated design. Our code will be made available.", "sections": [{"title": "1 Introduction", "content": "Audio-image matching is similar to the popular text-image matching problem, but it is comparatively less studied due to the increased complexity of modeling audio and the lower availability of paired audio-image data. Speech contains intricate information in the form of tone, timbre, stress patterns, and contextual cues that can exhibit substantial variation among speakers and languages. However, this complexity also means that speech carries richer information compared to text.\nRecently, multi-modal models (Radford et al., 2021; Chen et al., 2020; Su et al., 2019) has proven to be highly beneficial for retrieval tasks and other various downstream tasks. While numerous audio-image models have been proposed (Shih et al., 2023; Sanabria et al., 2021; Peng and Harwath, 2022), none have achieved the remarkable impact seen with recent text-image models. Current efforts in the audio-image matching domain can be broadly categorized into two branches: pipeline models and End-to-End (E2E) models.\nTranscription-based pipeline models tend to achieve superior performance due to the availability of well-developed Automatic Speech Recognition (ASR) systems and text encoders. Some studies even consider pipeline models with perfect transcription as the upper bound for performance on the task (Peng and Harwath, 2022). However, we argue that although pipeline models generally outperform E2E models, a well-learned audio representation jointly learnt with image should not be limited to textual information alone.\nIn light of these observations, we raise the question: Can we devise a method to learn a representation that accurately captures textual information while also retaining crucial audio-specific information? Furthermore, does audio contain additional information that is vital for the audio-image retrieval task? To address these questions, we propose a novel bifurcated model called BrewCLIP."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Vision Language Models", "content": "Recent progress in vision-language models has been made by exploiting powerful jointly-trained unimodal encoders. Text-visual learning has always been the predominant focus, applied to tasks like, image classification or detection (Yao et al., 2021; Bugliarello et al., 2021), text-image retrieval (Radford et al., 2021), text conditional image generation(Ramesh et al., 2021, 2022), audio-visual learning remains relatively under-explored, especially lack of a unified framework for multiple tasks. Most efforts in this domain have been concentrated at the video level, encompassing tasks like source separation and Localization(Tian et al., 2021; Tzinis et al., 2020; Mo and Tian, 2023), temporal synchronization (Korbar et al., 2018; Owens and Efros, 2018) and audio-video matching (Arandjelovic and Zisserman, 2018; Zhao et al., 2018)."}, {"title": "2.2 Audio-Image Retrieval Task", "content": "Cross-modal retrieval (Bugliarello et al., 2021; Kim et al., 2021) is considered as one of the gold standard to evaluate the joint representation across different modalities. Likewise, image-speech matching is to associate spoken descriptions with their corresponding images. Performing well on image-speech matching requires the model to effectively associate similar information from vastly different audio and image modalities. Image-speech matching finds immediate applications in content-based image retrieval, where spoken descriptions are used to retrieve relevant images. Furthermore, we hope our study can pave the road for the development of a strong audio-visual joint representation, which can be used for multiple downstream tasks."}, {"title": "2.3 End-to-End model and Pipeline Model", "content": "Existing literature primarily tackles the imagespeech retrieval task in one of two ways: End-toEnd(E2E) training or ASR based pipeline models. Both approaches have been explored in works such as (Sanabria et al., 2021), which provides exemplary implementations of each. End-to-End models are usually composed of a dual-encoder design which directly associate the raw audio and the image. In contrast, pipeline models rely on an ASR model to convert the raw audio into text format, either full transcription or related keywords, transforming the task into a transcription-image retrieval problem. FaST-VGS (Peng and Harwath, 2022) is an example of an E2E model that incorporates cross-modal attention within its transformerbased structure. A more recent method, SpeechCLIP(Shih et al., 2023), employs several largepretrained models, such as CLIP and Hubert (Hsu et al., 2021), to tackle the image-speech retrieval task. It offers two main model variants: parallel and cascaded, each representing explorations of the two aforementioned approaches. On the other hand, Higy et al. (2020) explores various means of exploiting textual supervision to enhance performance in the speech-image matching task. However, MILAN (Sanabria et al., 2021) also demonstrate superior performance for non-pipeline models under certain conditions, largely influenced by the performance of the ASR system."}, {"title": "2.4 Prompting", "content": "Prompting has become a prevalent approach for extremely cheap few-shot fine-tuning of transformerbased models. It involves finding (via optimization or heuristic prompt engineering) a set of tokens to prepend to a transformer model's input in order to improve its performance on a downstream task. By keeping the original model's weights frozen, prompted models are highly generalizable. Originally utilized in the NLP domain (Houlsby et al.,"}, {"title": "3 Method", "content": ""}, {"title": "3.1 Component Pretrained Models", "content": "CLIP(Radford et al., 2021) CLIP is a widelyused image/text encoder trained on a simple contrastive loss. It has been trained exhaustively on a staggering 400 million web-scraped image-text pairs, yielding a highly generalizable set of encoders. CLIP performs well on images of objects, scenes, and abstract concepts, and its encoders generalize to other vision-language tasks. In light of its strong performance, several variations of CLIP have been proposed. Our approach is inspired by AudioCLIP (Guzhov et al., 2022), Wav2CLIP (Wu et al., 2022), and SpeechCLIP, which apply CLIP-like methods to audio-related tasks.\nWhisper (Radford et al., 2023) To leverage CLIP for audio-image matching, we must first transcribe the raw input audio using an Automatic Speech Recognition (ASR) model. Fortunately, current ASR models have reached approximately human-level performance, paving the way for a strong new pipeline-based model. Whisper adopts an encoder-decoder transformer structure, leveraging a large-scale dataset of low-quality paired utterances and their transcriptions from the internet. This approach has enabled Whisper to achieve state-of-the-art performance for speech transcription."}, {"title": "3.2 Task Definition", "content": "Consider a dataset D = {(X_{V}^{i},X_{A}^{i}): i \u2208 [n]}, where (XV, XA) denotes paired image and audio. Each image or audio clip may appear in multiple pairs. The matching task can be defined as follows: given a query of either modality X_{V/A}^{q} the objective is to retrieve an associated instance X_{A/V}^{i} from the set of all given data of the other modality.\nZero-Shot Setting The model utilizes the pretrained modules without any fine tuning or prompting. It is directly evaluated on the test set of the target dataset without additional training on that specific dataset.\nSelf-Supervised Setting This is typical setting where the model is fine-tuned with all the samples from the target training dataset and then evaluated on the associated test set."}, {"title": "3.3 BrewCLIP: Bifurcated Whisper CLIP", "content": "Existing studies have revealed a recurring phenomenon where pipeline models tend to outperform End-to-End (E2E) models, particularly when a superior ASR model is available. However, we posit that pipeline models suffer from a crucial drawback: they lose vital information that only exists in the raw audio as they transcribe the audio into text. To investigate this hypothesis and address the issue, we propose a bifurcated design, depicted in Fig. 2, capable of processing raw audio and its transcription in two distinct but parallel channels. Our architecture builds upon two powerful pre-trained models, Whisper and CLIP, as previously introduced. By employing the bifurcated design, we aim to retain essential audio-specific information that might be lost during the transcription process while still benefiting from the strengths of the pre-trained text and image encoders."}, {"title": "Image Encoder", "content": "We start by tokenizing the raw image X_{V} into n patches, creating the patch embedding E_{V} \u2208 R^{n\u00d7dv}, and these patch embedding Ev are then fed into the pretrained CLIP image encoder V to obtain the image representation Fv.\nF_{V} = ImageEncoder(E_{V})"}, {"title": "Transcription based Textual Module", "content": "This module follows a typical pipeline approach to obtain the textual embedding from the audio by connecting Whisper and CLIP text encoder. The Whisper encoder takes the re-sampled audio embedding EA from the raw X A and outputs the intermediate audio feature ZA\nZ_{A} = Whisper Encoder(E_{A})\nThe decoder then takes the encoded audio features ZA as input and generate the corresponding transcription. The tokenized transcription embedding ET is then fed into the text encoder to obtain the Textual feature FT.\nF_{T} = TextEncoder(Whisper Decoder(E_{T}))"}, {"title": "Acoustic End-to-End Module", "content": "We augment the baseline pipeline model with an additional End-to-End channel that handles the raw audio features, in order to capture the lost non-textual information. Given the intermediate audio embedding ZA generated from the Whisper encoder, we feed it into a transformer encoder layer to obtain a broad acoustic representation.\nF_{A} =Transformer Encoder(Z_{A})\nSubsequently, we fuse FA and FT together by simply addition, yielding the final audio representation.\nF^{Final} = F_{A} + F_{T}"}, {"title": "Shared Prompting", "content": "Cross-modal interaction has been proved to highly beneficial in models that process multiply modalities. One popular design involves cross-modal attention(Tsai et al., 2019; Hendricks et al., 2021), encouraging the overlap between different modalities to promote learning of shared information. However, when applied to retrieval tasks, cross-modal attention introduces additional challenges(Kim et al., 2021). Besides, cross-modal transformers may not be applicable for large pre-trained models. Therefore, in our approach, we finetune the pre-trained model by adopting a lightweight shared prompt design to encourage the commutation between data from different modalities. We formulate our prompted textual embedding as:\n\\hat{E}_{T} = [P_{T}^{[1]}, P_{T}^{[2]}, ..., P_{T}^{[M]},\nE_{T}^{[1]}, E_{T}^{[2]}, ..., E_{T}^{[K]} ]\nWhere P_{T}^{[m]} : (m \u2208 1,\u2026\u2026\u2026, M) is a vector with the same dimension as the embeddings, M is a hyperparameter specifying the length of prompts, and K is the length of original textual embedding ET. The same PT is used in the E2E channel by similarly concatenating with the raw ZA, obtaining 2A. We connect the two prompts from different modalities using a linear layer, primarily to handle the dimension mismatch issues while also allowing some flexibility to each individual prompt.\nP_{V} = Linear(P_{T})"}, {"title": "Loss Function", "content": "We follow the same InfoNCE loss (Oord et al., 2018) used in CLIP. LV\u2192L is denoted as shown in Equation 1).\nL_{V\u2192L} = -\\sum_{i=1}^{N}log(\\frac{exp(\\frac{sim(F_{V}^{i}, F_{L}^{i})}{\u03c4})}{\\sum_{j=1}^{N}exp(\\frac{sim(F_{V}^{i}, F_{L}^{j})}{\u03c4})})  (1)\nwhere the similarity score is computed by dot product between FV and FL, and \u03c4 is a learnable temperature variable. The loss for the audio branch LL\u2192V exactly follows the same design.\nWe adopt an inner-outer loss design. Louter is the main loss function to link the audio and image modalities. Linner is a helper loss function to simultaneously regulate the learned acoustic embedding, ensuring it is not too distant from its textual counterpart, while facilitating the acquisition of new non-textual information.\nL_{outer} = L_{V\u2192L} + L_{L\u2192V}\nL_{inner} = L_{A\u2192T} + L_{T\u2192A}\nL_{Final} = \u03b1L_{inner} + (1 - \u03b1)L_{outer} (2)\nWhere LA\u2192T computes the contrastive loss from FA to FT, LT\u2192A is computed in the reverse direction and \u03b1 is a small hyper-parameter to control the contribution of the inner and outer losses."}, {"title": "4 Experiment", "content": "For the retrieval task, we conduct experiments on both spontaneous and non-spontaneous spoken caption datasets: Flickr8k, SpokenCOCO, and Localized Narratives-COCO, with average utterance lengths of 10.9, 10, and 36.5 words, respectively. Our hypothesis is that unscripted speech would contain more audio-specific features compared to speech read from text captions. Unscripted utterances often involve disfluencies and filler words, which pose additional challenges for audio processing. Additionally, we seek to explore whether our model could successfully extract non-textual information, primarily mood information, from the audio. To investigate this, we conduct a brief Speech Emotion Recognition (SER) experiment using our well-trained audio encoder on the RAVDESS dataset (Livingstone and Russo, 2018).\nConsistent with the literature, we measure the performance of the retrieval task using top-K recall (where K is 1/5/10), which refers to whether the model retrieved the correct element as at least one of its first K guesses. We also vary whether the query is an image or an audio clip, giving us image2speech and speech2image metrics, respectively. Furthermore, if there are multiple correct answers (as is the case with images that receive multiple captions), any correct answer in the top K counts as a valid recall."}, {"title": "4.1 Dataset", "content": "Flickr8k The baseline dataset we use is Flickr8k(Hodosh et al., 2013), which contains 5 captions per image and around 8k images. Flickr8k contains common images and brief, relevant textual captions. After these textual captions were written, the dataset was extended by asking people to read each textual caption.\nSpokenCOCO and LN-COCO SpokenCOCO and Localized Narratives (LN-COCO) are both derived from the MS-COCO image dataset (Lin et al., 2014), which consists of 123k images. SpokenCOCO utilizes the newer Karpathy train-val split and pairs each image with five spoken captions, where viewers are provided with the corresponding text, similar to Flickr8k. On the other hand, Localized Narratives contains unscripted spoken English descriptions of images that are generally more spontaneous in nature.\nRAVDESS This dataset comprises 1440 audio files recorded by 24 professional actors and labeled with eight different emotional expressions: neutral, calm, happy, sad, angry, fearful, disgust, and surprised. For our investigation, we focus on four regular emotions, namely neutral, happy, sad, and angry, consistent with previous studies. This dataset contain speech clips with the same textual content but different emotional tone. This aspect makes the dataset ideal for testing whether our model can successfully capture non-textual information contained in spoken communication."}, {"title": "5 Results and Discussion", "content": "As indicated in Table. 1, the pipeline-only model, even under the zero-shot setting, has already surpassed existing works that were trained or fine-tuned by target dataset. Moreover, our final model achieves even higher scores. To the best of our knowledge, no previous studies have evaluated audio-image retrieval on the LN-COCO dataset. Therefore, as a baseline, we implemented one of the most recent and well-performing E2E models, the parallel version of SpeechCLIP (SpeechCLIP-P), which has achieved SotA results on Flickr8k and SpokenCOCO dataset to test its performance on an unscripted dataset. As Shown in Table. 1, SpeechCLIP-P's performance on LNCOCO is much lower than BrewCLIP's.\nTo test our hypothesis that non-textual information is useful for the downstream task, we compare three variations of our model: the full model, pipeline-only model and E2E-only model. The E2E module is similar to the SotA SpeechCLIP-P but with the Whisper encoder replacing the Hubert model. We experiment on both the scripted SpokenCOCO and unscripted LN-COCO, which share the same source image set, enabling a fair comparison. As depicted in Table. 2, the pipeline-only model generally outperforms the E2E-only model, especially when a smaller number of guesses are allowed in the recall task. This observation is consistent with existing literature and indicates that transcribed text as an intermediate output, combined with a robust text encoder, better enables the learning of subtle and detailed information present in the paired utterances and images, benefiting retrieval tasks (qualitative analysis shown in Fig. 3). This difference becomes particularly evident when experimenting on the unscripted LN-COCO dataset, where the performance of the"}, {"title": "Robustness", "content": "By combining robust pipeline model with state of art end-to-end model (SpeechCLIP-P) . It would be really beneficial to make this kind of architecture."}, {"title": "Conclusion", "content": "In this work, we argue that current pipeline-based approaches for audio-image retrieval, while achieving remarkable performance, inevitably lose critical audio-specific information during the transcribing process. To address this issue, we propose a novel dual-channel architecture that effectively learns a compact joint representation between speech and image. Our study not only establishes a robust baseline for zero-shot pipeline models but also pushes the boundaries of the current state-of-the-art. Furthermore, we run comprehensive experiments evaluating various types of model on different types of dataset, elucidating the importance of employing a dual-channel model to build a more comprehensive image-audio joint representation."}, {"title": "Limitation", "content": "Our primary limitation lies in the inability to directly demonstrate that our audio-image joint embedding can capture the shared non-textual connection between image-audio pairs, due to the absence of a suitable dataset. Also, our model faces constraints imposed by the pre-trained models used. The CLIP text encoder can only process at most 77 tokens while some LN-COCO samples extend up to 2 minutes, exceeding the 77-token limit when"}]}