{"title": "BrewCLIP: A Bifurcated Representation Learning Framework for Audio-Visual Retrieval", "authors": ["Zhenyu Lu", "Lakshay Sethi"], "abstract": "Previous methods for audio-image matching generally fall into one of two categories: pipeline models or End-to-End models. Pipeline models first transcribe speech and then encode the resulting text; End-to-End models encode speech directly. Generally, pipeline models outperform end-to-end models, but the intermediate transcription necessarily discards some potentially useful non-textual information. In addition to textual information, speech can convey details such as accent, mood, and and emphasis, which should be effectively captured in the encoded representation. In this paper, we investigate whether non-textual information, which is overlooked by pipeline-based models, can be leveraged to improve speech-image matching performance. We thoroughly analyze and compare End-to-End models, pipeline models, and our proposed dual-channel model for robust audio-image retrieval on a variety of datasets. Our approach achieves a substantial performance gain over the previous state-of-the-art by leveraging strong pre-trained models, a prompting mechanism and a bifurcated design. Our code will be made available.", "sections": [{"title": "Introduction", "content": "Audio-image matching is similar to the popular text-image matching problem, but it is comparatively less studied due to the increased complexity of modeling audio and the lower availability of paired audio-image data. Speech contains intricate information in the form of tone, timbre, stress patterns, and contextual cues that can exhibit substantial variation among speakers and languages. However, this complexity also means that speech carries richer information compared to text.\nRecently, multi-modal models (Radford et al., 2021; Chen et al., 2020; Su et al., 2019) has proven to be highly beneficial for retrieval tasks and other various downstream tasks. While numerous audio-image models have been proposed (Shih et al., 2023; Sanabria et al., 2021; Peng and Harwath, 2022), none have achieved the remarkable impact seen with recent text-image models. Current efforts in the audio-image matching domain can be broadly categorized into two branches: pipeline models and End-to-End (E2E) models.\nTranscription-based pipeline models tend to achieve superior performance due to the availability of well-developed Automatic Speech Recognition (ASR) systems and text encoders. Some studies even consider pipeline models with perfect transcription as the upper bound for performance on the task (Peng and Harwath, 2022). However, we argue that although pipeline models generally outperform E2E models, a well-learned audio representation jointly learnt with image should not be limited to textual information alone.\nIn light of these observations, we raise the question: Can we devise a method to learn a representation that accurately captures textual information while also retaining crucial audio-specific information? Furthermore, does audio contain additional information that is vital for the audio-image retrieval task? To address these questions, we propose a novel bifurcated model called BrewCLIP."}, {"title": "Related Work", "content": ""}, {"title": "Vision Language Models", "content": "Recent progress in vision-language models has been made by exploiting powerful jointly-trained unimodal encoders. Text-visual learning has always been the predominant focus, applied to tasks like, image classification or detection (Yao et al., 2021; Bugliarello et al., 2021), text-image retrieval (Radford et al., 2021), text conditional image generation(Ramesh et al., 2021, 2022), audio-visual learning remains relatively under-explored, especially lack of a unified framework for multiple tasks. Most efforts in this domain have been concentrated at the video level, encompassing tasks like source separation and Localization(Tian et al., 2021; Tzinis et al., 2020; Mo and Tian, 2023), temporal synchronization (Korbar et al., 2018; Owens and Efros, 2018) and audio-video matching (Arandjelovic and Zisserman, 2018; Zhao et al., 2018)."}, {"title": "Audio-Image Retrieval Task", "content": "Cross-modal retrieval (Bugliarello et al., 2021; Kim et al., 2021) is considered as one of the gold standard to evaluate the joint representation across different modalities. Likewise, image-speech matching is to associate spoken descriptions with their corresponding images. Performing well on image-speech matching requires the model to effectively associate similar information from vastly different audio and image modalities. Image-speech matching finds immediate applications in content-based image retrieval, where spoken descriptions are used to retrieve relevant images. Furthermore, we hope our study can pave the road for the development of a strong audio-visual joint representation, which can be used for multiple downstream tasks."}, {"title": "End-to-End model and Pipeline Model", "content": "Existing literature primarily tackles the image-speech retrieval task in one of two ways: End-to-End(E2E) training or ASR based pipeline models. Both approaches have been explored in works such as (Sanabria et al., 2021), which provides exemplary implementations of each. End-to-End models are usually composed of a dual-encoder design which directly associate the raw audio and the image. In contrast, pipeline models rely on an ASR model to convert the raw audio into text format, either full transcription or related keywords, transforming the task into a transcription-image retrieval problem. FaST-VGS (Peng and Harwath, 2022) is an example of an E2E model that incorporates cross-modal attention within its transformer-based structure. A more recent method, Speech-CLIP(Shih et al., 2023), employs several large-pretrained models, such as CLIP and Hubert (Hsu et al., 2021), to tackle the image-speech retrieval task. It offers two main model variants: parallel and cascaded, each representing explorations of the two aforementioned approaches. On the other hand, Higy et al. (2020) explores various means of exploiting textual supervision to enhance performance in the speech-image matching task. However, MILAN (Sanabria et al., 2021) also demonstrate superior performance for non-pipeline models under certain conditions, largely influenced by the performance of the ASR system."}, {"title": "Prompting", "content": "Prompting has become a prevalent approach for extremely cheap few-shot fine-tuning of transformer-based models. It involves finding (via optimization or heuristic prompt engineering) a set of tokens to prepend to a transformer model's input in order to improve its performance on a downstream task. By keeping the original model's weights frozen, prompted models are highly generalizable. Originally utilized in the NLP domain (Houlsby et al.,"}, {"title": "Method", "content": ""}, {"title": "Component Pretrained Models", "content": "CLIP(Radford et al., 2021) CLIP is a widely-used image/text encoder trained on a simple contrastive loss. It has been trained exhaustively on a staggering 400 million web-scraped image-text pairs, yielding a highly generalizable set of encoders. CLIP performs well on images of objects, scenes, and abstract concepts, and its encoders generalize to other vision-language tasks. In light of its strong performance, several variations of CLIP have been proposed. Our approach is inspired by AudioCLIP (Guzhov et al., 2022), Wav2CLIP (Wu et al., 2022), and SpeechCLIP, which apply CLIP-like methods to audio-related tasks.\nWhisper (Radford et al., 2023) To leverage CLIP for audio-image matching, we must first transcribe the raw input audio using an Automatic Speech Recognition (ASR) model. Fortunately, current ASR models have reached approximately human-level performance, paving the way for a strong new pipeline-based model. Whisper adopts an encoder-decoder transformer structure, leveraging a large-scale dataset of low-quality paired utterances and their transcriptions from the internet. This approach has enabled Whisper to achieve state-of-the-art performance for speech transcription."}, {"title": "Task Definition", "content": "Consider a dataset $D = \\{(X_V^i, X_A^i): i \\in [n]\\}$, where $(X_V, X_A)$ denotes paired image and audio. Each image or audio clip may appear in multiple pairs. The matching task can be defined as follows: given a query of either modality $X_{V/A}^i$, the objective is to retrieve an associated instance $X_{A/V}^i$ from the set of all given data of the other modality.\nZero-Shot Setting The model utilizes the pre-trained modules without any fine tuning or prompting. It is directly evaluated on the test set of the target dataset without additional training on that specific dataset.\nSelf-Supervised Setting This is typical setting where the model is fine-tuned with all the samples from the target training dataset and then evaluated on the associated test set."}, {"title": "BrewCLIP: Bifurcated Whisper CLIP", "content": "Existing studies have revealed a recurring phenomenon where pipeline models tend to outperform End-to-End (E2E) models, particularly when a superior ASR model is available. However, we posit that pipeline models suffer from a crucial drawback: they lose vital information that only exists in the raw audio as they transcribe the audio into text. To investigate this hypothesis and address the issue, we propose a bifurcated design, depicted in Fig. 2, capable of processing raw audio and its transcription in two distinct but parallel channels. Our architecture builds upon two powerful pre-trained models, Whisper and CLIP, as previously introduced. By employing the bifurcated design, we aim to retain essential audio-specific information that might be lost during the transcription process while still benefiting from the strengths of the pre-trained text and image encoders."}, {"title": "Image Encoder", "content": "We start by tokenizing the raw image $X_V$ into n patches, creating the patch embedding $E_V \\in R^{n \\times d_V}$, and these patch embedding $E_V$ are then fed into the pretrained CLIP image encoder V to obtain the image representation $F_V$.\n$F_V = \\text{ImageEncoder}(E_V)$"}, {"title": "Transcription based Textual Module", "content": "This module follows a typical pipeline approach to obtain the textual embedding from the audio by connecting Whisper and CLIP text encoder. The Whisper encoder takes the re-sampled audio embedding $E_A$ from the raw $X_A$ and outputs the intermediate audio feature $Z_A$\n$Z_A = \\text{Whisper Encoder}(E_A)$\nThe decoder then takes the encoded audio features $Z_A$ as input and generate the corresponding transcription. The tokenized transcription embedding $E_T$ is then fed into the text encoder to obtain the Textual feature $F_T$.\n$F_T = \\text{TextEncoder}(\\text{Whisper Decoder}(E_T))$"}, {"title": "Acoustic End-to-End Module", "content": "We augment the baseline pipeline model with an additional End-to-End channel that handles the raw audio features, in order to capture the lost non-textual information. Given the intermediate audio embedding $Z_A$ generated from the Whisper encoder, we feed it into a transformer encoder layer to obtain a broad acoustic representation.\n$F_A = \\text{Transformer Encoder}(Z_A)$\nSubsequently, we fuse $F_A$ and $F_T$ together by simply addition, yielding the final audio representation.\n$F_{\\text{Final}} = F_A + F_T$"}, {"title": "Shared Prompting", "content": "Cross-modal interaction has been proved to highly beneficial in models that process multiply modalities. One popular design involves cross-modal attention(Tsai et al., 2019; Hendricks et al., 2021), encouraging the overlap between different modalities to promote learning of shared information. However, when applied to retrieval tasks, cross-modal attention introduces additional challenges(Kim et al., 2021). Besides, cross-modal transformers may not be applicable for large pre-trained models. Therefore, in our approach, we finetune the pre-trained model by adopting a lightweight shared prompt design to encourage the commutation between data from different modalities. We formulate our prompted textual embedding as:\n$\\hat{E}_T = [P_T^{[1]}, P_T^{[2]}, ..., P_T^{[M]}, E_T^{[1]}, E_T^{[2]}, ..., E_T^{[K]} ]$\nWhere $P_T^{[m]} : (m \\in 1, \\dots, M)$ is a vector with the same dimension as the embeddings, M is a hyperparameter specifying the length of prompts, and K is the length of original textual embedding $E_T$. The same $P_T$ is used in the E2E channel by similarly concatenating with the raw $Z_A$, obtaining $\\hat{Z_A}$. We connect the two prompts from different modalities using a linear layer, primarily to handle the dimension mismatch issues while also allowing some flexibility to each individual prompt.\n$P_V = \\text{Linear}(P_T)$"}, {"title": "Loss Function", "content": "We follow the same InfoNCE loss (Oord et al., 2018) used in CLIP. $L_{V \\rightarrow L}$ is denoted as shown in Equation 1).\n$L_{V \\rightarrow L} = - \\frac{1}{N} \\sum_{i=1}^N \\log \\frac{\\exp\\left(\\frac{\\text{sim}(F_V^{[i]}, F_L^{[i]})}{\\tau}\\right)}{\\sum_{j=1}^N \\exp\\left(\\frac{\\text{sim}(F_V^{[i]}, F_L^{[j]})}{\\tau}\\right)}                                                                                                                                                                             (1)$\nwhere the similarity score is computed by dot product between $F_V$ and $F_L$, and $\\tau$ is a learnable temperature variable. The loss for the audio branch $L_{L \\rightarrow V}$ exactly follows the same design.\nWe adopt an inner-outer loss design. $L_{outer}$ is the main loss function to link the audio and image modalities. $L_{inner}$ is a helper loss function to simultaneously regulate the learned acoustic embedding, ensuring it is not too distant from its textual counterpart, while facilitating the acquisition of new non-textual information.\n$L_{outer} = L_{V \\rightarrow L} + L_{L \\rightarrow V}$ \n$L_{inner} = L_{A \\rightarrow T} + L_{T \\rightarrow A}$\n$L_{Final} = \\alpha L_{inner} + (1 - \\alpha) L_{outer}$                                                                                                (2)\nWhere $L_{A \\rightarrow T}$ computes the contrastive loss from $F_A$ to $F_T$, $L_{T \\rightarrow A}$ is computed in the reverse direction and $\\alpha$ is a small hyper-parameter to control the contribution of the inner and outer losses."}, {"title": "Experiment", "content": "For the retrieval task, we conduct experiments on both spontaneous and non-spontaneous spoken caption datasets: Flickr8k, SpokenCOCO, and Localized Narratives-COCO, with average utterance lengths of 10.9, 10, and 36.5 words, respectively. Our hypothesis is that unscripted speech would contain more audio-specific features compared to speech read from text captions. Unscripted utterances often involve disfluencies and filler words, which pose additional challenges for audio processing. Additionally, we seek to explore whether our model could successfully extract non-textual information, primarily mood information, from the audio. To investigate this, we conduct a brief Speech Emotion Recognition (SER) experiment using our well-trained audio encoder on the RAVDESS dataset (Livingstone and Russo, 2018).\nEvaluation metrics Consistent with the literature, we measure the performance of the retrieval task using top-K recall (where K is 1/5/10), which refers to whether the model retrieved the correct element as at least one of its first K guesses. We also vary whether the query is an image or an audio clip, giving us image2speech and speech2image metrics, respectively. Furthermore, if there are multiple correct answers (as is the case with images that receive multiple captions), any correct answer in the top K counts as a valid recall."}, {"title": "Dataset", "content": ""}, {"title": "Results and Discussion", "content": "Comparison with SotA As indicated in Table. 1, the pipeline-only model, even under the zero-shot setting, has already surpassed existing works that were trained or fine-tuned by target dataset. Moreover, our final model achieves even higher scores. To the best of our knowledge, no previous studies have evaluated audio-image retrieval on the LN-COCO dataset. Therefore, as a baseline, we implemented one of the most recent and well-performing E2E models, the parallel version of SpeechCLIP (SpeechCLIP-P), which has achieved SotA results on Flickr8k and SpokenCOCO dataset to test its performance on an unscripted dataset. As Shown in Table. 1, SpeechCLIP-P's performance on LN-COCO is much lower than BrewCLIP's.\nHypothesis Validation To test our hypothesis that non-textual information is useful for the downstream task, we compare three variations of our model: the full model, pipeline-only model and E2E-only model. The E2E module is similar to the SotA SpeechCLIP-P but with the Whisper encoder replacing the Hubert model. We experiment on both the scripted SpokenCOCO and unscripted LN-COCO, which share the same source image set, enabling a fair comparison. As depicted in Table. 2, the pipeline-only model generally outperforms the E2E-only model, especially when a smaller number of guesses are allowed in the recall task. This observation is consistent with existing literature and indicates that transcribed text as an intermediate output, combined with a robust text encoder, better enables the learning of subtle and detailed information present in the paired utterances and images, benefiting retrieval tasks (qualitative analysis shown in Fig. 3). This difference becomes particularly evident when experimenting on the unscripted LN-COCO dataset, where the performance of the E2E model significantly lags behind the pipeline model. For the scripted SpokenCOCO, the performance of the pipeline-only model does not show a significant difference compared to the full BrewCLIP model. This is as expected since transcribing is easier for scripted utterances, and they might not contain as much non-textual information. On the other hand, we observe a significant improvement after adding the E2E module to the pipeline-only model when evaluating our full model on the unscripted LN-COCO dataset. To further investigate whether this improvement comes from the E2E channel compensating for mistakes made by the ASR model or from the non-textual information, we directly feed the exact ground truth captions into our pipeline-only model. As shown in Table. 2, the improvement is more likely attributed to the former as feeding ground truth captions into our pipeline-only model can even surpass our full model. To this end, we can conclude that the E2E channel can act as a remedy if the transcription process fails, but whether our model can effectively extract non-textual information remains inconclusive.\nMultimodal Dataset Aware of Non-Texutal Content To further test the capabilities of our proposed model, we recognize the need for a new dataset that emphasizes extra information beyond purely textual content. This dataset should focus on capturing elements such as tone, emotion, and other non-semantic aspects present in the audio, as they are likely to play a crucial role in determining the correct pairing of audio and image."}, {"title": "Speech Emotion Recognition", "content": "We fail to find such a dataset. As an alternative, we conduct a simple Speech Emotion Recognition (SER) experiment only testing our trained audio encoder. For this experiment, we train an additional linear classification head to test whether our model can accurately recognize emotions in the utterances. As shown in Table. 3, As anticipated, the pipeline-only model does not work as the textual content is the same for each utterance. Nonetheless, we observe some effects by applying a simple linear classifier head to our frozen audio encoder with even higher performance achieved through finetuning. These results robustly support our hypothesis that our proposed model can successfully capture critical non-textual information (primarily mood changes in this case) that exists in the representation learned from the E2E module. This non-textual information plays a significant role in constructing a holistic audio representation."}, {"title": "Effect of Prompting", "content": "A considerable portion of the model's mistakes are related to successfully transcribing the text but failing in a manner typical of CLIP, such as paying attention to common objects in the image rather than focusing on more specific elements. This issue was more pronounced in the unscripted LN-COCO dataset, where the abundance of details often confuses the CLIP model, resulting in the poor performance of Zero-Shot pipeline model. The shared prompting technique is effective in ameliorating the issues by establishing a connection between different modalities and has shown to enable the model to comprehend long and detailed LN-COCO utterances as shown in Fig. 4."}, {"title": "Impact of Transcription Quality", "content": "We evaluated the model's performance by comparing using both generated transcriptions and ground truth captions. As shown in Table. 4, for the SpokenCOCO dataset, the model perform slightly better on the ground truth text compared to the transcriptions. However, for the LN-COCO dataset, the performance gap is significantly larger due to a higher error rate in the transcriptions. We also analyze the per-sample Word Error Rate (WER) in relation to its recall accuracy and fits it into a logistic regression model. The results clearly demonstrates a clear pattern where the per-sample recall is inversely related to the WER, and confirms that ASR errors can have a substantial impact on the model's performance, further highlighting the need of an additional audio channel which has been proved to have the capability to correct these mistakes."}, {"title": "Cross Dataset Evaluation", "content": "To test the generalizability of our fine-tuned model on different dataset, we evaluate all three full BrewCLIP models in a cross-dataset way. Table. 5 shows the models trained with Flickr8K and SpokenCOCO adapt each other very well. However, the models trained with LN-COCO fail to adapt to Flickr8K and even SpokenCOCO which shares the same image set. Non-finetuned CLIP text encoders struggle with processing very long sentences and our prompting design successfully addresses this limitation but, unfortunately, compromising its original capability of processing short sentences."}, {"title": "Conclusion", "content": "In this work, we argue that current pipeline-based approaches for audio-image retrieval, while achieving remarkable performance, inevitably lose critical audio-specific information during the transcribing process. To address this issue, we propose a novel dual-channel architecture that effectively learns a compact joint representation between speech and image. Our study not only establishes a robust baseline for zero-shot pipeline models but also pushes the boundaries of the current state-of-the-art. Furthermore, we run comprehensive experiments evaluating various types of model on different types of dataset, elucidating the importance of employing a dual-channel model to build a more comprehensive image-audio joint representation."}, {"title": "Limitation", "content": "Our primary limitation lies in the inability to directly demonstrate that our audio-image joint embedding can capture the shared non-textual connection between image-audio pairs, due to the absence of a suitable dataset. Also, our model faces constraints imposed by the pre-trained models used. The CLIP text encoder can only process at most 77 tokens while some LN-COCO samples extend up to 2 minutes, exceeding the 77-token limit when transcribed. This results in the truncation of potentially important information. Also, CLIP image transformation is also a potential avenue for enhancement as discussed above."}, {"title": "Detailed Implementation", "content": "For the ASR model, we apply base (74M) model of Whisper model. Since the Whisper model is frozen entirely, we save the transcriptions as the intermediate output to avoid the need for inference every epoch.\nAs for text-image model, we opt for ViT-L/14 (427M) as our pre-trained CLIP model, since the transformer-based architecture of ViT-L/14 facilitates the use of prompting for fine-tuning.\nFollowing the standard of SpeechCLIP, all models are trained with Adam optimizer with a weight decay of 10-6, batch size of 32. The maximum training step is set to 100k steps in total. The learning rate linearly increases to 10\u20134 in the first 5k steps and linearly decreases to 10-8. The hyperparameter to control the contribution of the inner and outer losses, \u03b1 is set to be 0.1 and prompt length, m is set to be 4. All experiments can be conducted on three NVIDIA RTX A4000 GPUs\nThe attention map is generated by computing the gradient-updating direction of the penultimate layer of each module.\nCorruption of Audio Samples Some audio files either contain portion of corrupted signal, which actually causes the ASR model to crash and therefore, not able to generate transcriptions successfully, especially in the Loconarr-COCO dataset. There are 21 audio samples, out of 8573 samples, that cause the ASR model to crash in the LocoNarr-COCO validation dataset, and therefore we simply drop them."}]}