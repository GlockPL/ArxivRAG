{"title": "DISCOVERING MESSAGE PASSING HIERARCHIES FOR\nMESH-BASED PHYSICS SIMULATION", "authors": ["Huayu Deng", "Xiangming Zhu", "Yunbo Wang", "Xiaokang Yang"], "abstract": "Graph neural networks have emerged as a powerful tool for large-scale mesh-based\nphysics simulation. Existing approaches primarily employ hierarchical, multi-scale\nmessage passing to capture long-range dependencies within the graph. However,\nthese graph hierarchies are typically fixed and manually designed, which do not\nadapt to the evolving dynamics present in complex physical systems. In this pa-\nper, we introduce a novel neural network named DHMP, which learns Dynamic\nHierarchies for Message Passing networks through a differentiable node selection\nmethod. The key component is the anisotropic message passing mechanism, which\noperates at both intra-level and inter-level interactions. Unlike existing methods, it\nfirst supports directionally non-uniform aggregation of dynamic features between\nadjacent nodes within each graph hierarchy. Second, it determines node selection\nprobabilities for the next hierarchy according to different physical contexts, thereby\ncreating more flexible message shortcuts for learning remote node relations. Our\nexperiments demonstrate the effectiveness of DHMP, achieving 22.7% improve-\nment on average compared to recent fixed-hierarchy message passing networks\nacross five classic physics simulation datasets.", "sections": [{"title": "INTRODUCTION", "content": "Simulating physical systems with deep neural networks has achieved remarkable success due to their\nefficiency compared with traditional numerical solvers. Graph Neural Networks (GNNs) have been\nvalidated as a powerful tool for mesh-based physical scenarios, such as fluids and rigid collisions (Wu\net al., 2020). The primary mechanism driving the GNN-based models is message passing (Sanchez-\nGonzalez et al., 2020; Pfaff et al., 2021; Allen et al., 2023). In this process, time-varying physical\nquantities are encoded within the mesh structure at each time step, enabling each node to update\nits features by aggregating information broadcast from neighboring nodes. These existing methods\ngenerally rely on local message passing, which limits their ability to propagate influence over\nlong distances. A common solution involves using multi-scale graph structures to facilitate direct\ninformation shortcuts between distant nodes (Lino et al., 2022; Cao et al., 2023; Yu et al., 2024; Han\net al., 2022; Fortunato et al., 2022).\nHowever, as illustrated in Table 1, these approaches typically depend on heuristic methods to\ncreate coarser message passing structures using predefined graphs (Cao et al., 2023; Yu et al.,\n2024) or downsample the nodes based on spatial proximity (Lino et al., 2022). These hierarchies are\npreprocessed in one pass before training. This results in fixed graph hierarchies over the entire physical\nsequence that do not account for the diverse range of physical contexts; while in practical systems like\nturbulence, despite identical boundary conditions, even minor changes in initial conditions can lead\nto significant differences in subsequent dynamics. Moreover, the spatial correlations in a physical\nprocess can evolve over time, making static GNN hierarchies insufficient for accommodating the\ntime-varying node interactions.\nTo tackle the above challenge, we propose a novel deep learning approach named Dynamic Hierar-\nchical Message Passing (DHMP), which constructs context-aware and temporally evolving graph\nhierarchies based on the original mesh topology and the input physical quantities. The key insight is to\ndevelop a differentiable node selection method that allows for flexible modeling of node interactions."}, {"title": "PRELIMINARIES", "content": "Message passing. We consider simulating mesh-based physical systems, where the task is to\npredict the dynamic quantities of the mesh at future timesteps given the current mesh configuration.\nA mesh-based system is represented as a bi-directed graph G = (V, E)\u00b9, where V and E denote the\nset of nodes and edges, respectively. Message passing neural networks (MPNNs) compute the node\nrepresentations by stacking multiple message passing layers of the form:\nEdge update: \u00eaij = \u03c6(eij, Vi, vj); Node update: v\u2081 = \u00a2\u00ba (vi, \u03c8 ({\u0109ij | \u2200j, eij \u2208 \u03b5})), (1)\nwhere vi is the feature of node vi \u2208 V and \u03c8 denotes a non-parametric aggregation function. The\nfunction de updates the features of edges based on the endpoints, while u updates the node states\nwith aggregated messages from its neighbors. In existing mesh-based simulation methods, multi-layer\nperceptrons (MLPs) with residual connections are commonly employed for (.) and (\u00b7), with the\naggregation function (\u00b7) being defined as the sum of edge features. Notably, since the aggregation\nfunction treats all neighbors equally, the contributions from neighboring nodes may be averaged out,\nand the repeated message-passing process can further dilute distinctive node features. This issue is\nexacerbated in dynamic physical systems, where transferring directed patterns is crucial.\nHierarchical MPNNs. To facilitate long-range modeling, hierarchical MPNNs process information\nat L scales by creating a graph for each level and propagating information between them (Lino et al.,\n2022; Fortunato et al., 2022; Cao et al., 2023; Yu et al., 2024). Let G\u2081 = (V1, E1) represent the"}, {"title": "\u041c\u0415\u0422\u041dOD", "content": "In this section, we introduce the Dynamic Hierarchical Message Passing Networks (DHMP), a fully\ndifferentiable model that learns to dynamically generate coarser graphs over the sequence while\nsimultaneously learning to simulate the physical system over the learned hierarchical graphs."}, {"title": "OVERVIEW", "content": "Figure 1 demonstrates an overview of the proposed model, which operates in an encode-process-\ndecode pipeline. The encoder first maps the input field to a latent feature space V1 = {vi|Vi \u2208 V\u2081}\nat the original mesh resolution. Subsequently, we model the physical dynamics across the learned\nmulti-scale graph hierarchies with adaptive graph structures. To enhance the propagation of long-term\ndependencies between distant nodes, we propose an anisotropic message passing (AMP) mechanism,\nwhich is largely inspired by the directed nature of significant dynamic patterns.\nIn Section 3.2, we present the details of the AMP layer. In Section 3.3, we discuss the approach for\nlearning context-aware graph hierarchies. In Section 3.4, we describe the inter-level downsampling\nand upsampling processes that incorporate AMP-based feature propagation. Finally, in Section 3.5,\nwe outline the implementation details and hyperparameter choices."}, {"title": "ANISOTROPIC MESSAGE PASSING", "content": "We introduce the AMP layer, which facilitates information propagation both within and between graph\nhierarchies, enabling DHMP to effectively capture local and long-range dependencies simultaneously.\nAs shown in Eq. 1, a common method in mesh-based simulation is to use the summation aggregation\nfunction for node update: v\u2081 = $\" (vi, \u03a3vj Nu\u0108ij), where vj \u2208 Nv\u2081 is a neighboring node of vi\nin the graph if eij \u2208 E. Using the summation aggregation has two drawbacks: i) it can excessively\nsmooth the neighboring features, potentially failing to capture intricate local relations, as discussed in\nprevious literature (Alon & Yahav, 2021; Dong et al., 2023; Dwivedi et al., 2022), and ii) it does not\naccount for the directed nature that can be inherent in physics scenarios.\nTo address these issues, we propose the AMP layer, which employs a more flexible aggregation\nfunction to facilitate anisotropic feature propagation within each message-passing hierarchy. Instead\nof directly summing the edge features, AMP exploits learnable parameters w : RF \u2192 R to predict\nthe importance weight of edge feature \u00eaij to node vi:\nwij = $(eij, Vi, Vj). (2)\""}, {"title": "DIFFERENTIABLE MULTI-SCALE GRAPH CONSTRUCTION", "content": "With the AMP layer functioning within each graph level, local dependencies are effectively propagated\nthroughout the high-resolution graphs, guiding the selection of nodes to be discarded in the next\nhierarchy for improved long-range modeling. We now delve into the details of the differentiable node\nselection method (DiffSELECT) for hierarchical graph construction.\nIn the DiffSELECT operation, we train the node update module 4\u00ba based on anisotropic aggregated\nedge features to produce a probability pi for each node. This probability indicates the likelihood of\nretaining node vi in the next-level coarser graph G1+1. Accordingly, we rewrite Eq. 4 as follows:\n= \u03c8(Vi, \u03a3\u03b1ij\u00eaij). (5)\nVj ENvi"}, {"title": "INTER-LEVEL FEATURE PROPAGATION WITH AMP", "content": "During the downsampling process from G\u012b to the generated coarser graph G1+1, as illustrated in\nFigure 1, the REDUCE operation aggregates information to each node in Vi+1 from its corresponding\nneighbors in Vi. Conversely, the EXPAND operation unpools the reduced graph back to a finer\nresolution, delivering the information of the pooled nodes to their neighbors at the finer level.\nPrior works employed non-parametric aggregation in inter-level propagation, convolving features\nbased on the normalized node degree. It simplifies intricate relationships between nodes and neglects\nthe directional aspects of information flow. In comparison, the inter-level aggregation weights in\nDHMP are data-specific and time-varying. Notably, the importance weight a\u00a1 in the proposed\nAMP layer inherently captures the significance of node vj's features to node vi at the graph level l.\nConsequently, it can be directly reused for the REDUCE and EXPAND operations in the downsampling\nand upsampling processes. We provide details of these operations as follows:\nlij\n\u2022 REDUCE: Let vi be the node at the coarser graph level. The downsampling process aggregates the\ninformation of the current neighbors N\u2081 by reusing the weight aj: v1+1 \u2190 \u03a3jen; \u03b1\u03bd\n\u2022 EXPAND: We first unpool the node features from the next-level coarser graph G1+1. To achieve this,\nwe record the nodes selected during the downsampling process and use this information to place\nnodes back in their original positions in the graph. Next, we re-use the importance weight a to\nassign features in the coarser graph to nodes in the finer graph, i.e., v\u2190 jen vaij\n\u2022 FeatureMixing: Following the EXPAND operation, DHMP conducts an additional message\npassing step based on v. It then integrates the resulting features with the intra-level message\npassing outcomes in G\u0131 (before downsampling) through a skip connection."}, {"title": "IMPLEMENTATION DETAILS", "content": "We train DHMP using the one-step supervision that measures the L2 loss between the ground truth\nand the next-step predictions. We include detailed descriptions of the physical quantities represented"}, {"title": "EXPERIMENTS", "content": "Datasets. We evaluate our approach on five mesh-based physics simulation benchmarks established\nin previous literature (Pfaff et al., 2021; Cao et al., 2023; Wu et al., 2023; Narain et al., 2012).\n\u2022 CylinderFlow: Simulation of incompressible flow around a cylinder based on 2D Eulerian meshes.\n\u2022 Airfoil: Aerodynamic simulation around airfoil cross-sections based on 2D Eulerian meshes.\n\u2022 Flag: Simulation of flag dynamics in the wind based on Lagrangian meshes with fixed topology.\n\u2022 DeformingPlate: Deformation of hyper-elastic plates based on Lagrange tetrahedral meshes.\n\u2022 FoldingPaper: Deformation of paper sheets on Lagrangian meshes, with varying forces at the four\ncorners and evolving mesh graph.\nFor details regarding the datasets, including descriptions of the input physical quantities, please refer\nto Appendix A. Additional information concerning our implementation can be found in Appendix B."}, {"title": "MAIN RESULTS", "content": "Standard benchmarks. Table 2 presents the root mean squared error (RMSE) of one-step predic-\ntion (RMSE-1) and long-term rollouts for 100\u2013600 future time steps (RMSE-all). DHMP consistently\noutperforms the compared models across all benchmarks. This demonstrates the effectiveness of\nbuilding context-aware, temporally evolving hierarchies with learnable, directionally non-uniform"}, {"title": "RELATED WORK", "content": "Learning-based physics simulation. Recent literature has shown that learning-based simulators\ncan efficiently handle complex and high-dimensional problems, such as fluid dynamics (Zhu et al.,\n2024), structural analysis (Kavvas et al., 2018; Thai, 2022), and climate modeling (Kurth et al.,\n2018; Rasp et al., 2018; Rolnick et al., 2022). The models can be roughly categorized into three\ngroups based on data representation: those modeling partial differential equations (Raissi et al.,\n2017; 2019; Lu et al., 2019; Li et al., 2021; Wang et al., 2021), particle-based systems (Li et al.,\n2019; Sanchez-Gonzalez et al., 2020; Ummenhofer et al., 2020; Prantl et al., 2022), and mesh-based\nsystems (Pfaff et al., 2021; Lino et al., 2022; Fortunato et al., 2022; Cao et al., 2023). The rapid\ninference time and differentiable property of these models greatly facilitate downstream tasks, such\nas inverse design (Wang & Zhang, 2021; Goodrich et al., 2021; Allen et al., 2022; Janny et al., 2023).\nGNN-based physics simulation. Previous work has explored GNNs in various physical domains,\nsuch as articulated systems (Sanchez-Gonzalez et al., 2018), soft-body deformation and fluids (Li\net al., 2019; Mrowca et al., 2018; Sanchez-Gonzalez et al., 2020; Rubanova et al., 2022; Wu et al.,\n2023), rigid body dynamics (Battaglia et al., 2016; Li et al., 2019; Mrowca et al., 2018; Bear et al.,\n2021; Rubanova et al., 2022), and aerodynamics (Belbute-Peres et al., 2020; Hines & Bekemeyer,\n2023; Pfaff et al., 2021; Fortunato et al., 2022; Cao et al., 2023). Among them, MGN (Pfaff et al.,\n2021) is a key method that models mesh-based dynamics through graph interactions. Subsequent\napproaches primarily focus on enhancing modeling capabilities and reducing computational costs.\nHierarchical GNNs for physics simulation. Hierarchical GNNs employ multi-scale graph struc-\ntures (Lino et al., 2022; Han et al., 2022; Fortunato et al., 2022; Allen et al., 2023; Janny et al., 2023;\nCao et al., 2023; Yu et al., 2024) to decrease overhead by using fewer nodes at coarser levels and\nenabling long-range feature propagation. GMR-Transformer-GMUS (Han et al., 2022) employs a\nuniform sampling pooling method to select pivotal nodes. MS-MGN (Fortunato et al., 2022) uses\na dual-level hierarchical GNN and performs message passing at both fine and coarse resolutions.\nHierarchical GNNs with multi-level structures Lino et al. (2022); Cao et al. (2023); Yu et al. (2024)\nare most relevant to our approach, as they integrate message passing neural networks within the\nU-Net architecture (Ronneberger et al., 2015). Lino et al. (Lino et al., 2022) uses manually set\ngrid resolutions and spatial proximity for graph pooling, which requires predefined parameters.\nBSMS-GNN (Cao et al., 2023) introduces a bi-stride pooling strategy that pools nodes on alternating\nbreadth-first search frontiers while enhancing edges with two-hop connections. HCMT (Yu et al.,\n2024) refines the structure further by applying Delaunay triangulation to bi-stride nodes. Notably,\nthese methods construct multi-level structures as preprocessing and cannot change the graph hierar-\nhies under varying physical conditions. Moreover, they typically use uniform feature aggregation\nfor intra-level propagation, which may hinder the directed transfer of significant dynamic patterns, or\nuse attention-based aggregation, which increases computational overhead. Furthermore, inter-level\npropagation is often predefined or unlearnable, limiting flexibility in transferring information across\nhierarchy levels. In contrast, our model generates context-aware and temporally evolving graph\nhierarchies and incorporates learnable anisotropic feature propagation, allowing it to better adapt to\nvarious initial conditions and rapidly changing dynamic systems."}, {"title": "CONCLUSIONS AND LIMITATIONS", "content": "In this paper, we introduced DHMP, a neural network that significantly advances the state-of-the-art\nin mesh-based physics simulation. Our key innovation is dynamically creating the context-aware\ngraph structures of hierarchical GNNs through a differentiable node selection process. To this end,\nwe proposed an anisotropic message passing mechanism to enhance the propagation of long-term\ndependencies between distant nodes, aligning with the directed nature of significant dynamic patterns.\nExtensive experiments show that DHMP outperforms existing models, especially those with fixed\ngraph hierarchies, in both short-term and long-term predictions.\nA potential limitation of this work is the need to improve the interpretability of the learned hierarchy\nstructure. Additionally, we would consider incorporating specific physical priors into DHMP to\nfurther enhance the model's robustness and generalizability, particularly in resolution-free problem\nsettings, which have been less explored in existing mesh-based approaches."}]}