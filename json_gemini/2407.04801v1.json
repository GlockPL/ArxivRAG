{"title": "Revisiting Structured Sentiment Analysis\nas Latent Dependency Graph Parsing", "authors": ["Chengjie Zhou", "Bobo Li", "Hao Fei", "Fei Li", "Chong Teng", "Donghong Jil"], "abstract": "Structured Sentiment Analysis (SSA) was cast\nas a problem of bi-lexical dependency graph\nparsing by prior studies. Multiple formula-\ntions have been proposed to construct the graph,\nwhich share several intrinsic drawbacks: (1)\nThe internal structures of spans are neglected,\nthus only the boundary tokens of spans are used\nfor relation prediction and span recognition,\nthus hindering the model's expressiveness; (2)\nLong spans occupy a significant proportion in\nthe SSA datasets, which further exacerbates the\nproblem of internal structure neglect. In this\npaper, we treat the SSA task as a dependency\nparsing task on partially-observed dependency\ntrees, regarding flat spans without determined\ntree annotations as latent subtrees to consider\ninternal structures of spans. We propose a two-\nstage parsing method and leverage TreeCRFs\nwith a novel constrained inside algorithm to\nmodel latent structures explicitly, which also\ntakes advantages of joint scoring graph arcs\nand headed spans for global optimization and\ninference. Results of extensive experiments on\nfive benchmark datasets reveal that our method\nperforms significantly better than all previous\nbi-lexical methods, achieving new state-of-the-\nart.", "sections": [{"title": "Introduction", "content": "Structured Sentiment Analysis (SSA) aims to ex-\ntract the complete opinion tuple from a sentence.\nAs shown in Figure 1(a), the complete opinion tu-\nple includes an opinion expression e with sentiment\npolarity p, an opinion holder h, and the correspond-\ning target t. Given the complexity of detecting\nthree items and classifying one, SSA presents more\nchallenges than other related tasks, such as Opinion\nMining (Katiyar and Cardie, 2016; Xia et al., 2021),\nABSA (Aspect-based Sentiment Analysis) (Pon-\ntiki et al., 2014, 2016; Wang et al., 2016), TOWE\n(Target-oriented Opinion Words Extraction) (Fan\net al., 2019; Mao et al., 2021), ASTE (Aspect Sen-\ntiment Triplet Extraction) (Peng et al., 2020; Mao\net al., 2021; Zhai et al., 2022; Fei et al., 2022a; Li\net al., 2023, 2024), etc.\nRecent works of SSA mainly cast it as a problem\nof bi-lexical dependency graph parsing and propose\nmultiple formulations: (1) Barnes et al. (2021) pro-\nposed formulations namely head-first/head-final as\nillustrated in Figure 1(b). Their method cannot\nresolve the problem because head-first/head-final\ntreats the first/final word as the head of the span and\nstrictly restricts any word inside the span directly\nhead to span head, which decreases the height of\nthe converted trees to 2 and excludes the latent\nstructures completely. (2) Another label strategy\nwas proposed by Shi et al. (2022), which simplifies\nthe label set to only arcs linking spans boundaries,\nas shown in Figure 1(c). Despite the special label\nfor discontinuous span decoding, Zhai et al. (2023)\nutilize the same label set. Without distinct formu-\nlation about inside words, they attempt to utilize\nthe powerful neural models like Graph Attention\nNetwork (Velickovic et al., 2018) or Axial-based\nAttention Network (Huang et al., 2019; Wang et al.,\n2020) to implicitly encoder the inside structure in-\nformation, which is found lagging behind large\nwith explicitly modeling with graph-based parsing\nmethods (Wang and Tu, 2020; Fonseca and Martins,\n2020; Zhang et al., 2020; Yang and Tu, 2022a). It is\nevident that previous work does not address the key\nchallenge focused on the prediction of boundary\nwords (First/Final/Both) of spans, and neglect the\nwords and structures inside spans, which hinders\nthe model expressiveness seriously.\nShould we neglect the structure inside spans as\nprevious works have done? Table 1 list the statistics\nof span length and the max length of spans in the\nbenchmark datasets respectively. We present a real\nexample to illustrate our point, focusing on the ex-\npression \"conceded\" and the target \"US President\nlast year\" for brevity."}, {"title": "Related Work", "content": "As a key topic in the Sentiment Analysis commu-\nnity (Fei et al., 2023a; Wu et al., 2021), Structured\nSentiment Analysis (SSA) encompasses several\nsub-tasks, each targeting a specific component of\nthe goal tuple (holder, target, expression, polarity).\nSSA also involves closely with structure predic-\ntions (Fei et al., 2022c) and relevant tasks (Fei\net al., 2023b; Wu et al., 2023a,b).\nOM Opinion Mining primarily aims to extract\nthe (h, t, e) tuple. Existing OM studies generally\nadopt one of two approaches: 1) BIO-based ap-\nproach, which views OM as a sequence labeling\ntask (Katiyar and Cardie, 2016); and 2) span-based\napproach, which jointly predicts all span pairs and\ntheir interrelations (Xia et al., 2021). Additionally,\nZhang et al. (2019); Wu et al. (2022) introduced\na transition-based model for OM. However, these\nmethodologies neglect the sentiment polarity clas-\nsification sub-task.\nABSA Aspect-Based Sentiment Analysis is an-\nother important sentiment analysis task. Various\nmethods have been proposed to address ABSA,\nincluding: 1) Pipeline (Peng et al., 2020), which se-\nquentially predicts spans and their relationships; 2)\nEnd-to-End (Chen and Qian, 2020), utilizing inter-\nactive information from each pair of sub-tasks; and\n3) MRC (Mao et al., 2021; Zhai et al., 2022), which\nemploy a machine reading comprehension frame-\nwork to extract triplets. However, these method-"}, {"title": "SSA as Latent Graph Parsing Scheme", "content": "In the formulation of SSA as Latent Dependency\nGraph, we treat each sentiment span as latent tree.\nTo build the Dependency Graph, we deal with each\nexpression span separately and assumes each of\nthem corresponds to a single-root tree. In this tree,\nthe sentiment head word of the expression span\nserves as the dependency root, with each subtree\nof the corresponding holder/target span attaching\nto it. Consequently, our proposed latent depen-\ndency parsing task can be divided into two sub-\ntasks: (1) expression extraction; (2) corresponding\nholder/target extraction. Note that both of these\nsubtasks are solved by a consistent graph-based\nand headed-span-based parsing method, trained\njointly and decoded step-by-step, which is named\nas Two-stage Parsing 1."}, {"title": "Conversion and Training on Latent Tree", "content": "Formally, given an input sentence x = x1,..., Xn,\nour object is to obtain the corresponding tree struc-\ntures for each expression e \u2208 E and compose them\nto construct the Latent Dependency Graph ulti-\nmately.\nA directed dependency tree t is defined by as-\nsigning a head h\u2208 {x0,x1,..., xn}, accompanied\nby a relation label l\u2208 L to each modifier mex.\nHere, x0 is typically positioned before \u00e6, serving\nas the root node.\nFor an expression e \u2208 E within a consecutive\nword span xi,...,xj and assigned a sentiment\nlabel l\u2208 L, we constraint all potential subtrees\nwithin this span to be single-rooted at a potential"}, {"title": "Decoding and Recovery on Latent Tree", "content": "Assuming that we have trained a parser, our\nnext step involves recovering expression and cor-\nresponding holder/target structures after decod-\ning/parsing the highest-scoring dependency tree.\nWe give illustration in Figure 2(c), we initially\nidentify all expression spans by obtaining the\nhighest-scoring expression tree t* rooted at xo\nthrough our parsing method:\n$$t^* = \\arg \\max_t s(x, t)$$\nFollowing this, we determine the highest-scoring\ncorresponding tree t** for e using the same algo-\nrithm:\n$$t^{**} = \\arg \\max_{t:x_0 \\in e \\in t^*} s(x, t)$$\nwhere s(x, t) represents the score of the tree, with\np denoting the polarity label of expression spans\nrelative to xo. The tree is constrained to have its\nroot in one of the words in e. We then recover cor-\nresponding holder/target spans of the expression\nby transforming all subtrees headed by e into flat\nspans. If the label I of the dependency e \u2192 h is not\n\"\u00d8\" (indicating irrelevant spans), then a complete\nspan is formed, comprising h and its descendants,\nand is assigned l as its sentiment label. The fi-\nnal SSA output consists of a compilation of all\nrecovered expression spans and their respective\nholder/target spans."}, {"title": "Methodology", "content": "Following previous work on dependency parsing\n(Dozat and Manning, 2017; Zhang et al., 2020;\nYang and Tu, 2022a), our model consists of a con-\ntextualized encoder and scoring modules. We fur-\nther propose a constraint TreeCRF to compute the\nprobabilities of the partially-observed trees of SSA."}, {"title": "Encoder", "content": "For a given sentence x = X1,X1,...,Xn, we in-\ntroduce special tokens <bos> and <eos> as xo and\nXn+1, respectively. The vector representation for\neach token xi \u2208 \u00e6 is an amalgamation of five dis-\ntinct components:\nei =\n[eword; vord; elemma; epos; echar; BERT\nIn this composition, eword, epos, and elemma repre-\nsent word, part-of-speech (POS), and lemma em-\nbeddings, respectively. echar is derived from the"}, {"title": "Tree Scoring", "content": "We decompose a tree t into two distinct compo-\nnents: y, representing an unlabeled skeletal tree,\nand I, signifying the corresponding sequence of la-\nbels. The process of scoring an unlabeled skeletal\ntree involves the aggregation of arcs and head-span\nscores. For each head-modifier pair h\u2192 m\u0454\u0443,\nwe score them using two MLPs followed by a Bi-\naffine layer (first-order scorer on arcs in the tree):\n$$r_i^{\\text{head/mod}} = \\text{MLp}^{\\text{head/mod}}(h_i)$$\n$$S_{h \\rightarrow m}^{\\text{arc}} = [r_h^{\\text{head}}; 1]^T W [r_m^{\\text{mod}}; 1]$$\nThe scoring of the dependency h \u2192 m with label\nl\u2208 Lis calculated in a similar manner. We use two\nadditional MLPs and |L| Biaffine layers to compute\nall label scores.\nEnhancing the first-order biaffine parser for the\nunlabeled tree, we leverage adjacent-sibling infor-\nmation as mentioned in McDonald and Pereira\n(2006) and headed-span information as described\nin Yang and Tu (2022b). Additional MLPs and\nbiaffine/ triaffine layers are included to perform the\nscoring,\n$$r_i^{\\text{head/mod/sib}} = \\text{MLp}^{\\text{head/mod/sib}}(h_i)$$\n$$S_{h \\rightarrow s, m}^{\\text{sib}} = \\text{TriAff}(r^{\\text{sib}}, r_h^{\\text{head}}, r_m^{\\text{mod}})$$\n$$r_i^{\\text{left/right/head}} = \\text{MLp}^{\\text{left/right}}(c_i), \\text{MLp}^{\\text{head}}(h_i)$$\n$$S_{k \\rightarrow i/j}^{\\text{left/right}} = [r_{i/j}^{\\text{left/right}}; 1]^T W [r_k^{\\text{head}}; 1]$$"}, {"title": "Inside Algorithm", "content": "The calculation of the partition function Z(x) in\nEq. (10) can be resolved by the Inside algorithm\nof TreeCRF. Follow (Zhang et al., 2022), we loga-\nrithm the scores and define the labeled tree score\nas:\ns(x,t) = s(x, y) + log P(l | x, y)\nConsequently, the score represents the summation\nof the exponential scores of all legal labeled trees,\nas the logarithmic label probability of illegal trees\nis set to 0 in our conversion formulation.\nTo enumerate legal trees, we introduce con-\nstraints to the Inside Algorithm as proposed by\nEisner (1997); Li et al. (2016)\u00b2. The constraints\nare categorized into two groups, each defined by\nits specific purpose: (1) To prevent the arc h \u2192 m\nfrom crossing different spans, we apply a constraint\nto the rule (R-LINK), thereby prohibiting merging\nwith the relevant incomplete span Ih,m. (2) To pre-\nvent the presence of multiple headwords within\na single span, we restrict any word in expression\nspans e to merge solely with the completed span\nFh,i (R-COMB). Additionally, we permit a span\nto be considered complete only when i is posi-\ntioned at the endpoint of a span (R-FINISH). We\ndemonstrate the deduction rules via the parsing-as-\ndeduction framework (Pereira and Warren, 1983)\nin Appendix B, specifically in Figure 4. For addi-\ntional insights into the Eisner Algorithm, Appendix\nB provides further details."}, {"title": "Training Objective", "content": "During training, the objective is to maximize the\nprobability of tree Te and Th/t for each expression\ne \u2208 E. Consequently, we formulate the loss func-\ntion in the following manner:\n$$\\mathcal{L} = - \\log P(T_e | x) P(T_{h/t} | x)$$\nIn this equation, the term P(Te | x) and P(Th/t |\nx) are expanded as:\n$$P(T | x) = \\sum_{t \\in T} P(y | x) \\cdot P(l | x, y)$$\n$$= \\frac{1}{Z(x)} \\sum_{t \\in T} \\exp(s(x, y)) \\cdot P(l | x,y)$$"}, {"title": "Experiments", "content": "Following the previous work, we conduct exper-\niments on five benchmark datasets in four lan-\nguages. NoReCFine (\u00d8vrelid et al., 2020) is a multi-\ndomain professional reviews dataset in Norwegian.\nMultiBEU and MultiBCA (Barnes et al., 2018) are\nannotated hotel views in Basque and Catalan, re-\nspectively. MPQA (Wiebe et al., 2005)contains En-\nglish news and the main content of DSUnis (Toprak\net al., 2010) is online university reviews in English\nas well."}, {"title": "Baselines", "content": "We compare our proposed method with six state-of-\nthe-art baselines. RACL-BERT (Chen and Qian,"}, {"title": "Evaluation Metrics", "content": "Following the previous work (Zhai et al., 2023), we\nuse Holder F1, Target F1 and Exp. F1 for the to-"}, {"title": "Main Results", "content": "Table 2 shows the comparison of our method\nagainst other baselines across multiple evaluation\nmetrics. In terms of the Span F1 metric, our method\ndemonstrates superior performance on all datasets,\nincluding a notable 7.2% F1 score increase in\nholder extraction on the MPQA dataset. Further-\nmore, when evaluating the Sentiment Graph metric,\ndesigned to assess both span extraction and relation\nprediction accuracy, our method consistently out-"}, {"title": "Efficiency Comparison", "content": "Table 3 presents a comparative analysis of differ-\nent models based on their processing speeds. The\nspeed metrics for previous works were derived by\nrerunning their publicly available code. Our mod-\nels, the first-order scoring only method and the\ncomprehensive scoring method, demonstrate supe-\nrior performance, processing approximately 212\nand 174 sentences per second, respectively. This\nrate significantly surpasses that of prior models.\nTGLS (Shi et al., 2022) and USSA (Zhai et al.,\n2023) employ deeper networks and engage in com-\nputationally intensive tasks, which contributes to\ntheir slower processing speeds."}, {"title": "Discussion", "content": "SSA as a latent tree formulation effectively ad-\ndresses not only latent tree modeling but also the\nmanagement of overlap and discontinuous scenar-\nios, as highlighted by Zhai et al. (2023). We offer a\nsuccinct proof as follows: (1) Overlap Case: Our\napproach handles overlaps efficiently by treating\neach expression separately. This allows different\nexpressions to share the same target/holder without\nconflict in our framework. (2) Discontinuous Case:\nThis more complex scenario, predominantly found\nin expressions\u00b3, is also addressed in our scheme."}, {"title": "Does the latent tree structure benefit for long spans/tuples?", "content": "We address the question through results from two\nkey experiments: (1) Experiment on NoReCFine\ninvestigates the model's performance in\nlonger spans/tuples. The Expression F1 and Tu-\nple F1 metrics indicate our method's superiority\nover baselines, with an increasing performance gap"}, {"title": "Can SSA as latent tree formulation handle overlap and discontinuous cases?", "content": "6.1 Can SSA as latent tree formulation handle\noverlap and discontinuous cases?\nSSA as a latent tree formulation effectively ad-\ndresses not only latent tree modeling but also the\nmanagement of overlap and discontinuous scenar-\nios, as highlighted by Zhai et al. (2023). We offer a\nsuccinct proof as follows: (1) Overlap Case: Our\napproach handles overlaps efficiently by treating\neach expression separately. This allows different\nexpressions to share the same target/holder without\nconflict in our framework. (2) Discontinuous Case:\nThis more complex scenario, predominantly found\nin expressions\u00b3, is also addressed in our scheme."}, {"title": "Conclusion", "content": "In this study, we approach Structured Sentiment\nAnalysis (SSA) through latent dependency graph\nparsing, conceptualizing flat sentiment spans as\nlatent subtrees. We introduce an innovative pars-\ning methodology grounded in TreeCRF, designed\nto effectively integrate span structures. Our ex-\nperimental findings demonstrate that this method\nsurpasses all previous approaches across five bench-\nmark datasets. Comprehensive analyses validate\nthe efficacy and consistency of our method in en-\nhancing SSA."}, {"title": "Acknowledgments", "content": "This work is supported by the National Natural\nScience Foundation of China (No. 62176187)."}, {"title": "Limitations", "content": "We propose a two-stage parsing method to model\nthe converted latent tree derived from the origi-\nnal SSA structure. The experiments demonstrate\nthat our method outperforms the previous meth-\nods on benchmark datasets and proves its temporal\nefficiency. However, inevitable error propagation\noccurs in our decoding stage due to the sequential\nnature of the two-stage decoding process. Another\nchallenge is that considering each expression span\nseparately to construct their corresponding trees\nincurs increased space requirements for storing in-\ntermediate results."}, {"title": "Ethics Statement", "content": "Our work on the Structured Sentiment Analysis\n(SSA) as Latent Graph Parsing Scheme adheres to\nethical guidelines emphasizing transparency, fair-\nness, and responsible AI development. We rec-\nognize the ethical implications of this work and\nhave conducted our research with a commitment\nto minimizing biases, ensuring data privacy, and\npromoting the explainability of AI decisions. Our\nevaluations utilized publicly available or ethically\nsourced datasets, and we have made efforts to ad-\ndress and mitigate potential biases within these\ndatasets to ensure fairness and objectivity in our\nfindings.\nThe broader impact of Latent Graph Parsing\nScheme, aimed at improving sentiment analysis\nfor a more complete conceptualization, has the\npotential to contribute positively to various fields,\nincluding negation resolution, uncertainty/hedge\ndetection, and event extraction. By introducing\na graph parsing method, we foster more faithful,\nflexible, and explainable sentence-level sentiment\nanalysis capabilities. We encourage the responsible\nuse of our findings and technologies, and we com-\nmit to ongoing evaluation of our work's societal\nand ethical implications."}, {"title": "Algorithm Implement Details", "content": "For fair comparison, we obtain the contextual to-\nken representations from the pre-trained BERT-\nmultilingual-base and word2vec skip-gram embed-\ndings used by previous work (Barnes et al., 2021).\nFurthermore, we use 4-layer BiLSTMs to encoder\nthe sentence, with an output size of 768 and the\ndropout rate is set to 0.3. We train our model for\n60 epochs and save the model parameters based on\nthe highest SF1 score on the development set. We\nconduct training and testing on a NVIDIA A100\nServer. The reported results are the averages from\nfive runs with different random seeds. Our pro-\nposed model statistically outperforms the baselines\nat p < 0.05."}, {"title": "Inside algorithm", "content": "We give the pseudocode of the modified Inside\nalgorithm (Yang and Tu, 2022a) in Alg. 1 as well\nas the illustration of deduction rules in Figure 4 to\nhelp understanding the algorithm. We highlight the\nconstraint rules in Figure 4, it is the only difference\nbetween our proposed one and the vanilla inside\nalgorithm.\nIn Line 3, the term Ci,i denotes the axiom items"}, {"title": "Comparing to Large Language Models", "content": "In Table 5, we present a comparative analysis of\nour method against ChatGPT-3.5-Turbo and GPT4.\nThe results indicate that on the English dataset,\nGPT4 surpasses the baseline performance in a zero-\nshot setting and shows further improvement with\nin-context learning (1-shot or more). However,\nGPT4 does not reach the performance levels of\nmore robust baselines such as TGLS and USSA,\nwhich are specifically trained for the SSA task."}]}