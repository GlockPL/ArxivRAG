{"title": "IRL for Restless Multi-Armed Bandits\nwith Applications in Maternal and Child Health", "authors": ["Gauri Jain", "Pradeep Varakantham", "Haifeng Xu", "Aparna Taneja", "Prashant Doshi", "Milind Tambe"], "abstract": "Public health practitioners often have the goal of monitoring\npatients and maximizing patients' time spent in \"favorable\" or healthy states\nwhile being constrained to using limited resources. Restless multi-armed\nbandits (RMAB) are an effective model to solve this problem as they are\nhelpful to allocate limited resources among many agents under resource\nconstraints, where patients behave differently depending on whether they are\nintervened on or not. However, RMABS assume the reward function is known.\nThis is unrealistic in many public health settings because patients face unique\nchallenges and it is impossible for a human to know who is most deserving\nof any intervention at such a large scale. To address this shortcoming, this\npaper is the first to present the use of inverse reinforcement learning (IRL) to\nlearn desired rewards for RMABS, and we demonstrate improved outcomes\nin a maternal and child health telehealth program. First we allow public\nhealth experts to specify their goals at an aggregate or population level and\npropose an algorithm to design expert trajectories at scale based on those goals.\nSecond, our algorithm WHIRL uses gradient updates to optimize the objective,\nallowing for efficient and accurate learning of RMAB rewards. Third, we\ncompare with existing baselines and outperform those in terms of run-time and\naccuracy. Finally, we evaluate and show the usefulness of WHIRL on thousands\non beneficiaries from a real-world maternal and child health setting in India.\nWe publicly release our code here: https://github.com/Gjain234/WHIRL.", "sections": [{"title": "1 Introduction", "content": "Our work is motivated by challenges faced by Armman, a maternal and child health\n(MCH) nonprofit in India. Armman delivers telehealth care to beneficiaries during\nand after their pregnancy. The telehealth program delivers weekly automated voice\nmessages (AVMs) with health information. Since some beneficiaries tend to stop lis-\ntening to the AVMs, Armman also provides human service calls (HSCs) to encourage\nbeneficiaries to keep listening to the AVMs. Unfortunately, the number of human\ncallers (few tens) is much lower than the number of beneficiaries (few thousands),\nrequiring optimal allocation of the limited HSCs to beneficiaries. Past work models\nthis as an RMAB [21] problem where each arm is a beneficiary [13] and has two\nstates: whether they have listened or not listened to the AVM last week, resulting\nin rewards 1 or 0 respectively. The actions at each time step (each week) are a subset\nof the beneficiaries chosen to receive HSCs.\nA drawback of this homogeneous binary reward design is that it leads to policies\nthat maximize the number of beneficiaries in the listened state but ignores the specific\ncircumstances of each beneficiary. After close interviews with members of Armman, we\nlearned that the algorithm was giving similar priority to mothers at very different risk\nlevels for pregnancy complication (Figure 4a), which is counterintuitive because lower\nrisk mothers have many resources and social networks they can rely on, so achieving\nhigh listening from them is significantly less important [10]. This motivated us to\nchange the current {0,1} rewards. But risk is just one metric public health experts use\nto make decisions for their beneficiaries, and we quickly found that there was no clear\nway to manually design rewards for all of their goals. Additionally, we found that\nmanual reward design methods can end up being even more unfair (Section 4.2). Fur-\nthermore, public health experts may want to alter interventions depending on factors\nthat change over time (e.g., education level, health risk). Therefore, we seek to learn re-\nwards for an RMAB that can capture goals of public health experts at any given time.\nThe above problem naturally fits IRL [16], which learns rewards from expert\ndemonstrations, thereby aligning the planner's sequential actions with desired out-\ncomes. However, our problem is different from classic IRL in two crucial aspects.\nFirst, classic IRL predominantly focuses on single-agent MDPs whereas our setup\nhas thousands of agents (beneficiaries) active simultaneously. Second, typical IRL\nalgorithms are designed to learn from full expert trajectories, but our application's\nlarge scale makes it impossible for a human expert to completely specify the entire\npolicy trajectories.\nOur work (Figure 1) addresses these shortcomings with the following contributions:\n(i) we design Whittle-IRL (WHIRL), the first IRL algorithm for learning RMAB\nrewards; (ii) we propose a novel approach for public health experts to specify aggregate"}, {"title": "3 Methodology", "content": "Preliminaries An RMAB is composed of N arms where each follows an independent\nMDP. Our state space is discrete with S = M states, and actions A = {0,1}\ncorresponding to not pulling or pulling the arm. $P_i(s,a,s'):S\u00d7A\u00d7S \u2192 P$ defines\nthe probability distribution of arm i in state s transitioning to next state $s' \u2208 S$.\n$R_i(s)$ is the reward function for arm i in state s. At each time step $h\u2208 [H]$, the\nplanner observes $s_h = [s_{h,i}]_{i\u2208[N]} \u2208S^N$, the states of all arms, and then chooses action\n$a_h = [a_{h,i}]_{i\u2208[N]} \u2208 A^N$ denoting the actions on each arms, which has to satisfy a budget\nconstraint $Ei\u2208 [N] a_{h,i} \u2264 K$. Once the action is chosen, arms receive actions $a_t$ and\ntransitions under P. The total reward is defined as $\\sum_{h=1}^{H}\\gamma^{h-1}\\sum_{i\u2208[N]}r_{h,i}$, where 0<\n$\\gamma$\u22641 is the discount factor. A policy is denoted by $\\pi$where $\\pi(a|s)$ determines actions\ntaken at joint state of arms. $\\pi_{learner}$ denotes the policy being learned in WHIRL.\nThe Whittle Index The dominant approach to solve the RMAB problem is the Whittle\nindex policy [22]. The Whittle Index evaluates the value of pulling an arm i via a 'pas-\nsive subsidy', i.e., a hypothetical compensation m rewarded for not pulling the arm (i.e.,\nfor action a=0). Given state $u \u2208 S$, we define the Whittle index associated to state u by:\n$W_i(u)=inf_m\\{Q_i^m(u;a=0)=Q_i^m(u;a=1)\\}\\quad\\quad(1)$\n$V_i^m (s)=max_aQ_i^m(s;a)\\quad\\quad(2)$\n$Q_i^m(s;a)=m1_{a=0}+R(s)+\\gamma\\sum_{s'}P_i(s,a,s')V_i^m(s')\\quad\\quad(3)$\nThe subsidy m is added for action a=0. The Whittle policy then selects arms to\npull: $\\pi_w(s)=1_{top-k}([W_i(s_i)]_{i\u2208[N]}) \u2208 \\{0,1\\}^N$.\nSoft Top K For our setting, we use a soft-top-k policy selection, which gives the proba-\nbility of pulling each arm. In addition to its differentiability property, soft-top-k is a bet-\nter policy for our setting since it captures the bounded rationality of human planners.\n$\\pi_{\u03c9}^{soft}(s) =soft-top-k([W_j(s_i)]_{i\u2208[N]}) \u2208 [0,1]^N\\quad\\quad(4)$\nIRL In IRL, the learner does not know the reward function R(s) in advance. Each arm\ni can have a different reward in each state. The goal is to learn rewards that best fit a set\nof J expert trajectories $T=\\{\u03c4_i\\}_{j\u2208J}$, i.e., maximize $P(T|\u03c0_{learner},\\hat{R})$ where $\\hat{R}$ is the\nreward being learned for all beneficiaries. We denote a full trajectory over H timesteps\nby $T=(s_1,a_1,\u2026,s_H,a_H)$, where s,a are the joint state and action of all N arms.\nProblem Statement Formally, we consider an RMAB setup where at timestep H\nfor trajectory \u03c4, a human expert gives an aggregate level directive: \"Move interven-\ntions from category $C_{h, A} = \\{arm|arm\u2208 f_{source}(h)\\}$ to $C_{h, B} = \\{arm|arm\u2208 f_{target}(h)\\}$.\"\nThese categories represent combinations of static features about beneficiaries like\ntheir income, education, language spoken, etc., and/or dynamic features like the\ncurrent state they are in or last time they were acted on. According to the directive,\nwe look at the past trajectory data and at each previous timestep h=1,\u2026,H, we\nallocate service calls from a subset of beneficiaries $B^{A}_{C_{h,A}}$ to a subset $B^{B}_{C_{h,B}}$\nto generate expert trajectories $T_{expert}$. We then learn the underlying RMAB reward\nestimation $\\hat{R}$ by maximizing $P(T_{expert}|\u03c0_{learner},\\hat{R})$. From timestep H onward, we\nallocate service calls using an RMAB policy generated by the new R.\nIRL Approach for RMAB: WHIRL We propose WHIRL (Algorithm 1), which\nfirst generates trajectories, $T_{expert}$ from aggregate desired behavior by calling Al-\ngorithm 2 (line 2). It then calculates Whittle indices using the known transition\nprobabilities and estimated rewards $\\hat{R}$ and generates a soft top k policy on the indices"}, {"title": "Policy Evaluation Function", "content": "We now turn to policy evaluation, the green box in Figure\n2. We employ a maximum likelihood estimation (MLE) evaluation approach [1]. Given\na set of expert trajectories $T^{expert}$ of size H, we estimate $P(T^{expert}|\u03c0_{learner},\\hat{R})$ and use\nthis evaluation function to update the reward parameter via one step of gradient ascent.\nTo derive the MLE evaluation function, we use $s_{i}^{t}$ and $a_{i}^{t}$ to denote the state and\naction of arm i on trajectory \u03c4at timestep h. $P_i(s_{i}^{t},a_{i}^{t},s_{i}^{t+1})$ is the transition proba-\nbility P. $P(a_{s,i}^{t}|\u03c0_{learner})$ is the soft-top-k probability for pulling arm i given $\u03c0_{learner}$\nwhich is generated from the learned rewards in that iteration (Equation 4). We take the\nlog of this value since it preserves ordering but is differentiable. Lastly, since the transi-\ntion probabilities, we can drop them from the optimization and be left with Equation 7.\n$Eval(\u03c0_{learner},T^{expert}) = P(T^{expert}|\u03c0_{learner},\\hat{R})\u221dlogP(T^{expert} \u03c0_{learner},\\hat{R}) \\quad (5)$\n$T\u2208T^{expert} H$ \n$=\\sum_{i\u2208N}log([P(s_{i}^{1},a_{i}^{1})])+\\sum_{h=1}^{H} log(P(a_{s,i}^{h} | \u03c0_{learner}))\\quad (6)$\n$=\\sum_{T\u2208T^{expert}}\\sum_{i\u2208N}\\sum_{h=1}^{H}log(P(a_{s,i}^{h} | \u03c0_{learner})) \\quad (7)$"}, {"title": "Gradient Updates", "content": "The arm-wise gradient update method for updating the estimated\nreward is key to WHIRL's scalability. To apply it, we need to compute $\\frac{dEval}{dR}$\n$\\frac{dEval(\u03c0_{learner},T^{expert})}{dR} = \\frac{dEval(\u03c0_{learner},T^{expert})}{d\u03c0_{learner}} \\frac{d\u03c0_{learner}}{dW}\\frac{dW}{dR}\\quad (8)$\nwhere W is the Whittle indices of all states under learned rewards $\\hat{R}$. $\\frac{dEval(\u03c0_{learner},T^{expert})}{d\u03c0_{learner}}$\ncan be directly computed since it is a sum of differentiable log functions (Equation 7),\nand $\\frac{d\u03c0_{learner}}{dW}$ was shown to be differentiable in [20]. To show differentiability through\nWhittle index computation to derive $\\frac{dW}{d\\hat{R}}$, we can express whittle indices as a linear func-\ntion of $\\hat{R}$ as done in [20] allowing for the computation of $\\frac{dW}{d\\hat{R}}$ via auto differentiation."}, {"title": "4 Experiments", "content": "We first show WHIRL is more accurate and scalable than existing IRL baselines 6. We\nuse a synthetic dataset for this as it allows direct comparison with IRL baselines that\ntake expert trajectories as inputs, and to compare how close WHIRL can get to the\noriginal true policy, which we only have access to in synthetic settings. Next we show\nhow WHIRL achieves desired public health outcomes in the real world MCH setting."}, {"title": "4.1 Synthetic Dataset: Setup and results", "content": "Setup For the synthetic experiments, we generate $T^{expert}$ from a soft-top-k Whittle\npolicy on randomly generated R and $P(s,a,s')$ for all arms (instead of generating\nfrom Alg 2). Transition probabilities have an additional constraint that pulling the\narm (a = 1) is strictly better than not pulling the arm (a = 0) to ensure benefit\nof pulling. While learning, we only have access to $P(s,a,s')$ and $T^{expert}$. We first\nconsider a RMAB problem composed of N=2 arms, M=2 states, budget K=1, and\ntime horizon H=3. We use this small problem to compare WHIRL with baselines\nbecause the latter become computationally intractable for many arms. The results are\naveraged over 64 runs. For WHIRL only, we also consider RMAB settings composed\nof N = 50, 100, and 200, M=2, K=20, and T=10. These results are averaged over\n48 runs. We compare WHIRL with (1) Max Entropy IRL and (2) MAP-B-IRL.7 To\nevaluate performance we compute the L1 norm between the learned soft-k and expert\n6 WHIRL converges at 30 epochs. We tune the parameter e where e=0 is a regular top\nkand e=1 is a completely random policy. We saw best results with e=0.01, \u03b1=0.01\nfor our Adam optimizer, and discount factor \u03b3=0.99\n7 Code is used from [18] and max-ent python library"}, {"title": "4.2\nMaternal Health dataset: Setup and results", "content": "We also analyze our algorithm on an MCH dataset provided by Armman from a\ndeployment in 2021 in which each beneficiary was modeled as an RMAB arm with a\nnaive {0,1} reward. This data records beneficiary listening and beneficiaries selected\nfor human calls in the form (s,a) for 10 weeks of the program. s=1 if the beneficiary\nlistened to over 30 seconds of a call, and a=1 if the beneficiary was intervened on\nin that timestep. 8 The dataset contains 2127 Armman beneficiaries from the city of\nMumbai. They are fully anonymized. We also have access to 13 features about each\nbeneficiary including demographic features like income, education, and language, and\nalso health information like gestational age, previous number of children/miscarriages.\nSensitive features such as caste and religion are not collected. Armman sends out 71\nhuman calls per timestep for the 2127 beneficiaries, and we estimate beneficiary tran-\nsition data by aggregating each beneficiary's listening patterns over the 10 timesteps\nand clustering them [13]. Lastly, beneficiaries must consent to be in the program.\nSetup WHIRL is part of an overall system (Figure 1) which works as follows: Step\n1: Given execution of the current RMAB policy over the last H timesteps (H=10\nweeks in our experiments), the health expert (in this case, a service call manager) sees\naggregated statistics of which categories of beneficiaries obtained service calls (e.g.\nFigure 4a). For this step, we show a diverse set of aggregate statistics including actions\nbased on demographic information (i.e. income, education, language, phone ownership)\nbut also dynamic information like what states people are at when they are called. The\nservice call manager gives aggregate level feedback based on these statistics. Step\n2: The feedback is used to create $T^{expert}$ from which WHIRL generates new rewards.\n8 The dataset originally contains 7668 beneficiaries but Armman does not intervene on\nmothers who are already high listeners, so only 2127 mothers are eligible for calls."}, {"title": "Learning Rewards in Complex Settings", "content": "We consider a more challenging behav-\nior in which beneficiaries listen to at least 1 AVM in the preceding T weeks. There\nis repetition of content in the AVM [4], and it is desirable to maximize the amount of\ndistinct content heard by beneficiaries rather than total calls. The state space increases\nfrom $2^1$ to $2^T$. It is more difficult to define reward functions as the state space increases,\nbut it could make sense to call beneficiaries that have not heard any of the recent\nmessages. Consider T=3, a setting with 8 states: {000,001,010,011,100,101,110,111}.\nThe third digit is the most recent week's listening state. State 000 is the worst state.\nState 100 is problematic because a beneficiary may transition to 000 where they have\nnot listened to any of the last 3 AVMs (state 010 is slightly better, but still close to\nreaching the 000 state). To prevent such situations, the service call manager uses the\ncommand: Move all service calls from states {111,101,110,011,001} to {000,100,010}.\nAfter using WHIRL, actions change corresponding to the desired behavior (Figure\n7a), but we also see some interesting side effects. The number of interventions in\nstate 110 went up even though that was not specified in the command. This is likely\nbecause the next passive state for 110 is 100 and WHIRL seems to have figured out\nthat is an undesirable state and should be avoided. In terms of listening (Figure 7b),\nvisitation in states 000 and 111 decreases. As intervention actions in 000 increased,\nthe number of beneficiaries in 000 decreased. Perfect listening is not required, so\nservice calls in 111 went down and hence the number of beneficiaries in the state\ndecreased. Second, as desired, visitation for all of the middle states (i.e., states where\n1 or 2 of the last 3 AVMs were listened to) increased. Visits to 001 increased because\nof increased interventions in its neighboring state 000, causing transition to 001.\nWe lastly stress test our algorithm with diverse commands that capture more of"}, {"title": "5 Discussion and Deployment", "content": "Our experiments demonstrate a need for techniques such as WHIRL since desirable\noutcomes are not easy to define in the RMAB reward model and rewards can only be\nlearned subject to resource constraints and uncertain beneficiary transitions. WHIRL\nenables the human expert to view the implications of new desired behavior before\nutilizing the new rewards. Additionally, WHIRL's ability to incorporate large-scale\naggregate feedback enabled the creation of 2,127 expert trajectories, making it feasible\nto use IRL in public health settings in a way that was not previously possible. In\nIndia, outcomes for mobile health programs vary significantly based on state-level"}, {"title": "5.1 Ethical Discussion", "content": "Resource allocation implicitly brings ethical challenges with it. We are learning new\nrewards in a resource constrained environment, so people that initially were assigned\nto receive interventions perhaps will not receive them anymore. However, the moti-\nvation of our project in the first place was to better align allocation of resources to\nexpert preferences, and we show that with WHIRL we are able to send more calls to\nhigh risk beneficiaries (Figure 5). Another concern may be that the expert gives an\nincorrect directive; to safeguard against that, we rely on the what-if analysis, where\nwe show the expert summary statistics about how the new rewards will affect who\nis called, and the expert can decide to go forward with those rewards (Section 4.2)."}]}