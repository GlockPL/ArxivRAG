{"title": "AUTOMATED PROOF GENERATION FOR RUST CODE VIA SELF-EVOLUTION", "authors": ["Tianyu Chen", "Shuai Lu", "Shan Lu", "Yeyun Gong", "Chenyuan Yang", "Xuheng Li", "Md Rakib Hossain Misu", "Hao Yu", "Nan Duan", "Peng Cheng", "Fan Yang", "Shuvendu K Lahiri", "Tao Xie", "Lidong Zhou"], "abstract": "Ensuring correctness is crucial for code generation. Formal verification offers a definitive assurance of correctness, but demands substantial human effort in proof construction and hence raises a pressing need for automation. The primary obstacle lies in the severe lack of data there is much less proof than code for LLMs to train upon. In this paper, we introduce SAFE, a novel framework that overcomes the lack of human-written proof to enable automated proof generation of Rust code. SAFE establishes a self-evolving cycle where data synthesis and fine-tuning collaborate to enhance the model capability, leveraging the definitive power of a symbolic verifier in telling correct proof from incorrect ones. SAFE also re-purposes the large number of synthesized incorrect proofs to train the self-debugging capability of the fine-tuned models, empowering them to fix incorrect proofs based on the verifier's feedback. SAFE demonstrates superior efficiency and precision compared to GPT-40. Through tens of thousands of synthesized proofs and the self-debugging mechanism, we improve the capability of open-source models, initially unacquainted with formal verification, to automatically write proof for Rust code. This advancement leads to a significant improvement in performance, achieving a 70.50% accuracy rate in a benchmark crafted by human experts, a significant leap over GPT-40's performance of 24.46%.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have recently exhibited impressive capabilities in code generation (Roziere et al., 2023; Guo et al., 2024; Lozhkov et al., 2024; Google, 2024). However, the correctness of generated code cannot be guaranteed. To tackle this issue, prior research (Chen et al., 2022; Zhang et al., 2024a; Huang et al., 2023) has explored assessing generated code by test cases, which sometimes are also generated by LLMs. Although helpful, testing cannot cover all possible program inputs; they can reveal the presence of bugs but cannot prove their absence (Dahl et al., 1972). In contrast, formal verification employs a symbolic prover to mathematically prove that a program satisfies the desired properties under all possible inputs, without running the program. Unfortunately, to use formal verification, one needs to first formally express the desired properties, often in a special proof-oriented language, and then craft formal proofs, which takes substantial formal verification expertise (Zhang et al., 2024b). Therefore, to advance trustable code generation, automated formal verification, particularly automated proof generation, is in a pressing need.\nTwo main approaches of proof automation have been explored. The first (Misu et al., 2024; Sun et al., 2024a; Pei et al., 2023; Liu et al., 2023; Chakraborty et al., 2023; Kamath et al., 2023;"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "2.1 VERUS VERIFICATION TOOL\nTo formally prove whether a Rust function is guaranteed to satisfy its specification, Verus (Lattuada et al., 2023) statically analyzes the Rust code, the specification, and the proof annotations, and forms queries for the underlying SMT-solver (e.g., Z3 (De Moura & Bj\u00f8rner, 2008)) to formally solve. By leveraging the type system in Rust and allowing both specification and proof annotations to write in Rust syntax, Verus has become the state-of-the-art verification tool for Rust, one of the most popular programming languages (JE & CT, 2020; Fulton et al., 2021), and has been used to successfully verify large Rust systems (Sun et al., 2024b; Zhou et al., 2024). Unfortunately, since Verus has been developed for only about three years, there are less than 500 Verus verified Rust files on Github so far. Meanwhile, although Verus specifications and proof are written mostly in Rust, they are still difficult to generate for both human beings (Sun et al., 2024b; Zhou et al., 2024) and LLMs.\nListing 1 shows a Rust function that implements binary search. At high level, the function specification (Line 7-13) states that the post-condition, expressed in a ensures code block, is guaranteed to hold as long as the pre-condition, in a requires block, is true. For this function, its pre-condition (Lines 7-9) states that the input array is ordered in an ascending manner and that the input value to be searched k must exist in the array (\\forall and \\exists are Verus quantifiers). The post-condition requires that the return value r should be a valid index pointing to the input value k.\nFor simple programs/specifications, Verus can accomplish the formal verification without any extra annotations (we refer to these cases as trivial proof). Unfortunately, for functions and specifications that involve loops, collections, and quantifiers, Verus often needs users to provide proof annotations. The loop invariants specified on Line 18\u201321 are one type of proof annotations. They specify what properties are true right before and right after every loop iteration. Verus will prove the correctness of each loop invariant and then use proved loop invariants to help prove the function specification. If any loop invariant is incorrect or if needed invariants are missing, the proof will fail. For more com-plicated tasks, other types of proof annotations like assert and lemma functions may be needed."}, {"title": "2.2 PRIOR WORK IN PROOF-BENCHMARK BUILDING AND SELF-EVOLVING FRAMEWORK", "content": "There was much effort recently in building verification dataset of existing human written proofs (Loughridge et al., 2024; Zhang et al., 2024b; Chakraborty et al., 2024). Unfortunately, since writing formal proof takes expertise beyond normal coding, this dataset building approach is difficult to scale. Recent work has explored augmenting the Dafny-proof dataset using proofs synthesized by GPT-4 through few-shot learning and human-written proof examples (Misu et al., 2024; Sun et al., 2024a). However, due to the limited proof-generation power of GPT-4, the number of proofs in these dataset are fewer than 200, with many being trivial proofs.\nSelf-evolving style of learning has been explored before in other context. While (Gulcehre et al., 2023) propose a reinforced self-training framework in the machine translation task, (Polu et al., 2022) leverages the expert iteration strategy for mathematical theorem proving with Lean. Our work applies the idea of self-evolving to a very different task, synthesizing correctness proof for Rust code, with different challenges: unlike previous work, Verus does not have a large collection of existing data for bootstrapping; there is much less a concern of generating \u201cspecification\" in mathematical theorem proving; furthermore, unlike interactive theorem provers that decompose a proof problem into small steps through tactics (e.g., Lean), Verus uses SMT-solver with hints from proof annotations to prove the whole function correctness as a whole and hence presents completely different challenges to proof synthesis."}, {"title": "3 APPROACH", "content": "As illustrated in Figure 1, SAFE involves two self-evolving (Tao et al., 2024; Gulcehre et al., 2023) procedures. The first procedure synthesizes specifications that are used as inputs to the second procedure, while the second procedure produces the end-result of SAFE a fine-tuned LLM that can automatically synthesize proof for Rust code.\nIn Algorithm 1, we show the two procedures of our self-evolving framework. In both procedures, we use GPT-40 to generate the round-0 data spec/proof-data0. In each round r, we use data collected and filtered from rounds 0 to r 1 to fine-tune modelr, based on its preceding modelr-1."}, {"title": "3.1 STEP 1: GENERATING VERUS-COMPATIBLE CODE", "content": "This step ensures the compatibility of our input Rust programs with Verus, which does not support all Rust features. For instance, some Rust expressions and standard library functionalities like for, Iterators, HashMap, and others are not supported or only partially supported by Verus. Thus, it is necessary to adapt normal Rust code to Verus-compatible one before adding specifications.\nTo address this compatibility issue, we employ GPT-4o as a code translator, effectively substituting Verus-incompatible Rust code snippets with Verus-compatible alternatives, such as converting an"}, {"title": "3.2 STEP 2: SELF-EVOLVING SPECIFICATION SYNTHESIS", "content": "In this step, LLMs are tasked with generating preconditions and postconditions for a Rust function based on the function's implementation and docstring. Different from prior work in spec-generation (Flanagan & Leino, 2001; Ma et al., 2024), SAFE needs synthesized specification as input for its self-evolving proof-synthesis framework, which raises unique requirements on its spec-evaluation criteria (i.e., which specification to keep or discard) and the mechanism to evaluate the criteria.\nIn terms of criteria, generally speaking, a perfect specification S should be correct (i.e., any correct implementation should be accepted by S) and complete (i.e., any incorrect code implementation should be rejected by S). For example, the spec in the left column below is correct but incomplete for the binary search function, as an incorrect implementation that always returns 0 would be accepted by this spec; the spec in the right column below is incorrect, as a correct implementation may fail this spec when the input array has multiple elements matching the search key. In SAFE, getting perfect specification is not the goal. SAFE should discard incorrect specifications, which can never be proved no matter what proof annotations are used and hence will waste training cycles in the next step. However, SAFE is fine with incomplete specifications, as the usage of spec in SAFE is to stimulate proof synthesis not to judge code correctness. In fact, SAFE needs incomplete specifications, as complete specifications are likely too challenging to prove at the early stage of the proof-synthesis model's evolution. Of course, specifications that are too incomplete should be discarded, because trivial specifications can be proved without any proof annotations and hence offer no usage for proof-synthesis training (e.g., 0 <= r for the binary search function)."}, {"title": "3.3 STEP 3: SELF-EVOLVING PROOF SYNTHESIS", "content": "Now that we have tens of thousands of function-specification pairs, we can start the self-evolving procedure of proof synthesis. Comparing with spec-synthesis, deciding whether to accept or reject a synthesized item is much more straightforward - only proof that Verus can use to verify the Rust function satisfies its specification is accepted; if any verification errors are raised, the proof is discarded\u00b9. However, since proof synthesis is much more difficult than specification synthesis, we need to pay special attention to bootstrap the self-evolving procedure and to keep it moving forward.\nFor bootstrapping, we realized that a simple prompt to GPT-40, like \u201cYou are an expert in Verus. Your mission is to write proof annotations for the following function\", would not work. It is very difficult for GPT-40 to synthesize all the needed loop invariants without any incorrect ones in be-tween. More complicated proof annotations like proof blocks and lemma functions are even more out of reach. Therefore, we write a detailed prompt for GPT-40, explaining the principles and tricks of writing Verus proof. Designing these prompts took three Verus experts about 40 working hours. By doing so, we finally manage to make GPT-40 synthesize proof annotations that allow Verus to prove more than one thousand Rust functions at the cost of one whole month of non-stop GPT-40 invocation. Fortunately, this is sufficient for bootstrap. For the remainder of the self-evolving proce-dure, GPT-40 is not used any more. Instead, we use an open-source LLM much smaller than GPT-40 to efficiently synthesize data and improve its capability in a self-evolving manner as in Algorithm 1.\nTo keep the self-evolving procedure moving forward, we have observed that it is important to al-low the model to \"debug\" an imperfect proof. In many cases, the proof annotations generated by the model only contain small errors or miss one line of annotation, without the capability of self-debugging, the model would start from scratch and fail to synthesize a perfect proof even after many attempts. Fortunately, we do not need to worry about lack of training data here: \"Thanks to\" the in-capability of LLMs in proof generation, a huge number of incorrect proofs are generated, which can be re-purposed to fine-tune the proof generator on self-debugging tasks. Specifically, for every Verus program p (together with its specification) for which a perfect proof P\u221a is eventually synthesized in a round, we denote those incorrect proofs generated before P\u221a as [P1, ..., Pn]. For every incorrect proof Px \u2208 [P1, . . ., Pn], a triplet {Px, ErrorPx, p\u221a } is added to our self-debugging dataset, where ErrorPx represents the verification errors Verus reported about Px. Just in case too many similar incorrect proofs are generated, we sample at most 10 incorrect proofs from [P1, ..., Pn] that have the same verification error to keep the diversity of the training dataset. Note that, in SAFE, we put the self-debugging dataset and proof dataset together to fine-tune our model, which will be empowered to both generate proof from scratch and repair an existing proof based on verification error messages for a Rust function and its specification."}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENTAL SETUP\n4.1.1 DATASET\nAs an early exploration of automated formal verification, in this paper, we focus solely on the Rust code of algorithm types at the function level, as our source of data. We employ the MBPP dataset (Austin et al., 2021) (only training split for data synthesis) and the CodeNet dataset (Puri et al., 2021) as our data sources. These two datasets contain small programs written in different program-ming languages, each of which is an intended solution to a coding problem described in natural language and is associated with three test cases on average. We translate the Python programs in MBPP and extract the Rust programs in CodeNet. In total, we collect 45,395 Rust single-function programs. Of these, 21,398 have been successfully transformed into Rust code that is compatible with Verus by SAFE. We conduct the specification synthesis first, after two rounds of self-evolution, we obtain 19,017 high-quality specifications. Then we run three rounds of self-evolving proof syn-thesis, ending with 9,706 verified programs, and 10,486 self-debugging data pairs. To the best of our knowledge, this represents the most extensive synthetic dataset created for Verus code to date."}, {"title": "4.1.2 BENCHMARK AND METRICS", "content": "We assess the model's proficiency in generating proofs in two benchmarks, the human-written benchmark VerusBench and the synthetic benchmark CodeNet-Test.\nVerusBench is a human-written benchmark dataset with human-written Rust code and correspond-ing Verus specifications. It contains 139 code files in total. 23 of them are algorithmic programs from Verus tutorials. 38 of them come from the dataset of SV-COMP-2021 (Beyer, 2021) (in short, SV) a contest focused on program verification for C and Java languages; we utilize 38 Verus-translated tasks provided by (Yao et al., 2023). The remaining 78 tasks come from MBPP-DFY-153 (Misu et al., 2024), a Dafny version of the MBPP test set. We have translated 78 of them that are compatible with Verus.\nCodeNet-Test is an expansive benchmark, 10x larger than VerusBench, crafted by LLM and en-compassing 1,435 diverse tasks. Given the substantial human effort required to write Verus code and specifications, it's impractical to create a large test suite by human. To comprehensively measure the model capability of generating proofs, we split a subset of CodeNet for testing, ensuring it was not utilized in our data synthesis process. We leverage our specification generator to craft specifications for this subset and apply the same scoring mechanism to preserve reasonably well specifications.\nWe measure the capability of proof generation by Accuracy@K. Specifically, we use our model to sample K proofs during proof generation, and for each incorrect sample, we sample K proofs generated during self-debugging, resulting in $K * K$ debugged proofs. For Accuracy@1, we use greedy decoding, which only generate 1 output, and for Accuracy@10, we sample 10 outputs with a temperature of 0.7. Then, for each task in our benchmarks, Accuracy@K equals one if at least one proofs/debugged proofs is verified by Verus."}, {"title": "4.2 EVALUATION RESULTS", "content": "4.2.1 MAIN RESULTS\nIn Table 1, we compare the accuracy of SAFE with our baseline approaches. Due to the inherent complexity of this task, there is no pre-existing fine-tuned model to serve as a baseline. We employ GPT-40, DeepSeekCoder-33B-Instruct model (Guo et al., 2024) and Llama3.1-8B-Instruct model (Dubey et al., 2024) with basic prompts same as what we use for the fine-tuned models as our baselines. To offer some advantages to the baselines, we also feed four examples for in-context learning in these baselines, which we do not use for SAFE fine-tuned models.\nWe can see that SAFE achieves substantially higher accuracy compared to baseline approaches in both datasets and metrics. In VerusBench benchmark, SAFE achieves an Accuracy@1 of 52.52% and an Accuracy@10 of 70.50% while the best of our baselines is 11.51% in Accuracy@1 and 24.46% in Accuracy@10. We also measure the Accuracy@100 of SAFE to show the potential upper limit of our method. DeepSeekCoder achieves the best performance of 79.14%. With the long and carefully designed prompt, which was used to bootstrap SAFE proof synthesis, GPT-40 is reasonably effective for VerusBench. For the CodeNet-Test benchmark, we find the performance of baselines and prompt-based GPT-40, denoted as SAFE\u2070, drops significantly \u2014 by more than 10X comparing with VerusBench, while SAFE doesn't suffer as much. It is important to note that CodeNet-Test is significantly larger and exhibits a distinct data distribution compared to VerusBench, the former is derived from competitive programming, whereas the latter encompasses common algorithms. The results indict the prompt-based method has limited generalizability in automated proof generation, while SAFE no longer needs complicated prompts during inference. We further report paired t-test (Hsu & Lachenbruch, 2014) results for Table 1 in Table 5 in Appendix. As a result, by leveraging our self-evolving framework, SAFE can substantially improve the capability of open-source model, initially unacquainted with formal verification, and effectively generate proofs for Verus programs."}, {"title": "4.2.2 BENEFITS FROM SELF-EVOLVING", "content": "In Table 2, we use the performance on VerusBench to show how our self-evolving framework improves the capability of models. In this set of experiments, we use DeepSeekCoder-33B-Instruct for data synthesis and fine-tuning; Round 1, 2, 3 represent three models fine tuned from DeepSeekCoder-33B-Instruct. Table 2 illustrates that for every subset of VerusBench, the best Ac-"}, {"title": "4.2.3 IMPROVEMENT OF SELF-DEBUGGING", "content": "Table 1 demonstrates that SAFE+, which allows LLMs to do self-debugging, substantially improves the accuracy compared to direct generation, indicting the effectiveness of the self-debugging mecha-nism. In Table 2, self-debugging mechanism shows consistent improvements in all the three compo-nents of VerusBench. As we show in Listing 2 of the Appendix, Verus provides error messages for any incorrect proof about which proof annotation, which part of the specification, or which implicit proof target (e.g., no overflow for every arithmetic expression) cannot be verified, which can help repair the proof for experienced human users and well trained models.\nDecoding Strategies for Self-Debugging. From Table 1, we can see the enhancements of self-debugging become more significant with the increase in the number of sampled outputs. To further evaluate how decoding strategies affect the performance of self-debugging in VerusBench, we con-duct four settings by combining greedy and sampling decoding during the generation and debugging phases, and run two more self-debugging rounds with greedy decoding. Figure 2 shows that more rounds of self-debugging hardly improve the accuracy, with the improvement less than 1%. Besides, sampling decoding strategy is more useful for generation phase rather than for the self-debugging"}, {"title": "4.2.4 IMPACT OF SPECIFICATION QUALITY", "content": "As mentioned in Section 3.2, we only keep specifications with high Correctness scores and reason-ably high Completeness scores. To investigate the impact of this design decision, we zoom into the last round of proof-synthesis model fine-tuning to see how the quality of proof-synthesis model can be affected by the quality of specification dataset. Specifically, the current Round-3 SAFE model is obtained by fine-tuning the Round-2 SAFE model using correct proof synthesized by the boot-strapping model, the Round-1 model, and the Round-2 model based on the specifications S selected by SAFE, denoted as Po[S] \u222aP1 [S] \u222aP2[S] (Pi[S] denotes all the correct proofs synthesized in round i based on specification set S). In our two alternative settings, we go back to the set of all specifications synthesized during the spec-synthesis self-evolving procedure and randomly sample a set of specifications S\u2217 that have the same size as S; naturally S\u2217 contains many low-quality spec-ifications that do not pass our Correctness and Completeness threshold. In our alternative setting of 'Round 2 w/ mix-quality', we obtain a model by fine-tuning the Round-2 SAFE model using Po[S] \u222aP1 [S] \u222aP2[S\u2217]; in our alternative setting of 'low-quality spec-input', we obtain a model by fine-tuning the original DeepSeekCoder using P2[S\u2217] alone.\nFrom the results in Table 2, we can see that high-quality specifications contribute substantially to the end-to-end effectiveness of SAFE. If the model is trained without any high quality data ('low-quality spec-input'), the accuracy drops for all three subsets of VerusBench and even drops to 0% Accuracy@1 and 5.26% Accuracy@10 for SV. Probably because more debugging data is created in this setting, the self-debugging feature helps more for this setting, but the overall accuracy still lags way behind the default SAFE Round-3 model. When low quality data is mixed with high quality ones, the performance still drops as shown in the 'Mixed-Quality' row in the table. In Appendix, we further report paired t-test results for Table 3 in Table 7, and provide an example Rust program (Listing 14) whose low quality specification led to an extremely simple proof \u2013 this simple proof probably offered no benefit for proof-synthesis fine-tuning. In conclusion, specification selection matters to the fine-tuning and the evolution of proof-synthesis models."}, {"title": "5 CONCLUSION", "content": "In this paper, we propose SAFE, a novel self-evolving framework to advance automated proof gener-ation for Rust. SAFE alleviates the severe data lack challenge by coupling data synthesis and model fine-tuning in a self-evolving manner, demonstrating superior efficiency and precision compared to relying solely on GPT-40. Through ten of thousands of synthesized proofs and the self-debugging mechanism, we improve the capability of open-source models, initially unacquainted with formal verification, to automatically write proof for Rust code. Our evaluation shows that SAFE leads to"}]}