{"title": "Prompting in the Dark: Assessing Human Performance in Prompt Engineering for Data Labeling When Gold Labels Are Absent", "authors": ["Zeyu He", "Saniya Naphade", "Ting-Hao 'Kenneth' Huang"], "abstract": "Millions of users prompt large language models (LLMs) for various tasks, but how good are people at prompt engineering? Do users actually get closer to their desired outcome over multiple iterations of their prompts? These questions are crucial when no gold-standard labels are available to measure progress. This paper investigates a scenario in LLM-powered data labeling, \u201cprompting in the dark,\" where users iteratively prompt LLMs to label data without using manually-labeled benchmarks. We developed PROMPTINGSHEET, a Google Sheets add-on that enables users to compose, revise, and iteratively label data through spreadsheets. Through a study with 20 participants, we found that prompting in the dark was highly unreliable-only 9 participants improved labeling accuracy after four or more iterations. Automated prompt optimization tools like DSPy also struggled when few gold labels were available. Our findings highlight the importance of gold labels and the needs, as well as the risks, of automated support in human prompt engineering, providing insights for future tool design.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have empowered millions, if not billions, to perform a wide range of programming and data science tasks, even without formal technical backgrounds. People can ask LLMs to teach them step-by-step how to build a web app from scratch [96], have LLMs analyze data and generate insights [51, 60], or instruct LLMs to label thousands of data items [33, 34, 37]. All these were made possible by LLMs' ability to converse in natural language and to follow users' instructions, also known as \"prompts.\" The practice of \"prompt engineering\" has emerged to describe the process of developing and optimizing prompts to use LLMs efficiently [11, 27, 100]. Tools like LangChain\u00b9 and Chain-Forge [6] were developed to facilitate complex chaining of prompts; technologies such as DSPy [43] were also created to automate prompt optimization. However, how effective are people at prompt engineering in practice? Do users really get closer to their desired outcome over multiple iterations of their prompts? These questions are especially important when gold-standard labels are unavailable. Decades of research indicates that gold-standard labels-data inputs paired with known correct predictions or ideal outputs (also referred to as \"gold labels\" or simply \"gold\" [3, 45, 81, 84, 85, 104, 106]-are critical for evaluating annotation quality [25, 31, 64] and system performance [15, 21, 30]. However, in real-world scenarios, gold labels can be absent, expensive to obtain at scale, or difficult to use. Millions of users iterate on prompts daily using ChatGPT's text-based chat interface [13, 63], which does not provide any way to upload or evaluate against gold labels. Researchers conducting qualitative coding with LLMs often do not begin with stabilized gold codes [14, 16, 26, 71, 115]. Many automated data annotation efforts face insufficient gold labels due to the high cost of gathering labels [56, 101], difficulty recruiting experts for large-scale annotation tasks [10, 69, 107], privacy restrictions [32, 110], or the lack of universally agreed standards [68, 102]. In such cases, without reliable benchmarks to track prompting progress, users can only rely on their own prompting ability to drive toward desired outcomes. Yet, this ability is difficult to measure and thus remains understudied. Without understanding how well users can prompt LLMs through iterations, it is hard to determine how much support users need-and when they need it-to improve their prompt engineering efforts effectively.\nThis paper investigates a particular scenario in LLM-powered data labeling, which we refer to as \u201cprompting in the dark.\u201d In these scenarios, users' understanding of the data and their desired labeling scheme evolves through their interactions with the LLM and its outputs rather than through manual labeling. The process typically unfolds as follows: Users begin by writing a prompt to instruct the LLM to label unannotated text data. They then observe the outcomes, sometimes reviewing the LLM's explanations for its labels, and iteratively refine their prompt until satisfied with the results. This iterative refinement process is a common practice in research contexts, where \"the prompt provided"}, {"title": "Research Questions", "content": "The central research question guiding our study is:\n\u2022 RQ 1-1: How effective are people at prompt engineering in the \"prompting in the dark\" scenario?\nIn addition to examining prompt engineering effectiveness, we aim to understand how key factors influence human performance when prompting in the dark. Among the many variables that could impact outcomes [50], two stand out: (1) the size of the sample data in each iteration, and (2) whether the LLM's explanations for its"}, {"title": "Related Work", "content": "Prompts are the primary means by which users interact with, utilize, and instruct LLMs. Since the emergence of these models, researchers and developers have invested significant effort into understanding how to craft better prompts for more effective use.\nAutomatic Prompt Optimization. Much of the prior work has focused on automatically optimizing prompts. A common theme across these studies is the use of gold-standard labels to guide the optimization process. For example, Pryzant et al. [72] introduced an automatic prompt optimization method inspired by gradient descent; Ma\u00f1as et al. [61] presented an approach that begins with a user prompt and iteratively generates revised prompts to maximize consistency between the generated image and prompt, without human intervention; Wan et al. [97] explored two types of prompt optimization, instruction and exemplar, and suggested that combining both can yield optimal results; Sun et al. [87] combined zero-shot and few-shot learning to optimize prompts automatically; and Levi et al. [54] improved prompt optimization through synthetic data generation and iterative refinement, focusing on aligning prompts with user intent by creating challenging boundary cases for iterative prompt refinement. While these studies were interesting and relevant, they generally assumed the availability of gold-standard labels and did not address situations where labels are absent or where standards are constantly evolving.\nUser-Driven Prompt Optimization. In addition to automatic prompt optimization, some research has focused on human capabilities in optimizing prompts. Zhou et al. [119] found that manual prompting often outperforms automated methods in various scenarios; Zamfirescu-Pereira et al. [113] discovered that people tend to design prompts opportunistically rather than systematically, which often leads to lower success rates. To the best of our knowledge, the most relevant prior work is by Wang et al. [102], who developed an iterative refinement system that enables users to prompt LLMs to build a personalized classifier for social media content. Their study explored three user strategies for improving prompts and measured their effectiveness. While conceptually related to our work, their focus was not on how users evolve their understanding"}, {"title": "Tools for Prompt Engineering", "content": "With the advances in LLMs, numerous tools have been developed to assist with prompt engineering. Most of these tools follow a software-engineering paradigm, where testing (such as unit tests or integration tests) is a central concept, and thus often assume the existence of gold-standard labels. For example, PromptIDE is an interactive tool that helps experts iteratively refine prompts by providing various inputs, visualizing their performance on small validation datasets, and optimizing them based on quantitative feedback [86]; PromptAid is a visual analytics system for interactively creating, refining, testing, and iterating prompts while tracking accuracy changes [62]; ChainForge is an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text-generation LLMs [6]; and, promptfoo applies a test-driven approach to LLM development, producing matrix views that enable quick evaluation of outputs across multiple prompts [105]. While these tools are inspiring and valuable, the scenarios we focus on do not rely on the constant availability of gold labels."}, {"title": "Human-LLM Collaborative Data Annotation", "content": "Beyond simply treating LLMs as automatic labelers-common in countless NLP projects [88]-a growing body of work explores how to combine human and LLM efforts to achieve better annotation outcomes, such as improved accuracy or speed. Even as LLMs outperform humans in many labeling tasks, human-AI collaboration often produces better results than either alone [92]. For example, Kim et al. [46] introduced a human-LLM collaborative annotation system where LLMs handle bulk annotation tasks, while humans selectively verify and refine the annotations. Goel et al. [29] proposed an approach that combines LLMs with human expertise to efficiently generate ground truth labels for medical text annotation. Additionally, He et al. [34] demonstrated how aggregating crowd workers' labels with GPT-4's output can achieve higher labeling accuracy than either source alone. These studies generally aim to split the workflow of data labeling between humans and LLMs in a smart way, making the task more effective or efficient.\nIn contrast, our work does not focus on dividing or combining the workload, but on how humans can teach LLMs-through prompt refinement-to better label the specific type of data. Few prior studies have emphasized iterative prompt refinement in human-LLM collaborative data annotation. For example, Liu et al. [57] developed a workflow for video content analysis, refining prompts to improve LLM-generated annotations and align them with human judgment. Additionally, Zhang et al. [114] proposed LLMAAA, which uses LLMs as annotators in a feedback loop to label data efficiently. Their study shows that poorly designed prompts result in subpar performance, especially in complex tasks. Our work advances this relatively understudied area of human-LLM collaborative annotation research."}, {"title": "Gold-Standard Labels in Annotation Tasks", "content": "Decades of research have shown that gold-standard labels play a critical role in quality control for data annotation pipelines [17, 25, 31, 36, 52]. Embedding items with known labels into the data annotation process allows requesters to reliably capture quality signals, such as workers' level of expertise [2, 4, 109] or attentiveness to tasks [36, 66]. These insights enable requesters to take appropriate actions, such as retraining annotators [17, 36, 52], removing low-performing workers [18, 34, 52, 84], or identifying potential issues in the annotation interfaces [34, 48, 73, 90]. Gold labels are also beneficial for requesters during post-annotation data processing. They can be used to weight labels from different workers in label aggregation [1, 2], improve label aggregation strategies [44, 84], or exclude unreliable workers' outputs entirely [2]. Beyond requesters, gold labels are also beneficial for data labelers like crowd workers. Gold labels can be used to train workers [17, 25, 31, 52], provide real-time feedback to help them recalibrate their understanding of the task [36, 52], or remind them to pay more attention [36, 66].\nWhile gold labels are useful for quality control, as stated in the Introduction (Section 1), they are not always available in real-world scenarios due to constraints such as data privacy or the cost of gathering gold labels [58, 65, 83, 108]. To address these challenges, researchers have developed methods to generate (approximations of) quality signals without gold labels. In the realm of LLM-powered data annotation, for instance, CoPrompter evaluates how well an LLM's output aligns with user-specified requirements as a feedback mechanism [41]. Other studies also leverage the stability [12] or confidence [28] of LLM outputs to infer quality signals. Our research examines how effectively humans can refine prompts to guide LLMs in labeling data without gold-standard labels, providing insights into human prompting capabilities in the absence of reliable guidance signals."}, {"title": "PROMPTINGSHEET: A Spreadsheet-Based End-User Prompt Engineering Tool", "content": "In this paper, we present PROMPTINGSHEET, a Google Sheets add-on that allows users to load a dataset into a Dataset spreadsheet, manually compose each part of a prompt within Task Context, Labeling Rules, and Shots sheets, and use the composed prompt to instruct an LLM to annotate data-all within the same Google Sheets document. Motivated by the need to enable general users to prompt LLMs without installing and configuring professional tools like integrated development environments (IDEs) or Jupyter Note-books, we decided to build a tool based on spreadsheets, which most computer users are already familiar with. This section overviews PROMPTINGSHEET's design and workflows."}, {"title": "Design Goals", "content": "Adapting to Evolving Labeling Goals. The goal of PROMPTINGSHEET is to enable general users to create and refine prompts iteratively for LLM-powered data labeling, particularly in situations where they start without any labeled gold data or manual labeling, i.e., the \"prompting in the dark\" scenario. In these cases, users' understanding of the data and desired labeling scheme evolves through their interactions with the LLM, based on its predicted labels and explanations, rather than through their own manual efforts. The lack of"}, {"title": "User Interface and Pre-Defined Sheets", "content": "PROMPTINGSHEET is a Google Sheets add-on that enables users to load data, sample a subset for labeling, compose and edit prompts, use these prompts to request LLMs for data labeling, and iteratively revise the prompts. Following Google Sheets' design constraints, all functions are presented as manuals and buttons within the sidebar on the right. The sidebar remains consistent across all sheets, regardless of which sheet is in use. At the top of the sidebar, PROMPTINGSHEET provides a real-time notification that keeps users informed about its ongoing processes, such as \"Data Indexing,\" \"Data Sampling,\" \"Generating the Instructional Prompt,\" or \"Annotating.\"\nPre-Defined Sheets. PROMPTINGSHEET includes a set of predefined spreadsheets, each with a set of pre-defined columns. At the bottom of the interface, a series of tabs allows users to switch between sheets, with each sheet dedicated to a different part of the data labeling process. The following describes each sheet in detail. (To help readers easily identify which sheet we are referring to, we indexed each sheet as A, B, C, ..., and G in all the figures. These indexes were not present in the actual system to users.)\n\u2022 Dataset (Sheet A): This spreadsheet stores the full dataset. Users can copy and paste the dataset into this sheet or use any supported Google Sheets import method (in Step 0). The sheet includes three key predefined columns: (1) Data ID, (2) Group ID, and (3) Data Instance. Each data instance is uniquely indexed by its corresponding Data ID, which users can generate by clicking the \"Index Data ID\" function in the sidebar. The Group ID is used for annotating sequential data, such as when each sentence in an article is treated as a separate data instance, but all sentences from the same article share the same Group ID. In our design, this sheet is intended to serve as a static data source, and we anticipate that users will not modify it after loading the data.\n\u2022 Task Context (Sheet B): This spreadsheet stores the meta-information and context for the labeling task, which will later be incorporated into the prompt. The sheet includes predefined questions that characterize the task, such as the purpose of the data labeling, how the labels will be used, the source of the data, and the size of each data instance. Users provide answers to these questions (in Step 1 or 4), and PROMPTINGSHEET automatically incorporates both the questions and their answers into the prompt used for LLMs to label the data.\n\u2022 Rule Book (Sheet C): This spreadsheet contains the labeling rules that the LLM will follow. It includes two key predefined columns: (1) Label Name and (2) Rules for the Label. Users manually define the criteria and descriptions for each label in free text (in Step 1 or 4), detailing the guidelines for the annotation process. Multiple rules can be added for a single label, providing flexibility in defining the labeling criteria.\n\u2022 Shots (Sheet D): This spreadsheet stores all the high-quality examples, including data instances and their corresponding labels, which will be included in the prompt to guide the LLM in labeling the data. These examples, commonly referred to as \"shots\" in prompts, follow the same predefined column structure, with an additional \"Gold-Standard Label\" column. Users can add these examples manually (in Step 1) or use PROMPTINGSHEET's function to do so (in Step 4).\n\u2022 Working Data Sample (Sheet E): This spreadsheet stores the current subset of data selected from the full dataset, ready"}, {"title": "User Workflow", "content": "Users interact with PROMPTINGSHEET to craft a prompt, use it to instruct the LLM in labeling data, review the results, and then revise the prompt through an iterative process. To demonstrate the users' workflow, we present a scenario where a user wants to employ PROMPTINGSHEET to label a collection of tweets related to COVID with a 5-point sentiment scale, ranging from Very Negative (1) to Very Positive (5). The goal is to analyze the sentiment of Twitter (now X) users toward COVID, with an emphasis on ensuring that the classification of each tweet reflects the user's own judgment. In this case, the LLM's labels should align with the user's assessment of what is positive or negative, as well as the intensity of sentiment, rather than following an \"objective\" standard.\n\u2022 Step 0: Load and Index the Dataset. To begin using PROMPTINGSHEET, the user opens a new Google Sheets document and activates the PROMPTINGSHEET add-on. The system automatically sets up the necessary tabs, and the addon interface appears on the right side of the spreadsheet (Figure 2). The user then imports their data instances into the Dataset sheet, with the text of each tweet placed in the Data Instance column. The user must specify a Group ID for each instance. If the data are not sequential or grouped, they can assign unique Group IDs using Google Sheets' automatic numbering function. Once the data is entered, the user clicks the \"Index Data ID\" button in the sidebar, and PROMPTINGSHEET automatically assigns unique data IDs to each instance in the \"Data ID\" column.\n\u2022 Step 1: Compose/Refine the Prompt (Figure 3). This is the most critical step, where the user composes and refines prompts for the LLM to label the data. In PROMPTINGSHEET, the prompt consists of three parts, each corresponding to a separate sheet: (1) Context, (2) Rule Book, and (3) Shots. \u2022 Step 2: Sample/Resample a Subset . Next, the user employs PROMPTINGSHEET to sample a subset of data for labeling. Labeling only a subset, rather than the entire dataset, is necessary because the full dataset is too large for the user to thoroughly review the LLM's results. Additionally, labeling the entire dataset iteratively would be prohibitively expensive. In this step, the user can (1) randomly or (2) sequentially sample data from the Dataset sheet into the Working Data Sample sheet. Step 3: Use the Prompt to Instruct the LLM to Label the Data Sample After finalizing the three prompt sheets-Context, Rule Book, and Shots-in Step 1 and sampling data instances in Step 2, the user clicks the \"Start Annotation\" button in the sidebar to annotate all instances in the Working Data Sample sheet Step 4: Observe, then Revise the Prompt"}, {"title": "User Study", "content": "Our goal is to investigate how effective people are at prompt engineering when gold labels are absent, namely, \"prompting in the dark\". To study this, we conducted an in-lab user study in which participants used PROMPTINGSHEET to perform a 5-point sentiment labeling task on a tweet dataset. This section overviews the details of this study. This study has been approved by the IRB office of the authors' institute."}, {"title": "Study Procedure", "content": "Three participants were recruited through the authors' network for the pilot study. In the first pilot, we used CODA-19 [40] as the data annotation task, where participants labeled text segments from academic abstracts into categories such as background, purpose, and findings. We observed that the participant consistently agreed with nearly all the labels and did not suggest further refinements. This may have been due to the highly specialized nature of the abstracts, which made it difficult for a broader audience to fully understand and evaluate the labels. As a result, we decided to switch to a Twitter Sentiment task for the second pilot. In this second pilot, we found that our guidelines were too flexible, leading to participant confusion and uncertainty about how to proceed. We made adjustments to provide more structure, such as requiring participants to complete at least four iterations, with each iteration involving the annotation of 10 out of 50 instances. After verification, participants were instructed to refine their rules and add gold standard labels. Based on the results of the two pilot studies, we extended the study duration from 60 to 90 minutes to give participants enough time to learn the system and complete the tasks. Compensation was also adjusted to $20. We tested these revised settings with the third participant and confirmed that the procedure worked effectively.\nParticipants Recruitment, Backgrounds, and Grouping. For our main study, we focused on recruiting individuals with reasonable familiarity with LLMs but relatively new to large-scale text data annotation. While PROMPTINGSHEET is designed as an end-user prompting tool, in this study, we prioritized participants likely to represent the first wave of \"newcomers\" (as noted in our Design Goals in Section 3.1) entering LLM-powered data annotation. This"}, {"title": "Findings", "content": "In this section, we organize our findings under each research question (RQs) mentioned in Section 1.1."}, {"title": "RQ 1-1: How effective are people at prompt engineering in the \u201cprompting in the dark\" scenario?", "content": "We evaluated each prompt iteration by comparing the labels generated by the LLM (GPT-40) to the gold labels participants manually annotated for a set of 50 tweets at the end of their study session. Performance was measured using accuracy (ACC) and Mean Square Error (MSE), two commonly used metrics in sentiment analysis [76]. In the main study, each participant provided at least five prompts (with some offering more), and our evaluation focuses primarily on these five prompts, which were shared across all participants. The ACC and MSE were calculated by comparing the annotation results from participant-provided prompts with the manually generated gold-standard labels. Each label was treated as a distinct category, and accuracy was then calculated accordingly.\n 5-Point sentiment rating is a highly subjective task. The premise of our study is that users bring their own personal perspectives and judgments to seemingly identical labeling tasks. To validate this assumption, we calculated both Cohen's kappa between participants' manual ratings-the labels collected at the end of the study-and the \"gold-standard\" labels from the dataset, as well as the agreement between participants themselves. Participants' labels show a poor alignment with the labels from the original dataset, with an average Kappa of 0.114 (SD=0.070). The average Kappa value between participants was only slightly higher, with an average Kappa of 0.249 (SD=0.059). The low Kappa score indicated the Twitter Sentiment task we used in our study was a highly subjective task. Namely, each participant's gold labels were highly influenced by their personal interpretations and preferences.\nPrompting in the dark is ineffective. Table 1 and Figure 6 show the average ACC and MSE for the first prompt and the four subsequent revisions. Our analysis, as captured in PROMPTINGSHEET, indicates that prompting in the dark is not particularly effective. Among the 20 participants, average labeling accuracy (where higher is better) only slightly improved from 0.542 to 0.553 after four iterations (Figure 6a). Some participants went through more than four iterations. The average labeling accuracy at the end of their sessions-the final iteration for all participants-was improved to 0.546. Meanwhile, the average MSE (where lower is better) fluctuated, ultimately increasing from 0.782 to 0.810 by the fourth revision (Figure 6b). The average MSE for the end-of-session increased to 0.815. It is important to note that, since this is a 5-scale rating task, ACC is a more harsh metric, awarding credit only for exact matches,"}, {"title": "RQ 1-2: How does sample size affect human performance in prompt engineering?", "content": "Reviewing 50 instances per iteration leads to more frequent and consistent improvements compared to reviewing 10 instances. Table 2 and Figure 9 present a comparison of participants who reviewed 50 instances per iteration against those who reviewed 10 instances per iteration.  In terms of accuracy, at the end of the session (i.e., four or more revisions), 6 out of 10 participants in the 50-instance group showed improvement, while only 3 out of 10 participants in the 10-instance group improved. On average, every iteration in the 50-instance group resulted in better accuracy compared to the initial prompt, though the improvement was not strictly increasing with each iteration.\nFor MSE, participants in the 50-instance group improved across three iterations, while those in the 10-instance group showed no improvement over the initial prompt in any round.\nWe also note that participants in the 10-instance group began with higher initial performance, but this was before they viewed the labeling results and occurred by chance, unrelated to the experimental conditions. Our analysis focuses on performance differences between iterations across both groups.\nSignificant Tests. We conducted eight linear mixed-effects models to examine the effect of iteration across four different conditions. The dependent variables were ACC and MSE, and participants were treated as random effects. In the condition where participants reviewed only 10 instances per iteration, we found a significant increasing trend in MSE as the iterations progressed (\u03b2=0.019, \u0440-value=0.043*)."}, {"title": "RQ 1-3: How does displaying LLM explanations impact human performance in prompt engineering?", "content": "Participants without access to LLM explanations improved their accuracy over multiple revisions, while those with access did not. Table 3 and Figure 10 show a comparison between participants with and without access to the LLM's explanations during the labeling process. At the end of the session (i.e., four or more revisions), 7 out of 12 participants without access to explanations improved their accuracy, while only"}, {"title": "Additional Analysis: How Does Each Variable Influence Rule Editing?", "content": "Building on the impact of the two variables on prompting outcomes (RQ 1-2 and 1-3), a key follow-up question is why these variables affect the outcomes differently. To explore this, we examined how the sample size per iteration and the presentation of LLM explanations influence how users edit the labeling rules-one of the main components of the final prompt-in PROMPTINGSHEET. For each prompt collected, we compiled all rules written by participants in the Rule"}, {"title": "RQ 2: Can automatic prompt optimization tools like DSPy improve human performance in \u201cprompting in the dark\" scenarios?", "content": "The previous section demonstrated that only a few settings were effective in helping participants improve prompt performance. This raises the question of how automatic prompt optimization tools might assist with refining prompts. In our study, we explored DSPy [43], a framework designed to algorithmically optimize LLM prompts, to enhance the prompt at each stage of revision by participants. DSPy is particularly effective at working with small sets of labeled data and abstract, generic initial prompts, making it well-suited for the \"prompting in the dark\" scenario.\nStudy Setups. We experimented with the following four approaches offered by DSPy:\n\u2022 Simple Prompt (Baseline): This approach uses the abstract prompts constructed by DSPy and employs DSPy's simplest teleprompter, BootstrapFewShots, to generate optimized examples based on all the few-shot examples labeled by participants throughout the study session. It is a simple method that does not account for differences between iterations or the user's initial prompt, making it a baseline approach for using DSPy.\n\u2022 BootstrapFewShots: This approach uses the task context (from Context sheet in PROMPTINGSHEET), label definitions (Rule Book sheet), and few-shot examples (Shots sheet) provided by participants in each iteration, and applies DSPy's simplest teleprompter, BootstrapFewShots, for optimization. The BootstrapFewShots teleprompter automatically generates optimized examples to be included in the user-defined"}, {"title": "User Feedback", "content": "In addition to addressing the main research questions, a post-study survey consisted of twenty-two questions, including seven Likert scale ratings and fifteen free-text responses from participants provided valuable insights on both \"prompting in the dark\" practices and our system. We summarize these insights in this section."}, {"title": "Two Variables Impacting Participant Ratings", "content": "Figure 15 displayed the seven Likert scale rating responses by participants. The seven survey questions can be categorized into seven different categories. Participants reviewing 10 instances reported higher satisfaction ratings. Both groups provided similar ratings for system ease of use, system intuitiveness, and efficiency in processing prompt engineering, with comparable variation. However, participants who reviewed 10 instances found the annotation tasks more difficult to understand compared to those who reviewed 50. Comparatively, participants who reviewed 10 instances reported higher levels of satisfaction with their performance, a stronger sense of prompt improvement, and better task completion rating."}, {"title": "Is PROMPTINGSHEET Useful?", "content": "Participants considered PromptingSheet helpful and efficient.  Participants found the Shots and Rule Book useful."}, {"title": "About \"Prompting in the Dark\"", "content": "Prompting in the dark without any tool is common.  Prompting in the dark is hard, as participants lacked confidence in their labels."}, {"title": "Users' Suggested Features", "content": "More automated supports for rule creation."}, {"title": "Discussion", "content": "Our study shows that \"prompting in the dark\"-where users gradually refine their expectations and understanding of data characteristics while iteratively prompting LLMs-is indeed a challenging task. Using PROMPTINGSHEET, we quantitatively assessed the difficulty of this process and the unreliability of the progress. Many strategies we tested proved ineffective, and the few that showed promise offered only weak signals. So, what should be designed in response? From the study, we derived insights that we believe are valuable for shaping future systems intended to support iterative LLM-powered labeling, which we summarize into three design suggestions.\n\u2022 Gold Labels as a Necessary Evil. The key premise of \"prompting in the dark\" is to avoid assuming a stabilized gold standard from the outset. This practice embraces the potential for the so-called \"gold standard\" in data labeling to evolve and remain dynamic, realizing this through the interactive nature of LLMs. Our system, PROMPTINGSHEET, embodies this practice, using a spreadsheet format. That being said, throughout our user studies, many participants faced struggles, frustrations, and errors due to the lack of direction or guidance during the exploratory labeling process. The elephant in the room-as highlighted by decades of research on the importance of gold labels in crowdsourced data annotation pipelines (Section 2.4)-is that much of this lack of guidance stemmed from the absence of sufficient gold labels. To clarify, we mean \"sufficient\" as in 100 to 200 labeled text items before starting to use the system, not just the 10 examples users provided via the \"add gold shot\" function. With no gold labels or fewer than 10, we could not calculate metrics like similarity, accuracy, or error rates to give users meaningful feedback, nor could we track LLMs' behavioral shifts. These challenges arose as part of our effort to ensure that no substantial labeled items would anchor users' exploration, allowing them to freely evolve their goals. Our first design suggestion is that this cost might be too high. Based on our observations, having some form of guidance in the \"prompting in the dark\" scenario is likely worth it. We propose that users manually label at least a small set of data (e.g., 50-100 items), and that the system be intentionally designed to minimize the impact of these labels on constraining exploration. For example, the system could hide the labeled items from the user (even the ones they just labeled), show only signals such as accuracy, or periodically invite users to relabel data. One example worth highlighting\nAutomated Support to Reduce Distractions. Our second design suggestion is to incorporate automation to help reduce users' distractions during the \"prompting in the dark\" process\nDesign to Prevent Overreliance on LLMs. Our final and most critical design suggestion is to create systems that reduce the human tendency to overly trust and accept outputs from AI, particularly LLMs."}, {"title": "Limitations", "content": "We acknowledge several limitations in our work. First, we focused solely on the \"prompting in the dark\" practice for data labeling tasks, while many other scenarios, such as text generation or conversational agents, are also widely used when working with LLMs. Second, for RQ2, we only studied one automatic prompt optimization technique, DSPy. It is possible that other methods might achieve better performance. Third, we recognize that, aside from the inherent challenges of the open-ended and exploratory nature of \"prompting in the dark,\u201d the workflow of our system is somewhat complex. This complexity may have contributed to some of the reliability issues observed in our study, though we believe similar challenges would apply to any system supporting newly emerged user practices. Fourth, since PROMPTINGSHEET is a Google Sheets Add-on, it inherits limitations and design constraints typical of spreadsheets, such as limited support for images or videos. Fifth, it was difficult to track how thoroughly participants reviewed samples. Some may have directly modified rules or added gold shots without careful consideration. Sixth, we lack a clear understanding for how sensitive LLMs are to prompt revisions. Tools like DSPy [43] and related experiments show that prompt revisions can systematically influence LLM prediction performance, but the degree of sensitivity remains unclear. This limited understanding of LLM behavior impacts the generalizability of our findings. Finally, our user study, though already 1-2 hours long, only captures a single session's worth of behavior. A longitudinal study that observes how users interact with PROMPTINGSHEET over months or even years might offer different insights."}, {"title": "Conclusion and Future Work", "content": "This paper studies a scenario in LLM-powered data labeling called \"prompting in the dark,\u201d where users iteratively prompt LLMs to label data without relying on a stabilized, pre-labeled gold-standard dataset for benchmarking. We developed PROMPTINGSHEET, a Google Sheets add-on that allows users to compose, revise, and iteratively label data within spreadsheets. Our user study revealed that prompting in the dark was highly ineffective and unreliable, and"}]}