{"title": "Ask, Attend, Attack: A Effective Decision-Based Black-Box Targeted Attack for Image-to-Text Models", "authors": ["Qingyuan Zeng", "Zhenzhong Wang", "Yiu-ming Cheung", "Min Jiang"], "abstract": "While image-to-text models have demonstrated significant advancements in various vision-language tasks, they remain susceptible to adversarial attacks. Existing white-box attacks on image-to-text models require access to the architecture, gradients, and parameters of the target model, resulting in low practicality. Although the recently proposed gray-box attacks have improved practicality, they suffer from semantic loss during the training process, which limits their targeted attack performance. To advance adversarial attacks of image-to-text models, this paper focuses on a challenging scenario: decision-based black-box targeted attacks where the attackers only have access to the final output text and aim to perform targeted attacks. Specifically, we formulate the decision-based black-box targeted attack as a large-scale optimization problem. To efficiently solve the optimization problem, a three-stage process Ask, Attend, Attack, called AAA, is proposed to coordinate with the solver. Ask guides attackers to create target texts that satisfy the specific semantics. Attend identifies the crucial regions of the image for attacking, thus reducing the search space for the subsequent Attack. Attack uses an evolutionary algorithm to attack the crucial regions, where the attacks are semantically related to the target texts of Ask, thus achieving targeted attacks without semantic loss. Experimental results on transformer-based and CNN+RNN-based image-to-text models confirmed the effectiveness of our proposed AAA.", "sections": [{"title": "I. INTRODUCTION", "content": "Image-to-text models, referring to generating descriptive and accurate textual descriptions of images, have received increasing attention in various applications, including image-captioning [1], [2], visual-question-answering [3], [4], and image-retrieval [5], [6]. Despite the remarkable progress, they are vulnerable to deliberate attacks, giving rise to concerns about the reliability and trustworthiness of these models in real-world scenarios. For example, one may mislead models to output harmful content such as political slogans and hate speech by making imperceptible perturbations to images [7]\u2013[9].\nTo gain insight into the reliability and trustworthiness of the image-to-text models, a series of adversarial attack methods have been proposed to poison the outputted textual descriptions of given images [7]\u2013[10]. Specifically, based on the attacker's level of access to information about the target model, they can be divided into three categories: white-box attacks [7], [10], [11], gray-box attacks [8], [9], and black-box attacks [12], [13]. The white-box attacks can obtain target models' information including the entire architecture, parameters, gradients of both the image encoder and_text decoder, and probability of each word of the output text. Gray-box attacks can only access the architecture, parameters, and gradients of the image encoder, while black-box attacks cannot access any internal information of the target model, but only the output text of the model. Furthermore, black-box attacks can be divided into score-based and decision-based attacks. Score-based black-box attacks can access the probability of each word of the output text [13], while decision-based black-box attacks can only access the output text [12], [14], [15].\nBecause less information about the target models is provided, decision-based black-box attacks are more challenging than other categories [15]. Additionally, these attack methods can be categorized based on whether the attacker is able to specify the incorrect output text, dividing them into two types: targeted and untargeted attacks [8], [16].\nAlthough numerous adversarial attack methods for image-to-text models have been proposed, to our best knowledge, the study on black-box attacks is under-explored, especially decision-based black-box targeted attacks. This kind of attack is more challenging due to the following reasons. Firstly, less information on the target model can be accessed. Specifically, only the output text instead of gradients, architectures, pa-rameters, and the probability of each word in the output text is available. Secondly, the attackers not only cause the target model to output incorrect text, but also outputs the specified target text. Existing attacks easily suffer from the loss of semantics, resulting in the inability to effectively output the specified target text. Figure 1 (a) show that transfer+query [9] fabricates one target text to poison the target image-to-text model, leading to this model outputting an incorrect text. However, the output text could mismatch the original semantics of the target text, as the target image-to-text model may focus on secondary information while ignoring the crucial semantics of the target text behind the target image, resulting in semantic loss. More examples are in Appendix ??.\nTo narrow the research gap, we propose a decision-based black-box targeted attack approach for image-to-text models. In our work, only the output text of the target model can be accessed, which is closer to the real-world cases [12]. Additionally, Figure 1 (b) demonstrates our targeted attack method, which optimizes against the target text directly under the decision-based black-box conditions, preventing semantic loss and maintaining semantic consistency with the target text.\nPerturbing pixels in the image can change the output text. Therefore, the objective of the targeted attack can be considered to find the imperceptible pixel modification to make the output text similar to the target text. In this manner, the targeted attack can be formulated as a large-scale optimization problem, where pixels are decision variables and the optimization objective is to poison the output text. Inspired by the distinctive competency of evolutionary algo-rithms for solving large-scale optimization problems [17]\u2013[19], we develop a dedicated evolutionary algorithm-based framework for decision-based black-box targeted attacks on image-to-text models. However, directly applying evolutionary algorithms to solve this large-scale optimization problem could suffer from low search efficiency, due to the numerous pixels and their wide range of values. To address the issue, we embed three-step processes, i.e., Ask, Attend, Attack, into the proposed evolutionary algorithm-based attacks. As shown in Figure 2, during the Ask stage, attackers can arbitrarily specify words related to certain semantics, such as photograph. Then, candidate words (e.g., camera, scenic, and phone) that are related to certain semantics are searched. Meanwhile, these words are close to the clean image in the feature space of the target image-to-text model. By selecting words from the candidate words, the target text (e.g., a cute girl using a phone to take pictures of the fantastic TV) related to the attacker's specified semantics can be formed to poison the target model. Subsequently, based on the attention mechanism, Attend identifies the crucial regions of the clean image (e.g. attention heatmap), thus reducing the search space for the subsequent Attack. Lastly, Attack uses a differential evolution strategy to impose imperceptible adversarial perturbations to the crucial regions, where the optimization objective is to minimize the discrepancy between the target text in Ask stage and the output text of the target model. Our contributions can be summarized as follows:\n1) We first propose a decision-based black-box targeted attack Ask, Attend, Attack (AAA) for image-to-text mod-els. Specifically, our method achieves targeted attacks without losing semantics while only the model's output text can be accessed.\n2) We designed a target semantic directory to guide attack-ers in creating target text and utilized attention heatmaps to significantly reduce search space. This improves the search efficiency of evolutionary algorithms in adversar-ial attacks and makes attacks difficult to perceive.\n3) We conducted extensive experiments on the Transformer-based VIT-GPT2 model and CNN+RNN-based Show-Attend-Tell model, which are the two most-used image-to-text models in HuggingFace, and surprisingly found that our decision-based black-box method has stronger attack performance than existing gray-box methods."}, {"title": "II. RELATED WORK", "content": "In white-box attacks, the attacker has full access to all parameters, gradients, architecture of the target model, and the probability of each word of the output text. The authors in [7] add invisible perturbations to the image to make the image-to-text model produce wrong or targeted text outputs. The authors in [20] add global or local perturbations to the image to make the vision and language models unable to correctly locate and describe the content of the image. The authors in [11] modify the content of the image at the semantic level to make the image-to-text model output text that is inconsistent with the original image. The authors in [21] crafts adversarial examples with semantic embedding of targeted captions as perturbation in the complex domain. The authors in [22] preserves the accuracy of non-target words while effectively removing target words from the generated captions. The authors in [23] generate coherent and contex-tually rich story endings by integrating textual narratives with relevant visual cues. The authors in [10] add limited-area perturbations to the image to make the image-to-text model fail to correctly describe the content of the perturbed area. The above methods require complete information of the image-to-text target model, including architecture, gradients, parameters, and probability distribution of the output text, which limits their practicality.\nTo improve the practicality of adversarial attacks for image-to-text models, recent research explores how to attack with partial knowledge of the target model. All existing gray-box attack studies [8], [9], [16], [24] assume full access to the image encoder of the image-to-text model. The basic idea of gray-box targeted attacks is to reduce the distance between the adversarial image and the target image generated based on the target text in the image encoder's feature space. The authors in [24] generates adversarial images to mimic the feature representation of original images. The authors in [16] use a generative model to destroy the image encoder's features, achieving the untargeted attack. The authors in [8] minimize the feature distance in the image encoder between the adversarial image and the target image, thereby using gradient back-propagation to optimize the adversarial image and achieve the targeted attack. The authors in [9] combine existing gray-box method [8] with pseudo gradient estimation method [25] to achieve better performance in targeted attack. It is worth noting that they [9] call their method a black-box attack, but since they use the image encoder of the target model as the surrogate model, we classify their method as a gray-box attack. These gray-box attacks on image-to-text models are more practical than white-box attacks, but it is still unrealistic to assume that attackers can access the image encoder of the image-to-text model. Moreover, existing gray-box methods may have poor targeted attack performance due to the semantic loss mentioned above."}, {"title": "III. METHODOLOGY", "content": "The image-to-text model \\(G: \\mathcal{X} \rightarrow \\mathcal{Y}\\) maps the image domain \\(\\mathcal{X}\\) to the text domain \\(\\mathcal{Y}\\). A well-trained model should be able to accurately describe the content of the image using grammatically correct and contextually coherent text. Given a target text \\(y_t\\), the attacker's goal is to find an adversarial image \\(X_{adv}\\) that is visually similar to clean image x and can generate an adversarial text \\(Y_{adv}\\) that is semantically similar to \\(y_t\\). We formalize the optimization problem for black-box targeted attack as:\n\\(\\underset{X_{adv}}{\text{arg max}} S(G(X_{adv}), y_t) \\quad \text{s.t.} \\quad \\frac{1}{n} \\sum_{i=1}^{n} ||X_{adv}(i) - x(i)|| \\leq \\epsilon,\\)\nwhere \\(S(\u00b7,\u00b7)\\) represents the semantic similarity function between two texts, \\(\\epsilon\\) is the threshold for the average perturbation size per pixel, \\(X_{adv}(i)\\) and \\(x(i)\\) represents the value of the i-th pixel in the adversarial and clean images. n is the total number of pixels in all channels of the image.\nTo enhance the efficiency and stealth of decision-based black-box attacks, we propose the Ask, Attend, Attack (AAA) framework as shown in Figure 2. Ask: We compile a semantic dictionary from words within the input image's search space that align with the attacker's specified semantics. This facil-itates targeted text generation, meeting the attacker's target semantics while simplifying the search process. Attend: We employ attention visualization and a surrogate model to gen-erate an attention heatmap for the target text on the image, narrowing the search to significant decision variables and enhancing perturbation stealth. Attack: We use the differential evolution in the reduced search space to find the optimal solution that can mislead the target model to output target text. The framework's pseudo-code is detailed in Appendix ??.\nAccording to the target semantics, the goal of Ask is to find words in the feature space of the target model to form a target semantic dictionary. These words should be closer to the input image. Firstly, we treat each pixel in each channel of image x as a variable, which means the search space size is the product of length, width, and number of channels. And then generate NP (number of population) individuals to form a population based on the following formula:\n\\(x_{j(i)} = x(i) + rand(-1,1) \\cdot \\eta,\\)\nwhere \\(x(i)\\) is the i-th variable of clean image x, \\(x_{j(i)}\\) is the i-th variable of the j-th individual in the population, \\(\\eta\\) is a hyperparameter about the maximum search range, rand(-1,1) is a random number from the range of -1 to 1.\nSecondly, for each variable, random mutation occurs be-tween different individuals. The mutation for the i-th variable of the j-th individual \\(x_j(i)\\) is as follows:\n\\(v_{j}^{g}(i) = x_{1}^{g}(i) + F * (x_{2}^{g}(i) - x_{3}^{g}(i)),\\)\nwhere \\(v(i)\\) is the mutated variable for mutation in the g-th generation of \\(x_j(i)\\). \\(X_{1}(i)\\), \\(x_{2}(i)\\), and \\(x_{3}(i)\\) are three randomly selected individuals from the current population who are different from each other, F is the scaling factor."}, {"title": "D. Attend Stage", "content": "The goal of Attend is to calculate the target text's attention area on the image x. Because we do not have access to the internal information of the target model, we can only calculate the Grad-CAM attention heatmap [27] with the help of surrogate model f (such as ResNet trained in ImageNet). The surrogate model's sole purpose is to the compute attention heatmap. Since different models produce similar heatmaps for the same target text and input image, selecting a well-established visual model suffices [28]. The calculation formula of attention heatmap A is as follows:\n\\(A(i, j) = MAX \\left\\{ 0, \\sum_{k} \\sum_{i} \\sum_{j} \\frac{\\partial y_{c^*}} {\\partial F_{k}(i, j)} F_{k}(i, j) \right\\}\\),\nwhere \\(A(i, j)\\) is the decision-making contribution of the image to the target text at pixel (i,j), \\(F_{k}(i,j)\\) is the pixel (i, j) of the feature map of the k-th convolution kernel of the last convolutional layer of the surrogate model f, Z is the feature map's pixel count, \\(y_{c^*}\\) is the probability that f predicts that the image x belongs to class c*. We use \\(C = \\{C_1, C_2, \\ldots, C_{1000}\\}\\) for the ImageNet category names, where \\(c_i\\) is the i-th category name. We make the category text \\(Y_{c_i}\\) = \"a photo of + \\(C_i\\) from the category name \\(c_i\\). We calculate the category c* as:\n\\(c^* = \\underset{c \\in C}{\text{argmax}} \\frac{E(y_t) \\cdot E(y_{c_i})}{||E(y_t)||2||E(y_{c_i})||2},\\)\nwhere E is the text encoder of the pre-trained CLIP model, and c* is the closest category to the target text. We substitute c* into Formula 8 to get the target text's attention heatmap A. A(i, j) is the pixel (i, j)'s contribution to the target text. 1 means more contribution, and 0 means less contribution."}, {"title": "E. Attack Stage", "content": "The goal of Attack is to search for the best individual (adversarial sample) that outputs the target text yt in the smaller search space reduced by the attention heatmap. Firstly, we copy the attention heatmap A three times in the channel dimension to match the shape of the image x. We generated NP (number of population) individuals as a population with this formula:\n\\(x_{j}(i) = x(i) + rand(-A(i), A(i)) \\cdot \\eta,\\)\nwhere \\(x(i)\\) is the i-th variable of clean image x, \\(x_j(i)\\) is the i-th variable of the j-th individual in the population, A(i) is the contribution of the i-th variable to the target text, and rand(-A(i), A(i)) is a random number in the range from -A(i) to A(i). The value of A is less than 1, and its mean and median are about [0.3,0.4]. The search space volume from the attention heatmap is much smaller than a hypersphere with radius \\(\\eta\\), because the radius and volume have an exponential relationship. This improves the search efficiency and concealment of adversarial perturbation.\nSecondly, in order to accelerate convergence and better find the global optimal solution, we use the following Current-ToBest mutation [29]:\n\\(v_{j}^{g}(i) = x_{j}^{g}(i) + F * (x_{1}^{g}(i) - x_{2}^{g}(i)) + F * (x_{g_{best}}(i) - x_{j}^{g}(i)),\\)\nwhere \\(x(i)\\) is the i-th variable of the j-th individual in the g-th generation, \\(v(i)\\) is the mutated variable, \\(x_{gest}\\) is the best fitness individual in the g-th generation population, \\(x_1\\) and \\(x_2\\) are two randomly selected individuals in the g-th generation population, and F is the scaling factor. The main advantage of this mutation strategy is that it combines the information of the current individual \\(x_j\\) and the best fitness individual \\(x_{best}\\), which can better guide the search process towards the direction of the optimal solution.\nThirdly, we use Formula 4 to calculate the candidate individual u(i). We design the following formula to calculate the deep feature similarity \\(S_{clip}\\) between two texts (u and v):\n\\(S_{clip} = 1 - \\frac{E(u) \\cdot E(v)}{||E(u) ||2||E(v) ||2},\\)\nwhere E is the text encoder of the pre-trained CLIP model. Text is discrete and complex, so it cannot calculate the distance directly [30]. Therefore, we use the CLIP text encoder E to extract the deep features of the texts, and then calculate the feature distance to obtain the similarity \\(S_{clip}\\) between the texts. The closer \\(S_{clip}\\) is to 0, the higher the similarity between the two texts u and v.\nUltimately, we select offspring using the following formula:\n\\(x_{j}^{g+1} = \begin{cases}u, & S_{clip}(G(u), y_t) \\le S_{clip}(G(x_j^g), y_t), \\ x_j^g, & \text{otherwise},\\end{cases}\\)\nwhere \\(x^{g+1}\\) is the next individual with closer feature distance between the output text and the target text yt. After performing the above evolutionary calculations multiple times, the optimal solution (adversarial sample) for outputting the target text is found."}, {"title": "IV. EVALUATION AND RESULTS", "content": "a) Model and dataset: We experimented with the two most-used image-to-text models on HuggingFace: VIT-GPT2 (Transformer-based) [31] and Show-Attend-Tell (CNN+RNN-based) [32]. VIT-GPT2 was trained on ImageNet-21k. Show-Attend-Tell was trained on MSCOCO-2014. We only used the target model's output text, not its internal information like gradients, parameters, or word probability. Following this work [8], we used Flick30k as our dataset, which has 31783 images and 5 caption texts each. We removed samples with less than 0.7 similarities between predicted text and truth text to ensure the target model's accuracy on clean images.\nb) Evaluation metrics: We used these evaluation metrics in our experiments: (1) BLEU(#4), an early machine transla-tion metric that measures text precision [33]. 1 means similar, and 0 means dissimilar. (2) METEOR, a more comprehensive metric that considers synonyms, stems, word order, etc [26]. 1 means similar, and 0 means dissimilar. (3) CLIP, the distance between the CLIP text encoder's deep features for two texts [34]. 1 means similar, and 0 means dissimilar. (4) SPICE, an evaluation metric tailored for image-to-text models [35]. 1 means similar, and 0 means dissimilar. (5) \\(\\epsilon\\), the mean perturbation size of each pixel of the adversarial sample [8].\nWe evaluate state-of-the-art gray-box attacks [8], [9] on image-to-text models. We designate the gray-box attack [8] as transfer (gray) and the one [9] as transfer+query (gray). To simulate a black-box environment, we adapted these gray-box attacks by employing the CLIP model's image encoder in lieu of the target model's encoder, resulting in \u201ctransfer (black)\u201d and \"transfer+query (black)\" variants. As depicted in Table I, adversarial samples generated by the original gray-box attacks exhibit a marked increase in textual similarity to the target text when compared to clean samples. Conversely, the black-box adaptations maintain a similarity level akin to that of clean samples, indicating a significant loss of attack capability upon changing the image encoder. This underscores the dependency of gray-box attacks on the target model's image encoder. Our proposed method AAA demonstrates superior attack per-formance in black-box scenarios compared to the existing methods in their native gray-box settings. This is attributed to the semantic loss inherent in existing gray-box attacks, which constrains their attacking potential. It is noteworthy that our work represents the first black-box attack on image-to-text models. So we can only compare our approach with existing gray-box attacks. We have adapted these gray-box attacks into a black-box version solely to demonstrate their ineffectiveness in a black-box scenario.\nWe conducted ablation experiments on our AAA method. AAA (w/o Attend) means no attention heatmap to reduce the search space, but the proportional reduction of the search range. AAA (w/o Ask) means the target text is not from the target semantic dictionary, but random words. Table I shows that losing any module decreases our attack performance. In addition, Ask performs worse than AAA (w/o Attend), indicating that finding a target text with lower search difficulty contributes relatively more to the performance of our targeted attack.\nWe presented the optimization curves of AAA and AAA (w/o Attend) in Figure 3. Figure 3 (a) and (b) illustrate the best and average fitness values during AAA and AAA (w/o Attend) optimization of VIT-GPT2 and Show-Attend-Tell. It is evident that the inclusion of Attend expedites and enhances the convergence of the population, with an equivalent perturbation size. Consequently, AAA exhibits more effective concealment in adversarial per-turbations, maintaining the same level of attack efficacy, as depicted in Figures 3 (d) and (e). Furthermore, we evaluated the impact of selecting different surrogate models during Attend. Notably, the sole function of the surrogate model is to compute the attention heatmap. Figure 4 demonstrates that, despite significant structural variances among several surrogate models, they produce strikingly similar attention heatmaps for the same target text and input images. This similarity arises from mapping the target text to the most pertinent category within the surrogate model's label space (as Formula 9). The position of the same category of objects on the same picture is constant, and the model needs to focus on the object first, no matter what structure it is [28]. Performance comparisons, as shown in Figure 4, indicate that the similarity in attention heatmaps across different surrogate models leads to similar final attack performances. Therefore, we opted for a stable, well-established, pre-trained model, such as ResNet-50, to serve as our surrogate model.\nWe used the words mirror, cell phone, man, looking at from the target semantic dictionary (as shown in Appendix ??) to make the target text a man is looking at a cell phone in a mirror. We compared output texts of our black-box method AAA and the existing gray-box method [9] for adversar-ial samples with different \\(\\epsilon\\), the average pixel perturbation size, in Figure 5. The same conclusion drawn from both methods is that bigger perturbation causes worse concealment and better attack performance; too small perturbation causes attack failure. Moreover, (f) and (j) in Figure 5 show that the existing methods have a semantic loss that limits their attack performance. Subjectively, target image (j) accurately draws the semantics of the target text, and the output text of adversarial image (f) perfectly describes the content of the target image (j). However the adversarial sample (f)'s output text does not have the semantics of the target text. Our method does not have semantic loss, so our black-box method AAA does a better targeted attack than the existing gray-box method. More examples of semantic loss are in Appendix ??.\nWe evaluated the computational efficiency of various attack methodologies for generating adversarial samples in image-to-text models. As depicted in Figure 6, our black-box attack method AAA demonstrates a longer computation time to reach an optimal solution compared to existing gray-box attacks. For instance, the transfer approach [8] illustrated in Figure 6 (a) produces an adversarial sample with a CLIP score of 0.82 within a mere 29 seconds, while the transfer+query approach [9] achieves a CLIP score of 0.85 in just 97 seconds. Conversely, our AAA method requires 151 seconds to generate an adversarial sample with a superior CLIP score of 0.951. The shorter computation times of the existing gray-box methods are expected due to their ability to access real gradients, which significantly expedites the optimization process. Given that adversarial attacks are not time-sensitive operations and considering that our AAA method delivers a more potent attack capability and is applicable in a broader range of realistic black-box scenarios, the trade-off for a higher computational cost is deemed acceptable. Additional experiments on similar-ity measurements are included in the Appendix ??.\nwe show the impact of different forms of target semantics TS in Ask on the target semantic dictionary, as shown in Appendix ??. More ambigu-ous target semantics can enrich the target semantic dictionary, which also means that the attacker has more choices when designing yt. Secondly, we show the effect of different word selection strategies of yt based on target semantic dictionary on the final attack effect, as shown in Appendix ??. Thirdly,"}, {"title": "V. CONCLUSION", "content": "In our research, we introduce a novel and practical approach for adversarial attacks on image-to-text models. We propose the Ask, Attend, Attack (AAA) framework, a decision-based black-box attack method that achieves targeted attacks without semantic loss, even with access limited to the target model's output text. Our framework uses the target semantic directory to guide the creation of target text and attention heatmap to reduce the search space, thereby improving the efficiency of evolutionary algorithms and making our attack harder to detect. Our extensive experiments on the Transformer-based VIT-GPT2 model and the CNN+RNN-based Show-Attend-Tell model demonstrate that our decision-based black-box method outperforms existing gray-box methods in targeted attack performance. These findings highlight the vulnerabilities in current image-to-text models and underscore the need for more robust defense mechanisms, significantly contributing to the field of adversarial machine learning and enhancing the security of vision-language systems."}]}