{"title": "Ask, Attend, Attack: A Effective Decision-Based Black-Box Targeted Attack for Image-to-Text Models", "authors": ["Qingyuan Zeng", "Zhenzhong Wang", "Yiu-ming Cheung", "Min Jiang"], "abstract": "While image-to-text models have demonstrated significant advancements in various vision-language tasks, they remain susceptible to adversarial attacks. Existing white-box attacks on image-to-text models require access to the architecture, gradients, and parameters of the target model, resulting in low practicality. Although the recently proposed gray-box attacks have improved practicality, they suffer from semantic loss during the training process, which limits their targeted attack performance. To advance adversarial attacks of image-to-text models, this paper focuses on a challenging scenario: decision-based black-box targeted attacks where the attackers only have access to the final output text and aim to perform targeted attacks. Specifically, we formulate the decision-based black-box targeted attack as a large-scale optimization problem. To efficiently solve the optimization problem, a three-stage process Ask, Attend, Attack, called AAA, is proposed to coordinate with the solver. Ask guides attackers to create target texts that satisfy the specific semantics. Attend identifies the crucial regions of the image for attacking, thus reducing the search space for the subsequent Attack. Attack uses an evolutionary algorithm to attack the crucial regions, where the attacks are semantically related to the target texts of Ask, thus achieving targeted attacks without semantic loss. Experimental results on transformer-based and CNN+RNN-based image-to-text models confirmed the effectiveness of our proposed AAA.", "sections": [{"title": "I. INTRODUCTION", "content": "Image-to-text models, referring to generating descriptive and accurate textual descriptions of images, have received increasing attention in various applications, including image-captioning [1], [2], visual-question-answering [3], [4], and image-retrieval [5], [6]. Despite the remarkable progress, they are vulnerable to deliberate attacks, giving rise to concerns about the reliability and trustworthiness of these models in real-world scenarios. For example, one may mislead models to output harmful content such as political slogans and hate speech by making imperceptible perturbations to images [7]\u2013[9].\nTo gain insight into the reliability and trustworthiness of the image-to-text models, a series of adversarial attack methods have been proposed to poison the outputted textual descriptions of given images [7]\u2013[10]. Specifically, based on the attacker's level of access to information about the target model, they can be divided into three categories: white-box attacks [7], [10], [11], gray-box attacks [8], [9], and black-box attacks [12], [13]. The white-box attacks can obtain target models' information including the entire architecture, parameters, gradients of both the image encoder and text decoder, and probability of each word of the output text. Gray-box attacks can only access the architecture, parameters, and gradients of the image encoder, while black-box attacks cannot access any internal information of the target model, but only the output text of the model. Furthermore, black-box attacks can be divided into score-based and decision-based attacks. Score-based black-box attacks can access the probability of each word of the output text [13], while decision-based black-box attacks can only access the output text [12], [14], [15]."}, {"title": "II. RELATED WORK", "content": "In white-box attacks, the attacker has full access to all parameters, gradients, architecture of the target model, and the probability of each word of the output text. The authors in [7] add invisible perturbations to the image to make the image-to-text model produce wrong or targeted text outputs. The authors in [20] add global or local perturbations to the image to make the vision and language models unable to correctly locate and describe the content of the image. The authors in [11] modify the content of the image at the semantic level to make the image-to-text model output text that is inconsistent with the original image. The authors in"}, {"title": "III. METHODOLOGY", "content": ""}, {"title": "A. Problem Formulation", "content": "The image-to-text model $G : X \\rightarrow Y$ maps the image domain X to the text domain Y. A well-trained model should be able to accurately describe the content of the image using grammatically correct and contextually coherent text. Given a target text yt, the attacker's goal is to find an adversarial image Xadv that is visually similar to clean image x and can generate an adversarial text Yadv that is semantically similar to yt. We formalize the optimization problem for black-box targeted attack as:\narg max S(G(Xadv), yt)  s.t. $1 \\over n$  $\\sum_{i=1}$ ||Xadv (i) - x(i)|| \u2264 \u0454,    (1)\nwhere S(\u00b7,\u00b7) represents the semantic similarity function between two texts, \u0454 is the threshold for the average perturbation size per pixel, Xadv (i) and x(i) represents the value of the i-th pixel in the adversarial and clean images. n is the total number of pixels in all channels of the image."}, {"title": "B. Overview", "content": "To enhance the efficiency and stealth of decision-based black-box attacks, we propose the Ask, Attend, Attack (AAA) framework as shown in Figure 2. Ask: We compile a semantic dictionary from words within the input image's search space that align with the attacker's specified semantics. This facilitates targeted text generation, meeting the attacker's target semantics while simplifying the search process. Attend: We employ attention visualization and a surrogate model to generate an attention heatmap for the target text on the image, narrowing the search to significant decision variables and enhancing perturbation stealth. Attack: We use the differential evolution in the reduced search space to find the optimal solution that can mislead the target model to output target text. The framework's pseudo-code is detailed in Appendix ??."}, {"title": "C. Ask Stage", "content": "According to the target semantics, the goal of Ask is to find words in the feature space of the target model to form a target semantic dictionary. These words should be closer to the input image. Firstly, we treat each pixel in each channel of image x as a variable, which means the search space size is the product of length, width, and number of channels. And then generate NP (number of population) individuals to form a population based on the following formula:\nxj(i) = x(i) + rand(\u22121,1) \u00b7 \u03b7,    (2)\nwhere x(i) is the i-th variable of clean image x, xj(i) is the i-th variable of the j-th individual in the population, \u03b7 is a hyperparameter about the maximum search range, rand(-1,1) is a random number from the range of -1 to 1.\nSecondly, for each variable, random mutation occurs between different individuals. The mutation for the i-th variable of the j-th individual xj (i) is as follows:\nv3(i) = x1 (i) + F * (x2(i) \u2013 x3(i)),     (3)\nwhere v(i) is the mutated variable for mutation in the g-th generation of xj (i). X1(i), x\u21162(i), and x23(i) are three randomly selected individuals from the current population who are different from each other, F is the scaling factor."}, {"title": "D. Attend Stage", "content": "The goal of Attend is to calculate the target text's attention area on the image x. Because we do not have access to the internal information of the target model, we can only calculate the Grad-CAM attention heatmap [27] with the help of surrogate model f (such as ResNet trained in ImageNet). The surrogate model's sole purpose is to the compute attention heatmap. Since different models produce similar heatmaps for the same target text and input image, selecting a well-established visual model suffices [28]. The calculation formula of attention heatmap A is as follows:\nA(i, j) = MAX (0, $\\sum_{k=1}^Z$$\\sum_{i=1}^I$  $\\sum_{j=1}^J$ $\\partial yc \\over dFk(i, j)$  Fk(i, j)),    (8)\nwhere A(i, j) is the decision-making contribution of the image to the target text at pixel (i,j), Fk(i,j) is the pixel (i, j) of the feature map of the k-th convolution kernel of the last convolutional layer of the surrogate model f, Z is the feature map's pixel count, yc* is the probability that f predicts that the image x belongs to class c*. We use C = {$C1,C2,\u2026\u2026,C1000$} for the ImageNet category names, where ci is the i-th category name. We make the category text Yci = \"a photo of + Ci from the category name ci. We calculate the category c* as:\nC*  =  argmax  EC    ||E(yt) \u00b7 E(Yc\u1d62) || \\over ||E(yt)||||E(Yc\u1d62)||,  (9)\nwhere E is the text encoder of the pre-trained CLIP model, and c* is the closest category to the target text. We substitute c* into Formula 8 to get the target text's attention heatmap A. A(i, j) is the pixel (i, j)'s contribution to the target text. 1 means more contribution, and 0 means less contribution."}, {"title": "E. Attack Stage", "content": "The goal of Attack is to search for the best individual (adversarial sample) that outputs the target text yt in the smaller search space reduced by the attention heatmap. Firstly, we copy the attention heatmap A three times in the channel dimension to match the shape of the image x. We generated NP (number of population) individuals as a population with this formula:\nxj(i) = x(i) + rand(\u2212A(i), A(i)) \u00b7 \u03b7,  (10)\nwhere x(i) is the i-th variable of clean image x, xj(i) is the i-th variable of the j-th individual in the population, A(i) is the contribution of the i-th variable to the target text, and rand(-A(i), A(i)) is a random number in the range from -A(i) to A(i). The value of A is less than 1, and its mean and median are about [0.3,0.4]. The search space volume from the attention heatmap is much smaller than a hypersphere with radius \u03b7, because the radius and volume have an exponential relationship. This improves the search efficiency and concealment of adversarial perturbation."}, {"title": "IV. EVALUATION AND RESULTS", "content": ""}, {"title": "A. Experiment setups", "content": "a) Model and dataset: We experimented with the two most-used image-to-text models on HuggingFace: VIT-GPT2"}, {"title": "B. Experiment results", "content": "a) Comparison experiment of existing gray-box attacks.: We evaluate state-of-the-art gray-box attacks [8], [9] on image-to-text models. We designate the gray-box attack [8] as transfer (gray) and the one [9] as transfer+query (gray). To simulate a black-box environment, we adapted these gray-box attacks by employing the CLIP model's image encoder in lieu of the target model's encoder, resulting in \u201ctransfer (black)\u201d and \"transfer+query (black)\" variants. As depicted in Table I, adversarial samples generated by the original gray-box attacks exhibit a marked increase in textual similarity to the target text when compared to clean samples. Conversely, the black-box adaptations maintain a similarity level akin to that of clean samples, indicating a significant loss of attack capability upon changing the image encoder. This underscores the dependency of gray-box attacks on the target model's image encoder. Our proposed method AAA demonstrates superior attack per-formance in black-box scenarios compared to the existing methods in their native gray-box settings. This is attributed to the semantic loss inherent in existing gray-box attacks, which constrains their attacking potential. It is noteworthy that our work represents the first black-box attack on image-to-text models. So we can only compare our approach with existing gray-box attacks. We have adapted these gray-box attacks into a black-box version solely to demonstrate their ineffectiveness in a black-box scenario.\nb) Ablation experiment of our black-box attack.: We conducted ablation experiments on our AAA method. AAA (w/o Attend) means no attention heatmap to reduce the search space, but the proportional reduction of the search range. AAA (w/o Ask) means the target text is not from the target semantic dictionary, but random words. Table I shows that losing any module decreases our attack performance. In addition, Ask performs worse than AAA (w/o Attend), indicating that finding a target text with lower search difficulty contributes relatively more to the performance of our targeted attack.\nc) Qualitative experiment of attention.: We presented the optimization curves of AAA and AAA (w/o Attend) in Figure 3. Figure 3 (a) and (b) illustrate the best and average fitness values during AAA and AAA (w/o Attend) optimization of VIT-GPT2 and Show-Attend-Tell. It is evident that the inclusion of Attend expedites and enhances the convergence of the population, with an equivalent perturbation size. Consequently, AAA exhibits more effective concealment in adversarial per-turbations, maintaining the same level of attack efficacy, as depicted in Figures 3 (d) and (e). Furthermore, we evaluated the impact of selecting different surrogate models during Attend. Notably, the sole function of the surrogate model is to compute the attention heatmap. Figure 4 demonstrates that, despite significant structural variances among several surrogate models, they produce strikingly similar attention heatmaps for the same target text and input images. This similarity arises from mapping the target text to the most pertinent category within the surrogate model's label space (as Formula 9). The position of the same category of objects on the same picture is constant, and the model needs to focus on the object first, no matter what structure it is [28]. Performance comparisons, as shown in Figure 4, indicate that the similarity in attention heatmaps across different surrogate models leads to similar final attack performances. Therefore, we opted for a stable, well-established, pre-trained model, such as ResNet-50, to serve as our surrogate model.\nd) Qualitative experiment of different perturbation sizes.: We used the words mirror, cell phone, man, looking at from the target semantic dictionary (as shown in Appendix ??) to make the target text a man is looking at a cell phone in a mirror. We compared output texts of our black-box method AAA and the existing gray-box method [9] for adversar-ial samples with different e, the average pixel perturbation size, in Figure 5. The same conclusion drawn from both methods is that bigger perturbation causes worse concealment and better attack performance; too small perturbation causes attack failure. Moreover, (f) and (j) in Figure 5 show that the existing methods have a semantic loss that limits their attack performance. Subjectively, target image (j) accurately draws the semantics of the target text, and the output text of adversarial image (f) perfectly describes the content of the target image (j). However the adversarial sample (f)'s output text does not have the semantics of the target text. Our method does not have semantic loss, so our black-box method AAA does a better targeted attack than the existing gray-box method. More examples of semantic loss are in Appendix ??. \ne) Comparison experiment on computation time.: We evaluated the computational efficiency of various attack methodologies for generating adversarial samples in image-to-text models. As depicted in Figure 6, our black-box attack method AAA, demonstrates a longer computation time to reach an optimal solution compared to existing gray-box attacks. For instance, the transfer approach [8] illustrated in Figure 6 (a) produces an adversarial sample with a CLIP score of 0.82 within a mere 29 seconds, while the transfer+query approach [9] achieves a CLIP score of 0.85 in just 97 seconds. Conversely, our AAA method requires 151 seconds to generate an adversarial sample with a superior CLIP score of 0.951. The shorter computation times of the existing gray-box methods are expected due to their ability to access real gradients, which significantly expedites the optimization process. Given that adversarial attacks are not time-sensitive operations and considering that our AAA method delivers a more potent attack capability and is applicable in a broader range of realistic black-box scenarios, the trade-off for a higher computational cost is deemed acceptable. Additional experiments on similar-ity measurements are included in the Appendix ??. \nf) Further analyses.: Firstly, we show the impact of different forms of target semantics TS in Ask on the target semantic dictionary, as shown in Appendix ??. More ambigu-ous target semantics can enrich the target semantic dictionary, which also means that the attacker has more choices when designing yt. Secondly, we show the effect of different word selection strategies of yt based on target semantic dictionary on the final attack effect, as shown in Appendix ??. Thirdly,"}, {"title": "V. CONCLUSION", "content": "In our research, we introduce a novel and practical approach for adversarial attacks on image-to-text models. We propose the Ask, Attend, Attack (AAA) framework, a decision-based black-box attack method that achieves targeted attacks without semantic loss, even with access limited to the target model's output text. Our framework uses the target semantic directory to guide the creation of target text and attention heatmap to reduce the search space, thereby improving the efficiency of evolutionary algorithms and making our attack harder to detect. Our extensive experiments on the Transformer-based VIT-GPT2 model and the CNN+RNN-based Show-Attend-Tell model demonstrate that our decision-based black-box method outperforms existing gray-box methods in targeted attack performance. These findings highlight the vulnerabilities in current image-to-text models and underscore the need for more robust defense mechanisms, significantly contributing to the field of adversarial machine learning and enhancing the security of vision-language systems."}]}