{"title": "INTERTRANS: Leveraging Transitive Intermediate Translations to Enhance LLM-based Code Translation", "authors": ["Marcos Macedo", "Yuan Tian", "Pengyu Nie", "Filipe R. Cogo", "Bram Adams"], "abstract": "Code translation aims to convert a program from one programming language (PL) to another. This long-standing software engineering task is crucial for modernizing legacy systems, ensuring cross-platform compatibility, enhancing performance, and more. However, automating this process remains challenging due to many syntactic and semantic differences between PLs. Recent studies show that even advanced techniques such as large language models (LLMs), especially open-source LLMs, still struggle with the task. Currently, code LLMs are trained with source code from multiple programming languages, thus presenting multilingual capabilities. In this paper, we investigate whether such capabilities can be harnessed to enhance code translation. To achieve this goal, we introduce INTERTRANS, an LLM-based automated code translation approach that, in contrast to existing approaches, leverages intermediate translations to bridge the syntactic and semantic gaps between source and target PLs. INTERTRANS contains two stages. It first utilizes a novel Tree of Code Translation (ToCT) algorithm to plan transitive intermediate translation sequences between a given source and target PL, then validates them in a specific order. We evaluate INTERTRANS with three open LLMs on three benchmarks (i.e., CodeNet, HumanEval-X, and TransCoder) involving six PLs. Results show an absolute improvement of 18.3% to 43.3% in Computation Accuracy (CA) for INTERTRANS over Direct Translation with 10 attempts. The best-performing variant of INTERTRANS (with the Magicoder LLM) achieved an average CA of 87.3%-95.4% on three benchmarks.", "sections": [{"title": "I. INTRODUCTION", "content": "Automatically translating source code between different programming languages (PLs) can significantly reduce the time and effort required for software development teams. In the literature, researchers have proposed various auto-mated code translation methods. Data-driven learning-based approaches [35], [37] have shown impressive improvements over traditional rule-based methods [1], [2], [7]. Unlike rule-based approaches, which rely on handcrafted rules and pro-gram analysis techniques, learning-based methods can auto-matically learn syntactic and semantic patterns from large-scale code repositories.\nLarge language models (LLMs) represent the most advanced learning-based approaches developed in recent years and have demonstrated promising results across various software engi-neering tasks [16]. Pre-trained on vast amounts of code (across dozens of PLs) and text data, and equipped with billions of parameters, LLMs can be applied directly to code translation without the need for task-specific continuous training/fine-tuning. This would eliminate the need for costly and time-consuming processes involved in collecting training datasets and developing specialized models for code translation.\nHowever, recent studies have shown that the performance of LLM-based automated code translation, particularly with open-source LLMs, is still far from the production level, with correct translations ranging from 2.1% to 47.3% [30], [41]. These studies found that many errors in LLM-generated code translations stem from the models' lack of understanding of syntactic and semantic discrepancies between source and target languages, which can vary significantly across different pairs. For instance, 80% of the errors in translating from C++ to Go are due to syntactic and semantic differences, while only 23.1% of such errors occur when translating from C++ to C [30]. This variation is intuitive, as certain PLs naturally share more similarities in syntax and semantics than others.\nA similar phenomenon has been observed in machine trans-lation for human languages, where translating between certain languages is easier than others [21]. To improve translations for challenging language pairs, a common strategy is to use parallel corpora with a pivot (bridge) language [20]. In fact, traditional statistical machine translation between non-English languages, such as French to German, often involves pivoting through English [40]. This approach remains effective with the rise of multilingual neural machine translation models. For instance, in a recent work by Meta [15], training language pairs were collected based on linguistic families and bridge languages, facilitating translation across numerous language pairs without exhaustively mining every possible pair.\nInspired by this idea, this paper explores the potential of leveraging transitive intermediate translations from a source PL into other PLs before translating to the desired target PL, an idea not previously explored in the field of automated code translation. For example, to translate a program written in Python to Java, we might first translate it from Python to C++ and then from C++ to Java, as illustrated in Figure 1. This process is done through prompting, without additional training data, thanks to code LLMs that are pre-trained on text and code across multiple PLs and naturally possess multilingual capabilities. While this idea is inspired by machine translation, its potential in the inference stage of LLM-based translation approaches has not been explored. Despite the conceptual simplicity of the idea, a major challenge to address is the choice of the number and type of intermediate language(s), since the optimal choice might be different for each pair of PLs or even each pair of code snippets.\nThe idea of utilizing existing PLs as \u201cbridges\" is different than earlier work, TransCoder-IR [37], a non-LLM learning-based method that enhances source code pairs by incorporat-ing their corresponding low-level, language-agnostic compiler Intermediate Representations (IR), such as LLVM IRs [24], into the training dataset. Instead of relying on one unified IR to bridge any pair of cross-PL translations, we systematically explore different potential transitive intermediate translations using multiple existing PLs.\nINTERTRANS, our novel LLM-based code translation ap-proach that enhances source-target translations via transitive intermediate translations, operates in two stages. In the first stage, a method called Tree of Code Translations (ToCT) generates a translation tree containing all potential translation paths for a specific source-target PL pair, conditioned to a set of pre-defined intermediate PLs and the maximum number of intermediate translations to be explored. In the second stage, translation paths are turned into LLM prompts that are executed in a breadth-first order. INTERTRANS then uses a readily available test suite to validate whether the generated translation to the target language is correct, enabling early termination of translation path exploration if a successful path is found before completely exploring the translation tree.\nTo evaluate the effectiveness of INTERTRANS, we con-ducted experiments using three code LLMs (Code Llama [33], Magicoder [38], and StarCoder2 [25]) on 4,926 translation problems sourced from three datasets, i.e., CodeNet [32], HumanEval-X [42], and TransCoder [34]. Each translation problem aims to translate a program writing in a source PL to a target PL. These problems involve 30 different source-target PL pairs across six languages: C++, JavaScript, Java, Python, Go, and Rust. Our results show that INTERTRANS consistently outperforms direct translation (i.e., without in-termediate language translation) with 10 attempts, achieving an absolute Computational Accuracy (CA) improvement of 18.3% to 43.3% (median: 28.6%) across the three LLMs and datasets. Through ablation studies, we analyzed the effects of varying the number and selection of intermediate languages on INTERTRANS's performance. Generally, increasing the num-ber of intermediate translations enhances CA, though the ben-efits taper off after three translations. Similarly, incorporating more intermediate languages is advantageous, but gains slow after including three languages. The effectiveness of specific intermediate PLs varies across translation pairs, with notable patterns observed in translations from C++/Python to Java via Rust and from Rust to Go via C++. The main contributions of this paper are as follows:\n\u2022 We present the first study demonstrating that intermediate translations based on existing PLs can enhance the perfor-mance of LLM-based code translation.\n\u2022 We propose ToCT, a novel planning algorithm designed to explore intermediate translations effectively. We also introduce INTERTRANS, an LLM-based code translation approach that uses ToCT and is orthogonal to existing approaches for code translation.\n\u2022 We conducted a comprehensive empirical study to evaluate INTERTRANS. Our results highlight the effectiveness of INTERTRANS in enhancing LLM-based code translation. We also provide insights for the practical application of INTERTRANS.\nThe code for implementing INTERTRANS, the datasets, and the notebooks for generating the experiment results are available at: https://github.com/RISElabQueens/InterTrans."}, {"title": "II. INTERTRANS", "content": "INTERTRANS translates programs from a source to a target language using an LLM and a series of transitive intermediate translations. The input of INTERTRANS includes: (1) a LLM, (2) a program $P_s$ written in a source language $L_s$, (3) the target language $L_t$, (4) a non-empty intermediate PL set $L$ which contains $L_s$ but excludes $L_t$, (5) a hyper-parameter $maxDepth$, which determines the maximum number of tran-sitive intermediate translations. INTERTRANS utilizes a readily available test suite to evaluate the accuracy of the generated program(s) $TP$ written in the target language, i.e., $TP = \\{P_t | P_t \\in PP_{L_t}, L_s \\neq t \\}$, where $PP_{L_t}$ is the set of programs written in $L_t$ that represent translation candidates for $P_s$.\nGiven a translation problem aimed at converting a source program $P_s$ into a target language $L_t$, INTERTRANS operates in two stages. In Stage 1, it constructs all possible translation (PL) paths using a novel approach called the Tree of Code Translations (ToCT), which identifies potential sequences of transitive translations from $L_s$ to $L_t$ via intermediate lan-guages from the set $L$. Stage 2 then uses the source program $P_s$ and the PL paths generated from Stage 1 to perform inferences with an LLM to generate a set of target programs $TP$ written in $L_t$. These programs, each corresponding to a translation path, are generated and verified sequentially against a test suite. The algorithm terminates when a successful translation is identified, indicated by a $P_t$ that passes the test suite. The following subsections provide detailed descriptions of each stage, accompanied by a running example."}, {"title": "A. Stage 1: Generating Tree of Code Translations (ToCT)", "content": "Algorithm 1 specifies how ToCT creates (plans) translation PL paths for a given translation PL pair utilizing a set of intermediate languages. Since ToCT operates at the level of translation PL pairs, this planning algorithm only needs to run once for all translation problems involving the same source and target languages.\nIn ToCT, the intermediate language set $L$ includes the source language $L_s$ but excludes the target language $L_t$. This is because $L_t$ should be the final target and should not occur as an intermediate step in the translation process, while we should allow $L_s$ to appear in intermediate translations (for cases where a source program can be \"simplified\" by translating to and from another PL). Below, we use a running example, shown in Figure 1, to illustrate this algorithm. In this example, we aim to translate a Python program to Java ($L_s$ is Python, $L_t$ is Java), and we consider a maximum depth (max Depth) of 3, meaning that at most three edges can be included in a translation path. The set of intermediate languages ($L$) includes five programming languages: Python, Rust, JavaScript, C++, and Go.\nTOCT (see Algorithm 1) starts by enqueueing and then dequeueing the source PL, yielding the current path starting from the source PL, i.e., [Python] and the current depth 0 in our running example. Since Python is not the target language, and the current depth is less than the maximum depth of 3, the algorithm continuously explores possible transitions either to an intermediate language (excluding Python, since a PL cannot be translated to itself) or directly to the target language to complete the translation path. This results in the following paths: [Python, Java], [Python, Rust], [Python, JavaScript], [Python, C++], and [Python, Go]. Each of these new paths, along with the incremented depth of 1, is enqueued into Q.\nContinuing this process, the algorithm dequeues [Python, Java] (i.e., the direct translation path) and since it ends with the target PL, this path will be added to the final translation PL path output list. Next, the algorithm dequeues [Python, Rust] and explores further transitions, appending each language from the set $L$ to the current path, but excluding Rust to avoid trans-lation between the same PLs. This results in new paths like [Python, Rust, Java], [Python, Rust, JavaScript], etc., which are then enqueued with a depth of 2. This process repeats for all potential paths within the specified maximum depth, ensuring all possible translation paths from Python to Java are explored and recorded. By the end of the algorithm, the list paths will contain all feasible sequences of translations from Python to Java, considering all given intermediate languages and the maximum depth argument."}, {"title": "B. Stage 2: Sequential Verification of ToCT", "content": "For a specific translation problem (source program), the second stage of the INTERTRANS approach (see Algorithm 2) takes the ToCT-generated plan for the problem's source and target PL, i.e., the list paths from Algorithm 1, to (1) deter-mine the order of the paths that will be verified (i.e., checked if they lead to a successful translation), (2) generate the translations using an LLM and a prompt template PromptT, and (3) evaluate the translations to the target language using the given test suite T. To make INTERTRANS more efficient, an early-stopping mechanism is applied (Lines 19-20): as soon as one path successfully translates the code into $L_t$, Algorithm 2 terminates."}, {"title": "III. EXPERIMENT DESIGN", "content": "We evaluate the effectiveness of INTERTRANS by answering the following three research questions:\n\u2022 RQ1: How effective is INTERTRANS compared to direct translation and other baselines?\n\u2022 RQ2: How could varying the max Depth affect the perfor-mance of INTERTRANS?\n\u2022 RQ3: How could varying the selection of intermediate languages affect INTERTRANS?"}, {"title": "A. Benchmark Dataset Collection and Pre-Processing", "content": "Our experiment dataset consists of 4,926 translation prob-lems across 30 source-target translation PL pairs involving six PLs - C++, Go, Java, JavaScript, Python, and Rust. When creating our experiment dataset, we considered three existing datasets. Below, we describe the creation of our experimental datasets from these sources.\nTransCoder: The original TransCoder dataset [34] was cre-ated by manually collecting coding problems and solutions written in C++, Java, and Python from GeeksforGeeks [4]. Recently, Yang et al. [41] discovered quality issues in this dataset and subsequently conducted a manual verification and curation of the dataset to ensure its correctness. In this study, we reused their cleaned version, containing a total of 2,826 translation problems and corresponding test suites. We employed the full version of this dataset for comparisons with SOTA learning-based approaches.\nHumanEval-X: HumanEval-X [42] extends the python-only code generation evaluation dataset HumanEval [12] with addi-tional canonical solutions and test cases in six PLs: C++, Go, Java, JavaScript, Python, and Rust. We created translation pairs for all 164 tasks in HumanEval-X across the six languages, resulting in 4,920 translation problems. Due to computational constraints (particularly required by the ablation studies per-formed to understand the impact of varying variables on the performance of INTERTRANS), we randomly sampled 1,050 translation problems, stratified across the 30 source-target translation pairs, ensuring a 99.9% confidence level.\nCodeNet: CodeNet [32] contains programs written in 55 programming languages for learning and evaluating coding tasks and was adopted in a recent empirical study by Pan et al. [30] on LLM introduced translation bugs. Programming tasks in CodeNet are verified by matching the program outputs with the expected results. For our study, we selected tasks with at least three test cases to ensure adequate test suite coverage, resulting in 1,112 programming tasks. From these tasks, we generated 15,660 translation problems by concentrating on the six PLs featured in HumanEval-X, removing problems with a file size exceeding 1KB (as a proxy for token length, to prevent inputting into the prompt problems longer than the model's token limit) and ensuring that each translated code snippet could be assessed using three test cases. We created a subset of 1,050 pairs from this dataset using stratified random sampling, ensuring a 99.9% confidence level."}, {"title": "B. Selected Large Language Models", "content": "InterTrans relies on an LLM that understands multiple PLs. Almost all recent code LLMs possess this multilingual capability. We have chosen the following three instruct-tuned LLMs over their base models, as instruct-tuned models are fine-tuned to follow prompted instructions more effectively.\nMagicoder [38]: An open-source collection of LLMs trained on 75K synthetic instruction-response pairs and includes mul-tiple model variants with different base models. All Magicoder models have around 7B parameters. We use the Magicoder-S-DS variant [6].\nStarCoder2 [25]: An open-source collection of LLMs offered by the BigCode project [9]. StarCoder2 has instruction-tuned versions ranging from 1B to 34B parameters. We use the StarCoder2-15B variant [8].\nCodeLlama [33]: An open-source collection of LLMs offered by Meta based on Llama 2, specialized in code generation, with 7B, 13B, and 34B parameters. We use the CodeLlama-13B variant [3].\nWe chose these models because of their proven effective-ness in code generation tasks and their open-source nature, which promotes accessibility and collaborative development. Additionally, we prioritized models compatible with efficient inference frameworks, i.e., vLLM [22], while also ensuring they work well with platforms such as the HuggingFace Text Generation Interface [39]. This ensures that our selected mod-els are not only high-performing but also practically feasible for widespread use in both research and industry settings."}, {"title": "C. Compared Approaches", "content": "Direct translation (CA@1 and CA@10): We compare INTERTRANS with direct translation by evaluating perfor-mance with a single attempt (CA@1) and multiple attempts (CA@10). For CA@10, a single prompt is used to generate ten translation candidates. The translation is considered successful if any of these ten attempts result in a correct translation. Comparing with CA@1 reveals the additional opportunities INTERTRANS discovers via ToCT. Since INTERTRANS utilizes multiple translation paths, it inherently makes more than one attempt, making a comparison with CA@1 alone insufficient. Hence, to find a fair number of attempts (k) for direct transla-tion, we analyzed how many attempts INTERTRANS required to achieve a successful translation across the experiments.\nOn average, 3.9 attempts were needed, with 75% of cases successful within two attempts and less than 0.1% requiring between 59 and 83 attempts. Therefore, we chose CA@10 as a stronger baseline, allowing ten attempts with a high temperature setting to generate diverse variants and increase the chances of passing the test suite. The distribution of the number of attempts made by INTERTRANS in our experiments is presented in the supplementary material.\nNon-LLM SOTA approaches: TransCoder [34] is an unsu-pervised model pre-trained with cross-lingual language model-ing, denoising auto-encoding, and back-translation, leveraging a vast amount of monolingual samples. TransCoder-IR [37], an incremental improvement, introduces the idea of using a low-level compiler Intermediate Representation (IR) to enhance translation performance. In addition to TransCoder's pretrain-ing tasks, TransCoder-IR includes translation language model-ing, translation auto-encoding, and IR generation. TransCoder-ST [35] is another enhanced version of TransCoder that uses automatically generated test cases to filter invalid translations, improving performance. These models are trained on only a few PLs, i.e., Python, C++, and Java.\nGPT-3.5 and its enhanced version: GPT-3.5 is a powerful closed LLM provided by OpenAI that is capable of code generation. We consider the gpt-3.5-turbo-0613 version. Uni-Trans with GPT-3.5 is an enhanced version designed for code translation, proposed by Yang et al. [41]. UniTrans generates test cases to aid LLMs in repairing errors by integrating test execution error messages into prompts. Despite UniTrans with GPT-3.5 requiring additional program repair and extra test cases, we include it as a baseline since it represents the state-of-the-art performance on the TransCoder dataset."}, {"title": "D. Evaluation Metric", "content": "Similar to recent studies on LLM-based code transla-tion [30], [41], we adopt execution-based evaluation met-rics, i.e., Computational Accuracy (CA) [34]. CA assesses whether a transformed target program produces the same outputs as the source function when given identical inputs. CA on a benchmark is the ratio of translation problems that have correctly translated to the target language. We choose CA over text-based metrics like BLEU score because LLMs can produce valid translations that differ from human-written references; text-based metrics might be misleading when evaluating translated code against the reference, i.e., they can yield high scores despite the two code versions being functionally distinct [43]. In our study, we aim to explore the effectiveness of intermediate PL translations in generating functionally equivalent programs instead of merely focusing on textual similarity."}, {"title": "E. Implementation", "content": "Our scalable reference implementation of the INTERTRANS algorithms is written in Go and implemented as a client (Python) and server (engine written in Go) architecture that communicates over gRPC [17]. The INTERTRANS engine utilizes vLLM [22] as the inference engine, given its perfor-mance and dynamic batching capabilities. It queries VLLM endpoints using round-robin to achieve data parallelism during inference and distribute the computational load evenly. The computational infrastructure used for our experiments consists of 6x NVIDIA RTX A6000 GPUs on an AMD EPYC Server with 128 CPU cores.\nTo ensure deterministic inference results from vLLM across all experiments involving InterTrans, we randomly generated a fixed random seed for inference. We set the decoder param-eters top-p to 0.95, top-k to 10, and the temperature to 0.7. When evaluating the baseline performance of direct translation with CA@1 and CA@10, we do not fix the seed to ensure we generate diverse candidates. The selection of top-p, top-k, and temperature aligns with recent studies on code LLMs [14].\nDuring our experiments, even after identifying a successful translation, we still continue to explore and verify all potential translation paths. While one would not do this in practice when using INTERTRANS, it was essential in our empirical study to collect comprehensive data on all translation paths needed for addressing our research questions, particularly RQ3 (impact of removing intermediate PLs). However, this does not impact the reported CA results for INTERTRANS."}, {"title": "IV. RESULTS AND ANALYSIS", "content": "Approach: In InterTrans, the max Depth is set to 4, allowing for a maximum of four translations (edges) in a trans-lation PL path. This parameter enables us to explore various translation paths (with 85 maximum attempts). The six PLs of the CodeNet and HumanEval-X benchmarks, i.e., Python, C++, JavaScript, Java, Rust, and Go, serve as intermediate languages. While the TransCoder dataset includes only Python, C++, and Java, additional languages like Rust, JavaScript, and Go can be used as intermediates. This flexibility is possible because INTERTRANS does not verify the correctness of intermediate translations unless they result in a program written in the target language.\nAs shown in Table I, INTERTRANS consistently surpasses direct translation (CA@1_and_ CA@10) across all three datasets and all studied LLMs. It achieves an absolute im-provement of 18.3% to 43.3% compared to direct CA@10. Specifically, on CodeNet, INTERTRANS shows an average absolute improvement of 26.2% for Code Llama, 38.3% for Magicoder, and 43.3% for StarCoder2 when compared to Direct (CA@10), the largest among three datasets for all three models. Overall, INTERTRANS with Magicoder performs the best, with the highest CA on both CodeNet and HumanEval-X (see grey filled cells in Table I). On TransCoder, INTERTRANS with StarCoder2 performs the best with a CA of 93.8%, slightly higher than INTERTRANS with Magicoder (90.8%).\nWhen comparing INTERTRANS with StarCoder2 (the best variant of INTERTRANS on TransCoder), to other state-of-the-art approaches on TransCoder, our approach outperforms all others across all six source-target PL pairs (see Table II). The second best performance is achieved by UniTrans with GPT-3.5. All the LLM-based approaches considered in Table II perform consistently better than the TransCoder models, fur-ther showcasing the promising potential of LLMs in automated code translation."}, {"title": "B. RQ2: Impact of Varying maxDepth", "content": "Approach: INTERTRANS utilizes two hyper-parameters, one of which is max Depth. This parameter controls the depth of the translation tree generated by Algorithm 1. In this research question, we investigate how this parameter affects the per-formance of INTERTRANS. Specifically, we vary max Depth from 1 (direct translation) to 4. We conducted pairwise com-parisons across different depths (1 vs. 2, 1 vs. 3, 1 vs. 4, 2 vs. 3, 2 vs. 4, and 3 vs. 4) to evaluate the significance of the performance changes (i.e., the number of successful and unsuccessful translations) using the Chi-Square statistical test. To account for multiple comparisons across levels within the same model and dataset, we apply the Bonferroni correction to an alpha level of 0.05. The results of our experiments with varying values for max Depth are shown in Figure 2.\nResults: We can observe that as the max Depth increases, the performance of INTERTRANS consistently improves, although the rate of improvement slows down towards longer paths. For instance, on HumanEval-X, increasing the max Depth from 1 to 2 results in an absolute improvement of 23.7% for Code Llama, from 2 to 3 results in an improvement of 6.6%, and from 3 to 4, the improvement is 3.2%. Similar patterns are observed across all nine combinations of models and datasets.\nRegarding the statistical tests performed, we find that for all datasets and models, there is a statistically significant im-provement in terms of CA as the depth increases. Exceptions to this trend are noted for Code Llama and StarCoder 2 in the TransCoder dataset, where there is no significant increase in the CA metric when increasing the depth from 3 to 4, and for Code Llama and Magicoder in the HumanEval-X dataset with the same depth change. In other words, out of 54 comparisons (6 depth changes \u00d7 9) conducted, only 4 cases of increasing the depth do not lead to a statistically significant improvement in performance, all involving an increase from depth 3 to 4.\nWe observe small differences (an average of 2.8%) when comparing the Direct (CA@1) reported in Table I with the results of this experiment using max Depth set to 1. These differences may be attributed to the fact that for Direct CA@1 and CA@10, we did not fix the random seed. Consequently, a different seed was used."}, {"title": "C. RQ3: Impact of Varying the Intermediate Programming Languages", "content": "Approach: Besides max Depth, the other hyper-parameter of INTERTRANS is the set of intermediate PLs considered, which determines the width of the translation tree created by TOCT. In this RQ, we investigate the impact of reducing the set and specific types of intermediate PLs by addressing the following two sub-RQs:\n\u2022 RQ3.1: How does the number of available intermediate PLs influence the performance of INTERTRANS?\n\u2022 RQ3.2: How does the removal of a specific intermediate PL affect the performance of INTERTRANS?\nTo address the above two sub-RQs, we first conducted an ablation study across all possible combinations of intermediate PLs from the experiments conducted in RQ1 and RQ2, using a maxDepth of 4 with six PLs. Each ablation involves the removal of all translation paths that contain a subset of the set of intermediate PLs. In particular, for each translation, we computed all 31 possible combinations of removing 1 to 5 PLs from the intermediates (i.e. all combinations of intermediate PLs, except those that include the target language). We then re-moved the edges that involve each individual set and measured whether the translation remained successful (i.e., at least one translation path leads to a correct translation). This ablation was performed for each sample of the nine experiments (3 datasets and 3 LLMs), and we recorded which removed sets caused the translation to be unsuccessful. For this analysis, we leveraged the data we generated during our evaluation described in Section IV-A, where we recorded the execution result of all translation paths in the translation trees.\nTo answer RQ3.1 in specific, we aggregated the results from the 458,118 translations (4,926 tasks from 3 datasets, each with 31 removal combinations using 3 different models) based on the number of intermediate PLs removed, i.e., the cardinality of the set of removed PLs. This analysis helps us understand the overall impact of the number of intermediate languages on translation success rate. Additionally, in RQ3.2, to investigate whether specific lan-guages are more impactful as intermediates, we analyzed the results from the translations of RQ3.1 that are associated with the removal of a single intermediate PL. We then calculated the mean absolute decrease in translation success for each of the 30 PL pairs in our experiments, caused by the removal of each specific PL."}, {"title": "A. Implications", "content": "INTERTRANS vs. Other Tools: We demonstrate that INTER-TRANS is a better alternative to state-of-the-art approaches. By leveraging easily accessible open-source code LLMs, such as Magicoder, INTERTRANS achieved an average CA ranging from 87.3%-95.4% across three datasets involving 30 different source-target PL pairs. Furthermore, INTERTRANS requires only readily available LLMs and a relatively limited depth and set of intermediate languages to perform effectively.\nComputational Cost and Efficiency of INTERTRANS: De-spite implementing various optimizations in INTERTRANS, such as caching inferences of shared edges, the system remains computationally expensive. This is largely due to the ToCT al-gorithm, which explores multiple translation paths, each poten-tially containing numerous translations. However, the signifi-cant translation performance gains suggest that INTERTRANS can save considerable time compared to alternatives, especially in contexts where human resources are costly. Additionally, by leveraging existing LLMs, we avoid the expense of training a specific code translation model. In practice, the actual cost of INTERTRANS is much lower than the theoretical maximum, as our experiments indicate an average of only 3.9 attempts per successful translation. Future research can improve its efficiency by parallelizing the currently sequential inference process and developing methods to predict the most likely successful path for specific translation problems instead of iteratively evaluating different paths.\nMultiple and Dynamic Intermediates vs. Unified IR: Our study confirms that prior work was on the right track by utilizing intermediate representations. However, our approach innovates by employing multiple, dynamic intermediates tai-lored to each source-target language pair, utilizing existing PLs instead of a unified compiler-level representation like LLVM IR. Our findings suggest that a single, fixed intermediate language may not suffice, as the performance impact varies depending on the languages involved. Even though each successive intermediate translation can potentially increase the risk of propagating translation errors to the next translation, in practice this risk turned out to be moderate, with substantial improvements of translation quality. Through an initial analysis of three translation patterns, we uncovered several interesting insights. However, future work is needed to understand why certain paths are more effective in particular scenarios and to develop methods for recommending the optimal translation path for a given trans-lation problem."}, {"title": "B. Threats to Validity", "content": "Internal Validity: We performed the translation only once for each translation problem, using a fixed random seed for study LLMs when reporting the performance of INTERTRANS. This design reduces the risk of selecting a favorable seed across all nine experiments. However, altering this seed could affect the reported performance. Nonetheless, this does not affect the comparison between INTERTRANS with direct translation (as shown in Table 2, where depth=1 represents direct translation under identical conditions), or the empirical analysis of vary-ing parameters, which are our main goals.\nFurthermore, we employed a single prompt template for each dataset; changing this template might also alter the reported performance across all models. However, this does not affect comparison results, as we used the same prompt for all models, including direct translation and INTERTRANS. To mitigate the effects of LLMs' sensitivity to prompt templates, we adhered to best practices from the literature. Future practi-tioners can explore potential improvements in prompt design.\nThe temperature and top-p, top-k values were set consis-tently across all LLMs, following established literature. While these may not be the optimal parameters for a specific model, our primary objective is to demonstrate the improvement of INTERTRANS over direct translation, regardless of the LLMS used.\nAnother threat to internal validity arises from potential data leakage in LLMs, meaning there could be an overlap between the training data of the studied LLMs and the evaluation dataset used in this work. However, this issue would impact all baseline models, not just INTERTRANS, ensuring that the relative performance comparisons between models in our study remain valid. Additionally, unlike code generation, open-source code corpora typically do not contain paired code translation data (i.e., source and target code in a single file). We also carefully reviewed the documentation for Magicoder and StarCoder2 and they did not include code translation as a fine-tuning task.\nExternal Validity: Potential threats to external validity may arise from the selection of target PLs, LLMs, evaluation datasets, and compared approaches. To mitigate these threats, we selected six popular PLs with varying levels of maturity, encompassing different programming paradigms. The source-target PL pairs we considered include all those concerned in recent work on LLM-based code generation by Pan et al. and Yang et al.. For dataset selection, our evaluation set is sourced from three well-known benchmarks. Two of these benchmarks were used in the previously mentioned studies, and the third allows for a fair comparison with non-LLM-based models, such as the TransCoder family and GPT-3.5.\nWe selected three popular and recent open-source code LLMs as the base for INTERTRANS. These models are multilingual and have demonstrated strong performance on code generation tasks. In the future, additional LLMs can be seamlessly inte-grated, as INTERTRANS's implementation is LLM-agnostic, meaning all LLMs will be treated equally without requiring additional engineering steps.\nConstruct Validity: Similar to prior studies , we only consider execution-based evaluation metric, i.e., CA. While execution-based metrics align better with our goal to investigate the capability of LLMs in generating translated code that is functionally equal to the source program, its reli-ability will be impacted by the effectiveness of output control and the quality of test cases. To mitigate these threats, we applied output control following the best practices suggested by Macedo et al. and calculated the matching success rate (MSR) in extracting source code from inference output for all nine experiments. These values range from 97.7% to 100%, with an average MSR of 99.7%. This indicates that the reported performance is unlikely to be significantly influenced by the varying capabilities of the LLMs in generating source code that can be automatically extracted."}, {"title": "VI. RELATED WORK", "content": "Automated code translation approaches generally fall into two categories: rule-based methods and data"}]}