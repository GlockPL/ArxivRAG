{"title": "VIDEO-STAR: SELF-TRAINING ENABLES VIDEO INSTRUCTION TUNING WITH ANY SUPERVISION", "authors": ["Orr Zohar", "Xiaohan Wang", "Yonatan Bitton", "Idan Szpektor", "Serena Yeung-Levy"], "abstract": "The performance of Large Vision Language Models (LVLMs) is dependent on the size and quality of their training datasets. Existing video instruction tuning datasets lack diversity as they are derived by prompting large language models with video captions to generate question-answer pairs, and are therefore mostly descriptive. Meanwhile, many labeled video datasets with diverse labels and supervision exist - however, we find that their integration into LVLMs is non-trivial. Herein, we present Video Self-Training with augmented Reasoning (Video-STaR), the first video self-training approach. Video-STaR allows the utilization of any labeled video dataset for video instruction tuning. In Video-STaR, an LVLM cycles between instruction generation and finetuning, which we show (I) improves general video understanding and (II) adapts LVLMs to novel downstream tasks with existing supervision. During generation, an LVLM is prompted to propose an answer. The answers are then filtered only to those that contain the original video labels, and the LVLM is then re-trained on the generated dataset. By only training on generated answers that contain the correct video labels, Video-STaR utilizes these existing video labels as weak supervision for video instruction tuning. Our results demonstrate that Video-STaR-enhanced LVLMs exhibit improved performance in (I) general video QA, where TempCompass performance improved by 10%, and (II) on downstream tasks, where Video-STaR improved Kinetics700-QA accuracy by 20% and action quality assessment on FineDiving by 15%.", "sections": [{"title": "1 INTRODUCTION", "content": "The advent of large Vision-Language Models (LVLMs) marked a significant milestone in artificial intelligence. These models aim to create versatile systems capable of understanding and executing vision-and-language tasks aligned with human intentions. Early advancements in LVLMs, as exemplified by works such as BLIP (Li et al., 2022; 2023a) and LLaVA (Liu et al., 2023b;a), have been driven by the dissemination of pre-trained large language models (LLMs) (e.g., LLaMA (Touvron et al., 2023a;b)) and pre-trained vision/vision-language models (e.g., CLIP (Radford et al., 2021)). LVLMs connect the two model types via visual-language alignment and instruction tuning.\nWei et al. (2022); Liu et al. (2023a); Karamcheti et al. (2024) demonstrated the importance of visual instruction tuning on the resulting LVLM's performance. However, while much progress has been made in image-LVLMs, video-LVLMs still face challenges due to the relative difficulty in generating quality video instruction tuning datasets. Even though videos contain more complex scene dynamics and temporal information, indicating a need for larger and more diverse training datasets compared to images, the largest video instruction dataset, VideoInstruct-100K (VI-100K) (Maaz et al., 2023), comprises 100K video-text pairs but only 13K unique videos. This is small compared to image instruction datasets like LLaVA-1.5-Instruct (Liu et al., 2023a), which has ~ 665K image question pairs and ~ 350K unique images. Such limitation in video instruction tuning leads to performance saturation (Li et al., 2023b; Maaz et al., 2023; Jin et al., 2023; Wang et al., 2023; Lin et al., 2023).\nFurthermore, due to video instruction tuning dataset construction - mainly prompting large language models to produce question-answer pairs - these video datasets often degrade to simplistic questions, prompting for video captions 75% of VI-100K's questions are of this type."}, {"title": "2 VIDEO SELF-TRAINING WITH AUGMENTED REASONING (VIDEO-STAR)", "content": "Given a dataset of videos v and their corresponding labels l : \\(D = \\{(v_i,l_i)\\}_{i=1}^d\\), Video-STaR's objective is to create question q answer a pairs to instruction-tune the pre-trained model M on the dataset \\(D = \\{(v_i, q_i, a_i)\\}_{i=1}^a\\), producing the instruction-tuned model M. Note that videos need not be from the same task, and may contain multiple labels. We start by prompting a large language model with a task description T and video labels L to generate candidate questions q:\n\\(Y_{T,L} = \\text{A video is labeled \\{L\\} for the task of \\{T\\}. What questions could you ask someone about the video that should contain the video labels in the response?}\\)\nVideo-STaR performs generation-training cycles, where in cycle i the instruction-tuned model \\(M^{i^*}\\) is produced, while the instruction-tuned model from the previous cycle \\(M^{(i-1)^*}\\) is utilized for training data generation. We initialize the process with \\(M^{0^*}\\), an existing instruction-tuned model.\nTo prepare the training data in cycle i, answers are generated either directly via Answer Generation or through backward rationalization via Label Rationalization. In Answer Generation, \\(M^{(i-1)^*}\\) is prompted with questions (Sec. 2.1). Candidate answers are then filtered using the original video labels (Sec. 2.3). Videos rejected during direct Answer Generation are rationalized, where \\(M^{(i-1)^*}\\) is provided both a video \\(v_i\\) and labels \\(l_i\\), and then prompted with the question again (Sec. 2.2). Candidate answers are filtered again, creating the instruction tuning dataset in cycle i, \\(D_i\\) of size \\(d_i\\). A pre-trained model M is then finetuned on \\(D_i\\), producing \\(M^{i^*}\\). The next cycle generates data using \\(M^{i^*}\\), until the performance plateaus .\n2.1 ANSWER GENERATION\nEach Video-STaR cycle begins in direct Answer Generation. In this phase, \\(M^{(i-1)^*}\\) is prompted with the video-question pair to provide an answer along with a detailed rationale:\n\\(Y_Q = \\text{Question: \\{Q\\}. Rationalize your answer step-by-step; how can one arrive at this conclusion?}\\)\nWhen prompted with the question \\(q_i\\) on a particular video, \\(M^{(i-1)^*}\\) is expected to generate an answer \\(a_i\\) that contains the label \\(\\hat{l}_i\\) and the rationale \\(r_i\\) (\\(a_i = r_i \\cup \\hat{l}_i\\), see Fig. 2). We observe that answers containing the correct labels are of higher quality and suffer less from hallucination. Therefore, we filter the generated answers to include only those that contain the correct label (\\(\\hat{l}_i = l_i\\)) utilizing a verifier (Sec. 2.3). For an example of Answer Generation, see Fig. 3.\n2.2 LABEL RATIONALIZATION\nAnswer generation has two main drawbacks: (i) in some applications, especially on challenging/out-of-domain tasks, initial answer generation yield is low, resulting in almost no training samples after filtering (e.g., FineDiving, see Fig. 3); (ii) improvement plateaus as the model fails to solve new problems in the training set, and it is only trained on examples it answers correctly."}, {"title": "2.3 LABEL VERIFICATION", "content": "Video-STaR aims to utilize the labels as weak supervision in instruction tuning data generation. Gold labels are a grounding aspect of our datasets and represent some ground-truth knowledge. Therefore, we assume that answers that contain the ground-truth labels in their responses are higher quality than those that don't. While we would like to validate the existence of the different labels in the generated text, this can be non-trivial.\nTo this end, we introduce the Parser-Verifier. The Parser, P extracts the predicted labels from the generated text (\\(\\hat{l}_i = P(a_i)\\)), using a mixture of named entity recognition and Regex. Regex is used to identify easily identifiable string patterns, such as bounding boxes and time ranges, while named entity recognition is used for more nuanced entities, such as timestamps. The Verfier, V compares the extracted labels with the gold ones using the appropriate metrics (\\(V(l_i, \\hat{l}_i) \\rightarrow R\\)). For example, IoU for bounding boxes/temporal action localization, and BERT (Devlin et al., 2018) embedding similarity for sentence ordering. Each video has between 1-3 associated labels. To be classified as correct, the predicted labels must be within a 5% margin of error from the gold."}, {"title": "3 VIDEO-STAR GENERATED DATASET - VSTAR-1M", "content": "In this section, we detail the different source datasets utilized in our study (Sec. 3.1) and analyze the generated Video-STaR Dataset, VSTaR-1M (Sec. 3.2)."}, {"title": "3.1 SOURCE DATASETS", "content": "In selecting source datasets, we selected datasets that contain diverse video content and label types, please see Tab. 2.3. These include Kinetics700 (Smaira et al., 2020), which has action recognition annotations and is particularly large and diverse. FineDiving (Xu et al., 2022b) is an action quality assessment dataset of Olympic diving events and has both an overall score and action sequence annotations. Finally, STAR-benchmark (Wu et al., 2021), a video reasoning dataset, also contains bounding box and temporal action localization annotations. Tab. 2.3 contains the relevant dataset statistics, e.g., the number of videos and labels per dataset."}, {"title": "3.2 GENERATED DATASET ANALYSIS", "content": "Quantitative Analysis. Through the application of Video-STaR, significant dataset augmentation was achieved over two cycles of, illustrated in Fig. 4. This figure displays the Answer Generation and Label Rationalization yield across the datasets source. Notably, the initial application of Video-LLaVA on datasets like Kinetics700 and STAR-Benchmark showed significant Answer Generation success rates. However, the FineDiving dataset presented a notable challenge, with Answer Generation having no answers generated directly, underscoring the complexity of the dataset and the critical role of Label Rationalization. By the end of the second cycle, a substantial number of high-quality instances had been produced, showcasing both the effectiveness of Video-STaR in converting labeled video datasets into video instruction tuning dataset, as evidenced in Fig. 4.\nQualitative Analysis. See Tab. 3.2 for examples of generated question-answer pairs. From Kinetics700, we extracted an instance showcasing a video labeled 'smashing'. Video-STaR correctly identified a more fine-grained label, 'chopping wood'. In the FineDiving dataset, a clip depicting a complex dive was accompanied by the question 'On a scale from 1-100..' The model's output text provided a breakdown of the dive's components, leading to a score (label), as would be desired from an LVLM visual assistant. Finally, in the STAR benchmark, questions are already provided; therefore, we utilized them directly.\nIn 3, we show the qualitative improvement of the generated data over Video-STaR cycles. In the first cycle (Cycle 0), Video-LLaVA failed at Answer Generation and Label Rationalization. After one Video-STaR cycle (Cycle 1), Video-STaR still failed at Answer Generation but succeeded in Label Rationalization. After the final Video-STaR cycle (Cycle 2), Video-STaR managed to generate the answer without requiring the label via Answer Generation."}, {"title": "4 EXPERIMENTS", "content": "We experimented with Video-STaR and evaluated its enhanced video understanding capabilities. In Sec. 4.3, we evaluate how Video-STaR adapts Large Vision-Language Models (LVLMs) to the source datasets and how these capabilities are transferred zero-shot to similar benchmarks. In Sec. 4.2, we evaluate the video question-answering capabilities on video benchmark datasets."}, {"title": "4.1 EXPERIMENTAL SETTING", "content": "Implementation Details. We initialize from the Video-LLaVA (Lin et al., 2023) model, which utilizes the Vicuna-7B v1.5 (Chiang et al., 2023). We ran three Video-STaR cycles, and each cycle was initialized with the pre-trained Video-LLaVA weights. We train for one epoch using a 128 batch size, AdamW optimizer, and a cosine learning rate schedule. The learning rate is 2e - 5 with a 0.03 warmup ratio. In combination with the generated Video-STaR instruction tuning dataset, we additionally utilized the VideoInstruct-100K (Maaz et al., 2023) and the LLaVA v1.5 instruction tuning datasets (Liu et al., 2023a). Additional details are available in the appendix.\nBaselines. Besides comparing to Video-LLaVA, we also wanted to evaluate the effect of utilizing additional data and naively adapting the source datasets. Therefore, we utilized simple templates to generate question-answer pairs from the video labels and trained Video-LLaVA on the resulting dataset. We will reference this baseline as Video-LLaVA+. Another baseline for adapting Large Vision-Language Models to novel tasks is model distillation, where a stronger video model - in this work, Gemini 1.5 pro-vision - is utilized to label/annotate a small set of videos (500 from each dataset) and used to finetune the models. We will reference this baseline as Vid-LLaV\u0104Gemini\nEvaluation Details. We evaluate on the following benchmarks; the Zero-shot question-answer (QA) benchmarks: MSVD-QA, MSRVTT-QA, TGIF-QA, and ActivityNet-QA (Xu et al., 2017; 2016; Jang et al., 2017; Heilbron et al., 2015). TempCompass (Liu et al., 2024), a multiple-choice fine-grained QA benchmark. Adapted task performance is evaluated by converting source datasets using simple templates and applying the same evaluation protocol as Maaz et al. (Maaz et al., 2023), producing Kinetics700-QA, STAR-benchmark-QA, and FineDiving-QA. This protocol reports two metrics: accuracy (the percentage of correctly answered questions) and the average score (where ChatGPT rates each response on a scale of 1-5 and calculates the mean of these scores). All evaluations utilize the same GPT model (Wu, 2024) (\"gpt-3.5-turbo\") to ensure consistent comparisons. Due to cost considerations, 1000 videos were randomly selected from each dataset for Gemini eval-uation. The reported values are used on ActivitlyNet-QA."}, {"title": "4.2 QUANTITATIVE EVALUATION ON ZERO-SHOT BENCHMARKS", "content": "To evaluate Video-STaR's effect on general video question answering, we evaluated its effect on Video-LLaVA's performance on TempCompass, see Tab. 3. On TempCompass, Video-STaR outperformed Video-LLaVA across the board\u2013 by ~ 10%. To see if this performance boost is simply a factor of training on a larger dataset, we also evaluated Video-LLaVA+. Video-LLaVA+was trained on even a larger video dataset by naively utilizing video labels, and yields a more modest improvement of 3%, showing the utility of Video-STaR. TempCompass is also a fine-grained dataset that would be sensitive to hallucinations, indicating that Video-STaR is not more prone to hallucinations compared to existing methods. Gemini 1.5 pro scored an impressive 66.0 on TempCompass, showing there is still much room for improvement on this benchmark.\nWe then continued and evaluated Video-STaR's effect on zero-shot video QA performance on the MSVD-QA, MSRVTT-QA, TGIF-QA and ActivityNet-QA benchmarks. As can be seen in Tab. 4, Video-STaR achieves performance improvements where, for instance, on the MSVD-QA dataset, Video-STaR attains the highest accuracy of 71.3% vs Video-LLaVA's 69.7. On MSRVTT-QA, Video-STaR leads with an accuracy of 58.2% and maintains a competitive edge in other datasets like TGIF-QA and ActivityNet-QA. Seeing the relatively small performance gains compared to Temp-Compass, we additionally evaluated Gemini 1.5 pro-vision on 1000 video subsets of each dataset and found that its performance is on par with existing open-source models. We believe this shows that we are near the 'noise' limit of these benchmarks. Our qualitative analysis indicated that many of the questions selected as 'wrong' are actually due to the benchmark design-overly general questions with multiple correct answers. Concurrent work (Wu, 2024) has similarly concluded that the ChatGPT-3.5 version utilized in evaluation can lead to variations of \u00b110 in accuracy."}, {"title": "4.3 QUANTITATIVE EVALUATION ON ADAPTED DATASETS", "content": "Besides improving general visual question-answering performance, Video-STaR can also adapt Large Vision-Language models to novel takes. To demonstrate this, we converted the test sets (not included in training) of the source datasets \u2013 Kinetics700, STAR-benchmark, and FineDiving. The results of these evaluations are reported in Tab. 5. Adapting LVLMs with easier-to-collect labels can be helpful in various applications, leading to a more versatile, multi-domain capable assistant. When evaluating Video-STaR's impact on LVLM performance on the diverse source datasets, we found that it significantly improves model performance, particularly on complex tasks. For instance, on Kinetics700, known for its extensive action categories, Video-STaR enhanced Video-LLaVA's performance accuracy by an average of 20%, showcasing its ability to develop generalized models adept across multiple domains. Interestingly, Video-LLaVA+'s performance did not improve compared to Video-LLaVA, and in some cases, even worsened, showing that one cannot directly utilize labeled datasets for LVLM adaptation.\nAction Quality Assessment (AQA) is a complex video task requiring detailed action understanding, where Video-STaR significantly enhanced LVLM performance on the FineDiving dataset. Our results show a notable improvement from 17.6 to 20.2 in score prediction accuracy, highlighting Video-STaR's effectiveness in refining LVLM's temporal reasoning. However, Video-STaR allows LVLMs to not only rate a particular dive but also explain the rationale behind each assessment. This rationale is invaluable for many applications, effectively providing potential user feedback for"}, {"title": "4.4 ABLATIONS", "content": "In our ablation studies, we evaluated the impact of removing Label Rationalization and Answer Generation from Video-STaR, focusing on adapted datasets (Kinetics700, FineDiving, STAR-benchmark) and zero-shot benchmarks (MSVD-QA, MSRVTT-QA, TGIF-QA, ActivityNet-QA).\nAdapted Datasets For adapted datasets (Tab. 4.3), excluding Label Rationalization led to a significant performance drop in FineDiving, from 20.2 to 12.8 in accuracy, highlighting its critical role in complex reasoning tasks. This is likely due to the lack of conversion of any examples from the data. However, the removal of Answer Generation resulted in a more pronounced and uniform decline across all datasets. For example, Kinetics700's accuracy was reduced from 59.9 to 50.0, underscoring its foundational role in generating context-relevant responses.\nZero-shot benchmarks In zero-shot benchmarks (Tab. 4.3), the removal of Label Rationalization had a mixed impact, slightly affecting MSVD-QA where accuracy decreased from 71.3 to 70.6. The elimination of Answer Generation consistently lowered performance, such as a decrease in MSRVTT-QA accuracy from 58.2 to 57.4. ActivityNet-QA performance improved, probably because 100K-Instruct utilizes ActivityNet for instruction tuning. Therefore, the introduction of additional videos decreases performance."}, {"title": "5 RELATED WORKS", "content": "5.1 LARGE VISION-LANGUAGE MODELS\nInitial LVLMs, such as LLaVA (Liu et al., 2023b;a) and BLIP-2 (Li et al., 2023a), demonstrated the potential of merging image inputs with large language models. Methods like mPLUG-Owl (Ye et al., 2023) and Flamingo (Alayrac et al., 2022) further allowed for multiple image inputs without architectural changes. Li et al. (2023b) and Zhang et al. (2024) led the transition to video understanding, integrating video/image encoders and LLMs while training on small video instruction tuning datasets. Jin et al. (2023) introduced Chat-UniVi, a unified model employing dynamic visual tokens for both images and videos, optimizing visual token usage and higher frame count sampling. LLaMA-VID (Li et al., 2023c) showed that the token count can be further reduced by pooling the tokens selectively via the text prompt using Q-Former. Recently, Video-LLaVA (Lin et al., 2023)"}, {"title": "5.2 LARGE LANGUAGE MODELS AND SELF-TRAINING", "content": "The advent of GPT (Radford et al., 2018; Brown et al., 2020) marked significant milestones in natural language processing, showcasing LLMs' power in understanding and generating human-like text. Open-source LLMs like LLaMA (Touvron et al., 2023a;b) and their instruction-tuned variants like Alpaca and Vicuna (Taori et al., 2023; Chiang et al., 2023) further tailored these models for nuanced human-AI interactions. However, even LLMs have found it challenging to scale annotated datasets for training, prompting work on self-training and self-improvement (Singh et al., 2023; Huang et al., 2022; Ho et al., 2023; Marasovi\u0107 et al., 2022; Hosseini et al., 2024). In this line of work, LLMs cycle between instruction-tuning data generation and instruction tuning, iteratively improving LLM performance over cycles. For instance, Zelikman et al. (2022) introduced the the Self-Taught Reasoners method, used rationalization to generate chain-of-thought (CoT) reasoning, filtering poor rationalizations to retain correctly answered questions. Other self-training approaches include expectation-maximization-based approaches (Singh et al., 2023), which alternate between data generation and improvement between training cycles. Alternatively, majority voting has also been utilized to generate answers and rationale for unlabeled questions (Huang et al., 2022). These methods show the effectiveness of iterative self-training. In our work, we aim to introduce a weakly supervised self-training approach for video instruction tuning, leveraging video supervision that is often easier to collect and exists in many large and diverse datasets."}, {"title": "6 CONCLUSIONS", "content": "In conclusion, Video Self-Taught Reasoners (Video-STaR) presents a novel approach to enhance Large Vision-Language Models (LVLMs) by enabling the use of diverse labeled video datasets for visual instruction tuning. This method addresses critical data diversity and quality challenges, leading to significant performance improvements across various video understanding tasks. Our experiments demonstrate Video-STaR's effectiveness in both source dataset adaptation and zero-shot generalization, showcasing its potential in advancing LVLM capabilities for complex video content.\nThe promising results of Video-STaR open new research avenues, particularly in expanding LVLM knowledge bases using readily available image and video datasets. Future work could explore advanced self-training techniques and integration with emerging LVLM architectures, focusing on long-form video understanding to further boost LVLM understanding. Additional work is also needed to reduce hallucinations, perhaps by using grounded VLMs as auxiliary input."}, {"title": "7 LIMITATIONS", "content": "While Video-STaR introduces a novel approach to visual instruction tuning, it is not without its limitations. Firstly, the methodology can be computationally intensive due to the cycling of both generating and rationalizing question-answer and instruction tuning. Secondly, the assumption that all labels necessitate a rationale may not always hold true. Certain labels might be straightforward enough not to require elaborate rationalization, potentially leading to unnecessary computational overhead. Lastly, hallucinations, especially in label rationalization can be further reduced by perhaps implementing additional verifiers."}, {"title": "A APPENDIX", "content": "In Sec. A.1, we provide additional implementation details and compute used in developing Video-STaR. In Sec. A.2, we introduce explainable action quality assessment and provide good and bad examples of Video-STaR on the FineDiving test dataset. Finally, we provide additional qualitative Answer Generation and Label Rationalization examples in Sec. A.3 and A.4.\nA.1 IMPLEMENTATION DETAILS\nVideo-STaR utilized the Video-LLaVA model, which integrated the Vicuna-7B v1.5 for language processing and ViT-L/14 video and image encoders from LanguageBind for visual encoding. The system's tokenizer, adapted from LLaMA, has a vocabulary size of around 32, 000 classes and a dimensionality of 4096. Two cycles of Video-STaR were executed, each initialed with the pre-trained Video-LLaVA model (before instruction tuning). The training data was augmented by incorporating VideoInstruct-100K and LLaVA v1.5's visual instruction datasets.\nFour clusters of 10 NVIDIA Titan RTX GPUs were employed for 64 hours. The structured prompts for these tasks were as follows:\n\u2022 Answer Generation\nQuestion: {Q}.\nCan you explain step-by-step how one can arrive at this conclusion?\n\u2022 Label Rationalization\nQuestion: {Q}\nAnswer: {L}.\nCan you explain step-by-step how one can arrive at this conclusion?\nThese prompts guided the model in producing detailed answers and rationalizations, enhancing the depth and utility of the generated instruction-tuning dataset. Answer correctness was evaluated using template matching with Levenshtein Distance-based Levenshtein (1965) fuzzy logic Cohen (2020), considering an answer correct if all keywords from the label were present in the generated response with a minimum similarity score of 80%. For example, if in Kinetics the action label is 'eating apple pie', we would only consider a generated answer correct of \u2018eating', \u2018apple', \u2018pie' all appeared with a similarity score of 80.\nA.2 EXPLAINABLE ACTION QUALITY ASSESSMENT\nExplainable Action Quality Assessment (AQA) is critical for detailed analysis of performances in precision sports, such as competitive diving, where execution and complexity significantly impact scores. Unlike previous AQA works, which only provide a score Xu et al. (2022b;a); Yu et al. (2021); Zhang et al. (2023b) Video-STaR not only generates scores but also offers detailed justifications akin to expert analysis."}]}