{"title": "Let Curves Speak: A Continuous Glucose Monitor based Large Sensor Foundation Model for Diabetes Management", "authors": ["Junjie Luo", "Abhimanyu Kumbara", "Mansur Shomali", "Rui Han", "Anand Iyer", "Ritu Agarwal", "Gordon Gao"], "abstract": "Background: While previous studies of artificial intelligence (AI) in diabetes management focus on long-term risk, research on near-future glucose prediction remains limited but important as it enables timely diabetes self-management. Integrating AI with continuous glucose monitoring (CGM) holds promise for enhancing near-future glucose prediction. However, existing models have limitations in capturing complex patterns of blood glucose fluctuations and demonstrate poor generalizability across patients. A robust approach is needed to leverage massive CGM data for near-future glucose prediction.\nMethods: We propose large sensor models (LSMs) to capture latent knowledge in CGM data by modeling patients as sequences of glucose time. CGM-LSM is pretrained on 15.96 million glucose records from 592 diabetes patients for near-future glucose prediction. We evaluated CGM-LSM against state-of-the-art methods using the OhioT1DM dataset across various metrics, prediction horizons, and unseen patients. Additionally, we assessed its generalizability and performance variations across patient-specific factors like diabetes type, age, gender, and hour of day.\nResults: CGM-LSM achieved exceptional performance, with an rMSE of 29.81 mg/dL for type 1 diabetes (T1D) patients and 23.49 mg/dL for type 2 diabetes (T2D) patients in a two-hour prediction horizon. For the OhioT1DM dataset, CGM-LSM achieved a one-hour rMSE of 15.64 mg/dL, halving the previous best of 31.97 mg/dL without training on this dataset. Robustness analyses revealed consistent performance not only for unseen patients and future periods, but also across diabetes type, age, and gender. The model also demonstrated adaptability to different hours of day, maintaining accuracy across periods of various activity intensity levels.\nConclusions: CGM-LSM represents a transformative step in diabetes management by leveraging pretraining to uncover latent glucose generation patterns in sensor data. This approach significantly improves prediction accuracy, robustness, and generalizability, providing timely and personalized insights to support self-management. Our findings also underscore the broader potential of LSMs to drive innovation across domains involving complex sensor data.", "sections": [{"title": "Introduction", "content": "Diabetes imposes significant burdens on individuals, families, and healthcare providers. Uncontrolled diabetes has been associated with a variety of severe medical complications, including heart attacks\u00b9, kidney failure2, and diabetic neuropathy\u00b3. In 2021, 38.4 million Americans (11.6% of the U.S. population) had diabetes, making it the eighth leading cause of death. By 2022, diabetes management costs had soared to approximately $412.9 billion annually (25% of total healthcare expenditures)4. The rising societal and economic burden of diabetes combined with a growing global shortage of healthcare professionals necessitates new approaches to diabetes care and prevention5.\nEffective self-management is crucial for people with diabetes to manage their condition and avoid complications. To inform their daily decision-making processes, patients need actionable information. Artificial intelligence (AI) holds considerable promise to provide such information by generating personalized interventions tailored to individual needs. However, most AI applications in diabetes management have been centered on predicting the overall risk of diabetes progression over long time horizons, and such predictions are difficult for individuals to convert into daily diabetes management activities. This shortcoming underscores the need for diabetes management Al solutions that can provide timely, micro-level, personalized, actionable predictions to enable patients to improve their self-care6\u20138.\nIn recent years, continuous glucose monitoring (CGM) systems, which continuously measure glucose values every five minutes, have emerged as a critical tool in diabetes control. One particularly promising approach to meeting the patient needs described above lies in combining CGM with AI to predict near-future glucose values. However, little research has addressed this task, which is inherently challenging due to the complexity and variability of individual behaviors captured by CGM. This makes it difficult to accurately predict a patient's glucose levels for the next two hours10. Studies using simulated CGM data or datasets from a limited number of patients report poor prediction performance11-16. Such poor performance can be caused not only by inadequate data but also by training the model using random initialization with zero glucose generation knowledge. In this context, the question of how to utilize the hidden knowledge within CGM data patterns to empower prediction models remains unaddressed.\nTo address this gap, we present a new approach to glucose prediction for diabetes management that harnesses the pretraining technique exemplified in large language models (LLMs). We pretrain the model to learn the latent glucose generation mechanisms hidden in massive CGM data. Considering that language is represented as a sequence of tokens and LLMs learn through next- token prediction, we also model person-generated glucose data as a sequence of time steps (e.g., every five minutes) and propose a large sensor model (LSM) to learn massive sensor data with next-step prediction. We hypothesize that an LSM pretrained on CGM data (CGM-LSM) can learn hidden glucose generation patterns and achieve high performance in predicting glucose values.\nWe develop the CGM-LSM on a large CGM dataset with 15.96 million glucose records from 592 diabetes patients. We first validate its efficiency for improving near-future glucose prediction on metrics of accuracy, robustness, and prediction horizon flexibility. Our model demonstrates dramatically superior performance. Benchmarked against the public OhioT1DM dataset, CGM-"}, {"title": "Methods", "content": "1. Datasets\nThe key data point in our study is an \u201cinstance,\u201d defined as a combination of a patient and an observation datetime. This datetime represents the time when the model is triggered to observe the patient's records and generate predictions. As shown in Figure. 1b, inclusion as a valid instance requires 288 CGM entries from the 24 hours prior to the datetime and 24 entries in the subsequent 2 hours, for a total of 26 hours of CGM data."}, {"title": "Results", "content": "1. CGM-LSM Performance for Near Future Glucose Predictions\nAs reported in Table 2, CGM-LSM significantly outperformed baseline models on the OhioT1DM data, despite not being trained on it. At a 30-minute prediction horizon, rMSE dropped from 18.64mg/dL to 9.36mg/dL, and at one hour, from 31.97mg/dL to 15.64mg/dL-nearly a 50% reduction. These results underscore the effectiveness of our pretraining approach for modeling glucose generation from large-scale CGM data. Strong performance on unseen patients indicates that the model avoided overfitting and captured generalizable glucose generation patterns."}, {"title": "CGM-LSM Performance for an Unseen Future and Unseen Patients", "content": "Assessing the model's ability to generalize to real-world scenarios, such as future periods and unseen patients, is crucial for practical disease management. We hypothesized that temporal and hold-out datasets would pose greater challenges for CGM-LSM than the internal test set. These challenges arise from temporal shifts in the future set and unseen patients in the hold-out set, which deviate from the training data distribution.\nAs a result, despite the increased complexity, CGM-LSM's performance gap between the temporal and hold-out sets versus the internal set remained minimal. For T1D-2H predictions, rMSE was 28.28mg/dL for the internal set, increasing slightly to 29.43mg/dL (+4.07%) for the temporal subset and 29.81mg/dL (+5.41%) for the hold-out set. Similarly, for T2D-2H predictions, rMSE increased modestly from 22.65mg/dL for the internal set to 23.22mg/dL (+2.52%) for the temporal subset and 23.49mg/dL (+3.71%) for the hold-out set. These results show that while temporal and hold-out predictions were indeed more challenging, CGM-LSM maintained high accuracy, demonstrating robustness to temporal shifts and patient variability."}, {"title": "Performance Across Diabetes Type, Age, and Gender", "content": "Diabetes is a complicated disease with distinctive glucose generation mechanisms among patients. A patient's diabetes type, age, and gender significantly influence how glucose levels evolve over time19,20. Considering this heterogeneity, we analyzed CGM-LSM's performance across diabetes type, age, and gender.\nCGM-LSM demonstrated strong performance for both T1D and T2D patients. For T1D patients, rMSE scores were 8.93mg/dL (30min), 16.91mg/dL (1H), and 29.81mg/dL (2H). Performance was better for T2D patients, with lower rMSE scores of 7.77mg/dL (30min), 13.88mg/dL (1H), and 23.49mg/dL (2H). These results underscore the model's robustness across diabetes types. For T2D, the rMSE rose slightly by 3.71% from 22.65mg/dL (internal) to 23.49mg/dL (hold-out), as shown in Table 2. For T1D, the rMSE increase was larger, rising by 5.41% from 28.28mg/dL (internal) to 29.81mg/dL (hold-out), which was both proportionally and absolutely higher. This indicates that CGM-LSM has higher generalizability for unseen T2D patients than T1D patients."}, {"title": "Performance across Prediction Hour of the Day", "content": "Human glucose generation is influenced by lifestyle factors like diet, exercise, and medications. Glucose levels and fluctuations vary by time of day due to their association with human activities like eating and exercise. Given this link, understanding CGM-LSM's performance at different hours is crucial for its safe application. Figure 4 shows CGM-LSM's prediction performance at different hours of the day, measured via rMSE over 30-minute, one-hour, and two-hour horizons using the hold-out set for T1D and T2D patients.\nEach line in Figure 3 represents the model's rMSE values across different hours of the day. The model performed consistently well across prediction settings. Clear trends emerged based on prediction horizons and time of day. Shorter horizons (e.g., 30 minutes) had lower rMSE, reflecting better capture of short-term fluctuations. The rMSE score peaked at noon and evening hours. The higher rMSE during these hours likely reflects impacts from diet, exercise, and treatment routines. In contrast, rMSE was notably lower at night. Nocturnal glucose levels, which are less influenced by behavior, allowed CGM-LSM to predict more accurately. These findings demonstrate the importance of temporal factors and human behaviors in glucose prediction."}, {"title": "Discussion", "content": "Accurate near-future glucose prediction is critical for personalized and real-time diabetes management and is an intrinsically difficult task. In this study, we introduced the large sensor model for CGM to learn glucose generation mechanisms hidden in large CGM data. We hypothesized that the LSM could learn effectively, yielding improvements over existing methods. With 15.96 million glucose records (~152 patient-years of CGM data) from 592 diverse patients, our CGM-LSM was effectively pretrained, and it achieved significant improvements in near-future glucose predictions. The model demonstrated its accuracy and robustness across various prediction horizons, patient types, unseen periods, and prediction contexts.\nUsing a one-hour horizon, our model achieved an rMSE that is 50% lower than existing methods for T1D patients, demonstrating high predictive accuracy. Previous studies have not explored the two-hour prediction horizon or prediction performance for type 2 diabetes, which is crucial for glucose monitoring. Our model's rMSE for a two-hour horizon for hold-out unseen patients was 29.81 mg/dL for type 1 diabetes and 23.49 mg/dL for type 2 diabetes for WellDoc data, and 26.30 mg/dL for the OhioT1DM data. The superior performance of CGM-LSM enables patients and healthcare providers to make timely and precise interventions, potentially reducing patients' risk for hypoglycemia and hyperglycemia episodes. For both T1D and T2D patients, CGM-LSM's accuracy can support improved daily glucose management, potentially reducing the incidence of more severe complications. These advances could ultimately lead to better health outcomes, lower healthcare costs, empowered self-management, and enhancement in the quality of life.\nIn addition to accuracy, robustness is critical for evaluating prediction performance, requiring reliable performance for both unseen future periods and new patients. To examine CGM-LSM's robustness, we implemented a dual-dimension evaluation strategy with a \u201ctemporal\u201d test set for unseen future periods and a \u201chold-out\u201d test set for unseen new patients. Our model exhibited consistent prediction performance for all evaluation sets. The performance gap between the temporal and hold-out sets versus the internal set remained small, indicating that our model can effectively perform prediction tasks involving unseen future periods or unseen patients. It is noteworthy that many prior glucose prediction studies either omitted these evaluations or the model's performance deteriorated when applied to unseen patients, raising questions about its robustness and broader applicability14. Our second robustness evaluation examined performance across diabetes type, age, and gender. CGM-LSM maintained stable prediction performance across demographic groups and identified certain groups that require customized model adjustments for better prediction performance. For diabetes type, the model's performance for T1D is close to that for T2D, with T2D showing better results. This notably better performance is intriguing, as it suggests that T1D may present a more complex prediction challenge requiring further study. We also note that our model showed higher accuracy and robustness for older patients, male patients, and T2D patients than for younger, female and T1D patients. Overall, however, our results demonstrated significant improvements in prediction across all groups, indicating equitable and reliable performance. Our third robustness test examined prediction performance across different times of day and found that at all times, our model consistently outperformed the baseline model. We observed different accuracy across hours, with higher rMSE in the late afternoon/evening and lower rMSE late at night. This fluctuation in model performance reveals how human behaviors influence the performance of LSM and highlights the potential of incorporating human behaviors into foundation models for even more accurate glucose predictions.\nThe robustness of our model for future, unseen, and diverse patients has significant clinical implications. It enables immediate personalized care and timely interventions. Drawing on limited patient history, our model provides reliable predictions for clinical decision support and broader use. This reliability builds trust and facilitates preventive care. CGM-LSM's superior prediction accuracy and robustness is driven by its pretraining on the massive CGM data, which allows the model to understand the human behavior information and glucose generation mechanisms from sensor curves. Through five-minute glucose predictions, CGM-LSM distills knowledge from the glucose curves that develop temporally from the interaction between human behaviors and"}, {"title": null, "content": "biological mechanisms. Its success indicates the value of applying foundation models to large sensor data for improved prediction. Remarkably, CGM-LSM achieves accurate predictions even without detailed activity data, indicating that it captures activities and their effects using the curve's fluctuations. Given the challenges of collecting behavioral data, achieving accurate predictions using only glucose data represents significant progress.\nOur study has several key limitations. First, we did not include activity data (medication, diet, exercise, education) or laboratory measurements, which could potentially improve model performance. Second, the model's robustness was only assessed across age and gender, excluding critical demographic aspects like race, ethnicity, and socioeconomic status. Third, the lack of patient-specific fine-tuning in the CGM-LSM model limits its applicability and personalization. Lastly, once patients adjust their behavior based on the prediction, it is not clear whether CGM- LSM will continue to perform well. Extra investigations in pretraining and fine-tuning are needed for these problems.\nIn conclusion, this paper is the first to utilize sensor data, specifically CGM data, to pretrain a large sensor model. This innovative approach demonstrates the potential of integrating sensor data and advanced Al techniques, setting a foundation for future research in this area. The superior performance of CGM-LSM demonstrates the feasibility of introducing foundation models from texts, images, and videos to human sensor records. Such data is becoming increasingly massive and ubiquitous given the widespread adoption of wearable devices21. With these data, various LSMs could be developed to advance the understanding and management to other diseases, like uncovering new knowledge about body mechanisms from heart rate, blood pressure, body temperature, weight, respiratory rate, and oxygen saturation. These LSMs could enable significant improvement in human health prediction, not only in accuracy, but also in robustness, customization, and granularity."}, {"title": "1.3 Problem definition for pretraining", "content": "We represented a sequence of 312 CGM values with timestamps as $(S_1, S_2, S_3, ... S_n)$ for a 26-hour period (24 hours before the focal datetime and 2 hours after). This sequence has a natural sequential ordering, similar to natural language. Following the logic of a language model, the large sensor model is trained to predict the next glucose value given a CGM sensor data sequence using the autoregression method.\nMathematically, this can be described as maximizing the likelihood of a glucose value $s_i$ given the preceding glucose values $S_1, ..., S_{i-1}$. The objective function for training the CGM-LSM model is formalized as follows:\n$L(\\theta) = \\sum_{i=1}^{n} log p_{\\theta} (s_i|S_1, ..., S_{i-1})$\nwhere $\\theta$ represents the parameters of the model, $n$ is the length of the input sequence, and $P_{\\theta} (s_i|S_1, ..., S_{i-1})$ is the probability of the glucose value $s_i$ conditioned on the prior sequence of tokens, as computed by the model. This training paradigm encourages the development of internal representations that capture glucose generation mechanism dependencies between elements in the input sequence, enabling the model to generate coherent and contextually relevant glucose values in future glucose prediction tasks. By repeatedly adjusting $\\theta$ to maximize $L(\\theta)$, the CGM-LSM model learns to anticipate subsequent glucose value effectively. In our CGM-LSM model, we adopted the popular transformer decoder-only structure to model the above task, and we conduct our pre-training task based on the CGM data."}, {"title": "1.4 Problem definition for prediction/generation", "content": "After the CGM-LSM was trained, it could be used to generate new glucose sequences. The generation process of CGM-LSM is similar to that of the GPT models. It involves iteratively sampling a new generated glucose value (token) from a probability distribution of possible next glucose values (tokens), conditioned on a sequence of input glucose values (tokens). This is mathematically represented as:\n$p(S_{i+1} | S_1, ..., S_i; \\theta) = \\text{softmax}(h_i W)$\nwhere $\\theta$ denotes the model parameters, $h_i$ is the hidden state derived from transformer blocks, and $W$ is the output projection matrix. The generation starts with an initial sequence, i.e., the preceding 24-hour CGM data sequence. Each timestep's hidden glucose state is computed and used to generate a probability distribution over the vocabulary from which the glucose value in the next time stamp is sampled. This can be done using random sampling or techniques like top-k or top-p sampling to balance creativity and coherence. For our application, we used the greedy method, which samples the glucose value with the highest probability. This predicted glucose value is then treated as the real value and appended to the original sequence for the next glucose value prediction. The process repeats until the maximum length, i.e., 24 glucose values for the next two hours, is reached. Finally, the generated 2-hour glucose values are compared with the real glucose values to measure the model's prediction performance."}, {"title": "1.5 CGM-LSM model structure", "content": "Input Representation. The tokens used in CGM-LSM for glucose prediction were the glucose values themselves. Various methods exist for converting numerical values into categorical tokens. For simplicity and without losing generality, we directly used the glucose readings as tokens; for instance, a glucose level of 153 becomes the token \u201c153.\u201d In total, we had 400 glucose value tokens in the vocabulary because the maximum value of glucose measured by the CGM device is 400. In the pretraining tasks involving next glucose value prediction, the model was expected to learn the semantic meanings of the embeddings for these glucose value tokens.\nModel Structure. CGM-LSM adopted the transformer decoder-only structure\u00b2, which is widely used in GPT model series3,4. In contrast to traditional recurrent neural networks, this approach relies entirely on self-attention mechanisms to generate sequences. Such an architecture facilitates more parallelizable computations and effectively captures long-range dependencies in data.\nIn a decoder-only transformer, the model is composed exclusively of a stack of decoder blocks that process the input sequence to generate one glucose value token at a time. Each decoder block in the transformer comprises two main components: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.\nThe input to each decoder layer is initially transformed into three vectors\u2014query (Q), key (K), and value (V) vectors\u2014through linear projection of the embeddings of the input tokens. The self- attention mechanism in the decoder is mathematically represented as:\n$\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^\\top}{\\sqrt{d_k}})V,$\nwhere $d_k$ is the dimension of the key vectors. This mechanism allows each position in the decoder to attend to all positions up to and including that position in the previous layer. This is crucial for autoregressive models where the prediction for the next token can only depend on the known previous tokens, thereby preserving the causality in the sequence generation.\nNormalization and residual connections are employed around each of these sublayers (self- attention and feed-forward networks), which help in stabilizing the learning process. Mathematically, the output of each sublayer can be described as:\n$\\text{LayerNorm}(x + \\text{Sublayer}(x))$,\nwhere $\\text{Sublayer}(x)$ is the function implemented by the sublayer itself, either self-attention or a feed-forward network, and x is the input to the sublayer.\nFinally, after passing through the series of decoder blocks, the output is projected onto a vocabulary-sized vector using a linear transformation followed by a softmax layer to produce a probability distribution over possible next tokens:\n$p(S_{i+1} | S_1, ..., S_i) = \\text{softmax}(h_i W),$"}, {"title": null, "content": "where $h_i$ is the final layer's output at position i and Wis the weight matrix. The glucose value corresponding to the highest probability is then selected as the next token in the sequence. The decoder-only transformer architecture leverages the power of self-attention to efficiently process sequences in a manner that scales favorably with sequence length and allows for highly parallelizable implementation.\nLoss Function. Because we treated a glucose value as a unique token, we used cross-entropy to calculate the loss for the next glucose value prediction task. In the pretraining of the CGM-LSM model, the softmax function is instrumental in transforming the logits\u2014raw output scores for each glucose value in the vocabulary\u2014into a probability distribution over all potential next glucose values. This probabilistic framework is crucial for defining the cross-entropy loss, which quantifies the difference between the predicted probabilities and the actual distribution of the next glucose value. Specifically, given the logits $z_{t,k}$ for possible next glucose values at position t + 1, the softmax function calculates the probability of each token x\u2081 being the next token in the sequence as follows:\n$P_\\theta(S_{t+1} | S_1, ..., S_t) = \\frac{\\exp(z_{t, s_t})}{\\sum_{k=1}^{V} \\exp(z_{t,k})}$\nwhere V is the vocabulary size and $z_{t,k}$ represents the logit corresponding to the $k$-th vocabulary token. The cross-entropy loss for predicting a single token is thus:\n$Loss = -log p_\\theta (S_{t+1} | S_1, ..., S_t)$\nThis loss essentially measures the model's effectiveness in predicting the actual next token $S_{t+1}$ using the probabilities output by the softmax function. By minimizing the negative log-likelihood, or equivalently, the cross-entropy loss across all tokens in the training dataset, the GPT model learns to generate accurate and coherent text. The total loss for a batch of sequences, which aggregates the individual token losses, ensures comprehensive learning across diverse textual contexts:\n$Total Loss = - \\sum_{i=1}^{N} \\sum_{t=1}^{T_i} \\log (\\frac{\\exp(z_{i,t-1,S_{i,t}})}{\\sum_{k=1}^{V} \\exp(z_{i,t-1,k})})$\nThrough iterative minimization of this total loss during pretraining, the CGM-LSM model effectively hones its parameter set $\\theta$, thereby enhancing its capabilities in language modeling and next-token prediction."}]}