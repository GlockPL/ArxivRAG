{"title": "PeerArg: Argumentative Peer Review with LLMs", "authors": ["Purin Sukpanichnant", "Anna Rapberger", "Francesca Toni"], "abstract": "Peer review is an essential process to determine the quality of papers submitted to scientific conferences or journals. However, it is subjective and prone to biases. Several studies have been conducted to apply techniques from NLP to support peer review, but they are based on black-box techniques and their outputs are difficult to interpret and trust. In this paper, we propose a novel pipeline to support and understand the reviewing and decision-making processes of peer review: the PeerArg system combining LLMs with methods from knowledge representation. PeerArg takes in input a set of reviews for a paper and outputs the paper acceptance prediction. We evaluate the performance of the PeerArg pipeline on three different datasets, in comparison with a novel end-2-end LLM that uses few-shot learning to predict paper acceptance given reviews. The results indicate that the end-2-end LLM is capable of predicting paper acceptance from reviews, but a variant of the PeerArg pipeline outperforms this LLM.", "sections": [{"title": "Introduction", "content": "Peer review is a process where work is examined and evaluated by a group of people with expertise in the relevant field. The process is crucial to ensure the quality of the work. It has been adopted by many conferences and journals, to ensure the published papers are of adequate quality. Peer review is hence a core component of the progress in several academic areas. Nevertheless, fairness is the main weakness of the process. Peer review involves lots of discussion and evaluation from human reviewers, which are subjective and are prone to biases and irrationality. For example, the reviewers may be more likely to accept a paper whose results agree with what they believe, known as confirmation bias (Mahoney, 1977). Another bias is first-impression bias, where the initial impressions of the document (e.g. typographical layout (Moys, 2017)) affect the entire judgement.\nThere has been an emerging trend to study how to apply techniques from Natural Language Processing (NLP) to improve the peer review process. Zhou et al. (2024) study review generation and found that the generated reviews consider more aspects of a paper than human reviewers. Another example is given by Nuijten and Polanin (2020), with a tool that checks for statistical inconsistencies in papers. Review understanding is another application of NLP in peer review. This could lessen the burden for meta-reviewers or conference chairs who read the reviews before deciding whether to accept a paper or not. For example, Kumar et al. (2023) propose methods to generate pros and cons summary given reviews. Bhatia et al. (2020) present a system to generate a meta-review from reviews. Most of the studies for review understanding used deep learning models as their backbone. Even though they have impressive performances, the black-box nature of the models makes it difficult to understand the rationale behind the results and trust the models.\nIn this paper, we propose a new technique to address these shortcomings. We use symbolic AI methods, specifically methods from computational argumentation (Dung, 1995; Amgoud et al., 2008; Baroni et al., 2018), to enhance review understanding and to assess the alignment between paper acceptance decision-making process and the reviews. Computational argumentation studies how to model the way humans build and use arguments so that it can be represented in a machine (Atkinson et al., 2017). Whereas existing argumentation studies in peer review focus on the structure of arguments (Hua et al., 2019; Fromm et al., 2020), our work uses a form of bipolar argumentation frameworks (Amgoud et al., 2008), adding a support relation to abstract argumentation (Dung, 1995), to model both the reviews and the review aggregation process. Our main contribution is PeerArg, a novel framework for transparent review aggregation. PeerArg predicts the acceptance status of the paper, as illustrated in Figure 1.\nAs part of this paper, we furthermore propose a few-shot learning end-2-end LLM taking in reviews of a paper and predict the paper acceptance, inspired from Zhou et al. (2024) and Gorur et al. (2024). We evaluate PeerArg and the end-2-end LLM on three review datasets: two conference review datasets, and a journal review dataset. Our empirical studies indicate that enhancing LLMs with methods from computational argumentation has beneficial effects. The results show that the LLM can be used to predict the paper acceptance from reviews, and with a certain hyperparameters combination, the PeerArg pipeline outperforms the LLM in all the datasets."}, {"title": "Related Work", "content": "Methods for improving the interpretation and understanding of reviews using AI have received significant attention in recent years.\nUnderstanding Reviews with AI Several components of peer review have the potential to be improved using AI, such as pre-review screening, matching papers with reviewers, or review generation. One direction to enhance the reviewing process is review summarisation. Kumar et al. (2023) propose methods to generate a pros and cons summary of given reviews of a paper. Bhatia et al. (2023) propose a tool called MetaGen to generate a meta-review from given reviews.\nConferences give specific guidelines concerning with aspects they expect the reviewers to consider. Several studies have been done around accurately predicting and understanding these scores as this enhances the trustworthiness of the reviews. An example is PeerRead (Kang et al., 2018b), the first public dataset of scientific peer reviews with corresponding decisions and aspect scores, i.e., the ratings the reviewers gave for each of the aspects. This dataset was used in Li et al. (2020) to evaluate the proposed multi-task framework for peer-review aspect score predictions. Chakraborty et al. (2020) performed an aspect-based sentiment analysis and determined that there has been a correlation between the distribution of aspect-based sentiments and the acceptance decision of papers. Recent studies are shifting to the use of a Large Language Model (LLM). Zhou et al. (2024) evaluated two LLMs, GPT-3.5 and GPT-4, on the review scores prediction and review generation tasks. The results indicated that LLMs can infer aspect scores given a review; however, their performances were inadequate when given a paper.\nArgumentation in Peer Review Methods from computational argumentation have rarely been applied in peer review applications. Notable exceptions are the works by Hua et al. (2019) and Fromm et al. (2020). However, in contrast to our work, neither consider the relations between arguments.\nHua et al. (2019) applied argumentation to understand the content and structure of peer reviews. They detect argumentative text in a review and classify it into one of the following types: evaluation, request, fact, reference, or quote. The authors then analysed and compared argumentation in reviews across several ML and NLP conferences. The results show that there have been some discrepancies in the argumentation trend across different conferences. For example, ACL and NeurIPS tend to contain most arguments, strong reject/accept reviews tend to have fewer arguments.\nAnother proposal to was made in Fromm et al. (2020). The authors aimed to extract the most relevant arguments from a review and evaluated its effect towards the paper acceptance decision. The experiment empirically indicated that correct decisions can be made by using merely half of the review."}, {"title": "Preliminaries", "content": "We recall bipolar argumentation (Amgoud et al., 2008) and few-shot learning for LLMs (Brown et al., 2020)."}, {"title": "Bipolar Argumentation", "content": "In bipolar argumentation frameworks (Amgoud et al., 2008), arguments are abstract entities; relations between them are either supports or attacks. We define them as follows.\nDefinition 1. A bipolar argumentation framework (BAF) is a tuple $(X, Att, Supp)$ where $X$ is a finite set of arguments and $Att, Supp \\subseteq X \\times X$ are attack and support relations between arguments. An argument $a \\in X$ attacks an argument $b \\in X$ if and only if $(a, b) \\in Att$. Similarly, an argument $a \\in X$ supports an argument $b \\in X$ if and only if $(a, b) \\in Supp$.\nWe furthermore consider quantitative bipolar argumentation frameworks (QBAF) (Baroni et al., 2018).\nDefinition 2. A quantitative BAF (QBAF) is a tuple $(X, Att, Supp, \\beta)$ over range $D = [0,1]$ where $(X, Att, Supp)$ is a BAF and $\\beta : X \\rightarrow D$ is a total function that assigns a base score to each argument.\nBy $A(a) = \\{b|(b,a) \\in Att\\}$ we denote the attackers, by $S(a) = \\{b|(b,a) \\in Supp\\}$ the supporters of an argument $a$.\nA semantics $\\sigma_{\\rho} : X \\rightarrow D$ for a QBAF $Q$ determines the final strength of each argument. In this work, we use the DF-QUAD semantics (Rago et al., 2016) and MLP-based semantics (Potyka, 2021).\nDefinition 3. Let $Q = <X, Att, Supp, \\beta>$ be a QBAF over $D = [0,1]$. Let $\\delta : D^* \\rightarrow D$ denote the strength aggregation function,\u00b9 such that, for $T = (v_1, ..., v_n) \\in D^*$: \n\nif $n = 0$: $\\delta(T) = 0$;\nif $n = 1$: $\\delta(T) = v_1$;\nif $n = 2$: $\\delta(T) = f(v_1, v_2)$;\nif $n > 2$: $\\delta(T) = f (\\delta(v_1,..., v_{n-1}), v_n)$;\n\nwhere, $f(x,y) = x + (1 - x) \\cdot y = x + y - x \\cdot y$, $x, y \\in D$.\nLet $\\varphi : D \\times D \\times D \\rightarrow D$ denote the influence function, where for $v_o, v_a, v_s \\in D$:\n\n$\\varphi(v_o, v_a, v_s) = \\begin{cases}\nv_o - v_o \\cdot v_s - v_a & \\text{if } v_a \\geq v_s\\\\\nv_o + (1 - v_o) \\cdot v_s - v_a & \\text{if } v_a < v_s\n\\end{cases}$\n\nFor any $a \\in X$, the DF-QuAD semantics is defined by\n$\\sigma_{DF-QuAD}(a) = \\varphi(\\beta(a), \\delta(\\sigma(A(a))), \\delta(\\sigma(S(a))))$\ns.t. $\\sigma(A(a)) = (\\sigma(a_1),..., \\sigma(a_n))$ where $(a_1, ..., a_n)$ is an arbitrary permutation of the $(n > 0)$ attackers in $A(a)$, and $\\sigma(S(a)) = (\\sigma(s_1),...,\\sigma(s_m))$ where $(s_1,..., s_m)$ is an arbitrary permutation of the $(m > 0)$ supporters in $S(a)$.\nFor each argument, DF-QuAD semantics aggregates the strengths of its attackers and supporters, and determines how both aggregates influence the base score of the argument.\n\u00b9Here, $D^*$ is the set of all sequences of elements of D."}, {"title": "Few-Shot Learning for LLMs", "content": "Large language models (LLMs) are the pretrained transformer models (Vaswani et al., 2023) that take in a text input and generate the output text. LLMs are useful for multiple tasks such as machine translation, text summarisation, and text generation. Due to the extremely large number of parameters in the LLMs e.g. 175 billion parameters for GPT-3 (Brown et al., 2020), fine-tuning them for a particular task requires a large amount of computing resources and time.\nFew-shot learning (Brown et al., 2020) is a way to resolve such constraints. The method works by providing the LLMs some examples of the task we expect them to perform, and let the LLMs learn to produce the result in a similar format as the provided examples for our inputs. This learning is done without fine-tuning, utilising the general knowledge the LLMs have acquired during their pre-training period. Examples are called primer and the input is called prompt, as in (Gorur et al., 2024)."}, {"title": "Paper acceptance by end-to-end LLMs", "content": "In this section, we propose an LLM that applies the few-shot learning methodology to classify a paper acceptance given the reviews of the paper, called end-2-end LLM.\u00b2\nThe end-2-end LLM uses a quantised 4-bit pretrained Mistral-7B-0.1 from Mistral AI\u00b3 as a pretrained LLM. The template of an input prompt given to the LLM is inspired from (Gorur et al., 2024), consisting of a primer and a prompt. The primer consists of reviews of four papers each with the final decision, two accepts and two rejects, taken from (Ghosal et al., 2022a,c; Kang et al., 2018c,a). The prompt is a set of reviews of the paper we want to predict, with no labels."}, {"title": "Peer-Review Enhanced with KR", "content": "One downside of the end-2-end LLM is its black-box nature. The model only returns the final result without intermediary steps which makes it difficult to rationalise with. To resolve this issue, we incorporate knowledge representation into the process. In this section, we present the PeerArg pipeline.\u2074 Given the reviews of a paper, the pipeline represents each of them as an argumentation framework, combining these frameworks into a single framework, and aggregating to determine the paper acceptance.\nThe pipeline diagram is as illustrated in Figure 3, assuming that there are three reviewers for a paper. The pipeline consists of three main steps: Review QBAF extraction, Review QBAFs combination, and Pre-MPAF aggregation."}, {"title": "Review QBAF Extractor", "content": "The first step is to extract an argumentation framework from each review. Reviewers are typically encouraged to assess the quality of a paper under various aspects, each of which may affect the decision more/less than others, and provide justification for their impression of the paper w.r.t. these aspects. Following this idea, we can treat aspects as arguments (called aspect arguments) that attack or support the decision (the so-called decision argument). Each sentence in a review is associated with different aspects, and so can be considered as an argument (called text argument). As a result, each review can be considered as a three-level QBAF consisting of text and aspect arguments, and decision argument.\nThere are several different aspects that conferences expect reviewers to consider. These vary for different conferences. In this paper, we focus on the aspects provided by the ACL 2016 conference, namely appropriateness, clarity, novelty, empirical and theoretical soundness, meaningful comparison, substance, and impact. For our framework, we introduce an argument for each of these aspects; abbreviated by APR, CLA, NOV, EMP, CMP, SUB, and IMP, respectively. Below, we introduce our review QBAFs.\nDefinition 5. A review QBAF is a tuple $(X, Att, Supp, \\beta)$ over $D = [0, 1]$, where $X = T \\cup A \\cup \\{Decision\\}$ with $T$ as the set of text arguments, $A = \\{APR, CLA, NOV, EMP, CMP, SUB, IMP\\}$ as the set of aspect arguments, and Decision as the decision argument. The relations $Att, Supp \\subseteq (T \\times A) \\cup (A \\times \\{Decision\\})$ are such that $Att$ and $Supp$ are disjoint. A semantics $\\sigma : X \\rightarrow D$ assigns a strength to each argument."}, {"title": "Review QBAFs Combinator", "content": "The next step is to combine our extracted review QBAFs. Review QBAFs share the same aspect arguments and decision argument, but may have different attacks, supports and text arguments. Once all the review QBAFs are extracted from the reviews, their text arguments are trimmed off. Below, we define a trimmed QBAF, which simply removes the text arguments from the review QBAF.\nDefinition 6. Given a review QBAF $(X, Att, Supp, \\beta)$ under semantics $\\sigma$ where $X = T \\cup A \\cup \\{Decision\\}$, a trimmed review QBAF is $(X_{trim}, Att_{trim}, Supp_{trim}, \\beta_{trim})$ where\n$X_{trim} = A \\cup \\{Decision\\}$\n$Att_{trim} = \\{(b, Decision)|b \\in A, (b, Decision) \\in Att\\}$\n$Supp_{trim} = \\{(b, Decision)|b \\in A, (b, Decision) \\in Supp\\}$\n$\\beta_{trim} = \\{(a, \\beta(a))|a \\in A \\cup \\{Decision\\}\\}$\nA semantics of the trimmed review QBAF is a function $\\sigma_{trim} = \\{(a, \\sigma(a))|a \\in A \\cup \\{Decision\\}\\}$.\nIn the remainder, we will refer to trimmed review QBAFs simply as review QBAFs. Next, we combine the frameworks.\nDefinition 7. Given $n$ review QBAFs $Q_1,..., Q_n$ where $Q_i = (X, Att_i, Supp_i, \\beta_i)$ for each $i < n$, and semantics $\\sigma_1,..., \\sigma_n$. The pre-Multi-Party Argumentation Framework (pre-MPAF) is defined as $(X, Und, \\beta_{vec})$ where\n$Und = \\{(a, b) | (a, b) \\in (\\bigcup_{i=1}^{n} (Att_i \\cup Supp_i))\\}$\nand $\\beta_{vec}: X \\rightarrow [0,1]^n$ is a total function such that $\\beta_{vec}(a) = [\\sigma_1(a),..., \\sigma_n(a)]$ for each argument $a \\in X$.\nThe outcome of the semantics $\\sigma_i$ from the $n$ review QBAFs is the base score of the pre-MPAF."}, {"title": "Pre-MPAF Aggregator", "content": "To obtain the final decision, we aggregate the information obtained from the pre-MPAFs. We implement two types of aggregation methods in PeerArg, as illustrated in Figure 6.\nThe first method (path (1), left, in Figure 6) aggregates the strength vectors of the pre-MPAF and uses them to identify support and attack relations. The outcome is a QBAF, called multi-party argumentation framework (MPAF), which is then used to determine the strength of the decision argument, based on DF-QuAD and MLP-based semantics. In the second method (path (2), right, in Figure 6), we focus on the strength of the decision argument. We apply a decision strength interpretation to convert a list of strengths of the decision argument into a list of decisions and aggregate them to return the final accept/reject decision. The final strength of the decision argument Decision is then used to determine the paper acceptance. We use a simple threshold such that the paper is predicted to be accepted only if the strength is more than 0.5; otherwise, rejected.\nAggregation with Argumentation (Path 1) We aggregate our pre-MPAF to obtain an MPAF by applying an aggregation function to identify a strength for each argument and to complete the attack and support relations. The process is illustrated in Figure 7. Given a pre-MPAF $(X, Und, \\beta_{vec})$, we calculate the average of the sequence of strengths $\\beta_{vec}(a)$ for all arguments $a \\in X$ to determine the relations between aspect and decision argument(s).\nDefinition 8. Given a pre-MPAF $(X, Und, \\beta_{vec})$. For each $a \\in X$, we calculate the average\n$\\gamma(a) = \\frac{1}{|X|} \\sum_{\\beta_{vec}(a) \\in \\beta_{vec}(a)}$\nWe define our MPAF $(X, Att, Supp, \\beta)$ with\n$Att = \\{(a, Decision) \\in Und | \\gamma(a) < 0.5\\}$\n$Supp = \\{(a, Decision) \\in Und | \\gamma(a) \\geq 0.5\\}$\n$\\beta(a) = \\begin{cases}\n\\gamma(a) & \\text{if a = Decision}\\\\\n|2 \\cdot \\gamma(a) - 0.5| & \\text{otherwise}\n\\end{cases}$\nWe obtain the base score $\\beta(a)$ of the arguments by averaging their strength vectors $\\beta_{vec}(a)$ and recenter it around 0.5, similar as described in Section 5.1. The average $\\gamma(a)$ furthermore determines the relation between the decision and the aspect arguments.\nIn the next step, the DF-QuAD semantics and the MLP-based semantics, respectively, can be applied to calculate the strength of the decision argument of the MPAF.\nSimilar to review QBAFs, we depict the initial strength of the aspect arguments of MPAFs in the nodes in Figure 7; the updated base scores are depicted next to the arguments.\nDecision Vector Aggregation (Path 2) We convert the strength vector of the decision argument into a decision vector. We employ two different decision interpretations: binary and uniform five-level. Binary interpretation simply treats a strength of 0.5 or below to be 'reject', and over 0.5 to be 'accept'. For the uniform five-level interpretation, the argument's strength range between 0 and 1 is divided equally into five regions. For a decision argument with strength s, the decision d(s) is\n$d(s) = \\begin{cases}\n\\text{strong reject} & 0 < s < 0.2\\\\\n\\text{weak reject} & 0.2 \\leq s < 0.4\\\\\n\\text{borderline} & 0.4 < s < 0.6\\\\\n\\text{weak accept} & 0.6< s < 0.8\\\\\n\\text{strong accept} & 0.8 \\leq s \\leq 1.0\n\\end{cases}$"}, {"title": "Experiments", "content": "We evaluate the performance of PeerArg in paper acceptance prediction in comparison with the end-2-end LLM approach. We use three datasets for our classification evaluation: Peer-Review-Analyze (PRA), PeerRead, and Multi-disciplinary Open Peer Review Dataset (MOPRD). The PRA dataset (Ghosal et al., 2022b) contains reviews of accepted and rejected papers from ICLR 2018 conference. Each sentence in a review is annotated for which aspects it belongs to, and the sentiments the sentence has towards such aspects. The PeerRead dataset (Kang et al., 2018b) contains reviews from various computer science conferences. In this paper, only the reviews from the ACL 2017 conference were used since their corresponding papers were classified as accepted or rejected. Additionally, each review has scores for each aspect. We therefore consider two cases when these scores are set and not set as base scores of the aspect arguments. Finally, the MOPRD dataset (Lin et al., 2023) contains reviews from several journals in various fields such as computer science, biology, and medicine. In this paper, we used the reviews from the medical field in our evaluation."}, {"title": "Conclusion", "content": "We introduced two approaches, PeerArg and an end-2-end LLM, to enhance the peer reviewing process by predicting paper acceptance from reviews. In contrast to the end-2-end LLM that uses few-shot learning techniques to predict paper acceptance in a black-box nature, PeerArg adopts both methods from LLM and computational argumentation to support a decision in the peer reviewing process. Our experimental results show that PeerArg can outperform the end-2-end LLM, while being more transparent due to the interpretable nature of argumentation.\nFor future work, we plan to leverage the interpretability of the proposed argumentation model to improve the explainability of the (automated) review aggregation process, similar to how argumentation models are recently used to interpret neural networks, especially the multi-layer perceptrons (Potyka, 2021). Moreover, we aim to combine text arguments into pre-MPAFs in the reviews aggregation step. It would also be interesting to explore uncertainty in aggregation and how it would affect the acceptance prediction."}, {"title": "Appendix (Technical Details)", "content": "This section outlines all the relevant models involved in the argument mining process (aspect classification & sentiment analysis) of the PeerArg pipeline."}, {"title": "Aspect Classification", "content": "The LLM for aspect classification is a quantised 4-bit pretrained Mistral-7B-v0.1 model from Mistral AI6. We used few-shot learning method (Brown et al., 2020). Inspired from (Gorur et al., 2024), the template contains a primer and a prompt. The primer has a description of seven aspect criteria for a paper review, and ten different samples of review sentences with their corresponding aspects. The prompt contains the review sentence we want to classify the aspect(s) but with no labels. Note that we split the review into sentences using NLTK sentence tokeniser (Bird et al.,"}, {"title": "Sentiment Analysis", "content": "Sentiment analysis is done per sentence in a review using a pretrained ROBERTa model trained on Twitter tweets fine-tuned for sentiment analysis. The model takes an input text and returns an output in the form <label, strength> where label is the predicted sentiment (positive/neutral/negative) and strength is how likely this sentiment is."}]}