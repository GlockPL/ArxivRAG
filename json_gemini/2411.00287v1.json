{"title": "MBEXPLAINER: Multilevel bandit-based explanations for downstream models with augmented graph embeddings", "authors": ["Ashkan Golgoon", "Ryan Franks", "Khashayar Filom", "Arjun Ravi Kannan"], "abstract": "Graph Neural Networks (GNNs) are a highly useful tool for performing various machine learning prediction tasks on graph-structured data. GNNs are widely used in the industry for a variety of applica-tions. In many industrial applications, it is common that the graph embeddings generated from training GNNs are used in an ensemble model where the embeddings are combined with other tabular features (e.g., original node or edge features) in a downstream machine learning task. The tabular features may even arise naturally if, e.g., one tries to build a graph such that some of the node or edge features are stored in a tabular format. The goal of this paper is to address the problem of explaining the output of such ensemble models for which the input features consist of learned neural graph embeddings (possibly combined with additional tabular features). Therefore, we propose MBEXPLAINER, a model-agnostic explanation approach for downstream models with augmented graph embeddings. MBEXPLAINER re-turns a human-comprehensible triple as an explanation (for an instance prediction of the whole pipeline) consisting of three components: a subgraph with the highest importance, the topmost important nodal features, and the topmost important augmented downstream features. A game-theoretic formulation is used to take the contributions of each component and their interactions into account by assigning three Shapley values corresponding to their own specific games. Finding the explanation requires an efficient search through the local search spaces corresponding to each component. MBEXPLAINER applies a novel multilevel search algorithm that enables simultaneous pruning of local search spaces in a computation-ally tractable way. In particular, three interweaved Monte Carlo Tree Search are utilized to iteratively prune the local search spaces. MBEXPLAINER also includes a global search algorithm that uses contex-tual bandits to efficiently allocate pruning budget among the local search spaces. We demonstrate the effectiveness of MBEXPLAINER by presenting a series of comprehensive numerical examples on multiple public graph datasets for both node and graph classification tasks.", "sections": [{"title": "1 Introduction", "content": "Geometric data are now ubiquitous in our day-to-day life from social networks to networks of neurons and molecular structures. Due to their generalized message passing scheme Graph Neural Networks (GNNs) offer a very powerful framework for performing representation learning on geometric data. The goal of a neural graph embedding method is to learn some lower dimensional representation of the graph structure. Deep graph models have been successfully used in various fields such as modeling financial transactions, Physics, drug discovery, and recommender systems (see [Zhou et al., 2020]).\nImproving the performance of GNNs has been under intense investigations in recent years. Some of these works include graph convolution, graph attention, graph transformers, and graph pooling.\nDeep graph models, despite their robust capabilities, fall short in terms of transparency due to their black-box nature [Yuan et al., 2021]. This makes it hard to generate human-comprehensible explanations for GNNs predictions [Ying et al., 2019]. Nevertheless, it is important to generate explanations for GNNs or downstream models that utilize neural graph embeddings. This is because in many decision-critical applications (e.g., healthcare or financial services) it is vital (if not required by regulations)\u00b9 to explain the predictions of deep graph models. Even for non-decision-critical fields, explainablity is still useful because it can generate insights about the graph structure by identifying the most important components of the input data that render a given prediction (see, e.g., [Wong et al., 2023]).\nGenerating explanations for GNNs has been the focus of extensive research during past years. Some of the notable contributions include: GNNExplainer [Ying et al., 2019], SubgraphX [Yuan et al., 2021], PGExplainer [Luo et al., 2020], PGM-Explainer [Vu and Thai, 2020], and CF-GNNExplainer [Lucic et al., 2022] (see \u00a72.1 for a comprehensive review of the existing GNNs explainability methods). These methods can be used to effectively explain predictions of GNNs."}, {"title": "2 Related Work", "content": "In this section, we review some fundamental concepts and formulations of graph neural networks (see, [Kipf and Welling, 2016, Hamilton et al., 2017, Veli\u010dkovi\u0107 et al., 2017, Hu et al., 2019, Wang et al., 2019c, Li et al., 2020] for more details).\nLet G = (V,E) denote a graph with vertices V = {V1, V2,\u2026, vv} and edges E \u2286 V \u00d7 V. An edge\nCij = (Vi, vj) \u2208 E, connects vertices vi and v; if the graph is undirected.2 We denote the features of a node\nv and an edge e by h\u2082 \u2208 Rd and he \u2208 Rb, respectively.\nGraph Neural Networks formulation. In this section, we discuss some background on graph repre-sentation learning, including message passing and aggregation schemes [Fey and Lenssen, 2019, Yuan et al.,\n2021, Fang et al., 2023]. For simplicity, we assume that only node features are updated at each layer [Li\net al., 2020].3 Therefore, for a node i and its neighborhood N(i), one defines\n$m_{ij}^{k} = \\varphi^{k}_{e_{ij}}(h_{i}^{k}, h_{j}^{k}, h_{e_{ij}}), j \\in \\mathcal{N}(i)$        (2.1)"}, {"title": "2.1 Review of the existing interpretability methods for GNNS", "content": "Next, we review some of the commonly used graph explanation methods from the literature. For the rest of this section, we generally follow the taxonomy provided by Yuan et al. [2022] in a recent survey. One may classify explanation methods for GNNs into five broad categories: gradient, perturbation, decomposition, and surrogate methods for instance-level explanations, and generative methods for model-level explanations.\nMethods based on perturbing a continuous (or a discrete) mask have been intensely studied by GNNs explainability researchers [Ying et al., 2019, Luo et al., 2020, Yuan et al., 2021, Schlichtkrull et al., 2020]. These methods investigate the effects of input perturbations on how it varies the output predictions of deep graph models [Yuan et al., 2022].\nOne of the early approaches addressing explainability of GNNs (for instance-level explanations) was proposed by [Ying et al., 2019]. Here, we summarize the idea of the GNNExplainer method. Let us consider the node classification task for a node v for which the predicted class probability is given by \u0177. The computational graph of node v is given by Gc(v) = (Ac(v), Xc(v)), where Ac(v) \u2208 {0,1}n\u00d7n and\n$A' = A_{c} \\odot \\sigma(M_{E})$ and $X' = X_{c} \\odot M_{F}$        (2.4)"}, {"title": "3 Downstream Neural Graph Embeddings", "content": "In this section, we provide some insights about building downstream machine learning models using neu-ral graph embeddings. The purpose of this section is to provide the reader with scenarios that building downstream models with neural graph embeddings are useful. In particular, we discuss efficient end-to-end training of both GNNs and the downstream models as well as strategies for categorical features encoding. Note that the interpretability (or explainability) module of MBEXPLAINER and its underlying algorithms (developed in Section 4) are agnostic to how the GNNs model and the downstream model are trained. In fact, MBEXPLAINER only assumes that the trained GNNs fe and the downstream model fd are supplied to the algorithm."}, {"title": "3.1 End-To-End Formulation and Pipeline Training", "content": "GNNs are very efficient in encoding the underlying geometric structure between data points. However, they often have suboptimal performance when it comes to non-geometric data like tabular data (especially, when heterogeneous [Ivanov and Prokhorenkova, 2021]). On the other hand, Gradient Boosted Decision Trees (GBDTs) perform very well on tabular data. Therefore, we propose novel architectures and training strategies for leveraging the capabilities of both GNNs and GBDTs when graph embeddings are augmented with tabular features in a downstream machine learning model. In particular, we discuss employing GBDTs as either a stand-alone or iterative encoder (and imputer) to enrich node (or edge) features of GNNs. Additionally, we discuss concurrent (end-to-end) training of GNNs and downstream machine learning models (either a GBDT or a general model architecture). Our primary intention for devising a concurrent training strategy is to make use of the backpropagation errors from both models to concurrently update both models' parameters in order to achieve a more optimally trained modeling pipeline with faster convergence overall."}, {"title": "3.1.1 Leveraging GBDTs to natively handle categorical features and missing values", "content": "One caveat of utilizing DNNs in general\u2014and GNNs in particular is that they are simply unable to handle and encode categorical features and missing values natively. However, it is possible to employ tree-based models, along with GNNs to leverage their superior native encoding and imputing mechanisms (see, e.g., [Ivanov and Prokhorenkova, 2021, Stekhoven and B\u00fchlmann, 2012]).\nStand-alone GBDTs as encoder. In obtaining the graph neural embeddings from the GNNs model, we propose to train a GBDT using node (or edge) features on a supervised task, e.g., node classification or link prediction. Concretely, we utilize the output predictions (probability scores) from the trained GBDT as an extra feature that is added to the graph either as a node (or an edge) feature. Next, the neural graph embeddings are obtained by training the GNNs model. This way, we are able to take advantage of superior categorical features and missing values encoding handled efficiently by a GBDT approach, e.g., a CatBoost model. Finally, similar to the previous setup, the obtained embeddings are used, along with additional features, in training a downstream model."}, {"title": "3.1.2 Concurrent training of graph embeddings and downstream models", "content": "It is also possible to design a training pipeline such that both the GNNs and downstream models are trained concurrently. The details of the training algorithm depend ultimately on the type of a downstream model. For example, whether the model architecture allows for the model parameters update using SGD (or mini-batch SGD) like DNNs, or, whether the downstream model is a GBDT like CatBoost, where we can optimize the model iteratively using an approach similar to Algorithm 1 (i.e., by iteratively updating the target label for weak learners)."}, {"title": "4 MBExplainer", "content": "In this section, we carefully demonstrate the interpretability formulation for MBEXPLAINER. In particular, we formulate the interpretability problem for an ensemble model where neural graph embeddings generated from an upstream GNNs model are augmented with additional tabular features in a downstream ML model when performing predictions. To the best of our knowledge, this problem has not been addressed before in the literature. This novel interpretability approach offers a way to generate explanations for the whole pipeline which consists of two inherently different components: i) neural graph embeddings, possessing a complex geometric structure of the underlying graph data, and ii) augmented tabular data in downstream with absolutely no geometric structure.\nNote that one may be tempted to employ post-hoc explainers or self-interpretable explainers (if applicable) for the downstream model and treat the graph embeddings as some synthetic set of features. However, this method fails to take into consideration the complex dependencies of graph embeddings and original features, and thus, leads to ambiguous explanations. Moreover, our devised interpretability approach takes all the interactions between downstream features and graph embeddings into consideration. Therefore, this approach results in an explanation that properly measures the true importance of each component."}, {"title": "4.1 Formulating Explanations at the Downstream Level", "content": "In this section, we formulate the problem of graph embeddings explanations when performing predictions in a downstream model where in addition to the neural embeddings one uses augmented tabular features. Consider the previous setup, let fe and fd, respectively, denote the trained GNNs model and the trained downstream model. Let the k-hop neighborhood of a target node v be identified with N(v), consisting of the computational graph G with nodes {v1,..., vm} and the adjacency matrix A \u2208 Rmxm. The node and edge features are, respectively, given by Xv \u2208 Rn and Xeur \u2208 Rn vectors. The resulted graph embeddings for node v from the l-th layer of the GNNs model are given by h = fe(Gv) \u2208 RD, where D is the hidden dimension of the l-th output layer. The goal is to find an explanation for the whole pipeline viewed as a single model when the neural graph embeddings h are augmented with downstream features denoted by xv \u2208 Rn.\nxv = {x1,...,x}.  (4.1)\nIn other words, we are interested in generating an instance-level explanation for the prediction of the seed node v when its embeddings h are used at downstream such that:\nfd (xv|| hk) = fd (xv||fe (Gv)) .  (4.2)"}, {"title": "Game-Theoretical Explanation Formulation.", "content": "Next, we propose to employ Shapley values (see, [Kuhn and Tucker, 1953, Lundberg and Lee, 2017, Chen et al., 2018, Miroshnikov et al., 2021]) to obtain various importance scores in the course of an explanation. In doing so, we introduce the triple (S', G', M) such that:\n\u2022 S' denotes a downstream feature mask that is going to identify the topmost important augmented features at the downstream level.\n\u2022 G' denotes a subgraph of the computational graph in the trained GNNs which is having the highest importance for the pipeline prediction of the seed node.\n\u2022 M defines a node feature mask that is going to identify the topmost important nodal features in the subgraph G'.10 Note that, here, for the sake of simplicity, we assume that the feature mask is uniformly applied to all nodes. However, later we provide insights as to how the formulation changes if one wants to consider a nonuniform node feature mask.\nTo quantify the contributions of each component, we assign three Shapley values each corresponding to their own specific game. Therefore,\n$\\varphi^{(S',G',M)}(S') := \\sum_{S_1 \\subseteq N \\setminus S'} \\frac{|S_1|!.(n-s'-|S_1|)!}{(n - s' + 1)!} [f(X_{S_1},X_{N \\setminus (S_1 \\cup S')}, f^{e}((G')M)) - f(X_{S_1}, X_{N \\setminus S_1}, f^{e}((G')M))];$             (4.3)\n$\\varphi^{(S',G',M)}(G') := \\sum_{S_2 \\subseteq (Uk+1,..., Um)} \\frac{|S_2|!.(m-k-|S_2|)!}{(m - k + 1)!} [f(x_{S'}, x_{N \\setminus S'}, f^{e}((G' \\cup S_2)M)) - f(x_{S'}, x_{N \\setminus S'}, f^{e}((S_2)M))];$         (4.4)\n$\\varphi^{(S',G',M)}(M) := \\sum_{S_3 \\subseteq N \\setminus M} \\frac{|S_3|!.(n-|M|-|S_3|)!}{(n - |M|+1)!} [f(x_{S'}, x_{N \\setminus S'}, f^{e}((G')M \\cup S_3)) - f(x_{S'}, x_{N \\setminus S'}, f^{e}((G')M))];$          (4.5)\nwhere the set of players for each game is defined as:\n$P^{S'} := \\binom{x_{S'}, x_{S'+1},..., x_{n}}{ features not in S'}$                (4.6)\n$P^{G'} := \\binom{(G')M, U_{k+1},..., U_{m}}{nodes not in G'}$                      (4.7)\n$P^{M} := \\binom{x_{1},..., x_{n-|M|}, (x_{n-|M|+1},..., x_{n})}{features not masked by M features masked by M}$          (4.8)\nIn the first formula (4.3), we iterate over subsets S\u2081 of augmented downstream features not masked by S'\n(s' denotes the size of S'). Thus, (4.3) gives the Shapley value of S' in a game defined based on (S', G', M),\nwhose players are given by PS', i.e., the mask S' of the downstream features, along with downstream features not in S'. In the second formula (4.4), we iterate over subsets S2 of nodes which are not in G'. Hence, (4.4)\ngives the Shapley value of G' in a game defined based on (S', G', M), whose players are given by P\u00ba', i.e.,\nthe subgraph G' and the nodes of the computational graph G that are not in G' denoted by {Uk+1,\u2026\u2026, Um}.\nFinally, in (4.5) we iterate over subsets S3 of node features not masked by M (the size of M, which gives\nthe number of features masked by M is denoted by |M|). The players of a game defined with values in (4.5)\nare given by PM which includes the node features masked by M (as a single player) and those not masked\nby M. Note that x* denotes a baseline datapoint from downstream feature distribution (often obtained by\ntaking the average over the whole distribution). Moreover, we adopt a zero-padding strategy to compute\nterms such as fe((G')M) by setting features of nodes not belonging to G' to zero and applying the mask M\nto features of nodes belonging to M.\nHaving defined the Shaley values for each component in (S', G', M), we define the following measurement\nto attribute an importance score to a triple (S', G', M) as a whole:\n$\\varphi^{(S',G',M)}(S', G', M) := \\lambda_{S'}.\\varphi^{(S',G',M)}(S') + \\lambda_{G'}.\\varphi^{(S',G',M)}(G') + \\lambda_{M}.\\varphi^{(S',G',M)}(M),$  (4.9)\nwhere ' and AM are constants11 that control the relative importance of the subgraph G' and the node feature mask M as components of the explanation with respect to each other as well as relative to the downstream feature mask S'. One can set these hyperparameters to fine-tune the explanation score or even use them to make handcrafted explanation measurement scores depending on different use cases. In Section\n4.2, we discuss acceleration strategies and algorithms utilized to implement the described explainability\napproach. A promising explanation needs to have a high \u03c6(S',G',M) (S', G', M) value, and at the same time,\nit must be succinct enough so that the explanation is concise and useful."}, {"title": "4.2 Acceleration Strategies and Implementation Architecture", "content": "In this section, we provide our acceleration strategies and algorithms that are developed and carefully tailored to make the computation of Shapley values, formulated in Section 4.1, tractable and scalable to large graphs and feature sets. Obtaining an explanation (S', G', M) through searching all possible combinations (a brute-force method) is either intractable or comes at a high computational expense. Therefore, here, we develop a novel search algorithm to overcome the computational barriers of finding a promising explanation. Particularly, we employ a multilevel bandit-based search strategy that iteratively prunes the search spaces of subgraphs, their node features, and downstream features simultaneously. The search strategy consists of two parts, namely the local and global search algorithms. The local search strategy includes pruning the space of subgraphs, their node features, and downstream features, whereas the global search algorithm allocates the pruning budget among the three local search spaces in an efficient manner. The local search spaces are navigated using three interweaved Monte Carlo Tree search. The global search strategy is developed using a contextual bandits type algorithm which drives efficient search among the local search spaces."}, {"title": "4.2.1 Assembling the local search spaces", "content": "We propose to utilize the Monte Carlo Tree Search (MCTS) (see, e.g., [Yuan et al., 2021, Silver et al.,\n2017, Jin et al., 2020]) in order to locally navigate the three search spaces, namely the space of subgraphs,\nnode features, and augmented downstream features. Therefore, we employ three interweaved MCTSs that\niteratively prune the local search spaces. Let us denote the search trees corresponding to the space of\nsubgraphs, node features, and augmented downstream features by TG', TM, and TS', respectively. Note\nthat the tree root corresponding to S' is given by N (i.e., no augmented downstream features is set to the\nbaseline x*, and thus, |S' = 0) in the tree Ts'. The root for To' is the computational graph of the seed node\nv, i.e., G (with no nodes pruned). 12 Finally, the root for TM corresponds to N (i.e., no node features is\nmasked, and thus, |M|= 0). In the following, we describe the elements of the MCTS, including tree traversal\n(tree policy), node expansion, roll-out (Monte Carlo simulation or default policy), and back-propagation for\nthe three pruning actions that iteratively limit the three local search spaces.\nAssembling the MCTS for TS', TG', and TM. In the following, we discuss how the MCTS algorithm navigates the search trees TG', TM, and Ts'. Without loss of generality, we discuss the details of the MCTS for TS' as the process is similar for TM and TS'. Let us consider the computational graph of the seed node\nv in the GNNs model, i.e., Gu. We are interested in pruning the computational graph to find a subgraph\npart of the explanation (S', G', M). Starting with the root node, corresponding to the computational graph\n$U(s_i, a_j) = c.P(s_i, a_j).\\sqrt{\\frac{\\sum_{a_k \\in A}N(s_i, a_k)}{1+ N(s_i, a_j)}}$         (4.14)\n$Q(s_i, a_j) = \\frac{W(s_i, a_j)}{N(s_i, a_j)}$                     (4.15)"}, {"title": "4.2.2 Assembling the global search space", "content": "The next question we need to answer is how we properly allocate pruning budgets among the local search trees, i.e., TG', Ts', and TM. There are many heuristic approaches that one may adopt, e.g., one can uniformly explore the three local spaces. However, these heuristic approaches are not optimal and often lead to computational inefficiencies, and thus, could result in suboptimal explanations. Therefore, here, we propose to use a contextual bandits type algorithm [Cortes, 2018, Li et al., 2010] to efficiently allocate pruning budgets to each search tree. The contextual bandits algorithm gives us the flexibility to utilize various exploration-exploitation strategies depending on possible constraints on the desired explanation. Moreover, a contextual bandits method facilitates experimenting with and choosing different classes of oracles to fit to context-reward pair for each arm (or action).\nNext, we discuss how the bandit algorithm is formulated. In doing so, we assume that the arms or actions are the choice of selecting one of the three local search spaces on each episode from  M, where  $P^S' = \\binom{x_1, ..., x_n}{features not in S'}$ (4.20)\nor, alternatively, enforce a limit on the total size of the explanation such that:\n|(S', G', M) |:= |M\u00b0|+ |S'\u00b0| + |G'| 25,M).   (4.21)"}, {"title": "4.3 Implementation details for MBExplainer", "content": "In this section, we describe how our methodology is implemented. At a high-level, the algorithm starts out with an explanation T' = (S', G', M') consisting of the original model input or the output of a previous round of pruning. Next, a pruning method a is picked (randomly or using an oracle) and pruning actions are performed until the budget Ba is exhausted. Finally, an intermittent reward is computed using the pruned T' which is then propagated to its parents. Then, this process of action selection and pruning is repeated until T' satisfies our explanation requirements. We then repeat this process until we have generated a satisfactory number of explanation candidates and select the best one according to our importance score. For a more detailed outline of this procedure, see Algorithm 5. For a demonstration of how the algorithm progresses, see a sample of how MBEXPLAINER logs its progress in Appendix 7.1.\nAlgorithm 5 may be directly applied to graph classification models and naturally generalizes to node classification contexts. For example, given that we have an upstream neural graph embedding model fe : GM \u2192 (RD)|9|, a downstream model fd : RN \u00d7 RD \u2192 R and a prediction for node v that we seek to"}, {"title": "5 Experiments", "content": "In this section, through several detailed numerical examples we demonstrate how MBEXPLAINER can be applied to downstream models with neural graph embeddings. In particular, we study MBEXPLAINER'S performance for graph classification tasks using the MUTAG, PROTEINS, and ZINC datasets as well as node classification tasks on the ogbn-arxiv dataset. Note that all these datasets were downloaded from the Deep Graph Library (DGL) [Wang et al., 2019a, v1.1.0]."}, {"title": "5.1 MUTAG", "content": "The MUTAG dataset [Debnath et al., 1991] contains 188 nitroaromatic compounds represented as graphs with nodes corresponding to atoms and edges corresponding to bonds. Each compound also has a binary label corresponding to its mutagenicity on Salmonella typhimurium. Each node has a seven dimensional"}, {"title": "5.2 PROTEINS", "content": "The PROTEINS dataset [Borgwardt et al., 2005] consists of 1113 proteins where nodes are secondary struc-ture elements (SSEs) of those proteins and edges exist if SSEs are less than six angstroms apart or are neighbors in the amino-acid sequence. Each protein has a binary label corresponding to whether or not it is an enzyme. Each node has a three dimensional node feature which represents its SSE type (helix, sheet or turn) via one-hot encoding. We further augment each protein with tabular/graph-level features correspond-ing to the total count it contains of each SSE type (three features) by summing up the node features. We"}, {"title": "5.3 ZINC (Binary Classification)", "content": "The subset of the ZINC dataset we employ [Dwivedi et al., 2024] consists of 12,000 molecules represented with nodes corresponding to atoms and edges corresponding to bonds. From these 12,000 molecules, 1,000 of them are reserved for a validation and test set each. Each molecule has a label which quantifies its constrained solubility. Each node and bond have one-dimensional features describing what type of atom and what type of bond they correspond to, respectively. To prepare the dataset for graph classification, we one-hot encode both the node feature and edge feature to produce a 21 dimensional node feature and three dimensional edge feature. We also rescale the node features using the training and validation set as was done in the MUTAG experiment. Finally, we create a binarized label which is one if the constrained solubility exceeds its median value of about 0.4407 on the training dataset.\nWe further augment each molecule with seven tabular/graph-level features engineered from the one-hot encoded edge features. These engineered features include the total count of each bond type in the molecule, the proportion of bonds made-up of each bond type, and the atom-to-bond count ratio. We construct the upstream and downstream models using the same methodology as in the MUTAG experiment. Note that our GIN upstream model does not make use of edge features so only our downstream CatBoost model may make use of them, via our engineered augmented downstream features."}, {"title": "5.4 ogbn-arXiv", "content": "The ogbn-arXiv dataset consists of a 90941 node training set, 29799 node validation set and 48603 node test dataset where each node corresponds to an arXiv paper [Hu et al., 2020a]. Each paper also corresponds to one of forty arXiv subject areas. While ogbn-arXiv edges are directed and correspond to when one paper cites another, we append reverse edges for the purpose of link prediction. Each arXiv paper node is associated with a 128 dimensional feature vector obtained by averaging embeddings of words in its title and abstract.\nFor the upstream model, we construct a two layer GraphSAGE neural network with 64 dimensional hidden layers and a 32 dimensional output layer. It learns to produce node embeddings such that the dot product between the embeddings of two nodes corresponds to the logit of the probability that a link exists between them. Uniform neighborhood sampling is employed with four 1-hop and four 2-hop neighbors being sampled. The model is trained for one epoch.\nWe use this upstream model to generate node embeddings for the whole ogbn-arXiv dataset and generate a tabular dataset combining these node embeddings with the original word embedding features. Finally, we train a CatBoost model (100 epochs iterations, default parameters) on this tabular dataset to predict its subject area.\nFinally, we may employ MBEXPLAINER to produce multilevel explanations for our model. In this example, we adjust the budget and explanation requirements to accommodate the larger graph size and feature space. We let \u043a = 1, B1:3 = (6,18,6), min = (6,18,6) and estimate Shapley values using 25 permutations."}, {"title": "5.5 Results Summary", "content": "Table 1 describes the properties of these datasets at a high level. Table 2 summarizes certain quantitative observations about the experiment. On MUTAG and PROTEINS, MBEXPLAINER is somewhat slower than SubgraphX (given the extra work it needs to do) but much slower than SubgraphX on ZINC. This is because"}, {"title": "6 Conclusions", "content": "In many practical applications, graph embeddings generated from a trained GNNs model in the upstream are augmented with additional tabular features and utilized in order to train a downstream model for a particular prediction task. As a matter of fact, when it comes to encoding geometric data, GNNs are very efficient, but they often demonstrate suboptimal performance on tabular data. On the other hand, tree-based models such as GBDTs and CatBoost perform very well on tabular data and can natively encode categorical features and handle missing values. Therefore, it is very common to encounter downstream models with augmented neural graph embeddings, making the development of explainability approaches for these ensemble models critically important. Although there are existing interpretability methods for both GNNs and tabular ML models, they are not sufficient to fully explain these downstream models. For instance, employing tabular post-hoc (or self-interpretable) explainers for the downstream model, one would inevitably treat graph embeddings as extra synthetic features (which is equivalent to freezing the components of the upstream computational graph). This, in turn, would result in overlooking the dependencies and complex interactions of neural graph embeddings and augmented tabular features, leading to obtaining incomplete and misleading explanations.\nIn this paper, we proposed MBEXPLAINER, an interpretability approach to comprehensively explain downstream models with augmented graph embeddings in a single pipeline. To the best of our knowl-edge, this is the first attempt in generating explanations for pipelines that consist of two fundamentally distinct components, namely: graph embeddings with their complex geometric structure and augmented tabular features in downstream having no geometric structure. MBEXPLAINER returns a human-legible triple consisting of the most important subgraphs, their node features, and augmented downstream features as explanations. The method enjoys a game-theoretic formulation to account for the contributions of each component. In particular, three Shapley values are assigned each corresponding to their specific games in the spaces of downstream features, subgraphs, and node features. The formulation also considers the complex interactions between these spaces when corresponding Shapley values are calculated.\nTo ensure the computational cost of generating an explanation remains manageable, MBEXPLAINER enjoys a novel multilevel searching strategy that makes it possible to simultaneously prune the spaces of sub-graphs, their node features, and downstream features as the local search spaces. The local search algorithm is navigated using three interweaved Monte Carlo Tree Search which is responsible for iterative pruning of the local search spaces. The global search strategy leverages contextual bandits algorithm which efficiently allocates pruning budget to drive search among the three local search spaces.\nMBEXPLAINER is applicable to a broad range of machine learning prediction tasks on graphs and is model-agnostic in terms of both the upstream GNNs and the tabular ML model in the downstream. To demonstrate the effectiveness of our method, we presented a series of numerical experiments on public datasets. In particular, we presented MBEXPLAINER's performance on the MUTAG, PROTEINS, and Binarized ZINC for graph classification tasks and on the ogbn-arxiv dataset for node classification tasks."}, {"title": "7 Appendix", "content": "7.1\nExample MBExplainer Log Output\nMBEXPLAINER is implemented with an optional logging functionality which records how the pruning of an explanation progresses. An example log is shown below, corresponding to the first MBEXPLAINER iteration in our PROTEINS experiment \u00a75.2.\nThe log begins by announcing the start of an MCTS rollout, describing how many rollouts have been performed and the number of subgraphs that have been explored. At the start of this rollout, we return to an initial explanation composing the whole graph and all upstream and downstream features. The log then continues with the next step, determining the appropriate pruning action to apply to our starting explanation. Until a user-specified number of rollouts are completed (in this case, five), this pruning action is determined randomly. Afterward, it is determined by user-specified oracles.\nWith an initial explanation defined and a pruning action defined, the log continues by describing the progress of the pruning procedure. The log reports some details of the initial explanation, along with some details of the child explanation selected using criteria in Eq. (4.13). In this case, the pruning action is being applied to downstream features so the child has one fewer downstream feature than its parent. It turns out that this satisfies the explanation's requirement on the downstream feature count so the log announces that downstream pruning is complete and removes this action from consideration until the end of the rollout.\nThe intermittent reward for this pruning action is recorded by the oracle, but, as the log reports, it is not refit. The oracle is not needed yet so a refit is unnecessary.\nWith the downstream pruning action episode complete, the log announces that a new pruning action has been selected for a new episode. In this case, nodes are being pruned. As before, details about each parent"}]}