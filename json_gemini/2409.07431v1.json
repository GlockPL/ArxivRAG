{"title": "SYNTHETIC CONTINUED PRETRAINING", "authors": ["Zitong Yang", "Neil Band", "Shuangping Li", "Emmanuel Cand\u00e8s", "Tatsunori Hashimoto"], "abstract": "Pretraining on large-scale, unstructured internet text has enabled language models\nto acquire a significant amount of world knowledge. However, this knowledge\nacquisition is data-inefficient; to learn a given fact, models must be trained on\nhundreds to thousands of diverse representations of it. This poses a challenge\nwhen adapting a pretrained model to a small corpus of domain-specific docu-\nments, where each fact may appear rarely or only once. We propose to bridge this\ngap with synthetic continued pretraining: using the small domain-specific corpus\nto synthesize a large corpus more amenable to learning, and then performing con-\ntinued pretraining on the synthesized corpus. We instantiate this proposal with\nEntiGraph, a synthetic data augmentation algorithm that extracts salient entities\nfrom the source documents and then generates diverse text by drawing connec-\ntions between the sampled entities. Synthetic continued pretraining using Enti-\nGraph enables a language model to answer questions and follow generic instruc-\ntions related to the source documents without access to them. If instead, the source\ndocuments are available at inference time, we show that the knowledge acquired\nthrough our approach compounds with retrieval-augmented generation. To better\nunderstand these results, we build a simple mathematical model of EntiGraph, and\nshow how synthetic data augmentation can \"rearrange\" knowledge to enable more\ndata-efficient learning.", "sections": [{"title": "INTRODUCTION", "content": "Language models have demonstrated a remarkable ability to acquire knowledge from unstructured\ntext, enabling them to perform challenging knowledge-intensive tasks (Brown et al., 2020; OpenAI\net al., 2024; Gemini, 2024; Anthropic, 2024b; Dubey et al., 2024; Gunter et al., 2024). These suc-\ncesses are enabled by the combination of the next-token prediction objective (Shannon, 1951) and\nlarge-scale internet data (Common Crawl, 2007). However, it is becoming increasingly apparent\nthat this approach is data-inefficient; for example, a 13-year-old human acquires knowledge from\nfewer than 100M tokens, while state-of-art open-source language models are trained on 15T to-\nkens (Warstadt et al., 2023; Dubey et al., 2024). Recent works have highlighted a range of related\nproblematic phenomena, including the \u201creversal curse\u201d, where models struggle to learn the relation\n\"B=A\" when trained on \u201cA=B\u201d (Berglund et al., 2023), and the requirement that models be exposed\nto thousands of examples per fact for knowledge acquisition (Allen-Zhu & Li, 2024).\nThese drawbacks pose a challenge when adapting the next-token prediction paradigm to learn from\nsmall-scale corpora. Because large-scale pretrained models already capture much of public common\nknowledge, further advancements will necessitate learning from the tails of the distribution (Kandpal\net al., 2023): niche data that is either contained in small, private domains or appears only once or\ntwice on the internet. This challenge of data-efficient, parametric knowledge acquisition is becoming\nincreasingly important as growing compute capacity enables language model providers to exhaust\npublicly available data (Muennighoff et al., 2023; Villalobos et al., 2024)."}, {"title": "1.1 RELATED WORK", "content": "Synthetic data generation. There is a rich literature on using neural nets to generate synthetic\ndata. Many such approaches were originally developed for semi-supervised learning\u2014self-training\nand pseudo-labeling methods improve models by iteratively training them on their own predictions\n(Scudder, 1965; Lee, 2013; Yalniz et al., 2019; Berthelot et al., 2019; Xie et al., 2020), and co-\ntraining uses two models to supervise each other (Blum & Mitchell, 1998; Balcan et al., 2004).\nBefore language models rose to prominence, few approaches attempted to synthesize inputs. One\nexception is membership query synthesis, which explored the synthesis of inputs in a supervised\nlearning context (Angluin, 1988; Schumann & Rehbein, 2019).\nContemporary works employ co-training (Lang et al., 2022) and self-training to improve language\nmodel performance, often on mathematical reasoning tasks (Huang et al., 2023; Gulcehre et al.,\n2023; Zhang et al., 2024a), or synthesize input-output pairs for instruction tuning, usually by con-\nditioning on a curated seed set (Wang et al., 2023b; Honovich et al., 2023; Taori et al., 2023; Peng\net al., 2023; Yuan et al., 2024b; Li et al., 2024).\nMost relevant to the present work are methods that synthesize pretraining data using hierarchical\nprompting methods to promote dataset diversity. Eldan & Li (2023) prompt API-based LLMs to\ngenerate children's stories containing sampled keywords, and demonstrate that even small language\nmodels trained on their dataset can generate fluent text. Gunasekar et al. (2023) synthesize a diverse\ndataset of textbooks and code exercises by conditioning on topic, target audience, and function\nnames, and later release strong LLMs pretrained on synthetic data in follow-up work (Li et al.,\n2023b; Abdin et al., 2023; 2024). However, their datasets and prompts are not publicly available.\nMaini et al. (2024) prompt an LM to rephrase documents for pretraining, improving training effi-\nciency. Different from all above works, our focus is teaching a pretrained LLM the knowledge of\na small corpus. Mecklenburg et al. (2024) propose a fact-based synthetic generation approach but\ndid not show improvement on generic instruction following tasks beyond simple QA. Ovadia et al.\n(2024) continually pretrain Llama 2-based language models on synthetic paraphrases of Wikipedia\narticles, but do not observe consistent performance improvements. We adapt the approach of Maini\net al. (2024) and Mecklenburg et al. (2024) to our small corpus setting as the Rephrase baseline in \u00a74.\nWe find that our graph-based augmentation algorithm outperforms it, likely because our approach\nenforces diversity through entity-based generation.\nContinual learning and pretraining. Continual learning is rooted in historical work on connec-\ntionist networks (McCloskey & Cohen, 1989; Ratcliff, 1990) and considers learning with tasks ar-\nriving in an online manner (Schlimmer & Fisher, 1986; Grossberg, 2012). The main focus is on\nmitigating a neural net's \"catastrophic forgetting\" of previously encountered tasks (Robins, 1995;\nGoodfellow et al., 2015; Kemker et al., 2018). Approaches include regularizing parameter updates\nto preserve important parameters (Nguyen et al., 2017; Zenke et al., 2017; Kirkpatrick et al., 2017);\ndynamically modifying the architecture (Rusu et al., 2016; Golkar et al., 2019); and recalling or\nreplaying previous experiences (Rebuffi et al., 2017; Shin et al., 2017; Lopez-Paz & Ranzato, 2017).\nContinual or continued pretraining works (Gururangan et al., 2020) successfully adapt pretrained\nlarge language models to broad target domains such as code (Rozi\u00e8re et al., 2024), medicine (Chen\net al., 2023), or mathematics (Lewkowycz et al., 2022; Shao et al., 2024; Azerbayev et al., 2024) by\ncurating massive datasets (often >100B tokens, shown in Table 1) and developing efficient training\nrecipes using causal language modeling (Gupta et al., 2023; Ibrahim et al., 2024; Parmar et al.,\n2024). Catastrophic forgetting is effectively mitigated by scaling parameter count (Ramasesh et al.,\n2022) and mixing in updates on pretraining data (Ouyang et al., 2022). This work aims to extend\nthe success of continued pretraining to small, specialized domains such as proprietary document"}, {"title": "2 OUR METHOD", "content": "We focus on learning parametric knowledge from a small text corpus. More specifically, our goal is\nto continually pretrain a language model to acquire the knowledge of a niche corpus of documents.\nObserving that simple continued pretraining on this source corpus is ineffective (\u00a74), we propose to\nuse synthetic continued pretraining, which first uses the small corpus to synthesize a larger one more\namenable to learning, and then continues pretraining on the synthetic corpus. In this section, we first\noutline this problem setting and our evaluation approach in more detail (\u00a72.1). Then, we provide a\nconcrete instantiation of synthetic continued pretraining using a data augmentation algorithm called\nEntiGraph (\u00a72.2)."}, {"title": "2.1 PROBLEM SETUP", "content": "Continued pretraining effectively adapts language models to various downstream domains such as\nmathematics (Lewkowycz et al., 2022; Azerbayev et al., 2024), medicine (Chen et al., 2023) or\nlaw (Colombo et al., 2024a;b), but it requires diverse, large-scale corpora with billions of tokens\n(Table 1). Given the success of continued pretraining in high-resource domains, we aim to extend it\nto niche domains with only small, specialized text corpora available.\nContinued pretraining on small corpora. We focus on approaches that use continued pretraining\nto teach a pretrained language model the knowledge of a small set of source documents $D_{\\text{source}}$.\nThese approaches acquire \u201cparametric knowledge\u201d, i.e., the knowledge of $D_{\\text{source}}$ is learned in the\nmodel's parameters much like during the pretraining process.\nSynthetic continued pretraining (synthetic CPT). We find that simple continued pretraining\nfails to acquire the knowledge in a small corpus $D_{\\text{source}}$, even with data repetition (\u00a74). We hypoth-\nesize that this failure occurs because $D_{\\text{source}}$ is highly condensed (e.g., 1.3M tokens in our experi-"}, {"title": "2.2 ENTIGRAPH", "content": "Next, we present EntiGraph, our instantiation of a synthetic data augmentation algorithm $A_{\\text{synth}}$.\nAt a high level, EntiGraph generates diverse representations of knowledge from a small corpus\n$D_{\\text{source}}$ by using a prompted LLM to synthesize a knowledge graph representation of $D_{\\text{source}}$. In\npractice, EntiGraph is an operation applied independently to each seed document $D_i \\in D_{\\text{source}}$, so\nin the following, we use D to refer to a given document. For each document D, EntiGraph uses a\nprompted language model $LM_{\\text{aug}}$ to generate a knowledge graph over the entities of the document in\nnatural language. EntiGraph consists of three steps/prompts: extracting entities from the document,\ndescribing single entities in the context of the document, and finally, analyzing relations between\narbitrary subsets of the entities. Altogether, this hierarchical prompting strategy externalizes the\nproblem of generating diverse synthetic text to a combinatorial structure\u2014namely, a graph relating\nvarious entities appearing in the corpus documents. In what follows, we provide abbreviated prompts\nto illustrate the algorithm, and defer full prompts to Appendix B.1.\nStep 1: Entity extraction. First, EntiGraph extracts a list of salient entities {$E_1, E_2, ..., E_n$}\nfrom the document D by prompting $LM_{\\text{aug}}$ with an entity extraction prompt:\n{$E_1, E_2,..., E_n$} ~ $LM_{\\text{aug}}(\\text{entity\\_extraction}(D)).                                                                                                                                                                                                                                                                            (2)\nConcretely, the entity_extraction prompt is abbreviated below:\n## System message\nAs a knowledge analyzer, identify salient entities in the given\ntext. Include: (a) Names (b) People (c) Places (d) Concepts, etc.\n## User\n* Document {document_text}\nIn the linear algebra example, D could be one specific linear algebra textbook. We would expect to\nextract entities such as {$E_1$ = Linear space, $E_2$ = Vector, $E_3$ = SVD,...}."}, {"title": "3 EXPERIMENT SETUP", "content": "In this section, we describe in detail how we evaluate a given data augmentation algorithm $A_{\\text{synth}}$. As\ndescribed in the problem setup (\u00a72.1), we evaluate such algorithms $A_{\\text{synth}}$ by evaluating whether a"}, {"title": "4 \u039c\u0391\u0399\u039d EXPERIMENTS", "content": "In this section, we present our main experimental results\u00b9. Using GPT-4\u00b2 as our prompted model\n$LM_{\\text{aug}}$, we apply EntiGraph to the 1.3M token QUALITY corpus $D_{\\text{source}}$, generating a 600M token\nsynthetic corpus. For the remainder of the paper, we refer to the former as the \"Raw corpus\" and the\nlatter as the \"EntiGraph corpus\". Additional details on these corpora are provided in Appendix C.\nWe continually pretrain Llama 3 8B (Dubey et al., 2024) with standard causal language modeling\non the 600M token EntiGraph corpus. In \u00a74.1, we describe our continued pretraining procedure\nand introduce two natural baselines. In \u00a74.2, we evaluate all methods on the QuALITY test queries\n$Q_{\\text{test}}$ and find synthetic CPT using EntiGraph significantly outperforms both baselines; moreover,\nits accuracy scales log-linearly with the amount of EntiGraph synthetic data, up to 600M tokens.\nIn \u00a74.3, we show that synthetic CPT using EntiGraph is compatible with downstream instruction\ntuning (Ouyang et al., 2022), an important feature of real pretraining data."}, {"title": "4.1 CONTINUED PRETRAINING PROCEDURE", "content": "In all experiments, we continue pretraining the Llama 3 8B Base model with a context length of\n2048 and batch size of 16. We apply a linear learning rate warmup for 5% of total steps, followed by\na cosine decay with peak learning rate 5e-6. We perform full parameter training with Fully Sharded\nData Parallelism (FSDP, Zhao et al. (2023)). To mitigate the forgetting of pretrained knowledge, we\nperform replay with a rate of 0.1 using 1B RedPajama tokens (TogetherAI, 2023). More precisely,\nfor each training batch, we flip a biased coin such that with 10% probability, we load the RedPajama\ndata instead of the EntiGraph synthetic data.\nEntiGraph CPT. In our main continued pretraining experiment, we continually pretrain Llama\n3 8B Base on the 600M token EntiGraph corpus for 2 epochs. For the remainder of the work, we\nwill refer to this continually pretrained model as \"EntiGraph CPT\u201d. Next, we describe two baselines\nwhich we compare to EntiGraph CPT in closed-book QA (\u00a74.2) and instruction-tuned summariza-\ntion (\u00a74.3) evaluations."}, {"title": "4.2 QUESTION-ANSWERING EVALUATIONS", "content": "Next, we provide the detailed setup of our closed-book QA evaluations with QuALITY test queries\n$Q_{\\text{test}}$, and present results.\nEvaluation procedure. Each QuALITY question is a four-choice, single-answer multiple choice\nquestion (similar to MMLU, Hendrycks et al. (2021)). We evaluate with 4-shot chain-of-thought\nprompting (Brown et al., 2020; Wei et al., 2024) and provide our prompt in Appendix D.1. As\nfew-shot examples, we use manually drafted and fact-checked QA pairs. To avoid leakage about\ninformation in the QuALITY books, we use books that are well-known and not contained in the\nQUALITY test set."}, {"title": "4.3 INSTRUCTION FOLLOWING EVALUATIONS", "content": "In this section, we explore more general test queries beyond the QuALITY test queries $Q_{\\text{test}}$. Con-\ncretely, we train EntiGraph Instruct by performing instruction tuning on EntiGraph CPT. We demon-\nstrate that synthetic CPT on the EntiGraph corpus is compatible with instruction tuning. In partic-\nular, EntiGraph Instruct can directly use knowledge obtained during synthetic CPT in instruction\nfollowing tasks (Wei et al., 2022), without any test-time access to the QuALITY books and articles\n$D_{\\text{source}}$."}, {"title": "5 OPEN-BOOK EXPERIMENTS", "content": "Next, we consider an open-book setting in which the domain-specific corpus $D_{\\text{source}}$ is available at\ntest time. In this widespread setting, retrieval-augmented generation (RAG; Lewis et al. (2020);\nGao et al. (2024)) is the predominant and most convenient approach. It benefits from strong tooling\n(Chase, 2022; Han et al., 2023; Pinecone, 2024), avoids finetuning, supports continual learning as\nthe corpus is updated over time (Wu et al., 2024), and has high recall (proportion of queries for\nwhich the correct documents are retrieved).\nTherefore, it is a natural question whether the parametric knowledge learned through synthetic CPT\nusing EntiGraph complements the non-parametric knowledge accessed at test time using RAG. In\nthis section, we answer this question by comparing a state-of-the-art RAG pipeline with and without\nEntigraph CPT."}, {"title": "5.1 RAG EVALUATION SETUP", "content": "Our RAG pipeline follows established best practices (Lewis et al., 2020; Gao et al., 2024). It involves\nan offline stage which indexes document chunks, followed by inference-time retrieval, reranking,\nand placement of those chunks in a few-shot LM prompt.\nDetails on RAG pipeline. More specifically, our indexing stage chunks documents from the\ngiven corpus, obtains dense vector embeddings for each chunk using an API-based embedding\nmodel, and indexes the (embedding, chunk) pairs. Then, at inference time, we embed the query\nwith the API-based embedding model, retrieve K document chunks using an approximate nearest-\nneighbor search, and lastly, select the k < K most relevant chunks using an API-based reranker.\nThroughout, we use OpenAI text-embedding-3-large (Neelakantan et al., 2022) as our API-\nbased embedding model, FAISS as our similarity search index (Douze et al., 2024), and Cohere\nrera"}, {"title": "5.2 RESULTS", "content": "Our results in the open-book RAG setting are presented in Table 4."}, {"title": "6 THEORETICAL ANALYSIS OF ENTIGRAPH SCALING", "content": "It may seem surprising that simply \u201crewriting\" the factual content of the source documents $D_{\\text{source}}$\ncan improve performance at all (\u00a74), as the EntiGraph data augmentation algorithm does not ex-\nplicitly add new factual information beyond $D_{\\text{source}}$. In this section, we build a mathematical model\nbased on a stochastic process on graphs in order to offer an explanation for this phenomenon. We\npostulate that EntiGraph does not create knowledge de novo; rather, it simply \"rearranges\" the\nknowledge of $D_{\\text{source}}$ into a layout more amenable to learning. For example, in $D_{\\text{source}}$, the en-\ntity pair (A, B) may appear together in some sentences and (B, C) in others. As a result, models\ntrained directly on $D_{\\text{source}}$ with a next-token prediction objective may learn the (A, B) relation and\nthe (B, C) relation, but not the relation between A and C (Aky\u00fcrek et al., 2024) We will build a\nmathematical model that formalizes this intuition (\u00a76.1). Based on this model, we provide a quan-\ntitative prediction that the scaling trend of EntiGraph CPT follows a mixture-of-exponential shape\n(\u00a76.3), which fits well with our empirically observed scaling trend."}, {"title": "6.1 \u03a4\u039f\u03a5 MODEL SETUP", "content": "In this toy model, we use V to denote the set of entities, and represent the source documents $D_{\\text{source}}$\nwith pairs of known relations $D_{\\text{source}} \\subset {(x, y) \\in V^2 : x \\neq y}$. We assume that each relation\npair in $V^2$ appears in the source documents $D_{\\text{source}}$ independently at random with probability p.\nMathematically, IP [(x, y) \u2208 $D_{\\text{source}}$] = p for all x \u2208 V and y \u2208 V with x \u2260 y. We write V = |V|\nand assume that $p = \\lambda/V$, for some constant $\\lambda > 1$.\nTraining as memorization. We model the learning of factual knowledge as a memorization pro-\ncess, in which a model memorizes the relations it is explicitly trained on but does not meaningfully\ngeneralize beyond them (Yang et al., 2023; Feldman, 2020). In our knowledge graph setting, a lan-\nguage model's knowledge can be represented by a matrix M \u2208 {0,1}$^{V\\times V}$ such that M(x,y) = 1\nif the model \u201cknows\u201d the (x, y) relation and equals 0 otherwise. Then, training directly on the source\ndocuments $D_{\\text{source}}$ simply means setting all entries that appear in $D_{\\text{source}}$ to 1. This denotes that the\nmodel has memorized the relations given in the source documents. Mathematically, we denote this\nmodel trained on $D_{\\text{source}}$ by the matrix $M_0 \\in {0,1}^{V\\times V}$, which has i.i.d. Bernoulli off-diagonal\nentries with mean p.\nEntiGraph synthetic data augmentation. Given the source documents $D_{\\text{source}}$, we define the\nfollowing iterative procedure of synthetic data generation: for each t = 1,2,...\n1. Entity pair selection: Sample $(x_t, y_t) \\in {(x, y) \\in V^2 : x \\neq y}$ uniformly at random.\n2. Relation analysis: Generate the \u201crelation between $(x_t, y_t)$\" by performing a breadth-first\nsearch (BFS) on the directed graph represented by the adjacency matrix $M_0$ starting at $x_t$:"}, {"title": "6.2 RIGOROUS UPPER AND LOWER BOUND", "content": "In this section, we derive rigorous upper and lower bounds on the scaling trend of Acc($M_t$). We\nshow that Acc($M_t$) as a function of t can be bounded above and below by two exponential functions\nwith different growth rates. Note that these two bounds do not necessarily imply that Acc($M_t$) itself\ngrows exponentially. We will provide a precise formula for its growth in \u00a76.3 via an approximation\nthrough a Poisson branching process.\nDefinition 1. Let $C_{\\lambda} = (1 - p(\\lambda))^2$, where p(\u03bb) denotes the extinction probability for a Poisson(\u03bb)\nbranching process (i.e., p is the smallest solution in [0, 1] to the fixed-point equation p = exp(\u03bb(p-"}, {"title": "6.3 AN ANALYTICAL FORMULA", "content": "For the remainder of the section, we analyze the link density Acc($M_t$) using a Poisson branching\nprocess approximation of the cluster growth of vertices. This approach yields an approximation of\nthe form\n$\\text{Acc}(M_t) \\sim p + C \\Big( 1 - \\sum_{l=0}^{\\infty}  \\lambda^{-l}  \\sum_{k=1}^{\\infty} p_l(k) \\big( 1 - \\frac{k}{V(V - 1)} \\big) \\Big)$\nwhere A ~ B means that A/B converges to 1 in probability as V \u2192 \u221e. We refer the reader to\nAppendix F for a comprehensive derivation. Here $p_e$ denotes the probability mass function of the\ntotal progeny $Y_e$ of a Poisson(1) branching process at level l. Qualitatively, for a general represen-\ntation of source documents $D_{\\text{source}}$ beyond directed Erd\u0151s-R\u00e9nyi graphs, we still expect to observe a\nmixture-of-exponential scaling trend:\nAcc($M_t$) ~p+C (1 \u2013 \u03a3\u03bc(k) (1 \u2013 \u03b1\u03ba)\nIn this context, the parameter C governs the link density Acc($M_t$) as t \u2192 \u221e. In our model, C is\ndetermined by the proportion of reachable pairs of vertices in the initial matrix $M_0$. Here, we are\nessentially filling out the \u201cdeductive closure\u201d (i.e., all the facts or relations that can be deduced from\n$D_{\\text{source}}$; Stine (1976); Aky\u00fcrek et al. (2024)) of the original data\u2014if some facts cannot be deduced,\nthen Acc($M_t$) cannot approach 1. The measure \u03bc(\u00b7) is the probability mass function on k, which\ncontrols the proportion of pairs of vertices with a specific decay rate. The parameters \u03bc(\u00b7) depend on\n$M_0$ in a more intricate manner. We find that the formula in (5) accurately fits the empirical scaling\ntrend of EntiGraph CPT accuracy up to 600M synthetic tokens .\nSketch of derivation. Intuitively, the edge (i, j) will eventually be added if and only if j is reach-\nable from i in the original graph $M_0$. This explains the limiting behavior of Acc($M_t$) as t ap-\nproaches infinity: the proportion of links will converge to the proportion of connected vertex pairs\nin $M_0$. To understand the mixture-of-exponential functional form, consider that at time t, the proba-\nbility of adding each vertex pair follows an exponential pattern, with different vertex pairs exhibiting\ndifferent exponential growth rates. Specifically, think of a breadth-first search in $M_0$ starting from a\nvertex i. If j is very close to the root, there are many paths from i to other vertices passing through\nj, making it more likely that (i, j) will be included in each iteration. In contrast, if j is far from the\nroot (e.g., at the end of the exploration process), there are fewer such paths, making it less likely for\n(i, j) to be included in each iteration. This accounts for the mixture-of-exponential shape, where the\nmixture primarily reflects the distance of each vertex from the root, the number of such vertices, and\ntheir corresponding exponential growth rates.\nQualitative description. Finally, to help build an intuitive understanding, we provide a qualitative\ndescription of the mixture-of-exponential shape. We demonstrate in Appendix F that this mixture-\nof-exponential shape comprises three distinct phases: a fast growth phase, a slower growth phase,\nand a plateau phase. Mathematically, we show the existence of two distinct times, 0 < t1 < t2, such\nthat\n {\\Theta (p + t), &\\text{for } 0 \\leq t \\leq t_1, \\\\\n\\text{Acc}(M_T) = & \\Theta(\\log t), & \\text{for } t_1 \\leq t \\leq t_2, \\\\\n\\Theta(1), &\\text{for } t \\geq t_2, }"}, {"title": "7 DISCUSSION", "content": "7.1 LIMITATIONS\nBecause EntiGraph synthesizes data using a prompted language model, there is a risk that it may hal-\nlucinate and fabricate non-existent relations among the entities. Although our process of generating\nsynthetic data is grounded by the source documents as in Equations (3) and (4), it is an assumption\nthat $LM_{\\text{aug}}$ is capable enough to generate faithful synthetic data when conditioned on $D_{\\text{source}}$. In our\nexperiment with QuALITY books, we manually read a few books and fact-checked a subset of the\nsynthetic data generated for those books; we did not find factually incorrect synthesized text. We\npostulate that this is because we use a sufficiently strong prompted model $LM_{\\text{aug}}$ (gpt-4-turbo).\nIf EntiGraph were applied to more challenging content like a complex research paper, it is possible\nthat the prompted model could be more prone to hallucination.\nOn the other hand, because we use a very capable prompted language model gpt-4-turbo to\ngenerate synthetic data, one might be concerned that our performance gains come from distilling\nthe prompted LM's knowledge. The closed-book results show that distillation effects alone cannot\nexplain the performance of our approach (as we exceed GPT-4's closed-book performance), but our\napproach does not yet enable bootstrapping, where we use a model to generate its own synthetic\ndata for a small target domain. We view this as exciting future work."}, {"title": "7.2 FUTURE DIRECTIONS", "content": "Continued scaling beyond real data. The large but finite body of human-written text is rapidly\nbeing consumed. Villalobos et al. (2024) predict that frontier language models will exhaust all pub-\nlic, human-generated text in 2028. As we transition from a data-rich to a data-constrained regime\n(Kaplan et al., 2020; Muennighoff et al., 2023), further scaling will require us to extract more knowl-\nedge from existing data. We demonstrated that synthetic continued pretraining with EntiGraph ef-\nfectively extracts more knowledge from small corpora, which could help us learn from proprietary\ndatasets or tail knowledge that appears only once or twice on the internet. It is an open question\nwhether synthetic data generation methods like EntiGraph could improve data efficiency more gen-\nerally on standard pretraining data and without relying upon a stronger prompted model.\nAlternatives to long-context language models. Recent work handles long user queries (e.g., 1M-\n10M+ tokens) using efficient implementations of attention (Dao et al., 2022; Liu et al., 2023; Gemini,\n2024) or alternative architectures that are sub-quadratic in the context length (Tay et al., 2022; Gu\net al., 2022; Gu & Dao, 2024; Sun et al., 2024). In settings where many queries share the same"}, {"title": "7.3 CONCLUSION", "content": "Continued pretraining with next-token prediction is remarkably effective in teaching pretrained lan-\nguage models new knowledge, but to date has only been applied successfully in broad, data-rich\ndomains with 10B-100B+ tokens. We downscale continued pretraining to small, specialized cor-\npora with ~1M tokens using synthetic continued pretraining: converting a small corpus into a large\nsynthetic one with diverse representations of knowledge, and continuing pretraining on it.\nWe instantiate this approach using EntiGraph, a knowledge graph-inspired synthetic data augmen-\ntation algorithm. Synthetic continued pretraining with EntiGraph demonstrates consistent scaling in\ndownstream closed-book QA performance up to a 600M token synthetic corpus, whereas baselines\nsuch as continued pretraining on the small corpus or synthetic paraphrases show no improvement\nor asymptote early. Moreover, the acquired parametric knowledge composes with instruction tuning\nand retrieved non-parametric knowledge in an open-book setting. Lastly, we present a simplified\nmathematical model of EntiGraph and derive a functional form for its scaling trend, which closely\nmatches our empirical trend. We hypothesize that EntiGraph's \u201cexternalization\" of the synthetic data\ngeneration process to a combinatorial structure\u2014in this case, a knowledge graph over entities\u2014is a\ngenerally useful strategy in synthesizing highly diverse data and a promising object for future study."}, {"title": "A CODEBASE", "content": "We provide the codebase for reproducing all results discussed in the paper below:\nhttps://github.com/ZitongYang/Synthetic_Continued_Pretraining.git"}, {"title": "B SYNTHETIC DATA PROMPTS", "content": "We generate two synthetic corpora in this paper: EntiGraph (Appendix B.1) and the Rephrase base-\nline (Appendix B.2). As discussed in \u00a72.1, our synthetic augmentation procedure is applied to each\ndocument D in the collection of source documents $D_{\\text{source}}$. We will focus on a single document D\nfor the remainder of this section."}, {"title": "B.1 ENTIGRAPH PROMPTS", "content": "The EntiGraph procedure is described in detail in \u00a72.2. We will recap the three steps below.\nStep 1: Entity extraction. The first step is to extract the salient entities from the document D\nusing the entity_extraction operation (Equation (2)). The complete entity-extraction\nprompt is as follows:\nAs a knowledge analyzer", "steps": "n1. Summarize the Lecture Script: Provide a concise summary of the\nlecture", "Entities": "Identify and list all significant \"nouns\" or\nentities mentioned within the script. These entities should\ninclude", "to": "n* People: Any lecturers", "Places": "Specific locations or institutions"}]}