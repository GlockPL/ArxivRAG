{"title": "DH-VTON: Deep Text-Driven Virtual Try-On via Hybrid Attention Learning", "authors": ["Jiabao Wei", "Zhiyuan Ma"], "abstract": "Virtual Try-ON (VTON) aims to synthesis specific person images dressed in given garments, which recently receives numerous attention in online shopping scenarios. Currently, the core challenges of the VTON task mainly lie in the fine-grained semantic extraction (i.e., deep semantics) of the given reference garments during depth estimation and effective texture preservation when the garments are synthesized and warped onto human body. To cope with these issues, we propose DH-VTON, a deep text-driven virtual try-on model featuring a special hybrid attention learning strategy and deep garment semantic preservation module. By standing on the shoulder of a well-built pre-trained paint-by-example (abbr. PBE) approach, we present our DH-VTON pipeline in this work. Specifically, to extract the deep semantics of the garments, we first introduce InternViT-6B as fine-grained feature learner, which can be trained to align with the large-scale intrinsic knowledge with deep text semantics (e.g., \"neckline\" or \"girdle\") to make up for the deficiency of the commonly adopted CLIP encoder. Based on this, to enhance the customized dressing abilities, we further introduce Garment-Feature ControlNet Plus (abbr. GFC+) module and propose to leverage a fresh hybrid attention strategy for training, which can adaptively integrate fine-grained characteristics of the garments into the different layers of the VTON model, so as to achieve multi-scale features preservation effects. Extensive experiments on several representative datasets demonstrate that our method outperforms previous diffusion-based and GAN-based approaches, showing competitive performance in preserving garment details and generating authentic human images.", "sections": [{"title": "I. INTRODUCTION", "content": "Image-based virtual try-on (VTON) has recently attracted significant interests in generative research community [1] with the increasing popularity of online shopping [2]\u2013[10]. Despite significant progress having been witnessed, the existing VTON models still face several critical issues. One key issue lies in that the given garments must be naturally deformed to fit the target person's pose and body shape. Based on this, the other key issue is the patterns and texture details of the deformed garments need to be fine-grained preserved.\nTo address the above two critical issues, existing image-based VTON approaches generally can be categorized into two categories: warping-based and warping-free approaches.\na) The former [8]\u2013[20] typically perform garment warping before image synthesis via GANs [21] or LDMs [22]. Early approaches primarily rely on GANs [21] as image generator and tentatively reduce the mismatch between the warped garment and the target person such as in VITON-HD [8], HR-VITON [9] and GP-VTON [17]. Afterwards, researchers have considered leveraging LDMs [22] instead of GANs [21] as image generator due to their impressive generation capabilities [10], [16]. Specifically, DCI-VTON [10] and LaDI-VTON [16] are two representative works by utilizing LDMs [22] to merge the warped garment onto the target person. However, the main disadvantage of the warping-based approaches is the artifacts produced by the garment warping process, which may be difficult to eliminate during image synthesis. Furthermore, existing garment deformation methods such as TPS [23], STN [24], and FlowNet [25] basically all lack well customized dressing abilities under giving the various postures as conditions.\nb) In contrast, another type of warping-free approaches [2]\u2013[7], [10], [16], [27], [28] usually adopt LDMs [22] as image generator because of their strong intrinsic generation capa-bilities. To avoid generating artifacts, they generally bypass garment warping and utilize an feature extractor and several cross-attention blocks to capture and transfer the textures of the given garments. For instance, CLIP [29] has emerged as a robust image encoder and is frequently employed as feature extractor in various VTON methods, including MGD [3] and PBE [27]. However, CLIP [29] is pre-trained to align with the holistic features of coarse textual captions [30], [31]. Therefore, the extracted features are usually also coarse-grained, which may lead to undesirable effects [32]. Recent methods have improved the garment feature extraction abilities by utilizing a tale of two UNet modules, such as TryOnDiffusion [28], OOTDiffusion [5], and IDM-VTON [6]. However, these methods still suffer from preserving meticulous details of garments, dampening their applications to real-world scenarios.\nDriven by the above issues, we propose DH-VTON, a deep text-driven virtual try-on model featuring a special hybrid attention learning strategy and deep garment semantic preservation module. Specifcally, inspired by the success of PBE [27], we present our DH-VTON pipeline in this work. Furthermore, for extracting the deep semantics of the garments, we are the first to introduce InternViT-6B [33] into VTON tasks as fine-grained feature learner, which can be trained to align with the large-scale intrinsic knowledge with deep textual semantics to compensate for the deficiency of the commonly adopted CLIP [29] encoder. On this basis, to enhance the customized dressing capabilities, we further design GFC+ module and propose a novel deep text-driven virtual try-on (DH-VTON) model, which integrates a special hybrid attention strategy and deep garment semantic preservation module, as depicted in Fig. 2(a). DH-VTON mainly contains two parts: a fixed-parameter PBE [27] and a trainable GFC+. The former aims to ensure high realism of generated images, while the latter aims to further enhance the customized dressing abilities."}, {"title": "A. ControlNet Architecture", "content": "Given a target person image x0, DH-VTON gradually adds noise to xo, receiving a noisy image xt, with t representing the frequency of noise addition. And given a group of conditions including noisy image xt, mask m, masked image x6, given garment image g, time steps t as well as additional control conditions (e.g., pose p and densepose d), GFC+ generates a suite of control vectors ct. Then these vectors are incorporated into the SD Middle Block and the skip-connections of PBE's UNet, consequently guiding the generation process of PBE [27]. Similar to LDM [22], DH-VTON learns a network e\u0473 to predict the noise added to the noisy image xt with:\n$\\mathcal{L}_{DH-VTON} = \\mathbb{E}_{t, x_0, \\epsilon \\sim \\mathcal{N}(0,1)}[||\\epsilon - \\epsilon_{\theta}(x_t, x_0, m, g, p, d, t)||^2]$,\nwhere t \u2208 {1,...,T} denotes the time step of the forward diffusion process, x0 is the target person image and xt is xo with the added standard Gaussian noise \u0454 ~ N(0, 1)."}, {"title": "B. Garment Feature Extraction", "content": "To make up for the deficiency of the commonly adopted CLIP [29] encoder, we are the first to introduce InternViT-6B [33] into VTON tasks as fine-grained feature learner to extract the deep semantics of the garments.\nSpecifically, InternViT-6B [33] first dynamically matches the image to the optimal aspect ratio from a set of pre-defined aspect ratios. Once the appropriate aspect ratio is determined, the image is resized to the corresponding resolution. For example, as shown in Fig. 1, an 768 \u00d7 1024 image is resized to 896 \u00d7 1344. After that, the resized image is divided into 6 tiles of 448 x 448 pixels and each tile is processed independently. In addition to these tiles, a 448 \u00d7 448 thumbnail of the entire image is also included to capture the global context for comprehending the overall features."}, {"title": "C. Hybrid Attention Learning", "content": "In order to adaptively integrate fine-grained characteristics of the garments into the different layers of the VTON model, as shown in Fig. 2(b), we propose to leverage a fresh hybrid attention strategy for training, accordingly achieving multi-scale features preservation. Here, assuming Os represents the output of self attention and Ig represents the fine-grained features from InternViT-6B [33] at corresponding positions, the output of hybrid attention Oh can be defined as follows:\n$O_h = \text{Softmax}(\\frac{QK^T}{\\sqrt{d}}V + \\lambda \text{Softmax}(\\frac{Q (K')^T}{\\sqrt{d}}V'))$"}, {"title": "III. EXPERIMENTS", "content": "a) Datasets and Metrics: Our experiments are performed on two high-resolution (768 \u00d7 1024) VTON datasets, i.e., VITON-HD [8] and DressCode [34]. And test experiments are conducted under both paired and unpaired settings. In the paired and unpaired settings, we employ FID [37] and KID [38] for realism and fidelity assessment. Furthermore, in the paired setting with available ground truth, we additionally employ LPIPS [39] and SSIM [40] to evaluate the coherence of VTON images.\nb) Baselines: For more holistic comparisons, we compare DH-VTON with the two categories of baseline models: 1) warping-based models, including VITON-HD [8], HR-VITON [9], GP-VTON [17], and DCI-VTON [10]; 2) warping-free models, including MGD [3], StableVITON [4], OOTDiffusion [5], and CAT-DM [7].\nc) Implementation Details: During the experiments, we use an end-to-end training process. All experiments are con-ducted on four NVIDIA RTX A6000 GPUs with a batch size of 2. We utilize the AdamW optimizer and set the learning rate to 3 \u00d7 10-5. Moreover, the hyper-parameter \u5165 is searched from {0.25, 0.5, 0.75, 1.0, 1.25, 1.5}."}, {"title": "B. Ablation Studies", "content": "a) Hyper-parameter : We investigate the effect of hybrid attention learning as well as the different values of the guidance scale \u5165 on VITON-HD [8]. Experimental results are presented in Fig. 4 qualitatively and Tab. I quantitatively. We can find that the optimal A value is around 1.0 on VITON-HD [8]. Meanwhile, for more complicated dress images of DressCode [34], a larger \u5165 is needed to match more complex and detailed garment features. According to this study, we consistently conduct hybrid attention learning for DH-VTON, and empirically set x = 1.0 for VITON-HD [8] and x = 1.25 for DressCode [34] in the following experiments.\nb) Effect of InternViT-6B: We conduct a series of ab-lation studies to investigate the effect of InternViT-6B [33]. Experimental results on how different feature extractors affect the performance of DH-VTON are illustrated in Fig. 5 qualitatively and Tab. III quantitatively. With the integration of InternViT-6B [33], DH-VTON obtains the most realistic and natural VTON results and has shown great progress and improvement across all metrics on VITON-HD [8].\nc) Effect of GFC+: We also investigate the effect of GFC+ on VITON-HD [8]. Experimental results are shown in Fig. 6 qualitatively and Tab. II quantitatively. GFC+ visually enhances the customized dressing abilities of preserving the textures and patterns of the given garments (e.g., the texts and graphics of t-shirts) and quantitatively improves all evaluation metrics, which consistently shows the superior of our model."}, {"title": "C. Qualitative Results", "content": "Some test results of DH-VTON compared to other VTON methods on VITON-HD [8] and DressCode [34] datasets are visually shown in Fig. 3. Compared with other methods, we can observe that our DH-VTON significantly achieves the best try-on effect for various kinds of garments. Moreover, our DH-VTON not only generates realistic images but also preserves most of the fine-grained garment details."}, {"title": "D. Quantitative Results", "content": "The quantitative comparisons between DH-VTON and other methods are minutely reported in Tab. IV. DH-VTON outperforms other methods on the majority of metrics, particularly in KID [38] and LPIPS [39], demonstrating its effectiveness in image generation quality on both paired and unpaired tasks."}, {"title": "IV. CONCLUSION", "content": "In this paper, we present DH-VTON, a deep text-driven virtual try-on model. We are the first to introduce InternViT-6B [33] into VTON tasks as fine-grained feature learner, which significantly improves the deep semantic extraction abilities. Besides, we make full use of inherent power within PBE [27] and design an additional GFC+ module to enhance the customized dressing abilities. Based on this, we further propose a fresh hybrid attention strategy to ensemble different layers of fine-grained characteristics for multi-scale features preservation. Experiments on the two representative datasets demonstrate the effectiveness and superior of our approach."}]}