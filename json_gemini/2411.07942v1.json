{"title": "Towards Low-bit Communication for Tensor Parallel LLM Inference", "authors": ["Harry Dong", "Tyler Johnson", "Minsik Cho", "Emad Soroush"], "abstract": "Tensor parallelism provides an effective way to increase server large language model (LLM) inference efficiency despite adding an additional communication cost. However, as server LLMs continue to scale in size, they will need to be distributed across more devices, magnifying the communication cost. One way to approach this problem is with quantization, but current methods for LLMs tend to avoid quantizing the features that tensor parallelism needs to communicate. Taking advantage of consistent outliers in communicated features, we introduce a quantization method that reduces communicated values on average from 16 bits to 4.2 bits while preserving nearly all of the original performance. For instance, our method maintains around 98.0% and 99.5% of Gemma 2 27B's and Llama 2 13B's original performance, respectively, averaged across all tasks we evaluated on.", "sections": [{"title": "Introduction", "content": "The use of large language models (LLMs) [6, 8, 20, 21] has ballooned in countless areas due to their impressive capabilities. Even so, with the enormous size of server LLMs, inference-time efficiency becomes a dire issue for those who own the models and those who use them. Fortunately, techniques like sequence parallelism [9] and tensor parallelism [14, 19] distribute the computational load of transformer-based LLMs [23] onto different devices. However, these methods require communication between devices, which is especially a concern when serving models, so cheaper networking would greatly cut costs. In addition, as LLMs increase in size, we need to distribute them over more devices, which further drives up communication costs.\n\nA natural idea is to try quantization, but this comes with challenges. Quantization methods for LLMs have largely focused on weights [4, 7, 10, 15, 18] or multiplication between low-bit weights and low-bit input features [1, 5, 24, 25, 27]. These methods are most useful for hardware-constrained settings, but their savings are not as relevant to tensor parallel server LLMs. In particular, current LLM quantization methods keep the output features of each attention and feedforward block at high precision (BF16 or FP16) to preserve performance. However, this is exactly what needs to be communicated in tensor parallelism. There has been work in quantized communication for distributed inference [12, 13, 16], but applications in LLMs have been limited. Consequently, the main challenge with quantization is to find a way to communicate low-bit output features from tensor parallelized attention and feedforward blocks across devices while preserving the original model's performance.\n\nThankfully, there are a couple observations that we can leverage. First, the communicated features have consistent structures. Looking at the aggregated quantization ranges for each feature across a calibration set in Figure 1 (details in Section 3), we observe that a small number of features have enormous ranges, potentially resulting in large quantization errors. Second, tensor parallelism can counteract feature quantization error. Theoretically, instead of uniformly distributed quantization"}, {"title": "2 Tensor Parallelism Synchronization", "content": "Here, we formalize the communication problem in tensor parallelism that we aim to tackle. In tensor parallelism, weights in attention and feedforward blocks are partitioned across devices such that each partition can be computed in parallel until a synchronization step aggregates all outputs together on each device. Attention blocks can be split across the head dimension, and feedforward blocks can be split across the intermediate feature dimension. Synchronization in the form of an AllReduce is necessary after the output projection in attention blocks and after the down projection in feedforward blocks. With this setup, a tensor parallelized model and the original model produce the same outputs.\n\nDefine the final linear layer (i.e., the layer immediately before synchronization) in attention blocks or feedforward blocks as $f(x) = xW + b$ for $x \\in \\mathbb{R}^{1\\times D}$, weight $W \\in \\mathbb{R}^{D \\times E}$, and bias $b \\in \\mathbb{R}^{1 \\times E}$ when on a single device. When tensor parallelized, the input and weight take the form $x^{(i)} \\in \\mathbb{R}^{1 \\times \\frac{D}{N}}$ and $W^{(i)} \\in \\mathbb{R}^{\\frac{D}{N} \\times E}$, respectively, for $N$ devices. The pre-sync point linear layer on each device is:\n\n$f^{(i)} (x^{(i)}) = x^{(i)} W^{(i)}$,\n\nand a sum of the outputs from all devices produces the same activations as the original layer:\n\n$f(x) = \\sum_{i=1}^{N} [f^{(i)} (x^{(i)})] + b$.\n\nThis operation requires communicating $f^{(i)}(x^{(i)}) \\in \\mathbb{R}^E$ for each device, which can be expensive for large batch sizes and long sequences. Therefore, we aim to find compression and decompression functions, $C^{(i)}$ and $D^{(i)}$, so that we can communicate just $C^{(i)} (f^{(i)} (x^{(i)}))$ between devices and decompress as needed. Good functions should satisfy for all $i$ and $x^{(i)}$:\n\n$f^{(i)} (x^{(i)}) \\approx D^{(i)} (C^{(i)} (f^{(i)} (x^{(i)})))$.\n\nWe choose $C^{(i)}$ and $D^{(i)}$ to be quantization and dequantization operations based on the intuition that tensor parallelism can alleviate some of the quantization error in low-bit features."}, {"title": "3 Method", "content": "Outlined in Figure 2, we choose to transfer a fraction of features in BF16 and the rest in 4-bit precision. The BF16 features are chosen based on the range from the calibration set since a larger range would result in greater quantization error.\n\nCalibration. We let each feature on each device have static and independent quantization parameters (i.e., $N E$ sets of quantization parameters per tensor parallel block). These parameters are exponential moving averages of the minimum and maximum values that are obtained for each feature on each device on a calibration set. In other words, we find quantization parameters $(m_j^{(i)}, M_j^{(i)})$ for $1 \\leq i \\leq N$ and $1 \\leq j \\leq E$ following the recursive update rules:\n\n$m_j^{(i)} = (1 - \\gamma) m_j^{(i)} + \\gamma \\min(Y_j^{(i)})$ (1)\n\n$M_j^{(i)} = (1 - \\gamma) M_j^{(i)} + \\gamma \\max(Y_j^{(i)})$ (2)\n\nfor a sequence's features to communicate $Y^{(i)} = f^{(i)} (X^{(i)}) \\in \\mathbb{R}^{S \\times E}$ and constant $\\gamma$. For the first sequence, $m_j^{(i)} = \\min(Y_j^{(i)})$ and $M_j^{(i)} = \\max(Y_j^{(i)})$. Using symmetric quantization, the range on each device is $R_j^{(i)} = 2 \\max(-m_j^{(i)}, M_j^{(i)})$. For our experiments, we use 256 random WikiText [11] sequences for calibration following update rules (1) and (2) with $\\gamma = 0.01$.\n\nSelecting High Precision Features. Looking at the aggregated quantization ranges across devices after calibration, $R_j := \\sum_{i=1}^{N} R_j^{(i)}$, in Figure 1, we see a problem: a small set of features have wide ranges which harms the quantization quality. As a solution, we select the top-k features based on $R_j$ to be communicated at higher precision, fixed across all sequences and devices. All other features are symmetrically quantized to Int4. Compared to plain quantization, the additional overhead of our method includes the pre-sync BF16 feature selection and post-sync concatenation of features."}, {"title": "4 Experiments", "content": "Evaluating on multiple LLMs and tasks, we demonstrate that our quantization method preserves nearly all of the original performance at less than 4.2 bits per value. Furthermore, we show that even at lower and higher precision quantization, we still outperform all our baselines.\n\nOur two baselines and our method use the same symmetric quantization parameters per model. For the first baseline, we quantize everything to Int4. For the second baseline, we randomly select k features with uniform probability to be kept at BF16, and everything else is quantized to Int4. For all experiments, our method and the second baseline fix $k = \\lceil E/64 \\rceil$ which brings the average bits per value to under 4.2 for both. The choice of 1/64 as the fraction of BF16 features can be substituted,"}, {"title": "5 Conclusion & Future Work", "content": "We introduced a method to quantize features for compressed synchronization in tensor parallel LLMs with very little performance degradation. Directly inspired by the consistent nature of outliers in these communicated features, our method combines Int4 and BF16 representations to compress these features to less than 4.2 bits per value. Nevertheless, there are many possibilities for future work. First, we would like to develop a system level implementation of our method to better assess the efficiency gains. Second, our method is fit for AllReduce executed as an AllGather followed by a local reduction, so it would be interesting to see how we can adapt our method to other AllReduce algorithms (e.g. ring-AllReduce). Together, our work and these directions would greatly improve server LLM inference efficiency."}]}