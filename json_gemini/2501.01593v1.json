{"title": "BLAST: A Stealthy Backdoor Leverage Attack against Cooperative Multi-Agent Deep Reinforcement Learning based Systems", "authors": ["Yinbo Yu", "Saihao Yan", "Xueyu Yin", "Jing Fang", "Jiajia Liu"], "abstract": "Recent studies have shown that cooperative multi-agent deep reinforcement learning (c-MADRL) is under the threat of backdoor attacks. Once a backdoor trigger is observed, it will perform malicious actions leading to failures or malicious goals. However, existing backdoor attacks suffer from several issues, e.g., instant trigger patterns lack stealthiness, the backdoor is trained or activated by an additional network, or all agents are backdoored. To this end, in this paper, we propose a novel Backdoor Leverage Attack againST c-MADRL, BLAST, which attacks the entire multi-agent team by embedding the backdoor only in a single agent. Firstly, we introduce adversary spatiotemporal behavior patterns as the backdoor trigger rather than manual-injected fixed visual patterns or instant status and control the period to perform malicious actions. This method can guarantee the stealthiness and practicality of BLAST. Secondly, we hack the original reward function of the backdoor agent via unilateral guidance to inject BLAST, so as to achieve the leverage attack effect that can pry open the entire multi-agent system via a single backdoor agent. We evaluate our BLAST against 3 classic c-MADRL algorithms (VDN, QMIX, and MAPPO) in 2 popular c-MADRL environments (SMAC and Pursuit), and 2 existing defense mechanisms. The experimental results demonstrate that BLAST can achieve a high attack success rate while maintaining a low clean performance variance rate.", "sections": [{"title": "I. INTRODUCTION", "content": "Cooperative multi-agent deep reinforcement learning (c-MADRL) is a significant branch of deep reinforcement learning (DRL). c-MADRL enables multiple agents to cooperate to achieve the same goal to solve complex tasks [1] and has found applications in many areas such as cooperative game [2], autonomous driving [3], computation offloading [4], and other fields [1]. However, current research [5]\u2013[11] has revealed that DRL is vulnerable to a serious threat known as backdoor attacks. This attack involves injecting malicious triggers into the training data or the DRL model. Once a DRL model is injected with a backdoor, it will behave in an unexpected or even malicious way when the trigger occurs. Since c-MADRL inherits the characteristics of DRL, it also suffers from this threat.\nAs of now, only a handful of studies [12]\u2013[15] have delved into backdoor attacks against c-MADRL. Attacks [12], [13] directly implant backdoors in all agents and trigger them multiple times to carry out the attack, as shown in Fig. 1(a).\nWhile it is rather effortless to implant backdoor attacks in all agents, this approach comes with a high injection cost and lacks sufficient concealment. Attacks [14], [15] poison a single agent to attack the entire system, but ignore the mutual influence between agents [16]. This influence is the key to achieving fast cooperation in multi-agent systems [16] and can therefore weaken or even block the effectiveness of attacks [17]. In addition, existing attack methods all use specific instant visual patterns (as shown in Fig. 1(a)) as triggers, e.g., visual modified map patterns [13], spectrum signals [12], and agent spatial distance [15]. Once the trigger is observed, they immediately execute instantaneous or continuous attack actions. However, this paired backdoor triggering and attack behavior can be easily detected by data anomaly detectors, e.g., [18], [19]. Hence, although these backdoor attacks are effective against c-MADRL, they are still far from perfect. We believe that a perfect one should possess two key characteristics: 1) good concealment and operability, and 2) minimal injection cost with a high impact.\nTo achieve these two characteristics, we propose a novel stealthy backdoor leverage attack against c-MADRL, BLAST, which only requires implanting a backdoor in a single agent to pry an entire multi-agent team into failure, as shown in Fig. 1(b). We call this effect the leverage attack effect. First, most existing c-MADRL algorithms (e.g., VDN [20], QMIX [21], COMA [22], and MAPPO [23]) use recurrent neural networks (RNN) to memorize past information and make effective decisions in combination with current observations, thereby overcoming partial observability [24]. This, however, also allows attackers to hide backdoor triggers in unobservable states [25]. Hence, to achieve the first characteristic, we distribute our BLAST's trigger across a sequence of observations over a short period rather than within a single epoch. Once BLAST is deployed, the attacker only needs to act as a moving object (e.g., a user-controlled enemy unit in StarCraft) and perform a specific sequence of actions around the BLAST agent\u00b9 in a short period following the trigger to activate the attack. Besides, we perform BLAST's attack behaviors within a controllable period after the trigger occurs, rather than immediately. This method of decoupling attack triggers and actions and distributing them in different time series can ensure that BLAST can achieve high concealment and a high attack success rate at a low poisoning rate.\nFor the second characteristic, BLAST does not require additional networks to achieve the leverage attack effect. Specifically, c-MADRL agents are commonly trained by enlarging mutual influence between agents to achieve faster cooperation [16]. Under this influence, actions taken by agents will affect each other. Based on this, we design a reward hacking method for backdoor injection, in which we introduce the unilateral influence filter [17] to only enlarge the influence of the BLAST agent on other agents, but eliminate the opposite influence. It consists of two reward items: the former encourages the BLAST agent to perform actions that have long-term malicious effects on clean agents through target failure state guidance; the latter incentivizes the BLAST agent to quickly find actions that can mislead its teammates into performing non-optimal actions. With this hacking method, BLAST can quickly attack the entire system by poisoning only one agent.\nGiven a pre-trained clean c-MADRL model, we retrain it to implant BLAST and then deploy it to the multi-agent system. Our BLAST can be applied to team competition systems (e.g., MOBA games) or collaborative control systems (e.g., connected vehicle autonomous driving), where an attacker acts as a normal object in the observation of a c-MADRL agent and performs specific actions to activate the backdoor, resulting in poor performance or failure of the c-MADRL team. In summary, our contributions are as follows:\n\u2022 We use spatiotemporal behavior patterns as triggers of our BLAST and its attack period is controllable, which enhances its concealment and operability;\n\u2022 We introduce a reward function hacking approach based on unilateral influence, allowing BLAST to be implanted in only a single agent to achieve the leverage effect of attacking the entire multi-agent system;\n\u2022 The rich experiments show the effectiveness of our proposed BLAST attack and its resistance to existing representative backdoor defense strategies."}, {"title": "II. BACKGROUND", "content": "Most of the existing popular c-MADRL algorithms adopt the centralized training and decentralized execution (CTDE) framework [27], [28]. The CTDE framework centralizes the evaluation of the joint policies of all agents during training to address the issue of environmental non-stationarity. During execution, each agent is dispersed and executed separately, relying solely on its own local observation and observation history for action selection, greatly alleviating the problem of low efficiency in policy execution. c-MADRL can be roughly divided into two categories: value function decomposition (VFD) and centralized value function (CVF).\nVFD-based c-MADRL makes a strong decomposition assumption, namely individual-global-max, which assumes that the optimal joint action obtained by the joint action value function $Q_{tot} (\\tau, a)$ of all agents is equivalent to the set of optimal actions obtained by the individual utility function $Q_i(\\tau_i, a_i)$ of each agent (where $\\tau_i$ and $a_i$ are observation history and action, respectively) as follows:\n$\\arg \\max_a Q_{tot} (\\tau, a) = \\begin{cases}  \\arg \\max_{a_1} Q_1 (\\tau_1, a_1) \\\\  \\arg \\max_{a_2} Q_2 (\\tau_2, a_2) \\\\  ... \\\\  \\arg \\max_{a_n} Q_n (\\tau_n, a_n),  \\end{cases}$ (1)\nVFD-based c-MADRL research focuses on how to make the decomposition of the joint action value function more effective and how to enhance the expression ability of the decomposition. VDN [20] is a classic VFD-based c-MADRL algorithm, which assumes that $Q_{tot} (\\tau, a)$ is the linear sum of the utility function $Q_i(\\tau_i, a_i)$ of each agent as follows:\n$Q_{tot} (\\tau, a) = \\sum_{i=1}^n Q_i(\\tau_i, a_i)$. (2)\nQMIX [21] is another VFD-based c-MADRL algorithm that considers $Q_i(\\tau_i, a_i)$ is not linearly monotonic relative to $Q_{tot}(\\tau,a)$. Therefore, it adds a mixing network containing HyperNetworks to the network structure to enhance the representation capability of $Q_{tot} (\\tau,\\alpha)$:\n$\\frac{\\partial Q_{tot} (\\tau, a)}{\\partial Q_i(\\tau_i, A_i)} \\ge 0, \\forall i \\in N$. (3)"}, {"title": "III. THREAT MODEL", "content": "In this section, we logically represent our BLAST against c-MADRL as a decentralized partially observable Markov decision process (Dec-POMDP) which consists of a tuple $(\\{N, S, O, A, T, \\{R_b, R_c\\}, \\gamma)$.\n\u2022 N := {1, ..., n} represents the set of team agents in c-MADRL. We specify the agent k to implant the backdoor, and other agents remain clean without backdoors.\n\u2022 S represents the global environmental state space. Following the CTDE paradigm [27], [28], $s_t \\in S$ is only employed during training and not during execution.\n\u2022 O := $O_1 \\times ... \\times O_n$ represents the local observations of all agents. The individual observation $o_{i,t} \\in O_i$ at time step t serves as an input for the policy network of each agent i.\n\u2022 A := $A_1 \\times ... \\times A_n$ represents the joint actions of all agents. All clean agents and the BLAST agent use $\\pi_c(a_{i,t}|o_{i,t}) : O_i \\rightarrow A_i$ and $\\pi_b(a_{k,t}|o_{k,t}) : O_i \\rightarrow A_i$ to select action, respectively, where $a_{i,t} \\in A_i$ denotes the selected action of each individual agent.\n\u2022 T : S \u00d7 A \u2192 S represents environmental state transition function. Given state $s_t \\in S$ and joint action $a_t \\in A$ at time step t, T($s_{t+1}|s_t, a_t$) computes the probability of transferring to state $s_{t+1} \\in S$ at time step t + 1. Besides, depending on T, we can use F($o_{i,t+1}|o_{i,t}, a_t$) to represent the observation transition of agent i.\n\u2022 $R_c, R_b$ : S \u00d7 A \u00d7 S \u2192 R represents the reward function for the BLAST agent and clean agents after executing a joint action $a_t \\in A$ and the state is transited from $s_t \\in S$ to $s_{t+1} \\in S$, respectively. The design of $r \\in R_b$ plays a significant role in the success of backdoor attacks.\n\u2022 \u03b3 is the temporal discount factor where 0 \u2264 \u03b3 \u2264 1.\nWe only inject BLAST in a single agent to ensure the stealthiness and practicality of backdoor attacks. The BLAST agent behaves normally like a clean agent in the absence of an attacker-specified trigger within its observation. However, once the trigger appears, it will select disruptive behavior that influences the other clean agents and ultimately leads to performance downgrade or failure of the entire team."}, {"title": "IV. THE PROPOSED BACKDOOR ATTACK METHOD", "content": "In this section, we formulate our designed spatiotemporal backdoor trigger and the reward hacking method based on unilateral influence and describe the training procedure of the BLAST model. Fig. 2 shows the framework of BLAST."}, {"title": "A. Spatiotemporal Backdoor Trigger", "content": "In many input-driven multi-agent systems, such as MOBA games [2] and autonomous driving [3], the observations of each agent typically include four types of information: its own state, teammates' state, internal environmental information, and information from external inputs. The information from external inputs is not controlled by the environment or the agents' decisions and may be exploited by malicious attackers as backdoor triggers. For example, in a MOBA game (e.g., Starcraft), an attacker can act as a user in the user team to perform a series of special actions in a short time to activate the backdoor attack. In this section, we present a spatiotemporal behavior pattern trigger that represents a set of specific spatial dependencies between the attacker's unit and the BLAST agent, as well as a set of specific temporal behaviors of the attacker's unit. We use a logical formula to represent the spatial dependencies and a set of the enemy unit's controllable actions to represent the temporal behaviors. First, given a position state $s_b$ of the unit controlled by the BLAST agent and $s_e$ of the attacker's unit, we use $\u03c8 := g(s_b, s_e) \u223c C$ to describe the spatial constraint between them at a given time step, where $g \u2208 \\{+,-, \u00d7, \u00f7\\}$ is an operator, $\u223c\u2208 \\{>, \u2265, <, \u2264, =,\u2260\\}$ is a relator, and C\u2208 R is a constant. Specifically, we define the trigger as follows:\nDefinition 1. Given consecutive position states over a short period length $N_t$ ending at time step t, a spatiotemporal behavior trigger $T := (\u03a8, \u03b6)$ can be defined using a logic formula \u03a8 representing the spatial constraints \u03a9 of these states and a set of controllable actions:\n$\u03a8 := \u03c8(i,p)|\u03c8(i,p) \\& \u03c8(j,q) |ite(\u03c8(i,p), \u03c8(j,q), \u03c8(k,s))$, (5)\n$\u03b6:= (a_{t-N_t+1}, a_{t-N_t+2},\u2026,a_t)$, (6)\nwhere $\u03c8(i,p), \u03c8(j,q), \u03a8(k,s) \u2208 \u03a9$, i,j,k \u2208 N are time steps, and p,q, s are the position features of attacker's unit relative to BLAST agent; $\u2297 \u2208 \\{V,^\\}$ is a Boolean operator (i.e., \"or\" or \"and\"); ite represents \u03a8-assignment, e.g., \u03a8 $ite(\u03c8_1,\u03c8_2,\u03c8_3)$ means if $\u03c8_1$ is true, \u03a8 := $\u03c8_2$; otherwise \u03a8 := $\u03c8_3$; a is an action executed by the attacker's unit."}, {"title": "B. Reward Hacking based Unilateral Influence", "content": "In c-MADRL systems, all agents typically interact with each other, thus there is a mutual influence relationship between agents. Due to this mutual influence, the sphere of influence of reward hacking methods in the existing backdoor attacks [5]\u2013[8], [10], [11] may be limited only to the backdoor agent itself rather than the entire system during the attack period. Therefore, we design a new reward hacking method based on the unilateral influence filter [17] which can eliminate the detrimental influence from other agents to the backdoor agent and only enable the influence from the backdoor agent to other agents.\nIntuitively, the reason why agents fail or malfunction is often because they have entered or approached bad or failed states. Therefore, we expect the BLAST agent to exhibit malicious actions during attacks to induce other clean agents closer to their bad or failed states, leading to the failure of team tasks. Since the state information of an agent at a given time step is contained in its corresponding observation vector, our specific goal is to reduce the distance between all clean agents' observations at the next time step and their corresponding target failure observations as follows:\n$\\min_{\\pi_b} \\sum_{i=1,i\\neq k}^n d_1(\\hat{o}_{i,t+1}, o_{fail}^i), \\\\ s.t. \\hat{o}_{i,t+1} = F(o_{i,t}, \\hat{a}_{k,t}, a_{-k,t}), \\\\ a_{k,t}\u223c\u03c0_b(o_{k,t}), \\\\ a_{-k,t}\u223c\u03c0_c(o_{-k,t})$, (7)\nwhere $o_{fail}^i$ represents the target failure observation of the agent i; $\\hat{o}_{i,t+1}$ represents the observation of agent i at time step t + 1 if the BLAST agent k takes a malicious action $\\hat{a}_{k,t}$ at time step t; $o_{i,t}$ is the observation of agent i at t; $a_{-k,t}$ represents the joint actions taken by all clean agents except the BLAST agent k at t; F(\u00b7|\u00b7) represents the observation transition function which depends on environmental state transition function T(\u00b7|\u00b7); $\u03c0_c(\u00b7)$ is the policy of clean agents and $\u03c0_b(\u00b7)$ is the BLAST agent's policy that needs to be trained; $d_1(\u00b7,\u00b7)$ is a distance metric function that we use $l^2$ norm.\nWe adopt a data-driven approach to failure observation learning, i.e., mining failure observations from the collected dataset. Specifically, we make all agents interact with the environment using policy $\u03c0_c$, and store the observation transfer tuple ($o_t$, $a_t$, $o_{t+1}$, $R_t$) at each time step into the trajectory dataset D, where $o_t$ = ($o_{1,t}$, ..., $o_{i,t}$,..., $o_{n,t}$), $a_t$ = ($a_{1,t}$, ..., $a_{i,t}$, ..., $a_{n,t}$), $o_{t+1}$ = ($o_{1,t+1}$, ..., $o_{i,t+1}$, ..., $o_{n,t+1}$), and $R_t$ is the team reward. Alternatively, to explore more states, we also make the agents use stochastic policy to interact with the environment and store transfer tuples into D. We then sort the collected transfer tuples by ascending order of $R_t$ and select the target failure observations $o_{fail}$ to be $o_{t+1}$ which corresponds to the lowest $R_t$ in D. The procedure is described in Algorithm 1. After determining $o_{fail}^i$, we set the hacked reward function based on the target failure state as follows:\n$r^{FS}_t = \\sum_{i=1,i\\neq k}^n d_1(o_{i,t+1}, o_{fail}^i)$. (8)\nTo enhance the effectiveness of BLAST, we further consider from the perspective of action deviation, where the BLAST agent performs a malicious action causing the clean agents' actions at the next time step to deviate from the original optimal actions. We set the hacked reward term based on action deviation as follows:\n$r^{AD} = \\sum_{i=1,i\\neq k}^n d_2 (a_{i,t+1}, \\hat{a}_{i,t+1}) \\\\ = \\sum_{i=1,i\\neq k}^n d_2(\\pi_c (\\hat{o}_{i,t+1}), \\pi_c(o_{i,t+1})) \\\\ = \\sum_{i=1,i\\neq k}^n d_2(\\pi_c (F(o_{it}, a_{k,t}, a_{-k,t})), \\pi_c(F(o_{i,t}, \\hat{a}_{k,t}, a_{-k,t})))$. (9)\nSince $a_{i,t+1}$ depends only on $\u03c0_c(o_{i,t+1})$ and the policy $\u03c0_c(\u00b7)$ of the clean agents is frozen during our backdoor injection process. Hence, we can only induce the deviation of the action $a_{i,t+1}$ by changing $o_{i,t+1}$. Depending on F(\u00b7), the BLAST agent can select action $\\hat{a}_{k,t}$\u223c $\u03c0_b(o_{k,t})$ different from the action $a_{k,t}$\u223c $\u03c0_c(o_{k.t})$ to change the observation $o_{i,t+1}$ into $\\hat{o}_{i,t+1}$. Besides, $d_2(\u00b7,\u00b7)$ is a distance metric function with $d_2 (a_{i,t+1},a_{i,t+1})$ = 0 if $a_{i,t+1}$ = $a_{i,t+1}$ and $d(a_{i,t+1}, a_{i,t+1})$ = 1 otherwise.\nTo calculate $r^{AD}$, if the current time step t is in the attack period, we record and save the global state $s_t$ and the local observation $o_{i,t}$ of all clean agents. First, we let the BLAST agent k select action $a_{k,t}$\u223c $\u03c0_c(o_{k,t})$ and each clean agent i select action $a_{i,t}$\u223c $\u03c0_c(o_{i,t})$ to execute. The environment state will be transferred to $s_{t+1}$. We then can obtain $o_{i,t+1}$ and $a_{i,t+1}$\u223c $\u03c0_c(o_{i,t+1})$ according to $s_{t+1}$. Next, we roll back the environment simulator to the state $s_t$, and let agent k reselect action $\\hat{a}_{k,t}$\u223c $\u03c0_b(o_{k,t})$ and each clean agent i still use action $a_{i,t}$ to execute. The environment will enter a new state $\\hat{s}_{t+1}$ and we can get $\\hat{o}_{i,t+1}$, and then $\\hat{a}_{i,t+1}$. Note that since the model of agents contains RNNs, we also need to roll back the hidden layer states of agents. After getting all $\\hat{a}_{i,t+1}$ and $a_{i,t+1}$, we use the Equ. (9) to calculate $r^{AD}$.\nAfter calculating $r^{FS}_t$ and $r^{AD}$, we normalize them into the same range of values as the original reward $R_t$ to reduce the damage to the clean performance of the BLAST agent."}, {"title": "C. BLAST Model Training", "content": "To inject BLAST only in a single agent, we assume that all agent models have been trained well in the clean c-MADRL environment, and during backdoor injection, we only retrain a single model and leave others frozen. Our complete backdoor injection procedure is outlined in Algorithm 2.\nFirstly, the attacker specifies a spatiotemporal behavior trigger T := (\u03a8,\u03b6) with a trigger period $N_t$. Besides, an attack period L is introduced to attack only L time steps after the trigger appears completely, which can enhance the stealthiness of the backdoor attacks. In each training episode, the attacker determines whether to poison with the poisoning rate p. If poison, the attacker will insert the backdoor trigger by combining target failure states guidance and action deviation, we hack the reward of the BLAST agent during the attack period as follows:\n$r_t = (1 \u2212 \u03bb) \u00b7 r_t^{FS} + \u03bb \u00b7 r_t^{AD}$, (10)\nwhere \u03bb represents a hyperparameter that balances the tradeoff between the long-term and the short-term malicious unilateral influence on the clean teammates."}, {"title": "V. EVALUATION", "content": "In this section, we evaluate the effectiveness, stealth, and persistence of our proposed BLAST attack against c-MADRL. Besides, we perform ablation studies to demonstrate the importance and rationality of our design."}, {"title": "A. Experimental Settings", "content": "1) Environments: We use StarCraft Multi-Agent Challenge (SMAC) [2] and Pursuit [30] in pettingzoo [31] as experimental environments, as shown in Fig. 3: (a) SMAC is an environment of two teams against each other where all allied units (i.e., the red side) are controlled by c-MADRL agents, while enemy units (i.e., the blue side) are controlled by computer. If the enemies' health decreases, the agent team will receive corresponding positive rewards; If an enemy unit dies, the agent team will receive a reward of 10; If all enemy units die (i.e. win), the agent team will receive a reward of 200; If all allied units die (i.e. fail), the agent team will receive a reward of 0. We choose 8m, 3m, and 2s3z as our test maps, with 8, 3, and 5 agents in the agent team, respectively. (b) In the Pursuit environment, the pursuers are controlled by agents. Every time the agents fully surround an evader, each of the surrounding agents receives a reward of 5, and the evader is removed from the environment. Agents also receive a reward of 0.01 every time they touch an evader. We set the number of agents to 8 and the number of evaders to 10. Each agent receives a fixed penalty of -0.01 per time step, which means that the longer it takes to catch the same number of evaders, the lower the team reward.\n2) Algorithms and Network Structure: We pick VDN, QMIX, and MAPPO as the attacked algorithms, where the first two are based on VFD and the latter is based on CVF. For the SMAC environment, the policy networks of c-MADRL agents trained by these three algorithms adopt the same structure with a GRU layer as follows: |O|\u2192 64 \u2192 64 \u21c4 64 \u219264 \u2192 |A|,"}, {"title": "B. Attack Results in SMAC", "content": "Fig. 4 illustrates how the average episode rewards and the average winning rate change during the training of the BLAST models attacking VDN, QMIX, and MAPPO. In Fig. 4(a)-(f), we can see that the average episode rewards and the winning rates in poisoned episodes which are trigger-embedded decrease with the number of training steps. Affected by backdoor learning, the episode rewards and the winning rates in clean episodes which are trigger-free first decrease, and as learning progresses, they will rise again to levels similar to before training, as shown in Fig. 4(g)-(1). After the BLAST model training process is completed, more specific evaluation metrics are shown in Table 2. We can see that metrics $bER_{tg}$ and $bWR_{tg}$ are significantly lower compared to metrics $cER$ and $cWR$, while metrics $bER$ and $bWR$ are only slightly lower compared to metrics $cER$ and $cWR$. In other words, metrics AER and ASR are very high while metrics CPVR and WRVR are maintained at a very low level. In the attacks against multiple algorithms in multiple maps, the AER and ASR can be as high as 88.5% and 100.0%, respectively, while the CPVR and WRVR can be as low as 1.5% and 1.9%, respectively, which proves the effectiveness, stealthiness, and universality of our backdoor attacks. In addition, we can see that the performance of the attacks against VDN and QMIX is better than that against MAPPO, which indicates that MAPPO is a bit more robust against our attacks.\nWe further analyze the behaviors of the BLAST agent and clean agents during the attack period to explore how the BLAST agent affects the clean agents and thus leads to the team's failure. Fig. 5(a) and 5(b) show the snapshots of episodes of VDN agents in 8m and 2s3z maps, respectively. Firstly, we control an enemy unit to execute the trigger behavior. Once the trigger behavior is fully executed, the backdoor in the BLAST agent is triggered, as shown in Fig. 5(a)-(a) and 5(b)-(a). Then, the BLAST agent begins to perform malicious actions, specifically leaving the main battlefield and moving within the sight of its teammates, successfully inducing some teammates to move to unfavorable positions, resulting in other teammates being attacked by enemy units, as shown in Fig. 5(a)-(b-c) and 5(b)-(b-c). In the end, all teammates of the BLAST agent die, followed by the death of the BLAST agent itself, and the agent team fails, as shown in Fig. 5(a)-(d) and 5(b)-(d).\nMeanwhile, we analyze the difference in the joint actions distribution of all clean agents in a clean and a poisoned episode, as shown in Fig. 6. We can notice that in the clean episode, the agents show concentrated fire behavior, quickly killing the enemy units one by one. In the end, there are still multiple agents surviving. In the poisoned episode, before the attack begins, the clean agents' action distribution is the same as in the clean episode. During the attack period, the clean agents are affected by the BLAST agent and begin to exhibit abnormal behavior, including no longer performing concentrated fire, more dispersed action distribution, reduced attack actions, and increased movement actions (as shown in the blue boxes in Fig. 6). These changes lead to the deaths of all agents and the agent team's failure."}, {"title": "C. Attack Results in Pursuit", "content": "To verify the universality of our BLAST attacks, we conduct BLAST in the Pursuit environment [30]. Fig. 7 shows the training process of BLAST models of VDN and QMIX. We can see that the average episode reward in poisoned episodes gradually decreases as training progresses, while the average episode reward in clean episodes eventually converges to the original level. This indicates that our BLAST backdoor attacks are effective.\nWe further analyze the time-step reward of each agent in clean and poisoned episodes under the same random seed, as shown in Fig. 8. Note that the Pursuit environment supports viewing the individual reward value of each agent and the team reward is the average of the reward values of each agent. An increase in the reward of a certain agent indicates that it participates in capturing an evader and successfully captures it. It is not difficult to observe that the rewards of clean agents are not affected before the BLAST attack begins. In the poisoned episode, the reward values of all clean agents during the attack period decrease compared to the clean episode. This indicates that during the attack period, these agents do not collaborate well to capture evaders, that is, the BLAST agent plays a unilateral malicious role. After the attack period, we can find that most agents do not quickly experience an increase in rewards. This is because due to the lag effect of the BLAST attack (i.e., the BLAST agent guides some of the clean agents to the unfavorable position), the agents cannot immediately capture the remaining evaders, but need some time to do so. Once the BLAST attack is performed successfully, the agent team will spend more time capturing all the evaders."}, {"title": "D. Resistance to Backdoor Defense Methods", "content": "To further validate the stealthiness of our backdoors, we evaluate whether existing backdoor defense methods can detect our backdoors in VDN. We use two popular defense methods (activation clustering [18] and spectral signature [19]) to conduct this evaluation. We use the BLAST model to interact with the 8m environment and sample 28000 clean samples and 950 poisonous samples according to action types, dividing them into six sets (including 5 movement actions and 1 attack action). Following activation clustering, we extract the activations in the penultimate layer of the model and use principal component analysis to downscale the activations to 3 dimensions and then cluster them using k-means with k = 2. The clustering analysis results are shown in Fig. 9, in which we can discover the activations of poisonous samples are not distinguishable from the clean ones, and the prediction results are basically incorrect. Besides, the detection results of spectral signature are shown in Figure 9, where we can find that the distribution of outlier scores for the poisonous samples is roughly the same as that for the clean samples, with no significant deviation.\nBased on the above results, we can find that our BLAST attack is sustainable against existing advanced defenses. The key reason for the detection failure is that our attack has a lag relative to the trigger, that is, the attack occurs after the trigger appears rather than immediately. This is different from the backdoor attacks in existing supervised learning and the instant backdoor attacks in DRL, whose triggers and attacks appear in pairs. One possible mechanism for detecting our kind of backdoor attacks is to identify anomalies in the temporal and spatial features of sequence data. This is still an open challenge."}, {"title": "E. Ablation Studies", "content": "Considering the impact of different parameters on the BLAST attack effect, we conduct ablation evaluations on the selection of different \u03bb in hacked reward and poisoning rate.\n1) \u03bb in Hacked Reward: We further evaluate the performance of our BLAST backdoor attacks against VDN with different values of \u03bb\u2208 {0, 0.25, 0.50, 0.75,1} in hacked reward function, as shown in Fig 11 (a). We can find that when \u03bb = 0, the ASR can reach up to 84.4%. This indicates that including only the first term in the hacked reward, i.e., target failure state guiding is useful to some extent against c-MADRL. However, when \u03bb = 1, the ASR is only 16.4%, although it can maintain a very low CPVR which is 1.1%. This suggests that the attack is not very effective when considering only the second term in the hacked reward, i.e., the effect of the BLAST agent on the next time-step actions of its teammates. Attacks perform best when \u03bb = 0.5, being able to maintain a 1.6% CPVR and achieve a 96.7% ASR. This further demonstrates the effectiveness of our hacked reward for backdoor attacks against c-MADRL.\n2) Poisoning Rate pr: We further evaluate the performance of our BLAST backdoor attacks with different values of poisoning rate pr \u2208 {2%, 5%, 10%, 20%}, as shown in Fig 11 (b). We can observe that as the poisoning rate increases, ASR continues to rise, but CPVR also continues to increase. When the poisoning rate is 5%, ASR and CPVR are a good balance point, which can make the attack effective while ensuring good concealment."}, {"title": "VI. RELATED WORK", "content": "Most existing studies on backdoor attacks focuse on DNN [33", "5": "propose a backdoor attack method against DRL with image patch triggers. During training, when the model's input contains a trigger and the output is the target action or a random one, they maximize the corresponding reward value to achieve a targeted or untargeted attack. Yang et al. [6", "29": ".", "8": "use in-distribution triggers and multitask learning to train a backdoor agent. An in-distribution trigger is an observation that is not anomalous to the environment and is therefore more difficult to detect. Wang et"}]}