{"title": "Learning to Cooperate with Humans using Generative Agents", "authors": ["Yancheng Liang", "Daphne Chen", "Abhishek Gupta", "Simon S. Du", "Natasha Jaques"], "abstract": "Training agents that can coordinate zero-shot with humans is a key mission in multi-agent reinforcement learning (MARL). Current algorithms focus on training simulated human partner policies which are then used to train a Cooperator agent. The simulated human is produced either through behavior cloning over a dataset of human cooperation behavior, or by using MARL to create a population of simulated agents. However, these approaches often struggle to produce a Cooperator that can coordinate well with real humans, since the simulated humans fail to cover the diverse strategies and styles employed by people in the real world. We show learning a generative model of human partners can effectively address this issue. Our model learns a latent variable representation of the human that can be regarded as encoding the human's unique strategy, intention, experience, or style. This generative model can be flexibly trained from any (human or neural policy) agent interaction data. By sampling from the latent space, we can use the generative model to produce different partners to train Cooperator agents. We evaluate our method-Generative Agent Modeling for Multi-agent Adaptation (GAMMA)\u2014on Overcooked, a challenging cooperative cooking game that has become a standard benchmark for zero-shot coordination. We conduct an evaluation with real human teammates, and the results show that GAMMA consistently improves performance, whether the generative model is trained on simulated populations or human datasets. Further, we propose a method for posterior sampling from the generative model that is biased towards the human data, enabling us to efficiently improve performance with only a small amount of expensive human interaction data.", "sections": [{"title": "1 Introduction", "content": "Producing agents that can cooperate well with unseen partners such as humans [28] is an important problem for a variety of multi-agent systems across domains like robotics or software agents. While being cooperative is a notable characteristic of human intelligence [7, 32], training an artificial agent that cooperates well with humans poses a significant challenge. Human behaviors are uncertain and diverse, encompassing a wide range of preferences, abilities, and intentions. While humans can rapidly adapt to different partners, AI agents are particularly poor at generalizing to working with a novel human partner. Solving this issue of distribution shift between the human player's strategy and those seen during the training of an AI agent is crucial to achieving human-AI cooperation.\nIt is tempting to tackle this problem of learning to cooperate with humans using only data of human-human interactions [1]. While this may certainly provide some leverage, in many situations the amount of human-human data available is far less than the amount of data required by data-hungry"}, {"title": "2 Related Work", "content": "Training against simulated populations. Building cooperative agents that generalize well to humans and novel partners is a long-standing problem of AI, and is known as ad-hoc team-play [28], or zero-shot coordination [11]. Recently, FCP [29] adopted the idea of using a population of policies to train a strong zero-shot Cooperator agent, as a diverse population effectively prevents the Cooperator from exploiting one specific convention. Following this framework, many techniques [2, 37] have been proposed to improve the diversity of the population, such as Maximum Entropy Population (MEP)[40]. Reward shaping [30, 37] and quality diversity [22, 31, 35] utilize domain knowledge to create agents with diverse behaviors. Statistical diversity based on trajectory distributions [19] or population entropy [40] are straightforward optimization objectives. However, as indicated in [2], agents with different behavior distributions do not necessarily use different high-level strategies. To address this, the LIPO algorithm [2] instead tries to minimize the cross-play reward between different agents in the population to encourage them to form incompatible conventions. However, because LIPO agents can potentially sabotage the game, follow-up works [3, 24] propose strategies for preventing this, including Cross-play Optimized, Mixed-play Enforced Diversity (CoMeDi) [24]. While this extensive literature presents different population creation methods, in contrast we propose a novel approach to modeling this population with a generative model, and thus benefit the training process through the ability to flexibly sample unlimited new partners. Our experiments directly compare to FCP [29], CoMeDi [24], and MEP [40], and show that GAMMA can enhance performance above each of these population-generation techniques, even using the same underlying population of simulated agents, while also allowing for the incorporation of human data.\nTraining with human data. Another line of work has focused on collecting and training on human cooperation data [1]. Learning from human data is inherently challenging due to limited scale, arbitrary human behavior, subjective preferences, and relative speed of human actions compared to agents. Another challenge of coordination comes from the heterogeneity, uncertainty and suboptimal-"}, {"title": "3 Problem Formulation", "content": "A multi-agent system is typically modeled as a Markov game [17]. In this work, we focus on two-player Markov games, but the formulation and our methods can be easily extended to more agents. The state space is S. At each step, two agents execute their actions $a_t \\in A$, $b_t \\in B$ from their policies $ \\pi : S \\rightarrow \\triangle(A), \\mu : S \\rightarrow \\triangle(B)$. They then receive a shared reward $r_t : S \\times A \\times B \\rightarrow R$. The next state is determined by a transition function $T : S \\times A \\times B \\rightarrow \\triangle(S)$. The value function $V(\\pi, \\mu)$ of two policies is defined as the expectation of discounted cumulative reward $\\sum_t \\gamma^t r_t$.\nUnder this formulation of Markov games, the learning objective for the fully cooperative joint policy $max_{\\pi,\\mu} V(\\pi, \\mu)$ can be perfectly defined. However, it is unclear how to describe the learning objective of human-AI cooperation under the framework of Markov games.\nInspired by the latent Markov Decision Process [13], we model the problem of cooperating with humans as an MDP with latent variable $z$. We assume the human policy is conditioned on a latent variable $z \\in Z$. This latent variable $z$ can be regarded as a representation of the partner's unique style, skill level, reaction speed, etc. Then the human population can be defined as a distribution $D(Z)$ over the latent space. Given the human policy $ \\mu : S \\times Z \\rightarrow \\triangle(B)$ conditioning on the latent variable z, the conditional transition function $T_z(s' | s,a) = \\sum_{b\\in B} \\mu_z(b | s)T(s' | s,a,b)$ and the reward function $r_z(s, a) = \\sum_{b\\epsilon B} \\mu_z(b | s)r(s, a, b)$ can both be defined. Now with the value function for MDP $M_z = {T_z,r_z}$ is $V_z(\\pi) = V(\\pi, \\mu_z)$, the learning objective can be defined as $ \\pi^* \\in arg max E_{z~D(z)} [V_z(\\pi)]$"}, {"title": "4 GAMMA: Generative Agent Modeling for Multi-agent Adaptation", "content": "We will leverage generative models as our key tool for strategy diversification, to overcome the dual challenges of 1) lack of real human data and 2) the difficulty of synthetically covering the large strategy space of human partners. In being able to generate diverse strategy profiles beyond the data, these generative models can be used to train a Cooperator that can be deployed to coordinate with novel users, each with their own diverse styles, preferences and capabilities. Here, we first describe our procedure for learning a generative model from training data of discrete agents (or human-human data), and then describe how this can be used for targeted coordination with real human partners. We call our approach Generative Agent Modeling for Multi-agent Adaptation (GAMMA).\n4.1 Learning Generative Models of Partner Behavior\nThe first step of GAMMA is learning the generative model from pre-existing coordination data. We assume access to a dataset of trajectories $D_{coordination} = {\\tau}^N_{i=1}$, where each trajectory $ \\tau = {(s_t, a_t, b_t)}^T_{t=0}$ is a sequence of multi-agent coordination behaviors. This dataset can be derived from human playing records. Alternatively, if there is a population of simulated agents available, this dataset can be collected by pairing them together to generate joint trajectories. For instance, in our experimental evaluation, we use agents generated through techniques like fictitious co-play (FCP) [29] to generate this dataset. The purpose of generative modeling here is to be able to model the multimodal marginal distribution over various strategy profiles in $D_{coordination}$, a challenging distribution to model with standard maximum likelihood methods. Notably, the generative model is able to sample a landscape of agents that can be used to train a Cooperator, going beyond the quantity and diversity of the training data.\nWe develop a variant of the Variational Autoencoder (VAE) [12] to model the diverse strategies underlying the training data D. As is typical in variational inference frameworks, we propose to learn an encoder-decoder generative model with an approximate posterior (encoder) $q(z | \\tau; \\phi)$ that identifies the agent style from the trajectory. The decoder in this model, $p(a_t | z, \\tau_{0..t-1}; \\theta)$ uses the agent's own past experience $ \\tau_{0..t-1} = (s_0, A_0, ..., s_{t-1}, A_{t-1},s_t)$ and the latent variable $z$ to predict the agent's next action. This structure of the variational inference model can be thought of as multi-modal behavior cloning, where the encoder history provides the latent variable required to model the distribution of generated actions. The encoder-decoder architecture described above, and shown in Figure 2 can be trained using an evidence lower bound (ELBO) loss:\n$L(\\theta,\\phi) = E_{\\tau~D} E_{z~q(\\cdot, \\tau,\\phi)} [\\sum^T_{t=1} log p(a_t | z, \\tau_{0..t-1}; \\theta) + \\beta KL(q(z | \\tau, \\phi) || N(0, I)) ]$ (1)\nImportantly, this generative model is able to generate behaviors that go far beyond the training data, both in quantity and diversity, as is shown in Figure 1. We show in Fig 1 that while simulated behavior may not cover the space by itself, the generative model has significantly greater coverage over the strategy space. Importantly, we hypothesize that human behavior is more likely to lie within the span of strategies generated by the generative model. In this way, the interpolation and generalization of the generative model (even if it is not perfect) provides the expansion of the data required to effectively coordinate with humans.\n4.2 Training a Cooperator with Generative Coordination Models\nOnce a generative model has been obtained, it can be used to train a robust Cooperator agent. This is accomplished by treating the generative model as a partner generator, using it to simulate partner behavior that covers the space of real human behaviors, and training the Cooperator to optimize performance with these partners in simulation. In particular, the generative agent model $p(a_t | z, \\tau_{0..t-1};\\theta)$ can now be used as a generator of partner policies $ \\mu_z$ to train our Cooperator agent. As shown in Figure 2, for every episode we can sample latent variables from the prior $z ~ p(z)$, and use the conditioned action distribution $p(a_t | z, \\tau_{0..t-1}; \\theta)$ as a partner $ \\mu_z$ for that episode. We aim to leverage these sampled partners from the generative model to train a single Cooperator $ \\pi_\\theta$, treating the sampled partners $ \\mu_z$ as part of the environment. At each iteration, a batch of agents ${z_i}^N_{i=1}$ are generated using latent variable samples $z_i ~ D_z$. Then a batch of MDPs {$M_i$}^N_{i=1} can be derived from these training partner policies to learn our Cooperator policy $ \\pi_\\theta$. Then we use PPO [25] to optimize $ \\pi_\\theta$ over these training MDPs:"}, {"title": "5 Experiments", "content": "We evaluate GAMMA using the Overcooked environment [1] as a popular benchmark for prior work on human-AI cooperation [1, 24, 29, 36, 37]. In Overcooked, two players need to cooperate to divide the work of cooking and avoid getting in each other's way. This involves anticipating the intended goal and actions of the other player, and inferring which task would most usefully assist them. The layouts proposed in previous work are shown as the first five environments in Figure 3. The Counter Circuit layout poses a hard coordination challenge with multiple strategies such as passing items across the countertop to the partner or moving clockwise/counterclockwise around the ring. Additionally, we include a custom layout based on Counter Circuit, termed Multi-strategy Counter. This new layout involves recipes with multiple ingredients, significantly increasing the complexity of the strategy space. Failing to infer the intentions of the partners and adding the wrong ingredients to the pot could ruin the entire dish, which increases the importance of coordination.\nOur experiments investigate the following questions:\nH1: Using simulated agents. Will training a Cooperator against a generative model of partner strategies trained on a population of simulated agents outperform training the Cooperator against the simulated agents directly?\nH2: Using real human data. Can the generative model be used to effectively leverage (small amounts of) human data, and also combine it with simulated agents?"}, {"title": "4.3 Targeted GAMMA using Human-Adaptive Sampling and Fine-tuning", "content": "While the Cooperator described in Section 4.2 is trained by sampling partner agents from the generative model as $z ~ p(z), a_t ~ p(a_t | z, \\tau_{0..t-1}; \\theta)$, this does not make use of human specific data if available, instead simply treating any human and synthetic data as equivalent. However, when coordinating with real human partners, it is useful to target model adaptation to be human-specific rather than to span the entire space of potentially irrelevant synthetic strategies (as shown in Figure 1). The key insight we will leverage to do so is that the latent space afforded by our generative model provides the ability to do controllable sampling from any latent distribution. This distribution can be chosen to incorporate additional information about the desired population. In particular, when coordinating with real human partners, the latent prior distribution for sampling $p(z)$ can be replaced with a human-centric one $p_h(z)$, which is more focused on the part of the latent space relevant to human behavior. This suggests that given a small amount of human data $D_h$, a human-centered latent distribution $p_h(z)$ can be quickly inferred by encoding the human data and estimating the latent Gaussian distribution that best explains encoded human data. The latent Gaussian mean is given by:\n$ \\bar{z} = E_{\\tau_h \\in D_h} [E_{z~q(\\cdot|\\tau_h)}[z]]$. (3)\nGiven this human-centered latent distribution $p_h(z) = N(\\bar{z}, I)$, a targeted Cooperator that is meant for a particular target population can be trained by maximizing $J(\\pi_c) = E_{z\\sim p_h(z)} [V(\\pi, \\mu_z)]$.\nHuman-adaptive sampling makes the model focus less on adaptation to irrelevant synthetic partners and more on \"human-centric\" partners sampled from the generative model. As our experiments will show, this approach outperforms training a partner simulator using only human data, since with limited human data it is easy for the model to go out of distribution during generation and it can thus be brittle. In contrast, GAMMA is able to train a robust partner simulator using large amounts of synthetic data. Adding human-adaptive sampling to GAMMA can provide considerable generalization benefits even with modest amounts of data. Note that to better capture human data, we do not just target the latent space, but also perform some fine-tuning on the encoder and decoder of the VAE using human data."}, {"title": "6 Results", "content": "6.1 Evaluation in Simulation\nIn this section, we present the evaluation results against a human proxy (behavior cloning) agent.\nH1: Using Simulated Data. We train the generative models FCP+GAMMA, CoMeDi+GAMMA and MEP+GAMMA on the simulated agent population of FCP, CoMeDi and MEP, respectively. Figure 4 shows the learning curves of each method averaged on all layouts, and the final performance of different methods for each of the different layouts. The Cooperator agent is evaluated by the zero-shot cooperation performance with a human proxy BC model. GAMMA consistently improves performance over the baselines, across layouts. This demonstrates that GAMMA provides a more ef- ficient way to utilize the simulated agents by providing a landscape of partners to train the Cooperator. We also find the improvement gap increases as the layouts become more complex.\n6.2 Evaluation with Real, Novel Human Partners\nAs described in Section 5, we conduct an evaluation with real human players, recruiting new participants that were not in the training data. We evaluate all agents against the novel human players, and plot the cooperative scores achieved by each agent-human team in Table 3.\n6.2.1 H1: Training with Simulated Data.\nWe find that GAMMA offers significant improvements over prior techniques for training against simulated populations (FCP, CoMeDi, MEP). Although the trend of these results is consistent with the previous results for H1, we find that here GAMMA provides significantly enhanced performance improvements when tested with real humans, reaching the new state-of-the-art performance for both Counter Circuit and Multi-Strategy Counter.\nOn our newly proposed, more complex layout, Multi-Strategy Counter, we find that the CoMeDi baseline performs so poorly that it cannot discover strategies which make use of the new tomato ingredient. The reason is because playing the onion-soup strategy and the tomato-soup strategy together can recreate an unrecoverable game state, which is not favored by the CoMeDi algorithm to include both types of agents in the population. Therefore, when we use GAMMA to train a generative partner model using data generated from the CoMeDi population, it also fails to learn any strategies involving tomatoes, and both models generalize poorly to playing with humans, although CoMeDi+GAMMA still offers a performance benefit over CoMeDi. This points to the fact that GAMMA, which is based on training a generative model on cooperation data, can fail to perform well if the cooperation data is not sufficiently diverse. This problem can be aptly described by the well-known adage \"garbage in, garbage out\"."}, {"title": "6.2.2 H2: Training with Human Data.", "content": "Comparing methods that make use of human data reveals some interesting findings. First, our results directly compare two lines of prior research: training on simulated populations vs. training against a BC model trained with a limited amount of human data (PPO-BC). While previous works show that using only simulated data can exceed PPO-BC and reach state-of-the-art performance [29], we find that on the more complex layout, PPO-BC is actually the best performing baseline. Modeling the human data with the generative model (PPO+BC+GAMMA) only provides performance im- provements half the time. However, combining simulated and human data with Human Adaptive GAMMA provides significantly higher performance in both layouts, surpassing state-of-the-art zero-shot coordination techniques.\n6.3 H3: Can we obtain better performance than competitive baselines?\nAs revealed in Figure 5, GAMMA HA achieves the highest performance in both layouts, surpassing the most competitive baselines by 60% and 43% in Counter Circuit and Multi-Strategy Counter, respectively (See Table 3). We conducted a statistical analysis of these results using Holm-Bonferroni correction, which can be found in Table 4."}, {"title": "6.3.1 Subjective Human Ratings", "content": "As demonstrated in prior work [1], humans can adapt to deficiencies in the policies of AI agents and narrow the apparent performance gap between different agents by simply completing the task themselves. This means cooperation performance alone cannot measure whether a particular agent is frustrating or cumbersome for the human to cooperate with. Therefore, we also collect subjective ratings of the agents from the human participants. The results for the question about overall coopera- tion are plotted in Figure 6; additional results are available in the Appendix, which pertain to agent's ability to adapt (Fig. 9), and whether it was human-like (Fig. 10) or frustrating (Fig. 11).\nFor the overall ratings in Figure 6, the subjective ratings mirror the cooperative performance scores: when GAMMA is trained with the same data available to a baseline technique it improves perfor- mance in terms of the human ratings, and GAMMA HA gives consistently good performance on both layouts, in the sense that it receives a higher proportion of positive human ratings. We note that while PPO+BC+GAMMA did not show a performance benefit over PPO+BC on Multi-strategy Counter, human ratings of PPO+BC+GAMMA were more positive. One exception to the previous trends is that CoMeDi obtains high subjective ratings in Counter Circuit, although it performs poorly in Multi-strategy Counter. From the additional figures in the appendix, we can observe that GAMMA methods are rated as more adaptive, human-like, and less frustrating than other techniques."}, {"title": "6.3.2 Qualitative Findings", "content": "Analyzing the qualitative results reported by participants in the human study, we find that adaptation to the human partner was a core theme distinguishing agents that humans liked. Participants reported that the FCP + GAMMA agent demonstrated an ability to learn from the user's actions, such as mimicking the user's strategy of placing onions on the table to save time. This adaptive behavior was positively received by a study participant: \"I noticed that once I started to put back onions on the table that it did the same as I wanted to save time rather than going back for onions 3 times for soup. I thought it was interesting that it learned about my behavior.\" Because GAMMA models have been trained over a more diverse range of partners, they consequently exhibit the ability to better adapt to real humans during gameplay.\nAnother common theme that emerged from the qualitative analysis was consistency and predictabil- ity. Erratic behavior was a common observation regarding the baseline methods. Users observed the AI performing random actions, such as moving onions around without an evident purpose or failing to complete necessary steps in the cooking process. In contrast, users report that agents using GAMMA behave in a logical, consistent manner. For example, for FCP + GAMMA, participants provided the following feedback that the agent was \"more deliberate in its actions\" and \"its actions were logical\". In contrast users reported that the baseline FCP agent, \"didn't behave logically\" and \"the agent this time was inconsistent and did not help me with any of my orders at all.\" We hypothesize the reason for this \"inconsistency\u201d that occurred in baseline methods may be due to the human's behavior going out-of-distribution (OOD) of their training data, causing the resulting policy to make errors and behave in an unpredictable way. This points to the importance of obtaining better coverage of the human data distribution, as provided by GAMMA (see Figure 1)."}, {"title": "7 Conclusion", "content": "In this work, we propose GAMMA, a novel approach to training a coordinator agent by using generative models to produce training partner agents. We conduct a comprehensive analysis using data from a study with real human cooperation partners, and show GAMMA outperforms baselines over both subjective human ratings and quantitative measurements of cooperation performance. We also provide a new perspective to compare different populations under the latent space of a generative model, showing how the simulated populations may not provide sufficient coverage of the range of human players.\nLimitations. As shown by the performance of GAMMA+CoMeDi on Multi-Strategy Counter, obtaining good performance with our approach depends on having a reasonably diverse amount of cooperation data to train the model. If the quality of the simulated population data is too low, the approach can fail to provide significant benefits.\nIn this work, our human studies recruit participants from Prolific, which may not be representative of broader populations. Additionally, our human dataset is limited, which could reduce the diversity of strategies and force participants to adapt to strategies that the Cooperators are already familiar with.\nWe focus on the two-player setting in this study following prior work [29, 37, 40] because it is a first step toward enabling an AI assistant that could help a human with a particular task. Scaling up to"}, {"title": "A Reproducibility", "content": "Our demo website is https://sites.google.com/view/human-ai-gamma-2024/ and contains the code and more experiment results. We also provide information about the implementation details B and hyperparameters used in our experiments D to help reproduce our results."}, {"title": "B Implementation details", "content": "Generative models. The dataset used to train the VAE model contains the joint trajectories of two players. For the simulated agent population {\u03c01, ..., \u03c0\u03c1}, we create this dataset by evenly sampling \u03c0i \u00d7 \u03c0j to generate the trajectories. A simulated dataset contains 100K joint trajectories. To train a VAE on it, the dataset is split into a training dataset with 70% data and a validation dataset with the rest of 30% data. To compute the ELBO loss 1, the trajectories are truncated to length 100 for better optimization for the recurrent module. A linear scheduling of the KL penalty coefficient \u03b2 is adopted to control a target value for the KL divergence of the posterior distribution. The target KL value is set to 7 for the layout Forced Coordination over the CoMeDi population. All other VAE models are chosen by a target KL value of 32.\nTrain Cooperator agents. On Overcooked, the Cooperator is trained by PPO [25]. We based our implementations on HSP [37]. Reward shaping for dish and soup pick-up is used for the first 100M steps to encourage exploration. All results are reported with averaged episode reward and the standard error of at least 5 seeds.\nSimulated agent populations. We use MAPPO [38] to create the FCP [29] agent populations. Eight self-play agents are trained and three checkpoints for each agent are added to the population, making the population size 8 \u00d7 3. For the CoMeDi agent population [24], we download the population proposed by the authors 3 for the original five layouts [1]. The population size is 8 for CoMeDi. For our custom layout Multi-strategy Counter, we use CoMeDi's official implementation and keep the population size the same."}, {"title": "C Human Dataset", "content": "For the human dataset in the original Overcooked paper [1], their open-sourced dataset contains \"16 joint human-human trajectories for Cramped Room environment, 17 for Asymmetric Advantages, 16 for Coordination Ring, 12 for Forced Coordination, and 15 for Counter Circuit.\" with length of T\u2248 1200. In Multi-strategy Counter, we collect 38 trajectories with length T \u2248 400 which is closer to the actual episode length during training."}, {"title": "D Hyperparameters", "content": "We use MAPPO to train our Cooperator agent. The architectures and hyperparamers are fixed throughout all layouts. All policy networks follow the same structure where an RNN (we use GRU) is followed by a CNN.\nThe generative model follows a similar architecture to the policy model. An encoder head and a decoder head are used to produce variational posterior and action reconstruction predictions from the representations."}, {"title": "E Computational Resources", "content": "We conducted our main experiments on clusters of AMD EPYC 64-Core Processor and NVIDIA A40/L40. It takes about one day to train one Cooperator agent. The main experiments takes about 3600 GPU hours. We do some preliminary experiments to search for the best hyperparameters and training frameworks."}, {"title": "F Human Evaluation", "content": "Since diversity of humans is a key component in our approach, in this section we report the demo- graphics of our participants. The study demographics include a population of 54% female and 46% male participants. The user demographics skewed towards younger ages, with 39% of participants between ages 18-26, 44% of participants between 27 \u2013 37, 11% of participants between 38 \u2013 48, and 6% of participants ages 49 and above. The game experiences of the participants are shown in Figure 7. In our user study, we ensure that participants pass an attention check by reviewing the results of their survey responses. Participant data is discarded if they do not pass the attention check."}, {"title": "F.1 Humans are adapting during evaluation", "content": "As shown by Fig 8, human performance improves with the number of trials, indicating that the humans learn, change, and adapt their gameplay during the course of the evaluation. In our evaluation with real humans, each user can change their strategy at any point in the game. A significant proportion of our users self-identify as novice players, both for video games and for Overcooked (17.9% and 49.5%, respectively, Figure 8, thus often exhibiting improved performance over the course of an increased number of trials. Figure 8 provides evidence of this pattern, demonstrating that humans change their gameplay style over the course of the evaluation."}, {"title": "G Additional Human Study Results", "content": "G.1 Human-AI team scores\nWe conducted statistical significance tests and computed the p-value using the Holm-Bonferroni correction. See Table 4.\nG.2 Qualitative analysis\nWe include additional analyses of users self-reported responses for qualitative questions from our user study. Figure 9 shows the results for users' response to the agents' ability to adapt. The results indicate that two of our methods, FCP + GAMMA and GAMMA-HA-DFT, consistently receive higher ratings indicating better ability to adapt than their respective baseline agents, across both layouts. Figure 10 displays users' ratings for whether the agents demonstrated human-like behavior. The responses show that FCP + GAMMA, CoMeDi + GAMMA, and GAMMA-HA-DFT exhibit the most human-like gameplay in both layouts. Figure 11 includes responses for whether the agents' behavior was frustrating. Individuals consistently reported that FCP + GAMMA and GAMMA-HA-DFT demonstrated the least frustrating behavior.\nFurthermore, we provide additional participant feedback for our agents from the user study as follows, starting with feedback for agents using our methods:\nFeedback for FCP + GAMMA:\n\u2022 \"It was very coordinated.\"\n\u2022 \"This agent figured things out the fastest and worked with me well.\"\n\u2022 \"When it was in my way, it moved out of the way instead of blocking the path, which was an issue with other agents.\"\n\u2022 \"Much better cooperation by comparison.\"\nFeedback for CoMeDi + GAMMA:"}, {"title": "I Compare decoder-only fine-tuning and full fine-tuning", "content": "I.1 Full fine-tuning can suffer from insufficient human data\nIn some early experiments, we find that only fine-tuning the decoder with human data (GAMMA-HA- DFT) provides consistently strong performance on both layouts, whereas full fine-tuning (GAMMA- HA-FFT) provides the strongest performance on the first layout, but weaker performance on the second layout. At first, we hypothesize this is because the second layout is more complex, but the number of human coordination trajectories available for training (N = 11) is significantly less than the first layout (N = 37). With more diverse potential strategies and less human data, we find the data is insufficient to fully fine-tune the generative model for the second layout. Since the entire model is fully fine-tuned, there is a higher chance that the model overfits the training human samples when the amount of human data is extremely small. Therefore, we believe the results of the FFT model could be improved were we to collect more data. However, it is realistic to test the scenario where limited human data is available, since human data can be quite expensive and difficult to collect. We find that in this low data regime, GAMMA HA DFT still provides excellent performance. Whether to fine-tune the encoder is a design choice that can be tuned for a particular domain based"}, {"title": "1.2 Large regularization mitigates the problem of insufficient human data", "content": "Given the hypothesis that the human dataset is too small compared to the complexity of the Multi- strategy Counter environment, we find out that with a larger regularization over the fine-tuned model on the original model, we can mitigate this issue. See Figure 14."}, {"title": "JCan we condition the Cooperator policy on z?", "content": "One potential future work to improve the efficiency of online adaptation is to condition the policy on z. During testing with novel humans, the Cooperator can then infer the z of the human player and adapt to that z.\nThis method is orthogonal to our contributions where we focus on sampling partners with different z to train the Cooperator. We did some preliminary work to test the z-conditioned Cooperator. As shown in Figure 15, the performance of the z-conditioned Cooperator is not stable, it learns faster but the performance decreases when it is trained longer. Therefore, we do not include this method in our main experiment, but future work about how to better train the z- conditioned Cooperator is promising."}, {"title": "$J(\\pi_c) = E_{z~D_z} [V_z(\\pi)] = E_{z~p(z)} [V(\\pi, \\mu_z)]$.", "content": ""}]}