{"title": "SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains", "authors": ["Ran Xu", "Hui Liu", "Sreyashi Nag", "Zhenwei Dai", "Yaochen Xie", "Xianfeng Tang", "Chen Luo", "Yang Li", "Joyce C. Ho", "Carl Yang", "Qi He"], "abstract": "Retrieval-augmented generation (RAG) enhances the question answering (QA) abilities of large language models (LLMs) by integrating external knowledge. However, adapting general-purpose RAG systems to specialized fields such as science and medicine poses unique challenges due to distribution shifts and limited access to domain-specific data. To tackle this, we propose SimRAG, a self-training approach that equips the LLM with joint capabilities of question answering and question generation for domain adaptation. Our method first fine-tunes the LLM on instruction-following, question-answering, and search-related data. Then, it prompts the same LLM to generate diverse domain-relevant questions from unlabeled corpora, with an additional filtering strategy to retain high-quality synthetic examples. By leveraging these synthetic examples, the LLM can improve their performance on domain-specific RAG tasks. Experiments on 11 datasets, spanning two backbone sizes and three domains, demonstrate that SimRAG outperforms baselines by 1.2%-8.6%.", "sections": [{"title": "1 Introduction", "content": "Retrieval-augmented generation (RAG) (Lewis et al., 2020; Gao et al., 2023; Guti\u00e9rrez et al., 2024; Asai et al., 2024) is a powerful technique that enhances large language models (LLMs) for various knowledge-intensive tasks such as question answering (QA) by incorporating external knowledge sources. This method not only customizes responses to handle long-tail knowledge but also avoids the need for costly model retraining (Ovadia et al., 2023). Additionally, RAG helps reduce the issue of LLM hallucination by ensuring responses are grounded in relevant evidence (Shuster et al., 2021), thereby improving the overall accuracy and reliability of LLM outputs.\nWhile extensive research has focused on developing effective (Asai et al., 2024; Lin et al., 2024; Liu et al., 2024) and efficient (Xu et al., 2024a) RAG systems for general-domain QA tasks, adapting RAG to specialized domains for LLMs poses significant challenges. These models often struggle with distribution shifts and fail to accurately extract information from domain-specific contexts (Miller et al., 2020; Liu et al., 2022). Moreover, directly using black-box LLMs (OpenAI, 2023; Anthropic, 2023; Wang et al., 2023b) in specialized domains raises concerns about privacy when dealing with sensitive proprietary data. It is essential to fine-tune LLMs on domain-relevant QA tasks to unlock the full potential of LLM-based RAG systems in specialized domains.\nDespite the critical need for domain-specific fine-tuning, the primary challenge lies in the acquisition of high-quality fine-tuning data towards RAG applications. Prior works rely on continuous pretraining (Chen et al., 2023; Zhang et al., 2024a) on specialized corpora or fine-tuning on domain-specific instruction-tuning data (Wu et al., 2024; Wadden et al., 2024). However, the mismatch between these general-purpose tasks and domain-specific QA hinders their effectiveness. More recently, several approaches (Liu et al., 2024; Schimanski et al., 2024; Zhang et al., 2024c) use synthetic data from powerful LLMs (e.g., GPT-4) to create QA fine-tuning datasets. While promising, these methods are costly, inefficient, and lack explicit quality control over the generated outputs. Additionally, the direct use of proprietary corpora with black-box LLMs introduces privacy concerns, making these methods unsuitable for sensitive domains.\nTo tackle the data scarcity issue mentioned above, we propose SimRAG\u00b9, a self-improving approach to harness the LLMs' own capabilities to generate pseudo-labeled data for domain adap-"}, {"title": "2 Related Work", "content": "Retrieval-augmented generation. RAG has emerged as a powerful tool in knowledge-intensive NLP tasks such as language modeling (Borgeaud et al., 2022) and question answering (Lewis et al., 2020; Shi et al., 2024a). The typical approach involves integrating a retriever with the LLM generator and designing a fine-tuning process to align the retriever with LLM capabilities. To further refine RAG, recent research explored various enhancements. These include developing dynamical retrieval processes to refine the relevance of fetched content (Jiang et al., 2023; Jeong et al., 2024; Su et al., 2024), and filtering out irrelevant contexts to robustify RAG (Yoran et al., 2024; Yu et al., 2024, 2023; Wang et al., 2024b). Additionally, several studies have developed instruction-tuning methods aimed specifically at improving search and RAG capabilities of LLMs (Liu et al., 2024; Lin et al., 2024; Dong et al., 2024; Wei et al., 2024; Wang et al., 2024a).\nSelf-training. Self-training (or Pseudo-Labeling) is one of the earliest approaches to semi-supervised learning (Rosenberg et al., 2005). The method uses a teacher model to generate new labels on which a student model is fitted. Self-training has been widely adopted for various NLP tasks including text classification (Du et al., 2021), natural language understanding (Vu et al., 2021) and ranking (Wang et al., 2022). Recently, the idea of self-training has also been applied to LLM instruction fine-tuning (Yuan et al., 2024; Li et al., 2024), reasoning (Pang et al., 2024), and alignment (Gulcehre et al., 2023), yet to the best of our knowledge, this pipeline has not been widely explored for RAG applications. The major drawback of self-training is that it is vulnerable to label noise (Arazo et al., 2020). There are several approaches to stabilize the self-training, with sample selection (Li et al., 2024) and reweighting (Wang et al., 2021) strategies.\nDomain-specific LLMs. Most domain-specific"}, {"title": "3 Methodology", "content": "In a RAG problem, we aim to generate answers for queries based on a set of supporting documents or contexts. Specifically, for a query q, an retriever R is utilized to retrieve top-k most relevant contexts D = {d1, d2, ..., dk} from a large corpus C. The LLM Me then generates an answer a to the query q based on the retrieved context D.\nIn this work, we aim to improve the LLM's QA capability in RAG system towards specialized domains where only unlabeled corpus C is available. As shown in Figure 1, our approach first learns from retrieval-oriented instruction data in the general domain in Stage-I and then augments T with pseudo-labeled T' = (q',D', a') tuples in Stage-II, where D' is sampled from the specialized domain C for self-training. The overall objective of our study is to adapt the LLM Me to specialized domains with TUT'."}, {"title": "3.1 Problem Setup", "content": "3.2 Stage-I: Retrieval-oriented fine-tuning\nTo start with, we leverage instruction fine-tuned LLMs as the backbone (e.g. meta-llama/\nMeta-Llama-3-8B-Instruct). Although these models have been instruction finetuned, they still exhibit a deficiency in leveraging context information to answer domain-specific questions. To improve the their abilities on knowledge-intensive tasks, we fine-tune the LLM with retrieval-oriented tasks. Specifically, we follow Lin et al. (2024); Liu et al. (2024) and leverage the training data blend that consists of the following components:\n(1) General Instruction Fine-tuning (SFT) data. To help maintain the models' ability to comprehend and follow instructions, we leverage the SFT data including OpenAssistant (K\u00f6pf et al., 2023), Dolly (Conover et al., 2023), SODA (Kim et al., 2023), ELI5 (Fan et al., 2019), Self-Instruct (Wang et al., 2023a), and Unnatural Instructions (Honovich et al., 2022). Note that we make sure there is no overlap between SFT data and test data from target tasks.\n(2) General domain Context-aware QA data. To bolster the LLMs' general RAG skills of generating accurate answers grounded in relevant contexts, we fine-tune them on a diverse array of general domain question-answering datasets. This includes DROP (Dua et al., 2019), NQ (Kwiatkowski et al., 2019), Squad (Rajpurkar et al., 2016), Nar-rativeQA (Ko\u010disk\u00fd et al., 2018), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019), Open-"}, {"title": "3.3 Stage-II: Domain Adaptive Fine-tuning", "content": "The model after Stage-I is only trained in the general domains. When directly adopting them to specialized applications, the performance can still be suboptimal due to the distribution shift issue (Miller et al., 2020). To tailor the LLMs for specialized domains and address the scarcity of labeled data in these areas, we employ a selftraining approach leveraging domain-specific unlabeled corpora. This method capitalizes on the model's enhanced capabilities from the previous retrieval-augmented fine-tuning stage. We utilize the fine-tuned LLM to generate pseudo-labeled training samples T' = (q',D', a') by creating queries grounded in the unlabeled text and gathering the corresponding retrieved documents.\nSpecifically, we conduct a two-step procedure to synthesize additional training data, which corresponds to the two skills learned in Stage-I: (a) Answer Generation: for each document di \u2208 C, where C is the unlabeled corpus, we prompt our fine-tuned LLMs to generate several candidate spans a1, a2,..., am that are likely to be answers to some questions. Formally, the model generates a ~ po(di) for j = 1,...,m. (b) Answer-conditioned Query Generation: for each candidate answer a and its corresponding document di, we prompt the fine-tuned LLM again to generate candidate questions q ~ po(a, di), with a as the ground truth answer and di as the supporting context. This gives us the pseudo-labeled query-answer pair (q, a) based on the context di.\nDuring this process, we adopt two additional strategies, namely diverse question generation and data filtering, to further improve the quality of the synthetic pairs. For diverse question generation, we prompt the LLM to create various types of questions, including short-span question-answering, multiple-choice question-answering, and claim verification tasks. While short-span questions follow the same pipeline as previously described, multiplechoice questions are constructed by using alternative candidate answers from the same unlabeled corpus in step (a) as incorrect options. Claim verification, on the other hand, bypasses the answer generation step; instead, the LLM generates a claim that can be either supported or refuted by the provided document. By injecting different question types, we prevent the LLM from overfitting to a specific output format and improve the model's generalization ability across different QA tasks.\nAfter generating large amounts of candidate QA pairs, we implement a filtering step to keep only high-quality QA pairs. We define high-quality QA pairs as those that are answerable using the topk retrieved contexts. Specifically, we retain only those samples where the ground truth answer a is present in the top-k documents retrieved by a strong retriever, such as Dragon (Lin et al., 2023), based on the generated query q. Formally, the sample is retained if a \u2208 Dk, where Dk denotes the top-k documents retrieved for query q. From these retained samples, we create pseudo-labeled training tuples T' = (di, Di, a)=1\nWith the created synthetic tuples T', we augment it with the SFT data TSFT and the general domain context-aware QA data from Stage-I Tgen, to further fine-tune our models, enhancing the LLMs' QA abilities within the specific domain. The size and"}, {"title": "4 Experimental Setup", "content": "We evaluate our model across a total of 11 datasets spanning the medical, scientific and computer science domains. For the medical domain, we include the five datasets in the MIRAGE benchmark (Xiong et al., 2024), including PubMedQA (Jin et al., 2019), BioASQ (Tsatsaronis et al., 2015), MedQA (Jin et al., 2021), MedMCQA (Pal et al., 2022), the medical subsets in MMLU (Hendrycks et al., 2021), and two additional open-ended QA datasets LiveQA (Abacha et al., 2017), and MedicationQA (Abacha et al., 2019). For the scientific domain, we consider ARCchallenge (Clark et al., 2018), SciQ (Welbl et al., 2017)2, and the scientific subsets (14 subtasks in total) in MMLU (Hendrycks et al., 2021). For computer science, we use CS-Bench (Song et al., 2024) for evaluation. We distinguish the computer science domain from the broader scientific domain as the scientific domain predominantly covers natural and social sciences, with limited representation of computer science topics. We use accuracy as the evaluation metric for multiple-choice and True-or-False questions, Rouge-L and MAUVE for open-ended questions, Exact Match (EM) and F1 for Fill-in-the-blank questions, with Rouge-L and F1 as the main metrics, respectively. An exception is CS-Bench, where we follow the original paper's evaluation method by using GPT-4 as a judge for fill-in-the-blank and open-ended questions.\nFor the medical domain, we use the corpora from Textbooks (Jin et al., 2021), Wikipedia and PubMed articles\u00b3 to generate pseudo-labeled samples in Stage-II. For the scientific domain, we leverage Wikipedia. For the CS domain, we use Wikipedia CS Subset\u2074 and arXiv articles."}, {"title": "4.1 Tasks and Datasets", "content": "We categorize our baselines into four groups: (1) Off-the-shelf general domain LLMs, which include GPT-3.5 (OpenAI, 2022), GPT-4 (OpenAI, 2023),\nLlama3-8B-it (Meta-AI, 2024), and Gemma2-27B-it (Team et al., 2024). (2) Off-the-shelf domainspecific LLMs, including PMC-llama-13B (Wu et al., 2024), MEDITRON-70B (Chen et al., 2023), AdaptLLM-v2-8B (Cheng et al., 2024), BioMistral7B (Labrak et al., 2024) and MedLlama3-8B (John Snow Labs, 2024) in the medical domain, as well as SciTulu 7B and 70B (Wadden et al., 2024) in both the scientific domain and the computer science domain, due to the absence of LLMs specifically fine-tuned for the computer science domain. (3) General domain retrieval-augmented LLMs, which include Self-RAG-13B (Asai et al., 2024), ChatQA1.5-8B and 70B (Liu et al., 2024). (4) Domain-specific Retrieval-augmented LLMs, including RAFT (Zhang et al., 2024c) and Evi-denceRAG (Schimanski et al., 2024). Since RAFT and EvidenceRAG have not released their checkpoints, we re-implemented their methods using the same backbones as our approach. Note that for all the baseline models, we conduct the zero-shot evaluation and augment the context with retrieval for fair comparison. We also note that we do not compare with several domain-specific baselines such as (Zhang et al., 2024b; Nori et al., 2023) which have access to task-specific examples that overlap with our evaluation tasks."}, {"title": "4.2 Baselines", "content": "We use Llama3-it 8B (Meta-AI, 2024) and Gemma2-it 27B (Team et al., 2024) as our backbones. For the Gemma-2 model, we use LoRA (Hu et al., 2022) (r = 32, a = 32) during fine-tuning due to resource constraints. For both stages, we set the global batch size to 64, with gradient accumulation as 8 and train the model for 1 epoch. For Stage-I, the learning rate is set to 5e 7 and for Stage-II, it is set to 2e - 7 for the Llama3 backbone and 5e 7 for the Gemma backbone. AdamW optimizer (Loshchilov and Hutter, 2019) is used with \u03b2\u2081 = 0.9 and B2 = 0.95. To create contextenhanced examples for our synthetic queries, we use Dragon (Lin et al., 2023) to extend context length for SimRAG and baselines, which improves RAG model robustness (Yu et al., 2024, 2023). For retrieval during evaluation on medical datasets, we follow the original MIRAGE benchmark by using the top-10 retrieval results as context, ensembled from multiple models. For other datasets, we fetch"}, {"title": "5 Experimental Results", "content": "Table 1, Table 2, and Table 3 present the experimental results for the medical, scientific, and computer"}, {"title": "5.1 Main Results", "content": "ant and 1.19%, 3.50%, 3.20% over the Gemma variant, respectively. Besides, SimRAG also achieves comparable performance to strong proprietary models: when using Gemma2-27B as the backbone, we achieve 93.99%, 98.95% and 86.66% of the performance of GPT-4. This demonstrates the effectiveness and robustness of SimRAG in adapting general-domain LLMs to specialized domain knowledge using only unlabeled corpora.\n(2) Domain-specific LLMs (e.g. SciTulu and MedLlama), although fine-tuned on relevant data, underperform compared to SimRAG because they are not optimized for RAG tasks, where effectively utilizing retrieved context is crucial. As a result, they struggle to incorporate relevant context into their answers, leading to weaker performance. On the other hand, general-domain RAG models (e.g. ChatQA) face distribution shifts when applied to specialized tasks, as they struggle to integrate the retrieved domain-specific knowledge accurately.\n(3) Domain-specific retrieval-augmented LLMs such as RAFT and EvidenceRAG still show suboptimal performance despite utilizing the powerful (yet expensive) GPT-4 model to generate synthetic training data. In contrast, SimRAG, finetuned specifically for the QA generation task, produces more accurate and contextually relevant synthetic QA pairs, leading to better downstream performance across all QA tasks.\n(4) Although the CS domain is relatively new and less-studied compared to other natural and social sciences, SimRAG still demonstrates promising performance in this area. This justifies the potential for adapting SimRAG to emerging domains."}, {"title": "5.2 Ablation Studies", "content": "Effect of Stage-I and Stage-II. Table 1 to 3 show that retrieval-oriented fine-tuning (Stage-I) significantly enhances LLM performance on QA tasks compared to the original backbone, demonstrating its effectiveness. However, further improvements become challenging after this stage. When the LLMs are fine-tuned on self-synthesized training tuples, their performance on target tasks improves even more, with an average increase of 2.21% for Llama and 3.50% for Gemma.. This suggests that, with access to a target domain corpus, LLMs can generate high-quality synthetic data, enabling selfimprovement and further boosting performance.\nEffect of Different Retrievers. We show the performance of SimRAG using Dragon (Lin et al., 2023) as the retriever in Table 4. The results show"}, {"title": "5.3 Study on Pseudo-labeled Tuples", "content": "We mainly demonstrate the advantage of SimRAG in generating pseudo-labeled data from the following three perspectives.\nEffect of different question generation models. To demonstrate the benefit of training on question generation and question-answering data, we compare the performance of Stage-II using different synthetic question-answer pairs. These pairs are generated either directly by Llama-3-8b-it or by an off-the-shelf QG model with T5-Large (Raffel et al., 2020) as the backbone. The results demonstrate that our approach achieves better performance on average, demonstrating the clear advantage of leveraging the fine-tuned model itself for pseudo-labeled data generation.\nEffect of question filtering. We further demonstrate the advantages of question filtering in Figure 3, showing that removing low-quality data not only improves overall model performance but also accelerates the training process. It is also worth noting that even without filtering, SimRAG can achieve strong performance, suggesting that the synthetic questions generated from fine-tuned LLMs are already highly relevant to the context.\nEffect of diverse question types. From Figure 2, we observe that SimRAG achieves the best performance when all three different types are included. These results justify the necessity for incorporating different task types into the fine-tuning step in Stage-II. Besides, claim verification benefits PubMedQA and BioASQ more, while multiple-choice questions boost performance on MedQA, MedMCQA, and MMLU, aligning with the question types in each dataset. Lastly, we observe that removing short-span QA leads to the largest performance drops, indicating its central role in adapting the LLM's performance towards specialized domains."}, {"title": "5.4 Case Studies", "content": "To better illustrate the quality of pseudo-labeled samples generated by SimRAG after Stage-I finetuning, we present two case studies in Table 5, comparing the samples produced by SimRAG with those from the baseline model, Llama3-8B-it.\nIn the first case, where the model is asked to generate a claim supported by the context, Llama38B-it simply selects a sentence from the context. This results in relatively simple QA pairs, making the task less challenging for Stage-II training.\nIn the second case, the model is tasked with generating an answer first, and then formulating a question based on the context and the answer. While Llama3-8B-it does not copy a sentence exactly, it generates a lengthy question that closely paraphrases the context. This makes the question overly dependent on the original text, making it difficult to interpret without it. Additionally, the model misinterprets the context by implying that the research was focused on children when actually adults are the focus. In contrast, after fine-tuning on answer generation and query generation in Stage-I, SimRAG generates higher-quality QA pairs that are self-contained and understandable without relying on the context. These QA pairs also present more challenging tasks, as they require deeper comprehension of the context, providing harder and more effective training data for Stage-II."}, {"title": "6 Conclusion", "content": "We introduce SimRAG, an instruction fine-tuning framework designed to enhance LLMs for domainspecific question-answering tasks. By equipping LLMs with joint capabilities for both question answering and question generation, SimRAG enables the generation of diverse, high-quality synthetic questions from unlabeled domain-relevant corpora. This approach facilitates effective adaptation to specialized fields, where distribution shifts and limited domain-specific data typically pose challenges. Extensive experiments across 11 datasets in three domains show that SimRAG consistently outperforms baseline models, demonstrating its effectiveness in tackling the challenges of retrieval-augmented, domain-specific question-answering tasks."}, {"title": "Limitation", "content": "While SimRAG demonstrates notable improvements, there are some limitations to our approach:\nSingle Round Pseudo-Label Generation: Our current method relies on a single round of query generation from the corpus, which may restrict the refinement of pseudo label quality. Iterative refinement of generated synthetic queries could potentially lead to better results.\nAdditional Training Time: The incorporation of synthetic query generation and filtering adds time complexity compared to baseline models, which may affect efficiency in environments with limited computational resources. However, we would like to note that our method will not increase the inference time complexity compared to the existing RAG approaches with the same backbone models.\nStronger Query Generation Models: Although we achieved strong performance with Llama3 8B and Gemma2 27B models, leveraging more powerful query generation models, such as Llama-3.170B-it (Meta-AI, 2024), could yield further gains. However, using larger models would incur higher computational costs beyond our current budget."}, {"title": "E Prompt Details", "content": "Based on the context, generate several candidate spans within the passage that are likely to be answers to a question. The answers can be entities, verbs or even numbers. Make sure that the answers are different and diverse. Separate different"}, {"title": "E.1 Answer Generation", "content": "[System]\n[Context]"}, {"title": "E.2 Query Generation", "content": "[System]\n[Context]\nBased on the context, please generate a question that is relevant to the information provided. The question should stand alone and not refer back to the context explicitly. The question should be clear and understandable without needing the context. The answer to the question should be [Answer]."}, {"title": "E.3 Inference", "content": "[System]\n[Top 10 Contexts]\n[Specific Instruction]\n[Question]\nThe [Specifc Instruction] for each evaluation dataset depends on their question type and can refer to those in Table 6."}]}