{"title": "Machine Learning Should Maximize Welfare, Not\n(Only) Accuracy", "authors": ["Nir Rosenfeld", "Haifeng Xu"], "abstract": "Decades of research in machine learning have given us powerful tools for making\naccurate predictions. But when used in social settings and on human inputs, better\naccuracy does not immediately translate to better social outcomes. This may not be\nsurprising given that conventional learning frameworks are not designed to express\nsocietal preferences\u2014let alone promote them. This position paper argues that\nmachine learning is currently missing, and can gain much from incorporating, a\nproper notion of social welfare. The field of welfare economics asks: how should\nwe allocate limited resources to self-interested agents in a way that maximizes\nsocial benefit? We argue that this perspective applies to many modern applications\nof machine learning in social contexts, and advocate for its adoption. Rather than\ndisposing of prediction, we aim to leverage this forte of machine learning for\npromoting social welfare. We demonstrate this idea by proposing a conceptual\nframework that gradually transitions from accuracy maximization (with awareness\nto welfare) to welfare maximization (via accurate prediction). We detail applica-\ntions and use-cases for which our framework can be effective, identify technical\nchallenges and practical opportunities, and highlight future avenues worth pursuing.", "sections": [{"title": "1 Introduction", "content": "As the influence of machine learning on our lives continues to grow, there is high hope that this\nwill prove to be for the better. Given the unmatched ability of machine learning to make accurate\npredictions, we have, in principle, much reason for optimism. Accurate predictions have potential to\nimprove the choices we make for ourselves, from the mundane (e.g., what to buy or where to eat) to\nhighly consequential (e.g., what to study, how to invest, or even who to date). This can explain why\nmachine learning has become the backbone of most recommendation systems, media services, online\nmarketplaces, and social platforms. Better prediction can also provide better support for decisions\nmade about us, such as which medical treatment to apply, when to offer financial aid, or who to hire.\nLearned models are therefore making their way, slowly but surely, into more conventional social\ndomains such as health care, education, finance, transportation, law, and even government. Given how\nlearning is increasingly shaping how we communicate, express ourselves, and are productive-its\ninfluence and integration are only likely to grow. This requires us to think carefully about the role we\nenvision for machine learning in society, as it is forming now, and as we would like it to be.\nIf we accept that machine learning has the capacity to be socially beneficial, then ideally, we would\nlike it to help in improving outcomes for everyone. But economic theory casts doubt as to the\nfeasibility of such an aspiration. A basic economic truism is that whenever people benefit from\nsomething, that something will become a scarce resource, over which people will then contend.\nOur key point, which we will argue throughout, is that learning in social context is inherently, and"}, {"title": "1.1 Setting and scope", "content": "For concreteness, we focus on the fundamental learning task of maximizing predictive accuracy\nby training a model on a sample set of labeled data. However, all the ideas described here are\neasily applicable to other learning paradigms, including today's generative models. We follow the\nconventional supervised learning paradigm and assume data is in the form of feature-label pairs\n(x, y) sampled iid from some unknown distribution D. The data we consider represents humans;\nwe will typically think of inputs x as describing individuals (e.g., loan application, resume, movie\nviewing history, past consumer choices), labels y as describing outcomes of interest (e.g., loan return\nor default, job qualification, movie rating, satisfaction), and D as representing the population.\nAlthough our goal is to discuss how learning can promote social good, we will insist that accuracy\nremains an integral part of the learning objective. This is both because accuracy has value in and\nof itself, and because predictive methods are, and likely will continue to be, a primary tool used in\npractice. Thus, we seek to retool the conventional supervised learning framework towards the goal of\nwelfare maximization through the use of accurate predictions. This will allow us to capitalize and\nbuild on existing tools, knowledge, and practice. Because we consider limited resources and their\n(sometimes implicit) allocation, our discussion must extend to consider policies operating on the\nbasis of predictions, in the style of prediction policies of Kleinberg et al. [2015]."}, {"title": "1.2 Limitations of current practice", "content": "It is tempting to hope that more data, more compute, and more sophisticated learning algorithms will\npave the way to a better tomorrow. But merely improving predictions, whether to match or go beyond\nhuman capabilities, still presents inherent limitations. Consider an example:"}, {"title": "1.3 What has been done, and what is still missing", "content": "The issue of aligning learning with societal preferences is of course not new, nor is the introduction\nof social welfare [Shirali et al., 2024, Perdomo, 2024]. We believe however that the current literature\nrequires a unifying formal framework for this notion. Several subfields of machine learning have made\nadvances on some fronts, but remain to lack in others. A prime example is the literature on fairness\nin machine learning [Dwork et al., 2012], which clearly aims to advance some form of social good,\nand for this has received much attention. One concern that has been voiced is that fairness focuses on\nmaking things equal, but is unable to express the benefit that results from such efforts [Heidari et al.,\n2018, Hu and Chen, 2020]. From our perspective, fairness takes one step towards acknowledging\nthat something is of limited supply (e.g., positive predictions), but this is not made explicit. More\nimportantly, fairness constraints do not account for agency: they do not ask what users want, nor how\nmuch they stand to lose or gain. Another issue is that such constraints do not consider user actions;\nfor example, predictions for school admissions may satisfy demographic parity, but if members of the\nminority group do not choose to apply initially, then this guarantee is broken [Horowitz et al., 2024].\nOne line of research that is close to ours is that of strategic learning. This includes topics like strategic\nclassification [Br\u00fcckner et al., 2012, Hardt et al., 2016] and performative prediction [Perdomo et al.,\n2020] which have drawn much recent interest. Strategic learning acknowledges and explicitly models\nuser agency, and in some cases, scarce resources. And while some works discuss social aspects of\nlearning [e.g., Milli et al., 2019], the focus remains primarily on maximizing accuracy (and to some\ndegree, notions of stability at equilibrium). The idea of welfare is not a driving consideration, nor is\nit an integral part of these frameworks.\nWe believe there is need for an umbrella framework that ties these loose ends and incorporates them\nas special cases. Such a framework will also be useful for pointing out what is missing in current\nsolutions, and what broader questions still needs addressing. Welfare in economics is rarely something\nthat is forced on a system; rather, it should emerge as the solution to the question-\u2018how can we"}, {"title": "1.4 Paving the path to welfare-aware machine learning", "content": "Machine learning offers powerful tools for training accurate predictive models, but the above limita-\ntions suggest that this might not suffice if we wish to solve more stressing societal problems. Not\nby chance, social welfare offers a complementary perspective that enables such considerations. We\nbelieve that current machine learning is missing, and could much benefit from, incorporating a notion\nof welfare.\nPosition: In social contexts, machine learning should enable us to improve our collective well-being. This requires a learning framework that is capable of expressing, and provides means\nto optimize, the welfare of its users. To promote this goal, we should exploit the power of\nlearning to obtain accurate predictions\u2014not shy away from it. But accuracy cannot be the only\nconsideration; to support constructive social outcomes, learning must: (i) explicitly consider\nresources, scarcity, and allocations; (ii) account for human agency by modeling what users\nwant, know, and do; and (iii) adhere to well-specified social welfare objectives designated by a\nsocial planner.\nThroughout the paper we will make these ideas clearer, more precise, better grounded, and placed in\ncontext.\nPaper outline. Our main thesis is that machine learning can benefit from the ideas and tools that\nwelfare economics has to offer; Sec. 2 discusses its main principles, and highlights the potential for\nsynergy with machine learning. In Sec. 3 we present our proposed framework for welfare-maximizing\nmachine learning and details its three orders. Sec. 4 presents alternative views, and Sec. 6 gives\nconcluding remarks and a look to the future."}, {"title": "2 Welfare economics and what it can teach us", "content": "Welfare economics is the subfield of economics that is concerned with the characterization, evaluation,\nand maximization of social welfare in economies and societies. The main principles of welfare\neconomics date back to Adam Smith [1759, 1776], but its formal foundations were laid out only a\ncentury later by notable neoclassic economists such as Edgeworth [1881], Marshall [1890], Pareto\n[1906], and Pigou [1920]. Modern welfare economists include influential figures such as Hicks\n[1939], Arrow [1951], Sen [1970], and Stiglitz [2012]\u2014all of which received the Nobel Prize for\ntheir contributions to this field. Given its rich history, we believe that machine learning has much to\ngain from adopting ideas and perspectives from this well-established discipline."}, {"title": "2.1 Welfare economics: crash course", "content": "The distribution of wealth. The main question welfare economics asks is: how should wealth\nbe distributed across individuals in the economy? Under the working hypothesis that resources\n(and therefore wealth) are limited, the main object of interest in welfare economics is the Pareto\nfront-the set of all possible economic states in which no individual can be made better off without\nmaking things worse for another [see, e.g., Johansson, 1991]. In terms of welfare, there are two\nmain considerations for policymakers:\n1. Efficiency: How do we reach a Pareto state?\n2. Equity: Of all Pareto states, which are preferable?\nEfficiency requires an ability to optimize economic outcomes to a point where social benefit is maximal\n(in the Pareto sense). Markets are a classic example of how the actions of many self-interested agents\ncan combine to produce efficient outcomes [Arrow and Debreu, 1954]. However, there are typically\nmany states that are maximally beneficial, but that differ in how benefits are distributed across individ-\nuals; in regard to this, markets are mostly silent. As such, equity makes a statement about the relative\npreference ordering over all possible states, and considers means for steering towards preferable ones.\nA canonical example is income distribution: all governments likely seek higher overall income (ef-\nficiency), but may disagree about whether high inequality should be permitted or suppressed (equity)."}, {"title": "Social welfare functions.", "content": "In welfare economics, the primary tool for defining and promoting equity\nis the social welfare function, which ranks or scores all possible economic states. Our focus will\nbe on the common choice of cardinal (i.e., real-valued) welfare functions that take the form of an\nexpectation over weighted individual utilities:\nwelfare(\u03c0; w) = E(x,y)~D[W(x)u(x, y; \u03c0)]\nwhere \u03c0 is a policy (e.g., rules deciding which resources are allocated to whom) and u(x, y; \u03c0) is\nthe utility of user x under policy \u03c0, given y. Notably, w(x) is a weight function that the planner\nchooses: this defines the desired direction for overall improvement (i.e., efficiency) by balancing the\nimportances of individual outcomes (i.e., equity).\nWhen the policy \u03c0 = \u03c0\u03b7 is guided by a predictive\nmodel h, we will write welfare(h) to mean welfare(\u03c0\u03b7).\nThe social planner. Equity is inherently a subjective notion that requires making value judgements.\nSocial welfare functions make it possible to formally express these by setting appropriate weights.\nIn welfare economics, weights are designated by a social planner-either a real or fictitious entity\nthat represents societal preferences. A social planner can set weights to aid low income individuals;\nimplement affirmative action towards some social group; or ensure that all individuals obtain some\nminimal level of utility. Concrete examples include using weights to prioritize certain subgroups\n[Bj\u00f6rkegren et al., 2022] or balance different objectives [Rolf et al., 2020]. The simplest weighing\nscheme is of course using uniform weights, i.e., w(x) = 1 for any x. But note even this makes a\nstatement, which is that individuals should be weighted by their utility; this is known as 'utilitarian\nwelfare'. Hence, from the perspective of welfare economics, any objective that optimizes a (non-\nweighted) average\u2014such as accuracy in machine learning is in effect making a statement about\nhow value should be distributed.\nHuman agency. Welfare economics makes explicit the idea that individuals have agency. Intuitively,\nthis states that individuals (i) want things, (ii) know things, and (iii) act \u2013 to get what they want,\nusing what they know. These notions are formally accounted for by modeling utility functions (want),\nprivate information (know), and decision-making, e.g., rational or behavioral (act). Any policy\nthat aims to advance welfare must take these into account. Often this requires the planner to make\nadditional efforts, such as to elicit preferences, create incentives for truthful reporting, or infer how\nusers will respond to different policy choices. These are challenging, but give the planner power:\nif incentives can be aligned, then it becomes possible to harness the willingness of users to invest\neffort for improving outcomes for all; consider public goods and services, crowdfunding platforms,\nopen-source software, and collaborative knowledge bases."}, {"title": "2.2 Connections to machine learning.", "content": "We believe that machine learning has much to gain from adopting a welfare perspective: when inputs\nrepresent humans, it becomes possible to promote overall social benefit (efficiency), and imperative\nto consider its distribution across the population (equity). Current tools already push forward on these\nideas, but only to a limited extent. One reason is that standard learning objectives are notoriously\nunderspecified; this has implications on e.g. robustness [D'Amour et al., 2022], explainability [see\nRudin, 2019], and fairness [e.g., Rodolfa et al., 2020, Coston et al., 2021, Black et al., 2022]. But\nunderspecification also presents an opportunity for steering outcomes toward socially beneficial\nstates. Consider how the idea of model multiplicity [Breiman, 2001, Marx et al., 2020, Hsu and\nCalmon, 2022], i.e., that there is typically a large set of (approximately) optimal models, connects to\nthe notion of an efficient Pareto front. The challenge lies in how to provide a social planner effective\nmeans to choose a preferred operating point. In some cases, this will be diffucult; in others, it may be\nachievable with tools as simple as regularization-if chosen appropriately [Levanon and Rosenfeld,\n2021]. In terms of limited resources, machine learning already offers many relevant tools, such as\nconstraints on cardinality (e.g., top-k prediction) or error rates (e.g., precision and recall). Also\nrelevant are tools from cost-sensitive [e.g., Elkan, 2001] and decision-focused learning [Mandi et al.,\n2024]. But to be effective, these must be adapted to account for agency: how users report information,"}, {"title": "3 Three Orders of Welfare-Maximizing ML", "content": "Our framework presents a hierarchy of problem formulations for welfare-maximizing learning,\norganized into three 'orders' of gradually increasing complexity. The orders are built in a bottom-up\nfashion: The lowest order (Order 0) coincides with the standard objective of maximizing predictive\naccuracy, but provides a novel welfare perspective suited for social tasks. Each higher order then\nbuilds on and generalizes the one below it, this by adding to the objective another layer of economic\ncomplexity: first focusing on system decisions (Order 1), and then on user choices (Order 2). As such,\nour framework is based on accuracy maximization at its core, but enables\u2014and in fact requiresto\nexplicitly model resources, allocations, and agency. It also requires to specify the role predictions\nplay in shaping social outcomes.\nWe begin with a welfare interpretation of supervised learning, and then proceed to discuss the three\nframework orders. Further prospects and challenges are discussed in Appx. 5.\nSupervised learning from a welfare perspective. Given labeled training data of pairs (x, y)\nsampled iid from an unknown distribution D, the goal in supervised learning is to find a function h\nfrom a chosen class H whose predictions \u0177 = h(x) are accurate on future examples in expectation:\nargmaxheh ED[1{y = h(x)}]\nSince in our setting examples (x, y) are associated with individuals in the population D, then from\na welfare perspective, we can think of Eq. (2) as prescribing a particular social welfare function-a\nspecial case of Eq. (1)-in which: (i) utility to users derives from accurate predictions, (ii) all users\nshare the same utility function, u(x, y; \u03c0\u03b7) = 1{y = h(x)}, (iii) possible outcomes include correct\nand incorrect predictions, hence (iv) the policy is degenerate, u(x, y; \u03c0h) = u(x, y; h)), and finally\n(v) weights are uniform, w(x) = 1.\nFrom accuracy to welfare maximization. Despite an apparent 'neutrality', the above reveals that\naccuracy maximization in fact makes an (implicit) statement about social preferences. This entails a\nparticular notion of equity-one that derives operationally from the predictive task at hand, rather\nthan from a planned or designated social goal. But learning is of course not bound to maximizing\nobjectives only of the form of Eq. (2). If our true goal is to maximize welfare, then we should modify\nthe objective to support this goal. For example, if users differ in how much they benefit from accurate\npredictions, then we can plug in an appropriate utility function u; or if there are exogenous constraints\non predictions, then we can express these via the policy \u03c0\u03b7. Such steps are certainly possible, but\nintroduce challenges beyond those found in standard machine learning tasks. Towards this, we pave a\npath that gradually transforms Eq. (2) to support increasingly complex economic considerations."}, {"title": "3.1 Order 0: Allocating Accuracy", "content": "Accuracy will be beneficial to users in any a platform or service that relies on predictions; consider\nrecommendation systems, online market platforms, financial aid tools, or diagnistic health services.\nWe argue that in such cases, and as long as learning is prone to some level of error (which is plausible),\nthen accuracy itself is a limited resource\u2014simply because not all users can obtain the same level"}, {"title": "Challenge: Optimizing welfare objectives.", "content": "Maximizing welfare through learning presents new\nalgorithmic challenges. One task is to identify learners that optimize a given social welfare function-\nintrinsically, a question of identifying a desirable point on the Pareto front. Another task is to generate\nall points on the frontier [see, e.g., Navon et al., 2021]. One possible approach is to cast Eq. (3) as a\nproblem of distribution shift. Here shift can stem from several factors, including the welfare function\nweights, user utilities, and user inputs. But agency means that utilities can be misreported and\nfeatures manipulated; as a result, the way in which the distribution shifts becomes dependent on the\nchoice of classifier, indirectly through how users respond. This makes Eq. (3) a challenging instance\nof model-dependent distribution shift [Drusvyatskiy and Xiao, 2023]. Here the interaction between\nthe system and its users can be modeled as a Stackelberg game [Chen et al., 2020]. This perspective\nhas been useful for questions on learnability, e.g. via generalization bounds that rely on strategic VC\nanalysis [Zhang and Conitzer, 2021, Sundaram et al., 2023] or that accommodate feature-dependent\nutilities [Pardeshi et al., 2024]. Nonetheless, many important open questions remain."}, {"title": "3.2 Order 1: Incorporating System Decisions", "content": "Generally, predictions are useful for the system if they aid in making better decisions about uncertain\noutcomes. Since decisions in social settings often directly prescribe how to allocate limited resources\nto users, a useful next step is to encode them explicitly into the objective. We formalize this idea\nby modeling a system that makes decisions about users (e.g., who to hire) through predictions (e.g.,\nresume screening). Consider a user x with label y, and denote by a the action the system takes (e.g.,\nhire or not). Let r(a, y) be the reward for the system on this user given action a (e.g., the quality of the\ncandidate, if hired). The direct dependence of r on y means that if we know y, then we can write the\noptimal action as a* = \u03c0(y) for some policy \u03c0. Since y is generally unknown, the common approach\nis to replace y in \u03c0 with a prediction \u0177 = h(x), denoted a = \u03c0(h(x)) = \u03c0\u03b7(x), in hopes that better\npredictions translate to better decisions. Such policies, referred to as prediction policies [Kleinberg\net al., 2015], are appropriate when the uncertainty in y is a stronger factor for outcomes than potential\ncausal effects (we discuss this distinction further in Appendix 5.4). In this setting, choosing to work\nwith prediction policies gives reason for the system to optimize h for accuracy (Eq. (2)).\nFrom predictions to actions. We consider cases where there is a global restriction on the set of\nall actions a. These can express, for example, a limited number of available jobs (via cardinality\nconstraints), a total sum of funds that a bank can lend (knapsack constraints), regulation on the\namount of financial risk an insurer can take (bounded expected risk), or a limit on the number of\nposts a social platform can block to still enable free speech (lower-bounded rates). Given a set A of\nfeasible actions, the objective can be written as:\nargmaxheH ED[r(\u03c0\u03b7(x), y)] s.t. \u03c0\u03b7(D) \u2208 \u0391\nwhere \u03c0\u03b7 (D) is the set of actions over the entire population."}, {"title": "Agency.", "content": "Machine learning has many tools for coping with constraints. For example, if there is a\nquota on the number of available interview slots, then top-k classification [e.g. Lapin et al., 2015,\nPetersen et al., 2022] or ranking [e.g. Cao et al., 2007] seem like adequate solutions. The crux,\nhowever, is that such methods do not account for agency: since users seek to be 'allocated' (e.g.,\nget the interviews), they will likely try to present themselves as relevant (e.g. by tweaking their\nresume). A key point is that since users contend over resources, this introduces dependencies into\ntheir actions and subsequent outcomes. Such behaviors are ubiquitous in economics (e.g., costly\nsignaling, adverse selection), but have been underexplored in machine learning.\nSocial welfare. In terms of welfare outcomes, we make a distinction between two settings of interest:\n\u2022 An aligned setting in which r = welfare, and so the interests of the system align with societal\npreferences, e.g., as for government and nonprofit organizations. Note Eq. (3) becomes a special\ncase of Eq. (4) when utility derives from accuracy and A does not impose restrictions.\n\u2022 A misaligned setting in which r can be at odds with welfare, for example if it concerns revenue or\nuser engagement, as is more common in commercial settings. Here a social planner is needed to\nincentivize the learner to account for welfare outcomes, as we detail in Sec. 5.1."}, {"title": "Challenge #1: Decisions and externalities.", "content": "At Order 0, user responses are generally made\nindependently, and so the learning objective decomposes over examples. But once the system makes\ndecisions under constrained resources, user behavior can become intricately dependent. Consider for\nexample admissions under a limited quota: here applicants cannot simply pass a fixed acceptance\nthreshold, but instead, must surpass other applicants-i.e., the bar adjusts according to competition.\nThe tension for both decision-makers and applicants tends to focus on 'marginal' students; complex\ngaming behavior among such candidates is well-documented in the education literature [Bound\net al., 2009]. In economics, inter-user dependencies are known as externalities. One way to model\nhow learning creates externalities and coordinates user behavior is by casting the problem as a\nStackelbeg-Nash game [Liu, 1998], where the system plays first, and users respond collectively by\nplaying an induced simultaneous game (with externalities). Not much work has studied learning in\nthis challenging setting."}, {"title": "Challenge #2: Misaligned system and user objectives.", "content": "As we note, generally we cannot expect\nthe reward for the system to align with user interests. If no external forces intervene, then the\nchallenge is to learn a decision policy that is effective at the Stackelberg-Nash equilibrium. This\nnecessitates learning objectives that can express and anticipate how users will collectively respond\nto a deployed model. Contrarily, if there is regulation (e.g., on welfare or risk), then the system's\ndecision space becomes much more limited. An interesting question then is how to align incentives\nthrough regulation. For example, would requiring the system to optimize a weighted sum of accuracy\nand welfare help to promote equity? And if so, which weights, and by what means?"}, {"title": "3.3 Order 2: Enabling User Choices", "content": "Even if a system has the capacity to make decisions about users, often those very users will also have\nsome say regarding final outcomes. For example, even in hiring and admissions, candidates much\nfirst choose to apply, and if selected, choose to accept. Thus, both reward for the system and utility\nto users depend on outcomes that result from the interplay of system decisions and user choices.\nSimilarly to system decisions, we will model user choices as also depending on predictions, or more\ngenerally on the learned model h. Let C\u03c0\u03b7 (x) be the choice of user x in response to h. Incorporating\nuser choices into the system's objective gives:\nargmax ED[r(\u03c0\u03b7(X), C\u03c0\u03b7 (X), Y)] s.t. \u03c0\u03b7(D) \u2208 A\nheH\nwhere r now depends jointly on system decisions \u03c0\u03b7 and user choices Ch through the learned\npredictor h.\nAgency and resources. When users make choices, this makes them a part of the allocation\nprocess but also the reason for scarcity. Here we outline three scenarios of interest:\n\u2022 Self-selection: For most systems that makes decisions about users, those users must first choose\nto join the system; consider hiring, admissions, medical programs, educational aid, and welfare"}, {"title": "Social welfare.", "content": "A canonical property of classic markets is that they coordinate the behavior of many\nself-interested agents in a way which can naturaly lead to welfare maximization [e.g., Arrow and\nDebreu, 1954, Shapley and Shubik, 1971]. A key question in our context is whether this emerges\nalso in markets where coordination is mediated by predictions [Nahum et al., 2024]. Given the\ngrowing concerns regarding how recommendation systems may drive polarization, echo chambers,\ninformational barriers, economic inequity, and unhealthy usage patterns\u2014the answer is likely\nnegative. Welfare economics may be helpful in posing the question of why this happens as one\nof market failure. This provides tools for uncovering the mechanisms underlying failure, such as\nnegative externalities, public goods, information asymmetry, market power and control (e.g., by\nmonopolies), or collusion, as they manifest through learning."}, {"title": "Challenge #1: Escaping echo chambers.", "content": "Since its early days, the task of recommending content\nhas been treated as a pure prediction problem. But accumulating evidence of its likely ill effects\nhas motivated a search for more viable alternatives. A major issue is the reinforcing nature of\naccuracy-driven recommendation: users that are recommended certain items become 'locked in'\non similar content through a positive feedback loop of indefinite model retraining, choice behavior,\nand recommendation. One aspect of this loop that is often overlooked, and relevant to our context,\nis that recommendations also incentivize the creation of new content by exposure-seeking creators.\nRecommendation is essentially a problem of matching demand (of users) and supply (of new content)\nover a bi-partite user-content graph; thus, if we wish to prevent the formation of echo chambers and\nfilter bubbles, we should find ways to exploit this structure to promote socially-beneficial outcomes.\nSome examples include incentivizing exploration [Mansour et al., 2020] or the creation of diverse\ncontent [Eilat and Rosenfeld, 2023, Yao et al., 2024], but many other paths are possible."}, {"title": "Challenge #2: Learning to compete.", "content": "Once predictions are given as a service, it is only natural\nthat multiple providers will compete over who can give users better predictions. One example\nare housing market platforms such as Zillow and Redfin which compete over who provides better\npricing recommendations. To some extent, another example are LLMs who compete over better text\ngeneration. Competition is driven by the fact that users are free to choose a provider, but that choices\nare costly (e.g., time, premium features, opportunity costs). Thus, although users gain from accuracy,\noptimizing expected accuracy is likely not a good strategy for providers. As a result, welfare could\nsuffer. Welfare in such markets will be high if the market is efficient. This requires a planner that can\nsteer competing learners towards a favorable Nash-Stackelberg equilibrium, where users follow the\nsimultaneous moves of multilpe systems."}, {"title": "4 Alternative Views", "content": "While it is easy to agree that learning systems should be designed to enable the promotion of social\ngood, there will likely be disagreement as to how. We propose to adopt the perspective of welfare\neconomics, but there certainly exist alternative viewpoints and complementary approaches.\nGive accuracy time. One perspective is that if we give data enough time to accumulate and new\nmethods enough time to improve, then machine learning will organically overcome the challenges\nwe discussed. One example to draw on is how despite many advances in optimization, the simple\ngradient descent algorithm still drives most modern tools. Another is how large language models\nhave demonstrated that simply predicting the next word with sufficient accuracy and on enough data\ngives rise to emergent phenomena far beyond this basic task. Our position is that limited resources is\nan inherent problem of any social system, whether technology-driven or not. We believe that scarcity\nshould be addressed explicitly-but of course we may be proven wrong.\nDivide and conquer. Even if machine learning as a standalone solution does not suffice, one\ncould argue that an economic approach can be applied on top of existing learning tools, rather than\nintegrated within them as we propose. Hence, learning and policy can be advanced independently\nand combined only later. This is reasonable, and independent efforts and application will likely\nbe required regardless of whether direct integration works or not. But there is increasing evidence\nthat this will not suffice; in fact, the field of fair machine learning rose in response to the clear need\nfor embedding social considerations within the learning objective itself. Advances in the study of\nfairness in learning have also shown that fairness constraints alone cannot guarantee equity, such\nas when learning effects accumulate over time [e.g., Liu et al., 2018] or as a result of strategic user\nbehavior [Horowitz et al., 2024]. We take these to suggest that the novelty in the interface between\nlearning and economics requires a wholistic approach specialized for this intersection.\nWelfare without welfare. Welfare economics is not the only approach for reasoning about and\nfacilitating welfare, nor is it free from issues and limitations in itself. Criticism includes its subjective\nnature; the need to measure and compare utility across individuals; the emphasis on cardinal rather\nthan ordinal utilities; the reliance on assumptions of rational behavior; the susceptibility to externali-\nties and other sources of market failure; the need for a centralized social planner entity; and challenges\nin policy evaluation. Other schools of thought in economics offer alternatives: For example, Sen's\ncapabilities approach [Sen, 1999] focuses on ensuring people are capable of achieving what they\nseek, rather than the value of what they obtain. Within machine learning, there been have calls for\nalternative approaches as well, such as to \u2018democratize' the issue of alignment using social choice the-\nory [Conitzer et al., 2024, Ge et al., 2024, Fish et al., 2024]. We view these as complementary to ours,\nand believe there is merit advancing welfare in machine learning simultaneously along several fronts."}, {"title": "5 Further Prospects and Challenges", "content": "Sec. 3 lays out the main goals and principles of our framework, along with possible avenues to pursue\nwithin each of its three orders. But there are further opportunities, as well as challenges, that lie\nbeyond the framework's core and are of both interest and significance. In this section we highlight\nsome of these directions."}, {"title": "5.1 Regulation: Social planner, revisited", "content": "Our perspective of a social planner so far has been that of a useful construct for defining the social\npreferences that should guide learning. As such", "example": "n\u2022 Monitoring. One defining characteristic of data-driven systems is that they hold a distinct\ninformational advantage over their users. In economics, information asymmetry is a well-known\nsource of hazard for markets, with two prominent issues known as moral hazard [Holmstr\u00f6m,\n1979", "1978": ""}]}