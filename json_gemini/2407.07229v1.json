{"title": "Using Galaxy Evolution as Source of Physics-Based Ground Truth for Generative Models", "authors": ["Yun Qi Li", "Tuan Do", "Evan Jones", "Bernie Boscoe", "Kevin Alfaro", "Zooey Nguyen"], "abstract": "Generative models producing images have enormous potential to advance discoveries across scientific fields and require metrics capable of quantifying the high dimensional output. We propose that astrophysics data, such as galaxy images, can test generative models with additional physics-motivated ground truths in addition to human judgment. For example, galaxies in the Universe form and change over billions of years, following physical laws and relationships that are both easy to characterize and difficult to encode in generative models. We build a conditional denoising diffusion probabilistic model (DDPM) and a conditional variational autoencoder (CVAE) and test their ability to generate realistic galaxies conditioned on their redshifts (galaxy ages). This is one of the first studies to probe these generative models using physically motivated metrics. We find that both models produce comparable realistic galaxies based on human evaluation, but our physics-based metrics are better able to discern the strengths and weaknesses of the generative models. Overall, the DDPM model performs better than the CVAE on the majority of the physics-based metrics. Ultimately, if we can show that generative models can learn the physics of galaxy evolution, they have the potential to unlock new astrophysical discoveries.", "sections": [{"title": "Introduction", "content": "The intersection of high dimensional data and machine learning in fields such as particle physics, genomics, and astrophysics has seen huge advances in the ability to extract meaningful information and patterns from complex data. Image data, in particular, is one example of data with large numbers of features. Advances in machine learning have high potential to advance scientific discovery in astrophysics, which amasses large amounts of high dimensional data including multi-wavelength images, spectra, and time series (> 10\u2079 objects forthcoming in large-scale surveys over the next decade). Generative models have risen to the fore to study relationships among features and statistical structures of distributions behind audio, image or video data."}, {"title": "Related Works", "content": "In recent years, astronomers have been testing generative models such as CVAE and DDPM to evaluate their ability to produce galaxy images with good pixel statistics and visual quality (Fussell & Moews, 2019; Lanusse et al., 2021; Smith et al., 2022). We expand this approach by focusing on the physics governing the appearance of these galaxies over time using generative models.\nRavanbakhsh et al. (2017) employed conditional variational autoencoders (CVAEs) and conditional generative adversarial networks (CGANs) to simulate images from the GALAXY-ZOO and Hubble Space Telescope Advanced Camera for Surveys (HST/ACS) COSMOS survey. They conditioned these models on parameters including the half-light radius, magnitude, and redshift of galaxies, allowing the models to focus on the visual quality of the generated images. The generated dataset is evaluated using pixel statistics. This study found that CVAEs are useful tools to denoise galaxy images.\nFussell & Moews (2019) advanced this approach by simulating galaxy image data using chained generative adversarial networks, a technique inspired by the StackGAN text-to-image model (Zhang et al., 2016). They evaluated their generated datasets by comparing the distributions of the physical properties of the galaxies. Differing from the approach of Ravanbakhsh et al. (2017), Fussell & Moews (2019)'s models are not conditioned on physical parameters. They conclude that state-of-the-art generative models are useful for astronomy, but they did not examine the relationships of the generated features or as a function of the age of the galaxy.\nLanusse et al. (2021) developed variational autoencoders for simulating images of galaxies selected from the HST/ACS COSMOS survey and evaluating the generated galaxies based on their physical properties. They evaluated the generative models in terms of the statistical relationship between the features, such as between the ellipticity of galaxies and their brightness.\nMore recently, Smith et al. (2022) utilized denoising diffusion probabilistic models to simulate galaxy images, focusing on the visual qualities of the generated galaxies. Using DESI as the dataset source, they selected galaxies that are bright and nearby (redshift < 0.08) with clearly defined galaxy features for each image. Their model generated galaxy images that are visually indistinguishable from real galaxy images based on human evaluation. Smith et al. (2022) evaluated their generated dataset using the FID metric. They also explore DDPM's ability to regenerate images, through a full forward and reverse process. They proposed the Synthetic Galaxy Distance metric, which quantifies the difference between the regenerated image and the original image based on the galaxy's size and brightness.\nRecently, Yin et al. (2022) explored using conditional autoencoders conditioned on the mean value of the real image to generate new instances of the galaxy image. Their goal is to segment the original input galaxy images through passing them through the CAE, and interpret galaxy parameters from the latent space of the CAE model. They evaluated their models based on comparing the latent space extracted parameters to the measured values.\nOur study builds on previous approaches by exploring machine learning methods capable of reproducing galaxies with both accurate physical properties and high visual fidelity, conditioned solely on"}, {"title": "Evaluation Metrics for Generative Models", "content": "It is a challenge to quantitatively evaluate generated images because patterns in images are often subject to deep underlying relationships that are not quantifiable with pixel-based statistics. Previous research has produced metrics that correlate well with human perception. These metrics include the IS (IS) (Salimans et al., 2016) and the Frechet Inception Distance (FID) (Heusel et al., 2018). However, these metrics become less indicative when selecting generative methods for scientific use. Moreover, for cases where humans cannot assess the quality of generated images, or distinguish generated images from real ones, it is questionable to use a metric that corresponds well with human judgement. To address this gap in evaluation methods, we propose our metrics based on galaxy image data. In this study, we use galaxy images as a new form of ground truth in generative models of images. Galaxies possess deep patterns just like ordinary objects. Some of these patterns, such as the relationship between the flux and redshift, are difficult to perceive even for humans. Conversely, galaxies can be parametrized for their physical properties and quantitatively evaluated. Improving the physics of generated galaxies is more objective than metrics of human perception, and therefore, generated galaxies based metrics are more useful for scientific use."}, {"title": "Methods", "content": "We develop metrics to compare the distribution of physical properties of galaxies generated by our models to those of real images to assess how physically realistic the generated images are. We measure galaxy features from both the generated and real images in the same way, then compare the distribution of these properties using the KL divergence and what we term the galaxy fitting loss. These galaxy properties are chosen to represent physically meaningful features of galaxy evolution. Galaxies today have a larger range of sizes and shapes than galaxies from long ago (Conselice, 2014). In addition, we also put the generated galaxy images through a trained CNN model that can predict galaxy redshifts to examine how well the generated image has been conditioned on redshifts. We use this comparison to create a redshift loss. We describe these metrics in detail below."}, {"title": "Fitting Individual Galaxies", "content": "Source Extractor, a common astrophysical image analysis tool, is used to measure features of galaxies connected the physics of their formation and evolution (Bertin & Arnouts, 1996). Source Extractor first identifies sources in the image using a 2D Gaussian fit and detects sources above > 3\u03c3 above the background RMS pixel values to produce an image segmentation map. In this work, we concentrate on three parameters that describe the shape and flux distribution of the central galaxy: isophotal area, ellipticity, and Sersic index. Isophotal area is the number of pixels with values exceeding a threshold above the background (> 1.5 \u03c3). The ellipticity measures its shape in terms of a fit to an ellipse e=1-(b/a), where a is the semi-major axis and b is the semi-minor axis. Sersic profile measures how the intensity (I, flux per unit area) is distributed in the galaxy as a function of projected radius from the galaxy center:\nI(R) = I_e \\cdot \\exp \\left\\{ -b_n \\left[ \\left(\\frac{R}{R_e}\\right)^{\\frac{1}{n}} - 1 \\right] \\right\\} \\qquad(1)\nwhere R\u2091 is the half-light radius, I\u2091 is the intensity at that radius, and n is the Sersic index that measures how concentrated the light is at the center (larger values of n means more centrally concentrated)."}, {"title": "KL Divergence", "content": "The first metric, the galaxy Kullback-Leibler (KL) loss, quantitatively compares the distribution of physical parameters from the generated galaxies to those from real images. We use the Source Extractor output for each of isophotal area, ellipticity, and Sersic index to obtain the probability distribution for each parameter at different redshift, and calculate the KL divergence between these distributions. Since these parameters evolve with redshift (time), we split each distributions into 10 bins between the minimum and the maximum value to calculate the KL divergence. The galaxy KL loss is the average for the KL divergence across all parameters and bins.\nKL \\text{ Loss } = \\frac{1}{N_{\\text{params}}} \\sum_{i=1}^{N_{\\text{params}}} \\frac{1}{N_{\\text{bins}}} \\sum_{j=1}^{N_{\\text{bins}}} KL\\left(P_{\\text{real}, i, j}, P_{\\text{gen}, i, j}\\right) \\qquad(2)\nIn practice, we find it is more meaningful to compare the ratio of the KL divergence between each model and the real data."}, {"title": "Galaxy-fitting Loss", "content": "The second metric galaxy-fitting loss aims to measure the irregularity of the generated galaxies. Source Extractor code we use for fitting model intensity distributions to the images is able to also estimate uncertainties in the model parameters as well as the fit residuals. We find that such uncertainties correlate well with the noise-to-signal ratio of the image, or the irregularity of the generated galaxy. A combined uncertainty is calculated within Source Extractor (internally called the ERRCXY_IMAGE parameter). We define the galaxy-fitting loss the mean log of the \"ERRCXY_IMAGE\u201d values. The choice to take the mean after the log is to reduce the effects of outlier values:\n\\text{Galaxy Fitting Loss } = \\frac{1}{N} \\sum_{i=1}^N \\log(\\text{``ERRCXY\\_IMAGE''}_i) \\qquad(3)\nWe use the noise residuals from fitting real galaxies as a baseline for the expected loss."}, {"title": "Redshift Loss", "content": "We define the redshift loss as the difference between the redshift the generated models are conditioned on, compared to the redshift encoded in the generated images.\n\\text{Redshift Loss } = \\frac{1}{N} \\sum_{i=1}^N \\left( 1 - \\frac{1}{1 + \\left(\\frac{z_{\\text{pred}, i} - z_{\\text{cond}, i}}{0.15}\\right)^2 } \\right) \\qquad(4)\nWhere z\u209a\u1d63\u2091\ud835\udcb9 is the CNN predicted redshift (the redshift encoded in the images), and z\ud835\udcb8\u2092\u2099\ud835\udcb9 is true redshift (the redshift conditioned at time of image generation). The form of this loss is defined in Nishizawa et al. (2020). In this case, the redshift discriminator measures the difference between the redshift the model is asked to produce and the redshift it actually recovered in the generated images. The redshift loss is compared across the range of redshifts for model generated images and the real dataset.\nRedshift loss quantifies the generative models' ability to recover the relationship between the redshift of galaxies and their appearance. To compute redshift loss, we use a pre-trained convolutional neural network (CNN) redshift discriminator (Jones et al. in prep.). This approach is similar to the IS, where a CNN is employed for the metric. However, since the predicted redshift is a physically motivated value value, we can directly compare and measure the bias, scatter, and outlier rate between the conditioned values and the actual values encoded."}, {"title": "IS and Fr\u00e9chet Inception Distance", "content": "We compute the IS (IS) and the Fr\u00e9chet Inception Distance (FID) using the definitions from Salimans et al. (2016) and Heusel et al. (2018) (see Appendix B). Both metrics require feeding the data into an Inception V3 model with the input of 3-channel RGB images with size of 299 \u00d7 299 pixels. To use the galaxy images, which consists of 5 filters, we create five 3-channel images by duplicating each filter into the 3 channels. Each image is super-sampled from 64 \u00d7 64 pixels to 299 \u00d7 299 pixels using the \"cv2.INTER_LINEAR\u201d function. Then, we compute the IS and FID for each of the five images and average the results. We compute the FID for the testing dataset and both generated datasets against the validation dataset (2,000 real galaxy images)."}, {"title": "Datasets: Galaxy Images", "content": "We adopt our dataset from Do et al. (2024), based on the second data release of the Hyper Suprime-Cam survey (Aihara et al., 2019). The Hyper Suprime-Cam survey is a deep sky survey covering a wide range of galaxy (from those observed today to galaxies from 12 billion years ago), allowing us to explore galaxy evolution relationships in a large timescale. The data also only encompasses images in the visible and near-infrared band, increasing the challenge to correctly condition the redshift information. At last, the dataset we use is not truncated or normalized, in order to present the full range of physical properties to the model.\nThe dataset consists of 286, 401 galaxies spanning redshifts between 0 and 4, with each galaxy captured with images in five visible light bands (g, r, i, z, y filters) and represented with 64 \u00d7 64 pixels. Each galaxy has a true redshift (age) label assigned to it, from accurate spectroscopic measurements. Due to bias in the selection process, our dataset is skewed toward galaxies at lower redshifts (with 92.8 percent of galaxies with redshift < 1.5). This poses an additional challenge for the generative models - the model is likely to produce worse images for galaxies beyond redshift of 1.5. This dataset is available in Do et al. (2024) at https://zenodo.org/records/11117528.\nThe training dataset that we train the generative models on consists of 204, 513 galaxies, while the testing dataset for evaluating galaxy metrics (KL divergence, galaxy-fitting loss, and redshift loss) consists of 40914 galaxies. The IS and FID for the real dataset are evaluated on a subset of the testing dataset with 2,000 galaxies distributed uniformly across the bins shown in Figure 5. We also sample a validation dataset of 2, 000 galaxies with uniform redshift as the ground truth for the FID metric from the remaining 40914 galaxies."}, {"title": "Models", "content": ""}, {"title": "Conditional Denoising Diffusion Probabilistic Model", "content": "To create a conditional DDPM, we modified and added a condition mechanism to the DDPM developed by Ho et al. (2020). We include the conditional component using an approach similar to Mirza & Osindero (2014), we reshaped redshift input to match the input dimensions of each down-sampling and upsampling blocks of the U-Net. This reshaped hidden representation is concatenated with the feature map input of each down-sampling and upsampling blocks. The forward process follows the non-conditional x_t = \\sqrt{1 - \\beta_t} x_{t-1} + \\sqrt{\\beta_t} \\epsilon_{t-1}; where \\epsilon_{t-1} \\sim N(0, I). \u03b2_t is the series of hyperparameters defining the strength of noise added at each step (Ho et al., 2020). We sample during the reverse process from a modified distribution, which the model predicts the added noise based on the additional redshift input z (Lee et al., 2022):\np(x_{t-1}) = \\frac{1}{\\sqrt{\\alpha_t}} \\exp{\\left(\\frac{ - \\left(x_{t-1} - \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_{\\theta}(x_t, t, z)\\right)\\right)^2 }{2\\sigma_t^2}\\right)} + N(0, \\sigma_t) , \\qquad(5)\nwhere we assume \u03c3\u00b2\u209c = \u03b2\u209c. \u03b1\u209c and \u03b1\u0304\u209c are series precomputed from the noise schedule \u03b2\u209c (Sohl-Dickstein et al., 2015; Ho et al., 2020). \\epsilon_\\theta(x_t, t, z) is the DDPM model predicted value of \\epsilon_t sampled during the forward process. During training, we minimize the mean squared error between \\epsilon_\\theta(x_t, t, z) and \\epsilon_t. \\epsilon_\\theta(x_t, t, z) is dependent on the redshift (z) allowing the model to be conditioned to generate galaxies at a given redshift. Appendix A shows the model architecture and details."}, {"title": "Conditional Variational Autoencoder", "content": "Our CVAE uses a typical design with an encoder and a decoder, both of which are conditioned on the galaxy redshift. The encoder consist of a convolutional neural network (CNN) with the five mutli-wavelength images and the redshift of the galaxy as inputs to learn a latent probability distribution. This CNN uses techniques adopted from the HD-CNN (Yan et al., 2015), where features extracted from different levels of convolution are concatenated. We condition the redshift along with these flattened feature maps, and then pass the combination through dense layers. The encoder produces a 64-variable normal distribution, which is the vector for the latent posterior distribution. The decoder takes the latent vector and the condition to output 5 images conditioned on the redshift.\nThe decoder mirrors the encoder in terms of the dimensions at each convolutional step. The decoder inputs consists of the redshift and a latent vector sampled from the latent posterior distribution. The concatenated input is passed through dense layers, and then to an inverted HD-CNN architecture which produces the original image. Appendix X shows the model architecture and details.\nWe train the CVAE by of maximizing the variational lower bound as described in Sohn et al. (2015), expressed as:\nlog p_\\theta(y|x) \\geq - KL(q_\\phi(z|x, y)||p_\\theta(z|x)) + E_{q_\\phi(z|x, y)} [log p_\\theta(y|x, z)] \\qquad(6)\nIn this expression, x is the input condition, z is the latent space representation, and y is the output image [Sohn 2015]. The encoder output is q_\\phi(z|x, y), while the decoder output is log p_\\theta(y|x, z). KL-Divergence regulates the latent distribution. For our model, we assume a Gaussian prior p_\\theta(z|x). The weighted average of the log likelihood in the second term is approximated in the random sampling process of decoder input. The loss function composes of the mean squared error between the input"}, {"title": "Results", "content": "The CVAE and the DDPM both generate images with visually realistic central galaxies, but the DDPM generates fewer artifacts and more realistic background properties. We generated 2,000 images from the CVAE and 2, 000 images with the DDPM conditioned on redshift values drawn from a uniform distribution between 0.001 and 4. Qualitatively, the generated galaxies are larger (larger isophotal area) at lower redshifts (more recent lookback time) and are more compact at higher redshifts when galaxies have had less time to evolve and interact. However, the CVAE tend to generate more extended structures around the central galaxy at high redshifts than the DDPM. These structures are not in real images of galaxies at high redshifts. In addition, the DDPM images have much more realistic background properties than the CVAE. The background in real astronomical images are the result of sky and detector noise, which generally have Gaussian or Poissonian distribution in pixel values. The background pixels in real images are also usually uncorrelated. The CVAE produces structure and correlated pixels in the background which are not present in the original images. This is more apparent when the image pixel values are displayed on a log scale. The DDPM is able to produce more realistic random background properties likely because the training process uses uncorrelated Gaussian noise.\nThe quantitative galaxy metrics also generally agree with the qualitative visual assessment and contribute a more reliable and interpretable assessment of the models. As a galaxy evolves over billions of years, it grows to become more diverse in shape and size as it interacts with its environment and other nearby galaxies. In addition, the light distribution and color of the galaxy also evolves with time as its stars form, grow old, and die. These characteristics of galaxy evolution result in a large dispersion in the distribution of isophotal area, ellipiticity, Sersic index for real galaxies at low redshifts (closer lookback time) and sharper distributions at high redshift (further lookback time). The DDPM is better than the CVAE at producing the distribution of physical properties of the galaxies for redshifts > 0.5, but CVAE is better matched at redshift < 0.5. At the lowest redshift bins (from 0 to 0.5), the DDPM images have smaller galaxies than those found in the real images. However, beyond redshift 0.5, compared to the CVAE, the DDPM produces galaxies with distributions of isophotal area, ellipiticity, Sersic index much closer to the real galaxies (Fig. 4 & 12).\nThe KL divergence of the distribution of the real galaxy properties compared to the generated sample can be used to evaluate how well the models reproduce galaxy properties as a function of redshift. The KL divergence for the distribution in isophotal area is a factor of 3 worse for the DDMP model compared to the CVAE for redshifts < 0.5. However, for redshifts > 0.5 the DDMP is a factor of 10 to 100 better than the CVAE at producing the distribution of isophotal area seen in real galaxies; the CVAE tends to produce larger galaxies than measured in real images. This is in line with our visual inspection of the images of high redshift galaxies generated by the CVAE, which show unusual extended structures around the central galaxy. The ellipticity and the Sersic index also show similar trends with redshift. We note however that while the CVAE is producing galaxies closer to the real data at low redshifts (< 0.5), both models cannot yet produce the true diversity seen in real galaxies for this range of redshift. At higher redshifts (> 0.5), the DDPM produces galaxies with properties much closer to real galaxies.\nThe DDPM model outperforms the CVAE model in terms of the galaxy-fitting loss metric. The DDPM model shows a galaxy-fitting loss close to the true dataset. This means that the DDPM generated galaxies share a closer level of irregularity and noise-to-signal ratio as real galaxy images than the CVAE images. The CVAE model has a high galaxy-fitting loss over all the ranges, although at a lower redshift the loss is smaller.\nBoth the DDPM and CVAE models perform poorly in the redshift loss metric. The galaxies generated by these models are not predictive of the redshift they were conditioned on. The difference between the conditioned redshift and the predicted redshift by the CNN model can be greater than 1 (which"}, {"title": "Discussion & Conclusion", "content": "As we have demonstrated using the CVAE and the DDPM models, galaxy images can be used as a physics-based ground truth for generative models, and galaxy metrics offer additional important ways to evaluate model performance. Using galaxy metrics allow us to quantitatively and objectively evaluate a generative model's capability. The structural properties of galaxies are simple enough that they can be modeled and quantified in both real and generated images. Importantly, the physics of galaxies are complex enough to produce enough diversity that it can challenge the ability of generative models to produce realistic galaxy images over time (redshift).\nWhile the CVAE and DDPM produce visually very similar galaxies based on human inspection, the physics based metrics show the limitations of these models. From galaxy metrics, the DDPM model performed better in reproducing physics based features at higher redshifts (> 0.5), while the CVAE produced more realistic galaxies at lower redshifts (< 0.5). However, both models were not able to produce images of galaxies that encode accurate redshift values. In addition, neither model"}, {"title": "Models", "content": ""}, {"title": "Conditional Denoising Diffusion Probabilistic Model", "content": "As described in Section 3.8.1, our conditional DDPM model is built by adding a condition mechanism to the DDPM framework developed by Ho et al. (2020).\nThe forward process is non-conditional and identical to the mechanisms presented in Ho et al. (2020). We use a fixed linear noise schedule with \u03b2\u2081 = 10\u207b\u2074 and \u03b2_T = 0.02, for T = 1000. For each timestep, we add noise to the image by x_t = \\sqrt{1 - \\beta_t} x_{t-1} + \\sqrt{\\beta_t} \\epsilon_{t-1}; where \u03b5_{t-1} \u223c N(0, I). For un-normalized galaxy images, the large dynamic range is effectively reduced in the noising process, and at T = 1000 we obtain a Gaussian random pixel distribution.\nAs described by Sohl-Dickstein et al. (2015), property of the Markov chain allows the direct computation of a timestep x_t = \\sqrt{\\bar{a}_t} x_0 + \\sqrt{1 - \\bar{a}_t} \u03b5, with a_t = 1 - \\beta_t and \\bar{a}_t = \\Pi_{i=1}^t a_i. In the forward step, we do not recursive noise an image.\nThe reverse process is done recursively, with each iteration we predict the probability distribution of x_{t-1}, given the current image x_t, timestep t and redshift z. We assume this probability distribution is Gaussian, and has a standard deviation \u03c3_t = \\sqrt{\\beta_t} (Ho et al., 2020). The probability distribution can therefore be expressed as:\np_\\theta(x_{t-1}|x_t) = N(x_{t-1}; \u03bc_\\theta(x_t, t, z), \u03c3_t) \\qquad(7)\nWe are interested in knowing the mean value of the distribution \u03bc_\\theta(x_t, t). (Ho et al., 2020) derives from the forward process formula that\n\u03bc_\\theta = \\frac{1}{\\sqrt{a_t}} \\left(x_t - \\frac{1 - a_t}{\\sqrt{1 - \\bar{a}_t}} \u03b5_t\\right) \\qquad(8)\nTherefore, the problem becomes predicting the theoretical value of \u03b5_t as if the forward process is done recursively. Once we train a model to achieve this, we can carry out the conditional DDPM process and sample x_{t-1} from the following distribution. This formula is also presented by Lee et al. (2022) for a conditional DDPM model.\np(x_{t-1}) = \\frac{1}{\\sqrt{a_t}} \\left(x_t - \\frac{1 - a_t}{\\sqrt{1 - \\bar{a}_t}} \u03b5_\\theta(x_t, t, z)\\right) + N(0, \u03c3_t), \\qquad(9)\nOur model predicts \u03b5_\\theta(x_t, t, z), which should be as close as possible to \u03b5_t. We employ a U-Net architecture for this task (Ronneberger et al., 2015), and add conditioning mechanism to the U-Net using the approach of conditional GANs (Mirza & Osindero, 2014). Our U-Net has 5 \u00d7 64 \u00d7 64 inputs with five feature map resolutions. We use standard practices such as employing transformer sinusoidal position embedding for the noise step (time) condition (Vaswani et al., 2017), adding 4-head self-attention mechanism at each upsampling or downsampling block, or the \u201cmidblock\u201d, and accompanying convolution layers throughout the network with group normalization layers (Wu & Peek, 2020)."}, {"title": "Conditional Variational Autoencoder", "content": "As described previously, our CVAE adopts a classic structure of an encoder and a decoder. We experimented with different variations and settled on a convolution layer arrangement similar to the one adopted in Yan et al. (2015), where each convolution layer in the sequence has its own skip layer"}, {"title": "Evaluation Metrics for Generative Models based on Human Perception", "content": "The IS (IS) and the Fr\u00e9chet Inception Distance (FID) are efficient to calculate, and quantitatively are able to show the differences in model performances. In this work, we compare them with the metrics that we propose.\nIn light of developments in generative models, especially GANs, Salimans et al. (2016) argued that there is a lack of \"objective analytical function\" that evaluates generative models of images. They proposed the IS (IS) as an analytic metric that corresponds well to human perception. The IS is mathematically represented as:\nIS(G) = exp (E_{x\u223cpg} [D_{KL} (p(y|x) || p(y))])\nThe generated dataset is passed through the Inception model, which produces a probability distribution of the label for the input image. The KL divergence measures the difference between the probability distribution of labels over the whole dataset and the probability distribution of the labels evaluated on only the current input image x. The average KL divergence across all generated data is calculated, and the IS is calculated as the exponent of this value. A high IS strongly indicates a generative model's ability to produce images that are rated positively by human evaluators.\nThe FID (Fr\u00e9chet Inception Distance) is an improvement to the IS, which addresses the IS's limitation of not contrasting the statistics of real-world images with those of generated images. Introduced by Heusel et al. (2018), FID has been shown to be more sensitive to Gaussian disturbances compared to IS. The FID is calculated by first transforming both the original and generated datasets into feature vectors using the Inception model. The mean and covariance matrices of the real (\u03bcr, \u03a3r) and generated (\u03bcg, \u03a3g) feature vectors are then used to compute the FID:\nFID = ||\u03bcr \u2212 \u03bcg||\u00b2 + Tr(\u03a3r + \u03a3g \u2212 2(\u03a3r\u03a3g)^(\u00b9/\u2082))\nWhere Tr is the trace of the matrix. The formula assumes that the feature vectors follow multidimen-sional Gaussian distributions and calculates the Fr\u00e9chet Distance between these distributions.\nFurthermore, several other methods have been suggested for assessing the quality of images, such as the AM score (Zhou et al., 2017) and the Wasserstein Critic (Arjovsky et al., 2017). These metrics as well aim to either find an analytic value that correlates with human perception or to statistically compare the pixel values of generated and real images. However, it is questionable how objective a measurement that correlates well with human judgement means, especially for cases where humans cannot distinguish between real and generated images. To address this gap in evaluation methods, we propose our metrics based on galaxy image data.\nDespite their widespread successes, the difficulty to draw a direct line between human-evaluation based metrics and quantitative evaluations gives both metrics inherent limitations when used for scientific applications. Chong & Forsyth (2020) demonstrate that when the IS and FID are evaluated on finite datasets, the mean value, or expected value, do not align with their theoretical true values. Furthermore, the discrepancy between expected and true values is model-dependent, leading to potentially unreliable comparisons across different models. Barratt & Sharma (2018) show that certain artificially constructed distributions, which differ strongly from real data, can yield higher IS than even the true distributions. They conclude that the IS and human perception are not always positively correlated, and also demonstrate that the IS can be misleading when applied to non-ImageNET based models. Such limitations prevent objective evaluation of generative models, when used scientifically, and require new metrics and evaluations to fill in the gap."}]}