{"title": "Enabling Adaptive Agent Training in Open-Ended Simulators by Targeting Diversity", "authors": ["Robby Costales", "Stefanos Nikolaidis"], "abstract": "The wider application of end-to-end learning methods to embodied decision-making domains remains bottlenecked by their reliance on a superabundance of training data representative of the target domain. Meta-reinforcement learning (meta-RL) approaches abandon the aim of zero-shot generalization\u2014the goal of standard reinforcement learning (RL)\u2014in favor of few-shot adaptation, and thus hold promise for bridging larger generalization gaps. While learning this meta-level adaptive behavior still requires substantial data, efficient environment simulators approaching real-world complexity are growing in prevalence. Even so, hand-designing sufficiently diverse and numerous simulated training tasks for these complex domains is prohibitively labor-intensive. Domain randomization (DR) and procedural generation (PG), offered as solutions to this problem, require simulators to possess carefully-defined parameters which directly translate to meaningful task diversity-a similarly prohibitive assumption. In this work, we present DIVA, an evolutionary approach for generating diverse training tasks in such complex, open-ended simulators. Like unsupervised environment design (UED) methods, DIVA can be applied to arbitrary parameterizations, but can additionally incorporate realistically-available domain knowledge thus inheriting the flexibility and generality of UED, and the supervised structure embedded in well-designed simulators exploited by DR and PG. Our empirical results showcase DIVA's unique ability to overcome complex parameterizations and successfully train adaptive agent behavior, far outperforming competitive baselines from prior literature. These findings highlight the potential of such semi-supervised environment design (SSED) ap-proaches, of which DIVA is the first humble constituent, to enable training in realistic simulated domains, and produce more robust and capable adaptive agents. Our code is available at https://github.com/robbycostales/diva.", "sections": [{"title": "1 Introduction", "content": "Despite the broadening application of reinforcement learning (RL) methods to real-world problems [1, 2], generalization to new scenarios\u2014ones not explicitly supported by the training set-remains a fundamental challenge [3]. Meta-reinforcement learning (meta-RL), an extension of the RL frame-work, is formulated specifically for training adaptive agents, and is thus well-suited for overcoming these generalization gaps [4]. One recent work has demonstrated that meta-RL agents can be trained at scale to achieve adaptation capabilities on par with human subjects [5]. However, learning this human-like adaptive behavior naturally requires a large amount of data representative of the down-stream (or target) distribution. For task distributions approaching real-world complexity-precisely the ones of interest-designing each scenario by hand is prohibitively expensive."}, {"title": "2 Preliminaries", "content": "Meta-reinforcement learning. We use the meta-reinforcement learning (meta-RL) framework to train adaptive agents, which involves learning an adaptive policy \u03c0_\u03b8 over a distribution of tasks T."}, {"title": "3 Problem setting", "content": "One assumption underlying UED methods is that random parameters\u2014or parameter perturbations for ACCEL -produce meaningfully different levels to justify the expense of computing objectives on each newly generated level. However, when the genotype is not well-behaved-when meaningful di-versity is rarely generated through random sampling or mutations\u2014these algorithms waste significant time evaluating redundant levels. In our work, we discard the assumption of well-behaved genotypes in favor of making fewer, more realistic assumptions about complex environment generators. There are several assumptions we make about the simulated environments DIVA has access to.\nGenotypes. We assume access to an unstructured environment parameterization function E\u03c5 (\u03b8), where each @ is a genotype (corresponding to the QD solutions \u03b8\u00bf) describing parameters to be fed into the environment generator. QD algorithms can support both continuous and discrete genotype spaces, and in this work we evaluate on domains with both kinds. Crucially, we make no assumption of the quality of the training tasks produced by this random generator. We only assume that (1) there is some nonzero (and for practical purposes, nontrivial) probability that this generator will produce a valid level for training-one in which success is possible and positive rewards are in reach; and (2) that it is computationally feasible to discover meaningful feature diversity through an intelligent search over the parameter space-an assumption implicit in all QD applications."}, {"title": "Features.", "content": "We assume access to a pre-defined set of features, S = f(Rn), that capture axes of diversity which accurately characterize the diversity to be expected within the downstream task distribution. It is also possible to learn or select good environment features from a sample of tasks from the downstream distribution, which we discuss in Section 7. For the sake of simplicity, we use a grid archive as our tessellation G, where the k dimensions of the discrete archive correspond to the defined features. The number of bins for each feature is a hyperparameter, and can be learned or adapted over the course of training. We generally find it to be helpful to use moderately high resolutions to ease the search, since smaller leaps in feature-level diversity are required to uncover new cells. By default, we use 100 sample feature values across all domains, but demonstrate in ablation studies that that significantly fewer may be used (see Appendix C)."}, {"title": "4 DIVA", "content": "DIVA assumes access to a small set of feature samples representative of the target domain. It does not, however, require access to the underlying levels themselves. This is a key distinction, as the former is a significantly weaker assumption. Consider the problem of training in-home assistive robots in simulation with the objective of adapting to real-world houses. It is more likely we have access to publicly available data describing typical houses\u2014dimensions, stylistic features, etc. than we have access to corresponding simulator parameters which produce those exact feature values.\nFeature density estimation. DIVA begins by constructing a QD archive with appropriate bounds and resolution. Given a set of specified features {fi}k and a handful of downstream feature samples, we first infer each feature's underlying distribution. These can be approximated with kernel density estimation (KDE), or we can work with certain families of parameterized distributions. For our experiments, we assume each feature is either (independently) normally or uniformly distributed. We use a statistical test2 to evaluate the fit of each distribution family, and select the best-fitting. Setting the resolution for discrete feature dimensions is straightforward, and depends only on the range. For continuous features, the resolution should enable enough signal for discovering new cells, while avoiding practical issues that arise with too many cells\u00b3. See Section 5 for domain-specific details.\nTwo-stage QD updates. Once the feature-specific target distributions are determined, we can use these to set bounds for each archive dimension. A na\u00efve approach would be to set the archive ranges for each feature based on the confidence bounds of the target distribution. However, random samples from Eu may not produce feature values that fall within the target range. We found this to be a major issue in the ALCHEMY domain (see Figure 2), and for some features in RACING. We solve this problem by setting the initial archive bounds to include both randomly generated samples from Eu, as well as the full target region. As the updates progress, we gradually update the sample mask\u2014which is used to inform the sampling of new solutions\u2014towards the target region. We observe empirically that updating and applying this mask provides an enormous speed-up in guiding solutions towards the target region (see Figure 15). After this first stage, solutions are inserted into a new archive defined by the proper target bounds. See Appendix A for more specifics on these two QD update stages.\nOverview. DIVA consists of three stages. Stage 1 (S1) begins by initializing the archive with bounds that include both the downstream feature samples (the target region), as well as the initial population generated from \u0395\u03c5(\u03b8). S1 then proceeds with alternating QD updates, to discover new solutions, and sample mask updates, to guide the population towards the target region. In Stage 2 (S2), the archive is reinitialized with existing solutions, but is now bounded by the target region. QD updates continue to further diversify the population, now targeting the downstream feature values specifi-cally. The last stage is standard meta-training, where training task parameters are now drawn from P\u00e7(\u03b8), a distribution over the feature space approximated using the downstream feature samples, discretized over the archive cells. See Appendix A for detailed pseudocode."}, {"title": "5 Empirical results", "content": "Baselines. We implement the following baselines to evaluate their relative performance to DIVA. ODS is the \u201coracle\" agent trained over the downstream environment distribution Es(0), used for evaluation. With this baseline, we are benchmarking the upper bound in performance from the perspective of a learning algorithm that has access to the underlying data distribution.4 DR is the meta-learner trained over a task distribution defined by performing domain randomization over the space of valid genotypes, 0, under the training parameterization, E\u03c5 (\u03b8). Robust PLR (PLR+) [17] is the improved and theoretically grounded version of PLR [7], where agents' performance-based PLR objectives are evaluated on each level before using them for training. ACCEL [9] is the same as PLR but instead of randomly sampling over the genotype space to generate levels for evaluation, levels are mutated from existing solutions. All baselines use VariBAD [10] as their base meta-learner.\nExperimental setup. The oracle agent (ODS) is first trained over the each environment's down-stream distribution to tune VariBAD's hyperparameters. These environment-specific VariBAD settings are then fixed while hyperparameters for DIVA and the other baselines are tuned. For fairness of comparison-since DIVA is allowed NQD QD update steps to fill its archive before meta-training\u2014we allow each UED approach (PLR+ and ACCEL) to use significantly more environment steps for agent evaluations (details discussed below per environment). All empirical results were run with 5 seeds unless otherwise specified, and error bars indicate a 95% confidence region for the metric in question. The QD archive parameters were set per environment, and for ALCHEMY and RACING, relied on some hand-tuning to find the right combinations of features and objectives. We leave it to future work to perform a deeper analysis on what constitutes good archive design, and how to better automate this process."}, {"title": "5.1 GRIDNAV", "content": "Our first evaluation domain is a modified version of GRIDNAV (Figure 3), originally introduced to motivate and benchmark VariBAD [10]. The agent spawns at the center of the grid at the start of each episode, and receives a slight negative reward (r = -0.1) each step until it discovers (inhabits) the goal cell, at which point it also receives a larger positive reward (r = 1.0).\nParameterization. We parameterize the task space (i.e. the goal location) to reduce the likelihood of gen-erating meaningfully diverse goals. Specifically, each Eu (or Ek) introduces k genes to the solution genotype which together define the final y location. Each gene j can assume the values 0; \u2208 {\u22121,0,1}, and the final y location is determined by summing these values, and performing a floor division to map the bounds back to the original range of the grid. As k increases, y values are increasingly biased towards 0, as shown on the right side of Figure 3. For more details on the GRIDNAV domain, see Appendix B.1.\nQD updates. We define the archive features to be the x and y coordinates of the goal location. The objective is set to the current iteration, so that newer solutions are prioritized (additional details in Appendix B.1). DIVA is provided Ns2 = 8.0 \u00d7 104 (Ns1 = 0) QD update iterations for filling the archive. To compensate, PLR+ and ACCEL are each provided with an additional 9.6 \u00d7 106 environment steps for evaluating PLR scores, which amounts to three times as many total interactions\u2014since all methods are provided NE = 4.8 \u00d7 106 interactions for training. If each \u201creset\" call counts as one environment step\u00b3, the UED baselines are effectively granted 2.4\u00d7 more additional step data than what DIVA additionally receives through its QD updates (details in Appendix E.1).\nResults. From Figure 4a, we see that increasing genotype complexity (i.e. larger k) reduces goal diversity for DR\u2014which is expected given the parameterization defined for Eu. We can also see that DIVA, as a result of its QD updates, can effectively capture goal diversity, even as complexity"}, {"title": "5.2 ALCHEMY", "content": "ALCHEMY [18] is an artificial chemistry environment with a combinatorially complex task distri-bution. Each task is defined by some latent chemistry, which influences the underlying dynamics, as well as agent observations. To successfully maximize returns over the course of a trial, the agent must infer and exploit this latent chemistry. At the start of each episode, the agent is provided a new set of (1-12) potions and (1-3) stones, where each stone has a latent state defined by a specific vertex of a cube, i.e. ({0, 1}, {0, 1}, {0, 1}), and each potion has a latent effect, or specific manner in which it transforms stone latent states (see Figure 5a). The agent observes only salient artifacts of this latent information, and must use interactions to identify the ground-truth mechanics. At each step, the agent can apply any remaining potion to any remaining stone. Each stone's value is maximized the closer its latent state is to (1, 1, 1), and rewards are produced when stones are cast into the cauldron.\nTo make training feasible on academic resources, we perform evaluations on the symbolic version of ALCHEMY, as opposed to the full Unity-based version. Symbolic ALCHEMY contains the same mechanistic complexity, minus the visuomotor challenges which are irrelevant to this project's aims.\nParameterization. Es (0) is the downstream distribution containing maximal stone diversity. For training, implement Eur where k controls the level of difficulty in generating diverse stones. Specifi-cally, we introduce a larger set of coordinating genes 0; \u2208 {0,1} that together specify the initial stone latent states, similar to the mechanism we used in GRIDNAV to limit goal diversity. Each stone latent coordinate is specified with k genes, and only when all k genes are set to 1 is the latent coordinate is set to 1. When any of the genes are 0, the latent coordinate is 0. For our experiments we set k = 8, and henceforth use Eu to signify Eug.\nQD updates. We use features LATENTSTATEDIVERSITY and MANHATTANTOOPTIMAL -both of which target stone latent state diversity from different angles. See Appendix B.2 for more specifics on these features and other details surrounding ALCHEMY's archive construction. Like GRIDNAV, the objective is set to bias new solutions. DIVA is provided with Ns1 = 8.0 \u00d7 104 and Ns2 = 3.0 \u00d7 104 QD update iterations. PLR+ and ACCEL are compensated such that they receive 3.5\u00d7 more additional step data than what DIVA receives via QD updates (see Appendix E.1 for details).\nResults. Our empirical results demonstrate that DIVA is able to generate latent stone states with diversity representative of the target distribution. We see this both quantitatively in Figure 5b, and qualitatively in Figure 6. In Figure 5c, we see this diversity translates to significantly better results"}, {"title": "5.3 RACING", "content": "Lastly, we evaluate DIVA on the RACING domain introduced by [17]. In this environment, the agent controls a race car via simulated steering and gas pedal mechanisms, and is rewarded for efficiently completing the track, M\u2081 \u2208 T. We adapt this RL environment to the meta-RL setting by lowering the resolution of the observation space significantly. By increasing the challenge of perception, even competent agents benefit from multiple episodes to better understand the underlying track. For all of our experiments, we use H = 2 episodes per trial, and a flattened 15 \u00d7 15 pixel observation space.\nSetup. We use three different parameterizations in our experiments: (1) Es (0) is the downstream distribution we use for evaluating all methods, training ODS, and setting archive bounds for DIVA. Parameters are used to seed the random generation of control points which in turn parameterize a sequence of B\u00e9zier curves designed to smoothly transition between the control locations. Track diversity is further enforced by rejecting levels with control points that possess a standard deviation below a certain threshold. (2) EU (0) is a reparameterization of Es(0) that makes track diversity harder to generate, with the difficulty proportional to the value of k \u2208 N. For our experiments, we use k = 32 (which we will denote simply as Eu(\u03b8)), which roughly means that meaningful diversity is 32\u00d7 less likely to randomly occur than when k = 1 (which is equivalent to Es (\u03b8)). This is achieved by defining a small region in the center, 32 (or k, in general) times smaller than the track boundaries, where all points outside the region are projected onto the unit square, and scaled to the track size. (3) EF1 (0) uses as an RNG seed to select between a set of 20 hand-crafted levels official Formula-1 tracks [17], and is used to benchmark DIVA's zero-shot generalization to a new target distribution.\nQD updates. We define features TOTALANGLECHANGES (TAC) and CENTEROFMASSX (CX) for the archive dimensions. Levels from Eu lack curvature (see Figure 8) so TAC, which is defined as the sum of angle changes between track segments, is useful for directly targeting this desired curvature."}, {"title": "Combining DIVA and UED.", "content": "While PLR+ and ACCEL struggle on our evaluation domains, they still have utility of their own, which we hypothesize may be compatible with DIVA's. As a preliminary experiment to evaluate the potential of such a combination, we introduce DIVA+, which still uses DIVA to generate diverse training samples via QD, but additionally uses PLR+ to define a new distribution over these levels based on approximate learning potential. Instead of randomly sampling levels from Eu, the PLR evaluation mechanism samples levels from the DIVA-induced distribution over the archive. We perform experiments on two differ-ent archives generated by DIVA: (1) an archive that is slightly misspecified (see Appendix B.3 for details), and (2) the archive used in our main results. From Fig-ure 10, we see that while performance does not sig-nificantly improve for (2), the combination of DIVA and PLR is able to significantly improve performance on (1), and even statistically match the original DIVA results. These results highlight the potential of such hybrid (QD+UED) semi-supervised environment design (SSED) approaches, a promising area for future work."}, {"title": "6 Related work", "content": "Meta-reinforcement learning. Meta-reinforcement learning methods range from gradient-based approaches (e.g. MAML) [19], RNN context-based approaches [12, 11] (e.g. RL2), and the slew of emerging works utilizing transformers [20, 5, 21]. We use VariBAD [10], a state-of-the-art context variable-based approach that extends RL2 by using variational inference to incorporate task uncertainty into its beliefs. HyperX [22], an extension that uses reward-bonuses, was not found to improve performance on our domains. In each of these works, the training distribution is given; none address the problem of generating diverse training scenarios in absence of such a distribution.\nProcedural environment generation. Procedural (content) generation (PCG / PG) [6] is a vast field. Many RL and meta-RL domains themselves have PG baked-in (e.g. ProcGen [23], Meta-World, [24], Alchemy [18], and XLand [5]). Each of these works rely on human engineering to produce levels with meaningfully diverse features. A related stream of works apply scenario generation to robotics-some works essentially perform PCG [25, 26], while others integrate more involved search mechanics [27, 28, 29, 30]. One prior work [31] defines a formal but generic parameterization for applying PG to generate meta-RL tasks. It is yet to be shown, however, if such an approach can scale to domains with vastly different dynamics, and greater complexity.\nUnsupervised environment design. UED approaches-which use behavioral metrics to automati-cally define and adapt a curriculum of suitable tasks for agent training-form the frontier of research on open-endedness. The recent stream of open-ended agent/environment co-evolution works (e.g. [32, 33, 34]) was kickstarted by the POET [35, 36] algorithm. The \u201cUED\u201d term itself originated in PAIRED [8], which uses the performance of an \"antagonist\" agent to define the curriculum for the main (protagonist) agent. PLR [7] introduces an approach for weighting training levels based on learning potential, using various proxy metrics to capture this high-level concept. [17] introduces PLR+, which only trains on levels that have been previously evaluated, and thus enabling certain theoretical robustness guarantees. AdA [5] uses PLR as a cornerstone of their approach for generating diverse training levels for adaptive agents in a complex, open-ended task space. ACCEL [9] borrows PLR's scoring procedure, but the best-performing solutions are instead mutated, so the buffer not only collects and prioritizes levels of higher learning potential, but evolves them. We use ACCEL as our main baseline because it has demonstrated state-of-the art results on relevant domains, and like DIVA, evolves a population of levels. The main algorithmic differences between ACCEL and DIVA are that ACCEL (1) performs additional evaluation rollouts to produce scores during training and (2) uses a 1-d buffer instead of DIVA's multi-dimensional archive. PLR+ serves as a secondary baseline in this work; its non-evolutionary nature makes it a useful comparison to DR.\nScenario generation via QD. A number of recent works apply QD to simulated environments in order to generate diverse scenarios, with distinct aims. Some works, like DSAGE [37], uses QD to develop diverse levels for the purpose of probing a pretrained agent for interesting behaviors. In another line of work applies QD to human-robot interaction (HRI), and ranges from generating"}, {"title": "7 Discussion", "content": "The present work enables adaptive agent training on open-ended environment simulators by in-tegrating the unconstrained nature of unsupervised environment design (UED) approaches, with the implicit supervision baked into procedural generation (PG) and domain randomization (DR) methods. Unlike PG and DR, which requires domain knowledge to be carefully incorporated into the environment generation process, DIVA is able to flexibly incorporate domain knowledge, and can discover new levels representative of the downstream distribution. And instead of relying on behavioral metrics to infer a general, ungrounded form of \u201clearning potential\u201d, like UED\u2014which becomes increasingly unconstrained and therefore less useful a signal as environments become more complex and open-ended-DIVA is able to directly incorporate downstream feature samples to target specific, meaningful axes of diversity. With only a handful of downstream feature samples to set the parameters of the QD archive, our experiments (Section 5) demonstrate DIVA's ability to outperform competitive baselines compensated with three times as many environment steps during training.\nIn its current form, the most obvious limitation of DIVA is that, in addition to assuming access to downstream feature samples, the axes of diversity themselves must be specified. However, we imagine these axes of diversity could be learned automatically from a set of sample levels, or selected from a larger set of candidate features; it may be possible to adapt existing QD works to automate this process in related settings [41]. The present work also lacks a more thorough analysis of what constitutes good archive design. While some amount of heuristic decision-making is unavoidable when applying learning algorithms to specific domains, a promising future direction would be to study how to approach DIVA's archive design from a more algorithmic perspective.\nDIVA currently performs QD iterations over the environment parameter space defined by \u0395\u03c5 (\u03b8), where each component of the genotype 0 represents some salient input parameter to the simulator. Prior works in other domains (e.g. [42]) have demonstrated QD's ability to explore the latent space of generative models. One natural direction for future work would therefore be to apply DIVA to neural environment generators (rather than algorithmic generators), where @ would instead correspond to the latent input space of the generative model. If the latent space of these models is more convenient to work with than the raw environment parameters-e.g. due to greater smoothness with respect to meaningful axes of diversity-this may help QD more efficeintly discover samples within the target region. Conversely, DIVA's ability to discover useful regions of the parameter space means these neural environment generators do not need to be \"well-behaved\", or match a specific target distribution. Since these generative models are also likely to be differentiable, DIVA can additionally incorporate gradient-based QD works (e.g. DQD [15]) to accelerate its search.\nPreliminary results with DIVA+ demonstrate the additional potential of combining UED and DIVA approaches. The F1 transfer results (i.e. DIVA outperforming ODS trained directly on Es) further suggest that agents benefit from flexible incorporation of downstream knowledge. In future work, we hope to study more principle integrations of UED and DIVA-like approaches, and to more generally explore this exciting new area of semi-supervised environment design (SSED).\nMore broadly, now equipped with DIVA, researchers can develop more general-purpose, open-ended simulators, without concerning themselves with constructing convenient, well-behaved parameteri-zations. Evaluations in this work required constructing our own contrived paramterizations, since domains are rarely released without carefully designed parameterizations. It is no longer necessary to accomodate the assumption made my DR, PG, and UED approaches that either randomization over the parameter space should produce meaningful diversity, or that all forms of level difficulty ought to correspond to meaningful learning potential. So long as diverse tasks are possible to generate, even if sparsely distributed within the paramter space, QD may be used to discover these regions, and exploit them for agent training. Based on the promising empirical results presented in this work, we are hopeful that DIVA will enable future works to tackle even more complicated domains, and assist researchers in designing more capable and behaviorally interesting adaptive agents."}, {"title": "8 Reproducibility statement", "content": "The source code, along with thorough documentation for reproducing each result in this paper, is publicly available on Github6. Even without this code, researchers should be able to fully reproduce the algorithm from the details in the main body, the pseudocode provided in Appendix A, and training details (hyperparameters and hardware information) provided in Appendix E."}, {"title": "9 Ethics statement", "content": "Like all fundamental technologies, this work has the potential to be misapplied for malicious purposes. The authors do not believe, however, that the methods introduced in this work present a significant or unique risk for misuse or abuse. The authors intend for DIVA to be applied to use-cases that have the best interests of humanity (including concern for the earth and other sentient creatures) at heart."}, {"title": "10 Acknowledgements", "content": "This work was partially supported by NSF CAREER (#2145077) and the DARPA EMHAT project. We thank Tjanaka et al., the developers of pyribs [43], whose library served as the basis for our QD implementations. We thank Zintgraf et al., the authors of VariBAD [10], whose codebase served as the basis for our meta-RL agent. We thank Jiang et al. and Parker-Holder et al., the authors of PLR [7] and ACCEL [9], respectively, for their implementations which served as the basis for our UED baselines. We specifically thank Minqi Jiang for answering questions related to the PLR codebase in the early stages of development, and Varun Bhatt for helpful discussion at various stages of this work."}, {"title": "Appendix", "content": "A Algorithmic details\nAlgorithm. The pseudocode below walks through the entire training process for DIVA in abstract. All of the new components that DIVA introduces is written in green, and all DIVA+ modifications are in blue. Original VariBAD training steps are in black, and all inline comments are in orange.\nAlgorithm 2 DIVA (detailed)\n1: #Initialize VariBAD and QD components\n2: \u03c0o \u2190 init_policy(); fenc, fdec \u2190 init_vae() \u25b7 Initialize VariBAD components\n3: D\u03c0, DVAE \u2190 init_storage_buffers() \u25b7 Initialize VariBAD buffers\n4: \u0398o \u2190 {\u03b8}no ~ P(\u03b8) \u25b7 Sample initial solutions from space of valid genotypes\n5: Fo = [f(\u03b81), ..., f (\u03b8no)] \u2190 compute_features(\u0398) \u25b7 Compute env. features\n6: Jo = [J(\u03b81), ..., J(\u03b8no)] \u2190 compute_objectives(\u0398) \u25b7 Compute env. objectives\n7: G \u2190 initialize_archive (Fo, Fs) \u25b7 Init. archive to contain both Fo and target features Fs\n8: G \u2190 insert_solutions(G,(\u0398o, Fo, Jo)) \u25b7 Add random solutions to archive\n9: #QD stage 1: discover target region\n10: for i in range(Ns1) do\n11: G \u2190 QD_UPDATE(G, J, M, BQD) \u25b7 Perform QD update (batch size BQD) to populate archive\n12: M \u2190 update_sample_mask(M,G) \u25b7 Move mask gradually towards target region\n13: #QD stage 2: populate target region\n14: G \u2190 update_archive_bounds(G) \u25b7 Create final archive (target region) before S2 updates\n15: for i in range(Ns2) do\n16: G \u2190 QD_UPDATE(G, J, \u00d8, BQD) \u25b7 Perform QD update to populate target region\n17: #Meta-learning over QD archive\n18: for i in range(NVB) do\n19: \u03b8' ~ PG(\u03b8) \u25b7 Sample solution \u03b8' from QD archive using approximated target density from Fs\n20: M \u2190 generate_environment (\u03b8') \u25b7 Generate new training environment from solution \u03b8'\n21: #Produce meta-RL rollouts\n22: T \u2190 perform_policy_rollout (M, \u03c0\u03c6)\n23: D\u03c0, DVAE \u2190 add_to_buffers (D, DVAE, T)\n24: #Update VAE and policy\n25: fenc, fdec \u2190 varibad_vae_update (fenc, fdec, DVAE)\n26: if after_vae_pretraining() then\n27: varibad_policy_update(\u03c0, D\u03c0)\n28: #Perform DIVA+ QD updates\n29: if DIVA+ and (i % qd_update_interval = 0) then\n30: for qd_updates_per_iter do\n31: G \u2190 QD_UPDATE (G, JPLR, \u00d8, BQD) \u25b7 Perform QD update with PLR objective\nAlgorithm 3 QD update\n1: #Perform a single QD update on archive G with batch size B.\n2: function QD_UPDATE(G, J, M, B)\n3: \u0398Bxn = [\u03b81,..., \u03b8B] \u2190 sample_from_emitters(G, M, B) \u25b7 Get mutated batch of solutions\n4: FBxk = [f(\u03b81),\u2026\u2026\u2026, f(\u03b8B)] \u2190 compute_features(\u0398) \u25b7 Compute env. features\n5: JBx1 = [J(\u03b81), ..., J(\u03b8B)] \u2190 compute_objectives(\u0398) \u25b7 Compute env. objectives\n6: G' \u2190 add_solutions(G, (\u0398, F, J)) \u25b7 Add new solutions to archive if they are elites\n7: return G'\nDetails on the two-stage QD updates Here we provide more details on the process described in Section Section 4. Hyperparameters Ns1 and Ns2 are set to define the number of QD updates to perform in each stage (see Appendix D). In proportion to how many updates in S1 have elapsed, if the sample mask is enabled, the mask is moved at a linear pace from encapsulating the full S1 archive, to covering only the target region. We also set a hyperparameter, Nsm (see Appendix D), which"}, {"title": "B Domain details", "content": "B.1 GRIDNAV\nGRIDNAV features. The following features are defined for the GRIDNAV environment:\nTable 1: GRIDNAV features.\nName Abbr. Description\nXPOSITION XP x position of the goal.\nYPOSITION YP y position of the goal.\nB.2 ALCHEMY\nALCHEMY features. We defined the following features for the ALCHEMY environment:\nTable 2: ALCHEMY features.\nName Abbr. Description\nMANHATTANTOOPTIMAL MTO Average Manhattan distance between all stones (across all trials) to the optimal state.\nSTONETOSTONEDISTANCE STSD Average Euclidean distance between all pairs of stones (across all trials).\nGRAPHNUM BOTTLENECKS GNB The number of bottlenecks in the graph topology.\nLATENTSTATE DIVERSITY LSD The 'diversity' of the latent stone states (across all trials). Diversity is calculated as the standard deviation of each latent state coordinate across all stones.\nPARITYFIRSTPOTION PFP First potion location (first trial, first potion), as a parity measure.\nPARITYFIRSTSTONE PFS First stone location (first trial, first stone), as a parity measure.\nPOTION EFFECTDIVERSITY PED The 'diversity' of the potion effects (across all trials). Diversity is calculated as the standard deviation of each potion effect coordinate across all potions.\nPOTION PERMUTATION PP Potion permutation.\nPOTIONREFLECTION PR Potion reflection.\nSTONEREFLECTION SRE Stone reflection.\nSTONEROTATION SRO Stone rotation.\nSTONETOSTONEDISTANCE VARIANCE STSDV Variance of the distances between stones (across all trials)."}, {"title": "C Ablation analysis", "content": "Sample mask ablation. Figure 15 shows the benefit of updating the sample mask bounds during the first archive filling stage on ALCHEMY. Not only does this approach produce significantly more total archive solutions, but more importantly, progress towards filling the target region specifically is accelerated."}, {"title": "D Hyperparameter sensitivity analysis", "content": "Varying QD mutation rate. We perform an ablation on the QD mutation rate, which is the probability that a given gene will be mutated (for MAP-Elites). We perform this ablation on ALCHEMY because the its search is the most challenging of the three environments we consider (it is the sole environment that required a longer S1 and the sample mask trick for accelerating to accelerate the search). We see from Figure 16 that ALCHEMY results are not very sensitive to the setting of the mutation rate."}, {"title": "E Training details", "content": "E.1 DIVA hyperparameters\nTable 4 displays the hyperparameters used for DIVA across all domains.\nA note on NTRS computation The initial QD population (no) is implemented such that the first set of QD updates simply generates no random levels from Eu, before performing the actual mutations (for ME) or intelligent sampling (for ES). Thus, the formula we use for computing NTRS, the total reset steps provided to DIVA (see Table 4), which we use to compare the extra steps we provide PLR+ and ACCEL (discussed in Section 5), does not include no; it is simply the product of the batch size and the total number of QD iterations."}]}