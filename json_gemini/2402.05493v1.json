{"title": "Investigating White-Box Attacks for On-Device Models", "authors": ["Mingyi Zhou", "Xiang Gao", "Jing Wu", "Kui Liu", "Hailong Sun", "Li Li"], "abstract": "Numerous mobile apps have leveraged deep learning capabilities. However, on-device models are vulnerable to attacks as they can be easily extracted from their corresponding mobile apps. Although the structure and parameters information of these models can be accessed, existing on-device attacking approaches only generate black-box attacks (i.e., indirect white-box attacks), which are less effective and efficient than white-box strategies. This is because mo-bile deep learning (DL) frameworks like TensorFlow Lite (TFLite) do not support gradient computing (referred to as non-debuggable models), which is necessary for white-box attacking algorithms. Thus, we argue that existing findings may underestimate the harm-fulness of on-device attacks. To validate this, we systematically analyze the difficulties of transforming the on-device model to its debuggable version and propose a Reverse Engineering framework for On-device Models (REOM), which automatically reverses the compiled on-device TFLite model to its debuggable version, en-abling attackers to launch white-box attacks. Our empirical results show that our approach is effective in achieving automated trans-formation (i.e., 92.6%) among 244 TFLite models. Compared with previous attacks using surrogate models, REOM enables attackers to achieve higher attack success rates (10.23%\u219289.03%) with a hun-dred times smaller attack perturbations (1.0\u21920.01). Our findings emphasize the need for developers to carefully consider their model deployment strategies, and use white-box methods to evaluate the vulnerability of on-device models. Our artifacts 1 are available.", "sections": [{"title": "1 INTRODUCTION", "content": "The number of mobile devices worldwide is continuously growing. The capabilities of those devices also keep increasing, i.e., with powerful Central Processing Units (CPUs) and a large amount of memory, making them suitable for running deep learning (DL) mod-els. Indeed, mobile devices have now become an ideal platform for deploying the DL model. Many intelligent applications have already been deployed on mobile devices [39] and have already benefited millions of users. Though DL models can also be deployed on a cloud platform, data transmission between a mobile device and the cloud may compromise user privacy. Indeed, to achieve high-level security, users' personal data should not be sent outside the device. This could be the reason why more and more DL models are de-ployed on the device, which has been advertised as one of the most important features by the newly emerged OpenHarmony mobile system [22]), and the corresponding models are often referred to as on-device models.\nUnfortunately, such on-device models are directly presented on mobile devices, giving attackers a lot of opportunities to exploit since it is relatively easy to unpack mobile apps to locate the physi-cal models. As a result, on-device models are facing more and more serious security threats. Although on-device models are released to users as black-box ones for preventing potential attacks because at-tackers cannot obtain gradient information\u00b2 from on-device models (referred to as non-debuggable models), they do not fulfill such a purpose in practice. Indeed, as illustrated in Figure 1, attackers still find ways to attack black-box models without accessing their gradient information, e.g., via the so-called transferable attacks (referred to as indirect white-box attack). They achieve this by first, for target models, identifying debuggable surrogate models that are available to generate attacks. They then exploit surrogate models through white-box strategies. Once the strategies satisfy the attackers' needs, they apply the same strategies to attack on-device models.\nIn fact, many vulnerabilities of on-device models have already been discovered by our fellow researchers in recent years. For exam-ple, Huang et al. [15, 16] propose to achieve the purpose by parsing features of the on-device model to find a surrogate model from the web, which could then be used to launch transferable attacks on mobile models. Cao et al. [4] also use surrogate models for attack-ing mobile models under the black-box setting, albeit by obtaining"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "We now provide the necessary background about on-device DL models, DL model attack,s and the ONNX project."}, {"title": "2.1 On-device DL Models", "content": "DL frameworks: The open-source community has developed many well-known open-source frameworks for DL tasks such as TensorFlow [1], Theano [2], Caffe [19], Keras [9], and PyTorch [32]. These frameworks dominate the development of DL models and set standards for them [11]. PyTorch is one of the latest DL frame-works, and is gaining popularity for its ease of use and its capabil-ity to construct the dynamic computational graph, which is now widely used by the academic community. In contrast, TensorFlow is widely used by companies, startups, and business firms to automate things and develop new systems. It has distributed training sup-port, scalable production options, and support for mobile devices. Currently, the AI community has made huge efforts to develop open-source on-device frameworks like TensorFlow Lite (TFLite), Caffe2, Caffe, NCNN, and ONNX. As an on-device DL platform, TensorFlow Lite (TFLite) is the most popular framework for DL models on smartphones, as it has GPU support and is optimized for mobile devices [16, 39].\nTFLite Models. TFLite models have powerful features for run-ning models on edge devices but they do not provide APIs to access the gradient or intermediate outputs like other TensorFlow or Py-Torch models. TensorFlow provides a TensorFlow Lite Converter to convert a TensorFlow model into a TensorFlow Lite model. In addition, the models trained by other DL frameworks can also be converted to the TFLite model. For example, PyTorch provides the API to save the model as ONNX format, and then convert the ONNX model to TensorFlow and TFLite model Onnx-tf tool. For parsing"}, {"title": "2.2 Adversarial Attacks for DL models", "content": "Adversarial attacks add perturbation that can be considered a spe-cial noise to the original image to fool the DL models. Adversarial attacks can be categorized into white-box attacks such as gradient-based attacks [10, 13, 20, 26-28, 31], and black-box attacks [3, 6-8, 14, 17, 18, 29]. Gradient descent (GD) is an iterative optimization algorithm, used to find a local minimum/maximum of a given func-tion. This method is commonly used in training DL models. For the gradient-based (white-box) attack [13, 20], they use the gradient to compute the perturbation that can increase the model loss. Query-based black-box attacks [3, 5] estimate the gradients to compute the perturbation, e.g., randomly update the perturbation to estimate the right update direction. White-box attacks have full access to the model structure and its parameters to enable gradient computing. In black-box attacks, only partial information (i.e., model output) about the model is available. Goodfellow et al. [13] show that adver-sarial examples generated by surrogate models [30, 41] can fool the target model. Therefore, for adversarial attacks on devices, Huang et al. [16] and Cao et al. [4] evaluated the mobile model robust-ness by generating attacks from surrogate models. However, they heavily rely on the similarity between surrogate models and target models. According to this, we propose the REOM to transform the TFLite model to the PyTorch model, to explore the security issue of model deployment."}, {"title": "3 PRELIMINARY STUDY", "content": "Recall that the ONNX platform has provided various tools to sup-port the exchange from neural network models to ONNX models. In this preliminary study, we would like to investigate whether these tools can be leveraged to transform a TFLite model (will be regarded as a Tensorflow model) into a PyTorch model (i.e., TFLite\n\u2192ONNX model \u2192 PyTorch model).\nmodel\n3.1 Harvesting On-device Models\nTo conduct this preliminary study, we need to first collect a set of TFLite models that are actually included in Android apps. Since there is no existing dataset containing a set of apps with TFLite models, we have to construct such a dataset from scratch. To this end, we resort to the AndroZoo dataset [21] to first collect a set of real-world Android apps. AndroZoo is by far the largest app set well maintained by researchers and has been widely leveraged by researchers to support various Android-related studies. At the moment, AndroZoo contains over 19 million Android apps. It is extremely time-consuming for us to download and scan all of them to locate TFLite models. For the sake of simplicity, we only focus on the latest Google Play apps (published from 2021 to 2022) to fulfill our study. In total, we have collected 173,905 apps (it takes more than one month). After disassembling the apps, we find 674 of them contain TFLite-related packages (i.e., org.tensorflow). All of these apps are regarded as candidates to extract TFLite models. Among 674 apps, we were eventually able to extract 244 TFLite models, which are then taken into account to fulfill our preliminary study.\n3.2 Model Transformation Study\nFigure 2 illustrates the working process of our preliminary model transformation study. We would like to check if existing tools can be leveraged to transform TFLite models into debuggable PyTorch models. Since ONNX does not directly provide the tool for TFLite models, we naively regard them as Tensorflow models to fulfill this study, which is made up of three steps, as highlighted in Figure 2."}, {"title": "4 APPROACH", "content": "We now detail our approach proposed to transform on-device TFLite models into debuggable PyTorch models. Before presenting our method, we first analyze the errors (see Table 1) in existing tools:\n\u2022 Compatibility \u2013 Structure Mismatch: To accelerate the compu-tation on mobile devices equipped with mobile CPU and mo-bile GPU, compiled on-device models are optimized to contain some different data types and model structures with debuggable models. For example, float32 will be converted to uint8 when compiling the debuggable model to the on-device model. When converting the on-device model back to ONNX models, some extra operators (e.g., quantization operators, transformation op-erators) will be created to make it compatible with the ONNX data types and structures. Since the extra structures are non-debuggable, the structure mismatch will unfortunately result in the failure of the transformation. In our approach, we propose the Pruning Module to resolve this problem.\n\u2022 Not Implemented \u2013 Operator Mismatch: Some compiled opera-tors of on-device models are optimized for mobile computing. The optimized operators transformed from TFLite model are not compatible with the debuggable model format. For example, when the data type of the on-device operator is uint8 (optimized data type for on-device model), this operator will not be debug-gable because DL frameworks like PyTorch and TensorFlow do not have debuggable API for uint8. In addition, TFLite defines many unique operators (i.e., mismatched operators for debug-gable models) supported by its corresponding library. However, such unique operators are not supported by other DL frame-works (e.g., PyTorch, TensorFlow). Therefore, in this work, we propose the Translation Module to bridge the mismatched operators.\n\u2022 Not Implemented \u2013 Operators Not Supported: Those customized operators are not supported by other DL models, resulting in TFLite cannot be directly transformed. For example, if devel-opers want to implement an advanced Convolutional operator in their model but this advanced function is not supported by the current version of the on-device DL framework, they could add their customized C/C++ implementation of the advanced function to their TFLite model. Besides, developers can name this customized operator and define the interface freely. So, other DL frameworks cannot identify this customized operator because it is not included in the operator library. This problem also exists on the deprecated operators of TFLite models. Fortunately, on-device operators are usually a subset of the debuggable operators' library. Therefore, we propose Auto-matching Module to identify the equivalent operators in the debuggable model format to fulfill the transformation.\n\u2022 Input Type \u2013 Specification Mismatch: The specifications of some operators (e.g., computing operators, transformation oper-ators) may vary in different model formats, such as the order of"}, {"title": "4.1 Modifier", "content": "We now detail the Modifier of REOM with the three modules to resolve the aforementioned problems, respectively.\nPruning Module: To solve the structure mismatch issue, we propose Pruning Module. Before presenting technique details, we first show an example of the structure mismatch in Figure 5. When the on-device model (Figure 5(a)) tries to convert to the debuggable format, some non-debuggable components are not compatible with the debuggable format. As shown in green areas of Figure 5(b), those non-debuggable components need to be processed before connecting with other debuggable components. Therefore, the \u201cex-tra\" part will be produced, but it will confuse debuggable DL li-braries because they do not consider this special case in many functions like gradient computing. For example, in Figure 5, the type of weights in FullyConnected layer is uint8, which cannot be handled by ONNX operators and debuggable model format. To address it, the uint8 tensor needs to be transformed by the formula: $y = (x - y_0) \\times y'$, where the $y_0$ and $y'$ are stored in the model files and are the zero-point and scale parameters of this uint8 tensor. After converting to ONNX format, it needs to attach an extra new operator DequantizeLinear to achieve the above transformation. Unfortunately, the TFLite-converted ONNX model with such an extra branch is still not compatible with the debuggable model format like PyTorch. To address this problem, the Pruning Module is proposed to correct the mismatched structure so that it will be compatible with the debuggable format. To remove the mismatched structure, we first find the suspect non-debuggable extra operators using pruning rules. Specifically, we analyze the ONNX operator library to identify the operators that can be used to transform the model weights (e.g., DequantizeLinear, Reshape, Transpose). These operators are potential extra operators. The complete pruning rules"}, {"title": "4.2 Converting to The Debuggable Model", "content": "After modifying the converted ONNX model, it will save the modi-fied model as the new .onnx file. Then, we use the onnx2pytorch tool to load the structure information and parameter of the ONNX model and assemble them into the Python code using PyTorch API. It is also worth noting here that the generated debuggable model will share the same structure and parameters as the on-device model extracted from real-world Android apps. Consequently, the two models should share the same capabilities. They should also share the same attacking surfaces. In other words, the attack scenarios applicable to the debuggable PyTorch model could be directly ap-plied to attack the on-device TFLite model (indicated via dotted line in Figure 4). We present the experimental results in Section 5."}, {"title": "5 EVALUATION", "content": "Towards checking if our research objective is achieved, we propose to answer the following three key research questions.\n\u2022 RQ1: How effective is our approach in achieving automated model transformation?\n\u2022 RQ2: How accurate is the transformation approach?"}, {"title": "5.1 RQ1: Effectiveness", "content": "In this part, we use all 244 apps (i.e., 244 TFLite models) to fulfill our study. As shown in Figure 3, among the 244 models, only 16 of them can be successfully transformed into PyTorch models by existing tools. This baseline approach yields a failure rate of 93.4%, making it impossible to be adopted in practice to achieve our purpose, i.e., automatically transforming TFLite models to debuggable ones.\nIn contrast, REOM is able to successfully transform 226 of them, giving a success rate of 92.6%. Note that the 16 models that can be handled by existing tools do not have the non-debuggable component. For those models, the debuggable models produced by our method are the same as the models generated by existing tools. Table 2 further breaks down the detailed results with respect to the three types of issues summarized previously. Note that a given TFLite model may encounter several errors. Hence, the total number of errors (i.e., 289) is slightly larger than the number of TFLite models. All the failure cases are caused by the Operator Not Supported issue, for which the Auto-matching Module cannot find an existing ONNX operator that is equivalent to the customized or deprecated TFLite operator.\nObservant readers may have noticed that our approach has taken the parameter a to determine whether the newly generated ONNX operator (because of non-supported TFLite operators) should be accepted in the Auto-matching Module. We now go one step deeper to evaluate the sensitivity of this parameter. As shown in Table 3, when a is set to be 0.1 (the default value), 6 of 24 models with custom non-supported operators can be successfully converted (18 failures). When decreasing this threshold, the failure rate will increase. In the worst case, when a is set to zero, none of the non-supported operator problems can be resolved. However, in another extreme setting, when setting the a to be 100, all the non-supported opera-tors can be resolved, i.e., mapped to newly generated operators that are accepted by the debuggable model format. Subsequently, all the TFlite models can be successfully converted. However, such trans-formation will not make much sense as the transformed models"}, {"title": "5.2 RQ2: Accuracy", "content": "We then compare the output similarity between the transformed models and source models to evaluate the accuracy. Given a pair of models (i.e., a TFLite model and its PyTorch counterpart), with the same inputs, the accuracy of our approach is evaluated based on the similarity of the outputs yielded by the two models. The similar the results are, the higher the accuracy will be. In practice, we use the collected 244 on-device models as the test set. We generate 100 random inputs as the specification of the TFLite model and com-pare the outputs between the TFLite models and their debuggable PyTorch versions. However, the output ranges of the on-device models are different. To standardize the output difference between the two models, we use the scaled mean transformation difference, which can be calculated as follows:\n$d=\\frac{\\sum_{i=1}^{k} |y_i - \\hat{y_i}|}{rk}$"}, {"title": "5.3 RQ3: Supporting White-box Attacks", "content": "We evaluate the attacking performance of REOM to check whether attackers can directly perform white-box attacks for on-device mod-els. We choose nine TFLite classification models of the work [16] and the TensorFlow Hub to answer this research question. We choose these models because we can find large-scale public datasets (see our code repository) to evaluate the attack success rate to show the effectiveness. For the fruit app, we identify 848 images whose categories correspond to the task scope of models and then use these images as the test set. For other apps, we randomly sample 10000 images from the large-scale datasets as test sets.\nThen, we evaluate the attacking performance of the on-device model using the proposed REOM. In [4, 16], they focus on gener-ating adversarial attacks by the surrogate model to mislead the target on-device model in the black-box setting. The performance of adversarial attacks generated by the surrogate model relies on the similarity between the target model and the surrogate model. Our study proposes a method for transforming the compiled TFLite model into a debuggable model, eliminating the need to search for or train a surrogate model in order to achieve white-box-like attack performance. Therefore, we will go in-depth into the on-device adversarial attacks and show how our tool can be a general method to evaluate the robustness of the on-device model.\nWe calculate the attack success rate (i.e., fooling rate) by $p = \\frac{m}{n}$, where $n$ and $m$ are the number of successful adversarial examples and the number of images that can be correctly classified by the model, respectively. Note that we only perform attacks on the image which is correctly classified by the target model. $n$ is the number of successful adversarial examples. For non-targeted attacks, the attack succeeds when the target model outputs the wrong labels for the inputs. For targeted attacks, the attack succeeds when the target model outputs a specific wrong label. Generally, targeted attacks are more difficult to produce than non-targeted attacks."}, {"title": "6 DISCUSSION", "content": "In this section, we will discuss the genericity, other properties of our method, and potential defense strategies.\nGeneralizability of Our Approach: Although our method is designed for the most commonly used on-device model format TFLite, our approach should also work for other formats because the Modifier handles the non-debuggable component in the ONNX level, which has a unified model representation. we believe our approach should also work for transforming other on-device formats like Caffe2 to the debuggable model format. To experimentally validate this, we collect 10 Caffe2 models from the Caffe2 model zoo [34] to evaluate the effectiveness of REOM on the other on-device format. By default, only two out of the ten models can be successfully handled by the existing toolchain. We then integrate our approach into the process by applying our Modifier to automatically modify"}, {"title": "7 THREATS TO VALIDITY", "content": "We now discuss the potential threats to the validity of this work.\nFirst, our proposed conversion tool REOM is based on the ONNX platform, and we evaluate its performance on TFLite models and caffe2 models. However, some on-device model formats may have a higher level of security (e.g., do not use high-level representations like TVM models), which may disable the model parsing based on the operator-to-operator transformation rule list, including our approach proposed in this work. In this case, those on-device models are safe. However, reverse engineering methods may overcome this problem by modifying the conversion rules, e.g., building a mapping list from ONNX operators to TVM model representations.\nSecond, the development of the DL library is in rapid change. It may affect the performance of reverse engineering when the library updates the model format or conducts other major evolutionary changes. In such a case, the reverse engineering tool may fail to convert on-device models to debuggable versions. Therefore, we argue that there is a strong need for our approach to be aware of the evolution of given DL frameworks."}, {"title": "8 CONCLUSION", "content": "This study evaluates the importance of developing a reverse engi-neering tool that can transform the TFLite model into the debug-gable PyTorch model. Such transformation can enable attackers to perform direct white-box attacks for evaluating the vulnera-bility of on-device models. To achieve this, we propose a REOM framework to transform the on-device model into the PyTorch model. Our proposed REOM has three steps: (1) first, we use the tf2onnx tool to convert the TFLite to the ONNX model. (2) Second, we propose a three-module modifier, which has Pruning Module, Translation Module, and Auto-matching Module. It can modify the ONNX model to make it compatible with the debuggable PyTorch format. (3) Finally, the modified ONNX model can be successfully transformed into the PyTorch model by the onnx2pytorch tool. Ex-periments show the REOM can effectively transform most TFLite models to PyTorch models, with small transformation differences compared with the original TFLite model. Then, we test our method on adversarial attacks and find that on-device models can be directly attacked via white-box strategies. The current model deployment strategy is at serious risk. it enables attackers to perform white-box attacks on on-device models. In future works, we will comprehen-sively analyze the security and privacy issues of on-device models using the proposed REOM."}, {"title": "9 DATA AVAILABILITY", "content": "Our codes and experimental data are available online (https://github. com/zhoumingyi/REOM), which includes the setup steps and how to reproduce the main results of this paper."}]}