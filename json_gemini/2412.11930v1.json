{"title": "Hierarchical Meta-Reinforcement Learning via Automated Macro-Action Discovery", "authors": ["Minjae Cho", "Chuangchuang Sun"], "abstract": "Meta-Reinforcement Learning (Meta-RL) enables fast adaptation to new testing tasks. Despite recent advancements, it is still challenging to learn performant policies across multiple complex and high-dimensional tasks. To address this, we propose a novel architecture with three hierarchical levels for 1) learning task representations, 2) discovering task-agnostic macro-actions in an automated manner, and 3) learning primitive actions. The macro-action can guide the low-level primitive policy learning to more efficiently transition to goal states. This can address the issue that the policy may forget previously learned behavior while learning new, conflicting tasks. Moreover, the task-agnostic nature of the macro-actions is enabled by removing task-specific components from the state space. Hence, this makes them amenable to re-composition across different tasks and leads to promising fast adaptation to new tasks. Also, the prospective instability from the tri-level hierarchies is effectively mitigated by our innovative, independently tailored training schemes. Experiments in the MetaWorld framework demonstrate the improved sample efficiency and success rate of our approach compared to previous state-of-the-art methods.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) has rapidly advanced, enhancing decision-making capabilities in high-dimensional and complex systems. Recent innovations have expanded RL to address diverse challenges, including multi-task RL, safe RL, and offline RL, thus broadening its practical applications. Among these advancements, meta-learning emphasizes training an agent to adapt to new tasks by leveraging previously acquired knowledge. This often involves balancing the performance across multiple training tasks, exposing the learner to diverse task-parameters to facilitate future adaptation to any prospective test task within the same distribution.\nHence, due to the inherent challenge in meta-learning\u00b9, common methodologies are typically tested on narrow task distributional learning scenarios such as a jumping robot with varying loads (altering transition dynamics) and a running agent with different velocity criteria (changing optimality). Although it faces challenges when extended to more complex meta-learning scenarios, task representation learning\u00b2 shows its potential in such environments by identifying shared structures across given training tasks. The discovered representations offer fundamental insights into tasks, aiding the decision-maker in adapting and performing effectively on new tasks.\nBuilding on task representations, which expand the input space with additional conditioners, we aim to further distribute the policy's information load by utilizing high-level actions, or macro-actions. Specifically, macro-action provides directional action vectors to the low-level policy. Then, the policy fills with detailed low-level control input along the given direction. This approach is particularly useful for learning multiple tasks and adapting to new ones in hard-parameter sharing. In this setting, effective distribution of information processing can handle challenging task-specific instructions, ensuring that the policy retains its previously acquired optimal behavior in a particular task while learning new tasks.\nIn short, beyond the existing bi-level representation learning (task-representation | low-level control), we introduce a tri-level method (task-representation | high-level control | low-level control) that includes an additional layer for constructing macro-actions. For task-representation, we adopt the goal state concept from hierarchical RL literature3,4. For high-level control inputs, we propose a modified Variational AutoEncoder (VAE) to identify an effective placeholder for low-level control. This placeholder provides both sign-based and magnitude-level guidance, enabling the policy to refine it into precise low-level controls. In the architecture, the encoder predicts macro-actions, while the decoder uses imputations to bridge the current state with the goal state. Therefore, this approach brings appealing properties in complex, sparse, long-horizon problems. We provide mathematical views of the mechanism of the encoder and decoder.\nFor stacking three levels of layers, we design each to have an independent role in decomposition to avoid the \u201ccurse"}, {"title": "2 Preliminaries", "content": "Reinforcement learning operates within the mathematical framework of Markov Decision Processes (MDP)7, characterized by the tuple (S, A, T (\u00b7), R(\u00b7), \u03b3), where I \u2208 R represents the state set (observation), A \u2208 R\u00ba is the action set, T(\u00b7) : S \u00d7 A \u2192 I denotes the transition function, and R(\u00b7) : S \u00d7 A \u2192 R is a reward function with y as a discount factor. RL for any arbitrary task, I, comprises its own transitional and reward dynamics. Formally, the definition of a task, I, is (T(\u00b7), R(\u00b7)) \u2208 T.\nThe goal of deep RL is to find a policy parameterized by 0, denoted as \u03c0e (as), that maps the state to an optimal action, yielding the highest cumulative rewards (optimal trajectory) under task I, as shown below:\n$\\max \\limits_{\\theta} J(\\pi_{\\theta}, \\Theta) = E_{\\alpha \\sim \\pi_{\\theta}(s), s' \\sim T(s,a)} [\\sum \\limits_{t=0}^{T} \\gamma^t R(s_t, a_t)]$\nMeta-Learning\nMeta-learning extends multi-task learning by aiming for rapid adaptation to unseen tasks using prior knowledge. Formally, the trained policy aims to maximize the prospective optimality objective of the test task Itest:\n$\\max \\limits_{\\theta} J(\\pi_{\\theta}, \\Theta_{test}), \\text{ where }\\theta = \\theta - \\nabla_{\\theta} J(\\pi_{\\theta}, \\Theta_{train})$\nMeta-training is typically achieved by conditioning task representations or finding a parametric space that is close to the new task's optimal space as above."}, {"title": "Hierarchical Reinforcement Learning", "content": "Hierarchical RL is introduced as a solution for tasks with long horizons and sparse rewards by decomposing the original problem into several sub-problems. This is typically achieved with a new optimality objective, I, as shown below by additionally conditioning the policy on a goal z:\n$J = E_{\\alpha\\sim \\pi_{\\theta} (s,z), s' \\sim T(s,a)}[\\sum \\limits_{t=0}^{T} \\gamma^t R(s_t, a_t)] + \\alpha \\mathbb{I}(s_t = z)$\nwhere I(\u00b7) is an indicator function that adds a reward bonus, scaled by a, when the policy successfully reaches the given goal z. The z can be a latent representation, goal, or a raw state, goal state. With this definition, the policy is efficiently guided in sparse rewards and long horizons settings4,5,8,9."}, {"title": "3 HiMeta:\nHierarchical Meta-Reinforcement Learning", "content": "A high information load on the decision-maker can hinder learning by overwhelming its capacity to process information. This is evident in multiple-task settings where learning one task can downgrade the behavior in another. Thus, we propose decomposing representations into a more fundamental, task-agnostic language for an \"action-guided\" learning approach. We introduce three handcrafted definitions of goal states and add a layer that finds the future action embedding that can connect the current state to the goal state. In short, we propose carefully designed architectures and loss functions that discover macro-actions by bridging the action space to the state space using VAE and imputations."}, {"title": "High-level layer", "content": "The High-Level (HL) architecture aims to discover latent task representations as a probability distribution that reconciles the trajectory data across multiple tasks. Our HL module utilizes recurrent units for modeling the probability distributions. We use a categorical feed-forward network with an information-theoretic regularizer10 to induce explorations while penalizing allocations if it is not worthwhile.\nTo align our experimental settings with the baselines in the Experiment section, we use a GRU11 as the recurrent unit, though an LSTM12 can also be an alternative. For simplicity, we denote the HL layer's parameters by \u222e and use subscripts to indicate specific sub-architectures. For example, the recurrent model in Figure 2 is represented as oh, where oh denotes the encoder giving output h. The HL feed-forward process is denoted given MDP\u2081 = (st,at,rt, St+1):\n$h_t = \\phi_h(MDP_t, h_{t-1}),$\n$y_t = Cat(\\phi_y(h_t))$\nwhere ht\u22121 = $h(MDP0:t\u22121) is the hidden state of the recurrent unit, qy is a feed forward neural network, and Cat is a categorical activations in to discrete probability space."}, {"title": "HL-Training Scheme", "content": "Our HL design includes an encoder where the representation is inferred while being simultaneously trained alongside a value function using the Mean Squared Error (MSE) loss:\n$L_{HL(V\\pi)} = [V^{\\pi_{\\theta}} (\\phi(MDP_t), s_t) \u2013 V(s_t)]^2$\nwhere the first term is our estimation with the learned representations, and the second is a true state value function. The true value function is estimated via Monte-Carlo samplings.\nInspired by10, entropy regularizer is used to encourage exploration of representations within a task I, while occupancy loss ensures having likely correct task representations only when it successfully reduces the value loss. Formally, the entropy regularizer is defined as:\n$L_{HL(ent)} = log \\frac{p(y_t)}{\\phi_y (y_t | h_t)}$\nand the occupancy loss is:\n$L_{HL(occ)} = -(e y_t)^T log K$\nwhere K is the task inference dimension and e = [e-K+1,e-K+2,...,e-1,e\u00ba] \u2208 RK. We scale it with the upper bound of the entropy regularizer log K, encouraging their competition. Thus, by creating the HL objective with scaling parameters, the high-level architecture can successfully and efficiently infer representations y\u2081 for use by the lower-level architectures. The HL update is to optimize the following loss function LHL as\n$\\min \\limits_{\\Phi} \\alpha_V L_{HL(V\\pi)} + \\alpha_{\\varepsilon}L_{HL(ent)} + \\alpha_O L_{HL(occ)}$\nThe above training of the HL parameters \u222e is run across N tasks."}, {"title": "Intermediate-Level Layer", "content": "Our key contribution is the Intermediate-Level (IL) architecture. At this level, we infer a macro-action using a modified VAE. The objective is to further distribute the decision burden from the policy and guide the policy to the desired state space using goal-conditioning. This is fundamentally different from previous hierarchical methods that only aimed to reach the goal state while neglecting the complexity of learning systems such as multi-task and meta-learning.\nOur design leverages a missing-information technique, formally imputations13\u201315, which uses relevant data to infer the expected data pattern in a missing region. Given two arbitrary points in the state space where the actions to transition from one to the other are unknown, we aim to discover macro actions, z, that connect them. The term \u201cmacro-\" is used as a temporally extended action:\nDefinition 1 (Macro-action) Given a function f(\u00b7), a macro-action, z, is defined as information that connects the current state to the desired goal state within the state space: St+M = f(st,z) where M \u2208 N+.\nThe overall feed-forward process for macro-action discovery by the encoder is as follows:\n$z_t \\sim tanh(N(\\Psi_{z\\mu} (y_t, s_t), \\Psi_{z\\sigma} (y_t, s_t)))$,\nwhere a Multivariate Gaussian distribution N (\u00b7,\u00b7) is constructed with mean z\u00b5 (\u00b7,\u00b7) and variance z\u2030 (\u00b7,\u00b7).\""}, {"title": "IL-Training Scheme", "content": "Imputation here is to find a proper action space embedding Z \u2248 A that satisfies S \u00d7 Z \u2192 S. This is a complex problem, as the encoder and decoder have to concurrently bridge state and action spaces. Without proper treatment, Equation (10) will collapse, providing meaningless information only to lower the loss. Therefore, we modify the existing architecture and loss function of VAE to not only perform the standard VAE operations but also bridge the two spaces by replacing the prior distribution p(z) in the Evidence Lower Bound (ELBO) with action distributions. We also provide the state as input to the decoder to make it a transition predictor.\nThis strategy reshapes the latent space of the VAE by aligning the current belief, z(yt, St), with the action space. In other words, the encoder, I \u00d7 Y \u2192 A, remains within the action space given the state, while the decoder learns a function f(:) : S \u00d7 A \u2192 I in Definition 1 that maps the state and action to the future state. This models the decoder as a cumulative transition function since it predicts M steps forward future state with current state and macro-actions. This shapes the macro-actions as a compressed action embeddings within the action space and confined in the domain of [-1,1] ~ tanh(\u00b7).\nThis represents our novel contribution, which learns the latent macro-action to directly assist the policy in reducing the decision-making challenge in complex multi-task execution using successful goal states from the policy. This prevents the loss of optimal behavior in some tasks while learning others. Additionally, we introduce the ego-state for task-agnostic inference by masking task-related state-wise elements:\nDefinition 2 (Ego-state) Given a state set S, the set S consists of two subsets: ego-state and other-state, such that Ses \u222a Sother = S. The ego-state, Sego, includes only self-state elements.\nRemark 1 An example of an ego state is a blind robot. For the robot that is to perform any arbitrary task (e.g., navigating, grasping, or reaching), the task-agnostic components are only its body-related components: angle of joints and relative arm/body position. While state space decomposition is beneficial for task-agnostic learning, this instead voids out the necessary task information so that inference can stay in high-level space instead of detailed execution.\nThe ego-state is particularly appealing for distributing decision-making into a set of independent roles. Specifically, macro-actions provide high-level action embedding, while the policy executes the detailed actions. Finally, the macro-action is then trained over the encoder parameters 2 and decoder parameter ys by minimizing the following loss:\n$\\min \\limits_{\\Psi}ELBO(y_t, z_t, s_t, s_t^{ego}, s_{t+M}^{ego})$\n$ELBO_1(\u00b7) = L_{IL(KL)} + L_{IL(trans)}$\nGiven the decoder's forward pass $s_{t+M}^{ego} = \\Psi_s(y_t, z_t, s_t^{ego})$, each loss term is given below:\n$L_{IL(KL)} = log \\frac{p(a_t|y_t, z_t, s_t)}{\\Psi_z(z_t|y_t, s_t)}$\n$L_{IL(trans)} = \\sum \\limits_{t=0}^{T} log(\\psi_s(s_{t+M}^{ego} | y_1, z_1, s_t^{ego}))$\nThis approach is notably distinct from previous methods: SD10, VariBad16, LDM17, RL218, and PEARL19, as it involves shaping the prior with the action space and incorporating additional ego-state input into the decoder to shape it as a transitional function.\nIn Equation (12), St+M and sego are goal state and ego-state respectively. The goal state is the state the policy aims to achieve, typically at the designer's discretion, or it can be discovered using additional techniques. In our approach, we construct three definitions of goal state at the end of this section."}, {"title": "Goal State Generation", "content": "The goal state, st+M, adapted per our method, is defined as follows:\n1. Constant Discretization (CD): This naively discretizes the given episodic timelines into several segments. That is, we fix M as constant, thus obtaining a list of goal states: [SM, S2M,...,SnM].\n2. Constant Margins (CM): This method maintains a constant margin between the current state and the goal state, resulting in a list of [sm, $1+M... St+M], given a constant M.\n3. Adaptive at Sub-Task (ST): This uses the inferred sub-task from the HL layer, analyzing when the label changes in timescale. For example, given an array of states S = [$0, $1,..., st] and representations Y = [yo, y1,..., yt], goal states are states where the representations change in time scale (i.e., \u2203t : [yo : yt\u22121] \u2260 [y1 : yt] \u2192 S[t])."}, {"title": "Low-Level Layer", "content": "For policy training, which is the objective of the Low-Level (LL) layer, we chose Proximal Policy Optimization (PPO), a practical solution that effectively reduces sample complexity using an off-policy gradient clipping approach. While there are abundant details about PPO, we briefly introduce its idea here, leaving parametric details in the Appendix. PPO maximizes the following objectives:\n$\\pi_{\\theta} = arg \\underset{\\theta}{max} E[J(\\theta, \\Theta) - \\alpha_1 L(\\theta, \\Theta) + \\alpha_2 \\delta(\\theta)]$\nwhere I (0, I) is the gradient-clipped off-policy advantage for optimality, L(0,I) is the value loss as in Equation (6), 8(0) is the entropy loss for sufficient exploration, and 1 and 2 are scaling parameters.\nNote that our LL is trained without the value loss in Equation (13) since it is trained with the HL objective (Equation (6)).\nThe feed-forward inference of the LL layer is:\n$a = \\pi_{\\theta} (y_t, z_t, s_t)$\nFor effective and efficient collaboration between the macro-action and primitive-action, we additionally construct an intrinsic reward function that adds a bonus to the policy when it follows the given macro-action sign-wise (+/-)\u00b9. The reason for the sign-wise bonus is to assign independent roles in decision-making to macro-actions and primitive actions as high-level and low-level actions respectively.\nThe optimality objective, I (0, I), includes additional intrinsic reward incurred achieving macro-actions, starting with the non-clipped advantage function below.\n$\\hat{A}(s,a) = r + V(s') - V(s)$\nwhere r = rin + rext and V(s) = r + V (s')\nTo carefully design an intrinsic reward signal that does not disrupt the original task optimality, it is scaled with an extrinsic reward signal. The one-time step intrinsic reward function is defined as:\n$r_{in} = r_{ext} \\frac{1}{|A|} \\sum \\limits_{a} \\mathbb{I}[sgn(z) = sgn(a)]$\nThe intrinsic reward signal rin is high only when the extrinsic reward signal is high and is bounded by the maximum value of the extrinsic reward. This enables the policy to maintain optimal behavior in tasks it has already mastered while still allowing for exploration in tasks it is still learning. Pseudo-code is given in Algorithm 1."}, {"title": "Analysis", "content": "Here, we provide a mathematical view of macro-action discovery. To understand the trajectory of the system, consider the state transition over a discrete time interval [t,t +M]. Denoting the transition function at time t as T\u2081(St, at) = St+1, the trajectory from time t to t + M can be represented as a composite function of T\u2081(\u00b7):\n$s_t, s_{t+1},\u2026s_{t+M} = (T_{t+M-1} \u25cb \u2026 \u25cb T_{t+1} \u25cb T_t)(s_t,a_t,\u2026,a_{t+M-1})$\nwhere Tt+M-10\uff65\uff65\uff65 \u25cb T\u2081+1 0 T\u2081 denotes the composition of transition functions with the input of state-action pairs between time t and t+M-1.\nIn a model-free setting, the objective is to determine the next action at+1 based on the current state st, not the whole trajectory. Thus, we define a function approximator for learning such composite functions as in Equation (17) with one single input:\n$s_{t+M} = f(s_t, z_t|\\psi)$\nwhere the neural approximator y has at least two components for learning the composite function (T\u2081+M\u221210\uff65\uff65\uff65 \u25cb T\u2081+1\u00b0 T\u2081) with the decoder ys, and the action summary, z, of (at,\u00b7\u00b7\u00b7,at+M\u22121) with the encoder y. This reflects the design intent of an intermediate layer with an encoder and decoder for learning the composite function and the macro-action, respectively."}, {"title": "Challenges and Solutions", "content": "We additionally addressed two major challenges to enhance overall performance. First, we tackle the \"curse of hierarchy,\" which refers to the mismatch in focus across layers. As the policy gathers new trajectories to maximize rewards, higher-level layers must adapt to new incoming dynamical information. This issue, caused by transitional shifts in hierarchical RL20, is mitigated through (1) a data buffer that refreshes with recent trajectories, (2) independent roles for each layer by designing each layer to achieve a specific goal and preventing gradient overflow off layers, and (3) streamlined architectures to avoid model complexity with which the higher-level models easily track the traces of low-level policy.\nSecond, when learning multiple tasks, policies often prioritize easier tasks, leading to higher sample complexity and neglect of difficult tasks. To counter this, we adopt reward shaping with an asymptotic log function that boosts low rewards while maintaining the same increment rate beyond a specified point. This makes early rewards for difficult tasks are emphasized. For any non-negative rewards, the shaping function is:\n$g(x) = \\begin{cases} \\ln(e^{-a}+1), & \\text{if } x \\leq a, \\\\ (x-a)+a, & \\text{otherwise.} \\end{cases}$\nwhere a is a hyperparameter within the reward range. The reshaped rewards, g(R(st,at)), are visualized with a 3 in Figure 3."}, {"title": "4 Experiments", "content": "We utilized MetaWorld21 as our testbed. MetaWorld provides a structured environment for multi-task and meta-learning, designed to evaluate algorithms on their generalization capabilities. It improves upon earlier frameworks by systematically defining tasks with shared characteristics."}, {"title": "Evaluation Metric", "content": "MetaWorld introduced a success metric to complement reward-based evaluations. In our settings, we use an average episodic success metric that not only indicates task success but also measures how quickly the task is accomplished. Unlike21, which reports the maximum success rate achieved over the training process, or10, which measures success at the final time step without considering the time taken, our metric provides a more nuanced view of performance. In particular, we measure how macro-actions effectively guide policy via time-wise success considerations. However, we provide additional results for completeness using the previous success metric in10.\nThe evaluation is conducted using ML-10. This consists of 10 training tasks and 5 test tasks, expecting representations learned from training tasks transferred to performance in testing tasks. Sampling is based on 5 episodic samples per iteration for each task (a total of 50 trajectories). Given each episode lasting 500-time steps, 7.5k iterations are taken for training."}, {"title": "Evaluation", "content": "Evaluations were conducted with 5 seeds using the same hyperparameters and environmental settings for our method and baselines to allow a detailed comparison of convergence speed and final performance. The table presents average training"}, {"title": "5 Related Workds", "content": "Parameter-sharing explores \"which model components to share\" across tasks. Based on metadata like task labels, models determine the extent of weight and bias sharing, making non-shared parameters task-specific. Fully shared parameter architectures are termed \u201chard-parameter sharing\", while those with varying degrees of task-specific architectures are \u201csoft-parameter sharing\". This approach proves effective and straightforward in tasks with slight deviations across tasks22\u201324.\nTask grouping, in contrast, focuses on identifying tasks that can effectively share parameters by forming task groups. Unlike parameter-sharing, which assigns distinct parameters to each task, task-grouping simplifies model structures by grouping similar tasks together and treating them as a single task25\u201327.\nWhile effective, these approaches have drawbacks, requiring domain-dependent expert knowledge and careful network separation. Consequently, relevant to ours, several works have attempted to discover shared structures across tasks, z, and condition them into the input space, \u03c0(as,z). Instead of merely expanding the input space, this method enriches it with relevant task-wise information, merging operational spaces that share similarities and significantly reducing the information load on the policy. 19 was a pioneering work that discovered latent task-wise information via posterior sampling by training alongside the value function. Additionally,10,16 utilize recurrent encoder and dynamic decoder architectures for representation learning through state and reward dynamical predictions. This approach effectively discovers task-wise latent information, integrates similar tasks, and distinguishes different ones, facilitating multi-task and meta-learning.\""}, {"title": "6 Conclusions", "content": "Our approach introduces a novel hierarchical decomposition to alleviate the decision-making burden of policies in the context of hard-parameter sharing. We propose macro-actions, which provide compressed action information linking the current state to the handcrafted goal state. This is achieved by learning to produce both macro-actions and a composite transition"}, {"title": "Schematic Summary", "content": "We employ three hierarchical layers, each with distinct roles to enhance the learning process. Specifically, we designed high, intermediate, and low-level layers for task inference, macro-action discovery, and primitive actions, respectively, as illustrated in Figure 2.\n\u2022 High-level layer: The high-level layer is task-specific, analyzing the type of task representations, y, the agent is solving.\n\u2022 Intermediate-level layer: The intermediate layer converts the representation y into a task-agnostic macro-actions z, providing high-level idea of cumulative actions embeddings.\n\u2022 Low-level layer: Proximal policy optimization is used to reduce the sample complexity of meta-learning. Its input space is conditioned with each output of higher-level layers.\nThis training scheme simplifies policy decision-making using task representation with macro-actions and encoding valuable transitional information from the current state to the goal state. This approach is beneficial for complex domains with multiple tasks, sparse rewards, and long horizons. Moreover, relying on the policy's exploration to define goal states in the IL layer reduces the decision burden by helping the policy retain previously learned tasks while acquiring new ones."}, {"title": "Implementation Details", "content": "Here, we describe the details of network and training parameters for our method Table 3 and baselines Table 4 with the source codes we used."}, {"title": ".1 Experimental Remarks", "content": "It is noteworthy that the experiments were conducted with identical parametric settings, as well as the same number of epochs and iterations per epoch for HiMeta, SD, and PEARL. The main discrepancy between our results and those presented in10 arises from the MetaWorld version, which received a substantial update in 2023, resulting in significant changes to certain tasks. Additionally, the PEARL results in SD are based on 1000 epochs and 4000 iterations, as they reported using the initial parameters provided by garage\u2074. This number of iterations is considerably larger than ours, which consisted of 40 epochs and 200 iterations. Therefore, the large variance in the PEARL results can be attributed to under-converged results and our commitment to using exactly equivalent learning settings for a precise comparison of performance in terms of efficiency.\n\u2022 SD: https://github.com/suyoung-lee/SDVT\n\u2022 PEARL: https://github.com/rlworkgroup/garage/pull/2287"}]}