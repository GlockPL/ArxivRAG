{"title": "Economics of Sourcing Human Data", "authors": ["Sebastin Santy", "Prasanta Bhattacharya", "Manoel Horta Ribeiro", "Kelsey Allen", "Sewoong Oh"], "abstract": "Progress in AI has relied on human-generated data, from annotator marketplaces to the wider Internet. However, the widespread use of large language models now threatens the quality and integrity of human-generated data on these very platforms. We argue that this issue goes beyond the immediate challenge of filtering AI-generated content-it reveals deeper flaws in how data collection systems are designed. Existing systems often prioritize speed, scale, and efficiency at the cost of intrinsic human motivation, leading to declining engagement and data quality. We propose that rethinking data collection systems to align with contributors' intrinsic motivations-rather than relying solely on external incentives-can help sustain high-quality data sourcing at scale while maintaining contributor trust and long-term participation.", "sections": [{"title": "1. Human Data in Crisis", "content": "Artificial Intelligence relies heavily on human-generated data to develop ever more capable models and systems that can emulate human-like intelligent behavior. The currently used sources of human data include: (1) human annotations, from data collection platforms (e.g., Amazon MTurk) and (2) raw data, from the Internet (e.g., Wikipedia, social media platforms). Our understanding of how to effectively use these two data sources has been a driving force behind the two most prominent eras of artificial intelligence: the Deep Learning era which began with AlexNet [Krizhevsky et al. 2012] in 2012 and facilitated by ImageNet [Deng et al. 2009] collected at scale through MTurk, and the Pre-trained Language Models era ushered in by BERT [Devlin et al. 2018] in 2018 and enabled by the availability of large-scale Internet data.\nHowever, the emergence of the third Generative Large Language Chat models era, marked by ChatGPT in 2022 [OpenAI 2023], has made human-like language generation ubiquitous and accessible to the general public, disrupting the very mechanisms of data sourcing that have historically enabled AI development. The indiscriminate usage of large language models (LLMs) impacts the previous two key sources of human-produced data: participants in existing data collection systems are turning to LLMs to expedite their tasks [Veselovsky et al. 2023b,a], and the Internet is being flooded with LLM-generated content [Brooks et al. 2024]. This makes it increasingly challenging to obtain and discern authentic human-generated data, raising concerns about a looming shortage of human data needed for continued Al progress. To compensate, machine learning has further leaned into synthetic data-either to mimic human annotations [Dubois et al. 2024] or emulate human behavior [Argyle et al. 2023; Park et al. 2023, 2022]- albeit not yet at the highest quality [Geng et al. 2024] and facing other challenges, such as model collapse [Shumailov et al. 2024; Taori and Hashimoto 2023], keeping the ember of human-generated data still alive [Ashok and May 2024].\nWe argue that these flaws have always existed in data collection platforms but have been recently amplified by LLMs to the point where their very existence is being called into question [Pieces 2025]. Specifically, at the heart of this problem lies the issue of human incentives and motivations for contributing data-one that cannot be solved by simply increasing external rewards like pay but requires careful attention to intrinsic motivations that drive people to engage willingly and actively on platforms, which in turn leads to better-quality data. Designing new data collection systems necessitates a critical examination of the flaws in existing ones.\nIn this paper, we analyze the current data requirements in machine learning and how existing data collection systems attempt to meet them. We open up the black box of data collection-complex socio-technical systems shaped by human behaviors, idiosyncrasies, and technical constraints-drawing from popular theories and experiments in psychology, sociology, and economics. In doing so, we examine the quantity-quality tradeoff and argue that, while this tradeoff may not be entirely eliminable, the overall quality and quantity of data can still be improved by identifying and removing factors that undermine intrinsic human motivations. Given that data collection systems operate within broader economic and social structures, we also complement academic research with real-world discourse and case studies of different data collection strategies. Finally, we explore novel paradigms, including games, that offer promising directions for the future of human data sourcing."}, {"title": "2. Characterizing Human Data Needs", "content": "Progress in machine learning depends on the availability of data at a sufficient scale to inductively learn patterns from it [Hoffmann et al. 2022; Kaplan et al. 2020]. This need for data has grown exponentially as learning algorithms have evolved from statistical to deep learning and pre-trained language models. The quantity of data has uncontestedly been the key consideration for the field [Sutton 2019], with any data source that adds several orders of magnitude to the size of existing datasets, such as data from the Internet, being considered indispensable. A general trend in machine learning regarding data sourcing, especially after the advent of pre-training with BERT [Devlin et al. 2018], has been to leverage sources of large data wherever they can be found, such as BookCorpus [Zhu et al. 2015], Wikipedia [Raffel et al. 2020], Reddit [Gokaslan and Cohen 2019], and CommonCrawl [Common Crawl 2021].\nRecently, however, as datasets have grown larger, the importance of quality has become more apparent [Lee et al. 2021; Nguyen et al. 2022; Zhou et al. 2024]. While learning algorithms have improved in extracting signal from noise, they still have limits when faced with excessive noise or irrelevant data [e.g., DataComp-LM discards 99% of data and Text-Image DataComp filters out 70%; Gadre et al. 2024; Li et al. 2024]. Data quality has long been assumed to matter, but its significance has become clearer than ever as models trained on external proprietary datasets consistently outperform others on benchmarks and in real-world applications [Brown et al. 2020]. This outperformance-often attributed to access to \"high-quality\" proprietary datasets, such as paywalled content or licensed secondary sources [Bommasani et al. 2021]-has pushed the data quality discourse to the forefront and is now a high priority in machine learning."}, {"title": "Human Data Sourcing Desiderata", "content": "(1) High Quality\nCollecting high-quality data with a strong signal-to-noise ratio for training ML models.\n(2) High Quantity\nCollecting data large enough to satisfy the exponentially increasing complexity of tasks.\nWhile both high quality and high quantity are critical for data sourcing, they often come at the expense of each other-improving one typically leads to a decline in the other. However, this trade-off is not an inherent property of data itself but rather a consequence of system design choices. One way to conceptualize this trade-off is as resembling a Pareto frontier, as illustrated in Figure 1.\nThis trade-off explains why data collection systems struggle to balance quality and quantity. Platforms prioritizing quality, like freelance job platforms (e.g., UpWork), tend to be slower with lower output, while high-throughput systems, like rapid crowdwork platforms (e.g., MTurk), scale efficiently but often sacrifice consistency and quality [Douglas et al. 2023]. While this quantity-quality tradeoff may never be fully eliminated for any designed data collection system, it is not a fixed constraint-rather than eliminating the trade-off, the key is to expand the frontier by addressing structural inefficiencies in incentive design, annotation methods, and human oversight.\nThe dynamics of the quantity-quality trade-off are shaped by multiple interacting and, often, latent factors. Untangling these factors requires opening up current data collection systems and examining their trade-offs through the lens of human behavior, organizational processes, and technical constraints. At a system level, quality depends on balancing intrinsic motivation with external incentives, while quantity is largely driven by process efficiency, often through task fragmentation and parallelization. However, excessive fragmentation can erode intrinsic motivation, leading to long-term declines in quality. This self-reinforcing cycle lies at the heart of the quantity-quality trade-off in data collection system design."}, {"title": "3. Position vs. Current Stance", "content": "Position: Sustaining human-generated data for ML requires shifting focus toward intrinsic human motivations.\nMajoritarian Stance: Data quality is a major consideration in machine learning, and many researchers and companies are actively exploring how to best collect high-quality human-produced data. Researchers recognize that incentives influence the level of effort contributed, and as a result, they often rely exclusively on financial incentives to encourage greater effort in data production. While incentives are important, we contend that solely relying on financial rewards-counter to intuition-risks backfiring by reducing the quality of the data collected. Instead, ML researchers should prioritize enhancing the intrinsic motivation of human participants, using external incentives sparingly as supportive nudges rather than primary drivers."}, {"title": "4. Understanding Data Quality", "content": "While quantity is easily measurable and increasingly attainable through new data-sourcing methods, quality has become ever more elusive. As data availability has surged, the question of what constitutes \"high-quality\" data is increasingly debated in machine learning. The challenge then is to understand the existing notions of data quality and explore ways to make it more certain, before dissecting data collection systems.\nPrior Definitions. Defining \"data quality\" has long been a challenge in machine learning, as it lacks a universal, quantifiable standard. While it is widely acknowledged that human-generated data varies in quality (e.g., curated datasets from specific websites being more reliable than scraped content), there is no single, absolute definition for what makes data \u201chigh-quality\". Attempts to define quality span both subjective and objective perspectives. Subjectively, quality is often linked to trustworthiness-Wikipedia, for instance, is generally regarded as more reliable than personal blogs [Albalak et al. 2024; Soldaini et al. 2024]. Objectively, quality has been measured using statistical metrics (e.g., readability) or modeled metrics, such as GPT-3 Quality Filters [Gururangan et al. 2022] and Data-Comp's curated datasets [Li et al. 2024], which define quality in the context of their downstream use.\nNaturalness as the Basis of Data Quality. Without clarity on data's intended use, defining quality becomes challenging. We contend that one of the most intuitive ways to conceptualize quality-without presupposing a specific application-is by anchoring it in naturalness: how people behave in routine activities, online or offline, in an authentic manner. Unlike other definitions that rely on the perceived reliability of the source or application-specific criteria, naturalness provides an observable and generalizable signal for what constitutes high-quality data. This pattern is evident in organic data sources, where humans naturally generate valuable data through meaningful tasks, such as editing Wikipedia, participating in Reddit discussions, or sharing artwork and photos on platforms like Flickr. In these settings, data is generated naturally, often without direct external incentives, making it more representative of authentic human behavior.\nIs There a Case for Naturalness in AI Training? Naturalness has already been central to pretraining, where large-scale internet data-capturing naturally occurring human behavior-has been crucial to the success of LLMs. But its importance extends beyond achieving generalization. Even in supervised fine-tuning, where data is tailored for task-specific applications, naturalness matters, as collected data should ideally reflect real task engagement rather than behavior shaped by artificial constraints or incentives.\nInterestingly, this divide between pretraining and fine-tuning mirrors a broader debate in AI. Artificial Intelligence (AI)-as envisioned by John McCarthy [McCarthy 1987]-aims to achieve human-like general intelligence and, therefore, benefits from diverse, free-flowing human interactions, much like those found in pretraining data. In contrast, Intelligence Augmentation (IA)-as suggested by Douglas Engelbart [Engelbart 1962]-prioritizes enhancing human intelligence through specialized tools, requiring goal-oriented human interactions towards performing a task, similar to fine-tuning data. In both cases, naturalness remains key but plays distinct roles: capturing free-flowing or goal-oriented human interactions, free from artificial constraints or incentives.\nTo that end, the use of LLMs in data collection is not inherently bad-what matters is how they are used. The real risk to naturalness comes from indiscriminate, careless reliance on AI as a shortcut, rather than as a tool for balancing meaningful engagement and productivity. Instead of aggressively policing AI use, the focus should be on designing environments where contributors engage with tasks in ways that make shortcuts feel unnecessary-just as one wouldn't feel compelled to take shortcuts in a personally meaningful hobby. For example, a survey respondent outsourcing an entire essay writing task to Al without any personal input demonstrates an unwillingness to engage meaningfully with the task-this is the kind of AI use that undermines data quality."}, {"title": "5. Human Factors in Quality- Motivation & Incentives", "content": "Data collection platforms used in machine learning, such as MTurk, Prolific, UpWork, and ScaleAI, are designed in ways that include compensation structures, that directly influence both effort and data quality. While rapid crowdsourcing platforms (e.g., MTurk) favor low-effort and low-pay tasks that can scale easily, the quality of task output remains unreliable. In contrast, freelance job platforms tend to favor high-effort and higher-pay gigs which require more deliberate and engaged participation, often leading to higher quality outputs.\nThis difference aligns with a straightforward intuition-higher pay leads to greater effort and better-quality contributions [e.g., Ho et al. 2015; Laux et al. 2024; Mason and Watts 2009; Shah and Zhou 2016]. This assumption drives much of the current incentive-based data collection paradigm, where the goal is to use external compensation as a lever to elicit higher-quality data, when required. However, while external rewards can drive greater effort, evidence suggests that they are not always the primary determinant of high-quality engagement.\nFor instance, some of the highest-quality human contributions come from platforms where users are not financially compensated at all, such as Wikipedia, Reddit, and open-source communities. Here, participants contribute not because of pay, but because they find the activity meaningful, socially rewarding, or aligned with personal interests [Forte and Bruckman 2005; Lampe et al. 2010]. These platforms challenge the idea that quality data generation must always rely on financial incentives, illustrating that intrinsic motivations can sustain long-term engagement without depending on financial rewards as the primary driver.\nWhile many assume that external incentives and intrinsic motivation are correlated, research shows that their relationship is far more complex.\nOverjustification Effect [Lepper et al. 1973] explains how external rewards can diminish intrinsic motivation and affect task performance. In a classic experiment, preschool children who already enjoyed drawing were divided into three groups: (1) those who were promised and received a reward, (2) those who received an unexpected reward, and (3) those who received no reward. This experiment revealed two notable outcomes: first, children in the expected-reward group spent significantly less time drawing voluntarily after the reward was removed, compared to the other groups."}, {"title": "6. Human Factors in Quantity- Efficiency through Fragmentation", "content": "Data collection systems differ not only in how they compensate contributors but also in how they structure tasks for scalability. At one end, rapid crowdwork platforms fragment tasks into micro-tasks (e.g., HITs on MTurk) that take seconds to complete, optimizing for speed and mass throughput [Malsburg 2024]. Platforms like Prolific handle slightly larger but still modular tasks, spanning minutes to hours [Prolific 2024]. On the other end, freelance job platforms (e.g., UpWork) structure work as full projects, lasting days or weeks and offering greater autonomy and depth of engagement [at Home Smart 2024; Upwork 2024a,b].\nFragmentation into repeatable units that can be completed in a consistent and orderly manner allows tasks to be parallelized across multiple workers, replacing the traditionally serial process of creation. Many innovations and processes begin as creative, effortful tasks-akin to System 2 processes, requiring deliberate, conscious effort [Kahneman and Tversky 2013]. However, to scale, they are often refined into a System 1 process, where execution becomes fast, automated, and intuitive. This transformation-breaking down complex, uncertain tasks into simpler, repeatable steps-underpins mass production systems."}, {"title": "7. Elevating the Quality-Quantity Trade-off: Rethinking Control", "content": "Data collection systems have long operated under the assumption that control at the task level-through explicit instructions, incentive structures, and quality enforcement mechanisms-is necessary to ensure both high-quality and high-quantity data. However, as we have discussed, these very mechanisms often introduce unintended side effects. External incentives, while effective in driving participation, tend to crowd out intrinsic motivations over time, leading to disengagement and lower-quality contributions. Similarly, excessive task fragmentation, though useful for efficiency and scalability, can erode a sense of purpose and hence leads contributors to disengage from the task itself-resulting in an over-reliance on shortcuts, manifesting in careless task completion and non-judicious use of LLMs.\nIn contrast, data that we obtain from systems not intentionally designed for data collection-such as Wikipedia, Reddit, and open-source projects-demonstrate an alternative paradigm. These platforms do not enforce control at the task level but instead create environments where intrinsically motivated contributors engage meaningfully. Crucially, this lack of task-level control removes two major pitfalls seen in structured data collection systems: it prevents (a) the crowding-out effect, where external incentives replace intrinsic motivation over time, and (b) excessive fragmentation, ensuring that contributors remain connected to their purpose of participation. Taking inspiration, ceding control at task level could be the key to pushing the pareto-frontier forward.\nCeding Control at the Task Level. Rather than controlling individual (micro) tasks, data collection systems can benefit from structuring conditions that naturally guide contributor engagement. Moving from direct task management to a broader, environment-driven approach discourages individuals from attributing their participation to external rewards, helping preserve intrinsic motivation [Bem 1972].\nHowever, this shift presents a new challenge: relinquishing fine-grained control over tasks means that collectors must instead focus on shaping engagement at a more systemic level. Coarse-grained control-where engagement is influenced through platform design, incentives, and structural conditions-takes longer to align with desired outcomes and demands greater up-front effort. But once in place, it can lead to more sustainable data collection, enabling both higher-quality and higher-quantity contributions, as seen in rare but influential examples.\nDeployed Robots are a prime example of achieving this delicate balance. For example, robotic vacuum cleaners (e.g., Roomba) provide utility to users, by cleaning their homes, while simultaneously collecting spatial and navigation data that improves future performance [Astor 2017]. Users engage with the system for its primary function, yet their interactions naturally generate high-quality data that feeds back into the Al's development [Brynjolfsson and McAfee 2014]. This data collection approach also scales effectively, as data is gathered continuously and passively as a result of users' routine behaviors, without requiring any additional effort towards data contribution. This model extends to more complex, high-stakes deployments, such as electric vehicles equipped with driver-assist and self-driving features. As an example, consider how Tesla uses real-world driving data from its fleet to refine its self-driving AI algorithms, which ultimately benefit car owners by improving the technology [Karpathy 2021; Tesla 2021] and can drive future innovation for the company (e.g., driverless Robotaxi). Similarly, Waymo operates self-driving taxis in real-world environments and has gathered large-scale data that has proven valuable for advancing computer vision research [Sun et al. 2020].\nHowever, replicating such large-scale, product-driven data collection systems is exceptionally difficult. They demand massive hardware infrastructure, well-articulated and trusted benefits for both users and companies, and real-world applications with extensive safeguards and privacy protections. For entities whose primary goal is simply to collect human-generated data, establishing such ecosystems solely for this purpose is neither feasible nor sustainable.\nMore importantly, these symbiotic relationships rely on a delicate balance of implicit or explicit social contracts, mutual trust, and fair distribution of costs and benefits, as explained by theories of social interaction and exchange-such as Social Exchange Theory [Homans 1958] and Social Contract Theory [Rousseau 1762]. When the benefits clearly extend beyond the primary parties-for example, when collected data is repurposed to serve third parties-this balance can be disrupted. If users feel that their contributions are being exploited without fair reciprocity, trust erodes, and they may come to see the system as exploitative or unjust.\nThese feelings of mistrust and exploitation are already prevalent in AI, particularly in creative and knowledge-sharing communities. Artists have protested against their work being scraped to train Al models without consent or compensation [Jiang et al. 2023]1, with many calling for stronger protections against AI-generated art [Guardian 2025]. Similarly, Stack Overflow users, frustrated by their contributions being used for external profits, have intentionally altered or deleted their posts to hinder AI training-leading to bans, boycott, and subsequent decline in engagement on the platform [Hardware 2024; Technica 2024]. These examples highlight the fragility of trust in data collection and the potential consequences when contributors feel that their data is being repurposed beyond its original intent."}, {"title": "8. Games as a New Pareto-Frontier?", "content": "Games are uniquely positioned at the intersection of structured environments and intrinsic motivation. Designed worlds crafted by game developers set the rules and constraints, yet within them, players engage voluntarily, driven by curiosity, competition, and creativity [Koster 2005]. Unlike traditional work, where tasks are often externally imposed, games offer a space where challenge and engagement emerge naturally, sustaining long-term participation without the need for direct financial incentives.\nFor most players-aside from professional esports competitors-games are played purely for enjoyment. At the same time, they require diverse forms of reasoning, strategy, and problem-solving, making them a rich ground for capturing complex human behaviors and decision making [as already evidenced by their role in evaluating intelligence; Berner et al. 2019; FAIR; Silver et al. 2016; Vinyals et al. 2019]. In this way, games hint at a new frontier-one where many structured environments and organic engagements seamlessly coexist. This intersection offers a compelling model for AI training, where intrinsically motivated interactions can yield structured, high-quality data without the pitfalls of external incentive-driven systems.\nGames as a Tool for Data Collection. Games have long been explored as a tool for large-scale human annotation and AI training, most notably through von Ahn's Games with a Purpose (GWAP) [Von Ahn 2006]. One of the earliest and most influential examples was the ESP Game [Von Ahn and Dabbish 2005], introduced in 2004, which engaged thousands of players in a collaborative tagging game, generating millions of image annotations. While players simply enjoyed the game, their interactions helped bootstrap Google Image Search [Guardian 2006], which previously relied only on filenames, as large-scale labeled datasets like ImageNet were not available until 2009 [Deng et al. 2009].\nVon Ahn argued that the billions of hours spent on games-such as the 9 billion hours on Solitaire in 2003 alone, enough to build the Empire State Building in 6.8 hours or the Panama Canal in a day-could be repurposed for more meaningful tasks, inspiring the broader Games with a Purpose framework [Law and Von Ahn 2011]. Other notable games in this series included Peek-a-boom [Von Ahn et al. 2006b], which collected image segmentation data, and Verbosity [Von Ahn et al. 2006a], designed to gather commonsense factual knowledge.\nGames in Mainstream ML. While recent efforts towards this in machine learning have not reached the scale of GWAP, they have made notable progress. Examples include Google's Quick-Draw [Ha and Eck 2017] and AllenAI's Iconary [Clark et al. 2021], which focus on collecting freehand drawing data, and AI21's HumanOrNot [Jannai et al. 2023], which gathers conversational data through a gamified Turing Test [Dugan et al. 2023].\nHowever, developing entirely new games for data collection presents significant challenges. Machine learning researchers often lack expertise in designing engaging gameplay, making it difficult to ensure both high-quality data and sustained participation. To navigate this, some approaches have focused on repurposing existing games rather than building from scratch. For example, Family Feud has been adapted to generate QA pairs [Boratko et al. 2020], and Minecraft has been used as a platform for collecting conversational dialogues [Narayan-Chen et al. 2019].\nOn a smaller scale, some efforts have explored gamification, introducing game-like mechanics into traditionally non-game tasks to enhance engagement, without requiring a full game environment. For instance, CommonsenseQA 2.0 incorporates elements such as scoring, competition, and progression to make data contribution more engaging, improving both user participation and data quality [Talmor et al. 2022].\nDesign Considerations. Designing data collection games requires solving multiple challenges at once-the most critical challenge is optimizing data utility while preserving intrinsic player motivation. This involves balancing two often competing objectives:\n\u2022 Ensuring that collected data meets the needs of AI/ML tasks- requiring structure, reliability, and task relevance.\n\u2022 Designing a player experience that remains engaging over time-avoiding disengagement due to artificial constraints or coercive incentives.\nAchieving this balance is a multi-objective optimization problem that requires close collaboration between ML researchers and game developers. This design space spans multiple approaches, including creating entirely new games optimized for data collection, leveraging existing games to re-purpose organic player interactions, or introducing game-like elements into existing data collection tasks to enhance engagement. Each of these approaches comes with trade-offs in control, scalability, and long-term sustainability.\nWhile prior efforts have successfully optimized for quality and quantity, sustaining trust has proven far more elusive, leading to short-lived or unsustainable solutions. Historical implementations offer valuable lessons-GWAP demonstrated the potential for high-quality, high-quantity crowdsourced data but did not endure over time. In contrast, ReCAPTCHA [Von Ahn et al. 2008] conceptualized in 2008, used for annotating books and self-driving data [Anton 2018; O'Malley 2018], remains widely used today but has blurred the line between voluntary participation and coercion, with users reporting frustration and annoyance [Searles et al. 2023a,b], alluding to lack of trust. These examples illustrate the challenge of designing systems that optimize for all three aspects: high-quality, high-quantity, and high-trust data collection.\nThis challenge becomes even more evident when considering long-term sustainability. While data collection games have proven effective in gathering large-scale, high-quality data, few have endured over time. To our knowledge, no sustained data collection effort through games exists in machine learning. However, other research fields provide examples of systems that have successfully maintained engagement and trust over extended periods. Scientific discovery platforms such as Zooniverse for citizen science in astronomy [Cardamone et al. 2009; Lintott et al. 2008], Lab in the Wild for HCI and psychology user studies [Reinecke and Gajos 2015], FoldIt for protein folding [Cooper et al. 2010; Khatib et al. 2011], and even Eve Online for epidemiology research [Kafai and Burke 2016] demonstrate how engagement, motivation, and trust can be sustained without relying on short-lived external incentives. Psychology and cognitive science studies [Allen et al. 2024] further reinforce that game-like participation can be structured in ways that maintain engagement over time."}, {"title": "9. Reviving Human Data Sourcing: A New Path Forward", "content": "In the search for sustainable ways to collect human-generated data for AI, we examined existing data collection systems through the lens of the quantity-quality trade-off which arises as a consequence of system-level design choices that make achieving both difficult. While such trade-offs are inherent to designed systems, they can be mitigated by carefully addressing the factors that shape them.\nTo unpack the latent factors involved, we examined two core drivers of this trade-off: quality, influenced by motivation and incentives, and quantity, shaped by fragmentation and efficiency. We examined how over-reliance on external incentives and excessive task fragmentation erode intrinsic motivation, ultimately leading to long-term declines in data quality. To counteract this, we propose shifting from controlling individual tasks to designing structured and trustworthy environments that sustain engagement while still allowing data collectors to shape the kind of data they gather.\nWe explore games as a promising new frontier for data collection. By naturally balancing structure with voluntary participation, they offer a scalable, high-quality, and sustainable alternative to conventional data collection systems-one that is increasingly important as traditional approaches face growing limitations."}]}