{"title": "NAR-*ICP: Neural Execution of Classical ICP-based Pointcloud Registration Algorithms", "authors": ["Efimia Panagiotaki", "Daniele De Martini", "Lars Kunze", "Petar Veli\u010dkovi\u0107"], "abstract": "This study explores the intersection of neural networks and classical robotics algorithms through the Neural Algorithmic Reasoning (NAR) framework, allowing to train neural networks to effectively reason like classical robotics algorithms by learning to execute them. Algorithms are integral to robotics and safety-critical applications due to their predictable and consistent performance through logical and mathematical principles. In contrast, while neural networks are highly adaptable, handling complex, high-dimensional data and generalising across tasks, they often lack interpretability and transparency in their internal computations. We propose a Graph Neural Network (GNN)-based learning framework, NAR-*ICP, which learns the intermediate algorithmic steps of classical ICP-based pointcloud registration algorithms, and extend the CLRS Algorithmic Reasoning Benchmark with classical robotics perception algorithms. We evaluate our approach across diverse datasets, from real-world to synthetic, demonstrating its flexibility in handling complex and noisy inputs, along with its potential to be used as part of a larger learning system. Our results indicate that our method achieves superior performance across all benchmarks and datasets, consistently surpassing even the algorithms it has been trained on, further demonstrating its ability to generalise beyond the capabilities of traditional algorithms. We will make the code available for NAR-*ICP and the dataset generation on github.com/ori-mrg.", "sections": [{"title": "1 Introduction", "content": "Algorithms serve as the foundation for tackling a wide range of robotics tasks, from path planning and optimisation to perception and control. Classical robotics algorithms are valued for their inherent modularity, ensuring reliable and consistent task execution. Their interpretable nature enables transparency and accountability in robotic operations, which are essential for understanding and validating the behaviour of robotic systems. However, traditional algorithms require strictly pre-defined data specifications, limiting their ability to process raw inputs and handle complex datasets.\nNeural networks, in contrast, excel in efficiently handling raw sensor inputs and demonstrate robustness to noisy and complex data. By learning intricate patterns and dependencies within the input datasets, neural networks effectively generalise, performing reliably even in diverse and previously unseen scenarios. However, they lack interpretability, which poses challenges for transparent and accountable robotics operations.\nTraditionally, neural networks have been trained using the ground truth directly as the supervisory signal, establishing direct mappings from raw inputs to the respective outputs without providing insights into their internal computational steps. This approach often leaves a gap in understanding the decision-making processes of these models. Aiming to combine the strengths of classical algorithms and neural networks, the Neural Algorithmic Reasoning (NAR) (Veli\u010dkovi\u0107 and Blundell 2021; Veli\u010dkovi\u0107 et al. 2020) framework integrates algorithmic computations within the latter. NAR achieves this by training a network to robustly approximate an algorithm by teaching it to mimic its reasoning through the intermediate algorithmic steps and outputs (Numeroso et al. 2023). This hybrid approach offers flexible, scalable, and robust operation in complex and diverse inputs, robustness to noise and variability, and interpretable behaviour for prior black-box models. These features make this method well-suited for a wide range of tasks while bridging the gap between traditional algorithms and neural networks in robotics.\nIn this work, we focus on classical robotics perception algorithms, specifically addressing pointcloud registration as the primary task to be solved with NAR. We focus on Iterative Closest Point (ICP)-based algorithms as they are a fundamental methodology for rigid registration, widely accepted and used in robotics (Zhang et al. 2021). In addition, the compositional structure and iterative nature of ICP-based algorithms closely align with the characteristics of the algorithms in the CLRS Algorithmic Reasoning Benchmark (Veli\u010dkovi\u0107 et al. 2022), making them a strong candidate for extending the latter in the robotics domain. Here, we train Graph Neural Networks (GNNs) to mimic these traditional pointcloud registration algorithms on various complex and real-world datasets, inheriting the interpretability of the algorithms within the networks and surpassing their performances."}, {"title": "2 Related Work", "content": "This work intersects three key domains: robot learning, neural algorithmic reasoning, and pointcloud registration. In this section, we provide a brief overview of relevant works within each field, highlighting those that have particularly influenced our approach. We also discuss how our proposed method differs from and builds upon existing research.\nRobot Learning. Learning methods in robotics have shown significant advancements, enabling robots to adapt to complex and dynamic environments. Imitation learning mimics expert demonstrations, reducing the need for extensive trial-and-error training (Billard and Grollman 2012; Celemin et al. 2022). On the other hand, reinforcement learning allows robots to learn optimal policies through trial-and-error (Kober et al. 2013; Singh et al. 2022). Transfer learning techniques enable robots to transfer knowledge acquired in one task to another, significantly improving learning efficiency and performance in new tasks (Zhuang et al. 2020). Meanwhile, in multi-task learning, robots learn multiple tasks simultaneously, sharing knowledge across to enhance overall learning efficiency (Zhang and Yang 2021). In recent years, foundation models have demonstrated impressive performance in multiple domains, leveraging extensive pre-training on large datasets to develop versatile representations which can be fine-tuned for specific applications (Firoozi et al. 2023). Alternatively, GNNs learn rich representations of spatial, temporal, and relational data to address complex tasks (Pistilli and Averta 2023). Graph learning fundamentally interprets relationships between interconnected entities represented in graph form by iteratively aggregating information from related elements, effectively capturing complex relationships in datasets. In our proposed methodology, we leverage NAR and iterative deep graph learning (Chen et al. 2020b), integrating various pointcloud registration algorithms in the CLRS Benchmark (Veli\u010dkovi\u0107 et al. 2022).\nNeural Algorithmic Reasoning. Recent studies propose the concept of algorithmic alignment, suggesting that aligning the learning process with the steps of a target algorithm facilitates optimisation (Xu et al. 2020). NAR (Veli\u010dkovi\u0107 and Blundell 2021) relies on the convergence of neural networks, and recently GNNs (Veli\u010dkovi\u0107 et al. 2020), with classical algorithms, without relying on large datasets.\nA key distinction from traditional learning frameworks is that NAR models focus on learning and executing intermediate algorithmic steps rather than simply mapping inputs to outputs. NAR models leverage the flexibility of neural networks to learn intricate patterns and relationships within noisy and complex input data while training the networks to learn intermediate algorithmic computations. Studies demonstrate the effectiveness and usability of this framework in domains such as deep reinforcement learning (Deac et al. 2020), compositional dual reasoning (Numeroso et al. 2023), planning and decision-making (Deac et al. 2021; He et al. 2022), and explainability (Georgiev et al. 2022). Recent advancements combine language models with NAR by integrating the embeddings generated from both the transformers and NAR into a cross-attention layer (Bounsi et al. 2024). In this work, we extend the NAR framework and the CLRS Benchmark (Veli\u010dkovi\u0107 et al. 2022), integrating complex multi-step pointcloud registration algorithms and optimising the output using ground truth supervisory signals and a termination network. We evaluate our approach on various challenging synthetic and real-world datasets, and test our method as part of a larger learning system.\nPointcloud Registration. The ICP algorithm (McKay N. 1992; Bai 2023) is considered one of the fundamental algorithms in the field of robotics and is widely used to solve rigid registration tasks (Rusinkiewicz and Levoy 2001; Pomerleau et al. 2013). ICP iteratively finds correspondences between two sets of points and estimates the relative transformation between them to eventually align them. Multiple methods have extended ICP to address different robotics challenges, such as handling noise and outliers in input data (Vizzo et al. 2023; Saleh and Momeni 2024; Segal et al. 2010), dealing with non-rigid deformations (Amberg et al. 2007; Li et al. 2008), faster registration (Zhang et al. 2021; Y. Chen and G. Medioni 1991), and integrating with other perception algorithms for autonomous navigation (Mendes et al. 2016). Deep learning has also been employed for learned pointcloud registration with methods such as GeoTransformer (Qin et al. 2022), TMP (Pramatarov et al. 2024), DCP (Wang and Solomon 2019), MDGAT (Shi et al. 2021), and DeepICP (Lu et al. 2019) that aim to find strong correspondences in feature space. Although these methods perform well, they are trained directly on ground truth data and lack transparency and interpretability. In this work, we use ICP as the supervisory signal for training GNNs to"}, {"title": "3 Preliminaries on NAR", "content": "This section summarises the variation of the NAR framework used, focusing on the components that are of most interest for the following discussion, and introduces the notation used throughout the paper. This section is by no means an exhaustive description of NAR, and we invite the reader to consult the original work (Veli\u010dkovi\u0107 and Blundell 2021; Veli\u010dkovi\u0107 et al. 2020; Ibarz et al. 2022) for a more in-depth discussion.\nThe NAR framework proposes to approximate a specific algorithm A by mimicking not only its final output but also its intermediate state or steps by using its intermediate outputs as supervisory signals. In NAR, the steps of the algorithm can be represented as a graph, here denoted as a sequence $G = {G^{(0)} ... G^{(T)}}$, where $T \\in \\mathbb{N}$ is the final step of the algorithm, i.e. its termination. At each step $t \\leq T$, the graph $G^{(t)}$ is described as $G^{(t)} = (V^{(t)}, E^{(t)}, x^{(t)}, e^{(t)}, g^{(t)}))$, where $V$ and $E$ are the nodes and edges, and $x_i, e_{ij}, g_k$ are the node, edge, and graph features respectively. The initial graph $G^{(0)}$ at the first algorithmic step is the input to the network, and the last graph $G^{(T)}$ is the final output of A.\nAt each step, $t$, the algorithm A produces results $y^{(t)}$ that are used as target outputs for a GNN model to iteratively learn the sequential steps of the algorithm \u2013 i.e. its trajectory - by learning node, edge, and graph feature representations at each step to predict the next. The GNN follows the encode-process-decode paradigm introduced in (Hamrick et al. 2018) and implemented in (Veli\u010dkovi\u0107 et al. 2020; Ibarz et al. 2022). \nEncoders. To process the input features, we define linear encoders $f_v$, $f_e$, and $f_g$ for each $x_i, e_{ij}, g_k$ to generate their corresponding embedded representations in node, edge, and graph level, respectively:\n$Z^{(t)} = \\{z_i^{(t)} = f_v(x_i^{(t)}) | \\forall x_i^{(t)} \\in V^{(t)}\\}$\n$Z^{(t)} = \\{z_{ij}^{(t)} = f_e(e_{ij}^{(t)}) | \\forall e_{ij}^{(t)} \\in E^{(t)}\\}$\n$Z^{(t)} = \\{z_k^{(t)} = f_g(g^{(t)}) | \\forall g^{(t)} \\in G^{(t)} \\}$\nWe call the collection of such embeddings $Z^{(t)}$.\nProcessor. The processor network is a GNN and is responsible for the major part of the learning process. It takes as input the embedding $Z^{(t)}$ from the current step, as well as the previous latent features $H^{(t-1)}$ to compute the latent features from the current step using MPNN (Gilmer et al. 2017; Veli\u010dkovi\u0107 et al. 2022):\n$H^{(t)} = P(Z^{(t)}, H^{(t-1)})$\nWhere for each node:\n$h_i^{(t)} = f_i(z_i^{(t)} || h_i^{(t-1)}, m_i^{(t)})$\n$m_i^{(t)} = \\max_{1<j<n} f_m( z_i^{(t)} || h_i^{(t-1)}, z_{ij}^{(t)} || h_j^{(t-1)}, g^{(t)}, z_k^{(t)} ) $\nHere, $f_m$ is the message-passing function of the MPNN, $f_i$ is the readout function, and the embedding calculations start with $h_i, h_{ij} = 0 \\; \\forall h_i, h_{ij} \\in H^{(t-1)}$ in the input step $t = 0$ (Ibarz et al. 2022). As our task requires both edge and node-level reasoning, message-passing needs to also be performed in the edge embeddings (Dudzik and Veli\u010dkovi\u0107 2022), through triplet reasoning (Ibarz et al. 2022). Following the architecture of Triplet-MPNN, we compute representations over node-edge-node triplets, to obtain edge embeddings:\n$t_{ij}^{(t)} = t_e(h_i^{(t)}, h_j^{(t)}, z_{ij}^{(t)}, z^{(t)})$ and $h_{ij}^{(t)} = t_r(\\max t_{ij})$\nWhere $t_e$ is a function that generates the triplet embeddings and $t_r$ is a readout function used to extract each respective edge embedding.\nDecoders. The decoders, $g_v$, $g_e$, and $g_g$, then decode the outputs for each step as:\n$\\hat{x}_i^{(t+1)} = g_v(z_i^{(t)}, h_i^{(t)}),$\n$\\hat{e}_{ij}^{(t+1)} = g_e(z_{ij}^{(t)}, h_{ij}^{(t)}),$\n$\\hat{g}^{(t+1)} = g_g(z^{(t)})$\nWe call the collection of these outputs $\\hat{y}^{(t)}$. Each decoded output effectively corresponds to the predicted input graph representation of the next step."}, {"title": "4 Methodology", "content": "In this section, we introduce NAR-*ICP\u00b9 and detail the specific modifications and extensions made to the standard NAR framework to enable learning of complex robotics algorithms, such as ICP.\n4.1 Pointcloud Registration\nPointcloud registration algorithms take as input two scans captured at different timesteps m and n, represented as $P_m = \\{p_i | p_i \\in \\mathbb{R}^3\\}$ and $P_n = \\{p_j | p_j \\in \\mathbb{R}^3\\}$ where each $p_i, p_j$ represents a point in three-dimensional space. We assume that $P_n$ is transformed from $P_m$ by a rigid transformation denoted by $[R_{m,n} | t_{m,n}]$, where $R_{m,n} \\in SO(3)$ is an orthogonal matrix representing the rotation of a point and $t_{m,n} \\in \\mathbb{R}^3$ is a translation vector representing the displacement of the points in 3D space. ICP-based algorithms, here A, aim to minimise the difference between $P_n$ and the transformed $P_m$ by iteratively minimising an error function $e^{(t)}$, following a two-phase process. First, they find correspondences $C^{(t)} = \\{(p_i, p_j) | p_i \\in P_m, p_j \\in P_n\\}$ by matching each point in $P_n$ to a point in $P_m$. Then, they leverage these correspondences to estimate the relative transformation that aligns the pointclouds. This iterative process continues until the error criterion e is minimised or until the algorithm reaches a predefined maximum number of iterations. The general structure of ICP-based algorithms is detailed in the pseudocode in Algorithm 1."}, {"title": "4.2 Neural Execution of Pointcloud Registration", "content": "To learn to execute the intermediate algorithmic steps and reason like pointcloud registration algorithms, we represent the input, output, and transformed pointclouds as graphs with the same structure but different features corresponding to the execution steps of the algorithm. The input, output, and intermediate steps of the algorithm are passed to the model as probes \u2013 i.e. the learning signals \u2013 with the intermediate steps specifically referred to as hints (Veli\u010dkovi\u0107 et al. 2022)."}, {"title": "5 Experimental Setup", "content": "In this section, we discuss our experimental setup, detailing the specific algorithms, datasets, tools, and configurations used to evaluate our approach.\n5.1 Algorithms\nAs our approach is generic for pointcloud registration, we evaluated it in approximating several algorithms of the ICP family: point-to-point ICP (P2P-ICP) McKay N. (1992), point-to-plane ICP (P2L-ICP) Y. Chen and G. Medioni (1991), and Generalised-ICP (G-ICP) Segal et al. (2010). These algorithms will also constitute the baselines against which we evaluate our method. While all calculate correspondences $C^{(t)}$ at each iteration as the closest points in the transformed $P_m$ for each point in $P_n$ in Euclidean space, they differ in the calculation of the relative transformation $[R_{m,n} | t_{m,n}]$ and the error $e^{(t)}$. The iterative process repeats until convergence, i.e. the transformation estimations stabilise and the error is minimised. Please refer to Algorithm 1 for an overview of the generic ICP operations.\nPoint-to-point ICP. After finding correspondences $C^{(t)}$, P2P-ICP estimates the relative transformation between them, $[R_{m,n} | t_{m,n}]$, through Singular Value Decomposition (SVD). The transformation is then used to transform $P_m$, and the process repeats until it minimises the sum of square distances between corresponding points, denoted as the error:\n$e^{(t)} = \\sum_{(p_i,p_j)\\in C^{(t)}} ||\\Delta_{ij}^{(t)}||^2$\nWe define $\\Delta_{ij}^{(t)}$ as the vectorial distance between each set of points $(p_i, p_j) \\in C^{(t)}$, as:\n$\\Delta_{ij}^{(t)} = R_{m,n}p_i + t_{m,n} - p_j$\nPoint-to-plane ICP. At each iteration, P2L-ICP minimises the distance between each point in $P_n$ to the tangent plane of the corresponding transformed point in $P_m$, represented by its normal vector $n_j$. The error $e^{(t)}$ is then computed as follows:\n$e^{(t)} = \\sum_{(p_i,p_j)\\in C^{(t)}} n_j \\cdot \\Delta_{ij}^{(t)}$\nwhere $\\cdot$ denotes the dot product. The algorithm minimises the point-to-plane error, $e^{(t)}$, by iteratively calculating the relative transformation and solving a linear system derived by the Jacobian J of the error $e^{(t)}$ until convergence.\nGeneralised-ICP. Relying on the covariance matrices of the correspondences, G-ICP minimises the Mahalanobis distance between corresponding points. The algorithm combines P2P-ICP and P2L-ICP into a single optimisation framework. We denote with $M_{ij}^{(t)}$ the inverse covariance matrix between $(p_i, p_j) \\in C^{(t)}$:\n$M_{ij}^{(t)} = (\\Sigma_{p_i}^{(t)})^{-1} + R(\\Sigma_{p_j}^{(t)})(R^T)^{(t)}$\nwhere $\\Sigma_{p_i}^{(t)}$ and $\\Sigma_{p_j}^{(t)}$ are the covariance matrices of points in $P_m$ and $P_n$, respectively. The error function $e^{(t)}$ is then defined as:\n$e^{(t)} = \\sum_{(p_i,p_j)\\in C^{(t)}} \\Delta_{ij}^{(t)T} M_{ij}^{(t)} \\Delta_{ij}^{(t)}$\nThe gradient of $e^{(t)}$ is followed until convergence.\n5.2 Datasets\nTo thoroughly test our system's capabilities, we consider two distinct datasets: one synthetic and one real-world, each possessing very different characteristics.\nSynthetic. We extend the CLRS Benchmark (Veli\u010dkovi\u0107 et al. 2022) with a complex synthetic dataset consisting of random pointclouds with $(x, y, z)$ coordinates uniformly distributed within the range [-40, 40]. Each pointcloud is part of a set where the second pointcloud is generated by applying a random relative transformation to the first. The synthetic dataset is particularly interesting as, due to the large relative transformation between the pointclouds, ICP-based algorithms are expected to underperform or even fail to reach convergence.\nReal-World. Our real-world dataset consists of centroids of objects and associated semantic labels extracted from SemanticKITTI (Behley et al. 2019). Here, we define as object a local region in the scan with the same semantic class. To extract them, we cluster points within each semantic class based on spatial proximity, with constraints on minimum cluster size and maximum clustering tolerance as in (Kong et al. 2020; Panagiotaki et al. 2023). The centroids are then computed as the mean of the coordinates of all points belonging to an object. To assess the performance of our method, we evaluate our approach on scan pairs recorded at"}, {"title": "5.3 Metrics", "content": "We employ several quantitative metrics to evaluate the performance and effectiveness of our proposed approach in comparison to the baselines. These metrics are applied to different combinations of outputs and target signals, as outlined below:\n1. *T: The final step of the algorithmic execution, $\\hat{y}^{(T)}$, as predicted from the termination, compared to the final step of the algorithm, $y^{(T)}$.\n2. *t: Each step of the algorithmic execution, $y^{(t)}$, compared to each step of the algorithm, $y^{(t)}$.\n3. *GT: The final step of the algorithm, $y^{(T)}$, and the final step of the algorithmic execution \u2013 either before, as predicted from the termination, $\\hat{y}^{(T)}$, or after training with ground truth optimisation, $\\hat{y}^{(T+1)}$ \u2013 compared to the ground truth from the input dataset, $Y_{gt}$.\nWe utilise the definition above and apply it to our metrics, each denoted with the corresponding superscript, e.g. [metric]T.\nRegistration Accuracy. First, we assess the performance of our method as a pose regressor, which is the primary goal of pointcloud registration algorithms. We use the Relative Translation Error (RTE)(\u2193) and Relative Rotation Error (RRE)(\u2193) from the KITTI metrics (Geiger et al. 2013), where:\n$RTE(t) = ||\\hat{t} - t||, \\; RRE(R) = arccos( \\frac{Tr(R^T\\hat{R}) - 1}{2})$\nHere, $\\hat{t}$ and $\\hat{R}$ are the estimated translation and rotation from the model, $R, t$ are the ground truth values, and $Tr(.)$ represents the trace of a matrix. Lower values in RTE and RRE metrics (Geiger et al. 2013) indicate better performance. We compare our approach against the original algorithm used for its training, in $RTE^T$ and $RRE^T$, and the ground truth, in $RTE^{GT}$ and $RRE^{GT}$. As NAR-*ICP outputs correspondences and transformed pointclouds, we need to extract the estimated relative transformation from the predictions. To retrieve the estimated translation and rotation, we apply SVD on the nodes corresponding to the predicted target $P_n$ and transformed source $P_m$ pointclouds. These"}, {"title": "5.4 Training Details", "content": "To finalise the training configuration, we compared various network architectures, teacher-forcing probabilities, and hint configurations, as outlined in Section 7. During training, teacher-forcing was applied with a probability of $P_r = 0.1$ to stabilise the trajectories by feeding the ground truth hints back to the network at each step. We then used softmax for the categorical hints and sigmoids for the mask hints. For the synthetic dataset, 1000, 64, and 64 random samples were generated for training, evaluation, and testing, respectively. The real-world dataset was split using a 60/20/20% split across graphs extracted from sequences 00, 02, 04, and 06 of SemanticKITTI. These sequences were selected due to the varying complexity of the routes and the diversity of the environment in each. Models were trained for 10 000 steps with a batch size of 8, calculating the loss for each intermediate hint and output. The hidden size for model features was set to 256, and the Adam optimiser was used with a learning rate of $1 \\times 10^{-3}$. Finally, as the network requires fixed-length inputs, we adjust the number of the input nodes to ensure compatibility with the encoders' architecture. To maintain consistency in the representations, especially when working with real-world datasets, we either pad the input nodes by randomly repeating them or randomly mask them, depending on the number of nodes in the input."}, {"title": "6 Results", "content": "We compare the performance of our model against all baseline algorithms and their NAR approximation on our datasets. From P2P-ICP we learn two policies, NAR-P2Pv1 and NAR-P2Pv2, from P2L-ICP we learn NAR-P2L, and from G-ICP, NAR-GICP. These correspond to the neural execution of each algorithm, respectively. When the method is superscript with a +, such as NAR-GICP+, the model was trained with the additional ground truth optimisation step. For the neural execution of P2P-ICP, NAR-P2Pv1 and NAR-P2Pv2 differ in training strategy: NAR-P2Pv2 is trained to separately approximate the two phases of the algorithm, while NAR-P2Pv1 uses a single-phase approach containing only the output from the transformation and error estimation corresponding to the second phase of the algorithm. The other methods, NAR-P2L and NAR-GICP, follow the two-phase approach, similar to NAR-P2Pv2, ensuring a more comprehensive representation of the algorithm's trajectory.\nIn the following, we compare the approaches both quantitatively and qualitatively against each other and the baseline algorithms. We evaluate our method in solving the registration task and in its effectiveness at approximating the underlying algorithms.\n6.1 Registration Performance\nQuantitative Results: Registration Accuracy. To evaluate the performance of our pointcloud registration method, we assess the RTE and RRE of different combinations of outputs and target signals, as defined in Section 5.3.\nIn Table 2, we compare the algorithmic performance and the algorithmic execution against the ground truth, in RTEGT and RREGT. The results indicate that our NAR-based algorithmic execution, in most cases, outperforms the algorithm it was trained to approximate when comparing both with the ground truth. This behaviour is especially evident in more challenging datasets \u2013 such as synthetic and KITTI > 11.3 m \u2013 where the displacement between scans"}, {"title": "6.2 Optimisation of Neural Execution", "content": "A significant advantage of the algorithmic execution over the algorithms is that we can further improve their performance by adding a ground truth optimisation step. To evaluate the effectiveness of adding this step in NAR, we compared the predicted outputs after optimisation with the outputs of the baseline algorithms. Our results in Table 7 and Table 8 demonstrate that the optimised NAR-*ICP models consistently outperform the benchmarks across all datasets, both in predicting the ground truth transformed pointclouds and in identifying the correct correspondences. This is indicated by the significantly lower MSEGT scores and substantially improved classification metrics, $F1^{GT}, P^{GT}, R^{GT}, and A^{GT}."}, {"title": "6.3 Termination and Runtime Performance", "content": "We finally evaluate the effectiveness of our added termination criterion, stop, in comparison to the final step from the CLRS Benchmark. In particular, as the number of hints directly dictates the number of processor iterations"}, {"title": "7 Ablation", "content": "To find the optimal processor architecture, hints configuration, and teacher forcing probability, we compared the performance of each configuration on the metrics defined in"}, {"title": "8 Integration of NAR-*ICP in Learning Pipelines", "content": "This section describes how NAR-*ICP can be used as a sub-component of a larger learning pipeline. To demonstrate its successful integration, we developed a contrastive learning framework that generates latent feature representations of objects in SemanticKITTI (Behley et al. 2019). These learned representations then serve as input to the processor in NAR-*ICP, indicating its effectiveness in leveraging latent features rather than relying solely on Cartesian data, which is typical for traditional ICP-based algorithms.\nTo achieve this, we learn a useful embedding representation for each object in the real-world dataset to facilitate registration. In particular, we leverage a contrastive learning framework Chen et al. (2020a) along with a GNN to generate strong embedding representations for each object, which are then used as input $x_i^{(t)}$ to our approach. This contrastive approach is particularly effective in distinguishing between similar and dissimilar data points, facilitating robust feature learning for registration. Figure 7 illustrates the approach, depicting both the centroids used for registration and the points associated with each object, which are then used to extract their latent feature representations. Our results in Table 13 and Table 14 demonstrate that our combined contrastive and NAR framework achieves performance comparable to NAR-*ICP trained on Cartesian data, highlighting the ease of integrating NAR-*ICP into training pipelines.\nRegistration algorithms work well with three-dimensional pointclouds, while high-level semantics and learned embeddings are incorporated into learned methods but not directly into the calculations. This integration experiment is interesting as it presents noisy and complex data in the input of the algorithmic execution and demonstrates that we can incorporate semantics and latent features when solving ICP-based algorithms. This framework is used to demonstrate the flexibility and generalisability of our method as well as its usefulness as a fully differential component of a larger learning system."}, {"title": "9 Conclusion and Future Directions", "content": "This work proposes a novel NAR-based approach for learning to approximate the intermediate steps of ICP-based algorithms, extending the CLRS Benchmark into the field of robotics. Our method not only mimics pointcloud registration pipelines but also consistently outperforms them. We additionally demonstrate the robustness and flexibility of NAR-*ICP when handling noisy and complex data.\nThe NAR framework is further extended by leveraging the specific architecture of the ICP-based algorithms, enhancing its functionality and showcasing its effectiveness in approximating complex multi-step algorithms.\nOur method aims to advance current robotics systems by proposing a more interpretable and efficient learning framework. By integrating classical algorithms with deep learning, we combine structured reasoning and logical computations with the adaptability and generalisation capabilities of neural networks. NAR-based models provide access to their intermediate computations, leading to more reliable, robust, and transparent robotics systems. As such, we foresee that learning in the space of autonomous navigation, planning, and manipulation will reveal interesting applications of NAR, extending NAR-*ICP and the CLRS Benchmark further. Furthermore, the inherent interpretability of NAR can enhance human-robot interaction by enabling systems to explain their decision-making process, leading to more transparent and reliable systems for real-world applications."}]}