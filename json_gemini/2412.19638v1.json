{"title": "Xmodel-2 Technical Report", "authors": ["Wang Qun", "Liu Yang", "Lin Qingquan", "Qu Zhijiu", "Jiang Ling"], "abstract": "Xmodel-2 is a 1.2-billion-parameter large language model designed specifically for reasoning tasks. Its architecture enables different model scales to share a unified set of hyperparameters, allowing for extensive experimentation on smaller models and seamless transfer of optimal configurations to larger models. To maximize training efficiency and stability, Xmodel-2 employs the WSD learning rate scheduler from MiniCPM. Pretrained on 1.5 trillion tokens from diverse sources, Xmodel-2 achieves state-of-the-art performance in complex reasoning and agent-based tasks, while maintaining low training costs. These results highlight the potential of efficient model design and training strategies in advancing reasoning capabilities. Model checkpoints and code are publicly available on GitHub at https://github.com/XiaoduoAILab/Xmodel-2.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have made significant strides in natural language understanding, demonstrating impressive performance across a range of tasks. However, they still face challenges when tackling complex reasoning tasks. Effective reasoning is crucial for applications such as automated customer service and scientific discovery. While larger models typically show improved reasoning capabilities, they also require more computational resources, longer training times, and higher energy consumption.\n\nXmodel-2 is a 1.2B-parameter model designed to balance reasoning power and training efficiency. It excels in complex reasoning, code generation, and agent-based interactions. Unlike other models, Xmodel-2 incorporates an innovative architecture based on Tensor Programs [Yang et al., 2022] [Yang et al., 2023], enabling models of different scales to share the same set of hyperparameters. This approach allows for extensive hyperparameter search on smaller models, with the best configurations transferred seamlessly to larger models, enhancing both efficiency and performance.\n\nTo accelerate training and ensure stable convergence, Xmodel-2 uses the Warmup-Stable-Decay (WSD) learning rate scheduler from MiniCPM [Hu et al., 2024]. Pretrained on 1.5 trillion tokens, Xmodel-2 is capable of processing diverse inputs, such as text and code, which strengthens its performance in complex reasoning tasks.\n\nOur contributions are as follows:\n\n1.  Our Xmodel-2 is open-source, aimed at improving accessibility for researchers in language\n    model research. We believe its state-of-the-art performance and compact size make it an\n    ideal platform for both researchers and practitioners.\n2.  We advanced the decay phase by applying the WSD learning rate scheduler and exploring\n    data ratio search, addressing a gap in the literature and achieving significant improvements\n    in reasoning performance.\n3.  We conducted a focused evaluation of Xmodel-2 's agent capabilities, demonstrating its\n    strong potential for real-world applications such as customer service and task automation."}, {"title": "Pretraining", "content": "This chapter provides a detailed overview of the pretraining process for Xmodel-2. We begin with a description of the model architecture, followed by an explanation of the data distribution across the stable training and decay stages, and conclude with an outline of the overall training procedure."}, {"title": "Model Architecture", "content": "We adopt an architecture similar to LLama 2 [Touvron et al., 2023], with the following configuration:\n\nTokenizer: Unlike most large models that use the BPE tokenizer, Xmodel-2 employs a custom Unigram tokenizer [Kudo, 2018] with a vocabulary size of 65,280 tokens.\n\nEmbedding Sharing: In small language models (SLMs), the embedding layer constitutes a significant portion of the total parameters. To improve efficiency, we implement embedding sharing, which reduces the parameter count by 0.1B.\n\nDeep-and-Thin Architecture: The importance of a deep and thin architecture for SLMs is emphasized by [Liu et al., 2024], a concept that aligns with our observations.\n\nGrouped-Query Attention: To optimize training and inference efficiency, we adopt Grouped-Query Attention (GQA) [Ainslie et al., 2023], which utilizes 24 attention heads and 8 key-value (KV) heads."}, {"title": "Training Stages", "content": "The training of the Xmodel-2 base model consists of two key stages: the Stable Training Stage and the Decay Stage.\n\nStable Training Stage: In this phase, we train on approximately 1.5 trillion tokens (see Figure 2 for data distribution), primarily sourced from open datasets. The training follows the optimal configuration identified through model tuning experiments, using the WSD LRS [Hu et al., 2024], with a batch size of 3.93 million tokens and a maximum learning rate of 0.01.\n\nDecay Stage: This stage combines the pretraining data with high-quality supervised fine-tuning (SFT) data. We apply exponential annealing to the WSD learning rate scheduler, following the formula $f(s \u2013 T) = 0.5^{(s\u2212S)/T}$, where T is set to 5000 steps (20 billion tokens), allowing the learning rate to gradually decrease during the final training phase."}, {"title": "Data Ratio Optimization in the Decay Stage", "content": "Previous work [Ye et al., 2024] demonstrated that data ratio experiments on small models can effectively transfer to larger models. However, their focus was mainly on cosine learning rate decay and pretraining data ratio optimization. MiniCPM [Hu et al., 2024] emphasized the benefits of incorporating SFT data during the decay stage, but lacked detailed exploration of data ratios under WSD learning rate schedulers.\n\nIn contrast, our work explores the interaction between SFT data and domain-specific pretraining data during the WSD decay phase. We framed the data ratio search around two key questions: the overall proportion of SFT data and the distribution of categories within SFT. This approach significantly reduces the search space, enabling efficient optimization with fewer trials.\n\nThrough over 400 trials, we identified that the optimal SFT data ratio falls between 60% and 69%, with the precise value depending on the internal composition of the SFT-mixed dataset. We also observed that Chain-of-Thought datasets may enhance logical reasoning Suzgun et al. [2022b], while instruction-formatted datasets in mathematics and code outperform pretraining-format data in complex reasoning tasks."}, {"title": "Training Data Distribution", "content": "Figure 2 shows the data distribution across training stages, including CC_Chn (Chinese corpus), FineWeb-Edu [Penedo et al., 2024], Dolma [Soldaini et al., 2024] (English corpora), and Code Pretrain datasets like StarCoder [Li et al., 2023a] and The Stack [Kocetkov et al., 2022]. The decay stage incorporates diverse data, such as EvolInstruct [Xu et al., 2023], OssInstruct [Wei et al., 2024], and UltraChat [Ding et al., 2023].\n\nThe SFT-Mixed Dataset is composed of five distinct categories: Mathematics, Code, Logic, Knowledge, and Commonsense. Chain-of-Thought (CoT) data is categorized under Logic. To improve generalization, SFT prompts were diversified via rule-based transformations, though multilingual alignment and domain-specific data were excluded for future exploration. The SFT data underwent multiple rounds of SimHash deduplication with a bucket size of 1 million, improving performance by 1.7% compared to non-deduplicated data.\n\nData ratio experiments revealed the effectiveness of instruction-formatted SFT data during the annealing phase, leading us to allocate 64% to SFT data. These adjustments, combined with optimized data mixing and processing, improved complex reasoning performance by 29.31% compared to our baseline."}, {"title": "Training Loss", "content": "Figure 3 presents the training loss curve on the FineWeb-Edu dataset [Penedo et al., 2024]. The initial drop corresponds to increasing the batch size from 2M to 4M, which likely replicates the stabilizing effect of a reduced learning rate [Smith et al., 2018]. The second drop reflects the impact of the learning rate decay phase."}, {"title": "Results", "content": "We compared Xmodel-2 with recent decoder-only models containing 1\u20132 billion parameters, as identified in [Lu et al., 2024]. The baselines include TinyLLaMA [Zhang et al., 2024], InternLM2 [Cai et al., 2024], Qwen2 [Yang et al., 2024], MiniCPM [Hu et al., 2024], Llama 3 [Grattafiori et al., 2024], Phi-1.5 [Li et al., 2023b], StableLM-2 [Bellagente et al., 2024], OLMO [Groeneveld et al., 2024], MobiLlama [Thawakar et al., 2024], and SmolLM [Allal et al., 2024]. Our experiments demonstrate that Xmodel-2 achieves state-of-the-art (SOTA) performance among 1B-parameter models, demonstrating the effectiveness of our training strategies and optimized data ratios, especially in commonsense reasoning, complex reasoning and agent-based tasks."}, {"title": "Commonsense Reasoning", "content": "We evaluate Xmodel-2 on various commonsense reasoning benchmarks using the Language Model Evaluation Harness [Gao et al., 2021], which includes: ARC-Challenge [Clark et al., 2018], ARC-Easy [Clark et al., 2018], BoolQ [Clark et al., 2019], HellaSwag [Zellers et al., 2019], OpenBookQA [Mihaylov et al., 2018], PiQA [Bisk et al., 2019], SciQ [Welbl et al., 2017], TriviaQA [Joshi et al.,"}, {"title": "Complex Reasoning", "content": "To evaluate the complex reasoning abilities of Xmodel-2, we conducted tests using several well-established benchmarks, including GSM8K [Cobbe et al., 2021], MATH [Hendrycks et al., 2021b], BBH [Suzgun et al., 2022a], MMLU [Hendrycks et al., 2021a], HumanEval [Chen et al., 2021], and MBPP [Austin et al., 2021]. The first four benchmarks were assessed using the Language Model Evaluation Harness [Gao et al., 2021], while the last two were evaluated with the Code Generation LM Evaluation Harness [Ben Allal et al., 2022]."}, {"title": "Agent Capabilities", "content": "We evaluate Xmodel-2's performance on four agent tasks using the ReAct prompting technique [Yao et al., 2023b]. These tasks include HotpotQA [Yang et al., 2018], FEVER [Thorne et al., 2018], AlfWorld [Shridhar et al., 2021], and WebShop [Yao et al., 2023a]. We use EM(Exact Match) as the evaluation metric in FEVER and HotpotQA, and success rate in AlfWorld and WebShop.\n\nTo accomplish FEVER[Thorne et al., 2018] and HotpotQA[Yang et al., 2018], the agent retrieves information from Wikipedia. In FEVER, the agent verifies the truth of a claim via multiple-choice questions, while in HotpotQA[Yang et al., 2018], the agent reasons across multiple documents to answer complex, open-ended questions. For AlfWorld[Shridhar et al., 2021], the agent interacts with an environment of 25 containers, performing actions like retrieving or manipulating objects. This task requires spatial reasoning and decision-making. Finally, in WebShop[Yao et al., 2023a], the agent navigates a simulated e-commerce environment to search, customize, and purchase items. This tests the agent's ability to search efficiently and make decisions within real-world e-commerce constraints. These tasks pose significant challenges for small language models (SLMs) due to their requirements for complex reasoning, multi-step decision-making, and real-world interaction. The results are summarized in Table 4."}, {"title": "Case Study", "content": ""}, {"title": "Calibration", "content": "The pre-trained Xmodel-2-1.2B model exhibits strong calibration, with predicted confidence aligning closely to actual correctness probabilities. Figure 4 illustrates the calibration plot, where the x-axis represents confidence bins (log-probabilities) for A/B/C/D choices, and the y-axis shows accuracy within each bin. The dotted diagonal indicates perfect calibration."}, {"title": "Post-training Scaling Law", "content": "We explored the Post-training Scaling Law of Xmodel-2 on the Wikitext-2 dataset, focusing on how test-time loss changes as the number of prompt tokens increases. This analysis reveals that as the context token count grows, the model's prediction accuracy for the next token improves, with loss and token index following a power-law relationship. Figure 5 shows a consistent decrease in perplexity, with diminishing returns captured by the fitted curve:\n\n$L(t) = b + (t/c)^a$ ; a ~ -0.575, b ~ 1.772, c ~ 32.840"}, {"title": "Conclusions", "content": "This paper introduced Xmodel-2, a 1.2-billion-parameter model optimized for reasoning tasks. By leveraging the maximal update parametrization (\u00b5P), Warmup-Stable-Decay (WSD) learning rate scheduler, and data ratio optimization during the decay phase, Xmodel-2 showed significant improvements in complex reasoning capabilities. Most notably, Xmodel-2 achieved state-of-the-art performance in agent-based evaluations within the 1-2B parameter range, highlighting its strong potential for real-world applications such as e-commerce customer service and task automation."}, {"title": "Appendix : Model Wind Tunnel Experiments", "content": "Before pre-training, we conducted wind tunnel experiments on two small models: nano (6M) and tiny (54M) to validate our training strategy and data ratio. Key experiments included a \u00b5P hyperparameter search and data ratio optimization, which confirmed the strategy's suitability for Xmodel-2."}, {"title": "\u00b5P Hyperparameter Search", "content": "We observed that \u00b5P hyperparameters remained stable across model scales. Using Bayesian optimization, we optimized four key hyperparameters: scale_emb, dim_model_base, scale_depth, and learning_rate on the nano model with the C4 dataset. The search explored 300 configurations, compared to 570,000 in a grid search. Results showed:\n\n\u2022 Optimal Hyperparameters: learning_rate between 0.01 and 0.02, and dim_model_base below 256.\n\n\u2022 Loss Patterns: Loss below 4.1 concentrated around specific hyperparameters, indicating stable performance (Figure 6)."}]}