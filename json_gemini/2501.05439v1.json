{"title": "From Simple to Complex Skills: The Case of In-Hand Object Reorientation", "authors": ["Haozhi Qi", "Brent Yi", "Mike Lambeta", "Yi Ma", "Roberto Calandra", "Jitendra Malik"], "abstract": "Learning policies in simulation and transferring them to the real world has become a promising approach in dexterous manipulation. However, bridging the sim-to-real gap for each new task requires substantial human effort, such as careful reward engineering, hyperparameter tuning, and system identification. In this work, we present a system that leverages low-level skills to address these challenges for more complex tasks. Specifically, we introduce a hierarchical policy for in-hand object reorientation based on previously acquired rotation skills. This hierarchical policy learns to select which low-level skill to execute based on feedback from both the environment and the low-level skill policies themselves. Compared to learning from scratch, the hierarchical policy is more robust to out-of-distribution changes and transfers easily from simulation to real-world environments. Additionally, we propose a generalizable object pose estimator that uses proprioceptive information, low-level skill predictions, and control errors as inputs to estimate the object's pose over time. We demonstrate that our system can reorient objects, including symmetrical and textureless ones, to a desired pose.", "sections": [{"title": "I. INTRODUCTION", "content": "Dexterous in-hand manipulation has made significant progress in recent years [1], [2], [3]. A promising approach is to train a policy in simulation using reinforcement learn-ing and then transfer it to the real world [4], [5], [6]. These policies are empirically generalizable and robust due to the diverse data available in simulation. However, they are typically trained from scratch for each task, requiring careful tuning of the reward function and its coefficients. This process demands substantial human effort and poses challenges for scalability.\nIn contrast, humans acquire new skills by building upon existing ones [7]. Consider a beginner in tennis attempting their first serve: tossing a ball into the air, swinging their racket, and directing the serve. Each of these sub-skills is not practiced specifically within the context of tennis, but the individual can draw from past experiences with other balls or rackets. While the initial execution may be clumsy, it improves with practice.\nMotivated by how humans acquire new skills, we argue that learning robot behavior for new tasks should leverage ex-isting pre-trained skills. In machine learning, the use of pre-trained models has driven significant progress in computer vision [8], [9] and natural language processing [10], but its application to manipulation skill acquisition remains limited. To address this, we propose a hierarchical policy for in-hand object reorientation using pre-trained object rotation skills and demonstrate its effectiveness compared to training from scratch. We focus on this task because in-hand reorientation is a fundamental skill in daily life and exemplifies the complexity of dexterous manipulation.\nThe idea of exploiting hierarchies in robotics has also been widely studied in both task and motion planning [11] and reinforcement learning [12]. However, it often suffers because the low-level skill cannot provide enough feedback to the high-level policy, making it brittle if (inevitably) errors occur in the execution of the low-level skill. We tackle this problem by providing the high-level planner with feedback from the low-level skill and outputting a residual correction term to complement the low-level skills.\nSpecifically, we use the in-hand object rotation policies [6]"}, {"title": "II. RELATED WORK", "content": "In-Hand Manipulation. In-hand manipulation has been studied for decades [16], [17], [18], [19], [20], [21], [22], [23], [24], [25] in classic robotics. More recently, learning-based methods have achieved significant progress [1], [3], [4], [26], [27], [28], [29], [30]. One promising approach is to learn policies in simulation and transfer them to the real world (sim-to-real). This approach also demonstrates its flexibility for bimanual dexterous manipulation [31], [32], dynamic tasks such as pen spinning [33], and extensions to different modalities [34], [35], [36], [37]. These methods do not require an accurate dynamics model and can more easily leverage diverse available data. However, one of the major challenges that limits these methods from scaling up is the substantial human effort required to bridge the sim-to-real gap. In practice, this usually demands significant work for reward engineering, hyperparameter tuning, and domain randomization. Our work falls into the sim-to-real category but differs from others because we build a hierarchical policy for the in-hand manipulation system by reusing previously acquired skill policies instead of starting from scratch.\nPose Estimation in Reorientation. One key component of an in-hand object reorientation system is the pose estimation method. It has been extensively studied in both computer vision [38] and robotics [13], [39]. However, pose estimation for in-hand manipulation is still far from being solved due to large occlusions and the requirements for efficiency and generalizability. As a result, previous work usually simplifies this problem either by assuming known object shapes [23], [40] or by choosing to eschew general pose estimation. For example, Dextreme [5] focuses only on a single cube and can thus estimate the pose using manually defined keypoints, but it cannot generalize to different objects. Visual Dexterity [4] uses point clouds as a proxy for goal specification. However, this limits the flexibility of goal specification, as it cannot re-orient a simple sphere because the point clouds look identical from all rotation angles. Recently, proprioceptive feedback has also been used to estimate pose [15]. However, these methods train one policy to fit a single object and cannot generalize to different objects. Our work distinguishes itself by using generalizable low-level skills and utilizing feedback from low-level controllers, resulting in a generalizable pose estimator for multiple objects.\nSkill Hierarchy in Robotics. The idea of hierarchy has a long history in classical robotics [11], [41] and hierarchi-cal reinforcement learning [42], [43], [44]. The low-level controller can either be fine-tuned together with the high-level control or kept completely frozen [45], [46], [47], [48]. In the context of dexterous manipulation, Bhatt et al. [49] use manually designed action primitives to achieve open-loop dexterous manipulation. Khandate et al. [50] learn a switch between classic controllers and learning-based poli-cies. Morgan et al. [23] also decompose reorientation into several rotation sequences but do not use a learning-based method. HATO [51] defines low-level grasping skills for easy teleoperation. In learning-based skill reuse, Gupta et al. [52] study reset-free reinforcement learning using multiple skills, but the task transitions are defined by the user. Sequential Dexterity [40] learns a transition feasibility function for sub-skill selection. In contrast, our approach does not require learning the transition explicitly; instead, the transition is implicitly encoded in the hierarchical policy."}, {"title": "III. IN-HAND REORIENTATION WITH HIERARCHICAL SKILLS", "content": "Our system overview is shown in Figure 2 (A). It consists of two policies: the planner policy $\\pi^{plan}$ and the skill policy $\\pi^{skill}$. The planner policy $\\pi^{plan}$ takes an object's state, robot proprioception, feedback from low-level policies, and a goal orientation as input and outputs a rotation axis command $a^{plan}$ and a residual action $a^{res}$. This command is sent to the low-level policy $\\pi^{skill}$, which outputs the raw joint position targets at to the robot. To estimate the object's state in the real world, we additionally train a recursive estimator using feedback from sensory and low-level policies."}, {"title": "A. Preliminary", "content": "Skill Policy. Our skill policy is based on the in-hand object rotation policies in [6]. We select it because it demonstrates generalization in manipulating a diverse set of objects. We reimplement the framework and learn an axis-conditioned in-hand object rotation policy for a given axis k.\nFormally, the skill policy is defined as $a_t^{skill}, z_t = \\pi^{skill} (o^{skill}, k_t)$ where $o_t = [o_{t-T:t}, a_{t-1:t-1}^{skill}, d_{t-T:t}]$.\nAmong the observations, $O_t \\in \\mathbb{R}^{16}$ represents the robot's joint positions, $a^{skill} \\in \\mathbb{R}^{16}$ represents the commanded joint targets, and $d_t \\in \\mathbb{R}^{32}$ represents the embedding of the depth image output by a lightweight convolutional neural network. We use T = 30 in our experiments. The temporal sequence $o_t$ is fed into a transformer, which outputs a single vector as the representation. The skill policy also outputs $z_t$, which estimates the object's physical properties and shapes, represented by a feature vector. This vector serves as feedback from the low-level policy.\nObject State. We define the object state space as $s_t = [p_t, q_t]$, where $p_t \\in \\mathbb{R}^3$ denotes the object's 3D position, and $q_t \\in S^3$ denotes the object's orientation, represented as a unit quaternion. We define the relative pose as $\\triangle(q_{t_1}, q_{t_2}) = q_{t_2} \\overline{q_{t_1}}$, where $\\overline{q}$ denotes the conjugate of q."}, {"title": "B. Learning a Hierarchical Policy", "content": "Observation and Action. Our planner policy takes in ob-ject states, robot proprioception, feedback from low-level policies, and a desired orientation, and outputs the desired rotation axis $a^{plan}$. To adapt to variance in object dynamics, we include a short horizon of paired robot states and control actions. Formally, we have $o^{plan} = [s_{t-5:t}, \\acute{s}_{t-5:t}, a_{t-1:t}^{plan}]$ where $a^{plan} = a^{plan}$ is the planner action in the previous timestep and $\\triangle_t = \\triangle (q_t, q^{goal}) \\in \\mathbb{R}^4$ represents the relative trans-formation between object and goal orientation at timestep t. In addition, we augment the policy observation with the feedback from the low-level policy $z_t$. Formally, we have $a^{plan} = \\pi^{plan}(\\omega^{plan}, q^{goal}, z_t)$. Note that, although the inputs to our policy do not explicitly contain the object's shape or physical property information, it is implicitly encoded in the low-level policy feedback $z_t$.\nIn practice, we concatenate the inputs into a vector and pass it through the policy network. The policy network is a simple 3-layer MLP with ELU activation [53]. The network outputs a 7-dimensional categorical distribution, from which we sample a 7-dimensional one-hot action vector denoted as $a^{plan}$. The dimensions correspond to one of the six canon-ical rotation axes ($\\pm x, \\pm y, \\pm z$) and an additional STOP command. When inputting quaternions into the network, we convert them to 6D representations [54].\nResidual Actions. Using a good set of low-level skills can accelerate exploration for new tasks. However, they cannot adapt to new tasks since they remain frozen during training. We augment the planner to output an additional residual ac-tion to complement the output of the skill policy. This design enables additional error correction from the planner policy. In summary, we have $[a^{plan}; a^{res}] = \\pi(\\omega^{plan}, q^{goal}, z_t)$ and $a_t = a^{res} + a_t^{skill}$ will be sent to the robot.\nReward and Policy Optimization. Our reward function is simple (t omitted for simplicity) thanks for the robustness of pre-trained skill policy: $r = 1/(d(q_t, q^{goal}) + \\epsilon) + \\lambda \\mathbb{1}(Success)$, where are the coefficients for the re-ward terms. $1/(d(q_t, q^{goal}) + \\epsilon)$ is the rotational distance reward [1], [4], [5]. $\\mathbb{1}(Success)$ is the success bonus used to encourage the planner to complete the task. Without this bonus, the policy learns to approach the goal but fails to finish it, as it aims to maximize the product of rotation reward and episode length. Compared to previous works [1], [4], [5], our reward function contains only two terms and is significantly easier to tune. This simplification is feasible be-cause the low-level skills are already tuned to be transferable to the real world. Our central claim is that, when building policies for new tasks, we can avoid the tedious reward and hyperparameter tuning typically required for training from scratch.\nIn our design, the planner policy is trained using the ground-truth object states $q_t$ provided by the simulator. We intentionally separate the perception and control components because this modularized design allows us to benefit from ad-vancements in both reusable low-level skills and generalized"}, {"title": "C. Generalizable State Estimator", "content": "The policy described above takes noisy object state infor-mation from the simulator as input. To transfer it to the real world, a robust pose estimator is required for our system. Pose estimation has been extensively studied in computer vision; however, generalized pose estimation in uncontrolled environments remains an unsolved problem. Additionally, in-hand perception presents unique challenges. We propose a generalizable state estimator that takes a sequence of proprioceptive inputs and low-level skill feedback and out-puts the relative rotations over a specified time interval. During deployment, the estimated relative transformations are integrated to determine if the desired pose is achieved.\nRecursive State Estimator. Our state estimator is a neural network $\\phi$ that takes proprioception, action, control errors, low-level skill feedback, and previously estimated object state sequences as input and outputs the object state at the next timestep. We define the object's pose in the first frame as the canonical frame $q_0$.\nState Estimation with Generalizable Low-level Skills. Our approach differs from previous methods through the use of generalizable low-level skills and the feedback provided by these skills. Previous work requires training a sepa-rate policy for each object because it must simultaneously learn manipulation skills and object pose estimation [15]. In contrast, since our low-level skills are generalizable, the pose estimator addresses a simpler task, enabling our policy to manipulate multiple objects. The skill policy not only facilitates the reorientation of diverse objects but also provides its own estimate $z_t$ of the object's properties and shapes. For example, as shown in [6], it encodes information about the object's geometry. This information is essential for achieving generalized object pose estimation.\nWe implement the state estimator using a transformer. We concatenate a temporal history of proprioception, action, control errors, and the predicted extrinsics as the input $f_t = [q_t, a_{t-1}, q_t-a_{t-1}, \\acute{s}_{t-1}, z_t]$. Then we feed a sequence of features $f_t = \\{f_{t-k},..., f_{t-1}, f_t\\}$ as input to the transformer. The transformer outputs $\\acute{s}_t$, which is used as input to our policy $\\pi^{plan}$."}, {"title": "IV. EXPERIMENTS", "content": "We first introduce the experiment setup in Section IV-A. Then, we study the advantages of building a hierarchical policy by reusing in-hand object rotation skills compared to learning from scratch. We use the object state from the simulator as input and empirically analyze the training performance and robustness to out-of-distribution scenarios in Section IV-B. Next, we examine the performance when using the learned state estimator and the factors affecting the performance of the state estimator in Section IV-C. We also present ablation experiments on different design choices in Section IV-D, specifically showing the effectiveness of residual actions and low-level skill feedback. We demonstrate the sim-to-real results in the real world in Section IV-E."}, {"title": "A. Experiment Setup", "content": "Simulation Setup. We use the IsaacGym simulator [56] to train our skill policy, planner policy, and state estimator. The simulation frequency is 120Hz, and the control frequency is 20Hz. We follow the standard setting [3], [26] where the episode starts from a stable grasp sampled from a grasp set, and the target goal is randomly sampled from SO(3). For training, we use four different objects (cylinders, tennis balls, apples, and piggy banks) with randomized physics and sizes. Note that although the number of objects is not large, they still represent a class of different shapes. With randomized physical parameters, they provide enough variations to develop a robust policy [3].\nFast Depth Rendering for In-Hand Objects. One bottle-neck in many previous in-hand manipulation works is the slow vision rendering speed. Previous studies have shown"}, {"title": "B. Policy Hierarchy with Pre-trained Skills", "content": "Policy Learning Performance. We first study the sample efficiency and training performance of our method compared to the baseline. The results are shown in Figure 3. We examine three different levels of object state noise as input. Specifically, we add noise sampled from a normal distribu-tion, with the standard deviation annotated in the figure title. r = 0.05 represents a standard deviation of 0.05 radians, and p = 0.005 represents a standard deviation of 0.005 meters.\nIn the small noise case (A), both our policy and the base-line achieve an 85% success rate, but our policy converges 8x faster than the baseline. As we increase the noise level, we observe that the baseline policy becomes unstable and ex-hibits very high variance across different seeds (Figure 3 B), while our method remains stable. When we further increase the standard deviation of orientation noise from 0.05 rad to 0.15 rad and position noise from 0.5 cm to 1.5 cm, our policy remains stable, while the baseline policy fails to converge (Figure 3 C). The benefits of fast convergence and training stability come from the use of a pre-trained model, which provides a structured exploration space and avoids many meaningless random actions.\nNotably, although the low-level skill policy has experi-enced additional samples compared to the baseline, simply increasing the training time of the baseline policy does not improve performance. We have tried to train the baseline policy with 20\u00d7 more samples, but the conclusion remains the same. This experiment shows that the structural skill space of the low-level policy is more important than the number of samples it has seen during training.\nOut-of-Distribution Robustness. We then study the out-of-distribution robustness of observation noise, physical ran-domizations, and object shapes for a trained model. In this experiment, we use models trained under small noise from the previous section, as the baseline model tends to perform best in this training setting. The results are shown in Figure 4. The general trend across all three evaluations indicates that, although the baseline performs similarly to our method in the easiest case (left data point in each plot), its performance drops rapidly as the test setting becomes more out-of-distribution.\nSpecifically, in the larger observation noise test, our policy maintains an 80% success rate, while the baseline completely fails. When we increase the level of physical randomization, our method consistently outperforms the baseline. We also test shape generalization. For the tennis ball and apple, the baseline slightly outperforms our method. However, it fails"}, {"title": "C. Generalizable State Estimation", "content": "In the previous section, the object states $p_t$ and $q_t$ are directly obtained from the simulator. However, to transfer the learned policy to the real world, we also need a general state estimator system for a diverse set of objects. In this section, we study task accuracy using our proposed state estimator and analyze how the policy's performance changes with predicted object states. Similar to the robustness test, we use the model trained with small noise and multiple objects.\nComparison to Baseline. We evaluate the policy's perfor-mance using the predicted object state as input. The results are shown in Table I. We compare the performance gap from stage 1 (using noisy object states) to stage 2 (using predicted states). Ideally, the performance gap would be small if the state estimation is accurate. We find that, although the baseline and our method achieve similar performance in stage 1, performance drops significantly in stage 2.\nPolicy Smoothness. To understand why the baseline policy experiences a larger performance drop, we quantitatively analyze various smoothness and energy metrics for our policy and the baseline. The energy metric is a critical factor for successful sim-to-real transfer [3]. We measure energy metrics such as torque and work, robot smoothness metrics such as joint acceleration (DofAcc) and joint velocity (DofVel), and object stability (LinVel). Our results show that our method is significantly more stable than the baseline. As a result, object movement is smoother and easier to predict using sensory feedback. Note that we tune the baseline with all of these penalties and select the best-performing one without affecting the success metric.\nOur policy achieves greater stability than the baseline due to the stability of the low-level policy. We argue that, as the task becomes more difficult, balancing task performance with the smoothness and stability required for sim-to-real transfer"}, {"title": "D. Ablation Experiments", "content": "In addition to the high-level design of reusing pre-trained skills, we also make several critical design choices within our architecture. In this section, all experiments are conducted under the small noise setting.\nResidual Actions and Low-Level Skill Feedback. Our planner takes low-level skill feedback $z_t$ as input and outputs a residual action on top of the skill policy's output. We show their effect in Figure 6 (Left). Without these components, our method achieves 76.37% accuracy in stage 1 (using noisy ob-ject states) and 58.12% accuracy in stage 2 (using predicted object states), which is worse than training from scratch. With this basic design, adding residual actions improves performance to 83.63% (stage 1) and 68.84% (stage 2). This experiment shows that, even though the low-level policy is already tuned and achieves good performance on its tasks,"}, {"title": "E. Real-World Experiments", "content": "Finally, we transfer the learned policies to the real world. We test our policies on six objects (Figure 7, left), using proprioception and segmented depth as inputs. Note that there is no cube in our training set, so most of the real-world objects are out-of-distribution. We evaluate two settings: Single: We set the target to rotate along one of the x/y/z-axes by $\\pi/2$ or $\\pi$ radians. This setting does not require switching between different skills but tests the accuracy of pose estimation. We evaluate 30 trials per object. Multi: We set the target so that the policy needs to rotate over two axes by $\\pi/2$. We evaluate 20 trials per object.\nThe results are shown in Figure 7. We find that our policy performs well in the real world for most of the objects. The most challenging object is the tiny cube, as it is small relative to the size of the hand, making it difficult to manipulate. The successful sim-to-real transfer is attributed to two factors: 1) the use of transferable low-level skills and 2) a well-designed planner structure, as demonstrated in the ablation experiments (Section IV-D). We also provide qualitative videos showcasing the reorientation in the real world in our supplementary material."}, {"title": "V. CONCLUSIONS AND LIMITATIONS", "content": "In this work, we present a system for in-hand object re-orientation by building a hierarchical policy with pre-trained low-level skills. To achieve robust and generalizable state estimation in the real world, we also learn a state estimator. We demonstrate successful deployment on multiple symmet-ric and textureless objects. Our work highlights the potential of moving away from robot policies trained from scratch: for a given task, we can significantly improve training efficiency, robustness, and generalizability by leveraging pre-trained models for lower-level skills.\nLimitations and Future Work. Our approach relies on the effectiveness of generalizable low-level policies. Importantly, it requires that no slipping occurs between the finger and the object. This issue can be mitigated by incorporating tactile sensing into our framework. In our current setting, estimated pose errors accumulate over time. Integrating vision and touch for accurate and long-term pose tracking is a promising direction for future work."}]}