{"title": "HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models in Resource-Constrained Environments", "authors": ["Kazi Hasan Ibn Arif", "JinYi Yoon", "Dimitrios S. Nikolopoulos", "Hans Vandierendonck", "Deepu John", "Bo Ji"], "abstract": "High-resolution Vision-Language Models (VLMs) have been widely used in multimodal tasks to enhance accuracy by preserving detailed image information. However, these models often generate excessive visual tokens due to encoding multiple partitions of the input image. Processing these excessive visual tokens is computationally challenging, especially in resource-constrained environments with commodity GPUs. To support high-resolution images while meeting resource constraints, we propose High-Resolution Early Dropping (HiRED), a token-dropping scheme that operates within a fixed token budget before the Large Language Model (LLM) stage. HiRED can be integrated with existing high-resolution VLMs in a plug-and-play manner, as it requires no additional training while still maintaining superior accuracy. We strategically use the vision encoder's attention in the initial layers to assess the visual content of each image partition and allocate the token budget accordingly. Then, using the attention in the final layer, we select the most important visual tokens from each partition within the allocated budget, dropping the rest. Empirically, when applied to LLaVA-Next-7B on NVIDIA TESLA P40 GPU, HiRED with a 20% token budget increases token generation throughput by 4.7x, reduces first-token generation latency by 15 seconds, and saves 2.3 GB of GPU memory for a single inference.", "sections": [{"title": "1 Introduction", "content": "Vision-Language Models (VLMs), such as GPT-4v (Achiam et al. 2023), Gemini Pro (Reid et al. 2024), LLaVA (Li et al. 2023a), and Qwen-VL (Bai et al. 2023), have emerged as remarkable multimodal models that learn from visual and textual data. However, these VLMs inherently rely on low-resolution image encodings, which makes it challenging to process high-resolution images, thus resulting in loss of fine-grained visual information (Zhang et al. 2024a; Dong et al. 2024). To address this issue, recent VLMs, referred to as high-resolution VLMs, employ dynamic partitioning to encode high-resolution images (Liu et al. 2024a; Dong et al. 2024; Li et al. 2024b; Liu et al. 2024b; Lin et al. 2023; Chen, Pekis, and Brown 2024).\nA typical inference pipeline of high-resolution VLMs with dynamic partitioning is illustrated in Fig. 1. Specifically, a high-resolution input image is partitioned into multiple low-resolution sub-images (e.g., four sub-images for a square image in LLaVA-Next); a downsampled version of the original image, referred to as the full-image, is also included. Subsequently, a vision encoder such as Vision Transformers (ViTs) encodes each low-resolution image partition into image features, which are then converted to visual tokens in the text embedding space by a lightweight Projection Layer. These visual tokens are concatenated together and fed into a Large Language Model (LLM) (along with prompt tokens and system tokens) for generating the final response.\nHere, the image partitions have different spatial distributions of image contents and possess various levels of information, thus exhibiting different degrees of importance. While the full-image captures the global context of the original image, each sub-image is for a more detailed local representation of corresponding specific areas. This multi-partitioning approach enables the inclusion of more visual details, which can significantly boost the model accuracy. For example, MM1 (McKinzie et al. 2024) can improve accuracy by 15% when increasing the image resolution from 336x336 to 1344x1344.\nHowever, due to the need to encode multiple image partitions, high-resolution VLMs often generate 3-10 times more visual tokens than low-resolution VLMs (Dong et al. 2024; Hu et al. 2024). Such excessive visual tokens result in lower inference throughput, increased generation latency, and higher GPU memory usage. Furthermore, depending"}, {"title": "2 Related Work", "content": "We categorize highly related works into three groups.\nLightweight Architecture. Some traditional methods aim to downsize VLMs, such as LLaVA-Phi-2.7B (Zhu et al. 2024), TinyLLaVA-3.1B (Zhou et al. 2024), and MobileVLM-3B (Chu et al. 2023) but significantly sacrifice"}, {"title": "3 Key Insights", "content": "Sparse visual tokens with high attention scores. The LLM backbone processes visual, text, and system tokens together in a typical VLM. To understand the nature of visual tokens, we investigate the attention scores of all tokens in the LLM decoding phase in Fig. 2a.\nWe randomly select samples from TextVQA (Singh et al. 2019), ScienceQA (Lu et al. 2022), and DocVQA (Mathew, Karatzas, and Jawahar 2021) and perform inference using LLaVA-Next-7B. While visual tokens account for 80-90% of total tokens in LLM, Fig. 2a shows that they receive significantly less attention than other token types. To further examine the gap between tokens with high and low attention scores, we compute the Cumulative Distribution Function (CDF) of attention scores for the top visual tokens with the highest attention scores, as illustrated in Fig. 2b. The CDF clearly indicates that a small subset of visual tokens bring most of the context from the image to the LLM."}, {"title": "4 Our Design: HiRED", "content": "Only a small subset of visual tokens is crucial during the LLMs generation phase (by Insight 1) and varying importance of different image partitions (by Insight 2) presents a clear opportunity for dropping various numbers of visual tokens from image partitions a fixed token budget. Motivated by these insights, we explore the CLS token attention from the vision encoder to design an early dropping strategy based on the importance of image partitions in Section 4.1. We then present the overall design of HiRED in Section 4.2."}, {"title": "4.1 CLS Attention-Guided Token Dropping", "content": "It is relatively straightforward to identify unimportant visual tokens (those with lower attention scores) during the LLM decoding phase, as discussed in Section 3. However, early dropping requires identifying important tokens before the LLM decoding phase. To achieve this, we leverage the properties of ViT, a commonly used vision encoder in VLMS (e.g., CLIP-ViT (Radford et al. 2021) in LLaVA-Next). ViTs split the image into a sequence of fixed non-overlapping patches and prepend a CLS token to learn features from these patches using Multihead Self-Attention (Gandelsman, Efros, and Steinhardt 2023). The attention between the CLS token and patch tokens indicates the relevance of each patch to the overall image features. Since these patch embeddings are later transformed into visual tokens for the LLM, guided selection of patches can effectively reduce the number of tokens. The key question remains: How do we identify important visual tokens before the LLM phase that are crucial for generating the final response?\nTo answer this, we visualize the attention map between CLS and patch tokens across ViT layers. Here, we have two important findings in Fig. 3: (1) Attention maps in initial layers reveal the main content of the input image. The highlighted patches in these attention maps correspond to visually important parts of the image. For example, in Fig. 3, the attention maps of the first three layers highlight patches derived from 'the bird' while ignoring background areas with no significant visual content. (2) Attention maps in final layers indicate the informative areas, i.e., patches that contain most of the image features. In the final layers, the highlighted patches are distributed across both the image content and the background. As the ViT processes the image, it encodes local features into corresponding patches in the initial layers. However, in the final layers, it learns the relationships between these local features and encodes them into a few background patches as global features (Darcet et al. 2023). As a result, the highlighted areas in the attention map of final layers prioritize patches that are more informative than others (Pan et al. 2021)."}, {"title": "4.2 HIRED Design", "content": "From the above two characteristics of CLS attention map, we design HiRED to guide the budget distribution using ini-"}, {"title": "Algorithm 1: Token Dropping of HiRED", "content": "1: Input: $b_{total}$: token budget, a: full-image budget ratio, m: the\nnumber of sub-images, $S_{l,n}[p]$: attention scores of layer l and\nhead h between CLS and the t-th patch token of the p-th im-\nage partition, where the 0-th image partition is the full-image\nand others are sub-images, $l_{init}$: selected initial layer for bud-\nget distribution, $l_{final}$: selected final layer for important token\nselection, T: total number of tokens in one partition\n/* 1. Token Budget Distribution */\n// 1-1. Calculate the token budget of the full-image\n2: $b_{full} \\leftarrow a\\cdot b_{total}$;\n3: $b_{sub} \\leftarrow (1-a) b_{total}$;\n4: B \u2190 token_budget_distribution (m, S, bsub, linit);\n/* 2. Token Dropping */\n5: For p = 0: m do\n6: // 2-1. Compute the feature importance scores $I[t] [p]$ for\nt-th patch token from p-th partition\n7: For t \u2190 1: T do\n8: $I[t] [p] \\leftarrow \\sum_h S_{tfinal,h[P]}$;\n9: end For\n// 2-2. Select important tokens within each budget\nSelect $B[p]$ tokens with the highest $I[t][p]$;\n10: end For\n11: Function token_budget_distribution (m, S, bsub, linit)\n12: Initialize S[p] \u2190 0, B[p] \u2190 0;\n// 1-2. Compute visual content scores $S[p]$ for p-th partition\n13: For p = 1: m do\n$S[p] \\leftarrow \\sum_n S_{limit,h[P]}$;\n14: end For\n// 1-3. Allocate the budget for each sub-image\n15: B[p] \u2190 bsub X\n16: end Function"}, {"title": "5 Evaluation", "content": "We implemented HiRED and other baselines on LLaVA-Next using the Huggingface Transformers framework (Wolf et al. 2019). For performance evaluation, we use an entry-level accelerator, NVIDIA TESLA P40 (24 GB), with a batch size of 1.\nDownstream Tasks and Benchmarks. We used eight benchmarks from lmms-eval evaluation framework (Zhang et al. 2024b) across three different task types: 1) Visual Question Answering (Visual QA) includes high-level object recognition benchmarks such as VQA-v2 (Goyal et al. 2017)"}, {"title": "5.1 End-to-End Accuracy", "content": "We first evaluate the accuracy of LLaVA-Next (7B and 13B) with HiRED and the baseline methods across various tasks in Table 3. We also included the reported accuracy of notable models as a reference, including the closed-source models of GPT-4V and Gemini-Pro and the open-source models of Qwen-VL (7B), SPHINX (13B), Monkey (9.8B), and LLaVA-1.5 (7B and 13B).\nAccuracy vs. Token Reduction. Evaluation results show that with a 20% token budget (i.e., a maximum of 576 tokens), HiRED achieves nearly the same accuracy as full execution (i.e., a maximum of 2880 tokens) for visual question-answering tasks. With a 40% token budget (i.e., a maximum of 1152 tokens), it maintains comparable accuracy for fine-grained transcription tasks. Interestingly, for ScienceQA and POPE, we observe an increase in accuracy with fewer tokens. This suggests that, in some cases, reducing the number of tokens improves accuracy.\nComparison to Baselines. We observe a greater accuracy degradation across all task categories for both PruMerge and PruMerge+ methods. While these methods can dynamically adjust the token budget to retain more visual information when necessary, they still fall short compared to HiRED, particularly in transcription tasks. On average, PruMerge and PruMerge+ use 10% and 55% of tokens, respectively, for transcription tasks (see section 5.2). However, even with PruMerge+ using 55% of tokens on average, it achieves 11% to 26% lower accuracy than HiRED (20%) for TextVQA and DocVQA, respectively. Similarly, for PruMerge (on average 10% tokens), the accuracy is 13% to 37% lower."}, {"title": "5.2 Inference Efficiency", "content": "To evaluate the inference efficiency of HiRED under various token budgets, we measure its throughput, time to first token (TTFT), and GPU memory usage. The results are presented in Table 4. We also compare the number of tokens generated by HiRED with the adaptive number of tokens generated by PruMerge and PruMerge+ in Fig. 5.\nInference Efficiency of HiRED. The evaluation results presented in Table 4 demonstrate the effectiveness of HiRED in achieving inference efficiency. Notably, with a 20% token budget, our method increases throughput by 4.7\u00d7 compared to full execution. The time to first token is reduced by 15 seconds, which is particularly valuable for applications where low latency is crucial. Additionally, using a 20% token budget saves 2.3 GB of GPU memory due to reduced KV cache size. This reduction in memory usage may enable the use of larger batch sizes that the GPU can accommodate.\nEfficiency under Token Budget. As shown in Fig. 5, HiRED maintains a consistent number of visual tokens for LLaVA-Next-7B under a fixed token budget (e.g., 20%). In contrast, the full model execution, PruMerge and PruMerge+ exhibit significant variation in the number of visual tokens across different samples in TextVQA. The variation in full execution arises from different partitioning based on the image aspect ratio and the removal of some padding tokens. For PruMerge and PruMerge+, the variation is due to their adap-"}, {"title": "5.3 Ablation Study", "content": "We evaluate the key components of HiRED and the effectiveness of our design choices through an ablation study. For this study, we select two visual question answering tasks (i.e., ScienceQA, VQAv2) and two fine-grained transcription tasks (i.e., TextVQA, DocVQA).\nToken Selection. To understand the effectiveness of our Token Selection algorithm design, we evaluate it with different layer selection and head aggregation techniques in Table 5. Then, we apply our selection algorithm to the LLaVA-1.5-7B model. This model does not apply partitioning on the input image; hence, there is one image partition to encode. As a result, the budget distribution component is not required here. We evaluated our token dropping performance by comparing it with PruMerge and Prumerge+ for different token budgets (i.e., 20% and 40%). The results are presented in Table 6. Results show that our method with a 20% budget achieves similar or better results than PruMerge and PruMerge+, especially for text-rich tasks such as TextVQA.\nBudget Distribution. HiRED's budget distribution adapts to the varying importance of image partitions for efficient token use. We set the default hyperparameter a = 0.5 to allocate the token budget between the full image and sub-images. As shown in Fig. 6, varying a impacts accuracy, with balanced distribution (e.g., a = 0.5) yielding better results. Specifically, a = 0 assigns no budget to the full image, while a = 1 allocates the maximum. The remaining budget is distributed among sub-images based on importance. The experiment demonstrates that balancing the budget across full and sub-images is crucial, and a = 0.5 generally offers better results, making it our default choice.\nTo evaluate the significance of budget distribution in VLMs with dynamic image partitioning, we compare HiRED to a setting with equal budget distribution across image partitions. The results, shown in Table 7, illustrate that under a 10% token budget setting, HiRED's adaptive distribution achieves higher accuracy than equal division. Distributing the budget without our method involves simply dividing it equally among partitions."}, {"title": "6 Conclusion", "content": "High-resolution VLMs can enhance accuracy for a wide range of multimodal tasks by preserving detailed image information. However, due to the excessively generated visual tokens, it is challenging to achieve efficient high-resolution VLM inference in resource-constrained environments. In this paper, we addressed this key challenge by strategically dropping visual tokens within a given token budget. Specifically, we proposed HiRED, a plug-and-play early-token-dropping framework that effectively distributes a fixed token budget among image partitions, selects the most important visual tokens accordingly, and then drops the rest of the visual tokens before the LLM decoding phase. Our evaluation results demonstrated that HiRED substantially improves inference throughput, reduces time-to-first-token latency, and saves GPU memory usage while maintaining superior accuracy of high-resolution VLMs across diverse multimodal tasks. We hope that HiRED will shed light on future research into optimizing VLMs for more efficient and scalable performance in resource-constrained environments."}]}