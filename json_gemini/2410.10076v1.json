{"title": "VIDEOAGENT: SELF-IMPROVING VIDEO GENERATION", "authors": ["Achint Soni", "Sreyas Venkataraman", "Abhranil Chandra", "Sebastian Fischmeister", "Percy Liang", "Bo Dai", "Sherry Yang"], "abstract": "Video generation has been used to generate visual plans for controlling robotic systems. Given an image observation and a language instruction, previous work has generated video plans which are then converted to robot controls to be executed. However, a major bottleneck in leveraging video generation for control lies in the quality of the generated videos, which often suffer from hallucinatory content and unrealistic physics, resulting in low task success when control actions are extracted from the generated videos. While scaling up dataset and model size provides a partial solution, integrating external feedback is both natural and essential for grounding video generation in the real world. With this observation, we propose VideoAgent for self-improving generated video plans based on external feedback. Instead of directly executing the generated video plan, VideoAgent first refines the generated video plans using a novel procedure which we call self-conditioning consistency, utilizing feedback from a pretrained vision-language model (VLM). As the refined video plan is being executed, VideoAgent collects additional data from the environment to further improve video plan generation. Experiments in simulated robotic manipulation from MetaWorld and iTHOR show that VideoAgent drastically reduces hallucination, thereby boosting success rate of downstream manipulation tasks. We further illustrate that VideoAgent can effectively refine real-robot videos, providing an early indicator that robotics can be an effective tool in grounding video generation in the physical world.", "sections": [{"title": "INTRODUCTION", "content": "Large text-to-video models pretrained on internet-scale data have broad applications such as generating creative video content and creating novel games, animations, and movies. Furthermore, recent work show that video generation can serve as simulators of the real-world, as well as policies with unified observation and action space. These recent applications of text-to-video generation models hold the great promise of internet-scale knowledge transfer (e.g., from generating human videos to generating robot videos), as well as paving the way to generalist agent (e.g., a single policy that can control multiple robots with different morphologies in different environments to perform diverse tasks).\nNevertheless, text-to-video models have only had limited success in downstream applications in reality. For instance, in video generation as policy, when an observation image and a language instruction are given to a video generation model, generated videos often hallucinate (e.g., objects randomly appear or disappear) or violate physical laws (e.g., a robot hand going through an object). Such hallucinations and unrealistic physics have led to low task success rate when generated videos are converted to control actions through inverse dynamics models, goal conditioned policies, or other action extraction mechanisms."}, {"title": "BACKGROUND", "content": "In this section, we provide the background on video generation as policy in a decision making process. We also introduce consistent diffusion models, which VideoAgent builds upon for self-refinement."}, {"title": "VIDEO AS POLICY IN SEQUENTIAL DECISION MAKING", "content": "We consider a predictive decision process similar to  $P := (X,G,A,H,E,R)$, where $X$ denotes an image-based observation space, $G$ denotes textual task description space, $A$ denotes a low-level motor control action space, and $H \\in R$ denotes the horizon length. We denote $\u03c0(\u00b7|\u03bf, g) : X \u00d7 G \u2194 \u2206(XH)$ as the language conditioned video generation policy, which models the probability distribution over H-step image sequences $x = [x_0, ..., x_H]$ determined by the first frame $x_0$ and the task description g. Intuitively, $x \\sim \u03c0(\u00b7|x_0, g)$ correspond to possible visual paths for completing a task g. Given a sampled video plan x, one can use a learned mapping $\u03c1(x) : XH \u2194 \u2206(AH)$ to extract motor controls from generated videos through a goal-conditioned policy , diffusion policy, or dense correspondence. Once a sequence of motor controls $a \\in AH$ are extracted from the video, they are sequentially executed in the environment E, after which a final reward $R : AH \u2192 {0, 1}$ is emitted representing whether the task was successfully completed. For simplicity, we only consider finite horizon, episodic tasks. Given a previously collected dataset of videos labeled with task descriptions $D = {(x, g)}$, one can leverage behavioral cloning (BC)  to learn $\u03c0$ by minimizing\n$L_{BC}(\u03c0) = E_{(x,g)~D}[-log \u03c0(x|x_0, g)].$\nEquation 1 can be viewed as maximizing the likelihood of the videos in D conditioned on the initial frame and task description."}, {"title": "CONSISTENCY MODELS", "content": "Diffusion models have emerged as an important technique for data distribution modeling. During training, the model learns to map noisy data (at various noise levels) back to clean data in a single step. Concretely, let $x^{(0)}$ denote a clean image and $x^{(t)}$ denote the noisy image at noise level t, where $t \\in [0,T]$, the training objective for a diffusion model $f_\u03b8(x^{(t)}, t)$ can be written as\n$L_{diffusion} (\u03b8) = E_{x(0),e,t} [||f_\u03b8 (x^{(t)}, t) - x^{(0)}||^{2}]$,\nwhere $\u2208 \u2208 N (0, I)$ is the added noise, and $x^{(t)} = \u221aa_tx^{(0)} + \u221a1 - a_te$ where at are time-dependent noise levels. Although diffusion models have achieved high-quality image/video generation, they require hundreds or thousands of denoising steps during inference, which induces tremendous computational cost. To overcome the slow sampling speed of diffusion models, consistency models  were initially proposed by enforcing a consistency loss across different noise levels, i.e.,\n$L_{consistency} (\u03b8) = E_{x^{(0)},e,t1,t2} [||f_\u03b8 (x^{(t_1)}, t_1) - stopgrad(f_\u03b8(x^{(t_2)}, t_2))||^{2}]$,\nwhich encourages the output of the single-step map between different noise levels to be similar. In fact, both the diffusion loss in Equation 2 and the consistency loss in Equation 3 can be understood as exploiting the structure of the denoising procedure which corresponds to an ordinary differential equation (ODE). Specifically, as introduced in , the backward denoising procedure of a diffusion model can be characterized by an ODE, i.e.,\n$\\frac{dx(t)}{dt} = -t s(x^{(t)}, t)$,\nwith $s(x^{(t)}, t)$ is some score function. During the entire path along t \u2208 (\u20ac, \u221e], following this ODE should always maps xt to x0. If we parametrize the model $f(x^{(t)}, t)$ as the simulation following the ODE governed by $s(x^{(t)}, t)$, we obtain the diffusion loss (2). Meanwhile, for all t, t' \u2208 (\u20ac, \u221e],\nwe have $f(x^{(t)}, t) = f(x^{(t')}, t')$ along the simulation path, which induces the consistency loss (3). Therefore, we can combine the diffusion loss and the consistency loss together for model training, i.e.,\n$L(\u03b8) = L_{diffusion} (\u03b8) + \u03bb \\cdot L_{consistency} (\u03b8)$,\nwhere \u03bb denotes consistency regularization hyperparameter across different noise levels."}, {"title": "VIDEO GENERATION AS AGENT", "content": "In this section, we introduce VideoAgent to improve video plan generation. In Section 3.1, we establish a new notion of consistency for video diffusion models, which we called self-conditioning consistency. In Section 3.2, we discuss how the video diffusion model trained with self-conditioning consistency can be used to refine generated video plans during inference. In Section 3.3, we discuss how VideoAgent further closes the self-improvement loop by collecting additional online data to further train the video generation and refinement model."}, {"title": "VIDEO REFINEMENT THROUGH SELF-CONDITIONING CONSISTENCY", "content": "We first consider first-frame-and-language conditioned video generation following , which finds a sequence of image frames for completing the task described by the language starting from the initial image. When a sample is drawn from a video generation model, it is commonly found that a part of the generated video (e.g., the beginning) is realistic while another part of the video (e.g., the end) hallucinates . In other words, while a generated video plan may not fully complete the task specified, it provides meaningful information that can be further improved to achieve the correct plan. To leverage such partial progress, we consider a video consistency model to condition on previous self-generated samples to diffuse for the ground truth video, so that the model can learn to preserve the realistic part of the video while refining the hallucinatory part. Specifically, let $x^{(0)}$ be a ground truth video, and x be a generated video sample from the diffusion model. We define a self-conditioning consistency model as $f_\u03b8(x, x^{(t)}, t)$, which takes a generated video x and a noisy version of the ground truth video $x^{(t)}$ as input and predicts the clean video.\nWe make the observation that self-conditioning can be understood as a reparametrization of the implicit ODE solver for Equation 4 . For example, Song et al. (2020a) considered the 1st order ODE solver for Equation 4 following\n$x^{(t-1)} = \u221aa_{t-1}x+\u221a1 - a_{t-1} - \u03c3 \\cdot s(x^{(t)}, t)$,\nwhere at follows the noise scheduling in Equation 2. Higher-order ODE solvers have also been considered in , all of which depend on the previous sample 2. Based on this observation, we introduce a previous video sample x into our parametrization of $f_\u03b8(x, x^{(t)}, t)$ to mimic these ODE solvers. We emphasize that still follows the same parametrization for the score function $s(x^{(t)}, t)$, which is learned by the vanilla diffusion loss. The self-conditioning ODE solver in Equation 6 is only exploited for accelerating generation.\nWe can learn the ODE solver through self-conditioning consistency by directly predicting the clean video $x^{(0)}$ through a self-conditioning consistency\n$L_{self-conditioning-consistency}(\u03b8) = E_{x,x^{(0)},t} [|| f_\u03b8(x, x^{(t)}, t) - x^{(0)} ||^{2}] + \u03bbE_{x_1,x_2,t} [|| f_\u03b8 (x_1, x^{(t)}, t) - f_\u03b8 (x_2, x^{(t)}, t)||^{2}],$\nwhere $x_1, x_2$ are two independent samples from the first-frame-and-language conditioned video generation model, and \u03bb is a hyperparameter for regularizing the similarity between different samples. To enable the \u201cfirst guess\u201d for x, we consider $f_\u03b8(x^{(t)}, t)$, which is still learned by the vanilla objective for video diffusion as\n$L_{video-diffusion} (\u03b8) = E_{x^{(0)},e,t} [||f_\u03b8 (x^{(t)}, t) - x^{(0)}||^{2}]$\nThe overall objective for training a self-conditioning-consistent video diffusion model boils down to\n$L(\u03b8) = L_{video-diffusion}(\u03b8) + \u03bbL_{self-conditioning-consistency} (\u03b8)$.\nNote that despite the video generation model fe and the video refinement model $f_\u03b8$ having different input arguments, we can nevertheless share the parameter between fe and $f_\u03b8$ to train a single model for video generation and video refinement. We describe the training process for fe and $f_\u03b8$ in Algorithm 1."}, {"title": "INFERENCE THROUGH VLM GUIDED VIDEO GENERATION.", "content": "After training the video generation model fe and the video refinement model $f_\u03b8$ described in Equation 8 and Equation 7, we can sample from fe and iteratively apply $f_\u03b8$ for video refinement. Specifically, let \u03b7 be the step size for the noise schedule, ot be a time dependent noise term, VideoAgent first \"guesses\" the video plan using the first-frame-and-language conditioned video generation generation, i.e.,\n$x^{(t-1)} = x^{(t)} - \u03b7 \\cdot V_ef_\u03b8(x^{(t)}, t) + \u03c3_t \\cdot E.$\nThe sample x after T denoising steps corresponds to the generated video. Next, we can iteratively apply $f_\u03b8$ to refine the generated video sample\n$x^{(i+1)} = f(x^{(i)}, x^{(t)}, t)$,\nwhere i denotes the video refinement iteration, with $y^{(0)} = x = x^{(T)}$. We denote the final video after refinement as $x_{refined}$. A natural question is when to stop the iterative video refinement process. One option is to always refine for a fixed number of iterations. However, over-refinement may lead to less diverse output. To overcome this, we leverage a VLM as a proxy for the environment's reward to assess whether a refined video is likely to lead to successful execution in the environment. Specifically, we denote a VLM as R, which takes a refined video $x^{(i)}$ and returns a binary value {0, 1} to determine whether a video is acceptable based on overall coherence, adherence to physical laws, and task completion (See prompt for VLM in Appendix A). With R, the refinement stops when the VLM decides that the refined video is acceptable. Namely, we have\n$x_{refined} = x^{(i^*)}, where i^* = mini: {i: R(x^{(i)}) = 1 }.$\nAlgorithm 2 shows how video plans are generated, refined, and selected at inference time."}, {"title": "SELF-IMPROVEMENT THROUGH ONLINE FINETUNING", "content": "In addition to video refinement through self-conditioning consistency as described in Section 3.1, we can further characterize the combination of video generation and video refinement as a policy, which can be improved by training on additional real data collected from the environment during online interaction. Specifically, the goal is to maximize the expected returns of a policy through trial-and-error interaction with the environment:\n$J_{online} (\u03b8) = E[R(a) | \u03c0_\u03b8, \u03c1, E]$,\nwhere R is the true reward function, & is the interactive environment, and $\u03c0_\u03b8$ corresponds to Algorithm 2, which contains both the video generation model fe and the video refinement model $f_\u03b8$ as learnable components to be improved.\nA broad array of reinforcement learning methods such as policy gradient can be employed to maximize the objective in Equation 14. For simplicity, we consider the setup of first executing the policy in the environment, then filtering for successful trajectories, continuing finetuning the video policy using additional online data, and executing the finetuned policy again to collect more data. Specifically, each online iteration constructs an additional dataset by rolling out the policy $\u03c0_\u03b8$ at the current online iteration\n$D_{new} = {x_{refined} ~ \u03c0_\u03b8(x_0, g) | R(p(x_{refined})) = 1}$,\nwhere \u03c1 is the optical flow model that maps the refined video to low-level control actions. See Algorithm 3 for details of online policy finetuning."}, {"title": "EXPERIMENTS", "content": "We now evaluate the performance of VideoAgent, introducing the experimental settings and variants of VideoAgent in Section 4.1, end-to-end success rate of VideoAgent against the baselines in Section 4.2 and the effect of different components of VideoAgent in Section 4.3. Finally, we show that VideoAgent is effective in improving the quality of real robotic videos in Section 4.4."}, {"title": "DATASETS AND EXPERIMENTAL SETUPS", "content": "Datasets and Environments. We follow the same evaluation setting as , which considers three datasets: Meta-World , iTHOR , and BridgeData V2 . Meta-World consists of 11 robotic manipulation tasks performed by a simulated Sawyer arm, with video demonstrations captured from three distinct camera angles. iTHOR is a simulated 2D object navigation benchmark, where an agent searches for specified objects across four room types. BridgeData V2 is a real-world dataset of robotic manipulation. See more details of datasets and environments in Appendix C.\nBaselines and VideoAgent Variants. We consider the following methods for comparison:\n\u2022 AVDC (baseline). This is the Actions from Video Dense Correspondences (AVDC)  baseline, which synthesizes a video and predicts optical flow to infer actions.\n\u2022 AVDC-Replan (baseline). When the movement stalls, AVDC-replan re-runs video generation and action extraction from the flow model to execute a new plan.\n\u2022 VideoAgent. Our proposed video refinement model through self-conditioning consistency as introduced in Section 3.1. VideoAgent generates video and iteratively refines a video plan. We use GPT-4 Turbo for selecting the best video plan during inference (Section 3.2).\n\u2022 VideoAgent-Online. As actions are executed in the online environment, successful trajectories are collected and used to continue training the video generation and refinement model, as described in Section 3.3.\n\u2022 VideoAgent-Online-Replan. This variant incorporates online filtering of successful trajectories with the replanning mechanism, where replanning is conducted first, and more successful trajectories after replanning are added back to the training data."}, {"title": "END-TO-END TASK SUCCESS", "content": "Meta-World. We report the task success of baselines and VideoAgent in Table 1. Following Ko et al. (2023), we measure the average success across 3 camera poses with 25 seeds for each camera pose. Without online environment access, VideoAgent already improves the overall success rate from self-conditioned consistency alone over the baseline (from 19.6% to 22.3%). Furthermore, some specific tasks such as faucet-close shows drastic improvement from 12% to 46.7%. With online data collection, VideoAgent-Online further improves the success rate, and each online iteration (rolling out the policy, collecting successful trajectories, and continuing finetuning the policy) further improves the performance. The benefit of VideoAgent persists when replanning is introduced, brining the overall task success to 50%, achieving state-of-the-art in this setup. Qualitative improvement in the refined videos can be found in Figure 8 in Appendix H,\niTHOR. Next, we evaluate VideoAgent on iThor. Due to the high computational cost of running the iThor simulator, we focus only on evaluating self-conditioning consistency (without online access). We follow the same setup as , where we measure the average success rate across four rooms each with three objects using 20 seeds. As shown in Table 2, VideoAgent consistently outperforms the baseline, demonstrating the effectiveness of self-conditioning consistency in producing more plausible video plans."}, {"title": "UNDERSTANDING THE EFFECT OF DIFFERENT COMPONENTS IN VIDEOAGENT", "content": "In this section, we aim to understand the effect of different components of VideoAgent. Specifically, we focus on the effect of (1) different types of feedback given to the refinement model, (2) the number of refinement and online iterations, and (3) the quality of the VLM feedback."}, {"title": "EFFECT OF DIFFERENT VLM FEEDBACK.", "content": "In the previous section, we only used VLM during inference to determine when to stop refining a generated video. However, it is natural to wonder if information-rich feedback from the VLM, such as language descriptions of which part of a generated video to improve, might lead to better refined videos. To answer this question, we propose a few variants of VideoAgent according to the feedback available when training the video refinement model as in Equation 10. Specifically, we use VideoAgent to denote training the video refinement model only conditioned on the original task description. VideoAgent-Binary denotes additionally conditioning on whether a generated video is determined to be successful by the VLM. VideoAgent-Suggestive denotes conditioning additionally on language feedback from the VLM on"}, {"title": "EFFECT OF REFINEMENT AND ONLINE ITERATIONS.", "content": "Next, we want to understand whether more refinement iterations and online finetuning iterations generally lead to higher task success. We found that while different tasks require a different number of iterations to achieve the best performance, VideoAgent does perform better as the number of refinement and online iterations increases. During video refinement, specific tasks such as handle-press and faucet-close continue to see improvement even at the fifth refinement iteration. Faucet-close especially benefits from more refinement iterations, bringing success rate from 17.3% to 49.3% after five refinement iterations. The improved task success rates across refinement and online iterations suggests that self-conditioning consistency discussed in Section 3.1 and online interaction discussed in Section 3.3 can indeed effectively reduce hallucination and improve physical plausibility in the generated videos."}, {"title": "ACCURACY OF VLM FEEDBACK ON GENERATED VIDEOS.", "content": "Since this work is among the first to leverage a VLM to give feedback for video generation, it is crucial to understand whether a VLM can in fact achieve a reasonable accuracy in providing feedback for video generation. To quantify the performance of a VLM, we use human labels on whether a generated video is acceptable as the ground truth, and measure precision, recall, F1-score, and accuracy based on whether GPT-4 Turbo thinks the generated video is acceptable according to trajectory smoothness (consistent across sequential frames), physical stablility, and achieving the goal. We report the average result across 36 generated videos from the Meta-World dataset. We see that the original prompt we used (Unweighted) achieves 69% accuracy, suggesting that the VLM is capable of judging generated videos. Since VideoAgent uses multiple refinement iterations, we want to avoid false positives where a bad video is accidentally accepted. We can achieve this by penalizing false positives through reweighting its cost in the prompt, which leads to the VLM rejecting videos when the VLM is uncertain about the video's acceptability. This adjustment results in a significant increase in precision as shown in Table 4. This weighted version of the prompt is used in the experiments in Section 4.2.\nPartial Observability. In the AVDC experimental setup, center cropping the third camera (what is used in the pipeline) often results in most of the robot arm being outside of the frame. We found that the accuracy of the VLM is affected by such partial observatbility. As shown in Table 4, removing the third camera from the prompt leads to much higher accuracy."}, {"title": "EVALUATING SELF-REFINEMENT ON REAL-WORLD VIDEOS", "content": "In this section, we evaluate VideoAgent's ability to refining real-world videos, which often contain higher variability, intricate details, nuanced behaviors, and complex interactions. We study the effect of video refinement using both quantitative metrics and qualitatively for holistic evaluation.\nQuantitative Evaluation. Following previous literature on video generation, we consider two reference-free metrics, CLIP Score and Flow Consistency , as well as a set of Video-Scores . CLIP Score measures the cosine similarity between frame feature and text prompt, whereas Flow Consistency measure the smoothness and coherence of motion in the videos calculated from the RAFT model. Video-Scores use five sub-metrics with a focus on correlation with human evaluation and real-world videos.\nWe report the average across 2250 videos generated from the AVDC baseline and from VideoAgent in Table 5. VideoAgent performs better according to all metrics except for Dynamic Degree from Video-Score (which shows similar performance between the two methods). Notably, the gain is significant in metrics critical for real-world videos, such as CLIP Score, Factual Consistency, and Text-to-Video Alignment. Improvement in Flow Consistency and Temporal Consistency suggests that VideoAgent produces smoother and more physically plausible videos that adhere better to the physical constraints of the real-world. This directly translates to better performance in real-world robotic tasks in Table 1.\nQualitative Evaluation. Next, we qualitatively evaluate generated videos from the AVDC baseline and from VideoAgent. We collect 50 generated videos from each model and conduct human evaluation on whether a generated video looks realistic. Videos with refinement from VideoAgent improves the acceptance rate by 22% as shown in Table 5. We further show an example video"}, {"title": "RELATED WORK", "content": "Feedback and Self-improvement in LLMs. Incorporating feedback and preference signals from feedback into the finetuning process of LLMs, has led to the enormous popularity and practical usability of the current versions of LLMs as chatbots. Preference feedback from humans or other AI systems are first collected to train a reward model to guide the LLM's generation or do implicit policy optimization . Furthermore LLMs have shown the ability to further improve by iterative refinement during finetuning and inference . We incorporate this reward driven improvement mechanism in our work, but unlike the LLM setting where the feedback came from a reward model or some proxy of this prefernce model, in our VideoAgent we use natural feedback from real world when simulated videos are turned into actions that are executed in the real world.\nImage and Video Generation and Editing. With the advent of large scale foundation models pretrained on internet scale data , generation of super realistic multimodal content has become easier. Text generation, image or video generation, and cross-modal generation  has seen major advancements leveraging the autoregressive and diffusion based models architectures. And moving beyond simple generation, these models have been leveraged for guided text, image or video editing and enhancement  to improve textual and visual aesthetics applied mostly to generative media . But none of these existing methods focus on grounding a generative simulator in the real world to perform more complex interactive multi-turn agentic and physical tasks needing both perception and control. To solve this bottleneck, we propose VideoAgent to self-improve or edit generated plan based on grounded feedback from real-world to execute robot manipulation tasks.\nVideo Generation for Robot Learning. Video-based learning for robotics has been extensively studied . Methods use video datasets for visual representation learning, goal extraction, and dynamic models for planning , or imitation learning from expert actions , state sequences , and pretraining on videos followed by RL . Recently, generative models have advanced video-based learning and planning, framing decision-making as text-conditioned video generation to predict trajectories . Vision-language and text-to-video models generate long-horizon plans for robotic tasks through abstract, visually grounded planning . Generative models also simulate agent-environment interactions, enabling zero-shot deployment , and test-time feedback for replanning . Unlike these, our VideoAgent improves video generation during training with real-world feedback and refines actions through test-time self-iteration and replanning."}, {"title": "CONCLUSION AND FUTURE WORK", "content": "We have presented VideoAgent, where a video generation model acts as an agent by generating and refining video plans, converting video plans into actions, executing the actions in an environment, and collecting additional data for further self improvement. Through interaction with an external environment, VideoAgent provides a promising direction for grounding video generation in the real world, thereby reducing hallucination and unrealistic physics in the generated videos according to real-world feedback. In order to fully achieve this overarching goal, VideoAgent needs to overcome a few limitations, which calls for future work:\n\u2022 In the online setting, VideoAgent only considers filtering for successful trajectories for further finetuning. Exploring other algorithms such as online RL is interesting future work.\n\u2022 VideoAgent utilizes optical flow for action extraction. It would be interesting to see how VideoAgent works with inverse dynamics model or image-goal conditioned diffusion policy."}, {"title": "PROMPT STRUCTURE FOR VLM FEEDBACK", "content": "We employ a structured prompting strategy to provide feedback on video sequences for the zero-shot classification. The process consists of one Query-Evaluation Phase, each with distinct sub-goals."}, {"title": "BINARY CLASSIFICATION", "content": "Task: You are a video reviewer evaluating a sequence of actions presented as seven consecutive image uploads, which together represent a single video. You are going to accept the video if it completes the task and the video is consistent without glitches.\nQuery-Evaluation Phase:\n\u2022 Inputs Provided:\nTextual Prompt: Describes the task the video should accomplish.\nConditioning Image: Sets the fixed aspects of the scene.\nSequence of Images (7 Frames): Represents consecutive moments in the video to be evaluated.\n\u2022 Evaluation Process:\nView and Analyze Each Frame: Examine each image in sequence to understand the progression and continuity of actions.\nAssess Overall Coherence: Determine if actions transition smoothly and logically from one image to the next.\nCheck for Physical Accuracy: Ensure adherence to the laws of physics, identifying any discrepancies.\nVerify Task Completion: Confirm the sequence accomplishes the task described in the textual prompt.\nIdentify Inconsistencies: Detect inconsistencies in object movement or overlaps that do not match the conditioning image.\n\u2022 Evaluation Criteria:\nAccept the sequence if it is a coherent video that completes the task.\nReject the sequence if any frame fails to meet the criteria, showing inconsistencies or not achieving the task. Be very strict, rejecting even minor errors.\n\u2022 Response Requirement:\nProvide a single-word answer: Accept or Reject. Do not give reasoning.\n\u2022 Additional Notes:\nNo further clarification can be requested.\nElements from the conditioning image must match those in each frame of the sequence."}, {"title": "IDENTIFICATION AND SUGGESTION:", "content": "We employ a structured prompting strategy to provide descriptive feedback on video sequences via an in-context few-shot classification setup. The process consists of one Query-Evaluation Phase, each with distinct sub-goals."}, {"title": "IDENTIFICATION AND SUGGESTION", "content": "Task: You are a video reviewer tasked with evaluating a series of actions depicted through eight consecutive image uploads. These images together simulate a video. This task is structured as a few-shot learning exercise, where you will first review three examples and then apply learned principles to new queries. Query-Evaluation Phase:\n\u2022 Inputs Provided:\nTextual Prompt: Describes the intended outcome or task the video aims to accomplish.\nConditioning Image: Establishes the fixed elements of the scene.\nSequence of Images (7 Frames): Illustrates consecutive moments in the video, representing the action sequence.\n\u2022 Evaluation Process:\nFrame-by-Frame Analysis: Carefully examine each of the seven images to understand the progression and continuity of actions.\nAssess Overall Coherence: Evaluate the sequence as a whole to determine if the actions transition smoothly from one frame to the next while maintaining logical progression.\nCheck for Physical Accuracy: Ensure each frame complies with the laws of physics, identifying any discrepancies in movement or positioning.\nVerify Task Completion: Confirm if the sequence as a whole accomplishes the task described in the textual prompt.\nIdentify Inconsistencies: Detect inconsistencies in object movement or overlaps that contradict the fixed scene elements depicted in the conditioning image.\n\u2022 Evaluation Criteria:\nDescriptive Feedback: Based on your evaluation, provide a concise, constructive sentence suggesting specific improvements. Focus on enhancing physical accuracy and task fulfillment based on identified inconsistencies or discrepancies.\n\u2022 Response Requirement:\nFeedback must be derived from your observations during the evaluation and not exceed 20 words.\n\u2022 Additional Notes:\nNo further clarification can be requested.\nElements from the conditioning image must match those in each frame of the sequence."}, {"title": "TASK DESCRIPTIONS AND IN-CONTEXT EXAMPLES FOR VLM FEEDBACK", "content": ""}, {"title": "TASK DESCRIPTION AND SUCCESS CRITERIA", "content": "\u2022 door-open: The robot arm has to open the door by using the door handle.\n\u2022 door-close: The robot arm has to close the door by pushing the door or the handle.\n\u2022 basketball: The robot arm has to pick up the basketball and take it above the hoop.\n\u2022 shelf-place: The robot arm has to pick up the blue cube and place it on the shelf.\n\u2022 button-press: The robot arm has to press the red button from the side by pushing it inside.\n\u2022 button-press-topdown: The robot arm has to press the red button from the top by pushing it downward.\n\u2022 faucet-close: The robot arm has to use the red faucet handle and turn it anti-clockwise.\n\u2022 faucet-open: The robot arm has to use the red faucet handle and turn it clockwise.\n\u2022 handle-press: The robot arm has to press the red handle downward.\n\u2022 hammer: The robot arm has to grip and pick up the hammer with a red handle and hit the peg on the box inside.\n\u2022 assembly: The robot arm has to pick up the ring and place it into the red peg."}, {"title": "DATASET DESCRIPTIONS IN DETAIL", "content": "Meta-World is a simulation benchmark that uses a Swayer robotic arm to perform a number of manipulation tasks. In our experiments, we make use of 11 tasks as shown in Table 1. We capture videos from three distinct camera angles for each task and use the same camera angles for both the training and testing phases. We gather five demonstration videos per task for each camera angle. During the evaluation, we tested on each of the three camera angles with 25 seeds per camera angle. The position of the robot arm and the object is randomized at the beginning of each seed to ensure variability. A trajectory is considered successful if the Video Agent reaches within a really close threshold of the goal state."}, {"title": "EXTENDED EXPERIMENTS", "content": ""}, {"title": "FURTHER ANALYSIS OF VIDEOAGENT-ONLINE", "content": "We train VideoAgent-Online for multiple iterations and observe that after 2 iterations, the results start to stabilize. The results for iteration 3 are shown in table 6."}, {"title": "ARCHITECTURAL DETAILS OF VIDEOAGENT", "content": ""}, {"title": "VIDEO DIFFUSION TRAINING DETAILS", "content": "We use the same video diffusion architecture as the AVDC baseline. For all models, we use dropout=0, num head channels=32, train/inference timesteps=100, training objective=predict v, beta schedule=cosine, loss function=l2, min snr gamma=5, learning rate=1e-4, ema update steps=10, ema decay=0.999."}, {"title": "VLM FEEDBACK FOR CORRECTION", "content": ""}, {"title": "DETAILS OF HUMAN EVALUATION ON BRIDGEDATA V2", "content": "Qualitative Evaluation. Next, we qualitatively evaluate video generation quality using the five Video-Score dimensions: Visual Quality (VQ) for clarity and resolution, Temporal Consistency (TC) for smooth frame transitions, Dynamic Degree (DD) for capturing accurate object/environment changes, Text-to-Video Alignment (TVA) for matching the video to the prompt, and Factual Consistency (FC) for adherence to physical laws and real-world facts. Videos are rated on a 4-point scale based on the metric in He et al.: 1 (Bad), 2 (Average), 3 (Good), and 4 (Perfect). Our evaluation is based on 50 generated videos from a held-out set."}, {"title": "EXAMPLES", "content": ""}, {"title": "IMPROVEMENTS IN META-WORLD", "content": ""}, {"title": "IMPROVEMENTS IN ITHOR", "content": ""}, {"title": "IDENTIFICATION AND SUGGESTIVE FEEDBACK EXAMPLES", "content": ""}]}