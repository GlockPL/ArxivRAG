{"title": "Enhancing Long Context Performance in LLMs Through Inner Loop Query Mechanism", "authors": ["Yimin Tang", "Yurong Xu", "Ning Yan", "Masood Mortazavi"], "abstract": "Transformers have a quadratic scaling of computational complexity with input size, which limits the input context window size of large language models (LLMs) in both training and inference. Meanwhile, retrieval-augmented generation (RAG) besed models can better handle longer contexts by using a retrieval system to filter out unnecessary information. However, most RAG methods only perform retrieval based on the initial query, which may not work well with complex questions that require deeper reasoning. We introduce a novel approach, Inner Loop Memory Augmented Tree Retrieval (ILM-TR), involving inner-loop queries, based not only on the query question itself but also on intermediate findings. At inference time, our model retrieves information from the RAG system, integrating data from lengthy documents at various levels of abstraction. Based on the information retrieved, the LLM generates texts stored in an area named Short-Term Memory (STM) which is then used to formulate the next query. This retrieval process is repeated until the text in STM converged. Our experiments demonstrate that retrieval with STM offers improvements over traditional retrieval-augmented LLMs, particularly in long context tests such as Multi-Needle In A Haystack (M-NIAH) and BABILong.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have demonstrated a powerful ability to handle almost all kinds of NLP tasks with impressive performance [7, 1, 29]. As the size of LLMs increases, they tend to perform better and store more informative knowledge within their parameters [17, 25]. LLMs can also be further fine-tuned on downstream tasks [34]. However, the length of the input window in LLMs is constrained by the quadratic computational complexity of the self-attention mechanism, which is a fundamental structure in these models [31]. An alternative approach to processing longer contexts is to split large quantities of text into chunks and index these chunks as vectors in a separate information retrieval system [20, 5, 14]. Since the retrieval system can filter out unnecessary information, the LLM can handle user questions with long raw data within a limited context window. Additionally, this approach provides easier interpretability and provenance tracking compared to the opaque and unexplainable parameters within LLMs [2].\nHowever, existing retrieval-augmented approaches also have shortcomings. The issue we aim to address is that most existing methods retrieve only a few text chunks that are directly related to user queries, which limits the ability of LLMs to produce deeper answers to questions. This is particularly relevant when it comes to fully understanding or integrating knowledge from multiple parts that may not be directly related to the user's questions, such as understanding foreshadowing in novels or inferring the identity of the killer in detective fiction. When humans perform such tasks, we often have impressions in our minds related to unusual facts that may connect to our questions, indicating that memory plays a crucial role in advanced comprehension skills [6, 19]. Long-term and short-term"}, {"title": "Related Work", "content": ""}, {"title": "Large Language Models (LLMs)", "content": "LLMs such as the GPT [7, 1], Gemini [29, 10], and Claude [3] series have made remarkable strides across a broad spectrum of tasks and have increasingly become daily assistants for many people. However, the closed-source nature of these models prohibits researchers and companies from studying the inner mechanisms of LLMs and building domain-adapted applications. Consequently, many open-source LLMs have emerged in the community, such as Llama [30, 9], ChatGLM [12], and Mistral [15]. However, these models still have limited context windows, usually capped at 8k tokens, due to the complexity of self-attention and position encoding. The limited context windows imply that they lack long-term memory capabilities, which could be enhanced by integrating RAG systems with memory structures."}, {"title": "Retrieval-Augmented Generation (RAG)", "content": "Retrieval-Augmented Generation (RAG) uses retrieved tokens from long context raw data to extend the input window size of LLMs. The original RAG [20] integrates pre-trained sequence-to-sequence models with a neural retriever. [23] introduced the Joint Passage Retrieval (JPR) model, which employs a tree-decoding algorithm to handle passage diversity and relevance in multi-answer re-trieval. Dense Hierarchical Retrieval (DHR) and Hybrid Hierarchical Retrieval (HHR) represent advancements in retrieval accuracy by combining document-level and passage-level retrievals and integrating sparse and dense retrieval methods, respectively [21, 4]. RAPTOR [27] utilizes clustering and summarizing of text chunks, constructing a tree with varying levels of summarization from the bottom up, enabling multiple levels of understanding of long contexts. Nevertheless, these approaches still depend on a single retrieval query and do not include a mechanism for automatically acquiring additional information based on the evolving context, which makes it challenging to provide precise answers to user questions."}, {"title": "Memory Mechanisms", "content": "Efforts have been made to improve the memory capabilities of neural models. Memory-augmented networks (MANNs) [22], such as Neural Turing Machines (NTMs) [13], enhance the memory capacity of neural networks by incorporating an external memory matrix, allowing them to manage tasks that"}, {"title": "Method", "content": "Our Inner Loop Memory-Augmented Tree Retrieval (ILM-TR) method, as shown in Fig. 1, contains two parts: retriever and inner-loop query. For the retriever part, we primarily use the RAPTOR'S tree-build method [27]. The retriever first segments the raw data into short, contiguous text chunks of a certain length. If a sentence exceeds the length limit, it will be moved to the next chunk. After splitting, an summary model is used to summarize each chunk. However, unlike typical summarization methods such as RAPTOR, our model produces two kinds of summaries: one is the regular summary of the main text in the chunk, while the other includes all surprising facts that differ from the main text. The retriever architecture is illustrated in Figure 2.\nBuilding upon the idea that the informational value of a communicated message depends on the degree of surprise in its contents [8], the inclusion of surprising information, distinct from the main text, will also provide valuable insights to LLMs when handling long contexts. After generating summary texts and surprising information from each chunk, we group similar texts using Gaussian Mixture Models, as employed in RAPTOR (refer to [27] for more details). However, we only group the summaries without the surprising information.\nAll texts are embedded for searching and clustering using SBERT, a BERT-based encoder (multi-qa-mpnet-base-cos-v1) [26]. These summarized texts are then re-embedded, and the cycle of embedding, clustering, and summarization continues until further clustering becomes impractical, resulting in a structured, multi-layered tree representation of the raw data. For querying within this tree, similar to RAPTOR, we use a collapsed tree strategy that disregards the tree structure and directly traverses all the nodes as shown in Figure 2.\nFor inner-loop query part, as shown in Fig. 1, we use an LLM as the answer model to generate the final answer. This model can be the same as the summary model or a separate one. We create an additional area called Short-Term Memory (STM), which stores texts up to the answer model maximum output length. The STM is initially empty. Each time, the answer model generates an"}, {"title": "Experiments", "content": "We evaluate ILM-TR's long-context performance using two benchmarks: M-NIAH [16] and BABI-Long [18]. For both the summary and answer model inference, we utilize Meta-Llama-3-70B with llama.cpp [11], quantized using Q4_K_M due to hardware limitations. We do not employ smaller LLMs, such as the 8B or 7B models, as their instruction-following capabilities were found to be inadequate in our tests: they consistently failed to follow the summarization prompts correctly. We also set the maximum number of inner-loop queries to 5. All tests were conducted on a machine running Ubuntu 22.04, equipped with an Intel Xeon Gold 6242 processor and four NVIDIA Tesla V100 32GB GPUs. Details of all prompts and parameters are provided in Appendices A.1 and A.3.\nIn the M-NIAH test, several sentences are inserted into a specific area of a given long context. The question is related to all the inserted sentences, and the model is expected to retrieve all necessary information across these sentences. For example, we use three sentences: \u2018Figs are one of the secret ingredients needed to build the perfect pizza', 'Prosciutto is one of the secret ingredients needed to build the perfect pizza' and 'Goat cheese is one of the secret ingredients needed to build the perfect pizza'. The question would then be \u2018What is the first letter of each secret ingredient needed to build the perfect pizza?\u201d. The BABILong test is similar to the M-NIAH test but involves sentences with more complex logical relationships. For instance, it may include sentences like \u2018The apple is in the bathroom' and 'Jack takes the apple to the kitchen' The question in this case would be: 'Where is the apple before kitchen?\".\nWe present the M-NIAH and BABILong test results in Figure 3 and Figure 4(Appendix A). We tested RAPTOR as the baseline method, and our ILM-TR method with two settings, with token lengths ranging from 150k to 500k. There are three inserted sentences for M-NIAH test. Each testcase has four possible score levels: no keywords found (score 1, red), one keyword found (score 3, orange), two keywords found (score 7, yellow), and all keywords found (score 10, green). Figure 3 demonstrates\""}, {"title": "Conclusion and Future Work", "content": "We introduce a novel approach, Inner Loop Memory Augmented Tree Retrieval (ILM-TR), which incorporates inner-loop queries based not only on the initial query but also on intermediate findings. During inference, ILM-TR retrieves information from the RAG system. Based on the retrieved information, ILM-TR generates text that is stored in Short-Term Memory (STM), which is then used to formulate subsequent queries. This retrieval process is repeated until the text in STM converges. Our experiments demonstrate that retrieval with STM offers improvements over traditional retrieval-augmented LLMs in the M-NIAH and BABILong test. And since the answers for M-NIAH tests are known, in future work, we can explore fine-tuning the answer model output based on the intermediate results from STM and the reference answer. This could potentially improve model's active search capabilities with RAG system."}, {"title": "Appendix / supplemental material", "content": ""}, {"title": "Prompt", "content": ""}, {"title": "Baseline Prompt", "content": ""}, {"title": "Summary Model", "content": "[SYSTEM]: You are a reader who can summarize the given text while including important details. Do not provide any comments, just give the summary.\n[USER]: Write a summary of the following context, just including the most impor-tant details: {context}"}, {"title": "Answer Model", "content": "[SYSTEM]: You are Question Answering Portal.\n[USER]: Given Context: {Retrieved Info} Give the best full answer to question {User's Query}"}, {"title": "ILM-TR Prompt", "content": ""}, {"title": "Summary Model", "content": "[SYSTEM]: I will give you context. Most of it could be about the same things, but there may be some abnormal information. An surprising sentence is not related to most of the other content. You should summarize the context with the necessary information and also include any surprising information you think. Don't return any unrelated words."}]}