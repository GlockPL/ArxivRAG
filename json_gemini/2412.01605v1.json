{"title": "MedChain: Bridging the Gap Between LLM Agents and Clinical Practice through Interactive Sequential Benchmarking", "authors": ["Jie Liu", "Wenxuan Wang", "Zizhan Ma", "Guolin Huang", "SU Yihang", "Kao-Jung Chang", "Wenting Chen", "Haoliang Li", "Linlin Shen", "Michael Lyu"], "abstract": "Clinical decision making (CDM) is a complex, dynamic process crucial to healthcare delivery, yet it remains a significant challenge for artificial intelligence systems. While Large Language Model (LLM)-based agents have been tested on general medical knowledge using licensing exams and knowledge question-answering tasks, their performance in the CDM in real-world scenarios is limited due to the lack of comprehensive testing datasets that mirror actual medical practice. To address this gap, we present MedChain, a dataset of 12,163 clinical cases that covers five key stages of clinical workflow. MedChain distinguishes itself from existing benchmarks with three key features of real-world clinical practice: personalization, interactivity, and sequentiality. Further, to tackle real-world CDM challenges, we also propose MedChain-Agent, an AI system that integrates a feedback mechanism and a MCase-RAG module to learn from previous cases and adapt its responses. MedChain-Agent demonstrates remarkable adaptability in gathering information dynamically and handling sequential clinical tasks, significantly outperforming existing approaches. The relevant dataset and code will be released upon acceptance of this paper.", "sections": [{"title": "Introduction", "content": "At the intersection of artificial intelligence and healthcare lies one of medicine's most complex challenges: Clinical Decision Making (CDM). In healthcare delivery, CDM demands not only the synthesis of diverse data sources and continuous assessment of evolving clinical scenarios, but also evidence-based judgments for diagnosis and treatment (Sutton et al., 2020). While crucial for optimal patient care, this intricate process imposes significant cognitive demands on healthcare professionals, making it an ideal candidate for AI assistance (Sendak et al., 2020).\nRecent advances in Large Language Model (LLM)-based agents (OpenAI, 2023; Team et al., 2023; Gu et al., 2023; Shinn et al., 2024; Guan et al., 2023; Zhuang et al.) have emerged as an effective solution for complex decision-making tasks, from software development (Qian et al., 2024) to office automation (Wang et al., 2024c). In the medical domain, these agents have demonstrated impressive performance on medical licensing exams (Singhal et al., 2023; Pal et al., 2022) and knowledge-based assessments (Gilson et al., 2023; Eriksen et al., 2023). While LLMs have consistently scored well above passing thresholds in these assessments (Singhal et al., 2023), it is crucial to recognize that these assessments fall short of capturing the complexity of real-world CDM in three critical aspects.\nFirstly, these benchmarks rarely account for patient-specific information such as past medical history and present illness (Pal et al., 2022), which significantly influence clinical decisions in real clinical scenarios. This omission fails to capture the nuanced context that often shapes personalized diagnosis. Secondly, unlike real clinical scenarios where decisions build upon previous steps, existing benchmarks present clinical tasks as independent problems (Schmidgall et al., 2024), missing the critical interdependencies in the diagnostic process. In reality, clinical decision-making is a sequential process where each step is contingent upon the preceding ones, and an error in one stage can profoundly impact subsequent decisions. Thirdly, most benchmarks present all relevant information upfront, providing a static, comprehensive dataset (Tu et al., 2024). However, real clinical workflow demand multiple rounds of dynamic information gathering through ongoing patient interaction.\nMedChain: To address these critical gaps, we introduce MedChain, a novel benchmark designed to evaluate LLM-based agents in real-world clinical scenarios. Specifically, MedChain comprises 12,163 diverse cases spanning 19 medical specialties and 156 sub-categories, including 7,338 medical images with corresponding reports. Each case progresses through five crucial stages: specialty referral, history-taking, examination, diagnosis, and treatment. Unlike existing benchmarks, MedChain uniquely emphasizes three key features. 1) Personalization: Each case incorporates detailed patient-specific information. At first, agents are provided with only the patient's chief complaint and basic information. 2) Interactivity: Information must be actively gathered through dynamic consultation from patient. 3) Sequentiality: Decisions at each stage influence subsequent steps. Only after agent independently completes all five stages, the overall diagnostic process is evaluated.\nMedChain-Agent: Given the novel features and challenges presented by this benchmark, existing agent frameworks struggle to address these issues adequately. To overcome these limitations, we propose MedChain-Agent, a multi-agent collaborative framework that enables LLMs with feedback mechanism and MedCase-RAG to dynamically gather information and handle sequential clinical tasks. Specifically, MedChain-Agent facilitates a synergistic interplay among three specialized agent types: General Agents for task-specific expertise, a Summarizing Agent for insight synthesis, and a Feedback Agent for iterative refinement. This multi-layered, iterative approach ensures decisions are products of thorough analysis and diverse perspectives. Additionally, to address the multifaceted nature of CDM, which demands the integration of evidence-based research, and patient-specific factors, we incorporate a novel MedCase-RAG module into our MedChain-Agent framework. Unlike conventional medical RAG methods, MedCase-RAG dynamically expand its database and employs a structured approach to data representation, mapping each medical case into a 12-dimensional feature vector. This system enables efficient retrieval of relevant cases and helps the model make informed decisions.\nOur contributions are summarized as follows:\n\u2022 We represent the first effort to propose a CDM benchmark, MedChain, providing a holistic assessment of diagnostic capabilities of LLLM-based agents, closely reflecting real-world patient care.\n\u2022 We propose a multi-agent framework based on the characteristics of CDM. This system enables efficient retrieval of relevant cases and helps the model make informed decisions.\n\u2022 Through extensive experiments, we demonstrate the effectiveness of MedChain and the MedChain-Agent framework in improving clinical decision-making accuracy and reliability."}, {"title": "Related Works", "content": "Benchmarking plays a vital role as a key performance indicator, directing model improvements, pinpointing weaknesses, and shaping the course of model evolution. The evaluation of LLMs in medicine has primarily focused on testing general medical knowledge through structured assessments. Leading benchmarks such as MultiMedQA (Singhal et al., 2023) integrate various medical QA datasets (e.g., MedQA (Jin et al., 2021), MedMCQA (Pal et al., 2022)), emphasizing performance on medical licensing examination materials. Other benchmarks like PubMedQA (Jin et al., 2019) focus on research-oriented queries, while several Chinese medical benchmarks (Wang et al., 2023; Cai et al., 2024) evaluate models through multiple-choice questions from medical licensing exams.\nWhile these benchmarks effectively assess general medical knowledge, they fail to capture three critical aspects of real-world clinical decision-making (see Table 1), i.e., personalization in patient care, the interactive nature of clinical consultations, and the sequential dependency of medical decisions, where each step builds upon previous findings."}, {"title": "LLM-based Agent in Medicine", "content": "LLM-based agents have demonstrated significant potential across various medical applications, encompassing tasks such as medical examination questions, clinical diagnoses, and treatment plans. Recent research has explored different approaches: Agent Hospital (Li et al., 2024) provides medical scenario simulation, while several frameworks (Yue et al., 2024; Tang et al., 2024; Kim et al., 2024b) focus on specific medical stages with multi-agent architectures. Some works target specialized aspects, such as CoD (Chen et al., 2024a) for interpretable diagnostics and Ehragent (Shi et al., 2024) for electronic health records (EHRs) analysis. Others, like Almanac Copilot (Zakka et al., 2024), assist clinicians with EMR-specific tasks. AI Hospital (Fan et al., 2024) explores interactive clinical scenarios, but it falls short in handling multi-modal medical imaging and lacks a comprehensive benchmark for evaluating multi-agent performance. To enhance these agents' capabilities, researchers have integrated Retrieval-Augmented Generation (RAG), as demonstrated by MIRAGE (Xiong et al., 2024)'s search-enhanced framework and Medical Graph RAG (Wu et al., 2024)'s knowledge-based approach.\nHowever, current approaches face two major limitations. First, existing frameworks focus on isolated medical tasks rather than providing seamless integration across different clinical stages, making them insufficient for complex scenarios requiring effective inter-stage communication (Gronauer and Diepold, 2022). Second, current medical RAG systems' reliance on chunk-based indexing leads to context inconsistency and computational inefficiencies (Huang and Huang, 2024; Gao et al., 2024), highlighting the need for more sophisticated approaches to medical knowledge integration."}, {"title": "MedChain Benchmark", "content": "Overview. We introduce MedChain, a comprehensive clinical decision-making benchmark designed to simulate real-world scenarios. Built upon 12,163 Electronic Health Records (EHRs) spanning 19 medical specialties and 156 sub-categories, including 7,338 medical images with reports, MedChain uniquely emphasizes three key characteristics:\n\u2022 Personalization: Each case includes detailed patient profiles that influence decision-making\n\u2022 Sequentiality: Cases involve multiple interconnected decision-making stages"}, {"title": "Data Collection", "content": "Data Source and Processing. Our dataset is sourced from the Chinese medical website \"iiYi\" 1, which provides over 20,000 validated clinical cases spanning 28 disease categories. These cases are verified by professional doctors and have undergone de-identification to ensure patient privacy. We obtained formal permission from the website administrators to use the data for scientific research purposes. Each case typically contains the patient's chief complaint, medical history, examination results, treatment process, and other relevant information, which insure the personalization of MedChain. Following the government standards 2 and Medical Subject Headings 3, we extracted and organized key information including patient basics, chief complaints, specialty referrals, examinations, imaging reports, diagnoses, and treatments. Cases with incomplete information were removed, resulting in 12,163 high-quality cases.\nQuality Control. To ensure the highest standards of data integrity and clinical relevance in our benchmark, we implemented a rigorous quality control process involving a panel of five senior physicians, each with over 10 years of clinical experience. Our evaluation process examined a random sample of 6,000 cases (49.3% of the dataset). Each case was assessed on multiple dimensions including disease prevalence, clinical relevance, accuracy of patient history, appropriateness of diagnostic procedures, correctness of diagnosis, and suitability of treatment recommendations. We employed a standardized scoring system to quantify case quality across these dimensions and calculated inter-rater reliability using Cohen's kappa coefficient. The quality assessment yielded impressive results, with an overall quality rate of 94.7% of evaluated cases meeting or exceeding our stringent quality thresholds. Dimension-specific quality rates ranged from 92.9% to 97.2%, demonstrating consistently high standards across all evaluation criteria. Our inter-rater reliability analysis produced a Cohen's kappa coefficient of 0.82, indicating strong agreement among our expert reviewers. Cases that did not meet the thresholds (5.3%) underwent revision or were excluded from the final dataset to maintain"}, {"title": "Clinical Workflow Simulation", "content": "Sequential Stages. MedChain simulates the complete clinical workflow, comprising five sequential tasks, each representing a different stage of the clinical decision-making process, as shown in Figure 2. The results from each stage serve as inputs for the subsequent stage, creating a dependency where later decisions are influenced by the quality of earlier ones. This design guarantees the sequentiality of MedChain, mimicking the interconnected nature of real-world clinical decision-making processes. The pipeline consists of: 1) Specialty Referral: Assessment of case urgency and appropriate department selection; 2) History-taking: Dynamic information gathering through doctor-patient dialogue; 3) Examination: Medical image analysis and report generation; 4) Diagnosis: Comprehensive diagnosis based on accumulated information; 5) Treatment: Treatment plan formulation considering patient-specific factors.\nThe constrcution pipeline of MedChain is sketched out here. The details of benchmark can be refer to Appendix A. We demonstrate several examples in Appendix Figure 4."}, {"title": "Interaction Environment", "content": "To simulate the doctor-patient consultation process in real medical scenarios, we have developed an interactive environment where the LLM-based agent under evaluation must actively gather information through dynamic interactions. We employ a local large language model, i.e., gemma2 (9b) (Team et al., 2024), to serve as the patient agent. This patient agent is initialized with pre-defined case information but is unaware of the actual diagnosis, allowing it to provide symptom information and respond to inquiries in a manner that closely mimics real patient experiences. This setup ensures the interactivity for the MedChain benchmark, enabling it to evaluate the LLM's ability to conduct effective medical consultations, ask pertinent questions, and gather crucial information for accurate diagnosis and treatment planning. The inspiration for this patient agent setup comes from standardized patients in medical education and clinical practice. Standardized patients are trained individuals who simulate real patients, allowing healthcare professionals to practice their clinical and communication skills in a controlled environment without the risks associated with real-patient interactions (Barrows, 1993)."}, {"title": "MedChain-Agent Framework", "content": "The MedChain-Agent framework introduces a novel multi-agent system designed to simulate the complex, interconnected nature of medical decision-making processes. This system leverages the collective intelligence of multiple specialized agents, each contributing unique expertise to the overall decision-making process."}, {"title": "Agent Role", "content": "Our framework incorporates three distinct agent types, each fulfilling crucial roles in the medical decision-making pipeline:\nGeneral Agents: These specialized agents are recruited based on the specific requirements of each task. For instance, in Task 1 Specialty Referral, General Agents possess comprehensive knowledge of medical specialties and triage protocols, enabling them to effectively assess and route patients to appropriate departments. These agents engage in group discussions, simulating the collaborative nature of medical consultations.\nSummarizing Agent: In each stage of the process, a summarizing agent consolidates the discussions and analyses of the general agents. This agent synthesizes the collective expertise into a coherent summary and delivers the final decision for each task, mimicking the role of a senior physician or department head within a medical context.\nFeedback Agent: The feedback agent plays a crucial role in maintaining the quality and accuracy of decisions throughout the process. This agent evaluates the output of each task, providing constructive feedback and suggesting iterative improvements when necessary. By doing so, it creates a dynamic, self-correcting system that continually refines its decision-making capabilities."}, {"title": "Decision Making", "content": "With three types of agents, MedChain-Agent conducts a sophisticated decision-making process that mirrors real-world medical practice. General agents analyze patient information and engage in collaborative discussions, sharing assessments and recommendations. The summarizing agent then consolidates these insights, weighing different perspectives to formulate a preliminary decision. The feedback agent reviews this decision, evaluating its appropriateness and implications for subsequent tasks. If issues are identified, the feedback agent provides critiques and suggestions, initiating an iterative refinement process. General agents reconsider their assessments based on this feedback, while the summarizing agent refines the decision. This cycle continues until consensus is reached or a predefined number of iterations is completed, at which point the final decision is issued."}, {"title": "MedCase-RAG", "content": "To enhance the decision-making capabilities of our multi-agent framework, we introduce MedCase-RAG, a novel Retrieval-Augmented Generation technique tailored for medical contexts.\nUnlike conventional medical RAG methods that rely on unstructured medical QA databases, MedCase-RAG employs a structured approach to data representation. We map each medical case into a 12-dimensional feature vector, encompassing crucial attributes such as Age, Sex, Patient Description, Symptom Description, Patient History, and Patient Image. This structured representation allows for more comprehensive and standardized case encoding, facilitating more accurate and nuanced retrieval. Within this feature set, we identify \"Symptom Description\" as the most representative feature of a patient's condition. This feature undergoes quantification through a Text Embedding model, serving as the primary key for dense retrieval tasks. Moreover, MedCase-RAG incorporates a dynamic database expansion mechanism to continually enrich its knowledge base. As patients complete their medical processes and recover, their case information is reintroduced into the database as pseudo-data. This approach allows the system to learn from new cases and adapt to evolving medical knowledge and practices.\nWhen a new patient case is presented, MedCaseRAG performs a similarity search within the relevant medical department, using cosine similarity to identify the top three most similar cases, offering a more comprehensive basis for decision-making, allowing agents to consider various aspects and potential outcomes."}, {"title": "Experiments", "content": ""}, {"title": "Experimental Setup", "content": "We split the dataset into training, validation, and testing sets with a ratio of 7:1:2. For frameworks incorporating RAG techniques, both training and validation sets are used to construct the case retrieval database. Our study evaluates both single-agent and multi-agent systems. For single agents, we test two closed-source models (gpt-4o-mini (OpenAI, 2023), and claude-3.5-sonnet (Team et al., 2023)) and three open-source models (InternVL2-8b (Chen et al., 2023), llava-llama-3-8b-v1_1 (Liu et al., 2024a), and Qwen2-7B-Instruct (Wang et al., 2024a)), with model weights obtained from official Hugging Face repositories. In the multi-agent evaluation, we compare MedChain-Agent against MedAgent (Tang et al., 2023) and MDAgent (Kim et al., 2024a). All multi-agent framework are based on InternVL2-8b (Chen et al., 2023). The deployment was conducted using the LMDeploy framework (Contributors, 2023). All tests executed on NVIDIA A100 GPUs featuring 80GB of memory. To enhance output stability and reliability across all experiments, we consistently set the temperature parameter to 0."}, {"title": "Evaluation Metrics", "content": "To assess model performance, we employ diverse metrics tailored to each task. Accuracy is used for tasks with a single correct answer, such as Specialty Referral (Level 1). For tasks involving multiple key points, including Specialty Referral (Level 2), History-taking, and Treatment, we apply Intersection over Union (IoU) to capture the overlap between predicted and ground truth sets. Specifically for the Examination task, we utilize DocLens (Xie et al., 2024) to evaluate the quality of image interpretation reports."}, {"title": "Experiment Result and Analysis", "content": "The results of our evaluation in the MedChain are presented in Table 2. Our analysis yields three significant insights:\n(1) Sequential decision-making tasks continue to pose significant challenges, even for advanced models. For instance, within the single-agent frameworks, GPT-4o-mini and InternVL2 achieve average scores of 0.4442 and 0.3920, respectively. These results indicate that despite their sophistication, these models struggle to maintain consistent performance across the sequential stages of clinical decision-making, highlighting the inherent difficulty of these tasks.\n(2) Multi-agent frameworks based on InternVL2, such as MedAgent and MDAgent, exhibit inferior performance compared to their single-agent counterparts. This degradation suggests that traditional multi-agent approaches may exacerbate error propagation in sequential decision-making processes, leading to reduced overall performance. In contrast, our proposed MedChain-Agent significantly outperforms these multi-agent methods, achieving an average score of 0.5269. This improvement demonstrates that MedChain-Agent effectively mitigates error propagation, enhancing decision quality and reliability in clinical settings.\n(3) The integration of the MedChain-Agent framework with open-source LLMs demonstrates significant superiority over proprietary models like GPT-4o-mini. The substantial performance gain observed with MedChain-Agent (average score of 0.5269) implies that our framework can leverage the strengths of open-source LLMs to achieve superior outcomes. This suggests that open-source models, when enhanced with our framework, are not only competitive but can also excel in handling intricate medical decision-making tasks."}, {"title": "Ablation Study", "content": "Three key characteristics for MedChain: To validate the effectiveness of personalization, interactivity, and sequentiality within our benchmark, we conduct ablation study as in Table 4. We systematically remove each characteristic and observe its impact on model performance across Diagnosis and Treatment tasks. 1) Removing patient-specific information (w/o Person.) consistently degrades performance across all models in diagnosis tasks (with drops ranging from 3.0% to 13.4%). This demonstrates that personalized information is crucial for accurate clinical decision-making. 2) Interestingly, when removing the sequential dependency between stages (w/o Seq.), most models show improved performance. This improvement actually validates the effectiveness of sequentiality in our benchmark, as it indicates that sequential decision-making poses greater challenges that better reflect real-world clinical scenarios. 3) Similarly, the improved performance observed after removing interactive information gathering (w/o Inter.) confirms the effectiveness of interactivity in our benchmark design. These results collectively suggest that while both sequentiality and interactivity make the benchmark more challenging, they are essential components that better simulate the complexity of real-world clinical decision-making processes.\nKey components for MedChain-Agent: To assess the impact of individual components in our framework, we conducted ablation experiments, with results shown in Table 3. These findings highlight the crucial roles of both the Feedback mechanism and the MedCase-RAG module in enhancing overall performance. Each component independently contributes to significant improvements across various metrics. Notably, the MedCase-RAG module alone boosts Specialty Referral (Level 1) to 0.5928 and yields substantial gains in History-taking and Examination. When combined, Feedback and MedCase-RAG demonstrate a synergistic effect, achieving an impressive average score of 0.5269. This underscores the complementary nature of these components in optimizing our framework's capabilities."}, {"title": "Conclusion", "content": "In this paper, we introduced MedChain, a novel benchmark for evaluating large language models in clinical decision-making, and MedChain-Agent, an innovative multi-agent framework designed to address the complexities of real-world medical scenarios. Our work bridges a critical gap between current AI capabilities and the nuanced realities of clinical practice by incorporating personalization, sequentiality, and interactivity into the evaluation process. MedChain-Agent's sophisticated multi-agent system, enhanced by a feedback mechanism and the MedCase-RAG module, demonstrates unprecedented performance in sequential clinical decision-making tasks, even outperforming proprietary models when combined with open-source LLMs. By providing a more realistic and challenging evaluation framework, this research sets a new standard for evaluating and developing medical AI systems, paving the way for its responsible integration into clinical practice."}, {"title": "Limitations", "content": "This paper has two primary limitations that offer avenues for future research:\n\u2022 Data Source Diversity: The MedChain benchmark is constructed from 12,163 electronic health records obtained from the Chinese medical website \"iiYi.\" Although this dataset is extensive and covers 19 medical specialties and 156 sub-categories, it is derived from a single source. In our future work, we will incorporate additional data sources from different regions or healthcare systems to further enrich the dataset, providing a broader spectrum of clinical scenarios and enhancing the generalizability of the benchmark.\n\u2022 Patient Interaction Simulation: In our interactive environment, the patient responses during the history-taking stage are generated by the Gemma 2 language model. While this approach ensures consistency and control in evaluating the LLM-based agent, the real patient interactions can be more varied and complex. Future work could explore more advanced patient simulators or incorporate real dialogue data to capture a wider range of communication styles and behaviors."}, {"title": "Appendix for MedChain", "content": "Abstract.\nAppendix A describes the process of standardizing and organizing the dataset for the MedChainbenchmark.\nAppendix B provides a detailed explanation of the MedChain-Agent framework, including its implementation, feedback mechanism, and the novel Retrieval-Augmented Generation (RAG) approach used to enhance decision-making.\nAppendix C lists the additional details for experiment."}, {"title": "Benchmark Construction", "content": "We employed a combination of large language models and human verification to label data across different tasks. Our methodology involves task-specific prompt construction and output matching to ensure data quality and diversity while maintaining alignment with real clinical case scenarios.\nTo ensure consistency and comparability across the benchmark, we standardized the classification of examination items into two main categories: Physical Examinations and Auxiliary Examinations. Physical Examinations include evaluations of various body systems and general health indicators, while Auxiliary Examinations encompass different imaging techniques and laboratory tests. We utilized GPT-40 to extract and classify examination items from each case, followed by manual verification to ensure accuracy. For medical imaging, we classified images into seven types, and manual review ensured the correctness of the classifications. Additionally, treatment items were extracted and categorized from each case. This standardization process ensures that the dataset is consistent, facilitating accurate and comparable evaluations of LLM performance."}, {"title": "Tasks Details", "content": "This section provides a comprehensive description of the five specific tasks that make up the MedChainbenchmark. Each subsection elaborates on a particular task, detailing its input, output, and evaluation methods."}, {"title": "Task 1: Specialty referral", "content": "The specialty referral task evaluates the LLM's ability to assess the urgency of a patient's condition and determine the appropriate department based on the patient's chief complaint. The input consists of the patient's chief complaint, and the output space includes 19 first-level departments and 156 second-level departments. The LLM must first assign the patient to one of the 19 first-level departments, then to one or more of the 156 second-level departments based on the primary symptoms. Evaluation metrics include accuracy for first-level department assignment and Intersection over Union (IoU) for second-level department assignment. IoU is used for second-level departments to account for the possibility of multiple correct assignments and to reflect partial correctness, which can occur in complex cases. This metric better captures the nuanced nature of departmental referrals in clinical practice."}, {"title": "Task 2: History-taking", "content": "The history-taking task is designed to simulate doctor-patient communication, where the goal is to obtain relevant information and infer necessary examination items. We employ a multi-agent system to evaluate this process:\n\u2022 Doctor Agent: The LLM being evaluated plays this role, asking questions and suggesting examinations based on the patient's responses.\n\u2022 Patient Agent: A local large model (such as Google's gemma2/9b) simulates the patient, responding based on pre-defined case information. This includes the patient's chief complaint, medical history, and examination results.\n\u2022 Extraction Agent: This agent extracts examination items that the Doctor Agent inquires about or suggests from the dialogue between the Doctor and Patient Agents. These extracted items form the output for comparison with the ground truth examination set.\nThe evaluation metric is the IoU between the examination items extracted by the Extraction Agent (based on the Doctor Agent's inquiries and suggestions) and the ground truth set. This multi-agent design simulates a realistic clinical history-taking process, allowing the LLM to demonstrate its ability to ask relevant follow-up questions, interpret patient responses, and determine appropriate examinations. The use of a local large model as the"}]}