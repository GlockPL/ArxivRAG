{"title": "Evaluating the Effectiveness of Large Language Models in Automated News Article Summarization", "authors": ["Lionel Richy Panlap Houamegni", "Fatih Gedikli"], "abstract": "The automation of news analysis and summarization pre- sents a promising solution to the challenge of processing and analyzing vast amounts of information prevalent in today's information society. Large Language Models (LLMs) have demonstrated the capability to transform vast amounts of textual data into concise and easily compre- hensible summaries, offering an effective solution to the problem of in- formation overload and providing users with a quick overview of relevant information. A particularly significant application of this technology lies in supply chain risk analysis. Companies must monitor the news about their suppliers and respond to incidents for several critical reasons, in- cluding compliance with laws and regulations, risk management, and maintaining supply chain resilience. This paper develops an automated news summarization system for supply chain risk analysis using LLMs. The proposed solution aggregates news from various sources, summa- rizes them using LLMs, and presents the condensed information to users in a clear and concise format. This approach enables companies to op- timize their information processing and make informed decisions. Our study addresses two main research questions: (1) Are LLMs effective in automating news summarization, particularly in the context of sup- ply chain risk analysis? (2) How effective are various LLMs in terms of readability, duplicate detection, and risk identification in their summari- zation quality? In this paper, we conducted an offline study using a range of publicly available LLMs at the time and complemented it with a user study focused on the top performing systems of the offline experiments to evaluate their effectiveness further. Our results demonstrate that LLMs, particularly Few-Shot GPT-40 mini, offer significant improvements in summary quality and risk identification.", "sections": [{"title": "1 Introduction", "content": "In an increasingly interconnected and globalized economy, companies must ef- ficiently manage complex supply chains while identifying potential risks early."}, {"title": "2 Related Work", "content": "The field of automated text summarization, particularly in the context of supply chain risk analysis, has evolved significantly with the advent of LLMs. This section provides an overview of prior research, highlighting key advancements, methodologies, and persisting challenges."}, {"title": "2.1 Advancements in Automated News Summarization", "content": "Liu and Lapata (2019) demonstrated that pretrained transformer models offer substantial improvements in news summarization. By integrating extractive and abstractive techniques, they improved summary coherence and informativeness, particularly when fine-tuned for domain-specific applications [9]. Stiennon et al. (2020) explored human feedback-driven training for summa- rization. They developed a reward model based on human preference alignment, which significantly improved model performance compared to traditional super- vised learning techniques [15]. Goyal et al. (2022) assessed GPT-3's capabilities in news summarization. They found that GPT-3 could generate high-quality summaries without explicit fine-tuning, often outperforming human-generated summaries in coherence and fluency [3]. Wei et al. (2022) introduced chain-of-thought prompting, demonstrating how structured reasoning prompts enable LLMs to perform complex summarization tasks with greater contextual awareness [18]. Liu et al. (2023) proposed using LLM-generated summaries as reference train- ing data for smaller models. Their approach enhanced the efficiency of smaller- scale summarization models, presenting a cost-effective alternative to fine-tuning large models [11]. Zhang et al. (2024) studied the impact of instruction tuning on zero-shot sum- marization. They found that aligning models with high-quality reference summa- ries significantly improved performance, often rivaling human-written summaries [20]. Further, Zhang et al. (2024) provided a systematic review of text summariza- tion. They examined the transition from extractive to abstractive techniques and highlighted key challenges, such as factual accuracy, handling long documents, and mitigating biases in generated content [19]. An underexplored aspect of summarization is identifying related news arti- cles also known as news story chains. Gedikli et al. (2021) addressed this chal- lenge in [2] by leveraging clustering and Named Entity Recognition (NER) to cre- ate datasets for automated story chain detection, significantly reducing manual labeling efforts while maintaining high-quality outputs. Similarly, Stockem Novo and Gedikli (2023) [16] investigate BERT-based methods for near-duplicate news article detection, highlighting the importance of NER in identifying duplicate content. Accurate duplicate detection is essential for risk news summarization, as redundant information can distort risk assessments and lead to inefficiencies in decision-making."}, {"title": "2.2 Research Gaps and Contributions of This Work", "content": "Despite significant progress, several challenges remain unaddressed in the field of automated news summarization, particularly in the context of supply chain risk analysis. This study aims to bridge the following gaps: Domain-Specific Adaptation: Most existing LLMs are not optimized for supply chain risk analysis. This study explores methods to tailor LLMs for industry-specific summarization. Factual Accuracy and Bias Mitigation: Ensuring the reliability of gener- ated summaries remains a critical issue. We investigate strategies to enhance factual accuracy and mitigate biases [20]. Real-Time Integration: The deployment of LLMs for real-time news anal- ysis and early risk detection has been insufficiently explored. This work ex- amines the feasibility of integrating LLMs into dynamic monitoring systems. Evaluation of Readability and Duplicate Detection: While some mod- els produce coherent summaries, their effectiveness in detecting duplicate information and maintaining readability in large-scale analysis remains un- clear. Fine-Tuning for Risk Identification: There is a lack of research on opti- mizing LLMs specifically for identifying and categorizing risk-related infor- mation in news articles relevant to supply chain disruptions. By addressing these gaps, our work contributes to the ongoing development of AI-driven risk analysis solutions, demonstrating how LLMs can be effectively leveraged to enhance supply chain monitoring and resilience."}, {"title": "3 Methodology", "content": "This study adopts a mixed-methods approach, combining qualitative and quan- titative techniques to ensure a comprehensive and nuanced analysis. Data will be collected from a diverse range of news sources, including local, regional, and international news agencies, as well as digital magazines. When news articles were not originally in English, they were translated using state-of-the-art trans- lation models from Hugging Face\u00b9 to ensure consistency in analysis. However, due to copyright restrictions, the original news articles used in this study cannot be publicly shared. To develop the news summarization system, a variety of large language mod- els (LLMs) will be employed, including GPT-40, GPT-40 mini, GPT-3.5 Turbo, Mistral Large 2, Mistral 8x22b, Mistral 7b, Llama-3.2 90b, Llama-3.1 70b, Llama 3.1 8b, Llama 3 70b, Llama 3 8b, Gemma 2-9b, Gemma 7b, and two fine-tuned models (one based on GPT-40 mini and another on GPT-3.5 Turbo). These models will be evaluated under different learning paradigms-zero-shot learn- ing, few-shot learning, and fine-tuning-to optimize their performance and the accuracy of the generated summaries."}, {"title": "3.1 Experimental Design", "content": "Input: News articles from local, regional, and international sources, includ- ing digital magazines, translated to English when needed. Output: Summarized, duplication-free news articles that highlight potential risks in supply chains, enabling informed decision-making. Model Comparison: A thorough comparison of the performance of var- ious LLMs will be conducted. The evaluation will focus on the quality of the summaries and the models' ability to identify risks. Different learning strategies-zero-shot learning, few-shot learning, and fine-tuning-will be employed to optimize the models' effectiveness. Hypothesis: The latest LLMs will produce higher-quality summaries and more accurate risk identification compared to older models. Independent Variable: The specific language model used. Dependent Variables: The quality of the summaries and the accuracy of risk identification. Procedure: News articles will be input into the various language models, and the resulting summaries will be evaluated based on precision, relevance, and the ability to identify risks. Additional metrics such as price, speed, latency, and context windows will also be considered. The results will be subjected to statistical analysis to compare the performance of the models. Expected Results: It is anticipated that newer models, such as GPT-40 for proprietary models and LLaMA-3.2 90b for open-source models, will deliver superior summary quality and more precise risk identification. This im- provement is expected to provide a stronger foundation for decision-making in supply chain management. It is important to emphasize that these models represent the state of the art in LLMs at the time of our study (October 2024)."}, {"title": "3.2 Workflow", "content": "The workflow for this study is designed to systematically address the challenges of supply chain risk analysis using computational intelligence techniques. The process is divided into seven key stages, as illustrated in Figure 1, and described below: (1) Data Import and Exploratory Data Analysis (EDA): The raw dataset consists of 1,535 press articles, each enriched with meta- data, including URLs, publication dates, headlines, abstracts, and sentiment ratings. Additional attributes capture industry classifications, risk categories, and references to key entities such as people, organizations, and locations. The dataset, provided by graphworks.ai\u00b3\u2014an AI-powered platform for real-time news monitoring-spans 28 industries, with Industrial Manufacturing and Ser- vices, Transportation, and Energy being the most prevalent. This distribution aligns with key focus areas in supply chain risk analysis. Risk classifications en- compass broad categories such as Interruption and Shutdown, as well as more specific concerns like Cybersecurity and Data Privacy. Initial data cleaning in- volved removing irrelevant columns, correcting inconsistent values, and ensuring data integrity for downstream tasks. (2) Separation of Training and Test Data: To ensure a robust eval- uation, the dataset was divided into distinct training and test subsets using an 80/20 split. Specifically, 1,228 articles (80%) were allocated for training, while 307 articles (20%) were reserved for testing. This division ensures a bal- anced evaluation of model performance, allowing for an unbiased assessment of generalizability on unseen data. (3) Metadata Extraction: A computational pipeline is implemented to extract metadata, including article titles, original links, industry classifications, and risk perceptions. This metadata is integrated into the input parameters for summarization models to enhance contextual understanding and mitigate hallucinations, ensuring consistency across different LLMs. (4) Prompt Engineering: Effective prompt design is crucial for guid- ing LLMs in generating high-quality summaries. Drawing on best practices in prompt engineering4, we employ few-shot prompting techniques [1] to improve performance. Key parameters are carefully configured: Maximum Token Count: 4096 (total length of input and output) Temperature: 0.3 (optimized through iterative experimentation) Top P (nucleus sampling): 0.5 (determined via systematic testing) (5) Fine-Tuning: In addition to evaluating the base models, we fine-tune GPT-40 mini and GPT-3.5 Turbo for the specific task of news summarization in supply chain risk analysis using our domain-specific dataset. This step leverages computation techniques such as LoRA [7] to adapt the models to the unique characteristics of the dataset, enhancing their ability to capture nuanced risk- related information. Both fine-tuned and non-fine-tuned variants are tested to compare their performance. It should be noted that due to cost, time, and resource constraints, we could not utilize the entire training dataset during fine-tuning. Additionally, OpenAI's fine-tuning framework imposes a token limit of < 4096 tokens per training exam- ple. As a result, in some cases, not all 10 articles for a given summary fit within the context window, requiring truncation to meet the token limit constraint. However, each training example contained at least 7-10 news articles along with a corresponding summary. (6) Evaluation: A comprehensive evaluation framework is employed to com- pare model performance across zero-shot, few-shot, and fine-tuned settings. Key evaluation criteria include: Summary Quality: Assessed using metrics such as coherence, relevance, and informativeness. Computational Efficiency: Measured in terms of cost (USD per million tokens), output speed (tokens per second), latency (time to first token), and context window size. G-Eval: A novel evaluation framework that provides deeper insights into model strengths and weaknesses, enhancing the accuracy of performance assessments. This method follows best practices as described by Anadkat and Fishman. Human Evaluation: Essential for validating machine-generated summa- ries, human evaluation ensures alignment with domain-specific requirements and user expectations [13,10,14]. (7) Results and Discussion: The final results are presented, showcasing the effectiveness of the proposed workflow in generating high-quality summa- ries for supply chain risk analysis. The findings are analyzed in detail, with a discussion on their implications and potential applications. Additionally, future research directions are identified, particularly the integration of advanced LLM- based approaches (e.g., retrieval-augmented generation, reinforcement learning with human feedback) to further enhance model performance."}, {"title": "4 Results", "content": "This section presents the experimental results of employing multiple LLMs to generate summaries for news articles. In the context of LLMs, inference refers to the process of utilizing a trained model to generate predictions or derive insights from new, unseen data. Inference can be conducted using various approaches, including zero-shot, few-shot, and fine-tuning. This study evaluates the perfor- mance of several state-of-the-art language models to address the central research question: Can zero-shot prompting, few-shot prompting, and fine-tuning provide effective solutions for summarizing news articles in the context of supply chain risk analysis? In light of recent research findings demonstrating that few-shot learning con- sistently outperforms zero-shot approaches and significantly enhances model per- formance [1], GPT-40 is employed as the reference model in a few-shot learning context. As part of the latest GPT-4 generation, GPT-40 strikes an optimal bal- ance between performance and accessibility, making it particularly suitable for research applications. The model has demonstrated exceptional capabilities in complex language understanding tasks, including reasoning and knowledge-based benchmarks such as the Massive Multitask Language Understanding (MMLU) [4], MMLU-Pro [17], and General Purpose Question Answering (GPQA) [12]. These achievements underscore GPT-40's robustness and versatility, solidifying its position as a reliable choice for this study. Additionally, a human expert has reviewed and modified the GPT-40 reference summary where necessary, ensuring the accuracy and comprehensiveness of the information presented."}, {"title": "4.1 Inference with different LLMs", "content": "Automatic Analysis Using Similarity Metrics This analysis 1 compares various LLMs under different configurations (zero-shot, few-shot, fine-tuning). Note that R-1, R-2, and R-L represent ROUGE-1, ROUGE-2, and ROUGE-L scores, respectively. Tests were conducted by having each LLM summarize 10 ar- ticles in each summarization step. Key findings from the quantitative evaluation of automated summaries using similarity metrics include: Model Performance: GPT-40 (zero-shot) achieved the best performance with an average score of 0.7402 across all metrics. Mistral Large 2 (zero-shot) and GPT-40 mini (few-shot) followed in 2nd and 3rd place. Comparison of Prompting Methods: Zero-shot prompting showed sur- prisingly good results, especially with advanced models like GPT-40 and Mistral Large 2. Few-shot prompting improved performance for some mod- els but was not consistently superior. Fine-tuning (e.g., with GPT-40 mini) yielded strong results but did not achieve top performance. Model Sizes and Performance: Larger models (e.g., GPT-40, Mistral Large 2) tended to perform better, while smaller models like Gemma 7B showed significantly weaker performance. Metrics Comparison: BERTScore consistently showed high values, indi- cating good semantic similarity. Similarly, ROUGE-1 and ROUGE-L pro- vided consistent results for evaluating summary quality. Multi-criteria comparative analysis The multi-criteria comparative analysis includes metrics such as context window, output speed, latency, costs, and token processing. Key findings include: Summary Quality and Context Window: Models like GPT-40, GPT- 40 mini, LLaMA 3.1 (70B), LLaMA 3.2, and Mistral Large 2 achieve out- standing results in summarization and risk identification. They offer a large context window of 128,000 tokens, enabling detailed analyses. Efficiency in Speed and Cost: LLaMA models, especially LLaMA3-8B, are leaders in token processing rate, offering an ideal combination of efficiency and value for money. Open-source models like Gemma 2, Mistral 7B, Mistral 8x22B, and the LLaMA series generally have lower costs per token. Prompting Methods: Few-shot prompting proved more effective for many models, especially in complex tasks. However, zero-shot prompting showed surprisingly strong results with models like GPT-40, Mistral Large 2, and LLAMA 3 70B. Cost Analysis and Efficiency: Proprietary models like GPT-40 and Mis- tral Large 2 guarantee high quality but are more cost-intensive. Open-source models (e.g., LLaMA, Gemma, Mistral 8x22B, and Mistral 7B) offer more cost-effective token and processing times. Recommended Models and Configurations: Models with large context windows (e.g., GPT-40, GPT40-mini, Mistral Large 2, and LLaMA 3.1 70B) are particularly recommended for high-quality summaries and precise risk analyses. This analysis shows that model selection strongly depends on the specific appli- cation requirements. The decision between proprietary and open-source models should be based on budget, quality requirements, and efficiency needs."}, {"title": "4.2 Quantitative Evaluation", "content": "Evaluation with G-Eval The qualitative evaluation of models using G-Eval reveals interesting differences and similarities in terms of coherence, consistency,"}, {"title": "4.3 Qualitative Evaluation", "content": "Top-Performing Models: Zero-Shot and Few-Shot GPT-40 mini, Zero- Shot Gemma2 9B, and Few-Shot Mistral 822B achieved the highest average score of 9.8. These models excel in generating coherent and consistent con- tent, making them well-suited for applications where accuracy and precision are crucial. Potential Impact: Some models received lower scores (8) in potential im- pact. This could indicate difficulties in precisely capturing the context of supply chain management, potentially limiting their practical applicability in decision-intensive areas. Few-Shot vs. Zero-Shot Configurations: Interestingly, some Zero-Shot models (like Gemma2 9B and GPT-40 mini) performed as well as or better than their Few-Shot counterparts. This suggests these models can handle complex tasks well without specific training. Fine-Tuned Models: While achieving high scores in coherence and con- sistency, fine-tuned models lagged behind the best Few-Shot and Zero-Shot models in potential impact and relevance. Fine-tuning may meet specific re- quirements but shows less flexibility and generalization ability in processing new content. Weaknesses of Individual Models: The Few-Shot Gemma 7B model, with an average score of 4.8, showed clear weaknesses in coherence and rel- evance. These low values suggest limitations in architecture or training that could hinder reliable application. Open-Source Models and Consistency: Open-source models like Gem- ma2, Mistral 8x22B, Mistral 7B, and LLaMA consistently achieved high scores, often competing with proprietary models. This consistency demon- strates that open-source approaches can offer a valid alternative for high- quality, automated text generation. Human Evaluation To complement the quantitative analyses, a human eval- uation was conducted to assess the quality of summaries generated by large lan- guage models (LLMs). Initially, the evaluation was designed to include 27 models with 10 articles each. However, due to the significant workload for participants, the methodology was refined to optimize efficiency while maintaining relevance. The final approach involved evaluating three articles per model. The models were selected based on their performance in quantitative metrics (ROUGE, BLEU, and BERTScore) and vendor diversity to ensure balanced representation. The evaluation focused on five key criteria: coherence, consistency, fluency, potential impact, and relevance. Feedback was collected from 31 participants"}, {"title": "5 Discussion", "content": "Performance of Open-Source Models: Open models such as Gemma2 9B and Mistral 8x22B performed well in human evaluation, demonstrating their potential for practical applications. Influence of Prompting Approach: No consistent superiority was ob- served between Few-Shot and Zero-Shot configurations in human evaluation, contrasting with G-Eval, where Few-Shot models generally performed better. The human evaluation process faced several challenges. Participants reported fatigue due to the volume of text, leading some to abandon the evaluation or complete it over multiple days. Additionally, the technical language and sophis- ticated style of the articles posed barriers for some evaluators. Furthermore, the models exhibited a tendency to generalize, often replacing specific place names with higher-level regions, which resulted in inaccuracies in location information. The results of the human evaluation underscore the importance of comple- menting automated metrics with human judgment. The discrepancies between human and G-Eval assessments highlight the need for further research to de- velop evaluation methods that better align with human perceptions. This study emphasizes the value of human evaluation in refining LLMs for more precise and informative news analysis, particularly in applications requiring high readability and relevance."}, {"title": "5.1 Answering the Research Questions", "content": "The study addressed several key research questions, yielding the following in- sights: Suitability for News Summarization in Risk Analysis: Modern LLMs, particularly GPT-40 mini, GPT-40, and Mistral Large 2, show strong ef- fectiveness for automated risk analysis. Statistical analyses confirm their consistent high performance across multiple metrics, including coherence, relevance, and fluency. Summary Quality: The models exhibit strong readability and coherence, with their responses implicitly incorporating duplicate detection and risk identification. This validates their ability to generate high-quality summaries tailored to supply chain risk analysis. Comparison of Zero-Shot, Few-Shot, and Fine-Tuning: Zero-shot configurations prove highly efficient when using powerful models, while few- shot approaches often enhance qualitative outcomes. Few-shot prompting, especially with GPT-40 mini, demonstrated strong performance. Fine-tuning yielded mixed results, suggesting that its effectiveness depends on the specific context. The study confirms the potential of modern LLMs for automated news sum- marization in supply chain risk analysis. Few-shot GPT-40 mini excels across multiple evaluation dimensions. The discrepancy between automated metrics"}, {"title": "5.2 Practical Implications", "content": "The findings have significant practical implications for organizations leveraging LLMs in supply chain risk analysis: Efficiency Gains: Models like Few-Shot GPT-40 mini, Zero-Shot GPT- 40, and Zero-Shot Mistral Large 2 accelerate risk monitoring through rapid analysis and summarization, enabling timely decision-making. Cost Efficiency: Budget-friendly models such as Gemma2 9B (Few-Shot), Llama 3 70B (Zero-Shot), and Mistral 822B (Few-Shot) offer strong per- formance for resource-constrained organizations, making advanced AI tools more accessible. Real-Time Capability: LLaMA models' high processing speeds enable real-time analysis, critical for proactive risk management and dynamic sup- ply chain environments."}, {"title": "5.3 Model-Specific Recommendations", "content": "Based on the evaluation results, the following model-specific recommendations are proposed: GPT-40 mini (Few-Shot): The top performer in both human and quanti- tative evaluations, offering good cost-efficiency. Regression analysis confirms that it consistently maintains low latency and cost while delivering strong performance. This makes it an ideal choice for organizations seeking an op- timal balance between accuracy, speed, and resource utilization. GPT-40 (Zero-Shot): Exceptional quantitative performance, but at a higher cost. Its strong accuracy makes it suitable for high-stakes applica- tions where precision is critical. Mistral Large 2 (Zero-Shot): Correlation analysis indicates that Mistral Large 2 (Zero-Shot) achieves an effective balance between cost and perfor- mance. This model is recommended for organizations seeking a reliable, yet cost-effective solution. LLaMA 3.1 70B (Few-Shot): Optimal performance-efficiency balance, ideal for time-sensitive applications requiring rapid analysis. Gemma2 9B (Few-Shot): Strong human evaluation results and cost- efficiency, suitable for budget-limited projects or smaller-scale implemen- tations."}, {"title": "6 Summary and Conclusion", "content": "This study comprehensively investigated the potential of modern large language models (LLMs) for automated news summarization in supply chain risk analysis. The central research questions aimed to evaluate the models' ability to accurately summarize relevant content and analyze their performance in terms of readabil- ity, duplicate detection, and risk identification. The results demonstrate that LLMs provide valuable support for risk analysis by enabling companies to effi- ciently identify critical news content and respond swiftly to potential disruptions in their supply chains. Quantitative and qualitative analyses revealed that the Few-Shot GPT-40 mini model delivered outstanding performance, excelling in both automated metrics and human evaluations while offering excellent cost-effectiveness. Mod- els like Zero-Shot GPT-40 and Mistral Large 2 also demonstrated impressive results across various evaluation dimensions. Few-Shot GPT-40 mini, in partic- ular, showed exceptional performance. A key insight from this study is the importance of a combined evaluation methodology. Integrating quantitative metrics, LLM-based evaluators like G- Eval, and human assessments provided a comprehensive and nuanced under- standing of model performance. This multidimensional evaluation also revealed discrepancies between automated and human assessments, underscoring the ne- cessity of holistic evaluation approaches. The results highlight that LLMs, through targeted prompting techniques such as Zero-Shot and Few-Shot, can generate precise and consistent summaries. These summaries form a robust decision-making foundation for risk managers in global supply chains. At the same time, the study emphasized challenges related to model biases, factual accuracy, and ethical considerations. Regression analysis identified total costs and output speed as the primary fac- tors influencing latency, highlighting the importance of balancing performance and efficiency. These findings are particularly relevant for corporate settings, where practical implementation requires optimizing these trade-offs. From a business perspective, this study provides companies with actionable insights for selecting and adapting LLMs based on specific needs. The results emphasize that model choice should consider factors such as accuracy, speed, and cost."}, {"title": "6.1 Limitations", "content": "While the study provides valuable insights, several limitations must be acknowl- edged: Dataset Constraints: The limited dataset size may affect the generaliz- ability of the results. Future work should incorporate larger, more diverse datasets to validate findings across different contexts. Model Bias: Potential biases in LLMs may impact objectivity, requiring deeper investigation [20]. Addressing these biases is critical for ensuring fair and accurate risk analysis. Long-Term Performance: The stability of model performance over time remains unverified. Longitudinal studies are needed to assess how models adapt to evolving risks and data patterns. Fine-Tuning Limitations: Resource constraints prevented evaluation of alternative models like LLaMA 3.1/3.2. Future research should explore fine- tuning with a broader range of models and datasets."}, {"title": "6.2 Conclusion and Outlook", "content": "In summary, LLMs like GPT-40 mini and Mistral Large 2 represent promising tools for automated news summarization in risk management. However, the se- lection of models should align with the specific requirements for accuracy, speed, and cost efficiency. This study contributes significantly to the scientific discourse on AI in supply chain management and establishes a solid foundation for future research and applications in supply chain risk analysis. Future research should explore advanced fine-tuning techniques, domain- specific customizations, and diverse datasets to enhance model robustness and reliability. Expanding experimental studies to a wider range of news sources and risk scenarios will strengthen the generalizability of the findings. Furthermore, a deeper investigation of the ethical implications of automated AI systems, along with the development of clear guidelines for large language model (LLM) ap- plications in risk management, will help foster corporate confidence in adopting these technologies. By addressing these limitations and following the research directions de- scribed, the field can advance toward more reliable, efficient, and ethical AI- driven solutions for supply chain risk analysis, ultimately enabling organizations to navigate complex global challenges with greater agility, accuracy, and re- silience."}]}