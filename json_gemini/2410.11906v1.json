{"title": "EMPOWERING USERS IN DIGITAL PRIVACY MANAGEMENT\nTHROUGH INTERACTIVE LLM-BASED AGENTS", "authors": ["Bolun Sun", "Yifan Zhou", "Haiyun Jiang"], "abstract": "This paper presents a novel application of large language models (LLMs) to enhance user\ncomprehension of privacy policies through an interactive dialogue agent. We demonstrate\nthat LLMs significantly outperform traditional models in tasks like Data Practice Identifica-\ntion, Choice Identification, Policy Summarization, and Privacy Question Answering, setting\nnew benchmarks in privacy policy analysis. Building on these findings, we introduce an\ninnovative LLM-based agent that functions as an expert system for processing website\nprivacy policies, guiding users through complex legal language without requiring them\nto pose specific questions. A user study with 100 participants showed that users assisted\nby the agent had higher comprehension levels (mean score of 2.6 out of 3 vs. 1.8 in the\ncontrol group), reduced cognitive load (task difficulty ratings of 3.2 out of 10 vs. 7.8),\nincreased confidence in managing privacy, and completed tasks in less time (5.5 minutes vs.\n15.8 minutes). This work highlights the potential of LLM-based agents to transform user\ninteraction with privacy policies, leading to more informed consent and empowering users\nin the digital services landscape.", "sections": [{"title": "INTRODUCTION", "content": "The pervasive collection and processing of personal data by online services have elevated privacy concerns\nin digital interactions (Vicario et al., 2019). To address these issues, websites and applications are legally\nmandated to publish privacy policies detailing practices related to data collection, usage, sharing, and\nprotection. However, these policies are notoriously difficult for the average user to comprehend due to their\nlegal complexity (McDonald & Cranor, 2008), dense language, and considerable length. Consequently, they\nare frequently ignored or misunderstood, undermining the principle of informed consent and exposing users to\npotential privacy risks. This disconnect poses significant challenges (Harkous et al., 2018): it impedes users'\nability to make informed decisions about their data and complicates efforts by regulators and organizations to\nensure transparency, enforce compliance, and build trust. As privacy regulations become more stringent and\npolicies more intricate, innovative approaches are urgently needed to bridge this comprehension gap.\nRecent advances in natural language processing (NLP), particularly through the development of large\nlanguage models (LLMs), offer promising solutions to these challenges. LLMs, such as those based on\nGPT architectures, have demonstrated remarkable capabilities in understanding and generating human-like\ntext across various domains, including legal documents. These models have been successfully applied to\ntasks such as information extraction, content summarization, and question answering, showing potential in\nautomating the analysis and interpretation of complex legal texts like privacy policies.\nIn this paper, we present a novel application of LLMs to improve user interaction with website privacy policies\nby developing an AI agent based on GPT-40-mini. Our research aims to empower users by improving their\nability to navigate and comprehend privacy agreements, thus fostering better control over their personal data.\nThe study is structured in two parts.\nIn the first part, we evaluate the performance of LLMs across several core tasks relevant to privacy policies,\nincluding Data Practice Identification, Choice Identification, Policy Summarization, and Privacy Question\nAnswering (Wilson et al., 2016; Mysore Sathyendra et al., 2017; Ahmad et al., 2020; Keymanesh et al., 2020)."}, {"title": "RELATED WORK", "content": "As privacy becomes a key concern in modern society, regulations like GDPR (General Data Protection\nRegulation) have shaped global privacy policies by enforcing stricter transparency requirements (Wang et al.,\n2018). Despite these advancements, privacy policies are still often long and complex, making them difficult\nfor users to fully understand. McDonald and Cranor (McDonald & Cranor, 2008) estimated that reading all\nprivacy policies a user encounters would take 201 hours per year, highlighting the need for more user-friendly\nformats.\nHistory of Privacy Policies Development The development of privacy rights began with Warren and\nBrandeis's concept of \"the right to be let alone\" (Warren & Brandeis, 1890), which laid the foundation\nfor privacy as a legal right. The Universal Declaration of Human Rights (UN General Assembly, 1948)\nrecognized privacy as a fundamental human right, while Westin (Westin, 1968) further expanded the concept,\nemphasizing individuals' control over personal information. As the digital age progressed, Introna (Introna,\n1997) highlighted the importance of privacy in networked societies, and the Federal Trade Commission\n(Federal Trade Commission, 1998) introduced the \"Notice and Choice\" framework. Cate (Cate, 2010)\ncriticized its effectiveness, arguing that it fails to adequately protect user privacy. More recent work has\nfocused on the gap between the language of privacy policies and users' understanding (Reidenberg et al.,\n2015), and how big data and social inequalities affect privacy (Jain et al., 2016; Madden et al., 2017).\nPrivacy Policies with Natural Language Models Natural Language Processing (NLP) has been instrumen-\ntal in automating the analysis of privacy policies, addressing the increasing complexity of these documents.\nEarly efforts by Costante et al. (Costante et al., 2012) and Ammar et al. (Ammar et al., 2012) focused on\nidentifying data practices, while Liu et al. (Liu et al., 2014) and Ramanath et al. (Ramanath et al., 2014)\nworked on aligning privacy statements. The introduction of the OPP-115 dataset by Wilson et al. (Wilson et al.,\n2016) and the development of tools like Polisis (Harkous et al., 2018) advanced privacy policy classification\nand summarization. Machine learning techniques, such as CNNs for text classification and risk assessment\ntools (Zaeem et al., 2018), have improved the accessibility of privacy policies. Question-answering systems\nhave also been applied to enhance user interaction with privacy policies, as shown by Ravichander et al.\n(Ravichander et al., 2019) and the creation of the PolicyQA dataset by Ahmad et al. (Ahmad et al., 2020).\nLLM Agent LLM Agents, powered by large language models like GPT and BERT, have shown significant\npromise in various fields, including social sciences and engineering (Bubeck et al., 2023). These agents excel\nin natural language understanding and interaction, making them ideal for automating tasks like policy summa-\nrization and compliance analysis. The application of LLM Agents to privacy policy analysis can enhance user\naccessibility, allowing for more efficient and understandable interpretations of complex documents."}, {"title": "BENCHMARKING LARGE LANGUAGE MODEL ABILITY IN USABLE PRIVACY\nPOLICY TASKS", "content": "In this section, we present a comprehensive evaluation of OpenAI's state-of-the-art closed-source large\nlanguage models (LLMs) on key tasks within the domain of usable privacy policies. The models assessed\ninclude GPT-4o to GPT-3.5. To accurately reflect the models' inherent capabilities and generalization\npotential, we employed both zero-shot and few-shot prompt engineering approaches, testing the models\ndirectly via their respective APIs. This methodology aligns with our objective of utilizing LLMs to develop\nagents capable of addressing a wide range of issues in privacy policy comprehension."}, {"title": "DATA PRACTICE IDENTIFICATION", "content": "In our experiments, we evaluated several models, including GPT-40, GPT-40-mini, GPT-4-turbo, and GPT-3.5,\non the task of Data Practice Identification. The testing procedure involved direct API calls, employing the\nsame prompt across all models to ensure consistency. These experiments were conducted on the entire\nOPP-115 dataset (Wilson et al., 2016), a comprehensive collection of annotated privacy policies.\nGiven that the task required the models to perform classification, we set the temperature parameter to zero to\nensure deterministic outputs and eliminate randomness in predictions. This decision was made to maintain\nconsistency and reliability in the classification process.\nAfter considering factors such as response times, performance metrics, and computational cost, we selected\nGPT-40-mini for further evaluation. Table 1, Table 2 present the performance of GPT-40-mini on the Data\nPractice Identification task compared to the baseline(Wilson et al., 2016). Notably, GPT-40-mini, under\nzero-shot learning conditions without additional context, outperformed the baseline model on average.\nIt is important to note that (Wilson et al., 2016) divided the Other category into three smaller subcategories,\nwhich are displayed in the final chart. Consequently, we do not have baseline data for the classification of the\nwhole Other category. The results demonstrate the capability of GPT-40-mini in policy classification tasks\nand its potential for application in constructing real-world agents."}, {"title": "CHOICE IDENTIFICATION", "content": "We evaluated the performance of GPT-40, GPT-40-mini, GPT-4-turbo, and GPT-3.5 under the zero-shot learn-\ning setting without additional context on the Choice Identification task. The models exhibited performance\nslightly below the baseline but remained noteworthy. Specifically, we observed that precision was lower,\nwhile recall was relatively high. This suggests that the models were able to identify the majority of texts\ncontaining opt-out options, with minimal instances of missed true positives.\nThis trend indicates that the models rarely failed to detect texts that genuinely contained opt-out provisions.\nTo further enhance performance, we employed a few-shot learning approach, deliberately increasing the\nnumber of examples with true opt-out options in the prompt. After this adjustment, the models achieved\nperformance on par with, or slightly better than, the baseline.\nThe comparative results are illustrated in Table 3."}, {"title": "POLICY QUESTION ANSWER", "content": "In our experiments on the Privacy Question Answering task, we tested the performance of GPT-3.5 and GPT-\n40-mini on the PolicyQA test dataset, designed to assess their ability to answer questions within the context of\nprivacy policies. Additionally, we compared the performance of these models against the BERT-base model\nto further evaluate their effectiveness.\nOur results indicate that the GPT-40-mini model, utilizing a top-10 selection strategy, outperformed the\nBERT-base model in answering questions within the privacy policy context. The zero-shot performance\nof GPT-40-mini demonstrates its ability to generalize across unseen data, effectively extracting relevant\ninformation and generating accurate answers from the privacy policies. However, we also observed that\nautoregressive large language models like GPT-40-mini tend to hallucinate responses, even when strict output\ncontrols are in place. Despite our efforts to minimize such occurrences, hallucination remains a persistent\nchallenge.\nTo address cases where the model failed to generate a meaningful response, we applied a post-processing\nstep. Specifically, we filtered out samples where the model provided no answer at all, ensuring that the final\nevaluation included only valid and relevant outputs.\nassessment by removing uninformative outputs.\nThe comparative results are illustrated in Table 4."}, {"title": "POLICY SUMMARIZATION", "content": "In this experiment, following the methodology of Keymanesh et al. (2020), we used their dataset to evaluate\nGPT-40 and GPT-40-mini on ten publicly available user agreements from platforms like Google, Amazon,\nand CNN. These agreements were processed to extract the 'riskiest' sentences, focusing on privacy and data\nhandling at content ratios of 1/16 and 1/64. While GPT-40 slightly underperformed compared to domain-"}, {"title": "LLM AGENT FOR USABLE PRIVACY POLICY", "content": "We designed the AI agent as a guided and heuristic system to assist users in comprehending complex privacy\npolicies without requiring expertise in privacy law. It proactively identifies critical points that warrant user\nattention and employs guided, heuristic dialogues to effectively communicate this information(Ouyang et al.,\n2022; OpenAI, 2023; Wei et al., 2022). Subsequently, it facilitates an open-ended question-answering session\nto help users address any remaining uncertainties.\nAs illustrated in Figure 2, the agent operates through a multi-stage process, beginning with the retrieval of the\nprivacy policy from a specified URL, followed by document preprocessing, segmentation, summarization,\ndata practice identification, opt-out choice extraction, and question answering. The system autonomously\nrecognizes essential sections such as data-sharing practices, user rights, and opt-out mechanisms that\nare often obscured within extensive legal texts. Additionally, it features an interactive interface that enables\nusers to engage with the policy by posing specific inquiries regarding their privacy rights and options. By\nsimplifying the process of navigating and understanding privacy policies, the AI agent enables users to make\ninformed decisions about their privacy with ease and confidence. It provides clarity and support, allowing\nusers to focus on what matters most without the need for extensive legal knowledge."}, {"title": "SYSTEM ARCHITECTURE", "content": "The AI agent is built upon the LangChain framework (F\u00f8lstad & Skjuve, 2019), integrating large language\nmodels with specialized tools for privacy policy analysis. The architecture comprises the following key\ncomponents:\nDocument Retrieval and Preprocessing: The agent accepts a user-provided URL to retrieve the correspond-\ning privacy policy. Utilizing LangChain's request_url, it extracts relevant content, filtering out non-essential\nelements like advertisements to focus on the policy text.\nDocument Segmentation: We employ ASDUS (Athreya, 2018) to segment the HTML document into titles\nand prose, organizing content into <h2> and <p> tags. This process aids in identifying critical sections such as"}, {"title": "QUESTIONNAIRE DESIGN", "content": "We designed a questionnaire to evaluate participants across five dimensions\u2014comprehension, user experience,\ntime efficiency, cognitive load, and trust intention-based on established evaluation methods (Brooke, 1996;\nHart & Staveland, 1988; McKnight & Chervany, 2002). This allowed us to assess differences between\nparticipants who read privacy policies directly and those assisted by our AI agent. The complete questionnaire\ncontent is attached in the appendix.\nComprehension Assessment: Comprehension was assessed using three multiple-choice questions on key\naspects of privacy policies: data collection types, data sharing practices, and user rights (Reidenberg et al.,\n2016). Participants scored 1 point for each correct answer, with a total possible score ranging from 0 to 3.\nUser Experience Evaluation: We evaluated user experience using a 5-point Likert scale to measure ease of\nuse, satisfaction, and information quality (Brooke, 1996). Participants interacting with the AI agent answered\nadditional AI-specific questions.\nTime Efficiency: We recorded the time each participant took to complete the task-either reading the privacy\npolicy or using the AI agent-to compare time efficiency between the two groups.\nCognitive Load Evaluation: Cognitive load was assessed using an adapted NASA-TLX questionnaire (Hart\n& Staveland, 1988). Participants rated mental demand, task difficulty, and frustration on a scale from 0 (\"Very\nLow\") to 10 (\"Very High\").\nTrust and Intention Assessment: Participants rated their trust in the information received and their intention\nto use similar tools in the future using a 5-point Likert scale (McKnight & Chervany, 2002).\nOpen-ended Feedback: We collected open-ended feedback on any difficulties encountered and suggestions\nfor improving the reading experience or the AI agent, providing qualitative insights for future enhancements."}, {"title": "RESULT ANALYSIS", "content": "This section presents the analysis of data collected from participants who directly read the privacy policies\n(Control Group) and those who used the AI agent (Experimental Group). We evaluated the results across five\ndimensions: comprehension, user experience, time efficiency, cognitive load, and trust/intention."}, {"title": "PARTICIPANTS AND DATA ANALYSIS", "content": "A total of 100 participants were recruited for the study, comprising 52 males and 48 females, ranging in age\nfrom 18 to 65 years old (mean age = 35.2, SD = 10.4). Participants were randomly assigned to the Control\nGroup or the Experimental Group, with 50 participants in each. The sample included individuals with diverse\neducational backgrounds: 20% had a high school diploma, 50% held a bachelor's degree, and 30% had a\ngraduate degree (master's or doctoral). Regarding digital literacy, participants self-reported their proficiency\nlevels, with 45% identifying as beginners, 45% as intermediate users, and 10% as advanced users. This\ndiversity in demographics ensures that the findings are generalizable across different user profiles.\nPrior to analysis, we verified that the data met the assumptions for parametric tests, including independence\nof observations, normality, and homogeneity of variances using Shapiro-Wilk and Levene's tests. All\nassumptions were satisfied, allowing for the use of independent samples t-tests.\nThe detailed analysis of each measured dimension is summarized below, in conjunction with the results\npresented in Table 6.1."}, {"title": "DISCUSSION & LIMITATIONS", "content": "While our study demonstrates the potential of LLM-based agents in enhancing user comprehension of privacy\npolicies, it is important to acknowledge the limitations and consider areas for future work. One significant\nconcern is the possibility of biases and hallucinations inherent in LLMs (Bender et al., 2021), which could\naffect the accuracy and reliability of the information provided to users. These issues may inadvertently\nmislead users or reinforce existing biases, potentially impacting their understanding and decisions regarding\nprivacy. Addressing these concerns requires implementing robust validation mechanisms and incorporating\nhuman oversight to ensure the agent's outputs are trustworthy.\nAdditionally, our study focused on short-term interactions with the agent. To fully understand its impact on\nusers' privacy management behaviors, longitudinal studies are necessary. Evaluating the agent's effectiveness\nover extended periods will provide insights into its sustainability and long-term benefits, including whether\nusers continue to engage with the agent and how it influences their privacy decisions over time. Future\nresearch should also explore user retention rates, the agent's impact on long-term privacy awareness, and its\nintegration into daily digital practices."}, {"title": "CONCLUSION", "content": "In this work, we have pioneered the application of large language models to enhance user comprehension of\nprivacy policies through an interactive LLM-based agent. We investigated the challenges users face when\ninterpreting complex legal language and demonstrated how our agent significantly improves understanding,\nreduces cognitive load, and increases confidence in managing personal data. Simultaneously, we demonstrated\nnew benchmarks in privacy policy analysis tasks, showcasing the superior performance of LLMs over\ntraditional models. We strongly encourage further research in this area, including the development of\nmore advanced agents and evaluation methods. By fostering a collaborative and iterative approach to user\nempowerment and privacy management, we eagerly anticipate continued advancements in promoting informed\nconsent and user autonomy in the digital landscape."}, {"title": "APPENDIX", "content": null}, {"title": "PROMPT USED IN BENCH-MARKING AND AGENT", "content": null}, {"title": "PRACTICE IDENTIFICATION", "content": null}, {"title": "POLICY SUMMARIZATION", "content": null}, {"title": "OPT-OUT CHOICE IDENTIFICATION", "content": null}, {"title": "POLICY QA", "content": null}, {"title": "AGENT USAGE EXAMPLES", "content": null}, {"title": "USER INTERFACE", "content": null}, {"title": "DATA PRACTICES IDENTIFICATION", "content": null}, {"title": "THIRD-PARTY SHARING/COLLECTION", "content": null}, {"title": "OPT-OUT DETECTION", "content": null}, {"title": "POLICY SUMMARIZATION", "content": null}, {"title": "USER PRIVACY POLICY READING EXPERIENCE QUESTIONNAIRE", "content": null}, {"title": "BASIC INFORMATION", "content": null}, {"title": "EXPERIMENTAL GROUP", "content": null}, {"title": "COMPREHENSION ASSESSMENT", "content": null}, {"title": "USER EXPERIENCE ASSESSMENT", "content": null}, {"title": "TIME EFFICIENCY", "content": null}, {"title": "COGNITIVE LOAD ASSESSMENT", "content": null}, {"title": "TRUST AND WILLINGNESS", "content": null}, {"title": "OPEN-ENDED FEEDBACK", "content": null}]}