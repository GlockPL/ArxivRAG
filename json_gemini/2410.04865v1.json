{"title": "Mastering Chinese Chess Al(Xiangqi) Without Search", "authors": ["Juntong Lin", "Zhichao Shu", "Yu Chen"], "abstract": "We have developed a high-performance Chinese Chess Al that operates without reliance on search algorithms. This Al has demonstrated the capability to compete at a level commensurate with the top 0.1% of human players. By eliminating the search process typically associated with such systems, this Al achieves a Queries Per Second (QPS) rate that exceeds those of systems based on the Monte Carlo Tree Search (MCTS) algorithm by over a thousandfold and surpasses those based on the AlphaBeta pruning algorithm by more than a hundredfold. The Al training system consists of two parts: supervised learning and reinforcement learning. Supervised learning provides an initial human-like Chinese chess Al, while reinforcement learning, based on supervised learning, elevates the strength of the entire Al to a new level. Based on this training system, we carried out enough ablation experiments and discovered that 1. The same parameter amount of Transformer architecture has a higher performance than CNN on Chinese chess; 2. Possible moves of both sides as features can greatly improve the training process; 3. Selective opponent pool, compared to pure self-play training, results in a faster improvement curve and a higher strength limit. 4. Value Estimation with Cutoff(VECT) improves the original PPO algorithm training process and we will give the explanation.", "sections": [{"title": "Introduction", "content": "Deep reinforcement learning (RL) has seen notable success in developing autonomous agents capable of mastering a variety of tasks, from playing Atari games to solving complex real-world problems\u00b9\u20139,. In particular, its application to board games has led to milestones such as AlphaGo and AlphaZero by DeepMind10,11, which defeated human champions in Go, and AlphaStar which achieved grandmaster status in the real-time strategy game StarCraft II12. These accomplishments highlight reinforcement learning's potential to handle intricate strategic decision-making.\nChinese Chess, also known as Xiangqi, is a strategy board game for two players that share similarities with the international chess game. It is one of the most popular board games in China and has been played for centuries, boasting a complexity and strategic depth revered by enthusiast and scholar alike. The standard Xiangqi board is a 10x9 grid, with a distinctive feature the river that divides the two opposing camps and impacts piece movement.\nEach player begins with a set of 16 pieces: one General (king), two Advisors, two Elephants, two Chariots, two Cannons, two Horses, and five Soldiers. The mentioned pieces each have unique movement rules, such as the Elephant's inability to cross the river and the Cannon's requirement of a 'screen' piece for capture. Victory is achieved by checkmating or stalemating the opponent's General.\nChinese Chess invites a range of dynamic tactics and deeper strategies, with an emphasis on balance between attack and defense, and the meticulous timing of aggression. The game presents an astronomically large state space, estimated at around 1050 possible game positions. This sheer scale poses considerable challenges for AI, particularly for deterministic search algorithms that are prone to combinatorial explosion.\nTraditional Chinese Chess AI approaches have emphasized the use of alpha-beta pruning and other heuristic search methods-effective yet limited by the computational resources necessary to examine extensive game trees. Additionally, the algorithms require handcrafted evaluation functions that encapsulate human understanding of the game, which can be rigid and fail to adapt to novel situations.\nContrasting these search approaches, reinforcement learning (RL) agents such as those applied in recent successes with AlphaGo and AlphaZero have demonstrated the potential to learn and adapt from gameplay experience, with minimal human-crafted heuristics. However, applying these methods to Chinese Chess AI confronts two main challenges: one is the efficient encoding of game states in a way that preserves the rich context and enables the neural network to assess board positions effectively; and the other is formulating an RL algorithm that can lean into the expansive strategic depth of the game without being explicit guidance from heuristic searches.\nAlgorithms based on Monte Carlo Tree Search (MCTS)13\u201315, such as AlphaGo or KataGo16, typically require numerous simulations to compute the next move. For instance, KataGo simulates 800 times, necessitating at least 800 forward inferences"}, {"title": "Methods", "content": "The development of our Chinese Chess AI revolves around a robust training algorithm that addresses the challenges associated with the game's complex action space and strategic depth. The algorithm is divided into two primary phases as shown in Fig.(1):\n1. supervised learning with auxiliary tasks; 2. reinforcement learning using an opponent pool with diverse strategies."}, {"title": "Supervised Learning with Auxiliary Tasks", "content": "During the initial phase, we utilize a large dataset of human expert games to instill the AI with a base level of competency in Chinese Chess. Based on meticulously designed sampling curves, we sample from the collected game data to obtain game states that serve as inputs for the neural network. For each game state, there is an associated move selection, which may either"}, {"title": "Sample Curve", "content": "In the game of Chinese Chess, the distribution of samples is highly unbalanced. Specifically, the entire gameplay process can be divided into the following three stages: the opening, the middle game, and the endgame. The opening refers to the stage from the start of the game to when both sides have essentially formed their initial battle formations. The main objective during this phase is to arrange the pieces logically and develop them rapidly, particularly key offensive and defensive pieces such as chariots, horses, and cannons. During this stage, it is crucial to coordinate and protect the pieces, avoid early mistakes that could lead to the capture of important pieces, establish a solid defense, and look for vulnerabilities in the opponent's defense. The middle game is defined as the stage where both parties have essentially completed their opening setups and begin various tactical maneuvers involving attacks, defenses, and exchanges of pieces. This stage is usually characterized by intense combat and frequent changes in the situation. The endgame refers to the stage where there are fewer pieces on the board, and both sides are contending for the final victory. This phase typically requires high computational power and precise judgment. Therefore, as the game progresses, the demand for computational accuracy increases. Consequently, we have designed the following sampling function:\n$p \\sim log(a*t+b)+c$\nwhere t is the game step."}, {"title": "Auxiliary Task", "content": "We design the auxiliary task to improve the training process. The auxiliary task aims to forecast the win rate after a given move. By predicting the move's outcome, the AI can consider not only immediate tactics but also long-term strategy implications. This is particularly useful given that different pieces have unique movement capabilities and exert varying levels of influence over the game. Hence, the AI learns not only to evaluate specific moves, but also to understand the underlying principles that dictate piece value and positional strength. Besides, a well-learned win rate predictor can be used as the initial value function for the reinforcement learning."}, {"title": "Feature With Rule", "content": "Chinese Chess, also known as Xiangqi, is a complex board game that often necessitates extensive computational simulations to ensure that each move made is optimal. This requirement aligns closely with the functionalities of the Monte Carlo Tree Search (MCTS) algorithm. In our research, we have embedded the rules of Chinese Chess into the feature space to enhance the Al's comprehension of the game.\nFor the input to our neural network, we include not only the current board state but also two additional categories of channels: 1. Based on the current position, identifying which pieces can be moved and to which locations they can be moved. 2. Based on the current position from the opponent's perspective, identifying potential moves and their destinations.\nOur subsequent analyses clearly demonstrate that these features significantly expedite the training process of networks based on the ResNet architecture, and they contribute to a notable enhancement in overall performance."}, {"title": "Reinforcement Learning", "content": "Following the supervised learning phase, we endeavor to further improve the Al's performance and adaptability by introducing reinforcement learning (RL), which is usually described by Markov Decision Process(MDP). An agent can take some action at at time t according to the state st, and then the environment will transfer to next state st+1 and give a reward rt to the agent, obeying the dynamics $P(s',r'|s,a)$. The goal of RL is to maximize the accumulated reward along the trajectory,\n$max E\\sum_{n}\\gamma r_{t+n}$\nwhere y is the discounted factor to guarantee the convergence of the algorithm. We use the Proximal Policy Optimization (PPO) algorithm\u00b2, taking into account its advantages in balancing exploration and exploitation. However, PPO's standard application is modified to suit the deterministic and high-stakes nature of Chinese Chess. To address the challenges of PPO in the Chinese Chess context, we implement the following adjustments:"}, {"title": "Opening Strategy Diversification", "content": "In the opening phase of Chinese Chess, there are numerous distinct styles of initiating gameplay, ranging from defensive, to offensive, to more balanced approaches. Top-tier AI systems need to adeptly handle these various styles. To prevent the AI from settling into a suboptimal state during self-play, we have designed a concept called \"Opening Strategy Diversification.\" This approach ensures that the AI explores a wide range of strategic possibilities, thereby enhancing its adaptability and robustness in real-game scenarios. Instead of sampling from the policy distribution, we manually curate a set of opening moves from top human players. This diversification in the opening book ensures that our AI is exposed to a variety of game commencements, encouraging a broader strategic understanding from the outset of the game."}, {"title": "Dynamic Opponent Pool", "content": "It is well recognized that employing self-play as a training method in reinforcement learning can easily lead to cyclic traps, where Model A defeats Model B, Model B defeats Model C, and Model C defeats Model"}, {"title": "Value Estimation with Cutoff", "content": "For PPO training, people usually use GAE(General Adavantage Estimation) to estimate the advantage function17:\n$A^{GAE}_{t} = \\sum_{n=0}^{8+}\\ (\\gamma\\lambda)^{n} \\delta_{t+n}$\nwhere $\\delta_t = r_t + \\gamma V(S_{t+1}) - V(S_t)$.\nUpon analysis, we identified challenges unique to the use of the PPO algorithm in chess-like adversarial games, particularly concerning the estimation of the value function. Firstly, PPO is an on-policy algorithm that also utilizes entropy loss to balance exploration and exploitation. Consequently, it inevitably samples moves that are exceptionally good or poor during gameplay. This variability can cause the win rate to fluctuate dramatically from 0% to 100%, or vice versa, leading to significant volatility in the value function and increasing the neural network's fitting difficulty. Moreover, this phenomenon becomes more pronounced as the game progresses and the number of sampling steps increases. From this observation, it becomes necessary to implement truncation in the trajectory processing, which is called Value Estimation with Cutoff(VECT).\nThe core algorithm of VECT is:\n$A^{VECT} = \\sum_{n=0}^{L} \\ (\\gamma\\lambda)^{n} \\delta_{t+n}$\nThe above adjustments to the training algorithm are designed to cultivate an AI that not only grasps the objective nuances of Chinese Chess but also achieves a high level of gameplay creativity and adaptability. With these innovative algorithmic enhancements, we aim to elevate the proficiency of our AI to compete at a level indistinguishable from top human players, all in the absence of traditional, computationally demanding search algorithms like MCTS."}, {"title": "Training Data", "content": "In the course of our research, we have amassed a substantial dataset from the LightSpeed and Quantum Studio's \"TianTian Xiangqi\" platform. It is pertinent to note that during the data collection phase, careful measures were taken to filter out private data such as players' personal information and game details. Only the chess game records were retained, stored in the Portable Game Notation (PGN) format, which adheres to international standards.\nThe data collection was conducted in two distinct methods. The first method involved gathering data from the top 10 percent of players, encapsulating approximately five million records. For each game scenario, the moves selected by these elite players were specifically documented. The second method expanded the scope to include a broader spectrum of human expertise, ranging from beginners to top-level players, covering ten million game scenarios. In this approach, an agent created using the Alpha-Beta pruning algorithm annotated the moves to enhance the generalizability of the model training.\nFurthermore, to ensure the integrity and relevance of the data, the following rules were applied for data cleansing: 1. Elimination of game records where the number of rounds was less than ten. 2. Removal of games terminated due to players disconnecting, which were primarily attributed to network issues."}, {"title": "Model Architecture", "content": "Conceptually, our model maps observation of to action at with a neural network backbone fo.\nThe observation of is a tensor (of size H * W * (2*C)), including board information (of size H * W *C) and (valid move information of size H * W *C). H = 10 and W = 9 are height and width of the board, C = 7 denotes number of chess type.For board information, $board_{ij}$ is the one-hot representation of the chess on ith row and jth column. For valid move information, $validmove_{ijk}$ is a flag indicating whether ith row and jth column can accessed by chess of type k in one step."}, {"title": "Main Result", "content": "In this section, we will provide a comprehensive overview of the results obtained from our algorithm. In the main results section, we intend to present the outcomes of our model when pitted against a baseline model, as well as the performance achieved in ranked matches against human opponents. Initially, the evaluation process employed a baseline AI that utilizes the AlphaBeta pruning algorithm. This rule-based AI has demonstrated a level of proficiency approximately within the top 10% percentile in human ranked competitions. We present five representative milestone results that include their performance against baseline, rankings in human ladder tournaments, and data on predictive accuracy at various stages of the training process. Initially, the nomenclature of the five models adheres to a format combining the training algorithm with the model structure. Here, 'SL' denotes Supervised Learning, and 'RL' indicates Reinforcement Learning. The term modResNet refers to our modified version of the ResNet model, adapted specifically for the Chinese Chess scenario, while 'ViT' (Vision Transformer) repurposes the transformer structure commonly utilized in classical visual contexts."}, {"title": "Ablation Study", "content": "The distribution of samples in Chinese Chess is notably imbalanced, and there is a significant disparity in the styles of moves across different phases of the game. Consequently, it is crucial to perform necessary sampling procedures on the training samples. To address this, we designed a specific sampling curve:\n$p \\sim log(\\frac{t}{2T}+e^{-1})+1.5$\nwhere T is the total length of one game varying with each other, t is the current step of the game. This curve illustrates that as the game progresses, the selection of each move requires increasing precision. We observed that the overall prediction accuracy, particularly in the mid game and last game phases, was enhanced."}, {"title": "Feature With Rule", "content": "In this discussion, we explore effective feature modeling for Chinese Chess. The importance of feature modeling may be mitigated if search technologies are employed, as the agent can simulate downwards continuously, ultimately achieving a comprehensive understanding of the current situation. However, in the absence of search and simulation, it necessitates a deeper knowledge of the entire chessboard and the rules of the game on the part of the agent. To this end, we considered the following three methods of feature modeling:\n\u2022 Features composed solely of the information of the pieces on the board.\n\u2022 Features that include the state of the board and the various move options available to the player.\n\u2022 Features that encompass both the board information and all possible moves for both players based on the current situation.\nWe observe the following interesting phenomena:\n\u2022 The inclusion of permissible moves for the player into the features significantly accelerates the training speed in the early stages.\n\u2022 In the mid to late stages of training, given the same amount of training time, the inclusion of the opponent's permissible moves results in a higher win rate compared to including only the player's own moves."}, {"title": "DOP and VECT", "content": "In this section, we will compare the impact of two predominant methods on the effectiveness of the reinforcement learning training process: DOP and VECT. For adversarial games in general, iterative self-improvement can be achieved through"}, {"title": "Discussion", "content": "The development and validation of our Chinese Chess Al without search have led to several compelling findings with implications for both reinforcement learning (RL) methods and real-world applications. Here, we discuss the key insights from this research and their potential broader impact.\nOur results contribute to the growing literature that illuminates the versatility and effectiveness of Transformer architectures. Notably, the performance improvement observed when transitioning from a CNN-based model to a Transformer-based model underscores the superior spatial relation understanding that Transformers offer.\nThe enhancement of the AI's feature set with rules-based information significantly accelerated and improved the learning process.\nOur method of diversifying opponent strategies in the training phase highlights the limitations of traditional self-play methods and points to the need for more sophisticated training regimens.\nThe introduction of the VECT method in the RL phase provides a robust and stable method for evaluating value functions in turn-based game scenarios, particularly in situations where the overall rewards vary significantly. This is extremely beneficial for learning and developing chess AI, as it helps the AI system to more accurately predict the long-term benefits of its actions, thereby enabling it to make better decisions."}, {"title": "Conclusion", "content": "Our findings have notable implications for the future of game AI development. By demonstrating the potential to reach high levels of performance without reliance on search algorithms, we pave the way for more lightweight and adaptable Als in resource-constrained environments. This approach may be especially relevant for developing Als for real-world applications where computational resources are limited or where low-latency decisions are crucial.\nThough our results are promising, several limitations need to be addressed in future work. Most notably, our methods were applied in the context of a deterministic and perfect information game-Chinese Chess. As such, the applicability of our techniques to games or scenarios with elements of randomness or imperfect information remains to be demonstrated.\nIn conclusion, this research contributes valuable insights into the reinforcement learning landscape and champions methods that could have implications for decision-making systems in general. As we look to future applications and extensions of our work, the blend of RL strategies, model architectures, and feature representations established herein will undoubtedly inform and inspire ongoing developments in AI."}]}