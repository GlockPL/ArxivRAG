{"title": "Focused Large Language Models are Stable Many-Shot Learners", "authors": ["Peiwen Yuan", "Shaoxiong Feng", "Yiwei Li", "Xinglin Wang", "Yueqi Zhang", "Chuyi Tan", "Boyuan Pan", "Heda Wang", "Yao Hu", "Kan Li"], "abstract": "In-Context Learning (ICL) enables large language models (LLMs) to achieve rapid task adaptation by learning from demonstrations. With the increase in available context length of LLMs, recent experiments have shown that the performance of ICL does not necessarily scale well in many-shot (demonstration) settings. We theoretically and experimentally confirm that the reason lies in more demonstrations dispersing the model attention from the query, hindering its understanding of key content. Inspired by how humans learn from examples, we propose a training-free method FOCUSICL, which conducts triviality filtering to avoid attention being diverted by unimportant contents at token-level and operates hierarchical attention to further ensure sufficient attention towards current query at demonstration-level. We also design an efficient hyperparameter searching strategy for FOCUSICL based on model perplexity of demonstrations. Comprehensive experiments validate that FOCUSICL achieves an average performance improvement of 5.2% over vanilla ICL and scales well with many-shot demonstrations.", "sections": [{"title": "1 Introduction", "content": "The rapid development of large language models (LLMs) has facilitated the emergence and enhancement of their In-Context Learning (ICL) abilities (Wei et al., 2022a; Dong et al., 2023). As a training-free method, ICL can achieve fast model adaptation on specific tasks based on several demonstrations prefixed to the query, formally denoted as $ICL(response|demos, query)$. Intuitively, more demonstrations can help LLMs better understand the task and increase the likelihood of finding demonstrations that aid in responding queries, thus leading to better performance. Theoretically, a similar conclusion can be drawn. Previous studies (Dai et al., 2023; Irie et al., 2022; von Oswald"}, {"title": "2 Background", "content": "Formalization of ICL We follow (Dong et al., 2023) to define the general ICL paradigm. Given an LLM M and a query q, we choose N demonstrations from a candidate set $S_{demos} = \\{(q_i, r_i)\\}_{i=1}^N$ to attain the response r from M as follows:\n$r = Sampling(M(Cat [q_0; r_0; ...; q_n; r_n; q]))$                                                        (1)\n$demos$\nwhere $Sampling(\u00b7)$ denotes certain sampling strategy and $Cat[\u00b7]$ denotes the operation of concatenation.\nScaling Demonstration Number Due to restrictions on context window (2048 ~ 4096), early studies (Brown et al., 2020; Lu et al., 2022) on ICL are limited to few-shot scenarios where they generally observe gains from more demonstrations. As the context window expands recently, counterexamples occur. Agarwal et al. (2024) finds that the best performance of Gemini 1.5 Pro is achieved under settings where demonstration number is not the maximum one tested in over half of the benchmarks. Zhao et al. (2023) discoveries that increasing the number of demonstrations does not necessarily improve model performance across five LLMs. We observe similar phenomena in Figure 5."}, {"title": "3 Revisiting", "content": "In this section, we explore what impedes LLMs from becoming stable many-shot learners."}, {"title": "3.1 Approximating ICL as Finetuning", "content": "Since Dai et al. (2023) derives that ICL is formally equivalent to finetuning, with demonstrations analogous to training samples, we decide to revisit their derivation process below to explore why finetuning satisfies scaling laws (Hernandez et al., 2021) while ICL does not.\nLet $W_0, \\Delta W_{FT} \\in \\mathbb{R}^{d_{out} \\times d_{in}}$ be\nthe initialized parameter matrix and the update matrix, and $x \\in \\mathbb{R}^{d_{in}}$ be the input representation. The output of certain linear layer optimized by gradient descent can be formulated as follows:\n$\\\\hat{x} = xW_0+ x\\Delta W_{FT}$\n(2)"}, {"title": "3.2 Ignorance of Attention Competition", "content": "From Eq. (3) we can further derive as follows:\n$\\\\hat{h_r}$\n$= LinAtt (h_rW_q, Q_k, Q_v) + LinAtt (h_rW_q, D_k, D_v)$\n(4)\nwhich means that the existence of demonstrations does not affect the outcome from q. However, when we no longer approximate standard attention as linear attention, we arrive at the opposite conclusion:\n$\\\\hat{h_r}$\n$= Att (h_rW_q, Cat [D_k; Q_k], Cat [D_v; Q_v])$\n$= softmax(h_rW_q Cat [D_k; Q_k]) Cat [D_v; Q_v]$\n$=(1 - \\lambda(h_r)) softmax(h_rW_q Q_k)Q_v$\n$+ \\lambda(h_r) softmax(h_rW_q D_k)D_v$\n$=(1 - \\lambda(h_r)) Att (h_rW_q, Q_k, Q_v)$\n$+ \\lambda(h_r) Att (h_rW_q, D_k, D_v),$\n(5)\nwhere:\n$\\lambda(h_r) = \\frac{\\sum_i exp (h_rW_qD_i)}{\\sum_i exp (h_rW_qD_i) + \\sum_j exp (h_rW_qQ_j)}$   (6)\nWith the existence of $\\lambda(h_r)$ in Eq. (5), an increase in the number of demonstrations will lead to a larger $\\lambda(h_r)$, thereby decreasing the model attention towards q. At the same time, ICL does not necessarily adhere to the scaling law as it is no longer formally equivalent to finetuning. Therefore, we hypothesize that more demonstrations can divert model attention from the key contents (query), leading to possible performance decrease."}, {"title": "3.3 Experimental Evidence for Hypothesis", "content": "To validate our hypothesis, we first investigate whether the model attention towards the query decreases with the increase of demonstration number. To avoid potentially unreliable results caused by data contamination (Jiang et al., 2024), our exploratory experiments are conducted with longchat-7b-v1.5 (Li et al., 2023a) (32k context window) on the proposed COUNTA benchmark (See details in \u00a75.1), which requires the model to Count the number of character 'A' in the five candidates. As shown in Figure 1, the average attention weight of model towards each token in the query decreases by scaling up the demonstration number, corresponding to Eq. (5).\nWe further explore how the model's lack of attention towards the query affects the quality of the response. Specifically, we add several blank spaces at the end of each demonstration. This format maintains the ICL paradigm and the meaningless blank spaces will not introduce additional information. As shown in Figure 2, we find that more blank spaces disperse the model attention towards the query similar to the demonstrations, which in turn leads to a decline in accuracy. Based on the experiments above, we have confirmed our hypothesis."}, {"title": "4 Methodology", "content": "To mitigate the impact of LLMs' attention being dispersed by many-shot demonstrations, we propose FOCUSICL. The core idea behind FOCUSICL is to allocate model attention to more important contents at token-level by triviality filtering (\u00a74.1) and at demonstration-level by hierarchical attention (\u00a74.2), as shown in Figure 3."}, {"title": "4.1 Triviality Filtering", "content": "Humans benefit from selectively ignoring irrelevant parts (trivialities) of demonstrations to avoid attention dispersion. In contrast, the standard attention mechanism of LLMs fails to completely ignore (assign zero attention weight to) trivialities and leverage the prior that the tokens of query are generally important, for which we propose triviality filtering operation. To predict response r for given query q, in each attention layer, we first calculate the attention scores s as follows:\n$s = h_rW_qCat[D_k; Q_k]$\n(7)\nInstead of directly applying softmax on s like standard attention operation, we filter the trivialities in the demonstrations according to a pre-set threshold p in advance as follows:\n$\\\\index = arg\\\\{index\\\\mid count(s \\leq s_{index}) = p \\times |s|\\\\}$\n$mask(s) =$\n$\\begin{cases}\n-INF, s_i < s_{index} \\text{ and } i \\in demos \\\\\n0, \\text{else}\n\\end{cases}$\n(8)\n$\\tilde{h_r} = softmax(s + mask(s)) Cat[D_v;Q_v]$\nwhere $\\tilde{h_r}$ is the outcome of $h_r$. By applying triviality filtering operation, useless parts of demonstrations are assigned zero attention weights thus LLMs can focus on leveraging relevant contents of the demonstrations to solve the current query. To achieve a broad impact, apart from r, we also apply triviality filtering operation on tokens belong to responses of demonstrations by autoregressively treating $\\{(q_i, r_i)\\}_{i=1}^N$ as demonstrations of $(q_k, r_k), k \\in [2, N]$."}, {"title": "4.2 Hierarchical Attention", "content": "When there are numerous examples, humans draw inspirations for problem-solving from different examples separately and then integrate the insights to avoid distracting attention by focusing on too many examples simultaneously. Motivated by this, we introduce hierarchical attention mechanism for LLMs to learn from many-shot demonstrations while focusing on current query. We first split the demonstrations into T batches, where each one comprises B consecutive demonstrations. Without editing the token order, we change the position indexes to ensure that each batch is logically adjacent to the query (Figure 4). To ensure that batches are mutually invisible to each other, we use a mask matrix, allowing us to parallelly apply intra-batch attention within each batch i and query as follows:\n$\\tilde{h}^i, s^i = TrivialityFiltering Att (h^i_{j \\in batch i} \\bigcup q)$\n(9)\nBy controlling the batch size B, we can ensure that the model maintains enough attention towards the query within each batch. To further integrate insights from different batches, we conduct inter-batch attention as follows:\n$\\sum\\limits_{i=1}^{T} \\sum\\limits_k \\sum h_{ir}^i$\n(10)\nThe sum of the attention scores for all tokens within each batch can reflect the amount of useful information contained in that batch for the current query. Based on this, we calculate the weighted sum of $\\tilde{h}^i$ to attain the final output of the attention layer."}, {"title": "4.3 Hyperparameter Searching", "content": "To efficiently find suitable values of filtering threshold p and batch size B for different LLMs and tasks, we propose a hyperparameter searching strategy as shown in Algorithm 1. By treating $q_i$ as current query and $S_{1:i-1}$ as demonstrations, the model perplexity 1 (ppl) of $r_i$ can reflect the LLMs' capability when demonstration number is i - 1 (lower ppl indicates better performance). Thus, we choose the p that yields the lowest average ppl and B that first leads an increasing trend in ppl as our hyperparameter choices. We generally set $S_p$ as"}, {"title": "5 Experiments", "content": "Centered around FOCUSICL, we will empirically demonstrate its performance on different LLMs and tasks in \u00a75.2, verify whether it can help LLMs scale well with demonstration number in \u00a75.3, and delve into its working mechanism in \u00a75.4. We also investigate the choice of hyperparameters in Appendix \u00a7A.1."}, {"title": "5.1 Experimental Settings", "content": "Benchmarks We conduct experiments on the following benchmarks:\n\u2022 CSQA (Talmor et al., 2019) is a high-quality benchmark for commonsense reasoning task.\n\u2022 PIQA (Bisk et al., 2020) concentrates on testing physical commonsense answering ability.\n\u2022 CountA is our proposed benchmark to avoid the impact of data contamination (Jiang et al., 2024), making experimental results more comprehensive and reliable. It requires the model to count the number of character 'A' in the five candidates.\n\u2022 ARC (Clark et al., 2018) includes questions that require extensive knowledge and reasoning to answer.\n\u2022 GSM8K (Cobbe et al., 2021) serves as a testbed for evaluating multi-step mathematical reasoning (chain-of-thought) ability.\nWe evaluate the LLMs on the test set of the datasets"}, {"title": "5.2 Main Results", "content": "Our main experimental results are presented in Tables 1, 2, and 3. The compared methods exhibit similar performance trends across different LLMs.\nBaselines Under most settings, EARLYSTOP outperforms ICL, consistent with the observations of Agarwal et al. (2024) and Zhao et al. (2023) that more demonstrations does not necessarily lead to better performance. Compared to EARLYSTOP which avoids the negative impact of attention dispersion by not introducing more demonstrations, STRUCTICL leverages all the given demonstrations through structured input to achieve slightly better performance.\nOurs However, due to the lack of insights into the reasons behind performance degradation of ICL with more demonstrations, the baselines fail to maintain the model attention on critical input parts while fully leveraging all demonstrations. In contrast, by introducing triviality filtering operation and hierarchical attention mechanism to achieve the above vision, FOCUSICL outperforms the compared baselines, achieving an average of 5.2% (3.31 points) performance improvement over ICL across three LLMs. The results of the T-test also indicate that FocusICL is significantly superior to baselines, with a p-value less than 0.05. This validates the effectiveness and generalizability of FOCUSICL.\nAblations We also report the performance of only performing triviality filtering operation as an ablation study. The results show that FOCUSICL benefits 1.29 points improvement from the triviality filtering operation and 2.02 points improvement from the hierarchical attention mechanism.\nEfficiency By performing hierarchical attention mechanism, demonstrations between different batches does not need direct interactions, which can save a significant amount of inference overhead. Assuming each demonstration has an average of L tokens, the overhead of attention operation between N demonstrations for ICL is:\n$Cost_{ICL} = N^2L^2 \\times \\bigtriangleup$\n(11)"}, {"title": "5.3 Scaling with More Demonstrations", "content": "The recent significant advancements in LLMs mainly stem from scaling up in dimensions of model size and training data size. However, given the limitations of computation resource and data production speed, we are in eager need of exploring other potential scaling dimensions to continuously enhance the performance of LLMs. As shown in Figure 5, the demonstration number is not a stable scaling dimension when applying ICL, as the performance can sometimes exhibit an inverse-scaling phenomenon with more demonstrations. In contrast, FOCUSICL enables LLMs to become stable many-shot learners by directing their attention to important contents, thereby achieving good scalability in the dimension of demonstration number.\nIt should be noted that we find the advantage of FOCUSICL over ICL continues to grow as the number of demonstrations increases. This means that if we have more resources to conduct experiments with more demonstrations, the advantage of FOCUSICL over ICL can be larger."}, {"title": "5.4 Working Mechanism of FOCUSICL", "content": "To gain a deeper understanding of the working mechanism of FOCUSICL, we explore it from aspects of attention and hidden state distributions, following the experimental settings in \u00a73.3.\nAttention Distribution The primary purpose of FOCUSICL is to prevent the model attention from being scattered by the increased demonstrations, thereby ensuring a proper understanding of key contents. Therefore, we observe the attention weights allocated by the model towards the query as the number of demonstrations increases. As shown in Figure 6, by ignoring unimportant parts of the demonstrations and introducing the hierarchical attention mechanism, FOCUSICL consistently maintains sufficient attention towards the query.\nHidden States Distribution We further investigate the distribution of the hidden states of the last input token at the penultimate model layer through Principal Component Analysis (PCA). Intuitively, the distribution of the hidden states of the last token mainly depends on the current problem to be solved and should be independent of the demonstration number. However, as shown in Figure 7, we find that the hidden states of ICL change systematically with an increasing number of demonstrations, whereas FOCUSICL does not exhibit such behavior. We think that the systematic decline in attention towards the query in ICL with an increasing number of demonstrations continuously affects the hidden states during response generation, thereby impacting the quality of the generated response. In contrast, FOCUSICL avoids this issue by maintaining sufficient attention to the query as shown above, ultimately benefiting from more demonstrations."}, {"title": "5.5 Further Discussion", "content": "Based on our existing insights and experimental results, we attempt to understand the divergent phenomena of ICL observed in previous studies where more demonstrations sometimes lead to better performance, while sometimes the opposite occurs. We think the main reason leading to the above phenomena comes from the double-edged sword effect of learning from more demonstrations: on the one hand, they can help the model better understand the task and increase the likelihood of finding useful knowledge; on the other hand, they might also distract the model, leading to insufficient attention and understanding of current query. We consider that two aspects can influence the balance between the two effects:\nWeak models require more demonstrations to understand the task. As shown in Figure 5, we observe that the optimal number of demonstrations for LONGCHAT-7B-V1.5-32K is greater compared to the other two models across most benchmarks. Considering that its performance is also the worst, we believe the reason for the aforementioned situation is that weaker models require more demonstrations to help them better understand the task.\nMore demonstrations are needed when they have a closer relationship. We also notice that the LLMs are more demonstration-hungry on CountA compared to other benchmarks as shown in Figure 5. We analyze that the correlation between samples in other benchmarks is relatively weak, and even a single demonstration is sufficient to clarify the task format. In contrast, the demonstrations in CountA are closely related, collectively determining what the task definition is. In this scenario, LLMs cannot discern the complete task information if only given a few demonstrations. To sum up, when the samples are closely related, the model needs more demonstrations to analyze the correlations among them, so as to better understand and complete the task."}, {"title": "6 Conclusions", "content": "Noticing that the performance of LLMs under many-shot ICL does not consistently improve with more demonstrations, we analyze and validate the underlying reason as follows: more demonstrations can disperse the model attention to critical contents, resulting in an insufficient understanding of the query. Inspired by how humans learn from examples, we propose a training-free method FOCUSICL, which conducts triviality filtering at token-level and hierarchical attention at demonstration-level to rationally allocate model attention in each layer. Comprehensive experiments indicate that focused LLMs are stable many-shot learners, making demonstration number a possible scaling dimension for LLM-based AGI."}, {"title": "Limitations", "content": "From an objective perspective, we think there are two main limitations of this paper:\n1. Although we have extended the demonstration number to nearly or even beyond 100, due to computational resource limitations, we are unable to conduct experiments with larger demonstration numbers. We will further verify the applicability of FOCUSICL with larger demonstration numbers in the future.\n2. This work primarily discusses LLMs that apply the standard transformer decoder architecture. We look forward to further exploring the scaling performance with the demonstration number and the applicability of FOCUSICL on other variants of LLMs, such as the encoder-decoder architecture and sliding window attention, in the future."}, {"title": "Ethics Statement", "content": "All of the datasets used in this study were publicly available, and no annotators were employed for our data collection. We confirm that the datasets we used did not contain any harmful content and was consistent with their intended use (research). We have cited the datasets and relevant works used in this study."}, {"title": "A Additional Experimental Results", "content": "A.1 Hyperparameters\nTo investigate the influence of hyperparameters, we report the results of LONGCHAT-7B-V1.5-32K on GSM8K benchmark with varying hyperparameter settings.\nFiltering Threshold As shown in Table 7, with the increase of filtering threshold p, the model's performance first improves and then declines. This is because, when p is relatively small, the model benefits from ignoring unimportant content and focusing its attention on more beneficial parts. However, when p becomes larger, the model might overlook potentially useful information in the demonstrations, leading to a decrease in performance.\nBatch Size As shown in Table 8, a similar inverted U-shaped curve phenomenon occurs when scaling the batch size while maintaining the overall demonstration number fixed. As the batch size decreases from 80, the model attention to the query continues to increase, which can lead to a certain improvement in model performance. However, when the batch size is too small, the model may fail to fully understand the task definition due to excessive lack of interaction between demonstrations, consistent with the findings of Bertsch et al. (2024).\nLuckily, through our proposed hyperparameter searching strategy, we can efficiently attain suitable hyperparameters for the given tasks and LLMs."}, {"title": "A.2 Further Analyses of TRIVIALITY", "content": "When we identify tokens that are unhelpful for answering the current query through attention, TRIVIALITY directly masks them to prevent the model's attention from being distracted. Another more intuitive approach is to filter out demonstrations with minimal attention. We compared these two methods, and the results are shown in the Table 4. It can be seen that TRIVIALITY, which operates at a finer granularity at the token level, achieves better results.\nAdditionally, we conducted the following experiments to further validate the motivation that tokens with low attention are unimportant and should be masked. We set the following settings below on CountA with LONGCHAT-7B-V1.5-32K:\n\u2022 No Masking.\n\u2022 Masking 40% of tokens with the lowest attention.\n\u2022 Masking 40% of tokens with the highest attention.\n\u2022 Randomly masking 40% of tokens.\nThe experimental results in Table 5 demonstrate the following: compared to No Masking, randomly masking reduces accuracy from 79.04% to 35.00%. Masking high-attention tokens leads the model to repeatedly output the word 'nobody', indicating a loss of problem-solving ability. Conversely, masking low-attention tokens significantly improves performance.\nTo further analyze the underlying reasons, we calculated the model's perplexity across different settings. We found that random masking and masking high-attention tokens significantly increase model perplexity, likely due to the loss of critical token information. In contrast, masking low-attention tokens decreases model perplexity, indicating that filtering trivial tokens based on posterior attention information helps the model perform tasks more confidently."}, {"title": "A.3 FOCUSICL with Demonstrations Retrieval", "content": "Previous research (Rubin et al., 2022; Liu et al., 2024; Ye et al., 2023) have shown that selecting demonstrations relevant to the current query can enhance the performance of ICL. We investigated whether combining FOCUSICL with demonstration retrieval could yield better results. For simplicity, we used BERT embeddings rather than other complex retrieval methods (Yuan et al., 2024c) to"}, {"title": "B Derivation Details", "content": "The derivation details of Equation (5) are as follows:\noutput\n$= Att (h_rW_q, Cat [D_k; Q_k], Cat [D_v; Q_v])$\n$= softmax(h_rW_q Cat[D_k; Q_k]) Cat[D_v; Q_v]$\n$= \\frac{exp (h_rW_qQ_k) Q_v + exp (h_rW_qD_k) D_v}{\\sum_i exp (h_rW_qD_i) + \\sum_j exp (h_rW_qQ_j)}$\n$= \\frac{\\frac{exp (h_rW_qQ_k)}{\\sum_j exp (h_rW_qQ_j)} Q_v}{\\frac{\\sum_i exp (h_rW_qD_i)}{\\sum_j exp (h_rW_qQ_j)} + 1}$ + $\\frac{\\frac{exp (h_rW_qD_k)}{\\sum_i exp (h_rW_qD_i)} D_v}{\\frac{\\sum_j exp (h_rW_qQ_j)}{\\sum_i exp (h_rW_qD_i)} + 1}$\n$= \\frac{\\frac{exp (h_rW_qQ_k)}{\\sum_j exp (h_rW_qQ_j)}}{\\frac{\\sum_i exp (h_rW_qD_i)}{\\sum_j exp (h_rW_qQ_j)} + 1}$ x $softmax(h_rW_qQ_k)Q_v$\n$+ \\frac{\\frac{exp (h_rW_qD_k)}{\\sum_i exp (h_rW_qD_i)}}{\\frac{\\sum_j exp (h_rW_qQ_j)}{\\sum_i exp (h_rW_qD_i)} + 1}$ x $softmax(h_rW_qD_k)D_v$\n$=(1 - \\lambda(h_r)) softmax(h_rW_qQ_k)Q_v$\n$+ \\lambda(h_r) softmax(h_rW_qD_k)D_v$\n$=(1 - \\lambda(h_r)) Att (h_rW_q, Q_k, Q_v)$\n$+ \\lambda(h_r) Att (h_rW_q, D_k, D_v),$\n(13)\n$\\lambda(h_r) = \\frac{\\sum_i exp (h_rW_qD_i)}{\\sum_i exp (h_rW_qD_i) + \\sum_j exp (h_rW_qQ_j)}$  (14)"}, {"title": "C Inverse-scaling Phenomena with Gemini", "content": "Due to the limitations of computational resources and the unavailability of closed-source models, our experiments are primarily conducted on 7-8B open source LLMs. However, by utilizing APIs, we additionally explore the performance changes of more powerful models as the number of demonstrations increased, further validating the generalizability of the argument that LLMs are not stable many-shot learners. We choose to experiment with GEMINI 1.5 PRO for its long available context window (1M tokens). We test GEMINI 1.5 PRO on MATH benchmark (Hendrycks et al., 2021), which contains 7 subsets with 5 difficulty levels that can thoroughly evaluating the math reasoning abilities of LLMs. We use greedy searching decoding strategy with and report the outcomes averaged over 5 runs for credible results. As shown in Figure 8, obvious inverse-scaling phenomenon appears in 5 out of 7 subsets, with Precalculus and Intermediate Algebra as exceptions. This validates the generalizability of the argument that LLMs are not stable many-shot learners. Meanwhile, we observe that across different difficulty levels, GEMINI 1.5 PRO presents similar performance changing trends. Figure 8 clearly shows such phenomenon. This indicates that the task difficulty does not affects the optimal demonstration number of certain task."}, {"title": "D Further Discussions", "content": "FocusICL can be seen as a method that achieves performance gains through increased computation (more demonstrations). Similar approaches include Self-Consistency (Wang et al., 2023, 2024; Li et al., 2024b,a) and Chain-of-Thought (Wei et al., 2022b). In our experiments, we have confirmed that the gains brought by FOCUSICL are decoupled from those of Chain-of-Thought. We will further explore the interplay between FOCUSICL and other methods in the future.\nWe tested the performance of FOCUSICL in tasks such as QA and inference in the experimental section. In the future, we will delve into exploring"}, {"title": "E Prompt Template", "content": "The following is a template ICL input format when demonstration number is 2.\n### Human: I'm getting warm because I increased the thermostat in my bedroom. What might I be doing soon? Answer Choices: (a) feeling comfortable (b) overheat (c) increase of temperature (d) pleasure (e) starting fire\n### Assistant: A\n### Human: Where might I hear and see information on current events? Answer Choices: (a) internet (b) television (c) newspaper (d) book (e) radio\n### Assistant: B\n### Human: If somebody buys something and gives it to me as a free gift, what is the cost status of the gift? Answer Choices: (a) deadly (b) imprisoned (c) paid for (d) expensive (e) in prison\n### Assistant:"}]}