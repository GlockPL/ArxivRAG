{"title": "Focused Large Language Models are Stable Many-Shot Learners", "authors": ["Peiwen Yuan", "Shaoxiong Feng", "Yiwei Li", "Xinglin Wang", "Yueqi Zhang", "Chuyi Tan", "Boyuan Pan", "Heda Wang", "Yao Hu", "Kan Li"], "abstract": "In-Context Learning (ICL) enables large language models (LLMs) to achieve rapid task adaptation by learning from demonstrations. With the increase in available context length of LLMs, recent experiments have shown that the performance of ICL does not necessarily scale well in many-shot (demonstration) settings. We theoretically and experimentally confirm that the reason lies in more demonstrations dispersing the model attention from the query, hindering its understanding of key content. Inspired by how humans learn from examples, we propose a training-free method FOCUSICL, which conducts triviality filtering to avoid attention being diverted by unimportant contents at token-level and operates hierarchical attention to further ensure sufficient attention towards current query at demonstration-level. We also design an efficient hyperparameter searching strategy for FOCUSICL based on model perplexity of demonstrations. Comprehensive experiments validate that FOCUSICL achieves an average performance improvement of 5.2% over vanilla ICL and scales well with many-shot demonstrations.", "sections": [{"title": "1 Introduction", "content": "The rapid development of large language models (LLMs) has facilitated the emergence and enhancement of their In-Context Learning (ICL) abilities (Wei et al., 2022a; Dong et al., 2023). As a training-free method, ICL can achieve fast model adaptation on specific tasks based on several demonstrations prefixed to the query, formally denoted as  ICL(response|demos, query). Intuitively, more demonstrations can help LLMs better understand the task and increase the likelihood of finding demonstrations that aid in responding queries, thus leading to better performance. Theoretically, a similar conclusion can be drawn. Previous studies (Dai et al., 2023; Irie et al., 2022; von Oswald et al., 2023; Aky\u00fcrek et al., 2023) have theoretically inferred that ICL can be viewed as an implicit finetuning process, with demonstrations analogous to training samples. On this basis, as finetuning has been validated to comply with the scaling law (Hernandez et al., 2021) where performance increases with the number of training samples, the performance of ICL should also positively correlates with the number of demonstrations, which has been experimentally verified by previous studies (Bertsch et al., 2024; Duan et al., 2023).\nHowever, with the increase in available context length of LLMs (Reid et al., 2024), some studies (Zhao et al., 2023; Agarwal et al., 2024) observe counterexamples when scaling the demonstration numbers from few-shot to many-shot. Agarwal et al. (2024) finds that the optimal number of demonstrations for six out of eleven benchmarks is not the maximum number they have tested. Our experimental results (Figure 5) also indicate that the model performance might decline with increased demonstrations when applying ICL, exhibiting an inverse-scaling phenomenon (McKenzie et al., 2023). These findings indicate that LLMs are not stable many-shot learners."}, {"title": "2 Background", "content": "Formalization of ICL We follow (Dong et al., 2023) to define the general ICL paradigm. Given an LLM M and a query q, we choose N demonstrations from a candidate set Sdemos = {(qi, ri)}^N_{i=1} to attain the response r from M as follows:\n$$r = Sampling(M(Cat [q_0; r_0; ...; q_n; r_n; q]))$$  (1)\ndemos\nwhere  Sampling(\u00b7)  denotes certain sampling strategy and  Cat[\u00b7]  denotes the operation of concatenation.\nScaling Demonstration Number Due to restrictions on context window (2048 ~ 4096), early studies (Brown et al., 2020; Lu et al., 2022) on ICL are limited to few-shot scenarios where they generally observe gains from more demonstrations. As the context window expands recently, counterexamples occur. Agarwal et al. (2024) finds that the best performance of Gemini 1.5 Pro is achieved under settings where demonstration number is not the maximum one tested in over half of the benchmarks. Zhao et al. (2023) discoveries that increasing the number of demonstrations does not nec-"}, {"title": "3 Revisiting", "content": "In this section, we explore what impedes LLMs from becoming stable many-shot learners."}, {"title": "3.1 Approximating ICL as Finetuning", "content": "Since Dai et al. (2023) derives that ICL is formally equivalent to finetuning, with demonstrations analogous to training samples, we decide to revisit their derivation process below to explore why finetuning satisfies scaling laws (Hernandez et al., 2021) while ICL does not.\nFinetuning Let $$W_0, \\Delta W_{FT} \\in \\mathbb{R}^{d_{out} \\times d_{in}}$$ be the initialized parameter matrix and the update matrix, and $$x \\in \\mathbb{R}^{d_{in}}$$ be the input representation. The output of certain linear layer optimized by gradient descent can be formulated as follows:\n$$\\hat{r} = xW_0 + x \\Delta W_{FT}$$\nICL For each attention head of M, let $$h_i \\in \\mathbb{R}^{d_{in}}$$ be the representation of the ith input token, $$W_q, W_k, W_v$$ be the projection matrices for computing the queries, keys and values. We denote $$h_{i\\in demos}W_k$$, $$h_{i\\in demos}W_v$$, $$h_{i\\in q}W_k$$, $$h_{i\\in q}W_v$$ as $$D_k, D_v, Q_k, Q_v$$, respectively. To generate r, the output of hr can be derived below:\n$$\\begin{aligned}h_r & = Att(hW_q, Cat [D_k; Q_k], Cat [D_v; Q_v]) \\ & = LinAtt (hW_q, Cat[D_k; Q_k], Cat[D_v; Q_v]) \\ & =\\dfrac{h_rW_qCat[D_k; Q_k]^T}{\\sqrt{d}}Cat[D_v; Q_v] \\ & =\\dfrac{h_rW_qQ_k^T}{\\sqrt{d}}Q_v +\\dfrac{h_rW_qD_k^T}{\\sqrt{d}}D_v \\ & = h_rW_{ZSL} + h_r \\Delta W_{ICL} \\end{aligned}$$\nDai et al. (2023) approximate the standard attention to linear attention by removing the softmax operation for ease of qualitative analysis. Since $$h_rW_qQ_k$$ is the attention result in the zero-shot learning (ZSL) setting and $$h_rW_qD_k$$ is the extra outcome from demonstrations, they are denoted as $$h_rW_{ZSL}$$ and $$h_r\\Delta W_{ICL}$$ respectively. Comparing Eq. (3) with Eq. (2), we can understand ICL as finetuning by treating the $$\\Delta W_{ICL}$$ generated from demonstrations as the $$\\Delta W_{FT}$$ generated from training samples."}, {"title": "3.2 Ignorance of Attention Competition", "content": "From Eq. (3) we can further derive as follows:\n$$\\begin{aligned}h_r & = \\dfrac{h_rW_qQ_k^T}{\\sqrt{d}}Q_v + \\dfrac{h_rW_qD_k^T}{\\sqrt{d}}D_v \\text{outcome from q} \\text{outcome from demos} \\end{aligned}$$\nwhich means that the existence of demonstrations does not affect the outcome from q. However, when we no longer approximate standard attention as linear attention, we arrive at the opposite conclusion:\n$$\\begin{aligned}h_r & = Att (h_rW_q, Cat [D_k; Q_k], Cat [D_v; Q_v]) \\ & = softmax(\\dfrac{h_rW_q Cat [D_k; Q_k]^T}{\\sqrt{d}})Cat[D_v; Q_v] \\ & =(1 - \\lambda(h_r)) softmax(\\dfrac{h_rW_qQ_k^T}{\\sqrt{d}})Q_v + \\lambda(h_r) softmax(\\dfrac{h_rW_qD_k^T}{\\sqrt{d}})D_v \\ & =(1 - \\lambda(h_r)) Att (h_r W_q, Q_k, Q_v) \\text{outcome from q} \\ & + \\lambda(h_r) Att (h_r W_q, D_k, D_v),\\text{outcome from demos} \\end{aligned}$$\nwhere:\n$$\\lambda(h_r) = \\dfrac{\\sum_j exp (h_rW_qD_j)}{\\sum_j exp (h_rW_qD_j) + \\sum_i exp (h_rW_qQ_i)}$$\nWith the existence of  $$\\lambda(h_r)$$  in Eq. (5), an increase in the number of demonstrations will lead to a larger  $$\\lambda(h_r)$$, thereby decreasing the model attention towards q. At the same time, ICL does not necessarily adhere to the scaling law as it is no longer formally equivalent to finetuning. Therefore, we hypothesize that more demonstrations can divert model attention from the key contents (query), leading to possible performance decrease."}, {"title": "4 Methodology", "content": "To mitigate the impact of LLMs' attention being dispersed by many-shot demonstrations, we propose FOCUSICL. The core idea behind FOCUSICL is to allocate model attention to more important contents at token-level by triviality filtering (\u00a74.1) and at demonstration-level by hierarchical attention (\u00a74.2), as shown in Figure 3."}, {"title": "4.1 Triviality Filtering", "content": "Humans benefit from selectively ignoring irrelevant parts (trivialities) of demonstrations to avoid attention dispersion. In contrast, the standard attention mechanism of LLMs fails to completely ignore (assign zero attention weight to) trivialities and leverage the prior that the tokens of query are generally important, for which we propose triviality filtering operation. To predict response r for given query q, in each attention layer, we first calculate the attention scores s as follows:\n$$s = h_rW_qCat[D_k; Q_k]$$ (7)\nInstead of directly applying softmax on s like standard attention operation, we filter the trivialities in the demonstrations according to a pre-set threshold p in advance as follows:\n$$\\begin{aligned} index & = arg\\{index|count(s \\leq s_{index}) = p \\times |s|\\} \\ mask(s) & = \\begin{cases} -INF, s_i < s_{index} and i \\in demos \\\\ 0, else \\end{cases} \\ \\hat{h}_r & = softmax(s + mask(s)) Cat[D_v;Q] \\end{aligned}$$\nwhere $$\\hat{h}_r$$ is the outcome of hr. By applying triviality filtering operation, useless parts of demonstrations are assigned zero attention weights thus LLMs can focus on leveraging relevant contents of the demonstrations to solve the current query. To achieve a broad impact, apart from r, we also apply triviality filtering operation on tokens belong to responses of demonstrations by autoregressively treating $$\\left\\{(q_i, r_i)\\right\\}_{i=1}^N$$ as demonstrations of $$\\left(q_k, r_k\\right)$$,  $$k \\in [2, N]$$."}, {"title": "4.2 Hierarchical Attention", "content": "When there are numerous examples, humans draw inspirations for problem-solving from different ex-"}, {"title": "4.3 Hyperparameter Searching", "content": "To efficiently find suitable values of filtering threshold p and batch size B for different LLMs and tasks, we propose a hyperparameter searching strategy as shown in Algorithm 1. By treating qi as current query and $$S_{1:i-1}$$ as demonstrations, the model perplexity  (ppl) of ri can reflect the LLMs' capability when demonstration number is i-1 (lower ppl indicates better performance). Thus, we choose the p that yields the lowest average ppl and B that first leads an increasing trend in ppl as our hyperparameter choices. We generally set  $$S_p$$ as"}, {"title": "5 Experiments", "content": "Centered around FOCUSICL, we will empirically demonstrate its performance on different LLMs and tasks in \u00a75.2, verify whether it can help LLMs scale well with demonstration number in \u00a75.3, and delve into its working mechanism in \u00a75.4. We also investigate the choice of hyperparameters in Appendix \u00a7A.1."}, {"title": "5.1 Experimental Settings", "content": "Benchmarks We conduct experiments on the following benchmarks:\n\u2022 CSQA (Talmor et al., 2019) is a high-quality benchmark for commonsense reasoning task.\n\u2022 PIQA (Bisk et al., 2020) concentrates on testing physical commonsense answering ability.\n\u2022 CountA is our proposed benchmark to avoid the impact of data contamination (Jiang et al., 2024), making experimental results more comprehensive and reliable. It requires the model to count the number of character 'A' in the five candidates.\n\u2022 ARC (Clark et al., 2018) includes questions that require extensive knowledge and reasoning to answer.\n\u2022 GSM8K (Cobbe et al., 2021) serves as a testbed for evaluating multi-step mathematical reasoning (chain-of-thought) ability.\nWe evaluate the LLMs on the test set of the datasets"}, {"title": "5.2 Main Results", "content": "Our main experimental results are presented in Tables 1, 2, and 3. The compared methods exhibit similar performance trends across different LLMs.\nBaselines Under most settings, EARLYSTOP outperforms ICL, consistent with the observations of Agarwal et al. (2024) and Zhao et al. (2023) that more demonstrations does not necessarily lead to better performance. Compared to EARLYSTOP which avoids the negative impact of attention dispersion by not introducing more demonstrations, STRUCTICL leverages all the given demonstrations through structured input to achieve slightly better performance.\nOurs However, due to the lack of insights into the reasons behind performance degradation of ICL with more demonstrations, the baselines fail to maintain the model attention on critical input parts while fully leveraging all demonstrations. In contrast, by introducing triviality filtering operation and hierarchical attention mechanism to achieve the above vision, FOCUSICL outperforms the compared baselines, achieving an average of 5.2% (3.31 points) performance improvement over ICL across three LLMs. The results of the T-test also indicate that FOCUSICL is significantly superior to baselines, with a p-value less than 0.05. This validates the effectiveness and generalizability of FOCUSICL.\nAblations We also report the performance of only performing triviality filtering operation as an ablation study. The results show that FOCUSICL benefits 1.29 points improvement from the triviality filtering operation and 2.02 points improvement from the hierarchical attention mechanism.\nEfficiency By performing hierarchical attention mechanism, demonstrations between different batches does not need direct interactions, which can save a significant amount of inference overhead. Assuming each demonstration has an average of L tokens, the overhead of attention operation between N demonstrations for ICL is:\n$$Cost_{ICL} = N^2L^2 \\times \\bigtriangleup$$  (11)"}, {"title": "5.3 Scaling with More Demonstrations", "content": "The recent significant advancements in LLMs mainly stem from scaling up in dimensions of model size and training data size. However, given the limitations of computation resource and data production speed, we are in eager need of exploring other potential scaling dimensions to continuously enhance the performance of LLMs. As shown in Figure 5, the demonstration number is not a stable scaling dimension when applying ICL, as the performance can sometimes exhibit an inverse-scaling phenomenon with more demonstrations. In contrast, FOCUSICL enables LLMs to become stable many-shot learners by directing their attention to important contents, thereby achieving good scalability in the dimension of demonstration number.\nIt should be noted that we find the advantage of FOCUSICL over ICL continues to grow as the number of demonstrations increases. This means that if we have more resources to conduct experiments with more demonstrations, the advantage of FOCUSICL over ICL can be larger."}, {"title": "5.4 Working Mechanism of FOCUSICL", "content": "To gain a deeper understanding of the working mechanism of FOCUSICL, we explore it from aspects of attention and hidden state distributions, following the experimental settings in \u00a73.3.\nAttention Distribution The primary purpose of FOCUSICL is to prevent the model attention from being scattered by the increased demonstrations, thereby ensuring a proper understanding of key contents. Therefore, we observe the attention weights allocated by the model towards the query as the number of demonstrations increases. As shown in Figure 6, by ignoring unimportant parts of the demonstrations and introducing the hierarchical attention mechanism, FOCUSICL consistently maintains sufficient attention towards the query.\nHidden States Distribution We further investigate the distribution of the hidden states of the last input token at the penultimate model layer through Principal Component Analysis (PCA). Intuitively, the distribution of the hidden states of the last token mainly depends on the current problem to be solved and should be independent of the demonstration number. However, as shown in Figure 7, we find that the hidden states of ICL change systematically with an increasing number of demonstrations, whereas FOCUSICL does not exhibit such behavior. We think that the systematic decline in attention towards the query in ICL with an increasing number of demonstrations continuously affects the hidden states during response generation, thereby impacting the quality of the generated response. In contrast, FOCUSICL avoids this issue by maintaining sufficient attention to the query as shown above, ultimately benefiting from more demonstrations."}, {"title": "5.5 Further Discussion", "content": "Based on our existing insights and experimental results, we attempt to understand the divergent phenomena of ICL observed in previous studies where more demonstrations sometimes lead to better performance, while sometimes the opposite occurs. We think the main reason leading to the above phenomena comes from the double-edged sword effect of learning from more demonstrations: on the one hand, they can help the model better understand the task and increase the likelihood of finding useful knowledge; on the other hand, they might also distract the model, leading to insufficient attention and understanding of current query. We consider that two aspects can influence the balance between the two effects:\nWeak models require more demonstrations to understand the task. As shown in Figure 5, we observe that the optimal number of demonstrations for LONGCHAT-7B-V1.5-32K is greater compared to the other two models across most benchmarks. Considering that its performance is also the worst, we believe the reason for the aforementioned situation is that weaker models require more demonstrations to help them better understand the task.\nMore demonstrations are needed when they have a closer relationship. We also notice that the LLMs are more demonstration-hungry on CountA compared to other benchmarks as shown in Figure 5. We analyze that the correlation between samples in other benchmarks is relatively"}, {"title": "6 Conclusions", "content": "Noticing that the performance of LLMs under many-shot ICL does not consistently improve with more demonstrations, we analyze and validate the"}, {"title": "Limitations", "content": "From an objective perspective, we think there are two main limitations of this paper:\n1. Although we have extended the demonstration number to nearly or even beyond 100, due to computational resource limitations, we are unable to conduct experiments with larger demonstration numbers. We will further verify the applicability of FOCUSICL with larger demonstration numbers in the future.\n2. This work primarily discusses LLMs that apply the standard transformer decoder architecture. We look forward to further exploring the scaling performance with the demonstration number and the applicability of FOCUSICL on other variants of LLMs, such as the encoder-decoder architecture and sliding window attention, in the future."}, {"title": "Ethics Statement", "content": "All of the datasets used in this study were publicly available, and no annotators were employed for our data collection. We confirm that the datasets we used did not contain any harmful content and was consistent with their intended use (research). We have cited the datasets and relevant works used in this study."}, {"title": "A Additional Experimental Results", "content": "A.1 Hyperparameters\nTo investigate the influence of hyperparameters, we report the results of LONGCHAT-7B-V1.5-32K on GSM8K benchmark with varying hyperparameter settings.\nFiltering Threshold As shown in Table 7, with the increase of filtering threshold p, the model's performance first improves and then declines. This is because, when p is relatively small, the model benefits from ignoring unimportant content and focusing its attention on more beneficial parts. However, when p becomes larger, the model might overlook potentially useful information in the demonstrations, leading to a decrease in performance.\nBatch Size As shown in Table 8, a similar inverted U-shaped curve phenomenon occurs when scaling the batch size while maintaining the overall demonstration number fixed. As the batch size decreases from 80, the model attention to the query continues to increase, which can lead to a certain improvement in model performance. However, when the batch size is too small, the model may fail to fully understand the task definition due to excessive lack of interaction between demonstrations, consistent with the findings of Bertsch et al. (2024).\nLuckily, through our proposed hyperparameter searching strategy, we can efficiently attain suitable hyperparameters for the given tasks and LLMs."}, {"title": "A.2 Further Analyses of TRIVIALITY", "content": "When we identify tokens that are unhelpful for answering the current query through attention, TRIVIALITY directly masks them to prevent the model's attention from being distracted. Another more intuitive approach is to filter out demonstrations with minimal attention. We compared these two methods, and the results are shown in the Table 4. It can be seen that TRIVIALITY, which operates at a finer granularity at the token level, achieves better results.\nAdditionally, we conducted the following experiments to further validate the motivation that tokens with low attention are unimportant and should be masked. We set the following settings below on CountA with LONGCHAT-7B-V1.5-32K:\n\u2022 No Masking.\n\u2022 Masking 40% of tokens with the lowest attention.\n\u2022 Masking 40% of tokens with the highest attention.\n\u2022 Randomly masking 40% of tokens.\nThe experimental results in Table 5 demonstrate the following: compared to No Masking, randomly masking reduces accuracy from 79.04% to 35.00%. Masking high-attention tokens leads the model to repeatedly output the word 'nobody', indicating a loss of problem-solving ability. Conversely, masking low-attention tokens significantly improves performance.\nTo further analyze the underlying reasons, we calculated the model's perplexity across different settings. We found that random masking and masking high-attention tokens significantly increase model perplexity, likely due to the loss of critical token information. In contrast, masking low-attention tokens decreases model perplexity, indicating that filtering trivial tokens based on posterior attention information helps the model perform tasks more confidently."}, {"title": "A.3 FOCUSICL with Demonstrations Retrieval", "content": "Previous research (Rubin et al., 2022; Liu et al., 2024; Ye et al., 2023) have shown that selecting demonstrations relevant to the current query can enhance the performance of ICL. We investigated whether combining FOCUSICL with demonstration retrieval could yield better results. For simplicity, we used BERT embeddings rather than other complex retrieval methods (Yuan et al., 2024c) to"}, {"title": "B Derivation Details", "content": "The derivation details of Equation (5) are as follows:\n$$\\begin{aligned}\\text{output} & = Att (h_rW_q, Cat [D_k; Q_k], Cat [D_v; Q_v]) \\ & = softmax(\\dfrac{h_rW_q Cat[D_k; Q_k]^T}{\\sqrt{d}})Cat[D_v; Q_v] \\ & =\\dfrac{exp (h_rW_qQ_k^T)}{\\sum_j exp (h_rW_qD_j^T) + \\sum_i exp (h_rW_qQ_i^T)}Q_v \\\\ & + \\dfrac{exp (h_rW_qD_k^T)}{\\sum_j exp (h_rW_qD_j^T) + \\sum_i exp (h_rW_qQ_i^T)}D_v \\\\ & =\\dfrac{exp (h_rW_qQ_k^T)}{\\sum_i exp (h_rW_qQ_i^T)}Q_v\\\\ & + \\dfrac{exp (h_rW_qD_k^T)}{\\sum_j exp (h_rW_qD_j^T)}D_v \\\\ & + \\dfrac{\\sum_j exp (h_rW_qD_j^T)}{\\sum_j exp (h_rW_qD_j^T) + \\sum_i exp (h_rW_qQ_i^T)} \\times softmax(\\dfrac{h_rW_qQ_k^T}{\\sqrt{d}})Q_v \\\\ & + \\dfrac{\\sum_i exp (h_rW_qQ_i^T)}{\\sum_j exp (h_rW_qD_j^T) + \\sum_i exp (h_rW_qQ_i^T)} \\times softmax(\\dfrac{h_rW_qD_k^T}{\\sqrt{d}})D_v\\\\ & =(1 - \\lambda(h_r)) softmax(\\dfrac{h_rW_qQ_k^T}{\\sqrt{d}})Q_v \\\\ & + \\lambda(h_r) softmax(\\dfrac{h_rW_qD_k^T}{\\sqrt{d}})D_v \\\\ & =(1 - \\lambda(h_r)) Att (h_r W_q, Q_k, Q_v) \\text{outcome from q} \\\\ & + \\lambda(h_r) Att (h_r W_q, D_k, D_v), \\text{outcome from demos} \\end{aligned}$$\n$$\\lambda(h_r) = \\dfrac{\\sum_j exp (h_rW_qD_j^T)}{\\sum_j exp (h_rW_qD_j^T) + \\sum_i exp (h_rW_qQ_i^T)}$$ (14)"}, {"title": "C Inverse-scaling Phenomena with Gemini", "content": "Due to the limitations of computational resources and the unavailability of closed-source models, our experiments are primarily conducted on 7-8B open source LLMs. However, by utilizing APIs, we additionally explore the performance changes of more powerful models as the number of demonstrations increased, further validating the generalizability of the argument that LLMs are not stable many-shot learners. We choose to experiment with GEMINI 1.5 PRO for its long available context window (1M tokens). We test GEMINI 1.5 PRO on MATH benchmark (Hendrycks et al., 2021), which contains 7 subsets with 5 difficulty levels that can thoroughly evaluating the math reasoning abilities of LLMs. We use greedy searching decoding strategy with and report the outcomes averaged over 5 runs for credible results. As shown in Figure 8, obvious inverse-scaling phenomenon appears in 5 out of 7 subsets, with Precalculus and Intermediate Algebra as exceptions. This validates the generalizability of the argument that LLMs are not stable many-shot learners. Meanwhile, we observe that across different difficulty levels, GEMINI 1.5 PRO presents similar performance changing trends. Figure 8 clearly shows such phenomenon. This indicates that the task difficulty does not affects the optimal demonstration number of certain task."}, {"title": "D Further Discussions", "content": "FocusICL can be seen as a method that achieves performance gains through increased computation (more demonstrations). Similar approaches include Self-Consistency (Wang et al., 2023, 2024; Li et al., 2024b,a) and Chain-of-Thought (Wei et al., 2022b). In our experiments, we have confirmed that the gains brought by FOCUSICL are decoupled from those of Chain-of-Thought. We will further explore the interplay between FOCUSICL and other methods in the future.\nWe tested the performance of FOCUSICL in tasks such as QA and inference in the experimental section. In the future, we will delve into exploring"}]}