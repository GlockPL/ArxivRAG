{"title": "Explicit Modelling of Theory of Mind for Belief Prediction in Nonverbal Social Interactions", "authors": ["Matteo Bortoletto", "Constantin Ruhdorfer", "Lei Shi", "Andreas Bulling"], "abstract": "We propose MToMnet a Theory of Mind (ToM) neural network for predicting beliefs and their dynamics during human social interactions from multimodal input. ToM is key for effective nonverbal human communication and collaboration, yet existing methods for belief modelling have not included explicit ToM modelling or have typically been limited to one or two modalities. MToMnet encodes contextual cues (scene videos and object locations) and integrates them with person-specific cues (human gaze and body language) in a separate MindNet for each person. Inspired by prior research on social cognition and computational ToM, we propose three different MToMnet variants: two involving the fusion of latent representations and one involving the re-ranking of classification scores. We evaluate our approach on two challenging real-world datasets, one focusing on belief prediction while the other examining belief dynamics prediction. Our results demonstrate that MTOMnet surpasses existing methods by a large margin while at the same time requiring a significantly smaller number of parameters. Taken together, our method opens up a highly promising direction for future work on artificial intelligent systems that can robustly predict human beliefs from their non-verbal behaviour and, as such, more effectively collaborate with humans.", "sections": [{"title": "1 Introduction", "content": "Social interaction and collaboration are essential human skills [14]. To engage in them effectively, humans have developed the ability to predict mental states and beliefs of others by observing their non-verbal behavioural cues, such as gaze or body language - so-called Theory of Mind [36, ToM]. Humans are adept at integrating multiple modalities for this task, including contextual information. Given its importance in human-human interactions, computational ToM has recently emerged as a new frontier in developing intelligent computational agents that can understand and collaborate with humans [18]. Despite a surge of papers on this new task, deep learning methods for predicting other agents' mental states have mainly been studied in constrained artificial environments [3, 38, 32, 15, 40, 39, 30, 31, 8]. Moreover, existing methods typically rely on only one or a few modalities to predict beliefs, such as visual or linguistic cues [28, 42]. Effectively integrating a wider range of modalities for belief prediction remains an open research challenge. Moreover, existing approaches for belief prediction in real-world settings have not explicitly added a ToM mechanism.\nIn this work, we focus on predicting human beliefs from multimodal inputs and how these beliefs change dynamically in real-world scenarios involving naturalistic dyadic (human-human) interactions. Belief prediction is particularly challenging, and ToM is particularly important when verbal communication is impossible. In these situations, individuals must instead rely on nonverbal cues to convey their intentions and beliefs. Advancing from recent work [13, 12], we propose a multimodal ToM neural network (MToMnet) that leverages person-specific nonverbal communication cues (gaze, pose) and contextual cues (video frames, object bounding boxes) to predict beliefs and how they change over time.\nMToMnet encodes contextual cues using shared feature extractors and person-specific cues using two independent MindNets LSTM-based sub-networks that allow our model to encode individual traits. Without a clear theoretical framework for integrating ToM into neural networks, we draw inspiration from computational ToM and social cognition research and study three different variants of MToMnet that add explicit ToM modelling. The Decision-Based MToMnet (DB-MToMnet) adopts a decision-based strategy inspired by recent advancements in referential games [28]. Here, belief prediction for one individual is used to re-rank the predictions for the other. The other two approaches employ a model-based strategy, leveraging MindNets' latent representations. The Implicit Communication MToMnet (IC-MToMnet) enables the communication between MindNets via late fusion of internal representations. The Common Ground MToMnet (CG-MToMnet) is inspired by research in social cognition, in particular by the idea that human communication involves a shared, inter-subjective common ground [43]. We use MToMnet latent representations to create such common ground based on this insight.\nWe evaluate these MToMnet variants on two challenging multimodal real-world datasets that target complementary objectives. The Benchmark for Human Belief Prediction in Object-context Scenarios [12, BOSS] consists of videos of two people tasked to collaborate only using nonverbal communication. BOSS facilitates the evaluation of models' belief prediction capabilities, i.e., the ability to correctly predict the belief of both people for each video frame. In contrast, the Triadic Belief Dynamics dataset [13, TBD] focuses on communication events that emerge during in-the-wild social interactions between two people. TBD enables the evaluation of models' ability to predict changes in the belief dynamics of a person causally constructed by these events.\nWe report extensive experiments on both datasets, demonstrating that our approach significantly outperforms state-of-the-art methods while only using a fraction of the parameters. Our results emphasise the importance of explicit Theory of Mind (ToM) modelling for achieving these performance improvements. Moreover, analyses"}, {"title": "2 Related Work", "content": "Predicting beliefs is a challenging task, even in constrained artificial settings, such as grid-world environments [38, 15, 39, 30, 31, 8], 3D worlds consisting of basic geometric shapes [40] or virtual reality environments [37]. First, datasets that take a step towards mental state modelling in real-world settings have been proposed. These datasets consist of videos of human social interactions that have been annotated with rich social cues, such as gaze or body pose. The Benchmark for Human Belief Prediction in Object-context Scenarios (BOSS) focuses on belief prediction in dyadic collaborative interactions, i.e. the task of predicting beliefs of two people collaborating with each other [12]. Similarly, the Triadic Belief Dynamics dataset (abbreviated TBD here) focuses on the prediction of belief dynamics, i.e. predicting if and how someone's belief changes during social nonverbal interactions [13]. As such, both datasets complement each other in terms of the tasks they evaluate and the scenarios they cover, namely in-the-wild everyday activities (TBD) and collaborative scenarios (BOSS). In this work, we use both datasets to evaluate our method and show that employing a triadic structure and explicitly modelling ToM achieves better performance than existing methods for predicting both beliefs and belief dynamics."}, {"title": "2.2 Machine Theory of Mind", "content": "Theory of Mind (ToM) has been studied in cognitive science and psychology for decades, but our understanding of how humans develop this essential ability is still severely limited. Mirroring efforts to understand ToM in humans, an increasing number of works in the computational sciences have investigated means to equip artificial intelligent (AI) systems with similar capabilities. Previously proposed models that aim to implement a machine ToM have been based on partially observable Markov decision processes (POMDP) [11, 19], Bayesian methods [2, 27, 13, 29] and deep learning methods [38, 4, 47, 12, 28, 8, 6, 7]. Specifically for predicting beliefs of agents that engage in nonverbal communication, Duan et al. [12] and Fan et al. [13] follow different approaches. Duan et al. [12] have used deep learning methods based on a ResNet [21] feature extractor for video frames and linear feature extractors for gaze, pose, bounding boxes and object-context relations. In contrast, Fan et al. [13] has used a triadic hierarchical energy-based model to track beliefs dynamics and compared it to neural network baselines that use only RGB frames, histogram of oriented gradients [10, HOG] or handcrafted features. Current deep learning approaches either handle input modalities shallowly or restrict themselves to a limited set of modalities. Energy-based models rely on more upfront engineering work involving the use of handcrafted features. Moreover, none of these approaches models ToM explicitly in their formulation. In this work, we show how explicitly modelling ToM in the neural network architecture can lead to substantial improvements compared to previous approaches."}, {"title": "3 Method", "content": "Our multimodal Theory of Mind neural network (MToMnet) combines nonverbal human communication cues (gaze and pose) with contextual cues (e.g. RGB video frames and object bounding boxes) to predict the beliefs of two observed human agents. In stark contrast to previous approaches [13, 12], our method leverages shared feature extractors and two MindNets that individually model each person's beliefs (see Figure 1). This design choice is motivated by research in social cognition suggesting that triadic (human-human-context) joint attentional engagement is necessary for effective cooperation [43]. This triadic engagement is reflected by our choice of two MindNets that encode nonverbal cues of each human and shared feature extractors that encode contextual cues available to both humans."}, {"title": "3.1 Base MToMnet", "content": "Our base MToMnet consists of two separate MindNets and a set of shared feature extractors. Each MindNet encodes individual cues from one person (e.g. human gaze and body language), combines them with contextual features (e.g. scene videos and object locations) coming from the shared features extractors, and adds temporal information. Let $x_{ctx} \\in C$ be a set of contextual cues and $x_{ind} \\in I$ a set of individual cues for a specific person in the scene. Contextual cues are encoded by shared features extractors and concatenated\n$X_{ctx} = || C_{e \\in ctx} SharedFeatExtrc(x_e)$ (1)\nwhere || denotes concatenation. Similarly, individual cues for a particular person are encoded and concatenated:\n$X_{ind} = || I_{e \\in ind} MindNetFeatExtr_l(x_e)$ (2)\nIndividual and contextual features are subsequently concatenated and used as input to a normalisation layer [1, LN], followed by a bidirectional LSTM [17] to model temporal information. The final LSTM hidden state is passed on to one or more fully connected (FC) classification heads that yield a probability distribution over classes:\n$P(y_i|x) = \\frac{exp(FC(LSTM(LN(x))))}{\\sum_{j = 0}^{|y|} exp(FC(LSTM(LN(x))))} \\quad where\\; x = X_{ctx} || X_{ind}$ (3)\nwhere {$y_i$}=0 are dataset-specific classes and $X = X_{ctx} || X_{ind}$. The final belief predictions are obtained using the argmax of P(yix):\n$\\hat{b} = \\underset{i}{argmax} P(y_i|x)$ (4)\nIn the following, we refer to the inputs, latent representations, and outputs of the two person-specific MindNets with the indices {1, 2}. We investigate three different variants of explicitly adding Theory of Mind to this base model architecture."}, {"title": "3.2 MToMnet Variants", "content": "We study three different variants of MToMnet that draw inspiration from prior research on computational ToM and social cognition to add explicit ToM modelling: Decision-Based (DB-MToMnet), Implicit Communication (IC-MToMnet), and Common Ground (CG-MToMnet). Our goal is to explore whether an internal ToM mechanism can, similar to humans, also benefit computational agents. We study different operations to combine neural representations for each variant of MToMnet and identify the best for our tasks.\nInspired by previous work on referential games, where a speaker agent uses an internal listener model to re-rank potential utterances [28], DB- MToMnet incorporates a ToM mechanism to re-rank class label predictions. More specifically, we combine $P(y_i|x_1)$ and $P(y_i|x_2)$ within the MToMnet using a \"ToM weight\" hyper-parameter $\\tau$, and we take the argmax of this score as the final belief prediction:\n$\\hat{b}_1 = \\underset{i}{argmax}(P(y_i|x_1)^{\\tau} \\cdot P(y_i|x_2))$\n$\\hat{b}_2 = \\underset{i}{argmax}(P(y_i|x_2)^{\\tau} \\cdot P(y_i|x_1))$ (6)\nIn contrast to Liu et al. [28], we apply the weight to the original probability distribution, e.g. to $P(y_i|x_1)$ for MindNet1, and not to the other probability distribution. As such, the hyper-parameter $\\tau$ controls the extent to which the prediction from one MindNet impacts that of the other: the larger $\\tau$, the smaller the impact.\nConceptually similar to DB-MToMnet, IC-MToMnet enables communication between the two MindNets via internal representations instead of exchanging ranking scores. Specifically, given the LSTM outputs\n$h_1, c_1 = LSTM_1(X_1) \\quad h_2, c_2 = LSTM_2(X_2)$ (7)\nwhere h and c indicate the LSTM hidden state and cell state, we aggregate one MindNet's hidden state with the other MindNet's cell state, and vice versa. As we use bidirectional LSTMs, we use a fully connected layer to project the hidden state to the cell state dimension:\n$z_1 = FC(h_1) \\oplus c_2 \\quad z_2 = FC(h_2) \\oplus c_1$ (8)\nwhere $\\oplus$ can be one of the following aggregation operations: addition, multiplication, concatenation, or cross-attention [45]. $z_1$ and $z_2$ are used to obtain the predictions in the final classification layers.\nThis final variant is inspired by the idea that the human communicative context is not limited to the surrounding environment but involves a wider, shared and inter-subjective context known as common ground [9, 43]. Tomasello [43] refers to \"common ground\" as shared experience between individuals that is critical for all human communication. As such, common ground represents a broad concept that may include perception, attention, and knowledge. In this work, we build such common ground by combining the LSTM cell state c of each MindNet. We chose the LSTM cell state as it represents the memory of the network, storing information over time. In practice, we concatenate the cell state of the two LSTMs to form a common ground representation:\n$c_g = FC(c_1 || c_2)$ (9)\nThe $c_g$ tensor is then aggregated with the LSTM hidden states to obtain $z_1$ and $z_2$:\n$z_1 = FC(h_1) \\oplus c_g \\quad z_2 = FC(h_2) \\oplus c_g$ (10)\nwhere $\\oplus$ has the same meaning as before. $z_1$ and $z_2$ are used to obtain the predictions in the final classification layers."}, {"title": "4 Experiments", "content": "BOSS. The Benchmark for Human Belief Prediction in Object-context Scenarios (BOSS) is a real-world dataset consisting of videos"}, {"title": "4.2 Implementation Details", "content": "MToMnet. BOSS and TBD share most input modalities, with a few exceptions. For BOSS, the contextual cues consist of third-person RGB videos, object bounding boxes, and the OCR matrix. Individual cues are derived from 3D gaze and pose. TBD contains third-person RGB videos and object bounding boxes as contextual cues and first-person RGB videos, pose, and 2D gaze as individual cues. Given these differences, we implemented dataset-specific feature extractors. All MToMnet variants encoded RGB video frames using a three-layer CNN with 16, 32, and 64 filters, respectively. Each convolutional layer was followed by ReLU activation and max pooling. The OCR matrix and poses were processed by a graph convolutional network [26]. The OCR matrix naturally represents an adjacency matrix for our graphs, and we used its normalised values as node features. For the poses, we defined the adjacency matrix based on connections of body joints and used the 3D joint coordinates as node features. Object bounding boxes are fed into a fully connected layer. Each MindNet uses a one-layer bidirectional LSTM, preceded by layer normalisation. All layers in our MToMnet models have hidden dimension 64 and are followed by GELU activation [22] and dropout [41] with p = 0.1. For DB-MToMnet we set the \"ToM weight\" to $\\tau$ = 2. For BOSS, each MindNet outputs a belief prediction. For TBD, classification layers for $m^1$ and $m^{12}$ take $z_1$ as input, whereas classification layers for $m^2$ and $m^{21}$ take $z_2$ as input. Since $m^c$ represents the \"common mind\" between both agents [13], $z_1$ and $z_2$ are first aggregated by performing element-wise multiplication and then fed into the classification layer. Additional details on the architecture are provided in Appendix.\nTraining. We trained all models for 300 epochs using three distinct random seeds. Cross-entropy was employed as the loss function, and model checkpoints were saved based on the highest validation accuracy for BOSS and the highest macro F1 score for TBD. These metrics were chosen for easier comparison with the original works [12, 13]. We used the Adam optimiser [25] with a learning rate of 5$\\cdot$10-4. Additional details are provided in Appendix.\nBaselines. We compare our approach with the original models [13, 12]. For BOSS, Duan et al. [12] have proposed four models based on a ResNet34 backbone for encoding video frames and fully connected layers for other modalities like gaze, pose, bounding boxes, and OCR. The models are CNN, CNN+GRU, CNN+LSTM, and CNN+Conv1D, each incorporating different types of recurrent or convolutional layers before passing the concatenated latent representations to two classification layers. To ensure a fair comparison, we re-trained these models for 300 epochs, as the original models were only trained for five epochs.\nFan et al. [13] have evaluated similar approaches on TBD. Their first model (CNN) uses a ResNet50 to extract frame features, followed by fully connected layers for belief dynamics classification. The CNN+HOG-LSTM model incorporates histograms of oriented gradient [10, HOG] features of the frame patch gazed at by the participants along with full frame features. The CNN+HOG+Mem model concatenates the history of predicted belief dynamics with frame and HOG features, while the Feats+Memory model combines handcrafted features with the history of predicted belief dynamics using a multi-layer perceptron. Fan et al. [13] achieved state-of-the-art performance on TBD by using a hierarchical graphical model (denoted here as HGM) trained using a beam-search algorithm on handcrafted events derived from raw pixels."}, {"title": "4.3 Model Performance", "content": "Results for the different MToMnet variants and the baselines on BOSS are shown in Figure 3. We use the following notation to indicate the different aggregation operations:  = element-wise multiplication,  = element-wise sum, || = concatenation,  = self-attention. As can be seen from the figure, already the Base-MToMnet (i.e. without explicit ToM modelling) outperforms all baselines (0.653 accuracy), despite requiring less than 3% of their parameters (~ 450k vs 21M). Second, incorporating explicit ToM modelling (DB, CG, IC) yields further performance improvements, but the choice of aggregation is critical. CG||-MToMnet exhibits the highest overall performance amongst all models (0.729), followed by IC-MToMnet (0.713), and IC||-MToMnet (0.705). In contrast, the DB-MToMnet (0.659) only achieves a marginal improvement over the Base-MToMnet (0.653). Figure 3 also shows results for CG||-MToMnet and baselines obtained using the original bounding box annotations. While CG||-MToMnet still outperforms all the baselines, the baselines do not benefit from the improved bounding boxes. This is likely attributed to the shallow feature aggregation in the baseline models. The paired t-test revealed a significant difference (p < 0.05) between CG||-MToMnet and the other models. To further validate our architectural choice, we evaluated a single MindNet model. Our methods outperform this model and achieve an accuracy of 0.61, except for CGO and ICO. We report modality ablation studies in Appendix.\nEvaluation scores for belief dynamics prediction on TBD are shown in Table 1, where we also report paired t-test results (p < 0.05) between CG||-MToMnet and other MToMnet variants. As for BOSS, our Base-MToMnet already achieves better performance than the best baseline (HGM) \u2013 with the only exception of $m^1$ (0.276 vs. 0.431) and $m^{12}$ (0.313 vs. 0.351). Adding explicit ToM modelling leads to further performance gains for all three variants, up to 30% on HGM. Considering the average performance across different aggregation types, the best model is again the one inspired by social cognition, CG-MToMnet. In particular, the single best performing models are CG||-MToMnet and IC-MToMnet, on par (0.488). Remarkably, all our MToMnet variants achieved their highest F1 scores on $m^c$, except for IC-MToMnet. In contrast, baselines typically find classifying belief dynamics for $m^c$ one of the most challenging tasks, achieving lower scores. Our CG||-MToMnet (0.583) substantially outperformed the best baseline, HGM (0.299), by a substantial margin of improvement.\nThese results highlight the effectiveness of our proposed MTOMnet architecture and underline the significance of explicit ToM modelling in achieving superior performance with significantly reduced computational costs."}, {"title": "Modality Ablation Study", "content": "Figure 4 shows the accuracy achieved by ablated versions of our best-performing model, CG||-MToMnet, on the BOSS dataset. The results highlight the significant impact of including bounding boxes as input modality (rgb+ocr+bbox and rgb+bbox). This result aligns with previous research [12], emphasising the crucial role of knowing the objects present in the scene for the task. When excluding bounding boxes, the accuracy scores decreased. We suspected that the newly introduced bounding box annotations might be a major contributing factor to this outcome. Therefore, for the sake of completeness, we conducted additional experiments where CG||-MToMnet was trained and evaluated using the original bounding box data. The results, depicted in Figure 4 (teal), demonstrate that when utilising the original bounding boxes, the performance gap with other ablated versions of the model drastically decreases. Nevertheless, our model performs better than the baselines even when using original bounding box annotations.\nThe differences in averaged F1 scores across $m^i$, $i = \\{1, 2, 12, 21\\}$ on TBD between the complete CG||-MToMnet and its ablated versions are reported in Figure 5. Our full model achieves an F1 score of 0.488. Ablating modalities generally result in performance degradation, with the worst-performing version experiencing an 8.9% decline, attaining an F1 score of 0.449 (rgb1+rgb3). Integrating multiple behavioural cues, such as pose and gaze, contributes positively to performance. Specifically, the combination of third-person RGB frames with gaze and pose achieved an F1 score of 0.487, outperforming models where third-person RGB frames were combined with only gaze or pose."}, {"title": "4.4 Modelling Person-Specific Features", "content": "To assess the efficacy of encoding individual cues and predicting individuals' beliefs using independent MindNets, we compared the feature representations from the two LSTMs, h\u2081 and h\u2082, by employing Principal Component Analysis [35, PCA]. As the examples in Figure 6 show, the LSTMs hidden states h\u2081 and h\u2082 from CG||-MToMnet show a clear disentanglement of principal components both on BOSS and TBD. We found the same behaviour for all our MToMnet variants and provide examples for all of them in Appendix. This finding suggests that each MindNet represents person-specific cues and fuses them with contextual cues differently for different persons, highlighting the value of such architectural choice."}, {"title": "4.5 False Belief Dynamics Prediction", "content": "TBD involves predicting belief dynamics for both first- and second-order beliefs, i.e., self-beliefs ($m^1$, $m^2$) and beliefs held over another person's beliefs ($m^{12}$, $m^{21}$). Human beliefs, however, are not always aligned with reality: individuals may hold a first- or second-order false belief [49]. We show an example in Figure 7. Thinking her partner had left the room, a participant moved an apple from the backpack to behind her laptop. She believes her partner believes the apple is still in the backpack. However, she is unaware that her partner is actually standing by the door and saw her move the apple to the table. This situation exemplifies a second-order (she believes he thinks) false belief. Despite not being used in [13], the TBD dataset includes false belief annotations, allowing us to perform post-hoc analyses on models' capabilities to predict such false belief dynamics. That is, we do not train models specifically to recognise false beliefs but evaluate models' belief dynamics predictions that, according to the provided annotations, correspond to false beliefs.\nFigure 8 summarises the accuracy for false belief dynamics predictions on TBD, considering first-order, second-order, and both false belief types. We report accuracy as we were interested in detecting whether a model predicted correctly (positive class) or not (negative class) a (a certain type of) false belief. For first-order false belief and joint first- and second-order false belief dynamics prediction, all IC- and CG-MToMnet variants outperform Base, except for ICO. In particular, our CG-MToMnet variants achieve the highest accuracy, improving over Base-MToMnet by a large margin on first-order false beliefs (up to 0.78 vs. 0.49) and on joint first- and second-order false beliefs (up to 0.80 vs. 0.57).\nFigure 8 also shows that predicting second-order false belief dynamics - generally more difficult \u2013 is easier on TBD. To understand why, we compared the distribution of all labels with those corresponding to false beliefs, as shown in Table 2. In training and test sets, most false belief labels for all minds $m^i$, $i = \\{1, 2, 12, 21\\}$, are null. For false beliefs associated with $m^{12}$ and $m^{21}$, null is the only possible label. Thus, most false beliefs in the dataset correspond to situations where individuals assume that nothing changed although something did (occur, disappear, or update) \u2013 akin to the Sally-Anne test [5]. However, when considering the overall label distribution, the most frequent label for $m^1$ and $m^2$ is update, thus leading to biases towards predicting the update class during training. This ultimately leads to lower accuracy in predicting first-order false beliefs, where null is the most prevalent label. However, our MToMnet variants, especially CG-MToMnet, can overcome this bias."}, {"title": "5 Discussion", "content": "Social Cognition Is All You Need. In this work, we presented MToMnet a Theory of Mind neural network for predicting beliefs and their dynamics during nonverbal human social interactions.\nOur best performing MToMnet variant (CG-MToMnet) achieved new state-of-the-art results for belief (dynamics) prediction on both BOSS (Figure 3) and TBD (Table 1), as well as for false belief dynamics prediction on TBD (Figure 8). At the same time, MToMnet requires considerably fewer parameters than previous state-of-the-art methods \u2013 approximately 460k vs. 21M for BOSS and 1M for TBD (see Appendix) - and, as a result, shows faster training (less than 3 hours vs. 17 to 20 hours for baselines on BOSS [12]). These findings are important as they underline the significant potential of adopting concepts developed in the cognitive sciences when designing computational agents. This also represents a paradigm shift compared to the recent trend of improving performance mainly through upscaling model complexity. Our analyses of the latent representations showed that the two MindNets are effective in capturing individuals' information in distinct ways for modelling beliefs (Figure 6), further supporting the architectural design choices of MToMnet. Remarkably, CG-MToMnet can be easily adapted to interactions involving more than two interacting agents, thanks to its shared common ground representation across all MindNets. This adaptability is crucial for future work exploring scenarios with several human and computational agents. Extending DB- and IC-MToMnet to involve more than two agents is more challenging. For DB-MToMnet, it would be crucial to find the right set of $\\tau$ such that probabilities do not vanish. One interesting idea for future work is to make $\\tau$ learnable. IC||-MToMnet faces the challenge of aggregating numerous hidden states, which could result in a quite large vector. A broader limitation arises when dealing with dynamic environments, where the number of individuals in a scene can vary. Together with extending models to dynamic environments, another promising direction is to improve the integration of different input features, a facet not explored in this work. For example, on BOSS, the OCR matrix could be used to define an additional term in the loss function to enforce object-context relations.\nExplicit Modelling of ToM Improves Performance. A key contribution of our work is the explicit modelling of ToM using multimodal individual and contextual cues. Our experiments demonstrated that MToMnet not only achieves new state-of-the-art performance on the two most common benchmark datasets but also outperforms existing baselines by a large margin. Instrumental to these improvements is the explicit ToM modelling that allowed us to improve our Base-MToMnet by 19% on BOSS (CG||-MToMnet, Figure 3), and up to 60% on TBD (CG||-MToMnet, Table 1, Average). This particularly shows for the challenging task of false belief prediction on TBD for which explicit ToM modelling leads to significant improvements of up to 60% over the Base-MToMnet model (Figure 8). This finding is particularly significant considering the large body of work and long-standing efforts on false belief prediction [16, 3, 38, 44].\nLimitations of Current Benchmarks. Our ablation studies highlight a fundamental limitation inherent in current benchmarks due to the quality and consistency of the data, negatively affecting model performance. In this work, we found inaccuracies in bounding box annotations in the BOSS dataset and re-extracted them. This led to a substantial enhancement in performance, highlighting the importance of precise bounding box information. Rectifying similar issues in gaze data proved unfeasible, as participant faces in the BOSS dataset were deliberately obscured for privacy reasons. Despite data limitations, our models outperformed the baselines with both original and revised annotations (Figure 4)."}, {"title": "6 Conclusion", "content": "In this work, we proposed MToMnet, a Theory of Mind network that predicts beliefs and their dynamics during human social interactions from multimodal input. Building on social cognition and ToM research, we designed three MToMnet variations: one decision-based and two model-based. Across two real-world datasets, MToMnet outperformed existing methods in both belief prediction and belief dynamics prediction, despite having fewer parameters. These results advance the state-of-the-art in belief prediction thus facilitating better collaboration with humans."}]}