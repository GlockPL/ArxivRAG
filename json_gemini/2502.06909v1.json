{"title": "Satisfaction-Aware Incentive Scheme for Federated Learning in Industrial Metaverse: DRL-Based Stackbelberg Game Approach", "authors": ["Xiaohuan Li", "Shaowen Qin", "Xin Tang", "Jiawen Kang", "Jin Ye", "Zhonghua Zhao", "Dusit Niyato"], "abstract": "Industrial Metaverse leverages the Industrial Internet of Things (IIoT) to integrate data from diverse devices, employing federated learning and meta-computing to train models in a distributed manner while ensuring data privacy. Achieving an immersive experience for industrial Metaverse necessitates maintaining a balance between model quality and training latency. Consequently, a primary challenge in federated learning tasks is optimizing overall system performance by balancing model quality and training latency. This paper designs a satisfaction function that accounts for data size, Age of Information (AoI), and training latency. Additionally, the satisfaction function is incorporated into the utility functions to incentivize node participation in model training. We model the utility functions of servers and nodes as a two-stage Stackelberg game and employ a deep reinforcement learning approach to learn the Stackelberg equilibrium. This approach ensures balanced rewards and enhances the applicability of the incentive scheme for industrial Metaverse. Simulation results demonstrate that, under the same budget constraints, the proposed incentive scheme improves at least 23.7% utility compared to existing schemes without compromising model accuracy.", "sections": [{"title": "I. INTRODUCTION", "content": "Metaverse is undoubtedly one of the most popular and promising intelligent applications [1]\u2013[3], creating a collective virtual shared space that merges digital and physical realities, enabling users to interact with each other and digital envi-ronments in immersive, and real-time settings. Metaverse can also be combined with the Industrial Internet of Things (IIoT), giving rise to industrial Metaverse. For example, NVIDIA [4] developed an open platform called Omniverse, which supports multi-user real-time 3D simulation and visualization of physical properties in a shared virtual space for industrial applications such as automotive design. The meta-computing plays a crucial role in integrating distributed computing re-sources within the IIoT to build industrial Metaverse. It enables distributed data processing, storage, and computation across heterogeneous systems, bridging the gap between vir-tual and physical spaces [5]. This provides a seamless platform for industrial Metaverse, delivering a realistic, persistent, and smooth interaction experience for users.\nHowever, traditional distributed learning approaches may face certain limitations due to the large number of devices and data in industrial Metaverse enabled by meta-computing. First, IIoT nodes need to share data for model training, which increases the risk of privacy leakage. Second, the large volume of data transmission may result in latency issues. Fortunately, Federated Learning (FL) [6], [7] can effectively address these problems. FL is a distributed learning scheme proposed by Google to optimize global machine learning models without moving data out of local devices. In the industrial Metaverse, which uses meta-computing and incorporates FL, each IIoT node participates in training a shared AI model using its own dataset, and then uploads its local model to the server to build a new global model [8], [9], thus achieving the goal of building a high-quality Metaverse while ensuring privacy.\nTo ensure an immersive experience in industrial Metaverse, high-quality global models must be developed. This requires vast amounts of real-time sensing data and significant compu-tational resources from nodes [10]. Furthermore, achieving an immersive experience in the Metaverse demands not only high-quality models but also low-latency interactions. Balancing the reduction of latency with the enhancement of quality is crucial to maintaining the overall performance of Metaverse applications. However, due to the large number of nodes and the complex structure of the industrial Metaverse, designing a methodology that effectively balances quality and latency presents a significant challenge.\nTo address these challenges, we define a satisfaction func-tion based on data size, Age of Information (AoI) [11], and latency, aiming to balance training latency and model quality. This satisfaction function is then incorporated into the utility function, transforming the utility optimization problem into a Stackelberg game. However, traditional methods such"}, {"title": "II. RELATED WORKS", "content": "Node contribution evaluation: Node contribution evalu-ation in FL focuses on the extent to which each partici-pating node contributes to the overall learning process and outcome, enabling FL to achieve higher performance with minimal rewards [22]. The authors in [23] used historical learning records to estimate nodes' learning quality and an exponential forgetting function to assign weights, designed quality-aware incentives and model aggregation methods to improve learning effectiveness. The authors in [24] designed the quality-aware node selection framework AUCTION, which included factors such as data size, data quality, and learning budget within nodes that influence learning quality. These factors were encoded into an attention-based neural network to enhance strategy performance. The authors in [25] introduced reputation as a node evaluation metric to improve the relia-bility of FL tasks in mobile networks, resulting in a reliable worker selection scheme. Shapley values are also used in FL to measure how much a participant contributes to the overall model training quality. The authors in [26] leveraged Shapley value calculation to build FedCoin, a blockchain-based peer-to-peer payment system that ensures realistic and fair profit distribution.\nIncentive Schemes: Auction-based schemes are commonly applied in FL due to their simplicity of construction. The authors in [27] proposed an incentive mechanism named FMore, designed for multidimensional procurement auctions, to encourage more low-cost, high-quality edge nodes to par-ticipate in learning. The authors in [23] proposed an FL system called FAIR, where reverse auctions were modeled to"}, {"title": "III. SYSTEM MODEL", "content": "In order to promote efficient task processing and deci-sion making in the industrial Metaverse, we design a meta-computing framework based on FL in Fig. 1. The device management module contains multiple edge nodes. Its primary purpose is to collect data from production devices, integrate the computational, storage, and communication resources of the edge nodes, map these resources to the server, convert them into objects that can be easily accessed by the resource scheduler, and then train FL models locally based on the incentive scheme developed by the task manager. The resource scheduler module contains several virtual edge nodes that con-stantly monitor changes in the configuration details of physical nodes, simulate possible states of the nodes, and dynamically perform resource optimization. The task management module, located on the server, accepts requests from users, decomposes tasks, and designs incentive schemes based on their conditional constraints. The zero-trust computing management module performs global aggregation for FL via the blockchain, and the identity and access management module ensures users have the appropriate permissions to access the data. Since the zero-trust computing management and identity and access management modules are not the focus of this paper, detailed discussions are omitted. Comprehensive analyses of these modules are"}, {"title": "IV. SATISFACTION: QUALITY CONTROL FOR INDUSTRIAL METAVERSE", "content": "The structure of the industrial Metaverse is complex, con-taining multiple types of industrial devices and involving a large number of industrial nodes, where the trade-off between low latency and high model quality must be carefully con-sidered. Among them, latency directly affects screen lag and response time in virtual environments. High latency causes the screen to fail to update in time, resulting in users experiencing lags or delays when interacting with the environment [19], [20]. In other words, user interaction in the Metaverse is highly dependent on low latency, and any delay will significantly reduce user satisfaction. On the other hand, degraded model quality means that the information or imagery received by users is not realistic enough, which may lead to a lack of synchronization between the virtual environment and the user's actual behavior. Furthermore, model quality will greatly disturb users' decisions. With information from a declining quality model, users may make inappropriate responses. Tra-ditional quality metrics in FL do not adequately capture the requirements of the industrial Metaverse. Therefore, we pro-pose a satisfaction metric $G_i$, to balance quality and latency. The satisfaction metric $G_i$ of the task i is denoted as\n$G_i = \\tau Q_i - \\lambda E_i$,\nwhere $\\tau$ and $\\lambda$ are the conversion parameters for quality and latency, respectively.\nSince the timeout of gradient updates can negatively impact the learning results, we use Aol to measure the freshness of the model in order to ensure its quality. AoI, as a valid measure of information freshness, denotes the latency of the information from its generation to the completion of the model training after it has been uploaded to the server [35], and it can enhance the performance of time-critical applications and services. In FL, we assume that requests arrive at the beginning of each cycle. We focus on FL with a data cache buffer on the node and Aol [38]. The node i periodically updates its cached data with an update period $\\theta_i$ independently. It is denoted as\n$\\theta_i = c_it + a_it$,\nwhere $c_it$ ($c_i \\in N$) is the time spent by the node i to collect and process the model training data; $a_it$($a_i \\in N$) is the duration from the end of data collection to the beginning of the next phase of data collection. It may contains service time period and idle time period.\nWhen the request arrives during the data collection phase or at the beginning of phase ($c_i + 1)t$, the Aol is t. This is the minimum Aol value. For requests arriving in phase $lxt$, where $l \\geq (c_i + 2)$, the Aol will be $[l - (c_i + 1) + 1]t$. We suppose that t is fixed and that the update cycle $\\theta_i$ is affected by $c_i$ and $a_i$. Let us consider a case with an adjustable update phase, i.e., when $a_i = a$ is fixed, $c_i = \\frac{\\theta_i}{t} - a$, we use $\\theta_i$ instead of $c_i$. Therefore, the average Aol of the node i is\n$\\bar{A_i} = \\frac{t}{\\frac{\\theta_i}{t} + a} + \\frac{\\tau \\theta_i}{\\frac{\\theta_i}{t} + a} [\\frac{\\theta_i}{t} + 1 + (a - 1)(a+2)] / 2$\n-\\frac{\\frac{\\theta_i}{t} - a}{(\\frac{\\theta_i}{t} + a)^2} (\\frac{a^2-a}{2})\\,,"}, {"title": "V. SATISFACTION-AWARE INCENTIVES SCHEME", "content": "In order to meet the quality and latency requirements of industrial Metaverse tasks, we formulate the corresponding utility func-tions and optimization objectives. Node i is incentivized to handle incoming task requests, and each participating node receives a monetary reward $R_i$ from the server. Therefore, the utility of node i is the difference between the reward $R_i$ and the cost $C_i$ of participating in the FL task. The utility $U_i$ can be expressed as\n$U_i = R_i - C_i$,\nwhere the cost of the FL training task is defined as $C_i = \\frac{\\theta_i}{\\sigma_i}$, where $\\sigma_i$ is the unit cost required to maintain the update cycle $\\theta_i$, with respect to data collection, computation and transmission."}, {"title": "VI. DRL-BASED STACKELBERG GAME APPROACH", "content": "Traditional heuristic algorithms require full information about the game environment. However, due to the non-cooperative relationship, each game player is not willing to disclose its private information. DRL aims to learn decision-making based on past experiences, current states, and given rewards. To address decision-making problems with contin-uous action spaces, this paper employs the MADDPG algo-rithm [33], a DRL algorithm for use in a multi-intelligence environment. We describe the detailed definition of each term as follows.\nState Space: In the current decision round t, the state space is defined by the price strategy $R^t = {r^t_1,...,r^t_i,...,r^t_I}$ assigned by the server to each edge node, and the caching strategy $\\odot = {\\theta^t_1,...,\\theta^t_i,...,\\theta^t_I}$ of the edge node. Formally, the state space at round t is represented as $S_t \\equiv {R^t, \\odot}$.\nPartially Observable Space: For privacy protection rea-sons, the agents at the edge nodes cannot observe the complete state of the environment and can only make decisions based on their localized observations in the formulated partially observable space. At the beginning of each training round t, the server first decides on its strategy based on its own historical strategy, which can be regarded as the observation space of the server: $o_t^{server} = {R^{t-L},\\odot^{t-L}, ..., R^{t-1},\\odot^{t-1}}$. Then, edge node i determines its caching strategy based on the historical pricing strategy of the server and the historical update strategies ${...,\\theta^{t-1}_{i-1}, \\theta^{t-1}_{i+1}, ..., \\theta^{t-1}_I}$ of the other edge nodes. Therefore, the observation space of edge node i is denoted as $o_t^{node} = {..., \\theta^{t-1}_{i-1}, \\theta^{t-1}_{i+1}, ..., \\theta^{t-1}_I, R^{t-1}}$.\nAction Space: After receiving the observation $o_t^{server}$, the server agent must take an action $x^{server}_t = r^t_i$ with the goal of utility maximization. Considering the bid limit $r^{max}_i$, the action space is defined as $r \\in [0, r^{max}_i]$. The edge node determines a caching strategy $x^{node} = \\theta^t_i$ after receiving the observation $o^{node^{-i}}_t$\nReward Function: After all agents take actions, each agent receives an immediate reward $e_t$ corresponding to the current state and the action taken. The reward functions of the nodes and server are aligned with the utility functions in Eq. (9) and (10).\nIn each training cycle, the server determines a payment strategy. Upon observing the payment strategy from the server, the edge nodes determine their feedback. After the server receives the optimal training strategies from the edge nodes, it proceeds to determine the payment strategy. The server also updates the strategy and value function based on the rewards from each training cycle."}, {"title": "VII. SIMULATION RESULTS", "content": "To verify the impact of the proposed satisfaction-aware incentive scheme on FL in the industrial Metaverse, this paper conducts experiments using Python 3.7 and TensorFlow 1.15, and evaluates the performance of the scheme. The value ranges of the simulation parameters are shown in Table II. We take image classification tasks as an example of industrial Metaverse applications. In the physical space, the robots perform image classification tasks without loss of generality. We use the MNIST dataset, consisting of 60,000 samples for the training set and 10,000 samples for the test set. To evaluate the performance of the proposed scheme, we compare it with four reinforcement learning algorithms: MADDPG [33], MAPPO [40], MASAC [41], and MADQN [42], which are DDPG algorithm, PPO algorithm, SAC algorithm, and DQN algorithm in multi-agent environment."}, {"title": "VIII. CONCLUSION", "content": "In this paper, we have designed a satisfaction-aware FL incentive scheme for the industrial Metaverse. The scheme integrates a satisfaction function that incorporates data size, Aol, and latency into the utility function and formulates this utility optimization problem as a two-stage Stackelberg game. We have employed a DRL approach to learn the equilibrium of the Stackelberg game, with the goal of maximizing the server's utility by identifying the optimal equilibrium. Experimental results have demonstrated that, compared to traditional meth-ods, the scheme effectively balances model quality and update latency through efficient resource allocation, enhances FL per-formance, ensures real-time performance and high efficiency of FL in the Metaverse, and better addresses the needs of the industrial Metaverse. In the future, we will extend our meta-computing framework to incorporate efficient asynchronous FL, with the aim of enhancing learning efficiency in the industrial Metaverse."}]}