{"title": "AUTOREGRESSIVE ENZYME FUNCTION PREDICTION WITH MULTI-SCALE MULTI-MODALITY FUSION", "authors": ["Dingyi Rong", "Wenzhuo Zheng", "Bozitao Zhong", "Zhouhan Lin", "Liang Hong", "Ning Liu"], "abstract": "Accurate prediction of enzyme function is crucial for elucidating biological mechanisms and driving\ninnovation across various sectors. Existing deep learning methods tend to rely solely on either\nsequence data or structural data and predict the EC number as a whole, neglecting the intrinsic\nhierarchical structure of EC numbers. To address these limitations, we introduce MAPred, a novel\nmulti-modality and multi-scale model designed to autoregressively predict the EC number of proteins.\nMAPred integrates both the primary amino acid sequence and the 3D tokens of proteins, employing a\ndual-pathway approach to capture comprehensive protein characteristics and essential local functional\nsites. Additionally, MAPred utilizes an autoregressive prediction network to sequentially predict the\ndigits of the EC number, leveraging the hierarchical organization of EC classifications. Evaluations\non benchmark datasets, including New-392, Price, and New-815, demonstrate that our method\noutperforms existing models, marking a significant advance in the reliability and granularity of\nprotein function prediction within bioinformatics.", "sections": [{"title": "Introduction", "content": "In the realm of bioinformatics, accurately determining the functions of enzymes utilizing Enzyme Commission (EC)\nnumbers [1] is a long-standing and challenging task. This process can offer insights into their catalytic mechanisms,\nsubstrate specificities, and potential applications in various industries [2, 3, 4, 5]. However, the ability to experimentally\ndetermine the EC numbers of enzymes is significantly lagging behind the rapid pace at which enzyme data is being\ngenerated [6], for the experimental determination of function is a complex, time-consuming, and resource-intensive\nprocess that involves multiple steps [7]. Therefore, developing computational methods for predicting EC numbers is\nparticularly important.\nThe traditional bioinformatics methods, including sequence alignment-based approaches like BLASTp [8], have been\ncrucial in predicting protein EC numbers. However, they heavily rely on pre-existing knowledge stored in databases\n[9, 10, 11], which limits their capacity to accurately predict novel proteins lacking close homologs. Moreover, the\ncomplexity of protein evolution challenges the correlation between sequence similarity and functional conservation,\nleading to inaccuracies in predictions.\nIn recent years, deep learning-based approaches have emerged as a promising alternative in the field of EC number\nprediction, offering potential solutions to the limitations of traditional methods, yet they still face their own limitations.\nOn one hand, most existing approaches rely solely on either protein sequence [12, 13, 14, 15] or structure information\n[16, 17, 18, 19] for predictions, facing their respective limitations: sequence-only models lack the critical three-\ndimensional context necessary for function, potentially misinterpreting proteins with similar sequences but different\nstructures, while structure-only models struggle due to the challenges in obtaining protein structures and the inability to\ncapture dynamic conformational changes necessary for activity. On the other hand, current approaches treat the EC\nnumbers as a singular entity, relying on the protein's overall characteristics for their prediction [20, 21, 16, 22]. While\nfocusing on global features is valuable for capturing the overall information of a protein, it may overlook critical local\nregions within the protein. Moreover, treating EC numbers as a singular entity overlooks their inherent hierarchical\norganization. An enzyme's EC number is a concatenation of four numerical codes, each indicating different levels of\nspecificity in substrate and reaction type [1]. Ignoring this hierarchy can lead to a loss of information.\nIn this work, we propose MAPred (Multi-scale multi-modality Autoregressive Predictor) to overcome the above\nproblems, which features three designs. First, to obtain a joint representation of the protein's primary and tertiary\nstructures, given that 3Di alphabet [23] can discretize the structure into a set of sequences with strong positional\ncorrelation to the protein sequence and can serve as a simplified substitute for protein structure, we derive the\ncorresponding 3Di tokens from the protein sequence using ProstT5 [24], which serves as a combined input to our\nnetwork. Second, to capture both the global characteristics of the protein and the local functional site features, we have\ndeveloped a hybrid feature extraction network that includes both a global feature extraction pathway and a local feature\nextraction pathway. Considering that the input is composed of two parts, it is logical for the global feature extraction\nblock to employ an interlaced sequence-3Di cross-attention mechanism, and in the local feature extraction block, CNN\nis well-suited for extracting local structural information. Third, recognizing the strong sequential dependencies among\nthe four digits of the EC number, inspired by [25], MAPred utilizes an autoregressive prediction network to predict\neach of the four digits sequentially. As far as we know, MAPred is the first to use a hybrid feature of protein sequences\nand 3Di to predict the EC number, and we conduct extensive ablation studies to reveal the role of each component,\nwhich helps deepen the reader's understanding of MAPred and may provide further inspiration for subsequent research.\nIn summary, our contributions include:\n\u2022 We combined the protein sequence and its 3Di tokens as inputs to the model, incorporating both the primary\nsequence and tertiary structure information.\n\u2022 We propose a novel hybrid feature extraction network to learn global and local representations from multi-\nmodality protein representations.\n\u2022 We propose an autoregressive label prediction network, which establishes a sequential prediction logic by\ncreating a hierarchy for Enzyme Commission (EC) number predictions.\n\u2022 We comprehensively compare advanced models in real-world datasets, e.g., New-392, Price, and New-815,\nand demonstrate the potential of the components we proposed."}, {"title": "Related Work", "content": "Sequence similarity-based methods Sequence similarity-based methods, particularly those utilizing sequence\nalignment tools [26, 27], rely on the comparison of a query protein sequence against a database of sequences with\nknown functions to find matches with high sequence similarity. The effectiveness of these methods hinges on the\nquality of the sequence alignment and annotation quality of the databases used [28, 29]. Recent developments have\nintegrated traditional homology searches with more sophisticated algorithms. For instance, methods employing Hidden\nMarkov Models (HMMs) [30], as implemented in HMMER [31] and Gapped-BLAST [27], use probabilistic models of\nsequence evolution to detect homologous protein families and predict their function, including EC numbers, even in\ncases of low sequence similarity.\nSequence-based ML methods Sequence-based ML methods employ neural networks to capture the inherent patterns\nwithin amino acid sequences to predict the EC numbers of enzymes. The earliest attempts used Convolutional Neural\nNetworks (CNNs) to capture local sequence motifs that are indicative of enzymatic function [12, 13, 14]. Following\nthe success of CNNs, Recurrent Neural Networks (RNNs), particularly Long Short-Term Memory (LSTM) networks,\nwere explored due to their ability to capture long-range dependencies in sequences [20, 21, 32]. This capability is\ncrucial for understanding the full context of enzyme functions, as determinants of enzymatic activity can be dispersed\nthroughout the protein sequence. Recently, Transformer-based methods marked a significant milestone in protein\nfunction prediction. These methods [33, 15, 34, 35, 36] leverage self-attention mechanisms to capture complex\nrelationships between amino acids, regardless of their distance in the sequence, and achieve state-of-the-art performance\non benchmark datasets.\nStructure-based ML methods To delve deeper into the interactions between amino acids at each position within\nproteins, some methods utilize the protein's structure as input for the model. As the tertiary structure of proteins\nreveals, amino acids that are distant in the sequence can be closely positioned in the spatial arrangement. Convolutional\nneural networks (CNNs) have shown promising results in capturing the intricate patterns within protein structures.\nThese approaches typically represent the input structure as Grids [16, 17] or Residue Contact Map [37], and utilize\nmultiple convolutional layers to capture hierarchical features and learned representations that capture crucial structural\ninformation relevant to enzymatic function. Recent innovations have also explored the use of graph neural networks\n(GNNs) for this task. By representing protein structures as graphs, where nodes represent amino acids and edges"}, {"title": "Methodology", "content": "3.1 Overall Framework\nWe present the overall model framework in Figure 1. For the given protein sequences, we respectively use the pre-trained\nprotein models ESM [36] and ProstT5 [24] to obtain sequence features and corresponding 3Di features, and use them\nas inputs to the model. The architecture of MAPred consists of two networks: the Feature Extraction Network and\nthe Prediction Network. The Feature Extraction Network comprises a global feature extraction pathway and a local\nfeature extraction pathway. The global feature pathway is responsible for extracting the overall feature representations\nof the proteins, while the local feature pathway is designed to extract representations of functional sites. The Prediction\nNetwork operates on an autoregressive prediction scheme [25]. Instead of treating EC number prediction as a single\nmulti-label classification problem, it predicts the digits of the EC number sequentially.\n3.2 Feature Extraction Network\n3.2.1 Global Feature Extraction Block\nOur proposed Global Feature Extraction (GFE) pathway utilizes a strided sequence-to-3Di cross-attention mechanism\nto enhance the integration of protein sequence features with their corresponding structural features. As depicted in\nFigure 1, the global feature extraction pathway is constructed by stacking three GFE blocks, with each GFE block\nconsisting of two cross-attention layers. In the first layer, the 3Di features are updated by incorporating features from\nthe sequence. Conversely, in the second layer, the sequence features are enriched with characteristics derived from the\nupdated 3Di representations. This bidirectional exchange of information allows for a comprehensive integration of both\nsequence and structural information.\nSpecifically, in the first cross-attention layer of i-th block, the sequence features $F_{seq}^{global_i} \\in \\mathbb{R}^{L \\times N}$ serve as the query\nvectors, while the 3Di features $F_{3Di} \\in \\mathbb{R}^{L \\times N}$ act as the key-value pairs. Utilizing scaled dot-product attention, as\ndescribed in [40], we compute the Multi-Head Attention as:\n$\\text{MHA1}(Q_{seq}, K_{3Di}, V_{3Di}) = [\\text{head}_1, \\text{head}_2,\\dots,\\text{head}_h]$\t\t\t(1)\n$\\text{head}_i = \\text{Att}(Q W_i^Q, K W_i^K, V W_i^V)$\t\t\t(2)\n$\\text{Att}(Q, K, V) = \\text{Softmax}(\\frac{Q K^T}{\\sqrt{d_k}}) V$\t\t\t(3)\nHere, $Q_{seq} \\in \\mathbb{R}^{L_q \\times N}$, $K_{3Di} \\in \\mathbb{R}^{L_k \\times N}$, and $V_{3Di} \\in \\mathbb{R}^{L_v \\times N}$ represent the query, key, and value matrices, respectively,\nderived from $F_{seq}^{global_i}$ and $F_{3Di}^{global_i}$. $W_i^Q \\in \\mathbb{R}^{N \\times N_q}, W_i^K \\in \\mathbb{R}^{N \\times N_k}, W_i^V \\in \\mathbb{R}^{N \\times N_v}$ and $N_q = N_k = N_v = N/h$\nare learnable parameters. The $[\\cdot]$ means the concatenate operation. The attention mechanism allows the model to focus on\nthe most relevant parts of the protein sequence when considering the 3Di structural information.\nThe output of the first cross-attention layer is a set of updated 3Di features $F_{3Di}^{global_i}$, which now contains enriched\ninformation from the protein sequences. These updated features $F_{3Di}^{global_i}$, are then used as input to the second layer,\nwhere the roles are reversed, the updated 3Di features become the query, while the original sequence features act as the\nkey-value pairs:\n$\\text{MHA2}(Q_{3Di}, K_{seq}, V_{seq}) = [\\text{head}_1, \\text{head}_2,\\dots,\\text{head}_h]$\t\t\t(4)\nThis second layer further refines the sequence representations by aligning them with the structural features highlighted\nthrough the attention process. $F_{seq}^{global_i} \\in \\mathbb{R}^{L \\times N}$ and $F_{3Di}^{global_i} \\in \\mathbb{R}^{L \\times N}$ are the outputs of the GFE block. The sequential\napplication of these two cross-attention layers within each GFE block enables MAPred to iteratively refine the feature\nrepresentations, leading to a more nuanced and biologically meaningful fusion of sequence and structure.\n3.2.2 Local Feature Extraction Block\nFunctional sites are conserved regions within proteins that serve specific functions and are crucial to both the structure\nand functionality of proteins [41]. To gain an in-depth understanding of the functional site characteristics within these"}, {"title": "Prediction Network", "content": "After obtaining the integrated features $F_{fuse} = [F_{global}, F_{local}]$, considering that the EC number consists of four\ndigits with strong sequential dependencies, we adopted an autoregressive prediction strategy to progressively predict\neach digit. Specifically, we first predict the first digit of the EC number, then use the predicted digit as an input feature\nfor subsequent predictions, and so on, sequentially predicting the second, third, and fourth digits.\nTo implement this autoregressive prediction approach, we designed a multi-task learning framework comprising four\nMLPs, each responsible for predicting one digit. During the prediction process, the input features for $MLP_i$ include\nboth $F_{fuse}$ and the prediction results $\\hat{y}_{i-1}$ from the previous $MLP_{i-1}$. Formally, the autoregressive prediction process\ncan be represented as:\n$\\hat{y}_i = \\begin{cases}MLP_i(F_{fuse}; \\theta_i),\t\t\t\\text{if }i=1\\\\MLP_i([\\hat{y}_{i-1}, F_{fuse}]; \\theta_i),\t\t\\text{otherwise}\\end{cases}$\t\t\t(6)\nHere, $\\theta_i$ represents the corresponding MLP parameters. Through this approach, each MLP can utilize the predicted\nresults from the preceding MLP, thus better modeling the dependency relationships between labels.\n3.4 Model Training\n3.4.1 Training Loss\nA combined training loss is utilized which is constituted by 1) the triplet loss between samples of different classes, and\n2) the BCE loss between the ground-truth and predicted EC number. Details are as follows. We use $\\mathcal{L}_{triplet}$ to denote\nthe triplet loss between an anchor $a_i$, its positive sample $p_i$ and its negative sample $n_i$, which is mathematically defined\nas\n$\\mathcal{L}_{triplet} = \\sum_{i=1}^N \\max(0, d(a_i, p_i) - d(a_i, n_i) + margin)$\t\t\t(7)\nwhere $d(\\cdot, \\cdot)$ represents the distance, $margin$ is a predefined constant, and $N$ is the total number of triplets. In the\nmeantime, we use $\\mathcal{L}_{BCE}$ to denote the label prediction loss/error between the predicted and the ground-truth EC\nnumber, which is mathematically defined as,\n$\\mathcal{L}_{BCE}(y_{i,j}, \\hat{y}_{i,j}) = - \\sum_{j=1}^4 \\sum_{i=1}^N [y_{i,j} \\log(\\hat{y}_{i,j}) + (1 - y_{i,j}) \\log(1 - \\hat{y}_{i,j})]$\t\t\t(8)\nwhere $\\hat{y}_{i,j}$ refers to EC number predicted by the j-th MLP and $y_{i,j}$ refers to the actual EC number. The final combined\nloss could thus be formulated as\n$\\mathcal{L}_{total} = \\lambda_1 \\cdot \\mathcal{L}_{triplet} + \\lambda_2 \\cdot \\mathcal{L}_{BCE}$\t\t\t(9)\nHere, $\\lambda_1$ and $\\lambda_2$ are weighting factors that take different values during different training phases, as specified in the\nfollowing.\n3.4.2 Two-phase Training\nWe adopted a two-phase training scheme. In the first phase, we just train the feature extraction network using triplet\nlearning, and the $\\lambda_2$ in the loss function 9 is set to 0. Once the feature extraction network converges, we start the second\nphase of training. In the second phase, we train the EC number prediction network, by setting the $\\lambda_1 = 0$ and $\\lambda_2 = 1.0$\nin the loss function."}, {"title": "Experiments", "content": "In this section, we first give a detailed description of the experimental protocol for this study. We then present\ncomprehensive experiments to demonstrate: 1) the superior performance of our proposed framework for EC number\nprediction; 2) the effectiveness of the proposed components; and 3) MAPred can learn the functional regions of\nenzymes.\n4.1 Experimental Protocol\n4.1.1 Datasets\nWe use the same Swiss-Prot dataset and data splitting as CLEAN [34], encompassing 227,362 protein sequences\nthat cover 5,242 EC numbers. To evaluate the performance of MAPred on real-world datasets, we employed two\ndistinct datasets that are utilized by CLEAN during testing: New-392, Price-149 as well as a dataset we collected\nfrom Swiss-Prot named New-815. The New-392 dataset consists of 392 protein sequences, covering 177 different\nEC numbers. The Price dataset, as described by Price et al. [7], is a collection of experimentally verified results,\ncomprising 149 protein sequences that cover 56 different EC numbers. The New-815 dataset was collected using a\nmethod aligned with the data collection approach employed in the CLEAN, and consists of 815 protein sequences,\ncovering 380 different EC numbers, containing data from Swiss-Prot released after April 2022.\n4.1.2 Implementation Details\nIn the first training phase, we train the feature extraction network with a batch size of 40 for 1000 epochs. The learning\nrate is set to 5e - 4 and remains constant during the training process. In the second training phase, we freeze the feature\nextraction network and train EC number prediction network with a batch size of 50000 for 150 epochs, the learning\nrate is 1e - 3. We use Adam [44] optimizer with $\\beta_1 = 0.9, \\beta_2 = 0.999$ and use the CosineAnnealingLR [45] as the\nscheduler. Our approach is implemented with PyTorch [46] framework, and all experiments are conducted on a machine\nwith three NVIDIA RTX6000 ADA GPUs with AMD EPYC 7763 CPU and 256 G memory.\n4.2 EC Number Prediction\nWe evaluate the accuracy of EC number prediction based on the commonly used Precision, Recall and F1 metrics,\nwhich are widely adopted in classification tasks. For benchmarking, we compare the performance of our proposed\nmodel with state-of-the-art classification models including: DeepEC [12], DeepECTF (short for DeepECtransformer)\n[15], ProteInfer [13], TFPC [33], CLEAN [34].\nFrom the results shown in Table 1, we make the following important observations. Our method, MAPred, demonstrates\na remarkable performance, outperforming existing approaches in all evaluated metrics. Specifically, on the New-392\ndataset, MAPred achieves a Precision of 0.651, which is a significant improvement over the next best method, CLEAN,\nwith a Precision of 0.575. This indicates that MAPred is highly accurate in predicting the correct EC numbers without\na substantial number of false positives. In terms of Recall, MAPred also shows an impressive score of 0.632, which\nis higher than that of the CLEAN method, which has a Recall of 0.491, suggesting that MAPred is more effective in\ncapturing the true positive instances. Furthermore, MAPred attains an F1 score of 0.610, again surpassing the CLEAN\nmethod's F1 score of 0.502. This result highlights the robustness of MAPred in achieving a good balance between\nPrecision and Recall. Consistent performance gains are observed across the other datasets as well. For instance, on the\nPrice dataset, MAPred's Precision, Recall, and F1 scores are 0.554, 0.487, and 0.493, respectively, which are superior\nto those of the CLEAN method.\nFurthermore, we present a detailed analysis of the results in Figure 3. We use the F1 score to comprehensively evaluate\nthe performance. Figure 3(A) illustrates the methods' performance in predicting EC numbers with varying frequencies.\nIt is evident that MAPred consistently outperforms the other methods across all ranges of EC occurrence. Notably, in\nthe lower frequency intervals, such as (0, 5], MAPred maintains a leading position with an F1 score of about 0.486,\nindicating its superior ability to predict EC numbers in small sample sizes compared to other methods. In Figure 3(B),\nthe analysis focuses on the accuracy of predicting each of the four digits of the EC number. The results show that\nMAPred demonstrates high accuracy at each hierarchical level, starting with an F1 score of 0.918 for the first digit\n(EC X._._._) and maintaining a leading performance with an F1 score of approximately 0.635 for the final digit (EC\n\u03a7.\u03a7.\u03a7.\u03a7). Notably, MAPred, CLEAN, and TFPC exhibited similar F1 scores in predicting the initial three digits of the\nEC number. This similarity stems from the broader categorization of enzyme functions represented by these digits,\nwhich include a multitude of examples, thus simplifying the task for prediction models.\n4.3 Ablation Studies\nTo verify the effectiveness of the proposed components, we conduct the following component contribution analysis\nexperiment. As introduced above, MAPred mainly includes three components: 1) the global feature extraction pathway\n(denoted as G); 2) the local feature extraction pathway (denoted as L); and 3) the autoregressive label prediction model\n(denoted as H). During the experiment, we disable one of these three components and re-train the remaining network\nparameters. Besides, to test the impact of multi-modality inputs on the results, we removed one part of the input,\nobtaining two results with only one type of input each, namely, seq-only (short for sequence-only) and 3Di-only. The\nstudy results are presented in the Table 2. Note that we use tick/cross to indicate whether a certain component is enabled\nor disabled.\nFrom Table 2, we make the following observations: 1) Once the global feature extraction pathway (G) is removed, the\nperformance in predicting the EC number drops significantly. For example, on the New-392 dataset, the precision drops\nfrom 0.651 to 0.354. This substantial decline highlights the importance of the global feature extraction pathway, which\ncaptures the overall characteristics and long-range dependencies within the protein sequence.\n2) The local feature extraction pathway (L) also plays a significant role, as indicated by the performance decrease when\nit is disabled. However, the drop is less severe compared to the removal of the global pathway, suggesting that while\nlocal features contribute to the model's performance, the global context provides a more substantial impact on the\noverall prediction accuracy.\n3) The autoregressive label prediction model (H) shows its importance when considering the sequential dependencies\namong the four digits of the EC number. Disabling this component leads to a decrease in performance, emphasizing the\nvalue of treating the EC number as a hierarchical label rather than a flat label.\n4) Multi-modality inputs also enhance the results, removing any modality leads to a decrease in performance metrics.\nPlease note that MAPred performs better than CLEAN even with sequence-only input, demonstrating the model's\nsuperiority.\n4.4 MAPred Learns the Functional Regions of Enzymes\nMAPred can classify enzymes based on their EC numbers by utilizing the extraction of latent features from the amino\nacid sequences of enzymes. To assess whether MAPred has learned to identify the specific functional regions of\nenzymes, we analyzed the attention scores computed in the attention layer and visualized them on the three-dimensional\nstructure of the enzyme, with the visualization depicted in Figure 4.\nIn Figure 4(A) and 4(B), the highlighted regions reveal that MAPred primarily concentrates on the catalytic sites where\nreactions occur. Specifically, Figure 4(A) shows attention to the polymerase reaction site, and Figure 4(B) highlights\nwhere the substrate binds and the reaction happens. Figure 4(C) illustrates the Histone-lysine N-methyltransferase PRD\nprotein, characterized by a large part of disordered regions lacking specific functions. Here, the highlighted residues\nare predominantly and correctly located in the catalytic domain and near the substrate. These examples suggest that\nMAPred can identify potential reaction sites based on its attention scores, potentially influencing its predictions."}, {"title": "Conclusion", "content": "Our study presents MAPred, a novel approach to enzyme function prediction that integrates both sequence and structural\ndata. The model's performance, as demonstrated through rigorous testing, surpasses existing methods, and achieves\n0.610, 0.493, and 0.680 F1 scores on New-392, Price, and New-815 test sets, respectively. Our extensive ablation studies\nfurther validate the effectiveness of each component within MAPred, proving the effectiveness of our design. Future\nWork will focus on the excavation of key protein regions and the incorporation of more diverse datasets, expanding the\nprediction of labels to the Gene Ontology (GO) terms."}]}