{"title": "KNOW WHERE YOU'RE UNCERTAIN WHEN PLANNING WITH MULTIMODAL FOUNDATION MODELS: A FORMAL FRAMEWORK", "authors": ["Neel P. Bhatt", "Yunhao Yang", "Rohan Siva", "Daniel Milan", "Ufuk Topcu", "Zhangyang Wang"], "abstract": "Multimodal foundation models offer a promising framework for robotic perception and planning by processing sensory inputs to generate actionable plans. However, addressing uncertainty in both perception (sensory interpretation) and decision-making (plan generation) remains a critical challenge for ensuring task reliability. We present a comprehensive framework to disentangle, quantify, and mitigate these two forms of uncertainty. We first introduce a framework for uncertainty disentanglement, isolating perception uncertainty arising from limitations in visual understanding and decision uncertainty relating to the robustness of generated plans.\nTo quantify each type of uncertainty, we propose methods tailored to the unique properties of perception and decision-making: we use conformal prediction to calibrate perception uncertainty and introduce Formal-Methods-Driven Prediction (FMDP) to quantify decision uncertainty, leveraging formal verification techniques for the-oretical guarantees. Building on this quantification, we implement two targeted intervention mechanisms: an active sensing process that dynamically re-observes high-uncertainty scenes to enhance visual input quality and an automated refinement procedure that fine-tunes the model on high-certainty data, improving its capability to meet task specifications. Empirical validation in real-world and simulated robotic tasks demonstrates that our uncertainty disentanglement framework reduces variability by up to 40% and enhances task success rates by 5% compared to baselines. These improvements are attributed to the combined effect of both interventions and highlight the importance of uncertainty disentanglement which facilitates targeted interventions that enhance the robustness and reliability of autonomous systems. Fine-tuned models, code, and datasets are available here.", "sections": [{"title": "1 INTRODUCTION", "content": "Accurate perception and the generation of actionable plans are essential for effective robotic perception and planning (Song et al., 2023; Shah et al., 2022; Yang et al., 2023; 2022; Ichter et al., 2022). Recent advancements in multi-modal foundation models have equipped robots with the ability to process visual data and generate corresponding textual plans (Wu et al., 2023; Gu et al., 2023; Mu et al., 2023). While multimodal models provide integrated per-ception and planning capabilities, these two components - perception and decision-making \u2013 each contribute dis-tinct forms of uncertainty that affect overall performance. Without disentangling these sources, any observed failures or inconsistencies in model outputs remain difficult to diag-nose, as it is unclear whether they stem from inaccuracies in visual perception or limitations in planning.\nDisentangling uncertainty allows for targeted improvements: enhancing perception uncertainty focuses on refining sen-"}, {"title": "1.1 Related Work", "content": "Multimodal foundation models, such as GPT-4 (Achiam et al., 2023) and LLaVA (Liu et al., 2023b;a), have advanced robotic systems by jointly processing text and images to gen-erate actionable plans. Existing uncertainty quantification methods (Ye et al., 2023; Vazhentsev et al., 2022; Shen et al., 2021; Ren et al., 2023; Hori et al., 2023) treat these models as monolithic, providing an aggregate measure of uncertainty without distinguishing between perception and decision components. This results in a \"black box\" eval-uation, where any assessment of task failure or risk lacks insight into whether uncertainty originates from perception limitations or decision-making flaws.\nSuch aggregate uncertainty scores complicate model refine-ment since they obscure the root cause of performance is-sues, hindering targeted improvements (Rudner et al., 2024; Yu et al., 2022; Jang et al., 2024). For instance, while log-likelihood-based uncertainty estimation methods (Srivastava et al., 2022; Hendrycks et al., 2020) use conformal predic-tion techniques (Vovk et al., 2005; Angelopoulos et al., 2023; Balasubramanian et al., 2014) to calibrate confidence in plan correctness, they often rely on human-labeled ground truth for accuracy. This dependence can limit scalability and adaptability in dynamic environments.\nOur framework addresses these limitations by disentangling uncertainty into perception and decision components, en-abling targeted interventions to reduce each type of un-certainty independently. The proposed FMDP estimates decision uncertainty without requiring extensive human la-beling. This disentanglement and targeted quantification sig-nificantly enhance model robustness and scalability through targeted uncertainty mitigation."}, {"title": "2 PRELIMINARY BACKGROUND", "content": "Formal Methods in Decision-Making Formal meth-ods provide tools for modeling, analyzing, and verifying decision-making problems represented mathematically. By representing model outputs in a symbolic representation such as a Kripke structure, A, these methods provide formal verification of the model's output against task specifications (Browne et al., 1988).\nWe use temporal logic to express the task specifications. A temporal logic specification, $, constrains the temporal or-dering and logical relationship between sequences of events and actions. Subsequently, we perform model checking, a process to determine whether the Kripke structure satis-fies the specification, denoted as A |= $, which provides a formal guarantee associated with this verification.\nConformal Prediction Through the theory of conformal prediction, we can estimate the uncertainty of model predic-"}, {"title": "3 PERCEPTION UNCERTAINTY", "content": "A multimodal foundation model, M : I \u00d7 T \u2192 T \u00d7 [0, 1], takes an image observation I in the image space I, and a task description Pr in the text space T as inputs. It then produces a textual instruction T\u2208T for the given"}, {"title": "3.1 Conformal Confidence-Uncertainty Calibration", "content": "As ad-hoc confidence scores returned by machine learning models, such as softmax, do not accurately reflect predic-tion accuracies (Guo et al., 2017), it is essential to calibrate confidence scores to prediction accuracies for uncertainty estimation. However, we ask for a theoretical lower bound on the accuracy according to Def. 1, which existing calibra-tion methods cannot provide (Guo et al., 2017; Ding et al., 2021). Conformal prediction provides confidence intervals or sets for predictions guaranteeing a specified error rate (Vovk et al., 2005). We leverage the theory of conformal prediction for confidence-uncertainty calibration.\nWe first learn a projection head H : Rd \u2192 Rk that maps image embeddings to a k-dimensional space where vector v \u2208 Rk represents the softmax confidence scores over k object classes. We denote the confidence score for class l\u2208 [0,k] by v\u03b9. The class associated with the highest confidence score is the model's prediction.\nConsider a calibration set {Ii, Yi}=1, where I\u00bf \u2208 I and Yi \u2208 [0, k] is the true class label for the object of interest in I\u2081, and assume the calibration set is i.i.d. with the images we from evaluation tasks. We connect the vision encoder to the projection head H\u3002V:1 \u2192 Rk and obtain a set of nonconformity scores Snc = {1 \u2212 H(V(Ii))yi }=1\u00b7 Intuitively, a nonconformity score is the sum of the softmax confidences of wrong predictions.\nWe empirically estimate a probability density function (PDF) fnc from the nonconformity scores. For an image In+1 from the test task (beyond the calibration set), given an error bound \u20ac, we can find corresponding confidence c* such that \u0108(In+1) = {l : H(V(In+1))1 > 1 c*} satisfies P [Yn+1 \u20ac \u0108(In+1)] \u2265 1 -\u2013 \u0454 (Vovk et al., 2005).\nConversely, given a c*, we can compute the corresponding error bound \u20ac = 1- So fnc(x)dx. Given a confidence vector v \u2208 Rk of image In+1, let c* = 1 \u2212 sort(v)-2,"}, {"title": "4 DECISION UNCERTAINTY", "content": "Existing frameworks depend on human-annotated datasets to assess how well a generated plan aligns with human anno-tations, limiting their scalability (Malinin et al., 2017; Jiang et al., 2021). These approaches are limited in their ability to adapt to specific task requirements, resulting in less effective uncertainty estimates when applied to diverse or specialized scenarios (Ren et al., 2023). We use formal verification techniques alongside conformal prediction to eliminate the"}, {"title": "4.1 Formal-Methods-Driven Prediction", "content": "We present formal-methods-driven prediction (FMDP), which uses formal verification techniques alongside confor-mal prediction, to solve Problem 2. For each task description PT and input image I, FMDP aims to find a prediction band"}, {"title": "4.2 Estimating Decision Uncertainty Score", "content": "In this section, we use FMDP to estimate a decision uncer-tainty score for each output from the foundation model, i.e., solve Problem 2.\nGiven a new image In+1 and a task description PT+1, the foundation model outputs an instruction Tn+1 and a con-fidence Cn+1. We disregard the instructions whose confi-dence in satisfying the specifications is below 0.5. Then, we compute the decision uncertainty score ud using the nonconformity distribution fnc:"}, {"title": "5 FROM UNCERTAINTY DISENTANGLEMENT TO INTERVENTION", "content": "Building on the uncertainty disentanglement, we next present two key interventions, Active Sensing and Auto-mated Refinement, designed to reduce these uncertainties during model deployment. The active sensing mecha-nism leverages perception uncertainty scores to dynami-cally guide re-observations, ensuring propagation of high-confidence visual inputs for subsequent plan generation."}, {"title": "5.1 Active Sensing with Perception Uncertainty", "content": "We incorporate perception uncertainty scores into an active sensing process, using these scores as indicators to initiate re-observation when necessary. This approach allows as-sessment of whether the observed image meets a predefined certainty threshold, enhancing the reliability of perception outcomes and the subsequent text generation step."}, {"title": "5.2 Automated Refinement Using Both Uncertainties", "content": "However, even with precise visual inputs, foundation models may still produce instructions that fail to meet task speci-fications due to a lack of domain knowledge. To address this, we introduce an automated refinement process aimed at minimizing the likelihood of generating such specification-violating instructions.\nConsider an image set containing task environment images and a task bank of text-based task descriptions. This data can be collected from high-fidelity simulations or real task executions. The proposed automated refinement procedure is outlined in Algorithm 2. In this process, we filter out images with high perception uncertainty, use the foundation model to generate instructions for each image-task descrip-tion pair, verify the instructions against task specifications, and add only the valid instructions to the fine-tuning dataset.\nThe model is then fine-tuned in a supervised manner, where each image-task description pair serves as input, and the instruction is the expected output. If supervised fine-tuning is unavailable, we can generate positive and negative instruc-tion pairs and apply direct preference optimization (DPO) (Rafailov et al., 2024). Positive instructions are those whose automaton representations satisfy all specifications. Figure 2 visually demonstrates this fine-tuning framework. This ap-proach eliminates the need for human annotations or labels, allowing the model to be refined with virtually unlimited data samples until it reaches the desired performance."}, {"title": "6 EXPERIMENTS", "content": "We perform simulated and real-world autonomous driv-ing experiments to demonstrate (1) how we disentangle and quantify perception and decision uncertainty in driving tasks; (2) how the proposed active sensing mechanism en-hances visual input quality and reduces perceptual errors; and (3) how the proposed uncertainty-aware automated fine-tuning framework refines the model to improve its capa-bility to meet task specifications. In our experiments, we use LLaVA-1.5-7B (Liu et al., 2023a) for perception and plan generation; it consists of a CLIP-L-336px vision en-coder (Radford et al., 2021) and a Vicuna-7B-v1.5 language generator (Zheng et al., 2023)."}, {"title": "6.1 Uncertainty Disentanglement and Quantification", "content": "Perception Uncertainty: We collect 542 images with ground truth labels from the Carla simulator forming a cal-ibration set. Then, we apply CLIP with a pre-trained pro-jection head to classify objects such as stop signs, cars, and pedestrians. We classify a total of 3000 objects within the 542 images and collect their confidence scores. Then, we obtain a nonconformity distribution, as shown in Fig. 3, following the procedure outlined in Sec. 3.1.\nDecision Uncertainty: We collect a calibration set contain-ing 400 images from the Carla simulator each associated with a task description\u2014to obtain another nonconformity distribution (Fig. 3) using the procedure in Sec. 4.1 and 4.2.\nFor each image, we randomly select a task description from a set of tasks {go straight, turn left, turn right, make a U-turn} at the {traffic light, stop sign}. We define 10 safety-critical logical specifications for the driving tasks, some of which are listed below:"}, {"title": "6.2 Targeted Intervention 1: Active Sensing", "content": "We deploy the active sensing mechanism to the planning pipeline that uses a raw foundation model without fine-tuning, as described in Sec. 5.1. We set the threshold tp = 0.7, i.e., a 70% percent probability (and above) of correct visual interpretation is acceptable. On the simulated data, we observe a 35% reduction in decision uncertainty. Across a test set of 50 driving scenes from our ground robot"}, {"title": "6.3 Targeted Intervention 2: Automated Refinement", "content": "We investigate a complementary intervention that we cas-cade on top of active sensing, building upon its benefits. To improve the foundation model's capability to generate plans satisfying specifications, we apply our automated re-finement framework to fine-tune the foundation model for autonomous driving tasks using a train set containing 800 images from the Carla simulator and the aforementioned 10 pre-defined logical specifications. We follow the steps outlined in Sec. 5.2 to automatically generate refinement data and fine-tune the foundation model's text generator.\nBenchmark: To showcase the necessity of our uncertainty disentanglement framework, we select a refinement bench-mark (Yang et al., 2024) that fine-tunes the same model using the dataset without uncertainty disentanglement.\nThis framework works as follows: (1) Create an empty set of fine-tuning data, (2) select an image with a task descrip-tion and query the foundation model for a plan, (3) build automaton-based representation for the plan and verify it against the specifications, (4) add the plan to the set of fine-tuning data (without uncertainty disentanglement) if it passes all the specifications, (5) repeat step 1-5 until suffi-cient data is obtained, and (6) freeze the vision encoder and fine-tune the model.\nIn contrast to the benchmark, our framework first quantifies perception uncertainty and filters out high-uncertainty data, hence enhancing data quality."}, {"title": "7 CONCLUSION", "content": "We presented a novel framework for enhancing multimodal foundation models in robotic planning. Our framework disentangles perception uncertainty in visual interpretation and decision uncertainty in plan generation. The proposed quantification methods, leverage conformal prediction for perception and FMDP for decision-making. The targeted interventions improve model robustness by employing ac-tive sensing and automated refinement. Empirical results from both real and simulated robotic tasks show that our framework reduces performance variability and improves task success rates, attesting to the value of distinct uncer-tainty interventions. Future work will extend this approach by considering additional uncertainty types and exploring broader interventions such as task description optimization, paving the way for robust and reliable autonomous systems."}, {"title": "A ADDITIONAL BACKGROUND", "content": "Confidence Scores A deep learning model produces an output with a confidence score c\u2208 [0, 1]. The resulting confidence score represents the likelihood of the output being \u201ccorrect\" with respect to the ground truth labels in the training data. Foundation models such as GPT-series and LLaMA-series produce a distribution of outputs, sample one output from the distribution, and return the sampled output to users. Each output within the distribution is associated with a softmax confidence score indicating its probability of being sampled."}, {"title": "B DEMONSTRATION OF ALGORITHM 1", "content": "Given a set of atomic propositions AP = { green light, red light, car, pedestrian, wait, turn left,...}. The robot agent sends an image as presented in Fig. 9 with objects Y = {car, truck} and a task description \"turn left\" as inputs to the foundation model and obtains the instruction T:"}, {"title": "C ADDITIONAL EXPERIMENTAL DETAILS AND RESULTS", "content": "C.1 Task Description and Sample Response\nDuring plan generation, we provide a set of examples to the foundation model to enforce its output structure. An example is presented below, where the blue texts represent the inputs to the foundation model, and the red texts represent the outputs generated from the foundation model."}, {"title": "C.2 Autonomous Driving Task Specifications", "content": "To get the nonconformity distribution for decision uncertainty, we manually generate a set I of specifications regarding the driving rules. Next, we transform the textual decisions outputted from the foundation model to a Kripke structure through Algorithm 1. Then, we use a model checker to verify the Kripke structure against each specification & \u2208 \u03a6. In the experiment, the set of specifications I contains"}, {"title": "C.3 Sim2Real Transfer", "content": "Our uncertainty estimation framework assumes that the distribution of the calibration set is identically distributed with the test data distribution (Assumption 1).\nWe empirically demonstrate that the data (images) collected from the Carla simulator and the images collected from the real world are identically distributed. Hence Assumption 1 holds. Therefore, the uncertainty score from the nonconformity distribution bounds the probability of correctly classifying objects in the environment.\nWe collect the confidence scores of the correct predictions from CLIP for the two sets of images and use a Q-Q plot that shows that the two sets of images are identically distributed. The Q-Q plot (Gnanadesikan & Wilk, 1968) compares the distribution of the confidence scores for the two datasets by plotting their quantiles against each other. The closer the scatter"}]}