{"title": "Generative AI Uses and Risks for Knowledge Workers in a Science Organization", "authors": ["Kelly B. Wagman", "Matthew T. Dearing", "Marshini Chetty"], "abstract": "Generative AI could enhance scientific discovery by supporting knowledge workers in science organizations. However, the real-world applications and perceived concerns of generative AI use in these organizations are uncertain. In this paper, we report on a collaborative study with a US national laboratory with employees spanning Science and Operations about their use of generative Al tools. We surveyed 66 employees, interviewed a subset (N=22), and measured early adoption of an internal generative AI interface called Argo lab-wide. We have four findings: (1) Argo usage data shows small but increasing use by Science and Operations employees; Common current and envisioned use cases for generative Al in this context conceptually fall into either a (2) copilot or (3) workflow agent modality; and (4) Concerns include sensitive data security, academic publishing, and job impacts. Based on our findings, we make recommendations for generative AI use in science and other organizations.", "sections": [{"title": "1 Introduction", "content": "Generative AI, as a new core technology, has demonstrated significant potential to enhance scientific discovery by supporting knowledge workers in science organizations and institutions (e.g., government and industry research labs, universities, think tanks) [1, 49, 50]. However, the real-world applications and perceived risks of generative AI use in these organizations are uncertain. If generative AI could make science organizations with science- and operations-focused employees more efficient and speed up time to discovery on topics such as drug development and climate solutions, it would have important consequences for facilitating scientific gains to help society at large.\nPrior literature has looked at generative AI as a tool for scientific research, including the development of science-specific large language models [50, 55] and driving complex scientific tasks [10, 26, 39, 45]. This research, however, focuses on science research tasks and does not study the science workplace more broadly. Another area of literature looks at generative AI in the workplace, with a particular focus on professional knowledge workers [3, 12, 16, 21, 43, 48, 59]. Very little research, however, studies generative AI opportunities for scientists as knowledge workers [30], and to the best of our knowledge prior research has not studied generative Al for both science- and operations-focused workers in a science organization. In addition, prior work has investigated generative Al risks and concerns (e.g., [12, 30, 34, 37]), however, there has not been a focus on concerns that are specific to the novel context of a science organization.\nIn this paper, we add to the literature by reporting on an investigation of the practical, real-world applications and perceived risks of generative AI use (focusing predominately on large language models as opposed to image-generating models) across a multidisciplinary science and engineering research center with the primary mission to deliver scientific progress. To conduct our study, we collaborated with an Information Technology (IT) group at a US national laboratory.\u00b9 The national lab, Argonne, we worked with employs several thousand people and is tasked with basic and applied science and engineering research. The lab includes Science divisions working to publish academic papers in research areas such as climate science, materials science, high performance computing, and more. Science teams also include software engineers, data scientists, and engineers who specialize in building and operating scientific experiments. In addition, the lab has multiple Operations divisions that employ people in areas such as IT, Human Resources (HR), Finance, Communications, Facilities (e.g., grounds crew and building maintenance), the Fire Department, and Security. We studied generative AI perceptions and uses for employees spanning Science and Operations roles and measured early adopter usage for a recently released internal generative AI interface called Argo based on a private instance\u00b2 of OpenAI's GPT-3.5 Turbo large language model (LLM). We study all usage during the first several months Argo was available, and use the term early adopter to highlight that we study these initial users of the system."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Generative Al in Science", "content": "In recent years, scientists have been exploring how deep learning and generative AI might be useful for advancing scientific research [1, 49, 54]. Some have called for building science-specific large language models (LLMs) trained only on science literature [50, 55]. Another area of interest has been in the use of LLMs to generate science-focused programming code, including calculation kernels executed on HPC systems and parallel code snippets to train LLMs [8, 15, 31]. Generative AI is also being leveraged to create synthetic user research data for Human-Computer Interaction (HCI) research [19, 33] and drive automated process workflows for scientific tasks and experimental equipment [10, 26, 39, 45].\nLittle research exists, however, on how generative Al is impacting the day-to-day aspects of scientific knowledge work. One exception is a study by Morris [30] who interviewed 20 scientists, at a variety of institutions, and identified generative AI opportunities as well as concerns, ranging from literature reviews and data analysis to applications in higher education given many participants' roles as university professors. Our paper differs from Morris' by focusing on scientists at an organization that has a science mission (as opposed to university professors and scientists working in the tech industry). Notably, we include the perspective of Operations workers, investigate organization-level trends, and specifically seek perspectives on privacy and security. Another closely related study did not exclusively focus on generative AI. In this work, Crosby et al. [7] use a human-centered methodology to inform the design of a suite of tools for ocean scientists that leverage ML to process image data. Finally, some work has looked at the ability for LLMs to summarize academic papers for scholars [14, 57] or create data visualizations [29]. We expand on these studies by contributing a comprehensive look at how both Science and Operations staff in a science organization might use generative Al in their work and what concerns they have."}, {"title": "2.2 Generative AI in Knowledge Work", "content": "Researchers have also begun investigating the use of generative AI in knowledge work, a classification of labor involving the production of information-driven products and services as the key economic output [43, 59]. Some professions considered knowledge work include data science, law, marketing, and finance [43, 59]. Given generative Al's ability to process text-based information, there has been growing interest in how this technology will impact the jobs of professional knowledge workers [3, 12, 16, 21, 27, 28, 43, 48, 59].\nOne thread of research looks to measure the productivity gains by knowledge workers who have access to generative AI tools [20]. Multiple studies have found that generative AI tools increase productivity more for less skilled workers [2, 32], including for workers in consulting [9] and programming [6, 22, 36]. Other research on generative AI applications in professional settings has largely focused on creative knowledge work in journalism and science writing [23, 38], user experience (UX) and industrial design [56], marketing and public relations [53], and software engineering [6, 42]. Another research direction has investigated creative knowledge work tasks (rather than professions) that include writing [13, 25] or music composition [51]. We contribute to the literature on generative AI in knowledge work by focusing on science as a sub-domain that remains understudied.\nIn addition to a lack of research on scientific knowledge work, there is also minimal literature in the HCI and Computer Supported Cooperative Work and Social Computing (CSCW) communities studying generative AI at the organizational level. This oversight is significant because there can be networked impacts to organizations (both positive and negative) that might not be felt by individual users. For example, Corti\u00f1as-Lorenzo et al. [5] think critically about how enterprise knowledge systems accessed via AI have the ability to shape and distort how workers view themselves and others, which in turn changes worker behavior. One scenario they describe is a system that allows workers to search for \u201cexperts\u201d on a topic in their organization. By labeling some employees experts and others not the system might inadvertently inform who gets promoted or increase workload on certain staff, making it imperative organizations make these systems transparent to workers. We contribute an in-depth case study of a single organization's use of generative AI for knowledge work, considering the interplay between roles (Science and Operations) within the organization as well as risks at the organizational level."}, {"title": "2.3 Generative AI Concerns in the Workplace", "content": "Alongside the search for generative AI applications in the workplace, researchers have uncovered risks, harms, and concerns relevant for knowledge workers. One of the most prevalent is the fact that generative AI systems \"hallucinate\u201d false information with suggestive confidence that it is true [12, 24]. These hallucinations, as well as a lack of citations to original sources, can be particularly concerning for scientists [30]. There is also concern about overreliance, or users placing too much trust that a system is working and not checking outputs thoroughly enough [12, 34]. Overreliance is a concern that predates the development of generative AI extending to early automation systems, such as automated landing systems for airplanes [46]. Thus, researchers are considering how to design for the appropriate level of trust users should put in a system [58].\nPrivacy and security as well as copyright and plagiarism are also work-related generative Al concerns. Some research has found that programmers who used a generative AI coding assistant were more likely to write insecure code [35, 37]. Additionally, significant controversy surrounds the use of writing [41], music [4], and visual art [18, 47] to train generative AI, due to the lack of compensation for artists and authors as well as the ability for the models to regurgitate some training data verbatim triggering copyright laws.\nAnother important issue drawing commentary from both researchers and the public is the extent to which generative AI will impact workers and industries, particularly risks to knowledge workers' jobs [16, 27, 59]. For instance, Woodruff et al. [59] interviewed knowledge workers in several industries (their sample did not include scientists) about how they thought generative AI might change their field. They found that while participants did not foresee major changes to their industries, there was concern for the rise of deskilling, dehumanization, disconnection, and disinformation. Other research found workers can feel inferior or unaccomplished if generative Al models can largely do their tasks for them [24]. While the spectre of fully autonomous systems has loomed for many industries going back to the Industrial Revolution, there have always remained some aspects of a job that machines cannot do [17]. In this paper, we expand on some of these risks and concerns while highlighting how they surface in unique ways both in a science context as well in an organizational context."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Research Context", "content": "In January 2024, the IT group at Argonne National Lab broadly deployed a generative AI chatbot named Argo, powered by a private instance of OpenAI's GPT-3.5 Turbo,\u00b3 for use by all members of the Argonne National Lab community. Argo was designed for exclusive internal lab use so it does not save query and LLM response data and it does not share such information with OpenAI or other third-party services. Employees could use Argo as they would a service like ChatGPT: it included a browser-based interface that a user could type a prompt into and get a reply. Argo could also be accessed by employees via an API. Argo was intended to be used only for work purposes and required a lab login and VPN to access if not on-site at the lab. Beyond providing a secure generative AI assistant for employees, Argo was not released with a specific purpose. Employees could find out about Argo through official announcements, emails, or events targeted at raising awareness.\nArgonne National Lab is organized into Science and Operations divisions. The goal of Science divisions is to produce scientific research, while the goal of the Operations divisions is to keep the organization running smoothly. Science workers may be operating instruments, running experiments, managing data analysis, writing grants, and publishing papers. Operations workers may be producing science communication, working in administrative roles, ensuring lab safety (e.g. radiation exposure, cybersecurity), and building software and hardware infrastructure. Importantly, while the lab is divided into distinct divisions there is overlap in tasks between them. For example, both divisions employ technical workers such as software engineers. In addition, both divisions require some workers with a scientific background. In this paper, we distinguish between the two divisions to tease out if and when the need for generative Al differs between Science and Operations roles since this is a common division of labor in knowledge work organizations.\nWe conducted a survey (April-June 2024) and interviews (April-July 2024) during the first eight months of Argo's deployment to purposefully capture generative AI perceptions and use levels during this early stage. Our research was approved by the university Institutional Review Board (IRB). Since it is against the law to monetarily compensate federal employees in the course of their work, we did not provide gift cards to participants, but made it clear the research would be used by the lab to improve their systems."}, {"title": "3.2 Data Collection", "content": ""}, {"title": "3.2.1 Survey.", "content": "We first wanted to broadly capture national lab employee perceptions of generative AI uses and concerns in a survey. The survey, created in Qualtrics, included questions such as How familiar are you with large language models (LLMs)?, How often do you use LLMs as part of your work?, and Please describe ethical concerns you have about using LLMs in your work, if any. We also asked How often do you use LLMs for the following work tasks? and provided a list of 15 tasks such as writing code, feedback on experimental design, and editing human-written text. This list of tasks was drawn from a study of 20 scientists about their use of generative AI [30]. We also asked demographics questions. At the end of the survey, we gave participants the option to share their email if they would be willing to sign up for an interview. See the full survey in Appendix A.1.\u2074 We pilot-tested the survey with a small set of HCI experts in Computer Science and refined wording and ordering of the questions.\nAfter finalizing the changes, we deployed the survey to Argonne National Lab employees via mailing lists and at presentations given by our collaborators at the lab.\nWe received a total of 80 survey responses. We filtered out surveys that were incomplete, with the exception of three that only lacked demographic information (given that a valid option was \"Prefer not to answer\" we selected this option for these responses) or short answer responses, but had all the multiple choice questions completed. We also removed one response that was disingenuous based on the answers. After filtering, we were left with 66 responses to analyze."}, {"title": "3.2.2 Interviews.", "content": "As survey responses were completed, we contacted respondents who had provided email addresses for follow-up interviews. In total, we contacted 40 respondents, 22 of whom agreed to participate in semi-structured interviews. Each interview lasted 30 minutes and was conducted over Zoom and recorded. The interview protocol was designed to elicit more depth on generative AI applications, risks, and concerns than the survey. We asked participants to recall current or envisioned scenarios for generative AI, and also prompted them based on their survey responses. We then went in-depth on each scenario, asking questions such as To what extent have LLMs been helpful for this task? and When using an LLM to do this task doesn't work, what goes wrong? In addition, we dug into participant views on privacy, security, and ethics concerns. See our full interview protocol in Appendix A.2."}, {"title": "3.2.3 Argo Usage Statistics.", "content": "Between January and August 2024, we collected high-level metadata during the regular use of Argo including authenticated user name, time of use, LLM selected and related configuration options, and the size of user queries and responses as measured by token counts.\u2075 The text-based content of these queries and responses are not automatically stored by any external or internal database system. Only an Argo user may optionally save their LLM-based conversations to their local machines before disconnecting from their session with the service through a copy-to-clipboard mechanism or a transcript download feature. We roll-up the tracked user name to their organizational division, which is then categorized as either being within the Science or Operations workforce group."}, {"title": "3.3 Participants", "content": "Table 1 shows the demographic information for the survey participants. We received approximately equal numbers of Science and Operations employee responses. Most respondents were scientists or engineers. Respondents had been at the lab for a range of years, including both new and highly seasoned employees. We also represent a range of ages, primarily between 25 and 64 years old. The gender and race/ethnicity breakdown is representative of the lab population. Most (but not all) respondents had at least a Bachelor's degree, and many had advanced degrees.\nTable 2 shows demographic information for interview participants. All interview participants also provided a survey response, and the demographic breakdown is similar. It is skewed toward white men, although this is reflective of the overall national lab population. Throughout the paper, we denote interview participants IDs with \"P\" and survey participant IDs with \u201cS.\u201d"}, {"title": "3.4 Data Analysis", "content": ""}, {"title": "3.4.1 Interviews.", "content": "We had the interviews transcribed using Rev.com under a non-disclosure agreement. We qualitatively coded the interview transcripts and then analyzed them using thematic analysis [44]. First, one researcher read through the transcripts and created an initial codebook based on the interview questions. The team then discussed and refined the codebook and finalized the codes (the codes we analyzed for the final paper are bolded in Table 3, see Table 6 in Appendix A.3 for the full initial codebook). One researcher subsequently coded all the transcripts using MAXQDA. Then, the same member of the research team used axial coding to assign sub-codes through an iterative process to the following parent codes: Current/envisioned use cases for LLMs, Privacy/security concerns at work, and Issues or ethics concerns at work. Another member of the research team then did a second round of coding, reviewing all sub-codes labeled in the first round. All points of disagreement were discussed and resolved between the two coders. The team also met regularly after a set of transcripts were coded to ensure the coding process aligned with the goals of the study. After coding, two members of the research team extracted themes and discussed these with the full team (see themes in Table 3 as well as Table 6 in Appendix A.3) and further refined those that were most related to the research questions."}, {"title": "3.4.2 Survey.", "content": "We calculated descriptive statistics for the survey based on the multiple choice responses using Python. We qualitatively coded the short answer responses in MAXQDA. We began the coding process after extracting themes from the interview data. Thus we used the same codebook for the survey, as shown in Table 3, along with the unique number of survey responses for each code."}, {"title": "4 Findings", "content": "Our findings represent the perspectives of generative Al early adopters at the national lab. In Section 4.1, we describe initial generative Al usage at the lab, showing that less than 10% of all lab employees used Argo each month in the study period but that there is an upward trend in use. This is a lower bound for generative Al usage more broadly because in our surveys, more employees reported trying other commercial LLMs such as ChatGPT.\nDrawing from both the survey short responses and the interviews, we identified common current and envisioned use cases for generative Al and conceptually split these findings into two categories: copilot (Section 4.2) and workflow agent (Section 4.3). We review similarities and differences between Science and Operations workers with respect to the kinds work they want to accomplish in each category. In Section 4.4, we highlight the most common risks and concerns for using generative AI in the national lab that surfaced in both the survey short answers and interviews."}, {"title": "4.1 Generative AI Usage and Familiarity", "content": "We found that early adopters are familiar with generative AI and using it experimentally in their work from our survey data and Argo usage data. Figure 1 shows an upward trend in usage during the early months of launching Argo. While the use period seen in Figure 1 is brief for designating trends, we observed a general increase in generative AI use across Science and Operations during the study period. Specifically, unique users of both groups, excluding the initial launch month, increased an average of 19.2% each month, broadly ranging from approximately no change to as high as nearly 47%. Across this same period, monthly unique users in Science and Operations increased 158% and 200%, respectively (or 174.2% over all users between the second and final months of the reported statistics). While there were 26% more unique monthly Science users on average compared to Operations users, we observe an average monthly increase in the latter group at 21.2% compared to 19.3% for Science users. Overall, we see a suggestive early trend that the availability of a secure internal generative AI solution is of significant interest to Science and Operations users at a national lab, and this data provides a useful baseline for future studies.\nMost of our survey respondents, and all our interview participants, had some degree of familiarity or experimentation with generative AI. Based on the survey short responses and interviews, participants also had a sophisticated understanding of how generative AI models are trained. In our surveys, we asked employees about their familiarity with, and use of, LLMs more broadly. Figure 2 shows that 94% of participants had some level of familiarity with generative AI tools, although only 16.7% felt they were very familiar. Similarly, despite high levels of familiarity, only 28.8% of survey respondents said they agreed or strongly agreed that LLMs had become essential to their workflows (Figure 3). The majority of survey respondents had primarily tried ChatGPT (82%) and Argo (67%) as shown in Table 4. Our survey respondents also did not report a large difference between how much they used LLMs for work and personal tasks as shown in Figure 4.\nFigure 5 shows how often survey respondents used LLMs for a series of tasks in the context of their work. Nearly 60% of respondents had at least tried using LLMs for summarizing literature, editing human-written text, writing code, and formatting writing. While the fewest total respondents (11%) had tried using LLMs for grant writing, we note that one person responded always, suggesting that it might be that respondents want to use LLMs for these tasks but have not figured out how to given their relative complexity. Examples of personal generative AI use that came up un-prompted in the interviews included scripting narratives as part of the game Dungeons and Dragons (P5, P13, P19) and creating furniture arrangements for a floor plan (P12). This data suggests early adopters are in an experimentation phase."}, {"title": "4.2 Science and Operations Employee Copilot", "content": "We categorize the first set of generative AI use cases described by participants in both Science and Operations roles as copilot-style interactions. This means they entail conversational interactions between the user and the Al where the user gets real-time responses to questions posed to the AI. We use the conceptual framing of a copilot in order to arrange our findings around features and affordances this copilot would need to be most useful in a science organization. We note that participants themselves rarely used the term copilot, rather we impose it for conceptual organization of the findings.\nAt the time of data collection, participants said they were most often using LLMs such as ChatGPT to get help writing structured text that they can easily verify is correct, such as emails and reports. Participants envisioned goal, however, was to use a LLM to extract insights from unstructured text data such as scientific literature or survey results. We group Science and Operations employee responses together in this section since we did not find a large difference in use for copilot-style interactions."}, {"title": "4.2.1 Current Uses: Writing Structured Code/Text.", "content": "Survey and interview participants described numerous examples of structured writing that could be aided by LLMs such as creating emails, reports, grants, and the introduction to academic papers. By structured, we mean types of writing that follow standardized formats. Participants described that they typically already had the content needed for the writing, but they used LLMs to craft the the appropriate tone and format. In the survey, over 50% of respondents said they had tried using LLMs for each of the following related tasks: editing human-written text, writing code, formatting writing, and email writing (Figure 5). Moreover, 20% of short answer responses included use cases related to writing structured text or code (Table 3).\nWhen writing emails, participants most commonly described using LLMs to help with tone given a specific audience. P3, a senior IT employee, summarized the feelings of many participants saying that he would provide the LLM a document or paragraph and ask it to, \"make this more formal, make this less formal, make this friendlier\" to reduce his own emotional labor. Many participants described wanting to sound \u201cprofessional.\" For example, P18, an operations manager, said she asks the LLM to \u201cmake this sound better or make this more professional or make this more succinct.", "more formal and less scientific.": 16, "Al writes the email so that I don't have to respond with the emotional outrage I might have.": 12, "a guideline that is like a template\" and he felt that \"the AI can prompt you and make sure that you're giving it all of the paragraphs that it needs and then have it summarize the report for you.": 2}, {"title": "4.2.2 Envisioned Uses: Extracting Insights from Large Unstructured Text Data.", "content": "As science-focused knowledge workers, national lab employees must process significant quantities of unstructured textual data, such as scientific literature or organization regulations. At the time of the study, employees were hesitant to trust generative Al with extracting insights due to fears of hallucinations and reliability, which we return to in the Section 4.4.1. If these issues were resolved, however, we found a strong desire among participants to be able to get help from generative AI with managing, organizing, and learning from designated data sources. In the survey, 60% of respondents said they had at least tried summarizing literature (Figure 5) and 20% of survey free responses mentioned use cases related to querying unstructured text data (Table 3).\nMultiple interview participants envisioned interacting with unstructured data in a conversational manner. In a representative example, P3, a senior Operations employee responsible for anticipating the software needs of scientists, said \u201cI think the game-changing aspects would be in interactive conversational simulation modeling.", "have brainstorming sessions with the system itself, almost replicating the type of thing that happens in colloquia or in focus discussion groups, but making one of those partners an LLM system.": "n this section, we cover the types of text data participants were most interested in.\nScientific Literature: Scientists, in particular, wanted a tool to help search and summarize scientific literature. P11, a scientist, made a representative comment: \"And like every other academic in the world, we have to do a lot of lit reviews, and that's hugely time intensive and it's just tedious\" and he envisioned that once LLMs became more accurate they could \"get each paper down to a set of bullet points, that could be a huge time saving.", "Because nowadays, literature, there's too much literature, too many papers. So that could be very helpful. Maybe it can go out, go to other sources, and then in a particular field, summarize, this week, what happened in this field?\"\nPublic Data Sources: Lab employees who were not directly involved in writing research papers were also interested in the ability to mine public data sources to extract insights. For example, P4, an Operations employee, wanted to be able to track broader trends online to help with security-related tasks. He described his ideal tool saying that he wanted to be able to create \"a customized library where we can add resources to that library and then basically ping those things [with a LLM].": 4, "Data": "At the team level, participants wanted the ability to extract key points and notes from meeting transcripts. P10, an IT employee, said she helped process employee requests for approval to use third party tools and that many of these requests were tools that, \"would transcribe the text of the meeting, but it would also create brief notes or overview outlines of what meetings were about and takeaways from that.\u201d P9, a lab safety manager, echoed this desire and explained that he was part of a weekly supervisors meeting and he wanted to be able to store and query the meeting transcript for summaries, context on a topic discussed, action items, and other similar questions. Some teams had surveys that they wanted to analyze, for example, P8, an Operations manager, described a situation where her team collected 313 responses from the lab about a safety incident and they used Argo to help identify themes.\nOrganization Data: At the organizational level, many participants were eager to be able to more easily search lab-wide rules and regulations, including everything from vacation policies to science lab safety standards. P19, a scientist, had a representative perspective saying, \"We have a lot of procedures and documents and policies and all that stuff that's spread all over the place. I'd love to see [LLMs] be used for that.", "included": ""}, {"title": "4.3 Science and Operations Workflow Agents", "content": "We categorize the second set of generative AI use cases described by participants as workflow agents. As opposed to a copilot, an agent navigates a complex task autonomously or semi-autonomously and returns the output to the user. In the context of a science organization, we found agents were emerging as a way of driving workflows in both Science and Operations. Scientific workflows included steps such as downloading data from an instrument or database, running multiple data analysis steps, and producing graphs or other visualizations. Operations workflows included tracking if work is progressing on-time, managing procurement processes, and automating common database interactions.\nTasks related to workflow agents that survey respondents had tried included: analyzing data (43%); merging, cleaning, formatting data (35%); figure, graph generation (27%); creating synthetic data (21%); and labeling data (21%). In the survey free responses, 27% of answers (equally split between Science and Operations respondents) mentioned workflow automation as a use case for generative AI."}, {"title": "4.3.1 Current Uses: Initial Steps Towards Workflow Agents in Science and Operations.", "content": "Participants in both Science and Operations described cases where they were testing using LLMs to automate some of their workflow. Participants reported LLMs are already able to automate some of these workflows in a scientific environment to make them more efficient, but generative Al workflow agents are in early stages of development and use.\nScience workflows: Multiple participants described how they were beginning to use LLMs to automate their customized scientific workflows. P21, a scientist, described how he is starting to use LLMs to automate getting results from a specialized instrument. His research uses lensless imaging, in other words, rather than take a picture with optics \"you can do lensless imaging where you... just record how is the light or the x-ray scattered from the sample.\" In order for the data to be useful, P21 explained that researchers use an algorithm to reconstruct the data to create an image, but this is a complex and compute-intensive process that requires \"experts that really massage the data and tune parameters in order to make this work well.", "Think about Adobe Illustrator or Photoshop, right? Let's assume you've got a picture of someone and the background isn't great... So yes, you can play with Photoshop until you get that image about right, but with these large language models... where you can interact with the chatbot and say well can you sharpen the image or change the tone or whatever it is that you want.": "hile the technology is not fully functional yet, he sees a future where researchers can similarly ask for a scientific", "image": "o be sharpened without manually fine-tuning every parameter.\nP14 is a data scientist on the same Science team as P21 where the research used lensless imaging; he explained that he helped researchers use computational imaging techniques and that he was building an automated data analysis workflow using LLMs. He said that part of his data analysis process involved setting over 20 parameters in a script, and even though he wrote the script he still forgot the names of some of the variables, so he set up an LLM to \"convert natural language [parameter names] to Python code or MetaLab code, which directly feeds to the computer for actual data analysis.", "essentially a bunch of text files... a summary of my past experience with different techniques.... whenever I have new data coming in, I want to use the workflow for the new data. And as I process the new data, I pay more attention on my thought process. So in the end I summarize them into the knowledge file.": 14, "actually more useful to inexperienced users or people who just started to learn the technique.": "hile P14 was optimistic about the ability for LLMs to automate large swaths of his workflow, he said they were still limited to \"narrow environments.", "workflows": "At this point in time, only the more technically-oriented Operations employees had actually tested creating agent-driven workflows. P1, a safety expert, said he was automating workflows for his group, including building \"an application for performing instrument checks in the morning.", "a lot of the [instrument checking] software involves these really old scripting-based inputs from the 1960s, no one has a helpful GUI anymore.": "o, he designed a workflow agent where he could \"select"}]}