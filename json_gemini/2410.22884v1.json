{"title": "Stealing User Prompts from Mixture of Experts", "authors": ["Itay Yona", "Ilia Shumailov", "Jamie Hayes", "Nicholas Carlini"], "abstract": "Mixture-of-Experts (MoE) models improve the efficiency and scalability of dense language models by routing each token to a small number of experts in each layer. In this paper, we show how an adversary that can arrange for their queries to appear in the same batch of examples as a victim's queries can exploit Expert-Choice-Routing to fully disclose a victim's prompt. We successfully demonstrate the effectiveness of this attack on a two-layer Mixtral model, exploiting the tie-handling behavior of the torch.topk CUDA implementation. Our results show that we can extract the entire prompt using O(VM2) queries (with vocabulary size V and prompt length M) or 100 queries on average per token in the setting we consider. This is the first attack to exploit architectural flaws for the purpose of extracting user prompts, introducing a new class of LLM vulnerabilities.", "sections": [{"title": "1. Introduction", "content": "Mixture-of-Experts (MoE) architectures have become increasingly important for large language models (LLMs) to handle growing computational demands (Du et al., 2022; Fedus et al., 2022; Riquelme et al., 2021; Shazeer et al., 2017). This approach distributes the processing workload at each layer across multiple \"expert\" modules, allowing the model to selectively activate only the necessary experts for a given input. This selective activation improves efficiency and enables the development of larger, more capable LLMs. However, this approach can also introduce new vulnerabilities."}, {"title": "2. Background", "content": null}, {"title": "2.1. Primer on Language Models and Mixture-of-Experts", "content": "A Transformer-based Large Language Model (LLM) is a function $f_{\\theta} : V^L \\rightarrow P(V)$ that takes as input a sequence of tokens from a vocabulary V and outputs a probability distribution over the vocabulary, P(V). In particular, we are interested in functions of the form $f_{\\theta}(z) = softmax(W\\cdot h_{\\theta}(z))$, where W is an unembedding matrix and $W\\cdot h_{\\theta}(z)$ gives a set of logits over V.\nWe assume that the model $h_{\\theta}$ consists of multiple MoE layers. An MoE layer consists of N expert functions ${e_1, e_2, . . ., e_n }$ where $e_i : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ is a feed forward layer that takes in d-dimensional token representations and outputs new features of the same dimensionality. The MoE layer also consists of a gating function $g : \\mathbb{R}^d \\rightarrow \\mathbb{R}^N$ which is used to assign token representations to experts by outputting a probability distribution over the N experts.\nLLMs commonly process batches of inputs to improve hardware utilization and efficiency. This means $f_{\\theta}$ in reality operates on the domain $V^{B\\times L}$, where B is the batch size and L is the sequence length of an input. For models that do not use MoE layers, the computation is entirely parallel over a batch of inputs; the computations of one input in the batch cannot affect the computations of another input in"}, {"title": "2.2. Threat Model", "content": "Having described the mechanism of token dropping in MoE models, we now define the threat model for our attack. This model, though simplified, represents a critical first step in understanding a new class of vulnerabilities in MoE systems. By establishing how token dropping can be exploited to leak user information, we aim to encourage future research into other potential attacks arising from design choices optimized for efficiency.\nWe make the following three simplifying assumptions. First, we assume that the adversary has white-box access to the model that uses an MoE with cross-batch Expert-Choice-Routing strategy (Zhou et al., 2024). This can apply in a setting where a third party is using the base model that is available publicly, e.g. implementation is available through t5x (Roberts et al., 2022).\nSecond, the adversary can control the placement of its and the user inputs in the batch. Third, the adversary can query the model repeatedly, ensuring that the user-supplied input is consistently in the same batch as its own inputs; the adversary and user inputs are always batched together and sent to the model for processing.\nWhile the current attack requires strong assumptions, future work could explore techniques to relax these requirements. This might involve investigating methods to influence batch composition through timing attacks or through features in the model's serving infrastructure. We defer a more detailed discussion of the practicalities of the attack and potential methodological improvements to Section 5."}, {"title": "3. MoE Tiebreak Leakage Attack", "content": "We now proceed to describe the information leakage vulnerability (Section 3.1), the attack primitives (Section 3.2), and two variants of our attack:\n\u2022 Oracle Attack: Confirms whether a candidate prompt matches the victim's using only two queries."}, {"title": "3.1. Information-Leakage Vulnerability", "content": "The vulnerability arises when a target token we aim to extract falls precisely at the boundary of an expert's capacity. By strategically submitting a \u201cguess\u201d token, we can influence the model's routing decisions to reveal whether our guess is correct.\nIncorrect guess: The model's routing remains consistent for both our guess and the target, irrespective of their order in the input batch.\nCorrect guess: The model now perceives them as equivalent. Due to the expert's capacity constraint, their order in the batch becomes the deciding factor for processing. This behavioral discrepancy manifests as an observable difference in the model's output."}, {"title": "3.2. Attack Primitives", "content": "To carry out the attack, we rely on three key primitives:\n\u2022 Controlling Expert Capacity: The adversary extends the expert buffer capacity by including a padding sequence - a long, arbitrary sequence in the adversarial batch. This ensures that the victim's tokens are not dropped by default and enables predictable tie-breaking behavior.\n\u2022 Controlling Target Token Placement: The adversary uses pre-computed blocking sequences - sequences of tokens with high affinity for a specific expert - to fill the expert's buffer up to a desired position, leaving a single spot for the target token.\n\u2022 Recovering Target Token Routing Path: In models with multiple MoE layers, the adversary needs to recover the routing path of the target token to accurately interpret the effects of token"}, {"title": "3.3. Leakage Attack", "content": "The adversary strategically crafts a batch of inputs, referred to as the adversarial batch, to manipulate the expert routing within the MoE model. This manipulation forces the model to drop specific tokens from the victim's input, creating a detectable pattern that reveals information about the victim's prompt.\nWe detail the attack procedure in Appendix B. The steps involved are as follows:\n1. Step 1: Guess the Next Token and its Position: The attacker guesses the target token and its position in a chosen expert's buffer, assuming the prefix is known (initially empty).\n2. Step 2: Construct the Adversarial Batch: As illustrated in Figure 3 and using the primitives mentioned in Section 3.2, the attacker crafts an adversarial batch that:\n(a) Places blocking sequences to fill the expert buffer, leaving one spot for the guessed token;\n(b) Includes the probe sequence with the known prefix and guessed token, with the goal of triggering tie-handling;\n(c) Adds a padding sequence to extend the expect capacity."}, {"title": "3.4. Oracle Attack", "content": "The Oracle Attack offers an efficient way to verify a guessed prompt when the attacker has complete knowledge of the candidate message. This eliminates the complexity of the general Leakage Attack, which requires iterative guessing and routing path recovery.\nThis simplification is achieved by:\n\u2022 Targeting a single token: The attack focuses solely on the last secret token, whose representa- tion reflects the entire preceding sequence, thus a tie between target token and guess token will verify the complete candidate message at once.\n\u2022 Predicting token position: With knowledge of the candidate message, the entire adversarial batch is known, and therefore the position of the target token within the expert buffer.\n\u2022 Bypassing routing path recovery: With perfect knowledge of the candidate message, the at- tacker can deterministically predict token routing paths, avoiding the computationally expensive process of recovering them from the model's output."}, {"title": "3.5. Attack Complexity", "content": "The Leakage Attack extracts the victim's prompt token-by-token. For each of the M tokens in the victim's prompt, the attack iterates through the entire vocabulary (V) and M possible positions within the expert's buffer, resulting in O(VM\u00b2) guesses or queries to the target model. Verifying each guess requires a computation of all 2DN routing paths (for a model with D layers and N experts), leading to O(2DNVM2) queries to a local copy of the model. The Oracle Attack, with its knowledge of the prompt, requires only two queries to the target model and its local copy."}, {"title": "4. Evaluation", "content": "Setting We evaluate our attack on the first two transformer blocks of Mixtral-8x7B (Jiang et al., 2024), using PyTorch 2.2.0+cu118. We set the model router to be Expert Choice Router as described by Zhou et al. (2024). We restrict the vocabulary for guesses to lowercase letters and space, for a total of 27 tokens, and limit our extraction messages to the 1,000 most common words in English. We use a restricted vocab of only 9,218 out of 32,000 tokens for finding blockers, which we discuss in detail in Appendix D. We quantize the router weights to 5 digits to induce ties. We vary the length of padding sequences and enumerate all experts if needed to increase the success rate of our attack.\nWe list evaluation parameters in Appendix A. Our evaluation focuses on a specific MoE model and a limited vocabulary. Further research is needed to assess the attack's effectiveness on different MoE architectures, larger vocabularies, and real-world deployment scenarios.\nMoE Tiebreak Leakage We find that it is possible to extract secret user data for all of the possible inputs we considered. We managed to fully extract 996 out of 1,000 secret messages and 4,833 out of 4,838 total secret tokens as shown in Table 2. We further explored how length of the user-message, the length of the padding sequence (which effects expert capacity), and the use of multiple experts affects the performance."}, {"title": "5. Discussion", "content": "Methodological improvements: In this paper we show that it is possible to exploit Expert-Choice- Routing to extract private victim data placed in the same batch as an adversary's data. MoE Tiebreak Leakage currently requires 2 queries per guess for verification, and 2DN per-token queries for general extraction. This makes it infeasible at present to use our attack against real world systems. However, we believe that performance of the attack could be significantly improved. First, we hypothesize that refining the buffer shaping process could enable the selection of blockers that prevent inter-batch token interference. We discuss this further in Appendix D. Second, we suspect that an alternative approach exists to determine the processed token without exhaustively constructing all possible 2DN expert combinations, potentially by learning a mapping between outputs and routing paths. We discuss this further in Appendix E. Third, targeting the final MoE layer instead of the first may eliminate the need for routing path tracking altogether. Fourth, the current attack requires precise matching in its exploitation for tie-handling. We hypothesize that relative placement of tokens can similarly be used to signal what victim token is used. Fifth, a single query oracle attack might be possible by sending multiple probe inputs. Sixth, we believe might be possible to reduce O(VM\u00b2) using binary-search over the target token position, and by being able to leak information from non-equal priorities, or by simply using an LLM to propose guesses for the next token instead of naively trying them in order. Finally, we believe that a black-box variant of this attack is feasible a local clone of the model at present is only used to find blocking sequences and for inverting token routing paths. We hypothesize that both of the tasks could inefficiently be deduced from black-box access.\nOptimisation-Security Trade-Off: Within the domain of computer security, it is well-established that prioritizing performance optimization often inadvertently introduces vulnerabilities to side-channel"}, {"title": "6. Related Work", "content": null}, {"title": "6.1. Mixture-of-Experts", "content": "The concept of Mixture-of-Experts (MoE) was first introduced by Jacobs et al. (1991) and Jordan and Jacobs (1994), but has more recently become a popular tool for efficient inference on Transformer- based LLMs (Jiang et al., 2024; Renggli et al., 2022; Shazeer et al., 2017; Zoph et al., 2022), primarily because it allows the model to activate only a small fraction of its total parameters for any given input. An MoE layer in a large language model (LLM) consists of N expert modules and a gating function, which routes a token to an expert (or subset of the N experts). Since only a subset of experts is activated for an input token, the number of parameters activated is significantly smaller than the overall number of parameters in the LLM, which translates to fewer floating-point operations and faster inference. This in turn allows one to build extremely large networks without a corresponding increase in inference costs. Some of the best performing modern LLMs utilize MoE architectures e.g. Gemini-1.5 (Team, 2024), Mixtral (Jiang et al., 2024), and Grok-1 (xAI, 2023)."}, {"title": "6.2. Violations of User Privacy", "content": "Although previous work has investigated how user privacy can be compromised in LLMs (Debenedetti et al., 2023; Shen et al., 2024), none have thus far investigated vulnerability of user privacy due to the underlying model architecture, specifically how input representations can be influenced by other data within the same batch. Hayes et al. (2024) demonstrated previously that batch composition can be used by adversaries to exploit MoE routing and to launch Denial-of-Service (DoS) attacks; we instead exploit it to leak private user supplied prompts. Note that Expert-Choice-Routing considered in this work is one of many different routing strategies that breaks the implicit batch independence; we hypothesise that other routing strategies may be similarly vulnerable."}, {"title": "7. Conclusion", "content": "In classical dense LLMs, it is essentially impossible for one user's data to impact another user's output. But MoE models introduce a side-channel: one user's queries can impact a different user's outputs. The magnitude of this leak is minuscule and challenging to detect. But by carefully crafting adversarial input batches, we show how to manipulate the expert buffers within the MoE model"}, {"title": "8. Reproducibility Statement", "content": "To ensure reproducibility, we provide a comprehensive outline of our attack methodology in Section 3. We include all of the details about the attack and provide a detailed algorithmic description in Section 3.2. Our evaluation in Section 4 relies on the base model that is openly available (Mixtral- 8x7B). A number of supplementary figures in the appendix illustrate all of the details required to replicate the work. We list the hyperparameters and the code for the router in the Appendix."}, {"title": "A. Notations | List of parameters and their values", "content": null}, {"title": "B. MoE Tiebreak Leakage Algorithm", "content": null}, {"title": "C. Exploiting Tie-Handling in Topk Implementations", "content": "Our attack relies on the consistent and stable tie-handling behavior of the topk implementation in PyTorch 2.2.0+cu118 with a CUDA environment. However, this behavior is not guaranteed on CPUs, where topk can produce inconsistent results for duplicate elements (Issues, 2019, 2020, 2024). As demonstrated in the code listing below, topk reliably returns ordered indices for CUDA tensors (green highlights) but fails to do so for CPU tensors (red highlights). This necessitates an alternative approach for exploiting ties on CPUs.\nimport torch\nfor device in ['cuda', 'cpu']:\n  for size in [32, 33]:\n    for is_sorted in [True, False]:\n      print(size, is_sorted, device)\n      print(torch.topk(torch.Tensor([1] * size).to(device),\n                      k = size, sorted = is_sorted).indices)"}, {"title": "D. Find Blocking Sequences", "content": "To construct adversarial batches efficiently, the attacker must generate blocking sequences for each expert. These sequences consist of high-priority tokens that, when included in the batch, fill the expert's buffer up to a desired threshold. This process involves the following steps:\n1. Vocabulary Restriction: Use a restricted vocabulary \u03b2\u1e9e of prefix-free tokens. This prevents unintended token merging when combining blocking sequences.\n2. Priority Threshold: Set a priority threshold, denoted as \u03a6, to 0.85. This value determines the minimum priority for a token to be considered a blocker.\n3. Blocker Limit: Define $n_b$ as the maximum number of blocking tokens allowed in a single blocking sequence. This is calculated as (K\u22121)/(B\u22123), ensuring that the total number of blockers in the batch does not exceed the expert's capacity.\n4. Sequence Generation: Generate a blocking sequence with length bsl (where bsl \u2264 P) containing $n_b$ tokens, each with a priority $p_{e_i}$ less than \u222e for the target expert $e_i$.\nThe algorithm for generating a blocking sequence is as follows:\n1. Initialize an empty blocking sequence with the beginning-of-sequence token (<bos>).\n2. Randomly generate a candidate chunk of length bsl/$n_b$.\n3. If the chunk contains at least one token with priority $p_{e_i}$ \u2265 \u0424, append the chunk to the blocking sequence.\n4. Trim any unnecessary tokens from the end of the blocking sequence.\n5. Repeat steps 2-4 until $n_b$ chunks have been appended."}, {"title": "E. Recovering A Token Routing Path", "content": "We define a token's routing path for Expert-Choice-Routing as a binary matrix R of shape (N \u00d7 D), where $R_{ij} = 1$ if the token is routed to expert $e_i$ in layer j, and 0 otherwise. This matrix encodes the specific sequence of experts that a token is assigned to as it passes through the model's layers.\nDuring MoE Tiebreak Leakage we query the model with two adversarial batches corresponding to a guess of the target token and its position in the expert buffer. The two queries only differ in the position of the probe sequence. We expect to observe changes in the model's output for the probe sequence in the two queries if the guess is correct. However, these output variations could stem from several factors:\n\u2022 Numerical instability: Floating-point errors inherent in computations.\n\u2022 Suffix \u2192 prefix dropout: As attackers we only have partial knowledge of the state of the expert because of the suffix, or victim's message continuation which is unknown to us. This suffix could fill other experts and cause the to drop prefix tokens.\n\u2022 Double dropout: Simultaneous dropping of identical tokens at the boundaries of different expert buffers. In probe sequence we repeat both the target token and all the prefix tokens, therefore they would all have identical representations."}, {"title": "F. Expert-Choice-Routing implementation with Mixtral", "content": "The following code provides an implementation of: (a) Slicing a Mixtral model to two layers, (b) Expert-Choice-Routing, (c) Loading and Hooking models to use Expert-Choice-Routing.\ndef slice_model (model_name = 'mistralai/Mixtral-8x7B-Instruct-v0.1', num_layers = 2):\n  # load full model\n  full_model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n  cfg = copy.deepcopy(full_model.config)\n  # slice\n  cfg.num_hidden_layers = num_layers\n  sliced_model = transformers.AutoModelForCausalLM.from_pretrained(model_name, config=cfg)\n  # store\n  sliced_model.save_pretrained(f'./sliced_{model_name.replace(\"/\", \"_\")}_{num_layers}_layer{\"s\" if num_layers > 1 else \"\"}')\n# Same class but pass attention_mask to MoE layer so it can deprioritize <pad> tokens.\nclass ModifiedLayer (transformers.models.mixtral.modeling_mixtral.MixtralDecoderLayer):\n  def forward(\n      self,\n      hidden_states: torch.Tensor,\n      attention_mask: Optional[torch.Tensor] = None,\n      position_ids: Optional[torch.LongTensor] = None,\n      past_key_value: Optional[Tuple[torch.Tensor]] = None,\n      output_attentions: Optional[bool] = False,\n      output_router_logits: Optional[bool] = False,\n      use_cache: Optional[bool] = False,\n  ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n      Args:\n          ``hidden_states`` (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n          ``attention_mask`` (`torch.FloatTensor`, *optional*): attention mask of size\n          `(batch, sequence_length)` where padding elements are indicated by 0.\n          ``past_key_value`` (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n          ``output_attentions`` (`bool`, *optional*):\n              Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n              returned tensors for more detail.\n          ``output_router_logits`` (`bool`, *optional*):\n              Whether or not to return the logits of all the routers. They are useful for computing the router loss, and\n              should not be returned during inference.\n          ``use_cache`` (`bool`, *optional*):\n              If set to `True`, `past_key_values key value states are returned and can be used to speed up decoding\n              (see `past_key_values`).\n      residual = hidden_states\n      hidden_states = self.input_layernorm(hidden_states)\n      # Self Attention\n      hidden_states, self_attn_weights, present_key_value = self.self_attn(\n          hidden_states=hidden_states,\n          attention_mask=attention_mask,\n          position_ids=position_ids,\n          past_key_value=past_key_value,\n          output_attentions=output_attentions,\n          use_cache=use_cache,\n      )\n      hidden_states = residual + hidden_states\n      # Fully Connected\n      residual = hidden_states\n      hidden_states = self.post_attention_layernorm(hidden_states)\n      hidden_states, router_logits = self.block_sparse_moe(hidden_states, attention_mask)  # the only change is to pass attention_mask to MoE block\n      hidden_states = residual + hidden_states\n      outputs = (hidden_states,)\n      if output_attentions:\n          outputs += (self_attn_weights,)\n      if use_cache:\n          outputs += (present_key_value,)\n      if output_router_logits:\n          outputs += (router_logits,)\n      return outputs\nclass ExpertRoutingStrategy(transformers.models.mixtral.modeling_mixtral.MixtralSparseMoeBlock):\n  Based on: https://arxiv.org/abs/2202.09368, Section 3.2\n  jax implementation is available here:\n  https://github.com/google/flaxformer/blob/main/flaxformer/architectures/moe/routing.py#L647-L717\n  n = total_number_of_tokens = batch_size * seq_length\n  C = capacity factor # ~on average how many experts are utilized by a token\n  e = number of experts\n  k = n*c/e"}]}