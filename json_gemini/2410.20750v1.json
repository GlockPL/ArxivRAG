[{"title": "ODRL: A Benchmark for Off-Dynamics Reinforcement Learning", "authors": ["Jiafei Lyu", "Kang Xu", "Jiacheng Xu", "Mengbei Yan", "Jingwen Yang", "Zongzhang Zhang", "Chenjia Bai", "Zongqing Lu", "Xiu Li"], "abstract": "We consider off-dynamics reinforcement learning (RL) where one needs to transfer policies across different domains with dynamics mismatch. Despite the focus on developing dynamics-aware algorithms, this field is hindered due to the lack of a standard benchmark. To bridge this gap, we introduce ODRL, the first benchmark tailored for evaluating off-dynamics RL methods. ODRL contains four experimental settings where the source and target domains can be either online or offline, and provides diverse tasks and a broad spectrum of dynamics shifts, making it a reliable platform to comprehensively evaluate the agent's adaptation ability to the target domain. Furthermore, ODRL includes recent off-dynamics RL algorithms in a unified framework and introduces some extra baselines for different settings, all implemented in a single-file manner. To unpack the true adaptation capability of existing methods, we conduct extensive benchmarking experiments, which show that no method has universal advantages across varied dynamics shifts. We hope this benchmark can serve as a cornerstone for future research endeavors.", "sections": [{"title": "Introduction", "content": "Human beings are able to transfer the policies swiftly to a structurally similar task. This ability is also expected in decision-making agents, especially embodied AI [62, 13, 53]. For instance, we may train the robot in a simulated environment (i.e., source domain) and deploy the learned policy in real-world tasks (i.e., target domain), where the dynamics gap may pertain between the simulation and reality. It is anticipated that the robot is able to adapt itself to real-world dynamics quickly.\nHow to generalize policies across different domains with dynamics discrepancies efficiently remains an open problem in reinforcement learning (RL). Such a problem setup is referred to as off-dynamics RL in [16], but it is restricted in demanding the online source domain and target domain. We extend its scope and formally define a general off-dynamics RL setting (Definition 1), where the source domain and the target domain only differ in their transition dynamics. Existing researches realize policy adaptation under dynamics mismatch via system identification [82, 10], domain randomization [63, 66, 55], learning domain classifiers [16, 41], etc. However, we argue that this field lacks a standard and unified benchmark. Upon checking the latest off-dynamics RL methods [16, 76, 41], we found that they often manually construct their customized environments with dynamics shifts and"}, {"title": "Background", "content": "Reinforcement Learning (RL). We study sequential decision-making problems in a Markov Decision Process (MDP), which is specified by the tuple (S, A, P, r, po, \u03b3), where S, A are state space and action space, respectively, P(s'|s, a) : S \u00d7 A \u2192 \u25b3(S) is the transition probability, where A is the probability simplex, r : S \u00d7 A \u2192 R is the scalar reward signal, \u03b3\u2208 [0,1) is the discount factor, po is the initial state distribution. The goal of RL is to find a policy \u03c0(\u00b7|s) to maximize J(\u03c0) = \u0395\u03c0[\u03a3\u03c4\u03bf\u221e \u03b3tr (st, at)].\nOff-dynamics RL. In off-dynamics RL, we consider two infinite-horizon MDPs, the source domain Msrc:= (S, A, Psrc, r, \u03c1\u03bf, \u03b3) and the target domain Mtar := (S, A, Ptar, r, po, \u03b3). Note that the"}, {"title": "Related Work", "content": "Generalizing policies across varied domains is a broad and critical topic in RL, where domains can differ in transition dynamics [16, 69, 77, 12], observation spaces or action spaces [22, 6, 23, 83, 4, 79, 25], etc. As a benchmark paper, it is difficult to include all possible domain discrepancies. We set our focus on the policy adaptation across domains with dynamics mismatch, because this kind of problem often occurs in real-world applications [53, 36, 1, 85], e.g., enabling the quadruped robot that previously trained under indoor scenes to adapt its policy to outdoor varied landscapes.\nPrevious studies mainly handle this challenge by domain randomization [63, 49, 70, 35], system identification [10, 86, 15, 74, 17, 87, 11, 60], and meta learning approaches [50, 58, 2, 73]. These methods often rely on a manipulable simulator [9], or require access to the distribution of training environments. System identification can be expensive due to the need of calibrating the dynamics of the source domain, and domain randomization relies on manually-specified randomized parameters and nuanced randomization distributions. Another line of research tries to address the issue from the perspective of designing dynamics-aware algorithms without changing the parameters of the training environment. Typical methods include recognizing the dynamics change by training expressive models [24, 31, 75], selectively sharing transitions from the source domain by contrastive learning [71] or the proximity of paired value estimate targets across the two domains [76], learning domain classifiers for measuring the dynamics gap to penalize source domain rewards [16, 41, 68] or performing importance weighting [52, 51], capturing the representation mismatch between two domains for reward modification [44], leveraging action transformation methods that utilizing the trained dynamics models of the two domains convert transitions from the source domain [27, 14]. Furthermore, some studies close the dynamics gap between two domains by using the expert demonstration from the target domain [37, 28, 18, 61]. Despite the success, existing papers often conduct experiments within self-proposed environments, which is unhealthy for the advances of this field because it fails to truly reveal the merits of the proposed method. Meanwhile, the constructed tasks often lack sufficient diversity, and different papers set focus on distinct settings (e.g., whether the source domain is offline). These motivate us to develop a unified and standard benchmark for off-dynamics RL.\nComparison against other benchmarks. There are numerous benchmarks that are related to ODRL, including Gym-extensions [29], D4RL [19], DMC suite [65], Meta-World [81], RLBench [32], CARL [5], Continual World [72]. We compare ODRL against these benchmarks below."}, {"title": "Benchmark Details", "content": "ODRL mainly contains three task categories, including locomotion, navigation, and dexterous hand manipulation. Different domains are equipped with distinct and representative dynamics shift tasks. We treat the vanilla environments within these domains as the source domain and environments with dynamics shifts as the target domain. Both the source domain and the target domain in ODRL are allowed to be either online or offline, resulting in four varied training paradigms as illustrated in Figure 2, e.g., Online-Offline setting denotes that the source domain is online while the target domain is offline. The goal is to learn policies that can achieve better performance in the target domain by leveraging transition from the single source domain (with dynamics discrepancies) and single target domain. ODRL offers a diverse collection of tasks and covers all possible single-task policy adaptation settings, making it a reliable and unified benchmark for off-dynamics RL."}, {"title": "Benchmark Tasks", "content": "Locomotion. We adopt four tasks (Ant, Hopper, HalfCheetah, Walker2d) from the popular OpenAI Gym library [7], simulated by MuJoCo [67]. We consider four kinds of dynamics shifts:\n(a) Friction shift. In MuJoCo, friction is represented by three components: static, dynamic, and rolling friction, where static friction means the frictional force that needs to be overcome when the robot is stationary, while dynamic friction and rolling friction stand for the frictional force between objects when they are in motion and rolling, respectively. Modifying the friction attribute can significantly change the motion characteristics of the simulated robots. We modify all friction components and consider shift levels {0.1, 0.5, 2.0, 5.0}, where the friction components of the target domain are set to be 0.1, 0.5, 2.0, and 5.0 times the friction components of the source domain to span across both slight shifts and serious shifts.\n(b) Gravity shift. The gravity attribute corresponds to the gravitational force acting on objects within the simulation. We only modify the strength of the gravity and keep its direction unchanged. Similarly, we consider shift levels {0.1, 0.5, 2.0, 5.0} where the gravity in the target domain is established at 0.1 times, 0.5 times, 2.0 times, and 5.0 times the gravity within the source domain to cover both minor shift cases and extreme shift cases.\n(c) Kinematic shift. The kinematic discrepancies are realized by constraining the rotation angle ranges of certain joints in the simulated robot, i.e., some joints are broken such that it becomes infeasible to exhibit some motion characteristics in the target domain. We offer two choices of broken joints for each robot, which occur at varied parts of their body and are distinct across different robots since they possess varied structures and appearances, e.g., broken hips or broken ankles can occur in ant tasks, while the thigh joint or the foot joint can be broken for the halfcheetah task. For a specific type of kinematic shift, one can choose from three different shift levels (easy, medium, hard), where the joint's rotation angle range is restricted to distinct values.\n(d) Morphology shift. We achieve the morphology shifts by modifying the size of specific limbs or torsos of the simulated robot, without altering the state space space and action space. We also provide two types of morphological change for each robot, along with three different shift levels (easy, medium, hard) for each type, where the body part in the target domain is revised to different sizes, depending on the specific task. For example, the leg size in the ant task can be 1/2 of its leg size in the source domain given a medium shift level.\nGenerally, ODRL involves 4 friction shift tasks, 4 gravity shift tasks, 6 kinematic shift tasks, and 6 morphology shift tasks for a single robot, resulting in a total of 80 tasks with dynamics mismatch in the locomotion domain. If one considers the source domain to be offline, different qualities of source domain datasets can incur a substantial number of tasks, e.g., given a fixed target domain, the source domain datasets can be medium or expert. We adopt the offline source domain datasets from D4RL [19], which contains 6 different types of offline datasets (random, medium, medium-replay, medium-expert, full-replay, expert), leading to 480 MuJoCo tasks for the Offline-Online setting in principle. Similarly, abundant tasks can be used for evaluation under other settings like Online-Offline.\nAntMaze. The AntMaze domain requires navigating an 8-DoF Ant quadruped robot to the goal position within the maze. The morphologically sophisticated quadruped robot attempting to reach goals can mimic real-world navigation tasks. Following D4RL [19], we use a sparse 0-1 reward and a +1 reward is only assigned when the goal is reached. We employ three map sizes (small, medium, large), and construct 6 different map layouts for each map size (please see some examples in Figure 1), leading to an aggregate of 18 tasks. Note that the embodied Ant robot is unchanged, and only the map structures are modified, targeting examining the policy adaptation ability to varied landscapes or potential obstacles. The starting position and the unique goal position in the revised target map remain consistent with the original source domain map.\nDexterous Manipulation. We consider four tasks (pen, door, relocate, hammer) from Adroit [59] that is tailored for dexterous hand manipulation. It demands controlling a 24 DoF shadow hand robot to master tasks like opening a door, hammering a nail, etc. This domain is chosen because it resembles real-world hand manipulation tasks. It is challenging since it expects fine-grained hand operations, making it an ideal testbed for measuring the policy adaptation capability of the agent when encountering high-dimensional, sparse reward tasks. We do not alter the operated objects (e.g., the hammer), but modify the robotic hand to comprise the following two kinds of dynamics shifts:\n(a) Kinematic shift. Akin to MuJoCo tasks, we simulate broken joints by limiting the rotation ranges of some hand joints, and there are numerous design choices to fulfill that due to the complexity of the hand robot. We modify rotation ranges of all finger joints in the index finger and the thumb, which should cause substantial troubles in completing tasks like twirling a pen, since it becomes much harder for the dexterous hand to grasp an object. ODRL involves three kinds of shift levels (easy, medium, hard) for individual tasks, where the rotation angle ranges of the index finger and thumb are set to be 1/2, 1/4, and 1/8 the rotation angle range of the paired source domain task, respectively.\n(b) Morphology shift. We shrink the sizes of the proximal, intermediate, and distal phalanges in the index, middle, ring, and little fingers to realize the morphological mismatch. Despite that the thumb is unmodified, it is still very challenging to perform fine-grained manipulations under such shifts. We also provide three shift levels (easy, medium, hard) for each task where the phalanges sizes are configured as 1/2, 1/4, and 1/8 the phalanges sizes of the source domain robot hand, respectively.\nFor each task in Adroit, we offer 3 environments with kinematic shifts and 3 environments with mor- phology shifts, yielding a total of 24 tasks. Combined with the aforementioned tasks, it is evident that"}, {"title": "Offline Datasets", "content": "Since ODRL allows the target domain with dynamics discrepancies to be offline, we provide target domain offline datasets with distinct qualities (random, medium, expert) for locomotion and dexterous manipulation tasks, where the medium datasets are gathered by an early-stopped RL algorithm\u00b2 that has approximately one third or one half the performance of the expert policy. For the AntMaze domain, we only provide a mixing dataset for each map layout that contains both successful goal- reaching trajectories and unsuccessful ones. It is worth noting that we constrain the target domain offline dataset sizes (5000 transitions for locomotion and dexterous manipulation tasks, and 10000 samples for AntMaze tasks) owing to the fact that existing offline RL algorithms [39, 40, 21, 46, 20] can learn effective policies if a large amount of target domain data is available, potentially obviating the need of a source domain. It becomes difficult to train solely on the target domain dataset under such a low data regime. This is also reasonable in real-world scenarios, where collecting offline experiences can be expensive or time-consuming, and only limited data can be accessed, but sufficient data from another domain is available, e.g., a possibly biased simulator."}, {"title": "Evaluation Protocol", "content": "In ODRL, we suggest two metrics for evaluating the performance of the learned policy, the achieved return and its normalized score in the target domain. We do not care about the performance of the agent in the source domain. The normalized score (NS) is calculated by: NS =  $\\frac{J - J_r}{J_e - J_r}$ \u00d7100, where J is the return acquired by the learned policy, Jr is the return by a random policy, and Je is the return of an expert policy. We offer reference values of Jr and Je for all tasks."}, {"title": "Baseline Algorithms", "content": "We implement various off-dynamics RL algorithms and baselines in ODRL, categorized into 4 settings. Online-Online: DARC [16] that trains domain classifiers for penalizing source domain rewards, VGDF [76] that performs source domain data filtering from a value estimate perspective, PAR [44] that penalizes source domain data by measuring the representation mismatch between two domains; Offline-Online: H2O [52] that leverages the domain classifier for importance weighting, BC_VGDF [76] and BC_PAR [44] that incorporate the behavior cloning (BC) term for the source domain data; Online-Offline: H2O and PAR_BC that introduces the BC term to the target domain data; Offline-Offline: DARA [41], which is exactly the offline version of DARC and BOSA [42] that employs two support-constrained objective for regularization.\nFurthermore, we assemble the following baselines. Online-Online: SAC [26] that trains on data from both domains, SAC_IW that adopts the domain classifier for importance weighting, SAC_tune that first learns an SAC policy in the source domain and directly finetunes it in the target domain. Offline- Online: BC_SAC, CQL_SAC, MCQ_SAC that apply SAC loss on target domain samples and adopts BC loss, CQL [40] loss, MCQ [46] loss for source domain data, respectively, RLPD [3] that leverages random ensemble distillation and layer normalization for efficient online learning. Online-Offline: SAC_BC, SAC_CQL, SAC_MCQ where SAC loss is applied upon source domain data training and BC loss, CQL loss and MCQ loss are integrated for target domain data. Offline-Offline: IQL [39] and TD3_BC [20] that train on offline samples from both domains. Most of these methods are proposed to examine whether it is feasible to train a good policy by treating the two domains as one mixed domain (i.e., the transition is sampled from Pmix = XPsrc + (1 \u2212 1)Ptar, \u03bb \u2208 {0,1})."}, {"title": "Experiments", "content": "In this section, we investigate the dynamics adaptation capability of the approaches involved in ODRL. We run experiments across multiple experimental settings and examine how these methods behave given different dataset qualities and domain gaps. For each task, we report the final mean performance along with the standard deviations achieved in the target domain across 5 varied random seeds. The empirical evaluations are accompanied by some critical observations (Obs) and insights. Due to space limits, we defer wider experimental evaluations to Appendix E."}, {"title": "Benchmark Results", "content": "Considering the extensive array of tasks in our benchmark, it is costly to run algorithms on all of them. To enable a comprehensive evaluation as much as possible, we opt for two locomotion tasks (ant, walker2d), each featuring 4 types of dynamic shifts (friction, gravity, kinematic, morphology), with each shift comprising two tasks (0.5/5.0 times friction/gravity, medium/hard shift levels for kinematic and morphology tasks\u00b3). This allows us to examine the performance of existing methods under varying dynamics discrepancies. Additionally, we consider AntMaze tasks with two distinct map sizes (small, medium), each involving two maze structures (empty, centerblock for the small maze, and map type 1, 2 for the medium-size maze), to reveal the transfer capabilities of these methods across diverse landscapes and obstacles. Moreover, we include two dexterous manipulation tasks (pen, door) with two dynamic shifts (broken-joint, shrink-finger) and the easy shift level to examine whether off-dynamics RL methods are also effective in high-dimensional and sparse reward tasks. We initially turn our attention to the Online-Online setting. Recall that we only have a limited budget of data from the target domain. Hence, we run all algorithms for 1M environmental steps in the source domain, and 0.1M steps in the target domain. The aggregated normalized score comparison of baselines is depicted in Figure 3. Based on the results, we summarize the following key observations.\nObs 1. No single off-dynamics RL algorithm can exhibit advantages across all scenarios.\nObs 2. PAR achieves the best performance on locomotion tasks but fails on the Antmaze domain and Adroit domain.\nObs 3. AntMaze tasks are extremely challenging and no algorithm can achieve meaningful returns, indicating that adapting policies across barriers is hard for state-based methods."}, {"title": "Performance Comparison Given Offline Datasets", "content": "We then proceed to examine the performance of baseline methods when either the source domain or the target domain is offline. This presents substantial challenges for the algorithm to learn transferable policies because (a) if the source domain is offline, finding dynamics-consistent transitions in the dataset may be difficult due to limited coverage; (b) if the target domain is offline, gathering high- quality experiences that are closely aligned with the target domain may become challenging, and poor samples from the source domain can negatively impact the learning process of the agent.\nOffline Source Domain. We pick two tasks, ant-gravity with shift level 5.0, and walker2d-friction with shift level 0.5. We run experiments by leveraging three varied qualities of source domain datasets from D4RL (medium-replay, medium, expert). The agent undergoes training for 1M gradient steps and is allowed to interact with the target domain every 10 gradient steps, i.e., 0.1M environmental steps. We present the results in Figure 4 and have the corresponding observations.\nObs 6. A higher quality of the source domain dataset does not necessarily imply better performance in the target domain, even when an expert source domain dataset is provided.\nObs 7. Baseline methods that treat two domains as one mixed domain can achieve good performance on some tasks, sometimes even surpassing off-dynamics methods like BC_PAR, BC_VGDF, and H2O.\nObs 8. Methods that leverage the conservative value penalties (e.g., CQL_SAC) can outperform methods that involve the BC term (e.g., BC_PAR), given expert source domain datasets."}, {"title": "Connections Between Source Domain Performance and Target Domain Performance", "content": "We investigate if there is a positive relationship between the agent's performance in the source domain and the target domain. We select the ant-friction-0.5 task in the Online-Online setting, and walker2d- kinematic-footjnt-medium task in the Online-Offline setting with a medium-quality target domain dataset. We conduct experiments on these tasks using the respective algorithms within each category. We chart the learning curves in both the source domain and the target domain, and display the results in Figure 6. We observe that VGDF demonstrates strong performance in the target domain for the ant task, but its policy performance in the source domain is considerably weak. Conversely, DARC and SAC_IW learn relatively effective policies in the source domain, but they typically struggle to achieve good performance in the target domain. For other methods, they generally exhibit good performance in both domains. It is then interesting to conclude the following observation.\nObs 11. There is no definitive correlation between the policy's performance in the source domain and the target domain (it depends on the specific algorithm).\nThis observation is vital because it conveys that the performance in the source domain is not a reliable indicator for estimating the policy's performance in the target domain."}, {"title": "Conclusion and Limitations", "content": "In this paper, we propose ODRL, the first benchmark for off-dynamics RL. ODRL allows the source or target domain to be either online or offline, and introduces a wide spectrum of dynamics shift tasks to facilitate a comprehensive and persuasive evaluation. We implement many off-dynamics RL algorithms within a single file to bring together core algorithm designs, and additionally propose some baselines for each experimental setting. Our empirical results show that no existing method can lead all types of dynamics shifts. We conclude some critical empirical observations, which can serve as valuable takeaways for readers. Our benchmark is promised to be actively maintained. We hope our benchmark can pave the way for developing general dynamics-aware RL algorithms.\nLimitations. ODRL primarily supports sim-to-sim policy adaptation tasks. Adapting the policy to real-world scenarios may be more complicated (e.g., a combination of various dynamics shifts can occur). However, the proposed dynamics shifts in our study are common, and our benchmark should serve as a valuable testbed for developing more advanced off-dynamics RL algorithms."}, {"title": "Benchmark Summary", "content": "In this section, we offer a brief overview of our benchmark, as shown in Table 2. ODRL primarily consists of three domain categories: locomotion, navigation, and dexterous hand manipulation. Different domains feature various types of dynamics shifts and are designed to explore specific challenges. For each domain:\nLocomotion: We consider four tasks (ant, halfcheetah, hopper, walker2d), each with four kinds of dynamics shifts (friction shift, gravity shift, kinematic shift, morphology shift). We have 4 shift levels (0.1, 0.5, 2.0, 5.0) for friction and gravity shift tasks, and 3 shift levels (easy, medium, hard) for kinematic and morphology tasks. ODRL provides 4 tasks for friction mismatch, 4 tasks for gravity mismatch, 6 tasks for kinematic mismatch, and 6 tasks for morphology mismatch, resulting in 20 dynamics shift tasks for a single robot and a total of 80 tasks for the locomotion domain. As both the source and target domains can be offline, our benchmark can offer a vast number of tasks to facilitate a comprehensive evaluation of the agent's policy adaptation capability. We collect offline datasets for the target domain by training the SAC [26] agent solely in the target domain. We provide three types of target domain datasets (random, medium, expert), where the medium-quality datasets have approximately half the return of the expert trajectories. The offline target domain dataset size is limited to 5,000 transitions. The locomotion tasks aim to address the following open problem:\nOpen problem: How can we develop a general enough algorithm that can handle various kinds of dynamics shifts?\nNavigation (AntMaze): The AntMaze navigation task consists of three different maze sizes (small, medium, large). For each map structure, we provide 6 distinct types of maze layouts, resulting in a total of 18 tasks for the AntMaze domain. The target domain offline datasets are collected by training a goal-reaching policy and using it in conjunction with the high-level waypoint generator from Maze2D, which provides sub-goals to guide the agent towards the goal. We do not use SAC for locomotion tasks since it fails to reach goals after 5 million environmental steps in the target domain. We only provide one mixed offline target domain dataset for each map layout, where the offline dataset contains both unsuccessful and successful trajectories. The dataset size is limited to 10,000. Note that the embodied ant robot in the maze remains unmodified, and only the underlying maze layout changes to address the following open problem:\nOpen problem: How can we enable the agent to transfer across different landscapes and map layouts given limited data from the target domain?\nDexterous Manipulation: We adopt four tasks from Adroit (pen, door, relocate, hammer) and primarily focus on two dynamics shift problems: kinematic shifts and morphology shifts, each with three shift levels (easy, medium, hard). This results in a total of 24 tasks for the dexterous hand manipulation domain. For the target domain offline datasets, we provide three types of dataset qualities (random, medium, expert), where the medium-quality and expert-quality datasets are collected using the DARC [16] algorithm because we find that the SAC agent cannot learn any meaningful performance even after 5 million environmental steps of training in the target domain. The medium-quality datasets have approximately half the return of the expert datasets. The dataset size is limited to 5,000 transitions. By designing this domain, we aim to explore the following open problem:\nOpen problem: How can we achieve efficient policy adaptation in complex, high- dimensional, and sparse reward dexterous hand manipulation tasks?"}, {"title": "Off-Dynamics RL Algorithms and Baselines", "content": "In this section, we provide detailed descriptions of the implemented off-dynamics RL algorithms and the additional baselines that we introduce ourselves. We primarily introduce a fine-tuning method for the Online-Online setting (i.e., SAC_tune) and the rest of the methods that train on data from"}, {"title": "Online-Online setting", "content": "We implement the following off-dynamics RL methods for this setting:\nDARC: We follow the original paper and train two domain classifiers $q_{\\theta_{sas}}(target|s_t, a_t, s_{t+1})$, $q_{\\theta_{sa}} (target | s_t, a_t)$ parameterized by $\\theta_{sas}$ and $\\theta_{sa}$, respectively. These domain classifiers are trained using the following losses:\n$\\mathcal{L}(OSAS) = \\mathbb{E}_{D_{tar}}[log \\, q_{\\theta_{sas}}(target|s_t, a_t, s_{t+1})] + \\mathbb{E}_{D_{src}}[log(1 - q_{\\theta_{sas}}(target|s_t, a_t, s_{t+1}))],$\n$\\mathcal{L}(OSA) = \\mathbb{E}_{D_{tar}}[log \\, q_{\\theta_{sa}}(target | s_t, a_t)] + \\mathbb{E}_{D_{src}}[log(1 - q_{\\theta_{sa}}(target | s_t, a_t))],$\nwhere $D_{src}, D_{tar}$ are replay buffers of the source domain and the target domain, respectively. We set the Gaussian standard deviation $\\sigma = 1$ when training these domain classifiers, which is recommended by the authors. DARC compensates the source domain rewards by estimating the dynamics gap: $\\log \\frac{P_{M_{tar}}(s_{t+1} | s_t, a_t)}{P_{M_{src}}(s_{t+1} | s_t, a_t)}$. DARC approximates this term by leveraging the trained domain classifiers. Formally, its reward penalty term, $d_r$, gives\n$d_r(s_t, a_t) = -log \\frac{q_{\\theta_{sas}}(target | s_t, a_t, s_{t+1}) q_{\\theta_{sa}}(source | s_t, a_t)}{q_{\\theta_{sas}}(source | s_t, a_t, s_{t+1}) q_{\\theta_{sa}}(target | s_t, a_t)} $\n(1)\nThe source domain rewards are modified accordingly via:\n$\\mathcal{R}_{src}^{DARC} = r_{src}(s_t, a_t) - d_r.$\n(2)\nWe implement DARC by following its official code repository.\nVGDF: The core idea of VGDF is to filter source domain data that share similar value estimates as those in the target domain. To fulfill that, it trains an ensemble of dynamics model [33, 54, 57, 8, 56] in the raw state-action space of the target domain to predict the next state that follows the transition dynamics of the target domain given source domain data $(s_{src}, a_{src})$. Then it measures the mean and variance of the value ensemble {$Q(s',a)$}$_{i=1}^{M}$ to build a Gaussian distribution, where $s'$ is the predicted next state, $a$ is sampled from the policy, and $M$ is the ensemble size. Subsequently, the rejection sampling approach is employed to select a fixed percentage ($\\xi\\%$, $\\xi \\in [0,100]$) of source domain data with the highest likelihood estimation and share them with the target domain, i.e., VGDF optimizes the following objective:\n$\\mathcal{L}_{critic} =  \\mathbb{E}_{(\\tau,\\alpha,r,s')\\sim D_{tar}} [(Q_{\\Theta}(s,a) - y)^2] +  \\mathbb{E}_{(\\tau,\\alpha,r,s')\\sim D_{src}} [\\mathbb{I}(A(s,a,s') > A_{\\xi\\%}) (Q_{\\Theta}(s,a) - y)^2], i \\in \\{1,2\\},$\n(3)"}, {"title": "Offline-Online setting", "content": "We implement the following dynamics-aware algorithms:\nH2O: H2O also trains domain classifiers to estimate the dynamics gap", "as": "n$\\mathcal{L"}, {"y)^2": "mathbb{E"}, {"y)^2": "beta_{CQL"}, "mathbb{E}_{s\\sim D_{src},\\bar{a} \\sim \\pi_{\\phi}(\\cdot|s)}[w(s, a, s')Q_{\\theta_i}(s, \\bar{a})"], "steps.\nBC_VGDF": "This variant has the same critic objective function as VGDF (Equation 3). What makes it different is that we incorporate a behavior cloning (BC) term into its policy training to realize conservatism injection", "domain.\nBC_PAR": "Akin to BC_VGDF", "via": "n$\\mathcal{L}_{actor} = \\lambda \\cdot \\mathbb{E}_{s \\sim D_{src} \\cup D_{tar}} [\\min_{i=1,2} Q_{\\Theta_i} (s,a)] - \\alpha \\log \\pi_{\\phi}(\\cdot | s) + \\mathbb{E}_{(\\tau,\\alpha)\\sim D_{src}} [(a - \\hat{a})^2],$\n(12)\nwhere $\\lambda = \\frac{\\frac{1}{\\alpha v \\pi_{\\phi}(a|s)"}, {}]