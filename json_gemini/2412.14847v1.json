{"title": "A Survey of RWKV", "authors": ["Zhiyuan Li", "Tingyu Xia", "Yi Chang", "Yuan Wu"], "abstract": "The Receptance Weighted Key Value (RWKV) model offers a novel alternative to the Transformer architecture, merging the benefits of recurrent and attention-based systems. Unlike conventional Transformers, which depend heavily on self-attention, RWKV adeptly captures long-range dependencies with minimal computational demands. By utilizing a recurrent framework, RWKV addresses some computational inefficiencies found in Transformers, particularly in tasks with long sequences. RWKV has recently drawn considerable attention for its robust performance across multiple domains. Despite its growing popularity, no systematic review of the RWKV model exists. This paper seeks to fill this gap as the first comprehensive review of the RWKV architecture, its core principles, and its varied applications, such as natural language generation, natural language understanding, and computer vision. We assess how RWKV compares to traditional Transformer models, highlighting its capability to manage long sequences efficiently and lower computational costs. Furthermore, we explore the challenges RWKV encounters and propose potential directions for future research and advancement. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/RWKV-Survey.", "sections": [{"title": "1 INTRODUCTION", "content": "As an important branch of machine learning, deep learning originated from the perceptron model [1], which laid the foundation for the development of neural networks. Over the past decade, deep learning has rapidly advanced and been widely applied, profoundly impacting various fields. Classic deep learning models, including Multilayer Perceptron (MLP), Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN), have achieved significant success in areas such as image recognition [2], [3], [4], speech recognition [5], [6], [7], and natural language processing [8], [9]. For example, CNNs have become central to computer vision [10], [11], [12], with applications ranging from facial recognition to automatic driving [13], [14], [15]. Meanwhile, RNNs and their variants, particularly long-short-term memory (LSTM) networks [16], have proven essential for natural language processing tasks, especially in speech recognition and machine translation [17], [18], [19].\nProposed by Vaswani et al. in 2017, the Transformer model [20] has since become a dominant architecture in natural language processing (NLP) [21], [22], [23] and various other fields [24], [25], [26]. Transformer-based models such as BERT [27] and GPT [28] have demonstrated exceptional performance in tasks such as text classification [29], language generation [30], and question answering [31]. The Transformer utilizes a self-attention mechanism to adeptly capture long-distance dependencies in input sequences, providing a more robust approach for modeling intricate data relationships than earlier models like RNNs and LSTMs [32]. Furthermore, the parallelization of both training and inference processes enhances the scalability of the Transformer, significantly improving processing speed and model performance. However, despite these advantages, the Transformer faces significant challenges. The computational cost of the attention mechanism grows quadratically with the sequence length, leading to inefficient memory usage and slower processing of long sequences [33]. This issue becomes particularly problematic in tasks that require handling large amounts of data. Despite numerous endeavors to explore methods for reducing the complexity of the Transformer model, unfortunately, significant progress in substantially decreasing its complexity has yet to be achieved [34], [35], [36].\nTo address the limitations of the Transformer model, recent advancements have introduced state-space models [37], which provide a promising solution for efficiently handling long sequences while maintaining strong contextual understanding. Models such as Mamba [38] and RetNet [39] have been proposed to reduce computational complexity in sequence data processing. Mamba aims to enhance the efficiency of long-sequence processing by modifying the attention mechanism and reducing memory usage in the process. Similarly, RetNet employs a recurrent structure to improve memory retention and extend the model's ability to capture long-range dependencies, all while avoiding the quadratic time complexity that affects traditional Transformers. One of the most notable innovations in recent times is the Receptance Weighted Key Value (RWKV) model [40], which effectively combines the strengths of RNNs with the attention-based mechanism of Transformers. The RWKV model utilizes a unique key-value approach to manage dependencies sequentially, significantly reducing computational overhead while preserving the ability to model long-term dependencies with minimal memory requirements. RWKV has already demonstrated its potential across a wide range of applications, from NLP tasks such as text generation [41], machine translation [42], and sentiment analysis to time series prediction tasks [43].\nInspired by the exceptional efficiency of RWKV in handling long sequences and its growing influence in various fields, there has been a surge in research exploring its"}, {"title": "2 BACKGROUND", "content": "capacities and potential applications. Given the increasing interest in RWKV, this paper aims to provide a comprehensive overview of the model, its evolution, and future directions. As depicted in Fig. 1, the remainder of this paper is organized as follows: Section 2 presents a systematic review of foundational concepts, covering RNNs, Transformer architectures, and Attention-Free Transformer (AFT). Section 3 delves into the principles and implementations of RWKV, providing a comparative analysis with Transformer and Mamba, along with a detailed discussion of its various implementations. Section 4 thoroughly explores various applications of RWKV, including Natural Language Generation (NLG), Natural Language Understanding (NLU), other NLP tasks, computer vision, web applications, and evaluation methods. Finally, Section 5 provides the key challenges RWKV faces and its future research directions."}, {"title": "2.1 Recurrent Neural Networks (RNNs)", "content": "Recurrent Neural Network (RNN) [259] is a powerful tool for processing sequence data. It is capable of internally maintaining a state, which enables it to capture the dynamic features of time series data. The fundamental concept underlying RNN is to leverage the dependency among consecutive elements within a sequence and transmit information via cyclic connections. This allows the network to retain prior information and subsequently utilize the prior information for further computations, thereby endowing it with the ability to handle sequential data in a more effective and contextually aware manner.\nThe working principle of RNN can be described by the following mathematical model:\n$h_t = tanh(W_{hh}h_{t-1} + W_{hx}x_t + b_h)$                                                                    (1)\n$y_t = W_{ho} \\cdot h_t + b_o$                                                                                                                (2)\nWhere $h_t$ represents the hidden layer state at time t, $x_t$ represents the input at time t, and $y_t$ represents the output at time t. These two equations demonstrate the process of merging the current input with the preceding hidden state to derive the present hidden state. Subsequently, they also illustrate how this hidden state is transformed into an output.\nWhile RNN models are effective at handling time series data, they struggle with recognizing long-term dependencies. This issue arises primarily due to the phenomena of vanishing or exploding gradients. In RNN models, the issue of gradient disappearance arises when gradients are multiplied by the weight matrix across time steps. When the eigenvalue of this weight matrix is less than 1, it results in the exponential diminishment of gradient values over time, leading to vanishing gradients. Conversely, when the eigenvalue exceeds 1, the gradient values increase exponentially with time, resulting in gradient explosion.\nThe structure of RNN models imposes a restriction on their capability to learn long-term dependencies. In typical RNN models, each time step's hidden state is influenced solely by the hidden state from the preceding time step. Consequently, RNN models struggle to effectively capture and understand dependencies that span across extended time periods.\nIn order to solve the problem of gradient disappearance or gradient explosion that may occur when RNN processes long sequences, researchers have proposed several variants of RNN: Long short-term memory network (LSTM) [16]: By introducing a gating mechanism to control the flow of information, it can capture long-term dependencies. Gated recurrent unit (GRU) [260]: The structure is simpler than LSTM, but it can provide similar performance. Bidirectional recurrent neural network (Bi-RNN) [261]: By processing the forward and reverse information of the sequence at the same time, it can more comprehensively understand the patterns in the sequence.\nRecently, with increasing attention on overcoming the scalability limitations of Transformer-based architectures, the potential of RNN-based variants has garnered renewed interest. Models such as minLSTMs and minGRUs [262], XLSTM [263], and Aaren [262] show that addressing the limitations of traditional RNNs can make these models perform as well as or even better than Transformers in various tasks. These developments highlight the enduring significance and promising potential of RNN-based approaches in advancing modern deep learning."}, {"title": "2.2 Transformers", "content": "In RNNs, the computation of each time step is based on the output of the previous step, which requires sequential processing and prohibits parallel computation. Conversely, the Transformer architecture employs the Self Attention mechanism to process all time steps simultaneously, enabling parallel computation. Unlike conventional recurrent neural network models like LSTM and GRU, the Transformer [20] eschews the recursive architecture entirely. Instead, it leverages self-attention to establish dependencies across all positions within the input sequence, thereby enhancing both training speed and efficiency.\nAttention mechanisms have the capability to identify and record the relationships across all input and output tokens. In Self-Attention, the input is denoted by the matrix x. This undergoes a linear transformation using the matrices $W_q$, $W_k$, and $W_v$, allowing the calculation of the Query Q, Key K, and Value V. It is important to highlight that each row within the matrices x, Q, K, and V corresponds to an individual word.\n$Q = x \\cdot W_q, K = x \\cdot W_k, V = x \\cdot W_v$                                                                                                                                (3)\nwhere $W_q$, $W_k$, $W_v$ are the trainable parameters. After obtaining the matrices Q, K, V, it can calculate the output of Self-Attention. The calculation formula is shown as follows:\n$Attn(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}})V$                                                                                                                                       (4)\nwhere $d_k$ is the dimension of the key vectors. It scales the result by $\\sqrt{d_k}$ in order to prevent the inner product from being too large.\nIn a single Attention mechanism, a singular representation space is derived. When utilizing multiple attentions, it is possible to derive several distinct representation spaces. This concept leads to the development of Multi-Head Attention by integrating several Self-Attentions. Each Attention employs unique weight matrices for Query, Key, and Value, with these matrices initialized in a random manner. Training subsequently projects word embeddings into diverse representation spaces. The mathematical expression representing the multi-head attention mechanism is as follows:\n$Q_i = x \\cdot W_{qi}, K_i = x \\cdot W_{ki}, V_i = x \\cdot W_{vi}, i \\in [1, n],$                                                           (5)\n$head_i = Attention(Q_i, K_i, V_i), i \\in [1, n],$                                                                                  (6)\n$MultiHead(Q, K, V) = Concat(head_1, ..., head_n)W_o$                                                                                      (7)\nwhere n means the number of attention heads, Concat means concatenate and $W_o$ is the projection matrix which is a trainable parameter. Multi-head attention enables the simultaneous processing of input sequences from various perspectives, enhancing the model's capacity to comprehend and identify intricate dependencies. This capability substantially boosts the Transformer model's effectiveness in handling natural language processing tasks as well as other types of sequence data processing."}, {"title": "2.3 Attention Free Transformer (AFT)", "content": "The attention mechanism in Transformers excels at capturing long-term dependencies and emphasizing pertinent elements within the input sequence. Nevertheless, the self-attention mechanism demands matrix multiplication involving the transpose of the query and key. This operation exhibits quadratic space complexity relative to sequence length, rendering it inefficient for handling extended sequences effectively. Attention Free Transformer (AFT) [264], represents a Transformer variant that does not rely on matrix multiplication. This change leads to a reduction in memory complexity\u2014from $O(T^2d + Td)$ to O(Td), as noted by [40]\u2014and a decrease in time complexity-from $O(T^2d)$ to $O(Tsd)$, according to [40]. Here, T denotes the sequence length, d stands for the embedding dimension, and s represents the size of a local window specific to AFT. Two primary AFT variants exist: AFT-full and AFT-local. The latter, AFT-local, arose from the author's observation that attention typically exhibits pronounced local patterns; thus, utilizing a reduced attention window was deemed advantageous.\nSame as Transformer, for the input feature x, AFT-full uses three different linear layers to calculate Query Q, Key K, and Value V, $Q \\in \\mathbb{R}^{T \\times d}$, $K \\in \\mathbb{R}^{T \\times d}$, $V \\in \\mathbb{R}^{T \\times d}$, where T is the maximum length of the sequence, d is the number of hidden layer nodes. Unlike Transformer, AFT adds a learnable positional encoding $w_{t',t}$ to $K_{t'}$, where $1 < t' < T$. And use softmax to normalize K with position encoding.\n$Weighted(K_{t'}) = \\frac{exp(K_{t'} + w_{t,t'})}{\\sum_{t'=1}^{T} exp(K_{t'} + w_{t,t'})}$                                                                                                                              (8)\nSubsequently, the sigmoid activation function is applied to normalize Qt, resulting in the weight matrix \u03b1, which is derived by conducting a dot multiplication with Weighted(Kt'):\n$\\alpha = \\sigma_o(Q)$                                                                                                                                                                              (9)\n$\\alpha_{t'} = \\sigma_o(Q_t) \\odot Weighted(K_{t'})$                                                                                                                          (10)\n$Attn(Q, K, V)_t = Y_t$ is calculated as follow:\n$Y_t = \\sum_{t'=1}^{T} \\alpha_{t'} \\odot V_{t'}$                                                                                                                                                                          (11)\n$=\\sum_{t'=1}^{T} \\sigma_o(Q_t)  \\frac{exp(K_{t'} + w_{t,t'})}{\\sum_{t'=1}^{T} exp(K_{t'} + w_{t,t'})} \\odot V_{t'}$                                                                                            (12)\n$= \\sigma_o(Q_t) \\odot \\sum_{t'=1}^{T}  \\frac{exp(K_{t'} + w_{t,t'})}{\\sum_{t'=1}^{T} exp(K_{t'} + w_{t,t'})} \\odot V_{t'}$                                                                                        (13)\nAnalyzing equation 13 along with the implementation of AFT-full reveals that the complexity of AFT-full is O($T^2d$), similar to that of the Transformer. However, the distinct feature of AFT-full is that this complexity arises from the incorporation of w into the computations, which allows for modifying the style of w in order to enhance AFT's speed. The authors of AFT introduced AFT-local as a refinement. In this variant, any position embedding values that extend beyond the windows, with the condition s < T, are assigned a value of zero.\n$w_{t,t'} =\\begin{cases}w_{t,t'}, & \\text{if } t-t' < s \\\\0, & \\text{otherwise}\\end{cases}$                                                                                                                                                                                    (14)\nAFT-local assigns a value of 0 to the position embedding outside the window. However, since these zeroed values remain part of the entire computational process, there is no reduction in the computational load. Thus, the time complexity for AFT-local remains O(Tsd)."}, {"title": "3 RWKV", "content": "The presence of a cyclical architecture in RNNs [259] is responsible for their elevated computational complexity and reduced training efficiency. This architecture also makes them susceptible to issues such as gradient vanishing and gradient exploding, thereby limiting the scalability of RNNs. In contrast, the Transformer [20] employs a self-attention mechanism, enabling it to handle entire sentences at once. This allows for improved training efficiency and makes parallel processing possible. Nonetheless, Transformers are characterized by high computational complexity, with their time complexity being O($T^2d$), making them less suitable for handling long sequences efficiently. Grounded in the classic Transformer framework, AFT [264] replaces matrix multiplication with a technique akin to Residual Connection. This change allows AFT to bypass the attention mechanism, thereby lessening the space requirements. Consequently, AFT is capable of computing the attention weight with substantially reduced space complexity compared to the Transformer model; however, it does not achieve major improvements in reducing time complexity.\nRWKV [40] was developed in response to the challenges posed by RNNs, Transformers, and AFT issues, primarily focusing on enhancing the linear attention mechanism. This innovation aims to address the difficulty of parallelizing RNNs. The model maintains a time complexity comparable to that of RNNs while achieving performance effects akin to Transformers."}, {"title": "3.1 RWKV-4", "content": "RWKV-4 [40] represents the first version officially made available to the public, with RWKV-1, RWKV-2, and RWKV-3 existing as experimental predecessors. Referring to figure 2, the architecture of the RWKV-4 model includes a series of stacked residual blocks. Each of these blocks contains sub-blocks dedicated to time-mixing and channel-mixing, which efficiently leverage previous information through an incorporated recursive framework.\nThe time-mixing block is designed for global interaction, akin to how self-attention functions in a Transformer. Initially, the term \"RWKV\" is composed of the following four letters:\n\u2022\tR: Receptance vector, responsible for gauging the amount of past information that is permitted;\n\u2022\tW: Weight vector, includes a position weight decay vector and contains the model's trainable parameters;\n\u2022\tK: Key vector, analogous to the K in the traditional attention mechanism;\n\u2022\tV: Value vector, analogous to the V present in the traditional attention mechanism.\nFor time t, given word $x_t$ and the previous word $x_{t-1}$, the Time-Mix module formulas are as follows:\n$r_t = W_r \\cdot (\\mu_r \\odot x_t + (1 - \\mu_r) \\odot x_{t-1})$                                                                                                                                                     (15)\n$k_t = W_k \\cdot (\\mu_k \\odot x_t + (1 - \\mu_k) \\odot x_{t-1})$                                                                                                                                                   (16)\n$v_t = W_v \\cdot (\\mu_v \\odot x_t + (1 - \\mu_v) \\odot x_{t-1})$                                                                                                                                                   (17)\n$wkv_t = \\frac{\\sum_{i=1}^{t-1} exp(-(t-1-i)w + k_i) \\odot v_i + exp(u+k_t) \\odot v_t}{\\sum_{i=1}^{t-1} exp(-(t-1-i)w + k_i) + exp(u+k_t)}$                                                           (18)\n$o_t = W_o \\cdot (\\sigma(r_t) \\odot wkv_t)$                                                                                                                                                                            (19)\nThe terms $r_t$, $k_t$, and $v_t$ utilized in this context are analogous to the Q, K, and V components found in AFT or Transformer architectures. A notable distinction in the computation of R, K, and V as compared to the Transformer model is that the input x is not merely the embedding of the current token. Instead, it represents the weighted sum of the current token's embedding and that of the preceding token. Additionally, the computation of $wkv_t$ is linked to the attention mechanism's implementation. This implementation represents a linear interpolation utilizing both historical and present moment data, and it is important to note its exponential nature. The relationship between the current token and all preceding tokens is characterized by an exponential decay sum. This specific aspect endows $wkv_t$ with the features of linear attention. Notably, there exist two primary distinctions between equation 18 and equation 19 as well as AFT's equation 13:\n1.\tAFT modifies the bias $w_{t,t'}$ from being based on absolute position to being based on relative position, resulting in the need to train only a single parameter w vector.\n2.\tThe parameter u is introduced to handle the current position independently within the process.\nThe RWKV model employs Time Mixing to capture the relationship between tokens, and the Channel Mixing module models RWKV by exploring the dimensions of the hidden layer that correspond to individual tokens. The channel-mixing block utilizes equations 15 and 16 again to determine a fresh set of $r'_t$ and $k'_t$ using the result from the time-mixing block. Subsequently, the final output is computed in the following manner:\n$r_t' = W_r' \\cdot (\\mu_r' \\odot x_t + (1 - \\mu_r') \\odot x_{t-1})$                                                                                                                   (20)\n$k_t' = W_k' \\cdot (\\mu_k' \\odot x_t + (1 - \\mu_k') \\odot x_{t-1})$                                                                                                                  (21)\n$\\psi = \\sigma(r_t') \\cdot (W_v' \\cdot max(k_t', 0)^2)$                                                                                                                                                            (22)\nChannel Mixing conducts integration within the feature dimension. Given that the feature vector has a dimension of d, each element in these dimensions must gather data from other dimensions to refresh its own value. In this context, each feature vector dimension is referred to as a \"channel\".\nThe RWKV architecture combines the strengths of both Transformers and RNNs. Unlike conventional RNNs, it offers the benefit of having a stable gradient conducive to deeper architecture, similar to that of Transformers. Additionally, RWKV excels in reasoning efficiency. RWKV redefines the attention framework by adopting linear attention, substituting the usual dot-product interaction between tokens with a more efficient attention directed at channels. This adaptation results in a computational complexity of O(Td) and decreases the memory complexity down to O(d)."}, {"title": "3.2 RWKV-5 (Eagle)", "content": "The sequential models Eagle (RWKV-5) and Finch (RWKV-6) [265] have been developed as advancements of the RWKV (RWKV-4) architecture. In these models, enhancements have been incorporated into the architecture design, such as multi-headed matrix-valued states and dynamic recursive mechanisms, which bolster the models' expressive capabilities while preserving the inference efficiency typical of RNNs. Compared with RWKV-4, RWKV-5 and RWKV-6 mainly made changes to the time mixing block.\nEagle (RWKV-5) [265] enhances the architecture and learning decay strategy derived from RWKV-4 [40] through the adoption of expressive multi-head matrix-valued states rather than the traditional vector-valued states. It also includes a reconfiguration of receptive states and incorporates supplementary gating mechanisms.\nEagle introduced a new notation lerp, which means the linear interpolation between $x_t$ and $x_{t-1}$ used in RWKV-4 and it is defined as:\n$lerp(a, b) = a + (b - a) \\odot \\mu$                                                                                                                   (23)\nwhere each $\\mu_\\Box \\in \\mathbb{R}^{D}$ is a learnable vector. The formula of Eagle Time Mixing can be written as follows:\n$t = lerp(x_t, x_{t-1})W_\\Box,  \\Box \\in {r, k, v, g}$                                                                                                     (24)\n$w = exp(- exp(w))$                                                                                                                                                          (25)\n$wkv_t = diag(u) \\cdot k_\\cdot v_t + \\sum_{i=1}^{t-1} diag(w)^{t-1-i} \\cdot k_\\cdot v \\in \\mathbb{R}^{(D/h) \\times (D/h)}$\n$o_t = concat(SiLU(g_t) LayerNorm(r_t\\cdot wkv_t)) W_o \\in \\mathbb{R}^{D}$                                                                                           (27)\nLayerNorm is applied individually to each of the h heads, which can be seen as analogous to the Group-Norm applied over h groups. Additionally, it should be highlighted that w is derived using the formula $w = exp(-exp(w))$, with $w \\in \\mathbb{R}^{D/h}$ representing the trainable parameters for each head. This computation ensures that w remains within the range (0, 1), thus assuring that diag(w) functions as a contraction matrix. Then, The $wkv_t$ attention calculation can alternatively be written in a recurrent form:\n$wkv' = s + diag(u) \\cdot k^T \\cdot v$                                                                                                                                        (28)\n$s' = diag(u) \\cdot s + k^T \\cdot v$                                                                                                                                                     (29)"}, {"title": "3.3 RWKV-6 (Finch)", "content": "Finch (RWKV-6) [265] enhances the architecture's expressiveness and adaptability by incorporating innovative data-driven functions into the time mixing and token shifting modules, such as parameterized linear interpolation. Furthermore, Finch (RWKV-6) introduces a novel application of low-rank adaptation functions [266], allowing the weight matrices to be trainable and effectively augment the learned data decay vectors in a way that is sensitive to the context.\nThe token shift of Finch is ddlerp, which means data dependent linear interpolation between $x_t$ and $x_{t-1}$, and it is defined as:\n$lora(x) = x + tanh (xA) B$                                                                                                                                            (30)\n$ddlerp(a, b) = a + (b - a) \\odot lora(a + (b - \\alpha) \\mu_a)$                                                                                            (31)\nIn this setup, both $\\mu_a$ and each A$\\_Box$ bring in a trainable vector with dimension D, while each $A_{\\Box} \\in \\mathbb{R}^{D \\times 32}$ and $B_{\\Box} \\in \\mathbb{R}^{32 \\times D}$ represent new trainable weight matrices. The LORA mechanisms [266] described above enable the cost-effective enhancement of learned vectors, similar to those in Eagle, by incorporating additional offsets that are defined by the incoming input. By integrating this innovative form of Token Shift with data-dependence, Finch aims to extend the model's capabilities beyond the Token Shift method utilized in RWKV-4/Eagle. The model now computes the distribution of new and existing data per channel based on the input at both the present and preceding time steps. Thus, Finch's time mixing block is defined accordingly:\n$t=ddlerp (x_t, x_{t-1})W,  \\in {r,k,v,g}$                                                                                                    (32)\n$d_t = lorad(ddlerpd(x_t, x_{t-1}))$                                                                                                                                                   (33)\n$w_t = exp(-exp(d_t))$                                                                                                                                                                 (34)\n$wkv_t = diag(u) \\cdot k_\\cdot v_t + \\sum_{i=1}^{t-1}(  \\sum_{j=i+1}^{t-1} diag$\\cdot k$\\cdot v  \\in \\mathbb{R}^{(D/h) \\times (D/h)}$                                                (35)\n$o_t = concat(SiLU(g_t) LayerNorm(r_t wkv_t)) W_o \\in \\mathbb{R}^{D}$                                                                                          (36)\nUnlike in Eagle, $w_t$ of Finch is not static across the sequence. The fundamental transformation in Finch involves a shift to decay, allowing each component of $w_t$ to independently fluctuate over time in accordance with data, unlike the prior model which employed a static learned vector. The repetitive structure of the $wkv_t$ attention computation remains identical to that of Eagle."}, {"title": "3.4 Compare RWKV with Other improved models based on Transformer", "content": "Linear Transformers [36] represent a variant of the Transformer architecture. While the conventional Transformer relies on the Self-Attention mechanism, Linear Transformers focus on approximating or enhancing this mechanism through a more efficient linear approach. Their primary goal is to decrease computational complexity and boost the model's training and inference efficiency without sacrificing the transformative sequence processing capabilities inherent to Transformer models. Linear Transformers employ kernel methods to approximate self-attention. For instance, in linear self-attention, they utilize a low-rank matrix to approximate the query-key (Q-K) product found in standard self-attention mechanisms. Consider Q as the query matrix and K as the key matrix. In standard self-attention, the expression $QK^T$ involves a quadratic computation. Linear transformers can incorporate a low-rank matrix M, altering the operation to QMKT, where M has a lower rank. Selecting a suitable M allows the computational complexity of this operation to decrease to a linear level. Utilizing kernel methods, linear transformers achieve a computational complexity of O($Td^2$). In contrast, the computational complexity of RWKV is O(Td), indicating that linear transformers require more computation. Additionally, as linear transformers approximate the typical self-attention mechanism, there could be a diminution in the model's representational capabilities.\nIn addition to RWKV, Mamba [267] is another frequently encountered and notable improved model developed from"}, {"title": "3.5 Various Implementations", "content": "the Transformer architecture. Mamba leverages state-space models (SSMs), setting itself apart by effectively incorporating Structured State Space (S4) models into a large language model (LLM) framework. This inventive approach allows Mamba to achieve linear scalability concerning sequence length in complexity, marking a significant advancement compared to the quadratic complexity typically associated with traditional Transformer-based models. The carefully crafted architecture of Mamba, which incorporates selective SSM layers, enhances both computational efficiency and flexibility. Compared to RWKV, Mamba achieves better results in sequence segmentation. However, it demands greater computational resources. Conversely, RWKV excels by offering greater efficiency, quicker inference times, and reduced memory usage during operation [151].\nRetentive Network (RetNet) [39] can simultaneously meet the three requirements of parallel training, low-cost inference, and good performance. By introducing the Retention mechanism, RetNet effectively solves the problems of computational complexity and memory occupation that Transformer has when dealing with long sequence data. Compared with the Attention mechanism, the Retention mechanism is more efficient during the calculation process and can significantly reduce the consumption of computational resources and memory. When processing sequence data, RetNet combines the parallel representation and the recurrent representation to achieve a balance between efficiency and performance. In addition, RetNet can generate multi-scale sequence representations. This means that it can understand sequence data from different granularities or levels. As a work emerging after RWKV, compared with RWKV, RetNet's retention mechanism preserves high-dimensional states to encode sequence information, thereby further enhancing the expressive ability and leading to better performance.\nHyena [268] is a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. Unlike traditional RNNs and some Transformer-based architectures, the Hyena Hierarchy processes data through a hierarchical structure. The Hyena hierarchy consists of two operators (long convolution and element-wise multiplication gating) that are recursively defined by efficient quadratic primitives. Among them, the long convolution can effectively extract local features in sequences and, through the sliding window method, can capture similar patterns at different positions. The element-wise multiplication gating is generally located after the long convolution or between different layers in the Hyena hierarchy. Its main function is to screen and adjust the features extracted by the long convolution or the features transferred between different layers. The time complexity of Hyena is O($ndt(log t+d)$), and the space complexity is O(td), where n is the number of projections, d is the dimension, and t is the sequence length. In the case of long sequences, the growth of its computational complexity is relatively slow, enabling it to handle longer sequence data without a computational explosion. As for RWKV, both the time complexity and the space complexity are O(td). Compared with the traditional Transformer architecture, the computational efficiency has been significantly improved. Especially in the long-sequence inference scenario, its linear time complexity allows it to handle large-scale sequence data more efficiently.\nThe RWKV authors offer an official PyTorch implementation as detailed in version [44], encompassing the implementation specifics for RWKV-1 through RWKV-6. Their efforts are further extended with the ongoing development of a laboratory version of RWKV-7. Additionally, they introduced a Chatbot framework called ChatRWKV [45], designed to enable users to construct their own RWKV inference engines.\nBeyond the primary implementation of PyTorch, RWKV has motivated a host of third-party versions developed in an array of programming languages and frameworks, thus showcasing its adaptability and wide-ranging utility. These implementations cover lightweight options in languages such as C [46], [47], C++ [48], [49], [50], and Fortran [63]. Additionally, RWKV has been integrated into modern programming languages including Julia [64] and Zig [65]. Notably, the ecosystems for Go [66], [67], [68], [69], [70], [71] and Rust [51], [52], [53], [54], [55] exhibit strong support, offering models, tokenizers, and tailored platform optimizations. Moreover, RWKV has been integrated into popular deep learning frameworks such as TensorFlow [56], [57], [58], [59], [60], [61], [62], Keras [72], [73], and PaddlePaddle [74], [75], allowing advanced features such as multi-GPU training and inference. Additional contributions in JAX [76], [77], [78], Node.js [79], [80], and MLX [81] further highlight the model's versatility, establishing RWKV as a prominent tool for various computational and application needs.\nRWKV's capabilities have been notably improved in various areas, including training, inference, and applications, through additional implementations. In the realm of training, time-parallel methods have been implemented, with more sophisticated options incorporating LoRA, 4-bit quantization, as well as support for platforms like CUDA and ROCm [82], [83], [84]. For inference purposes, enhancements include Flash-Linear-Attention in RWKV v6 [85], execution support on Android CPUs using ONNX [86], compatibility with the NCNN framework [87], and deployment on Qualcomm HTP with the QNN SDK [88]. Moreover, a Java/JNI wrapper for RWKV.cpp has been developed to enable CPU-based inference with quantization, eliminating the need for Python [89]. Additionally, a kernel operator has been integrated into the bert4keras3 library to improve compatibility with RWKV6 models [90]. Implementations focused on specific applications include an API using the Flask framework, specifically tailored for role-playing scenarios [91]. Furthermore, inspired by NanoGPT, there exists an implementation utilizing RWKV's RNN architecture to deliver LLM performance on par with GPT [92]."}, {"title": "4 APPLICATIONS OF THE RWKV MODEL", "content": "The RWKV model integrates the advantages of recurrent mechanisms with those of attention-based approaches. This unique architectural design enhances its efficiency and scalability, enabling it to adeptly tackle a wide range of problems. This chapter provides an overview of the practical applications of the RWKV model, focusing on its deployment in areas like natural language generation, natural language understanding, computer vision, and web-related tasks. Figure 3 illustrates examples of"}]}