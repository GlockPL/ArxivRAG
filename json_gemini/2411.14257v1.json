{"title": "DO I KNOW THIS ENTITY? KNOWLEDGE AWARENESS\nAND HALLUCINATIONS IN LANGUAGE MODELS", "authors": ["Javier Ferrando", "Oscar Obeso", "Senthooran Rajamanoharan", "Neel Nanda"], "abstract": "Hallucinations in large language models are a widespread problem, yet the mech-\nanisms behind whether models will hallucinate are poorly understood, limiting\nour ability to solve this problem. Using sparse autoencoders as an interpretability\ntool, we discover that a key part of these mechanisms is entity recognition, where\nthe model detects if an entity is one it can recall facts about. Sparse autoencoders\nuncover meaningful directions in the representation space, these detect whether\nthe model recognizes an entity, e.g. detecting it doesn't know about an athlete or\na movie. This suggests that models can have self-knowledge: internal representa-\ntions about their own capabilities. These directions are causally relevant: capable\nof steering the model to refuse to answer questions about known entities, or to\nhallucinate attributes of unknown entities when it would otherwise refuse. We\ndemonstrate that despite the sparse autoencoders being trained on the base model,\nthese directions have a causal effect on the chat model's refusal behavior, suggest-\ning that chat finetuning has repurposed this existing mechanism. Furthermore, we\nprovide an initial exploration into the mechanistic role of these directions in the\nmodel, finding that they disrupt the attention of downstream heads that typically\nmove entity attributes to the final token.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have remarkable capabilities (Radford et al., 2019; Brown et al.,\n2020; Hoffmann et al., 2022; Chowdhery et al., 2023) yet have a propensity to hallucinate: gen-\nerating text that is fluent but factually incorrect or unsupported by available information (Ji et al.,\n2023; Minaee et al., 2024). This significantly limits their application in real-world settings where\nfactuality is crucial, such as healthcare. Despite the prevalence and importance of this issue, the\nmechanistic understanding of whether LLMs will hallucinate on a given prompt remains limited.\nWhile there has been much work interpreting factual recall (Geva et al., 2023; Nanda et al., 2023;\nChughtai et al., 2024), it has mainly focused on the mechanism behind recalling known facts, not on\nhallucinations or refusals to answer, leaving a significant gap in our understanding.\nLanguage models can produce hallucinations due to various factors, including flawed data sources\nor outdated factual knowledge (Huang et al., 2023). However, an important subset of hallucinations\noccurs when models are prompted to generate information they don't possess. We operationalize\nthis phenomenon by considering queries about entities of different types (movies, cities, players,\nand songs). Given a question about an unknown entity, the model either hallucinates or refuses to\nanswer. In this work, we find linear directions in the representation space that encode a form of\nself-knowledge: assessing their own knowledge or lack thereof regarding specific entities. These\ndirections are causally relevant for whether it refuses to answer. We note that the existence of this\nkind of self-knowledge does not necessarily imply the existence of other forms of self-knowledge,\nand may be specific to the factual recall mechanism.\nWe find these directions using Sparse Autoencoders (SAEs) (Bricken et al., 2023; Cunningham et al.,\n2023). SAEs are an interpretability tool for finding a sparse, interpretable decomposition of model\nrepresentations. They are motivated by the Linear Representation Hypothesis (Park et al., 2023;"}, {"title": "2 SPARSE AUTOENCODERS", "content": "Dictionary learning (Olshausen & Field, 1997) offers a powerful approach for disentangling features\nin superposition. Sparse Autoencoders (SAEs) have proven to be effective for this task (Sharkey\net al., 2022; Bricken et al., 2023). SAEs project model representations \u00e6 \u2208 Rd into a larger dimen-\nsional space a(x) \u2208 RdsAE. In this work, we use the SAEs from Gemma Scope (Lieberum et al.,\n2024)\u00b9, which use the JumpReLU SAE architecture (Rajamanoharan et al., 2024), which defines the\nfunction\n$SAE(x) = a(x)W_{dec} + b_{dec},$ \nWe use the default sparsity for each layer, the ones available in Neuronpedia (Lin & Bloom, 2024)."}, {"title": "3 METHODOLOGY", "content": "To study how language models reflect knowledge awareness about entities, we build a dataset with\nfour different entity types: (basketball) players, movies, cities, and songs from Wikidata (Vrande\u010di\u0107\n& Kr\u00f6tzsch, 2024). For each entity, we extract associated attributes available in Wikidata. Then, we\ncreate templates of the form (entity type, entity name, relation, attribute) and prompt Gemma 2 2B\nand 9B models (Team et al., 2024) to predict the attribute given (entity type, relation, entity name),\nfor instance:\nThe movie 12 Angry Men was directed by"}, {"title": "4 SPARSE AUTOENCODERS UNCOVER ENTITY RECOGNITION DIRECTIONS", "content": "We find that the separation scores of some of the SAE latents in the training set are high, i.e. they\nfire almost exclusively on tokens of either known or unknown entities. An interesting observa-\ntion is that latent separation scores reveal a consistent pattern across all entity types, with scores\nincreasing throughout the model and reaching a peak around layer 9 before plateauing. This indicates that latents better distinguishing between known and unknown entities are found in\nthe middle layers.\nWe also examine the level of generality of the latents by measuring their minimum separation score\nacross entity types (t): players, song, cities and movies. A high minimum separation score indicates\nthat a latent performs robustly across entity types, suggesting strong generalization capabilities. For\nthis purpose, for each layer (l) we compute MaxMinknown,l = max, mint sknown,t, and similarly for\nunknown entities. The increasing trend suggests that more generalized latents\u2014those\nthat distinguish between known and unknown entities across various entity types are concentrated\nin these intermediate layers. This finding points to a hierarchical organization of entity represen-\ntation within the model, with more specialized, worse quality, latents in earlier layers and more\ngeneralized, higher quality entity-type-agnostic features emerging in the middle layers.\nNext, we compute the minimum separation scores by considering every SAE latent in every layer,\ni.e. $mint s_{u}^{t} s_{l,j}^{known,t} $ for 1 < 1 < L and 1 \u2264 j \u2264 dsAE, and equivalently for unknown entities."}, {"title": "5 ENTITY RECOGNITION DIRECTIONS CAUSALLY AFFECT KNOWLEDGE\nREFUSAL", "content": "We define knowledge refusal as the model declining to answer a question due to reasons like a lack of\ninformation or database access as justification, rather than safety concerns. To quantify knowledge\nrefusals, we adapt the factual recall prompts as in Example 5 into questions:\nWho directed the movie 12 Angry Men ?\n\nand we define a set of common knowledge refusal completions and detect if any of these occur\nwith string matching, e.g. 'Unfortunately, I don't have access to real-time information...'. Gemma\n2 includes both a base model, and a fine-tuned chat (i.e. instruction tuned) model. In Section 4 we\nfound the entity recognition latents by studying the base model, but here focus on the chat model, as\nthey have been explicitly fine-tuned to perform knowledge refusal where appropriate , and the factuality of chat models is highly desirable.\nWe hypothesize that entity recognition directions could be used by chat models to induce knowl-\nedge refusal. To evaluate this, we use a test set sample of 100 questions about unknown entities,"}, {"title": "6 MECHANISTIC ANALYSIS", "content": "Entity Recognition Directions Regulate Attention to Entity. In the previous section, we saw\nthat entity recognition latents had a causal effect on knowledge refusal. Here, we look at how they\naffect the factual recall mechanism (aka circuit) in prompts of the format of Example 5. This has\nbeen well studied before on other language models (Nanda et al., 2023; Geva et al., 2023; Meng\net al., 2022a). We replicate the approach of Nanda et al. (2023) on Gemma 2 2B and 9B and find a\nsimilar circuit. Namely, early attention heads merge the entity's name into the last token of the entity,\nand downstream attention heads extract relevant attributes from the entity and move them to the final\ntoken position (Figure 4 (a, b)), this pattern holds across various entity types and model sizes (Ap-\npendix I and Appendix J). To do the analysis, we perform activation patching (Geiger et al., 2020;\nVig et al., 2020; Meng et al., 2022a) on the residual streams and attention heads' outputs (see Ap-\npendix H for a detailed explanation on the method). We use the denoising setup (Heimersheim &\nNanda, 2024), where we patch representations from a clean run (with a known entity) and apply it\nover the run with a corrupted input (with an unknown entity).\nExpanding on the findings of Yuksekgonul et al. (2024), who established a link between prediction\naccuracy and attention to the entity tokens, our study reveals a large disparity in attention between\nknown and unknown entities, for instance the attribute extraction heads L18H5 and L20H3 which are overall relevant across entity types in Gemma 2 2B (see example of attributes\nextracted by these heads in Appendix L). Notably, attention scores are higher when faced with a\nknown entity. We also observe a causal relationship between the entity recognition latents and the\nbehavior of these attention heads. Steering with the top unknown entity latent reduces the attention\nto the last token of the entity, even in prompts with a known entity , while steering\nwith the known entity latent increases the attention scores . We show the results of steering with a random vector baseline for comparison, and in Appendix K the re-\nsults of steering with a random SAE latent. In Appendix M we illustrate the average attention score\nchange to the entity tokens after steering on the residual streams of the last token of the entities\nWe use a validation set to select an appropriate steering coefficient a. In Appendix G we show generations\nof Gemma 2B IT with different steering coefficients. We select a \u2208 [400, 550], which corresponds to around\ntwo times the norm of the residual stream in the layers where the entity recognition latents are present (Ap-\npendix E).\nWe show the proportion of logit difference recovered after each patch in Figure 4 (a). A recovered logit\ndifference of 1 indicates that the prediction after patching is the same as the original prediction in the clean run."}, {"title": "7 UNCERTAINTY DIRECTIONS", "content": "Having studied how base models represent features for entity recognition, we now explore internal\nrepresentations that may differentiate between correct and wrong answers. Our investigation fo-\ncuses on chat models, which are capable of refusing to answer, and we search for directions in the\nrepresentation space signaling uncertainty or lack of knowledge potentially indicative of upcoming\nerrors. For this analysis we use our entities dataset, and exclude instances where the model refuses\nto respond, and leave only prompts that elicit either correct predictions or errors from the model.\nOur study focuses on the study of the residual streams before the answer. We hypothesize that end-\nof-instruction tokens, which always succeed the instruction, may aggregate information about the\nwhole question (Marks & Tegmark, 2023).\nWe select the token model and use examples such as:\n<start_of_turn>user\nWhen was the player Wilson Brown born?\n<end_of_turn>\n\nFor each entity type and layer with available SAE we extract the representations of the model resid-\nual stream, for both correct and mistaken answers, and gather the SAE latent activations. We are\ninterested in seeing whether there are SAE latents that convey information about how unsure or\nuncertain the model is to answer to a question, but still fails to refuse, giving rise to hallucinations.\nTo capture subtle variations in model uncertainty, which may be represented even when attributes are\ncorrectly recalled, we focus on quantifying differences in activation levels between correct and incor-\nrect responses. For each latent, we compute the t-statistic using two activation samples: a(xcorrect)\nfor correct responses and $a_{l,j}(\u00e6_{error})$ for incorrect ones. The t-statistic measures how different the\ntwo sample means are from each other, taking into account the variability within the samples:\n$t-statistic_{i,j} = \\frac{\\mu_{a_{l,j}(x_{correct}))} \u2013 \\mu_{a_{l,j}(x_{error}))}}{\\sqrt{\\frac{\\sigma^{2}_{a_{l,j}(x_{correct}))}}{n_{correct}} + \\frac{\\sigma^{2}_{a_{l,j}(x_{error}))}}{n_{error}}}}$"}, {"title": "9 CONCLUSIONS", "content": "In this paper, we use sparse autoencoders to identify directions in the model's representation space\nthat encode a form of self-knowledge about entities. These directions, found in the base model,\nare causally relevant to the knowledge refusal behavior in the chat-based model. We demonstrated\nthat, by manipulating these directions, we can control the model's tendency to refuse answers or\nhallucinate information. We also provide insights into how the entity recognition directions influ-\nence the model behavior, such as regulating the attention paid to entity tokens, and their influence in\nexpressing knowledge uncertainty. Finally, we uncover directions representing model uncertainty to\nspecific queries, capable of discriminating between correct and mistaken answers. This work con-\ntributes to our understanding of language model behavior and opens avenues for improving model\nreliability and mitigating hallucinations."}]}