{"title": "What You See Is Not Always What You Get: An Empirical Study of Code Comprehension by Large Language Models", "authors": ["BANGSHUO ZHU", "JIAWEN WEN", "HUAMING CHEN"], "abstract": "Recent studies have demonstrated outstanding capabilities of large language models (LLMs) in software engineering domain, covering numerous tasks such as code generation and comprehension. While the benefit of LLMs for coding task is well noted, it is perceived that LLMs are vulnerable to adversarial attacks. In this paper, we study the specific LLM vulnerability to imperceptible character attacks, a type of prompt-injection attack that uses special characters to befuddle an LLM whilst keeping the attack hidden to human eyes. We devise four categories of attacks and investigate their effects on the performance outcomes of tasks relating to code analysis and code comprehension. Two generations of ChatGPT are included to evaluate the impact of advancements made to contemporary models. Our experimental design consisted of comparing perturbed and unperturbed code snippets and evaluating two performance outcomes, which are model confidence using log probabilities of response, and correctness of response. We conclude that earlier version of ChatGPT exhibits a strong negative linear correlation between the amount of perturbation and the performance outcomes, while the recent ChatGPT presents a strong negative correlation between the presence of perturbation and performance outcomes, but no valid correlational relationship between perturbation budget and performance outcomes. We anticipate this work contributes to an in-depth understanding of leveraging LLMs for coding tasks. It is suggested future research should delve into how to create LLMs that can return a correct response even if the prompt exhibits perturbations.", "sections": [{"title": "1 INTRODUCTION", "content": "The effectiveness of Large Language Models (LLMs) at natural language processing has led to their popularity and application across many different fields, one of which is as an assistive tool for software developers. The ability to interact with them using natural language makes it very easy for developers to integrate such tools into their existing workflows. LLMs can assist with tasks such as code generation [13], program repair [26], and vulnerability detection [27].\nHowever, appearing to respond like a human does not mean that an LLM can reason the way a human can, and integrating these systems into workflows means taking on new vulnerabilities brought on by these LLMs. Considering their widespread adoption and continued growth in their sector, it is important to ensure the robustness of LLMs with regards to natural language processing tasks through studying their responses to attacks and understanding their mechanisms.\nThere have been several recent studies on text-based attacks against LLMs, targeting performance outcomes of natural language processing (NLP) tasks [8][18][22]. While these studies have been successful in proving the feasibility of text-based attacks, the feasibility of these attacks being applied in a real-world setting is still in question. In [14], it is found that an effective adversarial attack against a text-based LLM should produce text that is both valid and natural. Attacks that do not exhibit these characteristics may cause failed NLP model decisions, but their real-world effectiveness is limited if semantic consistency is not maintained, or if the adversarial example is clearly computer-generated. Through a survey of 378 human participants, Dyrmishi et al. concludes that existing text attacks are impractical in real-world scenarios where humans are involved [14]."}, {"title": "2 MOTIVATION", "content": "Recent advancements in machine learning technology have led to their widespread adoption across many different sectors. In particular, the ability to interact with a large language model using natural language has made them very easy to use for many people. However, as such systems have become more accessible \u2013 with ChatGPT exhibiting the fastest growth in userbase \u2013 the need to guarantee their security and reliability increases. Adversarial attacks against these models made through the prompting alone have been demonstrated to lead to a degradation in performance outcomes. There exists extant research confirming the deleterious effect of various attack techniques that inject perturbations into the prompt to manipulate the outcome of the LLMs. However, most research focuses on more typical natural language processing tasks such as sentiment analysis [15] or text classification [16].\nThe particular focus of this work is on the intersection between natural language processing and software testing. A very popular application of LLMs is in tools or processes that assist with computer programming, with LLMs demonstrating extraordinary potential in tasks such as code generation [10] and code comprehension [17]. However, due to the interaction with these tools being through the medium of natural language, they are vulnerable to sophisticated attack vectors that target the discrepancy between human comprehension and software comprehension. For example, the Unicode specification [11] defines many special Unicode characters that affect the rendering of symbols. This can lead to the content that the screen shows the user being extremely"}, {"title": "3 BACKGROUND", "content": "In this section, we briefly review the transformer architecture firstly introduced in 2017 [25], which underpins most of the popular large language models including BERT, BART, and GPT. Before transformer, the state-of-the-art approach to sequence modelling and transduction problems was considered to be recurrent neural networks (RNN). However, since RNNs factor computation along symbol positions of the input and output sequences, they exhibit an inherently sequential nature, which prevents parallelization. Without parallelization, the length of the sequence lengths is limited by hardware memory constraints. Increasing computational efficiency can curtail this limitation, but the fundamental restriction of sequential computation remains.\nThe key innovation the Transformer model exhibits is relying entirely on an attention mechanism in lieu of a recurrent network. Simply put, an attention mechanism allows the model to focus on different parts of the input sequence when trying to determine which parts are the most relevant to the eventual output sequence. Unlike a recurrent network, attention mechanisms are highly parallelizable, meaning that Transformer-model LLMs scale well with large amounts of training data. Critically, the attention mechanism also allows the model's predictions to be interpreted, through analysis of the weights placed on each part of the model."}, {"title": "3.2 Applications of LLMs", "content": "Potential applications of large language models are versatile and far reaching, with transformative potential across many different sectors.\nIn healthcare, Cascella et al. [9] evaluated the feasibility of using ChatGPT in four areas: support-ing clinical practice, scientific text production, misuse in medical research, and reasoning about"}, {"title": "3.3 Attacks against LLMs", "content": "Now that the applications of LLMs have been demonstrated with regards to programming tasks, this section will outline some existing text-based attack strategies.\nFormeto et al. [18] introduced the novel attack technique named the Special Symbol Text Attack (SSTA) in 2021. This technique exploits the fact that special symbols like punctuation marks or dashes contain downstream task information that can affect the LLM in NLP tasks such as sentiment analysis. The key finding is in the demonstration that symbols are an attack vector for text-based prompt injection attacks. However, the adversarial examples generated from this technique are clearly visible as confounding artifacts to the human eye. This leads us to the next paper.\nLi et al. [22] proposed BERT-attack in 2020, a method to generate adversarial examples using BERT against itself. BERT is a transformer-based language model, notable for being a dramatic improvement over other contemporaneous models [12]. The BERT-attack method consists of two steps: identify the most important word in the input sequence which contributes to the target prediction and keep replacing them with semantically similar words until the target model is fooled. The study used human evaluation to confirm that the adversarial examples generated with this method maintained high semantic similarity and grammatical correctness when compared to the clean samples.\nThe final piece of the background research is Boucher et al. [8] entitled \u2018Bad Characters: Imper-ceptible NLP Attacks' in 2022. This paper introduces a new class of human-imperceptible attacks against NLP systems that exploit the visual similarity \u2013 and sometimes sameness \u2013 of certain Unicode characters. Four classes of imperceptible attacks are defined: Reordering, invisible charac-ters, deletions, and homoglyphs, and adversarial examples were generated for a variety of models. Findings demonstrated that this new class of attacks could significantly degrade model performance on task-appropriate benchmarks, could target attacks, and could slow down processing speed by at least a factor of two. The potential for attacks such as these is enormous, with the authors discussing"}, {"title": "4 STUDY DESIGN", "content": "The central aim of this work is to examine the effectiveness and identify the limitations of imperceptible-character-based attacks against LLMs oriented towards code comprehension tasks."}, {"title": "4.1 Research questions", "content": "To guide this investigation, and keeping in line with the overall research aims and objectives, this study seeks to answer the following questions:\nRQ1: To what extent do imperceptible perturbations within code correlate with LLM performance outcomes in software testing scenarios?\nRQ2: Does a correlation exist between imperceptible perturbations and LLM \u2018confidence', and what implications does this have for software testing?\nRQ3: How significantly do different imperceptible perturbation methods impact the responses of an LLM model?\nRQ4: How do advancements in recent LLM iterations impact their robustness against impercepti-ble adversarial perturbations in software testing scenarios?"}, {"title": "4.2 Methodology", "content": "This study tested three different GPT models. We framed the research using a unified prompting strategy, which consists of three key elements:\n(1) Code snippet\n(2) Description of code snippet"}, {"title": "4.3 Models tested", "content": "In this study, we select three different GPT models for evaluation. In order of recency:\n\u2022 gpt-3.5-turbo-0613: A snapshot of the GPT-3.5 turbo model taken on June 13th 2023, representing a previous iteration of the GPT-3.5 turbo model.\n\u2022 gpt-3.5-turbo-0125: At the time of writing, the latest GPT-3.5 turbo model available. The results for this model demonstrate the impact of advancements made over a full year cycle.\n\u2022 gpt-40-2024-05-13: The most advanced model available in the OpenAI API [7]. This model represents the cutting edge of large language models, and conclusions can be drawn as to the different way it handles perturbation when compared to the older 3.5 generation models."}, {"title": "4.4 Dataset", "content": "To find the dataset for this experiment, inspiration was taken from a similar study [20] which also aimed to prompt ChatGPT with software-related questions. In that study, ChatGPT was tasked with answering questions from a \u2018popular software testing textbook'. Since this study needed a greater number of code snippets, that solution was not considered, but a similar solution was arrived at. A dataset of 2644 LeetCode coding questions was found through huggingface.co [1], an AI com-munity website featuring user-submitted models and datasets. This dataset was deemed appropriate due to containing code snippets as well as their related descriptions. This meant that it would not be necessary to find a 'base truth' for all code snippets due to the veracity of provided descriptions being effectively guaranteed by LeetCode. The questions in the dataset have three categories of difficulty: easy, medium, and hard. Each question has answers available in four different coding languages: java, c++, python, and JavaScript. For this experiment, only JavaScript code snippets were used, to control for the confounding variable of different languages."}, {"title": "4.5 Attack taxonomy", "content": "In this section, we discuss our solution for the imperceptible character attack design, which devises four categories of perturbation for evaluation, each featuring a special Unicode character implementation:\n\u2022 Reordering: Character U+202E is used so the Unicode specification can support characters from languages that read from right-to-left [2]. Inserting it at the head and tail of the reversed string means that to the human eye, the perturbed section is rendered in the correct order, but the input will be the reversed string.\n\u2022 Invisible characters: Character U+200C is a character that does not render to a visible glyph. When placed between two characters that would otherwise be joined by a ligature, it keeps them disconnected. It is also effectively a space character."}, {"title": "4.6 Attack generation", "content": "To increase the control of the experiment, the decision was made to create perturbed samples programmatically, rather than generatively. This means that the data perturbation process was open-source and consistent, rather than generating them using a process that both involves closed-source software and produces inconsistent output.\nThe perturbation process could not be exactly replicated across each category of perturbation, but each process shared in common the following 5 traits:\n(1) Each perturbation is rendered identically to the clean code\n(2) The perturbations are inserted starting from beginning to end\n(3) The levels of perturbation budget are set at 20%, 40%, 60%, 80%, and 100%\n(4) The perturbations are created with special Unicode characters, but not exclusively so\n(5) The floor is taken for any mathematical operation involving non-integer numbers"}, {"title": "4.7 Prompting", "content": "All three GPT LLMs were accessed through the OpenAI API with the written script, which can be found in the replication package. Each prompt was made using the Chat Completions API. A zero-shot prompting technique was used. The template of each prompt remained consistent across all categories of perturbations and all models.\nThe prompt template consisted of the following three messages:\n(1) Question\n(2) Code\n(3) Description\nThe question remains constant across all prompts, and reads:\nDoes the provided code match the provided description? Answer with either Yes or No.\nWe note the similar question in the study of [17], which is open-ended. However, due to the different methodology design and evaluation scheme, we reword the question in a subtly different way, which only allow a one-word answer. Moreover, the code and description were taken from the LeetCode dataset. The description is thus treated as the \u2018base truth', in that it can be guaranteed due to the nature of the dataset that the correct answer to the question is always \u2018Yes'."}, {"title": "4.8 Logprobs", "content": "To present a thorough data analysis, we used the 'logprobs' parameter of the Chat Completions API as the main vector. With logprobs enabled, the Chat Completions API returns the log probabilities of each output token in the response. Log probabilities of output tokens indicate the likelihood of"}, {"title": "5 RESULTS", "content": "From the Tables 2 - 5, we can immediately find many similarities across the 4 categories. As expected, the confidence score for the clean code snippet is high with none below 95%, and the correctness is 100% across the board. The highest confidence score is held by invisible characters at"}, {"title": "5.2 GPT-3.5-turbo-0125", "content": "The results for this model displays a similar profile to the model previous, with all categories sharing characteristics in common. The clean code is 100% correct and has the highest confidence score across the board, with all categories at 93\u00b11%. The highest confidence score is held by invisible"}, {"title": "5.3 GPT-40-2024-05-13", "content": "Tables 10 - 13 display a clear and significant difference to the two GPT-3.5 models. However, there is one characteristic held in common across the results of all three models. Across all 4 categories, the clean code is still 100% correct and still holds the highest confidence score, with all of them holding a score of 81\u00b11%. However, from there, things change drastically. The GPT-4 results are the only results to show negative confidence scores, which is due to the disproportionate number of incorrect answers, with the lowest score of -19.08% held by reorders at 20% perturbation budget. As with confidence scores, correctness is also extremely low across the board, with the next highest"}, {"title": "5.4 Summary", "content": "The similarities between the two GPT-3.5 models and their difference compared to the GPT-40 model reveal nuanced insights into the impact of perturbations on their respective performance outcomes. The GPT-3.5 models exhibit uniform decline in both confidence and correctness, with some categories exhibiting a natural limit when approaching 100% perturbation. In contrast to that, the GPT-40 model shows a massive decrease in both confidence and correctness at any perturbation budget level, with correctness approaching 0 across all categories. This seems to indicate that the newer model has some kind of special character detection and vetting that the older GPT-3.5 generation models do not have.\nCritically, all models responded with 100% correctness for the clean code, and all models demon-strated a significant decline in both scores with any amount of perturbation. Whilst evaluating code comprehension was not an explicit experiment objective, it is helpful for the overall research goals to see data that validates the baseline code comprehension skills of these large language models."}, {"title": "5.5 Answer to research questions", "content": "To answer this question, we take the term \u2018performance outcomes' to refer to the correctness of a model's responses. The introduction of perturbations can be found to greatly affect the correctness of the model responses, which can be gleaned simply from looking at the tables containing the summary of results. Statistical analysis confirms this conclusion using Pearson's correlation coefficient, with a moderate-to-strong negative correlation coefficient found between perturbation budget and response correctness for all perturbation categories, as shown in Table. 14. Simply put, this means that as perturbation goes up, correctness goes down proportionally. The two GPT-3.5 generation models have very similar results, with both appearing to exhibit significantly higher negative correlation than the GPT-40 model. If only considering the correlation coefficient, it seems as if the GPT-40 model also shows a very high degree of normalization between the four categories of perturbation compared to the two 3.5 models. However, knowing that the results show a distribution discrepancy between the GPT-3.5 generation models and the GPT-40 model, we can investigate further.\nIf we consider the entire range of perturbation budget levels \u2013 i.e.: 0% - 100% \u2013 the correlation coefficient for the GPT-40 model results stands at around -0.6 for all categories a moderately strong negative correlation. However, if we discard the results for the clean code, and only consider the perturbation budget levels greater than 0% \u2013 i.e.: we only consider code that has been perturbed we find strong positive correlation for the reordering and deletion categories, whilst maintaining strong negative correlation for the remaining categories.\nTaking another look at the raw data explains why this is the case."}, {"title": "6 DISCUSSION", "content": "The purpose of this study is to contribute to the depth of research in large language model per-turbation, rather than the breadth. Critical insight is gained into the inner mechanisms of three iterations of the most popular large language model in use [21], which has been widely adopted in certain security-sensitive sectors whilst being closed-source.\nThe process of undertaking this study has also provided insight into how the experiment method-ologies of subsequent studies should be designed. Knowing now that the 'flagship' OpenAI product [7] exhibits behaviour that indicates the existence of deterministic guardrails put in place between the 3.5 and 4 generation models, we hope that ensuing research can investigate the finer details of such guardrails, and their effects on a system that is designed to be interacted with using natural language. Guardrails alone can negate all false positives, but a truly sophisticated system should rec-ognize the presence of confounding characters, parse the code according to the spirit of the prompt, and return a correct answer, nonetheless; there should be no correlation between perturbation and correctness at all. However, in the absence of such a sophisticated system, it is encouraging to see that the evolution from 3.5-generation GPT to 4-generation GPT brings with it an unequivocally more consistent model."}, {"title": "6.2 Contributions and implications", "content": "In this study, we devise four different categories of perturbation on three different LLM models. For the two older generation models, a moderately strong negative linear correlation was found between the level of perturbation and the performance outcomes of the response. The newer generation model appears to exhibit behaviours that indicate the existence of deterministic guardrails.\nThe perturbation category of deletions was found to have the highest overall effect on perfor-mance outcomes, followed by reordering, invisible characters, and finally by homoglyphs. The results of homoglyphs are not entirely conclusive as the study methodology calculates their pertur-bation budget differently to the other three categories; further investigation can be made.\nFindings of the study further establish the baseline effectiveness of LLMs in code comprehension tasks, with all 18 000 clean code prompts returning responses with the correct answer and a high internal confidence score.\nThe methodology of this study draws on practices of preceding studies. However, efforts have been made to reduce the amount of qualitative analysis in data gathering, so that results are not hampered by the subjective confounding variables inherent to human analysis. Particular emphasis is placed on not gathering data on model confidence through model self-reporting, instead, using quantitative tools to provide evidence in this regard. This study made use of the 'logprobs' property of the Chat Completions API so that conclusions regarding model confidence can be drawn from quantitative data rather than qualitative model self-reporting \u2013 it is the author's hope that subsequent studies will implement similar quantitative analysis procedures regarding confidence."}, {"title": "6.3 Assumptions and limitations", "content": "This study was designed with the assumption that a 'No' response is a strong indication that the large language model does not understand the code at all. The \u2018ideal behaviour' of the model in the event of an imperceptible prompt injection attack was defined as it being able to parse it as a human does \u2013 i.e.: ignore or \u2018reverse-engineer' the perturbed code and parse the \u2018cleaned' version. However, the results of the study \u2013 especially the discrepancy between the GPT-3.5 models and the GPT-40 model \u2013 show that this projected \u2018ideal behaviour' is of dubious import, and that it may rather be better to create a system which can guarantee no false negatives, rather than only being able to answer a subset of adversarial examples.\nA secondary objective of this research has been to harden data gathering procedures against confounding variables which are exhibited in extant research. LLM self-evaluation has been used in preceding studies to record LLM confidence scores, but an explicit requirement in the methodology design of this study is to avoid self-reporting to side-step potential confounding variables inherent in relying upon a closed-source system to evaluate itself. However, it may be the case that log probabilities are also not a perfect representation of model confidence.\nWhat the log probabilities represent is the probability a token is chosen to appear in a specific context [6]. For example, imagine a model is prompted with the straightforward question: \u2018Is the sky blue?' and it returns a single-token response \u2018Yes'. This token has a log probability which converts to a linear percentage value of 90%. What that log probability tells us is that this hypothetical model had a 90% chance to respond with 'Yes', and a 10% chance to respond with anything else. Now, let us imagine that we prompt the model with a more subjective question such as: \u2018Is the Maserati Quattroporte a beautiful car?' and it again returns a single-token response \u2018Yes'. The"}, {"title": "6.4 Future work", "content": "The greatest limitation faced by this study was finding a way to quantitatively assess model confidence. It is suggested to use novel and effective practices or tools in the future that can better guarantee the validity of confidence scores to produce data from which concrete conclusions can be drawn.\nFuture research should also delve into how we can create LLM-based systems that can generate a correct response even if the prompt exhibits certain level of perturbations. This is to bridge the gap between what the user experiences and expect, and what the model truly \u2018understands'. Guardrails have been shown to negate false positives entirely, which is a marked improvement upon previous generations of LLMs. However, a truly sophisticated natural language processor should be able to follow the spirit of a prompt, rather than the letter of it."}, {"title": "7 CONCLUSION", "content": "The findings in this paper indicate that where some LLMs were susceptible to imperceptible attacks, some newer version of LLMs have enacted measures to guard against these false positives, such ChatGPT40. However, even between the older 3.5 generation models, the newer model showed marked improvement in comparison to the older model. Throughout the study, we observed that perturbation budget had a strong negative correlation to performance outcomes for code analysis and comprehension task. There is a strong negative correlation between the presence of perturbation and performance outcomes for newer version of LLMs, but no valid correlational"}]}