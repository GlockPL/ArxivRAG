{"title": "The Only Way is Ethics: A Guide to Ethical Research with Large Language Models", "authors": ["Eddie L. Ungless", "Nikolas Vitsakis", "Zeerak Talat", "James Garforth", "Bj\u00f6rn Ross", "Arno Onken", "Atoosa Kasirzadeh", "Alexandra Birch"], "abstract": "There is a significant body of work looking at the ethical considerations of large language models (LLMs): critiquing tools to measure performance and harms; proposing toolkits to aid in ideation; discussing the risks to workers; considering legislation around privacy and security etc. As yet there is no work that integrates these resources into a single practical guide that focuses on LLMs; we attempt this ambitious goal. We introduce LLM ETHICS WHITEPAPER, which we provide as an open and living resource for NLP practitioners, and those tasked with evaluating the ethical implications of others' work. Our goal is to translate ethics literature into concrete recommendations and provocations for thinking with clear first steps, aimed at computer scientists. LLM ETHICS WHITEPAPER distils a thorough literature review into clear Do's and Don'ts, which we present also in this paper. We likewise identify useful toolkits to support ethical work. We refer the interested reader to the full LLM ETHICS WHITEPAPER, which provides a succinct discussion of ethical considerations at each stage in a project lifecycle, as well as citations for the hundreds of papers from which we drew our recommendations. The present paper can be thought of as a pocket guide to conducting ethical research with LLMs.", "sections": [{"title": "1 Introduction", "content": "As LLMs grow increasingly powerful, their advancements in natural language understanding and generation are impressive (Min et al., 2023). However, mitigating the risks they present remains a complex challenge, and categorising these risks is a crucial aspect of ethical research related to LLMs (Weidinger et al., 2022). Key concerns include the potential to perpetuate and even amplify existing biases present in training data (Gallegos et al., 2024), challenges in safeguarding user privacy (Yao et al., 2024), hallucination or incorrect responses (Abercrombie et al., 2023; Xu et al., 2024), malicious use of their capabilities (Cuthbertson, 2023), and infringement of copyright (Lucchi, 2023). Given that many of these ethical challenges remain unresolved, it is essential for those involved in developing LLMs and LLM-based applications to consider potential harms, particularly as these models see broader adoption.\nSeveral frameworks have already been developed to address AI ethics and safety. For example The U.S. National Institute of Standards and Technology (NIST) has an AI Risk Management Framework (RMF)\u00b9, which provides broad guidelines for managing AI-related risks. NIST has also recently released a document outlining specific risks and recommended actions for Generative AI\u00b2. Whilst widely adopted, the NIST guidelines are voluntary. In contrast, the EU AI Act\u00b3 represents a legally binding regulatory framework designed to ensure the safe and ethical use of AI within the European Union. It emphasises transparency, human oversight, and the prevention of discriminatory outcomes, with the goal of protecting fundamental rights and promoting trustworthy AI.\nThe NIST AI RMF and EU AI Act are broad, focusing on AI deployment and risk management across industries. There are other frameworks which are more research-focused, guiding ethical considerations in academic AI work. For example the Conference on Neural Information Processing Systems (NeurIPS) Ethics Guidelines\u2074 evaluates Al research for ethical concerns as part of the paper submission process. A similar effort from the Association of Computational Linguistics (ACL) has created an Ethics Checklist which guides authors"}, {"title": "2 Methodology", "content": "A primary goal of LLM ETHICS WHITEPAPER is to provide a comprehensive directory of resources for ethical research related to LLMs. As such, a systematic literature review of the ACL Anthology was conducted (note that LLM ETHICS WHITEPAPER is not itself strictly a systematic literature review paper, see below). The Anthology was searched for paper abstracts containing at least one term from the following key term lists: related to the type of resource = tool[a-z]*, toolkit, [A-za-z]*sheets*, guidelines*, principles, framework, approach; related to ethics = ethic[a-z]*, harms*, fair[a-z]*, risks*. These lists were determined by first using more comprehensive lists then eliminating terms to improve the precision of the search. The resulting papers were manually reviewed to determine which were relevant to the scope of LLM ETHICS WHITEPAPER. During the search we identified a 2023 EACL tutorial titled \u201cUnderstanding Ethics in NLP Authoring and Reviewing\" (Benotti et al., 2023). The references for this tutorial were manually reviewed and where relevant included in LLM ETHICS WHITEPAPER.\nA second literature review was conducted using Semantic Scholar using the search terms: toolkit OR sheets OR guideline OR principles OR framework OR approach ethics OR ethical OR harms OR fair OR fairness OR risk AND \"language models\". These were likewise manually reviewed for inclusion."}, {"title": "3 Project Start", "content": "The social risk of generative AI including LLMs, can have wide-reaching effects from representational harms to safety concerns, which has been widely recognised (see e.g., Weidinger et al., 2021; Bender et al., 2021; Uzun, 2023; Wei and Zhou, 2022). This recognition has given rise to a large number of efforts seeking to evaluate their risks and actualised harms, each effort presenting its own limitations (Solaiman et al., 2024; Goldfarb-Tarrant et al., 2023; Blodgett et al., 2020). Nevertheless, efforts towards developing technologies that minimise harms, in particular to marginalised communities, are vital. Best efforts require considering a wide range of topics and questions that must be adapted to each individual application and deployment context. In this Section we explain why ethics is relevant to all practitioners (Section 3.1), then highlight resources to aid in the initial discovery process (Section 3.2). We also lay out best practice that will be valuable to all those working with language technologies, namely related to working with stakeholders, and environmental considerations (Section 3.3 and Section 3.4)."}, {"title": "3.1 Who needs ethics?", "content": "As computer science becomes pervasive in modern lives, so too does it become intertwined with the experience of those lives. Decisions made by researchers and developers compound together to influence every aspect of the technical systems which ultimately govern how we all live (Winner, 1980). This is often at a scale, or level of complexity, which makes it impossible to seek clear resolutions when outcomes are harmful (Van de Poel, 2020; Kasirzadeh, 2021; Miller, 2021; Birhane et al., 2022; Santurkar et al., 2023; Pistilli et al., 2024).\nTechno-cultural artefacts, such as LLMs, have political dimensions (Winner, 1980), because they further entrench certain kinds of power e.g. marginalised peoples' data is often used without consent or compensation; technology typically works best for language varieties associated with whiteness (Blodgett and O'Connor, 2017); benchmarks are published which are biased against minorities (Buolamwini and Gebru, 2018). Unfortunately, the training and work cultures of computer scientists can condition us to believe we can ignore power relations (Malazita and Resetar, 2019), because the \"objective\" nature of our work seems to absolve us of having to consider issues of our technologies in the world (Talat et al., 2021) \u2013 when dealing with code and numbers it becomes easier to forget about the real humans who are impacted by our design choices. LLMs are no exception (Leidner and Plachouras, 2017), though their recent rise in prevalence has made their ethical dimensions more salient (and more vital to address).\nThe design of techno-cultural artefacts like LLMs should be considered interdisciplinary by its very nature, as it requires an understanding of the physical and social systems that they must interact with in order to achieve their function. Experts exist in all of these other areas of study, as well as their intersections, but very often our lack of appreciation for their expertise, or lack of shared language, impede us from seeking them out. This is especially true for expertise in the social sciences and philosophy (Raji et al., 2021; Inie and Derczynski, 2021; Danks, 2022).\nThere is a tendency to assume that social and ethical issues are not designers' problems but someone else's (Widder and Nafus, 2023), but this is not the case. If you do not reflect on your design decisions as you make them then you are complicit"}, {"title": "3.2 Laying the Groundwork", "content": "It is important to think about ethics from the very beginning, in order to be able to question all aspects of the project, including if specific tasks should even be undertaken. One way of doing this is by using ethics sheets (Mohammad, 2022), which are sets of questions to ask and answer before starting an AI project. It includes questions like \"Why should we automate this task?\", and \"How can the automated system be abused?\u201d.\nAn alternative is using the Assessment List for Trustworthy Artificial Intelligence (ALTAI), which is a tool that helps business and organisations to self-assess the trustworthiness of their AI systems under development. The European Commission appointed a group of experts to provide advice on its artificial intelligence strategy and they translated these requirements into a detailed Assessment List, taking into account feedback from a six month long piloting process within the European AI community. You could use question sets such as these to ensure ethical considerations are present from the start of your project.\nRegulated industries such as aerospace, medicine and finance have critical safety issues to address, and a primary way these have been addressed is using auditable processes throughout a project. Audits are tools for interrogating complex processes, to determine whether they comply with company policy and industry standards or regulations (Liu et al., 2012). Raji et al. (2020) introduce a framework for algorithmic auditing that supports artificial intelligence system development, which is intended to contribute to closing the gap between principles and practice. A formal process such as this can help by raising awareness, assigning responsibility, and improving consistency in both procedures and outcomes (Leidner and Plachouras, 2017). At the very least, your organisation should establish an ethics review board to evaluate new products, services, or research plans."}, {"title": "3.3 Stakeholders", "content": "Given the vast amounts of training data required and the wide-reaching applications of LLMs, every project will have many stakeholders e.g. those who provide the data (Havens et al., 2020), end-users of the application (Yang et al., 2023), or those a model will be used on, who are often given limited power to influence design decisions e.g. migrants (Nalbandian, 2022). A vital early step is identifying key direct stakeholders and establishing the best ways to work with them in order to build systems that are widely beneficial \u2013 which will be highly context dependent. The ideation toolkit (Sloane et al., 2022) will help you to identify stakeholders, and can be used alongside existing taxonomies e.g. (Lewis et al., 2020; Langer et al., 2021; Bird et al., 2023; Havens et al., 2020, i.a.). Crucially, stakeholders should be identified before development, so they can (if they wish) be involved in co-production \u2013 and object to proposed technologies. Kawakami et al. (2024) present a toolkit for early stage deliberation with stakeholders which includes question prompts, while Caselli et al. (2021) provide 9 guiding principles for effective participatory design \u2013 design which involves mutual learning between designer and participant \u2013 in the context of NLP research.\nYou must consider power relations between stakeholders (Havens et al., 2020), and also between yourself and the stakeholders. Reflexive considerations about a researcher's own power are rare in computer science research (Ovalle et al., 2023; Devinney et al., 2022) but can help establish the limitations of your work (Liang et al., 2021; Liang, 2021).\nWhen working on technologies for indigenous and endangered languages, sensitive stakeholder collaboration is particularly important (Bird, 2020; Liu et al., 2022; Mahelona et al., 2023). Work on stakeholder engagement in NLP can learn much from the Indigenous Data Sovereignty movement (Sloane et al., 2022)."}, {"title": "3.4 Energy Consumption", "content": "Throughout the life cycle of a project, you should consider the energy consumption of your model, which relates to data sourcing practices, model design, choice of hardware, and use at production. Strubell et al. (2019) suggest that model development likely contributes a \"substantial proportion of the... emissions attributed to many NLP"}, {"title": "3.5 Key Resources", "content": "Do's and Don'ts\n\u2022 Do engage with affected communities from the beginning \u2013 don't just ask for their feedback\n\u2022 Do allow for flexibility in project direction as informed by stakeholder input \u2013 don't assume what communities want and need\n\u2022 Do consider the power relations between stakeholders - don't forget about the relationships with yourself\n\u2022 Do engage with ethics review boards to ensure oversight, or set one up if necessary - don't assume because it's computer science that moral and political values are out of scope\n\u2022 Do create an internal audit procedure to ensure ethical processes are developed and followed - don't just leave it to a post-hoc review\n\u2022 Do consider use of compressed models and cloud resources to minimise energy impact - don't assume you need energy intensive models for the best performance\nUseful Tool(kit)s:\n\u2022 Ethics sheets to discover harms and mitigation strategies - Mohammad (2022)\n\u2022 The Assessment List for Trustworthy Artificial Intelligence (ALTAI)9\n\u2022 Internal audit framework to ensure that ethical processes are implemented and followed \u2013 Raji et al. (2020)\n\u2022 Value Scenarios framework to identify likely impact of technology \u2013 Nathan et al. (2007)\n\u2022 Guiding principles for effective participatory design - Caselli et al. (2021)\n\u2022 Best practice for reducing carbon footprint during training \u2013 Patterson et al. (2022)\n\u2022 Taxonomy of tools available to measure environmental impact of NLP technologies \u2013 Bannour et al. (2021)\n\u2022 Software package to estimate carbon dioxide required to execute Python codebase - https://github.com/mlco2/codecarbon"}, {"title": "4 Data compilation", "content": "In this Section of LLM ETHICS WHITEPAPER we discuss best practice for compiling original data sets, and critique typical practices such as the position of data as a raw resource rather than something that is transformed by the act of collection. We discuss best practice for addressing issues of consent and safety which includes distinguishing those who produce data and those featured in the data (\"data subjects\") and respecting their potentially distinct rights. We also discuss best practice for sharing or using shared resources such as thorough documentation and using API such as Elazar et al. (2024) to explore large data sets. For a full discussion of this section and following sections, please see the LLM ETHICS WHITEPAPER.\nKey Resources\nDo's and Don'ts\n\u2022 Do reflect on and document the decisions you make when collecting data \u2013 don't forget that how you collect data transforms it\n\u2022 Do consider if it is ethical to scrape web content, even for content that is publicly available (e.g., by relying on frameworks of ethical data scraping such as Mancosu and Vegetti (2020)) - don't crawl content that website creators have indicated should not be crawled (e.g. via robots.txt files)\n\u2022 Do consider the subjects of the data \u2013 don't just think about the rights of data producers\n\u2022 Do respect copyright and privacy from the beginning - don't expect the public to do the work of requesting removal (but give them the"}, {"title": "5 Data preparation", "content": "In this Section of LLM ETHICS WHITEPAPER we discuss how attempts to clean and filter data can cause harm, even where the intention was to prevent harm! For example, toxicity detection systems, be it word lists or ML models, are typically biased in flagging sentences containing marginalised identity terms as toxic (Bender et al., 2021; R\u00f6ttger et al., 2021; Calabrese et al., 2021). Each cleaning step should be carefully justified. We also discuss best practice for working with crowd workers, who we recommend be treated as human participants (e.g. research is subject to approval from ethical review board where possible). We offer guidance for those designing target label taxonomies, such as carefully consider what is assumed and what is lost through your choice of proxy (Guerdan et al., 2023). We discuss how to handle disagreement between annotators, which are common in subjective tasks and can reflect ideological differences.\nKey Resources\nDo's and Don'ts\n\u2022 Do carefully reflect on whose data you are excluding when cleaning - don't rely on popular tools to give you fair results\n\u2022 Do make explicit what information you are trying to record with your choice of proxy - don't forget that labels and proxies are simplifications\n\u2022 Do work with affected communities to define labels and annotate your data - don't forget that harm is subjective, and a spectrum\n\u2022 Don't release low quality data that may be repurposed for evaluation\n\u2022 Do gather information about your annotators - don't assume annotators with similar identities will give similar annotations\n\u2022 Do treat crowdworkers as human participants and follow best practice for human participant research, such as collecting informed consent; seek formal ethics (e.g. Institutional review board) approval where applicable \u2013 don't assume that when using paid annotators you do not need to follow typical ethics procedures\nUseful Tool(kit)s:\n\u2022 Recommendations for those conducting data filtering - Hong et al. (2024)\n\u2022 Taxonomy of personal information and best practice for privacy \u2013 Subramani et al. (2023)\n\u2022 Guidance of selecting proxy labels \u2013 Guerdan et al. (2023)\n\u2022 Best practice when using identity terms as labels - Larson (2017)\n\u2022 Detailed overview of risks of using crowdworkers - Shmueli et al. (2021)"}, {"title": "6 Model Development + Selection", "content": "In this Section of LLM ETHICS WHITEPAPER we discuss the ethical ramifications of model design and training, and pre-trained model selection decisions. We echo Hooker (2021) in arguing against the belief that all bias issues stem from data imbalance and explain how subtle model design changes can have big impacts on fairness. We also discuss the limitations of debiasing, as current techniques often make superficial changes (Gonen and Goldberg, 2019), fail to relate to downstream improvements (Steed et al., 2022), and can in fact exacerbate harm (Xu et al., 2021). We also briefly touch on alignment techniques, exploring the difficulty of defining human values (Kasirzadeh and Gabriel, 2023; Kasirzadeh, 2024) and of maintaining alignment throughout a project.\nKey Resources\nDo's and Don'ts\n\u2022 Do consider how subtle changes can improve performance for marginalised people \u2013 don't"}, {"title": "7 Evaluation", "content": "Here we discuss some of the ethical problems that can arise during performance evaluation, due for example to evaluation not being robust. We offer best practice and cautions for effective performance evaluation. For example, we caution that benchmarks are not objective and can encourage chasing scores which do not relate to real world improvements. We also discuss in detail the benefits and limitations of many harm evaluation strategies. Despite the ubiquitous nature of the harms of LLMs (Rauh et al., 2022; Weidinger et al., 2021), the study of such harms has yet to be standardised. Attempts often lack rigour (Blodgett et al., 2020, 2021; Goldfarb-Tarrant et al., 2023). We briefly present some popular methods for evaluating harms in LLMs, discuss ethical implications and make recommendations.\nKey Resources\nDo's and Don'ts\n\u2022 Do pair bias metrics that relate to real world (downstream) harms with human evaluation - don't rely on quick, quantitative metrics alone, as evaluation in language generation can be unreliable\n\u2022 Do develop and use benchmarks to evaluate concrete, well-scoped and contextualised tasks - don't present benchmarks as markers of progress towards general-purpose capabilities\n\u2022 Do carefully reflect on what specific harm you are trying to measure and why the methodology you have created or borrowed is appropriate - don't assume bias metrics can be re-used in all new contexts\n\u2022 Do use alternatives to benchmarks which attempt to capture broader capabilities and risks e.g. audits (e.g. Buolamwini and Gebru (2018)), adversarial testing (e.g. Niven and Kao (2019)) and red teaming (Ganguli et al., 2022)\n\u2022 Do involve experts and community members in the evaluation of the models \u2013 don't rely on your intuitions and initial assumptions alone\nUseful Tool(kit)s:\n\u2022 Tools to facilitate test ideation - Ribeiro et al. (2020)\n\u2022 Taxonomy of LLM evaluations \u2013 Chang et al. (2023) - in particular Section 3.2 on evaluating robustness, ethics, bias, and trustworthiness\n\u2022 Repository of tests for (English) language generation safety \u2013 Dinan et al. (2022)\n\u2022 Test suite to identifying exaggerated safety behaviour - R\u00f6ttger et al. (2024)\n\u2022 Taxonomy of tests for safety and trustworthiness in LLMs \u2013 Huang et al. (2023)\n\u2022 Framework for testing reliability of NLP systems - Tan et al. (2021)\n\u2022 Bias tests across hundreds of identities (in English) - Smith et al. (2022)\n\u2022 Framework for addressing Sociotechnical (contextualised) Safety - Weidinger et al. (2023)"}, {"title": "8 Deployment", "content": "In this Section of LLM ETHICS WHITEPAPER we summarise likely harms of LLMs after deployment. We introduce the notion of dual \u2013 both negative and positive \u2013 use of LLMs. We discuss the impact of different deployment strategies and the limitations of guardrails. We explain the ramifications of using LLMs to replace humans. Finally we discuss best practice when disseminating your ideas.\nHerein we provide a summary of our discussion of risks. In their broad overview of the harms that"}, {"title": "9 Limitations and Future Directions", "content": "Whilst our Do's and Don'ts are applicable regardless of model language, some of our recommended resources are specific to English. Further, all language-specific resources we discuss in LLM ETHICS WHITEPAPER are specific to English. This reflects a tendency for evaluation resources to be produced only for English, but also the first authors' lack of familiarity with non-English language resources. We extend a similar qualifier in our inclusion of ethical resources that reflect a largely Western moral perspective. Similarly, this paper primarily addresses the text modality, and does not cover other modalities like speech and images. As LLM ETHICS WHITEPAPER is intended as a living document, we can integrate further non-English, non-Western and non-text based resources in future. Moreover, the do's and don'ts are presented in terms of languages for which large amounts of resources already exist. Languages for which few resources exist may need additional consideration in terms of data and data subject safety.\nThis paper and LLM ETHICS WHITEPAPER do not provide full considerations of the topics covered, but rather serve as syntheses with directions for future reading. Moreover, LLM ETHICS WHITEPAPER and this paper are informed by the literature they rely on, and do not claim to cover all topics of relevance for the development of LLM and LLM-based applications. The Do's and Don'ts we have drafted are not intended to be the final rule in LLM design. Further, it is crucial that readers of this and LLM ETHICS WHITEPAPER situate the considerations of harms of their work within the contexts that their tools will be applied in. LLM ETHICS WHITEPAPER and the Do's and Don'ts can be thought of as starting points, which we will revise in response to community feedback and further consideration for the subject matter of each section. We welcome input from practitioners on how to make this resource most useful. We are hosting LLM ETHICS WHITEPAPER on Github to"}, {"title": "10 Conclusion", "content": "In this paper, we have briefly summarised the topics covered in LLM ETHICS WHITEPAPER, and highlighted topics of particular interest. We synthesise arguments that LLMs and LLM-based applications can have large impacts on society, and therefore developers of such systems need to attend to the types of harms they risk, and seek to mitigate such risks. Here, and in LLM ETHICS WHITEPAPER, we seek to address the gap in resources for conducting ethical research with LLMs that falls between professional association guidelines, and AI frameworks with extremely broad scope, and provide researchers and practitioners with a starting point for their inquiry into ethical research with and development of LLM applications."}]}