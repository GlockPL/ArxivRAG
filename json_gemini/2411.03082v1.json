{"title": "Self-supervised cross-modality learning for uncertainty-aware object detection and recognition in applications which lack pre-labelled training data", "authors": ["Irum Mehboob", "Li Sun", "Alireza Astegarpanah", "Rustam Stolkin"], "abstract": "This paper shows how an uncertainty-aware, deep neural network can be trained to detect, recognise and localise objects in 2D RGB images, in applications lacking annotated training datasets. We propose a self-supervising \"teacher-student\" pipeline, in which a relatively simple \"teacher\" classifier, trained with only a few labelled 2D thumbnails, automatically processes a larger body of unlabelled RGB-D data to teach a \"student\" network based on a modified YOLOv3 architecture. Firstly, 3D object detection with back projection is used to automatically extract and \"teach\" 2D detection and localisation information to the student network. Secondly, a weakly supervised 2D thumbnail classifier, with minimal training on a small number of hand-labelled images, is used to teach object category recognition. Thirdly, we use a Gaussian Process (GP) to encode and teach a robust uncertainty estimation functionality, so that the student can output confidence scores with each categorization. The resulting student significantly outperforms the same YOLO architecture trained directly on the same amount of labelled data. Our GP-based approach yields robust and meaningful uncertainty estimations for complex industrial object classifications. The end-to-end network is also capable of real-time processing, needed for robotics applications. Our method can be applied to many important industrial tasks, where labelled data-sets are typically unavailable. In this paper, we demonstrate an example of detection, localisation, and object category recognition of nuclear mixed-waste materials in highly cluttered and unstructured scenes. This is critical for robotic sorting and handling of legacy nuclear waste, which poses complex environmental remediation challenges in many nuclearised nations.", "sections": [{"title": "I. INTRODUCTION", "content": "This paper addresses the computer vision problems of detecting, recognising and localising objects. Our proposed method has broad potential to be used for many applications and object types. It is especially useful for industrial or applied problems, where large amounts of application-specific annotated training data are typically unavailable. We demonstrate such an application with a motivating example of robotics challenges in extreme environments, for example, robotic sorting of nuclear waste objects and materials, for the safe remediation of legacy nuclear facilities [1].\nThe UK alone contains an estimated 4.9 million tonnes of legacy nuclear waste [2], much of it dating back many decades. Waste items can comprise numerous objects, e.g. contaminated gloves, respirators, swabs, tools, containers, and pipework sections. At the Sellafield site (dating back to the 1940s), a new plant is being built which will use robot arms for the next 50 years. These will cut open old containers, for which there is some uncertainty about the contents. The robots must sort and identify waste items, separate them according to the estimated hazard level, and repackage them into safer modern containers. In addition to the potential for computer vision to help guide robots during e.g. pick and place operations, there is also a need to create inventory lists for the contents of the new containers. Since the quantities of waste are extremely large, automating such inventory generation will be necessary. It is also an essential requirement to estimate and document the uncertainty associated with the inventory for each storage container.\nThis, and many other real-world industrial problems, pose particular challenges for modern computer vision approaches. Large, annotated, and ground-truthed data-sets are generally unavailable and may be prohibitively difficult, slow, or expensive to create. For example, it has been estimated that labelling the benchmark ImageNet dataset [3], with 14 million images, took approximately 22 human years of effort.\nMeanwhile, the objects and materials in industrial (or domestic) waste-handling problems are extremely diverse and unstructured, often appearing in arbitrary random heaps. For example, a contaminated rubber glove can appear in numerous different shapes and configurations. To incorporate such a perception system with robots, e.g. for autonomous grasping, relatively fast processing speeds are needed. Furthermore, for optimal robotic action planning [4], [5], the system needs to make explicit use of representations of uncertainty."}, {"title": "B. Background", "content": "In recent years, modern computing hardware has enabled rapid advances in computer vision recognition tasks, via deep neural network structures. However, these methods are predominantly based on extensive supervised learning, depending on very large training data-sets, in which each image must be laboriously hand-labelled with ground-truth information. As a result, much of the deep learning computer vision literature is demonstrated on open-source benchmark data-sets.\nMany of these benchmark data-sets feature domestic objects, e.g. furniture, kitchen utensils etc., which do not readily transfer to practical industrial problems. In addition to the labour-intensive nature of collecting and hand-labelling data, such human labour can be prone to error. Sometimes an object may not be accurately bounded by bounding boxes or may be assigned a wrong class label. In some cases, it is difficult for a human annotator to categorize some images [6]. Many objects, e.g. a cat with a long tail, or a frying pan with a long handle, do not neatly fit within a bounding box. It is not clear what the correct definition of a bounding box should be, since a complete bounding box will contain large areas of non-object background pixels. Conversely, a box that is tightly fitted to the body of the cat or the frying pan, will omit key parts and features of these objects (the tail or handle). An uncertainty-aware approach to image-based learning, is valuable for such problems.\nState-of-the-art CNN based object detectors such as Mask Region-based convolutional neural network (Mask R-CNN) [7], Fast Region-based convolutional neural network (Fast R-CNN) [8] and Single Shot MultiBox Detector (SSD) [9] have demonstrated impressive object detection capabilities. However, most of these models are unable to estimate the uncertainty accompanying each detection or classification.\nMore recently, object detection YOLOv3 [10] network does assign a confidence estimate alongside its output detections. However, this capability is conventionally trained by inputting confidences that are derived from a simplistic calculation (essentially defining \"confidence\" as the proportion of the network's output box which overlaps with the ground-truth bounding box).\nIn this work, we have chosen to use YOLOv3 [12] over its more recent variants for the following reasons. You Only Look Once (YOLOv3) is a fast object detector that integrates the feature pyramids network and achieves a good balance between detection accuracy and detection speed, making it one of the most popular methods in this field. Redmon and Farhadi [10] proposed a balanced and optimised algorithm regarding the speed and accuracy of object detection.\nLater variants of YOLOv3 has been developed such as v4,v5,v6 and v7 [13]. New variants have developed an efficient backbone and a more understandable label assignment strategy and have minimal to no impact on calculation overhead. Despite that, YOLOv3 is still providing the base network to these variants. It is still very popular in the research community as it provides a simple implementation and deployment structure [14]. Ge et al. [15] articulate this perspective by stating that, while YOLOv4 and YOLOv5 have indeed made significant strides in object detection accuracy, they may potentially grapple with issues pertaining to over-optimization. The YOLOv3 algorithm is a popular choice in the industry for its high detection efficiency among the YOLO family, with a broad range of applications in various domains such as human nail abnormality detection [16], pavement distress detection [17], pedestrian detection [18], tracking smart robot car [19] apple growth stage detection [20], industrial distress detection [21], and perception systems for driver-less cars [22].\nThe main motivation for our use of YOLOv3 is that it incorporates functionality for explicitly encoding and outputting an estimate of confidence alongside its object categorization decisions.\nLater versions of YOLO do not possess this functionality. In our work, we modify and enhance this uncertainty-awareness functionality by using a Gaussian Process to model uncertainty in a teacher classifier. The teacher then teaches robust uncertainty estimations to our modified YOLO3 classifier during teacher-student training."}, {"title": "C. Approach and novel contributions", "content": "We use our previous work [23] as a baseline method, which also introduced our nuclear waste objects computer vision data-set. This method successfully detected objects, and accurately assigned category labels compared to contemporary methods from the literature. However, it was computationally expensive (execution time for detection was 100ms-200ms). It sometimes made false positive detections of background regions as objects, and object category assignment could be noisy and variable. It also struggled to detect small objects or partially occluded objects in cluttered scenes. Most importantly, this system also lacked an \u201cuncertainty-aware\" functionality.\nIn this study, we address these problems. We describe a new approach which yields more accurate detections, with less computational complexity, while adding a new functionality enabling the system to output confidence estimates to accompany each detection.\nCommon sense suggests that a robust model, with a meaningful and useful \"uncertainty-awareness\u201d capability, should output low confidence scores whenever it outputs false-positive detections or incorrect object category labels. In contrast, as seen in Fig. 1, in our example nuclear waste application we can see that the conventional approach to training confidence estimates in YOLOv3, often results in inappropriate output confidence scores during testing. To provide an improved uncertainty-awareness capability, this study proposes the fusion of a Gaussian Process (GPC) model for classification with a YOLOv3 detector, in a \u201cteacher-student\u201d paradigm, enabling real-time detection accompanied by robust and useful confidence scores. In contrast to previous methods for assigning confidences (discussed above), we adopt a \"teacher-student\" approach (related to \"knowledge distillation\" methods [24]. We use the GPC as the teacher and YOLOv3 as the student. The GPC proposes confidence scores associated with object image thumbnails and teaches these confidences to the YOLOV3 network during its object category recognition training.\nNote that previously, \u201cknowledge distillation\u201d has been used in a very different way. Typically a complex (and computationally expensive) strong classifier is used as the \u201cteacher\u201d, and trains a simpler (and cheaper) classifier which serves as the \"student\" [25]. I.e. a large amount of knowledge, encoded in the large and complex teacher network, is \u201cdistilled\" [26] into a much smaller and computationally cheaper student network.\nIn contrast, a key novelty of our work is that we show how a relatively simple and cheap classifier can be bootstrapped as a \"teacher\", which generates inputs to a much more complex and powerful \"student\" classifier during its training. The resulting strong classifier (student) then outperforms its teacher and also outperforms the same network structure when trained in a conventional way, without the teacher, on the same data-set.\nFirst, we use a 3D-detector from our previous work [23] to generate objectness proposals from RGB-D video streams, and generate corresponding 2D object thumbnails from the RGB-D data. We manually label a small number of these thumbnails. Some are retained for testing, and a few are used as a training input to a\u201cweakly supervised\" system. The system then bootstraps on this small input data, becoming \"self-supervised'. I.e. based on this small labelled data, our system effectively creates and labels more training data, while training itself by using the teacher-student paradigm.\nWe train the classic pre-trained Resnet-50v2 on this small labelled dataset, by using transfer learning [27]. Then we augment this Resnet network with a Gaussian Process (GP) model to provide a sophisticated functionality for learning uncertainty-awareness. The resulting \"teacher\" then generates a much larger scale of automatically labelled, or \u201cself-labelled\" data as inputs to the training of the YOLOV3 network. Meanwhile, the GP component of the teacher is used to provide input to the uncertainty-awareness learning component of our modified YOLOv3 network (in contrast to the more simplistic uncertainty learning approach of the original YOLOv3 as discussed above).\""}, {"title": "Fig. 3: Deep kernel learning architecture with Stochastic variational inference procedure.", "content": "The knowledge of the \"teacher\" network is thus \"distilled\" into a YOLOv3 \u201cstudent\" network, using the variation of loss for classification. This variation of loss is composed of knowledge distillation loss and the sum of squared loss. This technique improves the classification loss compared to the original YOLOV3 object detector method. The resulting network, informed by the GP component of the teacher during training, also generates significantly improved confidence/uncertainty values for each classification, compared to the original YOLOv3.\nThe main contributions of this paper are as follows:\n1) A self-supervised 2D objectness detection, trained by automatically extracting and labelling 2D RGB object thumbnails from 3D RGB-D data. We use 3D conditional clustering within the point clouds to automate the extraction and labelling of bounding boxes without human effort. This automatically generates 2D object bounding box annotations as inputs for training the YOLOv3 network, hence our term \u201cself-supervised\" learning.\n2) A novel use of \"teacher-student\" and \"knowledge distillation\" concepts, to enable boot-strapping a weak classifier (based on a small amount of annotated training data) to train a more complex and strong classifier (by automatically generating and feeding it training examples).\nNot that this is significantly novel in contrast to conventional knowledge distillation methods. Such methods use complex, strong classifiers, to teach effective classification capabilities to a smaller, simpler classifier (e.g. for implementations on small processors). In contrast, we show how to invert this concept, using a weakly trained classifier to automatically generate large amounts of training data for teaching a larger and more complex student classifier, which eventually outperforms the teacher.\n3) We propose a new way to enable a classifier network to learn uncertainty-awareness, i.e. the ability to output a confidence value alongside each object detection and classification decision. In contrast to the conventional YOLOv3 approach, by using items 1) and 2) we enable self-supervised training of a Gaussian Process Classifier (GPC) as part of the \"teacher\" in our teacher-student paradigm. The purpose of the GPC is to teach confidence/uncertainty scores to the YOLO student network, alongside its learning of objectness detection and object category values during teacher-student training. This yields significantly better quality confidence outputs than conventional approaches to YOLOV3 confidence"}, {"title": "Fig. 4: Schematic of the Knowledge distillation pipeline for categorization. a) The transfer of knowledge from the teacher backbone, as shown in Figure 3, to the student backbone utilizing the YOLOv3 architecture. (b) Illustration of the YOLOv3 output structure, where bounding box coordinates are generated by a 3D detector, defining the spatial location and size of each detected object within the 3D space. The objectness score indicates the confidence level that the bounding box contains an object. The final part of the output comprises probabilistic class scores, which provide a probabilistic distribution over possible classes, thereby incorporating uncertainty in the classification process.", "content": "training, in our example industrial waste objects application.\n4) We redesign the loss function of classic knowledge distillation, which works more effectively with our waste object data-set and achieves SOTA performance, while reducing computational complexity.\n5) Our semi-supervised and self-supervised methods can be readily applied to new industrial applications, where no large ground-truthed or annotated training data-sets exist. We demonstrate this capability by using our unique nuclear waste objects data-set, motivated by the robotics and AI challenges of environmental clean-up and remediation on legacy nuclear sites in hazardous environments,\nII. RELATED WORK\nIn this section, we frame our contributions in the context of related literature on: i) object detection and category recognition in computer vision; ii) weakly-supervised learning; iii) pretext learning; iv) unsupervised and self-supervised learning; v) methods for representing uncertainty; vi) teacher-student and knowledge distillation learning paradigms.\nA. object detection and category recognition\nObject detection, and object category recognition, remain challenging problems in modern computer vision research. Such technology is proliferating with the ubiquity and miniaturisation of digital cameras, especially in smartphones. It has numerous applications in areas such as content-based image retrieval [28], autonomous driving [29], robotic grasping [30], security [31], and human-computer interaction [32].\nEarly object detection methods required hand-coded, prior knowledge of explicit object properties, such as shape, size, or colour [33]-[36]. From the 1990s, advances were increasingly made in discovering features that: i) could be \"learned\" or parameterised from a few thumbnail examples, or a single example, of the object in question; and ii) were invariant"}, {"title": "VI. CONCLUSION", "content": "This paper has presented a novel knowledge distillation paradigm for \u201cself-supervised\" learning, requiring minimal labelled training data. This is particularly useful for many industrial applications where large annotated datasets are often unavailable.\nIn contrast to conventional knowledge distillation methods (which \"distil\" the knowledge of a complex network into a simpler classifier), our approach uses a weakly-trained \"teacher\" classifier to automatically annotate additional training data, to train a more powerful \u201cstudent\u201d network based on a modified YOLOv3 network.\nWe utilize the predictive probabilities from a hybrid model combining DCNN and GP models, to enhance the performance of the YOLOv3 object detector. This enables the modified YOLOv3 network to output significantly improved confidence scores alongside each classification. By incorporating uncertainty levels into the perception task, the proposed system can facilitate robot decision-making with increased accuracy. Such uncertainty-aware decision-making is particularly important in safety-critical industries like nuclear decommissioning or handling other hazardous waste forms such as, e.g. disassembly of the large (and potentially flammable or explosive) lithium-ion battery packs of electric vehicles for recycling and circular economy [99], [100].\nMoving forward, future research will explore the application of the proposed ideas in this paper to 3D data-sets, while simultaneously incorporating uncertainties in both localization and object categorization into the YOLOv3 or its subsequent versions like YOLOX. Additionally, we are exploring the combination of these object localisation and categorization algorithms with our lab's work on advanced robotics methods for vision-guided autonomous grasping and manipulation [101]\u2013[103]."}, {"title": "V. EVALUATION", "content": "Table I shows the Precision, Recall and F-Score values for each object category, for: our method (YOLOv3 trained with Knowledge Distillation); a conventionally trained YOLOv3 network using the same amount of labelled training data; and the baseline method [23]. Figure 7 shows the confusion matrix for our method across all object categories. These results demonstrate the consistent and competitive performance of our object detection method compared to both the baseline method [23] and conventional YOLOv3 across various categories in a realistic mockup nuclear waste object dataset. Notably, our method yielded an overall precision of 85.4%, which is a significant achievement considering the complexity and challenges presented by our dataset.\nmethod exhibits a faster execution time of 40-45ms per image for detection tasks, significantly outpacing the baseline's 100-200ms. This improvement in computational efficiency makes our method more useful for real-time applications such as vision-guided robotic waste handling [1], and potentially more scalable to more extensive or demanding tasks in future applications.\nOur proposed method also provides an additional functionality, since the baseline method of [23] is not capable of outputting confidence/uncertainty scores alongside each of its categorizations."}, {"title": "E. Uncertainty Awareness in Object Detection", "content": "Many pattern recognition approaches, including most neural network methods, are able to make classification or detection decisions, but typically cannot simultaneously output an indication of confidence or uncertainty associated with each such decision.\nMore recently, neural networks have been used with Bayesian modelling to perform prediction while modelling the uncertainty of each decision. However, such work has predominantly focused on uncertainty associated with localisation. Jian et al. [80] used the intersection-over-union (IoU), between classifier output bounding boxes and ground-truth 2D bounding box regions, as a simple metric of uncertainty in object localisation. Choi et al. [81] enhanced this by incorporating Gaussian modelling and loss reconstruction in YOLOv3 to predict localization uncertainty. [82] propose a combination of experts method. They combine the classification results from a Gaussian Mixture Model (GMM) and YOLO, weighted according to the confidence metric output by each model.\nThe above methods localize the objects while also encoding awareness of localization uncertainties. However, they lack the ability to also categorize the objects with uncertainty estimates of the categorization decisions. For example, it may be the case that a detected object is not a pedestrian but a cyclist. Hence, an ability to estimate uncertainty in a categorization decision is also very important, in addition to localization uncertainty.\nThis is even more critical on problems where smaller amounts of training data may be available, and where critical decision-making needs to be made based on vision system outputs, e.g. action planning of robots in uncertain or unstructured environments, especially in high-consequence safety-critical industries.\nThis paper proposes a solution to this challenge. We use an end-to-end detection and categorization pipeline based on a YOLOv3 network. However, during training, we use a Gaussian Process as \"teacher\", to predict categorisation uncertainty probabilities, which become part of the training data fed into (i.e. \"taught\" to) the \"student\" YOLOv3 network. The resulting network is able to output uncertainty estimates alongside each object categorization decision."}, {"title": "F. Knowledge Distillation in Object Detection", "content": "Knowledge distillation has received significant attention in the research of deep learning. The main idea is to utilize computationally expensive, large-scale deep models in real-time applications, by \"distilling\" the knowledge of the large-scale network into a smaller network that is computationally cheap and fast at run-time. [83] distil the knowledge from a fully supervised teacher model to a student model using unlabeled data in a semi-supervised learning task. The term knowledge distillation is specified to this technique of learning in the seminal paper of [24]. Different knowledge distillation methods and terminologies have been used in research such as teacher-student learning [24], mutual learning [84], assistant teaching [85], lifelong learning [86] and self-learning [87]. [88] apply knowledge distillation for compressing the training data, to reduce the training loads of deep learning models.\nThese methods are predominantly used to distil knowledge from a large network to a small network to compress the model. In contrast, our approach is to boot-strap a small \"teacher\" classifier, trained on minimal data, by using it to train a much larger and stronger classifier based on a deep YOLOv3 architecture. Furthermore, we combine the knowledge distillation philosophies of both teacher-student architecture and also data-set distillation. In this paradigm, we use a GPC model as teacher and train it on a small subset of data. We then deploy this model to automatically generate class probabilistic scores based on weakly supervised learning. It distills the knowledge of features and data to the large YOLOv3 model which serves as the \"student\"."}, {"title": "III. METHODOLOGY", "content": "Our method comprises the following steps: i) high-quality objectness proposals are generated in RGB-D video streams by building on our real-time 3D-based object detection methods of [23]; ii) probabilistic object classification of 3D thumbnails using a GPC model which is weakly-trained on a small number of hand-labelled images; iii) end-to-end training of a YOLOv3 detector through self-supervised learning and a teacher-student paradigm, in which the GPC model teaches object classification and confidence values to the YOLOv3 network, while it trains on a large number of object instances that are automatically generated by the method of i).\nA. 3D Objectness Detection and Automatic Generation of Large-scale 2D Data\nOur objectness detection approach extends methods we developed in [23] for real-time 3D objectness detection. Salient region proposals are obtained through point cloud segmentation. RANSAC is used to detect and remove large planes, so that we can obtain only the table-top and ground-top objects. Additionally, we incorporated a more efficient conditional clustering approach to acquire objectness proposals, using perceptual grouping [89] based on colour, shape and spatial cues. We first detect large planes (using RANSAC) in point clouds and remove them, as we are interested in table-top or ground-top objects. We then use a multi-cues conditional clustering approach based on colour, shape and spatial cues to acquire objectness proposals. Given two voxels $p_1$ and $p_2$, the connectability between them $C(p_1,p_2)$ is defined by distance connectability $C_d(p_1, p_2)$, color connectability $C_c(p_1,p_2)$ and shape connectability $C_s(p_1, p_2)$:\n$C_s (p_1, p_2) = \\max \\left(0, \\min \\left(1, \\sigma_s - \\frac{\\left|N_{p_{1}} \\cdot N_{p_{2}}\\right|}{\\| N_{p_{1}} \\| \\cdot \\| N_{p_{2}} \\|} \\right)\\right)$ (1)\n$C_c(p_1, p_2) = \\max \\left(0, \\min \\left(1, \\sigma_c - \\|I_{p_{1}} - I_{p_{2}}\\|\\right)\\right)$\n$C_d(p_1,p_2) = \\max \\left(0, \\min \\left(1, \\sigma_d - \\|p_{1} \u2013 p_{2}\\|\\right)\\right)$\n$C(p_1,p_2) = C_d(p_1, p_2) \\cap (C'_s (p_1, p_2) \\cup C_c(p_1, p_2))$\nwhere $n_{p_1}, n_{p_2}$ are the surface normals, $I_{p_1}, I_{p_2}$ refer to the intensity values of $p_1, p_2$, and $\\sigma_d$, $\\sigma_c$, $\\sigma_s$ are the connectability thresholds. Neighbouring voxels are then clustered iteratively through this connectability criteria until all clusters become constant. Parameter values $\\sigma_d$, $\\sigma_c$, $\\sigma_s$ are set as 2.0cm, 8.0 and 10\u00b0, perform well for our application.\nGiven 3D objectness proposals detected in 3D world co-ordinates, each point in the proposal $p(x_w, y_w, z_w)$ can be back-projected to its 2D image coordinates (u, v) and it depth d:\n$\\begin{bmatrix} u \\\\ v \\\\ d \\end{bmatrix} = C \\begin{bmatrix} R & t \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\\\ 1 \\end{bmatrix}$ (2)\nwhere C is the camera intrinsic matrix, and R and t are the rotation matrix and transformation vector respectively. A 2D bounding box is formed for each 3D objectness proposal, which is then used as training data. YOLOv3 takes this data as input, and learns to detect and localise object regions in 2D RGB images, as explained in the following section.\nAs a result, 2D bounding boxes with boundary-aware segmentation is achieved from 3D objectness proposals. Our method thus automatically generates a large-scale partially annotated dataset. This comprises: i) a large set of 2D images, in which every image has been automatically annotated with ground-truth bounding boxes for all objects; ii) cropped object thumbnail images for each bounding box. This partially annotated data does not yet have object category annotations.\nB. Probabilistic Object Classification for the Teacher Classifier\nWe use a weakly-supervised method to train a DCNN architecture to form part of the \u201cteacher\u201d process that enables our self-learning pipeline. Firstly, we create the dataset, as described above, using the 3D detector, Fig.2. Secondly, we manually label the object categories for a small subset of 2D object thumbnail images from the automatically generated large-scale dataset. Thirdly, the DCNN (ResNet50) [90], which is pre-trained on public large-scale dataset ImageNet, is subjected to further training on this small set of cropped and labelled image data, i.e. a transfer-learning paradigm. Finally, the DCNN-GPC is trained to predict the probabilities (confidence scores) for each category of the cropped images.\nThe resulting DCNN-GP classifier can now be used to: i) automatically label the object categories of all remaining images and thumbnails of the large-scale dataset; ii) automatically predict probabilities (confidence levels) for each such classification. The weakly-trained DCNN-GP can thus be bootstrapped to strongly train (i.e. \u201cteach\u201d) the end-to-end learning of a large YOLOv3 network. The Architecture of the teacher network is described as follows.\nOur previous work on this topic, [23], used VGG-16 [91] for object classification. In this paper, we use ResNet50, since it is a more recent and efficient model. This network is similar to VGG-16 except for the new identity mapping capability. These features enable ResNet to overcome the problem of vanishing gradients. This is important, because our nuclear waste object dataset is very challenging, and it contains many objects which can be easily confused with each other, e.g. separate categories of plastic pipes and pipe joints. The dataset also includes objects with shiny, reflective surfaces (e.g. metal objects of various kinds), and a variety of deformable objects (e.g. rubber gloves, chains, hoses, sponges, face-masks). These objects are significantly different from those in datasets more commonly used in mainstream academic research on object detection [92]\u2013[94].\nWe added two fully connected layers with 4096 hidden nodes at the end of the network to avoid over-fitting and removed the softmax layer. This helps in increasing the accuracy. We set these parameters according to experience gained in our previous related work. Our network is pre-trained on ImageNet [3].\nDuring this stage, the features from the last layers $(fc1 \\in R^{4096})$ and $(fc2 \\in R^{4096})$ are given as input X $(\\in R^{8192})$ to the GPC model.\nThe GPC is trained using the small set of hand-labelled object thumbnail data described above. We use a multi-class classification GP model. Essentially this is a regression problem from an input xeX to discrete labels yeY. We adopted the Wilson et al. method [95] as the basis for our experimental design, with this critical modification to the approach by incorporating features from transfer learning into the GPC model. This adjustment is predicated on the hypothesis that utilizing pre-trained models to extract features can enhance the model's ability to capture relevant information from the data, thereby potentially improving the overall predictive performance of the GP framework. Rather than using the whole dataset as input, we use a subset of dataset Z \u2208 Rm\u00d7D known as inducing points where m C total number of observations n, and D is the input dimensionality. We use inducing points to overcome computational complexity O(n\u00b3). For these inducing points we define latent variables uj and employ a variational distribution q(u) to approximate the true posterior of u.\nThe optimization of the variational distribution and the inducing points is achieved by maximizing the evidence lower bound(ELBO) on the log marginal likelihood."}, {"title": "D. End-to-End training of YOLOv3 Uncertainty-Aware Object Detection Network", "content": "We train YOLOv3 in two stages. In the first stage, we train the objectness-detection and localization part by using data that has been automatically annotated by our 3D detector, as described in section III-A. In the second stage, we train YOLO's object category recognition capability, coupled with uncertainty awareness capability, via a knowledge distillation method, using DCNN-GP as a \"teacher\" for the YOLO \u201cstudent\", as described in section III-B.\nIn the conventional YOLOv3, the training phase utilizes the sum of squared error (SSE) loss for optimizing bounding box predictions and binary cross-entropy (BCE) loss for refining confidence scores and classifying detected objects. In contrast, we redesign the loss function for the classification task using the concept of knowledge distillation. The GP of the DCNN-GP \"teacher\" network, described in section III-B, is used to teach the YOLOv3 \"student' network how to predict confidence values for each categorization decision. We therefore use GP predictive class probabilities as input to YOLOv3, giving:\n$Loss = \\sum_{i=1}^{n} ((S_i^T - S_i^P)^2)$ (5)\nwhere o refers to the Softmax function applied to logits and labels, labels are the predictive probabilities of our GPC model. T is utilized to smooth the outputs from the teacher classifier. Then, we apply the sum of squared error (SSE) on the logits and labels."}, {"title": "IV. IMPLEMENTATION AND EXPERIMNENTS", "content": "A. Training through transfer learning with DCNN\nIn this study", "23": "in the ROS (Robot Operating System) package [98", "Fig": 1, "98": "and we use a high computational computer with i7-8 cores CPU a NVIDIA GTX 1070 for DCNN-GPC task and use NVIDIA TITAN RTX GPU 24GB for YOLOv3 training. During the transfer learning phase of our project, we utilized the ResNet50V2 network as a foundational model. To enhance efficiency and reduce training time, we kept the layers of ResNet50V2 frozen and integrated our layers specifically designed to extract features relevant to our dataset. This approach allowed us to minimize the computational resources and time required for training.\nB. Training uncertainty-awareness using GP model\nWe teach uncertainty-awareness in two steps. The first step was explained in Subsection IV-A, where Resnet50 architecture is employed for feature extraction to transform raw input into a set of informative features for GP modelling. In the second step, we utilise a similar strategy and use the extracted features as input to build the kernel of the GP. We assign a distinct Gaussian Process to each feature emanating from the ResNet50 network. This one-to-one mapping between GPs and features allows for a fine-grained analysis and modelling of the intricate"}]}