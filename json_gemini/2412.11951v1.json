{"title": "The Impact of Generalization Techniques on the Interplay Among Privacy, Utility, and Fairness in Image Classification", "authors": ["Ahmad Hassanpour", "Amir Zarei", "Khawla Mallat", "Anderson Santana de Oliveira", "Bian Yang"], "abstract": "This study investigates the trade-offs between fairness, privacy, and utility in image classification using machine learning (ML). Recent research suggests that generalization techniques can improve the balance between privacy and utility. One focus of this work is sharpness-aware training (SAT) and its integration with differential privacy (DP-SAT) to further improve this balance. Additionally, we examine fairness in both private and non-private learning models trained on datasets with synthetic and real-world biases. We also measure the privacy risks involved in these scenarios by performing membership inference attacks (MIAs) and explore the consequences of eliminating high-privacy risk samples, termed outliers. Moreover, we introduce a new metric, named harmonic score, which combines accuracy, privacy, and fairness into a single measure.\nThrough empirical analysis using generalization techniques, we achieve an accuracy of 81.11% under (8, 10\u22125)-DP on CIFAR-10, surpassing the 79.5% reported by De et al. (2022). Moreover, our experiments show that memorization of training samples can begin before the overfitting point, and generalization techniques do not guarantee the prevention of this memorization. Our analysis of synthetic biases shows that generalization techniques can amplify model bias in both private and non-private models. Additionally, our results indicate that increased bias in training data leads to reduced accuracy, greater vulnerability to privacy attacks, and higher model bias. We validate these findings with the CelebA dataset, demonstrating that similar trends persist with real-world attribute imbalances. Finally, our experiments show that removing outlier data decreases accuracy and further amplifies model bias.", "sections": [{"title": "INTRODUCTION", "content": "Privacy and fairness are important elements in developing responsible machine learning (ML) models. Privacy ensures that individual data contributions remain confidential and are not identifiable in the model's outputs. On the other hand, fairness involves ensuring that the model's outputs are unbiased and equitable across various demographic groups, preventing discrimination and ensuring inclusivity. While significant progress has been made in understanding and addressing individual trade-offs, such as the balance between privacy and utility and the balance between fairness and utility, the interplay between these trade-offs has not been thoroughly investigated. This gap is particularly evident in image classification, where models need to handle complex and diverse data inputs. Understanding how privacy and fairness affect each other in this domain is essential for creating ML models that are both secure and equitable.\nDifferential privacy (DP) is a gold standard for ensuring privacy in ML models, offering mathematical guarantees that individual data entries in aggregated datasets remain protected. Introduced by Dwork et al. [13], DP works by adding controlled noise to the data, which masks the contributions of individual entries. This noise makes identifying any single data point within the dataset difficult, thus safeguarding personal information. Despite this protection, DP still preserves the overall patterns in the data, allowing ML models to be trained effectively. However, this approach also highlights a significant challenge: the privacy-utility trade-off. Adding more noise increases privacy but can also reduce the accuracy of the ML model, thereby lowering its utility. Thus, balancing privacy and utility is challenging for successfully implementing DP in ML models.\nA set of generalization techniques, including group normalization, optimal batch size, weight standardization, augmentation multiplicity, and parameter averaging, have been shown to significantly enhance the utility of deep ML models trained using differentially private stochastic gradient descent (DP-SGD) [8]. However, the impact of a recently proposed generalization technique, differentially private sharpness-aware training (DP-SAT) [25], which can serve as an alternative to DP-SGD, has not been thoroughly investigated. This raises an important question: Q1: Would combining DP-SAT with the other generalization techniques lead to an even better utility-privacy trade-off?\nFairness in ML models can be defined as the goal of producing unbiased and equitable predictions across demographic groups. However, model bias may occur when systematic errors or prejudices arise in predictions, potentially disadvantaging certain groups."}, {"title": "BACKGROUND", "content": "This section provides an overview of the DP definition and discusses a succinct overview of the generalization techniques and MIAs to offer a clearer understanding of their implications in our study."}, {"title": "Differential Privacy (DP)", "content": "DP ensures the privacy of individual contributions in statistical databases by asserting that removing or adding an individual's data does not significantly affect the outcome of any analysis. Introduced by Dwork et al. [14]:\nDefinition 2.1. A mechanism M satisfies (\u20ac, \u03b4)-DP if for all datasets D\u2081 and D2 differing on at most one element, and for all S \u2286 Range(M), it holds that\n$\\Pr[M(D_1) \\in S] \\leq \\exp(\\epsilon) \\Pr[M(D_2) \\in S] + \\delta$.\nHere, e referred to as the privacy loss, where a smaller value indicates higher privacy. 8 represents a small probability that the privacy loss may be exceeded.\nDP establishes a framework for developing private ML models, highlighting the role of DP-SGD as a pivotal technique in this context [2]. In DP-SGD, noise is added to the gradients during each update step to mask the influence of individual data points, thus ensuring that the training process remains differentially private. The effectiveness of DP-SGD relies on the privacy accountant-a numerical algorithm that calculates precise upper bounds on cumulative privacy loss, which is called privacy budget [2]. The privacy accountant tracks how privacy loss accumulates over multiple training iterations, ensuring that the total privacy loss remains within a specified budget. In our study, we employ the accounting method for DP-SGD introduced by Mironov et al. [24], which is available in the TensorFlow Privacy library [1]. De et al. [8] utilize this approach, combining privacy accounting with meticulous hyper-parameter optimization to enhance the accuracy of over-parameterized models. This results in a refined trade-off between privacy and utility, achieving state-of-the-art outcomes. In the subsequent sections, we elaborate on the generalization techniques proposed by De et al. [8] and DP-SAT by Park et al. [25], and detail their implications in our study."}, {"title": "Generalization Techniques", "content": "Group Normalization (GN). Following recent studies [8, 19, 22], we replace Batch Normalization (BN) layers by GN layers. This modification is important because DP-SGD requires independent gradients evaluated on different training examples. This fails to include any method that enables communication between training examples, such as BN. GN, on the other hand, divides the channels of the hidden activations of a single image into groups and normalizes these activations within each group independently. This maintains the independence between gradients evaluated on different examples. Following De et al. [9], we place the GN layers on the residual branch of the network to recover the benefits of BN for training deep networks.\nOptimal Batch Size (OBS). Previous studies [3, 8, 11] have noted that using larger batch size can notably boost the privacy-utility balance in DP-SGD. On the other hand, in non-private models using SGD, the batch size is typically smaller (e.g., 8, 16, 32, 64) to achieve higher accuracy [12, 34].\nWeight Standardization (WS). Several studies [18, 27, 28] have shown that using WS combined with GN can be an effective replacement for BN in non-private training, especially when training with large BS. Adopting the approach of [8], we use this technique for all convolutional layers to normalize the rows of the weight matrix for each convolution and demonstrate its advantage in private learning.\nAugmentation Multiplicity (AM). As in [8], we use multiple augmentations for each sample in DP-SGD updates to regain the advantages of data augmentation in private training, and instead of calculating a clipped gradient for every augmented image-which would increase privacy costs-we average the gradients from various augmentations of a single training sample before gradient clipping. This method does not impose extra privacy expenses.\nParameter Averaging (PA). The PA technique [26] leverages the stability of parameters over training iterations to enhance model generalization by averaging parameters across multiple steps. This approach helps smooth parameter updates, leading to more robust model performance. The privacy analysis of DP-SGD assumes that revealing training parameters does not breach privacy; therefore, PA does not result in additional privacy concerns. Following [8], we adopt an exponential moving average for PA, which continuously updates a weighted average of the parameters during training. This method improves accuracy on both training and validation data by reducing the variance of parameter updates, providing a stable optimization path for improved model performance.\nSharpness-Aware Training (SAT). Sharpness-aware minimization (SAM) targets flat minima to mitigate the issue of sharp minima in the loss landscape [16]. This characteristic of sharp minima is recognized as a limitation of SGD in yielding generalized models, particularly in the context of over-parameterized models.\nUnlike SGD, which computes the gradients of the loss function relative to the parameters and then guides the parameter updates in a single descent step, SAM introduces a two-step optimization. This method initially perturbs the parameters within a certain radius in the ascent step to evaluate the sensitivity of the loss function (a measure of the landscape's sharpness) and subsequently steers the perturbed parameters toward flatter regions of the loss landscape in the descent step.\nHowever, SAM's two-step optimization may negatively impact the privacy budget and computational time of DP-SAM [12, 25]. Specifically, Park et al. [25] prove that DP-SAM requires twice the privacy budget than that of DP-SGD and requires more computational time. This is because DP-SAM employs the training samples within the same mini-batch twice, i.e., it needs to inject noise into both the gradients of the current parameters and the perturbed parameters to ensure the privacy of both ascent and descent steps. To mitigate this challenge, Park et al. propose DP-SAT, which can improve performance without additional privacy or computational burden. Their idea is to reuse the perturbed gradient of the previous step to steer the direction of updated parameters at the current step. Our study distinctively examines the impact of generalization techniques introduced by [8] and the SAT optimizer on the fairness and accuracy of ML models, with and without DP, addressing a gap not explored in prior research."}, {"title": "Membership Inference Attacks (MIAs)", "content": "An MIA targets a built ML model to deduce membership of individual training samples. This can have privacy implications, especially when the training data includes sensitive or personal information [30, 31]. MIAs can be conducted using two main strategies differing in their need for training shadow models.\nFirst, as discussed by Shokri et al. [31], the adversary initially trains multiple shadow models to simulate the target model, assuming that the target model is a black-box API. Then, based on shadow models' outputs on their own training and test examples, the adversary obtains a labeled (member vs non-member) dataset and finally trains multiple neural network classifiers, one for each class label, to perform MIAs against the target model. This approach capitalizes on the similarity between the shadow models and the target model to infer membership.\nThe second approach, by Salem et al. [30], relies on training one shadow model to distinguish between member and non-member data points. This approach is computationally more efficient as it avoids the overhead of training multiple shadow models.\nTo conduct MIAs, we use the TensorFlow Privacy library [1] in which the focus is on MIAs against black-box models, where the adversary can only observe the model's output but not its parameters. We employ four different MIAs implemented in this library: MultiLayered Perceptron (MLP), Random Forest (RF), Threshold, and Threshold Entropy attacks. The first two options require training one shadow model; the last two do not require training any shadow model and leverage statistical measures, such as maximum confidence score and entropy, applied to the target model's results. The Threshold attack uses a simple decision rule based on the confidence score, while the Threshold Entropy attack uses the entropy of the confidence scores to make membership decisions."}, {"title": "METHODOLOGY AND EXPERIMENTAL SETUP", "content": "This section provides the methodologies and metrics employed in our experiments. It first discusses our selection and manipulation of datasets. It then explains the architectural choices and training settings for our ML models. Additionally, it introduces the MIA AUC and bias metrics from the literature, which are used for assessing privacy and fairness. It also describes our HS metric, an approach that simultaneously evaluates model accuracy, fairness, and privacy."}, {"title": "Datasets", "content": "Our analysis examines bias in both controlled, simplified settings and more complex, real-world scenarios. In the simplified setting, we use the CIFAR-10 and CIFAR-100 datasets for training with unbiased data and introduce bias through their skewed counterparts, CIFAR-10S [36] and CIFAR-100S.\nSynthetic Bias with CIFAR-10S and CIFAR-100S. CIFAR-10S is created by converting a subset of images to grayscale, maintaining a 95% to 5% ratio between color and grayscale images per class. Specifically, five classes are predominantly color (95%), while the other five are mainly grayscale (95%). Despite this skew at the class level, the overall distribution between color and grayscale images remains balanced. For evaluation, we use two test sets: a color-only version (COLOR) and a grayscale-only version (GRAY), each assessed independently for the 10-class classification. CIFAR-100S follows the same structure but extends to the 100 classes of CIFAR-100. Unless otherwise specified, the bias ratio is set at 95% to 5%, though we also explore a 75% to 25% ratio to understand the impact of varying degrees of skewness in the data distribution.\nReal-World Bias with CelebA. To extend our analysis to a realistic setting, we select the CelebA dataset [20], which naturally exhibits imbalances in attribute distribution across genders. We focus on the Aligned&Cropped subset, which contains images with 39 facial attributes, allowing us to study how attributes like \"smiling\" correlate with gender. Among the attributes, 21 are more commonly associated with women, while 18 are more frequent in men, showing an average gender bias of 80.0% when an attribute is present. We exclude the \"Male\" attribute and focus on attributes that have a sufficient number of validation and test images, ultimately analyzing 34 attributes. This dataset allows us to validate our findings from the controlled setting and assess how models handle the more nuanced biases that arise in real-world data."}, {"title": "Metrics", "content": "Bias Metric. To measure bias amplification in our models, we employ two distinct metrics introduced in [36] suited to the nature of the biased datasets used in our study.\n\u2022 Synthetic Bias. In the context of CIFAR-10S and CIFAR-100S, we use a bias metric that calculates the mean bias in model predictions across classes:\n$\\text{Bias} = \\frac{1}{|C|} \\sum_{c \\in C} | \\frac{\\text{max}(Grc, Colc)}{Grc + Colc} - 0.5 |$,\nwhere Gre is the number of grayscale test set examples predicted as class c, and Colc is the same for color. Here, C represents the set of all classes. The test set is evenly distributed across the domains {Gr, Col}, ensuring that the average accuracy directly reflects the model's learned bias towards one of the domains in the model's predictions. This metric helps identify the extent to which the model's predictions favor either grayscale or color images, reflecting how spurious correlations learned during training influence the model's outcomes.\n\u2022 Real-World Bias. For each attribute in the CelebA dataset, we calculate model bias to assess how much the model's predictions reflect or amplify inherent gender imbalances:\n$\\text{Bias} = | \\frac{P_w}{P_w + P_m} - \\frac{N_w}{N_w + N_m} |$,\nwhere Pw and Pm are the numbers of positive classifications for women and men, respectively, and Nw and Nm are the actual counts of images with the attribute present for women and men. If an attribute is more common among women (e.g., \"smiling\"), a positive value indicates that the model is amplifying the gender imbalance by predicting more women as having the attribute than the original distribution suggests. A negative value implies a reduction in bias. This metric helps assess how the model's predictions align with or deviate from the underlying distribution of attributes across genders, aiming for more balanced outcomes.\nMIA AUC. Following [8, 30, 37], we use the area under the curve (AUC) metric to measure the effectiveness of MIAs on our ML models. It represents the area under the receiver operating characteristic (ROC) curve, which plots the true positive rate against the false positive rate. High AUC values demonstrate a model's vulnerability by showing its ability to distinguish between training dataset members and non-members, thus revealing susceptibility to MIAs. In contrast, AUC values around 0.5 imply the model's robustness against such attacks.\nHarmonic Score (HS). We introduce a scoring metric based on the harmonic mean to evaluate models in terms of privacy (measured by MIA AUC), fairness (adjusted by bias), and accuracy. To ensure all aspects are considered equally, we map values to the [0, 1] range:\n\u2022 Bias is measured using Eq. 1, resulting values within the range [0, 0.5], where 0 represents no bias (perfect fairness) and 0.5 represents maximum bias. We scale this to [0, 1], resulting in Biasscaled. We then use 1 \u2212 Biasscaled, where values closer to 1 indicate lower bias, and values closer to 0 represent higher bias.\n\u2022 AUC for MIA falls in the range [0.5, 1], with 0.5 indicating random guessing (no successful attack) and 1 representing a successful attack. We map this to [0, 1], yielding AUCscaled, and then use 1 \u2212 AUCscaled, where values closer to 1 indicate stronger privacy, and values closer to 0 indicate weaker privacy.\n\u2022 Accuracy is already in the [0, 1] range, where higher values indicate better model performance.\nThe Harmonic Score (HS) is calculated using the following formula:\n$\\text{HS} = \\frac{3}{\\frac{1}{\\text{Accuracy}} + \\frac{1}{1 - \\text{AUCscaled}} + \\frac{1}{1 - \\text{Biasscaled}}}$\nHS is heavily influenced by the lowest value among the three aspects-accuracy, 1 - Biasscaled, and 1 - AUCscaled ensuring that poor performance in one dimension significantly reduces the overall score. High values for each of these aspects lead to a higher harmonic mean, indicating balanced and strong performance across all three dimensions. HS ranges from (0, 1], with higher values (closer to 1) indicating better overall performance, while lower values (closer to 0) reflecting poorer performance in at least one aspect. For example, consider the following two cases:\n\u2022 Case 1: (accuracy = 0.5, 1 \u2212 bias = 0.5, 1\u2212 AUC = 0.5)\n$\\text{HS} = \\frac{3}{\\frac{1}{0.5} + \\frac{1}{0.5} + \\frac{1}{0.5}} = 0.5$\n\u2022 Case 2: (accuracy = 0.1, 1 \u2212 bias = 0.5, 1 \u2212 AUC = 0.9)\n$\\text{HS} = \\frac{3}{\\frac{1}{0.1} + \\frac{1}{0.5} + \\frac{1}{0.9}} \\approx 0.136$\nBoth cases have the same arithmetic mean value of $\\frac{0.5 + 0.5 + 0.5}{3} = 0.5$. However, HS for Case 2 is significantly lower due to its poor accuracy performance. This illustrates how the harmonic mean is more sensitive to low-performing aspects, emphasizing the importance of balanced performance across all three dimensions."}, {"title": "Implementation Details", "content": "We train our deep ML model using the Wide-ResNet (WRN) architecture, specifically WRN-16-4 for CIFAR-10(S) and WRN-28-10 for CIFAR-100(S) and CelebA. The baseline model is trained from scratch on CIFAR-10(S) but pre-trained on ImageNet [29] for CIFAR-100(S) and CelebA experiments.\nWe use the parameters relevant to the generalization techniques and our private and non-private settings, as detailed in Table 1. For experiments on CIFAR-10 and CIFAR-100 datasets, we follow the setting for private models as used in [8, 25]. For non-private scenarios, we determined the optimal parameters through experimental exploration. This included testing various batch sizes (8, 16, 32, 64, 128), setting different learning rates (0.01, 0.1), evaluating multiple values (1, 2, 4, 8, 16) for GN and AM, and assessing different values (0.03, 0.04, 0.05) for radius. The CelebA's training is conducted using binary cross-entropy loss with logits. We employ early stopping, as [32], to prevent overfitting in non-private models. We perform five independent runs with different seeds on each experiment and report their median, and utilize a server with 8 RTX 4090 GPUs for the computational requirement. Our implementation is publicly available at https://anonymous.4open.science/r/PriFa_ML-D04A."}, {"title": "Privacy-Utility Trade-off", "content": "We first assess the influence of the generalization techniques on the privacy-utility trade-off (addressing Q1). For this assessment, we train our models on CIFAR-10, CIFAR-10S, CIFAR-100, and CIFAR-100S. We cumulatively integrate each generalization technique into the SGD training process as proposed by [8], in settings with and without DP. As the culmination of our technique integration, we replace SGD with SAT as the final generalization technique in our sequence. After each integration, we subject the model to the four listed MIA attacks. This approach allows us to systematically evaluate the cumulative impact of these techniques and understand their effect on model accuracy and privacy. We provide a deeper analysis of how these techniques impact privacy leakage later in this section. We also compare DP-SGD and DP-SAT for different privacy budgets and datasets to evaluate their accuracy when both leverage the generalization techniques.\nNotably, incorporating all generalization techniques leads to an accuracy boost of 31.64% and 21.17% for private and non-private baseline models, respectively. These substantial increases highlight the effectiveness of the generalization techniques in enhancing model performance. Furthermore, this approach experiences a slight decrease in MIA AUC for the private baseline model but increases MIA AUC by 0.1 for the non-private baseline model. This shows that DP fortifies the models' defense against MIAs despite the accuracy gains attributed to the generalization techniques. OBS and AM are the most effective techniques for increasing the accuracy of both private and non-private models. Their significant contributions underscore their critical role in optimizing model performance. Additionally, WS significantly increases MIA AUC in the non-private setting. This increase indicates a trade-off between accuracy improvement and privacy risk, which needs consideration in model development. DP-SAT and SAT contribute to a boost of 2.05% and 0.66% in the accuracy of private and non-private models, respectively. These results demonstrate the nuanced impact of the SAT method on model performance. In the context of MIA AUC, DP-SAT does not impact the private model, and SAT shows a marginal increase of 0.02 in the non-private setting.\nIn Table 2, we also conduct our experiments on CIFAR-100, in which applying all generalization techniques notably enhances model accuracy by 42.7% and 23.47% for the private and non-private baseline models, respectively. The considerable improvements for CIFAR-100 further validate the effectiveness of these techniques across different datasets. This approach increases MIA AUC by 0.08 for the non-private baseline model while exhibiting minimal fluctuation in the private models, underscoring the protective impact of DP against MIAs in private learning. OBS and AM are the key strategies for improving accuracy across the private and non-private models. Their repeated effectiveness across different datasets highlights their importance in enhancing model utility. OBS also plays a negative role in elevating MIA AUC in the non-private context. This negative impact emphasizes the need to balance accuracy improvements with potential privacy risks. Furthermore, both SAT and DP-SAT increase accuracy by approximately 0.7%. This consistent improvement with the SAT technique indicates its beneficial role in refining model accuracy. Finally, upon applying all generalization techniques to achieve peak accuracy, MIA AUC of the private model trained on the CIFAR-10 and CIFAR-100 experiences a decrease of 0.12 and 0.1, respectively, compared to the non-private model trained on these datasets.\nBy using the early stopping technique in non-private settings, we prevent the training process from reaching the point of overfitting. In private settings, the limited privacy budget restricts the number of training iterations, which also helps prevent overfitting."}, {"title": "Detailed Analysis of the Impact of Generalization Techniques on Privacy Leakage", "content": "We now provide a closer examination of the impact of generalization techniques on privacy leakage (measured by MIA AUC), specifically for models trained on unbiased datasets CIFAR-10 and CIFAR-100, as shown in Table 2, and contextualize these results with previous studies. While Table 2 highlights that certain generalization techniques can lead to increased MIA AUC, this outcome aligns with findings in prior research [21, 33], which indicate that generalization techniques may be less effective in safeguarding vulnerable samples. Similarly, in the context of large language models, [6] demonstrates that memorization of vulnerable samples can occur independently of overfitting. Here, memorization is defined following Feldman [15] as unintended retention of specific samples, where the model's prediction probability for a training sample changes significantly if the sample is removed from the dataset.\nIn our work, despite using early stopping techniques to prevent overfitting, we observe evidence of this memorization. We track MIA AUC throughout the training process, both for private and non-private models, across baseline models and after incorporating all generalization techniques. We observe that this sample is memorized well before the stopping point indicated by the early stopping methods. This finding suggests that, even with generalization techniques in place, certain samples are memorized early in the training phase, highlighting that generalization techniques may leave specific samples more susceptible to privacy risks."}, {"title": "The Comparison of DP-SGD and DP-SAT", "content": "Table 4 compares DP-SGD and DP-SAT for different privacy budgets when both methods use the generalization techniques of [8]. DP-SAT exhibits superior classification accuracy consistently compared to DP-SGD in all scenarios. It shows that the identification of flat minima becomes notably advantageous."}, {"title": "Fairness-Utility Trade-off", "content": "This section measures model bias, according to Eq. 1, while applying generalization techniques in private (i.e., (8, 10\u22125)-DP) and non-private models trained on biased datasets CIFAR-10S and CIFAR-100S. We explore how this relationship evolves across different privacy budgets (\u20ac = 1, 2, 4, 8) and dataset bias levels, specifically at 95% and 75%. Additionally, we investigate how the HS of a model fluctuates under varying conditions of privacy settings and data bias. By examining these variables, we aim to provide a comprehensive understanding of the trade-offs involved in differentially private learning under various scenarios.\nAs demonstrated in Table 6, the model bias (computed by Eq. 1) for the model trained with DP on CIFAR-10S is 0.15, 0.06 higher than the non-private model. Even though the private model reduces MIA AUC by a marginal 0.03, it significantly drops the overall accuracy by approximately 21%. This significant drop in accuracy indicates a substantial trade-off when implementing DP, highlighting the challenge of maintaining utility while ensuring privacy. Similarly, examining the model trained on CIFAR-100S with DP, the bias metric increased by 0.06 compared to the scenario without DP. This increase in model bias, although indicative of potential fairness issues, accompanies a 10% decrease in MIA AUC, suggesting an improvement in privacy. These findings underscore the intricate balance that must be navigated between improving privacy and maintaining model accuracy, particularly in biased datasets. For CIFAR-10S, the comparison of HSs exhibits how DP, even with a relatively lenient privacy budget of e = 8, disrupts the delicate balance between accuracy, MIA AUC, and bias. For CIFAR-100S, the introduction of DP significantly improves privacy and consequently helps the balance between accuracy, MIA AUC, and bias.\nFurthermore, despite beginning at a high value, the bias in model outcome shows a slight reduction as the epsilon rises. This reduction, albeit small, indicates that increasing the privacy budget can slightly mitigate the exacerbation of bias, though it does not fully resolve the fairness issues introduced by biased training data.\nFigure 4 demonstrates that increasing privacy budgets and reducing the training data bias result in higher HS, offering a better balance between accuracy, MIA AUC, and bias. These results suggest that careful adjustment of privacy budgets and efforts to reduce (training) data bias are essential for achieving a more balanced model performance."}, {"title": "Onion Effect Impact", "content": "This section explores the Onion Effect [5], a phenomenon where peeling away a layer of outlier samples that are most vulnerable to a privacy attack unveils a subsequent layer of samples newly vulnerable to the same attack. Our investigation extends and differentiates from the work of [5] by conducting a broader analysis. This includes examining models trained without DP not only on the unbiased CIFAR-10 but also those trained without DP on the unbiased CIFAR-100 and models trained with and without DP on the biased CIFAR-10S and CIFAR-100S. We evaluate not only MIA AUC but also the models' accuracy when relevant, bias, and HS.\nFigure 5a shows removing outlier layers decreases accuracy, suggesting such removals deprive the model of essential information for more precise predictions. The decline in accuracy underscores the critical role that even the most vulnerable samples play in the overall predictive capability of the model.\nFigure 5b illustrates that MIA AUC stays the same or goes slightly down in private and non-private models. This describes the Onion Effect, continuously exposing vulnerable data points as layers of outliers are peeled away. The persistence of high MIA AUC values indicates that removing outliers does not mitigate the inherent vulnerability of the remaining data, confirming the findings of Carlini et al. [5] even though their attack type differs from ours. A reason for this effect lies in counterfactual influence [5]: the model's behavior toward a target data point can be significantly shaped by the presence of nearby samples in feature space. When the most influential outliers are removed, previously stable inliers near the boundary of the data distribution gain higher counterfactual influence, effectively becoming the new outliers. This shift increases the membership inference advantage for these remaining points, as their prediction behaviors now exhibit heightened sensitivity to changes in the training set composition, making them more vulnerable to privacy attacks.\nFigure 5c indicates removing outliers may increase bias, likely because discarding outlier layers lowers accuracy for underrepresented groups, exacerbating bias in the model's predictions. This increase in bias reveals a potential trade-off between privacy and fairness, where efforts to protect privacy might inadvertently harm model equity.\nFinally, Figure 6 shows that outlier removals can be useful in balancing accuracy, privacy, and fairness, particularly in private contexts. The improvement in HS values after outlier removal suggests that such interventions may provide a viable solution for achieving a balanced optimization of these key metrics."}, {"title": "REAL-WORLD SETTING: VALIDATION WITH CELEBA ATTRIBUTES", "content": "To validate our findings from synthetic datasets, we use the CelebA dataset for a real-world evaluation of facial attribute recognition.\nFor an attribute present in the images of Nm men and Nw women, we weight positive images of men by $\\frac{(N_m + N_w)}{2 N_m}$ and positive images of women by $\\frac{(N_m + N_w)}{2 N_w}$. This adjustment ensures equal consideration of positive samples from men and women. Additionally, we apply the bias metric, as described in Eq. 2, to assess how the model's predictions might amplify existing gender imbalances. This approach enables us to assess the model's ability to handle real-world biases and validate the trends observed in controlled, synthetic settings.\nIntegrating generalization techniques increases the mAP of both the non-private Baseline model and the private Baseline + (8, 10\u22125)-DP, aligning with the trend observed. However, this integration also amplifies bias in both the non-private Baseline and private Baseline + (8, 10\u22125)-DP, consistent with the trend shown. Furthermore, observing higher MIA AUC values for attributes with greater skewness (\u2265 0.8) supports the conclusion from Figure 3c and Figure 8c that increased training data bias makes models more susceptible to MIAs."}, {"title": "CONCLUSION", "content": "Our research contributed to the differentially private ML field by exploring the balance between accuracy and privacy when generalization techniques are used. We improved model accuracy by integrating DP-SAT with other methods, such as group normalization, optimal batch size, weight standardization, parameter averaging, and augmentation multiplicity. Specifically, we attained a new accuracy milestone of 81.11% under (8, 10-5)-DP on CIFAR-10, surpassing the previous benchmark [8]. Our analysis demonstrated the superior performance of DP-SAT compared to DP-SGD across various privacy parameters and standard image classification benchmarks. These advancements underscored the potential of our approaches to improve the privacy-utility trade-off, significantly boosting model accuracy in private and non-private learning scenarios.\nFurthermore, our investigation into the impact of generalization techniques on model bias showed a trade-off between accuracy and fairness. While these techniques enhanced accuracy, they also increased model bias, particularly in models trained on biased datasets. A key finding of our study is that the generalization techniques may not account for the diverse representations in the data. As these methods optimize for overall accuracy, they may excessively improve predictions for the well-represented groups, thus widening the accuracy gap and increasing bias against underrepresented groups. Another insight of our study is that biased data undermines the fairness of models and increases the privacy risks of MIAs, even in the presence of DP. Biased training data can lead to skewed model outputs, which adversaries may exploit to infer membership information about specific individuals in the dataset.\nSpecifically, we showed that even with early stopping and generalization techniques, certain samples are memorized early in the training process, indicating that these techniques may leave specific samples vulnerable to privacy risks. This highlights a limitation in generalization methods for mitigating privacy concerns. We further expand our experiments to a real-world setting using the CelebA attribute dataset, showing that our results consistently align with real-world attribute imbalances.\nAdditionally, we introduced the HS metric as a novel tool to evaluate the interplay between accuracy, bias, and privacy, providing a framework for assessing model performance. Our findings highlight that reducing training data bias is instrumental in increasing HS, thus enhancing the balance of privacy, accuracy, and fairness. Our study extended the understanding of the Onion Effect, revealing that removing layers of outliers consistently affected model accuracy, privacy, and bias, further complicating the balance between these aspects in ML models."}, {"title": "Limitations and Future Work", "content": "Our ablation study investigated the contributions of generalization techniques when added iteratively in a fixed sequence (GN \u2192 OBS \u2192WS \u2192 AM \u2192 PA), as suggested by De et al. [8"}]}