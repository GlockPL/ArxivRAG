{"title": "MEDMAX: Mixed-Modal Instruction Tuning for Training Biomedical Assistants", "authors": ["Hritik Bansal", "Daniel Israel", "Siyan Zhao", "Shufan Li", "Tung Nguyen", "Aditya Grover"], "abstract": "Recent advancements in mixed-modal generative models have enabled flexible integration of information across image-text content. These models have opened new avenues for developing unified biomedical assistants capable of analyzing biomedical images, answering complex questions about them, and predicting the impact of medical procedures on a patient's health. However, existing resources face challenges such as limited data availability, narrow domain coverage, and restricted sources (e.g., medical papers). To address these gaps, we present MEDMAX, the first large-scale multimodal biomedical instruction-tuning dataset for mixed-modal foundation models. With 1.47 million instances, MEDMAX encompasses a diverse range of tasks, including multimodal content generation (interleaved image-text data), biomedical image captioning and generation, visual chatting, and report understanding. These tasks span diverse medical domains such as radiology and histopathology. Subsequently, we fine-tune a mixed-modal foundation model on the MEDMAX dataset, achieving significant performance improvements: a 26% gain over the Chameleon model and an 18.3% improvement over GPT-40 across 12 downstream biomedical visual question-answering tasks. Additionally, we introduce a unified evaluation suite for biomedical tasks, providing a robust framework to guide the development of next-generation mixed-modal biomedical AI assistants.", "sections": [{"title": "Introduction", "content": "Recently, there has been rapid advancement in the development of mixed-modal foundation models that can perceive and generate data from multiple modalities such as Chameleon [54], Transfusion [76], Aurora [8], Gemini-2.0-Flash [18], and others [75, 58, 51, 70]. In particular, these models offer a flexible design, which embeds multiple modalities into a shared embedding space and uses a transformer backbone to learn interactions between these diverse modalities. During pretraining, these models are exposed to internet-scale data that equips them with the knowledge to perform real-world tasks involving multiple modalities within a unified architecture (e.g., image captioning and image generation). The ability of foundation models to understand and reason across the vast landscape of biomedical data\u2014including multiple modalities (e.g., image-caption pairs), sources (e.g., medical research papers, YouTube channels), and domains (e.g., radiology, histology) positions them as powerful tools for biomedical diagnosis, prognosis, and prevention [6, 59]. their multimodal generation capabilities can facilitate medical treatment planning by predicting a patient's CT scan outcomes following specific treatments.\nHowever, existing mixed-modal foundation models struggle to perform well on vision-language biomedical data (e.g., visual question answering on X-rays) due to significant distribution shifts from the more commonly occurring natural data found on the internet (e.g., everyday objects and scenes). In this context, instruction tuning [60, 39, 29] offers a promising approach to understanding novel user intents and unlocking new capabilities for developing advanced biomedical assistants. But, there is a lack of large-scale multimodal biomedical instruction-tuning datasets, which are crucial for enabling mixed-modal models to reason and solve complex biomedical tasks across diverse domains.\nTraditional biomedical visual question answering (VQA) datasets such as VQA-RAD [30], SLAKE [36], and PathVQA [20] are crucial for imparting domain-specific knowledge. However, they suffer from several limitations such as lack of scale (e.g., typically only thousands of instances), task diversity (e.g., not suitable for teaching biomedical captioning or generation), and narrow focus on specific domains (e.g., radiology or pathology). In contrast, LLaVA-Med [39] collects biomedical vision-language alignment data (i.e., image-text pairs) and curated synthetic instruction-tuning datasets (i.e., image-conditioned conversational data). This aids in allowing the foundation models to assist the practitioners for novel queries about the biomedical images. However, it relies on PMC-15M [72], which is not openly available. Moreover, much of the LLaVA-Med data consists of figures and plots rather than biomedical images, impacting its overall quality.\nSubsequent work like PubMedVision [71] aims to improve data quality through synthetic data curation across diverse biomedical domains. Nevertheless, it is limited to the knowledge available in medical research papers, restricting its ability to generate medical reports (e.g., findings and impressions based on biomedical imagery) that could assist doctors. Other datasets, such as Quilt-LLaVA, independently create visual instruction-tuning data for histopathology using alternative data sources, such as YouTube videos. Similarly, MIMIC-CXR provides expert-written radiology reports for chest X-rays. Furthermore, the ability to generate biomedical images is crucial for alleviate the lack of annotated medical imaging datasets [13] and disease progression modeling [19]. However, there are no existing biomedical instruction tuning datasets that focus on biomedical text-to-image generation and interleaved image-text content.\nTo address these challenges, we propose MEDMAX, a dataset designed to develop a biomedical mixed-modal foundation model. It comprises a total of 1.47M instances spanning a wide range of biomedical tasks and domains. Specifically, MEDMAX includes tasks such as biomedical image captioning, image generation, visual question answering (VQA), visual chatting, report understanding, and multimodal (interleaved text-image) content generation. Moreover, the dataset encompasses diverse biomedical domains, including radiology and histopathology. A key component of MEDMAX is a newly curated dataset for generating interleaved image-text content (MEDMAX-INSTRUCT), which paves the way for enhanced clinical understanding and support for complex medical decision-making. Additionally, MEDMAX aims to equip mixed-modal models with a diverse skill set by integrating various high-quality multimodal datasets, including VQA datasets, instruction-following datasets, alignment datasets, and medical reports.\nSubsequently, we fine-tune a mixed-modal foundation model, Chameleon [54, 15], on the MEDMAX dataset. The dataset comprises a total of 1.7B multimodal discrete tokens for instruction tuning. In our experiments, the MEDMAX-fine-tuned model outperforms the base Chameleon and GPT-40 [23] by 26% and"}, {"title": "MEDMAX", "content": null}, {"title": "Background", "content": null}, {"title": "Mixed-Modal Foundation Models", "content": "Mixed-modal foundation models are a class of generative models that can reason over the sequence of arbitrarily interleaved multimodal (e.g., image, text) content [54, 76]. In this work, we primarily focus on the autoregressive sequence modeling objective, as used by Chameleon [54], Unified-IO [40], and Emu-3 [58], for its simplicity. Formally, these methods model interleaved multi-modal tokens x = (x1,x2,...,xn) where n is the length of the sequence. Here, the text content is represented as BPE tokens, while image tokens are obtained from an image encoder. For instance, Chameleon uses 1024 tokens obtained from a VQGAN [17] encoder to represent each image. Given a dataset D, an autoregressive pretraining objective for multi-modal data x \u2208 D can be formulated as:\n$\\max_{\u03b8} \\mathbb{E}_{x \\sim D} [\\sum_{k=1}^{n} \\log P_\u03b8(x_k | x_{1:k-1})]$\nBy incorporating a diverse set of multimodal sequences at an internet scale, these pretrained models can achieve strong performance on downstream tasks such as image generation and image captioning. We illustrate the architecture for a mixed-modal foundation model in Figure 3."}, {"title": "Instruction Tuning", "content": "While internet-scale pretraining equips foundation models with diverse world knowledge, further instruction tuning is conducted to develop new skills and teach them how to interact with humans as assistants [60, 53]. In this context, it is essential to instruction-tune mixed-modal foundation models to impart task-specific skills. Formally, the instruction tuning data comprises paired multimodal sequences (x, y), where x represents the 'instruction' and y represents the 'response.' Both x and y may include image tokens, text tokens, or a combination of both. For instance, in the context of visual question answering (VQA) tasks, x includes an input image along with a corresponding question, while y provides the correct response. The instruction tuning objective for a dataset D\u2081 can be formalized as:"}, {"title": "MEDMAX", "content": "Due to the internet-scale pretraining and flexibility of the mixed-modal foundation models, we aim to imbibe the capability to understand human intents to solve diverse biomedical tasks (e.g., VQA, generation), domains (e.g., radiology, histopathology), and modalities (e.g., image and text content) through instruction tuning. Thus, we present MEDMAX, an instruction tuning data designed training mixed-modal foundation models for the biomedical applications. Specifically, the dataset construction involves: (a) designing a new instruction-tuning data that allows interleaved image-text content as a model response, (b) collecting various data sources to endow diverse skills into the model, and (c) performing dataset-specific curation to ensure high-quality. We illustrate the examples for diverse skills in Figure 4. We will go through each of these components in this section."}, {"title": "Biomedical Multimodal Generation Instruction Tuning (MEDMAX-INSTRUCT)", "content": "With the emergence of mixed-modal foundation models, generating interleaved image-text content has become possible. Specifically, such multimodal output capabilities have several novel applications, including advanced diagnostics (e.g., visualizing the effects of specific treatments on biomedical image markers), generating patient reports that are often multimodal, and enhancing medical training by providing contextually relevant multimodal content for learners. However, no instruction-tuning datasets currently exist to equip mixed-modal assistants with this capability. To address this challenge, we present a multimodal output instruction-tuning"}, {"title": "Dataset Curation for Diverse Skills", "content": "We aim to collect a diverse set of biomedical datasets to teach novel skills to the mixed-modal foundation models. We curate high-quality datasets for individual skills as follows:\nVisual question answering (VQA): The ability to answer questions for the images is crucial for biomedical assistants. In this regard, we include biomedical visual questions that involve answering close-ended questions (yes/no), open-ended questions, and multiple-choice questions in MEDMAX. In this context, the biomedical image and question constitute the input (or context), while the ground-truth answer serves as the output (or response).\nPrior work such as LLaVA-1.5 [38, 32] have included the data from the general-purpose VQA benchmarks in their instruction tuning mix to enhance the model's visual capabilities. Hence, we combine the training sets of popular VQA datasets including VQA-RAD [30], SLAKE [36], PathVQA [20], and PMC-VQA [73]. To embed the knowledge about different anatomical regions (20+ regions), we split the OmnimedVQA [22] dataset into a training (81K) and testing set (1K), and add the training set into the MEDMAX data. In addition to the diversity in the domains and question styles, these datasets are also a rich source of expert-annotated data (e.g., clinician-driven annotation of VQA-RAD) that is critical for building high-quality data. In total, we have 284K VQA instances in the MEDMAX dataset.\nImage captioning and generation: The ability to interpret and analyze the biomedical images is critical for accurate disease monitoring and diagnosis [46]. On the flip side, the capability to generate realistic"}, {"title": "Experimental Setup", "content": "Post data creation, we intend to instruction-tune a mixed-modal foundation model on the MEDMAX data (\u00a75.1). Subsequently, we will present the evaluation methods to assess the performance of our model across diverse skills (\u00a75.2)."}, {"title": "MEDMAX Mixed-Modal Model", "content": "In this work, we will instruction-tune an instantiation of the Chameleon-7B [54] mixed-modal foundation model. Specifically, it can understand and generate multimodal content including images and text. Primarily, it represents the raw images as discrete visual tokens using a VQGAN [17], and the text data into discrete text tokens using BPE tokenizer [48]. Subsequently, each instance in the training dataset is represented as a sequence of discrete tokens, and the model is trained to predict the next token based on the preceding tokens in the sequence, following an autoregressive objective. The model consists of a vocabulary size of 65536 where 8192 are visual tokens apart from beginning of image and end of image tokens. In addition, the vocabulary includes a reserved token \u2018<reserved08706>\u2019 that separates the instruction (context) from the response (output) for instruction-tuning. Post-tokenization, the entire MEDMAX data consists 1.7B tokens where 0.7B and 1B are visual and text tokens, respectively.\nWhile the Chameleon-7B model weights are publicly available, they were safety-tuned and support mixed-modal inputs and text-only output to be used for research purposes. To unlock the mixed-modal output capabilities, Anole [15] selectively finetunes the output embeddings of the image tokens using high-quality images from LAION [47]. This strategy does not interfere with the input mixed-modal understanding and text-only output abilities of the original Chameleon model. Hence, we choose Anole-7B as our base model for"}, {"title": "Evaluation", "content": "Post-training, it is critical to evaluate the capabilities of the instruction-tuned mixed-modal model across diverse skills. We provide the summary of the tasks and datasets in our comprehensive evaluation suite in Table 2. We will present the individual components here:\nBiomedical VQA: This task involves answering complex questions about the biomedical images from different domains and anatomical regions of the human body. Specifically, we include the test set of VQA-RAD (radiology), SLAKE (semantic knowledge over radiology), PathVQA (pathology), and the entire QuiltVQA (histopathology) dataset. These datasets ask closed-ended (yes/no) and open-ended questions that require one word, phrase or sentence answer. Here, we use exact match to assess the accuracy of the models on the"}, {"title": "Biomedical image captioning and generation", "content": "Here, we compare the capability of the MEDMAX model and base model to interpret and generate biomedical image data across different biomedical domains. We present the results in Figure 7. Our empirical findings that the MEDMAX consistently outperforms the base model on the biomedical image captioning as well as the image generation task. In particular, the MEDMAX outperforms Chameleon by achieving a relative gain of 28%, 33% and 14% on biomedical captioning for the images in the PMC-OA, Quilt, and MIMIC-CXR datasets, respectively. Additionally, the MEDMAX also outperforms the Chameleon by achieving a relative gain of 100%, 14%, and 50% on the image generation tasks across PMC-OA, Quilt,and MIMIC-CXR captions, respectively.\nFurther, we compare the captioning abilities of our model against other models in Appendix H. In particular, we observe that the average BioMedCLIPScore for our model is better LLaVA Med-v1.5 and at par with baselines such as HuatuoGPT-Vision, GPT-40-mini, and GPT-40. However, we highlight that none of the those models can perform image generation natively, unlike MEDMAX and Chameleon models. Overall, our results demonstrate that the MEDMAX model excels at reasoning about images and generating realistic biomedical visuals aligned with user intent."}, {"title": "Biomedical Multimodal Generation", "content": "Here, we aim to study the capability of the MEDMAX model to generate multimodal (interleaved image-text) content. We present the results in Figure 8. Our empirical findings highlight that the MEDMAX outperforms Chameleon by achieving a relative improvement of 25.2% on the quality of the text content in the multimodal output, as measured by LLM score. Furthermore, we observe that the MEDMAX outperforms Chameleon by achieving a relative improvement of 31.5% on the synthesized biomedical image in the multimodal output, as measured by the image-image similarity score using BioMedCLIP. This indicates that instruction tuning with MEDMAX data equips a mixed-modal model with strong multimodal generation capabilities in the"}, {"title": "Biomedical Visual Chat", "content": "Here, we study the ability of the MEDMAX model and other baselines including Chameleon, LLaVA-Med-v1, LLaVA-Med-v1.5, and HuatuoGPT-Vision to answer novel queries about the biomedical images in the PMC dataset. We present the results in Figure 9. We observe that the overall LLM score of MEDMAX model is higher than the base Chameleon model and LLaVA med-v1 by 34% and 10.2% percentage points. This indicates instruction-tuning enables strong visual chatting capability to the mixed-modal models. In addition, the MEDMAX model is quite competitive in comparison to the best-performing model, LLaVA Med-v1.5, with a difference of 2.1% percentage points.\nThe fine-grained analysis indicates that the LLaVA med-v1.5 gains most of the performance on the conversation split of the dataset, while HuatuoGPT-Vision is better on the description split of the dataset. Such improvements in visual chat performance for LLaVA med-v1.5 and HuatuoGPT-Vision can be attributed to the quality of their language backbones, namely Mistral-v0.2-Instruct [25] and Qwen-2 [66], respectively. These backbones enhance the models' ability to interpret human queries and generate high-quality textual outputs. We believe that future advancements in base mixed-modal models will further enhance performance on biomedical visual chatting using the MEDMAX dataset."}, {"title": "Data scaling", "content": "Now, we explore how the benefits of mixed-modal instruction tuning scale with the size of the dataset. Specifically, we finetune the Anole-7B (instantiation of Chameleon-7B) with three subsets of the MEDMAX including 25%, 50%, and 75% of the data. Subsequently, we evaluate the average performance on the twelve VQA tasks for these subsets and the entire dataset. We present the performance vs the dataset scale in Figure 10. Our empirical finding suggests that the downstream performance of the instruction-tuned mixed-modal"}, {"title": "Impact of Specialized Visual Encoder", "content": "In the visual instruction tuning literature [33], a two-stage fine-tuning approach is commonly employed. In the first stage, the visual encoder is specialized to match the distribution of images in the instruction-tuning dataset, while keeping the transformer backbone of the architecture frozen. In the same spirit, we explore whether finetuning of the Chameleon's VQGAN encoder with biomedical images before instruction-tuning with MEDMAX leads to a better downstream model. In this regard, we finetune the base VQGAN with 300K images from the MEDMAX dataset for 8 epochs. We find that the L1 reconstruction loss for the finetuned VQGAN was 7.8 in comparison to the base VQGAN 8.1 on a set of held-out biomedical images. This indicates that the fine-tuned encoder effectively represents the new domain better than the base visual encoder. Next, we select a random subset of 800K samples from the MEDMAX dataset and tokenize the images using the newly fine-tuned visual encoder. We then fine-tune the multimodal model using both the original subset and the re-tokenized subset under identical settings.\nWe present the performance of the two instruction-tuned mixed-modal models in Figure 12. We find that the model finetuned with the new discrete visual tokens achieves an inferior performance to the model finetuned with the original (base) visual tokens from the VQGAN. Specifically, we observe a gap of 3% averaged across the VQA datasets in our evaluation suite. This can be attributed to the distribution shift in the discrete visual tokens relative to the base model, where the base VQGAN visual tokens are better aligned with the base model's representations. Further exploration of fine-tuned, specialized visual encoders for discrete multimodal models is left for future work."}, {"title": "Related Work", "content": "Multimodal biomedical assistants. While early biomedical language models like ChatDoctor [34], MedicalGPT [63], and HuatuoGPT [71] advanced text-only medical reasoning (often built upon large language models such as LLaMA or Alpaca variants), they lacked multimodal capabilities for integrating visual information. This limitation prompted the development of multimodal biomedical models, ranging from encoder-only architectures like BiomedCLIP [72] to generative vision-language models capable of producing medical explanations. For instance, Med-Flamingo [42] extended OpenFlamingo [9] to a few-shot medical VQA paradigm via continued pre-training on curated image-text pairs. MedVInT [73], based on a pre-trained"}, {"title": "Conclusion", "content": "In this work, we present MEDMAX, an instruction-tuning dataset designed for training mixed-modal foundation models. This dataset equips models with diverse multimodal biomedical capabilities across several domains. Notably, it is the first instruction-tuning dataset to enable multimodal generation capabilities for biomedical AI (MEDMAX-INSTRUCT). Additionally, MEDMAX allows mixed-modal models to respond effectively to novel human instructions, functioning as capable biomedical assistants. Our experiments demonstrate that the MEDMAX-tuned model achieves competitive results across various downstream tasks, including biomedical visual question answering, multimodal generation, image captioning, image generation, and visual chatting. Overall, our work establishes a strong foundation for training the next generation of mixed-modal biomedical AI assistants."}, {"title": "Limitations", "content": "In this work, we focus on diverse multimodal biomedical skills, including VQA, multimodal generation, visual chat, image captioning, image generation, and report understanding. While these skills cover a broad spectrum of tasks, there remain additional possibilities that could further enhance biomedical applications. For instance, we do not address setups that involve understanding and generating multiple images, which are critical for applications such as counterfactual biomedical image generation [19] and reasoning from multiple images [69]. Achieving this capability presents significant challenges, including the lack of openly available, large-scale, high-quality multi-image biomedical datasets and the limited context length of the pretrained (base) Chameleon model. To bridge this gap, more efficient methods for representing image data are required, rather than always encoding all images as 1024 tokens, which occupy a substantial portion of the model's context length. We leave these explorations for future work.\nFurthermore, we emphasize that the MEDMAX model is a research prototype designed to encourage community engagement in building capable biomedical assistants. In cases of medical emergencies, we strongly advise users to consult a healthcare professional rather than relying solely on the model's output. With ongoing community feedback and the collection of high-quality expert data, we aim to improve the model's faithfulness and safety over time."}, {"title": "MEDMAX-INSTRUCT Data Curation Prompts", "content": "We present the GPT prompt to filter the bad captions from the real image-caption data in Table 4. Further, we provide the prompt for generating multimodal generation conversation using GPT in Table 5."}, {"title": "LLAVA-Med-PMC Curation", "content": "We curated a dataset of 37.8K medical images filtered from an initial pool of 538K images, sourced from the data released by LLaVA-Med [33], which originates from PMC-15M [72]. The initial dataset contained a significant number of statistical figures, as the images were extracted from research articles. To filter these out and retain only the desired medical images, we utilized the pretrained BiomedCLIP [72] model to classify images based on the taxonomy defined in PMC-15M.\nOur focus was on retaining images from the classes \u201cMagnetic Resonance,\u201d \u201cCT,\u201d \u201cX-Ray,\u201d \u201cECG,\u201d \u201cLight Microscopy,\u201d \u201cDermatology,\u201d and \"Endoscopy,\u201d which represent the primary topics used in LLaVA-Med. To determine class-specific confidence thresholds, we manually labeled 80 images and evaluated the model's predictive confidence using ROC curves, identifying optimal thresholds through Youden's J statistic to balance sensitivity and specificity. Additionally, we applied a heuristic to exclude images with high prediction scores for statistical figures within the top five predictions. This filtering process resulted in a high-quality dataset of 37.8K images, focused on key medical imaging modalities."}, {"title": "Data creation templates for captioning, generation, and report understanding", "content": "Table 7 and Table 8 present complementary approaches to image captioning, with the former focusing on concise, brief descriptions and the latter encouraging comprehensive, detailed analyses of image content. Table 9 demonstrates various prompts for generating images from text descriptions, using diverse language"}]}