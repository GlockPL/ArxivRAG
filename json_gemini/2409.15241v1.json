{"title": "Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping", "authors": ["Guanhua Wang", "Chengming Zhang", "Zheyu Shen*", "Ang Li*", "Olatunji Ruwase"], "abstract": "Given the popularity of generative AI, Large Language Models (LLMs) often consume hundreds or thousands of GPUs for parallelizing and accelerating the training process. Communication overhead becomes more pronounced when training LLMs at scale. To eliminate communication overhead in distributed LLM training, we propose Domino, which provides a generic scheme to hide communication behind computation. By breaking data dependency of a single batch training into smaller independent pieces, Domino pipelines these independent pieces training and provides generic strategy of fine-grained communication and computation overlapping. Extensive results show that, comparing with Megatron-LM, Domino achieves up to 1.3x speedup for LLM training on Nvidia DGX-H100 GPUs.", "sections": [{"title": "Introduction", "content": "Recent advances in Generative AI (GenAI) enable new application scenarios in various domains, such as chat-bot [53], text generation and summary [11, 52], image and video content creation [62]. These GenAI applications are based on carefully trained foundation models as large language models (LLMs). Well-establised LLMs are transformer models, such as GPT [11, 52, 60] and Llama [5, 6, 72, 73] series. Given LLM model sizes are usually ranging from tens to hundreds of billion parameters which is far exceeding a single GPU's memory and computation limit, distributed model training over hundreds to thousands of GPUs is necessary.\nFor LLM transformer training, three prominent paradigms have emerged: data parallelism (DP), tensor parallelism (TP) and pipeline parallelism (PP). Vanilla data parallel training refers to every GPU maintaining a full copy of whole model parameters but training on different input data. Model parameter need to be synchronized at the end of each training iteration among all GPUs in use. To mitigate memory pressure from LLMs' huge volume of parameters in DP training, ZeRO [61]\nAs described in the literature [67], every transformer layer needs to communicate twice in forward and another twice in the backward using NCCL collectives [30] (\u00a7 2.3). Given these collective communications happen on the critical path of execution, it is hard to hide these communications behind successive computations with the general communication overlapping strategy used in DP [61] or PP [24] training process. Prior arts [54, 55] report this communication overhead can be up to 45% of end-to-end training iteration time. As one of our measurements depicted in Figure 1, even with the lastest DGX-H100 nodes connected with 400GB/s IB, communication still takes from 17% to 43% of end-to-end GPT-3-13B training iteration time. Furthermore, the communication ratio would continue to grow when scale up to more nodes. To mitigate this high communication overhead in TP, prior work [54, 78] provides kernel fusion of a GeMM (General Matrix Multiplication) with its subsequent collective calls (e.g., NCCL [30]) to achieve fine-grained computation-communication overlapping. However, this type of kernel fusion technique limits the overlapping scope and is not general enough to always hide communication behind computation. Especially in the cases where collective communication takes much longer than a single GeMM computation, most of the communication time still stands out as the major training overhead. Furthermore, given that computation on the latest GPUs (e.g., DGX-H100 [22], DGX-B200 [45]) is becoming faster, communication overhead is more pronounced in both single node and multi-node cases.\nTo provide a generic approach of hiding communication behind computation in TP, we propose Domino, a generic approach that breaks data dependency of transformer model training into pieces, and then pipelines these pieces training to overlap communication with computation. Besides traditionally TP can only be used within a node, Domino provides a uniformed TP solution for both single-node multi-GPU and multi-node multi-GPU cases. Compared with previous GeMM+NCCL fusion solutions, Domino provides a much wider scope of computation and communication overlapping (e.g., AllReduce not only overlaps with a single GeMM, but also LayerNorm, DropOut, etc). Additionally, any kernel fusion and optimization techniques can be easily integrated with Domino as a drop-in replacement to further boost overall system efficiency.\nExtensive benchmark results are collected from Nvidia latest hardware DGX-H100 boxes, which are connected with 3200 Gb/s (i.e., 400 GB/s) InfiniBand (IB) fabrics [46]. We benchmark training with popular transformer models such as GPT-3 [11] and Llama-2 [73]. Compared with state-of-the-art TP implementation Megatron-LM from Nvidia, Domino achieves up to 1.3x speedup for both single-node and multi-node cases. Overall, Domino provides a generic approach of flexible overlapping of communication with a wide range of computation kernels for transformer training.\nWe summarize our key contributions as follows:\n\u2022 To the best of our knowledge, Domino is the first work providing an end-to-end solution of generic communication-computation overlapping for tensor-parallelism-only training in both single-node and multi-node cases.\n\u2022 Compared with previous arts, Domino provides a more flexible and wider range of computation and communication overlapping strategies.\n\u2022 Experiment results on DGX-H100 boxes show that, compared with Megatron-LM, Domino achieves up to 1.3x speedup for GPT and Llama models training.\n\u2022 Domino will be open-sourced and released as part of https://github.com/microsoft/DeepSpeed"}, {"title": "Background and Motivation", "content": "In this section, we first describe the most widely used distributed transformer training schemes as data parallelism (DP), tensor parallelism (TP) and pipeline parallelism (PP) as \u00a7 2.1. Then we illustrate why TP is becoming increasingly popular among these three approaches as \u00a7 2.2. Finally, we analyze the communication overhead of TP in both single-node multi-GPU and multi-node multi-GPU cases in \u00a7 2.3."}, {"title": "Distributed Training Schemes", "content": "There are mainly three different kinds of paradigms for distributed LLM training, namely, data parallelism (DP), tensor parallelism (TP), and pipeline parallelism (PP).\nIn vanilla DP, each GPU maintains a full copy of model weights and consumes different input data. At the end of each training iteration, all GPUs involved conduct an AllReduce operation to synchronize model parameters. Specifically for transformer models, ZeRO [61] and FSDP [81] are widely used to reduce memory pressure on devices. In these fully shared data parallel schemes, whole model weights are often evenly split across all GPUs. When computation needs to happen on a specific layer, every GPU recollects the full weights of this particular layer by conducting an AllGather operation among all GPUs for this layer. Once the computation is done, every GPU releases the whole layer weights and only maintains the portion of weights that were originally assigned on each GPU. Therefore, ZeRO/FSDP can be regarded as a memory efficient data parallelism scheme that trades more communication with less on-device memory footprint.\nPP and TP are both representative of model parallelism techniques. PP partitions a layer or a group of layers on a single GPU and then pipeline executes from GPU holding the first layer to GPU holding the last layer during forward propagation, and then backward propagation in the reverse order. Compared with PP, TP partitions the model in an orthogonal direction, where each GPU holds a portion of every"}, {"title": "TP is Trending", "content": "Tensor parallelism is gaining popularity for LLM workloads on Nvidia GPUs. AI practitioners have recently witnessed substantial improvements in both TP software and hardware stacks.\nOn the software side, Nvidia consistently enhances its Megatron-LM [33, 67] software stack as the state-of-the-art TP implementation. Megatron-LM achieves greater efficiency through integrating with more fine-tuned and customized compute kernels sourced from libraries like apex [44], cutlass [17], cublas [15], cudnn [16]. Besides that, Megatron-LM also involves new features to enhance overall system throughput, including selective activation checkpointing, and sequence parallelism strategies [33].\nMore importantly on the hardware front, Nvidia is pushing hard to bridge the bandwidth gap between intra-node and cross-node links, which is essential for extending TP to cross-node use cases. The latest DGX-H100 node is equipped with eight Nvidia ConnectX-7 InfiniBand (IB) cards [42], and each provides 400 Gb/s bandwidth. Thus each DGX-H100 box achieves 400 GB/s cross-node communication bandwidth, which is comparable to intra-node NVLink/NVSwitch bandwidth as 900 GB/s [22]. Furthermore, advancements in Nvidia's network infrastructure suggest that future DGX systems could potentially integrated with Nvidia ConnectX-8 IB cards [48], offering up to 800 GB/s aggregated cross-node bandwidth, approaching the bandwidth available with intra-node NVLink/NVSwitch connections.\nWith these advancements in software and hardware, both PyTorch [69] and raising vLLM [75] communities lean to-"}, {"title": "TP Communication Overhead", "content": "We conduct measurements using state-of-the-art TP implementation from Nvidia as Megatron-LM [33, 47, 67].\nTP communication possesses more unique characteristics compared with PP or DP solutions, mainly because it resides on the critical path of every input batch training. Hiding communication behind computation is standard practice not only limited to LLM training but also extensively applied in all distributed system environments [18, 23, 38, 77, 82]. For transformer training using DP or PP, the overlapping of communication and computation is quite straightforward since we can schedule communication on a side channel thus bypassing the critical execution path. For DP approaches like ZeRO [61] or FSDP [81], pre-fetching weights enable overlap with computation, as weights inherently do not have any sequential data dependency. PP naturally overlaps communication and computation by processing on different input batches. For instance, on each GPU, PP can overlap the previous batch's communication with computation for current batch data.\nAs described in Megatron-LM [47, 67], each transformer block comprises a self-attention layer and an MLP (multi-layer perceptron) layer. As shown in Figure 2, both self-attention and MLP layers trigger an AllReudce operation"}, {"title": "Domino Design", "content": "In this section, we describe the detailed design of Domino architecture. We first provide an overview of system architecture (\u00a7 3.1). Then we detail how to generically partition computation and overlap sequences of computation kernels with communication (\u00a7 3.2,\u00a7 3.3, \u00a7 3.4)."}, {"title": "Overview", "content": "We first describe overall workflow of Domino. Given standard transformer architecture, we abstract both self-attention layer and MLP (multi-layer perception) layer as weight tensors of A_FULL and B_FULL, where A_FULL stands for attention weights (i.e., Wq,Wk,Wv) for self-attention layer but linear weights for MLP, and B_FULL is linear weights for both self-attention and MLP layer. For ease of illustration, we describe our partition strategy in forward propagation since backward propagation is just in reverse execution order. Given layer input data X, both self-attention and MLP layers' computation can be abstracted as Equation 1.\nXA_FULL&B_FULL=Y_FULL (1)\nAs shown in Figure 4, TP (e.g., Megatron-LM) splits whole weights tensor A_FULL column-wise as set{A | Ai on GPU;}, and weights tensor B_FULL row-wise as set {B | Bi on GPU;} for both self-attention and MLP layers. After every GPU getting its own A and B weights partitions, TP executes X A B = Y and then conducts AllReduce on set {Y | Yi on GPU;} sequentially to recover Y_FULL, which makes communication overhead completely stand-out.\nTo hide TP communication behind computation, Domino provides extra and generic tensor partition in two dimensions on every GPU: row-wise split on inputs X and column-wise split on weights B on top of original TP model partitions.\nAt high level, Domino generically breaks TP's X A B into smaller compute units without data dependency. Then we pipeline these independent compute units with collective communication to achieve fine-grained computation and communication overlapping. With the latest trend that attention computation is modularized and highly optimized like flash-attention [19, 20], windowed-attention [10], etc., we keep A untouched and do not conduct any tensor partitioning on A. Therefore, we only conduct tensor slicing on input tensor X (\u00a7 3.2) and the second group of linear weights as B (\u00a7 3.3). We also provide a hybrid tensor partition strategy of both X and B (\u00a7 3.4). After these tensor slicing, Domino breaks XA B into pieces and removes data dependency. Then we enable computation-communication overlapping on these independent pieces to reduce communication overhead in TP.\nPrior to real model training, we benchmark system efficiency to determine the tensor partition granularity with grid search. These benchmarks guide our selection of tensor computation sizes to ensure minimal impact on computation kernel efficiency. To further enhance system efficiency, Domino also adopts the latest features like kernel fusion, torch.compile [59] and CUDAGraph [43, 58] techniques from PyTorch and Nvidia as described in \u00a7 4.3."}, {"title": "Row Split on Inputs", "content": "We first discuss row-wise split on input data, which refers to tensor partitioning on X in \u00a7 3.1. For ease of illustration in Figure 5, we simplify and assume all tensors are with 2 dimensions. Then X can be split in either row dimension or column dimension. In reality, each layer's input tensor usually is 3D as (batch, seq, hidden). We map row/column dimensions of our example 2D tensor into batch/hidden dimensions of real 3D tensor, respectively.\nXA = {softmax((XW)(XW)) (X * W) for AttnX*A for MLP (2)\nNote that our row-wise split happens on input's batch dimension, it is mathematically equivalent to vanilla baseline. Given row (batch-dim) split on X mainly affects abstracted computation of XA, we illustrate in details on XA to show equivalence of our row-split on X and baseline. Element-wise operations like GeLU() and dropout() are completely independent along batch dimension of X, we exclude them for simplicity. Then we get simplified X A as Equation 2.\nFor MLP, X A is just GeMM between X and A. Therefore, as a toy example in Figure 5, row-wise split on X is equivalent to baseline as Equation 3.\nXA=XA (3)\nFor self-attention, we abstract it as softmax(f(X))g(X). For the second part g(X) = X * Wv, the equivalence proof here is the same as Equation 3 since it is just a GeMM operation. For f(X) = (XWq)(XWk), its output dimensions are (batch, seq, seq). Since Softmax() is conducted on the last dimension of f(X) output as sequence-dim, which is completely independent of first batch dimension. Since softmax(f(X)) and g(X) are both independent in batch dimension and their product is also independent in batch dimension, row-wise split on X for self-attention layer is also equivalent to baseline without tensor slicing.\nData dependency: Since the batch dimension of input tensor is completely independent, no synchronization is needed across all transformer layers. As depicted in Figure 5, Domino's row-split on input achieves both intra-layer and inter-layer (i.e., overlap communication with successive layer computation) computation and communication overlapping."}, {"title": "Column Split on Weights", "content": "With similar analysis as \u00a7 3.2, partitioning weights tensor B in row-dimension for N partitions will lead to N2 times communication volume blow-up. To avoid this, we split the weight tensor B on the column dimension to keep the communication volume the same as the vanilla baseline.\nAs shown in Figure 6, for B, we split it column-wise to N partitions, and each partial output will have the shape of (a,d/N). After collecting all N chunks, we concatenate these partial results ((e.g., Concat(Y3, Y4) in Figure 6)) at the end of each XA B layer computation.\nNow we prove column-wise split on weights B is equivalent to baseline without tensor partition. Since dropout() happens after our concatenation as concatenation output is identical to baseline, it can be safely removed from our proof domain. By excluding element-wise dropout() operation, (XA) B is just GeMM for both self-attention and MLP layers. Thus, the equivalence is shown as Equation 4.\n(XA) B = (XA) [B1, B2] (4)\nData Dependency: given this column-wise split on B, for both self-attention layer and MLP layer, the computation output needs to be synchronized at the end of layer execution. As a toy example of column-wise split of 2 shown in Figure 6, Domino achieves intra-layer computation communication overlapping but needs synchorize (i.e., Concat(Y3,Y4) in Figure 6) before moving to next self-attention or MLP layer computation."}, {"title": "Hybrid Split", "content": "For extremely large LLMs [52], we provide a hybrid model split on both input X and last weight tensor B. This hybrid solution is necessary since either row-split or column-split alone would cause narrow shape tensor which is impossible to achieve good computation efficiency. After doing row-wise split on X together with column-wise split on B, Domino can achieve super fine-grained computation and communication overlapping. The aggregated communication size of X AB still remains the same as vanilla baseline.\nData Dependency: Inherited from column-wise split on B, for both self-attention layer and MLP layer, final computation outputs need to be synchronized column-wise (i.e., Concat(Y3,Y4) in Figure 6) but non-blocking row-wise. Therefore, the hybrid split can only achieve intra-layer computation and communication overlapping."}, {"title": "Implementation", "content": "We now discuss implementation details, which includes row-wise partitioning strategy for input data (\u00a7 4.1), column-wise partitioning approach for model weights (\u00a7 4.2), and further optimization on computational kernels (\u00a7 4.3)."}, {"title": "Tensor Partitioning on Inputs", "content": "We first illustrate the implementation of our novel input partitioning in both forward and backward propagations, separately."}, {"title": "Forward phase", "content": "Users can define the number of partitions p1 for the input, after which the input is divided into p1 partitions along the batch dimension. A for-loop iterates through each partitioned \u03bc-batches sequentially. Figure 7 depicts the forward phase of a simple example where the layer input is split into two \u03bc-batches (i.e., p1 = 2).\nIn Figure 7(a), to hide AllReduce communication (i.e., AllReduce (attn) in Fig. 7(a)) after the self-attention layer, we first execute the self-attention of u-batch 0 and then initiate its AllReduce (i.e., AllReduce(attn0) in Figure 7(b)) asynchronously to prevent GPU blocking on communication. Subsequently, we immediately proceed self-attention on u-batch 1. The communication of self-attention of u-batch 1 (i.e., AllReduce(attn1)) overlaps with layer normalization, residual, and dropout operations. The reason for grouping multiple \u03bc-batches' dropout, residual, layerNorm not only enables hiding AllReduce(attn1) in Figure 7(b) for forward pass, but also provides proper overlapping space for Allreduce(MLPO) in backward pass shown in Figure 8(b).\nSimilarly to hide AllReduce(MLP0) communication in Figure 7(b) in MLP forward, we initiate this AllReduce after MLP computation on u-batch 0 asynchronously, enabling immediately execute MLP of u-batch 1 to achieve overlapping. Additionally, AllReduce(MLP1) after MLP of u-batch 1 will overlap with the computation of u-batch 0 in the successive transformer block."}, {"title": "Backward phase", "content": "The corresponding backward is mostly generated by torch.autograd(). Figure 8 shows a toy example of backward pass where the input hidden states are split into two u-batches (p1 = 2). We carefully organize the execution of gradient computation in these u-batches to overlap gradient computation and communication.\nIn Figure 8, we first adopt similar cross u-batch computation and communication overlapping as described in \u00a7 4.1.1. To further broaden overlapping scope, we also adopts overlapping communication with weights gradient computation within the same u-batch. For example, AllReduce(MLP1) in Figure 8(b) partially overlaps with grad matmul computation of its own u-batch 1 (i.e. 3rd orange block from left). Each grad matmul usually involves two separate kernel computation as inputs gradient and weights gradient computation. This sub-module overlapping can be achieved by first calculating inputs gradient inside 2nd grad matmul in MLP layer of u-batch 1 (i.e. 3rd orange block from left), and then trigger its weights gradient computation and inputs gradient communication simultaneously.\nHowever, accurate control of gradient communication behavior to overlap with gradient computation is challenging because PyTorch automatically generates the gradient computation graph [69]. To precisely control communication start/end time, our initial attempt to manually implement customized backward pass leads to poor throughput performance due to triggering less efficient kernels than torch.autograd(). To tackle this issue, we developed a no-operation module. This module receives the communication handle during the forward phase and retains it for use during the backward phase. Our no-operation module integrates seamlessly with torch.autograd(). This approach allows us to precisely control the completion time of asynchronous communication without complicated code modification.\nTo sum up, Domino enables up to ~100% communication hiding behind computation with our batch-split on inputs."}, {"title": "Tensor Partitioning on Weights", "content": "Users can also define the number of partitions p2 for weights. Subsequently, p2 linear modules are initialized, each with hidden dimension scaled by \u221ap2 .\nBottom half of Figure 6 shows a toy example of the weight partition where the number of partitions for weights is 2. Specifically, we first execute the first linear module (i.e., XA B1) to generate the first half result (i.e., Y3). We then trigger asynchronous non-blocking AllReduce on the first half result. After that, we immediately execute the second half linear module (XA B2). Therefore, AllReduce(Y3) is overlapped with XA B2. In the backward, we adopt similar sub-module overlapping strategy as discussed in \u00a7 4.1.2.\nAn obstacle here is to fully restore hidden dimension (i.e. concat(Y3,Y4) in Figure 6) for subsequent operations (e.g., layerNorm, dropout, etc.). torch.cat() often allocates GPU memory more than needed [34], which may trigger unnecessary OOM (out-of-memory) errors. To achieve concatenation on hidden dimension without torch.cat(), we pre-allocate a big buffer to store the first half (i.e., Y3) and the second half (i.e., Y4) result sequentially in Figure 6. However, this method still incurs extra memory copy (MemCpy) overhead due to non-contiguous memory addresses. We believe this MemCpy overhead can be mitigated or eliminated by implementing customized kernels that simultaneously read from and write to non-contiguous memory addresses. Given current impact of this extra MemCpy is minimal, we defer its optimization to future work.\nIn practice, Domino achieves up to 50% to 70% communication hiding by employing column-wise split on weights. Although this overlapping percentage is lower than batch-wise input split (\u00a7 4.1), this approach remains essential, since that batch-split alone results in tensors with narrow shapes that hinder kernel computation efficiency."}, {"title": "Generic Kernel Optimization", "content": "Here we discuss generic kernel-level optimizations with CUDA-MultiStream and PyTorch-native compiling techniques."}, {"title": "MultiStream", "content": "After splitting the computation into smaller units, the computation required for each kernel is significantly reduced compared to the original TP baseline. To increase GPU utilization while reducing sequential kernel launching overhead, we execute independent operations in parallel using multiple CUDA streams.\nTo obtain a new CUDA stream, one can retrieve it from the CUDA stream pool. However, this method generates an excessive number of new streams and utilizes them in a round-robin fashion, leading to a high overhead from frequent stream switching. To mitigate this, we first initialize and create a fixed number of global streams before execution. Then, we use an index to obtain a specific stream, thereby reducing the overhead associated with stream switching."}, {"title": "CudaGraph & Torch.compile", "content": "torch.compile() functionality from PyTorch accelerates code execution by just-in-time (JIT) compiling PyTorch operations into optimized kernels, enabling improved performance with minimal code modifications [59]. Many operations from the torch library are employed to construct our modules. By fusing distinct operations, we leverage torch.compile() to enhance our computational efficiency.\nAfter Domino slicing tensor into multiple chunks, the computation needed for each chunk is significantly less than the original baseline, leading to GPU idleness between adjacent operations (i.e., bubble time). The primary reason for bubbles is the computation time for different operations is less than the PyTorch scheduling latency. To address this issue, we employ CudaGraph [43, 58] to eliminate the gap between adjacent operations, thereby reducing the overall computation time. However, commonly-used on-device random number generator (RNG) feature is incompatible with CudaGraph. As a workaround, we utilize a fixed seed instead of random numbers to mimic the behavior of the RNG."}, {"title": "Evaluation", "content": "This section provides detailed evaluation and benchmark results of Domino. We first discuss the model and hardware settings of our experiments (\u00a7 5.1). After that, we describe baseline and evaluation metrics (\u00a7 5.2) Then we evaluate"}, {"title": "Model and Hardware", "content": "Before discussion on detailed evaluation results, we first describe models and hardware settings of our experiments."}, {"title": "Model", "content": "We focus on evaluation of GPT [11, 52, 60] and Llama [5, 6, 72, 73] model series. More specifically, we conduct our benchmark tests using GPT-3 [11] model and Llama-2 [73] model with different model sizes. All model configuration details are illustrated in Table 1. For model size calculation, we follow the equation from Nvidia Megatron team [41] as Equation 5, where h refers to the hidden size and l is the number of layers. seq_len represents sequence length and vocab is vocabulary size.\nmodel_size = (1 + 1312 * h + vocab + seq_len12* h) *12*l*h212*h*l (5)"}, {"title": "Hardware", "content": "We conduct experiments on Nvidia DGX-H100 boxes [22], each with 8 H100 GPUs. Within each DGX-H100 node, GPUs are connected with NVLink [50] and NV-Switch [51]. Each DGX-H100 node is equipped with 8 Nvidia InfiniBand ConnectX-7 network cards [42] for cross node communication, which provides an aggregated network bandwidth of 400 GB/s per node. We have three different hardware settings: 1 node, 2 nodes and 4 nodes, which represents both single node and distributed training environments. All nodes run in the same PyTorch environment with NCCL version 2.18 and CUDA version 12.2."}, {"title": "Metrics", "content": "Similar to previous arts on computation-communication overlapping [32, 54, 78], we report results analysis mainly with overall training iteration time. In Equation 6, throughput or TFLOPs can be inferred since it is just inversely proportional to iteration time measurement (i.e., iter_time).\nTFLOPS \u221d 1iter_time (6)\nWe believe iteration time represents more thorough and end-to-end results because CPU side execution (e.g., data pre-processing, learning rate adaptation, etc.) is also taken into account, which may not be included in pure TFLOPs measurements on GPUs. Since we only use TP for model partition and each GPU in TP domain shares the same input, our global batch size is equivalent to our micro batch size.\nOur baseline is using a stable release of Megatron-LM with two different settings: synchronous (sync) and asynchronous (async), where sync (i.e., Megatron-LM(sync)) means all collective operations are blocking calls and async (i.e., Megatron-LM(async)) is to enable backward pass only, coarse-grained computation and communication overlapping feature in Megatron-LM [67]. By default, we compare Domino and Megatron-LM (async) with the vanilla Megatron-LM (sync) as the baseline.\nOne thing worth mentioning is that our Domino scheme is mathematically equivalent to vanilla TP solutions like Nvidia Megatron-LM (as proof in \u00a7 3.2, 3.3). With fixed random seed and the same learning rate schedule, we monitored through weights & bias tool [1], which shows that Domino's loss curve matches with Megatron-LM baseline. For the sake of brevity, we exclude these convergence results here."}, {"title": "GPT-3", "content": "GPT is popular and representative model series of transformers. We conduct model training benchmark of GPT-3 with different model sizes ranging from 2.7B to 30B using 1 to 4 DGX-H100 nodes (i.e., 8 to 32 H100 GPUs). We use two different sequence lengths 512 and 1024. Given that for row-split (i.e., batch-split) on input, our smallest micro-batch size starts from 4 and up to the maximum batch size without triggering OOM. We exclude the cases of micro-batch sizes 1 and 2. Micro-batch size of 1 is impractical for batch-wise input splitting. Additionally, micro-batch size of 2 is excluded because with minimum half-half split, each half batch size is 1, which is impossible to achieve good training throughput.\nAs described in \u00a7 3.1, we conduct grid search and only report the best performance numbers of Domino via both row-wise input split and column-wise weights split. Additionally, we also tried to enable or disable features like CudaGraph() and torch.compile() and report the best numbers that Domino achieved. To achieve good throughput and system efficiency, we report benchmark results of top-2/3 largest micro-batch sizes that \u2265 4 and without causing OOM."}, {"title": "Single-node", "content": "For single node training, we tested model size with 2.7B, 6.7B and 13B. In summary, compared with Megatron-LM, Domino achieves up to 1.3x throughput speed-up. Furthermore, in multiple cases, Domino achieves near-optimal or even exceeds the optimal settings. The optimal solution refers to disabling all the communication in TP training of both forward and backward passes.\nOne tricky part is whether we should enable CudaGraph or not. Based on our experimental results, we found that if the batch size is small (i.e., training job is not compute-heavy), enabling CudaGraph could squeeze the bubble/idle time between adjacent kernels thus improving end-to-end performance. On the other hand, if the training job is compute-heavy and does not have much idle time between adjacent kernels, we disable CudaGrpah for faster model training initialization and less on-device memory copy overhead. Taking GPT-3 13B training as an example, with sequence length of 512 and micro-batch size of 4, we notice significant training iteration time reduction (around 10-15%) if we switch from cudaGraph off mode to on mode, which shows the benefits of reducing idle time between adjacent compute kernels. On the other hand, if we increase the micro-batch size to 16, enabling cudaGraph leads to 5-10% longer iteration time than disabling it, which is mainly due to extra memory copy overhead introduced in CudaGraph.\nOverall, as shown in Figure 9, enabling Megatron's coarse computation and communication overlapping (i.e., Megatron-LM (async)) achieves around 2-5% throughput gain compared with vanilla megatron baseline. Compared with the Megatron-LM baseline, Domino achieves higher speedup gains when training batch is large and relatively low performance gains for small batch size cases.\nFor GPT-3 2.7B training shown in Figure 9a, Domino achieves 1.14x to 1.26x speedup over Megatron baseline for both sequence lengths of 512 and 1k. In GPT-3 6.7B training as Figure 9b, since we increase model size from 2.7B to 6.7B, the largest micro-batch sizes are reduced compared with 2.7B cases. However, we achieve the highest throughput gain in 6.7B model compared with 2.7B and 13B cases. More specifically, in Figure 9b, for both sequence lengths of 512 and 1k, we achieve from 1.15x to 1.3x speedup over Megatron baseline with increasing micro batch sizes. For 13B cases in Figure 9c, we have the smallest micro-batch sizes for training, which leads to 12% to 23% throughput speedup over the Megatron baseline with increased batch sizes. In summary, Domimo generally outperforms Megatron baseline in varied batch sizes and sequence lengths. Our performance gain increases as the batch size grows.\nWe also depict comparison of Domino performance with optimal settings with different sequence lengths and batch sizes on a single node. As shown in Figure 10, the horizontal benchmark index numbers strictly follow the same order of training settings in Figure 9. Compared with the optimal setting that removes all communications in Megatron-LM, Domino reaches over 90% of optimal throughput in all cases and has a few cases even exceeding the optimal settings. We conduct an ablation study and performance gain breakdown. We find that, for cases where Domino exceeds the optimal setting, the extra performance gain is primarily attributed to our kernel-side optimization as discussed in \u00a7 4.3."}, {"title": "Multi-node", "content": "Compared with single node results, multi-node cases are different given cross-node IB bandwidth is still 2-3x lower compared with intra-node NVLink/NVSwitch. Therefore, it is still possible that a single NCCL collective can be longer than the maximum number of computation kernels that Domino can overlap with."}, {"title": "GPT-3", "content": "For 2 and 4 DGX-H100 nodes experiments, we test three different model sizes as 6.7B, 13B and 30B across 16 to 32 H100 GPUs with TP model partition strategy. As shown in Fig. 11, we report normalized throughput speed-up when comparing Domino with Megatron baseline. For both sequence lengths of 512 and 1k, we present our best throughput results with proper batch sizes ranging from 4 to 64. Coarse-grained computation and communication overlapping provided by Megatron-LM (i.e., Megatron-LM (async) in Fig. 11) gives around 2%-4%"}]}