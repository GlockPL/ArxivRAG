{"title": "Robust off-policy Reinforcement Learning via Soft Constrained Adversary", "authors": ["Kosuke Nakanishi", "Akihiro Kubo", "Yuji Yasuit", "Shin Ishii"], "abstract": "Recently, robust reinforcement learning (RL) methods against input observation have garnered significant attention and undergone rapid evolution due to RL's potential vulnerability. Although these advanced methods have achieved reasonable success, there have been two limitations when considering adversary in terms of long-term horizons. First, the mutual dependency between the policy and its corresponding optimal adversary limits the development of off-policy RL algorithms; although obtaining optimal adversary should depend on the current policy, this has restricted applications to off-policy RL. Second, these methods generally assume perturbations based only on the \\(L_p\\)-norm, even when prior knowledge of the perturbation distribution in the environment is available. We here introduce another perspective on adversarial RL: an f-divergence constrained problem with the prior knowledge distribution. From this, we derive two typical attacks and their corresponding robust learning frameworks. The evaluation of robustness is conducted and the results demonstrate that our proposed methods achieve excellent performance in sample-efficient off-policy RL.", "sections": [{"title": "1 Introduction", "content": "In recent years, advancements in computational technology, coupled with the practical successes of deep neural networks (DNNs) [Krizhevsky et al., 2012, Simonyan and Zisserman, 2014, He et al., 2016], have fueled expectations for automated decision-making and control in increasingly complex environments [Kober et al., 2013, Levine et al., 2016, Kiran et al., 2021]. Deep reinforcement learning (DRL) is a promising framework for such applications, demonstrating performance that surpasses human capabilities by acquiring high-dimensional representational power through function approximation [Mnih et al., 2015, Silver et al., 2017]. However, in real-world applications, significant performance degradation in control due to adverse perturbations raises practical concerns [Huang et al., 2017, Lin et al., 2017]. Therefore, the development and testing of algorithms that consider such challenges are crucial.\nRecent research [Zhang et al., 2021, Sun et al., 2021, Liang et al., 2022] identifies there are two types of vulnerabilities in DRL. The first is related to the smoothness of the policy function, which primarily arises from the function approximation properties of DNNs. The second vulnerability stems from the dynamics of the environment and is considered within the framework of Markov Decision Processes (MDPs). To understand the latter case, imagine a situation where you are going to cross over a deep valley and there are two bridges. The one is short length but a narrow bridge and the other is a large bridge but has a little bit longer path. If you have clear vision, you may prefer the former, but if you get noisy vision in foggy conditions, the latter path is the best choice to achieve your goals without the risk of falling. To realize such comprehensive decision makings, we need to consider long-term reward appropriately into adversaries and robustness for an RL problem, rather than the temporal DNN smoothness or consistency of output as in the supervised learning."}, {"title": "2 Related Work", "content": "Building on a seminal work by Goodfellow et al. [2014], there has been a surge of research activity in the field of supervised learning, particularly focusing on various adversarial attacks and corresponding defense methods [Kurakin et al., 2016, Papernot et al., 2016, 2017, Carlini and Wagner, 2017, Ilyas et al., 2018]. In the context of RL, Huang et al. [2017] demonstrated that similar challenges could arise from small perturbations, such as the Fast Gradient Sign Method (FGSM). This study led to the early proposal of various attacks on observations and corresponding robust methods [Kos and Song, 2017, Behzadan and Munir, 2017, Mandlekar et al., 2017, Pattanaik et al., 2018].\nRecently, a line of research has focused on maintaining the consistency (smoothness) of the agent's policy to acquire robustness against observation perturbations [Zhang et al., 2020, Shen et al., 2020, Oikarinen et al., 2021, Sun et al., 2024]. Zhang et al. [2020] first defined the adversarial observation problem as State Adversarial Markov Decision Processes (SA-MDPs). They proved and demonstrated that the loss in performance due to adversarial perturbations could be bounded by the consistency of the policy. However, in this study, they did not show practical methods to create the adversary that could estimate the long-term reward assumed in the SA-MDPs. Due to this gap, the proposed robust methods were not robust enough against stronger attacks [Zhang et al., 2021].\nTo address this issue, Zhang et al. [2021] proposed that the optimal (worst-case) adversary for the policy could be learned as a DRL agent. The policy, learned alternatively with such an adversary, can become robust against strong attacks (ATLA). Building on the ATLA framework, Sun et al. [2021] suggested dividing the adversary into two parts: searching for the mislead direction in the action space for the policy and calculating the actual perturbations in the state space through numerical approximation (PA-AD). This makes the adversary capable of handling high-dimensional state problems (such as Atari), which are difficult to address in the ATLA framework. These frameworks provide practical methods for on-policy algorithms (PPO, A2C), but applications for off-policy"}, {"title": "2.1 Adversarial Attack and Defense on State Observations", "content": "Building on a seminal work by Goodfellow et al. [2014], there has been a surge of research activity in the field of supervised learning, particularly focusing on various adversarial attacks and corresponding defense methods [Kurakin et al., 2016, Papernot et al., 2016, 2017, Carlini and Wagner, 2017, Ilyas et al., 2018]. In the context of RL, Huang et al. [2017] demonstrated that similar challenges could arise from small perturbations, such as the Fast Gradient Sign Method (FGSM). This study led to the early proposal of various attacks on observations and corresponding robust methods [Kos and Song, 2017, Behzadan and Munir, 2017, Mandlekar et al., 2017, Pattanaik et al., 2018].\nRecently, a line of research has focused on maintaining the consistency (smoothness) of the agent's policy to acquire robustness against observation perturbations [Zhang et al., 2020, Shen et al., 2020, Oikarinen et al., 2021, Sun et al., 2024]. Zhang et al. [2020] first defined the adversarial observation problem as State Adversarial Markov Decision Processes (SA-MDPs). They proved and demonstrated that the loss in performance due to adversarial perturbations could be bounded by the consistency of the policy. However, in this study, they did not show practical methods to create the adversary that could estimate the long-term reward assumed in the SA-MDPs. Due to this gap, the proposed robust methods were not robust enough against stronger attacks [Zhang et al., 2021].\nTo address this issue, Zhang et al. [2021] proposed that the optimal (worst-case) adversary for the policy could be learned as a DRL agent. The policy, learned alternatively with such an adversary, can become robust against strong attacks (ATLA). Building on the ATLA framework, Sun et al. [2021] suggested dividing the adversary into two parts: searching for the mislead direction in the action space for the policy and calculating the actual perturbations in the state space through numerical approximation (PA-AD). This makes the adversary capable of handling high-dimensional state problems (such as Atari), which are difficult to address in the ATLA framework. These frameworks provide practical methods for on-policy algorithms (PPO, A2C), but applications for off-policy"}, {"title": "3 Preliminaries and Background", "content": "Notations We describe the environment using a Markov Decision Process (MDP) characterized by parameters \\((\\mathcal{S}, \\mathcal{A}, \\mathcal{F}, r, \\gamma, \\mathcal{S}_0)\\), where \\(\\mathcal{S}\\) is the state space, \\(\\mathcal{A}\\) is the action space, \\(\\mathcal{F} : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\Delta(\\mathcal{S})\\) defines the environment's transition probabilities, where \\(\\Delta(\\mathcal{S})\\) denotes the set of probability distributions over the state space \\(\\mathcal{S}\\), \\(r\\) is the reward function, \\(\\gamma\\) is the discount factor, and \\(\\mathcal{S}_0\\) is the set of initial states. In RL framework, the objective is to find a policy \\(\\pi(a|s) : \\mathcal{S} \\rightarrow \\Delta(\\mathcal{A})\\) that maximizes the cumulative discounted reward along the trajectory, \\(\\max_{\\pi} E_{\\mathcal{S}_0, \\pi, \\mathcal{F}} [\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)]\\). To reduce the variance in episodic trajectory estimates, RL maintains an action-value function \\(Q^{\\pi}(s_t, a_t)\\) and/or a state-value function \\(V^{\\pi}(s_t)\\)."}, {"title": "3.1 Max-Entropy Off-Policy Actor Critic Algorithm", "content": "In this study, we utilize the Soft Actor Critic (SAC) [Haarnoja et al., 2018a,b] as our base algorithm. SAC is one of the most popular off-policy RL methods due to its theoretically sound foundations, sample-efficient, excellent performance, and simplicity. SAC assume a modified reward function:\n\\[\\tilde{r}(s_t, a_t) = r(s_t, a_t) + E_{s_{t+1} \\sim F}[\\alpha_{ent} \\mathcal{H}(\\pi(\\cdot | s_{t+1}))],\\]\nwhere \\(\\mathcal{H}(\\pi(\\cdot | s_{t+1}))\\) represents the entropy term of the policy \\(\\pi\\) for the state at time-step \\(t + 1\\), and \\(\\alpha_{ent}\\) is an entropy coefficient to balance obtaining the original reward and encouraging exploration of actions.\nThen, policy evaluation and improvement for \\(\\pi\\) and \\(Q^{\\pi}\\) are done as:\n\\[Q^{\\pi}(s_t, a_t) = r(s_t, a_t) + E_{s_{t+1} \\sim F} [E_{a_{t+1} \\sim \\pi} [Q^{\\pi}(s_{t+1}, a_{t+1}) - \\alpha_{ent} \\log \\pi(a_{t+1}|s_{t+1})]],\\]\n\\[\\mathcal{L}(\\pi) = E_{s_t \\sim \\mathcal{D}(\\cdot)} [D_{KL}(\\pi(\\cdot | s_t) || \\frac{\\exp(Q^{\\pi}(s_t, \\cdot) / \\alpha_{ent})}{\\int_a \\exp(Q^{\\pi}(s_t, a) / \\alpha_{ent}) da} )],\\]\nwhere \\(D_{KL}\\) denotes Kullback-Leibler Divergence, and \\(\\mathcal{D}(\\cdot)\\) represents batch data from the replay buffer. By ignoring the constant term, we derive the final loss for the policy:\n\\[\\mathcal{L}(\\pi) = E_{s_t \\sim \\mathcal{D}(\\cdot)} [E_{a_t \\sim \\pi} [\\alpha_{ent} \\log \\pi(a_t|s_t) - Q^{\\pi}(s_t, a_t)]].\\]"}, {"title": "3.2 Reinforcement Learning under Adversarial Attack on State Observation", "content": "In scenarios with noisy observations, we consider adversarial perturbations \\(\\nu(\\tilde{s}_t | s_t) \\in \\mathcal{N}\\), where \\(\\mathcal{N} : \\mathcal{S} \\rightarrow \\Delta(\\mathcal{S})\\) represents all possible perturbation functions mapping true state \\(s_t\\) to the set of probability over the state space \\(\\mathcal{S}\\). The perturbation occurs at each time step \\(t\\), misleading the agent's policy to output action \\(\\tilde{a}_t\\), while the environment dynamics transition to state \\(s_{t+1}\\) based on \\(s_t\\) and \\(\\tilde{a}_t\\). Crucially, only the policy is deceived by the perturbation \\(\\nu(\\tilde{s}_t | s_t)\\), altering its action choice from \\(a_t \\sim \\pi(\\cdot|s_t)\\) to \\(\\tilde{a}_t \\sim \\pi(\\cdot|\\tilde{s}_t)\\). Previous research [Pattanaik et al., 2018, Zhang et al., 2020, 2021, Sun et al., 2021, Oikarinen et al., 2021, Liang et al., 2022] limits adversarial strength using an \\(L_p\\)-norm constraint (typically \\(p = \\infty\\)), defining a restricted perturbation subset \\(B_{\\epsilon p}(\\nu; s_t) \\subset \\mathcal{N}\\).\n\\[B_{\\epsilon p}(\\nu; s_t) := {\\nu_{\\epsilon p} \\in \\mathcal{N} : \\forall_{\\epsilon p} (s_t | s_t) = 0 \\text{ if } |\\tilde{s}_t - s_t|_p > \\epsilon, \\text{ other } \\nu_{\\epsilon p} (s_t | s_t) > 0}.\\]"}, {"title": "4 Methodology", "content": "To accommodate more flexible perturbation scenarios, we revisit Eq. (6), assuming prior knowledge of the perturbation distribution \\(p(\\tilde{s}_t | s_t) \\in \\mathcal{N}\\) in our target environment. We add a mild assumption that the adversarial attacker \\(\\nu(\\tilde{s}_t | s_t)\\) aligns with the prior distribution \\(^1\\), \\(\\forall s_t, \\tilde{s}_t \\in \\mathcal{S}\\), if \\(\\nu(\\tilde{s}_t | s_t) > 0 \\Rightarrow p(\\tilde{s}_t | s_t) > 0\\). This framework allows us to define the soft constrained optimal adversary:\nDefinition 1 (Soft Constrained Optimal Adversary on State Observation).\n\\[\\nu^{\\ast}(\\tilde{s}_t | s_t) = \\arg \\min_{\\nu \\in \\mathcal{N}} E_{s_t \\sim p} [E_{\\tilde{s}_t \\sim \\nu(\\cdot | s_t)} [E_{\\tilde{a}_t \\sim \\pi(\\cdot | \\tilde{s}_t)} [Q^{\\pi}(s_t, \\tilde{a}_t)]] + \\alpha_{attk} D_f(\\nu(\\cdot | s_t) || p(\\cdot | s_t))].\\]\nHere, \\(D_f(\\nu || p)\\) represents the general f-divergence between the perturbation distribution \\(\\nu(\\tilde{s}_t | s_t)\\) and the prior distribution \\(p(\\tilde{s}_t | s_t)\\). \\(\\alpha_{attk}\\) is the coefficient term used to balance the worst action value and the constraints on the attacker distribution imposed by the prior knowledge distribution \\(p(\\tilde{s}_t | s_t)\\).\nAs discussed in Sun et al. [2021], using \\(Q(s_t, \\tilde{a}_t)\\) and \\(Q^{\\pi}(s_t, \\tilde{a}_t)\\) in Eq. (7) differ from a strict perspective. \\(Q(s_t, \\tilde{a}_t)\\) can estimate the sequential effect of perturbation \\(\\nu\\), while \\(Q^{\\pi}(s_t, \\tilde{a}_t)\\) only estimates the one-step influence from \\(\\nu\\). Then, using \\(Q^{\\pi}(s_t, \\tilde{a}_t)\\) results in a stronger attacker by correctly estimating the long-term effects. However, from the attacker's perspective, it is difficult to specify whether DRL methods account for robust frameworks or not. Therefore, we widely use \\(Q^{\\pi}(s_t, a_t)\\), even though it does not account for perturbations, for practical applicability.\nAlthough various attackers can be characterized by specifying different f-divergences, in this study, we specifically propose two typical attacks, each detailed in subsequent subsections."}, {"title": "4.1 Soft Constrained Representation of Adversarial Attack on State Observation", "content": "To accommodate more flexible perturbation scenarios, we revisit Eq. (6), assuming prior knowledge of the perturbation distribution \\(p(\\tilde{s}_t | s_t) \\in \\mathcal{N}\\) in our target environment. We add a mild assumption that the adversarial attacker \\(\\nu(\\tilde{s}_t | s_t)\\) aligns with the prior distribution \\(^1\\), \\(\\forall s_t, \\tilde{s}_t \\in \\mathcal{S}\\), if \\(\\nu(\\tilde{s}_t | s_t) > 0 \\Rightarrow p(\\tilde{s}_t | s_t) > 0\\). This framework allows us to define the soft constrained optimal adversary:\nDefinition 1 (Soft Constrained Optimal Adversary on State Observation).\n\\[\\nu^{\\ast}(\\tilde{s}_t | s_t) = \\arg \\min_{\\nu \\in \\mathcal{N}} E_{s_t \\sim p} [E_{\\tilde{s}_t \\sim \\nu(\\cdot | s_t)} [E_{\\tilde{a}_t \\sim \\pi(\\cdot | \\tilde{s}_t)} [Q^{\\pi}(s_t, \\tilde{a}_t)]] + \\alpha_{attk} D_f(\\nu(\\cdot | s_t) || p(\\cdot | s_t))].\\]\nHere, \\(D_f(\\nu || p)\\) represents the general f-divergence between the perturbation distribution \\(\\nu(\\tilde{s}_t | s_t)\\) and the prior distribution \\(p(\\tilde{s}_t | s_t)\\). \\(\\alpha_{attk}\\) is the coefficient term used to balance the worst action value and the constraints on the attacker distribution imposed by the prior knowledge distribution \\(p(\\tilde{s}_t | s_t)\\).\nAs discussed in Sun et al. [2021], using \\(Q(s_t, \\tilde{a}_t)\\) and \\(Q^{\\pi}(s_t, \\tilde{a}_t)\\) in Eq. (7) differ from a strict perspective. \\(Q(s_t, \\tilde{a}_t)\\) can estimate the sequential effect of perturbation \\(\\nu\\), while \\(Q^{\\pi}(s_t, \\tilde{a}_t)\\) only estimates the one-step influence from \\(\\nu\\). Then, using \\(Q^{\\pi}(s_t, \\tilde{a}_t)\\) results in a stronger attacker by correctly estimating the long-term effects. However, from the attacker's perspective, it is difficult to specify whether DRL methods account for robust frameworks or not. Therefore, we widely use \\(Q^{\\pi}(s_t, a_t)\\), even though it does not account for perturbations, for practical applicability.\nAlthough various attackers can be characterized by specifying different f-divergences, in this study, we specifically propose two typical attacks, each detailed in subsequent subsections."}, {"title": "4.1.1 Soft Worst Attack (SofA) Sampling Method for the KL-divergence Constraint", "content": "When we set the f-divergence to KL-divergence, the optimal attacker for a fixed policy \\(\\pi\\) and the corresponding action-value function \\(Q^{\\pi}\\) can be derived by the Fenchel-Legendre transform (detailed in Appendix B.1) as follows:\n\\[\\nu_{soft}^{\\ast}(\\tilde{s}_t | s_t) = \\frac{p(\\tilde{s}_t|s_t) \\exp (E_{\\tilde{a}_t \\sim \\pi(\\cdot | \\tilde{s}_t)} [-Q^{\\pi}(s_t,\\tilde{a}_t) / \\alpha_{attk}])}{\\int_{\\tilde{s}} p(\\tilde{s} | s_t) \\exp (E_{\\tilde{a}_t \\sim \\pi(\\cdot | \\tilde{s}_t)} [-Q^{\\pi}(s_t,\\tilde{a}_t) / \\alpha_{attk}]) d \\tilde{s}}.\\]\nWhen dealing with continuous state and action spaces, direct access to this distribution is not possible. We can approximate this distribution using Markov Chain Monte Carlo (MCMC) or variational inference method; however, these methods require multiple accesses to the policy \\(\\pi\\) and the action-value function \\(Q^{\\pi}\\) to obtain even a single sample at each time-step \\(t\\). To address this, we propose approximating a limited number (\\(N\\)) of samples using the prior knowledge distribution \\(p(\\tilde{s}_t | s_t)\\), and then adjusting the probability with importance weights:\n\\[\\begin{aligned}\n& \\tilde{s}_t^i \\sim p(\\tilde{s} | s_t), i = 1, 2, ..., N, \\\\\n& \\nu_{soft}^{\\ast}(\\tilde{s}_t^i | s_t) \\propto \\frac{1}{N} p(\\tilde{s}_t^i | s_t) \\exp(E_{\\tilde{a}_t^i \\sim \\pi(\\cdot | \\tilde{s}_t^i)} [-Q^{\\pi}(s_t, \\tilde{a}_t^i) / \\alpha_{attk}]) \\bigg/ \\sum_{i} p(\\tilde{s}_t^i | s_t) \\exp(E_{\\tilde{a}_t^i \\sim \\pi(\\cdot | \\tilde{s}_t^i)} [-Q^{\\pi}(s_t, \\tilde{a}_t^i) / \\alpha_{attk}]) .\n\\end{aligned}\\]"}, {"title": "4.1.2 Epsilon Worst Approximation Attack (EpsA) for the a-divergence Constraint", "content": "Considering the case when the f-divergence in Eq.(7) is specified as the a-divergence, we can explore broader categories that encompass the SofA case (Section4.1.1). These a-divergence constraint problems tend to have more mode-seeking solutions as \\(\\alpha\\) decreases [Belousov and Peters, 2017, Xu et al., 2023]. Specifically, by setting \\(\\alpha < 0\\) and restricting the attacker's distribution selection to a uniform distribution within the \\(L_{\\infty}\\)-norm range with an attack scale \\(\\epsilon\\), denoted as \\(U(\\tilde{s} | s - \\epsilon, s + \\epsilon)\\), we can approximate the distribution by using the mode probability, denoted as \\(\\kappa_{worst}\\), as follows:\n\\[\\nu^{\\ast}(\\tilde{s} | s) \\sim\n\\begin{cases}\n\\kappa_{worst} + \\frac{1 - \\kappa_{worst}}{S_{\\epsilon}} & \\text{if } \\tilde{s} = \\arg \\min_{s' \\in B_{\\epsilon}} E_{\\tilde{a} \\sim \\pi(\\cdot | s')} [Q^{\\pi}(s, \\tilde{a})], \\\\\n\\frac{1 - \\kappa_{worst}}{S_{\\epsilon}} & \\text{otherwise},\n\\end{cases}\\]\nwhere \\(S_{\\epsilon}\\) represents the measure of the state space within the \\(\\epsilon\\)-bounded domain. The details of this approximation are discussed in Appendix C.1. Eq. (10) can be approximated by combining the uniform distribution with a numerical gradient approach, similar to the Critic attack [Pattanaik et al., 2018, Zhang et al., 2020]. We refer to this strategy as the Epsilon (worst) Attack (EpsA) and utilize it for training and evaluation in Sections 4.1.2 and 5.1.2."}, {"title": "4.2 Robust off-policy Reinforcement Learning via Soft Constrained Adversary", "content": "As the previous discussion, we introduced the two typical adversaries as the solution of the soft constrained dual problems. By defining the soft (epsilon) worst-attack action-value function for policy \\(\\pi\\) as \\(Q^{\\pi}(\\tilde{s}, a) := E_{\\mathcal{F}, \\tilde{a}_t \\sim \\pi(\\cdot | \\tilde{s})} [\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, \\tilde{a}_t) | s_0 = s, a_0 = a]\\), we can propose two robust off-policy RL algorithms which assume the corresponding adversaries in the appropriate MDP manners. We should note that this framework is sample-efficient because it can work not only with off-policy algorithms but also does not require additional interaction with the environment for the adversary."}, {"title": "4.2.1 Soft Worst Max-Entropy Reinforcement Learning (SofA-SAC)", "content": "We assume the policy is degraded by the adversary, and therefore introduce an additional modification to the reward function in Eq. (1) as follows:\n\\[\\tilde{r}(s_t, a_t) = r(s_t, a_t) + E_{s_{t+1} \\sim F}[\\alpha_{ent} \\mathcal{H}(\\pi(\\cdot | \\nu^{\\ast}_{soft}(\\cdot | s_{t+1})))].\\]\nFor simplicity, we define \\(V^{\\pi}(s, \\tilde{s}) := E_{\\tilde{a} \\sim \\pi} [Q(s, \\tilde{a}) - \\alpha_{ent} \\log \\pi(\\tilde{a} | \\tilde{s})]\\), then we get the following soft worst attack for the max-entropy version:\n\\[\\nu^{\\ast}_{soft}(\\tilde{s}_t | s_t) = \\frac{p(\\tilde{s}_t | s_t) \\exp(-V^{\\pi}(s_t, \\tilde{s}_t) / \\alpha_{attk})}{\\int_{\\tilde{s}_t} p(\\tilde{s} | s_t) \\exp(-V^{\\pi}(s_t, \\tilde{s}_t) / \\alpha_{attk}) d \\tilde{s}}.\\]\nUnder such adversary, we can define the corresponding Bellman operator with a contraction property:\n\\[(T_{soft}Q(s_t, a_t) = r(s_t, a_t) = r(s_t, a_t) + E_{s_{t+1} \\sim F} [-\\alpha_{attk} \\log (E_{\\tilde{s}_{t+1} \\sim p} [\\exp(-V^{\\pi}(s_{t+1}, \\tilde{s}_{t+1}) / \\alpha_{attk})])].\\]"}, {"title": "4.2.2 Epsilon Worst Max-Entropy Reinforcement Learning (EpsA-SAC)", "content": "As in the previous subsection, we assume the policy is mislead by the attack, then we consider the max-entropy version of the epsilon worst attack in Eq. (10) as:\n\\[\\nu^{\\ast}_{epsilon} (\\tilde{s}_t | s_t) \\sim\n\\begin{cases}\n\\frac{\\kappa_{worst} + \\frac{1 - \\kappa_{worst}}{S_{\\epsilon}}}{\\alpha_{ent}} & \\text{if } s = \\arg \\min_{s' \\in B_{\\epsilon}} E_{\\tilde{a} \\sim \\pi} [Q^{\\pi}(s, \\tilde{a}) - \\alpha_{ent} \\log \\pi(\\tilde{a} | s')], \\\\\n\\frac{\\frac{1 - \\kappa_{worst}}{S_{\\epsilon}}}{\\alpha_{ent}} & \\text{otherwise}\n\\end{cases}\\]\nUnder this perturbation, we define the epsilon-worst Bellman operator as:\n\\[(T_{epsilon}Q)(s_t, a_t) = r(s_t, a_t) + E_{s_{t+1} \\sim F} [E_{\\tilde{s}_{t+1} \\sim \\epsilon} [V^{\\pi}(s_{t+1}, \\tilde{s}_{t+1})]]\\]\n\\[=r + E_{s_{t+1} \\sim F} [\\kappa_{worst} E_{s^{\\ast}_{epsilon}}[V^{\\pi} (s_{t+1}, \\tilde{s}_{t+1})] + (1 - \\kappa_{worst}) E_{s_{t+1} U_{\\epsilon}}[V^{\\pi} (s_{t+1}, \\tilde{s}_{t+1})]].\\]\nFor simplicity, we again use \\(V^{\\pi} (s, \\tilde{s}) := E_{\\tilde{a} \\sim \\pi} [Q(s, \\tilde{a}) - \\alpha_{ent} \\log \\pi(\\tilde{a} | \\tilde{s})]\\). By considering the same divergence minimization problem in Eq. (14), we can improve the policy through:\n\\[\\mathcal{L}(\\pi) = E_{s_t \\sim \\mathcal{D}} [E_{\\tilde{s}_{epsilon}} [E_{\\tilde{a} \\sim \\pi} [\\alpha_{ent} \\log (\\tilde{a} | s_t) - Q^{\\pi}(s_t, \\tilde{a})]]\\]\n\\[= E_{s_t \\sim \\mathcal{D}} [\\kappa_{worst} E_{s^{\\ast}_{epsilon}} [-V^{\\pi}(s_t, \\tilde{s}_t)] + (1 - \\kappa_{worst}) E_{s U_{\\epsilon}} [-V^{\\pi}(s_t, \\tilde{s}_t)]].\\]\nWe refer to this RL framework as the Epsilon (worst) Attack SAC (EpsA-SAC). We can assert that this Bellman operator also possesses a contraction property under a fixed policy, and once the adversary is fixed, the policy can be improved monotonically. For detailed information, please see Appendix C.3. In Eq. (15), obtaining the analytical worst sample (\\(\\tilde{s} \\sim \\nu^{\\ast}_{worst} (\\cdot | s)\\)) is infeasible when the environment comprises continuous states and actions. However, we have found that numerical approximations of the worst samples using the Critic attack [Pattanaik et al., 2018, Zhang et al., 2020] are practically effective. We employ the policy mean \\(\\mu(\\tilde{s})\\) and the action-value \\(Q^{\\pi} (s_t, \\tilde{a}_t)\\), then apply gradient descent iteration to solve \\(\\tilde{s} \\sim \\arg \\min_{\\tilde{s} \\in B_{\\epsilon}} Q^{\\pi} (s_t, \\mu(\\tilde{s}))\\) in subsequent experiments."}, {"title": "5 Experiments", "content": "In this section, we set up experiments to address the questions posed in the introduction: (1) Can we develop a robust off-policy algorithm that accounts for long-term rewards without requiring additional interactions? (2) Is it possible to incorporate more arbitrary distributions, based on prior knowledge, beyond the conventional \\(L_p\\)-norm constrained range, into both adversaries and defenses? Responses to (1) are addressed in Sections 5.1.1 and 5.1.2, and those to (2) are presented in Section 5.1.1."}, {"title": "5.1 Training and Evaluation", "content": "We have established two evaluation metrics for our analysis. The first metric aims to assess the effectiveness against adversaries not constrained by the \\(L_p\\)-norm, as discussed in Sections 4.1.1 and 4.2.1. The second metric evaluates the resilience of our methods against conventional strong attacks within the \\(L_{\\infty}\\)-norm ball, detailed in Sections 4.1.2 and 4.2.2."}, {"title": "5.1.1 Evaluations of Soft Worst Case Scenarios under Gaussian-Based Attacks", "content": "Task Setup In this study, we use a Gaussian distribution as the prior knowledge perturbation \\(p(\\tilde{s}_t | s_t)\\), setting the standard deviations for the attack scales as \\(\\sigma = 0.15, 0.30, 0.15, 0.15\\) for HalfCheetah, Hopper, Walker2d, and Ant, respectively. We train the standard SAC (Vanilla-SAC), SA-SAC, and our proposed defense method SofA-SAC using identical training steps. Subsequently, we conduct evaluations under various attack settings.\nWe prepare four attack settings: one using the prior knowledge distribution(Gaussian) as-is, another applying our proposed method, denoted as SofA(\\(\\alpha_{attk}\\)), with varying degrees of adversarial preference parameter \\(\\alpha_{attk}\\). Finally, for reference, we report results for the MaxActionDiff(MAD) [Zhang et al., 2020] and the Critic [Pattanaik et al., 2018, Zhang et al., 2020], where the standard deviation value serves as the noise constraint range in Appendix D. For the sampling approximation, we use \\(N = 64\\) both for the evaluation attacker (SofA) and the proposal DRL methods (SotfA-SAC). For SA-SAC and SofA-SAC training, we appropriately tune the coefficient terms that achieve robustness"}, {"title": "5.1.2 Evaluations of Strong Attackers under Conventional L\u221e-norm Constraints", "content": "Task Setup For the robustness evaluation, we incorporate strong attackers as used in the most recent studies [Zhang et al., 2020, 2021, Sun et al., 2021, Oikarinen et al., 2021, Liang et al., 2022", "2021": "and PA-AD [Sun et al., 2021"}]}