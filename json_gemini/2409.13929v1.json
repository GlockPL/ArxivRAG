{"title": "Failures in Perspective-taking of Multimodal AI Systems", "authors": ["Bridget Leonard", "Kristin Woodard", "Scott O. Murray"], "abstract": "This study extends previous research on spatial representations in multimodal AI systems. Although current models demonstrate a rich understanding of spatial information from images, this information is rooted in propositional representations, which differ from the analog representations employed in human and animal spatial cognition. To further explore these limitations, we apply techniques from cognitive and developmental science to assess the perspective-taking abilities of GPT-40. Our analysis enables a comparison between the cognitive development of the human brain and that of multimodal AI, offering guidance for future research and model development.", "sections": [{"title": "1 Introduction", "content": "Visual perspective-taking, or the ability to mentally simulate a viewpoint other than one's own, is a critical aspect of spatial cognition. It allows us to understand the relationship between objects and how we might have to manipulate a scene to align with our perspective, which is essential for tasks like navigation and social interaction. Although past research has examined AI spatial cognition, it lacks the specificity found in human spatial cognition studies where processes are broken down into sub-components for more precise measurement and interpretation. In cognitive psychology, established tasks are carefully controlled to isolate specific variables, reducing bias and alternative strategies for task performance. By applying these established methods, we can evaluate AI spatial cognition more rigorously, beginning with perspective-taking. The rich human literature on these spatial skills provides a valuable benchmark, allowing us to compare model performance against the human developmental timeline and identify key areas for future research and model improvement."}, {"title": "1.1 Background of Perspective Taking", "content": "Perspective-taking is a cornerstone of human spatial reasoning. For multimodal models to function as effective cognitive systems and daily assistants, they must develop robust perspective-taking abilities. In the human developmental literature, perspective-taking has been stratified into two levels. Level 1 refers to knowing that a person may be able to see something another person does not, and it appears fully developed by the age of two [7]. A common Level 1 task might ask if an object is viewable (or positioned to the front or back) of a person or avatar in a scene. Level 2 refers to the ability to represent how a scene would look from a different perspective, often measured by having subjects assess the spatial relationship between objects. Although success on some simple Level 2 tasks is first seen around age 4 [8], Level 2 perspective-taking continues to develop into middle childhood [10] and even into young adulthood [1]."}, {"title": "1.2 Limitations of Spatial Assessment in Current Multimodal AI", "content": "Two primary limitations appear within AI spatial cognition literature: 1) linguistic reasoning can inflate performance on spatial benchmarks, and 2) benchmark scores can be hard to interpret when models perform poorly. For example, text-only GPT-4 achieves a score of 31.4, while multimodal GPT-4v achieves a score of 42.6 on the spatial understanding category of Meta's openEQA episodic memory task [6]. The strong baseline score achieved by the text-only GPT-4 suggests that many \"real-world\" questions based on visual scenes can be deduced linguistically. Additionally, the limited improvement when moving from a blind LLM to a multimodal one suggests that vision models do not gain a significant understanding of space beyond what can be inferred through language.\nAdditionally, BLINK [4], a benchmark more specifically focused on visual perception capabilities, contains categories related to spatial cognition, such as relative depth and multi-view reasoning. On this benchmark, GPT-4v achieved an accuracy of 51.26%, only 13.17% higher than random guessing and 44.44% lower than human performance. When benchmarks are highly focused on visuospatial tasks, the significant shortcomings of multimodal models suggest that further advancements are needed before these models can reliably perform in real-world scenarios. Even within specific categories, it is often difficult to determine why models fail on certain tasks while succeeding on others, as these failures cannot be easily linked to the absence of a particular cognitive process.\nHere we apply established tasks in cognitive psychology that measure spatial cognition in a precise manner. By applying these tasks to AI systems, we gain not only improved measurement precision but also the ability to compare AI performance with human development, providing clear insights into model limitations and areas for improvement."}, {"title": "1.3 Perspective Taking Benchmark", "content": "Leveraging the distinction between Level 1 and Level 2 perspective-taking [12], we propose a small perspective-taking benchmark that assesses multimodal model capabilities across three tasks: Level 1, Level 2 with spatial judgments, and Level 2 with visual judgments. Although human performance remains stable regardless of judgment type, we include this differentiation of Level 2 stimuli to examine potential egocentric biases that may arise in multimodal models when interpreting spatial relations compared to optical character recognition (OCR). This benchmark aims to address gaps in current AI spatial cognition measures by increasing process specificity, limiting language-based solutions, and offering straightforward comparisons to human cognition."}, {"title": "2 Methods", "content": "Our study utilized GPT-40 (\u201cgpt-4o-2024-05-13\" via OpenAI's API) to conduct a series of perspective- taking experiments designed to capture the system's spatial reasoning abilities. The top_p parameter was set to 0.5 to restrict the model from choosing from the top 50% of words that could come next in its response.\nOur experimental design was inspired by previous studies that evaluated viewpoint dependence using targets like toy photographers [2] and avatars with blocks [12]. In our study, we used an avatar as a target and different stimuli, either cubes with numbers and letters or cubes and spheres, to investigate the influence of visual and spatial judgments on model performance. Each task consisted of 16 trial types, featuring images at 8 different angles (0\u00b0, 45\u00b0, 90\u00b0, 135\u00b0, 180\u00b0, 225\u00b0, 270\u00b0, 315\u00b0) with 2 response options for each task (e.g., cube in front or behind, 6/9 or M/W on the cube, and cube left or right). Examples of the stimuli are shown in Figure 1.\nTen iterations of each image were passed through the model to calculate the percentage of correct responses. The images were accompanied by the prompt below which varied slightly by task:\n\"For the following images respond with (in front | left | 6 | M) or (behind | right | 9 | W) to indicate if the (number/letter on the) cube is (in front | to the left | a 6 | an M) or (behind | to the right|a9|a W) from the perspective of the person.\"\nChain of Thought Prompting To further examine how language might be used to solve spatial tasks, we included chain-of-thought prompting to the Level 2 spatial task with the prompt:\n\"Analyze this image step by step to determine if the cube is to the person's left or right, from the person's perspective. First, identify the direction the person is looking relative to the camera. Second, determine if the cube is to the left or right, relative to the camera. Third, if the person is facing the camera, then from their perspective, the cube is to the inverse of the camera's left or right. If the person is facing away from the camera, then the cube is on the same side as seen from the camera. Respond with whether the cube is to the person's left or right.\""}, {"title": "3 Results", "content": "Level 1 GPT-40 performed with near-perfect accuracy on 6 out of the 8 image angles (Figure 2). Its poor performance on 0\u00b0 images is likely due to an accidental viewpoint where the avatar blocked one of the shapes. However, poor performance on 315\u00b0 image types is less interpretable, especially in contrast to GPT-40's impressive performance on 45\u00b0 images, which have the same angular perspective.\nLevel 2 Spatial & Visual Judgments As previously mentioned, human response times increase on perspective-taking tasks as the angular difference between the target and observer increases [12]. We administered the task to a small number of human participants part of a larger, IRB-approved study and replicated this effect with both our stimuli types, finding a bell-shaped curve in the relationship between response time and angle. Response times peaked when the target required a full mental rotation (180\u00b0), as seen in the green line in Figure 3. Error bars were calculated by taking the standard error of all trials from each participant. As expected, GPT-40 struggled with the task when mental rotation was involved, beginning around a 90\u00b0 angular difference. Interestingly, in both tasks, GPT-40 exhibited a response bias toward either \"left\" or \"6\" or \"W\" when the angular difference of the avatar is 90\u00b0 or 135\u00b0 in either direction. This likely reflects uncertainty from an egocentric perspective, and thus, a default to one response over another.\nChain of Thought GPT-40 performance significantly improved with chain-of-thought prompting on 180\u00b0 stimuli (Figure 4). However, this linguistic strategy did not improve the model's ability to handle intermediate rotations between 90\u00b0 and 180\u00b0. This suggests that while language can convey some level of spatial information, it lacks the precision required for human-level spatial cognition. This demonstration of surface-level perspective-taking abilities can partially explain how multimodal models achieve high performance on certain spatial benchmarks."}, {"title": "4 Discussion", "content": "This study highlights the value of applying cognitive science techniques to explore AI capabilities in spatial cognition. Specifically, we investigated GPT-40's perspective-taking abilities, finding it fails when there is a large difference between image-based and avatar-based perspectives. We developed a targeted set of three tasks to assess multimodal model performance on Level 1 and Level 2 perspective-taking, with spatial and visual judgments. While GPT-40 can do Level 1, an ability that aligns with the spatial reasoning abilities of a human infant or toddler, it fails on Level 2 tasks when mental rotation is required (i.e., the avatar's perspective is not aligned with image perspective). We further investigated if chain-of-thought prompting could elicit more spatial reasoning through language. Although this technique enabled GPT-40 to succeed on 180\u00b0 tasks, it continued to fail at intermediate angles, underscoring its limitations in performing true mental rotation.\nWhile GPT-40's performance decreases on tasks that humans typically solve using mental rotation, this does not necessarily indicate that GPT-40 struggles with or cannot perform mental rotation. Instead, it suggests that GPT-40 likely employs a fundamentally different strategy to approach these tasks. Rather than engaging in mental rotation, GPT-40 appears to rely primarily on image-based information processing. We found more support for this when testing an open prompt for Level 2 visual images that did not specify which letters or numbers to respond with. GPT-40 often responded with \"E\" and \"0\" for images around a 90\u00b0 angular difference, where from the image view, an M/W would look like an E, and a 9/6 would look like a 0.\nOne might suggest that current multimodal models aren't trained on the appropriate data to achieve the reasoning necessary for Level 2 perspective-taking. However, considering the developmental trajectory of humans, it becomes evident that this issue may not be solely data-related. Level 2 perspective-taking typically develops between the ages of 6 and 10 [2, 3], even after children have had exposure to extensive amounts of \"data\" through experience. This late development suggests that the challenge may be more computational than data-driven. Specifically, this ability likely relies on computations occurring outside of the visual and language networks, perhaps in areas responsible for cognitive processes like mental rotation or spatial transformation or even theory of mind [5, 9, 11, 12]. While the argument that better or more focused training data could improve model performance remains valid, it is also possible that entirely new computational strategies are needed to mirror the complex, integrative processes that enable Level 2 reasoning in humans.\nThis study demonstrates the potential of cognitive science methods to establish baselines for AI assessment. Using these well-established techniques, we achieve clear, interpretable measures that are less susceptible to bias. Additionally, these measures can be directly compared to human performance and developmental trajectories, providing a robust framework for understanding AI's strengths and weaknesses in relation to well-researched human cognitive processes."}, {"title": "A Appendix / supplemental material", "content": ""}, {"title": "A.1 Code and Data Availability", "content": "All code and data used in the paper are available at: (https://github.com/bridgetleonard2/\nPerspective Taking) including perspective-taking image stimuli, GPT API task code, and Matlab/PsychToolBox code to run human tasks.\nFor easy benchmark data loading, the perspective-taking stimuli are as a Hugging Face dataset at\n(https://huggingface.co/datasets/bridgetleonard/PerspectiveTaking)"}]}