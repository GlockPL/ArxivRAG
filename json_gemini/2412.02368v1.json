{"title": "SCIMAGE: HOW GOOD ARE MULTIMODAL LARGE LANGUAGE\nMODELS AT SCIENTIFIC TEXT-TO-IMAGE GENERATION?", "authors": ["Leixin Zhang", "Steffen Eger", "Yinjie Cheng", "Weihe Zhai", "Jonas Belouadi", "Christoph Leiter", "Simone Paolo Ponzetto", "Fahimeh Moafian", "Zhixue Zhao"], "abstract": "Multimodal large language models (LLMs) have demonstrated impressive capabilities in generating\nhigh-quality images from textual instructions. However, their performance in generating scientific\nimages-a critical application for accelerating scientific progress\u2014remains underexplored. In this\nwork, we address this gap by introducing ScImage, a benchmark designed to evaluate the multimodal\ncapabilities of LLMs in generating scientific images from textual descriptions. ScImage assesses\nthree key dimensions of understanding: spatial, numeric, and attribute comprehension, as well as\ntheir combinations, focusing on the relationships between scientific objects (e.g., squares, circles).\nWe evaluate five models, GPT-40, Llama, AutomaTikZ, Dall-E, and StableDiffusion, using two\nmodes of output generation: code-based outputs (Python, TikZ) and direct raster image generation.\nAdditionally, we examine four different input languages: English, German, Farsi, and Chinese. Our\nevaluation, conducted with 11 scientists across three criteria (correctness, relevance, and scientific\naccuracy), reveals that while GPT-40 produces outputs of decent quality for simpler prompts involving\nindividual dimensions such as spatial, numeric, or attribute understanding in isolation, all models\nface challenges in this task, especially for more complex prompts.", "sections": [{"title": "1 INTRODUCTION", "content": "Artificial intelligence (AI) has become an increasingly valuable tool in academic research, offering support across\nvarious aspects of the scientific process (Byun & Stuhlm\u00fcller, 2023; Chen & Eger, 2023; Lu et al., 2024a; Nechakhin\net al., 2024; Shao et al., 2024). For instance, platforms such as Elicit (Byun & Stuhlm\u00fcller, 2023)2 and ResearchRabbit3\nfacilitate finding relevant literature for specific research topics. Tools like Grammarly assist with grammatical refinement\nand phraseology in academic writing and LLM assisted text production is nowadays common (Liang et al., 2024).\nLLMs can also generate new ideas for scientific papers that rival the ideas produced by human scientists (Si et al.,\n2024). Even more holistically, approaches like The AI Scientist (Lu et al., 2024a) have demonstrated the capability to\ngenerate entire research output, encompassing everything from initial conceptualization to experimental design and\npaper drafting.\nDespite these advancements, a critical subproblem remains relatively unexplored: the AI-driven generation of scientific\nvisualizations, including illustrative figures, charts, and plots (Voigt et al., 2024). These visual elements play a pivotal\nrole in scientific communication (Lee et al., 2016), serving as essential tools for researchers, educators, and students\nto convey complex ideas, data, and concepts. The ability to automate the creation of accurate scientific images from\ntextual descriptions could significantly enhance both the efficiency and effectiveness of scientific communication\nand production. Compared to previous attempts at automating image generation, AI-driven generation of scientific"}, {"title": "2 RELATED WORK", "content": "In computer vision and multimodal studies, there are many benchmarks and datasets serving various purposes, including\nobject detection (Lin et al., 2014), image classification (Krizhevsky, 2009, Deng et al., 2009), hand-written digits\nrecognition (Deng, 2012), and image captioning (Sharma et al., 2018, Chen et al., 2015), but the majority focus on\nreal world images. Although datasets like Paper2Fig (Rodriguez et al., 2023) and DaTikZ (Belouadi et al., 2024a;b)\ninclude scientific figures and captions extracted from research papers, there is no structured evaluation of the limitations\nand capabilities of scientific text-to-image models. In Section 2.1 and 2.2, we review existing benchmarks designed to\nassess model abilities: visual understanding (e.g., using images as inputs (Thrush et al., 2022; Huang et al., 2023; Wu\net al., 2024), discussed in Section 2.1) and ability of text-to-image generation (Section 2.2). All surveyed datasets and\nbenchmarks in this study are summarized in Table 8 in Appendix A."}, {"title": "2.1 IMAGE AS INPUT", "content": "Benchmarks that use images as input often take the form of visual question answering (VQA), where images are\npaired with questions about their content (Biten et al., 2019; Das et al., 2024; Yue et al., 2023; Wang et al., 2024a).\nFor example, the Multimodal Visual Patterns (MMVP) Benchmark (Tong et al., 2024) focuses on challenging cases,\ncomprising 150 CLIP-blind pairs (images that the CLIP model perceives as similar despite clear visual distinctions)\nwith questions designed to probe specific image details, such as relative position, object counting, or other attributes.\nIn the scientific domain, VQA examples are typically sourced from exams, quizzes, or textbooks (Yue et al., 2023; Lu\net al., 2024b; Li et al., 2024a). Additionally, ScienceQA (Lu et al., 2022) is a benchmark that uses images as contextual\ninputs for questions, rather than directly asking about the image's content. This dataset also incorporates Chain of\nThought (CoT) reasoning to enhance interpretability alongside the answers. CharXiv (Wang et al., 2024b) evaluates\nmodels' abilities to describe and reason about charts through multiple-choice questions.\nAnother type of visual understanding benchmark focuses on caption-image alignment. Winoground (Thrush et al.,\n2022), for instance, challenges models to match images with their corresponding captions. The dataset includes pairs\nwhere objects or predicates are swapped, such as \u201cthere is a mug in some grass\u201d versus \u201cthere is some grass in a mug\",\nto test fine-grained comprehension of texts. MMSCI (Li et al., 2024b) extends this focus to the scientific domain,\noffering a figure-captioning benchmark spanning 72 subjects. Additionally, SciFIBench (Roberts et al., 2024) evaluates\nfigure-caption alignment through tasks such as selecting the appropriate figure for a given caption or choosing the\ncorrect caption for a specific figure from multiple choices.\""}, {"title": "2.2 IMAGE AS OUTPUT", "content": "Compared to visual understanding, benchmarks that assess individual dimensions of abilities in text-to-image generation\nmodels remain relatively scarce. One benchmark designed for this purpose is T2I-CompBench (Huang et al., 2023),\nwhich includes 6k compositional text prompts, categorized into three groups: attribute binding (e.g., color and shape),\nobject relationships (e.g., spatial arrangements), and complex compositions. While we are inspired by this benchmark,\nwe note that it does not target the scientific domain.\nIn the context of vector graph and scientific figure generation, Zou et al. (2024) develop an evaluation set to assess\nmodels' abilities in prompt comprehension and vector graph generation. Belouadi et al. (2024a) introduce a dataset\nthat pairs scientific paper captions (as input) with TikZ code (as output), which can be compiled into vector graphs.\nAdditionally, Shi et al. (2024) explore models' capabilities to replicate chart images by converting them into Python\ncode. Compared to these works, which focus on specific evaluation settings (such as TikZ or vector graphic or\nchart generation), our evaluation setup is broader, more targeted and more structured: we assess model performance\nacross different input languages and output formats (TikZ vs. Python vs. plain image), object types and aspects of\nunderstanding."}, {"title": "2.3 EVALUATION OF TEXT-TO-IMAGE MODELS", "content": "Existing evaluations of text-to-image models primarily focus on text-image alignment and image quality for real-world\nimages, as demonstrated by benchmarks such as MS COCO (Lin et al., 2014) and studies in Sharma et al. (2018)\nand Chen et al. (2015). Later works, such as Lee et al. (2024) and Cho et al. (2023), broaden the scope of evaluation\nto include aspects like aesthetics, originality, social bias, and efficiency, but these still remain within the domain of\nreal-world images."}, {"title": "3 SCIMAGE", "content": "he ScImage evaluates the capability of multimodal LLMs to generate scientific graphs from textual descriptions. We\ndesign prompts that require models to understand and visualize scientific concepts, emphasizing three key dimensions of\nunderstanding: (a) Spatial understanding: Assessing the models' ability to interpret and represent spatial relationships\nbetween objects, such as \u201cleft of\" and \u201con top of\". (b) Numeric understanding: Evaluating the models' capacity to\nhandle and visualize numerical requests accurately, such as the exact number of objects or requests like 'more' and\n'half'. (c) Attribute binding: Testing the models' ability to correctly represent object attributes such as color, size,\nand shape. Figure 2 demonstrates these three key dimensions of understanding."}, {"title": "3.1 TASK SETUP", "content": "he ScImage evaluates the capability of multimodal LLMs to generate scientific graphs from textual descriptions. We\ndesign prompts that require models to understand and visualize scientific concepts, emphasizing three key dimensions of\nunderstanding: (a) Spatial understanding: Assessing the models' ability to interpret and represent spatial relationships\nbetween objects, such as \u201cleft of\" and \u201con top of\". (b) Numeric understanding: Evaluating the models' capacity to\nhandle and visualize numerical requests accurately, such as the exact number of objects or requests like 'more' and\n'half'.7 (c) Attribute binding: Testing the models' ability to correctly represent object attributes such as color, size,\nand shape. Figure 2 demonstrates these three key dimensions of understanding.\nOutput mode The task involves generating images either (i) directly text-image or (ii) text-code-image through\nintermediate code (Python or TikZ) which then has to be compiled to images based on textual prompts.\nPrompting We instruct the models to generate scientific graphs with prompts. Each prompt consists of an auxiliary\ninstruction and a generation query. The auxiliary instruction is used to constrain the model to generate scientific graphs\nin either (i) direct text-image or (ii) text-code-image mode. Language models can exhibit sensitivity to variations in\nprompts (Leiter & Eger, 2024). To mitigate the impact of this variability and ensure a fair comparison between models,\nwe conduct pilot tests to find prompts that generally lead all tested models to generate required output type (i.e., Python"}, {"title": "3.2 DATASET CONSTRUCTION", "content": "We begin with a comprehensive survey of relevant scientific datasets and benchmarks, as detailed in Table 8 in Ap-\npendix A, also including math and science textbooks. This gave us the intuition that scientific graphs are described by\nobjects and their properties (attributes) as well as their relative positioning (spatial relations) and numeric information\n(e.g., how many objects). Additionally, annotations often emphasize parts of the scientific image.\nThus, we develop prompt requirements to ensure that varying aspects of scientific text generation are covered. We\nrequire that each prompt must explicitly define: (a) the core visual elements (objects) to be generated in the graph,\ne.g. cycle, square, etc.; (b) specific attributes of the object (attribute binding), e.g., red cycle, or count of the object\n(numeric), e.g. three cycles. the positioning arrangement and placement (spatial) of objects within the graph, e.g. on\nthe bottom or in relation to another object (to the left). (d) We finally consider any required labels, legends, or additional\ntextual elements (annotations). Further, for graphs containing multiple objects, the prompt must additionally specify the\nquantity of each object type, the relative spatial or logical relationships between objects, and the individual properties of\neach object group. Individual aspects are typically optional, i.e., not every prompt has to specify numerical or spatial\ncomponents. Specific details on dataset construction follow below.\nGeneration Queries Q We adopt a structured methodology that leverages a dictionary D along with a set of query\ntemplates T to create a diverse, comprehensive, and traceable set of generation queries Q for the ScImage evaluation\ndataset.\nDictionary D defines key elements relevant to scientific figures, including objects (e.g., square and circle), attributes\n(e.g., color and size), spatial relations (e.g., left, right, between), and numeric values (e.g., three, five, two more). To\nconstruct this dictionary, we begin by building the list of objects. First, we manually extract frequent object entities\nfrom DATIKZ (Belouadi et al., 2024a), an image-caption dataset derived from scientific publications."}, {"title": "3.3 EVALUATION", "content": "We employ a multi-faceted evaluation approach to assess the quality and accuracy of the generated scientific graphs:"}, {"title": "4 EXPERIMENTS", "content": "We employ two types of models for image generation, corresponding to the two output modes described in Section 3.1.\nFor (i) direct text-image mode, we include DALL-E and STABLE DIFFUSION; for (ii) text-code-image, we include\nGPT-40, LLAMA 3.1 8B and AUTOMATIKZ (Belouadi et al., 2024a), where the model is prompted to generate\nPython or TikZ code. AUTOMATIKZ is specifically fine-tuned for generating TikZ code, therefore, we only prompt\nAUTOMATIKZ to generate TikZ."}, {"title": "5 ANALYSIS", "content": "We conduct a more detailed analysis of model performance, focusing on understanding types (attribute, numerical and\nspatial understanding) and object types to identify which categories present the greatest challenges for the models.\nTypes of Understanding Table 4 presents the fine-grained correctness scores for different understanding types. Figure\n3 illustrates the performance of two modes of generation (text-code-image and text-image) separately. Notably, spatial\nunderstanding appears to be the most challenging across all textual models. For instance, while GPT-40 achieves\nscores around 4.0 for attribute binding, its performance drops substantially for spatial understanding, remaining well\nbelow 3.5.\nIn contrast, for the image generation models STABLE DIF-\nFUSION and DALL\u00b7E, numerical comprehension poses\nthe greatest challenge (Figure 3). Both models score\nbetween below 1.8 for numerical understanding, substan-\ntially lower than their scores for attribute understanding\n(~2.7) and spatial understanding (above 2.0). This indi-\ncates an interesting discrepancy between model types.\nDue to their weakness in spatial understanding, tasks\nthat involve combined understanding types including\nnumerical & spatial understanding, as well as numerical\n& spatial & attribute understanding also tend to receive\nlower scores. Both GPT-40_python and GPT-40_tikz\nrecord their lowest scores when addressing prompts that\nrequire all three understanding types, in comparison to\nprompts focused on individual understanding types."}, {"title": "6 CONCLUDING REMARKS", "content": "Our study presents the first comprehensive evaluation of multimodal LLMs for scientific image generation, using\nour novel ScImage benchmark. Our assessment reveals both significant progress and persistent challenges in the\nfield. While models like GPT-40 sometimes demonstrate proficiency in tasks involving individual dimensions of\nunderstanding (spatial, numeric, or attribute-based in isolation), all evaluated models struggle with complex tasks\nrequiring combined understanding. On average, even GPT-40 performs below 4 on correctness on our benchmark. For\nexample, due to its lack of world knowledge or an inability to correctly plan how a 3D object should be presented, GPT4"}]}