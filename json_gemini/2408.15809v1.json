{"title": "Object Detection for Vehicle Dashcams using Transformers", "authors": ["Osama Mustafa", "Khizer Ali", "Anam Bibi", "Imran Siddiqi", "Momina Moetesum"], "abstract": "The use of intelligent automation is growing significantly in the automotive industry, as it assists drivers and fleet management companies, thus increasing their productivity. Dash cams are now been used for this purpose which enables the instant identification and understanding of multiple objects and occurrences in the surroundings. In this paper, we propose a novel approach for object detection in dashcams using transformers. Our system is based on the state-of-the-art DEtection TRansformer (DETR), which has demonstrated strong performance in a variety of conditions, including different weather and illumination scenarios. The use of transformers allows for the consideration of contextual information in decision-making, improving the accuracy of object detection. To validate our approach, we have trained our DETR model on a dataset that represents real-world conditions. Our results show that the use of intelligent automation through transformers can significantly enhance the capabilities of dashcam systems. The model achieves an mAP of 0.95 on detection.", "sections": [{"title": "I. INTRODUCTION", "content": "Dashcams are an important tool for increasing road safety and efficiency, as they enable real-time monitoring and analysis of the driving environment. One key aspect of intelligent truck dashcams is object detection, which involves the identification and classification of various objects and events in the environment. Accurate object detection is crucial for the proper functioning of intelligent truck dashcam systems, as it allows for the identification of potential hazards, the tracking of vehicles and pedestrians, and the recognition of traffic signs and signals. This object detection and classification is also an important step towards autonomous /self-driving.\nIn this paper, we present a novel approach for object detection in dashcams which make them intelligent using transformers. Transformers are a class of neural network architectures that have achieved state-of-the-art performance in many natural language processing tasks and have recently been applied to a variety of computer vision tasks as well [1], [2]. We demonstrate the effectiveness of the DEtection TRansformer (DETR) [3] for object detection in intelligent dashcams, and show that our system performs well in a variety of different conditions. In addition to describing our proposed approach, we also present experimental results that validate the effectiveness of our method. DETR has outperformed state of the art object detectors like YOLO and RCNN variants in other challenging scenarios like underwater object detection [4].\nObject detection in vehicle dashcams is a challenging problem with the following challenges:\n\u2022 Highly dynamic environment on road. The traffic state on a road is continuously changing especially in long-route traveling vehicles such as Trucks\n\u2022\n\u2022\nDifferent illumination conditions due to different daytime, weather and scene\nDifferent challenging scenarios such as angle, orientation, occlusion and small size of stop signs\n\u2022 Many of the object detectors perform well in training but performance drops in deployment conditions, so the detecting network is not able to generalize well on real-world conditions\nThe trucking industry is using intelligent automation in trucks for early warning and decision systems to prevent accidents. In this work, the trained object detection network DETR performs very well in challenging conditions even when a human driver would face difficulty in decision-making. The network has been trained on a dataset that has been collected in real-world conditions by deploying a dashcam on a fleet of trucks. This proposed solution can play an efficient role in the intelligent automation of truck dashcams. The main contributions of our work are as follows:\n\u2022\nDetection of vehicles and road signs on real-world dash-cam datasets that contains images from different challenging scenarios."}, {"title": "II. RELATED WORK", "content": "In autonomous driving, Traffic signs (Stop-signs), traffic signals and other object detection is an important and challenging problems due to the illumination variations and background clutter. The importance of addressing these challenges, as autonomous driving has the potential to significantly improve the safety, efficiency, and accessibility of vehicles. Accurate perception and understanding of the environment are crucial for the vehicle to be able to navigate safely and avoid collisions. Real-time performance is also essential for the vehicle to respond to changing situations and events in a timely manner. Most of the previous studies focused on the recognition or classification of traffic signs and other objects."}, {"title": "A. Object Detection", "content": "In computer vision, Object detection is a challenging problem and a highly active area of research. The goal of object detection is to determine the object's location and class within an image or video. In recent years, deep learning techniques are powerful for the representation of feature learning from object detection data directly and in the fields of generic object detection the deep learning techniques are huge (main) breakthroughs [13]. The deep learning models are divided into two categories for the tasks of object localization and recognition namely Two-Stage and One-stage [14]. Although the detection performance of two-stage detectors is good, their processing speed is slow and requires high computational costs. The One-Stage detector created a balance between accuracy and speed. However, in the last few years, the most popular Two-Stage and One-Stage models are Faster R-CNN [15], Mask RCNN [16], FPN [17], SSD [18], YOLO [19] for object detection. Recently, transformer-based techniques or methods have been used in various fields for object detection. In 2020, Carion et al. [3] introduced a new method for object detection known as \"Detection Transformer\" which was based on the transformer and bipartite matching loss with parallel decoding. The previous detector with RNNs used autoregressive decoding [3]. Due to using parallel processing (Not using NMS and anchors boxes techniques) DeTr performs fast as compared to previous detectors. The proposed model DeTr performance was evaluated on MS-COCO (Large Dataset). In addition, the overall architecture of DeTr is simple and more powerful in the image where context is important as compared to other detectors.\nIn recent years, most of the studies used deep learning techniques for this problem but most of them have not been successful when applied in-field (real-world) environment. Due to the constrained environment database and small benchmarks. In [20] the authors introduced the database of street-based parking sign detection. They used different YOLOv5 models including YOLOv5s, YOLOv5m, and Swim Transformer. However, the proposed solution was based on the YOLOv5 model and achieved 96.8% accuracy. But the proposed solution sometimes failed when testing the model on dashcam video. In another research, Mian et al. presented a CNN-based solution and used the large Malaysia traffic sign database only for the recognition of traffic sign [6].\nIn the last few years, several databases are introduced for traffic sign detection and recognition. The large real-world dataset is \"German Traffic Sign Detection Benchmark\" [8] presented in competition at IJCNN and used for the localization and classification of the traffic signs. In this dataset, the captured images contain a natural and illumination variation but it only used the cropped around the traffic sign images. However, most of the datasets contain cropped images of traffic lights and traffic signs, which have been extracted from tencent and google (other search engines) and most of the studies collected datasets using cameras footage mounted in vehicles such as [21], [22], [23] and [24].\nRoad accidents are often caused by the violation of stop signs in daily life. Bravi et al. developed an automatic system for stop sign violation detection. The proposed solution is based on the YOLOv3 model and the performance evaluated on the video dashcam dataset [7]. In another study [10], the traffic violation system was developed based on YOLOv3 using dashcam video. The object detection of traffic violations such as the number of vehicles, speed of vehicles and the jump signal. The proposed model obtained 89.2% for the detection of traffic violations and achieved 97.6% accuracy for the count of vehicle detection. Besides, for detection of road stop signs detection using driving data [25] introduced a novel algorithm based on a statistical analysis of obtained drive history data.\nAs already mentioned, the main contribution of this study is to detect objects in challenging problems such as illumination variation, different weather conditions, and multi-scales objects (Small, medium, large) using truck dashcam data. More recently, In [11] authors presented an efficient algorithm based on YOLOv3 for the improvement and enhancement of the performance of the Advanced Driver assistance system (ADAS). In this study, they addressed the real-time condition challenges. But they used the large old German Traffic Sign Detection Benchmark (GTSDB) [9] for the detection and recognition of traffic signs for self-driving. GTSDB database contains the cropped traffic sign images although only the traffic sign cropped images detect and recognized, however, in real-time we have multiple challenges regarding the resolution of images. GTSDB is also an unbalanced dataset. However, they used two separate models: for the detection they achieved 89.9% accuracy and"}, {"title": "III. DATASET", "content": "In this study, the dataset plays a key role. As discussed above, the major challenges in object detection are highly dynamic environments, truck blind spots, low light, and noisy environments. Thus we experimented with a competition dataset i.e Motive AI challenge dataset. The dataset contains 39,998 training images and 4001 validation images along with annotations respectively. There are four classes: traffic signals, stop signs, cars, and trucks. Motive is a leading fleet management company working on intelligent automation in trucking. Often, it has been observed that object detectors perform ideally on synthetic or self-curated datasets but performance drops significantly in real-world conditions. Thus this dataset has been prepared by considering the special focus on real-world conditions. By employing a dashcam on a fleet of trucks, this dataset has been collected in different day-lights and different weather. different scenes and different traffic conditions. Objects of interest i.e traffic signals, stop signs, cars, trucks and others are captured from different angles and positions in order to achieve generalization."}, {"title": "IV. METHODOLOGY", "content": "In this work, Transformer based detection architecture i.e DEtection TRansformer (DETR) is utilized for the task of object detection in imagery from the dashcams. This work is focused on the detection of objects from a dashcam point of view. There are many challenges in the case of object detection in intelligent dashcams. Some of the challenges are the dynamic environment on the road, the large number of vehicles on road, the large size of trucks as compared to the small size of other vehicles. We have seen in recent work, architectures such as RCNN and YOLO perform really well on object detection problems but they do not consider much contextual information while making decisions and it has been observed that contextual information is the key point in efficient decision-making in such highly dynamic environments [26]. Transformers-based architectures have been the best performing in Natural Language Processing (NLP) problems for a long time as they are still considered a key milestone by introducing self-attention-based decision-making. Recently they have also shown outstanding benchmark performances in the field of vision such as DETR, DINO, SWIN-Transformer and Vanilla ViT. Thus we have employed a DEtection TRansformer(DETR) to perform object detection in this research.\n1) Architecture: The network architecture is such that a transformer as prediction head on top of ResNet-50 backbone for feature extraction. Figure II-A illustrates the pipeline of our proposed solution. The input image passes through a ResNet which performs the feature extraction. Features along with positional embeddings are passed into an encoder-decoder respectively. Figure 4 further details out the architecture of encoder-decoder. An encoder starts with a multi-head self attention followed by an add & norm layer which connects with a FFN and finally an Add & Norm layer is repeated. The decoder architecture follows a similar pattern with two FFN's on the head for final prediction, one for bounding box and one for class. The embedding size and no of layers for encoder-"}, {"title": "V. EXPERIMENTS AND RESULTS", "content": "In this section, we present the details of experimentation carried out in order to perform quantified validation of our proposed methodology. This section will be used as a reference to reproduce the results anytime. Details of the dataset, evaluation metrics and experimentation are provided further in this section respectively."}, {"title": "A. Evaluation Metrics", "content": "As this is an object detection problem, the basic evaluation metrics are Precision and Recall used in to evaluate the detection performance. Precision and recall are calculated using True Positive (TP), False Positive (FP), True Negative (TN) and False Negative (FN).\nPrecision and Recall are calculated using:\n$Precision = \\frac{TP}{TP+FP}$ (2)\n$Recall = \\frac{TP}{TP+FN}$ (3)\nThe average precision and recall for each class is calculated, and the mean Average Precision (mAP) and mean Average Recall (mAR) is determined at different IoU values as follows:\nmAP50 is computed by averaging the mAP using a 50\n1) mAPThe mAP is calculated by taking the average of its values at 10 different IoU thresholds, which range from 50% to 95%\n2) mAP50 is calculated by taking the average mAP over 50% IoU threshold.\n3) mAR10d is calculated by the maximum recall values given 10 detections per image by taking average over IoUs and all the classes..\n4) mAR100d is calculated by the maximum recall values given 100 detections per image taking average over IoUs and all the classes."}, {"title": "B. Results and Analysis", "content": "This section details the experimental results and analysis. DETR fine-tuned on our custom dataset achieves an average mAP of 0.95 with the IOU threshold set to 0.50 as shown in Table III. It is observed that this mAP is a great result considering the challenging conditions covered in the dataset. It can be seen in Figure IV-1 that the mAP is improving as the epochs increase which shows smooth learning. Considering the different harsh real-world conditions such as low light, noisy environments, and occluded and cluttered environments, the results are quite impressive. As this is a private and proprietary dataset there is no related work for comparison.\nFigure 6 displays inference results, images on the left are actual inputs and on the right are output images from DETR. In the first row, it can be seen that the network accurately detects and objects of interest. In the first row, two bounding boxes are detected, one with a confidence of 0.90 and the other with a confidence of 0.79. The one with confidence 0.79 is rarely visible if we just consider its visual characteristics, it would be extremely difficult to detect it if a detector just considers its visual features as the \"stop-sign\" text is completely not visible, but DETR has efficiently detected this stop sign with solid confidence by considering global contextual information. Furthermore, in the second row, it can be seen that it is a low-light environment, you can check by zooming in that even a human would find it extremely difficult to detect many cars parked alongside left and right on the road in the parking of a gas station. DETR performs outstandingly in this condition and detects all the vehicles parked in dark parking. If a detector just considers the visual features in this image, the darkness is more dominant and it would not be able to detect all those cars, but DETR being a transformer has efficiently detected by considering global contextual information. These experimental results validate that state-of-the-art DETR can be a robust object detection network for truck dashcams in harsh conditions."}, {"title": "VI. CONCLUSION", "content": "In this study we have proposed a transformers based object detection solution for the challenging problem of object detection in vehicle dashcams in a highly dynamic environment. We have fine-tuned a state-of-the-art DEtection TRansformer (DETR) for this purpose on our custom dataset. The experimental results discussed in results and analysis section validates that DETR is a robust network for performing object detection in harsh deployment conditions such as low levels of illumination due to to weather and different daytime, occluded and noisy environment and highly dynamic environments. The point that in this proposed solution, network considers contextual information makes it robust and an efficient solution for this highly challenging problem. This study validates that if DETR is trained on a generalized data distribution covering major deployment conditions DETR generalizes really good in deployment and the performance does not drops. This study can be utilized as one of the references for future work in this domain."}]}