{"title": "Unsupervised Cognition", "authors": ["Alfredo Ibias", "Hector Antona", "Guillem Ramirez-Miranda", "Enric Guinovart", "Eduard Alarcon"], "abstract": "Unsupervised learning methods have a soft inspiration in cognition models. To this day, the most successful unsupervised learning methods revolve around clustering samples in a mathematical space. In this paper we propose a state-of-the-art primitive-based unsupervised learning approach for decision-making inspired by novel cognition models. This representation-centric approach models the input space constructively as a distributed hierarchical structure in an input-agnostic way. We compared our approach with current state-of-the-art in unsupervised learning classification, and with current state-of-the-art in cancer type classification. We show how our proposal outperforms previous state-of-the-art. We also evaluate some cognition-like properties of our proposal where it not only outperforms the compared algorithms (even supervised learning ones), but it also shows a different, more cognition-like, behaviour.", "sections": [{"title": "Introduction", "content": "Unsupervised learning is a huge field focused on extracting patterns from data without knowing the actual classes present in it. Due to this particularity, the field is full of methods that cluster data from the properties of their mathematical representation. This hampers their applicability to data whose mathematical relationships do not directly correlate with their cognitive relationships, relationships that cognitive agents (like humans) find between the data. For example, the MNIST dataset has a clear cognitive relationship between its different samples: the number they represent. However, when transformed into numerical values for clustering, their relationships fade out in favour of relationships between their encodings, that do not necessarily correspond with the cognitive ones.\nIn this field, there are multiple algorithms that focus on the unsupervised classification problem. However, due to their soft inspiration in cognition models, most of them address the problem from an optimisation perspective. This approach leads to the need of building a mapping between any input and a valid output (ideally, the best output), and thus it is equivalent to dividing the input space into subspaces. In that regard, the representations become spatial, in the sense that the classes are represented by subspaces of an infinite space, independently of the similarity between the inputs that fall in that subspace.\nIn contrast, novel theories about how the brain works propose that the brain models the world in a constructive way, that is, it generates constructive representations of the world (Hawkins and Blakeslee 2004; Leibo et al. 2015; Yon, Heyes, and Press 2020). A constructive representation would be an abstraction or archetype of a class, in the sense that, it would be a representation to which any (or at least some) elements of the class are similar to. This implies that, to assign a class to an input, it has to be similar enough to one of the already learned representations, and if it is not similar enough to any of them then it can not be classified. Mathematically speaking, the difference between both approaches is that the first, traditional one focuses on splitting a representation space, and the second, novel one focuses on building a set of representations.\nTo empirically evaluate the performance of this new approach, we propose a novel unsupervised learning algorithm for decision-making based on a novel cognition framework (Ibias et al. 2024). The goal of this approach is to model the input space generating constructive representations, which implies that, in a way, our proposal has to be able to automatically generate abstractions from the inputs.\nTo do so, it requires a representation-oriented universal data structure. Recent research has shown that such data structure is a Sparse Distributed Representation (Ahmad and Hawkins 2016; Cui, Ahmad, and Hawkins 2017) (SDR), which allows an universal representation of the inputs independently of their type. This has been proven to be the actual way in which the brain processes its inputs (Foldiak 2003; Cui, Ahmad, and Hawkins 2017). Thus, we will need a method to transform our input spaces into SDRs. We will do so through an encoder decoder pair we call Embodiment.\nUsing SDRs as inputs, we propose an unsupervised learning algorithm for decision-making that models the input space generating constructive representations in an input-agnostic way. We call such algorithm a Primitive and we expect it to be the building block of a future cognition algorithm. In this paper we present its essence as well as one of its modulators: the Spatial Attention modulator. This modulator will auto-regulate the spatial discriminability of the algorithm. Additionally, we developed our proposal to be transparent and explainable, as it is desirable that any so-"}, {"title": "Related Work", "content": "There are multiple algorithms for unsupervised learning developed along the years, from generic clustering algorithms like K-Means (Lloyd 1982), to more specific, usually neural network based, algorithms that deal with only one task. In this second category we can find algorithms that deal with topics as unrelated as representation learning (Wang et al. 2020), video segmentation (Araslanov, Schaub-Meyer, and Roth 2021) or speech recognition (Baevski et al. 2021). However, none of them try to build constructive representations, but instead they divide a mathematical representation of the input space into clusters that represent the different classes present in the input space.\nAmong these clustering algorithms, there are few that stand out, specially for the task of unsupervised classification. One of them is K-Means due to its performance clustering tabular data. This algorithm clusters the samples based on their closeness in the mathematical space of their encodings. Another one is Invariant Information Clustering (IIC) (Ji, Vedaldi, and Henriques 2019) due to its performance clustering images. This algorithm takes an image, transforms it with a given transform, and then run both of them over two neural networks with the goal of learning what is common between them. To that effect, it aims to maximise the mutual information between encoded variables, what makes representations of paired samples the same, but not through minimising representation distance like it is done in K-Means. In any case, both algorithms stand out due to their performance in their respective domains, but none of them is able to obtain good accuracy across domains. Thus, we will use them as baseline for comparison purposes, even though they cannot be applied in all cases.\nFinally, regarding brain-inspired methods that try to model the input space, the only research we are aware of is the Hierarchical Temporal Memory (Cui, Ahmad, and Hawkins 2017) (HTM) and SyncMap (Vargas and Asabuki 2021), although they are algorithms suited for learning sequences instead of static data, and HTM is not unsupervised. Thus, as far as we are aware, ours is the first proposal of a brain-inspired constructive unsupervised learning algorithm for modelling static data."}, {"title": "The proposal", "content": "Our proposal, based on the novel cognition framework presented at (Ibias et al. 2024), is composed by: an Embodiment, a Primitive, and a Spatial Attention modulator. The goal of the Embodiment is to transform the input space into Spatial Distributed Representations (SDRs), the goal of the Primitive is to process those SDRs and model the input space generating constructive representations, and the goal of the Spatial Attention modulator is to self-regulate the Primitive."}, {"title": "The Embodiment", "content": "To translate inputs to SDRs we need an encoder architecture. To interpret the SDRs the Primitive generates we need a decoder architecture too. Both architectures conform the Embodiment of our Primitive. In our case, as in our experiments we only explore tabular and image datasets, we only present four kinds of encoder decoder pairs: one for float point numbers, one for categorical data, one for grey images, and one for colour images. All these encoders are lossless and thus allow us to recover the encoded data.\nThe grey images translation to SDR is straightforward: a grey image's SDR is a flattened version of the image (where each pixel is a dimension) with the values normalised to be between 0 and 1. In the case of colour images, we perform the same transformation to each one of the RGB channels and we concatenate their SDRs to form the image SDR. For floating point numbers its translation to SDR is a bit more nuanced. We take the input space of the number (that is, the possible values it can take) and divide it into bins. Those bins will be the dimensions of the SDR. Then, each number will fall into one of those bins. However, in order to allow some overlap between numbers (what is fundamental for finding similarities using our Primitive), we also activate as many bins around the number bin as another parameter we call bin overlap. With this, the SDR is a long list of zeros and some ones around the bin where the number falls. By default, we use an overlap of 10% of the number of bins, that by default is set to 100 bins. In the case of categorical data we create one bin per category and set the overlap to 0%, following a one-hot encoding.\nHaving these representations, we define the SDR representation of a tabular input as a concatenation of the SDR"}, {"title": "The Primitive", "content": "Once we have an SDR representation of the inputs (and a way to recover the values from the SDR representation), we need to process them. To that end we need an internal representation of the SDRs, that we called a Footprint. This Footprint contains a data SDR, and can contain (for evaluation purposes only) a metadata SDR (i.e. an SDR representing a label). This Footprint also has an updating function and an activation function. The updating function modifies the data and metadata SDRs when necessary mixing the Footprint SDRs with an external SDR (Algorithm 1), while the activation function computes an SDR containing the element-wise average between the Footprint SDRs and an external SDR.\nAs it is clear from Algorithm 1, our approach to build constructive representations consists on merging similar inputs to build the abstraction or archetype. The core of our algorithm revolves around building and organising these representations in the form of Footprints. To organise them, they are grouped inside the nodes of a tree structure. These nodes (that we call cells) contain a set of Footprints and a threshold common to all their respective Footprints. This threshold will be deeply defined in the next section.\nAt the beginning of training the only node is the root node (which we call seed cell), that starts with no Footprints. When new inputs are processed during training, new Footprints are generated and the seed cell grows. And when an input is considered similar to an existing Footprint, then a new cell is created as a child of the seed cell, and it is associated with that Footprint. Thus, a cell can have as many children as Footprints, and each child cell has an associated parent Footprint.\nTo decide if a new input is considered similar to an existing Footprint, we use a Similarity Function to get an score of the similarity of both SDRs, and if that score is over the cell threshold, then both the input and the Footprint are considered similar to each other. When there are more than one Footprint with a similarity over the cell threshold, we consider similar only the one with higher similarity. This similar Footprint is then the active Footprint.\nTo compute the previously mentioned similarity score between SDRs we need a Similarity Function that compares SDRs. This Similarity Function should take two SDRs and return a similarity value stating how similar we consider them to be. The specific Similarity Function we developed for this paper is a variation of the euclidean distance, but we also tested using the euclidean distance between vectors and the differences in results are minimal. An important remark here is that the Similarity Function should use only the data part of both SDRs to compute the similarity in order for our method to be fully unsupervised, leaving the metadata part outside of any decision making.\nNow let us show how a new input is processed by our Primitive. To follow this description, a general schema of this algorithm is displayed at Figure 2. As we can see in the schema, our input processing method has three phases. The first phase is a filtering one, where we look for the Footprint most similar to the input. The second phase is an abstracting one, where we go up the tree generating an abstraction of the input with the help of the activation function. In this phase is where the Footprint update also happens. Finally, the third phase is a concreting one, where we take the generated abstraction from the previous phase and filter it down the tree to find the Footprint most similar to the abstraction of the input.\nWe start the filtering phase (left side of Figure 2) with an SDR that is a new input (this SDR contains both a data part and a metadata part). This input is then compared to all the Footprints present in the tree, and we select the Footprint that gets the higher similarity and that, at the same time, surpass its cell's similarity threshold. If a Footprint is activated, then we check if it has a child cell, and if it has it, we create a new Footprint copy of the input in such cell. If the Footprint does not have a child cell, then one is created that contains two new Footprints: one is a copy of the input, the other is a copy of the parent Footprint. If no Footprint is activated, then a new Footprint copy of the input will be created in the seed cell and will be activated. If we are in evaluation mode, no cell or Footprint are created in this phase.\nIn the abstracting phase (centre side of Figure 2), the activated Footprint executes its updating and activation functions in that order. The output of the activation function is what we call an Archetype, and it becomes the output of the cell. This output is then passed up as input to its parent cell, where the parent Footprint is activated and executes its updating and activation functions with its child cell Archetype. This way a Footprint update is performed with an aggregation of the input and the active child Footprints. This process is repeated until it has been done in the seed cell. If we are in evaluation mode, no Footprint is updated in this phase.\nFinally, in the concreting phase (right side of Figure 2), the Archetype generated by the seed cell is used to perform the inverse of the abstracting phase, but this time without learning. That is, the Archetype is provided to the seed cell as input to be compared against the existing Footprints, and if any Footprint is activated, then an output is produced by performing the \u201cinverse\u201d operation of an average between the input and the activated Footprint's representation. That is, we multiply the input by 2 and then subtract the activated Footprint's representation from the result. Finally, that output is passed down as input to the corresponding child cell to repeat the process. When there is no child cell, or no Footprint is activated, the SDRs of the last activated Footprint are considered the Projection of the method. If there is no Footprint activated in the seed cell then there is no Projection.\nThis Projection is supposed to be a very concrete version of the generated Archetype, which is in itself an abstraction of the input. Then, the output of the whole method is this Projection, that includes a data SDR with a representation of the input and a metadata SDR with the additional information about the data SDR provided (like a label). This Projection is later processed by the decoder of the Embodiment to retrieve the final label of the input. A resume of the global algorithm is displayed at Figure 3. It is important to note that, if we are in evaluation mode, then the update function of the Footprints is not executed to not modify the internal representations, and no new Footprints neither cells are created. Thus, if there is no Footprint activated in the seed cell in the third phase then there is no Projection and our method answers an \"I do not know\".\nHere it is important to remark the relevance of the second and third phases: they allow for generalisation. Without them, our algorithm would be a set of copies of the inputs, deciding the class by the most similar one. That is, it would be a convoluted implementation of the K-Means algorithm with $K=1$. Adding these phases allows to construct aggregated representations, that in turn may be more similar to potential inputs than the already processed ones. Doing the aggregations in an intelligent way, we have more potential for generalisation. This also allows us to avoid overfitting in subsequent epochs, because the aggregated representation will never be equivalent to an individual input.\nTo end with the Primitive, it is important to note that each cell has its own similarity threshold. This allows for a better discrimination policy, as we will see in the next section. Additionally, it is important to note that, during training, the label is provided in the metadata SDR to include it in the Footprints, but it is not used for comparing Footprints, and thus it is avoided for building and organising the internal representations. Later, when in evaluation mode, the label of each Footprint would be a mixture of the labels of the inputs that updated such Footprint. When classifying, the returned label will be the strongest label of the Projection.\nFinally, it is important to note how our Primitive is representation-centric, as the whole algorithm focuses on generating the right internal representations of the inputs, without explicit guide by the classification goal. We aim that these representations would produce the right Projections for classification, but we do not base our updating in how well they are classifying. Instead, we focus our learning in building representations that make sense and can be considered proper abstractions of the individual inputs that made them. This is useful to detect patterns, as different instances of the same pattern will eventually collide into one representation, building an abstraction of such pattern."}, {"title": "The Spatial Attention Modulator", "content": "In the previous Section, a threshold was used to decide if two SDRs were considered similar or not. This threshold can be set arbitrarily, but that would hamper the performance of the Primitive and would generate multiple extra parameters of the model. Thus, a way to automatise the threshold selection was needed, and that is the role of the Spatial Attention Modulator. The whole role of this modulator is to measure the variability of the input space of the cell and dynamically set a similarity threshold. The algorithm we developed to set such threshold is the average of the mean similarities between the Footprints of the cell, and its pseudo-code is displayed in Algorithm 4.\nThe rationale behind using this approach is that any input space has a certain variability, and the right threshold will be that one that sits in the middle of such variability. Such variability is unknown, but the Footprints have captured part of it in the form of aggregations. Thus, each aggregation (that is, each Footprint) represents a class and the average of the similarities between them is the \"captured\" variability.\nThis actually allows for child cells to have higher thresholds than their parent cells, as their input space are limited to those inputs that are similar to their associated parent Foot-"}, {"title": "Empirical Evaluation", "content": "To evaluate our proposal, we performed three different experiments: a comparison in classification task versus other unsupervised learning algorithms, a state-of-the-art comparison over a medical dataset, and a comparison in cognition-like capabilities versus other clustering algorithms. All the experiments were run in an Ubuntu laptop with an Intel Core i9-13900HX at 2.60GHz with 32 cores, 32Gb of memory, and a NVIDIA GeForce RTX 4060 with 8Gb of VRAM.\nExperimental Subjects\nOur experimental subjects for these experiments where five datasets: two tabular datasets full of numerical values, two image datasets, and a medical data dataset. Those datasets are the widely known Wisconsin Breast Cancer dataset (Dua and Graff 2017; Wolberg, Street, and Mangasarian 1995), Pima Indians Diabetes dataset (Smith et al. 1988), MNIST dataset (LeCun et al. 1998), CIFAR10 dataset (Krizhevsky, Hinton et al. 2009), and Cancer Type dataset (extracted from The Cancer Genome Atlas (TCGA) database (Weinstein et al. 2013)). The different properties of these datasets"}, {"title": "Experiments", "content": "The first experiment we performed aimed to test how well our proposal deals with a classification task compared to other unsupervised learning algorithms. For tabular data we compared to K-Means with as many centroids as labels, and with the number of centroids that the elbow method (Thorndike 1953; Liu and Deng 2021) proposes. To evaluate its classification power, each cluster was assigned the label that was most repeated between the training elements of that cluster. In the case of our proposal, the label selected is the one associated to the Projection. When comparing over the image datasets (MNIST and CIFAR10), the Invariant Information Clustering (IIC) algorithm was computed. In this case, the IIC algorithm was setup with the recommended parameters set by the authors for each dataset, and we compared different number of epochs (1, 10, 100) because we could not try the author recommended number of epochs (3200 for MNIST and 2000 for CIFAR10) or any higher number of epochs due to resource constraints. We display the resulting learning curves in Figure 5.\nThe results of this experiment clearly show that our proposal is a better option for unsupervised classification. As we can observe, for tabular data our alternative is on par with K-Means for the Pima Indian Diabetes dataset (loosing by a 2.6%) and for the Wisconsin Breast Cancer dataset (winning by a 1.17%). When we move to image data, we can observe how our proposal is on par with IIC (loosing by 1.13% for MNIST with 100 epochs, but winning by 6.32% for CIFAR10 and by 7.09% for MNIST with 10 epochs).\nWe want to explicitly remark the fact that our proposal is able to obtain very good accuracies with fewer samples. For contrast, IIC needs around 1700 training samples to obtain an stable accuracy over 70% in test MNIST, while our proposal needs less than 200 samples. Moreover, our proposal does not need multiple epochs to obtain such results: it only goes through the training samples once, although more epochs also improve results (as shown by the \"10 epochs\" lines). Finally, if we compare with IIC with only 1 training epoch, then IIC is not able to overcome our proposal in any"}, {"title": "Ablation Studies", "content": "In this Section we analyse the effects of the different parameters of our proposal. Let us start by stating that the main \"parameter\" of our proposal is the Embodiment. In this paper we have presented very basic Embodiments, and our algorithm works decently well with them. However, a fine tuned embodiment can cause huge increases in performance. For example, during our experiments with the Pima Indian Diabetes dataset we discovered that our initial embodiment (with a 10% overlap) did not obtain the best results. Thus, after testing multiple overlaps we settled in the 50% overlap. In general, for numerical data, the overlap between numbers is a fundamental parameter, because usually with a 0% overlap the performance is low, then it quickly raises with a small overlap and eventually falls down when the overlap is too big. Other \"parameters\" of our proposal are the similarity and Spatial Attention functions. We presented the ones that we have discovered that produce the best results, after trying a lot of alternatives like the similarity between the input and the aggregation of the Footprints of the cell for the Spatial Attention function or the Jaccard distance for the similarity function. Furthermore, we are aware that alternative functions can be developed with the potential to improve furthermore the results, but looking for those improved functions is matter of future work."}, {"title": "Discussion and Limitations", "content": "In this Section we would like to discuss the transparency and explainability of our algorithm, its capability of saying \"I do not know\", and its limitations.\nRegarding transparency and explainability, it is fundamental to note that, as our algorithm has an internal hierarchical organisation of Sparse Distributed Representations (SDR), it is possible to recall how our algorithm decided which label corresponds to the input. To that effect, we need the decoder from the Embodiment to transform the internal SDRs to understandable outputs. Thus, we can interpret any decision as a filtering from the seed cell, based on its Footprints, and down the hierarchy until the last Footprint is activated. Then, the generated Projection is the output, and the strongest label of that Projection is the one selected.\nRegarding the capability of our algorithm to say \"I do not know\", it is easily derived from our threshold setup. If a new input does not surpass the threshold for any Footprint, that is, its similarity with each one of the Footprints in the tree is lower than their associated thresholds, then our algorithm returns a value stating it cannot associate that input to any knowledge it has learned. Moreover, that answer is not only an \"I do not know the label\", but it actually means that it does not have a model for such input, so it can not return any Projection of it neither. This is an important and novel feature in an unsupervised learning algorithm. Its importance lies in the fact that saying \"I do not know\" ensures the user understands that the algorithm was not trained to recognise the pattern that was given, instead of falsely providing an answer and hallucinate (Ortega et al. 2021; Ji et al. 2023).\nFinally, regarding the limitations of our proposal, its main one is the high memory costs involved compared to other alternatives due to the storage of a huge number of SDRs. We are aware that this limitation can hamper its scalability and applicability over very huge datasets and we are working in ways to diminish it, from developing growth inhibition and death mechanisms for the Footprints and cells, to improving our Embodiments to generate smaller SDRs."}, {"title": "Conclusions", "content": "Current well known unsupervised learning methods have a dim capability of extracting cognition-like relationships due to their optimisation oriented setup. The biggest exponent of this field is K-Means, that clusters samples based only on the mathematical distance between them. In this paper we have proposed an alternative input-agnostic representation-centric unsupervised learning algorithm for decision-making that extracts cognition-like relationships between samples through constructive representations.\nOur proposal transforms the inputs into SDRs, and then generates an internal representation of those SDRs in order to later recall that representation when asked about the class of an specific input. We tested our proposal against K-Means and IIC for unsupervised classification tasks in four datasets, and show that our proposal is equivalent to them, even although it only process each sample once. Moreover, we have compared it with the state-of-the-art for identifying cancer types, and overcome them. Finally, we have evaluated how well it can discover cognition-like relationships compared to other clustering algorithms, and we have found that it is better than the three main clustering algorithms: K-Means, IIC and K-NN. This is important because it means that our proposal does not only have a different, better behaviour than unsupervised learning algorithms, but also than supervised learning clustering ones.\nAs future work, we would like to explore how our proposal performs in other datasets and against other unsupervised learning algorithms, and perform an in deep analysis of the relevance of each \"parameter\" of our model. We would also like to develop new Embodiments for different input types, like sound. We would like to explore the extension of our algorithm with other modulators too, like a conditioning modulator that allows us to have a reinforcement learning-like algorithm, or a temporal modulator that allows us to process sequences. Finally, we would like to explore different algorithms to compute the similarity function or the spatial attention function."}]}