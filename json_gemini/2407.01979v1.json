{"title": "Unveiling Global Interactive Patterns across Graphs: Towards Interpretable Graph Neural Networks", "authors": ["Yuwen Wang", "Shunyu Liu", "Tongya Zheng", "Kaixuan Chen", "Mingli Song"], "abstract": "Graph Neural Networks (GNNs) have emerged as a prominent framework for graph mining, leading to significant advances across various domains. Stemmed from the node-wise representations of GNNs, existing explanation studies have embraced the subgraph-specific viewpoint that attributes the decision results to the salient features and local structures of nodes. However, graph-level tasks necessitate long-range dependencies and global interactions for advanced GNNs, deviating significantly from subgraph-specific explanations. To bridge this gap, this paper proposes a novel intrinsically interpretable scheme for graph classification, termed as Global Interactive Pattern (GIP) learning, which introduces learnable global interactive patterns to explicitly interpret decisions. GIP first tackles the complexity of interpretation by clustering numerous nodes using a constrained graph clustering module. Then, it matches the coarsened global interactive instance with a batch of self-interpretable graph prototypes, thereby facilitating a transparent graph-level reasoning process. Extensive experiments conducted on both synthetic and real-world benchmarks demonstrate that the proposed GIP yields significantly superior interpretability and competitive performance to the state-of-the-art counterparts. Our code will be made publicly available\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "Graphs, serving as data structures capable of naturally modeling intricate relationships between entities, have pervasive applications in real-world scenarios, such as transportation networks [6, 71], social networks [36, 65], power system [9, 11, 34], and biological molecules [24, 44]. In recent years, to effectively uncover potential information in graphs for applications, graph neural networks (GNNs) [10, 23, 29, 52] have emerged as a prominent paradigm and made remarkable achievements. Following a message-passing mechanism, GNNs aggregate the information from the local neighbors of each node to obtain node-wise representations, bolstering the development in various downstream tasks including node classification [10, 35, 56, 73] and graph classification [12, 23, 29, 52].\nDespite the remarkable effectiveness of GNNs, their lack of explainability hinders human trust and thus limits their application in safety-critical domains. To mitigate this issue, recent efforts have explored identifying informative subgraphs that serve as either post-hoc or intrinsic explanations for the decisions made by GNNs.\nSpecifically, a line of post-hoc studies [37, 54, 66] work on a pre-trained model and propose different combinatorial search methods for identifying the most influential subgraphs based on model predictions. However, since these methods train another explanatory model to provide explanations, they may be disloyal to the original model, resulting in distorted attribution analysis. In contrast to the post-hoc methods, the intrinsically interpretable ones endeavour to identify subgraphs during training and make reliable predictions guided by these subgraphs [39, 64, 68]. The pioneering works, e.g., GIB [39] and GSAT [68], adopt the information bottleneck principle [59] to constraint the information flow from input graph to prediction, ensuring the label-relevant graph components will be kept while the label-irrelevant ones are reduced. Additionally, Prot-GNN [72] learns representative subgraphs (i.e., prototypes) from inputs by prototype learning [30] and makes predictions based on the similarity between new instances and prototypes. Unfortunately, the explanation graph is generated by an extra projection process based on the prototype embedding, which can introduce explanatory biases.\nGraph-level tasks often necessitate global-level explanations to depict long-range dependencies and global interactions considering the whole graph [4, 16, 32, 63]. For example, in the case of protein molecules, enzymes are distinguished from other non-enzyme proteins by having fewer helices, more and longer loops, and tighter packing between secondary structures [50]. Identifying such global structural patterns often requires the collective participation of dozens or even hundreds of amino acids. It is time-consuming to entail expert examination over the subgraph explanations of each node provided by previous subgraph-specific methods. Beyond the node-wise representations of early GNNs, recent state-of-the-art GNNs [8, 31, 42, 60] have shifted the focus towards considering global interactions for graph-level tasks, enhancing the expressive power of GNNs by a large margin. Hence, there exists a significant gap between local subgraph-specific explanations and global-level explanations, which are required by both graph-level tasks and advanced GNNs.\nIn this paper, we propose the Global Interactive Pattern (GIP) learning, a new interpretable graph classification task that approaches the problem from a global perspective. This task poses two key challenges for existing techniques, namely, high computational complexity and diverse global patterns. Firstly, the presence of a large number of nodes, along with their intricate connectivity, presents a significant challenge in modeling long-range dependencies and accurately extracting global interactions. Simply extending subgraph-specific methods to identify global interactive patterns would result in exponentially increasing computational complexity. This is particularly true in real-world graphs where these patterns typically involve dozens or even hundreds of nodes. Secondly, there exist multiple interactive patterns for graphs belonging to the same class. Existing techniques either provide instance-level explanations or entail high costs for extracting graph patterns. Hence, it becomes crucial to identify representative and diverse patterns within an acceptable computational overhead for more comprehensive and accurate explanations.\nTo tackle these challenges, we explore an innovative framework for sloving GIP, by first performing compression of the graph and then identifying inter-cluster interactions in the coarsened graph instances, which we call interactive patterns, to determine the intrinsic explanations. Specifically, the framework consists of two key modules: clustering assignment module and interactive pattern matching module. First, in the clustering assignment module, we iteratively aggregate components with similar features or tight connections to form a cluster-level representation, and then extract global structure information based on the interactions between local structures, thus realizing the modeling of the global interactions while aggregating the information of local substructures. Then, in the interactive pattern matching module, different from prior re-searches [15, 72] in graph pattern recognition that target at learning representative embeddings in hidden space, we define learnable interactive patterns in the form of graph structure to directly reveal the vital patterns in the graph level. Additionally, we introduce graph kernels as a measure of similarity between the coarsened graph and the interactive patterns, thereby propelling the learning and matching of interactive patterns based on the similarity. Finally, with the similarity scores, a fully connected layer with softmax is applied to compute the output probabilities for each class.\nIn summary, the main contributions of our work are as follows:\n\u2022 We explore a novel interpretable graph classification task termed as Global Interactive Pattern (GIP) learning, taking a step further from local subgraph explanation to global interactive patterns.\n\u2022 We propose a holistic framework for solving GIP, which achieves a double-win of high computational efficiency and accurate pattern discovery. By integrating learnable cluster constraints and graph prototypes, we can adaptively provide the decisions with reliable graph-level explanations.\n\u2022 Extensive experiments on both real-world and synthetic datasets demonstrate the effectiveness of our framework in achieving accurate prediction and valid explanation. In addition, visualization of the explanations further demonstrates the superior capability of our framework in identifying global interactive patterns."}, {"title": "2 RELATED WORK", "content": "2.1 Graph Neural Networks\nDriven by the momentous success of deep learning, recently, a mass of efforts have been devoted to developing deep neural networks for graph-structured data [5]. As one of the pioneer works, graph neural networks (GNNs) [23, 26, 29, 67] have demonstrated effectiveness in various real-world scenarios [25, 27, 58, 74] such as traffic analysis [6, 71], drug generation [1] and recommendation systems [65]. Generally, classic GNN variants adopt the message-passing mechanism [22] to update the embeddings of each node based on the calculated message set between the node and each of its neighbors. Then, these node-wise representations are manipulated through concatenation or pooling operations to form graph-level representations for graph-level tasks. Although this unique message-passing mechanism enables GNNs to fully leverage the relationships between nodes in graph structure, such GNNs may suffer from over-smoothing due to repeated local aggregation and over-squashing due to the exponential growth of computational cost with increasing model depth. Recent years have witnessed many successful architectures that shift the focus towards considering global interactions for graph-level tasks. These approaches [8, 31, 42, 60] model long-range dependencies and global structures to facilitate a more comprehensive acquisition of the global information in graphs, thus enhancing the expressive power of the model. Owing to the powerful representation capability, these GNNs have achieved state-of-the-art performance."}, {"title": "2.2 Explainability of Graph Neural Networks", "content": "Despite the great success of GNNs, their black-box nature undermines human trust, thereby hindering their application in high-stake domains. To bolster understanding of GNNs and provide more credible evidence for decision-making, plenty of researches focus on the explainability of GNNs is emerging. Such studies concentrate on identifying vital subgraphs, offering intrinsic or post-hoc explanations for GNNs. The post-hoc explainable methods focus on designing different combinatorial search method to explore important subgraphs based on model outputs [37, 41, 66, 70]. As an initial endeavour, GNNExplainer [66] learns soft masks from edge and node features to identify pivotal subgraphs for explaining the prediction result. Furthermore, PGExplainer [37] employs a reparameterization trick to obtain approximated discrete masks instead of soft masks. In addition, XGNN [69] generates representative subgraphs for different classes as model-level explanations.\nSince these methods focus on providing post-hoc explanations for a trained GNN, they might fail to fit the original model precisely and generate biased explanation. Though it would be preferable to design interpretable GNNs [7, 39], there are still limited efforts in this regard [14, 15, 72]. The goal of these methods is to identify subgraphs during training and make reliable predictions guided by subgraphs [39, 64, 68]. GIB [39] and GSAT [68] adopt the information bottleneck principle [59] to constraint the information flow from the input graph to the prediction, ensuring the label-relevant components will be kept while the label-irrelevant ones are reduced. In addition, some existing works attempt to apply prototype learning for exploring important subgraphs from instances and make predictions based on the similarity between new instances and prototypes [15, 72]. For example, ProtGNN [72] applies the Monte Carlo tree search [49] to identify subgraphs in the original graphs as prototypes, while PxGNN [15] obtains prototypes from learnable prototype embeddings by a pre-trained prototype generator.\nHowever, the aforementioned methods only provide one-side attribution analysis from a localized viewpoint, which may lead to under-representative explanations when higher-order node interactions or global graph structure play a pivotal role. To address this issue, in this paper, we propose an interpretable scheme for graph classification called GIP, that explicitly extracts global interactive patterns to deliver graph-level explanations."}, {"title": "3 METHOD", "content": "In this section, we elaborate the details of the proposed framework for GIP. First, in the clustering assignment module, we extract inter-cluster interactions from coarsened graph as global structural information. Then, in the interactive pattern matching module, we match the coarsened graph with a batch of learnable interactive patterns based on the similarity calculated by the graph kernel. Finally, with the similarity scores, the fully connected layer with softmax computes the probability distributions for each class. The architecture of the proposed framework is shown in Figure 1."}, {"title": "3.1 Preliminaries", "content": "3.1.1 Notations. We denote an attributed graph with N nodes by G = (V, X, A), where V = {v1, ..., vN } is the set of nodes in graph, X \u2208 \\mathbb{R}^{N \\times d} is the matrix consisting of the d-dimensional feature vector of each node, A \u2208 {0, 1}^{N\\times N} is the adjacency matrix. Aij = 1 if nodes vi and vj are connected; otherwise Aij = 0.\nIn this paper, we take graph classification as the target task. Given a set of M graphs G = {G1, G2, ..., GM }, and each graph Gm is associated with a ground-truth class label ym \u2208 C, where C = {1, 2, ..., C} is the set of candidate labels. The graph classification task aims to learn a graph classifier that predicts the estimated label \u0177m for an input graph Gm.\n3.1.2 Graph Normalized Cut. Graph normalized cut is an effective approach for realizing graph clustering. The goal is to construct a partition of the graph into K sets, such that the sets are sparsely connected to each other while the internal structure of the sets exhibits high cohesion [43]. We formalize the objective of the K-way normalized cut as follows:\n$\\displaystyle \\min_{V_1,..., V_K} \\frac{1}{K} \\sum_{k=1}^K \\frac{cut(V_k, \\overline{V_k})}{vol(V_k)}$\nwhere Vk represents the nodes belonging to cluster k, $vol(V_k) = \\sum_{i,j \\in V_k} A_{ij}$ counts the number of edges within cluster k, and $cut(V_k, \\overline{V_k}) = \\sum_{i \\in V_k, j \\in V\\setminus V_k} A_{ij}$ counts the edges between the nodes in cluster k and the rest of the graph [48]. Let P \u2208 {0, 1}^{N\\times K} be the cluster assignment matrix, where K denotes the number of target clusters and Pij = 1 when node i belongs to cluster j. The objective function of the normalized cut can be further defined according to the derivation in [13, 18]:\n$\\displaystyle \\min_{P\\in \\{0,1\\}^{N\\times K}} \\frac{1}{K} Tr(\\frac{P^T L P}{P^T D P})$\n$\\displaystyle \\min_{P\\in \\{0,1\\}^{N\\times K}} Tr(\\frac{P^T L P}{P^T D P})$\nwhere Pk represents the k-th column in P, D is the corresponding degree matrix, and L = D \u2013 A is the graph Laplacian matrix.\nThe optimization problem is NP-hard because the clustering assignment matrix P takes discrete values [47]. Therefore, following the traditional approach of solving the probabilistic approximation of the K-way normalized cut [13, 18], we perform a continuous relaxation for P such that it satisfies Pij \u2208 [0, 1] and Vi, \u2211j Pij = 1.\n3.1.3 Random Walk Graph Kernel. Random walk graph kernel is a kind of kernel function for graph similarity evaluation, whose core idea is to compute the similarity of two input graphs by counting the number of common paths in the two graphs. R-step random walk means that the length of paths formed by the random walk does not exceed R. To efficiently compute the random walk kernel, we follow the generalized framework of computing walk-based kernel [53], and use the direct product graph for equivalence calculation.\nGiven two graphs G = (V, X, A) with N nodes and G' = (V', X', A') with N' nodes, the direct product graph Gx = (Vx, Xx, Ax) is a graph with NN' nodes, each representing a pair of nodes from G and G'. The adjacency matrix Ax is equal to the Kronecker product of the adjacency matrices of G and G', that is $A_x = A \\otimes A'$ [2]. The attribute of node (v, v') in Gx is calculated based on the attribute of node v in G and node v' in G', i.e. $X_x(v,v') = Xv \\otimes Xv'$. Performing a random walk on the direct product graph Gx is equivalent to performing the simultaneous random walks on graphs G and G'. Therefore, The R-step random walk kernel for attributed graphs [19] can be calculated as:\n$\\displaystyle K(G, G') = \\sum_{r=0}^R K_r(G, G')$\n$\\displaystyle K_r(G, G') = \\sum_{i,j=1}^{|V_x|} \\langle X_{x_i}, X_{x_j} \\rangle [A_x]_{ij}^r$\nwhere $X_{x_i}$ denotes the feature of i-th nodes in Gx and the (i, j)-th element of $A_x^r$ represents the number of common walks of length r between the i-th and j-th node in Gx."}, {"title": "3.2 Clustering Assignment Module", "content": "In this module, the underlying idea of our approach stems from related work on graph pooling [67], which progressively creates coarser versions to represent cluster-level interactions by applying a series of compression blocks to the input graph. In each compression block, we first obtain the embedding vector Z \u2208 \\mathbb{R}^{N\\times d'} of nodes by encoder, which can be any model, and we apply GCN [29] as encoder for implementation.\n$\\tilde{A} = D^{-1/2} \\hat{A} D^{-1/2}$ ,\\\n$\\displaystyle Z = f(\\{X, A\\}; \\Theta_{GCN}),$\nwhere $\\hat{A} = A + I_N$ is the adjacency matrix with added self-loop, D is the degree matrix of $\\hat{A}$, and $ \\Theta_{GCN}$ are parameters of the encoder.\nThen, we divide the original input graph into the cluster-level representation based on the generated node embeddings in a trainable manner. Specifically, we define a trainable cluster assignment matrix S to map each node to a corresponding cluster, and each entry Sij represents the probability of node i belonging to cluster j. Considering that the similarity of node features can affect clustering assignment to some extent, node feature embedding is incorporated into the learning process of S. We take Z as input and use a multi-layer perceptron (MLP) with softmax on the output layer to compute S:\n$\\displaystyle S = Softmax(MLP(Z; \\Theta_{MLP})),$\nwhere S satisfies $S_{ij} \\in [0, 1]$ and $\\forall i \\sum_j S_{ij} = 1$, $ \\Theta_{MLP}$ denotes the learnable parameters in the MLP.\nUnlike the unconstrained learning process in [67], we aim to impose constraints on S in order to obtain clustering assignment results that better reflect the clustering characteristics of nodes in the real-world graphs. First, we optimize the learning of S by minimizing an unsupervised loss term $L_{clu}$, which defined on a relaxation formula that approximates the K-way normalized cut (3):\n$\\displaystyle L_{clu} = \\frac{1}{K} Tr(\\frac{S^T L S}{S^T D S}),$\nwhere D is the corresponding degree matrix, and L = D \u2013 A is the graph Laplacian matrix. However, without additional constraints on the assignment matrix S, cluster assignment may fall into a local optimal solution: assigning all nodes to the same cluster. Hence, we introduce an balanced loss term $L_{bal}$ to encourage more balanced and discrete clusters:\n$\\displaystyle L_{bal} = \\frac{1}{K} \\sum_{i=1}^N \\| S_i - \\frac{1}{K} \\|_F,$\nwhere $|| \\cdot ||_F$ indicates the Frobenius norm, N is the number of nodes and K is the number of target clusters.\nIn summary, the optimization objective of this module can be expressed as:\n$\\displaystyle L_{CA} = \\alpha_1 L_{clu} + \\alpha_2 L_{bal},$\nwhere \u03b1\u2081 and \u03b12 control the ratio of the loss terms.\nAssuming the input adjacency matrix in the l-th compression block is $A^{l-1}$, the input node embedding matrix is $Z^{l-1}$, and the computed clustering assignment matrix is $S^l$, we can generate a new coarsened adjacency matrix $A^l$ and a new embedding matrix $X^l$ for next compression block. Specifically, we apply the following two equations:\n$\\displaystyle X^l = S^l{^T} Z^{l-1} \\in \\mathbb{R}^{N_l\\times d},$\n$\\displaystyle A^l = S^l{^T} A^{l-1} S^l \\in \\mathbb{R}^{N_l\\times N_l},$\nwhere $N_l$ denotes the number of target clusters in l-th block and d denotes dimension of node features. By stacking compression blocks, we can obtain AL and XL for cluster-level representation CG, where L is the number of compression blocks. Considering the impact of the enormous edges in the coarsened graph, we propose to filter the edges. Specifically, we define the matrix $Mask \\in \\{0,1\\}^{N_L\\times N_L}$ to filter the edges in the coarsened graph, where NL is the number of nodes in the coarsened graph. If $A_{ij}^L$ exceeds threshold \u03b4\u2081, the element at the corresponding position in Mask is set to 1, otherwise it is set to 0:\n$\\displaystyle Mask_{ij} = \\{\\begin{array}{lr} 1, & if \\ A_{ij}^L > \\delta_1; \\\\ 0, & else, \\end{array}$\nThus, we obtain the filtered adjacency matrix $A^L' = A^L \\odot Mask$ for cluster-level representation, where $\\odot$ is the element-wise product."}, {"title": "3.3 Interactive Patterns Matching Module", "content": "In this module, we aim to learn representative inter-cluster structures and interactions for each class, which we call interactive patterns, to give accurate predictions and reliable explanations.\nFirst, we define a total of T learnable interactive patterns, i.e. $P = \\{P_1, P_2, ..., P_T\\}$, and allocate them evenly to C classes. In order to provide a more understandable explanation, we define each interactive pattern Pt as a combination of the following two parts: (i) randomly initialized feature matrix $X^{P_t}$ with pre-defined size; (ii) the topology $A^{P_t}$ generated from the feature matrix, and the generation process of $A^{P_t}$ is defined as follows:\n$\\displaystyle A^{P_t} = \\sigma (MLP([X_{u}^{P_t}; X_{v}^{P_t}]; \\Theta_{MLP2}))$\nwhere \u03c3(\u00b7) is the Sigmoid function, $\\Theta_{MLP2}$ is trainable parameters of MLP, [\u00b7; \u00b7] is concatenation operation, $X_{u}^{P_t}$ and $X_{v}^{P_t}$ are features of nodes in interactive pattern. Therefore, the generated interactive patterns can be directly used for explanation without the need for additional graph projection or graph generation processes [15, 72].\nThen, for the coarsened graph CG and interactive pattern Pt, we propose to calculate their similarity through graph kernels [3, 28]. The choice of graph kernels can be changed according to the actual application scenario. Here, we choose the R-step random walk graph kernel [21, 53] which compares random walks up to length R in two graphs. Then, the similarity between the coarsened graph CG and the interactive pattern Pt can be expressed as:\n$\\displaystyle sim(C_G, P_t) = K(C_G, P_t),$\nwhere K(CG, Pt) is calculated by equations (4) and (5).\nConsidering the desired representativeness of the interactive patterns for their corresponding classes, we suppose that the learning objective of interactive patterns is to encourage each coarsened graph to approach the interactive patterns belonging to the same class, while moving away from the interactive patterns belonging to other classes. To achieve this, we introduce the multi-similarity loss [55] to constrain learning of patterns:\n$\\displaystyle \\mathcal{L}_{mul} = \\frac{1}{M} \\sum_{m=1}^M (\\frac{1}{\\gamma_1} log (1 + \\sum_{P_i \\in Pos_m} e^{\\gamma_1 (d_{mi} - \\Delta)}) + \\frac{1}{\\gamma_2} \\sum_{P_i \\in Neg_m} log (1 + e^{-\\gamma_2 (d_{mi} - \\Delta)})),$\nwhere Posm denotes the set of interactive patterns belonging to the same class as the coarsened graph CGm, Negm denotes the set of interactive patterns apart from these, dmi denotes the distance between coarsened graph CGm and interactive pattern Pi, \u03b3\u2081 and \u03b3\u2082 control the contributions of different items, and \u0394 represents the margin which controls the distribution range of interactive patterns belonging to the certain class. For the computation of dmi, we apply the distance in kernel space [46]:\n$\\displaystyle d_{mi} = \\sqrt{(K(C_G{_m}, C_G{_m}) + K (P_i, P_i)) - K(C_G{_m}, P_i)}$\nAdditionally, we encourage diversity in interactive patterns by adding the diversity loss, which penalizes interactive patterns that are too close to each other:\n$\\displaystyle L_{div} = \\sum_{c=1}^C \\sum_{P_i, P_j \\in P_c} max(0, sim(P_i, P_j) - \\delta_2)$\nwhere Pc denotes the interactive patterns belonging to class c and \u03b42 is the threshold for similarity measurement.\n$\\displaystyle L_{IPM} = \\alpha_3 L_{mul} + \\alpha_4 L_{div}$"}, {"title": "3.4 Interpretable Classification with interactive patterns", "content": "3.4.1 Classification and Learning Objective. Finally, the T similarity scores between the coarsened graph and each interactive pattern are fed into the fully connected layer to obtain the output logits. Then, the logits processed with softmax to yield the probability distribution hi for a given graph Gi. To ensure the accuracy of the proposed framework, we apply a cross-entropy loss to leverage the supervision from the labeled set:\n$\\displaystyle L_{CE} = \\sum_{i=1}^M CrsEnt(h_i, Y_i)$\nwhere Yi is the true label of input graph. To sum up, the objective function we aim to minimize is:\n$\\displaystyle L = L_{CE} + \\beta_1 L_{CA} + \\beta_2 L_{IPM}$\nwhere LCA and LIPM are loss terms of the clustering assignment module and interactive patterns matching module, \u03b2\u2081 and \u03b22 control the contribution of these loss terms.\n3.4.2 Explainability. From the class perspective, the learned interactive patterns P reveal the cluster-level interaction characteristics of the graphs in each class. From the instance perspective, for the test graph Gt, we can identify the most similar interactive pattern in class \u0177t with Gt as the instance-level explanation:\n$\\displaystyle G_{ex}^t = arg\\max_{P_i \\in P_{\\hat{y}_t}} sim(G_t, P_i)$\nwhere $P_{\\hat{y}_t}$ is the set of interactive patterns belonging to class \u0177t. Since the prediction of Gt is based on several patterns, the instance-level explanation can be several similar patterns in class \u0177t, thereby bringing deeper insights into the graph itself."}, {"title": "4 EXPERIMENTS", "content": "4.1 Experimental Settings\n4.1.1 Datasets. In the experiment, we use five real-world datasets with different characteristics (e.g., size, density, etc.) for graph classification. Additionally, to better demonstrate the explainability provided by our framework, we design two synthetic datasets. The specific information of the datasets is as follows:\n\u2022 Real-world Datasets: To probe the effectiveness of our framework in diffrent domains, we use protein datasets including ENZYMES, PROTEINS [20], D&D [17], molecular dataset MUTAG [61] and scientific collaboration dataset COLLAB [62]. The statistics of the datasets are presented in Appendix A.1.\n\u2022 Synthetic Datasets: To better demonstrate the interpretability of our framework, we design two synthetic datasets: GraphCycle and GraphFive. Their labels are based on the interactive patterns between local structures. GraphCycle consists of two classes: Cycle and Non-Cycle, while GraphFive consists of five classes: Wheel, Grid, Tree, Ladder, and Star. The specific implementation details are presented in Appendix A.1.\n4.1.2 Baselines. We extensively compare our framework with the following three types of baselines:\n\u2022 Widely Used GNNs: We compare the prediction performance with the powerful GNN models including GCN [29], DGCNN [57], Diffpool [67], RWNN [40] and GraphSAGE [23].\n\u2022 Post-hoc Explainable GNNs: We compare the explanation performance with the post-hoc explainable methods including GNNExplainer [66], SubgraphX [70] and XGNN [69].\n\u2022 Interpretable GNNs: We compare the prediction and explanation performance with interpretable models including ProtGNN [72], KerGNN [19], \u03c0-GNN [64], GIB [68], GSAT [38] and CAL [51].\nMore experimental settings will be presented in Appendix A.2"}, {"title": "4.2 Quantitative Analysis", "content": "To validate the effectiveness of our framework, we first compare it with the baselines in terms of prediction and explanation performance on several graph classification datasets.\n4.2.1 Prediction Performance. To demonstrate the effectiveness of our approach in providing accurate predictions, we choose classification accuracy and F1 scores as evaluation metrics, and compare them with widely used GNNs and interpretable GNNs on both real-world and synthetic datasets. We apply three independent runs and report the average results along with the standard deviations in Table 1. From the Table 1, we can observe that:\n\u2022 Our framework achieves superior prediction performance compared to most of widely used GNNs. Specifically, in terms of classification accuracy, our framework outperforms widely used GNNs on six of the seven datasets. Particularly on MUTAG, our framework outperforms widely used models by 5.66%~35.79%. Furthermore, for the dataset in which our framework lagged behind (D&D), our framework only falls behind by 0.5% compared to the best-performing widely used model. For the F1 score metric, our framework surpasses all widely used baselines in two of the seven datasets. Additionally, it achieves second-best performance in three datasets. In the remaining two datasets, it also performs comparably to most of widely used baselines.\n\u2022 Our framework significantly outperforms the leading interpretable GNNs in prediction performance. On four of the seven datasets, our framework exceeds previous interpretable methods in terms of both accuracy and F1 score. On the remaining three datasets, although its accuracy/F1 score is slightly lower than the best-performing interpretable method, it still maintains the best performance in another metric. This demonstrates that our framework can consistently learn high-quality patterns for accurate predictions on different datasets; while simply selecting subgraphs might result in sub-optimal results.\n4.2.2 Explanation Performance. We further compare the explanation performance of our method with that of interpretable methods and post-hoc explainable methods with three evaluation metrics, including explanation accuracy, consistency and silhouette score. We perform three independent runs and report the average results.\n\u2022 Explanation Accuracy. We use trained GNNs to predict the explanations produced by different methods and take the confidence score of the prediction as the accuracy of the explanation [15, 33]. We compare our framework with interpretable methods and post-hoc explainable methods, the results are shown in Figure 2. Compared to previous interpretable methods, our method exhibits the highest explanation accuracy in five out of seven datasets, and achieves the second-best performance in the remaining dataset. Compared to post-hoc explainable methods, our method also achieves the highest explanation accuracy on most datasets.\n\u2022 Consistency. In the two synthetic datasets, we calculate the similarity between the explanations produced by different methods and the ground-truth. Here, we use the normalized results of random walk graph kernel as the measure of similarity. The results are presented in Table 3. Our framework outperforms other baselines by a significant margin across all datasets. This indicates that our framework can provide more accurate explanations.\n\u2022 Silhouette Score. High-quality interactive patterns can tightly cluster instances in dataset. Therefore, we use generated interactive patterns as centers to assign each graph to the nearest interactive pattern and then calculate the silhouette scores [45] to evaluate the compactness and separability of the clusters. We compare our method with another prototype-based approach ProtGNN, and the results are shown in Table 4. Our method consistently achieves better performance on all datasets, which further demonstrates that our framework can obtain more representative patterns."}, {"title": "4.3 Qualitative Analysis", "content": "To qualitatively evaluate the performance, we visualize the obtained interative patterns of our framework.\nFrom class perspective, we present the explanations on the synthetic dataset GraphCycle by visualizing part of the interactive patterns of different classes. The results is shown in Figure 2(a). We can find that our framework manages to learn patterns that are consistent with the ground-truth of \"Cycle\" and \"Non-Cycle\". For comparison, we also show the identified explaintions of another methods (ProtGNN) that can provide class-level explanations, the results are shown in Figure 2(b). It can be observed that the explanations identified by ProtGNN do not exhibit distinctiveness across different classes. The reason may lie in the fact that the GraphCycle dataset does not exhibit distinctive properties in local structures, and the method based on subgraph exploration fails to capture the interactions between local substructures, thus resulting in weaker explanations. Therefore, we believe that our framework is able to unveil representative global patterns. More results of the explanation from class perspective will be presented in Appendix B.\nFrom instance perspective, we identify one or more interaction patterns similar to the input graph in the decision-making process of the model to serve as instance-level explanations."}, {"title": "4.4 Efficiency Study", "content": "In this section, we compare the efficiency of our proposed framework with several interpretable baselines. In Table 5, we show the time required to finish training for each interpretable model. It can be observed that the efficiency of our method is only slightly inferior to KerGNN and \u03c0-GNN. According to the analysis above, our method outperforms both KerGNN and \u03c0-GNN in terms of both prediction performance and explanation performance. Therefore, we believe that the slight additional time cost is worthwhile."}, {"title": "4.5 Ablation Studies", "content": "In this section, we perform ablation studies of our framework to explore the impact of different experimental setups on the effectiveness of the framework and explore the role of different modules. Due to space limitations, we only present a portion of results here. More results will be shown in Appendix C.\n4.5.1 Influence of the Number of Compression Blocks. First, we investigate the effect of the number of compression blocks L and the compression ratio q, where q represents the ratio of the number of nodes after compression to the number of nodes before compression. We alter the values of L and q as {1, 2} and {0.1, 0.2, 0.3, 0.5}. We conduct experiments on GraphCycle dataset, and the results of classification accuracy and explanation accuracy are presented in Figure 3. We can find that when the compression ratio is too high or too low, there is a degradation in both classification accuracy and explanation accuracy. This may be due to the fact that when the compression ratio is too low, the presence of noisy structures may interfere with the extraction of global information, while a high compression ratio may result in the loss of some information. Additionally, we also find that the effect of the number of compression blocks on the results varies with different compression ratios. Therefore, it is crucial to select appropriate number of compression blocks and compression ratios for optimal model performance.\n4.5.2 Influence of the Number of interactive patterns. Then, we vary the number of interactive patterns per class T/C as {2, 4"}]}