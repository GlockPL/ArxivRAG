{"title": "Formal Logic-guided Robust Federated Learning\nagainst Poisoning Attacks", "authors": ["Dung Thuy Nguyen", "Ziyan An", "Taylor T. Johnson", "Meiyi Ma", "Kevin Leach"], "abstract": "Federated Learning (FL) offers a promising solution\nto the privacy concerns associated with centralized Machine\nLearning (ML) by enabling decentralized, collaborative learn-\ning. However, FL is vulnerable to various security threats,\nincluding poisoning attacks, where adversarial clients manipulate\nthe training data or model updates to degrade overall model\nperformance. These attacks can introduce critical malfunctions,\nsuch as biased predictions or reduced accuracy, undermining\nthe integrity and robustness of the global model. Recognizing\nthis threat, researchers have focused on developing defense\nmechanisms to counteract poisoning attacks in FL systems.\nHowever, existing robust FL methods predominantly focus on\ncomputer vision tasks, leaving a gap in addressing the unique\nchallenges of FL with time series data. These tasks, which often\ninvolve sequential dependencies and temporal patterns, have been\nlargely overlooked in the context of poisoning attack defenses.\nIn this paper, we present FLORAL, a defense mechanism\ndesigned to mitigate poisoning attacks in federated learning for\ntime-series tasks, even in scenarios with heterogeneous client data\nand a large number of adversarial participants. Based on our\ninvestigation of the effectiveness of poisoning attack defenses\nwithin the Federated Time Series (FTS) domain, we pinpoint\nthe limitations of mainstream defenses against such attacks.\nUnlike traditional model-centric defenses, FLORAL leverages logic\nreasoning to evaluate client trustworthiness by aligning their\npredictions with global time-series patterns, rather than relying\nsolely on the similarity of client updates. Our approach extracts\nlogical reasoning properties from clients, then hierarchically\ninfers global properties, and uses these to verify client updates.\nThrough formal logic verification, we assess the robustness\nof each client contribution, identifying deviations indicative\nof adversarial behavior. Experimental results on two datasets\ndemonstrate the superior performance of our approach compared\nto existing baseline methods, highlighting its potential to enhance\nthe robustness of FL to time series applications. Notably, FLORAL\nreduced the prediction error by 93.27% in the best-case scenario\ncompared to the second-best baseline. Our code is available at\nhttps://anonymous.4open.science/r/FLORAL-Robust-FTS.", "sections": [{"title": "I. INTRODUCTION", "content": "Federated Learning (FL) has emerged as a promising so-\nlution that enables using data and computing resources from\nmultiple clients to train a shared model under the orchestration\nof a central server [33]. In FL, clients use their data to\ntrain the model locally and iteratively share the local updates\nwith the server, which then combines the contributions of\nthe participating clients to generate a global update. The\nsecurity aggregation mechanism and its distinctive distributed\ntraining mode render it highly compatible with a wide range of\npractical applications that have stringent privacy demands [21],\n[40], [49], [59]. Recently, FL has been demonstrated to be\nefficient in time-series related tasks [3], [10], [48] to securely\nshare knowledge of similar expertise among different tasks\nand protect user privacy. Although FL has many notable char-\nacteristics and has been successful in many applications [2],\n[21], [22], [41], [46], [52], [66], recent studies indicate that FL\nis fundamentally susceptible to adversarial attacks in which\nmalicious clients manipulate the local training process to\ncontaminate the global model [6], [44], [55]. Based on the\nattack's goal, adversarial attacks can be broadly classified\ninto untargeted and targeted attacks. The former aims to\ndeteriorate the performance of the global model on all test\nsamples [9], [14]; while the latter focuses on causing the model\nto generate false predictions following specific objectives of\nthe adversaries [6], [62]."}, {"title": "A. Poisoning Attacks in FL", "content": "Unlike traditional centralized learning, where data is aggre-\ngated in a single location, FL operates under a decentralized\nparadigm where data remains on client devices and only model\nupdates are shared with a central server. This distributed\nnature makes FL susceptible to various types of poisoning\nattacks [25], [39]. A poisoning attack in federated learning\noccurs when an attacker alters the model submitted by a\nclient to the central server during the aggregation process,\neither directly or indirectly, causing the global model to update\nincorrectly [67].\nDepending on the goal of a poisoning attack, we can classify\npoisoning attacks into two categories: (1) untargeted poisoning\nattacks [9], [19], and (2) targeted poisoning attacks [58],\n[64], [70]. Untargeted poisoning attacks aim to make the\nlearned model have a high testing error indiscriminately for\ntesting examples, which eventually results in a denial-of-\nservice attack. Byzantine attack is among the most popular\nsuch attacks [9], [61]. In targeted poisoning attacks, the\nlearned model produces attacker-desired predictions for par-\nticular testing examples, e.g., predicting spam as non-spam\nand predicting attacker-desired labels for testing examples with\na particular trojan trigger (these attacks are also known as\nbackdoor/trojan attacks [44]). In the context of a time-series\ntask, the targeted attack can be adding imperceptible noise to\nthe original sample such that the model predicts the incorrect\nclass [53] or manipulating the prediction into the extreme\nvalue/specific directions [38].\nTo further strengthen the attack, some model poisoning\nattacks are often combined with data poisoning ones [6],\n[44]. In such attacks, adversaries intentionally manipulate their\nlocal model updates before sending them to the central server.\nIn [8], [45], the projected gradient descent (PGD) attack is\nintroduced to be more resistant to many defense mechanisms.\nIn a PGD attack, the attacker projects their model on a small\nball centered around the previous iteration's global model. In\naddition, scaling and constraining-based techniques [15], [26]\nare commonly used to intensify the poisoning effect while\nstealthily bypassing robust aggregators."}, {"title": "B. Defenses against Poisoning Attacks in FL", "content": "Defending against poisoning attacks in Federated Learning\nrequires novel approaches tailored to the unique characteris-\ntics of the FL paradigm. Existing defense strategies can be\nbroadly categorized into (1) robust aggregation mechanisms,\n(2) anomaly detection, and (3) adversarial training. Robust\nAggregation Mechanisms are designed to mitigate the impact\nof malicious updates during the model aggregation process [9],\n[47], [50], [71]. Blanchard et al. [9] proposed the Krum\nalgorithm, which selects updates from a majority of clients\nthat are most similar to each other, thereby excluding po-\ntentially malicious updates. Another approach, Median-based\naggregation [50], [72], takes the median of the updates from\nall clients, which is more resilient to outliers and adversarial\nmanipulations. RLR [47] adjusts the global learning rate based\non the sign information contained within each update per\ndimension and combines the differential privacy noising.\nThe second approach is backdoor detection, which detects\nthe backdoor gradients and filters them before aggregation [9],\n[17], [18], [37], [42], [73]. To begin, Cao et al. [11] introduced\nFLTrust, a method that establishes a trust score for each client\nbased on the similarity of their updates to a trusted server-\nside model. Updates with low trust scores are either down-\nweighted or excluded from the aggregation process. More\nrecently, Zhang et al. [73] proposed to predict a client's model\nupdate in each iteration based on historical model updates and\nthen flag a client as malicious if the received model update\nfrom the client and the predicted model update is inconsistent\nin multiple iterations. In addition Nguyen et al. [42] developed\nan approach that combines anomaly detection with clustering,\ngrouping similar updates and filtering out those that deviate\nsignificantly from the majority.\nTo this end, the effectiveness of these methods in Federated\nTime Series (FTS) scenarios remains unclear, as they lack\ncomponents specifically designed to capture and investigate\nlogical reasoning properties inherent in time-series data. Time\nseries data introduces unique temporal dependencies and\npatterns that may require additional strategies to accurately\ndetect adversarial behavior. Existing defenses, being largely\nmodel-centric, do not account for the temporal relationships\nor dynamic changes in the data, which are critical for ensuring\nrobust performance in FTS. As a result, new approaches that\nintegrate logical reasoning and temporal consistency checks\nare needed to enhance adversarial defenses in such contexts."}, {"title": "III. BACKGROUND", "content": "In this section, we first provide the formulation of FTS\nand the threat model which is addressed in our method;\nthen provide the background on logical reasoning property\ninference and verification the key components of FLORAL."}, {"title": "A. Federated Time Series (FTS)", "content": "This work focuses on time series forecasting, which involves\npredicting future values based on historical data. Formally,\nlet $X_{1:L} = (x_1, ..., x_L) \\in \\mathbb{R}^{L \\times M}$ be a history sequence\nof L multivariate time series, where for any time step t,\neach row $x_t = (X_{t1},...,X_{tM}) \\in \\mathbb{R}^{1\\times M}$ is a multivariate\nvector consisting of M variables or channels. Given a his-\ntory sequence $X_{1:L}$ with look-back window L, the goal of\nmultivariate time series forecasting is to predict a sequence\n$X_{L+1:L++} = (X_{L+1},..., X_{L+T}) \\in \\mathbb{R}^{T \\times M}$ for the future T\ntimesteps.\nTo achieve this in a decentralized and privacy-preserving\nmanner, we consider a federated learning (FL) system where\nN participants collaborate to build a global model $G_T$ that\ngeneralizes well to their future observations. In our FL sys-\ntem, there is a central server S and a client pool $C =$\n$[C_1, C_2,...,C_N]$, where each client i has a dataset of size\n$|D_i| = N_i$. The training procedure involves T rounds of\ninteraction between the server and the clients:"}, {"title": "B. Threat Model", "content": "We consider a FL system consisting of a central server\nS and a pool of clients C with size N, where a fraction\nof these clients are malicious. These malicious clients can\neither operate independently (non-colluded) or be controlled\nby a single adversary A (colluded). In the colluded scenario,\nA controls a subset of compromised clients $C_p$, manipulating\ntheir local training process to execute data poisoning or model\nreplacement attacks with the same poisoning objective.\nIn this system, the malicious clients aim to compromise the\nglobal model $G_T$ by introducing poisoned time-series data\nor tampering with model updates. The primary goal is to\nmanipulate the poisoned model $G'_T$ to output the predictions\nin the poisoning objective that the adversaries expect. In the\nuntargeted case, the objective is to produce predictions with\na high testing error indiscriminately across testing examples,\nwhich is represented as: $max(L(G_T(X), Y))$ where $G_T(X)$\ndenotes the predictions of the model $G_T$ on input X, and Y\nrepresents the true labels. Meanwhile, in the targeted case, the\ngoal is to minimize the loss between the model's predictions\nand the adversarial target labels: $min(L(G_T(X), \\hat{Y}))$, where\n$\\hat{Y}$ is the adversarial target label. The key parameters include:\nAttack Ratio ($\\epsilon$): $\\epsilon = \\frac{C_p}{N}$, where N is the total number\nof clients in the FTS system. In each communication round,\nthe server randomly selects m clients from the client pool C,\nmeaning that the number of malicious clients participating in\na given round can range from 0 to $min(\\epsilon N, m)$.\nAttacker Capabilities: We consider the malicious clients with\nwhite-box attack capability. This means they can manipulate\nlocal training data, where poisoned time-series data $D_p$ is\ninjected into the training set $D'_i$ of each compromised client i.\nIn addition, they can control the local training procedure, and\nmodify local model updates before sending them to the server\nfor aggregation."}, {"title": "C. Temporal Reasoning Property Inference", "content": "Signal temporal logic (STL) [32] is a precise and flexible\nformalism designed to specify temporal logic properties. Here,\nwe first provide the syntax of STL, as defined below in\nDefinition 1."}, {"title": "IV. METHODOLOGY", "content": "Figure 2 provides an illustration of four components that\ncomprise FLORAL: (i) local logic inference; (ii) global logic"}, {"title": "A. Local Logic Inference", "content": "We employ stochastic gradient descent (SGD) [3] as the\noptimization algorithm to update the neural network models\n(line 4, Algorithm 1), where $\\eta$ denotes the learning rate for\nthe gradient descent. The SGD update rule can be substituted\nwith any other gradient descent-based algorithm.\n$G \\leftarrow SGD (G_t, D_i, \\eta)$                                                                                                                  (2)\nAfter each local training process is completed using Equa-\ntion 2, each client $C_i$ will conduct a local logic inference to\nacquire their logic property and submit them to the server.\nGiven a local model of client $C_i$ at round t, i.e., $G_i^t$, and $D_i$\nbe a small sample of centralized server data. During each clus-\ntering process, our method generates the logic property $\\varphi(p_i^t)$\nfor each participating client based on their prediction $Y_i^t$ on\nthis set. Our method uses Equation 1 to infer a logic reasoning\nproperty $\\varphi$ from the client prediction. Recall that any locally\ninferred logic reasoning property must be satisfied by every\ndata point in the client dataset. Moreover, any STL formula\ncan be represented by its equivalent Disjunctive Normal Form\n(DNF), which typically has the form of $P \\vee Q \\vee R \\vee ...$. Each\nclause within a DNF formula consists of variables, literals, or\nconjunctions (e.g., $P := p_1 \\wedge \\neg p_2 \\wedge ... \\wedge p_n$). Essentially,\nthe DNF form defines a range of satisfaction where any\nclause connected with the disjunction operator satisfies the\nSTL formula $\\varphi$. After this step, each client $C_i$ has a logical\nproperty represented $\\{\\varphi^i\\} := \\varphi_1(\\varphi^i) \\wedge \\varphi_2(\\varphi^i) \\wedge ... \\wedge \\varphi_n(\\varphi^i)$\nextracted by logic inference."}, {"title": "B. Global Logic Inference", "content": "Given a set of local logic properties $P =$\n$\\{\\varphi(p_1), \\varphi(p_2), ...,\\varphi(p_m)\\}$ at the tth round, the server\naggregates them to construct the global property $P_g$ that\nall clients should satisfy. The core idea is to group clients\nwith similar properties and derive a global property from\nthese clusters. This approach helps mitigate both targeted\nand untargeted attacks, as malicious clients with properties\nconsiderably different from benign ones can be isolated,\nreducing their impact on the aggregated property $P_g$.\nTo categorize samples based on their styles, we use\nFINCH [56], a hierarchical clustering method that identifies\ngroupings in the data based on neighborhood relationships,\nwithout requiring hyperparameters or predefined thresholds.\nFINCH is particularly suitable for scenarios with an uncer-\ntain number of clusters, making it ideal for clustering client\nproperties. Mathematically, given a set of features F, FINCH\nprovides a set of partitions $L = \\{\\Gamma_1, \\Gamma_2,\\cdots, \\Gamma_i\\}$, where each\npartition $\\Gamma_i = \\{C_1,C_2,\\cdots,C_{r_i} | C_{r_i} > C_{r_i+1} \\forall i \\in L\\}$\nrepresents a valid clustering of F, with $\\Gamma_L$ having the smallest\nnumber of classes. Using FINCH, we group clients based on\ntheir local properties $\\{P_i\\}$ as follows:\n$P \\xrightarrow[]{FINCH} \\Gamma_{TL}^k = \\{C_i\\}_{i=1}^K \\ \\mid \\ r_{i,k} \\in \\mathbb{R}^{m*K}$                                                                                                 (3)\nHere, $\\{C_i\\}_{i=1}^K$ is a set of clusters and r is the cluster\nassignment metric, where $r_{i,k} \\in \\mathbb{R}^{m*K}$ and $r_{i,k} = 1$ if\n$i \\in k$ else $r_{i,k} = 0$. This technique implicitly clusters clients\nwith similar logical properties, as properties from different\ndomains are less likely to be adjacent. A key advantage of\nthe FLORAL framework is its dynamic assignment of client\nmodels to clusters, allowing the aggregation process to adapt\nto changes in logic properties over time. Clients with similar\ntemporal reasoning properties are grouped, while those with\ndiffering properties are not combined. Next, we will discuss\nhow to extract cluster property for each cluster $C_i$.\nCluster Property Inference. For a given cluster $C_i =$\n$\\{\\varphi(p_i^j)| \\forall j \\in [1,m] \\wedge r_{i,i} = 1\\}$, where local property $\\varphi(p_i^j)$\nis from client $j^{th}$ in cluster $C_i$, the corresponding cluster\nproperty $P_{L_i}$ is computed as:\n$P_{L_i} := \\varphi(\\overline{p}), \\overline{p} := \\frac{1}{|C_i|}\\sum_{j=1}^{|C_i|}P_j$                                                                                          (4)\nIn Equation 4, $\\overline{p}$ represents the average of the properties\ninferred by the local clients within the cluster, and $P_{L_i}$\ndenotes the cluster property expressed in the same STL (Signal\nTemporal Logic) syntax as the local models. The rationale\nbehind this approach is to derive a representative property\nfor the cluster by aggregating the individual properties of\nthe clients. By averaging the properties, we create a cluster\nproperty that captures the commonalities among clients in the\ncluster, thus providing a robust and collective representation\nof their logic properties.\nGlobal Property Inference. Given a set of cluster properties\n$\\{P_{L_i} = \\varphi(\\overline{p}_{L_i})| \\forall i \\in [1,K]\\}$, the global property is then\ncalculated as the median of these parameters from cluster\nproperties.\n$P_g := \\varphi(\\overline{p}), \\overline{p} := Med(\\overline{p}_{L_i}),\\forall i \\in [1, K],$                                                                                            (5)\nThe idea of using clustered clients is popular in the context\nof heterogeneous data and non-IID (non-independent and\nidentically distributed), where the local properties may vary\nwidely. Motivated by this, we construct the global property\nby treating each cluster property equally instead of each local\nproperty. This helps the global property to reflect the properties\nof that group's data better, which results in better model\ngeneralization within each group and avoids a one-size-fits-\nall property that may not suit clients with vastly different data\ndistributions. Since median use is less sensitive to extreme\nvalues or outliers, we can minimize the influence of adversarial\nclients, whose properties deviate significantly from the benign\nones. This aggregation process aligns to build a global model\nthat is resilient to both targeted and untargeted attacks, as\nit reduces the impact of malicious clients whose properties\ndeviate significantly from the benign ones."}, {"title": "C. Global Property Verification", "content": "Given a global property $\\varphi(\\overline{p})$, this component assesses the\nlevel of satisfaction for each percentage of network predic-\ntions, denoted by $\\hat{Y} = (\\hat{Y}_{L+1},...,\\hat{Y}_{L+T})$, with respect to\na specified property $\\varphi$. The property $\\varphi$ is induced by the\ninput sequence $X = (x_1,...,x_L)$, and its satisfaction level\nindicates how well the predicted sequence $\\hat{Y}$ for the next T\ntime-steps adheres to the property when conditioned on the\ninput sequence X. This framework enables us to quantify\nthe degree to which the network's predictions align with the\ndesired property. We formalize the concept of a robustness\nscore $\\theta$ in Definition 4.\nDefinition 4 (Robustness Score $\\theta$). Given a network's predic-\ntion $\\hat{Y} = (\\hat{Y}_{L+1},..., \\hat{Y}_{L+T})$, logical property $\\varphi$ and scoring\nfunction S, robustness score $\\theta$ quantify the degree to which\nthe predicted sequence $\\hat{Y}$ adheres to the specified property $\\varphi$.\nFormally,\n$\\theta := S(\\varphi, \\hat{Y}) = \\{\\rho(\\varphi, y, t)| \\forall t \\in [L + 1, L + T]\\},$                                                                                         (6)\nwhere $\\rho(\\varphi,x,t)$ maps an STL formula $\\varphi$, a signal trace y at\ntime t, to a value.\nDepending on the selection for the scoring function, for each\nproperty, $\\rho$ can return a binary value of satisfaction status or\nreturn a real value for the satisfaction level [20], [32]. We\nprovide a practical example of STL property verification in\nExample 2.\nFor each training round, after the robustness score is cal-\nculated using Definition 4, the server collects a set of scores\nfor each client, denoted as $\\theta^t := \\{\\theta_1^t, \\theta_2^t, ..., \\theta_n^t\\}$. To convert\nthe robustness score $\\theta$ from qualitative semantics (i.e., boolean\nvalues) to numerical values, we compute the percentage of the\n'True' signal predicate, denoted by T. Using this verification\nscore helps assess the trustworthiness of each client because\nthe global property represents the collective standards and\nrequirements derived from all clients in the system. Therefore,\nhigher scores $\\theta$ indicate that a client's predictions align well\nwith these global properties or at least with the majority of\nbenign clients, suggesting that the client's model is reliable and\ntrustworthy, i.e., more likely to be a benign client. Conversely,\nlower scores $\\theta$ may reveal discrepancies or issues with the\nclient's model or data, potentially indicating anomalies or\nmalicious behavior, and thus warrant further investigation.\nClients whose predictions do not satisfy the global property\nmay cause significant deviations from the global objective and\nthe predictions of benign clients, thereby affecting the overall\nmodel performance and reliability. Over time, benign clients\nwill increasingly agree on the training objective, leading their\nmodels to become more similar and showing higher alignment\nin their predictions when given the same dataset. As a result,\nthese benign clients should have higher verification scores $\\theta$,\nreflecting their consistent adherence to the global property and\nreinforcing their trustworthiness.\nFor each training round, after verification, the server collects\na set of robustness scores for each client, denoted as $\\theta^t :=$\n$\\{\\theta_1^t, \\theta_2^t, ..., \\theta_n^t\\}$. These scores are used to detect malicious\nclients. The intuition is that benign local models align with\nthe global model objective and share the same learning goals,\nwhile malicious models often distort the global model's behav-\nior, leading to discrepancies in their predictions. Consequently,\na higher robustness score indicates greater trustworthiness.\nDetecting and excluding updates from malicious clients is\ncrucial to prevent the global model from being poisoned.\nTo further strengthen the reliability of the robustness score,\nhistorical information is used to update scores cumulatively.\nSpecifically, the score for client i is updated as follows:\n$\\theta_i^t = \\frac{f_{i-1}}{f_i} \\theta_i^t + \\frac{1}{f_i} \\theta_i^t,$                                                                                              (7)"}, {"title": "D. Malicious Client Detection", "content": "To identify and filter out malicious clients, we use a binary\nmask $M\\in \\mathbb{R}^m$, where each element $M_i$ indicates whether\na client i is considered malicious. The mask is defined as\nfollows:\n$M_i = \\begin{cases}\n1 & \\text{if } \\theta_i \\geq \\gamma \\max(\\theta_i),\\\\\n0 & \\text{otherwise.}\n\\end{cases}$                                                                                                                              (8)\nHere, $\\gamma$ is a threshold parameter that determines the cutoff for\nfiltering out malicious models. Note that the trustworthy scores\n$\\theta$ are normalized before applying Equation 8. The rationale\nbehind this approach is that models producing predictions\nwith significantly lower robustness scores are more likely to\nbe compromised or adversarial, especially if they are in the\nminority. The assumption is that the properties of poisoned\nmodels, which are often outliers or deviate from the global\nproperty, will exhibit lower scores compared to the majority\nof benign clients. The robustness threshold for each training\nround is determined based on the distribution of the robustness\nscores across all clients. This threshold helps in distinguishing\nbetween reliable and unreliable client updates.\nTo ensure the integrity of the global model, only updates\nfrom non-malicious clients are used for aggregation. The\nglobal model $w_t$ at round t is updated as follows:\n$G_t = A(G_{t-1}; \\{\\nabla_i \\ \\mid \\forall i \\in [1, m] \\text{ and } M_i = 1\\}),$                                                                                         (9)\nwhere A represents the aggregation function used by the\nserver to combine the updates from the selected non-malicious\nclients. In this formulation, $\\nabla_i$ denotes the model update\nfrom client i. By excluding updates from clients marked as\nmalicious (i.e., those with $M_i = 0$), the server can ensure that\nthe global model $w_t$ is not adversely affected by compromised\nor unreliable client contributions. This method enhances the\nrobustness of the global model and helps maintain its perfor-\nmance by focusing on contributions from trustworthy clients."}, {"title": "V. EXPERIMENTS", "content": "In this section, we evaluate the performance of FLORAL\nagainst various poisoning attack scenarios, including untar-\ngeted and targeted attacks. We show that FLORAL outperforms\nstate-of-the-art FL defenses including Krum/Multi-krum [9],\nFoolsGold [17], RFA [50], RLR [47], FLAME [42], and\nFLDetector [73] in mitigating these attacks. We used an Ama-\nzon EC2 g5.4xlarge instance for all experiments, equipped\nwith one NVIDIA A10G Tensor Core GPU, 16 vCPUs, 64\nGB of memory, and 24 GB of dedicated GPU memory."}, {"title": "A. Experiment Setup", "content": "In this subsection, we first describe the datasets and models\nused in this work, followed by details on the attack settings,\nbaselines, evaluation metrics, and FL simulation.\nDatasets. In this work, we conducted experiments using two\ntime-series datasets which are LTE Physical Downlink Control\nChannel (PDCCH) measurements [48] and Federal Highway\nAdministration [16]. The PDCCH dataset was collected from\nthree different base stations in Barcelona, Spain, which is\npublicly accessed by Perifanis et al. [48]. We further separate\ndata from these three stations into 30 sub-clients to simulate\nthe FL environment. 11 features related to downlink and\nuplink traffic are provided for each site. Following the settings\nby Perifanis et al. [48], the objective is to predict the first\nfive measurements for the next timestep using as input the\nobservations with a window of size $\\tau$ = 10 per base station.\nThis is considered a multi-variate time series regression task.\nThe FHWA dataset is a real-world dataset of highway traffic\ndata. We obtained a publicly available dataset from the Fed-\neral Highway Administration (FHWA 2016) and preprocessed\nhourly traffic volume from multiple states. After preprocessing\nand excluding low-quality and missing data, we continue with\nthe final data from 15 states, this setting is followed [3].\nFurther, we design a testing scenario where a neural network\nis trained to predict the traffic volume for the next $\\tau$ = 24\nconsecutive hours based on the past traffic volume at a location\nover the previous five days. This is considered a univariate time\nseries regression task. In both datasets, we specifically focus\non the operational range property, as outlined in Table I, which\ncaptures important aspects of the traffic volume dynamics for\neach client during two-hour windows. For these two datasets,\nwe focus on using Operational Range (Table I) for logic\nreasoning inference and qualitative semantics for verification.\nAttack settings. We evaluate different defenses under both\nuntargeted attacks and targeted attacks.\nRegarding untargeted attacks, we consider Gaussian Byzan-\ntine attack followed by Fang et al. [15], where for a byzantine\nclient i in iteration r, the message to be uploaded is set\nto follow a Gaussian distribution, i.e., $\\nabla \\sim N(0,\\sigma^2)$.\nThis attack randomly crafts local models, which can mimic\ncertain real-world scenarios where compromised devices might\nproduce erratic or unexpected updates.\nRegarding targeted attacks, we consider the case where\nmalicious clients use a flip attack to manipulate the training\ndata, i.e., data poisoning. Specifically, the adversary aims to\ndisrupt a machine learning model by modifying a subset of\ndata points to their extreme target values [38]. This attack\nis conducted by computing the distance of each target value\n$Y_t \\in Y$ from the nearest boundary (i.e., $Y_{min}, Y_{max}$) of\nthe feasibility domain, then selecting the points with the\nlargest distances and flipping their target values to the opposite\nextremes. Under this attack, the victim model shifts the target\nvariable to the other side's extreme value of the feasibility\ndomain. In FL, to make poisoning attacks stronger and\nmore stealthy against defenses, adversaries often combine\ndata poisoning (as discussed above) with model poisoning\ntechniques [6], [44]. We consider three such model poisoning\ntechniques: Constrain-and-scale, Projected Gradient Descent\n(PGD), and Model Replacement attacks [6], [44], which are\ncombined with data poisoning to strengthen the poisoning\neffect. Consequently, when evaluating targeted attacks, we\nexamine four strategies: (i) Targeted Attack (without model\npoisoning), (ii) PGD Attack (Targeted Attack combined with\nPGD), (iii) Constrain-and-Scale Attack (Targeted Attack com-\nbined with Constrain-and-Scale), and (iv) Model Replacement\nAttack (Targeted Attack combined with Model Replacement).\nDefense baselines. We implement seven representative FL\ndefenses for byzantine attacks and targeted attacks in-\ncluding Krum/Multi-Krum [9], RFA [50], Foolsgold [17],\nFLAME [42], RLR [47], and FLDectector [73]. We provide\ndetailed parameter settings in the Appendix.\nEvaluation metrics. We evaluate the model performance\nusing the Mean Absolute Error (MAE) and Mean Squared\nError (MSE). MAE measures the absolute difference between\nthe expected and predicted output. MSE measures how far\npredictions are from the ground truth values using the Eu-\nclidean distance. The considered metrics are defined as fol-\nlows: $MSE = \\sum_{i=1}^N (Y_i-\\hat{Y}_i)^2$; and $MAE = \\sum_{i=1}^N |Y_i-\\hat{Y}_i|$.\nSimulation setting. We simulate an FL system with a total\nof N clients; in each training round, the server randomly\nselects k% of clients to participate, a process known as client\nsampling. Unless otherwise specified, N is set to 30 and 100\nfor the PDCCH and FHWA datasets, respectively, with k fixed\nat 50%. The number of communication rounds is set to 20\nfor both datasets, with 3 local training epochs per round. For\nall experiments and algorithms, we use SGD with consistent\nlearning rates and a batch size of 128. A vanilla RNN model\nis employed as the backbone network. More details can be\nfound in our Appendix."}, {"title": "B. Results", "content": "First", "42": [45], "Attack": "We first evaluate our\nmethod against Byzantine attacks and then compare it with the\nstate-of-the-art defenses for both full and partial scenarios. In\na full scenario, all clients participate in the training while in\nthe partial scenario, the server S randomly selects m clients to\nparticipate in the training each round. We show the results in\nTable II and highlight the best in bold. As presented, most\ndefenses failed to mitigate the effect of malicious clients.\nThe performance results show that our method achieves the\nbest results in both full and partial data scenarios across the\nPDCCH and FHWA datasets. In contrast, methods like Multi-\nKrum and RLR show considerable performance"}]}