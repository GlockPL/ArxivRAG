{"title": "FAKESHIELD: EXPLAINABLE Image ForGERY DE-\nTECTION AND LOCALIZATION VIA MULTI-MODAL\nLARGE LANGUAGE MODELS", "authors": ["Zhipei Xu", "Xuanyu Zhang", "Runyi Li", "Zecheng Tang", "Qing Huang", "Jian Zhang"], "abstract": "The rapid development of generative AI is a double-edged sword, which not only\nfacilitates content creation but also makes image manipulation easier and more dif-\nficult to detect. Although current image forgery detection and localization (IFDL)\nmethods are generally effective, they tend to face two challenges: 1) black-box\nnature with unknown detection principle, 2) limited generalization across diverse\ntampering methods (e.g., Photoshop, DeepFake, AIGC-Editing). To address these\nissues, we propose the explainable IFDL task and design FakeShield, a multi-\nmodal framework capable of evaluating image authenticity, generating tampered\nregion masks, and providing a judgment basis based on pixel-level and image-\nlevel tampering clues. Additionally, we leverage GPT-40 to enhance existing\nIFDL datasets, creating the Multi-Modal Tamper Description dataSet (MMTD-\nSet) for training FakeShield's tampering analysis capabilities. Meanwhile, we\nincorporate a Domain Tag-guided Explainable Forgery Detection Module (DTE-\nFDM) and a Multi-modal Forgery Localization Module (MFLM) to address var-\nious types of tamper detection interpretation and achieve forgery localization\nguided by detailed textual descriptions. Extensive experiments demonstrate that\nFakeShield effectively detects and localizes various tampering techniques, offer-\ning an explainable and superior solution compared to previous IFDL methods.", "sections": [{"title": "INTRODUCTION", "content": "With the rapid development of AIGC, powerful image editing models have provided a breeding\nground for convenient image tampering, blurring the boundaries between true and forgery. People\ncan use AIGC image editing methods (Rombach et al., 2022; Zhang et al., 2023; Suvorov et al.,\n2022; Mou et al., 2023) to edit images without leaving a trace. Although it has facilitated the work\nof photographers and illustrators, AIGC editing methods have also led to an increase in malicious\ntampering and illegal theft. The authenticity of images in social media is difficult to guarantee,\nwhich will lead to problems such as rumor storms, economic losses, and legal concerns. Therefore,\nit is important and urgent to identify the authenticity of images. In this context, the image forgery\ndetection and localization (IFDL) task aims to identify whether an image has been tampered with\nand locate the specific manipulation areas. It can be widely applied in the real world, such as filtering\nfalse content on social media, preventing the spread of fake news, and court evidence collection.\nState-of-the-art IFDL methods have utilized well-designed network structures, elaborate network\nconstraints, and efficient pre-training strategies to achieve remarkable performance (Yu et al., 2024b;\nMa et al., 2023; Dong et al., 2022). However, previous IFDL methods face two key problems, lim-\niting their practicality and generalizability. First, as shown in Figure 1(a), most existing IFDL\nmethods are black-box models, only providing the authenticity probability of the image, while the\nprinciple of detection is unknown to users. Since the existing IFDL methods cannot guarantee\nsatisfactory accuracy, manual subsequent judgment is still required. Given that the information\nprovided by the IFDL methods is insufficient, it is difficult to support the human assessment and\nusers still need to re-analyze the suspect image by themselves. Second, in real-world scenarios,\ntampering types are highly diverse, including Photoshop (copy-and-move, splicing, and removal),"}, {"title": "RELATED WORKS", "content": ""}, {"title": "IMAGE FORGERY DETECTION AND LOCALIZATION", "content": "Prevailing IFDL methods mainly target at the localization of specific manipulation types (Wu et al.,\n2022; Salloum et al., 2018; Islam et al., 2020). In contrast, universal tamper localization meth-\nods (Kwon et al., 2021; Wu et al., 2019; Hu et al., 2023; Li et al., 2024; Yu et al., 2024a; Zhang\net al., 2024b) aim to detect artifacts and irregularities across a broader spectrum of tampered images.\nFor instance, MVSS-Net (Dong et al., 2022) utilized multi-scale supervision and multi-view feature\nlearning to simultaneously capture image noise and boundary artifacts. OSN (Wu et al., 2022)\nemployed a robust training strategy to overcome the difficulties associated with lossy image pro-\ncessing. HiFi-Net (Guo et al., 2023) adopted a combination of multi-branch feature extraction and\nlocalization modules to effectively address alterations in images synthesized and edited by CNNs.\nIML-ViT (Ma et al., 2023) integrated Swin-ViT into the IFDL task, employing an FPN architecture\nand edge loss constraints to enhance its performance. DiffForensics (Yu et al., 2024b) adopted a\ntraining approach akin to diffusion models, strengthening the model's capacity to capture fine image\ndetails. Additionally, some researchers (Zhang et al., 2024a;c; Asnani et al., 2023) have pursued\nproactive tamper detection and localization by embedding copyright and location watermarks into\nimages/audio/videos preemptively. However, despite their acceptable performances, these IFDL\nmethods cannot explain the underlying principles and rationale behind their detection and local-\nization judgments, offering no interaction. Moreover, they suffer from limited generalization and\naccuracy, exhibiting significant performance disparities across different testing data domains."}, {"title": "LARGE LANGUAGE MODEL", "content": "Large language models (Dubey et al., 2024; OpenAI, 2023) have garnered global attention in re-\ncent years for their exceptional instruction-following and text-generation abilities. Based on the\nTransformer architecture, LLMs are pre-trained on massive datasets, allowing them to accumulate\nbroad world knowledge that enhances their ability to generalize across a wide range of downstream\ntasks. Subsequently, some researchers (Li et al., 2022) expanded LLMs' powerful understanding\nand world knowledge to the visual domain by incorporating image encoders and projection layers,\nwhich enable images encoded into tokens that align with the text. Some recent works (Chen et al.,\n2023a; Wang et al., 2023; Chen et al., 2023b) equipped M-LLMs with enhanced visual understand-\ning capabilities by expanding the visual instruction datasets and increasing the model size during\nfine-tuning. Currently, M-LLMs demonstrate impressive performance across various downstream\ntasks. LISA (Lai et al., 2024) integrated SAM (Kirillov et al., 2023) with M-LLM to implement rea-\nsoning segmentation, enabling the generation of masks from text descriptions. GLaMM (Rasheed\net al., 2024) further enhanced this by using a more advanced region image encoder to improve text-\nto-mask grounding. Additionally, some studies (Yang & Zhou, 2024; Zhang et al., 2024d) have\nexplored the application of M-LLMs in DeepFake detection. For instance, (Zhang et al., 2024d) in-\ntroduced the DD-VQA dataset, combining a manual inference process for rating real and fake faces\nthat can be distinguished using common sense. However, using M-LLMs to realize universal tamper\nlocalization and detection remains unexplored."}, {"title": "METHODOLOGY", "content": ""}, {"title": "CONSTRUCTION OF THE PROPOSED MMTD-SET", "content": "Motivation: Most existing IFDL datasets consist of a single visual modality, lacking training visual-\nlanguage samples adapted to M-LLMs. The challenge of constructing our MMTD-Set lies in ac-\ncurately translating the visual tampering information from the existing IFDL image datasets into\nprecise textual descriptions. To address this challenge, our core contributions focus on two aspects:\n(1) We leverage GPT-40 to generate text description and provide both the tampered image and its\ncorresponding mask to GPT-40, enabling it to accurately identify the tampered location. (2) For each"}, {"title": "OVERALL FRAMEWORK OF FAKESHIELD", "content": "Our goals involve two issues: 1): Utilizing the textual understanding ability and world prior knowl-\nedge of the M-LLM to analyze and judge the authenticity of tampered images; 2): Adopting the\nanalysis and interpretation of tampered images to assist the segmentation model in pinpointing the"}, {"title": "DOMAIN TAG-GUIDED EXPLAINABLE FORGERY DETECTION MODULE", "content": "Motivation: In real-life scenarios, images can be tampered with and attacked through various meth-\nods, including copy-move, splicing, removal, DeepFake, and AIGC-based methods. However, these\ntampered images have different distribution characteristics, and domain differences, making it dif-\nficult to apply a single IFDL method to all forgery data. For example, DeepFake focuses on face\nmodification, often resulting in partial blurring of the face, as well as unnatural appearances of the\nlips, teeth, and eyes. In contrast, tools like PhotoShop (splicing, copy-move, removal) tend to leave\nnoticeable artifacts at the edges of the tampered areas. In the case of AIGC-Editing, the characteris-\ntic is blurring within the tampered region, which frequently alters or obscures some texture details.\nTo mitigate these significant domain discrepancies, inspired by (Sanh et al., 2022), we introduce\nthe Domain Tag Generator(DTG), which utilizes a specialized domain tag to prompt the model to\ndistinguish between various data domains.\nFirst, the original image $I_{ori}$ is input into a classifier $G_{dt}$ to obtain the domain tag $T_{tag}$. Specifically,\nwe classify all common tampering types into three categories: Photoshop-based editing, DeepFake,\nand AIGC-based tampering, and use the template \u201cThis is a suspected {data domain}-tampered"}, {"title": "MULTI-MODAL FORGERY LOCALIZATION MODULE", "content": "Motivation: Although $O_{det}$ provides a textual description of the tampered area, it lacks preci-\nsion and intuitive clarity. To address this issue, we aim to transform $O_{det}$ into an accurate binary\nmask, providing a clearer and more accurate representation of the tampered region. Existing prompt-\nguided segmentation algorithms (Kirillov et al., 2023; Lai et al., 2024) struggle to capture the seman-\ntics of long texts and hard to accurately delineate modified regions based on detailed descriptions.\nInspired by (Lai et al., 2024), we propose a Tamper Comprehension Module (TCM), which is\nan LLM serving as an encoder aligns long-text features with visual modalities, enhancing SAM's\nprecision in locating the forgery areas. To generate the prompt fed into SAM, following (Lai et al.,\n2024), we introduce a specialized token <SEG>.\nAs illustrated in Fig.3, the tokenized image $T_{img}$ and the tampered area description $O_{det}$ are input\ninto the TCM $C_t$. Then, we extract the last-layer embedding of TCM and transform it into $h_{<SEG>}$\nvia an MLP projection layer. Simultaneously, the original image $I_{ori}$ is processed through the SAM\nencoder $S_{enc}$ and decoder $S_{dec}$, where $h_{<SEG>}$ serve as a prompt for $S_{dec}$ guiding the generation of\nthe mask $M_{loc}$.\n$E_{mid} = S_{enc}(I_{ori})$\n$h_{<SEG>} = Extract(C_t (T_{img}, O_{det}))$\n$M_{loc} = S_{dec}(E_{mid} | h_{<SEG>})$,\nwhere $E_{mid}$ represents the intermediate features of SAM, and $Extract(\u00b7)$ denotes the operation of\nextracting the last-layer embedding corresponding to the <SEG> token. Similar to DTE-FDM, we\nalso apply LoRA fine-tuning to MFLM for greater efficiency. With the integration of TCM, SAM\nwill achieve more precise localization of the forgery areas."}, {"title": "TRAINING OBJECTIVES", "content": "The two submodules of our FakeShield are trained end-to-end separately. For DTE-FDM, the\ndomain tag generator utilizes cross-entropy loss $l_{ce}$ as its training objective, enabling it to distinguish\nbetween different data domains. Following the approach of LLaVA (Liu et al., 2024), our LLM's\ntraining objective is the autoregressive text generation cross-entropy loss. The training target of\nDTE-FDM $l_{det}$ can be formulated as:\n$l_{det} = l_{ce}(\\hat{O}_{det}, O_{txt}) + \\lambda \u00b7 l_{ce}(T_{tag}, T_{tag})$,\nwhere $l_{ce}$ represents cross-entropy loss, and $\\lambda$ denotes the weight used for balancing different loss\ncomponents. For MFLM, we apply $l_{ce}$ to constrain TCM to produce high-quality prompt $y_{txt}$ with\n<SEG> token. Meanwhile, we use a linear combination of binary cross-entropy loss $l_{bce}$ and dice\nloss $l_{dice}$ to encourage the output of MFLM $M_{loc}$ to be close to the GT mask $M_{loc}$. Given the\nground-truth prompt $y_{txt}$ (such as \u201cIt is <seg>\u201d) and mask $M_{loc}$, our training losses for MFLM\n$l_{loc}$ can be formulated as:\n$l_{loc} = l_{ce}(\\hat{y}_{txt}, y_{txt}) + \\alpha\u00b7 l_{bce}(M_{loc}, M_{loc}) + \\beta\u00b7 l_{dice}(M_{loc}, M_{loc})$,\nwhere $\\alpha$ and $\\beta$ are weighting factors used to balance the respective losses. $l_{ce}, l_{bce}$, and $l_{dice}$ refer\nto cross-entropy loss, binary cross-entropy loss, and dice loss (Sudre et al., 2017) respectively."}, {"title": "EXPERIMENT", "content": ""}, {"title": "EXPERIMENTAL SETUP", "content": "Dataset: We employ the dataset construction method outlined in Section 3.1 to build the train-\ning and test sets of the MMTD-Set. For the training set, we utilize PhotoShop tampering (e.g.,\nCASIAv2 (Dong et al., 2013), Fantastic Reality (Kniaz et al., 2019)), DeepFake tampering (e.g.,\nFFHQ, FaceApp (Dang et al., 2020)), and some self-constructed AIGC-Editing tampered data as\nthe source dataset. For the testing set, we select several challenging public benchmark datasets\nincluding PhotoShop tampering (CASIAv1+ (Dong et al., 2013), Columbia (Ng et al., 2009),\nIMD2020 (Novozamsky et al., 2020), Coverage (Wen et al., 2016), DSO (De Carvalho et al., 2013),\nKorus (Korus & Huang, 2016)), DeepFake tampering (e.g., FFHQ, FaceApp (Dang et al., 2020)),\nand some self-generated AIGC-Editing data. See the Appendix for more details.\nState-of-the-Art Methods: To ensure a fair comparison, we select competitive methods that provide\neither open-source code or pre-trained models. To evaluate the IFDL performance of FakeShield,\nwe compare it against SPAN (Hu et al., 2020), MantraNet (Wu et al., 2019), OSN (Wu et al., 2022),\nHiFi-Net (Guo et al., 2023), PSCC-Net (Liu et al., 2022), CAT-Net (Kwon et al., 2021), and MVSS-\nNet (Dong et al., 2022), all of which are retrained on the MMTD-Set for consistency with the same\ntraining setup. For DeepFake detection, CADDM (Dong et al., 2023) and HiFi-DeepFake (Guo\net al., 2023) are chosen as comparison methods. Additionally, to assess the explanation ability of\nFakeShield, we compare it with open-source M-LLMs such as LLaVA-v1.6-34B (Liu et al., 2024),\nInternVL2-26B (Chen et al., 2024), and Qwen2-VL-7B (Wang et al., 2024), as well as the closed-\nsource model GPT-40 (OpenAI, 2023).\nEvaluation Metrics: For detection, we report image-level accuracy (ACC) and F1 scores. We use\nIntersection over Union (IoU) and F1 scores for localization. To evaluate interpretability, we use\nCosine Semantic Similarity (CSS) to assess the similarity between the predicted text and ground\ntruth text by calculating the cosine similarity between their high-dimensional semantic vectors. For\nthe IFDL, a default threshold of 0.5 is applied unless otherwise specified.\nImplementation Details: On the MMTD-Set, we initially fine-tune the M-LLM using LoRA\n(rank=128, alpha=256), such as LLaVA-v1.5-13B (Liu et al., 2024), while simultaneously training\nthe Domain Tag Generator with full parameters. The model is trained for 10 epochs on 4 NVIDIA\nA100 40G GPUs, with a learning rate of $2\u00d710^{-4}$ and a batch size of 24. Afterward, we fine-tune\nthe Tamper Comprehension Module and Segment Anything Model (Kirillov et al., 2023) with LoRA\n(rank=8, alpha=16), training for 24 epochs on the same hardware configuration, with a learning rate\nof $3\u00d710^{-4}$ and a batch size of 48."}, {"title": "COMPARISON WITH IMAGE FORGERY DETECTION METHOD", "content": "To verify the superiority and generalization of our method on the image forgery detection\ntask, we test the detection accuracy on MMTD-Set (PhotoShop, DeepFake, AIGC-Editing).\nAs shown in Table 1, our FakeShield almost achieves optimal performance across various\ntampering and testing data domains. For example, our method outperforms the second-best"}, {"title": "COMPARISON WITH M-LLMS", "content": "To assess the quality of explanation text generation, we employ the tampered area descriptions\ngenerated by GPT-40 as ground truth on the MMTD-Set(PhotoShop, DeepFake, AIGC-Editing),\nusing cosine semantic similarity (CSS) to compare the performance of pre-trained M-LLMs against\nFakeShield. The results, presented in Table 3, demonstrate that our approach consistently achieves\nthe best performance across nearly all test sets. For instance, on the DSO, our method attains a CSS\nscore of 0.8873, significantly surpassing the second-best result from InternVL2-26B, which only\nachieves 0.6484. A selection of model outputs is illustrated in the Appendix.\nIt is important to highlight that some pre-trained M-LLMs still exhibit a degree of proficiency in de-\ntecting tampered content. For instance, in cases involving blatant violations of physical laws due to\ntampering, these M-LLMs leverage their pre-training knowledge to make reasonably correct judg-\nments. However, they struggle to perform more precise analyses, such as identifying inconsistencies\nin lighting or perspective, leading to lower overall accuracy."}, {"title": "COMPARISON WITH IMAGE FORGERY LOCATION METHOD", "content": "To assess the model's capability to locate tampered regions, we conduct comparisons to establish\nsome competitive IFDL methods on MMTD-Set (PhotoShop, AIGC-Editing). As present in Table 4,\nour method consistently surpasses the performance of others across almost all test datasets. For\ninstance, on the IMD2020, our method outperforms the suboptimal method, OSN, with notable\nadvantages of 0.12 in IoU and 0.1 in F1 score. On the CASIA1+, we also lead the sub-optimal\nmethod OSN with an IoU of 0.07 and an F1 of 0.09.\nAdditionally, subjective comparisons of several methods are illustrated in Figure 4, where it is ev-\nident that our approach precisely captures the boundaries of tampered areas, producing clean and\ncomplete segmentations. In contrast, methods like PSCC-Net exhibit dispersed attention over the\nimage, resulting in blurred segmentations and an overly broad predicted tampering range. Notably,\nas our segmentation module MFLM is based on the pre-trained visual segmentation model SAM, it"}, {"title": "ROBUSTNESS STUDY", "content": "With the widespread use of the Internet and social media, individuals are increasingly receiving im-\nages degraded by transmission artifacts such as JPEG compression and Gaussian noise. Our model's\nperformance on MMTD-Set(CASIA1+) under these degradations is reported in Table 5, which in-\ncludes four common degradation types: JPEG compression qualities of 70 and 80, and Gaussian\nnoise variances of 5 and 10. As M-LLMs primarily emphasize high-level semantic information,\nalthough we do not specifically add degraded data during training, FakeShield demonstrates robust-"}, {"title": "ABLATION STUDY", "content": ""}, {"title": "Ablation Study on Domain Tag Generator", "content": "To validate that the proposed domain tag effectively\nmitigates domain discrepancies and enhances the model's generalization across diverse data do-\n10\nmains, we conducted an ablation study. Specifically, we removed the domain tag generator (DTG)\nand trained the FakeShield with identical configurations on the MMTD-Set, the test results are dis-\nplayed in Table 6. Without the DTG module, the model's detection performance declined across\ntest sets from each data domain. Notably, the detection ACC and F1 score for DeepFake decreased\nby 0.09. This demonstrates that without the support of the DTG module, the model struggles to\neffectively differentiate between various data domains, leading to more pronounced data conflicts\nand a significant reduction in both generalization and practical applicability."}, {"title": "Ablation Study on LLM in the DTE-FDM", "content": "Furthermore, to verify the necessity of the\nLLM in the DTE-FDM, we design a variant of\nFakeShield, which removes the LLM and di-\nrectly input $T_{ins}, T_{tag}, T_{img}$ into the tam-\nper comprehension module, adjusting its train-\ning objective to directly produce the description\n$O_{det}$ and the mask $M_{loc}$. Using the same train-\ning configurations, we train the variant and our original framework for 25 epochs, evaluating their\nlocalization accuracy on the CASIA1+ dataset. As shown in Figure 5, after removing the LLM,\nthe localization performance consistently lags behind the original framework throughout the entire\ntraining process and it converges earlier. It proves that joint training detection and localization via\na single MFLM tends to cause notable performance degradation than our decoupled module design,\nwhich further highlights the critical role of our LLM module in enhancing the semantic understand-"}, {"title": "CONCLUTION", "content": "In this work, we present the first application of an M-LLM for the explanation IFDL, marking a\nsignificant advancement in the field. Our proposed framework, FakeShield, excels in tampering de-\ntection while delivering comprehensive explanations and precise localization, demonstrating strong\ngeneralization across a wide range of manipulation types. These features make it a versatile and\npractical tool for diverse real-world applications. Looking to the future, this work can play a crucial\nrole in several areas, such as aiding in the improvement of laws and regulations related to digital\ncontent manipulation, informing the development of guidelines for generative artificial intelligence,\nand promoting a clearer and more trustworthy online environment. Additionally, FakeShield can as-\nsist in evidence collection for legal proceedings and help correct misinformation in public discourse,\nultimately contributing to the integrity and reliability of digital media."}, {"title": "APPENDIX", "content": ""}, {"title": "LIMITATIONS AND FUTURE WORKS", "content": "One limitation of our current framework is its suboptimal performance when handling more complex\ntypes of deepfake tampering, such as identity switching and full-face generation. These types of\nmanipulations introduce unique challenges that our model currently struggles to address effectively.\nTo address this, future work will focus on several key optimizations. First, we plan to incorporate a\nChain-of-Thought (CoT) (Wei et al., 2022) mechanism to enhance the model's reasoning abilities,\nenabling it to detect more subtle manipulations in deepfake content. Second, we will expand our\ntraining dataset to include a broader range of deepfake samples, encompassing various tampering\ntechniques and scenarios, to improve the model's generalization. Finally, we will optimize specific\nmodules within the framework to better handle these complex tampering types, creating a more\nrobust and adaptable detection system. These improvements are expected to significantly enhance\nthe framework's performance across a wider spectrum of deepfake domains."}, {"title": "DATA SOURCES", "content": "We collected source data from the dataset mentioned in Section 4.1, with the details provided in\nTable 7.\nIt is noted that the FaceApp and FFHQ datasets are part of the DFFD (Dang et al., 2020) dataset.\nWe follow their original configuration to divide the training and validation sets. FFHQ (NVIDIA\nCorporation) consists of real face images, while FaceApp (FaceApp Limited, 2017) contains fake\nfaces generated by the FaceApp (FaceApp Limited, 2017) tool, which manipulates facial attributes\nin the images.\nRegarding the construction process of the AIGC-Editing dataset, we first collected 20,000 real im-\nages from the COCO (Lin et al., 2014) dataset and used the SAM (Kirillov et al., 2023) tool to\nsegment the masks of all targets. We then selected the target mask with the third-largest area in\neach image and applied the Stable-Diffusion-Inpainting (Lugmayr et al., 2022) method to partially\nredraw this section. To ensure a balance between positive and negative samples, we further extracted\nan additional 20,000 images from the COCO dataset, distinct from the previously selected ones."}, {"title": "ANSWER ANALYSIS", "content": "Figure 6 presents the adjective and noun word clouds for the ground truth (GT) descriptions in the\nMMTD-Set and the answer descriptions generated by our FakeShield. It is evident that, through\neffective fine-tuning, FakeShield can be guided by the dataset to assess both image-level semantic\nplausibility (e.g., \"physical law,\" \"texture\u201d) and pixel-level tampering artifacts (e.g., \"edge,\" \"reso-\nlution\u201d) to determine whether an image is real."}, {"title": "PROMPTS", "content": "During the process of using GPT-40 to construct the MMTD-Set, we meticulously designed distinct\nprompts for each category of tampered data to guide GPT-40 in focusing on specific aspects for\nimage analysis, as illustrated in Figure 7 and Figure 8."}, {"title": "EXAMPLES", "content": "Comparison of subjective results of mainstream M-LLM: As mentioned in Section 4.3, we se-\nlected some M-LLM output samples, as shown in Figures 9 and 10.\nFakeShield output subjective samples: We selected several results from FakeShield's testing on\nPhotoShop, DeepFake, and AIGC-Editing datasets, as displayed in Figures 11, 12, 13, 14, 15 and 16.\nMMTD-Set data set example: We select some samples from the MMTD-Set data set and display\nthem in Figures 17, 18, and 19."}]}