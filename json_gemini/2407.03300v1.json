{"title": "DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents", "authors": ["Yilun Xu", "Gabriele Corso", "Tommi Jaakkola", "Arash Vahdat", "Karsten Kreis"], "abstract": "Diffusion models (DMs) have revolutionized gen-erative learning. They utilize a diffusion process to encode data into a simple Gaussian distribu-tion. However, encoding a complex, potentially multimodal data distribution into a single contin-uous Gaussian distribution arguably represents an unnecessarily challenging learning problem. We propose Discrete-Continuous Latent Variable Diffusion Models (DisCo-Diff) to simplify this task by introducing complementary discrete latent variables. We augment DMs with learnable discrete latents, inferred with an encoder, and train DM and encoder end-to-end. DisCo-Diff does not rely on pre-trained networks, making the framework universally applicable. The discrete latents significantly simplify learning the DM's complex noise-to-data mapping by reducing the curvature of the DM's generative ODE. An addi-tional autoregressive transformer models the dis-tribution of the discrete latents, a simple step be-cause DisCo-Diff requires only few discrete vari-ables with small codebooks. We validate DisCo-Diff on toy data, several image synthesis tasks as well as molecular docking, and find that introduc-ing discrete latents consistently improves model performance. For example, DisCo-Diff achieves state-of-the-art FID scores on class-conditioned ImageNet-64/128 datasets with ODE sampler.", "sections": [{"title": "1. Introduction", "content": "Diffusion models (DMs) (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021) have recently led to breakthroughs for generative modeling in diverse domains. For instance, they can synthesize expressive high-resolution imagery (Saharia et al., 2022; Ramesh et al., 2022; Rombach et al., 2022; Balaji et al., 2022) or they can generate accurate molecular structures (Corso et al., 2023; Yim et al., 2023; Ingraham et al., 2023; Watson et al., 2023). DMs leverage a forward diffusion process that effectively encodes the train-ing data in a simple, unimodal Gaussian prior distribution. Generation can be formulated either as a stochastic or, more conveniently, as a deterministic process that takes as input random noise from the Gaussian prior and transforms it into data through a generative ordinary differential equation (ODE) (Song et al., 2021). The Gaussian prior corresponds to the DM's continuous latent variables, where the data is uniquely encoded through the ODE-defined mapping.\nHowever, realistic data distributions are typically high-dimensional, complex and often multimodal. Directly en-coding such data into a single unimodal Gaussian distri-bution and learning a corresponding reverse noise-to-data mapping is challenging. The mapping, or generative ODE, necessarily needs to be highly complex, with strong curva-ture, and one may consider it unnatural to map an entire data distribution to a single Gaussian distribution. In prac-tice, conditioning information, such as class labels or text prompts, often helps to simplify the complex mapping by offering the DM's denoiser additional cues for more accu-rate denoising. However, such conditioning information is typically of a semantic nature and, even given a class or text prompt, the mapping remains highly complex. For instance, in the case of images, even within a class we find images with vastly different styles and color patterns, which corresponds to large distances in pixel space.\nHere, we propose Discrete-Continuous Latent Variable Diffusion Models (DisCo-Diff), DMs augmented with addi-tional discrete latent variables that encode additional high-level information about the data and can be used by the main DM to simplify its denoising task (Fig. 1). These discrete latents are inferred through an encoder network and learnt end-to-end together with the DM. Thereby, the discrete la-tents directly learn to encode information that is beneficial for reducing the DM's score matching objective and making the DM's hard task of mapping simple noise to complex data easier. Indeed, in practice, we find that they significantly re-duce the curvature of the DM's generative ODE and reduce the DM training loss in particular for large diffusion times, where denoising is most ambiguous and challenging. In con-trast to previous work (Bao et al., 2022; Hu et al., 2023; Har-"}, {"title": "2. Background", "content": "DisCo-Diff builds on (continuous-time) DMs (Song et al., 2021), and we follow the EDM framework (Karras et al., 2022). DMs perturb the clean data $y \\sim P_{\\text{data}}(x)$ in a fixed forward process using $\\sigma^2(t)$-variance Gaussian noise, where $y \\in \\mathbb{R}^d$ and $t$ denotes the time along the diffusion process. The resulting distribution is denoted as $p(x; \\sigma(t))$ with $x \\in \\mathbb{R}^d$. For sufficiently large $\\sigma_{\\text{max}}$, this distribution is almost identical to pure random Gaussian noise. DMs leverage this observation to sample $x_0 \\sim \\mathcal{N}(x_0; 0, \\sigma_{\\text{max}} I)$ and then iteratively denoise the sample through a sequence of $M + 1$ gradually decreasing noise levels $\\sigma_{i+1} < \\sigma_i$ ($\\sigma_0 = \\sigma_{\\text{max}}$), where $i \\in [0, ..., M]$ and $x_i \\sim p(x; \\sigma_i)$. The $\\sigma_i$ correspond"}, {"title": "3. DisCo-Diff", "content": "In Sec. 3.1, we first formally define DisCo-Diff's genera-tive model and training framework, before discussing and carefully motivating our approach in detail in Sec. 3.2. In Sec. 3.3, we highlight critical architecture considerations."}, {"title": "3.1. Generative Model and Training Objective", "content": "In our DisCo-Diff framework (Fig. 1), we augment a DM's learning process with an $m$-dimensional discrete latent $z\\in \\mathbb{N}^m$, where each dimension is a random variable from"}, {"title": "3.2. Motivation and Related Work", "content": "We will now critically discuss and motivate our design choices and also discuss the most relevant related works. For an extended discussion of related work see App. A.\nThe curvature of diffusion models. DMs, in their simpler ODE-based formulation ($\\beta(t) = 0$ in Eq. (1)), learn a com-"}, {"title": "3.3. Architecture", "content": "As discussed, DisCo-Diff enhances the training of continu-ous DMs by incorporating learnable discrete latent variables that are meant to capture the global underlying discrete structure of the data. To ensure that DisCo-Diff works as in-tended, suitable network architectures are necessary. Below, we summarize our design choices, focusing on DisCo-Diff for image synthesis. However, the framework is general, re-quiring only an encoder to infer discrete latents from clean input data and a conditioning mechanism that integrates these discrete latents into the denoiser network. In fact, we also apply our model to 2D toy data and molecular docking.\nEncoder. For image modeling, we utilize a ViT (Doso-vitskiy et al., 2021) as the backbone for the encoder. We extend the classification mechanism in ViTs, and treat each discrete token as a different classification token. Concretely, we add $m$ extra classification tokens to the sequence of im-age patches. This architectural design naturally allows each discrete latent to effectively capture the global characteristic of the images, akin to performing data classification.\nDiscrete latent variable conditioning. For image experi-ments, DisCo-Diff's denoisers are U-Nets as widely used for DMs (Karras et al., 2022; Hoogeboom et al., 2023). For the discrete latent variable conditioning, we utilize cross-attention (Rombach et al., 2022). Drawing inspiration from text-to-image generation, DisCo-Diff's discrete latents func-tion analogously to text, exerting a global influence on the denoiser's output. Specifically, image features act as queries and discrete latents are keys and values in the cross-attention layer, enabling discrete latents to globally shape the image features. We add a cross-attention layer after each self-attention layer within the U-Net. In our main models, all discrete latents are given to all cross-attention layers.\nGroup hierarchical models. To enhance the interpretability of discrete latents, we also explore the inductive bias inher-ent in the U-Net architecture and feed distinct latent groups into various resolution features in the up-sampling branch of the U-Net, as shown in Fig. 5. This approach draws inspiration from StyleGAN (Karras et al., 2019), where dis-tinct latents are introduced at different resolutions, enabling each to capture different image characteristics by the neural network's inductive bias. This design fosters a group hier-archy, where the groups associated with higher-resolution features offer supplementary information, conditioned upon the groups related to lower-resolution features. We refer to this refined model as the group hierarchical DisCo-Diff.\nIn the molecular docking task, existing denoisers oper-ate through message passing in a permutation equivariant way over 3D point clouds representing molecular struc-tures (Corso et al., 2023). We build this property and archi-tectural bias directly into the latent variables, allowing them to take values indicating one node in the point cloud (there-fore, for every point cloud, the codebook size equals the number of nodes). This latent design choice aligns with the intuition of the encoder determining the atoms playing key roles in the structure and allows for minimal modification of the score model where the latents simply represent addi-tional features for every node. The encoder is also composed of a similar equivariant message passing, e3nn (Geiger & Smidt, 2022), network where for each node one logit per latent will be predicted. More details on the architecture for the molecular docking task can be found in App. E.4.\nThe auto-regressive model over the distribution of the dis-crete latents is implemented in image experiments using a standard Transformer decoder (Vaswani et al., 2017). For molecular docking, it again uses an e3nn network that is fed the conditioning information of the protein structure and molecular graph. Generally, DisCo-Diff is compatible with other conditional inputs, e.g. class labels, which can be added as inputs to denoiser and auto-regressive model. We use an auto-regressive model for simplicity and expect DisCo-Diff's second stage to work equally well with other discrete data generative models, e.g. discrete state diffusion"}, {"title": "4. Experiments", "content": "4.1. Image Synthesis\nWe use the ImageNet (Deng et al., 2009) dataset and tackle both class-conditional (at varying resolutions 64\u00d764 and 128x128) and unconditional synthesis. To measure sample quality, we follow the literature and use Fr\u00e9chet Inception Distance (FID) (Heusel et al., 2017) (lower is better). We also report the number of neural function evaluations (NFE).\nIn the class-conditional setting, the DisCo-Diff's denoiser is initialized using pre-trained ImageNet models, except for the new components: the cross-attention layers between"}, {"title": "5. Conclusions", "content": "We have proposed Discrete-Continuous Latent Variable Diffusion Models (DisCo-Diff), a novel and universal frame-work for combining discrete latent variables with continuous DMs. The approach significantly boosts performance by simplifying the DM's denoising task through the help of aux-iliary discrete latent variables, while introducing negligible overhead. Extensive experiments and analyses demonstrate the unique benefits of global discrete latent variables that are learnt end-to-end with the denoiser. DisCo-Diff does not rely on any pre-trained encoder networks. As such, we validated our method not only on image synthesis, but also for molecular docking, demonstrating its universality.\nLimitations and Future Work. There are several potential future directions and limitations in both the experiments and design of DisCo-Diff. First, our experiments have been pri-marily focused on standard benchmarks such as ImageNet. With more compute resources, DisCo-Diff could be further validated on tasks such as text-to-image generation, where we would expect discrete latent variables to offer comple-mentary benefits to the text conditioning, similar to how discrete latents boost performance in our class-conditional experiments. Secondly, the Group Hierarchical model relies on inductive biases in its architecture, such as the differ-ent image characteristics captured at different resolutions in the U-Net. It would be interesting to explore how such architectures could be constructed and similar hierarchical effects could be achieved when working with different data modalities (molecules, etc.). Thirdly, one could apply the idea of DisCo-Diff to other continuous flow models, such as flow-matching (Lipman et al., 2022) or rectified flow (Liu et al., 2022), to further boost their performance. Concep-tually, due to the close relation between diffusion models and flow matching, we expect discrete latents to behave similarly there and improve performance. Finally, the cur-rent DisCo-Diff framework leverages a two-stage training process. Initially, we jointly train the denoiser and the en-coder, followed by the post-hoc auto-regressive model in the second stage. Future work could investigate combining the two-stage training into a seamless end-to-end fashion."}]}