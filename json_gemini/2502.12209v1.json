{"title": "SUBOPTIMAL SHAPLEY VALUE EXPLANATIONS", "authors": ["Xiaolei Lu"], "abstract": "Deep Neural Networks (DNNs) have demonstrated strong capacity in supporting a wide variety of applications. Shapley value has emerged as a prominent tool to analyze feature importance to help people understand the inference process of deep neural models. Computing Shapley value function requires choosing a baseline to represent feature's missingness. However, existing random and conditional baselines could negatively influence the explanation. In this paper, by analyzing the suboptimality of different baselines, we identify the problematic baseline where the asymmetric interaction between x (the replacement of the faithful influential feature) and other features has significant directional bias toward the model's output, and conclude that p(y|x) = p(y) potentially minimizes the asymmetric interaction involving \u00e6. We further generalize the uninformativeness of x toward the label space L to avoid estimating p(y) and design a simple uncertainty-based reweighting mechanism to accelerate the computation process. We conduct experiments on various NLP tasks and our quantitative analysis demonstrates the effectiveness of the proposed uncertainty-based reweighting mechanism. Furthermore, by measuring the consistency of explanations generated by explainable methods and human, we highlight the disparity between model inference and human understanding.", "sections": [{"title": "Introduction", "content": "Nowadays Deep Neural Networks (DNNs) have demonstrated impressive results on a range of tasks. For example, Language Models Devlin et al. [2018], Chowdhery et al. [2022], Thoppilan et al. [2022], OpenAI [2023] show strong capacity in the field of Natural Language Processing (NLP), Computer Vision (CV) and speech processing. Unlike traditional models (e.g. conditional random fields) that optimize weights on human interpretable features, deep neural models operate like black-box models by applying multiple layers of non-linear transformation on the vector representations of input data, which fails to provide insights to understand the inference process of deep neural models.\nFeature attribution methods Kim et al. [2018] identify how much each feature contribute to the model's output, which could indicate how a deep model make decisions. Shapley value Shapley et al. [1953], measuring the marginal contribution that a player makes upon joining the group by averaging over all possible permutations of players in the group, has been the prominent feature attribution to analyze feature importance toward model's output. For example, SHAP Lundberg and Lee [2017], L-Shapley Chen et al. [2018] and WeightedSHAP Kwon and Zou [2022].\nComputing Shapley value function requires choosing a baseline to represent feature's missingness. The most common solution is to fill in the missingness by randomly sampling from the dataset \u0160trumbelj and Kononenko [2014]. Another way is to generate features by conditioning on the observed features to replace the missing parts. However, Frye et al. [2020] and Hooker et al. [2021] argue that random sampling operation ignores inherent feature dependency. Also, it is difficult for conditional sampling based Shapley value to distinguish correlated feature with different sensitivity toward the model's output Janzing et al. [2020], Kumar et al. [2020]. Watson [2022] considered the baselines sampled from interventional distribution is optimal but it requires accessing to the underlying causal structure.\nIn this paper, we study the faithfullness of Shapley-based model interpretation. Our contributions are summarized as follows:\n\u2022 We analyze the suboptimality of different baselines for computing Shapley value in interpreting feature importance, and introduce asymmetric interaction to identify the problematic baseline where the interaction between x (the replacement of the faithful influential feature) and other features has significant directional bias toward the model's output."}, {"title": "Related work", "content": "2.1 Interpretability of deep models\nDifferent from white-box models (e.g. decision tree-based models) that are intrinsically interpretable, deep models operate like black-box models where the internal working mechanism are not easily understood. Existing methods for interpreting deep models could be categorized into two types: feature attribution that focuses on understanding how a fixed black-box model leads to a particular prediction, and instance attribution is to trace back to training data and study how a training point influence the prediction of a given instance. For example, Integrated Gradients Sundararajan et al. [2017] measures feature importance by computing the path integral of the gradients respect to each dimension of input. LIME Ribeiro et al. [2016] generates explanation by learning an inherently interpretable model locally on the instance being explained. Shapley value Shapley et al. [1953] that is derived from cooperative game theory treats each feature as a player and computes the marginal contribution of each feature toward the model's output. Regarding instance attribution, typical methods include influence function Koh and Liang [2017] which attends to the final iteration of the training and TracIn Pruthi et al. [2020] that keeps tracing the whole training process.\n2.2 Shapley value\nLloyd Shapley's idea Shapley et al. [1953] is that players in the game should receive payments or shares proportional to their marginal contributions. \u0160trumbelj and Kononenko [2014] and Lundberg and Lee [2017] generalize Shapley value to measure feature importance toward the model's output by averaging marginal contribution of the feature being explained over all possible permutations among features. There are many variants of Shapley value to address efficiency and faithfulness. For example, to improve efficiency of Shapley value, Chen et al. [2018] proposes L-Shapley to explore local permutations and C-Shapley to compute valid connected nodes for structured data. Kwon and Zou [2022] proposes WeightedSHAP to optimize weight of each marginal contribution under the user-defined utility function to emphasize influential features.\nComputing Shapley value function requires choosing a baseline to represent feature's missingness. Random baseline Lundberg and Lee [2017], Chen et al. [2018], Kwon and Zou [2022] has been widely used due to its immediate availability. Since random baseline ignores inherent feature dependency and thus could result in inconsistent context, Frye et al. [2020] and Hooker et al. [2021] address the importance of conditional baseline where the missingness is replaced with the features generated by conditioning on the observed features. However, Janzing et al. [2020] and Kumar et al. [2020] use simple linear model to demonstrate that conditional baseline based Shapley value fails to distinguish correlated feature with different sensitivity toward the model's output. Watson [2022] considers that the optimal baseline should follow the interventional distribution but it requires accessing to the underlying causal structure. Generally for the above baselines none is more broadly applicable than the others, which motivates us to analyze suboptimality of these baselines for Shapley value on interpreting black-box models and find out the optimal baseline."}, {"title": "Suboptimal baselines for Shapley value toward black-box model interpretation", "content": "3.1 Preliminaries\nIn supervised setting, given an input x = (x1, ..., xn) and a prediction model f, let fy(\u00b7) denotes the model output probability on y, the Shapley value of ith feature in \u00e6 for the model prediction y is weighted and summed over all possible feature combinations as\n$(i) = \\sum_{S \\subseteq n\\{i}} \\frac{(n-1-\\|S\\|)!\\|S\\|!}{n!} [v(S \\cup i) \u2013 v(S)],\nwhere S is the subset of feature indices. v(S) is the value function that measures the change in prediction caused by observing certain subsets S in the instance \u0160trumbelj and Kononenko [2014] and is defined as\nv(S) = E(fy|xs U x's) \u2013 E(fy|x'),\nwhere x' collectively denotes all possible instances and E(fy|x') represents the model's prediction averaged across these instances (this item could be excluded when computing the difference between v(SU i) \u2013 v(S)). xs Ux refers to the instances that retain unchanged subset values \u00e6s. The contribution of the subset S is determined by the difference between the effect of the subset S and the average effect.\nThere are three common baselines Watson [2022] to fill in \u00e6\u00b4s that are described below.\nRandom baseline: \u00e6 is drawn randomly from the distribution p(x's).\nConditional baseline: sample x by conditioning on \u00e6s.\nInterventional baseline: \u00e6 is generated by following the causal data structure p(x's|do(xs)). Since accessing causal data structure is challenging we focus on discussing the above two baselines in this work.\n3.2 A motivation example\nIt could be trivial to show how the random and conditional baselines lead to incorrect interpretation on white-box models (analysis of a linear model is described in Appendix A). In interpreting deep models with Shapley value, we analyze an empirical example with a movie review \u201cplot is fun but confusing\" that is predicted as \"Negative\u201d sentiment by our finetuned BERT classification model\u00b9, the faithful feature contribution ranking\u00b2 is [ \"confusing\", \u201cplot\", \u201cbut\", \"is\", \"fun\" ]. Fig.1 shows the ranking of Shapley values computed by the random and conditional baselines with different number of sampling instances.\nRandom baseline: As shown in Fig.1a, by increasing the number of sampling instances, \"confusing\" and \"fun\" are always identified as the most influential features while the ranks of \"plot\" and \"but\" are misleading in most cases.\nConditional baseline: With smaller sampling size (i.e. [1, 20]) that could ensure feature consistency via conditional sampling, we observe that in these scenarios \u201cconfusing\" and \"fun\" are the most influential features but the contribution of \"plot\" and \"but\" could be misinterpreted.\nGiven a fixed subset S, the contribution of ith feature is computed with\n$\\phi_S(i) =v(S\\cup i) \u2013 v(S) =E(f_y|x_{S\\cup i} \\cup x'_{n \\backslash (S\\cup i)}) \u2013 E(f_y|x_{S} \\cup x'_{n \\backslash S}),$\nwhere S\u2081 \u2264 n \\ (SU i) and S2 \u2286 n \\ S.\nFor positively-contributing features, it is expected that without its participation E(fy|xsUx52) should be lower than E(fyxsui Ux'). However, for highly influential positive features, under some random and conditional baselines fy(SU i) could be much lower than f(S), which may results in lower $s(i) compared to less influential positive features. Similarly $s(i) for a highly influential negative feature could be higher than that of less influential negative features.\nTaking x = \u201cplot\u201d with \u00e6s = \u201cis fun but\u201d as an example (more baselines with random sampling and conditional sampling are shown in Appendix C). With conditional sampling, fy(\u201cpractice is fun but brief\") = 0.5147 is much higher than fy(\"plot is fun but brief\") = 0.0347 and fy(\u201cscript is fun but vague\") = 0.8203 exceeds fy(\u201cplot is fun but vague", "plot": "practice\" and \"script\" contribute more positively to y.\n3.3 Error analysis: asymmetric feature interaction view\nAs described in Eq.(1)"}, {"title": "Proposed method", "content": "To mitigate the directional bias toward y generated by the substitute of \u00e6\u017e (i.e. x) in \u00e6s U x5, and then improve the importance of influential features, we should reduce the contribution of x to Eq.(12) as\n$\\min_{x'\\in C} |I(y; x') + \\sum_{\\substack{T(x')\\subseteq n\\\\ T' \\subseteq C_n \\backslash T(x)}} \\phi(T' \\rightarrow T(x';)) |,$\nwhere C denotes the set of possible features of x to fill in xs Ux5.\nBased on the triangle inequality (derivation details are provided in Appendix B), the above minimization problem is formulated with\n$\\min_{x'\\in C} |p(y|x') - p(y)| + \\sum_{\\substack{T(x')\\subseteq n\\\\ T' \\subseteq C_n \\backslash T(x)}} C_1 \\Delta(T', T(x),x),$\n$\\Delta(T',T(x),x) = |I(y; x_{T(x)}|x_S) \u2013 I(y;x_{T(x)}|x_{S\\backslash T'}) |,$\nFor all possible T', if I (y; XT(x)|xs) = I(y; XT(x)|XS\\T') holds true, it indicates that T(x) consistently contributes to y across different conditions. In the special case where XT(x) is uninformative about y, we will have\np(y; XT(x)|xs) = p(y|xs)p(XT(x)|XS),\np(y; XT(x)|XS\\T') = p(y|xs\\T')p(XT(x)|XS\\T'),\nI(y; XT(x)|xs) = I(y; XT(x)|XS\\T') = 0.\nWhen considering all possible combinations XT() involving xi, if each combination is uninformative about y, it implies that x\u2081 does not contribute to y. This is because if x is informative about y, there would be at least some configurations of XT() contributing to y. Therefore given the special case that can minimize the asymmetric interaction of x, we conclude that\np(yx) = p(y),\nwhere x is independent of y.\np(y|x) = p(y) is the necessary but not sufficient condition for the uninformativeness of XT() involving \u00e6 about y. Therefore p(yx) = p(y) leads to minimized I(y; x) and potentially achieves the special case where the asymmetric interaction involving \u00e6 is minimized. Minimizing Eq.(12) is conditionally equivalent 4 to\n$\\min_{\\substack{x'\\in C}} |(y; x) + \\sum_{\\substack{T(x)\\subseteq n\\\\ T' \\subseteq C_n \\backslash T(x)}} \\phi(T' \\rightarrow T(x)) |,$\n$ = \\min_{x'\\in C} |p(y|x) - p(y)|,$\nAs p(y) depends on the size and characteristics of the data, we generalize the uninformativeness of \u00e6 toward the label space L to avoid estimating p(y) as\n$\\min_{x'\\in C} \\sum_{j=1}^L p (y_j|x_i) \\log_2\\frac{p (y_j|x_i)}{p(y_j|x_i)},$\nBy maximizing the entropy H(Y|x), the obtained \u00e6* is less certain regarding the label space, which is a practical and efficient alternative to minimize the difference between p(y|x) and p(y).\nThe greedy search for optimal \u00e6* in each baseline to compute E(fy|xs U X'52) is computationally expensive, we design an uncertainty-based reweighting mechanism to accelerate the computation process as\nE(fyxsUx's\u2082) =Eas [\u0124(x) fyxs xs2],\nwhere H(x) is the normalized entropy as\n\u0124(x) = H(x)/log2(L).\nIn the reweighting mechanism higher weight \u0124(x) is assigned to the sequence where x is less certain toward y and these baselines are encouraged to compute E(fy|xs U x5). Furthermore, with flexible generation of x5, this reweighting mechanism could be generalized to different baselines, which could improve the faithfulness of feature importance, especially in the scenario where the sampling size is limited. We present the results of applying the uncertainty-based reweighting mechanism to the random and conditional baselines on the motivation example in Fig.1b and it shows that the Shapley interpretations are consistent with the faithful ranking in most cases."}, {"title": "Experiments", "content": "In this section, we evaluate the faithfulness of Shapley value with and without the uncertainty-based reweighting mechanism. By introducing the reweighting coefficient to the predicting expectation over random baselines (i.e. random-uw) and conditional baselines (i.e. condition-uw), we compare these Shapley interpretations with the original Shapley values computed by random and conditional baselines (denoted as random and condition). We adopt Shapley Sampling \u0160trumbelj and Kononenko [2014] to compute Shapley value (as summarized in Algorithm 1 and 2 in Appendix D).\n5.1 Datasets and Models\nTo effectively leverage language model for conditional sampling, we conduct experiments on various NLP tasks including sentiment analysis (SST-2 Socher et al. [2013] and Yelp-2 Zhang et al. [2015]), natural language inference (SNLI Bowman et al. [2015]), intent detection (SNIPS Coucke et al. [2018]) and topic labeling (20Newsgroup), and study the performance on BERT-base Devlin et al. [2018] and RoBERTa-base Liu et al. [2019] models. Appendix E and F provide the configurations of finetuning pretrained BERT-base and ROBERTa-base models in these downstream tasks.\n5.2 Faithful evaluation metrics\nWe choose three widely used faithful evaluation metrics and employ padding replacement operation. Since deletion-based evaluation yields similar results, we report the corresponding results in Appendix H.\nLog-odds (LOR) Shrikumar et al. [2017]: average the difference of negative logarithmic probabilities on the predicted class over the test data before and after replacing the top k% influential words from the text sequence. The lower LOR, the more faithful feature importance ranking.\nSufficiency (SF) DeYoung et al. [2019]: measure whether important features identified by the explanation method are adequate to remain confidence on the original predictions. The lower SF, the more faithful feature importance ranking.\nComprehensiveness (CM) DeYoung et al. [2019]: evaluate if the features assigned lower weights are unnecessary for the predictions. The higher CM, the more faithful feature importance ranking."}, {"title": "Quantitative Analysis", "content": "To alleviate Out-of-Distribution (OOD) issue we choose k within the range [10, 50]. The sampling size of computing value function is set to 1000 to enhance the robustness of feature importance ranking. The performance over ROBERTa architecture is reported in Appendix I.\nFig.4 demonstrates the evaluation performance over BERT architecture on different datasets. We can observe that random-uw generates more faithful explanations than other baselines. random-uw and condition-uw always perform better than the corresponding baselines without uncertainty-based reweighting. In SST-2, SNLI and Yelp-2, random-based baselines obtain better results compared with condition-based baselines while for SNIPS and 20Newsgroup condition-uw can outperform random.\nBased on the above observations, introducing uncertainty-based reweighting to random and conditional baselines can improve the faithfulness of explanations. Furthermore, although random-based baselines are criticized for ignoring feature dependency, these baselines could generate informative local features without introducing additional dependency that tends to result in biased interaction Janzing et al. [2020]. In particular, immediate availability of random baselines can greatly improve the efficiency of Shapley interpretation. We also show the robustness of Shapley Sampling method in Appendix J."}, {"title": "Analysis of in-distribution baselines", "content": "To address out-of-distribution baselines generated by random sampling or general pretrained language model Kumar et al. [2020], we pretrain BERT-base on these five datasets to ensure in-distribution sampling and evaluate in-distribution baselines (i.e. condition-in and condition-in-uw) over BERT architecture. Fig.5 shows the evaluation performance with k = 20 (the results of Yelp-2 and SNIPS are reported in Appendix K). In-distribution baselines achieve significant improvement over their corresponding baselines without in-distribution sampling. In-distribution sampling does produce in-domain data. However, random sampling from training data also yields in-domain-like data. For example, for the input \"most new movies have a bright sheen\", given \u00e6s = \u201cmost new movies have\", in-distribution conditional sampling could generate \u201cmost new movies have gone the back\" and random sampling yields \u201cmost new movies have an extraordinary bore\" from the sequence \u201ca technical triumph and an extraordinary bore\"."}, {"title": "Human evaluation", "content": "There are some research reports T\u00f6rnberg [2023], Feng et al. [2023] showing that large language models (LLMs), such as GPT-3.5 and GPT-4, can provide high-quality annotations like an excellent crowdsourced annotator does. Therefore we use GPT-4 to provide explanations toward the model's output (details of the prompt for generating explanation is given in Appendix L). Using the rankings generated by GPT-4 as machine-generated version, we also provide human explanation grounded in human comprehension.\nTo enable user-friendly human evaluation, we select 83 explained instances for BERT and 74 for RoBERTa on SST-2, and 86 explained instances for both BERT and RoBERTa on SNIPS. First, we employ Spearman's Rank Correlation Coefficient to measure the correlation of feature importance ranking between GPT-4 (or human) and other baselines, and report the average results in Table 1. It could be observed that GPT-4 and human achieve great consistency, which demonstrates the quality of GPT-4 in human role."}, {"title": "Conclusions", "content": "In this paper we analyze the suboptimality of existing random and conditional baselines and identify the problematic baseline where the asymmetric interaction between x (the replacement of the faithful influential feature) and other features has significant directional bias toward the model's output. We further design a simple uncertainty-based reweighting mechanism to mitigate these biased interactions. By evaluating different baselines on various tasks over BERT and RoBERTa architectures, quantitative analysis shows that our proposed uncertainty-based reweighting mechanism improves the faithfulness of Shapley-based interpretation. We measure the consistency of explanations generated by explainable methods, GPT-4 and human, which demonstrates the gap between model inference and human understanding."}, {"title": "Suboptimal baselines for Shapley value toward the linear model analysis", "content": "Consider a simple example f(x1,x2) = x1 + x2 where both X\u2081 and X2 follow Bernoulli (1) and\nP(x1, x2) =  \\begin{cases}  \\frac{1}{2} & \\text{if } [X1=X2], \\\\  0 & \\text{otherwise}.  \\end{cases} \nFor linear models, Hooker et al. [2021] shows that permutation-based importance methods associate variable importance with the magnitude of the corresponding coefficient when variable values are standardized. Therefore X1 and X2 should be equally informative in permutation-based approaches. Next we will introduce how is the Shapley values computed with different baselines.\nRandom baseline: E(f(x1, X2)) and E(f(x1,x2)) is computed as\nE(f(x1, X2)) = Ep(x2)(f(x1, X2))\n= p(x2) f(x1,x2)|x2=0 + P(x2) f(x1,x2)|x2=1\nx1+(x1+1)\n=\nx1 +\nwhich yields\nE(f(X1,X2)) = Ep(x1)(f(X1,X2))\n= p(x1) f(x1, x2)|x1=0 + p(x1)f(X1,X2)|x1=1\n1\n= x2 + (x2+1)\n=$1 =  [(001) \u2013 v(0) + (2 1) \u2013 v(2)]\n=>  [Ep(x2) (f(x1, x2)) - Ep(x1,x2) (f(X1, X2)) + f(x1,x2) - Ep(x1)(f(X1,X2))]\n1\n=x1\n$2 =  [(002) \u2013 (0) + 2(201) \u2013 v(1)]\n=>  [Ep(x2) (f(x1, X2)) \u2013 Ep(x1,x2) (f(X1, X2)) + f (x1, x2) - Ep(x1) (f(X1,X2))]\nand $1 \u2260 $2.\nConditional baseline: E(f(x1, X2)) and E(f(X1, x2)) is computed as\nE(f(x1, X2)) = Ep(x2|x1)(f(x1, X2))\n= p(x2|x1) f(x1,x2)|x2=0 + P(x2|x1)f(x1,x2)|x2=1\n= 2x1\nE(f(X1,X2)) = Ep(x1|x2) (f(X1,X2))\n= p(x1|x2) f(x1, X2)|x2=0 + P(X1|X2) f(x1,x2)|x2=1\n= 2x2,"}, {"title": "Derivation details of Eq.(13)", "content": "|I(y; x) +\n\n(T' \u2192 T(x);)) |\n\u2264  |I(y; x)|+ \n\n(T' \u2192 T(x'))\nAs we focus on the items related to x, \n(T' \u2192 T(x)) |\nis minimized as\n(T' \u2192T(x)) |\n< \n(T' \u2192 T(x))"}]}