{"title": "MLLM CAN SEE? DYNAMIC CORRECTION DECODING FOR HALLUCINATION MITIGATION", "authors": ["Chenxi Wang", "Xiang Chen", "Ningyu Zhang", "Bozhong Tian", "Haoming Xu", "Shumin Deng", "Huajun Chen"], "abstract": "Multimodal Large Language Models (MLLMs) frequently exhibit hallucination phenomena, but the underlying reasons remain poorly understood. In this paper, we present an empirical analysis and find that, although MLLMs incorrectly generate the objects in the final output, they are actually able to recognize visual objects in the preceding layers. We speculate that this may be due to the strong knowledge priors of the language model suppressing the visual information, leading to hallucinations. Motivated by this, we propose a novel dynamic correction decoding method for MLLMs (DeCo), which adaptively selects the appropriate preceding layers and proportionally integrates knowledge into the final layer to adjust the output logits. Note that DeCo is model agnostic and can be seamlessly incorporated with various classic decoding strategies and applied to different MLLMs. We evaluate DeCo on widely-used benchmarks, demonstrating that it can reduce hallucination rates by a large margin compared to baselines, highlighting its potential to mitigate hallucinations.", "sections": [{"title": "INTRODUCTION", "content": "Recently, the rapid development of Multimodal Large Language Models (MLLMs) has demonstrated a potential pathway towards achieving Artificial General Intelligence (AGI) (Wang et al., 2024; Yao et al., 2024; Lu et al., 2024a; Team, 2024; OpenAI, 2023; Liu et al., 2023b; Chern et al., 2024). However, in practice, the development of MLLMs is hindered by the phenomenon of hallucination, which typically results in the model generating statements about non-existent images while neglecting to mention certain visible objects, effectively causing it to fool itself (Bai et al., 2024; Liu et al., 2024a; Li et al., 2023b; Liu et al., 2023a; Rawte et al., 2023). This issue poses significant risks in high-stakes fields such as medical imaging (Chen et al., 2024a; Hu et al., 2023; Wang et al., 2023b), autonomous driving (Cui et al., 2024; Wang et al., 2023c), and human-computer interaction systems (Brie et al., 2023), where such errors could result in irreparable consequences.\nThe reasons behind hallucinations in MLLMs are complex. Unlike analyses focused on unimodal LLMs (Chuang et al., 2024; Chen et al., 2024c; Orgad et al., 2024; Chen et al., 2024d; Lu et al., 2024b), many current works assume that MLLM may indeed 'see' visual information. However, due to factors such as excessive model depth (Chen et al., 2024b; Zhang et al., 2024a), aggregation patterns (Huang et al., 2024), or priors knowledge inherent in the MLLMs (Leng et al., 2023; Zhang et al., 2024b), these models ultimately still experience hallucinations. Concretely, our understanding of the underlying mechanisms of hallucinations in MLLMs remains limited. It is still uncertain whether the visual information is never correctly recognized or if it is recognized but subsequently suppressed by later information streams.\nHallucinated MLLM can see (to some extent). Inspired by the aforementioned works, we conduct an empirical analysis and find that MLLMs are not blind; they can recognize objects in the preceding"}, {"title": "WHY DO MLLMS GENERATE NON-EXIST OBJECTS?", "content": "In this section, we conduct a series of empirical analysis to investigate the internal mechanisms of MLLM and elucidate the underlying reasons for its generation of non-existent objects. To strike a balance between the realism and complexity of the experiments, we primarily focus on the generation of objects in image description scenarios (image caption tasks).\nPreliminaries of MLLM generation. MLLMs typically concatenate visual tokens, processed by the visual encoder and projection layer, with embedded textual tokens before feeding them into an autoregressive language model. We denote the visual tokens as $X^{V} = {X_{v1}, X_{v2},...,X_{vp}}$ and textual tokens as $X^{C} = {X_{c1}, X_{c2}, ..., X_{cq}}$. Here P and Q are the lengths of the visual tokens and textual tokens respectively. Finally, the input is $X = concat{X^{V}, X^{C}}$. Then X would be passed into MLLM with N stacked transformer layer. The intermediate variable generated by the"}, {"title": "FINDING 1: MLLM KNOWS TO SOME EXTENT WHETHER AN OBJECT EXISTS", "content": "Inspired by (Ye et al., 2024), we explore how MLLMs comprehend objects in the image captioning task. For simplicity, we abstract this process into a function called isexist(obj), which determines whether an object is present in an image. To examine the application of this function within the MLLM's image captioning workflow, we conduct probing experiments at the conclusion of object descriptions in each layer of the MLLM's language model component, which consists of 32 transformer layers in a 7-billion-parameter model (Detailed setup in Appendix A.1).\nWe employ the prompt template, \u201cUSER: <image>Describe the image. ASSISTANT: The image contains obj.\" Both the training and testing datasets are formatted accordingly before being input into MLLMs. We train a probe classifier at the final position of the hidden state outputs for each transformer layer, resulting in a total of 32 classifiers. (For details on the subset division, OOD and in-distribution splits, and prompt templates, please refer to Appendix A.1.) The model is evaluated using the test set, as shown in Figure 1(a) (left). Further experiments are conducted on three splits of the evaluation dataset proposed by POPE, with results reported in Figure 1(a) (right). These evaluations provide a comprehensive understanding of the model's object recognition capabilities across diverse scenarios.\nWe select the best-performing probe classifier from the 32 classifiers to compare accuracy across all objects, existing objects, and non-existing objects. Our results show that the MLLM achieves high accuracy for correctly generated objects in image captions. Despite generating many non-existent objects, the MLLM still maintains around 80% accuracy in our probing experiments. This suggests that MLLMs possess a certain level of understanding regarding object existence in images.\nAdditionally, our probing experiments reveal higher accuracy in the preceding layers, as illustrated in Figure 1(b), which aligns with previous findings (Zhang et al., 2024b; Leng et al., 2023). Furthermore, we show that increasing the resolution of the visual encoder (from 224px to 336px) enhances accuracy for non-existing objects, indicating that token information at the last position in the preceding layers better represents visual information. (For a detailed explanation of the different"}, {"title": "FINDING 2: LANGUAGE MODEL PRIORS SUPPRESS THE VISUAL INFORMATION THAT MLLM ALREADY SEE.", "content": "We hypothesize that the representations in the preceding layers effectively capture (to some extent) visual information. However, the prior knowledge embedded in the MLLM reduces the probabilities of ground truth tokens in deeper layers. Figure 2 illustrates this hypothesis with running examples. We analyze the Top-4 tokens ranked by probability in the final layer's output. Non-hallucinated tokens like \u201cpeople\u201d, \u201cleft\u201d, \u201cblue\u201d, and \u201cumbrella\u201d exhibit high probabilities from the 18th layer. In contrast, hallucinated tokens like \u201cbird\u201d and \u201cgreen\u201d only show comparatively high probabilities around the 30-th layer. Interestingly, the probabilities of ground truth tokens \"umbrella\" and \"blue\" sharply decline from the 30-th layer onwards, eventually falling below the hallucinated tokens' probabilities in the final layer.\n\nTo further investigate this phenomenon, we conduct an early exit experiment (Teerapittayanon et al., 2016; Elbayad et al., 2020; Schuster et al., 2022) to analyze the evolution of the MLLM's internal representations across transformer layers. We randomly select 500 images from the MSOCO dataset and use random prompts to elicit raw responses from LLaVA-1.5-7b. We then extract all non-existent objects along with their corresponding preceding text and input this data into the MLLM. We observe the probabilities of the next token across the transformer layers to gain insights into the model's behavior (see Appendix A.2 for detailed experimental setup). The output of the i-th layer is denoted as $h^{i}$, and the probability distribution of the next token is represented as $p(\\cdot|x_{<s})^i = softmax(\\phi(h^{i-1}))$. To reduce the observation tokens and simulate the real sampling process, we truncate the vocabulary, similar to Top-p sampling, and obtain the candidate tokens, denoted as $V_{candidate}$ with a default threshold of 0.9. We then label the tokens in $V_{candidate}$. Specifically, we filter out data where $V_{candidate}$ contains at least one ground truth token and observe whether an activated ground truth token exists among the candidate tokens, formally expressed as:\n$\\exists x_{a} \\in V_{candidate} \\wedge i \\in (0, N], p(x_{a}|X_{<s})^i - p(x_{h}|X_{<s})^i \\geq threshold,$\nwhere $x_{a}$ is the activated ground truth token, $x_{h}$ is the token with the highest probability of being a hallucinated token in the probability distribution of the final layer and $threshold \\in (0, 1)$. Based on the experimental setup described above, we conducted the following investigation:\nWhat suppresses the expression of visual facts? We analyze the occurrence of $x_{a}$ at each decoding layer, as shown in Figure 3. The results reveal that the activated ground truth tokens are primarily present between layers 20 and 28, indicating that MLLMs accurately recognize the image content in the latter layers. However, the activated ground truth tokens are suppressed in the final output layer. This suppression may stem from the guidance of the input image or the inherent knowledge bias of the MLLM. To investigate this, we generate candidate tokens $V^{'}_{candidate}$ in the absence of an input image, representing tokens based on the MLLM's inherent knowledge. We calculate that the overlap rate of $V_{th}$ existing in $V^{'}_{candidate}$ reaches 91.05%, suggesting that even without expressing image information, MLLMs still tend to generate the original hallucination tokens. This finding reveals that the inherent knowledge in MLLMs may diminish the probability of the ground truth token in the deeper layers."}, {"title": "PROPOSED APPROACH: DYNAMIC CORRECTION DECODING WITH PRECEDING-LAYER KNOWLEDGE", "content": "After investigating the reasons why MLLMs generate non-existent objects, inspired by (Chuang et al., 2024), we introduce Dynamic Correction Decoding with preCeding-Layer Knowledge (DeCo), which can alleviate hallucinations during inference. The overall framework of Deco is"}, {"title": "DYNAMIC PRECEDING-LAYER SELECTION", "content": "Candidate token acquisition. Due to the vast vocabulary space, we track only the changes in the top-ranked tokens as candidate tokens across different layers for computational convenience. This is based on the hypothesis that ground tokens usually appear in the top position of the MLLM's last layer output logits. Inspired by (Li et al., 2023a), we use a truncation strategy to select the candidate tokens, with the default truncation strategy being top-p truncation, formally:\n$V_{candidate} (x_{T}|x_{<T}) = { x_{T} \\in V: \\sum_{v \\in V_{p}} P(x = v|x_{0}, x_{1},..., x_{T-1}) \\leq p}$\nwhere V is the whole vocabulary, and p refers to the parameter used in top-p.\nPreceding-layer selection. Our findings in Section 2 demonstrate that activated ground truth tokens typically exhibit higher probabilities in preceding layers compared to hallucinated tokens. Based on this observation, we hypothesize that selecting the token $x_{th}$, where $x_{th} \\in V_{candidate}$, with the highest probability from the interval layers corresponds to the ground truth token. We compute the accuracy of $x_{th}$ as the ground truth token and denote this metric as the hit rate, as shown in Table 1. The results indicate that within a specific range of layers (e.g., 15-28), $X_{th}$ indeed has a high universal probability of representing the ground truth token. Intuitively, we track"}, {"title": "DECODING CORRECTION WITH PRECEDING-LAYER KNOWLEDGE", "content": "Dynamic soft modulation. We introduce a dynamic modulation coefficient, defaulting to the maximum probability. Formally, we have:\n$max\\_prob = max(softmax(\\phi(h^{N-1}))).$\nThis coefficient can help prevent hard changes in logits, particularly when the probability differences between candidate tokens in preceding layers are insignificant. From the example in Figure 4, we can observe that the absence of the dynamic modulation coefficient may lead to semantic incoherence or even more severe hallucinations.\nPreceding-layer knowledge guided decoding. Given the selected preceding layers, we integrate information from these layers into the final layer to correct the logit distribution. We utilze a hyperparameter, a, to control the proportion of early-layer information incorporated. Additionally, dynamic soft modulation is employed to preserve the generative style of the original model. By utilizing the correction of preceding-layer representations, the probability of predicting the next token and the logits are updated as follows:\n$p(x_{T} | x_{<T}) = softmax (logits^{N}),$\n$logits^{N} = \\phi(h^{N-1}) + \\alpha \\times max\\_prob \\times (\\phi(h^{A-1}),$\nwhere N is the last layer of MLLM and A is the selected preceding layer.\nComparison of previous methods. Our work shares a similar assumption with OPERA (Huang et al., 2024) and VCD (Leng et al., 2023), positing that the knowledge priors inherent in MLLMS may suppress the model's ability to comprehend visual information. However, our approach is comparatively simpler than that of OPERA (Huang et al., 2024) and VCD (Leng et al., 2023). Additionally, our work differs from the assumption in unimodal LLMs, where the semantic information present in the shallow layers interferes with factual recall in the final layer (Chuang et al., 2024; Chen et al., 2024c). However, our method is actually parallel to previous approaches and can be combined to achieve better results."}, {"title": "EXPERIMENT", "content": "Baselines. We integrate DeCo with various decoding methods, including greedy decoding, nucleus sampling, and beam search, and compare it against several baselines for mitigating hallucinations, as outlined below: Dola (Chuang et al., 2024) is specifically designed for alleviating hallucinations in factual tasks for LLMs by reducing shallow semantic influences to improve the factuality of the final layer's output. VCD (Leng et al., 2023) mitigates the influence of language model's priors in MLLMs by generating representations that enhance visual information through the subtraction of interfering knowledge prior during each sampling step. OPERA (Huang et al., 2024) dynamically penalizes overconfident tokens based on the emergence of aggregation patterns, while proposing a retrospective allocation strategy to avoid cases where hallucinations have already occurred. For all the baselines, we use the default hyperparameters from the source code for a fair comparsion."}, {"title": "BENCHMARK AND METRICS", "content": "CHAIR. Caption Hallucination Assessment with Image Relevance (CHAIR) (Rohrbach et al., 2018) metric, widely used in image captioning, identifies hallucinated objects by comparing the extracted objects with ground truth labels and evaluates both at the instance level (CHAIR\u2081) and sentence level (CHAIRs), as shown in Eq. 8. Following (Huang et al., 2024), we conduct experiments using the same settings, including the consistent 500 images from the MSCOCO 2014 validation dataset and the identical prompt, \u201cPlease help me describe the image in detail.\".\n$\\text{CHAIR}_{I} = \\frac{|\\{\\text{hallucinated objects}\\}|}{\\text{all mentioned objects}},  \\text{CHAIR}_{S} = \\frac{|\\{\\text{captions with hallucinated objects}\\}|}{\\text{all captions}}$\nPOPE. The Polling-based Object Probing Evaluation (POPE) (Li et al., 2023b) is a VQA-based metric for assessing object hallucination in MLLMs. It evaluates hallucinations by asking questions such as \"Is there a <object> in the image?\" where <object> is derived from three types of splits: random (randomly selected objects), popular (frequently occurring objects), and adversarial (objects closely related to those in the image). The evaluation includes 500 MSCOCO images, with six questions per image for each split. We use F1 score for performance evaluation.\nMME. The comprehensive MLLM Evaluation benchmark (MME) (Fu et al., 2023) assesses the perceptual and cognitive abilities of MLLMs across a total of 14 subtasks, including tasks such as OCR, visual knowledge, attribute relationships, and object recognition."}, {"title": "EXPERIMENTAL RESULTS", "content": "Results of hallucination in image captioning. Note that we use the baseline's original decoding settings for a fair comparison and run DeCo under the same settings. From Table 2, we notice that DeCo consistently outperforms other approaches in mitigating hallucinations across four MLLMs-InstructBLIP, MiniGPT-4, LLaVA-1.5, and Qwen-VL-using three decoding strategies: greedy search, beam search, and nucleus sampling. We find that DeCo slightly outperforms OPERA, while our method demonstrates higher efficiency and simplicity in inference (see Section 4.4). Additionally, VCD does not perform as well, likely due to producing an increased number of hallucinated descriptions during the generation process. In conclusion, the proposed approach DeCo effectively reduces hallucinations in visual description tasks solely through dynamic decoding correction, achieving an average suppression rate of approximately 10.8% on image captioning datasets. Additionally, we further evaluate the performance of DeCo on the AMBER image caption dataset, as detailed in Table 7 of the Appendix.\n\nResults of hallucination in VQA. In contrast to image captioning, POPE employs a simple polling approach to assess hallucination levels in MLLMs with respect to object recognition. As shown in Table 3, DeCo demonstrates superior performance across all settings, further validating the effectiveness of the proposed approach. Additionally, Figure 5 reveals that DeCo also achieves better results on MME, which evaluates the multifaceted VQA capabilities of LLaVA-1.5. These findings suggest that the underlying mechanism we identified not only applies to object recognition but also extends to attribute-related tasks and more complex reasoning tasks.\nResults of GPT-40's assistance. Following (Huang et al., 2024; Leng et al., 2023), we further use GPT-40 to evaluate our method against greedy decoding across four distinct models. From Table 4, we notice that our approach consistently outperform greedy decoding in terms of accuracy, demonstrating its efficacy in hallucination suppression. The impact of decoding intervention is evident in the level of detail produced: for some models, our method yield only marginally higher or, in certain cases, slightly lower levels of detail compared to greedy decoding. Nonetheless, our method exhibit a clear advantage in mitigating hallucinations across all evaluated models."}, {"title": "ANALYSIS", "content": "Latency and throughput analysis. To evaluate the efficiency of DeCo, we compare its latency and throughput with several baselines, including DoLa, OPERA, and VCD based on Greedy, Beam Search, and Nucleus Sampling, respectively. Figure 6 illustrates the results of this comparison. The findings indicate that DeCo operates within an acceptable efficiency cost, striking a bal-\nPerturbation in the selected preceding-layer. To evaluate the effectiveness of the dynamic layer selection method, we introduce a random perturbation strategy. Specifically, for the predetermined preceding layers, we add random values ranging from -5 to 5 to modify the selection of layers. We randomly select 200 images from the MSCOCO dataset and prompt MLLMs to generate descriptions. The results after incorporating the perturbations are presented in Table 5. Notably, the perturbed results demonstrate a significant degradation in performance, further validating the effectiveness of our proposed method.\n\nHyperparameter analysis. Our method incorporates two primary hyperparameters: a and the selection of interval layers. In the experiments, we employ DeCo based on greedy decoding. On the one hand, the hyperparameter a regulates the intensity of early information enhancement. Figure 7(a) illustrates the performance across various a values. We observe that hallucination suppression is most effective when a approximates 0.6. As a increases, the efficacy of DeCo in mitigating hallucinations improves. However, it is crucial to note that excessively high a values may lead to the generation of atypical image descriptions, characterized by repetitive word usage. On the other hand, the layer interval hyperparameter [a, b] determines the candidate layers for inclusion in the enhancement process. We conduct experiments using intervals of four layers, with results presented in Figure 7(b). Our analysis reveals that hallucination suppression for MLLM is negligible in layers 1-16, while layers 20-28 demonstrate substantial mitigation of hallucinations. Notably, layers 29-32 exhibit minimal hallucination suppression, aligning with our findings discussed in Section 2.2.\n\nMitigating snowballing hallucinations. Snowballing hallucinations are a prevalent issue in the responses generated by MLLMs. This phenomenon occurs when an initial hallucination triggers a sequence of subsequent errors, leading to a compounding effect that significantly degrades the quality and coherence of the generated text. Figure 8 illustrates a typical example of snowballing hallucinations, where an initial misinterpretation of the visual input propagates through the decoding process, resulting in a highly inconsistent and erroneous output. Our approach can reduce the accumulation of errors and improves the overall consistency and accuracy of the generated responses. The effectiveness of DeCo is further demonstrated through additional cases based on diverse MLLMs, which can be found in Figures 9, 10, 11, and 12 in Appendix C."}, {"title": "RELATED WORK", "content": "MLLM HALLUCINATION MECHANISM\nHallucination in MLLMs, characterized by contradictions between image input and textual output, has been a prevalent issue (Liu et al., 2024a; Chen et al., 2024f). Current research on the mechanism of hallucination in MLLMs focuses on two key aspects: the interaction between images and text at different layers, and the prior bias of the LLM during decoding. Several studies have investigated the role of image-text interaction at different layers in MLLMs. Grad-CAM (Zhang et al., 2024a) visualizations reveal that image-text interaction exists in the preceding layers (1-11) but not in the deep layers. OPERA (Huang et al., 2024) further proposes that the \"Aggregation Pattern\" leads to hallucination, where visual information from preceding layers is gradually aggregated to anchor tokens, and focusing solely on these tokens during prediction while ignoring visual information leads to a high probability of hallucination in the generated sequence. However, other studies have revealed that MLLMs exhibit biases towards LLM priors, even in the presence of noisy or absent visual information. VCD (Leng et al., 2023) discovers that MLLMs generate high-confidence answers even when the image is noisy or absent, indicating a bias towards LLM priors. Similarly, PAI (Liu et al., 2024b) describes this phenomenon as \u201cText Inertia\u201d and posits that it stems from existing paradigms that map visual representations onto the text representations as tokens. This leads to an inference process that fails to adequately account for image tokens, resulting in hallucinations.\nHALLUCINATION MITIGATION FOR MLLMS\nOne straightforward approach to mitigate hallucination is to reduce the knowledge gaps and data bias between vision and language during model training. Finetuning-based methods have been explored, focusing on crafting specific datasets (You et al., 2024; Gunjal et al., 2024; Chen et al., 2024e) and alignment training (Sun et al., 2023; Yu et al., 2023; Chen et al., 2023; Li et al., 2023c) to achieve better knowledge alignment between images and text. While these methods have shown promising results, they often require expensive annotated paired data and substantial computational resources.\nHallucination can also be mitigated by post-processing methods, which usually involve using additional tools or self-reflection strategies to revise the response. For instance, LURE (Zhou et al., 2024) detects hallucinations using manually-crafted features and revises the generated text accordingly. Woodpecker (Yin et al., 2023) combines MLLM outputs with an expert VQA model to post-edit hallucinations. VOLCANO (Lee et al., 2023) trains MLLMs to provide self-feedback and reflect on the original generated text. However, these methods incur additional inference costs and delays, and require task-specific procedures and prompts to be designed (Xu et al., 2024). Training-free decoding methods have been explored to mitigate hallucination. OPERA (Huang et al., 2024) identifies an abnormal attention pattern that often accompanies hallucinated descriptions and proposes the mitigation method based on this pattern. VCD (Leng et al., 2023) introduces the notion that visual uncertainty increases hallucination and proposes a contrast decoding method to alleviate the issue. VDD (Zhang et al., 2024b) proposes a \"Post-Hoc debias\" approach that ensures uniform scores for each answer in the absence of an image to mitigate the influence of LLM priors."}, {"title": "CONCLUSION AND LIMITATIONS", "content": "In this paper, we demonstrate that MLLMs exhibit an awareness of hallucinated objects, with earlier layers showing higher confidence, while tokens shaped by prior knowledge diminish the likelihood of true tokens in the final layers. Based on this insight, we introduce DeCo, dynamic correction decoding with preceding-layer knowledge to mitigate hallucinations. Extensive experiments demonstrate the efficacy of our approach, which also shows advantages in latency and throughput.\nLimitations. (1) Lack of generalized research. Due to the GPU cost consideration, we conduct experiments solely on limited MLLMs, without exploring additional MLLMs or those with larger parameter sizes. (2) No free lunch. The results shown in Table 4 indicate that our method has a little negative impact on the level of detailedness metric. In future work, we aim to integrate DeCo with other strategies and explore approaches that can effectively balance truthfulness and diversity."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "We have submitted the relevant code in the supplementary materials. The names of the experimental benchmarks, the prompt templates used, and the model's hyperparameter settings can all be found in Section 4. The Appendix A.1 and A.2 provides a detailed description of the experimental setup for the mechanism experiments."}, {"title": "DETAILED EXPERIMENTAL SETUP", "content": "DETAILED SETTINGS FOR FINDINGS 1\nIn the probing experiment, we utilize the pipeline proposed in the POPE (Li et al., 2023b) to construct 1,200 balanced positive and negative sample pairs from the MSCOCO dataset as training data for the probe classifier, where each sample consists of an object accompanied by a label indicating its existence or non-existence. (Note: There is no overlap between the training data and the evaluation data for object hallucination proposed by the POPE). We select the AMBER dataset (Wang et al., 2023a), which has a different distribution from the MSCOCO dataset, to test whether our conclusions can generalize. The AMBER dataset contains 1,004 carefully annotated images, each labeled with existent objects as well as non-existent objects. We use the prompt \u201cDescribe the image.\" to generate raw responses from LLaVA-1.5 on the images and then extract all object category tokens and label them with whether they exist. Given that the training set contains only 80 object categories, we denote the object tokens in test data belonging to these 80 categories as in-distribution (in-dist), while the remaining tokens are categorized as out-of-distribution (OOD).\nPrevious work (Karamcheti et al., 2024) has demonstrated that increasing the resolution of the vision encoder enhances the visual comprehension capabilities of MLLMs. In our study, we compare LLaVA trained with a resolution of 224px against the original LLaVA with a resolution of 336px in probing experiments. Notably, the language model's weights differ between the two MLLMs, although both initial models are based on Vicuna-1.5-7b. Our results, as illustrated in the Figure 1(b), further affirm the scaling law associated with visual resolution, while also providing indirect validation of the reliability of the probing experiments.\nDETAILED SETTINGS FOR FINDINGS 2\nIn the early exit experiment, we randomly select 500 images from MSOCO and use random prompts (shown in Table 6) to elicit raw responses from LLaVA-1.5-7b. We then extract all non-existent objects along with their corresponding preceding text. Specifically, for the sentence \"Additionally, there is a car.\", we extract the hallucinated object token \"car\" and the preceding text \"Additionally, there is a\u201d. We re-input the preceding text into the MLLM and observe the changes in its internal state when predicting the next token. We denote that a total of K preceding texts are selected, with the j-th preceding text denoted as s\u00b9."}]}