{"title": "EvoAL 2048", "authors": ["Bernhard J. Berger", "Christina Plump", "Rolf Drechsler"], "abstract": "Explainability and interpretability of solutions generated by AI products are getting more and more important as AI solutions enter safety-critical products. As, in the long term, such explanations are the key to gaining users' acceptance of AI-based systems' decisions [4].\nWe report on the application of a model-driven optimisation to search for an interpretable and explainable policy that solves the game 2048. This paper describes a solution to the Interpretable Control Competition [6]. We focus on solving the discrete 2048 game challenge using the open-source software EVOAL [2, 8] and aimed to develop an approach for creating interpretable policies that are easy to adapt to new ideas. We use a model-driven optimisation approach [5] to describe the policy space and use an evolutionary approach to generate possible solutions. Our approach is capable of creating policies that win the game, are convertible to valid Python code, and are useful in explaining the move decisions.", "sections": [{"title": "1 INTRODUCTION", "content": "Explainability and interpretability of solutions generated by AI products are getting more and more important as AI solutions enter safety-critical products. As, in the long term, such explanations are the key to gaining users' acceptance of AI-based systems' decisions [4].\nWe report on the application of a model-driven optimisation to search for an interpretable and explainable policy that solves the game 2048. This paper describes a solution to the Interpretable Control Competition [6]. We focus on solving the discrete 2048 game challenge using the open-source software EVOAL [2, 8] and aimed to develop an approach for creating interpretable policies that are easy to adapt to new ideas. We use a model-driven optimisation approach [5] to describe the policy space and use an evolutionary approach to generate possible solutions. Our approach is capable of creating policies that win the game, are convertible to valid Python code, and are useful in explaining the move decisions."}, {"title": "2 APPROACH", "content": "The proposed solution builds on EvoAL-a Java-based data-science research tool-which allows users to express optimisation problems using domain-specific languages (DSLs) and offers a rich extension API for problem-specific extensions. EvoAL offers different optimisation algorithms, such as evolutionary algorithms, genetic programming, and model-driven optimisation.\nNormally, EvoAL uses two DSLs to configure an optimisation problem. Using the data description language, a user can specify the problem-specific data. The mapping to an optimisation algorithm and the algorithm configuration is done by using the optimisation language. As we aim to use a model-driven approach, we use a third DSL of EVOAL-the definition language-to describe the abstract syntax [3] of the policy. The generated model is then turned into Python code by using model-to-text concepts.\nOur policy model builds upon the idea that a game is in a state, which influences the action taken. The state can be queried by simple functions, such as Is a certain move valid?, or What is the gain of a specific move?. A policy then combines these functions into boolean expressions, which are checked to determine if a specific action should be executed. By using this approach, it is possible to add new query functions by a) implementing the Python code and b) extending the description file. It is not necessary to adapt EVOAL or our Java-based extension to do so."}, {"title": "3 EXPERIMENTAL RESULTS", "content": "The allowed budget contains 200.000 evaluations of the game. We configured the EA to use a population size of 100 individuals. For a fitness evaluation, we decided to simulate six games with the same policy, resulting in $\\frac{200000}{600}=333$ generations that can be executed.\nInitial Population The initial population contains random policies containing a single element in array-like references, c.f. Figure 2. Thus, the policies start with simple conditions that need to be mutated into more and more complex ones.\nOperators We mainly employ general (not specialised for the prob-lem) operators. We use a mutator that changes values (numbers, and boolean values), a mutator that changes array sizes (by removing or adding elements), a mutator that changes the order of arrays, and a custom mutator that rotates a given policy. We limit the number of mutators applied to an offspring to one, to reduce the changes in a policy. Additionally, we use a standard recombination operator, that swaps subtrees of two provided policies.\nFitness Our fitness function executes a policy a configurable num-ber of times and calculates different statistical information, such as the minimum, maximum, and average of these runs. For one run, we store the maximum tile value and the overall score. This allows us to focus on searching for robust algorithms. We use a priority-ordered pareto-comparison as a fitness comparator.\nAvailable Functions We aimed at using query functions that are simple and do not implement complex board situations. In total, we provide ten different query functions, such as canMoveInDirec-tion allows the policy to query if a single move into a direction is possible, whereas canMoveInDirections checks for two subsequent moves. scoreGain will calculate the score improvement gained by a single move, whereas scoreGains checks two subsequent moves. The complete list of functions can be found in the policy model, which is specified in the file model.dl.\nPipeline The pipeline configuration is stored in the aforementioned DSL files, which can be found in the folder evoal-configuration of the supplemental repository [1]. The files can be read with a text editor. EvoOAL's Eclipse-based DSL editors provide additional syntax highlighting and cross-referencing. The pipeline has been tested on Linux and MacOS using a Java 17 JRE. To run the pipeline, it is necessary to checkout the repository, set up the Python en-vironment, download an EvOAL release [7] and extract it to the folder evoal-release in the repository and execute the script file 01-run-search.sh.\nThe result of the optimisation run is a policy that reached a max(highest-tile) of 2.048, an average(highest-tile) of 1.276, and an average (total-score) of 14.093. The policy is, during the opti-misation, an instantiation graph of the model shown in Figure 2, which can be converted into a graphical representation or a textual representation. Nevertheless, Listing 1 shows the best policy after converting it into a Python program as this is the executable policy.\nThe shown policy focuses on increasing the score gain and chooses to go in the direction that promises higher score gain. The remaining queries, such as willBeSorted, are part of some poli-cies but did not make it into the best policy. At the same time, the policy only uses three out of four directions, which might leave room for further improvement, but we assume that the situation where the board would have to be moved into the fourth direction occurs very seldom.\nHaving a given board situation, the policy allows one to explain precisely why a certain move was made. On the one hand, the state queries are easy to understand and, on the other hand, they can be calculated for a given board to show the decision process. While being explainable, our approach is flexible and can easily be extended with additional queries without having to change the optimisation process."}]}