{"title": "Towards Automated Cross-domain Exploratory Data Analysis through Large Language Models", "authors": ["Jun-Peng Zhu", "Boyan Niu", "Peng Cai", "Zheming Ni", "Jianwei Wan", "Kai Xu", "Jiajun Huang", "Shengbo Ma", "Bing Wang", "Xuan Zhou", "Guanglei Bao", "Donghui Zhang", "Liu Tang", "Qi Liu"], "abstract": "Exploratory data analysis (EDA), coupled with SQL, is essential for data analysts involved in data exploration and analysis. However, data analysts often encounter two primary challenges: (1) the need to craft SQL queries skillfully, and (2) the requirement to generate suitable visualization types that enhance the interpretation of query results. Due to its significance, substantial research efforts have been made to explore different approaches to address these challenges, including leveraging large language models (LLMs). However, existing methods fail to meet real-world data exploration requirements primarily due to (1) complex database schema; (2) unclear user intent; (3) limited cross-domain generalization capability; and (4) insufficient end-to-end text-to-visualization capability.\nThis paper presents TiInsight, an automated SQL-based cross-domain exploratory data analysis system. First, we propose hierarchical data context (i.e., HDC), which leverages LLMs to summarize the contexts related to the database schema, which is crucial for open-world EDA systems to generalize across data domains. Second, the EDA system is divided into four components (i.e., stages): HDC generation, question clarification and decomposition, text-to-SQL generation (i.e., TiSQL), and data visualization (i.e., TiChart). Finally, we implemented an end-to-end EDA system with a user-friendly GUI interface in the production environment at PingCAP. We have also open-sourced all APIs of TiInsight to facilitate research within the EDA community. Through extensive evaluations by a real-world user study, we demonstrate that TiInsight offers remarkable performance compared to human experts. Specifically, TiSQL achieves an execution accuracy of 86.3% on the Spider dataset using GPT-4. It also demonstrates state-of-the-art performance on the Bird dataset. During six months of public testing, TiSQL demonstrates an execution success rate of 82.3% on real-world EDA tasks.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 Problem Statement", "content": "Exploratory data analysis (EDA) [36, 50, 51, 53, 81], coupled with SQL, assumes a crucial role for data analysts engaged in data exploration and analysis. The EDA harnesses SQL to construct queries capable of extracting vital information from databases. Generally, it involves \"hands-on\" interaction with a dataset, where users iteratively apply analysis actions (e.g., filtering, aggregations, sorting, visualizations), generating statistical insights, and constructing comprehensive views and reports. The EDA process facilitates a deeper understanding of the data, revealing latent patterns and trends that offer valuable insights to guide subsequent decision-making endeavors. In particular, the user explores a dataset D = {databases, schema, tuples, columns}, where databases refer to the data sources the user wants to explore, which may encompass multiple databases; schemas refer to the database schema, encompassing tables, indexes, views, and other related elements; tuples represent individual records of data, and columns represent the attributes of those tuples. The data analyst then performs a series of data analysis operations $s_1, d_1, s_2, d_2, ..., s_n, d_n$, where $s_i$ indicates an SQL statement and $d_j$ represents the visualization of the results. The user determines the next steps after executing $s_t$ and $d_t$. The EDA"}, {"title": "1.2 Limitations of Prior Art", "content": "There are many state-of-the-art (SOTA) approaches to address both challenges. Specifically, regarding SQL proficiency, there are numerous SOTA text-to-SQL approaches, which effectively reduce the complexity of crafting SQL queries for data analysts. However, in the EDA context, SOTA text-to-SQL approaches still have their limitations:\n(1) Complex database schema. In real-world EDA scenarios, the data to be explored is typically stored in databases with complex schema [32, 43, 77]. For example, in financial data analysis, each database contains numerous wide tables with hundreds of columns [32, 77]. The schema complexity far exceeds that of existing benchmarks, such as Spider [76]. Existing text-to-SQL methods struggle to handle such complex scenarios, as evidenced by low accuracy [77] on this dataset. Moreover, large language model-based methods must consider context window limits [54, 55, 68] when constructing prompts, significantly impacting accuracy in complex scenarios. In the worst-case scenario, a query involving numerous tables and hundreds of columns may exceed the context window limits during prompt construction, preventing the method from functioning properly. On the other hand, the large number of tables and columns increases the context length, which can significantly decrease accuracy. In real-world customer scenarios at PingCAP, users define numerous abbreviations that may refer to a business, a location, or other entities. This further increases the complexity of the schema, requiring more domain knowledge.\n(2) Unclear user intent. In real-world EDA scenarios, users often find it challenging to express their thoughts in natural language, which makes it difficult to convey their intent [32, 36]. This challenge is evident in users' inability to formulate clear intent parameters. For example, in the customer scenarios of PingCAP, customers often ask, \"What is the growth rate?\". This query may lack a time parameter; a more precise formulation would be, \"What is the growth rate for the current year?\". Additionally, user queries frequently include abbreviations [77]; for example, \"DoD analysis for daily bills.\". In the worst-case scenario, users may struggle to clearly articulate their intent at the beginning of a data analysis task. In many cases, the user's natural language (NL) intentions cannot be captured by a single SQL statement [32, 57, 70]. Accurately addressing these queries requires not only the semantic parsing capabilities of large language models but also robust data analysis skills to infer intent beyond the explicit query [36]. Existing SOTA text-to-SQL approaches struggle to generate SQL in this context, let alone address accuracy issues [32, 33, 57].\n(3) Limited cross-domain generalization. Existing text-to-SQL methods [43, 77] are designed for specific domains, which results in poor execution accuracy when applied to a different data domain, requiring fine-tuning for each domain. In an end-to-end enterprise-grade system, this becomes virtually impossible. Furthermore, fine-tuning introduces substantial overhead in terms of time, space, and cost. Consequently, integrating SOTA text-to-SQL approaches into an end-to-end EDA system proves challenging. For example, in the customer scenarios of PingCAP, there are applications in finance, retail, and other industries, but SOTA methods"}, {"title": "1.3 Key Technical Challenges", "content": "Enabling an end-to-end SQL-based automated EDA system offers significant benefits for data exploration and analysis. However, there is no free lunch in this context. There are three major challenges that need to be addressed:\nC1. How to mitigate the impact of complex database schema and unclear user intent? This is primarily due to the prevalence of complex database schema and ambiguous user intentions in real-world EDA scenarios, significantly reducing EDA task accuracy. However, addressing these challenges without incurring additional costs remains highly difficult.\nC2. How to improve generalization and adaptability across different data domains? In industrial practice at PingCAP, various business data scenarios, such as finance and retail, are commonly encountered. EDA methods are typically tailored to specific data domains, leading to the absence of a universal EDA system that"}, {"title": "1.4 Proposed Approach", "content": "In this paper, we propose TiInsight, a SQL-based enterprise-grade multi-stage automated cross-domain exploratory analysis system through large language models. To address the first two challenges, we propose HDC, a hierarchical data context approach that leverages LLMs to summarize the database schema context, which is vital for open-world EDA systems to generalize across exploratory data domains. First, TiInsight summarizes database-related context information using HDC, such as column descriptions and table summaries. The primary data entities representing the database are subsequently abstracted. Ultimately, a database summary is generated that encompasses the entity descriptions, the tables associated with those entities, the key attributes of the entities, and the columns to which these entities may be mapped. These data contexts are stored in vector databases (e.g., ChromaDB [3], Pinecone [12], TiDB Vector [17]). Drawing on this data context, TiInsight also recommends several exploratory questions to facilitate the user's understanding of the data.\nTiInsight is divided into four components (i.e., stages): HDC generation, question clarification and decomposition, text-to-SQL generation (i.e., TiSQL), and data visualization (i.e., TiChart). Subsequently, TiInsight processes the natural language questions posed by users. Initially, it clarifies the user questions within the hierarchical data context, generating a precise data exploration question. It then determines whether the question can be answered with a single SQL statement. If not, it decomposes the complex question into multiple sub-questions and provides a detailed description of each sub-question. These clarification questions are subsequently provided to the TiSQL, which generates the corresponding SQL statement and integrates it with the self-refinement chain for further improving text-to-SQL accuracy. Finally, TiInsight generates the visualization chart utilizing the integrated TiChart. An end-to-end EDA system with a user-friendly GUI interface has been developed and deployed in the production environment at PingCAP."}, {"title": "1.5 Key Contributions and Outline", "content": "To summarize, this paper makes the following contributions:\n(1) We investigate how to design key components in an SQL-based automated EDA system and analyze why existing methods are inadequate for enterprise-grade automated EDA systems.\n(2) We propose a hierarchical data context approach called HDC. TiInsight with HDC is designed to facilitate data exploration of complex enterprise-grade datasets and enable cross-domain data analysis. Furthermore, we introduce a map-reduce framework into"}, {"title": "2 The Overview of TiInsight", "content": "This section gives a high-level introduction of TiInsight. As shown in Figure 1, Tilnsight has four functional components to facilitate cross-domain exploratory data analysis tasks.\nHDC generation. We perform cross-domain exploratory data analysis. To address C1 and C2, we design HDC generation, which uses LLMs to generate the hierarchical data context of the database schema efficiently. The LLMs are powerful for summarizing"}, {"title": "3 Hierarchical Data Context and Question Clarification and Decomposition", "content": "In this section, we first present a detailed implementation of the hierarchical data context (\u00a73.1). Then, we give the design for the clarification and decomposition of the questions (\u00a73.2)."}, {"title": "3.1 Hierarchical Data Context", "content": "The hierarchical data context (i.e., HDC) forms the foundation of TiInsight, with all subsequent component designs built upon it. The hierarchical data context is designed primarily to improve the understanding of the database schema, which includes column information, table information, table-to-table relationships, and overall database details. This design is derived from experience during the proof-of-concept (POC) process at PingCAP. In PingCAP's data analysis business operations, data analysts first collect and organize relevant database schemas information when encountering unfamiliar datasets to gain a better understanding of the database. We leveraged this experience and fully automated the process using LLMs. As illustrated in Figure 2, the HDC comprises three components: column summary (\u00a73.1.1), table summary (\u00a73.1.2), and database summary (\u00a73.1.3). Due to space constraints, the prompt used in this section is not included in the paper.\n3.1.1 Column Summary. Columns (a.k.a., fields or attributes) are the fundamental components of a database schema. In real-world scenarios, two main challenges arise with columns: (1) column names are often abbreviated, and (2) there can be a very large number of columns, sometimes reaching thousands, and each column contains a lot of data. To address the first technical challenge, we propose building an embedding service to store domain knowledge (a.k.a., terms) in a vector database. When the LLMs generate a summary of column descriptions, they first retrieve relevant domain knowledge from the vector database, dynamically constructing prompts as supplementary explanations.\nA column-by-column summarization approach would result in high latency due to repeated calls to the large language model. Additionally, using the LLM for each column individually is inefficient, leading to significant token wastage and increased costs. To address the second challenge, we propose a parallel, grouping-based approach to mitigate this issue by vertically partitioning the table. First, HDC groups columns into fixed sets (i.e., vertical partitioning); empirically, groups contain 40 columns for GPT-4 and 80 columns for other models. This limitation is primarily due to the context window constraints of these models. Each group is subsequently allocated threads from a thread pool before the LLM initiates the column description process. Comments may be included in the schema information provided by the schema for each column. For example, the date column might include a description of the date format, such as YYYY-mm-dd. After the aforementioned process, we append the column comments to each column description. Additionally, for each column, we randomly select several rows (i.e.,"}, {"title": "3.1.2 Table Summary", "content": "Next, we utilize the column summary obtained in Section \u00a73.1.1 to summarize the information for each table in a table-by-table manner. The table summary consists primarily of two components: the table description and the table relationship.\nTable Description. In real-world scenarios, tables may contain thousands of columns, each potentially with millions of rows. Encoding the table data within a prompt is impractical, so we encode only the column summaries. However, large and wide tables cannot encode all columns in a single prompt, as this often exceeds the context window limits of LLMs. Consequently, context window limits are a significant challenge in table summarization for LLMs. In this paper, we propose a map-reduce framework to summarize large tables. Algorithm 1 provides a detailed implementation of the table summary obtained using the map-reduce framework. First, it divides the wide table into multiple blocks using a vertical partitioning of columns. Then, it creates a summary for each block (lines 1-9). In this process, we retrieve the column descriptions corresponding to each block from the vector database. The prompt is constructed dynamically using this information (line 6). Finally, it uses the reduce mode to generate the table description (lines 10-27). Additionally, we save the context information of the table description obtained from the exploration to the vector database (line 31). The map-reduce framework offers two key benefits: (1) It prevents exceeding the context window limits of the LLM, and (2) It supports multi-threads in parallel summarization across different stages.\nTo create a comprehensive table description, we need to gather additional information, including the primary key, key attributes, table type, table entity, and natural language description. Specifically, the LLM may identify multiple primary keys, and we prompt it to select the most likely one. This primary key should better reflect the importance and uniqueness of the table. Key attributes play a vital role in understanding the purpose and content of the table. We prompt the LLM to identify and list a maximum of five key attributes from the table presented in this paper. In addition, we categorize the table type as either dimension, bridge, or fact. This classification is especially crucial for the OLAP analysis. Finally, we identify the main entity that the table focuses on.\nTable Relationship. Table relationships identify all referential integrity relationships between a specified table and others, which helps to optimize complex queries. We need to identify the following"}, {"title": "3.1.3 Database Summary", "content": "To facilitate efficient data exploration across multiple databases, we propose extracting representative entities for each database, enabling a database summary based on these abstracted entities. A database consists of numerous tables with complex relationships. Following the influence maximization principle [46], a table with a higher number of relationships to other tables is considered more important within the database. Conversely, for a table with very few relationships, its properties are generally insignificant. Therefore, we propose an entity extraction method that leverages the number of relationships between tables, as outlined in Algorithm 3. The algorithm first selects the top N tables (line 2) with the highest number of relationships from the current database. In the PingCAP business, the value of N is typically set to 20. A prompt is then constructed using these tables and their summaries, and the LLM is employed to infer their entities (lines"}, {"title": "3.2 Question Clarification and Decomposition", "content": "Question Clarification. Upon data import into TiInsight, our system initiates a rapid analysis to establish a hierarchical data context, enabling the user to explore the dataset effectively. A key challenge for TiInsight is that user questions can be semantically unclear. For example, in the user scenario of the PingCAP, a common question"}, {"title": "4 TiSQL: Text-to-SQL with Self-refine Chain", "content": "In real-world scenarios, a database typically includes thousands of tables, each with hundreds or thousands of columns, and each table holds millions of records. Encoding all of this information into a prompt would exceed the context window limits of LLM. Therefore, in text-to-SQL tasks, it is essential to streamline the database schema, as an excessive number of schemas can cause errors during the schema-linking [42] process. To address these challenges, we propose a schema filtering framework based on the map-reduce paradigm to filter tables and columns. The schema filtering algorithm is presented in Algorithm 4.\nAs shown in Algorithm 4, TiSQL begins with a coarse-grained search, retrieving the top N relevant tables from the vector database based on the clarified question (i.e., clarified_task) and cosine similarity (line 4). In PingCAP practice, this parameter is set to 30. Then, TiSQL performs fine-grained filtering to exclude irrelevant tables and columns. To prevent the associated table and column summaries from exceeding the LLM's context limit, TiSQL divides these tables into more granular groups of related tables (line 5)."}, {"title": "5 TiChart: Rule-Based Visualization", "content": "Data visualization is a key component of TiInsight, aiding users in comprehending data exploration results more effectively. In previous data analysis systems, most users needed to use d3js [10] and other tools for data visualization. However, these systems lacked an end-to-end solution with minimal human intervention. To address these limitations, we propose a rule-based data visualization approach. In the PingCAP business scenario, we collected chart visualization preferences of users and combined these rules with LLM recommendations to generate visual output."}, {"title": "6 Experimental Evaluation", "content": "In this section, we empirically evaluate the design of TiInsight using representative benchmarks. In particular, we focus mainly on the following research questions (RQs).\n\u2022 RQ1: How accurate is TiSQL in performing text-to-SQL tasks? We focus on performance, specifically referring to execution accuracy (EX). In the SQL-based EDA system, the accuracy of SQL significantly influences the overall performance of the EDA system.\n\u2022 RQ2: How effective is Tilnsight in supporting user tasks? This performance includes relevance - Do TiInsight accurately identify and address the core aspects of the given analysis objective?, completeness - Does TiInsight provide necessary information to address the given analysis objective?, and understandability - Does TiInsight response easily interpretable, clear, and understandable for users, even with complex analysis objectives?.\n\u2022 RQ3: How does latency between different components affect the effectiveness of the design in achieving the analysis objectives?\n\u2022 RQ4: How do different LLMs compare in terms of costs? We focus on balancing accuracy and cost in the production environment at PingCAP.\nIn the following, we answer RQ1 in Section \u00a76.2, RQ2 in Section \u00a76.3, RQ3 in Section \u00a76.4 and RQ4 in Section \u00a76.5."}, {"title": "6.1 Experimental Setup", "content": "6.1.1 LLMs. We support prompt-based LLMs, including GPT-4-0613 (referred to as GPT-4) [54], GPT-40 [9], GPT-40 mini [8],"}, {"title": "6.2 Performance of TiSQL (RQ1)", "content": "In this section, we evaluate the performance of TiSQL using the popular text-to-SQL benchmarks, Spider and Bird. We employ the execution accuracy (EX) metric for the Spider primarily because our main concern in practical text-to-SQL is whether the generated SQL statements yield the correct execution results. We utilize the EX metric and the reward-based valid efficiency score (R-VES) for the Bird benchmark. The experimental evaluation of TiSQL is available at https://github.com/tidbcloud/tiinsight.\nIn Table 1, we compare the EX metric in TiSQL with those of the rule-based, PLM-based, and LLM-based text-to-SQL approaches using the Spider test dataset. The results indicate that TiSQL with GPT-4 (i.e., TiSQL + GPT-4) achieves SOTA performance, reaching 86.3%. The DAIL-SQL employs the GPT-4 model along with the self-consistency SQL refinement, achieving an accuracy of 86.6%. In comparison, TiSQL is only 0.3% lower in execution accuracy than DAIL-SQL + GPT-4 + SC. However, the self-consistency approach is time-consuming and incurs significantly higher costs than the original DAIL-SQL method. Consequently, self-consistency is not employed in TiSQL. In the published DAIL-SQL, the original DAIL-SQL remains the primary focus; however, our TiSQL achieves a performance improvement of over 0.1% compared to the original. It is noted that TiSQL does not undergo any fine-tuning process, yet its execution accuracy still exceeds that of the SOTA PLM-based"}, {"title": "6.3 User Study of TiInsight (RQ2)", "content": "We invited 20 participants for this user study. These individuals have data analysis experience, typically perform data analysis tasks, and are relatively familiar with SQL. Initially, TiInsight recommends three exploration questions to the participants, allowing them 60 minutes to explore the three datasets, including Financial, Spider,"}, {"title": "6.4 Experiments on Latency (RQ3)", "content": "We evaluate the average latency of various components of TiInsight on the Spider dataset. For users, average latency serves as a crucial indicator of system performance. Figure 9 depicts the average time required to generate the HDC for a database and to generate an SQL during concurrent exploration of varying numbers of databases. The concurrency involves using N databases for HDC and SQL generation concurrently. The average latency of HDC generation increases with the number of explored databases. This is primarily because as the number of databases increases, more tables and columns must be explored to generate hierarchical data contexts. The average latency of SQL generation by TiSQL is also increasing, primarily because TiSQL must filter a greater number of tables and columns. It is also observed that a single HDC generation process requires more time than that of one text-to-SQL query. However, it is important to note that HDC generation occurs only once during the entire data exploration process; thus, this time is distributed across multiple analysis targets, resulting in an acceptable average latency. The design of TiInsight adopts an asynchronous mode, wherein upon submission of the analysis target, TiInsight promptly returns a bound ID to the user. At the same time, a background thread handles the binding task. This design enhances the overall user experience."}, {"title": "6.5 Experiments on Cost (RQ4)", "content": "In Figure 10, we analyze the average cost of Tilnsight when utilizing various LLMs across different datasets. Table 5 presents the prices"}, {"title": "7 Related Works", "content": "Data Exploration. Data exploration is a time-consuming task. Numerous research and commercial systems have been developed to facilitate this task [1, 13, 18, 24, 25, 27, 29, 31, 37, 40, 49-53, 56, 66, 81]. Some research [1, 13, 40] focuses on the interface within the data exploration process, implementing interactive methods to assist non-expert users. Some research [24, 31] offers user-friendly data exploration interfaces for non-expert users via query-by-example. These systems provide user input and output examples, after which the EDA system infers the necessary query. Advancements in text-to-SQL technology have diminished the appeal of these systems. Several end-to-end EDA systems [29, 52, 53] utilize deep reinforcement learning to enhance data exploration. The complexity of reinforcement learning hinders the widespread adoption of these systems in real-world industries. DataPrep.EDA [56] is a task-centric EDA system that uses Python. FEDEX [25] provides coherent explanations for the data exploration steps. All of these systems automatically generate insights to guide the user. Some research [27, 49, 51, 66] focuses on the automatic discovery of insights in multidimensional data. These studies concentrate on exploring and mining significant patterns within the data. ChatGPT [55] can also be utilized for data exploration. Chat2Query [81] is an SQL-based automated exploratory data analysis system through LLMs. However, these systems cannot perform cross-domain exploratory data analysis and cannot adapt to rapidly changing business scenarios.\nText-to-SQL. Several state-of-the-art methods [38, 39] for text-to-SQL are broadly classified into rule-based, neural network (NN)-based, pre-trained language model (PLM)-based, and large language model (LLM)-based approaches. (1) Rule-based Approaches. Duoquest [20], Athena [61] and Athena++ [63] are three representative rule-based approaches. These methods rely on pre-defined rules or semantic parsers. However, these methods are constrained in their generalization and adaptability. (2) NN-based Approaches. Some research [21-23, 35, 73, 75, 79] utilize neural networks to learn the mapping from natural language to SQL queries, with the"}, {"title": "8 Conclusion", "content": "In this paper, we propose TiInsight, a SQL-based multi-stage automated exploratory data analysis system through LLMs to facilitate real-world cross-domain scenarios. We propose a hierarchical data context (i.e., HDC) approach to achieve cross-domain data analysis. HDC employs a large language model to efficiently explore the database schema, including columns, tables, and database summaries, thus facilitating a better understanding of the databases. We propose a series of algorithms combined with a map-reduce framework to enable efficient parallel HDC generation. Building on this foundation, we propose an efficient text-to-SQL method TiSQL and a rule-based data visualization method TiChart. Extensive experiments conducted on various evaluation datasets demonstrate the effectiveness of Tilnsight. We open source Tilnsight and its APIs to facilitate research within the data analysis community."}]}