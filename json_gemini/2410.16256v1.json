{"title": "CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution", "authors": ["Maosong Cao", "Alexander Lam", "Haodong Duan", "Hongwei Liu", "Songyang Zhang", "Kai Chen"], "abstract": "Efficient and accurate evaluation is crucial for the continuous improvement of large language models (LLMs). Among various assessment methods, subjective evaluation has garnered significant attention due to its superior alignment with real-world usage scenarios and human preferences. However, human-based evaluations are costly and lack reproducibility, making precise automated evaluators (judgers) vital in this process. In this report, we introduce CompassJudger-1, the first open-source all-in-one judge LLM. CompassJudger-1 is a general-purpose LLM that demonstrates remarkable versatility. It is capable of: 1. Performing unitary scoring and two-model comparisons as a reward model; 2. Conducting evaluations according to specified formats; 3. Generating critiques; 4. Executing diverse tasks like a general LLM. To assess the evaluation capabilities of different judge models under a unified setting, we have also established JudgerBench, a new benchmark that encompasses various subjective evaluation tasks and covers a wide range of topics. CompassJudger-1 offers a comprehensive solution for various evaluation tasks while maintaining the flexibility to adapt to diverse requirements. Both CompassJudger and JudgerBench are released and available to the research community at CompassJudger. We believe that by open-sourcing these tools, we can foster collaboration and accelerate progress in LLM evaluation methodologies.", "sections": [{"title": "1 Introduction", "content": "The evaluation of large language models (LLMs) typically falls into two major categories: objective evaluation and subjective evaluation. Objective evaluation evaluates LLMs using questions with ground-truth answers (multiple-choice questions, fill-in-the-blank questions, etc.). Given LLM responses and answers, rule-based approaches are adopted to check if the responses are correct or not. Subjective evaluation, on the other hand, often requires a more comprehensive assessment of models' responses from multiple perspectives such as helpfulness, honesty, or creativity (Liu et al., 2023). Most questions in subjective evaluation do not have a ground-truth answer, thus requiring human judgement for response quality assessment.\nDespite the effectiveness, human-based subjective evaluation is time-consuming, labor-intensive, as well as hard to reproduce. Therefore, in early stages of subjective evaluation, using the best-performing model (such as GPT4 (Achiam et al., 2023)) to evaluate the responses of LLMs became the most adopted approach, which has been applied to multiple subjective evaluation benchmarks including AlpacaEval (Dubois et al., 2024a), ArenaHard (Li et al., 2024), WildBench (Lin et al., 2024), and AlignBench (Liu et al., 2023). However, those judge models with stronger critique capabilities are often commercial APIs with limited transparency and charge per use. The cost can be prohibitive for research studies when evaluations are frequently conducted.\nRecent efforts in the research community have focused on developing open-source judge models for subjective evaluation of large language models (LLMs). For example, Cri-"}, {"title": "2 CompassJudger-1", "content": "In this section, we first introduce the training data used to build CompassJudger-1 in Sec. 2.1, followed by the details of training strategies in Sec. 2.2"}, {"title": "2.1 Data Collection", "content": "High-quality data is essential for training an effective model, and the judge model is no exception. Therefore, we focus on data collection and the composition of the training dataset. Our training data is primarily divided into three categories: publicly available judge data, self-collected subjective evaluation data, and reward data used for training reward models.\nWe present the relevant information of all the data in Table 1. The table clearly shows that the number of entries in the training data pool from different sources can vary greatly, making it highly imbalanced. Due to the unique nature of judge data, the output format from the same dataset is often uniform. This can cause the model's response patterns to become rigid, which is a key reason why most judge models struggle with out-of-domain evaluation. Therefore, appropriate sampling strategy is necessary when incorporating the data into the final training set.\nAdditionally, the proportion of generative data in the overall data pool is relatively small, with most data containing only judgment results rather than the reasoning behind them."}, {"title": "2.2 Training Strategy and Ablation Study", "content": "In this section, we present our training configuration and discuss our ablation study on different data sources.\nTraining Configuration Based on the processed training data pool, we conduct experiments with varying data ratios. We employ Xtuner (Contributors, 2023b) as our training framework and, through extensive experimentation, determine that an epoch of 2 and a learning rate of 2e-5 are optimal parameters. We perform ablation experiments specifically on the proportions of reward data and general SFT data.\nAblation Study on Reward Data We first examine the proportion of the reward data. As the pairwise evaluation tasks in reward data only require the categorical annotation, we can easily collect a large number of data from the public community. While incorporating reward data benefits the training of an all-in-one judge model, our experiments reveal that excessive reward data can lead to model overfitting, resulting in simplistic outputs resembling a reward model and compromising the ability to perform complex critique tasks. Through experimentation (See in Table 2), we determine that the optimal proportion of reward data lies between 50% and 70%, enabling the model to achieve strong judging capabilities while maintaining generalizability.\nInfluence of General SFT Data We then investigate the impact of general SFT data proportions. Since our goal is to create a powerful all-in-one model with judging capabilities rather than a model specific to particular datasets, incorporating general SFT (G-SFT)"}, {"title": "3 JudgerBench", "content": "In this section, we introduce JudgerBench, a specialized evaluation dataset designed to evaluate the judge models. To replicate realistic judge model application scenarios, JudgerBench incorporates two distinct types of annotations: human annotations for the arena part and LLM annotations for the benchmark component. We detail the construction process of these components in Sec. 3.1 and present CompassJudger's performance on JudgerBench in Sec. 3.2."}, {"title": "3.1 JudgerBench Construction", "content": "JudgerBench consists of two primary components: the Arena component (denoted as JDB-A) and the Benchmark component (denoted as JDB-B). JDB-A, similar to RewardBench, focuses on alignment with human preferences and requires only simple judge outputs such as [[A]] or [[B]]. JDB-B, on the other hand, assesses the model's critique capabilities and its ability to provide judgments following specific formats."}, {"title": "JudgerBench A (Arena Part)", "content": "\u2022 Data Source. JDB-A consists of two sections: English and Chinese. The English section is derived from the released data of Chatbot Arena (Chiang et al., 2024), while the Chinese section comes from the collected data of CompassArena (Contributors, 2023a). These data are the results of human voting, with each entry containing a question and the corresponding responses from two models, along with the vote on which model is the winner. Both Chinese and English include single-turn and multi-turn dialogue data, with approximately 500 single-turn dialogues and 250 multi-turn dialogues, totaling around 1500 pieces of data. We introduced in next paragraph how we screen and obtain these data.\n\u2022 Screening Process. The 1,500 data points in JDB-A were obtained through the following screening process: We first performed unsupervised clustering (specifically, k-means with k set to 50 in our implementation) on all the data to get rough categories. Then, the Processor model (Qwen2.5-72B) summarized specific category names based on typical cases within these categories (detailed category names can be found in the results of Table 7). After obtaining the top 10 category names summarized by the processor, we used the processor to process each data point individually, assigning them to their respective categories. Following"}, {"title": "JudgerBench B (Benchmark Part)", "content": "\u2022 Construction Methods. JDB-B primarily includes four datasets (AlignBench (Liu et al., 2023), ArenaHard (Li et al., 2024), FoFo (Xia et al., 2024), and WildBench (Lin et al., 2024)), which are very commonly used in subjective evaluations, covering different subjective scenarios (such as daily chat, instruction following), different evaluation methods (such as scoring, head-to-head competition), different languages (Chinese and English), and single and multi-turn dialogues. We detail the relevant properties of these datasets in Table 4. For these four subjective datasets, we sampled 100 questions from each dataset according to their respective subcategories, totaling 400 questions. Then, we used the top 10 closely ranked models from the OpenCompass leaderboard\u00b2 to obtain their responses to these 400 questions (specific model information can be found in the data json), thus acquiring a total of 4000 QA pairs, note that these 10 models have very similar and high capabilities, which also demonstrates the difficulty of judging with JDB-B. We then used GPT-403 to judge these pairs, using this judgment as a benchmark to check whether the judgment results of other models align with those of GPT-40.\n\u2022 Evaluation Metrics. To facilitate the research, we adopt the GPT-40's judgement as the reference ground-truth (though there may exist noise and errors). We calculate from two metrics for JDB-B, accuracy and correlation."}, {"title": "3.2 JudgerBench Results", "content": "Overall Results\nWe test baseline Chat models, current SOTA Judge Models, and our CompassJudger series on RewardBench and JudgerBench, reporting the overall results in Table 5, with more detailed results presented in subsequent tables. Notably, many judge models failed to adhere to the prompt templates of the subjective datasets, leading to test failures, and we showcase one of these failure cases in Appendix A.3. From the table, it can be observed that our CompassJudger series outperforms all open-source models and achieves over 95% of GPT-40's judging capability in the relevance tests on JDB-B. While GPT-40 demonstrates high consistency with human evaluation results across different data domains, some models (such as Selftaught, skyworker), despite achieving high scores in one domain like RewardBench, lose a certain degree of generalization in other domains and do not possess good universal judging capabilities. Even though they are generative models, they are no longer able to follow instructions to evaluate common subjective datasets. In contrast, CompassJudgerv1 achieved relatively balanced results on both RewardBench and JDB-A, and showed a significant lead on JDB-B.\nReward Bench Results\nWe present the detailed results on RewardBench in Table 6. For RewardBench, some existing Judge Models perform well (e.g. Skywork and Selftaught), even surpassing GPT-40. However, upon closer inspection, the main gap is evident in the Chat Hard category. The number of questions in this category does not constitute a large proportion of the total questions on RewardBench. There is a possibility of over-training with respect to these Judge Models, and our CompassJudger series also shows improving scores in this category as the model size increases.\nJudgerBench A Results\nThe detailed results for JudgerBench A are presented in Tables 7 and 8, showing that there are slight differences in the judging capabilities of various models in both Chinese and English domains. For instance, the English reasoning judging performance of Qwen2.5-7B-Chat is lower than its Chinese reasoning performance, and the English scores for humanities are also much lower than the Chinese scores for all models. On the other hand, GPT-40"}, {"title": "4 Conclusion and Discussion", "content": "In this report, we propose the all-in-one Judge model, CompassJudger, and the JudgerBench for evaluating model judging capabilities. Our CompassJudger model has achieved the best results among open-source models on JudgerBench and is truly capable of replacing GPT-40 for evaluating common subjective datasets, greatly reducing the cost of subjective evaluations. In addition, we have many issues for further discussion.\nHow can the Judge Model assist in the iteration of models? The potential of a good judge model is not limited to just judging and critiquing capabilities; it can also help models iterate and evolve. It should point out the shortcomings of the model when answering questions and provide guidance, which is more conducive to the model improving its responses or achieving a more standardized answer\u2014something that a regular reward model cannot accomplish. This has been validated in related experiments, and our internal experimental results are coming soon, which will demonstrate how we use the judge model to facilitate better iteration of the model.\nIf the model's judging capability is part of its general abilities, can judge training enhance the model's overall general capabilities? Just as GPT-4 can handle all judge tasks, we believe that judge capability is just a part of the model's general abilities, with a focus on reasoning and instruction following. Our experiments have also observed that good training in instruction following and reasoning abilities can significantly improve the model's judging capability. Conversely, relevant judge data can further enhance the model's reasoning and instruction following abilities."}, {"title": "5 Acknowledgement", "content": "We would like to express our sincere gratitude to InternLM's post-training team for generously providing the data used in this research. Their contribution is essential to the success of this project. We also extend our thanks to Jiaye Ge for their invaluable support, guidance, and coordination throughout the project."}, {"title": "A Appendix", "content": "A.1 Prompt and category label for data categorization of public judge data.\nPrompt for categorization\nCATEGORY MAP = [\"General Q&A\", \"Tech Consulting\", \"Education Tutoring\", \"Healthcare\", \"Travel\", \"Finance & Investment\u201d, \"Legal Advice\", \"Psychological Counseling\", \"Entertainment Gossip\", \"Cuisine & Cooking\", \"Home Improvement\", \"Auto Maintenance\", \"Video Games\", \"Sports & Fitness\", \"Literature & Art\", \"History & Humanities\", \"Politics & Current Events\", \"Religion & Faith\", \"Parenting Education\", \"Pet Care\", \"Career Planning\", \"Shopping Recommendations\", \"Lifestyle Services\", \"Relationships & Emotions\", \"Social Networking\u201d, \"Programming & Development\", \"Data Analysis\", \"Marketing\", \"Business Management\", \"Entrepreneurship Guidance\", \"Scientific Exploration\", \"Environmental Protection\", \"Other\"]\nCATEGORY PROMPT = I need to categorize the user's question. Below is a category map. Please help me categorize the user's question into one of these categories. The category map is as follows: {category map}\n[Start of User Question]\n{question}\n[End of User Question]\nPlease provide the category strictly in the following format and do not output any other statements:\n[Start of Question Categorization]\nXXXX\n[End of Question Categorization]\nA.2 Prompt for getting critique of reward data.\nPrompt for getting critique\nPlease act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user's instructions and answers the user's question better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better.\n[User Question]\nXXX\n[The Start of Assistant A's Answer]\nXXX\n[The End of Assistant A's Answer]\n[The Start of Assistant B's Answer]\nXXX\n[The End of Assistant B's Answer]\nA.3 Failure Case for JudgerBench B"}, {"title": "Judge Prompt:", "content": "Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user prompt displayed below. You will be given assistant A's answer and assistant B's answer. Your job is to evaluate which assistant's answer is better.\nBegin your evaluation by generating your own answer to the prompt. You must provide your answers before judging any answers.\nWhen evaluating the assistants' answers, compare both assistants' answers with your answer. You must identify and correct any mistakes or inaccurate information. Then consider if the assistant's answers are helpful, relevant, and concise. Helpful means the answer correctly responds to the prompt or follows the instructions. Note when user prompt has any ambiguity or more than one interpretation, it is more helpful and appropriate to ask for clarifications or more information from the user than providing an answer based on assumptions. Relevant means all parts of the response closely connect or are appropriate to what is being asked. Concise means the response is clear and not verbose or excessive.\nThen consider the creativity and novelty of the assistant's answers when needed. Finally, identify any missing important information in the assistants' answers that would be beneficial to include when responding to the user prompt.\nAfter providing your explanation, you must output only one of the following choices as your final verdict with a label:\n1. Assistant A is significantly better: [[A> > B]]\n2. Assistant A is slightly better: [[A> B]]\n3. Tie, relatively the same: [[A=B]]\n4. Assistant B is slightly better: [[B> A]]\n5. Assistant B is significantly better: [[B> > A]]\nExample output: My final verdict is tie: [[A=B]].\n[User Prompt]\nCan you come up with a 12 bar chord progression in C that works in the lydian mode?\n[The Start of Assistant A's Answer]\nSure! In the Lydian mode, the chords are built on the scale degrees of the Lydian scale. Here's a 12bar chord progression in C Lydian:| Cmaj7 | Dmaj7 | Em7 | F#m7b5 || Cmaj7 | Dmaj7 | Em7 | F#m7b5 | | Cmaj7 | Dmaj7 | Em7 | Cmaj7 | This progression uses the I, II, iii, and iv chords of the C Lydian mode.\n[The End of Assistant A's Answer]\n[The Start of Assistant B's Answer]\nCertainly! The Lydian mode is the fourth mode of the major scale and is known for its #4 interval, which gives it a distinctive, bright quality. To create a 12bar chord progression in C Lydian, we'll use chords that are diatonic to the C Lydian scale (C D E F# G A B). Here's a simple 12-bar chord progression in C Lydian, focusing on the characteristic #4 (F#) to give it that Lydian flavor: | Cmaj7 | Cmaj7 | Cmaj7 | Cmaj7 || || F#m7 | F#m7 | G7 | G7 | G7 || Cmaj7 | | Am7 | D7 | G7 || Cmaj7 | | F#m7 | G7 | G7 | Explanation:\nBars 1 to 4: Start with four bars of Cmaj7, establishing the tonic.\nBars 5 to 6: Use F#m7, which is a minor chord built on the #4 (F#) of the C Lydian scale. This is the key chord that gives the progression its Lydian character.\nBars 7 to 8: Transition with G7, which is the dominant chord in C Lydian. It sets up a nice tension before resolving back to the tonic.\nBars 9 to 10: Return to Cmaj7, then use Am7, which is a ii chord in C Lydian, and D7, which is a V7 chord. This ii-V movement creates a bit of tension leading back to the tonic.\nBars 11 to 12: Use F#m7 and G7 again to reinforce the Lydian mode and create a nice tension-resolution dynamic before ending on G7, which can lead back to Cmaj7 if you want to loop the progression. This progression highlights the unique quality of the Lydian mode while maintaining a coherent harmonic flow. Feel free to experiment with different voicings and extensions to further explore the Lydian sound!\n[The End of Assistant B's Answer]"}, {"title": "Judge Responses of Different Models:", "content": "GPT-40 Judge Response:\nTo create a 12-bar chord progression in the Lydian mode in C, we need to understand the characteristics of the Lydian mode. The Lydian mode is the fourth mode of the major scale and is characterized by a raised fourth degree. In the key of C Lydian, the notes are C, D, E, F#, G, A, and B. The chords built on these scale degrees would be as follows:\n1. Cmaj7 (I)\n2. Dmaj7 (II)\n3. Em7 (iii)\n4. F#dim7 (iv)\n5. G7 (V)\n6. Am7 (vi)\n7. Bm7 (vii)\nHere's a 12-bar chord progression that emphasizes the Lydian sound by highlighting the raised fourth (F#):\nCmaj7 | Dmaj7 | Cmaj7 | Em7 |\nF#dim7 |G7 | Cmaj7 | Dmaj7 |\n| Cmaj7 | Em7 | F#dim7 | Cmaj7 |\nThis progression uses the I, II, iii, and iv chords, while also incorporating the V chord to add some tension and resolution within the progression. The use of F#dim7 helps to highlight the Lydian characteristic of the raised fourth degree.\n**Evaluation of the Responses:**\n**Assistant A:**\n1. **Accuracy**: Assistant A initially explains the F Lydian mode instead of the C Lydian mode, which is a misunderstanding of the prompt. The progression it provides is more related to C major with emphasis on F major, rather than a true exploration of C Lydian, which should focus on the raised fourth (F#).\n2. **Helpfulness and Relevance**: While Assistant A provides a progression, it doesn't accurately reflect the Lydian characteristics in C as requested. The explanation of the Lydian mode is slightly off because it references the F Lydian mode.\n3. **Clarity and Conciseness**: The response is detailed but includes some irrelevant information about the F Lydian mode, which could confuse the user.\n**Assistant B:**\n1. **Accuracy**: Assistant B correctly uses the chords from the C Lydian scale, which includes the raised fourth (F#), though it incorrectly labels F#m7b5 as iv instead of iv7.\n2. **Helpfulness and Relevance**: The response is more relevant to the prompt since it focuses on C Lydian. It provides a progression that more closely reflects the Lydian mode, though it could use more explanation on why these chords are chosen.\n3. **Clarity and Conciseness**: The response is concise and directly addresses the prompt with a correct Lydian mode progression.\n**Final Verdict**: Assistant B's response is more accurate and relevant to the prompt, although both assistants have areas for improvement. Assistant B correctly maintains the C Lydian mode focus, despite a minor error in chord labeling. Thus, my final verdict is: [[B>A]]\nSkywork-Critic-Llama-3.1-8B Judge Response:\n[[A]]"}, {"title": "Data Recipes", "content": "Based on our findings, we establish the optimal training data ratio as:\n$\\left\\{ critique data : reward data : general SFT data = 1 :3:1 \\right\\}$\nOur final training dataset comprises approximately 900k entries. We select the recently released open-source Qwen2.5 series (Yang et al., 2024) as the foundation models for our judge SFT training. We detail the related results in the following sections."}]}