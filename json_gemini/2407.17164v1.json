{"title": "Robust Deep Hawkes Process under Label Noise of Both Event and Occurrence", "authors": ["Xiaoyu Tan", "Bin Li", "Xihe Qiu", "Jingjing Huang", "Yinghui Xu", "Wei Chu"], "abstract": "Integrating deep neural networks with the Hawkes process has significantly improved predictive capabilities in finance, health informatics, and information technology. Nevertheless, these models often face challenges in real-world settings, particularly due to substantial label noise. This issue is of significant concern in the medical field, where label noise can arise from delayed updates in electronic medical records or misdiagnoses, leading to increased prediction risks. Our research indicates that deep Hawkes process models exhibit reduced robustness when dealing with label noise, particularly when it affects both event types and timing. To address these challenges, we first investigate the influence of label noise in approximated intensity functions and present a novel framework, the Robust Deep Hawkes Process (RDHP), to overcome the impact of label noise on the intensity function of Hawkes models, considering both the events and their occurrences. We tested RDHP using multiple open-source benchmarks with synthetic noise and conducted a case study on obstructive sleep apnea-hypopnea syndrome (OSAHS) in a real-world setting with inherent label noise. The results demonstrate that RDHP can effectively perform classification and regression tasks, even in the presence of noise related to events and their timing. To the best of our knowledge, this is the first study to successfully address both event and time label noise in deep Hawkes process models, offering a promising solution for medical applications, specifically in diagnosing OSAHS.", "sections": [{"title": "Introduction", "content": "Temporal point processes (TPP) are designed to predict the nature and timing of forthcoming events by understanding the patterns of their occurrence [8]. These models are particularly suited for scenarios where events happen asynchronously (at unpredictable intervals), are multimodal (involving diverse event types), and are influenced by past occurrences. As a distinctive TPP approach, the Hawkes process is designed to simulate a self-exciting process over time. Essentially, as one event happens, it increases the likelihood of subsequent events, though this increased intensity fades as time passes. The model's applications span high-frequency trading [2], bioinformatics gene mapping [20], earthquake forecasting [3], traffic pattern analysis [39], and predicting patient disease trajectories and timings [23, 21].\nTo estimate the likelihood of an event's occurrence, the Hawkes process employs an intensity function influenced by past events. Traditional Hawkes models, however, often presume that prior events invariably have a positive impact on upcoming ones. This presumption can limit the model's adaptability, sometimes yielding suboptimal outcomes with limited versatility [37, 40]. To address this, several approaches incorporating deep neural structures have been proposed to enhance the dynamic modeling and demonstrate improved results in various real-world data experiments [37, 40, 14]. Yet, even with the benefits of deep neural networks, these models overlook the significant label noise present in real-world scenarios. In real-world settings, data often suffers from label noise, which can hinder the efficiency and reliability of the learning process. Such noise might arise from human labeling mistakes or intrinsic flaws in electronic logging systems [4, 19, 27]. Especially in the medical domain, label noise is primarily induced by the inherent subjectivity in diagnoses, expert errors in manual labeling, equipment variability, and the complexity of medical conditions [5], which introduce significant risk in medical diagnosis predictions using the Hawkes process.\nTo explore the impact of label noise on both event types and timing, we first conduct an ablation study focusing on the errors in learning intensity functions. We compare datasets with clean, accurate labels against those with noisy labels to understand this influence. Our analysis in Section 2.2 reveals that label noise, whether in events or timing, significantly impairs the learning of intensity functions, leading to errors in model predictions. Furthermore, when both types of label noise are present, the resultant error in learning intensity functions surpasses the sum of errors caused by each type of noise independently. This indicates a compounded effect, where the interaction between event and time noise amplifies the learning challenges more than if each noise type acted alone. The compounded impact of event and time noise not only challenges conventional label noise mitigation strategies but also emphasizes the importance of developing a more sophisticated approach to improve robustness under both label noise and influence.\nTo separately mitigate the influence of label noise from events and timing, reduce the compounded effects, and improve the robustness of the Hawkes process under the deep learning framework, we therefore propose a novel framework called Robust Deep Hawkes Process (RDHP) to improve the performance under label noise, especially in real-world medical scenarios. We conduct experiments on synthetic open-source datasets and a real medical dataset on obstructive sleep"}, {"title": "Methods", "content": "2.1 Preliminary\nWe consider a TPP sequence X = {(t1, 1), (t2, U2), ..., (ti, Vi)}, where each (ti, vi) denotes the i-th event of a time series, with vi representing the event type and ti the arrival time. The goal is to predict the type and occurrence time of the next event based on X. In such processes, past events influence future ones, and the occurrence of events is modeled using a conditional intensity function \u03bb*(t), which depends on historical events Ht. This function is defined by the conditional density f(t | Ht) and the cumulative distribution function F(t | Ht) is as follows:\n\u03bb*(t) = \\frac{f(t | H_t)}{1- F(t | H_t)} \\tag{1}\nConsidering a small time interval [t, t + dt], the event probability in this interval is given by:\n\u03bb*(t)dt = E [N([t, t + dt]) | Ht], \\tag{2}\nwhere N() is the counting function for events in the interval. The conditional intensity function can be simulated through the Hawkes process:\n\u03bb*(t) = \u03bc + \u03b1\\sum_{t_{i-1}<t}e^{-(t-t_{i-1})}, \\tag{3}\nwhere \u03bc is the base intensity, \u03b1 is the incentive weight, and each new event increases the conditional intensity, which decays exponentially back to \u03bc over time. This framework allows for the modeling of event sequences, considering both self-excitement and mutual excitation between different event types.\n\u03bb*(t) = \u03bc(t) + \u03b1 \\sum_{t_{i-1}<t}\u03c6(t \u2212 t_{i\u22121}), \\tag{4}\nwhere \u03c6(t) = e^{-(\u03b3t)} is the density function influenced by historical events, with \u03b3 controlling the decay rate."}, {"title": "The Compounded Effect of Label Noise from Both Events and Occurrence", "content": "In real-world scenarios, however, there is typically some noise in the Hawkes process. For example, for electronic health records (EHRs) [1], there are a number of instances where the wrong disease type or time point is recorded during the manual recording process. As shown in Figure 1, such instances can have a significant impact on the Hawkes process prediction. In this study, we conducted an experiment to examine the impact of label noise on the MIMIC dataset using the SAHP algorithm [37]. Initially, we trained a model on a clean dataset, using the values of the intensity layers as a benchmark. Subsequently, we introduced two types of noise: Gaussian noise (with a mean of 0 and a standard deviation of 0.8) to corrupt the time labels, and uniform noise for the event labels. These were used to train two separate models to assess how each type of noise affected the learning of the intensity function.\nThe final step involved combining both types of label noise to train another model. This helped us understand the combined impact of event and time label noise on the intensity layers. We compared the performance of each model by measuring the deviation of their learned intensity layers from those of the clean dataset, as illustrated in Figure 3.\nOur findings reveal that both event and time label noise independently affect the learning of intensity layers, leading to errors. More notably, the concurrent presence of both noise types leads to a compounded effect, resulting in errors that exceed the sum of their individual impacts. This complex interaction between event and time label noise hasn't been extensively explored in prior studies. Therefore, developing a robust learning framework that effectively counters the detrimental effects of both types of label noise is crucial."}, {"title": "Deep Hawkes process", "content": "We use a linear embedding layer to encode event types of a time series, transforming one-hot embeddings into a vector Sj using a matrix Ws:\nS_j = I_jW_s, \\tag{5}\nwhere Ij is the one-hot embedding of event type j. Event occurrence times are encoded using a sinusoidal function, following the method by [37]:"}, {"title": "Robust Deep Hawkes Process", "content": "The historical event's influence on subsequent events can be analyzed using the deep Hawkes process. This analysis uncovers hidden details that can be instrumental in predicting upcoming events. However, as introduced in Section 2.2, real-world point process data often comes with inaccuracies regarding the event type and timing, which induces a significant compounded error effect during hidden intensity layer learning. Hence, in this section, we begin by addressing the label noise issue, both in event classification and time regression. We approach this through targeted adjustments in loss functions and network design. Subsequently, we develop a loss balance network, similar to the model discussed in [26], which is designed to effectively reduce compounded errors by ensuring balanced updates."}, {"title": "Mitigate Label Noise Effects in Events Learning", "content": "In standard deep Hawkes processes, such as SAHP or THP, the frequently employed cross-entropy loss function is not particularly resistant to label noise [38]. For handling label noise in classification,"}, {"title": "Mitigate Label Noise Effects in Occurrence Time Learning", "content": "For event occurrence time prediction, the noise is imposed on the occurrence time point. We design the timing label noise mitigation part by extending the method proposed by [18] in RDHP. The basic idea is to introduce a hyperparameter on each data point to overparameterize the model. Then, during the model learning process, the bias induced by the label noise can be mainly learned by these hyperparameters, and the main model can remain highly generalized. This learning process can significantly reduce the impact of noisy data [18].\nHence, in the timing prediction part of RDHP, our goal is to minimize the difference between Mt(X) + pi and ti, where Mt (X) is the output of the time prediction layer. Based on the sparse overparameterization trick[18], we can formulate the parameter pi by:\np_i = m_i \u2299 \\frac{m_i}{t_i - n_i} \u2299 \\frac{n_i}{(1 \u2013 t_i)}, \\tag{13}\nwhere mi \u2208 [-1,1] and ni \u2208 [-1,1] denote two data-specific parameters, and is element-wise production. Please note that since the purpose of the hyperparameter term pi is to learn the noise information during the training process, pi is only optimized in the training process and dropped in inference. To further improve the robustness of Mt in handling large outliers, which might be the data points with large label noise, we implement MAE loss as a time-prediction loss function to optimize this model:\nL_t = |M_t(X) + p_i - t_i|. \\tag{14}"}, {"title": "Mitigate Compounded Label Noise Effect From Both Events and Occurrence", "content": "As discussed in Section 2.2, the impact of label noise can be compounded from both events and occurrences, substantially affecting the learning of intensity layers. This, in turn, can lead to significant errors in prediction. Besides, when the dataset is subjected to label noise originating from both events and occurrences, the impact of this noise is also heterogeneous across individual data points. Moreover,"}, {"title": "Experimental results", "content": "In this section, the noise resistance of our proposed framework is evaluated using both synthetic and real-world clinical OSAHS datasets provided by the collaborative university hospital."}, {"title": "Dataset", "content": "The datasets: we use two public medical benchmarks to analyze the robustness of RDHP. The datasets we selected are the following, and the details are shown in Table 1. The MIMIC-II and MIMIC-III are two widely recognized public datasets that contain extensive records of hospital visits over an extended period. These datasets comprehensively document each visit, capturing key details such as the purpose of the visit and the specific time it occurred. The detail of these datasets is as follows:\n\u2022 MIMIC-II: This dataset is an electronic medical record containing hospitalizations of patients over a seven-year period. Each patient's visit history is considered as a time series, with the events comprising occurrence time points and diagnoses [24].\n\u2022 MIMIC-III: This dataset encompasses information relevant to patients in the intensive care unit (ICU) of a large-scale tertiary hospital [15]. We have selected a dataset from [23] that captures the medical service types received by patients. By combining the patient-received medical service types with their corresponding timestamps, a temporal point process is formulated. This process is employed to predict the subsequent patient visit time and the corresponding medical service type to be administered.\n\u2022 OSAHS dataset: The OSAHS dataset is comprised of OSAHS patients, who sleep at night with different snoring types and may have different noise disturbances in the real environment. We extract the corresponding snoring events and observe a sparse point distribution of snoring events throughout a single night of patient sleep, which is considered as a TPP. Each patient's snoring is recorded for a complete night of 8 hours, and the entire night of sleep is divided into four parts, with each sequence length of 2 hours. Snoring event types are classified as central sleep apnea (CSA), mixed sleep apnea syndrome (MSAS), obstructive apnea (OA), and hypopnea. The last event point in the sequence is used as the target to predict the event type and event occurrence time of the next snoring event. The discrimination of snoring events relies on the subjective judgment and analysis of medical professionals and is influenced by the performance of PSG instruments and environmental factors during data collection. In the OSAHS dataset presented in Table 1, the longest sequence spans two hours and encompasses a total of 207 recorded events. Extrapolating to a full 8-hour night of sleep results in a total of 1656 events. If the misclassification rate of snoring events exceeds 3%, it can potentially lead to the erroneous diagnosis of mild OSAHS in patients."}, {"title": "Noise setup", "content": "To assess the model's robustness to label noise, we introduce noise into the training set, while maintaining the integrity of the validation and test sets. The dataset is modified with synthetic noise using a noise probability p. Consider a time series X = {(t1, 1), (t2, U2), ..., (ti, vi)}, where vi represents the event type of the i-th event, taking values from 1 to K, and ti signifies the event occurrence time within the range [0, tmax]. Label noise means that for each event (ti, vi), there is a probability p that either ti or vi, or both, may be incorrect. With a noise probability of p, the timestamp ti could be modified to ti + tnoise, where tnoise is a Gaussian-distributed variable with a mean of 0 and a standard deviation of 0.8. This alteration can lead to changes in the chronological order of events, as illustrated in Figure. 1.\nWe incorporate three distinct forms of label noise into our analysis: Uniform noise, Flip noise, and Flip2 noise. The matrices illustrating how each type of noise affects the labels are depicted in Figure 4. For a more comprehensive explanation of these noise types, including their specific characteristics and how they are applied to the data, please refer to Appendix B."}, {"title": "Experiment Setup and Implementation", "content": "Before introducing noise, in a synthetic experiment, we randomly extract 5% clean datasets from the training set, which are used for optimizing the re-weighting net. For different noise dataset scenarios, we simulated synthetic datasets with a noise rate of p, and the noise was added by introducing Gaussian distribution on timestamps and the three aforementioned types of label noise. This corresponds to the situation where experts incorrectly record time points and event types during data labeling.\nWe utilized a dataset of OSAHS patients obtained in a noisy environment and manually labeled by human experts. This dataset is used to test the robustness of RDHP under TPP with real human label noise. The dataset is divided into 60%, 20%, 10%, and 10% for train, validation, test, and clean, respectively. Here, we invite the human expert to relabel the validation and test sets carefully, which are considered clean sets. For synthetic noise datasets, we divide the dataset into 80% for training, 10% for validation, and 10% for testing.\nSince the event type is a multi-classification problem, as shown in Table 1, we choose Macro F\u2081 score to evaluate the noise robustness of RDHP. For the event occurrence time prediction, we choose"}, {"title": "Main Results", "content": "These experiments involved the application of three different noise types and were performed in five separate trials. The average number of iterations for the experiments was 200, with an average runtime of approximately 3000 seconds. The experimental results represent the mean values obtained from five independent trials with random seeds, which have been summarized and listed in Table 2. The hyperparameter settings for the RDHP model experiment can be found in Appendix C.\nRDHP achieves the most stable and best classification F\u2081 on MIMIC-II in all noise cases. It converges to the best RMSE for all the event occurrence time prediction. It also stabilizes the training on MIMIC-III and achieves superior noise robustness compared to baseline methods. In the uniform noise case, the RDHP model generally exhibits greater robustness in classification compared to THP and SAHP, with a significantly lower RMSE. In summary, the classification performance of SAHP and THP deteriorates under moderate to high noise conditions, resulting in a rapid increase in RMSE. Furthermore, noise has a catastrophic impact on SAHP and THP, particularly in high-noise scenarios, where their classification performance notably declines. This supports our previous assertion that existing deep Hawkes processes may lack robustness in the presence of label noise. However, through robust structural modifications and overparameterization regression, RDHP maintains high accuracy and low RMSE even in high-noise environments."}, {"content": "Table 3 shows the performance of RDHP with baseline methods on the OSAHS dataset. This dataset is a time series of snoring events taken from OSAHS patients throughout the night, and the presence of marker noise is assumed. After the experiments, we can observe that RDHP achieves superior performance in the real case, on both classification and regression. THP overfits the noise data and demonstrates its unstable performance in the real circumstances under label noise. The classification effect of SAHP in the real environment is also greatly reduced, and the prediction error for the time of event occurrence is large. The results indicate that, so far, deep Hawkes models remain unstable under real-world label noise."}, {"title": "Ablation Study", "content": "In this section, we conduct an ablation study to evaluate the contributions of each component in our RDHP model, particularly focusing on its label noise resilience capability. For event type prediction, the model employs a training loss based on the Generalized Cross Entropy (GCE) and utilizes a dedicated event prediction subnet, denoted as Me. In contrast, time regression is handled by a separate subnet, Mt, which applies sparse overparameterization regression. Additionally, we incorporate a re-weighting network, r, to ensure balanced training between classification and regression tasks.\nThe ablation study is structured to assess the individual impact of these components. Initially, we explored the model's performance under various noise conditions, including scenarios with only temporal noise, only label noise, and situations where both occur simultaneously. This investigation aimed to understand how different types of noise affect the model's robustness. We also examined the convergence patterns of the weights for classification and regression losses during balanced training. This was crucial to ensuring that both aspects of the model were trained effectively. Lastly, we analyzed the"}, {"title": "Impact of Noise", "content": "The experimental results provide insights into the impact of noise on the SAHP and RDHP using different noise scenarios applied to the MIMIC-II dataset. The time point noise is configured following Section 3.2, with a Gaussian distribution for time point noise and uniform noise for label noise. The results are presented in Table 4. In the study, the introduction of only occurrence noise significantly affected the RMSE, with a minor reduction in the F\u2081 score. Conversely, when only label noise was introduced, it notably impacted the accuracy of event type prediction, leading to a substantial increase in RMSE as the noise level increased. Moreover, when both time and label noise were present, the performance in terms of F1 score for classification and the RMSE for regression was the poorest. This decline in performance is attributed to the nature of the Hawkes process, a self-exciting model, where noise considerably affects the computation of intensity, as evident in Figure 3. This, in turn, adversely affects the final model's performance in both classification and regression tasks. However, our proposed RDHP demonstrates superior classification performance and maintains a low RMSE under these conditions, indicating its resilience to noise. These findings highlight the Deep Hawkes Process's vulnerability to noise and underline the necessity of implementing noise reduction strategies or robust training methods to ensure dependable performance in practical applications."}, {"title": "The Learning of loss weight \u03c3", "content": "In Figure 5, we present the dynamic adjustments of the loss weight \u03c3 in the RDHP model, considering both time loss and event loss, with 30% uniform noise. The X-axis represents the progression of training epochs, while the Y-axis tracks the difference Ao in weight values between epochs. This visualization reveals a notable pattern in the training phase: the weights assigned to each loss function adaptively adjust over time, eventually reaching a state of equilibrium. The re-weighting network plays a crucial role in this process, effectively managing the balance and mitigating the compounded bias from the learning loss for events and timing. This equilibrium not only enhances the model's performance but also incorporates a systematic approach to mitigating label noise in both the event and timing aspects of the training data. In Table 5, the RDHP's performance in both classification and regression tasks decreases without the re-weighting net."}, {"title": "Regularization", "content": "As introduced above, robust regularization utilizes new hyperparameters on each data to separate the noisy information. The results demonstrate that the robustness of the classification task has been slightly enhanced, whereas the regression task has been significantly enhanced. In Table 5, we present a comparative analysis of various models. It is evident that the incorporation of robust regularization leads to a notable reduction in the RMSE. Given that each sequence in the MIMIC-II dataset represents a patient's medical history over a period of 7 years, normalized within the range [0, 5.3846], even minor inaccuracies in time prediction can result in significant deviations. This context underscores the exceptional performance of the RDHP model, which maintains an RMSE of approximately 1.0, indicating its high accuracy. Furthermore, the model demonstrates resilience against label noise, showing minimal impact on its regression capability."}, {"title": "Conclusion", "content": "In this study, we introduce a novel robust deep Hawkes process (RDHP) model specifically designed to handle corrupted training labels. Prior research has largely overlooked the compounded bias arising from label noise in both event types and their occurrence times. To the best of our knowledge, our RDHP model is the first to tackle this challenge. It improves the resilience of event classification and time prediction in noisy environments through a robust loss function, sparse overparameterization regularization, and loss re-weighting strategies. We demonstrate the limitations of existing Hawkes process models for handling label noise through experiments on synthetic and real OSAHS datasets. Our RDHP model consistently outperforms traditional models, achieving higher classification accuracy and a lower RMSE. Further, we conduct ablation studies to assess the impact of each component of our model. This research paves the way for enhancing robustness in broader point process models, offering a significant advancement in handling real-world data with inherent label noise. Future works will focus on further refining accuracy in noisy conditions and expanding applicability to a wider range of practical noisy scenarios."}, {"title": "Related Work", "content": "A.1 Temporal Point Process\nThe temporal point process (TPP)[8, 6, 10] is an approach of modeling time series that employs point patterns, which can be employed to model historical events. It is assumed that events in the point model occur instantaneously and can be represented as points on a time series. These points are interconnected, and what occurred in the past may have an impact on what will occur in the future [22].\nThe conventional point process methods include the poisson process [17], the self-correcting process [13], and the Hawkes process [10]. The Hawkes process is a self-exciting model, which means that previous events have an incentive effect on future events, and this incentive effect decays with time, in accordance with the real world. Hawkes process is commonly used in earthquake prediction, finance, and disease prediction [11].\nWith the recent advances in deep learning technology in recent years, the deep Hawkes process technique has attracted considerable interest. [37] proposes the SAHP model by combining the Hawkes process with the attention mechanism to increase the connection between event points in a sequence. [40] combines the Hawkes process with Transformer to capture long-term dependencies using a self-attention mechanism while increasing computational efficiency. However, the aforementioned models' assumptions are modeled in a noise-free situation, i.e., the event type and event time point are precisely recorded. Real-world data is always contaminated with various types of labeling noise, such as human labeling errors and technical recording techniques, significantly affecting the learning process's efficiency and stability. [4, 19, 27]."}, {"title": "Label Noise", "content": "When considering a time series with noise, there are typically two types of noise: one is caused by the time shift of the event time point (i.e., label noise of occurrence time), and the other is the event type being recorded incorrectly (i.e., label noise of the event). Both of these types of noise are prevalent in real-world scenarios and might have an impact on the robustness of the model.\nA number of methods have been developed to improve the model's robustness under label noise of event. Modifying the model's structure, adjusting sample selection, incorporating regularization, and designing loss functions are typical techniques for improving model robustness [27].\nNoise modeling [35] proposes a method employing two independent networks, both for predicting noise type and label shift probability, respectively. After the training step, both networks are trained with numerous noisy labels and a small amount of clean data, making their adaptability to more general architectures challenging.\nSample selection the Co-teaching family [9] [36] [34] as the main representative, multiple network structures are used to filter each other, and there is the problem of increasing the training parameters and decreasing the training efficiency.\nRobust regularization data augmentation [25], weight decay [16], dropout [28], and batch normalization [12], can improve model robustness, but these methods suffer from reduced generalization when the noise is high [29]. The loss function is reviewed in the following. Moreover, the mentioned methods also have a uniform problem, that is, only the label noise of the event is considered without considering the label noise of the occurrence time point.\nRecent studies have also extensively focused on the label noise of the event's occurrence time point. [30] claim that each sequence has a fixed time shift (synchronous noise), and the causal relationship between the sequences can be reconstructed by modifying the excitation kernel function and estimation method. Subsequently, [31] amend the noise assumption and correct the sequence causality by the cumulative amount of MHP, with each sequence noise belonging to different independent distributions. However, the above-mentioned models have certain limitations: they concentrate solely on the relationship between time sequences without accounting for the simultaneous presence of label noise existing in both event types and occurrence time points, which will have a negative effect on the classification accuracy of the following event.\nCategorical cross-entropy loss (CCE) is the most commonly used loss function in event classification tasks due to its fast convergence and high generalization capabilities. [7] demonstrate that in noisy tasks, MAE has superior generalization performance than CCE and is more robust under noise by creating a risk minimization model. However, if the data is more complicated and the event types are more diverse, MAE is difficult to converge, and its generalization performance will subsequently decrease. Subsequently, [38] derives the GCE loss function from the gradient properties of MAE and CCE. It has both the fast convergence property of CCE and the robustness of MAE in terms of noise performance.\nWe present a deep robust Hawkes process (RDHP) that combines a robust loss function and robust regularization in order to deal with the simultaneous presence of label noise in both event types and occurrence time points. The model encodes a sequence of event points with event type and occurrence time noise, produces point-to-point attention scores, and then models them using the Hawkes process in order to obtain historical event hidden information. GCE is presented as a loss function for event prediction. The prediction of occurrence time points, which is more probable to be a regression problem, employs the more robust MAE loss function. Meanwhile, robust regularization is implemented, and new hyperparameters are added to create a data model for learning clean data by separating noise information. The overview of our proposed model is shown in Figure 2."}, {"title": "Noise Setting", "content": "Here we describe three types of label noise. Consider a time series X = (t1, 1), (t2, U2), ..., (ti, vi), where vi \u2208 1,2,3, ..., K denotes the event type of the i-th event, and ti \u2208 [0, tmax] represents the event occurrence time.\n\u2022 Uniform Noise: Uniform noise refers to each event type having an equal probability of p to transform into any of the other K \u2013 1 classes. Specifically, the dataset with a corruption probability of p means that vi has probability p being corrupted into other K - 1 labels in event point (ti, vi), which is a one-to-K 1 type of label noise."}]}