{"title": "3DFacePolicy: Speech-Driven 3D Facial Animation with Diffusion Policy", "authors": ["Xuanmeng Sha", "Liyun Zhang", "Tomohiro Mashita", "Yuki Uranishi"], "abstract": "Audio-driven 3D facial animation has made immersive progress both in research and application developments. The newest approaches focus on Transformer-based methods and diffusion-based methods, however, there is still gap in the vividness and emotional expression between the generated animation and real human face. To tackle this limitation, we propose 3DFacePolicy, a diffusion policy model for 3D facial animation prediction. This method generates variable and realistic human facial movements by predicting the 3D vertex trajectory on the 3D facial template with diffusion policy instead of facial generation for every frame. It takes audio and vertex states as observations to predict the vertex trajectory and imitate real human facial expressions, which keeps the continuous and natural flow of human emotions. The experiments show that our approach is effective in variable and dynamic facial motion synthesizing.", "sections": [{"title": "1 Introduction", "content": "Speech-driven 3D facial animation is crucial topic both in research and application fields. It focus on creating realistic and precise 3D facial movements on 3D vertex or blendshape templates with speech input, which is widely deployed in live streaming, film production and video games. Its goal is to create vivid and natural facial expressions that are similar to real human facial movements [26].\nRecently, traditional generative methods can produce promising facial animations and keep lip-sync accuracy with audio guidance. Karras et al. [12] proposes an end-to-end CNN based method mapping input waveforms to the 3D vertex coordinates. VOCA [5] proposed a CNN-based method with pretrained DeepSpeech [8] model including identity control. Recent progress has grown with the Transformer-based autoregression approach. The FaceFormer [7] applies the extensive contextual capabilities of Transformer for auto-regressive facial motion generation. CodeTalker [25] incorporates self-supervised Wav2Vec 2.0 together with the idea of having a latent codebook using VQ-VAE inspired by [16]. Though these deterministic regression methods achieve relatively promising results, the dynamic and variable human expressions are overlooked.\nIn contrast to deterministic methods, diffusion models [10] can gradually remove noise from a signal rather than learning a mapping from a noise distribution to the data distribution, which supports various forms of data, especially high-dimensional generations [27]. It can also support conditional input to guide the denoising process to meet the conditions. To this end, the diffusion model suits the 3D facial animation generation in a better modality. However, few work leverages the diffusion model in their methods. [19] is the first work that integrates the diffusion model into 3D facial animation, which proves that diffusion mechanism is effective for handling diverse facial animation sequences. DiffSpeaker [15] combines Transformer architecture with diffusion-based animation sequence generation and proposes unique biased conditional self/cross-attention mechanism. However, these methods lack the ability of processing intensive contexts.\nTo address the limitations and disadvantages of the aforementioned methods, our approach aims to generate flexible, variable, and dynamic facial motions from audio input while maintaining accurate facial expressions. To achieve this, we propose 3DFacePolicy, a facial motion prediction model based on a diffusion policy framework. A conceptual comparison of our method with two other mainstream approaches is shown in Figure 1. The diffusion policy was first introduced by [3], which is a visuomotor imitation learning algorithm designed to teach robots a broad range of motor skills. This method conditions on high-dimensional visual inputs, such as images or depth maps, and generates denoised robot actions for visuomotor tasks using Denoising Diffusion Probabilistic Models (DDPMs) [10].\nLeveraging this approach, we decompose 3D facial animation into sequences of vertices and audio and actions, where the action sequence represents frame-by-frame deviations in the vertices. By sampling a noisy action sequence from Gaussian noise, and conditioning it on the audio sequence and facial vertices sequence, the diffusion policy model outputs a denoised action sequence. For handling complex, high-dimensional data distributions such as actions, we employ a sequence sampler following [28], which segments the long-term data sequence into smaller fractions. This technique generates denoised actions over a limited horizon, effectively reducing vibrational artifacts. Consequently, our architecture ensures not only smooth 3D facial animations but also more flexible and variable facial motions.\nWe evaluate our proposed method quantitatively. With VOCASET as benchmarks, 3DFacePolicy holds notable advantages over other state-of-the-art methods. The main contributions of our work are as follows:\n\u2022 3DFacePolicy is the pioneering work to innovatively integrate the robot imitation learning diffusion policy with 3D facial animation synthesis task.\n\u2022 Our model introduces a new approach for the disentanglement of the animation sequence, which refines vertices motion sequences from single vertices sequence in a whole animation frame.\n\u2022 Leveraging sequence sampler for smooth action generation in a limited local space during policy training."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Speech-Driven 3D Facial Animation", "content": "Speech-driven 3D facial animation is to create realistic and natural facial movements with the driving speech. The most crucial component of this problem is the synchronization of the tone, rhythm and dynamics that mirrors the real-human expressions with the speech. Over the years, researches mainly focus on traditional generative methods and diffusion-based methods.\nTraditional generative methods: These kind of methods design the deterministic mappings for audios and facial motions with deep neural networks. Taylor et al. [21] proposes a sliding window approach to transform audio information into animation parameters with deep neural network. Karras et al. [12] used an end-to-end method to animate face directly from audio with emotion component based on CNNs. Edwards et al. [6] proposed two anatomical actions to synchronize the lips and jaws with speech signal. Zhou et al. [29] focus on the phonemes processing with the combination model of JALI and LSTM-based neural network. However,"}, {"title": "2.2 Diffusion Policy Models", "content": "The visuomotor policies are presented in the Robotics, which guides the robot to perform tasks with visual observations like images or depth information etc.. The methods can be classified as reinforcement learning, reward learning, motion planning and imitation learning. Diffusion Policy [3] is the simple but powerful work that leverages a conditional denoising diffusion model in a visual imitation learning method for robot visuomotor policy generation. It takes 2D images sequence as conditions and generate robot actions from a randomly sampled noise sequence with diffusion model inside a small closed-loop action space with horizon control, which achieves notable results in multiple robot control tasks. From this work, 3D Diffuser Actor [13] lifts the 2D single-view visual condition into multi-view observations and disentangle the robot actions into position, rotation and end-effector pose. 3D Diffusion Policy [28] is the closest to our work. It consists of two critical parts, the perception and decision. In the perception, it directly takes 3D point cloud and robot state as conditions with highly-effective 3D encoder. In the decision part, the conditional denoising diffusion model generates the robot actions systematically from a random gaussian noise in the denoising process. In our work, we consider the facial motions as the actions. With facial mesh vertices sequence and audios sequence as conditions, we can gradually generate the denoised motion data distribution from a gaussian noise motion sequence with conditional denoising diffusion model. To this end, it can prevent from vague facial animations and present dynamic and variable facial movements."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Problem Formulation", "content": "The traditional speech-driven 3D facial animation approach with diffusion model aim to generate denoised vertices $x_{1:N} = (x_1, x_2, ..., x_N) \\in R^{N \\times V \\times 3}$ conditioned on speech audio $s_{1:N} = (s_1,s_2, ..., s_N) \\in R^{N \\times D}$, where N is the number of visual frames that sampled from a whole speech sequence, each frame $n \\in \\{1: N\\}$ contains the vertex information $x_n$ for the speech dataset, which is represented as $V \\times 3$, where V is the number of mesh vertices for the template face mesh, 3 is the coordinates of the vertices. However, these approach are not relatively bound by the facial movement in animation sequence. In our method, we design facial movements diffusion policy model (3DFacePolicy) following [28] to predict the trajectory of vertices movements for every frame, represented as the action $a_{1:N} = (a_1, a_2, ..., a_N) \\in R^{N \\times V \\times 3}$, from noised action $a_t$ with the condition of audio and vertices states, where $t \\in \\{1, ..., T\\}$ is the diffusion step. The vertices $x_{1:N}$ are disentangled into actions $a_{1:N}$ with the same shape $N \\times V \\times 3$ and are considered as one of the conditions. Therefore, our goal of the proposed architecture 3DFacePolicy is to predict the almost identical movements for vertices throughout given animation sequence from noised actions that sampled from Gaussian noise with the conditional input of audio and vertices state. The problem could be formulated as:\n$a_0 = 3DFacePolicy(a_t, s, x, t)$                                                   (1)\nwhere $a_0$ is the predicted action of each vertex in the topology in animation sequence, $a_t$ is action after t diffusion steps, x and s are the vertex and audio sequence respectively. With the predicted action sequence $a_0$ and the vertices in first frame $x_1$, the vertices in n frame is presented as:\n$x_n = x_1 + \\sum_{n=1}^{N} a_n$                                                   (2)\nwhere $x_n, n \\in \\{1 : N\\}$ is the output animation sequence with audio input following frame-by-frame predicted actions."}, {"title": "3.2 Diffusion Policy based Action Prediction Model", "content": "We need to learn a action predicting policy with the perception of vision and audio via diffusion model. To this end, we introduce 3DFacePolicy, which contains three critical parts: (a) Preprocessing. We disentangle the 3D animation sequence into vertices, audio and template, the sequence sampler will resample the long-term vertices and audio data into several limited sequences preserved in a buffer. (b) Perception. The vertices sequence x and audio sequence s are represented into features with pretrained encoders. The visual feature $o_x$ and audio feature $o_s$ are considered as observation features $o = \\{o_x, o_s\\}$. (c) Desicion. It refers the 3D Diffusion Policy [28] as backbone, which predicts the action sequence conditioning on the observation features. The overview of proposed model is illustrated in Figure 2.\n3.2.1 Preprocessing. It has been proved that the 3D facial animation synthesis task is limited due to the highly intensive context in the data distributions [19]. To address this problem, we separate the long-term sequence into a series of fixed duration in a given length, then train the motion predicting policy in a relatively local context, which provides a specific context for motion prediction. This process is achieved by sequence sampler following [3]. In data preprocessing module, we resample the long sequence of vertices and audio into short-term sequences list with sequence sampler and saved in a buffer, then we sample the small vertices sequence and audio sequence to encode into observations for policy training in Perception module.\nSequence Sampler: To sample a long sequence into several fixed duration, we define Horizen H as the time length of a sampled fraction for action prediction, then $N_{obs}$ as time length for observation conditions, $N_{act}$ as time length for action making steps. With these Preliminaries, for a given time step t, the policy takes $\\{O_{t-N_{obs}}: O_t\\}$ steps as observation conditions to predict $\\{a_t: a_{t+H}\\}$ action steps. However, We choose shorter time length $\\{a_t: a_{t+N_{act}}\\}$ as action making steps. This keeps smoothness and consistency for action prediction. During training, we set H = 4, $N_{obs}$ = 2, $N_{act}$ = 2.\n3.2.2 Perception. The fixed durations of vertices and audio are sampled from buffer, then the pretrained visual encoder and audio encoder represent the durations into features respectively. Each duration is considered as temporal observation fraction, which contains the audio feature and visual feature that are considered as condition for action prediction with diffusion policy."}, {"title": "3.2.3 Decision", "content": "The conditional denoising diffusion model [10] is chosen as the backbone of our decision making module following [28]. For K iterations, A noised action sequence $a_k$ is sampled from Gaussian noise, with the condition of visual features x and audio features s, it is gradually denoised into noise-free action sequence $a_0$ with reverse process [10]. The equation is formulated as follows:\n$a_{k-1} = a_k (a_k \u2013 \\gamma_k \\epsilon_\\theta (a_k, k, x, s)) + \\sigma_k N(0, I),$                                              (3)\nwhere $\\epsilon_\\theta$ is the denoising network, $\\alpha_k$, $\\gamma_k$ and $\\sigma_k$ are the functions of k iteration. N(0, I) is Gaussian noise. After K iterations, we can get the actions for current sequence.\nTraining Objective: In the diffusion process [10], the noise $\u20ac_\u03b8$ is added on a randomly sampled action sequence $a_0$ at k iteration to train the denoising network $e_\u03b8$. The objective is to predict the noise added on the sequence:\n$L = MSE(\\epsilon_k, \\epsilon_\\theta (\\alpha_k a_0 + \\beta_k \\epsilon_k, k, x, s))$                                        (4)\nwhere $a_k$ and $\u1e9e_k$ are noise schedules during diffusion steps.\nImplementation details: We use DDIM [18] as denoising scheduler and sample prediction following [28]. For the sequence sampler, we set horizen H = 4, observation condition length $N_{obs}$ = 2 and action making length $N_{act}$ = 2."}, {"title": "4 Experiments", "content": "We trained and tested our model on the representative 3D facial animation datasets, VOCASET [5], then compared our method with other benchmarks on quantitative evaluations, which shows great performance and competitive results through state-of-the-art methods."}, {"title": "4.1 Datasets", "content": "VOCASET dataset: It contains 480 3D facial animation sequences both with facial motions and audios from 12 subjects. Each sequence is recorded at 60 frames per second and about 3 to 4 seconds long. The template mesh for facial motions is a FLAME [14] topology, which consists of 5023 vertices. For fair comparison, we use the same training set (VOCA-Train), validation set (VOCA-Val) and test set (VOCA-Test) as in [25] and [7]."}, {"title": "4.2 Experimental Settings", "content": "Training: We train 50 epochs on 1 batch size. The audio encoder and visual encoder generate 512 dimensions of hidden states each. 1024 concatenated dimensions for observation features. Our model was trained on a single V100 GPU with 32GB RAM, which took 5 hours to complete. We employed the AdamW optimization algorithm, setting the learning rate to 0.0001.\nMetrics: Here we choose facial dynamics deviation (FDD) and Mean Vertex Error (MVE) as our evaluation metric following [25]. FDD calculates the deviation of generated facial movements and groundthuth, which is suitable for evaluating our proposed motion policy generation methods. MVE is for measuring the deviation of all the face vertices of a sequence with respect to the ground truth.\nBaselines: We compare 3DFacePolicy with three state-of-the-art methods, Faceformer [7], CodeTalker [25] and FaceDiffuser [19]. We use the same official implementations of VOCASET dataset as these two methods for a fair comparison."}, {"title": "4.3 Quantitative Evaluation", "content": "Table 1 shows the quantitative results of our method and other baselines. From the evaluation result, our proposed method 3DFacePolicy performs better than other approaches on FDD metric in the order of magnitude, which benefits from our motion policy module. Its direct action prediction makes the generated face motion mirror the realistic facial movement in speeching animation. This result shows that 3DFacePolicy holds the ability for generating highly dynamic facial motions syncing to the ground truth motions. However, the other state-of-the-art methods outperforms our method in MVE metric. It is caused by the motion prediction instead of animation visual frame prediction. Our method tends to generate dynamic facial motions, which may produce exaggerating facial expressions in vertex space."}, {"title": "5 Conclusion", "content": "In this paper, we present 3DFacePolicy, which disentangles the facial motions from 3D facial animation sequence and innovatively leverages diffusion policy to predict the facial motions conditioning on the vertices sequence and audio sequence with the sequence sampler. The core contribution of our method is employing the diffusion policy mechanism from Robotics to 3D facial motion prediction. From the experiment, quantitative results demonstrate that our method outperforms the current baselines in dynamic facial motion synthesis and presents realistic and vivid facial expressions comparing to other methods. We argue that our work provides novel approach of motion prediction for 3D facial animation task that benefits from diffusion policy mechanism.\nLimitation and future work: 3DFacePolicy shows great performance on dynamic facial motion generation. However, it is less consistent when synthesizing whole animation sequence in vertex space. It also lacks more evaluation experiment on other benchmarks. In the future work, we will add a global condition in vertex space to keep the consistent animation generation and add more benchmark to evaluate our method in multiple aspects."}]}