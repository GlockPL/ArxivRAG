{"title": "LLMs can implicitly learn from mistakes in-context", "authors": ["Lisa Alazraki", "Maximilian Mozes", "Jon Ander Campos", "Yi Chern Tan", "Marek Rei", "Max Bartolo"], "abstract": "Learning from mistakes is a fundamental feature of human intelligence. Previous work has shown that Large Language Models (LLMs) can also learn from incorrect answers when provided with a comprehensive rationale detailing why an answer is wrong or how to correct it. In this work, we examine whether LLMs can learn from mistakes in mathematical reasoning tasks when these explanations are not provided. We investigate if LLMs are able to implicitly infer such rationales simply from observing both incorrect and correct answers. Surprisingly, we find that LLMs perform better, on average, when rationales are eliminated from the context and incorrect answers are simply shown alongside correct ones. This approach also substantially outperforms chain-of-thought prompting in our evaluations. We show that these results are consistent across LLMs of different sizes and varying reasoning abilities. Further, we carry out an in-depth analysis, and show that prompting with both wrong and correct answers leads to greater performance and better generalisation than introducing additional, more diverse question-answer pairs into the context. Finally, we show that new rationales generated by models that have only observed incorrect and correct answers are scored equally as highly by humans as those produced with the aid of exemplar rationales. Our results demonstrate that LLMs are indeed capable of in-context implicit learning.", "sections": [{"title": "Introduction", "content": "A crucial aspect of human cognition is the ability to learn from mistakes (Metcalfe, 2017). Analogously, LLMs have been shown to benefit from observing incorrect answers in their context (Madaan et al., 2024; Shinn et al., 2024) and even their training data (An et al., 2023; Paul et al., 2024), provided"}, {"title": "Related Work", "content": "Incorrect outputs have been leveraged in prior work to improve LLM responses for challenging tasks. The existing literature can be largely categorised into three approaches: (i) Self-refinement, where an LLM critiques its own erroneous generations, (ii) External feedback, where the critic is a distinct LLM, and (iii) Multi-agent debate, where two or more models take turns at providing feedback on a previously generated response.\nSelf-refinement. The self-refinement pipeline is well exemplified by Madaan et al. (2024). They devise a framework where an LLM first answers a question, then generates feedback for that answer, and finally outputs a new answer based on the feedback. Note that the model is not fine-tuned and each step is elicited via prompting. The refinement process can be repeated multiple times until a stopping criterion is met, to iteratively improve the final answer. Kim et al. (2024) and Shinn et al. (2024) adopt a similar strategy with agentic LLMs: the model executes a task and, based on the signal received from the environment, outputs a"}, {"title": "Prompt Construction", "content": "Let $E = \\{e_n\\}_{n=1}^N$ be the text sequence resulting from concatenating N in-context examples. In the typical few-shot CoT setting (Wei et al., 2024), an individual example\n$e_n^{CoT} = (q^{(n)}, a^{(n)})$  (1)\nis defined by the question $q^{(n)}$ and the corresponding correct step-by-step answer $a^{(n)}$. This can be extended to\n$e_n^{explicit} = (q^{(n)}, w^{(n)}, r^{(n)}, a^{(n)})$ (2)\nwhich additionally includes a wrong step-by-step answer $w^{(n)}$ and a rationale $r^{(n)}$ that explicitly identifies the errors in $w^{(n)}$ that need correcting to obtain $a^{(n)}$. This learning setup has been widely explored in prior literature (Madaan et al., 2024; Kim et al., 2024; Shinn et al., 2024). We additionally consider examples of the form\n$e_n^{implicit} = (q^{(n)}, w^{(n)}, a^{(n)})$  (3)\nwhere the explicit rationale $r^{(n)}$ is removed. Figure 1 illustrates instances of (2) and (3).\nIn our experiments, we set N = 8 for all example types. We investigate how the example formulations in the set $E = \\{E^{CoT}, E^{explicit}, E^{implicit}\\}$ affect LLMs across different tasks: labelling the correctness of an entire answer or an individual reasoning step, editing an incorrect answer, or solving a new question (we further elaborate on"}, {"title": "Experiments", "content": "We study LLMs of different sizes:\n\u2022 Command R\u00b9, 35 billion parameters;\n\u2022 Llama 3 70B Instruct (Grattafiori et al., 2024), 70 billion parameters;\n\u2022 Command R+\u00b9, 100 billion parameters;\n\u2022 WizardLM (Xu et al., 2024a), 141 billion parameters.\nNote that for the Command models, we use both the original and the Refresh versions, as preliminary experiments showed significant differences in their output and results for math reasoning tasks. We also test Titan Text G1 Express2, whose exact number of parameters has not been publicly disclosed. We note, however, that this model is substantially less capable than the others in reasoning tasks, as evidenced by the lower scores in Table 1. Therefore, we consider seven LLMs in total. We employ a greedy sampling strategy with all models. LLMs are accessed via API; further details including the inference hyperparameters and model IDs are given in Appendix C.\nOur main focus is understanding whether LLMs learn implicitly in tasks that require complex reasoning. Contemporary work investigating LLM reasoning has primarily focused on math reasoning as an early and convenient proxy for complex reasoning ability evaluation (Ahn et al., 2024; Paul et al., 2024; Ruis et al., 2024; Liu et al., 2025). Consistent with this approach, we test our method on several math reasoning benchmarks.\nGSM8K includes grade-school-level arithmetic problems that require multiple reasoning steps to solve (Cobbe et al., 2021). All problems in GSM8K can be tackled using basic arithmetic operations (addition, subtraction, multiplication, division).\nASDiv contains diverse problems of varying difficulty (Miao et al., 2020). In addition to arithmetic operations, questions can be solved using algebra, number theory, set operations and geometric formulas. They can also require pattern identification and unit conversion."}, {"title": "Tasks", "content": "In addition to evaluating on diverse math reasoning datasets, we consider auxiliary tasks that can be carried out within those datasets. We illustrate them below and in Figure 2.\nLabelling an answer. In this task we have the model assign a binary label to a CoT-style answer, identifying whether it is correct or not, given the question. Previous work has found that LLM-assigned labels are more robust when they are accompanied by a model-generated rationale (Trivedi et al., 2024; Zheng et al., 2024). Hence, we require LLMs to first output a rationale explaining their choice, followed by the label. Performance in the binary labelling tasks is measured by the macro-averaged F1-score, weighted by support to account for label imbalance. The answers to be labelled are generated by running Llama 2 7B and Llama 3 8B on the test set of each dataset (refer to Appendix D for details).\nLabelling a reasoning step. We leverage the step-wise reasoning annotations in PRM800K to have models score the correctness of a single reasoning step given the question and any previous context. Similar to the above setting, the LLM outputs a rationale followed by a binary label ('correct' or 'incorrect'). As the other datasets do not contain step-wise annotations, we perform this task only on PRM800K.\nEditing an incorrect answer. We show the model a question and a corresponding incorrect answer, then ask it to output a new, edited answer that leads to the correct solution. Performance is measured by computing the accuracy of the numerical solution. For this task, we use the incorrect portion of the pre-generated answers obtained by running Llama 2 7B and Llama 3 8B on the test sets.\nSolving a math question. In this task, we simply show the model a test set question and ask it to output the solution. As in the previous task, we compute the accuracy of the final numerical solution.\nTo encourage the models to output responses conditioned on the context, as opposed to text that merely mimics the format of the examples in it, we append the task-specific instruction after the examples. We further aid generalization by prepending the text 'Now apply what you have learned' to the"}, {"title": "Results", "content": "We find that CoT and prompting with explicit rationales have similar overall performance on the answer labelling task and when solving new questions, while the latter outperforms CoT when labelling reasoning steps (+3.2%, averaged across all models and all datasets) and editing an incorrect answer (+2.1% avg.). This advantage is aligned with previous findings that LLMs benefit from observing incorrect answers and corrective feedback in their context. On the other hand, prompting for implicit learning achieves the highest overall performance, as evidenced in Table 1. When considering all combinations of model, dataset and task, implicit learning outperforms CoT in 85% of cases. It also outperforms explicit learning in 88% of cases. In nearly half of these, the advantage of implicit over explicit learning is substantial\u2014well above 3%. This advantage is present even in tasks where, intuitively, we would expect in-context rationales to be particularly helpful, for example when editing an incorrect answer to make it correct. In fact, implicit learning gives the largest accuracy boost in the editing task: +4.4% over CoT and +2.2% over explicit learning, averaged across all models and datasets. On the solving task, its accuracy increases by 1.6 and 1.9 percentage points, respectively. Labelling answers also benefits from implicit learning prompts, with averaged F1-scores 5.6% above CoT and 6.2% above explicit learning.\nFinally, looking at the individual datasets, implicit learning gains the most on GSM8K, where it outperforms both explicit learning and CoT in"}, {"title": "Discussion", "content": "Our results demonstrate that LLMs perform better across several mathematical reasoning tasks when they are prompted for implicit learning, even over CoT prompting and providing the models with additional information through rationales. To minimise any risk that spurious correlations may be influencing these results, here we provide further, in-depth analysis of our findings, their robustness and implications.\nEffect of Context Length and Diversity\nIn our experiments, we use the same number of in-context examples across all setups. As a result, there is a mismatch between the context length of CoT and that of implicit learning, since incorporating incorrect answers introduces additional tokens into the context. As an extended context length can in itself be responsible for improved performance, we investigate the hypothesis that the additional tokens may be driving the improvement, rather than the presence of incorrect answers. We thus extend CoT's context by increasing the number of examples from eight to fourteen (we refer to this setup as CoT+). Additional examples are randomly selected from an identical sample distribution to the original eight examples. We compare this setup to implicit"}, {"title": "Human Evaluation of Generated Rationales", "content": "A follow-up research question aims to investigate what the effects of incorporating error information are on model outputs. We hypothesize that if the models are incorporating error signal implicitly to improve reasoning, this should also be reflected in downstream generated rationales.\nTo ascertain whether, and to what extent, LLMs infer implicit information between incorrect and correct answers with different prompting strategies, we carry out a blind human evaluation study of rationales generated using distinct prompts. We randomly select 300 rationales generated by running the answer labelling task on GSM8K. We select 100 rationales for each prompting strategy (CoT, explicit learning, implicit learning). We then have four annotators with domain expertise score them as 0-Poor, 1\u2013Fair or 2\u2013Good. Table 3 illustrates the average human evaluation scores achieved under each prompting strategy. We observe that CoT's performance is considerably lower than either explicit or implicit learning, with an average score of 0.68. The performance of explicit and implicit learning is similar (1.01 and 0.98 respectively). It is noteworthy that rationales generated with implicit learning prompts achieve an average score that is within only 0.03 of that achieved by explicit learning. This is evidence that LLMs can infer high-quality corrective rationales implicitly, simply observing correct and incorrect answers side by side, and that the effect of adding example rationales to the context is negligible.\nIn Figure 4 we show a breakdown of the labels assigned by human evaluators to rationales produced with each prompting strategy. While most rationales generated using CoT are assigned the minimum score, explicit and implicit learning prompting exhibit similar trends, with explicit learning"}, {"title": "Representative Rationales", "content": "Consider the math reasoning problem \u201cThere are 4 snails in one aquarium and 32 snails in another aquarium. The difference between the number of snails in the two aquariums is twice the amount of fish in both aquariums. If both aquariums have the"}, {"title": "Conclusion", "content": "We have investigated in-context implicit learning across a range of LLM sizes, and found that it outperforms both explicit learning and chain-of-thought prompting in challenging math reasoning tasks. We have further shown that LLM-generated rationales obtained via implicit learning are comparable in quality to those conditioned on in-context example rationales. Our findings are as noteworthy as they are surprising, since they call into question the benefits of widely-used corrective rationales to aid LLMs in learning from mistakes. These rationales are prevalent in current frameworks despite being expensive to curate at scale, yet our investigation suggests that they are redundant, and can even hurt performance by adding unnecessary constraints.\nLimitations\nWe have carried out an exhaustive investigation of implicit learning from mistakes, focused on in-context learning. It is worth noting that implicit learning examples\u2014which consist of triples of the form (question, incorrect answer, correct answer)\u2014can be obtained at scale by simply running more and less capable LLMs on training set questions. This opens up the possibility of investigating performance differences between explicit and implicit learning also in other paradigms, such as in the fine-tuning setting. Future work can investigate whether the results established in this paper extend to models fine-tuned using similar strategies."}, {"title": "Ethical Considerations", "content": "The licenses of all the datasets used in this paper permit their use and modification. For each dataset, we have provided a citation to the original work. Any future, non-commercial data distribution will be accompanied by the original licenses and appropriate credits."}, {"title": "Preliminary Experiments", "content": "We ran an experiment to establish the effect of context length on the results, in addition to the one shown in Section 5.1. In this preliminary experiment, we provide two CoT-style answers, both correct, for each in-context question. We refer to this setup as CoT-2.\nIt is worth noting that at the time of the preliminary experiment, there were some differences in our setup: (1) LLM instructions had slightly different wording. In particular, the labelling tasks were set up so that the LLM would output the label directly. As per Section 4.3, we later changed this to have the model output a rationale justifying its choice first, followed by the label; (2) AQUA was not yet part of our test suite.\nTable 4 illustrates the results obtained with Command R+. We observe that implicit prompting is superior to CoT-2, with the largest overall advantage in the editing and solving tasks. Surprisingly, observing incorrect answers alongside correct ones does not help the LLM label new answers for correctness in the case of GSM8K. Overall, however, the advantage of implicit prompting over CoT-2 is consistent. This, together with the results of our human analysis study of the generated rationales (Section 5.2), points to the fact that LLMs prompted for implicit learning appear to gain a better understanding of the patterns that inform correct answers and how these differ from incorrect answers\u2014which prompting with only correct reasoning traces may not sufficiently elicit."}, {"title": "Prompts", "content": "All prompts are of the form\n[examples]\\n\\n\\n\\n[instruction],\nwhere the examples are specific to the prompting strategy (i.e., CoT, explicit learning, implicit learning), while the instruction is task-dependent (i.e., labelling an answer or step, editing an incorrect answer, solving a new question). In the next sections we illustrate the examples for each prompting strategy and the instructions for each task.\nExamples\nAll few-shot examples shown below are for GSM8K. Examples for all datasets are included in https://github.com/lisaalaz/implicit-learning-with-llms.\nCoT Examples. Below you will find some questions followed by the answer."}]}