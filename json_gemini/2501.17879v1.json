{"title": "Task and Perception-aware Distributed Source Coding for Correlated Speech under Bandwidth-constrained Channels", "authors": ["Sagnik Bhattacharya", "Muhammad Ahmed Mohsin", "Ahsan Bilal", "John M. Cioffi"], "abstract": "Emerging wireless AR/VR applications require real-time transmission of correlated high-fidelity speech from multiple resource-constrained devices over unreliable, bandwidth-limited channels. Existing autoencoder-based speech source coding methods fail to address the combination of the following (1) dynamic bitrate adaptation without retraining the model, (2) leveraging correlations among multiple speech sources, and (3) balancing downstream task loss with realism of reconstructed speech. We propose a neural distributed principal component analysis (NDPCA)-aided distributed source coding algorithm for correlated speech sources transmitting to a central receiver. Our method includes a perception-aware downstream task loss function that balances perceptual realism with task-specific performance. Experiments show significant PSNR improvements under bandwidth constraints over naive autoencoder methods in task-agnostic (19%) and task-aware settings (52%). It also approaches the theoretical upper bound, where all correlated sources are sent to a single encoder, especially in low-bandwidth scenarios. Additionally, we present a rate-distortion-perception trade-off curve, enabling adaptive decisions based on application-specific realism needs.", "sections": [{"title": "1 Introduction", "content": "Upcoming use cases in wireless augmented reality (AR) and virtual reality (VR) (Hu et al. 2020), as well as other immersive applications, often require real-time transmission of high-fidelity speech data from resource-constrained edge devices, such as AR glasses or VR headsets, over inherently unreliable and bandwidth-limited wireless channels (Petrangeli et al. 2019). Efficient and adaptive transmission methods, which can handle dynamic channel conditions and limited computational resources, are critical to prevent disruptions that can degrade the immersive experience. Reliable speech source coding for transmission over unreliable dynamic wireless channels is thus the need of the hour (Weng et al. 2023; Wang et al. 2023; Bourtsoulatze, Kurka, and G\u00fcnd\u00fcz 2019).\nCurrent research in deep learning-aided source coding (Xiao et al. 2023; Wu et al. 2023; Yue et al. 2023; G\u00fcnd\u00fcz et al. 2024) has advanced compression strategies for speech data under fixed maximum output bitrates. However, three critical aspects for practical variable bitrate compression in real-world wireless systems remain overlooked: (1) A robust framework for deriving maximum permissible bitrate based on wireless channel characteristics, such as dynamic channel capacity computed from channel state information (CSI), is missing. (2) Existing autoencoder-based methods are designed for fixed bitrates with predetermined output dimensions, requiring retraining for every new dimension, which is impractical in dynamic environments. (3) Scenarios with multiple correlated sources, like speech captured by devices such as VR headsets, AR glasses, and smartphones, are largely ignored. Exploiting correlations among these sources can improve compression efficiency and bandwidth utilization, but most methods treat sources independently. While some works address correlated sources (han Li et al. 2023; Whang et al. 2024), these techniques have not been applied to speech data.\nRecent advancements in source coding have moved beyond traditional methods focused solely on bit reconstruction accuracy (Salehkalaibar et al. 2023; Zhang et al. 2021). For AR/VR and next-generation wireless applications, where tasks like speech enhancement, source separation, cloning, and generation are critical, task-aware source coding directly enhances task-specific performance. This approach enables higher compression rates, improved bandwidth efficiency, and superior task performance. For speech source coding, preserving perceptual realism is equally essential (Zhang et al. 2021). Reconstructed speech must sound natural to maintain authenticity, making realism preservation a vital complement to task-aware optimization. Perception-aware source coding has introduced perceptual loss to maintain audio realism, showing promising results. However, the interaction between perceptual loss and task-aware optimization in distributed correlated source coding for speech remains unexplored. Addressing this gap offers an opportunity to design systems that balance task performance and perceptual realism in dynamic, bandwidth-constrained wireless channels."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Wireless CSI based rate adaptation/source coding", "content": "(G\u00fcnd\u00fcz et al. 2024) provides a comprehensive survey of wireless CSI-based rate adaptation methods used in current research (He, Yu, and Cai 2024). (Yue et al. 2023) proposes and implements a neural joint source-channel coding-based talking head semantic transmission system for varying SNR levels, however, the bandwidth is assumed to be largely constant and pre-provided. (Li et al. 2024) implements reinforcement learning (RL)-based variable bitrate video chunk sizing for efficient transmission, however, assuming the bandwidth to be pre-provided, and not computing it from wireless channel capacity measurements."}, {"title": "2.2 Speech source coding", "content": "Current research extensively explores single source single receiver source coding for speech data. Works like (Xiao et al. 2023; Wu et al. 2023; Casebeer et al. 2021) implement efficient source coding under noisy channels, while (Salehkalaibar et al. 2023; Zhang et al. 2021) analyze rate-distortion-perception trade-offs. However, these approaches rely on fixed output encoder representations followed by entropy coding, which is less efficient than variable output dimensional representations. Designing such variable output representations with existing methods would require creating new encoder models for every output dimension, making it impractical. Additionally, most research targets variational autoencoder-based methods, unlike our proposed algorithm, which employs NDPCA-aided autoencoder mechanisms for fixed representations instead of stochastic latent-based generation."}, {"title": "2.3 Distributed source coding", "content": "There is a lack of prior work on distributed source coding for speech data under unreliable channel conditions. (han Li et al. 2023) introduces and implements a distributed source coding algorithm utilizing the correlation between multiple participating sources using NDPCA, and this becomes the basis of our proposed algorithm. We utilize the NDPCA-aware distributed source coding mechanism from (han Li et al. 2023), while modifying the distributed autoencoder design and introducing task-perception loss for speech-specific sources. In summary, there is a lack of current research which implements practical downstream task-aware distributed source coding schemes under dynamic bandwidth wireless channels, specifically targeted to speech data, and preserves high perceptual quality at the receiver post decoding."}, {"title": "3 Wireless Channel-Aware Distributed Source Coding", "content": ""}, {"title": "3.1 Channel State Information (CSI)-aware Dynamic Bitrate", "content": "Fig. 5 illustrates a distributed source coding scenario where multiple microphones at different room locations record"}, {"title": "3.2 NDPCA-aided Distributed System Model", "content": "We design a neural distributed spectral autoencoder with encoders at the transmitting sources and a decoder at the central receiver, as shown in Fig. 1. The autoencoder processes the spectrogram's magnitude and phase, which include frequency bins, channels, batch size, and timestamps. To handle real-time dynamic bandwidth allocation without having to retrain the autoencoder model for each new output dimension requirement, we apply neural distributed principal component analysis (NDPCA) (han Li et al. 2023) on the latent embeddings from the encoders. The architecture details of the encoder, decoder, and the NDPCA mechanism for efficient, dynamic output dimensions are elaborated in the following paragraphs.\nNDPCA-aided Distributed Encoder. The designed distributed neural encoder comprises two steps an individual multichannel spectral encoder at each of the sources, followed by one distributed PCA encoder.\n(a) Individual Spectral Encoder. We use a multichannel spectral autoencoder leveraging a deep residual-based encoder-decoder architecture for feature extraction, representation learning, and data reconstruction. The encoder architecture (E) employs a deep residual network with an initial frequency projection followed by convolutional and residual blocks. The input tensor $x \\in \\mathbb{R}^{B\\times C\\times F\\times T}$, representing batch size B, channels C, frequency bins F, and time steps T, is initially reshaped to handle the spectral dimensions effectively.\n1. Initial Frequency Projection: We project the frequency dimension to a lower dimension:\n$f = ReLU(W_2 \\cdot ReLU(W_1\\cdot x)) \\in \\mathbb{R}^{B\\times C\\times 128\\times T}$   (4)\nwhere $W_1 \\in \\mathbb{R}^{F\\times 256}$ and $W_2\\in \\mathbb{R}^{256\\times 128}$ are learned weights.\n2. Residual and Convolutional Layers: The frequency-projected data is passed through temporal convolutional layers and residual blocks. Each residual block $R_i$ introduces non-linearity and stability in feature transformation, defined as:\n$R_i(h) = ReLU(L_2(Conv(ReLU(L_1(Conv(h))))))$  (5)\nwhere $L_1$ and $L_2$ are layer normalization layers. The output of the convolutional layers and residual blocks is then flattened and transformed to the latent space $z \\in \\mathbb{R}^{B\\times Z}$\n(b) Distributed PCA encoder We train a single distributed encoder-decoder model, as described above, which would be used for variable bit rate requirements. We derive the total uplink bandwidth, B derived from channel capacity at the central receiver. The encoder output latent vector $z_i E \\mathbb{R}^{v_i} \\forall i \\in \\{1,2, . . . S\\}$ at each source, where S is the total number of sources and $v_i$ is the output dimension of the ith source's encoder. PCA is applied to each of the source encoder output latent vectors, followed by a distributed selection of the B maximum correlation components from across all the PCA components across all the sources.\n$\\sum_i = U_i \\cdot \\sum_i V_i^T$  (6)\nwhere $U_i$, $\\sum_i$, and $V_i^T$ are the principal vectors and principal component correlations obtained at the ith source. On selecting the B maximum PCA components from across all the components from all the sources, we get the optimum correlations (the selected singular values), the optimum directions to transform the latent vectors to (the corresponding columns of U), as well as the individual allocated optimal bandwidth for each source (number of components chosen from each source). The selected PCA projection vectors from each of the sources are then transmitted over wireless channels to the single receiver decoder, which concatenates the individual incoming vectors to construct the decoder input, $\\hat{z}$."}, {"title": "4 Task and Perception-aware Loss Function", "content": "Directly optimizing the end to end NDPCA-aided distributed encoder-decoder model for the required downstream task enables better compression performance than task-agnostic source coding. However, since we are dealing with human speech reconstruction at the central receiver, it is also crucial to preserve realism, that is, the reconstructed speech at the decoder output should sound human speech-like. To that end, we propose a task and perception-aware loss function to train the distributed encoder-decoder pipeline. The loss function comprises three parts a task-agnostic loss, a downstream task loss, as well as a perceptual loss component. We formulate each of these components in the subsequent subsections, after explaining an initial feature extraction on the original speech data, which is done in the very beginning of the entire pipeline.\nFeature Extraction. The initial step in this architecture is transforming the speech signal into the time-frequency domain using STFT given as:\n$X(f,t) = \\sum_{n=-\\infty}^{\\infty} x(n) \\cdot w(n - \\tau)e^{-i2\\pi fn}$ (9)\nwhere X (f, T) represents the time-frequency domain representation of the discrete-time signal x(n). Here, w(n - r) is a window function centered at 7, enabling localized analysis of x(n), and $e^{-j2\\pi fn}$ is a complex exponential isolating the frequency component f. The STFT captures how the frequency content of x(n) evolves over time. We use STFTs as source encoder inputs, and recover reconstructed STFTs as decoder outputs. The STFT window length was 2048 with hop length 512 and window length 2048."}, {"title": "4.1 Task Agnostic Loss", "content": "The task agnostic loss for the distributed autoencoder model is divided into following subparts:\n1. Mean Squared Error (MSE): The MSE loss calculates the average squared difference between the original ground truth speech data, that is, the STFT matrix of the clean speech, and the reconstructed STFT at the output of the decoder at the central receiver. For ground truth speech STFT $x_{gt}$ and reconstructed decoder output STFT $X_{dec}$, the MSE loss $L_{MSE}$ is defined as:\n$L_{MSE} = \\frac{1}{2}E\\Big[\\sum_{i=1}^B \\sum_{j=1}^F \\sum_{k=1}^T (X_{gt, ijk} \u2013 X_{dec,ijk})^2\\Big]$ (10)\nwhere B is the batch size, F is the frequency dimension, T is the temporal dimension, and the expectation is over the dataset. The MSE loss is representing the simple task-agnostic reconstruction loss.\n2. Cosine Similarity Loss: We want to train the NDPCA-aided distributed encoder-decoder model such that the higher compression performance is achieved utilizing the correlation between the sources. To ensure this, we incorporate a subcomponent in the loss function which would penalize high correlation between the latent embedding vector output by the different source encoders. Cosine similarity compares the similarity between two latent vectors $z_1$ and $z_2$, providing a measure of directional alignment. The Cosine loss $L_{cos}$ is:\n$L_{cos} = 1-E\\Big[\\sum_{i,j\\in\\{1,2,...S\\}} \\frac{z_i \\cdot z_j}{|| z_i |||| z_j ||}\\Big]$ (11)\nwhere S is the total number of sources, and $z_i$ is the encoder output for the ith source.\n3. Spectral SNR: Spectral SNR measures the fidelity of the reconstructed spectral data relative to the original, in decibels (dB). The spectral SNR loss is given by:\n$L_{SNR} = 10 log_{10} \\Big( \\frac{E [(X_{gt}^2]}{E [(X_{gt}-X_{dec})^2]}\\Big)$  (12)\n4. Peak Signal-to-Noise Ratio (PSNR): PSNR represents the ratio between the maximum possible value and the reconstruction error in dB. The PSNR loss is given by:\n$L_{PSNR} = -10log_{10} (\\frac{X_{max}^2}{L_{MSE}})$ (13)\nwhere $x_{max}$ is the maximum possible value of the ground truth speech STFT. Each of the aforementioned subparts contributes uniquely to achieving the balance between accurate spectral reconstruction and regularized latent representations."}, {"title": "4.2 Downstream Task: Speech Enhancement with Score-Based Generative Model", "content": "We choose speech enhancement, that is, the task of extracting the clean speech signal from the background noise, as our downstream task of interest. This makes sense from a practical standpoint, considering that speech enhancement serves as a common initial block in multiple speech pipeline tasks in wireless AR/VR. To do speech enhancement, we use a score-based Langevin diffusion model (Welker, Richter, and Gerkmann 2022a) directly on the decoder output reconstructed STFT. The score output by the diffusion model learns to estimate the gradient of the log probability density of the noise-removed clean speech given the decoder output reconstructed noisy speech.\nForward Process. Here we refer to the decoder output STFT as the \"noisy\" spectrogram, and the clean speech STFT, produced by the downstream speech enhancement diffusion model, as the clean spectrogram. The forward process of the score-based Langevin diffusion model is implemented as an Ornstein-Uhlenbeck (OU) SDE:\n$dx = -\\theta(y - x)dt + \\sigma(t)dw$,  (14)\nwhere x is the clean spectrogram, y is the noisy spectrogram, @ is the stiffness parameter, o(t) is the time-dependent noise level, and w is the Wiener process. It adds noise gradually to the spectrogram in subsequent timesteps as:\n$\\sigma(t) = \\sigma_{min} + \\frac{(\\sigma_{max}^2 - \\sigma_{min}^2)log(t)}{\\tau} $ (15)\nwhere $\\sigma_{min}$ and $\\sigma_{max}$ are the minimum and maximum noise levels.\nScore Network. The score network is based on a speech enhancement diffusion model (Welker, Richter, and Gerkmann 2022b). It uses time embedding and conditioning the noisy spectrogram to output the score estimate (gradient of log probability). The score computation is given as:\n$s_{\\theta}(x,y,t) = -\\sigma(t)^2\\nabla_x log p(x|y, t)$ (16)\nwhere $s_{\\theta}$ represents the score function, $\\sigma^2(t)$ represents time-dependent noise schedule, $\\nabla_x log p(x|y, t)$ represents gradient of the log-probability of the clean spectrogram x given the noisy spectrogram y. The score network learns"}, {"title": "4.3 Perceptual Loss: Preserving Realism", "content": "The last part of the loss function deals with preserving human-like speech, or realism, at the decoder output. Note that, to stay task-agnostic with the perceptual loss component, we intend to preserve realism at the decoder output, not at the downstream speech enhancement diffusion model output. The MS-STFT Discriminator (D\u00e9fossez et al. 2022)(Multi-Scale Short-Time Fourier Transform Discriminator) is a neural network architecture designed to extract perceptual features from audio data by leveraging multi-resolution frequency analysis and convolutional operations.\nHierarchical Convolution Mapping. To extract meaningful perceptual features from the decoder output speech STFT, we use a hierarchical convolution mapping(Kavukcuoglu et al. 2010). Hierarchical convolutional mapping involves applying a series of stacked convolutional layers to extract progressively abstract and meaningful features. Each convolutional operation extracts localized features from the spectrogram using a kernel as:\n$Y_{ij} = \\sum_{m=0}^{k_h} \\sum_{n=0}^{k_w}X_{(i+m)(j+n)} \\cdot W_{mn}$,  (18)\nwhere $x_{ij}$ is the input at position (i,j), $W_{mn}$ represents the kernel weights, and $k_h$, $k_w$ denote the kernel height and width. This operation slides the kernel over the input, computing a weighted sum of overlapping regions to produce the output $Y_{ij}$. The use of progressively abstract feature extraction at the later layers of the hierarchical CNN is particularly suited for speech, which comprises hierarchical features, starting from localized phonemes, to broader semantic features across words or sentences spoken.\nMulti-Scale Discriminator The MS-STFT Discriminator contains multiple discriminators, each configured with different STFT parameters. For each discriminator Di:\n$L_i, F_i = D_i(x)$,  (19)\nwhere $D_i(x)$ represents the i-th discriminator applied to input x, producing logits $L_i$ and feature maps $F_i$. This formulation captures both the decision output and intermediate perceptual features extracted by the discriminator. The input to the model is x \u2208 RB\u00d71\u00d7T, producing complex spectrograms CB\u00d72\u00d7F\u00d7T'. It uses N-layer 2D convolutions with kernel size k, strides, dilation d, and normalization to iteratively map features CB\u00d7C\u00d7T'\u00d7F \u2192 CB\u00d7C'\u00d7T\" \u00d7F'. Multiple discriminators at M-scale STFT resolutions capture hierarchical time-frequency features, yielding logits and feature maps across scales."}, {"title": "Perceptual Loss.", "content": "We extract perceptual features of both the ground truth and the enhanced audio after passing it through the discriminator. The difference of their logits is then minimized to compute the perceptual loss. The logits are computed using MS-STFT Discriminator. $F_l$ at intermediate layers represent perceptual features of the audio. These features correspond to local energy distributions in the spectrogram, harmonic structures and transient details. For the l-th layer, the feature map $F_l$ is defined as:\n$F_l = \\phi(W_l * F_{l-1} + b)$, (20)\nwhere $ \\phi$ is the activation function, $W_l, b_l$ are the weights and biases and * is the convolutional operator. These hierarchical features capture perceptual cues such as pitch, timbre, and temporal modulations.\nLogits as Output. The final output logits L are computed after the last convolutional layer. Each discriminator produces a scalar logit $L_i$ indicating the realism of the input audio:\n$L = Conv2D(F_l)$. (21)\nThe logits are trained using adversarial objectives with real and generated signals as inputs:\n$L_D = -E[log D(x_{real})] - E[log(1 - D(x_{fake}))].$ (22)\nBy employing multiple STFT configurations, the model captures audio features across a wide range of temporal and spectral resolutions. The convolutional layers learn hierarchical features, progressively refining perceptual representations and the normalization techniques ensure stable gradient flows, improving convergence. The overall model architecture has been illustrated in Fig. 2."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Baseline Methods", "content": "The lack of prior work in task and perception-aware distributed source coding for speech data makes it impossible to make a fair comparison with the existing source coding baselines. Hence, we design the following two approaches as baselines.\nJoint Autoencoder (JAE) The joint autoencoder approach, which would be referred to as joint E1D1 henceforth, deals with the simple case where all the speech data from all the sources are combined as a single super source, and processed by a single encoder for source coding. This is the best case when it comes to dynamic bandwidth-based source coding rate adaptation, since all the information is available at a single source. Thus, in this case, we take the encoder output, followed by simple PCA, which chooses the top k principal components, where k is the maximum available bandwidth. This best case forms an upper bound on our proposed NDPCA-based distributed source coding architecture, where the individual sources cannot communicate with each other. For joint E1D1, the input spectral data includes magnitude and phase components and is represented as $X = [X_m, X_p] \\in \\mathbb{R}^{B\\times 2\\times F\\times T\\times S}$, where B is the batch size, 2 represents the channels (magnitude and phase), F is the number of frequency bins, T is the number of time"}, {"title": "Encoder.", "content": "processes the input X to generate a latent representation Z, from which the top k PCA components are transmitted over the channel, and decoded at the receiver. This process is mathematically expressed as:\n$Z = Encoder(X), \\hat{Z} = PCA(Z), \\hat{X} = Decoder(\\hat{Z})$.  (23)\nDistributed E2D1 & E4D1 without NDPCA. Our second category of baselines involves naive separated encoders for every individual source, or partially separate encoders combining a subset of the sources, without distributed PCA. Since in our experiments, we deal with speech recorded by 4 different mics (sources), we refer to these baselines as E4D1 and E2D1. E4D1 has a separate encoder for each of the mics, and each encoder processes the spectrogram of the recorded speech by that mic to extract latent representations. For the E4D1 architecture, the spectral data input consists of magnitude and phase components, represented as $X_i = [X_{i,m}, X_{i,p}] \\in \\mathbb{R}^{B\\times 2\\times F\\times T}, i\\in \\{1,2,3,4\\}$. Each input $X_i$ is processed by its corresponding encoder to produce latent representations, $Z_i$ as $Z_i = Encoder (X_i), i\\in \\{1,2,3,4\\}$. The latent features are decomposed into private and shared components as $Z_i = [Z_{i,p}, Z_{i,s}], i\\in \\{1, 2, 3, 4\\}. and are then concatenated as:\n$Z_i = [Zip, Zi,s], i\\in \\{1,2,3,4\\}$.   (24)\nE2D1 follows the architecture along the same lines with a small difference of two encoders instead of four."}, {"title": "5.2 Experimental Setup", "content": "The experimental setup leverages a structured spectral dataset comprising clean and noisy audio signals. Clean spectral samples $X_{clean} \\in \\mathbb{R}^{1025\\times 600}$ were paired with their noisy counterparts $X_{noisy}^c$, sourced from four distinct channels, $c\\in \\{3,4,5,6\\}$, ensuring consistent input dimensions through zero-padding where necessary. Training was performed with a batch size of N = 16 and a learning rate of $\\eta$ = 2 \u00d7 10-4, over T = 100 epochs. Metrics such as mean squared error $L_{MSE}$, nuclear norm $||Z||_*$, cosine similarity, spectral SNR, magnitude loss $L_{mag}$, phase loss $L_{phase}$, and PSNR for both clean and noisy reconstructions were monitored. Training utilized PyTorch, executing on a CUDA-compatible GPU with a fixed random seed seed 0 for reproducibility."}, {"title": "5.3 Results", "content": "Task Agnostic: Comparison between Baselines and Proposed Algorithms. Fig. 3b shows the PSNR achieved for the reconstructed speech versus the total bandwidth. In the task-agnostic setting, the total bandwidth is optimally allocated among the various sources involved using the proposed NDPCA-aided perception-aware source coding algorithm. We see from Fig. 3b that the baseline Joint E1D1, which represents all source data being concatenated at one encoder, and thus maximum utilization of correlation of the different sources, achieves the upper bound PSNR values. Similarly, the baselines Distributed E4D1 and E2D1, where"}, {"title": "6 Conclusion", "content": "In this paper, we introduce a novel task and perception-aware distributed speech source coding algorithm under dynamic bandwidth conditions. The NDPCA-aided autoencoder design, coupled with the direct downstream speech enhancement loss and perception loss-aware optimization, leads to superior PSNR values under a given bandwidth than those obtained for the following two baselines - (1)naive source coding where each of multiple sources is treated as an individual source and encoded without considering the correlation with the other sources, and (2) the task-agnostic case which is optimized for reconstruction loss only."}, {"title": "A Dataset Preprocessing", "content": "For our system model, we used Chime 6 dataset (Barker et al. 2018) which is a widely used for speech separation and recognition. The dataset consists of conversational speech audio with 8 speakers and 6 microphones spread across a room. The audio signals are aligned, compensated for frame drops and clock skew. Furthermore, all audio data are distributed as WAV files with a sampling rate of 16 kHz. Each session consists of the recordings made by the binaural microphones worn by each participant. We extract the noisy and clean audio signals for speakers and microphones through the given .json file for each start and end time for all the speaks as ground truth and the microphones as noisy data for that particular time. The extracted data is converted into .pkl with magnitude, phase, and parameters across time."}, {"title": "B Dataset Retrieval Explanation", "content": "As shown in Fig. 5, there are 8 speakers and 6 microphones in the room located at different positions as shown. Conversational audio during a party session is recorded through all the microphones. The social gatherings consist of close acquaintances who are encouraged to act casually and authentically. Multiple four-channel microphone systems capture and document these gatherings in their entirety. While participants can freely move between different areas, they must spend a minimum of half an hour in each location. The conversations are unscripted, allowing attendees to discuss any subject they choose, without following predetermined scenarios. For privacy protection, certain identifying information has been removed from the recordings during post-processing, in accordance with participant consent agreements.\nThe technical setup involves six Microsoft Kinect units positioned strategically throughout the space, ensuring that every area is monitored by at least two devices simultaneously. To ensure clear audio recording for transcription purposes, each participant is equipped with Soundman OKM II Classic Studio binaural microphones. These devices connect through a Soundman A3 adapter to personal Tascam DR-05 stereo recorders worn by the participants."}, {"title": "C Spectrogram Distributions", "content": "To obtain spectrogram distributions of noisy and clean audio files, first we preprocess the data to manually separate the clean and noise wave file. we input a .wav file that contains raw waveform data and metadata describing speaker segments, including, start and end time along with session ID. Using timestamps from the JSON file, audio segments for a specific speaker are"}, {"title": "extracted by converting them into seconds.", "content": "Using librosa, the waveform y(t) the waveform is sampled between start time and end time with a sampling rate fs to yield:\ny[n] = y\\Big(\\frac{n}{fs}\\Big)$  n = 0, 1, ..., N \u2013 1   (25)\nwhere N is duration times $f_s$.\nShort-Time Fourier Transform. The core transformation involves converting the time domain signal y[n] to time frequency representation D(f, t) to plot a spectrogram as:\n$D(f,t) = \\sum_{n=0}^{N-1}y[n]\\cdot w[n \u2212 t \u00b7 H] \u00b7 e^{-j2\\pi fn/N}$,  (26)\nwhere w[n] is the windowing function, H is the hop length, and t is the time frame index. As shown in Fig 6, we obtain a magnitude and a phase response of the audio file, depicting and differentiating the characteristics of noisy and clean audio files. Fig. 6a and Fig. 6b represent the magnitude and phase of clean audio spectrograms with a clear time-frequency representation. The consistent patterns suggest the presence of tonal and structured features in the clean signal. The maximum magnitude of sound goes up to Odb for the clean signals. While the phase is typically harder to interpret directly, it remains consistent with the underlying clean signal. Fig. 6c and Fig. 6d demonstrate the noisy magnitude and phase response, where the noisy signal magnitude go as high as 20db. The frequency components are less distinct, and additional energy appears spread across frequencies, characteristic of added noise."}, {"title": "3D Power Spectrgram.", "content": "Fig. 7 shows the 3D power spectrogram distribution of the clean and the noisy signals. By applying Fourier transform and using mesh in Matlab we plot the power spectrogram of both noisy and clean audio signals as shown in Fig. 7a and Fig. 7b. In Fig. 7a we observe sharp and concentrated power peaks at specific frequencies, indicating well-defined tonal or harmonic components of the signal. Most of the power is concentrated in lower frequencies, which is typical for many natural sounds like speech. Fig. 7b shows a broader distribution of power across the frequency spectrum, indicating the presence of noise. Furthermore, the increased power levels, especially in high-frequency regions, suggests noise contamination."}, {"title": "D Training Hierarchy and Loss values", "content": "In this section, we observe the convergence and the loss for both task agnostic and task aware perceptual pipelines.\nLoss Convergence. From Fig. 8, we observe that E2D1 performs better than E4D1 on the overall loss curve and converges to a lower value. As expected, the E2D1 cases perform better than the E4D1 cases since pairs of sources are processed at one encoder for E2D1, creating higher correlation utilization than E4D1, where each source has its own encoder. Mathe-"}, {"title": "matically, consider the latent representations $z_i \\in R^{d_i}$", "content": "produced by the encoders. In E4D1, the four encoders independently encode features as $Z_1, Z_2, Z_3, Z_4$, allowing the system to represent input modalities or perspectives as a concatenated vector $Z = [Z_1; Z_2; Z_3; Z_4] \\in R^d$, where d = $d_1 + d_2 + d_3 + d_4$. However, this independent encoding limits the correlation utilization between sources, resulting in a suboptimal alignment of features. In E2D1, the encoders process pairs of sources, producing latent representations $Z_{1,2}, Z_{3,4} \\in R^{d1+d2}$. This structure enables better correlation utilization between paired sources, minimizing the projection error \u20ac = ||x \u2212 Pz(x)||, where Pz(x) is the projection onto the latent space. The improved inter-source alignment in E2D1 results in richer feature extraction and higher reconstruction accuracy. Furthermore, the proposed algorithm with E2D1 achieves comparable performance to the upper bound Joint E1D1, demonstrating its efficiency and near-optimality. By leveraging pairwise correlation, E2D1 achieves enhanced representational capacity, significantly reducing latent divergence E[||zi - zj ||]. This improves both cross-reconstruction and global decoding consistency, thus outperforming E4D1 in tasks requiring efficient utilization of correlated features."}]}