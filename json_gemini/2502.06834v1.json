{"title": "A Unified Knowledge-Distillation and Semi-Supervised Learning Framework to Improve Industrial Ads Delivery Systems", "authors": ["Hamid Eghbalzadeh", "Yang Wang", "Rui Li", "Yuji Mo", "Qin Ding", "Jiaxiang Fu", "Liang Dai", "Shuo Gu", "Nima Noorshams", "Sem Park", "Bo Long", "Xue Feng"], "abstract": "Industrial ads ranking systems conventionally rely on labeled impression data, which leads to challenges such as overfitting, slower incremental gain from model scaling, and biases due to discrepancies between training and serving data. To overcome these issues, we propose a Unified framework for Knowledge-Distillation and Semi-supervised Learning (UKDSL) for ads ranking, empowering the training of models on a significantly larger and more diverse datasets, thereby reducing overfitting and mitigating training-serving data discrepancies. We provide detailed formal analysis and numerical simulations on the inherent miscalibration and prediction bias of multi-stage ranking systems, and show empirical evidence of the proposed framework's capability to mitigate those. Compared to prior work, UKDSL can enable models to learn from a much larger set of unlabeled data, hence, improving the performance while being computationally efficient. Finally, we report the successful deployment of UKDSL in an industrial setting across various ranking models, serving users at multi-billion scale, across various surfaces, geological locations, clients, and optimize for various events, which to the best of our knowledge is the first of its kind in terms of the scale and efficiency at which it operates.", "sections": [{"title": "Introduction", "content": "When users surf on applications with large amounts of users (e.g., social media, streaming services, online shops, etc), they would like to see ads that they find relevant to their interests and needs. To achieve this goal, ads ranking systems are built to find a handful of best matched ads from the candidate pool of large number of ads within seconds.\nTo balance the accuracy and efficiency under such tight time and large scale constraints, industrial ads ranking systems use a cascade multi-stage ranking design Wang et al. (2023a). Each stage is responsible for scanning through the candidates provided by the previous stage, and provides the top candidates for the next stage (see Figure 1). Typically, the models used in earlier stages (e.g., retrieval stage) are simpler, more efficient but"}, {"title": "Related Work", "content": "Multi-stage ranking A multi-stage ranking framework is widely adopted within an industrial recommendation system in order to balance between the efficiency and accuracy Zhang et al. (2019). Covington et al. Covington et al. (2016) employed deep neural network models for both candidate generate stage and ranking stage for Youtube recommendation, Huang et al. Huang et al. (2020) designed the Facebook search system with three stages (indexing, retrieval and ranking) under a unified embedding based framework. Six different stages including retrieval, pre-ranking, relevance ranking, ranking, re-ranking, mix-ranking were deployed to build a search system as described in Li et al. (2021).\nSample selection bias Within a multi-stage ranking system, it has been shown that sample selection bias may be induced in earlier stages, if the data related to delivered items was used to train the early stage models Wang et al. (2023b). There have been different types of approaches to mitigate this bias in the literature. For instance, authors in Wang et al. (2023b) proposed two variants of unsupervised domain adaptation, and have associated a manually chosen threshold for hard labeling. Qin el, al. Qin et al. (2022) trained different stages in a cascading flow fashion by training each stage separately followed by cross stage distillations. Building on top of Qin et al. (2022), Zhao et, al. Zhao et al. (2023) emphasized the importance of rank order alignment between stages with bid information, and designed a chunk based data sampling schema to facilitate learning this order alignment. Zheng et al. Zheng et al. (2022) tackled this problem from a multi-objective perspective by treating samples at by different stages as different class of labels for the model to learn the preference order among relevance, exposure, click and purchase.\nAnother line of research is around finding or creating new data sources to break the data-model feedback loop"}, {"title": "Problem Statement", "content": "In multi-stage ads ranking systems, one of the major issues is the inconsistency and miscalibration between early and late-stage models. In this section, we first analytically show how bias is introduced in ranked items, despite the use of unbiased ranking estimators. Further, we analyze model calibration in various stages via simulation. Through this, we uncover inherent miscalibrations in such systems. We then hypothesize a potential solution to this problem which we empirically validate in Section 4."}, {"title": "Introduction of Bias in Ranked Items via an Unbiased Model", "content": "In this section, we start by analyzing a simple case of one-stage ranking. Suppose we have n ads candidates with the underlying ground truth Cost Per Mille (CPM) following the distribution of $y_i \\sim N(\\mu, \\sigma^2)$. Let $z_i$ be a random variable representing the CPM predictions by some model (here we are simplifying the problem by considering a model outputting the whole eCPM while in reality eCPM was constructed by multiple models). If we assume $M_1$ to be an unbiased predictor with the prediction error $e_1 \\sim N(0, \\sigma_1^2)$, we will have $z_i \\sim N(\\mu, \\sigma^2 + \\sigma_1^2)$. The true value of $y_i$ is unknown and only model predictions $z_i$ can be observed. Under a typical ranking setup, the model will select the top-k ads based on its own predictions. Let $S_1$ be the top-k predicted ads subset selected by $M_1$ from n available ads, such that $S_1 = \\{z_1, z_2, ..., z_k \\}$ where $z_1 > z_2 > ... > z_k,$. We can obtain the expectations of eCPMs following the method described in Royston (1982):\n$\\mathbb{E}(z_i | \\mu, \\sigma^2, \\sigma_1^2, n) = \\mu + \\sqrt{\\sigma^2 + \\sigma_1^2} \\sqrt{\\frac{n-i}{n-1}} \\frac{\\Gamma(\\frac{n-i}{2}) }{\\Gamma(\\frac{n-2\\alpha +1}{2})}$ (1)\nwhere $\\alpha = 3.375$, $\\Gamma$ is the Gaussian CDF. With Eq.1 in hand, we can now do some analysis on the relationship between eCPMs and CPMs of the selected ads. The goal of this ranking system is to maximize the total return which is the sum of the CPMs of the selected ads. If the model is perfect without any errors, the system reaches its optimal state by finding the top-k CPM ads from the given n ads. The optimal return can be also calculated via Eq.1 by setting $\\sigma_1 = 0$. From Eq.1 we can observe that the estimation of the ith ranked item increases as variance of the predictor becomes larger while the total return becomes smaller. In other words, the model predictions are over-calibrated on the set of selected ads. With this toy example, we can see that an unbiased estimator can produce a biased result under a ranking system context. This phenomenon can be understood intuitively by the following argument: Although the model's overall prediction is unbiased on the entire candidate set, it is under-calibrated on some parts of the population and over-calibrated on other parts of the population. Therefore, when the model selects the top-k ads based on its prediction, it naturally tends to pick the ones that are over-calibrated."}, {"title": "The inherent Miscalibration in Multi-stage Ranking", "content": "In this section we extend our analysis to a two stage ranking system which better resembles a real world multi-stage ranking system. Suppose we have a first-stage unbiased predictor $M_1$ with prediction error of $e_1 \\sim N(0, \\sigma_1^2)$, and a second-stage unbiased predictor $M_2$ with prediction error of $e_2 \\sim N(0, \\sigma_2^2)$. Given the common assumption that early-stage models have higher variance than later stage models, let us further assume that $\\sigma_1^2 > \\sigma_2^2$. Like the discussion in the previous section, we assume we have n ad candidates following the distribution of $y_i \\sim N(\\mu, \\sigma^2)$. The first stage model will select top $k_1$ ads and send them to the second stage, and the second stage model will select top $k_2$ ads and send them to the users.\nThe model calibration on the set of ads selected by the first stage ($S_1$) is defined as:\n$cal(i,j) = \\frac{\\mu'_i}{\\mu'_j}$ (2)\nwhere $\\mu'_i$ and $\\mu'_j$ are average CPM estimation of stages i and j on $S_1$, respectively. We use this quantity as an example to illustrate the inherent miscalibration that exist in a multi-stage ranking system. The cross-stage calibration ($M_1$ w.r.t $M_2$) is calculated by $cal(1, 2)$. Similarly, we can calculate the calibration of each stage model e.g, $M_j$ via $cal(j, 0)$ where $\\mu'_0$ is the average of true CPMs on $S_1$.\nIn order to analyze the calibration as a function of number of retrieved samples and model characteristics, we run a simulation and show the results in Figure 2. Here, we depict the result of simulation for cross-stage calibration, as well as for each stage's calibration as a function of $k_1$ and different variants of model noise level. Regardless of the choice of parameters, we can see that on $S_1$ the first stage model is over-calibrated (Figure 2-a,b) while the second stage model is well calibrated (Figure 2-c). This observation aligns with the intuition discussed in Section 3.1 in that an unbiased model only becomes biased on the set of candidates selected by itself. This simulation result also aligns well with the experimental observation described in Section 4.1, and at the same time suggests the direction of using the second stage model to correct the bias of the first stage model."}, {"title": "Tackling Miscalibration and Introduced Bias", "content": "In Sections 3.1 and 3.2 we presented two important and fundamental problems that are inherent to multi-stage ranking systems. In the next section (Section 4), we investigate solutions to these two problems via A"}, {"title": "A Unified Framework for Knowledge Distillation and Semi-Supervised Learning (UKDSL)", "content": "In this section, we introduce UKDSL, a Unified Framework for Knowledge Distillation and Semi-Supervised Learning of industrial scale ranking systems, that has been successfully deployed in multi-billion scale industrial settings across various ranking models, serving users at multi-billion scale, across various surfaces, geological locations, clients, and optimize for various events, which to the best of our knowledge is the first of its kind in terms of the scale and efficiency at which it operates. In the following, we start by the motivation of the framework, and further detail various modules of UKDSL.\nSemi-Supervised Learning (SSL) techniques have been proven useful for mitigating the distribution gap between training and serving data, as discussed in Section 2. However, for industrial systems, it is difficult to applied SSL techniques at scale. In our attempts to adopt SSL in industrial-scale systems, we face the following 4 variety and scale challenges:\n1. Scale and variety of data\n2. Scale and variety of model types\n3. Variety of ranking stages\n4. Scale and variety of features\nTo tackle these challenges, we proposed the Unified Framework for Knowledge Distillation and Semi-Supervised Learning (UKDSL), a framework that's designed to be flexible and scalable for almost all scenarios in a modern ads delivery system with billions of users. UKDSL has 3 main components: (1) Cross-stage knowledge distillation; (2) Distillation from foundation models; and (3) Semi-supervised feature selection. Figure 3 shows the components of UKDSL. We describe each of the components in detail in the following sections, and show our results of how they mitigate the cross-stage miscalibration and improve model performance."}, {"title": "Semi-Supervised Cross-Stage Knowledge Distillation", "content": "The typical knowledge distillation setup involves a teacher model and a student model. The student model is the model that is used to serve production traffic, which is also the model that we aim at improving. The teacher model is usually a separate model or an ensemble that's more complex and more accurate. Due to the scale of data and variety of models, it is difficulty and cost inefficient to develop and maintain separate dedicated teachers for all models in an ads delivery system. Within the framework of UKDSL, we solve this"}, {"title": "Semi-Supervised Knowledge Distillation", "content": "We make use of the standard binary cross entropy loss for cross-stage distillation. Typical ads ranking models such as CTR and CVR models produces a single probability value denoted by z, and the models are trained using binary cross entropy loss $L_{BCE}(y, z)$ where y is the ground truth label:\n$L_{BCE}(y, z) = - [y \\log(z) + (1 - y) \\log (1 \u2013 z)]$ (3)\nIn our cross-stage distillation setup, instead of relying on the ground truth labels y, we rely on the predictions made by a teacher model $z_T$ as pseudo labels. The student model is similarly trained using binary cross entropy loss $L_{BCE}(z_T, z)$."}, {"title": "Performance of Semi-Supervised Cross-Stage Distillation", "content": "In table 1, we present our model performance results after applying cross-stage distillation. We use the following two metrics for performance measurement.\n1. Calibration: $\\sum p_{pre-ranking} / \\sum p_{ranking}$. Calibration is defined as the ratio of the average predictions of the test model over the average predictions of a reference model or the ground truths. A value close to 1 indicates consistency across two ranking stages when using a reference model, or unbiasedness when using ground truths.\n2. Normalized Entropy (NE): NE is defined as:\n$\\sum_{i,c} y_{i,c} log(p_{i,c}) / \u2013 \\sum_{i,c} y_{i,c} log(\\bar{y}_c)$, where $y_{i,c}$ is the label for the $i^{th}$ data on class c, and $p_{i,c}$ is the corresponding prediction. $\\bar{y}_c$ is the average probability for class c being positive. The smaller NE suggest better model performance. In the case of consideration data, where there is no ground truth label, as they were not sent to users, we use the later stage model's prediction as the label to calculate NE as a measure of cross stage consistency.\nThe results show that on impression data, the average predictions from pre-ranking model and late stage ranking model are very close to each other. However, pre-ranking model generates higher prediction values than late stage ranking model on consideration data which aligns with the theoretical analysis done in Section 3.2. Given consideration data represents the actual serving traffic of our models, improving the consistency on consideration data traffic would be crucial for our system performance. By following the analysis done in Section 3.2 and adding cross stage co-training and distillation on consideration data, we allow late stage models generate predictions as supervision for pre-ranking models on consideration data on the fly. This approach improved cross stage consistency by reducing cross stage mis-calibration and reduce cross stage NE, with minimal impact on performances on impression traffic."}, {"title": "Semi-Supervised Feature Selection (SSFS)", "content": "Selection bias in training data can inadvertently lead to the choice of features that aren't necessarily the most beneficial for the serving data. This challenge highlights the necessity for a robust feature selection approach, specifically optimized for distillation, that can effectively reconcile the differences between training and serving data. This forms the motivation for a novel feature selection methodology.\nOur approach is illustrated in Figure 5, which consists of following main steps:\n1. Building the Teacher Model: We begin by training a teacher model on the impression dataset. This teacher model is then used to generate labels for the unlabeled consideration dataset.\n2. Label Augmentation: Next, we augment the unlabeled dataset with the labels generated by the teacher model."}, {"title": "Training a Simplified Model", "content": "Subsequently, we train a simpler version of the student model using all candidate features on a mixed dataset with both labeled and unlabeld data."}, {"title": "Perturbation-based Features Importance", "content": "Then, we compute the feature importance scores for each feature in a withheld dataset using a perturbation-based feature importance algorithm with the simplified model."}, {"title": "Combine biased and unbiased features", "content": "Finally, we combine the biased features ranked using regular approach and the unbiased features ranked using our approach based on their relative rank and/or weighted feature importance, and select the best mix that balances the results on both the impression and consideration data sets."}, {"title": "Semi-Supervised Learning from Foundation Models (SSLFM)", "content": "In our semi-supervised cross-stage distillation described in Section 4.1, we always use the next-stage model as the teacher. However, this approach does not work for the model in the last stage, for which no production model can be used as a teacher. With the recent rise in popularity of foundation models, we leverage"}, {"title": "Baselines", "content": "The experiment comparisons in this manuscript are all compared against the latest production models in a multi-billion-scale industrial ads ranking system, prior to the adoption of UKDSL. Our criteria for selecting baselines was to identify models that 1) have been proven to operate effectively at the industry scale; 2) represent the state-of-the-art ads ranking product models in the industry. We consider these production recommendation models to be among the state-of-the-art baselines that meet the above criterion."}, {"title": "Conclusion and Future Work", "content": "One big drawback of the traditional ads ranking approaches that rely solely on supervised learning is their inherent inability to generalize to the ad candidates that have not yet been delivered to users for ad impressions. Semi-supervised learning can be used to mitigate this drawback, and our proposed UKDSL framework enables the application of semi-supervised learning in an industrial-grade ads ranking systems at scale, allowing developers to deploy semi-supervised learning to all model types across all stages with minimal additional cost and effort. Our experiment results show that the UKDSL framework helps overcome the inherent mis-calibration issue, thus improving model calibrations, but also improves their performance on both labeled and unlabeled data. UKDSL has been launched to major industrial-scale ads recommendation models across different ranking stages and traffic. This indicates that it can be generalized to diverse user demographics and content types, considering the scale and reach of the deployed ads platform.\nBy sharing our success stories in leveraging unlabeled data in an industrial-scale ads ranking systems, we hope to motivate more future work in this important and impactful area of research. Particularly, the use of foundation models for distillation on earlier stage models is an interesting and currently under-explored direction. Having the same foundation model as the teacher for all ranking stages can not only potentially improve their individual model performance, but also further improve the cross-stage prediction consistency. Other areas worth further exploring include developing more efficient foundation models, improving knowledge transfer efficiency, and improved sampling methods for learning from unlabeled data more efficiently."}]}