{"title": "A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction Following", "authors": ["Yin Fang", "Xinle Deng", "Kangwei Liu", "Ningyu Zhang", "Jingyang Qian", "Penghui Yang", "Xiaohui Fan", "Huajun Chen"], "abstract": "Large language models excel at interpreting complex natural language instructions, enabling them to perform a wide range of tasks. In the life sciences, single-cell RNA sequencing (scRNA-seq) data serves as the \"language of cellular biology\", capturing intricate gene expression patterns at the single-cell level. However, interacting with this \"language\" through conventional tools is often inefficient and unintuitive, posing challenges for researchers. To address these limitations, we present INSTRUCTCELL, a multi-modal AI copilot that leverages natural language as a medium for more direct and flexible single-cell analysis. We construct a comprehensive multi-modal instruction dataset that pairs text-based instructions with scRNA-seq profiles from diverse tissues and species. Building on this, we develop a multi-modal cell language architecture capable of simultaneously interpreting and processing both modalities. INSTRUCTCELL empowers researchers to accomplish critical tasks such as cell type annotation, conditional pseudo-cell generation, and drug sensitivity prediction using straightforward natural language commands. Extensive evaluations demonstrate that INSTRUCTCELL consistently meets or exceeds the performance of existing single-cell foundation models, while adapting to diverse experimental conditions. More importantly, INSTRUCTCELL provides an accessible and intuitive tool for exploring complex single-cell data, lowering technical barriers and enabling deeper biological insights.", "sections": [{"title": "Main", "content": "Advances in artificial intelligence (AI), particularly large language models (LLMs) such as GPT-4 [1], PaLM [2], LLAMA [3], and Claude [4], have transformed natural language into a powerful medium for managing real-world tasks [5, 6]. Similarly, in the life sciences, single-cell RNA sequencing (scRNA-seq) data serves as a \"language of cellular biology\", encoding diverse gene expression patterns. Like grammar and vocabulary in natural language, scRNA-seq data reveals the complexity of biological systems, with each cell type and state presenting a unique expression [7, 8, 9].\nTo unlock the potential of this \"life science language\", researchers have increasingly turned to natural language processing (NLP) technologies to analyze single-cell gene expression data. These approaches typically rely on extensive gene expression datasets [10, 11] for pre-training, followed by fine-tuning for specific downstream tasks [12, 13, 14, 15]. While effective, such methods often demand significant domain expertise and face challenges in adaptability. As an alternative, recent efforts have reformulated single-cell gene expression data into sequences of gene names ranked by expression levels, enabling language models to process both task-specific instructions and gene name lists directly [16, 17]. However, focusing exclusively on top-expressed genes overlooks important information from lower-expressed ones, while incorporating all genes inflates context length and computational demands. Furthermore, converting quantitative gene expression data into text risks losing essential numerical precision.\nTo address these limitations, it is crucial to develop an approach that bridges the structured numerical nature of single-cell data with the expressive flexibility of natural language. Drawing inspiration from how humans integrate multiple sensory inputs to enhance comprehension, we propose INSTRUCTCELL, a multi-modal AI copilot specifically designed for single-cell analysis. It interprets numerical single-cell data and natural language instructions, as well as generates outputs in either modality, effectively bridging the gap between these distinct data types.\nFirst, we construct a multi-modal single-cell instruction dataset that unifies essential single-cell analysis tasks into a cohesive collection. Each cell in the dataset is represented by its gene expression profile and enriched with biological attributes specifying species, tissue, sequencing protocol, and other pertinent biological contexts, all organized into natural language instructions. To equip our AI copilot with fundamental conversational abilities, we incorporate traits reflecting diverse personalities, motivations, and proficiency levels. Using LLMs, we simulate various communication styles, enabling the copilot to adapt to diverse research contexts and expertise levels.\nSecond, building upon this dataset, we develop a multi-modal cell language model capable of harmonizing single-cell gene expression data with textual information. The architecture includes a Q-Former module for embedding gene expression profiles, a backbone pre-trained language model (LM) for robust textual processing, and a cell reconstruction block for generating detailed gene expression profiles. Through instruction tuning, the model gains domain-specific expertise in single-cell analysis, enabling it to adeptly handle interleaved biological and textual data. This design allows INSTRUCTCELL to handle diverse input and output formats, unifying tasks of cell comprehension and generation.\nFinally, we thoroughly evaluate INSTRUCTCELL across a range of single-cell analysis tasks, demonstrating robustness to varied instruction styles and its ability to produce accurate, relevant outputs. Extensive experiments highlight its capacity to uncover biological insights and validate the necessity of each architectural component. INSTRUCTCELL extends instruction-following capabilities to the cell-language multi-modal space, laying the groundwork for advancing single-cell research."}, {"title": "Results", "content": "Overview of Instruct Cell\nIn this paper, we propose INSTRUCTCELL, a multi-modal AI copilot specifically designed for single-cell analysis. The development of INSTRUCTCELL involves two critical components: (1) constructing multi-modal single-cell instruction data and (2) training a multi-modal cell LM. An overview of INSTRUCTCELL is illustrated in Fig. 1.\nMulti-modal single-cell instruction data construction INSTRUCTCELL aims to advance single-cell analysis by leveraging natural language as an interactive medium. Current models typically rely on a single modality, focusing either on single-cell data alone [14, 12, 13], text data alone [16, 17], or deriving cell embeddings directly from text [18, 19]. Consequently, available datasets are predominantly rooted in scRNA-seq data [10, 11] or textual representations derived from it [16, 18]. Thus, our primary goal is to construct a multi-modal single-cell instruction dataset that enables the model to comprehend and process both the \"language\" of life sciences and human language effectively."}, {"title": "InstructCell enables conditional pseudo-cell generation", "content": "CPCG is a task that requires the model to generate a single-cell gene expression profile matching specified conditions, such as cell type, tissue, and species, as illustrated in Fig. 1(c). To explore the generative modeling capability of INSTRUCT CELL, experiments are conducted using scRNA-seq data from 9 tissues\u2014bladder, blood, liver, lung, spleen, thymus, and vasculature\u2014collected from humans and mice. The datasets utilized in these experiments include Tabular-Sapiens, Mouse-Atlas, and PBMC68K. First, we directly provide the generation conditions to INSTRUCTCELL in textual form, and it generates cells based on these conditions. Fig. 2(a) visualizes the comparison between the real cells' gene expression profiles and those generated by INSTRUCTCELL. The generated cell distributions closely resemble those of real cells, even accurately replicating some outliers. Notably, the instruction-response templates used for generation are not included in the training phase, demonstrating that the model is robust to unseen input templates. This suggests that INSTRUCTCELL effectively generalizes to new instructions by capturing the underlying biological patterns rather than overfitting to specific training templates. Additionally, we observe that the UMAP visualizations of generated cells exhibit slightly higher clustering compared to real cells. While this clustering might suggest a slight oversimplification of the inherent biological variability, it reflects the model's ability to faithfully capture the overall structure of the data and preserve key features in the reduced-dimensional space.\nFurthermore, we investigate the model's ability to learn single-cell expression patterns specific to each cell type. As depicted in Fig. 2(b), each dot plot organizes genes on the x-axis and different cell types on the y-axis, with each cell type being grouped by its top three most significant genes. To identify the key expressive genes for each cell type, we compare individual cells to the average expression levels of other cells within the same cell type. Specifically, for each cell, we consider its gene expression levels and compare them to the average gene expression levels across all other cells of the same cell type. We conduct Welch's t-test [31] for each gene, comparing the expression level in the individual cell to the average expression in the other cells. A significant result indicates that the gene is particularly expressive or distinctive in that cell compared to others of the same type. We systematically identify the top three most significant genes for each cell type, recognizing these as the key expressive genes. For the cells generated by the model, we directly display the expression proportions and average expression levels of these significant genes. By closely examining these patterns, we find that the generated cells faithfully replicate the gene expression profiles of real cells, particularly in terms of gene-specific expression levels and cell-type-specific patterns. These results demonstrate the model's capacity to generate biologically plausible gene expression profiles that preserve intricate gene-level and cell-type-specific patterns critical for realistic simulations.\nWe further compare INSTRUCTCELL against existing cell generation methods across three dimensions, as illustrated in Fig. 2(c). As detailed in Methods, AsKNN quantifies the structural divergence between generated and real data by examining label consistency within cell clusters in the reduced-dimensional space, while pKNN evaluates positional alignment by assessing how accurately generated cells match specific cell types. We report the average AsKNN and pKNN values for K of 5, 10, 25, and 50 across all datasets in the radar charts. The radial axis range is set according to the maximum value observed in the data, providing a clear visual reference. All radar charts in this paper follow this convention, and no further explanation will be provided for subsequent figures. Both the instruct and chat versions of INSTRUCTCELL closely align with real cells, effectively replicating biological structures and maintaining accurate cell positioning. The MMD metric, which measures the overall similarity between generated and real cells, shows that both versions of INSTRUCTCELL achieve significantly lower MMD values compared to baseline models.\nAmong the baseline methods, scDiffusion [32] and scGAN [33] focus solely on cell generation. Unlike the end-to-end training strategy of INSTRUCTCELL, ScDiffusion requires separately trained modules, which introduces additional complexity. scGAN, on the other hand, is known for training difficulties, mode collapse, and instability. Moreover, scDiffusion and scGAN produce continuous gene expression values, whereas scRNA-seq data are inherently discrete, and some downstream tasks require discrete inputs. Similarly, the gene expression profiles generated by Cell2Sentence [16] are reconstructed in expression space through normalization and log1p-transformation, losing their original discrete characteristics. In contrast, INSTRUCTCELL preserves the discrete nature of gene expression profiles by treating them as a distinct modality, maintaining precise numerical information and capturing low-expression genes. This approach makes INSTRUCTCELL more scalable and flexible, offering distinct advantages in handling complex datasets and adapting to diverse experimental scenarios.\nOverall, these findings suggest that INSTRUCTCELL effectively captures the essential characteristics of different cell types, ensuring robust performance in cell simulation tasks. Its intuitive design allows the direct generation of single-cell"}, {"title": "InstructCell boosts the performance of cell type annotation", "content": "Annotating cell types based on scRNA-seq data is a fundamental task in single-cell analysis. INSTRUCTCELL innovates in CTA by reframing it as a single-cell question-answering task. Given an interleaved sequence of texts and gene expression profile, INSTRUCTCELL generates the textual response that conveys its prediction regarding the cell type (Fig. 1(c)). To assess whether the model can accurately differentiate between various cell types, we first benchmark it across 5 scRNA-seq datasets\u2014Xin-2016, Segerstolpe-2016, He-2020-Liver, Ma-2020, and Bastidas-Ponce-2019. These datasets span 3 major organs/tissues (liver, pancreas, and skin) in humans and mice, covering more than 40 cell types and over 94,000 cells, utilizing mainstream scRNA-seq protocols such as Smart-seq2 and HiSeq X Ten sequencing. The datasets chosen showcase a range in both data size and complexity, as depicted in Fig. 1(a) and Fig. 3(a).\nFrom Fig. 3(a), we observe that INSTRUCTCELL performs on par with, or even surpasses, foundation models (scBERT, scGPT, and Geneformer) across nearly all datasets, despite not relying on large-scale unlabeled pre-training. This indicates that INSTRUCTCELL remains robust and reliable when handling a diverse range of single-cell data. In contrast, foundation models often require extensive fine-tuning on each dataset for multiple tasks, resulting in greater training complexity, more cumbersome deployment, and a substantially larger number of parameters. While Cell2Sentence simplifies representation, it may omit valuable quantitative details and exclude potentially informative, lower-expressed genes. By contrast, INSTRUCTCELL preserves a broader spectrum of gene expression information. This design enables the model to better capture the biological complexity of single-cell data, enhancing its ability to align with natural language instructions and deliver more accurate outputs.\nFig. 3(b) shows that the cell types predicted by INSTRUCTCELL are highly consistent with the original cell types. This is particularly evident for similar cell types with overlapping characteristics, where the model demonstrates an ability to distinguish subtle differences with minimal errors. Fig. 3(c) indicates that INSTRUCTCELL achieves high accuracy (\u00bf0.9) for most cell types displayed in the confusion matrix. This consistent performance across diverse cell types demonstrates the model's capacity to accurately interpret single-cell data and predict cell types, even in the presence of interleaved text, without requiring extensive pre-training or dataset-specific fine-tuning. The confusion matrices for the remaining datasets are presented in Fig. 13. Beyond its consistent performance across diverse cell types, INSTRUCTCELL demonstrates adaptability in handling data from different species, ensuring accurate predictions across varied biological contexts. This cross-species compatibility enhances its potential as a practical tool for single-cell data analysis, supporting tasks such as comparative studies and integrated research pipelines."}, {"title": "InstructCell enhances precision of drug sensitivity prediction", "content": "As illustrated in Fig. 1(c), DSP involves providing the model with drug information and single-cell gene expression data, allowing it to predict whether a cell is resistant or sensitive to a given drug. For our experiments, we gathered scRNA-seq data from three organs, including datasets from humans (GSE149383 and GSE117872) and mice (GSE110894), paired with drug sensitivity information. Notably, the GSE117872 dataset includes an additional category, labeled 'holiday', which refers to observations made during off-treatment periods.\nFig. 4(a) shows that INSTRUCTCELL achieves superior performance across all three evaluation metrics on the GSE117872 and GSE110894 datasets, while on the GSE149383 dataset, it delivers results comparable to single-cell foundation models. These results demonstrate the model's ability to predict single-cell responses to different drugs based on textual descriptions of conditions and gene expression data.\nFig. 4(b) presents a UMAP visualization comparing annotated and predicted single cells with different labels. The visualization demonstrates that INSTRUCTCELL effectively distinguishes these categories, accurately capturing the separation between drug-sensitive and drug-resistant cells while appropriately positioning holiday cells. This suggests that the model successfully reflects biological differences in drug responses within the reduced-dimensional space.\nFig. 4(c) presents a detailed confusion matrix for each label, showing that INSTRUCTCELL achieves high accuracy levels (\u00bf0.95) across different species and tissues. This consistent performance highlights the model's robustness in integrating single-cell gene expression data with drug information, making it a reliable tool for drug response prediction across diverse biological contexts."}, {"title": "Robustness of InstructCell", "content": "As a tool designed to assist researchers, INSTRUCTCELL must effectively understand and address questions phrased differently. Fig. 5 provides quantitative evidence for this adaptability. To assess how the model reacts to unfamiliar instruction templates, we first test it on the CPCG task, which is more demanding than the two classification tasks. We measure several metrics on cells generated using both unseen and seen templates across three datasets. As shown in Fig. 5(a), the AsKNN, pKNN, and MMD values remain similar in both conditions, indicating that INSTRUCTCELL maintains consistent performance regardless of prior exposure to the instruction templates. This implies that INSTRUCTCELL preserves both detailed structural features and overall distributional patterns, allowing it to adapt effectively to different researchers' phrasing styles.\nTo further investigate this adaptability, we evaluate both the instruct and chat versions of INSTRUCTCELL on all tasks, as shown in Fig. 5(b). The first two panels focus on classification scenarios. In some templates, we provide multiple-choice options; however, whether these options are present or not has almost no impact on accuracy. This flexibility is beneficial, as it means the model does not need pre-defined options to achieve high performance, helping keep inputs short and avoiding constraints that could slow inference due to quadratic complexity. That said, the instruct version proves slightly more robust than chat, likely because it is optimized for direct, goal-oriented instructions, thus maintaining stable performance across varied input formats. In contrast, the chat variant, designed for open-ended exchanges, may be more sensitive to subtle stylistic shifts.\nFinally, the last panel in Fig. 5(b) presents average results for the CPCG task over three datasets. Both instruct and chat perform comparably, with consistently low standard deviations, indicating that either approach can reliably produce realistic pseudo-cell data. This outcome suggests that the model's internal representation of biological patterns"}, {"title": "InstructCell demonstrates robustness to varied instruction styles", "content": "is sufficiently robust that differences in instruction style exert minimal influence on generation quality."}, {"title": "InstructCell identifies biologically significant marker genes", "content": "To evaluate the model's ability to uncover biologically meaningful insights, we explore whether it can identify marker genes without prior knowledge of known cell-type markers injected into the model. Since scRNA-seq datasets provide gene expression profiles, we employ gradient-based saliency methods to determine which genes most strongly influence the model's predictions. Specifically, we employ the Vanilla Gradient method [34] to extract the top 10 most significant genes for each cell type, as detailed in the Methods section. We then evaluate the biological relevance of these genes by comparing them with documented marker genes in the CellMarker2.0 database and recent literature [35].\nFig.6 presents results for two datasets: (a) He-2020-Liver and (b) Xin-2016. Red markers in each row indicate that genes among the top 10 key genes identified by the model, are either reported as marker genes for the corresponding cell type in the CellMarker2.0 database or in recent literature. From two heatmaps, we observe that the genes assigned higher scores by the model are highly likely to be documented as marker genes for their respective cell types. For most cell types, more than half of the top 10 genes identified by the model are found in the CellMarker2.0 database. In cases where a gene from the top 10 is not explicitly reported as a marker gene in the database, it may still be highlighted in the literature as a relevant marker. For example, HHEX is identified as a highly expressed gene for human Delta cells [36, 37], although it is not categorized as a marker gene for human Delta cells in the database. These findings suggest that INSTRUCTCELL not only effectively identifies established marker genes but also has the potential to uncover novel marker genes."}, {"title": "InstructCell exhibits high-quality expressive capabilities", "content": "To maximize the scientific utility of INSTRUCTCELL, it is crucial that its interactions with researchers remain both accurate and efficient. Accordingly, we assess the quality of its responses by evaluating fluency, grammatical integrity, and the inclusion of predictive results. We randomly sample 500 entries from the multi-modal single-cell instruction templates for quality assessment.\nAlthough manual evaluation is the gold standard for capturing human preferences, it is slow and resource-intensive. As an alternative, we adopt an \u2018LLM-as-a-judge' approach, using Claude 3.5 Sonnet [4] as a surrogate evaluator. This"}, {"title": "Multi-modal instruction data construction", "content": "We first gather publicly available scRNA-seq datasets for each task and convert them into multimodal instruction-following data using instruction-response templates. However, constructing instruction-response templates for each task by hand is labor-intensive, resulting in a limited number of templates that often lack diversity. Such constraints can lead to trained models that are less robust and produce outputs with limited variety (Fig. 7(f)). To address this issue, we propose leveraging an existing LLM to synthesize a wide range of instruction-response templates. We further increase"}, {"title": "Instruct Cell model architecture", "content": "Input embeddings\nINSTRUCTCELL can take a mixture of text and single cells as input. Consider a sequence $X = (x_1, x_2,..., x_m)$ of length $m$, where text and single cells are interleaved. Each single cell is encapsulated between two special tokens. Specifically, if $x_i$ represents a single cell's gene expression profile, then $x_{i-1}$ and $x_{i+1}$ are marked with $\\langle\\text{CELL}\\rangle$ and $\\langle/\\text{CELL}\\rangle$, respectively.\nIf $x$ represents a text token within the input sequence, it is converted into a $d$-dimensional text embedding $e$ by the LM's input embedding layer $\\text{embed}(\\cdot; W)$:\n$e = \\text{embed}(x; W)$.\nHere, $W$ is a learnable matrix with dimensions $(|V| + 3) \\times d$, where $V$ is the original vocabulary of the pre-trained LM. Note that three special tokens, $\\langle\\text{CELL}\\rangle$, $\\langle/\\text{CELL}\\rangle$, and $\\langle\\text{SIGNAL}\\rangle$, are added to the vocabulary. These tokens are initialized randomly at the beginning of training. On the other hand, if $x$ is a single cell's gene expression profile in the input sequence, it is transformed into $k$ ($k \\geq 1$) cell embeddings of dimension $d$ by the cell encoder $\\text{qformer}(\\cdot; \\phi)$:\n$(e_1, e_2,..., e_k) = \\text{qformer}(x; \\phi)$,\nwhere $\\phi$ represents the learnable parameters of the cell encoder.\nAll input embeddings, whether derived from text tokens or single cells, are then concatenated in sequence to form the matrix $E_X = (e_1, e_2, ..., e_n)$, where the $i$-th embedding $e_i$ could be a text embedding or a cell embedding. This entire process is denoted as $E_X = \\text{map}(X; W, \\phi)$.\nQ-Former module\nThe Q-Former module comprises three primary components: a set of learnable $d$-dimensional query vectors $Q^{(0)} = (q_1, q_2,......, q_k)$, a multi-layer perceptron (MLP) that encodes raw input cells into keys and values, and a transformer model [49] that connects the outputs of the MLP to the query vectors."}, {"title": "Cell reconstruction module", "content": "The cell reconstruction module of INSTRUCTCELL is a conditional variational autoencoder (CVAE) parameterized by \u03c8 = (\u03c8e,\u03c8\u03b1) where \u03c8e and \u03c8a represent the encoder and decoder parameters, respectively.\nDuring training, the encoder encodes the current condition c and the corresponding single-cell s to produce a posterior distribution. The decoder then reconstructs the original input single-cell s from the latent variable z sampled from the posterior distribution q(z|s, c; \u03c8e). The log probability density of the conditional distribution for the single-cell s can be expressed as:\n$\\log p(s | c)=\\log \\int p(s, z | c) d z=\\log \\int \\frac{p(s, z | c) q(z | s, c)}{q(z | s, c)} d z \\geq \\int q(z | s, c) \\log \\frac{p(s, z | c)}{q(z | s, c)} d z=\\mathbb{E}_{z \\sim q(z | s, c)}[\\log p(s | z, c)]+\\mathbb{E}_{z \\sim q(z | s, c)}\\[\\log \\frac{p(z | c)}{q(z | s, c)}\\]=\\mathbb{E}_{z \\sim q(z | s, c; \\psi e)}\\[\\log p(s | z, c; \\psi a)\\]-K L(q(z | s, c; \\psi e) \\| p(z | c)).$\nThe right-hand side of the inequality represents the evidence lower bound (ELBO). The training objective of the cell reconstruction module is to maximize the ELBO: [52]:\n$\\mathcal{L}_{\\text {recon }}(\\psi ; s, c)=-\\mathbb{E}_{z \\sim q(z | s, c; \\psi e)}[\\log p(s | z, c; \\psi a)]+K L(q(z | s, c; \\psi e) \\| p(z | c)).$\nscRNA-seq data have several key characteristics including overdispersion [53, 54], sparsity [55, 56], and non-negative integer expression values for each gene. Considering these properties, specifying the decoder-generated distribution as a Gaussian distribution is inappropriate. Researchers commonly analyze scRNA-seq data using either the Negative Binomial (NB) distribution or the Zero-Inflated Negative Binomial (ZINB) distribution [57, 58]. For the NB distribution, the variance increases with the mean, which aligns with the overdispersion observed in scRNA-seq data. The ZINB distribution builds on the NB distribution by accounting for non-biological factors contributing to excess zeros. Since the NB distribution is encompassed by the ZINB distribution, we select the ZINB distribution as the output distribution of the decoder.\nThe inference and generative processes in the cell reconstruction module are similar to those in scVI [59]. Specifically, the latent variable z is divided into two independent variables zs and l. Therefore, the loss function can be rewritten as:\n$\\mathcal{L}_{\\text {recon }}(\\psi ; s, c)=-\\mathbb{E}_{z\\_s \\sim q(z\\_s | s, c ; \\psi e), l \\sim q(l | s, c ; \\psi e)}[\\log p(s | z\\_s, l, c ; \\psi a)]+\\alpha\\[K L(q(z\\_s | s, c ; \\psi e) \\| p(z\\_s | c))+K L(q(l | s, c ; \\psi e) \\| p(l | c))],$\nwhere \u03b1 balances the trade-off between the reconstruction quality and output diversity of CVAE [60]. We specify p(zs|c) as an isotropic Gaussian distribution N(0, I), and p(l|c) as a log-normal distribution Lognormal(l\u00b5, l2) where $l_{\\mu}=\\mathbb{E}\\_s\\[\\log(\\sum\\_i s\\_i)\\]$, and $l\\_{\\sigma}^{2}=\\mathbb{E}\\_s\\[(\\log(\\sum\\_i s\\_i)-l\\_{\\mu})^{2}\\]$."}, {"title": "Pre-trained LM", "content": "The architecture of pretrained language models (PLMs) can be categorized as encoder-only [62, 63, 64, 65], decoder-only [66, 67, 68, 3], or encoder-decoder [69, 24, 70, 71]. Since the three single-cell tasks considered in this study can all be formulated as sequence-to-sequence (seq2seq) tasks, any decoder-only or encoder-decoder PLM can flexibly serve as the backbone network of INSTRUCTCELL. To simplify the notation in mathematical formulations, we denote the backbone network's forward computation process (excluding the transformation of input sequences into input embeddings) as $\\text{PPLM}(\\cdot; \\theta)$.\nSingle-cell understanding task For the single-cell understanding task, given an input sequence $X$ and the corre-sponding target sequence $Y = (y_1, y_2,..., y_{\\text{Ltarget}})$, where $y_i$ is the $i$-th token in the target sequence, the objective of LM is to minimize the negative log joint probability of the target sequence [62]:\n$\\mathcal{L}\\_{\\text{scu}}(\\theta, W, \\phi ; X, Y)=-\\sum\\_{i=1}^{\\text{Ltarget }} \\log P\\_{\\text{PPLM }}(y\\_i | \\text{map}(X ; W, \\phi), \\text{map}(Y\\_{i-1} ; W, \\theta);\theta)=\\sum\\_{i=1}^{\\text{Ltarget }} \\log P\\_{\\text{PPLM }}(y\\_i E\\_X, E\\_{Y\\_{i-1}} ;\\theta)=\\mathcal{L}\\_{\\text{text }}(\\theta, W, \\phi ; X, Y),$"}, {"title": "Single-cell generation task", "content": "In the single-cell generation task, the target sequence $Y$, with a length of $\\text{Ltarget}$, ends with a special signal token $\\langle\\text{SIGNAL}\\rangle$ that marks the transition to generating a single cell. In this task, the LM's objective is not only to generate the target sequence but also to produce conditional information $h_{\\langle\\text{CPCG}\\rangle}$ based on $(X, Y)$. This conditional information $h_{\\langle\\text{CPCG}\\rangle}$ enables the decoder in the cell reconstruction module to generate the target single cell s. The overall process can be formalized as follows:\n$\\mathcal{L}\\_{\\text{scg}}(\\theta, W, \\phi, \\psi ; X, Y, s)=\\mathcal{L}\\_{\\text{text }}(\\theta, W, \\phi ; X, Y)+\\mathcal{L}\\_{\\text{recon }}\\left(\\psi ; s, h_{\\langle\\text{CPCG}\\rangle}\\right),$\nwhere, in this study, $h_{\\langle\\text{CPCG}\\rangle}$ represents the hidden state of the $\\langle\\text{SIGNAL}\\rangle$ token in the last layer of the LM. By leveraging the LM as an encoder for the biological characteristics of single cells, INSTRUCTCELL enhances the flexibility and expressiveness of user-specified input conditions.\nCollectively, INSTRUCTCELL can be trained end-to-end and the corresponding optimization objective is defined as:\n$\\arg \\min \\_{\\theta, W, \\phi, \\psi} \\mathbb{E}\\_{(X, Y, s) \\sim \\mathcal{D}}\\[\\mathbb{I}(s=\\emptyset) \\mathcal{L}\\_{\\text {scu }}(\\theta, W, \\phi ; X, Y)+\\mathbb{I}(s \\neq \\emptyset) \\mathcal{L}\\_{\\text {scg }}(\\theta, W, \\phi, \\psi ; X, Y, s)],$\nwhere $\\mathcal{D}$ represents the training set, $s=\\emptyset$ indicates the absence of the cell modality in the output, and $\\mathbb{I}(\\cdot)$ is the indicator function. From the perspective of multitask learning, the majority of the model's parameters (i.e., the LM without the language modeling head) are shared across multiple tasks, which effectively prevents overfitting on any single task [72, 73]."}, {"title": "Evaluation", "content": "Answer extraction Unlike INSTRUCTCELL-instruct, INSTRUCTCELL-chat generates free-form responses, making direct evaluation of its performance challenging. For the CPCG task, the generated gene expression profiles are placed at the end of the output, which enables straightforward extraction and analysis. In contrast, answers for the other two classification tasks are embedded within conversational text, which requires a reliable extraction tool. To address this, we employ xFinder [74], a state-of-the-art extraction model whose reliability is evaluated meticulously in our setting. Specifically, we first construct a test set by replacing the $\\langle\\text{output}\\rangle$ placeholders in single-round dialogue templates with the labels derived from all datasets. For example, GSE110894 contains 2 distinct labels (Figure 9). Since there are 2,359 different templates used for DSP (see the section Instruction-response template construction details), we can construct 4,718 test samples. Overall, the test set comprises 178,411 samples. We find that xFinder achieves an accuracy exceeding 99.97% on this test set, demonstrating its effectiveness in automatically extracting answers.\nMetrics For the CTA and DSP tasks, we use standard evaluation metrics, including weighted F1, macro F1, and accuracy. When evaluating INSTRUCTCELL-chat, there are rare instances where the model fails to provide an answer. In calculating accuracy, we use the 'true accuracy' metric, treating unanswered cases as incorrect predictions. However, for F1 scores, we exclude unanswered cases because F1 relies on valid predictions for precision and recall calculations. Including unanswered cases as errors would misrepresent the balance between precision and recall, leading to an inaccurate evaluation of the model's performance.\nFor the CPCG task, we evaluate the model's performance using Maximum Mean Discrepancy (MMD), sKNN, and pKNN. Specifically, for a real single-cell dataset $\\{(Y\\_i, C\\_i)\\}\\_{i=1}^{M}$ unseen during the training, the trained model generates a pseudo single-cell dataset $\\{(x\\_i, c\\_i)\\}\\_{i=1}^{M}$ based on the corresponding label set $\\{c\\_i\\}\\_{i=1}^{M}$. Since both real and generated single-cell data are count data, we normalize each sample to ensure the gene counts sum to 10,000, followed by log1p-transformation. To evaluate the model, we first apply principal component analysis (PCA) to reduce the dimensionality of the real single-cell data to 50 dimensions, then further reduce it to a two-dimensional embedding space using UMAP [75]. The pseudo single-cell data is mapped into the same two-dimensional embedding space. Finally, we compute MMD, sKNN, and pKNN within this embedding space to assess the model's performance.\nMMD is a kernel-based statistical test, used to measure the difference between two probability distributions [76]. Given samples $\\{x\\_i\\}\\_{i=1}^{M}$ from the estimated distribution $p$ and $\\{y\\_i\\}\\_{i=1}^{M}$ from the real distribution $q$, the empirical estimate of MMD between $p$ and $q$ can be calculated using the following formula:\n$\\operatorname{MMD}[p, q]=\\frac{1}{M} \\sum\\_{i=1}^{M} \\phi\\left(x\\_{i}\\right)-\\frac{1}{M} \\sum\\_{i=1}^{M} \\phi\\left(y\\_{i}\\right) \\|_{\\mathcal{H}}^{2}$"}, {"title": "", "content": "where $\\mathcal{H"}, "is a universal Reproducing Kernel Hilbert Space (RKHS) and $\\phi(\\cdot)$ is the corresponding reproducing kernel. Similar to [77, 33"], "MMD": "n$K(x, y)=\\sum\\_{i=1}^{3} \\gamma\\_{i} \\exp \\left(-\\gamma\\_i \\|x-y\\|^{2}\\right),$MMD$[p, q]=\\sqrt{\\frac{1}{M^{2}} \\sum\\_{i=1}^{M} K\\left(x\\_{i}, x\\_{j}\\right)-\\frac{2}{M^{2}} \\sum\\_{i=1}^{M} \\sum\\_{j=1}^{M} K\\left(x\\_{i}, y\\_{j}\\right)+\\frac{1}{M^{2}} \\sum\\_{i, j="}