{"title": "Northeastern Uni at Multilingual Counterspeech Generation: Enhancing\nCounter Speech Generation with LLM Alignment through Direct\nPreference Optimization", "authors": ["Sahil Wadhwa", "Chengtian Xu", "Haoming Chen", "Aakash Mahalingam", "Akankshya Kar", "Divya Chaudhary"], "abstract": "The automatic generation of counter-speech\n(CS) is a critical strategy for addressing hate\nspeech by providing constructive and informed\nresponses. However, existing methods often\nfail to generate high-quality, impactful, and\nscalable CS, particularly across diverse lin-\nguistic contexts. In this paper, we propose a\nnovel methodology to enhance CS generation\nby aligning Large Language Models (LLMs)\nusing Supervised Fine-Tuning (SFT) and Di-\nrect Preference Optimization (DPO). Our ap-\nproach leverages DPO to align LLM outputs\nwith human preferences, ensuring contextu-\nally appropriate and linguistically adaptable\nresponses. Additionally, we incorporate knowl-\nedge grounding to enhance the factual accuracy\nand relevance of generated CS. Experimental\nresults demonstrate that DPO-aligned models\nsignificantly outperform SFT baselines on CS\nbenchmarks while scaling effectively to mul-\ntiple languages. These findings highlight the\npotential of preference-based alignment tech-\nniques to advance CS generation across var-\nied linguistic settings. The model supervision\nand alignment is done in English and the same\nmodel is used for reporting metrics across other\nlanguages like Basque, Italian, and Spanish.", "sections": [{"title": "1 Introduction", "content": "Traditional methods for tackling hate speech, par-\nticularly on social media platforms, have predomi-\nnantly relied on removing offensive posts or ban-\nning users to deter future violations. While these\nmeasures can be effective in the short term, they\noften lead to unintended consequences.\nFor example, banned users may create new ac-\ncounts to continue spreading harmful content, per-\npetuating the problem. Additionally, such ap-\nproaches raise critical concerns about balancing the\nsuppression of harmful speech with the protection\nof free expression, as overly aggressive measures\ncan inadvertently stifle legitimate discussions and\nopen debate.\nIn contrast, Counter Narrative (CN) strategies\noffer a more constructive and nuanced approach"}, {"title": "2 Related Work", "content": "Hate Speech in the past has been tackled in mul-\ntiple ways. Some works have focused on hope\nspeech, which tackles HS with a constructive view. However, unlike\na CN, hope speech does not directly respond to\nhate speech or counter a message in opposition.\ncomparest different strate-\ngies for tackling hate speech like counter-trolling,\nanti-stereotyping , detoxification\nand misinformation counter-\ning. Each of these\nmethods has its own merits and demerits, but for the\nscope of this task, we focus on CN generation. Dif-\nferent CN generation strategies have been explored.\nConstraint-based CN generation leverages various\nlinguistic , and outcome constraints to guide the generation of text. With the\nadvent of Large Language Models (LLMs), there\nhas been a paradigm shift towards leveraging these\nmodels for constraint-based counter-narrative (CN)\ngeneration, as they don't require prior knowledge\nof fixed templates or rigid rule sets. LLMs can dy-\nnamically adapt to context and generate a wide va-\nriety of responses, offering greater flexibility than\ntraditional constraint-based methods. Studies have\ndemonstrated that LLMs, when fine-tuned on hate\nspeech and counter-speech datasets, can produce\nmore contextually relevant and diverse responses.\nFor instance, research by evaluated the zero-shot capabilities of models like GPT-\n2, DialoGPT, ChatGPT, and FlanT5 in generating\ncounter-speech, highlighting the potential and limi-\ntations of LLMs in this domain.\nPrevious studies, such as  and\nhave highlighted that Large\nLanguage Models (LLMs) often hallucinate when\nthey lack sufficient context. For instance, early\nmethods focused on predefined responses or tem-\nplates, limiting their flexibility and scalability. Su-\npervised learning models, while more adaptable,\nrequire extensive labeled datasets, which are chal-\nlenging to obtain for the diverse manifestations of\nhate speech. These limitations have prompted the\nexploration of more sophisticated techniques, such\nas leveraging large language models and reinforce-\nment learning, to enhance the effectiveness and\nadaptability of CS generation . Research has demonstrated that incorporating an\nexternal grounded knowledge base significantly\nenhances the generation capabilities of both con-\nversational agents and LLMs. For conversational\nagents, grounding responses in external knowledge\nleads to more accurate, contextually relevant, and\nfact-based outputs, as shown in studies like  and . Similarly,"}, {"title": "3 Dataset", "content": "We used the multilingual dataset* provided for the\nshared task as shown in Table 2. The Hate Speech\n(HS) examples are sourced from the MTCONAN\ndataset, while the Counter Narratives (CN) are\nnewly generated. Additionally, each HS-CN pair is\naccompanied by five background knowledge sen-\ntences, some of which are specifically curated to\nprovide relevant context for generating the Counter\nNarratives.\nWe did not use any external dataset for this\nshared task besides the one in the shared task."}, {"title": "4 Architecture", "content": "4.1\nPre-trained Models\nIn this shared task, we leveraged DPO on the\nLlama-3 model to generate\nCounter Speech (CS) and demonstrated its superi-\nority over the SFT-only model. We selected Llama-\n3 as our base model due to its proven effectiveness\nacross multiple NLP benchmarks*. While we also\nexperimented with smaller fine-tuned models like\nGPT-2 and Llama-2 , their performance was found to\nbe inferior compared to Llama-3.\n4.2 Generating Rejected Answers\nWe optimized LLMs using DPO, leveraging their\nSFT counterparts as a reference to guide preference-\nbased alignment. Rejected CS responses, as illus-\ntrated in Figure 1, were generated using GPT-40\nto ensure diversity and contextual\nrelevance. The quality of these rejected responses\nis directly proportional to the quality of the HS.\nThus, a low-quality HS would result in a low qual-\nity rejected response. These rejected responses\nwere utilized as negative samples in conjunction\nwith preferred responses to fine-tune the LLMs\nthrough DPO alignment, improving the quality of\ngenerated CS and enabling scalability across di-\nverse linguistic contexts.\nIn the context of Counter-Narrative (CN) gen-\neration, rejected answers serve two critical pur-\nposes:\nDefining Negative Samples for Learning:\nRejected answers act as negative examples\nthat help the model understand what consti-\ntutes a less-effective or less-preferred counter-\nnarrative. These rejected responses might lack\nrelevance, contextual accuracy, or the nec-\nessary persuasive tone to effectively counter\nhate speech, making them valuable for con-\ntrastive learning.\nReinforcing Desirable Counter-Narrative\nBehavior: By contrasting rejected an-\nswers with ground-truth (preferred) counter-\nnarratives, DPO trains the model to prioritize\ngenerating responses that are more contextu-\nally appropriate, impactful, and aligned with\nhuman preferences. This process helps the\nmodel learn to avoid unpersuasive, factually"}, {"title": "5 Experimental Results", "content": "All training processes in this paper were executed\non a single 32 GB V-100 GPU. Initially, we applied\nsupervised fine-tuning using the Llama3 basic and\ninstruct models, utilizing default parameters and\nLORA  fine-tuning techniques. The\ndefault parameters included a batch size of 4, com-\nbining gradients over 4 steps, and weight decay of\n0.01. For LoRA, we set the rank (r) to 16, the scal-\ning factor (alpha) to 16, and applied a dropout of 0\nto the low-rank layers, targeting the attention lay-\ners. The training dataset provided in the shared task\nwas relatively small, consisting of only 1,500 lines,\nnecessitating a higher number of epochs to suffi-\nciently train the SFT model. To prevent excessively\nlong outputs, we set the maximum sequence length\nto 640. We employed the Adam optimizer with a\nlearning rate of 2e-4, conducting training for 500\nepochs for each model. The entire training process\nspanned approximately 70 hours. After evaluating\nthe models on the validation dataset, we selected\nthe checkpoints at 150 epochs for the Llama3 ba-\nsic model and 200 epochs for the Llama3 instruct\nmodel, referred to as run1 and run2 respectively.\nNext, we extended the training on our DPO\ndataset based on the SFT checkpoint. For this\nphase, we adjusted the learning rate to 5e-4 and con-\ntinued for an additional 80 epochs for each model.\nUpon further validation testing, we observed some\nimprovements in the basic model, while the instruct\nmodel showed signs of degradation. Finally, we\nopted for the 80 epochs checkpoint of the Llama3\nbasic model as our run3.\nThe overall comparison across runs can be seen\nin Table 3. We provide a detailed evaluation of\nthe models across various metrics to measure their\nperformance in Counter Speech (CS) generation\ntasks. The metrics used include AVG BLEU-2\n, BERTScore , JudgeLM , and AVG\nROUGE-L . These metrics assess\nthe quality of the generated outputs by measuring\ntheir similarity to ground-truth counterspeech, with\nhigher values indicating better alignment with ref-\nerence texts. Our results show that run3, the DPO-aligned Llama3 base model, outperforms the other\nruns across all metrics, followed by run2 (SFT\nLlama3 instruct model) and run1 (SFT Llama3\nbase model).\nThe findings highlight several key lessons\nlearned. First, the superior performance of run3\nreinforces the efficacy of Direct Preference Op-\ntimization (DPO) for improving text generation\ntasks, including Counter Narrative (CN) genera-\ntion. By fine-tuning models with human-aligned\npreferences, DPO enables outputs that are not only\nfactually accurate but also more assertive and con-\ntextually relevant. Second, the comparison under-\nscores the limitations of standard supervised fine-\ntuning (SFT), which, while effective in generating\ncoherent text, often fails to directly challenge and\ndismantle hate speech in a targeted manner. Lastly,\nthe integration of metrics such as BERTScore and\nJudgeLM provides deeper insights into how models"}, {"title": "6 Error Analysis", "content": "Below is an example output generated by the three\nmodels:\nHate Speech: Jews are less than animals.\nGround Knowledge: Religion holds up the\nideal of the equal dignity of human beings.\nWhy are Americans so ignorant and unedu-\ncated about other cultures and religions? Ev-\nery religion holds up the ideal of the equal\ndignity of human beings. Teach the human\nrace the futility of attempting to define an un-\nfathomable supreme entity. Why are Ameri-\ncans so ignorant and uneducated about other\ncultures and religions? Big question.\nrun1 output: (sft Llama3 base model)\nEvery religion holds up the ideal of the equal\ndignity of human beings. No matter what,\nhuman race must be taught the futility of at-\ntempting to define an unfathomable supreme\nentity.\nrun2 output: (sft Llama3 instruct model)\nEvery religion holds up the ideal of the equal\ndignity of human beings. We should all aim to\nteach the human race the futility of attempting\nto define an unfathomable supreme entity.\nrun3 output: (dpo Llama3 base model)\nEvery religion holds up the ideal of the equal\ndignity of human beings. Are you saying that\nyou are not aware of it and you should teach\nthe human race the futility of attempting to\ndefine an unfathomable supreme entity?\nComparing the three outputs, run1 and run2 em-\nphasize education and collective responsibility, sug-\ngesting a more generalized and neutral approach\nto countering hate speech. Both outputs highlight\nthe importance of universal dignity and teaching\nthe futility of defining an unfathomable supreme\nentity. However, their responses lack specificity\nin addressing the explicit stereotype presented in\nthe hate speech. The tone remains passive and\nnon-confrontational, making them less impactful\nin directly opposing the harmful statement. While\nthese outputs might be effective for audiences that\nare neutral or uninformed, they fail to actively chal-\nlenge the hateful perspective, potentially limiting\ntheir ability to provoke meaningful reflection or\nchange.\nIn contrast, the output from run3, generated by\nthe DPO-aligned model, adopts a more assertive\nand interrogative stance. By directly questioning\nthe ignorance implied in the hate speech, it ac-\ntively confronts the harmful viewpoint and forces\nthe reader to reconsider their stance. This approach,\ngrounded in factual knowledge, provides a stronger\nrebuttal and creates an opportunity for cognitive\ndissonance. It balances politeness with firmness,\nmaking it more effective in counter-narrative sce-\nnarios where directly opposing hate speech is criti-\ncal. This comparison underscores the importance\nof fine-tuning with alignment techniques, such as\nDPO, to produce counter-narratives that are not"}, {"title": "6.1 Future Improvements", "content": "The generation of rejected outputs in this work re-\nlied on a simple prompt instructing the model to\ncreate sentences supporting the Hate Speech (HS)\nwithout repeating its content. While this approach\nserved its purpose, the simplicity of the prompt\nlimited the diversity and contextual richness of the\nrejected outputs. Future improvements could focus\non designing more advanced prompts or leveraging\ntechniques such as reinforcement learning to pro-\nduce more varied and representative outputs. This\nwould enhance the dataset's robustness and sup-\nport a more comprehensive evaluation of Counter-\nNarrative (CN) generation models.\nWe were unable to utilize some of the latest and\nlarger models, such as GPT-4 and certain variants\nof LLaMA, primarily due to their substantial com-\nputational and memory requirements, which ex-\nceeded the available resources. Additionally, sev-\neral state-of-the-art models are not open-sourced,\nlimiting their accessibility for integration into this\nwork. Addressing these constraints in future re-\nsearch could enable the exploration of these power-\nful models for more advanced and scalable Counter-\nNarrative (CN) generation.\nAdditionally, the criteria for rejecting outputs,\nwhile necessary for ensuring quality, were some-\nwhat rigid and manual in nature. This limited the\nexploration of alternative, potentially valid CNs\nthat did not conform strictly to predefined guide-\nlines. Future work could focus on refining rejection\ncriteria to allow for greater nuance and diversity in\nacceptable CNs, perhaps through semi-automated\nmethods involving human-in-the-loop systems or\nadaptive scoring mechanisms. By integrating such\nmethodologies, the evaluation process could bet-\nter capture a broader range of valid CNs, leading\nto richer datasets and more nuanced performance\nassessments of CN generation models."}, {"title": "7 Conclusion", "content": "The integration of Direct Preference Optimiza-\ntion (DPO) has significantly enhanced the genera-\ntive capabilities of Llama3 models, refining align-\nment and improving output quality through iter-\native training on the DPO dataset. This process\nenables the model to produce contextually relevant\nand human-preferred outputs, showcasing its poten-"}, {"title": "A Appendix", "content": "A.1\nTraining Prompt\nIn this paper, two versions of the LLaMA-3 model\nwere employed. Each model utilized a distinct\nprompt format during training.\nPrompt used while training LLaMA-3-base:\nBelow contains a HS (hate speech) and a KN\n(knowledge). Based on the knowledge, write a CN\n(counter narrative) to contradict the hate speech\nwithin 50 words.\n### HS:\nDo you have any proof that the Holocaust ever\nhappened?\n### KN:\nThe ninth is Holocaust Against Jews is a Total\nLie - Proof. Yet for the Holocaust survivors and\nwitnesses, the trial is clearly about more than that.\nThe third result is the article The Holocaust Hoax;\nIT NEVER HAPPENED. At the Eichmann trial\nclose to 100 witnesses testified about their suffering.\nIt has changed both the questions being asked - did\nthe Holocaust actually happen?\n### CN:\nAt the Eichmann trial close to 100 witnesses\ntestified about their suffering. There are living\nsurvivos of the Holocaust. What other evidences\ndo you need?<lend_of_text|>\nPrompt used while training LLaMA-3-\ninstruct:\n<|begin_of_text|><|start_header_id|> system\n<lend_header_id|>\nYou are an AI assistant.\nBelow con-\ntains a HS (hate speech) and a KN (knowl-\nedge). Based on the knowledge, write a CN\n(counter narrative) to contradict the hate speech\nwithin 50 words.<leot_id|><|start_header_id|> user\n<lend_header_id|>\n[HS] Do you have any proof that the Holo-\ncaust ever happened? [KN] The ninth is Holo-\ncaust Against Jews is a Total Lie - Proof. Yet\nfor the Holocaust survivors and witnesses, the\ntrial is clearly about more than that. The third\nresult is the article The Holocaust Hoax; IT\nNEVER HAPPENED. At the Eichmann trial close\nto 100 witnesses testified about their suffering.\nIt has changed both the questions being asked\n- did the Holocaust actually happen?<leot_id]>\n<lstart_header_id|>assistant<lend_header_id|>\n[CN] At the Eichmann trial close to 100 wit-\nnesses testified about their suffering. There are\nliving survivos of the Holocaust. What other evi-\ndences do you need?<leot_idl>\nA.2 Sample Output\nIn this section, we present examples from the\ndataset that illustrate the differences in Counter Nar-\nrative (CN) quality generated by the SFT (Super-\nvised Fine-Tuned) model and the DPO (Direct Pref-\nerence Optimization) model. These examples pro-\nvide a comparative analysis, demonstrating how the\nDPO model outperforms the SFT model in terms of\nfactual accuracy, alignment with the ground truth\nCN, and overall quality.\nFor example in Table 4, the differences between\nSFT and DPO model outputs when addressing\nhate speech. The hate speech in question suggests\nracial inferiority, while the ground truth counter-\nnarrative (CN) states that no privileges or oppor-\ntunities should be based on color or origin. The\nSFT model generates a verbose response that in-\ncorporates irrelevant details from the knowledge\nsource, diluting the clarity and focus of the counter-\nnarrative. In contrast, the DPO model produces\na concise and directly relevant response, effec-\ntively aligning with the ground truth CN by stating,\n\"No jobs should be given to anyone because of\ntheir colour or origin.\" This demonstrates the DPO\nmodel's superior ability to filter extraneous infor-\nmation and generate precise, impactful counter-\nnarratives."}]}