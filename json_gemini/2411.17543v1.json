{"title": "Rapid Deployment of Domain-specific Hyperspectral Image Processors with Application to Autonomous Driving*", "authors": ["Jon Guti\u00e9rrez-Zaballa", "Koldo Basterretxea", "Javier Echanobe", "\u00d3scar Mata-Carballeira", "M. Victoria Mart\u00ednez"], "abstract": "The article discusses the use of low cost System- On-Module (SOM) platforms for the implementation of efficient hyperspectral imaging (HSI) processors for application in au- tonomous driving. The work addresses the challenges of shaping and deploying multiple layer fully convolutional networks (FCN) for low-latency, on-board image semantic segmentation using resource- and power-constrained processing devices. The paper describes in detail the steps followed to redesign and customize a successfully trained HSI segmentation lightweight FCN that was previously tested on a high-end heterogeneous multiprocessing system-on-chip (MPSoC) to accommodate it to the constraints imposed by a low-cost SOM. This SOM features a lower-end but much cheaper MPSoC suitable for the deployment of automatic driving systems (ADS). In particular the article reports the data- and hardware-specific quantization techniques utilized to fit the FCN into a commercial fixed-point programmable AI coprocessor IP, and proposes a full customized post-training quantization scheme to reduce computation and storage costs without compromising segmentation accuracy.", "sections": [{"title": "I. INTRODUCTION", "content": "The advent of small-size, snapshot-type hyperspectral cam- eras has enabled the widespread use of hyperspectral imag- ing (HSI) in new application domains [1]\u2013[3]. HSI provides valuable information about how materials reflect different light wavelengths (spectral reflection), which can be used to detect and segment surfaces and objects in a scene [4]. Advanced driver assistance systems (ADAS) and autonomous driving systems (ADS) are particularly promising targets for this technology. However, the successful deployment of HSI- aided ADS requires the production of high-performance pro- cessing systems that are cost-effective and have low power consumption. In this context, adaptive System on Modules (SOMs) based on multiprocessing system-on-chip (MPSoC) devices have emerged as an interesting technological choice for implementing high-performance, AI-enabled ADS with reduced development time and cost.\nIn this study, we present the process of adapting and recu- stomizing a previously developed AI-enabled HSI segmentation system for a mosaic-filter snapshot hyperspectral camera. We specifically focus on the steps taken to achieve a successful 8- bit quantized model that can be efficiently processed on these devices while meeting the latency requirements imposed by this application. Additionally, we provide performance metrics for the image segmentation system running on an AMD-Xilinx Kria K26 SOM, consuming 7.6W."}, {"title": "II. FCN MODEL DEVELOPMENT", "content": "In the research presented in this article, we built upon our previous findings: a tiny FCN model that required the compo- sition of multiple small-size image patches, which is described in [5], and a deeper, more capable FCN that is presented in [6]. In order to maximize processing performance, we have re- designed the larger FCN reducing the model depth from 5 to 4, allowing the processing of a complete hyperspectral cube in a single pass, thus reducing 4x the number of parameters without significant degradation in accuracy (Table I). By processing the entire image at once, we eliminate the time and memory overhead associated with reconstructing the full image from overlapping patches. Moreover, processing the whole image allows for a larger context to be captured for the extraction of spatial features, although it requires increasing the depth of the FCN. As a general rule, in encoder-decoder architectures, increasing the depth-level by one implies reducing the spatial size by a factor of 4. The increase in model size is not solely due to larger image sizes, but also to the utilization of the recently published extended version of the HSI-Drive dataset, HSI-Drive v2.0. This extended dataset has enabled the training of a more robust and accurate model, albeit at the cost of more trainable parameters. For a comprehensive review of the dataset, we refer the reader to the website https://ipaccess.ehu.eus/HSI-Drive/, where the this dataset is available upon request.\nThe architecture of the lightweight FCN used in this study is a modification of the model depicted in Fig. 6 in [5]. It consists of 32 filters in the first convolutional block, 3x3 convolution kernels and 4 levels of depth. Consequently, the input image size is restricted to be a multiple of 24, which results in 208x400 pixels for this model. This involves framing the image by cropping 8/9 pixels from the edges. In this article, we adopt a previously designed 5-class classification system as the reference model for implementation, which was designed to segment the images into Tarmac, Road Marks, Vegetation, Sky and \"Others\" classes. This segmentation is useful to differentiate the Road (Tarmac and Road Marks) from the background (Vegetation and Sky typically) and from unknown objects and obstacles such as vehicles, cyclist or pedestrians or informative still objects such as road signs, traffic lights and information panels [5]."}, {"title": "III. MODEL QUANTIZATION", "content": "Quantizing the FCN is a necessary step to deploy the model on the fixed-point arithmetic AMD-Xilinx Deep Learning Processor Unit (DPU) [7] AI coprocessor. However, achieving a successful post-training quantization that keeps segmentation accuracy requires carefully analyzing the model parameters and signal ranges, and potentially applying tailored adapta- tions. In fact, applying a range-preserving quantization method directly to this model resulted in poor performance, with a decrease in IoU of over 45%. A thorough analysis of the signal ranges reveals that the normalization method applied to the reflectance values of the cubes in the preprocessing stage (see [6]) leads to data accumulation around the value 0.04, which corresponds to the inverse of the number of spectral channels, as shown in Fig. 1. As depicted, the [0, 0.08] range contains 99.7175% of all pixels.\nAs a result, we first performed an adaptive clipping of the normalized reflectance values based on the data distribution in each spectral channel. The obtained clipping values ranged from 0.0711 to 0.1495, ensuring that 99.95% of the data are accurately represented. Applying this method allows for saving 3 integer bits to increase the resolution of the fixed point num- ber representation in the quantization process. After retraining the model using these savings, we verified that the overall performance of the floating-point clipped range network was practically unaffected, with IoU index variations of +0.086 for Road, -1.146 for Road Marks, +1.014 for Vegetation, +1.574 for Sky and +0.182 for Others. Furthermore, and more importantly, as explained below, the obtained model proved to be robust to Min-Max quantization to 8-bit precision.\nSecondly, since some of the quantization techniques and segmentation recovery methods that aid to produce accu- rate quantized models require the activation functions of the convolutional layers to be piecewise linear, all activation functions in the original FCN model were set to ReLU units. This is particularly important for techniques like cross layer equalization (CLE), which tries to minimize the difference in magnitude of the elements in the same tensor without the need to use per-channel quantization [8]. Similarly, bias absorption is a segmentation recovery method which intends to decrease the differences in the dynamic ranges of activations, especially after applying CLE. In that case, a special requirement is to use the ReLU activation function [9].\nWith all this in mind, we performed a customized quantiza- tion pipeline of the lightweight FCN model (inputs, weights, bias and activations) using AMD-Xilinx Vitis AI 3.0 tool. This"}, {"title": "IV. NETWORK DEPLOYMENT AND TESTING", "content": "ADAS/ADS applications need to adhere to demanding con- straints in terms of energy consumption, cost, and processing latency. Furthermore, the rapid evolution of neural network models requires processing hardware to be adaptable in order to avoid the obsolescence of fixed silicon solutions. The AMD- Xilinx's K26 SOM is an adaptive computing platform that enables the development of high-performance, production- ready Al systems at the edge in shorter development times [12]. This SOM features a XCK26-SK-KV260-G, which is a custom-built Zynq UltraScale+ MPSoC with a 64-bit quad- core ARM Cortex-A53 processor (1.333GHz of maximum theoretical frequency), a dual-core Cortex-R5F real-time pro- cessor (533GHz of maximum theoretical frequency) and an ARM Mali-400MP2 3D graphics processor in the Processing System (PS) connected to a 16nm FinFET Programmable Logic (PL) with access to a 4 GB 64-bit wide, 2400Mb/s DDR4 external memory [12]. The PL can host up to 4 DPU cores to accommodate different neural network architectures to be efficiently processed using high speed data pipes and parallel processing elements with fixed-point arithmetic units [7].\nA. Processor implementation\nThe quantized lightweight FCN processor has been im- plemented embedding only one DPUCZDX8G IP core with B4096 architecture. There is no point in using a parallel DPU configuration since this FCN model processes full images in a single pass with no previous image patching. In this configuration, 4096 operations are performed per DPU clock cycle, which is was 300MHz. More specifically, at each clock cycle 8 pixels of 16 channels of the input feature map are multiplied by the corresponding weights of 16 convolutional filters, thus 8x16x16 = 4096 multiplication operations and sums are performed. Consequently, the theoretical raw com- pute power of this DPU is 1.229 TOPs. The logic resources occupied by this particular implementation are 50,322 LUTS (42,97%), 99,035 flip-flops (42,27%), 75 BRAM (52,08%), 48 dual-port URAMs (75,00%), and 710 DSP48E2 slices (56,89%).\nThe use of 27x18 multipliers enables the DPU to perform two concurrent INT8 multiplications increasing the overall throughput as shown in Fig. 2, although it is required to apply a specific technique to correctly accumulate output products and prevent overflow [13]. On-chip memory, including BRAM and URAM, is used to store input feature-maps, intermediate activations, and output feature maps in order to improve throughput. Data are reused as much as possible to reduce the accesses to external memory which, nevertheless, is necessary as the model (Table II) does not fit in on-chip memory."}, {"title": "V. CONCLUSIONS", "content": "Small-size, snapshot-type hyperspectral cameras are en- abling the use of HSI in novel application domains, including ADS. The development of robust HSI processing systems on adaptive, low-cost hardware platforms with low latency and reduced power consumption is necessary for the gradual penetration of this technology in the field of autonomous driving.\nAs described in this article, achieving this goal involves both, a careful design of lightweight while capable neural network models, and the tailoring of the signal processing flows to the characteristics of domain-specific AI coprocessors. This approach enables the development of an efficient system that effectively leverages hardware resources while ensuring that the segmentation quality does not deteriorate during the quantization process. As described in this paper, the imple- mentation process carried out on an MPSoC-based SOM has shown to be a successful approach to deploy practical HSI segmentation systems as it provides cost and development time savings while allowing for meeting the demanding per- formance requirements of ADAS/ADS. The system has been tested and characterized in terms of memory footprint, latency, and power consumption including the cube preprocessing and the inference in the FCN. The evaluation is performed using images obtained with a snapshot hyperspectral camera in real driving scenarios, encompassing diverse lighting and weather conditions. This ensures that the obtained results are realistic and can be extrapolated to a potential real-world implementation based on the same technology."}], "equations": ["x ~ x = Sx(Xint - zx),", "W\u00e2y = Sw(Wint - zw)Sx(Xint - 2x)\nSwSxWintxint - SwZwSxXint - SwSxZxWint + SwZwSxzx"]}