{"title": "Selective Preference Optimization via Token-Level Reward Function Estimation", "authors": ["Kailai Yang", "Zhiwei Liu", "Qianqian Xie", "Jimin Huang", "Erxue Min", "Sophia Ananiadou"], "abstract": "Recent advancements in large language model alignment leverage token-level supervisions to perform fine-grained preference optimization. However, existing token-level alignment methods either optimize on all available tokens, which can be noisy and inefficient, or perform selective training with complex and expensive key token selection strategies. In this work, we propose Selective Preference Optimization (SePO), a novel selective alignment strategy that centers on efficient key token selection. SePO proposes the first token selection method based on Direct Preference Optimization (DPO), which trains an oracle model to estimate a token-level reward function on the target data. This method applies to any existing alignment datasets with response-level annotations and enables cost-efficient token selection with small-scale oracle models and training data. The estimated reward function is then utilized to score all tokens within the target dataset, where only the key tokens are selected to supervise the target policy model with a reference model-free contrastive objective function. Extensive experiments on three public evaluation benchmarks show that SePO significantly outperforms competitive baseline methods by only optimizing 30% key tokens on the target dataset. SePO applications on weak-to-strong generalization show that weak oracle models effectively supervise strong policy models with up to 16.8\u00d7 more parameters. SePO also effectively selects key tokens from out-of-distribution data to enhance strong policy models and alleviate the over-optimization problem.", "sections": [{"title": "1 Introduction", "content": "The recent development of large language models (LLMs) has focused on aligning model outputs with human intentions and preferences (Ji et al., 2023). During alignment, LLMs are directly optimized on pairwise data and response-level supervision, where popular methods such as reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022; Stiennon et al., 2020) and direct preference alignment (Rafailov et al., 2024c; Yuan et al., 2023; Meng et al., 2024) only introduce supervision signals at the end of each response. As deriving preference optimization as bandit problems can lead to sub-optimal solutions and unstable training processes (Zhong et al., 2024; Zeng et al., 2024), many works propose to model LLM decoding as token-level Markov Decision Processes (MDP) and introduce step-wise supervision signals that quantify the value of each action, successfully applied in tasks such as instruction following (Zhong et al., 2024; Yoon et al., 2024) and mathematical reasoning (Xie et al., 2024; Chen et al., 2024b; Lai et al., 2024).\nThough achieving outstanding performance, most of these methods are optimized on all available tokens from the training dataset. To validate the effectiveness of this setup, in Figure 1, we present the token-level reward accumulations for 1,000 samples from an instruction following dataset (Cui et al., 2023) and a question answering (QA) dataset (Wu et al., 2024), where the token-level rewards are assigned by GPT-4 (Achiam et al., 2023). According to the results, a limited number of tokens with extreme reward values (key tokens) dominate the total rewards. For instruction following, the Top-35.6% tokens occupy the highest 80% rewards for chosen responses, while the lowest 37.4% tokens only occupy 20% rewards for rejected responses. In QA, selecting top-50% key tokens can cover over 80% of effective supervision signals. These observations prove that not all tokens are equally effective in preference alignment, and optimizing on all available tokens can be noisy and inefficient (Lin et al., 2024; Chen et al., 2024c). Some works explored only optimizing on selected response fragments, but their selection strategies are complex and expensive, such as iterative Monte-Carlo tree search (Xie et al., 2024; Chen et al., 2024b) or annotations from human/capable LLMs (Lai et al., 2024; Yoon et al., 2024). The above limitations underscore the importance of selective training and efficient token selection strategies in improving preference optimization algorithms.\nBased on these intuitions, we propose Selective Preference Optimization (SePO), a novel selective alignment strategy that centers on efficient key token selection without requiring strong, fine-grained supervision signals. We theoretically show that Direct Preference Optimization (DPO) (Rafailov et al., 2024c) inherently learns a reward function that decouples the response-level reward values into token level (Rafailov et al., 2024b). Based on this conclusion, we propose the first DPO-based token selection method, which trains an oracle model on a moderate-scale subset of the target data distribution, aiming to parameterize an optimal token-level reward function. This token selection strategy has two key advantages: 1) the oracle modeling process is based on the original response-level preference annotations without requiring any extra supervision signals, making it directly applicable to any existing alignment datasets; (2) the cost for token selection can be easily reduced by controlling the oracle model size and the scale of its training subset, which enables cost-efficient selective alignment. We then utilize the estimated reward function to score all tokens within the large-scale target dataset, where tokens with the highest reward values in the chosen responses and tokens with the lowest reward values in the rejected responses are selected as key tokens for alignment. Finally, we design a reference model-free contrastive objective function to optimize the target policy model on the selected tokens. As SePO enables small oracle models to steer selective alignment for much larger policy models, we further explore it as a new paradigm for weak-to-strong generalization (Burns et al., 2023), where the focus is two-fold: 1) we leverage weak oracle models to select tokens from in-distribution data for training strong policy models; 2) we train oracle models on out-of-distribution data, which select key tokens to improve target policy model performance and avoid over-optimization (Gao et al., 2023; Rafailov et al., 2024a) on the weak supervision data.\nIn summary, our main contributions are:\n\u2022 We propose Selective Preference Optimization, a simple yet effective selective training strategy for preference alignment. It achieves alignment efficiently by obtaining key tokens with an estimated token-level reward function and only optimizing on these selected key tokens;\n\u2022 We explore the application of SePO on weak-to-strong generalization. The results show that weak oracle models effectively supervise strong policy models with up to 16.8\u00d7 more parameters. SePO also selects useful tokens from weak supervision data to enhance strong policy model performance, alleviating the performance degradation problem commonly encountered in full optimization on out-of-distribution data;\n\u2022 We examine SePO on three public evaluation benchmarks. Experimental results show that SePO significantly improves performances on five policy models and outperforms competitive baseline methods by only optimizing 30% key tokens on the target dataset."}, {"title": "2 Preliminary", "content": "Preference alignment directly optimizes LLMs with human preferences by maximizing the reward values of model outputs, which are obtained via a response-level reward function r(q, y). The reward function is defined under the Bradley-Terry (Bradley and Terry, 1952) model of preferences. Specifically, for the same prompt q and two completed responses (y1, y2) under data distribution D, the model assumes:\n$P_D(y_1 \\succ y_2|q) = \\sigma(r(q, y_1) \u2013 r(q, y_2))$\t\t\t(1)\nwhere \u03c3 denotes the logistic function and $P_D(y_1 \\succ y_2)$ denotes the probability that y1 is preferred against y2. The alignment of language models is typically cast as a KL-constrained optimization problem on the reward values, formalized as\n$argmax_\\pi E_{q~D,y\u223c\u03c0(y|q)} [r(q, y)]$\ns.t. $E_{q~D}D_{KL} [\u03c0(Y|q)||\u03c0_{ref}(y|q)] \u2264 \u03c3$ (2)\nwhere \u03c0 denotes the aligned policy model, $\u03c0_{ref}$ denotes the reference policy model. Taking the following Lagrangian, the problem is transformed as:\n$argmax_\\pi E_{q~D,y~\u03c0(y|q)} [r(q, y)]\n\u03b2D_{KL} [\u03c0_\u03c6(y|q)||\u03c0_{ref} (y|q)]$ (3)\nZiebart et al. (2008) have shown that Eqn 3 has a closed-form optimal solution, which enables the reward function to be re-parameterized by the optimal policy:\n$r(q,y) = \\beta log \\frac{\u03c0*(y|q)}{\u03c0_{ref}(y|q)} + \\beta log Z(q)$ (4)\nwhere \u03c0* denotes the optimal policy, and Z(q) is the partition function. DPO (Rafailov et al., 2024c) bypasses the reward modeling stage by directly substituting this closed-form solution into Eqn. 1, which cancels out Z(q) as it un-changes with the same q, yielding the following DPO objective:\n$L_{DPO} = \u2212E_{(q,y_w,y_l)~D} log \u03c3 (\u03b2u(x, y_w, y_l))$\n$u(x, y_w, y_l) = log \\frac{\u03c0_\u03b8(y_w|x)}{\u03c0_\u03b8(y_l|x)} - log \\frac{\u03c0_{ref}(y_w|x)}{\u03c0_{ref}(y_l|x)}$ (5)\nwhere yw and yl denote the preferred and dis-preferred responses to the prompt q."}, {"title": "2.1 Preference Alignment of Language Models", "content": "Preference alignment directly optimizes LLMs with human preferences by maximizing the reward values of model outputs, which are obtained via a response-level reward function r(q, y). The reward function is defined under the Bradley-Terry (Bradley and Terry, 1952) model of preferences. Specifically, for the same prompt q and two completed responses (y1, y2) under data distribution D, the model assumes:\n$P_D(y_1 \\succ y_2|q) = \\sigma(r(q, y_1) \u2013 r(q, y_2))$\t\t\t(1)\nwhere \u03c3 denotes the logistic function and $P_D(y_1 \\succ y_2)$ denotes the probability that y1 is preferred against y2. The alignment of language models is typically cast as a KL-constrained optimization problem on the reward values, formalized as\n$argmax_\\pi E_{q~D,y\u223c\u03c0(y|q)} [r(q, y)]$\ns.t. $E_{q~D}D_{KL} [\u03c0(Y|q)||\u03c0_{ref}(y|q)] \u2264 \u03c3$ (2)\nwhere \u03c0 denotes the aligned policy model, $\u03c0_{ref}$ denotes the reference policy model. Taking the following Lagrangian, the problem is transformed as:\n$argmax_\\pi E_{q~D,y~\u03c0(y|q)} [r(q, y)]\n\u03b2D_{KL} [\u03c0_\u03c6(y|q)||\u03c0_{ref} (y|q)]$ (3)"}, {"title": "2.2 Direct Preference Optimization", "content": "Ziebart et al. (2008) have shown that Eqn 3 has a closed-form optimal solution, which enables the reward function to be re-parameterized by the optimal policy:\n$r(q,y) = \\beta log \\frac{\u03c0*(y|q)}{\u03c0_{ref}(y|q)} + \\beta log Z(q)$ (4)\nwhere \u03c0* denotes the optimal policy, and Z(q) is the partition function. DPO (Rafailov et al., 2024c) bypasses the reward modeling stage by directly substituting this closed-form solution into Eqn. 1, which cancels out Z(q) as it un-changes with the same q, yielding the following DPO objective:\n$L_{DPO} = \u2212E_{(q,y_w,y_l)~D} log \u03c3 (\u03b2u(x, y_w, y_l))$\n$u(x, y_w, y_l) = log \\frac{\u03c0_\u03b8(y_w|x)}{\u03c0_\u03b8(y_l|x)} - log \\frac{\u03c0_{ref}(y_w|x)}{\u03c0_{ref}(y_l|x)}$ (5)\nwhere yw and yl denote the preferred and dis-preferred responses to the prompt q."}, {"title": "3 Methodology", "content": "In this section, we first show DPO training as inherently fitting a token-level reward function that decouples the response-level reward function (Sec. 3.1). Based on this conclusion, we propose Selective Preference Optimization (SePO), which optimizes the target policy model only with selected key tokens by the estimated optimal token-level reward function (Sec. 3.2). We also propose the application of SePO in weak-to-strong generalization (Sec. 3.3). The training pipeline of SePO is shown in Figure 2."}, {"title": "3.1 DPO as Token-level Reward Function Estimator", "content": "Due to the auto-regressive nature, the decoding process of LLMs can be naturally formulated as a token-level MDP. The MDP is defined as a tuple M = (S, A, f, r(st, at), p), where S and A denote the state space and action space. st \u2208 S deontes the current state, consisting of all prompt tokens and current generated tokens (i.e. st = {q|yo, ..., yt}, | denotes concatenation). st denotes the terminal state. at \u2208 A denotes the current action, where A is the token vocabulary. f is the deterministic state transition function that concatenates the current state st and action at. p is an initial state distribution over prompts q, where the initial state s1 consists of the tokens from q. r(st, at) denotes the token-level reward function.\nIn this section, we show that DPO inherently learns the best estimate on a token-level distribution of the response-level reward values along the token-level MDP trajectory. We begin with the following widely used mild assumption (Zhong et al., 2024; Zeng et al., 2024):\nAssumption 1. Any response-level reward function r can be decoupled into the token level as a linear combination of reward values modeled by another token-level reward function r\u02c6 along the trajectory for its MDP.\n$r(q, \u03c4) = \\sum_{t=1}^T r\u02c6(st, at)$\t\t\t\t(6)\nwhere st, at are states and actions within the token-level MDP trajectory \u03c4 = {s1, a1, s2, ..., sT}.\nWith the above assumption, the Bradley-Terry model can be replaced into token level:\n$P_D(\u03c4^w \\succ \u03c4^l|q) = \u03c3(\\sum_{i=1}^N r\u02c6(s_i, a_i) \u2013  \\sum_{j=1}^M r\u02c6(s_j, a_j))$ (7)\nwhere \u03c4w and \u03c4l are the chosen and rejected trajectories (responses), which are assumed to start and end at the same state s1 and sT.\nTheorem 1. With a reference model $\u03c0_{ref}$, fitting any reward functions r that are consistent to the Bradley-Terry model with the DPO algorithm leads to an optimal estimation of another reward function that decouples the response-level reward values into the token level, which satisfies:\n$r\u02c6(s_t, a_t) \u03b1 log \\frac{\u03c0*(a_t|s_t)}{\u03c0_{ref}(a_t|s_t)}$ (8)\nwhere \u03c0* denotes the oracle policy obtained by DPO.\nThis reward function marks the contribution of each action given the current state at the token level. In practice, the quality of the training data for DPO determines how well the calculated reward quantifies the token-level contribution.\nProof. This proof is inspired by Rafailov et al. (2024b). Common policy gradient-based RL practices (Schulman et al., 2017) optimize Eqn. 3 in token-level MDP with an entropy-bonus H(\u03c0) and a KL-constraint with $\u03c0_{ref}$:\n$\\begin{aligned} \\pi =& \\underset{\\pi}{\\text{max}} E_{\\pi} \\sum_{t=1}^{T}\\left[\\gamma \\left(r\\left(s_{t}, a_{t}\\right)+\\frac{1}{\\beta} \\log \\pi_{\\text {ref }}\\left(a_{t} \\mid s_{t}\\right)\\right)+\\beta H(\\pi)\\right] \\text { where } s_{1} \\sim p \\end{aligned}$ (9)\nwhere s1 ~ p. Its optimal solution is given by Ziebart et al. (2008) under the maximum entropy RL setting:\n$\u03c0*(at|st) = exp((Q*(st, at) \u2013 V*(st))/\u03b2)$ (10)\nwhere Q*(st, at) is the optimal Q-function that estimates the partial returns of at under st, and V*(st) estimates the expected future returns under current state st. Under a KL-divergence regularization with the reference model, the relationship between Q-function and token-level reward values can be established as follows with the Bell-man equation:\n$Q* (st, at) = r(st, at) + \u03b2log \u03c0_{ref}(at|st) + V*(st+1)$ (11)\nwhere V*(sT) = 0. Combining Eqn. 10 and 11, we have:\n$r(st, at) = \u03b2log \\frac{\u03c0*(a_t|s_t)}{\u03c0_{ref}(a_t|s_t)} +V*(st) - V*(st+1)$ (12)\nUnder Assumption 1, we substitute Eqn. 12 into Eqn. 6, the response-level reward is factorized as follows:\n$r(q, \u03c4) = \\sum_{t=1}^T r(st, at)$\n$ = \\sum_{t=1}^T \u03b2log \\frac{\u03c0*(a_t|s_t)}{\u03c0_{ref}(a_t|s_t)} + V*(s_1)$ (13)\nNote that in DPO, V*(s1) remains unchanged for each response pair as they have the same start state s1. This means the preference modeling process for each response pair only depends on the first term of Eqn. 13. Therefore, we conclude that the optimal policy \u03c0* learned by DPO inherently fits the response-level reward value with another token-level reward function r\u02c6(st, at), which is parameterized as\n$r\u02c6(st, at) = \u03b2log \\frac{\u03c0(a_t|s_t)}{\u03c0_{ref}(a_t|s_t)}$ (14)\nThis indicates our results in Eqn. 8 and completes the proof."}, {"title": "3.2 Selective Preference Optimization", "content": "SePO is guided by the simple idea that \"not all tokens are equally effective\", which has been widely evaluated (Lin et al., 2024; Chen et al., 2024b; Lai et al., 2024). We explore fully utilizing DPO to efficiently select the most useful tokens in modeling human preferences in LLM alignment. Firstly, we train an oracle model on a moderate-scale preference dataset with DPO, aiming to model a token-level reward function for the target data distribution. The reference model is then applied to large-scale data to score all the tokens. The policy model is only trained on selected tokens with highest scores in chosen responses and lowest scores in rejected responses, which are expected as key tokens in achieving alignment.\nOracle Modeling with DPO. We explore fitting an oracle model with DPO utilizing a moderate-scale dataset via random sampling over the target preference dataset D, which we expect to parameterize a pessimistic estimation of the target reward function.\nTheorem 2. Let D be the target preference dataset with N samples, and S be a random selection of m samples from D (m\u2264 N), which is drawn independently and uniformly. Then we have:\nThe reward function $r_S$ modeled by fitting $S$ with DPO is a pessimistic estimation of the target reward function $r_D$. The result can be formalized as:\n$E_S(r_S(q,y)) \u2264 r_D(q,y)$ (15)\nwhere q, y denote any query-response pairs drown from D. The equality holds when m = N.\nProof. As the reward functions are parameterized via fitting the DPO algorithm on the datasets, we substitute Eqn. 4 into Eqn. 15. As the term \u03b2 log $\u03c0_{ref}$(y|q) and \u03b2 log Z(q) are unrelated to S, they are easily canceled and we transfer the proof target into comparing the optimal policy functions:\n$E_S [log \u03c0_S^*(y|q)] \u2264 log \u03c0_D^*(y|q)$ (16)\nLet $D = {X_1,X_2, ..., X_N }$, where xi represents an (q, y, r) data point, and $S = {X_{i1}, X_{i1}, ..., X_{im} }$, where $X_{ij}$ is selected from D. To show that S is an unbiased estimator of the target data distribution, we calculate its empirical distribution over all possible random samples drawn from D. The empirical distribution $P_S(X)$ based on the sampled dataset is as follows:\n$P_S(X) = \\frac{1}{m}  \\sum_{j=1}^{m} \u03b4(X = X_{ij})$\t\t\t\t(17)\nwhere \u03b4 indicates the presence of a sample X. Taking its expectation over all possible sampled datasets, we have:\n$E_S [P_S(X)] = E_S [ \\frac{1}{m}  \\sum_{j=1}^{m} \u03b4(X = X_{ij}) ]$ (18)\n$ = \\frac{1}{m}  \\sum_{j=1}^{m} E_S [\u03b4(X = X_{ij}) ]$\nAs each $X_{ij}$ is equally likely to be any xi \u2208 D, we have\n$E_S [\u03b4(X = X_{ij})] = \\frac{1}{N}  \\sum_{i=1}^{N} \u03b4(X = X_{i}) = P_D(X)$ (19)\nSubstituting Eqn. 19 into Eqn. 18, we have\n$E_S [P_S(X)] = P_D(X)$ (20)\nBased on the same reference model and empirical data distribution (Eqn. 20), we expect training on S with DPO to obtain an unbiased estimation of the target optimal policy function:\n$E_S [\u03c0_S^*(y|q)] = \u03c0_D^*(y|q)$ (21)\nBecause logarithm is a strictly concave function, according to Jensen's inequality, we have:\n$E_S [log \u03c0_S^*(y|q)] \u2264 log E_S [\u03c0_D^*(y|q)]$ (22)\nSubstituting Eqn. 21 into Eqn. 22, we prove Eqn. 16, which completes the proof. Note that when m = N, we have S = D, and the training process gives an unbiased estimation of target reward function $r_D$.\nWith the results from Theorem 2, we first perform SFT on the chosen responses of the moderate-scale dataset S to obtain the base model $\u03c0_{ref}$:\n$L_{SFT} = -E_{(q,y_w)~S}  \\sum_{y_i} log \u03c0_{ref}(y_i|q, y_{<i})$ (23)\nwith the base model, we further perform DPO on the small dataset S with the objective function Eqn. 5 to obtain the oracle model $\u03c0_{ora}$. With Theorem 1, we can utilize $\u03c0_{ref}$ and the oracle model $\u03c0_{ora}$ to parameterize the optimal token-level reward function for human preferences, which are used for token selections.\nToken Selection. We score all tokens within the target preference dataset with the estimated token-level reward function. Based on the token-level MDP and Theorem 1, for each prompt-response pairs (q, y), the score for token yi is calculated as follows:\n$s(y_i) = log \\frac{\u03c0_{ora}(y_i|q, y_{<i})}{\u03c0_{ref}(y_i|q, y_{<i})}$ (24)\nWe define a token selection ratio k, which determines the selected proportion for each response. For chosen responses, we utilize the following indicator function for selection:\n$\\begin{aligned} &I(s(y_i))=\\left\\{\\begin{array}{ll} &1, \\text { if } s(y_i) \\text { ranks in highest } k % \\text { in } y \\\\ &0, \\text { otherwise } \\end{array}\\right. \\end{aligned}$ (25)\nFor rejected responses, we change the condition for indicating 1 to \"if s(yi) ranks in lowest k% in y\" and mark the indicator function as $I\u02c6(s(y_i))$. The intuition behind this action is that key tokens for chosen responses are likely to contribute high token-level rewards, while key tokens for rejected responses are likely with low token-level rewards, whose probabilities are significantly suppressed in $\u03c0_{ora}$ compared to the base model.\nSePO Objective. We design a simple contrastive preference optimization objective on the target policy model $\u03c0_t$ with the selected tokens. Specifically, the objective function is designed as follows:\n$L_{SePO} = -E_{(q,y_w,y_l)~D} log \u03c3 (u\u02c6_w(q, y_w) \u2013 u\u02c6_l(q, y_l))$\n$u\u02c6_w (q, y) =  \\frac{\u03b3}{y_k%}\\sum_{i=1}^{y} I(s(y_i)) log \u03c0_t(y_i|q, y_{<i})$\n$u\u02c6_l (q, y) =  \\frac{\u03b3}{y_k%}\\sum_{i=1}^{y} I\u02c6(s(y_i)) log \u03c0_t(y_i|q, y_{<i})$ (26)\nwhere \u03b3 controls the scaling of the rewards. The SePO objective enables direct adjustment of only crucial tokens for alignment, which avoids noise in both chosen and rejected responses. This design is expected to improve the efficiency and stability of the preference optimization process. The objective is also length-normalized and reference model-free, which prevents bias towards over-length responses and enables memory-efficient alignment (Meng et al., 2024; Yuan et al., 2023)."}, {"title": "3.3 Application: Weak-to-Strong Generalization", "content": "A unique advantage of SePO is that the cost of its token selection process can be easily reduced by controlling the oracle model size, which enables small oracle models to steer the alignment for policy models with much stronger capabilities. Therefore, we explore the application of SePO in weak-to-strong generalization (Burns et al., 2023), which aims to elicit the capabilities of strong student models with weak supervision signals. As illustrated in Figure 3, our focus is two-fold:\nWeak Oracle Modeling. We propose to leverage weak oracle models that underperform the policy models to select the key tokens from strong in-distribution data. Our intuition is that weak supervisors (oracle models) only identify which tokens are most effective in enhancing the alignment performance, rather than directly providing supervision signals, which normally requires stronger capabilities than the student model.\nWeak Data Supervision. During alignment, as the policy model becomes stronger, continual full optimization on the original data distribution can lead to over-optimization on the reward function (Gao et al., 2023; Rafailov et al., 2024a), which can seriously degrade policy model performance. Online preference optimization (Xiong et al., 2024; Xie et al., 2024) alleviates over-optimization with online annotations of new in-distribution data, but can be costly for strong policy models.\nWeak data supervision focuses on scenarios when only weak out-of-distribution data is available for strong policy models. We propose to leverage the SePO framework to select key tokens from the weak dataset, and only the selected tokens are utilized to supervise the strong policy model. Instead of full optimization on the training data, we expect selective optimization on the key tokens to prevent over-optimization on the out-of-distribution data, while still leveraging effective supervision signals to further improve the policy model."}, {"title": "4 Experiments", "content": "We comprehensively evaluate SePO on two representative model families: LLaMA (Touvron et al., 2023) and Pythia (Biderman et al., 2023). To approximate the optimal token-level reward function, we first obtain the base model by training on the UltraChat-200K dataset (Ding et al., 2023) in an SFT manner. For the LLaMA model family, we train the base model on the pre-trained TinyLLaMA-1.1B (Zhang et al., 2024) model. For Pythia, we separately train on Pythia-(70M, 160M, 410M, 1B, 1.4B) to facilitate research on the effect of oracle models with different sizes. For each base model, we obtain the oracle models by further fine-tuning with DPO on the Ultra-Feedback dataset (Cui et al., 2023) with 61,135 preference pairs. We examine the selective preference training process on target policy models with various capabilities. For Pythia, we perform SFT on Pythia-(2.8B, 6.9B) with UltraChat-200K and use them as policy models, which we refer as Pythia-SFT. For LLaMA, we test on TinyLLaMA-Chat-1.1B, LLaMA2-Chat-7B, and LLaMA2-Chat-13B.\nWe compare the performance of SePO with state-of-the-art offline preference optimization methods to indicate its effectiveness. DPO (Rafailov et al., 2024c) explicitly models the relation between the reward functions and optimal policy, which enables reward model-free preference alignment. IPO (Azar et al., 2024) provides a variant of DPO that prevents the requirement that pairwise preferences can substitute reward values. We introduce another two competitive reference model-free alignment methods: RRHF (Yuan et al., 2024) directly optimize the probability of the target response pairs with a pairwise ranking loss, with an SFT-based regularization on the chosen responses. SimPO (Meng et al., 2024) optimizes the length-regularized probability of the response pairs with a margin.\nWe quantify the contribution of each method by evaluating on three widely utilized instruction-following benchmarks: AlpacaEval 2.0 (Dubois et al., 2024), MT-Bench (Zheng et al., 2024b), and Arena-Hard (Li et al., 2024). AlpacaEval 2.0 consists of 805 queries to evaluate the models' versatile conversational abilities. We report the win rates and length-controlled win rates (LC win rate) of evaluated models against the responses of GPT-4-turbo. The LC win rates are designed to reduce influences of model verbosity. MT-Bench covers eight categories with 80 queries. We report the average scores ranging from 0 to 10. Due to the widely reported poor separability of MT-Bench reported by previous works (Meng et al., 2024; Li et al., 2024), we further display fine-grained scores of model capability, which we reorganize the original 8 categories as follows: Assistant (Writing, Roleplay, Extraction), QA (Reasoning, STEM, Humanities), Math, and Coding. Scores for Assistant and QA are averages of the original categories. Arena-Hard extends MT-Bench with 500 high-quality queries, where we report win rates against GPT-4-0314 model outputs. All judgments are performed by the latest GPT-40 model."}, {"title": "4.1 Experimental Settings", "content": "We comprehensively evaluate SePO on two representative model families: LLaMA (Touvron et al., 2023) and Pythia (Biderman et al., 2023). To approximate the optimal token-level reward function, we first obtain the base model by training on the UltraChat-200K dataset (Ding et al., 2023) in an SFT manner. For the LLaMA model family, we train the base model on the pre-trained TinyLLaMA-1.1B (Zhang et al., 2024) model. For Pythia, we separately train on Pythia-(70M, 160M, 410M, 1B, 1.4B) to facilitate research on the effect of oracle models with different sizes. For each base model, we obtain the oracle models by further fine-tuning with DPO on the Ultra-Feedback dataset (Cui et al., 2023) with 61,135 preference pairs. We examine the selective preference training process on target policy models with various capabilities. For Pythia, we perform SFT on Pythia-(2.8B, 6.9B) with UltraChat-200K and use them as policy models, which we refer as Pythia-SFT. For LLaMA, we test on TinyLLaMA-Chat-1.1B, LLaMA2-Chat-7B, and LLaMA2-Chat-13B."}, {"title": "4.2 Overall Performance", "content": "Performances of SePO and other baseline methods on three benchmark datasets are presented in Table 1. According to the results, SePO significantly improves performance over the base policy models. For example, on Arena-Hard, SePO achieves a 6.71% improvement in win rates on Pythia-6.9B. SePO also outperforms other strong baseline preference optimization methods. On Arena-Hard, SePO achieves the best win rates on all policy models, surpassing state-of-the-art methods such as SimPO and DPO. As SePO models are only optimized on 30% of the tokens trained on other methods, these results directly strengthen the effectiveness of selective training strategies applied in preference optimization.\nOn AlpacaEval 2.0, SePO continues to achieve superior performances over baseline methods on both win rates and LC win rates, further proving its effectiveness on different benchmarks. Notably, SePO outperforms all other methods on length-controlled win rates, including SimPO and RRHF, which are specifically designed for length-normalized reward formulation. These results show that selective training strategies also enhance policy models in avoiding over-length responses. We believe the reason is that during token selection, the token-level reward function can assign the end-of-sentence tokens with low-ranking scores, which can be discarded during optimization if the response ends inappropriately (e.g. over-length or repetition). In contrast, though SimPO and RRHF design length-normalized rewards, the end-of-sentence tokens are included for all responses, still fitting the response lengths for all training samples.\nOn MT-Bench, SePO outperforms all other methods on average scores. Due to the widely discussed poor separability of overall scores for"}]}