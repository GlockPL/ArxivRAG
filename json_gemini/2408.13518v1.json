{"title": "Selective Preference Optimization via Token-Level Reward Function Estimation", "authors": ["Kailai Yang", "Zhiwei Liu", "Qianqian Xie", "Jimin Huang", "Erxue Min", "Sophia Ananiadou"], "abstract": "Recent advancements in large language model alignment leverage token-level supervisions to perform fine-grained preference optimization. However, existing token-level alignment methods either optimize on all available tokens, which can be noisy and inefficient, or perform selective training with complex and expensive key token selection strategies. In this work, we propose Selective Preference Optimization (SePO), a novel selective alignment strategy that centers on efficient key token selection. SePO proposes the first token selection method based on Direct Preference Optimization (DPO), which trains an oracle model to estimate a token-level reward function on the target data. This method applies to any existing alignment datasets with response-level annotations and enables cost-efficient token selection with small-scale oracle models and training data. The estimated reward function is then utilized to score all tokens within the target dataset, where only the key tokens are selected to supervise the target policy model with a reference model-free contrastive objective function. Extensive experiments on three public evaluation benchmarks show that SePO significantly outperforms competitive baseline methods by only optimizing 30% key tokens on the target dataset. SePO applications on weak-to-strong generalization show that weak oracle models effectively supervise strong policy models with up to 16.8\u00d7 more parameters. SePO also effectively selects key tokens from out-of-distribution data to enhance strong policy models and alleviate the over-optimization problem.", "sections": [{"title": "1 Introduction", "content": "The recent development of large language models (LLMs) has focused on aligning model outputs with human intentions and preferences (Ji et al., 2023). During alignment, LLMs are directly optimized on pairwise data and response-level supervision, where popular methods such as reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022; Stiennon et al., 2020) and direct preference alignment (Rafailov et al., 2024c; Yuan et al., 2023; Meng et al., 2024) only introduce supervision signals at the end of each response. As deriving preference optimization as bandit problems can lead to sub-optimal solutions and unstable training processes (Zhong et al., 2024; Zeng et al., 2024), many works propose to model LLM decoding as token-level Markov Decision Processes (MDP) and introduce step-wise supervision signals that quantify the value of each action, successfully applied in tasks such as instruction following (Zhong et al., 2024; Yoon et al., 2024) and mathematical reasoning (Xie et al., 2024; Chen et al., 2024b; Lai et al., 2024). Though achieving outstanding performance, most of these methods are optimized on all available tokens from the training dataset. To validate the effectiveness of this setup, in Figure 1, we present the token-level reward accumulations for 1,000 samples from an instruction following dataset (Cui et al., 2023) and a question answering (QA) dataset (Wu et al., 2024), where the token-level rewards are assigned by GPT-4 (Achiam et al., 2023). According to the results, a limited number of tokens with extreme reward values (key tokens) dominate the total rewards. For instruction following, the Top-35.6% tokens occupy the highest 80% rewards for chosen responses, while the lowest 37.4% tokens only occupy 20% rewards for rejected responses. In QA, selecting top-50% key tokens can cover over 80% of effective supervision signals. These observations prove that not all tokens are equally effective in preference alignment, and optimizing on all available tokens can be noisy and inefficient (Lin et al., 2024; Chen et al., 2024c). Some works explored only optimizing on selected response fragments, but their selection strategies are complex and expensive, such as iterative Monte-Carlo tree search (Xie et al., 2024; Chen et al., 2024b) or annotations from human/capable LLMs (Lai et al., 2024; Yoon et al., 2024). The above limitations underscore the importance of selective training and efficient token selection strategies in improving preference optimization algorithms. Based on these intuitions, we propose Selective Preference Optimization (SePO), a novel selective alignment strategy that centers on efficient key token selection without requiring strong, fine-grained supervision signals. We theoretically show that Direct Preference Optimization (DPO) (Rafailov et al., 2024c) inherently learns a reward function that decouples the response-level reward values into token level (Rafailov et al., 2024b). Based on this conclusion, we propose the first DPO-based token selection method, which trains an oracle model on a moderate-scale subset of the target data distribution, aiming to parameterize an optimal token-level reward function. This token selection strategy has two key advantages: 1) the oracle modeling process is based on the original response-level preference annotations without requiring any extra supervision signals, making it directly applicable to any existing alignment datasets; (2) the cost for token selection can be easily reduced by controlling the oracle model size and the scale of its training subset, which enables cost-efficient selective alignment. We then utilize the estimated reward function to score all tokens within the large-scale target dataset, where tokens with the highest reward values in the chosen responses and tokens with the lowest reward values in the rejected responses are selected as key tokens for alignment. Finally, we design a reference model-free contrastive objective function to optimize the target policy model on the selected tokens. As SePO enables small oracle models to steer selective alignment for much larger policy models, we further explore it as a new paradigm for weak-to-strong generalization (Burns et al., 2023), where the focus is two-fold: 1) we leverage weak oracle models to select tokens from in-distribution data for training strong policy models; 2) we train oracle models on out-of-distribution data, which select key tokens to improve target policy model performance and avoid over-optimization (Gao et al., 2023; Rafailov et al., 2024a) on the weak supervision data. In summary, our main contributions are:\n\u2022 We propose Selective Preference Optimization, a simple yet effective selective training strategy for preference alignment. It achieves alignment efficiently by obtaining key tokens with an estimated token-level reward function and only optimizing on these selected key tokens;\n\u2022 We explore the application of SePO on weak-to-strong generalization. The results show that weak oracle models effectively supervise strong policy models with up to 16.8\u00d7 more parameters. SePO also selects useful tokens from weak supervision data to enhance strong policy model performance, alleviating the performance degradation problem commonly encountered in full optimization on out-of-distribution data;\n\u2022 We examine SePO on three public evaluation benchmarks. Experimental results show that SePO significantly improves performances on five policy models and outperforms competitive baseline methods by only optimizing 30% key tokens on the target dataset."}, {"title": "2 Preliminary", "content": "2.1 Preference Alignment of Language Models\nPreference alignment directly optimizes LLMs with human preferences by maximizing the reward values of model outputs, which are obtained via a response-level reward function r(q, y). The reward function is defined under the Bradley-Terry (Bradley and Terry, 1952) model of preferences. Specifically, for the same prompt q and two completed responses (y1, y2) under data distribution D, the model assumes:\n$P_D(Y_1 > Y_2|q) = \\sigma(r(q, y_1) \u2013 r(q, y_2))$\nwhere o denotes the logistic function and $P_D(y_1 > y_2)$ denotes the probability that y1 is preferred against y2. The alignment of language models is typically cast as a KL-constrained optimization problem on the reward values, formalized as\n$\\underset{\\pi}{argmax} E_{q\\sim D,y\\sim\\pi(y|q)} [r(q, y)]$\ns.t. $E_{q\\sim D}D_{KL} [\\pi(Y|q)||\\pi_{ref}(y|q)] \\le \\sigma$\nwhere \u03c0 denotes the aligned policy model, Tref denotes the reference policy model. Taking the following Lagrangian, the problem is transformed as:\n$\\underset{\\pi}{argmax} E_{q\\sim D,y\\sim\\pi(y|q)} [r(q, y)]$\n$BD_{KL} [\\pi_{\\varphi}(y|q)||\\pi_{ref} (y|q)]$\n2.2 Direct Preference Optimization\nZiebart et al. (2008) have shown that Eqn 3 has a closed-form optimal solution, which enables the reward function to be re-parameterized by the optimal policy:\n$r(q,y) = \\beta log \\frac{\\pi^*(y|q)}{\\pi_{ref}(y|q)} + \\beta log Z(q)$\nwhere \u03c0* denotes the optimal policy, and Z(q) is the partition function. DPO (Rafailov et al., 2024c) bypasses the reward modeling stage by directly substituting this closed-form solution into Eqn. 1, which cancels out Z(q) as it un-changes with the same q, yielding the following DPO objective:\n$L_{DPO} = \u2212E_{(q,y_w,y_l)\u223cD} log \u03c3 (\u03b2u(x, y_w, y_l))$\n$u(x, y_w, y_l) = log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{\\theta}(y_l|x)} - log \\frac{\\pi_{ref}(y_w|x)}{\\pi_{ref}(y_l|x)}$\nwhere yw and yi denote the preferred and dis-preferred responses to the prompt q."}, {"title": "3 Methodology", "content": "In this section, we first show DPO training as inherently fitting a token-level reward function that decouples the response-level reward function (Sec. 3.1). Based on this conclusion, we propose Selective Preference Optimization (SePO), which optimizes the target policy model only with selected key tokens by the estimated optimal token-level reward function (Sec. 3.2). We also propose the application of SePO in weak-to-strong generalization (Sec. 3.3). The training pipeline of SePO is shown in Figure 2.\n3.1 DPO as Token-level Reward Function Estimator\nDue to the auto-regressive nature, the decoding process of LLMs can be naturally formulated as a token-level MDP. The MDP is defined as a tuple M = (S, A, f, r(st, at), p), where S and A denote the state space and action space. st \u2208 S deontes the current state, consisting of all prompt tokens and current generated tokens (i.e. $St = {q|yo, ..., yt}$, | denotes concatenation). st denotes the terminal state. at \u2208 A denotes the current action, where A is the token vocabulary. f is the deterministic state transition function that concatenates the current state st and action at. p is an initial state distribution over prompts q, where the initial state s1 consists of the tokens from q. r(st, at) denotes the token-level reward function. In this section, we show that DPO inherently learns the best estimate on a token-level distribution of the response-level reward values along the token-level MDP trajectory. We begin with the following widely used mild assumption (Zhong et al., 2024; Zeng et al., 2024):\nAssumption 1. Any response-level reward function r can be decoupled into the token level as a linear combination of reward values modeled by another token-level reward function \u00ee along the trajectory for its MDP.\n$r(q, \\tau) = \\sum_{t=1}^{T} \\hat{r}(s_t, a_t)$\nwhere $s_t$, $a_t$ are states and actions within the token-level MDP trajectory\n$T = {s_1, a_1, s_2, ..., s_T}$.\nWith the above assumption, the Bradley-Terry model can be replaced into token level:\n$P_D(\\tau^w > \\tau^l | q) = \\sigma (\\sum_{i=1}^{N} \\hat{r}(s_i, a_i) - \\sum_{j=1}^{M} \\hat{r}(s_j, a_j))$\nwhere \u03c4w and \u03c4l are the chosen and rejected trajectories (responses), which are assumed to start and end at the same state s1 and sT.\nTheorem 1. With a reference model \u03c0ref, fitting any reward functions r that are consistent to the Bradley-Terry model with the DPO algorithm leads to an optimal estimation of another reward function that decouples the response-level reward values into the token level, which satisfies:\n$\\hat{r}(s_t, a_t) \\propto log \\frac{\\pi^*(a_t | s_t)}{\\pi_{ref}(a_t|s_t)}$\nwhere \u03c0* denotes the oracle policy obtained by DPO.\nThis reward function marks the contribution of each action given the current state at the token level. In practice, the quality of the training data for DPO determines how well the calculated reward quantifies the token-level contribution.\nProof. This proof is inspired by Rafailov et al. (2024b). Common policy gradient-based RL practices (Schulman et al., 2017) optimize Eqn. 3 in token-level MDP with an entropy-bonus H(\u03c0) and a KL-constraint with ref:\n$\\underset{\\pi}{max} E_{a_t\u223c\\pi(1|s_t)} [\\hat{r}(s_t, a_t) + \\beta log \\pi_{ref} (a_t|s_t) + \u03b2H(\u03c0)]$\nwhere $81 \u223cp$. Its optimal solution is given by Ziebart et al. (2008) under the maximum entropy RL setting:\n$\\pi^*(a_t|s_t) = exp((Q^*(s_t, a_t) \u2013 V^*(s_t))/\u03b2)$\nwhere Q*(st, at) is the optimal Q-function that estimates the partial returns of at under st, and V*(st) estimates the expected future returns under current state st. Under a KL-divergence regularization with the reference model, the relationship between Q-function and token-level reward values can be established as follows with the Bellman equation:\n$Q^*(s_t, a_t) = \\hat{r}(s_t, a_t) + \u03b2log #ref(a_t|s_t) + V^*(S_{t+1})$\nwhere V*(ST) = 0. Combining Eqn. 10 and 11, we have:\n$\\hat{r}(s_t, a_t) = \u03b2log \\frac{\\pi^*(a_t | s_t)}{\\Pi_{ref}(a_t | s_t)} +V^*(s_t) - V^*(S_{t+1})$\nUnder Assumption 1, we substitute Eqn. 12 into Eqn. 6, the response-level reward is factorized as follows:\n$r(q, \\tau) = \\sum_{t=1}^{T} \\hat{r}(s_t, a_t)$\n$= \\sum_{t=1}^{T} \u03b2log \\frac{\\pi^*(a_t | s_t)}{\\Pi_{ref}(a_t | s_t)} + V^*(s_1)$\nNote that in DPO, V*(s1) remains unchanged for each response pair as they have the same start state s1. This means the preference modeling process for each response pair only depends on the first term of Eqn. 13. Therefore, we conclude that the optimal policy \u03c0* learned by DPO inherently fits the response-level reward value with another token-level reward function (st, at), which is parameterized as:\n$\\hat{r}(s_t, a_t) = \u03b2log \\frac{\\pi(a_t | s_t)}{\\pi_{ref}(a_t|s_t)}$\nThis indicates our results in Eqn. 8 and completes the proof.\n3.2 Selective Preference Optimization\nSePO is guided by the simple idea that \"not all tokens are equally effective\", which has been widely evaluated (Lin et al., 2024; Chen et al., 2024b; Lai et al., 2024). We explore fully utilizing DPO to efficiently select the most useful tokens in modeling human preferences in LLM alignment. Firstly, we train an oracle model on a moderate-scale preference dataset with DPO, aiming to model a token-level reward function for the target data distribution. The reference model is then applied to large-scale data to score all the tokens. The policy model is only trained on selected tokens with highest scores in chosen responses and lowest scores in rejected responses, which are expected as key tokens in achieving alignment.\nOracle Modeling with DPO. We explore fitting an oracle model with DPO utilizing a moderate-scale dataset via random sampling over the target preference dataset D, which we expect to parameterize a pessimistic estimation of the target reward function.\nTheorem 2. Let D be the target preference dataset with N samples, and S be a random selection of m samples from D (m\u2264 N), which is drawn independently and uniformly. Then we have:\nThe reward function rs modeled by fitting S with DPO is a pessimistic estimation of the target reward function rd. The result can be formalized as:\n$E_s(r_s(q,y)) \u2264 r_d(q,y)$\nwhere q, y denote any query-response pairs drown from D. The equality holds when m = N.\nProof. As the reward functions are parameterized via fitting the DPO algorithm on the datasets, we substitute Eqn. 4 into Eqn. 15. As the term $\\beta log \\pi_{ref}(y|q)$ and \u03b2 log Z(q) are unrelated to S, they are easily canceled and we transfer the proof target into comparing the optimal policy functions:\n$E_s [log \\pi^*_S(y|q)] \u2264 log \\pi^*_D(y|q)$\nLet D = {X1,X2, ..., XN }, where xi represents an (q, y, r) data point, and S = {Xi1, Xi1, ..., Xim }, where xi; is selected from D. To show that S is an unbiased estimator of the target data distribution, we calculate its empirical distribution over all possible random samples drawn from D. The empirical distribution Ps(X) based on the sampled dataset is as follows:\n$P_s(X) = \\frac{1}{m} \\sum_{j=1}^{m} \u03b4(X = X_{i_\u03bb})$\nwhere & indicates the presence of a sample X. Taking its expectation over all possible sampled datasets, we have:\n$E_s [P_s(X)] = E_s [\\frac{1}{m} \\sum_{j=1}^{m} \u03b4(X = X_{i_\u03bb})]$\n$= \\frac{1}{m} \\sum_{j=1}^{m} E_s [\u03b4(X = X_{i_\u03bb})]$\nAs each xi; is equally likely to be any xi \u2208 D, we have\n$E_s [\u03b4(X = X_{i_\u03bb})] = \\frac{1}{N} \\sum_{i=1}^{N} \u03b4(X = X_{i_\u03bb}) = P_D(X)$\nSubstituting Eqn. 19 into Eqn. 18, we have\n$E_s [P_s(X)] = P_D(X)$\nBased on the same reference model and empirical data distribution (Eqn. 20), we expect training on S with DPO to obtain an unbiased estimation of the target optimal policy function:\n$E_s [\u03c0^*_S(y|q)] = \u03c0^*_D(y|q)$\nBecause logarithm is a strictly concave function, according to Jensen's inequality, we have:\n$E_s [log \\pi^*_S(y|q)] \u2264 log E_s [\u03c0^*_D(y|q)]$\nSubstituting Eqn. 21 into Eqn. 22, we prove Eqn. 16, which completes the proof. Note that when m = N, we have S = D, and the training process gives an unbiased estimation of target reward function rd.\nWith the results from Theorem 2, we first perform SFT on the chosen responses of the moderate-scale dataset S to obtain the base model Tref:\n$L_{SFT} = -E_{(q,y_w)\u223cS} \\sum_{Yu} log \\pi_{ref} (Yu|q, y)$"}, {"title": "3.3 Application: Weak-to-Strong Generalization", "content": "A unique advantage of SePO is that the cost of its token selection process can be easily reduced by controlling the oracle model size, which enables small oracle models to steer the alignment for policy models with much stronger capabilities. Therefore, we explore the application of SePO in weak-to-strong generalization (Burns et al., 2023), which aims to elicit the capabilities of strong student models with weak supervision signals. As illustrated in Figure 3, our focus is two-fold:\nWeak Oracle Modeling. We propose to leverage weak oracle models that underperform the policy models to select the key tokens from strong in-distribution data. Our intuition is that weak supervisors (oracle models) only identify which tokens are most effective in enhancing the alignment follows:\n$L_{SePO} = -E_{(q,y_w,y_l)\u223cD} log \u03c3 (\u00fb_w (q, y_w) \u2013 \u00fb_l(q, y_l))$\n$\u00fb_\u03c9 (q, y) = \\frac{\u03b3}{y}\\sum_{i=1}^{yk%} I(s(y_i)) log \u03c0_t (y_i|q, y_{<i})$\n$\u00fbn (q, y) = \\frac{\u03b3}{y}\\sum_{i=1}^{yk%} \u03a3I(s(y_i)) log \u03c0_t (y_i|q, y_{<i})$\nwhere y controls the scaling of the rewards. The SePO objective enables direct adjustment of only crucial tokens for alignment, which avoids noise in both chosen and rejected responses. This design is expected to improve the efficiency and stability of the preference optimization process. The objective is also length-normalized and reference model-free, which prevents bias towards over-length responses and enables memory-efficient alignment (Meng et al., 2024; Yuan et al., 2023).\nwhere $I(s(y_i)) = 1, if s(y_i) ranks in highest k% in y,  0, otherwise$\nFor rejected responses, we change the condition for indicating 1 to \"if s(yi) ranks in lowest k% in y\" and mark the indicator function as I (s(yi)). The intuition behind this action is that key tokens for chosen responses are likely to contribute high token-level rewards, while key tokens for rejected responses are likely with low token-level rewards, whose probabilities are significantly suppressed in Tora compared to the base model.\nSePO Objective. We design a simple contrastive preference optimization objective on the target policy model \u03c0t with the selected tokens. Specifically, the objective function is designed as"}, {"title": "4 Experiments", "content": "4.1 Experimental Settings\nModels and Training Data We comprehensively evaluate SePO on two representative model families: LLaMA (Touvron et al., 2023) and Pythia (Biderman et al., 2023). To approximate the optimal token-level reward function, we first obtain the base model by training on the UltraChat-200K dataset (Ding et al., 2023) in an SFT manner. For the LLaMA model family, we train the base model on the pre-trained TinyLLaMA-1.1B (Zhang et al., 2024) model. For Pythia, we separately train on Pythia-(70M, 160M, 410M, 1B, 1.4B) to facilitate research on the effect of oracle models with different sizes. For each base model, we obtain the oracle models by further fine-tuning with DPO on the Ultra-Feedback dataset (Cui et al., 2023) with 61,135 preference pairs. We examine the selective preference training process on target policy models with various capabilities. For Pythia, we perform SFT on Pythia-(2.8B, 6.9B) with UltraChat-200K and use them as policy models, which we refer as Pythia-SFT. For LLaMA, we test on TinyLLaMA-Chat-1.1B, LLaMA2-Chat-7B, and LLaMA2-Chat-13B.\nBaseline Methods We compare the performance of SePO with state-of-the-art offline preference optimization methods to indicate its effectiveness. DPO (Rafailov et al., 2024c) explicitly models the relation between the reward functions and optimal policy, which enables reward model-free preference alignment. IPO (Azar et al., 2024) provides a variant of DPO that prevents the requirement that pairwise preferences can substitute reward values. We introduce another two competitive reference model-free alignment methods: RRHF (Yuan et al., 2024) directly optimize the probability of the target response pairs with a pairwise ranking loss, with an SFT-based regularization on the chosen responses. SimPO (Meng et al., 2024) optimizes the length-regularized probability of the response pairs with a margin.\nEvaluation Benchmarks We quantify the contribution of each method by evaluating on three widely utilized instruction-following benchmarks: AlpacaEval 2.0 (Dubois et al., 2024), MT-Bench (Zheng et al., 2024b), and Arena-Hard (Li et al., 2024). AlpacaEval 2.0 consists of 805 queries to evaluate the models' versatile conversational abilities. We report the win rates and length-controlled win rates (LC win rate) of evaluated models against the responses of GPT-4-turbo. The LC win rates are designed to reduce influences of model verbosity. MT-Bench covers eight categories with 80 queries. We report the average scores ranging from 0 to 10. Due to the widely reported poor separability of MT-Bench reported by previous works (Meng et al., 2024; Li et al., 2024), we further display fine-grained scores of model capability, which we reorganize the original 8 categories as follows: Assistant (Writing, Roleplay, Extraction), QA (Reasoning, STEM, Humanities), Math, and Coding. Scores for Assistant and QA are averages of the original categories. Arena-Hard extends MT-Bench with 500 high-quality queries, where we report win rates against GPT-4-0314 model outputs. All judgments are performed by the latest GPT-40 model.\n4.2 Overall Performance\nPerformances of SePO and other baseline methods on three benchmark datasets are presented in Table 1. According to the results, SePO significantly improves performance over the base policy models. For example, on Arena-Hard, SePO achieves a 6.71% improvement in win rates on Pythia-6.9B. SePO also outperforms other strong baseline preference optimization methods. On Arena-Hard, SePO achieves the best win rates on all policy models, surpassing state-of-the-art methods such as SimPO and DPO. As SePO models are only optimized on 30% of the tokens trained on other methods, these results directly strengthen the effectiveness of selective training strategies applied in preference optimization.\nOn AlpacaEval 2.0, SePO continues to achieve superior performances over baseline methods on both win rates and LC win rates, further proving its effectiveness on different benchmarks. Notably, SePO outperforms all other methods on length-controlled win rates, including SimPO and RRHF, which are specifically designed for length-normalized reward formulation. These results show that selective training strategies also enhance policy models in avoiding over-length responses. We believe the reason is that during token selection, the token-level reward function can assign the end-of-sentence tokens with low-ranking scores, which can be discarded during optimization if the response ends inappropriately (e.g. over-length or repetition). In contrast, though SimPO and RRHF design length-normalized rewards, the end-of-sentence tokens are included for all responses, still fitting the response lengths for all training samples.\nOn MT-Bench, SePO outperforms all other methods on average scores. Due to the widely discussed poor separability of overall scores for MT-Bench, we look into category-based evaluations that provide fine-grained assessments. As shown, SePO achieves the best performances on 70% of comparisons on Assistant and QA, indicating its significant improvement on subjective tasks that require high-level intention understanding and writing skills. However, SePO outperforms baseline methods in math and coding on only 40% of the comparisons, underperforming baseline methods such as IPO and SimPO on several policy models. A possible reason is that objective tasks such as math and coding require coherent logic along the token-level MDP for response generation (Xie et al., 2024; Chen et al., 2024b; Lai et al., 2024), while SePO is only optimized on selected tokens, which brings discontinuity in learning the logic during training. Baseline methods that optimize all tokens enable policy models to learn the full chain of reasoning and show advantages in objective scenarios."}, {"title": "4.3 Impact of Data Scale", "content": "This section evaluates how training data scales of SePO and oracle modeling impact policy model performance. We focus on two research questions:\nHow do token selection rates influence SePO performance? We investigate the influences of token selection rates on SePO performances by introducing various combinations for chosen and rejected responses. The ratio for chosen/rejected responses is each selected from {0.1, 0.3, 0.5, 0.7,0.9} and matched pair-wise, with 25 combinations in total. The experimental results are presented in Figure 4.\nAccording to the results, increasing selection rates from 0.1 for chosen/rejected responses rapidly improves policy model performance, but the momentum decreases as the ratio continues to grow. For example, the LC win rate of LLaMA2-Chat-7B improves from 8.37% to 14.8% as the ratios for chosen/rejected responses rise from 0.1 to 0.5 progressively, then stabilizes around 14.7% with higher selection rates. These observations prove our hypothesis that not all tokens are equally effective for LLM alignment. Training only on key tokens effectively improves alignment performance, while other tokens provide limited supervision information. From the figures, we conclude that training on Top-30% tokens for TinyLLaMA or Top-50% tokens for LLaMA2-Chat-(7B, 13B) already provides comparable performance to alignment on all tokens.\nIncreasing selection rates for chosen responses generally outperforms increasing ratios for rejected responses. For example, with a fixed rejected selection rate, the TinyLLaMA-Chat performance smoothly improves as the chosen ratios grow. However, improving rejected ratios from 0.1 to 0.9 leads to decreased model performance in 4 out of 5 fixed chosen selection rates. Similar results can be observed in the other two policy models. These results indicate that compared to increasing probabilities for irrelevant tokens in chosen responses, suppressing probabilities for high-reward tokens in rejected responses can significantly affect the model performance.\nHow much data trains a good oracle model? In Theorem 2, we proved that training an oracle model on a randomly selected moderate-scale dataset is a pessimistic estimation of the target reward function. In this section, we empirically investigate the influence of the training data scale for oracle models on SePO performance. Specifically, we randomly sample different proportions of data from UltraFeedback as training data for the TinyLLaMA-based oracle model. The results are shown in Figure 5.\nAccording to the results, training the oracle model on higher proportions of the target data generally leads to superior model performance. LC win rates on all policy models improve as the estimated token-level reward function becomes more accurate. Training on high data proportions also retains the majority of token selection capabilities. For example, supervising LLaMA2-Chat-7B policy model with an oracle model trained on 70% of the data still achieves 13.66% of LC win rates, which outperforms strong baseline methods such as SimPO and RRHF. However, the continual decrease in training proportions can significantly harm model performance. For the TinyLLaMA-Chat policy model, an oracle model trained with less than 40% of target data leads to LC win rates of less than 1.26%, which even underperforms the original policy model. For LLaMA2-Chat-(7B,13B), this threshold increases to 50% and 70%. These results indicate the importance of accurate estimation of the reward function, where false selection of key tokens degrades the capability of the policy model. These thresholds also increase with the size of policy models, showing that the high quality of key tokens becomes more important in supervising models with strong capabilities."}, {"title": "4.4 Weak-to-Strong Generalization", "content": "In this section, we empirically evaluate SePO on enhancing weak-to-strong generalization.\nWeak Oracle Modeling. In Table 1, we presented the performance of TinyLLaMA and Pythia-1B oracle models on guiding stronger policy models (e.g. Pythia-SFT-6.9B, LLaMA2-Chat-13B). The competitive results of SePO prove the viability of weak oracle modeling. To provide a clear landscape, we further train oracle models with Pythia-(70M, 160M, 410M, 1B, 1.4B) on the same target data and compare their performances on the Pythia-SFT-(2.8B, 6.9B) policy models. The results are shown in Figure 6 (a).\nAccording to the results, oracle models with weak capabilities can provide effective supervision to strong policy models. For example, training with the Pythia-410M oracle model achieves 6.58% on Pythia-SFT-2.8B and 13% on Pythia-SFT-6.9B policy models, with up to 16.8\u00d7 more parameters than the oracle model. These results outperform full optimization on the target dataset with baseline methods such as DPO and SimPO. In addition, the performance of target policy models continually improves as the oracle model size increases. For example, on Pythia-SFT-6.9B policy model, the 1.4B oracle model outperforms the 410M orcale model by 1.84% and the 70M model by 9.15%. These results show that oracle models with stronger capabilities can better model the token-level reward function and accurately select key tokens.\nTo provide an intuitive view, we present the token-level score distributions of different oracle models in 6 (b)(c). For the chosen scores distribution, strong oracle models such as Pythia-(1B,1.4B) show higher densities in extreme (large and small) reward values, which facilitates separating key tokens from the other tokens. In contrast, small oracle models tend to fit a Gaussian distribution, where most tokens have similar scores. These results show that weak oracle models struggle to distinguish key tokens within the dataset. We observe similar results on rejected scores distribution, which further proves the capability of the oracle models as crucial in modeling the token-level reward function.\nWeak Data Supervision. We evaluate the weak data supervision performance of SePO by training on HH-RLHF (Bai et al., 2022), an early-released preference dataset with relatively lower quality (Yang et al., 2024a) on responses. We perform SePO with a TinyLLaMA-based oracle model and 30% token selection rates, and comparisons with baseline methods are shown in Table"}]}