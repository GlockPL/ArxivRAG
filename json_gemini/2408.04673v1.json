{"title": "AutoFAIR : Automatic Data FAIRification via Machine Reading", "authors": ["Tingyan Ma", "Wei Liu", "Bin Lu", "Xiaoying Gan", "Yunqiang Zhu", "Luoyi Fu", "Chenghu Zhou"], "abstract": "The explosive growth of data fuels data-driven research, facilitating progress across diverse domains. The FAIR principles emerge as a guiding standard, aiming to enhance the findability, accessibility, interoperability, and reusability of data. However, current efforts primarily focus on manual data FAIRification, which can only handle targeted data and lack efficiency. To address this issue, we propose AutoFAIR, an architecture designed to enhance data FAIRness automately. Firstly, We align each data and metadata operation with specific FAIR indicators to guide machine-executable actions. Then, We utilize Web Reader to automatically extract metadata based on language models, even in the absence of structured data webpage schemas. Subsequently, FAIR Alignment is employed to make metadata comply with FAIR principles by ontology guidance and semantic matching. Finally, by applying AutoFAIR to various data, especially in the field of mountain hazards, we observe significant improvements in findability, accessibility, interoperability, and reusability of data. The FAIRness scores before and after applying AutoFAIR indicate enhanced data value.", "sections": [{"title": "INTRODUCTION", "content": "In the age of big data, the management and utilization of data have become pivotal for scientific advancement and industrial innovation. The FAIR principles [21]-Findability, Accessibility, Interoperability, and Reusability-have emerged as essential guidelines for enhancing the usability and value of data across diverse domains. These principles aim to address the challenges associated with inadequate metadata, and inefficient data discovery processes.\nThe introduction of the FAIR principles in 2016 [21] marked a significant shift in the acceptance and support of FAIR principles by various institutions. Subsequent efforts by the FAIR Metrics Working Group [22] led to a more comprehensive understanding of the FAIR principles, culminating in the proposal of evaluation metrics for FAIR compliance. From initial conceptual frameworks to practical implementations, the FAIR principles have gained wide popularity in research communities and institutions around the world. Initiatives like GO FAIR and FAIRsFAIR have played crucial roles in promoting standardized approaches to data management, emphasizing the importance of FAIR compliance for accelerating scientific discovery and innovation.\nFor domain-specific data, such as agriculture, healthcare, and climate, data publishers have developed specialized data FAIRification strategies tailored to each field. In the healthcare, the strategies [20] emphasize the privacy and security of sensitive patient data while promoting interoperability between different health information systems, facilitating better clinical decision-making and research outcomes. These specialized strategies highlight the importance of detailed FAIRification methods to effectively implement the FAIR principles and enhance data utility across various scientific disciplines.\nHowever, despite the advancements in FAIR adoption, practical challenges remain in technology to automate the handling process. 1. Complex Standards: Despite the detailed implementation steps proposed by various groups, achieving uniformity across diverse"}, {"title": "RELATED WORK", "content": "The FAIR Principles were first proposed by M.D. Wilkinson et al. in 2016 [21] and have been widely promoted and applied in subsequent years. The FAIR Indicators Working Group [22] conducted a comprehensive study and eventually proposed evaluation indicators for FAIR compliance. A.Jacobsen et al. further interpreted the FAIR Principles [12], highlighting the key points to consider during implementation. Additionally, M. Hahnel, V. Dan, A. Dunning, et al. evaluated selected data repositories [7, 9], analyzing their performance concerning data FAIRness. To more accurately assess the extent to which data repositories have implemented the FAIR principles, organizations such as the FAIR Indicators Group [22], the Dutch Data Archiving and Networking Service (DANS) [8], the Australian Research Data Sharing Organization (ANDS) [3], FAIRsFAIR [6] and Open Science Cloud Europe [17] have proposed their own assessment models and tools. These tools typically contain clear assessment indicators and scoring mechanisms, providing"}, {"title": "PRELIMINARY", "content": "As an initial effort to automate data FAIRification, we establish a domain-agnostic data handling process to facilitate the discovery and reuse of data. We observe that in the implementation of FAIR principles, the most important guidelines evolve into the scoring details of FAIRsFAIR [6] through step-by-step development [1, 18, 21, 22]. We improve the FAIRness of data based on the FAIRsFAIR metrics, and in Table 1 we show the operations at the data and metadata level and also illustrate the correspondence of each operation to the FAIR metrics.\nAt the data level, we focus on extracting permanent unique identifiers, core elements such as creator, title, publisher, release date, and keywords, as well as license information. These operations correspond to specific FAIR metrics, ensuring that each aspect of the data is properly accounted for and documented. At the metadata level, we aim to complete the metadata profile by embedding identifiers, ensuring machine retrievability, and standardizing descriptions using models like DCAT. Additionally, we emphasize"}, {"title": "METHOD", "content": "AutoFAIR extracts and standardizes metadata from data webpages using two main components: Web Reader and Fair Alignment module, showed in Figure 1. Web Reader converts the webpage HTML into a DOM tree and uses Graph Neural Networks (GNNs) to classify nodes, locating the desired metadata. For nodes with complex text, a language model extracts the most relevant information. Fair Alignment module standardizes fields through ontology guidance and semantic matching, creating data entries on the DataExpo website [16] and embedding DCAT [2] metadata to ensure machine-readability and unified findability across sites. Additionally, the system processes temporal and spatial information using techniques like coordinate transformation, geographic registration, and interpolation to support spatiotemporal map search."}, {"title": "Web Reader", "content": "Firstly, we convert the HTML of the data webpage into a DOM tree, where each node corresponds to an element in the HTML. Next, a node-wise classifier utilizing graph neural networks is employed to locate the desired metadata information. For nodes containing complex text, we use a language model to extract the most relevant information. Through this pipeline, we can effectively obtain the metadata of the data."}, {"title": "Node-wise Classifier", "content": "The process of converting an HTML document into a DOM tree involves several steps:\n(1) Tokenization: The HTML document is tokenized into a sequence of tokens representing HTML elements, attributes, and text.\n(2) Tree Construction: A DOM tree is constructed recursively from the HTML tokens. Each HTML element becomes a node in the tree, with its attributes stored as properties of the node. Child elements become children of their parent nodes in the tree.\nOnce the HTML document is represented as a DOM tree, we apply Graph Neural Networks (GNNs) model to classify nodes within the tree. Let X be the node feature matrix of size |V| \u00d7 d, where V is the number of nodes and d is the dimension of the node features. Each row of X corresponds to the feature vector of a node in the DOM tree. At each layer of the GNN, the message passed from node vj to node vi is computed as:\n$m_{j}^{(l)} = ReLU(W^{(l)}h_{j}^{(l-1)})$, \nwhere $h_{j}^{(l-1)}$ is the representation of node vj at the (l \u2212 1)-th layer, $W^{(l)}$ is a learnable weight matrix. After computing the messages, they are aggregated to update the node representations:\n$h_{i}^{(l)} = AGGREGATE({m_{j}^{(l)} | v_{j} \\in N(v_{i})})$, \nwhere N(vi) represents the set of neighboring nodes of vi. Finally, the node representations are fed into a softmax classifier to predict the label probabilities:\n$p(y_{i} | v_{i}) = softmax(W^{(K)}h_{i}^{(K)} :$ \nwhere K is the number of GNN layers and $W^{(K)}$ is the weight matrix of the classifier.\nThe parameters of the classifier are learned by minimizing the cross-entropy loss over the labeled nodes:\n$L = -\\sum_{(v_{i}, Y_{i}) \\in L} \\sum_{c=1} I(y_{i} = c) log p(y_{i} = c | v_{i})$, \nwhere I() is the indicator function."}, {"title": "Element-wise Extractor", "content": "Once the node-wise classifier identifies the HTML nodes corresponding to the metadata fields, we utilize language models to extract the relevant information from these nodes. Specifically, we take the text content of each identified HTML node and use the BERT model to encode this text, guiding the extraction process with a carefully designed prompt. For instance, when we need to extract key spatiotemporal information from a dataset description, the prompt is tailored to emphasize the importance of these details. The final piece of relevant information is then obtained by processing the output from BERT."}, {"title": "Fair Alignment", "content": "To address the challenges posed by inconsistent information descriptions within FAIR principles and their varied formats, we adopt techniques such as ontology guidance and semantic matching. These"}, {"title": "Theoretical Analysis of FAIR Principles' Implementation in AutoFAIR", "content": "AutoFAIR's design is rooted in the FAIR principles (Findability, Accessibility, Interoperability, and Reusability). The Web Reader component enhances Findability by extracting comprehensive metadata from webpages using Graph Neural Network (GNNs) and language models that analyzes the DOM structure for accurate metadata extraction. This metadata is made Accessible by structuring it in a machine-readable format. The FAIR Alignment component ensures Interoperability through ontology guidance and semantic matching, aligning metadata with widely accepted standards such as DCAT. This alignment guarantees that metadata can be easily integrated"}, {"title": "DATA FAIRNESS ANALYSIS", "content": "To analyze the effectiveness of our architecture, we take the field of mountain hazard as an example and collect data from various websites within this domain for data FAIRification. Specifically, we analyze 7124 data from 512 domains. The metadata in mountain hazard domains exhibits three key characteristics:\n(1) Metadata adheres to FAIR principles: Data webpages align with FAIR principles, ensuring findability, accessibility, interoperability, and reusability of associated resources.\n(2) Metadata embeds in HTML structures: Data webpages with metadata embedded within HTML structures, increasing the difficulty of machine retrieval of data.\n(3) Metadata scatters in textual descriptions: Data webpages with metadata fragments dispersed within textual descriptions, requiring sophisticated extraction techniques.\nIn response to the situation where the metadata of the last two types does not meet the FAIR principles, we have successfully implemented AutoFAIR to extract webpage information, thereby enhancing the machine-readable metadata profile."}, {"title": "The Impact of FAIRification on Dataset Fairness", "content": "Based on the FAIRsFAIR [6] released in 2020, we conducted an assessment of adherence to FAIR principles, comparing the FAIRness scores of the original data domains with those processed through AutoFAIR. Different from manual domain-oriented processing methods, the automated workflow of AutoFAIR enables it to efficiently"}, {"title": "Findable and Accessible", "content": "Based on the powerful information extraction capability of Web Reader, we standardize the extraction of fields into metadata profiles that adhere to the FAIR principles. This effort enhances the findability and accessibility of data, as the FAIR metadata profiles contain information such as data titles, descriptions, keywords, and in some cases, references to other datasets. This improvement is showcased through the data website built on AutoFAIR."}, {"title": "Interoperable and Reusable", "content": "The interoperability of data has revolutionized the landscape of large-scale data analysis, enabling machine-based processing through metadata documents. Leveraging the capabilities of the AutoFAIR, we have pioneered the development of dataset spatiotemporal mapping webpages within the realm of mountain hazard datasets. By employing a standardized metadata vocabulary, we have effectively mapped dataset descriptions onto spatiotemporal maps, thus facilitating seamless retrieval based on temporal or spatial parameters. Moreover, our extensive metadata resources empower users to filter and access desired data efficiently, whether by keywords or research institution affiliations."}, {"title": "The Information Extraction Ability of Web Reader", "content": "The effectiveness of our AutoFAIR tool depends on the information extraction capability of Web Reader. Table 3 demonstrates the extraction capability of Web Reader across 7124 websites, using data DOI and license fields as examples. It can be observed that Web Reader can accurately extract information from virtually all domains.\nAdditionally, through the institution information extraction of Web Reader, we have successfully identified the top data research and publishing institutions in the field of mountain hazard. Table 4"}, {"title": "CONCLUSION", "content": "In conclusion, AutoFAIR presents a comprehensive solution for automating data FAIRification, significantly enhancing data compliance with the FAIR principles. By integrating the Web Reader's two-stage information extraction method, which considers both DOM tree structures and textual semantics, and the FAIR Alignment module, which utilizes ontology guidance and semantic matching, AutoFAIR effectively extracts and standardizes metadata across diverse and unstructured data sources. The application of AutoFAIR in the domain of mountain hazards demonstrated notable improvements in data FAIRness, proving the system's effectiveness. Future work could extend AutoFAIR's applicability to other domains and refine its capabilities to manage complex data structures, leveraging advanced natural language processing and machine learning techniques to further optimize metadata extraction. This work contributes to the broader adoption of FAIR principles, facilitating data sharing and promoting innovation across various fields."}]}