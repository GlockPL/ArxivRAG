{"title": "AutoFAIR : Automatic Data FAIRification via Machine Reading", "authors": ["Tingyan Ma", "Wei Liu", "Bin Lu", "Xiaoying Gan", "Yunqiang Zhu", "Luoyi Fu", "Chenghu Zhou"], "abstract": "The explosive growth of data fuels data-driven research, facilitating progress across diverse domains. The FAIR principles emerge as a guiding standard, aiming to enhance the findability, accessibility, interoperability, and reusability of data. However, current efforts primarily focus on manual data FAIRification, which can only handle targeted data and lack efficiency. To address this issue, we propose AutoFAIR, an architecture designed to enhance data FAIRness automately. Firstly, We align each data and metadata operation with specific FAIR indicators to guide machine-executable actions. Then, We utilize Web Reader to automatically extract metadata based on language models, even in the absence of structured data webpage schemas. Subsequently, FAIR Alignment is employed to make metadata comply with FAIR principles by ontology guidance and semantic matching. Finally, by applying AutoFAIR to various data, especially in the field of mountain hazards, we observe significant improvements in findability, accessibility, interoperability, and reusability of data. The FAIRness scores before and after applying AutoFAIR indicate enhanced data value.", "sections": [{"title": "INTRODUCTION", "content": "In the age of big data, the management and utilization of data have become pivotal for scientific advancement and industrial innovation. The FAIR principles [21]-Findability, Accessibility, Interoperability, and Reusability-have emerged as essential guidelines for enhancing the usability and value of data across diverse domains. These principles aim to address the challenges associated with inadequate metadata, and inefficient data discovery processes.\nThe introduction of the FAIR principles in 2016 [21] marked a significant shift in the acceptance and support of FAIR principles by various institutions. Subsequent efforts by the FAIR Metrics Working Group [22] led to a more comprehensive understanding of the FAIR principles, culminating in the proposal of evaluation metrics for FAIR compliance. From initial conceptual frameworks to practical implementations, the FAIR principles have gained wide popularity in research communities and institutions around the world. Initiatives like GO FAIR and FAIRsFAIR have played crucial roles in promoting standardized approaches to data management, emphasizing the importance of FAIR compliance for accelerating scientific discovery and innovation.\nFor domain-specific data, such as agriculture, healthcare, and climate, data publishers have developed specialized data FAIRification strategies tailored to each field. In the healthcare, the strategies [20] emphasize the privacy and security of sensitive patient data while promoting interoperability between different health information systems, facilitating better clinical decision-making and research outcomes. These specialized strategies highlight the importance of detailed FAIRification methods to effectively implement the FAIR principles and enhance data utility across various scientific disciplines.\nHowever, despite the advancements in FAIR adoption, practical challenges remain in technology to automate the handling process. 1. Complex Standards: Despite the detailed implementation steps proposed by various groups, achieving uniformity across diverse data sources remains a significant challenge. 2. Lack of Automate Technology: Automating the FAIRification process poses technological challenges. Current methods often require manual intervention, especially for domain-specific data. These manual processes lack efficiency and scalability, hindering the broader adoption and consistent application of FAIR principles.\nTo address the above issues, we propose an automated FAIRification architecture AutoFAIR, which converts non-FAIR compliant data into a FAIR-compliant format. By integrating Web Reader and FAIR Alignment, AutoFAIR can automatically process data, reducing the need for manual intervention while improving data FAIRness. The architecture not only complies with the FAIR Principles, but can also be executed automately to realize efficient processing and management of data from multiple sources. The application of automated processes not only improves the efficiency of data processing, but also greatly expands the scope of application of the FAIR principle and promotes the progress of data sharing and utilization. Our main contributions are as follows:\n\u2022 We introduces AutoFAIR, an innovative architecture that automates the process of making dataset webpages FAIR-compliant.\n\u2022 We propose a novel two-stage information extraction method in Web Reader which automatically extracts metadata from diverse data sources using web structure analysis and language models (LMs) to ensure comprehensive metadata generation even without standardized schemas.\n\u2022 We design FAIR Alignment that employs ontology guidance and semantic matching to standardize information, converting heterogeneous metadata into a unified FAIR data profile.\n\u2022 A case study in mountain hazard research demonstrate the effectiveness of AutoFAIR. By applying AutoFAIR, the FAIR scores of different websites have been significantly improved. Based on the FAIR data profile, it generates spatiotemporal distribution maps that enhance data reusability and support keyword searches, providing data support for research."}, {"title": "RELATED WORK", "content": "The FAIR Principles were first proposed by M.D. Wilkinson et al. in 2016 [21] and have been widely promoted and applied in subsequent years. The FAIR Indicators Working Group [22] conducted a comprehensive study and eventually proposed evaluation indicators for FAIR compliance. A.Jacobsen et al. further interpreted the FAIR Principles [12], highlighting the key points to consider during implementation. Additionally, M. Hahnel, V. Dan, A. Dunning, et al. evaluated selected data repositories [7, 9], analyzing their performance concerning data FAIRness. To more accurately assess the extent to which data repositories have implemented the FAIR principles, organizations such as the FAIR Indicators Group [22], the Dutch Data Archiving and Networking Service (DANS) [8], the Australian Research Data Sharing Organization (ANDS) [3], FAIRsFAIR [6] and Open Science Cloud Europe [17] have proposed their own assessment models and tools. These tools typically contain clear assessment indicators and scoring mechanisms, providing an objective basis for the management and improvement of data repositories.\nSome methods have been proposed on how to implement the FAIR principles. A. Jacobsen et al. proposed a generic process for make digital resources FAIR [13], while GO FAIR [1] proposes a full set of methods and tools for the FAIRification of data in a particular domain [6]. In addition, several groups and organizations have published recommendations to guide the implementation of FAIR, such as the European Commission's \"Making FAIR a reality\" report, which provides 27 detailed recommendations and actions for different stakeholders [4]. The European Open Science Cloud FAIR Working Group has also made six recommendations to advance the implementation of FAIR [10]. These generic processes cover all aspects of data creation, storage, sharing, and reuse, emphasizing the importance of standardization and normalization of operations. However, despite these detailed implementation steps, practical challenges remain in technology to automate the handling process. For domain-specific data, such as agriculture [11], healthcare [20], and climate [19], data publishers have developed specialized data FAIRification strategies based on their respective domains' characteristics. However, these manual methods can only perform on targeted data and lack cross-domain and automation support.\nWeb information extraction has long been a critical area of study, focusing on efficiently extracting information from webpages. Traditional approaches predominantly rely on the HTML text of webpages, leveraging the DOM tree's graph structure or serialized processing methods. Approaches such as SimpDOM[23] and FreeDOM[15] exploit the inherent tree structure of the DOM for node classification, while Transformer-based methods like DOM-LM[5] and MarkupLM[14] integrate text and markup within a unified framework. In contrast, the Web Reader component of AutoFAIR introduces a novel two-stage information extraction method that simultaneously considers the DOM tree structure and textual semantics. This ensures comprehensive metadata extraction without the need for structured data schemas, facilitating further compliance with FAIR principles through the FAIR Alignment module."}, {"title": "PRELIMINARY", "content": "As an initial effort to automate data FAIRification, we establish a domain-agnostic data handling process to facilitate the discovery and reuse of data. We observe that in the implementation of FAIR principles, the most important guidelines evolve into the scoring details of FAIRsFAIR [6] through step-by-step development [1, 18, 21, 22]. We improve the FAIRness of data based on the FAIRsFAIR metrics, and in Table 1 we show the operations at the data and metadata level and also illustrate the correspondence of each operation to the FAIR metrics.\nAt the data level, we focus on extracting permanent unique identifiers, core elements such as creator, title, publisher, release date, and keywords, as well as license information. These operations correspond to specific FAIR metrics, ensuring that each aspect of the data is properly accounted for and documented. At the metadata level, we aim to complete the metadata profile by embedding identifiers, ensuring machine retrievability, and standardizing descriptions using models like DCAT. Additionally, we emphasize the periodic verification of metadata accessibility and the inclusion of links to other entities, which are crucial for maintaining data interoperability and reusability."}, {"title": "METHOD", "content": "AutoFAIR extracts and standardizes metadata from data webpages using two main components: Web Reader and Fair Alignment module, showed in Figure 1. Web Reader converts the webpage HTML into a DOM tree and uses Graph Neural Networks (GNNs) to classify nodes, locating the desired metadata. For nodes with complex text, a language model extracts the most relevant information. Fair Alignment module standardizes fields through ontology guidance and semantic matching, creating data entries on the DataExpo website [16] and embedding DCAT [2] metadata to ensure machine-readability and unified findability across sites. Additionally, the system processes temporal and spatial information using techniques like coordinate transformation, geographic registration, and interpolation to support spatiotemporal map search."}, {"title": "Web Reader", "content": "Firstly, we convert the HTML of the data webpage into a DOM tree, where each node corresponds to an element in the HTML. Next, a node-wise classifier utilizing graph neural networks is employed to locate the desired metadata information. For nodes containing complex text, we use a language model to extract the most relevant information. Through this pipeline, we can effectively obtain the metadata of the data."}, {"title": "Node-wise Classifier", "content": "The process of converting an HTML document into a DOM tree involves several steps:\n(1) Tokenization: The HTML document is tokenized into a sequence of tokens representing HTML elements, attributes, and text.\n(2) Tree Construction: A DOM tree is constructed recursively from the HTML tokens. Each HTML element becomes a node in the tree, with its attributes stored as properties of the node. Child elements become children of their parent nodes in the tree.\nOnce the HTML document is represented as a DOM tree, we apply Graph Neural Networks (GNNs) model to classify nodes within the tree. Let X be the node feature matrix of size |V| \u00d7 d, where V is the number of nodes and d is the dimension of the node features. Each row of X corresponds to the feature vector of a node in the DOM tree. At each layer of the GNN, the message passed from node vj to node vi is computed as:\n$m_{j}^{(l)}=ReLU(W^{(l)}h_{j}^{(l-1)})$, (1)\nwhere $h_{j}^{(l-1)}$ is the representation of node vj at the (l \u2212 1)-th layer, $W^{(l)}$ is a learnable weight matrix. After computing the messages, they are aggregated to update the node representations:\n$h_{i}^{(l)}=AGGREGATE({m_{j}^{(l)}|v_{j} \\in N(v_{i})})$, (2)\nwhere N(vi) represents the set of neighboring nodes of vi. Finally, the node representations are fed into a softmax classifier to predict the label probabilities:\n$p(y_{i}|v_{i})=softmax(W^{(K)}h_{i}^{(K)})$, (3)\nwhere K is the number of GNN layers and $W^{(K)}$ is the weight matrix of the classifier.\nThe parameters of the classifier are learned by minimizing the cross-entropy loss over the labeled nodes:\n$L=-\\sum_{(v_{i},y_{i})\\in L}\\sum_{c=1}I(y_{i}=c)log p(y_{i}=c|v_{i})$, (4)\nwhere I() is the indicator function."}, {"title": "Element-wise Extractor", "content": "Once the node-wise classifier identifies the HTML nodes corresponding to the metadata fields, we utilize language models to extract the relevant information from these nodes. Specifically, we take the text content of each identified HTML node and use the BERT model to encode this text, guiding the extraction process with a carefully designed prompt. For instance, when we need to extract key spatiotemporal information from a dataset description, the prompt is tailored to emphasize the importance of these details. The final piece of relevant information is then obtained by processing the output from BERT."}, {"title": "Fair Alignment", "content": "To address the challenges posed by inconsistent information descriptions within FAIR principles and their varied formats, we adopt techniques such as ontology guidance and semantic matching. These methods aim to standardize each field. To establish a comprehensive data index and enhance data interoperability, we align each field by creating a dataset entry on the DataExpo website [16] and embedding DCAT [2] metadata within the page. This strategic approach ensures the provision of machine-readable metadata, enabling unified dataset searches across multiple sites.\n$DCAT\\_Metadata = f(Original\\_Metadata)$.\nFor temporal and spatial alignment, which is crucial for effective spatiotemporal map retrieval, we employ techniques like coordinate transformation, geographic registration, and interpolation. Let Ti denote temporal information in diverse formats and Si represent spatial information in various coordinate systems. We define functions ft for temporal standardization, fs for spatial transformation, fg for geographic registration, and fi for interpolation. This ensures alignment across all fields, with standardized temporal and spatial data represented as tu and s\u00fa respectively:\n$t_{u} = f_{t}(T_{i}),\\ s_{u}=f_{i}(f_{g}(f_{s}(S_{i})))$."}, {"title": "Theoretical Analysis of FAIR Principles' Implementation in AutoFAIR", "content": "AutoFAIR's design is rooted in the FAIR principles (Findability, Accessibility, Interoperability, and Reusability). The Web Reader component enhances Findability by extracting comprehensive metadata from webpages using Graph Neural Network (GNNs) and language models that analyzes the DOM structure for accurate metadata extraction. This metadata is made Accessible by structuring it in a machine-readable format. The FAIR Alignment component ensures Interoperability through ontology guidance and semantic matching, aligning metadata with widely accepted standards such as DCAT. This alignment guarantees that metadata can be easily integrated and used across different systems. Lastly, the comprehensive and standardized metadata enhances Reusability, making it easier for researchers to discover, access, and reuse data."}, {"title": "DATA FAIRNESS ANALYSIS", "content": "To analyze the effectiveness of our architecture, we take the field of mountain hazard as an example and collect data from various websites within this domain for data FAIRification. Specifically, we analyze 7124 data from 512 domains. The metadata in mountain hazard domains exhibits three key characteristics:\n(1) Metadata adheres to FAIR principles: Data webpages align with FAIR principles, ensuring findability, accessibility, interoperability, and reusability of associated resources.\n(2) Metadata embeds in HTML structures: Data webpages with metadata embedded within HTML structures, increasing the difficulty of machine retrieval of data.\n(3) Metadata scatters in textual descriptions: Data webpages with metadata fragments dispersed within textual descriptions, requiring sophisticated extraction techniques.\nIn response to the situation where the metadata of the last two types does not meet the FAIR principles, we have successfully implemented AutoFAIR to extract webpage information, thereby enhancing the machine-readable metadata profile."}, {"title": "The Impact of FAIRification on Dataset Fairness", "content": "Based on the FAIRsFAIR [6] released in 2020, we conducted an assessment of adherence to FAIR principles, comparing the FAIRness scores of the original data domains with those processed through AutoFAIR. Different from manual domain-oriented processing methods, the automated workflow of AutoFAIR enables it to efficiently process data webpages adaptively, covering a wide range of domains, and improving the FAIRness of data. However, the improvements achieved through our FAIRification process are limited and depend on the data information presented on the original webpages."}, {"title": "Findable and Accessible", "content": "Based on the powerful information extraction capability of Web Reader, we standardize the extraction of fields into metadata profiles that adhere to the FAIR principles. This effort enhances the findability and accessibility of data, as the FAIR metadata profiles contain information such as data titles, descriptions, keywords, and in some cases, references to other datasets. This improvement is showcased through the data website built on AutoFAIR."}, {"title": "Interoperable and Resuable", "content": "The interoperability of data has revolutionized the landscape of large-scale data analysis, enabling machine-based processing through metadata documents. Leveraging the capabilities of the AutoFAIR, we have pioneered the development of dataset spatiotemporal mapping webpages within the realm of mountain hazard datasets. By employing a standardized metadata vocabulary, we have effectively mapped dataset descriptions onto spatiotemporal maps, thus facilitating seamless retrieval based on temporal or spatial parameters. Moreover, our extensive metadata resources empower users to filter and access desired data efficiently, whether by keywords or research institution affiliations."}, {"title": "The Information Extraction Ability of Web Reader", "content": "The effectiveness of our AutoFAIR tool depends on the information extraction capability of Web Reader. demonstrates the extraction capability of Web Reader across 7124 websites, using data DOI and license fields as examples. It can be observed that Web Reader can accurately extract information from virtually all domains.\nAdditionally, through the institution information extraction of Web Reader, we have successfully identified the top data research and publishing institutions in the field of mountain hazard."}, {"title": "CONCLUSION", "content": "In conclusion, AutoFAIR presents a comprehensive solution for automating data FAIRification, significantly enhancing data compliance with the FAIR principles. By integrating the Web Reader's two-stage information extraction method, which considers both DOM tree structures and textual semantics, and the FAIR Alignment module, which utilizes ontology guidance and semantic matching, AutoFAIR effectively extracts and standardizes metadata across diverse and unstructured data sources. The application of AutoFAIR in the domain of mountain hazards demonstrated notable improvements in data FAIRness, proving the system's effectiveness. Future work could extend AutoFAIR's applicability to other domains and refine its capabilities to manage complex data structures, leveraging advanced natural language processing and machine learning techniques to further optimize metadata extraction. This work contributes to the broader adoption of FAIR principles, facilitating data sharing and promoting innovation across various fields."}]}