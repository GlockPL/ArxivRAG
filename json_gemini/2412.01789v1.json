{"title": "FROM CHEBNET TO CHEBGIBBSNET", "authors": ["Jie Zhang", "Min-Te Sun"], "abstract": "Recent advancements in Spectral Graph Convolutional Networks (SpecGCNs) have led to state-of-the-art performance in various graph representation learning tasks. To exploit the potential of SpecGCNs, we analyze corresponding graph fil-ters via polynomial interpolation, the cornerstone of graph signal processing. Different polynomial bases, such as Bernstein, Chebyshev, and monomial basis, have various convergence rates that will affect the error in polynomial interpolation. Although adopting Chebyshev basis for interpolation can minimize maximum er-ror, the performance of ChebNet is still weaker than GPR-GNN and BernNet. We point out it is caused by the Gibbs phenomenon, which occurs when the graph frequency response function approximates the target function. It reduces the approximation ability of a truncated polynomial interpolation. In order to miti-gate the Gibbs phenomenon, we propose to add the Gibbs damping factor with each term of Chebyshev polynomials on ChebNet. As a result, our lightweight approach leads to a significant performance boost. Afterwards, we reorganize ChebNet via decoupling feature propagation and transformation. We name this variant as ChebGibbsNet. Our experiments indicate that ChebGibbsNet is su-perior to other advanced SpecGCNs, such as GPR-GNN and BernNet, in both homogeneous graphs and heterogeneous graphs.", "sections": [{"title": "INTRODUCTION", "content": "High dimensional data are ordinarily represented as graphs in a variety of areas, such as citation, energy, sensor, social, and traffic networks. Consequently, Graph Neural Networks (GNNs), also known as CNNs on graphs, have shown a tremendous promise.\nSince the emergence of GCN (Kipf & Welling, 2017), numerous GNNs have been developed to generalize convolution operations on graphs. Graph convolution is based on graph signal process-ing (Shuman et al., 2013; 2016), where the graph filter is a crucial component. A graph filter is a matrix which processes a graph signal by amplifying or attenuating its corresponding graph Fourier coefficients. Generally, a graph filter is a combination of different powers of a graph shift operator, which is a normalized adjacency matrix or Laplacian matrix.\nRecently, enlightened by graph diffusion (Vigna, 2016; Masuda et al., 2017), GNNs such as SGC (Wu et al., 2019), APPNP (Klicpera et al., 2019), S2GC (Zhu & Koniusz, 2021), GPR-GNN (Chien et al., 2021), and BernNet (He et al., 2021) focus on designing graph filters, and demonstrate strong performance in node classification. We call those GNNs as Spectral Graph Con-volutional Networks (SpecGCNs). In order to exploit the potential of SpecGCNs, we summarize those SpecGCNs into a single architecture. We find most of those SpecGCNs' graph filters are monomials basis, with the exception of ChebNet (Defferrard et al., 2016) and BernNet. Besides, except GPR-GNN and BernNet, other SpecGCNs' graph filters are accompanied with fixed coeffi-cients.\nNext, we point out that the Gibbs phenomenon (Hewitt & Hewitt, 1979; Jerri, 1998), which troubles signal processing, is also a tricky issue for SpecGCNs. When the target graph frequency response function is discontinuous or singular at some points in the polynomial interpolation interval, the Gibbs phenomenon will occur around discontinuities or singularities. Consequently, polynomial-"}, {"title": "RELATED WORK", "content": "Spectral Graph Convolutional Networks. Based on graph signal processing, ChebNet (Defferrard et al., 2016) is proposed with a localized graph filter. ChebNet is the first SpecGCN with graph filter modification, which utilizes Chebyshev polynomials. Then GCN (Kipf & Welling, 2017) improves ChebyNet by proposing a method called the renormalization trick which inspires several SpecGCNs to design filters. As a simplified version of multi-layer GCN, SGC (Wu et al., 2019) eliminates nonlinear activation functions and Dropout method (Srivastava et al., 2014) in order to retain performance and achieve the same results as GCN.\nSince graph convolution is related to graph signal processing, several researchers have tried to design SpecGCNs from the view of devising graph filters. One common approach is focusing on graph dif-fusion (Vigna, 2016; Masuda et al., 2017). The Personalized PageRank filter (Jeh & Widom, 2003) is a well-known graph diffusion filter adopted by APPNP (Klicpera et al., 2019). Subsequently, S2GC (Zhu & Koniusz, 2021) adopts Markov Diffusion kernel (Fouss et al., 2006) to demonstrate its graph performance in the node classification task. Afterwards, based on the Generalized PageR-ank filter (Baeza-Yates et al., 2006; Gleich, 2015) with learnable coefficients, GPR-GNN (Chien et al., 2021) shows its high performance in both homogeneous graphs and heterogeneous graphs. Hereafter, BernNet (He et al., 2021) utilizes Bernstein polynomials to adaptively learn any graph filter.\nGraph Signal Processing and Polynomial Interpolation. As a branch of signal processing, the graph signal processing is based on polynomial interpolation, a mathematical approach that utilizes a finite polynomial to approximate any target function. In traditional signal processing, a polynomial-based function interpolates nodes, which are sampled from an interval, to approximate target signal function. As for graph signal processing, the corresponding sampling nodes are eigenvalues of a graph shift operator. Due to high time complexity of eigendecomposition, directly obtaining eigen-values would not be generally executed. Hence, how to design an appropriate graph filter is a challenge for SpecGCNs.\nKernel Polynomial Method and Gibbs Damping Factors In some areas of physics, such as ther-modynamics and quantum mechanics, the study of the eigenfunctions of a dynamical matrix or Hamiltonian operator is crucial. Based on polynomial interpolation, kernel polynomial method is invented as a core component for above studies (Wei\u00dfe et al., 2006). The approach called Cheby-shev expansion with modified moments, i.e., Chebyshev polynomials with Gibbs damping factors, is introduced to mitigate Gibbs oscillations while kernel polynomial method is widely applied in physics."}, {"title": "PRELIMINARY AND BACKGROUND", "content": "\u041d\u043e\u043cOGENEOUS GRAPHS AND HETEROGENEOUS GRAPHS\nA undirected graph G is represented as G = {V,E}, where V = {Vo, V1, ..., Vn\u22121} is the set of ver-tices with |V| = n, and E \u2286 V \u00d7 V is the set of edges. Let A \u2208 Rnxn denote the adjacency matrix of G. Given two nodes u and v, A(u, v) = 1 if there is an edge between node u and node v. Otherwise, A(u, v) = 0. The diagonal degree matrix D \u2208 Rn\u00d7n is obtained by D(u,u) = \u2211v\u03b5\u03bd A(u, v). Then, the combinatorial Laplacian (Chung, 1997) is defined as L = D \u2013 A for an undirected graph G. The symmetric normalized Laplacian matrix is defined as L = In \u2013 D-AD\u00af\u00bd. We denote the symmetric normalized adjacency matrix as A = D\u00afAD-\u00bd.\nGraphs can be either homogeneous or heterogeneous. The homophily and heterophily of a graph are used to describe the relation of labels among nodes. A homogeneous graph is a graph where the labels of all the nodes are consistent. On the contrary, in a heterogeneous graph, labels for nodes are of different types. The node homophily index for a graph (Pei et al., 2020) is denoted as\nDefinition 3.1 (Node Homophily).\n\\(H_{node}(G) = \\frac{1}{|V|} \\sum_{v \\in V} \\frac{|\\{v | v \\in N_u, Y_v = Y_u \\}|}{|N_u|}\\),\nwhere Nu is the neighbor set of node u and Yu is the label of node u.\nNote that Hnode (G) \u2192 1 indicates strong homophily and vice versa.\nGRAPH SIGNAL PROCESSING\nA column feature vector x in graph signal processing (GSP) is called a graph signal (Sandryhaila & Moura, 2013; 2014). A graph shift operator (GSO) (Sandryhaila & Moura, 2013; 2014) is a matrix which defines how a graph signal is shifted from one node to its neighbors based on the graph topology. More specifically, GSO is a local operator that replaces graph signal value of each node with linear combination of its neighbors'. In graph signal processing, it is common to take a normalized adjacency matrix or Laplacian matrix as a GSO.\nA function h(\u00b7) of an eigenvalue \u5165j, where j \u2208 [0, n \u2212 1], as h(Aj) is called the graph frequency re-sponse function. It is a discrete function, which extends the convolution theorem from digital signal processing to graphs. For a normalized Laplacian matrix, its graph frequency response function is defined as h(x) = 1;. For a normalized adjacency matrix, its graph frequency response function is h(x) = 1 \u2013 Aj. We denote g(j) as graph shifting for an eigenvalue Aj.\nA graph filter H(S) \u2208 Cnxn (Sandryhaila & Moura, 2013; 2014) is a function of a graph shift operator S. Apparently, a GSO is a graph filter. In graph signal processing, it is common to utilize a polynomial-based graph filter which is defined as H(S) = \u2211k=0 $kSk, where \u03dak is the correspond-ing coefficient. This kind of graph filter is named Moving-Average (MA) filter (Isufi et al., 2017), which is also named Finite Impulse Response (FIR) filter. The capacity of a SpecGCN with an FIR filter is determined by the degree k of a polynomial. We denote it as a MAK or FIRK filter.\nFor an undirected graph G, its graph filter can be eigendecomposed as H(S) = UAU*, where U \u2208 Rn\u00d7n is a matrix of orthogonal eigenvectors, A = diag ([h(g(\u03bb\u03bf)), ..., h(g(An\u22121))]) \u2208 Rn\u00d7n is a diagonal matrix of filtered eigenvalues, and * means conjugate transpose. Since U is real-valued, we have U* = UT.\nBased on the theory of graph signal processing, the graph Fourier transform for a graph signal vector x on an undirected graph is defined as x = U*x, and the inverse graph Fourier transform is x = Ux. Given a graph filter H(S) and a feature matrix X, the convolution operator on a graph is defined as H(S) *G X = U ((U*H(S)) \u2299 (U*X)) = Uh(g(A))U*X = \u03a3= h(g(x))uiui*, where h(g(A)) is a matrix form of the graph frequency response function, and uo, u1, ..., un\u22121 are eigenvectors of U."}, {"title": "FROM CHEBNET TO GCN", "content": "ChebNet is the first SpecGCN with a localized graph filter based on Chebyshev polynomials. Through three-term recurrence relations, Chebyshev polynomials can be obtained as follows.\nDefinition 3.2 (Chebyshev Polynomials of the first kind).\n\\(T_k(x) = \\begin{cases}1 & \\text{if } k = 0, \\\\x & \\text{if } k = 1, \\\\2xT_{k-1}(x) - T_{k-2}(x) & \\text{if } k \\geq 2,\\end{cases}\\)\nwhere x \u2208 [-1,1].\nSince eigenvalues of a normalized Laplacian L are in [0, 2], consider the orthogonality of Cheby-shev polynomials, it is forbidden to directly replace x by L. Hence, the scaled normalized Lapla-cian L = 2L \u2013 In is proposed, where Amax denotes the maximized eigenvalue of the nor-malized Laplacian. In the original definition of ChebNet, a graph convolution layer is defined as H(C)X = 0CkT(C)X, where ck denotes the k-th Chebyshev coefficient. In practice, the graph convolution layer is defined as\n\\(Z^{(l+1)} = \\sigma(\\sum_{k=0}^K T_k(\\tilde{L})Z^{(l)}W^{(l)}),\\)\nwhere \u03c3(\u00b7) is a non-linear activation function such as ReLU (Nair & Hinton, 2010), Z(l) denotes the l-th hidden layer, and Z(0) = X. The Chebyshev coefficient vector is replaced by a learnable weight matrix W(l). This operation bewilders numerous researches who deem the k-th Chebyshev coefficient is fixed as ck = 1, and misinterpret Z(l)W(l) as feature transformation.\nGCN. To simplify ChebNet, a linear version named GCN is proposed in (Kipf & Welling, 2017). Furthermore, the renormalization trick is proposed. It is defined as \u0100 = \u010e\u00af\u00bd(A + nI\u33a5)\u010e\u2212\u00bd, where D(u, u) = \u03a3\u03c5\u03b5\u03bd(A + nIn)(u, v), and \u03b7 = 1 in general. Then, a graph convolution layer of GCN is defined as Z(1+1) = \u03c3(\u0100Z(1)W(1))"}, {"title": "PROPOSED METHOD", "content": "POLYNOMIAL INTERPOLATION FOR SPECTRAL GRAPH CONVOLUTIONAL NETWORKS\nBy decoupling feature transformation and propagation, SpecGCNs can be generalized into a single architecture as\n\\(Y_{SpecGCN} = Softmax(H(S) \\cdot f_\\theta(X)),\\)\nwhere fe(X) is an Multi-Layer Perceptron (MLP). In this unified architecture, the effectiveness of the graph filter is accentuated. Then, we summarize the category of the basis and the corresponding coefficients of SpecGCNs we mentioned in Section 2 into Table 1. Next, we discuss the process of polynomial interpolation via the graph frequency response function.\nTheorem 4.1 (Weierstrass Approximation Theorem (Weierstrass, 2013)). Suppose f be a continu-ous function on [a, b]. For every \u0454 > 0, there exists a polynomial p such that || f (x) \u2013 p(x)||\u221e < \u20ac.\nTheorem 4.1 tells us that we can utilize any polynomial-based graph frequency response function to approximate the target function. The whole approximation process can be described into two steps. The first step is graph shifting, i.e., \u5165 \u2192 g(x). The second step is graph polynomial interpolation, which is formulated as follows."}, {"title": "CHEBYSHEV POLYNOMIAL INTERPOLATION", "content": "Definition 4.4 (Chebyshev Polynomial Interpolation). Given a target function f(x) and a Cheby-shev polynomial p with K orders, for x \u2208 [\u22121, 1], the target function f(x) can be approximated as\n\\(f(x) \\approx p(x) = \\mu_0 + \\sum_{k=1}^K \\mu_k T_k(x) \\approx \\frac{1}{2} \\mu_0 + \\sum_{k=1}^K \\mu_k T_k(x),\\)\nwhere\n\\[\\mu_k = \\frac{2}{\\pi} \\int_{-1}^1 \\frac{f(x)T_k(x)}{\\sqrt{1 - x^2}} dx \\approx \\frac{2}{K+1} \\sum_{j=0}^K f(x_j) T_k(x_j),\\]\nis the Chebyshev coefficients, and xj is a sampling Chebyshev node.\nSince the target graph frequency response function is unknown, we can replace the target eigenvalue f(x) with a learnable parameter wj. It allows the model to simulate f(xj) via gradient descent. Furthermore, we can simplify \u03bc\u03b5 with a learnable parameter wk. This is the original purpose pro-posed in ChebNet, where the Chebyshev coefficient vector \u03bc = [\u03bc\u03bf, \u03bc1,\uff65\uff65\uff65, \u03bc\u03ba] is replaced by a learnable weight matrix W."}, {"title": "GIBBS PHENOMENON AND GIBBS DAMPING FACTORS", "content": "In real world datasets, the target graph frequency response function probably discontinuous or singu-lar in the polynomial interpolation interval. It will cause intense oscillations near discontinuities or singularities during approximation process. This phenomenon is known as the Gibbs phenomenon. We demonstrate it in Figure 4.3."}, {"title": "CHEBGIBBSNET AND ANALYSIS FROM SPECTRAL DOMAIN", "content": "By restructuring ChebNet with learnable coefficients and Gibbs damping factors into the architecture summarized in Eq. 4, we propose ChebGibbsNet.\nDefinition 4.5 (ChebGibbsNet).\n\\(Y_{ChebGibbsNet} = Softmax(\\sum_{k=0}^K w_k g_{k,K} T_k(S) \\cdot f_\\theta(X)),\\)\nwhere wk is a learnable coefficient with one initialization that represents the corresponding Cheby-shev coefficient \u03bc\u03ba, S = A for homogeneous graphs and S = -A for heterogeneous graphs. Besides, the self-gated activation function SiLU (Hendrycks & Gimpel, 2016; Elfwing et al., 2018; Prajit Ramachandran, 2018) is employed in fe(X) to stabilize performance.\nRecent research works (Wu et al., 2019; NT & Maehara, 2019; Chien et al., 2021) indicate that a low-pass filter or band-stop filter can handle homogeneous graphs, and a high-pass filter or band-pass filter can handle heterogeneous graphs. Thus, the node homophily index Hnode (G) is adopted in ChebGibbsNet to adaptively change the GSO. When Hnode(G) \u2208 (0.5, 1), it indicates that the graph is homogeneous. Then, A as a GSO is adopted in the model. If learnable coefficients are non-negative, ChebGibbsNet with Gibbs damping factors performs low-pass or band-stop filtering on graph signals. When Hnode (G) \u2208 (0,0.5), the graph filter of ChebGibbsNet with Gibbs damping factors becomes a high-pass or band-pass filter.\nThere is an issue in graph representation learning called the over-smoothing issue (Li et al., 2018; 2019), which prevents deep graph convolutional networks from gaining better performance. To elaborate the over-smoothing issue, we need to analyze the global smoothness of a graph filter. Thus, we introduce the spectral gap (Hoory et al., 2006) and the graph diffusion distance (Masuda et al., 2017)."}, {"title": "EXPERIMENTS", "content": "DATASETS AND EXPERIMENTAL SETUP\nDatasets. We use five different types graph datasets, three citation networks, three webpage net-works, two Wikipedia networks, an actor co-occurrence network, and a Wikipedia computer science network, for node classification tasks. The detailed statistics of those datasets are shown in Table 5. Notice that a Wikipedia computer science network and citation networks are homogeneous graphs, and an actor co-occurrence network, Wikipedia networks and webpage networks are heterogeneous graphs.\nCitation Networks. CoRA, CiteSeer, and PubMed are standard citation network benchmark datasets. In these networks, every node represents a paper and every edge represents a citation from one paper to another. The edge direction is defined from a citing paper to a cited paper. The feature is a vocabulary of unique words. We follow (Kipf & Welling, 2017) to preprocess the data into training, validation and test sets.\nWebpage Networks. It is a webpage dataset collected from computer science departments of vari-ous universities by Carnegie Mellon University (Lu & Getoor, 2003). In WebKB, there are 3 datasets named Cornell, Texas, and Wisconsin. Each node represents a webpage and each edge represents a hyperlink between two webpages. The edge direction is from the original webpage to the referenced webpage. The feature of each node is the bag-of-words representation of the corresponding page. For webpage networks, in order to evaluate supervised graph representation learning, we follow previous work (Pei et al., 2020) to randomly split nodes of each class into 60%, 20%, and 20% for training, validation, and test sets, respectively."}, {"title": "CONCLUSION AND FUTURE WORK", "content": "In this research, we utilize the property of Chebyshev polynomials and Gibbs damping factors to propose ChebGibbsNet. We test our model in both homogeneous graphs and heterogeneous graphs. The experimental results demonstrate that ChebGibbsNet is widely applicable for realistic datasets. It verifies our assumption that Gibbs oscillations weaken the performance of ChebNet. Besides, our analysis for polynomial interpolation in graph signal processing illustrates that Chebyshev poly-nomials have a faster convergence rate than Bernstein polynomials. Out of curiosity, we will tap potentials of other orthogonal polynomials for exploiting the future of graph representation learn-ing."}]}