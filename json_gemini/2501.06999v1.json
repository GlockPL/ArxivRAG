[{"title": "LIKELIHOOD TRAINING OF CASCADED DIFFUSION MODELS VIA HIERARCHICAL VOLUME-PRESERVING MAPS", "authors": ["Henry Li", "Ronen Basri", "Yuval Kluger"], "abstract": "Cascaded models are multi-scale generative models with a marked capacity for producing perceptually impressive samples at high resolutions. In this work, we show that they can also be excellent likelihood models, so long as we overcome a fundamental difficulty with probabilistic multi-scale models: the intractability of the likelihood function. Chiefly, in cascaded models each intermediary scale introduces extraneous variables that cannot be tractably marginalized out for likelihood evaluation. This issue vanishes by modeling the diffusion process on latent spaces induced by a class of transformations we call hierarchical volume-preserving maps, which decompose spatially structured data in a hierarchical fashion without introducing local distortions in the latent space. We demonstrate that two such maps are well-known in the literature for multiscale modeling: Laplacian pyramids and wavelet transforms. Not only do such reparameterizations allow the likelihood function to be directly expressed as a joint likelihood over the scales, we show that the Laplacian pyramid and wavelet transform also produces significant improvements to the state-of-the-art on a selection of benchmarks in likelihood modeling, including density estimation, lossless compression, and out-of-distribution detection. Investigating the theoretical basis of our empirical gains we uncover deep connections to score matching under the Earth Mover's Distance (EMD), which is a well-known surrogate for perceptual similarity. Code can be found at this https url.", "sections": [{"title": "1 INTRODUCTION", "content": "A widely used strategy for generating high resolution images x is to first draw from a low resolution model po (z(1)), then refine this representation with a series of super-resolution models pe (z(s) |z(s\u22121)) for k = 2, ..., S where z(s) = x. This idea, known as cascaded or multi-scale modeling, has been employed to great effect in generative models at large (Oord et al., 2016; Karras et al., 2017; De Fauw et al., 2019), as well as diffusion models in particular (Ho et al., 2022). However, multi-scale models pose a fundamental issue for likelihood computation, as the modeled likelihood function obtained as a product of all conditional super-resolution models and the base model is now the joint likelihood pe (z(1), . . ., z(K)) rather than the desired likelihood pe(x) = pe(z(K)). This prevents cascaded models from being used for likelihood-based training and inference on x. For pe (x) to regain exact evaluation capabilities, the intermediate resolutions must must either be marginalized out - a very expensive operation in high dimensions - or undergo complex redesign to avoid such marginalization (Menick & Kalchbrenner, 2018; Reed et al., 2017).\nIn this work, we propose a different approach to overcoming this problem using a class of transforma- tions we call hierarchical volume-preserving maps. These are a special subset of homeomorphisms to which the likelihood function is invariant - i.e., a class of functions H such that for all h \u2208 H, po(x) = po(h(x)). Under these transformations, the joint likelihood computed by a cascading model coincides directly with the desired likelihood function pe(x). When these transformations are simple and produce useful hierarchical representations of x, we are able to retain maximum likelihood"}, {"title": "2 RELATED WORK", "content": "Likelihood Training The maximum likelihood training of generative models involves designing a model with a tractable likelihood pe (x) - i.e., the probability that a model with parameters 0 generates an image x and maximizing this function with respect to 0. This practice can be traced back to the wake-sleep algorithm (Hinton et al., 1995), which is used to select parameters for deep belief networks and Helmholtz machines (Dayan et al., 1995). More modern treatments include variational autoencoders (VAEs) (Kingma & Welling, 2013), normalizing flows (NFs) (Dinh et al., 2014), and autoregressive models (ARMs) (Van Den Oord et al., 2016; Salimans et al., 2017). Diffusion models also fit into this category, as they were originally formulated in a variational Bayesian framework (Sohl-Dickstein et al., 2015). However, they are often trained with a non-probabilistic loss function (Ho et al., 2020; Song et al., 2020b) due to the instability of the originally derived loss. Recently, (Song et al., 2021) explore likelihood training in a frequentist setting, whereas (Chen et al., 2021) do so via the lens of the Schrodinger Bridge problem and Forward-Backward SDES, while (Kingma et al., 2021) introduce several modifications to the originally derived probabilistic loss function that greatly improve its stability during training and inference. But there is to our knowledge no known likelihood-based treatment of diffusion models in a multi-scale setting, to which we turn our attention.\nMultiscale Generative Models Hierarchical modeling of spatially structured data with progres- sively growing resolution scales has seen widespread use across image (Karras et al., 2017; Child, 2020; Han et al., 2022) and audio (Oord et al., 2016) domains. A hierarchical structure encourages the data synthesis task to be split into smaller sequential steps, allowing models to learn spatial correlations at each resolution scale separately, and counteracts the tendency to overly focus on local structure (De Fauw et al., 2019). In normalizing flows, (Yu et al., 2020) construct a multiscale flow by leveraging the orthogonality of the wavelet transform, which can be seen as a volume-preservation condition on linear equidimensional transforms our Eq. 8 is a generalization of this observation. In diffusion models, (Ho et al., 2022); (Guth et al., 2022; Wang et al., 2022) formulate a non-probabilistic hierarchical framework for diffusion models and demonstrate its effectiveness in generating high resolution images at impressive levels of perceptual fidelity. In spite of these advances, likelihood training of multi-scale models is generally unwieldy, requiring expensive marginalization, loose vari- ational bounding, or complex factorization schemes where pixels in low-resolution images must be directly utilized as pixels in the higher resolution images (Reed et al., 2017; Menick & Kalchbrenner, 2018).\nDiffusion Modeling with Earth Mover's Distances We also show that likelihood training with our hierarchical model approximates score matching with an optimal transport cost. Existing works have also noted relationships between diffusion modeling and optimal transport (De Bortoli et al., 2021; Chen et al., 2021; Lipman et al., 2022; Shi et al., 2022; Kwon et al., 2022), showing that diffusion models learn to generate data by transporting samples x \u2208 Rd along paths that minimize a cost between the prior p(x\u012b) ~ N(0, I) and data distribution p(x0). Our result is markedly different: we show that our model learns the solution to an optimal transport problem in the two-dimensional coordinate space of pixel histograms. That is, we consider the pixel values of each image as an unnormalized and discretized histogram on a square in R2, and optimize a transport cost over this grid. This metric on images (as well as other spatial data) is well-studied, and known to correlate well with perceptual similarity (Rubner et al., 2000; Zhang et al., 2020; Zhao et al., 2019). Our perpespective is thus orthogonal and complementary to prior work."}, {"title": "3 BACKGROUND", "content": "Diffusion models (Ho et al., 2020; Song et al., 2020b) are inspired by non-equilibrium thermodynam- ics (Sohl-Dickstein et al., 2015) designed to model po(x) = \u222b po(x0:T) dx1:T where data x0 := x are related to a set of latent variables x1:T by a diffusion process. Namely, x1:T := (x(t1),...,x(tr)) are distributed as marginals of a process governed by an It\u00f4 stochastic differential equation (SDE)\n dx = f(x, t) dt + g(t) dw \n(1)\nevaluated at times {tr}T=1. f and g are typically called drift and diffusion functions, and w is the standard Wiener process. Samples can then be generated by modeling the reverse diffusion, which"}, {"title": "4 HIERARCHICAL VOLUME-PRESERVING MAPS", "content": "Due to the intractability of the likelihood function in many hierarchical models, we are interested in multi-scale decompositions of x that can sidestep the need for marginalization during likelihood computation. In this section, we shall demonstrate the existence of a subset of homeomorphisms H we call hierarchical volume-preserving maps to which the likelihood function pe displays a form of probabilistic invariance - i.e., po(x) = po(h(x)) for all h \u2208 H. The utility of such transformations is clear: if h(x) is the scales of the hierarchical model, then we can directly use the joint likelihood over these scales as the desired likelihood function. Finally, we consider two simple homeomorphisms that satisfy our proposed condition: the Laplacian pyramid and wavelet transforms."}, {"title": "4.1 A PROBABILISTIC INVARIANCE", "content": "To build towards this result, we review the notion of volume-preserving maps, which can be under- stood as transformations that are constrained with respect to a local measure of space expansion or contraction, the matrix volume.\nDefinition 1 (Volume-preserving Maps (Berger & Gostiaux, 2012)). A homeomorphic (i.e., smooth, invertible) transformation h : X \u2192 Z is known to be volume-preserving if it everwhere satisfies\n \\sqrt{ det (A^T A) } = 1, \n(8)\nwhere det(\u00b7) is the determinant and A \u2208 Rdim(Z)\u00d7dim(X) is the Jacobian of h(x).\nCrucially, dim(Z) may be larger than dim(X) \u2014 h may be a manifold-valued function, so long as A is full rank (i.e. rank(A) > dim(Z)). Our notion of hierarchical volume-preserving maps further requires that the transformation is multi-scale (Mallat, 1999; Burt & Adelson, 1987; Horstemeyer, 2010), i.e., h : X \u2192 Z(1) x... x Z(S), where the original data x \u2208 X is decomposed into a sequence of representations z(s) \u2208 Z(s) that describe the data at varying scales an essential property for cascaded modeling. The following result reveals the utility of working with such transformations.\nLemma 4.1 (Probabilistic Invariance of Hierarchical Volume-preserving Maps). Let h be a hierarchi- cal volume-preserving map such that h(x) = (z(1), z(2), . . ., z(S)), and po be a likelihood function on z(1), z(2),..., z(S). Then the likelihood function with respect to the original data po(x) can be recovered by the simple relation\n log p_\\theta (x) = log p_\\theta[h(x)]. \n(9)\nIn other words, the joint likelihood given by the cascaded model under a hierarchical volume- preserving map is precisely the desired likelihood function with respect to the data pe (x), and we no longer require any additional computation or design considerations to remove the intermediary scales. In this sense, Eq. 9 displays a probabilistic invariance to h 1.\nStandard Cascaded Hierarchy We now demonstrate how the standard hierarchy of a cascaded model such as in (Karras et al., 2017) or (Ho et al., 2022) is not a hierarchical volume-preserving map. Here, the scales z(1), z(2), . . ., z(s) are subsampled versions of the full-resolution sample x, with each scale twice the resolution of the preceding scale. Consider a simple illustrative example in Rd. When using the nearest neighbor resampling method, h may be defined as the concatenation of S different identity-like functions\n h = [I_{d\\times d}, 2^{-1} \\cdot I_{\\frac{d}{2}\\times \\frac{d}{2}}, 2^{-2} \\cdot I_{\\frac{d}{2^2}\\times \\frac{d}{2^2}}, ..., 2^{-(S-1)} \\cdot I_{\\frac{d}{2^{S-1}}\\times \\frac{d}{2^{S-1}}}], \n(10)\nwhere dk := d/2k and Iaxa denotes a d \u00d7 d identity matrix where each column is repeated l times. Inspecting this operator we find that its determinant grows exponentially with d. Therefore, while hierarchical, the standard transformation is certainly not volume-preserving. Accordingly, as has been established, there is no tractable form for the likelihood function po(x) in terms of the joint likelihood pe (z(1), z(2), ..., z(S))."}, {"title": "4.2 SPECIAL INSTANCES OF HIERARCHICAL VOLUME-PRESERVING MAPS", "content": "While our formulation is fully general for arbitrary homeomorphisms h that satisfy Eq. 8, we highlight two special and well-known classes of maps that are particularly noteworthy for their ease of implementation, computational tractability, and theoretical properties (Section 5.2).\nLaplacian Pyramids Though the standard cascaded hierarchy is not a hierarchical volume- preserving map, it is closely related to the Laplacian pyramid. This representation involves a set of band-pass images, each spaced an octave apart, obtained by taking differences between a se- quence of low-pass filtered images with their unfiltered counterparts. Formally, consider base images"}, {"title": "5 LIKELIHOOD TRAINING OF CASCADED DIFFUSION MODELS", "content": "In this section, we construct the probabilistic cascaded diffusion model (PCDM), which is a multi- scale hierarchical model capable of likelihood training and inference. We then investigate the theoretical properties of our model, and uncover a rigorous connection between likelihood training and score matching with respect to an optimal transport cost."}, {"title": "5.1 COUPLED DIFFUSION MODELING ON A h-INDUCED LATENT SPACE", "content": "Letting h : X \u2192 Z(1) x ... \u00d7 Z(S) be a hierarchical volume-preserving map (where x \u2208 X and z(s) \u2208 Z(s)), we return to the cascaded diffusion model on h(x) = (z(1), z(2), . . ., z(s)) introduced in Section 3, which is defined by way of a standard unconditional diffusion model po(z(1)) and a sequence of conditional super-resolution diffusion models pe (z(s)\\z(<s)). This produces S coupled diffusion processes, where each scale z(s) is governed by its own It\u00f4 SDE with dynamics that depend on the processes that precede it.\nBy Lemma 4.1, we see that the desired likelihood function po(x) can be recovered by the simple relation\n log p_\\theta (x) = log p_\\theta (z^{(1)}) + \\sum_{s=2}^{S} log p_\\theta (z^{(s)} | z^{(<s)}). \n(15)\nLikelihood training then proceeds by lower bounding each likelihood with its corresponding loss function L(z(1)) as in Eq. 5 for the unconditional base model, and\n  L(z^{(s)} | z^{(<s)}) = \\mathbb{E}_{x_{1:T} \\sim q} \\bigg[log p_\\theta(z_0^{(s)}|z_1^{(s)}, z_0^{(<s)}) - KL(q(z_T^{(s)} | z_0^{(s)}, z_0^{(<s)})||p_\\theta(z_T^{(s)} | z_0^{(<s)})) \\\n                                         - \\sum_{k=1}^{T-1} log KL(q(z_k^{(s)} | z_{k+1}^{(s)}, z_0^{(s)}, z_0^{(<s)})||p_\\theta(z_k^{(s)} | z_{k+1}^{(s)}, z_0^{(<s)}))\\bigg], \n(16)\nfor the conditional super-resolution models. Combining all terms, the full training loss of our cascaded diffusion model may be written as\n C(x) = \\mathcal{L}(z^{(1)}) + \\sum_{s=2}^{S}\\mathcal{L}(z^{(s)} | z^{(<s)}), \n(17)\nover all x in the dataset."}, {"title": "5.2 A CONNECTION TO OPTIMAL TRANSPORT", "content": "A notable property of distances in the latent spaces induced by maps from Section 4.2 is their connection to an optimal transport cost. We begin by introducing the Wasserstein distance on the image domain.\nDefinition 2 (Wasserstein p-Metric). Let x, y \u2208 RH\u00d7W be images, which we interpret as unnormal- ized densities on the product space of pixel indices {h}H=1 \u00d7 {w}W=1. The Wasserstein p-metric can be written as\n W_p(x, y) = \\bigg( inf_{v} \\sum_{i,j,k,l} v(i, j, k, l)||x_{ij} - y_{kl}||^p \\bigg)^{1/p} \n(18)\nwhere xij (or yij) is the i, jth pixel of x (or y) and v(i, j, k, l) is a scalar-valued function such that \u2211i,j v(i, j, k, l) = xke and \u2211k,ev(i, j, k,l) = xij.\nOur next statement builds on prior theoretical work in linear-time approximations to the Earth Mover's Distance (Shirdhonkar & Jacobs, 2008).\nTheorem 5.1 (Cascaded Diffusion Modeling and EMD Score Matching). Let h be one of the hierarchical volume-preserving maps defined in Section 4.2, and po be a cascaded diffusion model on h(x) = (z(1), z(2),..., z(S)). Then there exists a constant a depending only on the map h such that\n aC(x) \\geq \\sum_{k=1}^{T-1} \\mathbb{E}_{x_k \\sim q(x_k|x_0)} [W_p(\\nabla_x log q(\\Tilde{x}_k|x_0), s_\\theta(\\Tilde{x}_k, t_k))], \n(19)\nwhere wk is the likelihood weighting of the variational lower bound, q(xk|x0) is the marginal of the forward diffusion process, and 0 < p < 1."}, {"title": "6 EXPERIMENTS", "content": "We evaluate both the the Laplacian pyramid-based and wavelet-based variants of our proposed probabilistic cascading diffusion model (LP-PCDM and W-PCDM, respectively) in several settings. First, we begin on a general density estimation task on the CIFAR10 (Krizhevsky et al., 2009) and ImageNet 32, 64, and 128 (Van Den Oord et al., 2016) datasets. Then we investigate the anomaly detection performance of our models with an out-of-distribution baseline where the model is tasked to differentiate between in-distribution (CIFAR10) and out-of-distribution (SVHN (Netzer et al., 2011), uniform, and constant uniform) data. Finally, we evaluate our model on a lossless compression benchmark against several competitive baselines in neural compression literature."}, {"title": "6.1 LIKELIHOOD AND SAMPLE QUALITY", "content": "We adapt the architecture and training procedure in (Kingma et al., 2021) to our framework (Appendix B), and compare our model to existing likelihood-based generative models in terms of bits per dimension (BPD) on standard image-based density estimation datasets: CIFAR10, ImageNet 32x32,"}, {"title": "6.2 OUT-OF-DISTRIBUTION DETECTION", "content": "Finally, we evaluate the out-of-distribution (OOD) detection capabilities of our proposed model. In this problem, a probabilistic model is given a set of in-distribution (i.e., training) and out-of- distribution (i.e., outlier) points, and tasked with discriminating between the two distributions in an unsupervised manner. Likelihood-based models are surprisingly brittle under this benchmark (Hendrycks et al., 2018; Nalisnick et al., 2018; Choi et al., 2018; Shafaei et al., 2018), even though likelihoods pe (x) permit an intuitive interpretation for inlier/outlier classification. We use as our OOD statistic the typicality test ho(x) = |M=1 -log po(xm) \u2013 H(pe(x))| proposed in (Nalisnick et al., 2019), where log po (xm) is approximated by our likelihood bound C(x) from Eq. (17). We compute our statistic under the most difficult scenario of M = 1. Following (Du & Mordatch, 2019; Meng et al., 2020), we evaluate our models against the Street View House Numbers (SVHN) (Netzer et al., 2011), constant uniform, and constant distributions as OOD distributions. Table 2 shows that we obtain significant gains in OOD performance. More experimental details are in Appendix B.1."}, {"title": "6.3 LOSSLESS PROGRESSIVE CODING", "content": "As discussed in (Ho et al., 2020; Kingma et al., 2021; Hoogeboom et al., 2021), a probabilistic diffusion model can also be seen as a latent variable model in a neural network- based lossless compression scheme. We implement a neural compressor us- ing our proposed model as the latent variable model in the Bits-Back ANS algorithm (Townsend et al., 2019), and compare against several promi- nent models in the field, demonstrat- ing significant reductions in compres- sion size on the CIFAR10 dataset. We report performance in terms of average bits per dimension of the codelength over the entire dataset in Table 3. One downside of a bits back based implementation is that there is a significant overhead of bits required to initialize the algorithm, and encoding and decoding must be performed in a fixed sequence decided at encoding-time, meaning that single image compression is expensive. This limitation can be removed by implementing a direct compressor (Hoogeboom et al., 2021), which can also be applied to our model. We leave this avenue of research for further work."}, {"title": "7 CONCLUSION AND FUTURE DIRECTIONS", "content": "We demonstrated the existence of a class of transformations we call hierarchical volume-preserving maps that sidesteps a common difficulty in multi-scale likelihoood modeling, the intractability of the likelihood function po(x). Hierarchical volume-preserving maps are homeomorphisms to which the likelihood function are invariant, allowing the likelihood function pe (x) to be directly computed as a joint likelihood on the scales. We then presented two particular instances of these transformations that warrant attention due to their simplicity and performance on empirical benchmarks. Finally, we demonstrate a connection between our training and optimal transport. An open question is whether score matching under an EMD norm enjoys the same statistical guarantees as the standard score matching, e.g., consistency, efficiency, and asymptotic normality (Hyv\u00e4rinen, 2006; Song et al., 2020a). We hope that this work paves the way for future inroads between hierarchical and likelihood-based modeling, and that our theoretical framework opens the door for the further diversity and improvements in the design of diffusion models."}, {"title": "A PROOFS", "content": "Lemma 4.1 (Probabilistic Invariance of Hierarchical Volume-preserving Maps). Let h be a hierarchi- cal volume-preserving map such that h(x) = (z(1), z(2), . . ., z(S)), and po be a likelihood function on z(1), z(2), ..., z(S). Then the likelihood function with respect to the original data po(x) can be recovered by the simple relation\n log p_\\theta (x) = log p_\\theta[h(x)]. \n(9)\nProof. We begin with the generalized change-of-variables formula on Riemannian manifolds (Ben- Israel, 1999), which relates the density function on x with another on y = f(x) under the (potentially manifold-valued) transformation f via the relation\n p(x) = p(x|y) \\bigg| det ( \\frac{\\partial }{\\partial x} [f(x)]^T \\frac{\\partial }{\\partial x} [f(x)])\\bigg|. \n(20)\nConsidering now the likelihood function pe, letting f = h, and taking logs of both sides, we see that\n log p_\\theta(x) = log p_\\theta(z^{(1)}, z^{(2)},...,z^{(S)}) + log \\bigg| det ( \\frac{\\partial }{\\partial x} [h(x)]^T \\frac{\\partial }{\\partial x} [h(x)])\\bigg|\n = log p_\\theta (z^{(1)}, z^{(2)}, ..., z^{(S)}) + log |1|\n = log p_\\theta (h(x)),\nwhere the second equality follows due to the fact that h is volume-preserving. We have thus shown that the likelihood function pe(x) coincides with the joint likelihood pe (z(1), z(2), . . ., z(s)). This concludes the proof.\nTheorem 5.1 (Cascaded Diffusion Modeling and EMD Score Matching). Let h be one of the hierarchical volume-preserving maps defined in Section 4.2, and po be a cascaded diffusion model on h(x) = (z(1), z(2),..., z(S)). Then there exists a constant & depending only on the map h such that\n aC(x) \\geq \\sum_{k=1}^{T-1} \\mathbb{E}_{x_k \\sim q(x_k|x_0)} [W_p(\\nabla_x log q(\\Tilde{x}_k|x_0), s_\\theta(\\Tilde{x}_k, t_k))], \n(19)\nwhere wk is the likelihood weighting of the variational lower bound, q(xk|x0) is the marginal of the forward diffusion process, and 0 < p < 1.\nProof. According to (Ho et al., 2020), the likelihood bound Eq. 5\n \\mathcal{L}(z^{(1)}) > \\mathbb{E}_{x_{1:T} \\sim q} \\bigg[log p_\\theta(z_0 |z_1) - KL(q(z_T | z_0)||p_\\theta(z_T)) \\\n                                         - \\sum_{k=1}^{T-1} log KL(q(z_k | z_{k+1}, z_0)||p_\\theta(z_k | z_{k+1}))\\bigg], \n(21)\ncan be decomposed into the three elements: a decoder reconstruction term Lo(x), a prior loss term LT(x), and a weighted sum over the diffusion score matching terms =1Lk(x). The conditional likelihood bound Eq. 16 can be decomposed in a similar fashion:\n L(z^{(s)} | z^{(<s)}) > \\mathbb{E}_{x_{1:T} \\sim q}\\bigg[log p_\\theta(z_0^*|z_1^*, z_0^{(<s)}) - KL(q(z_T^* | z_0^*)||p_\\theta(z_T^* | z_0^{(<s)})) \\\n - \\sum_{k=1}^{T-1} log KL(q(z_k^* | z_{k+1}^*, z_0, z_0^{(<s)})||p_\\theta(z_k^* | z_{k+1}^*, z_0^{(<s)}))\\bigg] \n(22)"}, {"title": "B MODEL AND IMPLEMENTATION", "content": "All training is performed on 8x NVIDIA RTX A6000 GPUs. We construct our cascaded diffusion models with antithetic time sampling and a learnable noise schedule as in (Kingma et al., 2021). Our noise prediction networks ee also follow closely to the VDM U-Net implementation in (Kingma et al., 2021), differing only in the need to deal with the representations induced by the hierarchical volume-preserving map h. A hierarchical model with S scales is composed of S separate noise prediction networks. For each input x, our model decomposes x into its latent scales {z(s)}$_1 via h and directs each z(s) to its corresponding noise prediction network e\u0189(z(s)|z(<s), t). The noise prediction network can be related to the approximated score function via\n \\epsilon_\\theta(z^{(s)} | z^{(<s)}, t) = \\sigma(t)s_\\theta(z^{(s)}|z^{(<s)}, t), \n(42)\nwhere (t) is the variance of the diffusion process at time t. The base noise prediction network is constructed entirely identically to the VDM architecture, as it does not rely on any additional input. The conditional noise prediction networks at each subsequent scale additionally incorporate the conditioning information from previous scales (i.e., z(<s)) via a channel-wise concatenation of the conditional information to the input, resulting in the full input vector\n z_{input}^{(s)} = [z^{(s)}, z_{cond}^{(<s)}]\n z_{cond}^{(<s)} = (d \\circ d \\circ...\\circ d)(h^{-1}(z^{(<s)}, 0,..., 0))\n(43)\n(44)\nwhere h\u00af\u00b9 is the inverse of the hierarchical volume-preserving map, d() is a downsampling operator (Section 4.2), and z(<s) can be seen as the reconstructed image containing only the conditioning information available at scale s. Thus the noise prediction model at each scale ee(z(s)|z(<s), t) takes as input zinput and outputs z(s).\nIn the case where h is the wavelet transform, the scales z(<s) can directly be used to construct an image with the same extent as z(s), resembling a downsampled version of the original input x"}, {"title": "B.1 OUT-OF-DISTRIBUTION DETECTION EXPERIMENTAL DETAILS", "content": "As proposed in (Hendrycks et al., 2018; Nalisnick et al., 2018; Choi et al., 2018; Shafaei et al., 2018; Du & Mordatch, 2019; Meng et al., 2020), a benchmark for evaluating the anomaly detection capabilities of a likelihood model po(x) trained on true data x ~ p(x) can be framed as a binary classification task between in-distribution Xin ~ p(x) and out-of-distribution Xout ~ v(x) data. Namely, we define data-label pairs {xin,i, 0} in U {xout,j, 1} out where Nin = Nout. (Du & Mordatch, 2019; Meng et al., 2020) consider three different \"out-of-distribution\" densities, the Xout drawn from the Street View House Numbers (SVHN) dataset, the distribution of constant zeros (Const. Uniform), i.e., v = \u03b4\u03bf is the Dirac delta at the all-zeros vector, and the uniform distribution, i.e., v = Unif([-1,1]). The rows of Table 2 show classification performance under the AUROC operator."}, {"title": "CA VARIATIONAL LOWER BOUND WITHOUT THE VOLUME-PRESERVING PROPERTY", "content": "Our proposed hierarchical volume-preserving maps retain exact likelihood evaluation capabilities in all cascaded diffusion models with latent scales satisfying h(x) = (z(1)", "bound": "nlog p_\\theta(x) \\geq \\mathbb{E"}, {"x_{1": "T"}, "sim q} [log p_\\theta (x, z) - log q(z|x)"], "mathbb{E}_{x_{1": "T"}, ["log p_\\theta (x, z_0^{(1)},..., z_0^{(S)}) -log q(z_1:T^{(1)},..., z_1:T^{(S)}|x)"], {"E}_{x_{1": "T"}, ["log p_\\theta(x|z_0^{(1)}, ..., z_0^{(S)}) + \\sum_{s=1}^{S}log q(z_1:T^{(1)}|x)"], {"s=1}^{S}\\mathbb{E}_{x_{1": "T"}, ["log p_\\theta(z_0^{(s)}|z_1^{(s)}, z_0^{(<s)})) +log p_\\theta (z_0|z_1, x)\n+\\log p_\\theta (z_1|z_0) -log q(z|x)\n= \\sum_{s=1}^{S}\\mathbb{E}_{x_{1:T} \\sim q} [log p_\\theta(z_0^{(s)}|z_1^{(s)}, z_0^{(<s)})) +KL(q(z_T | z_0^{(<s)}) || p_\\theta (z_T | z_0^{(<s)}))\n+ KL(q(z^{(s)} | z, x)||p_\\theta (z_T | z_0^{(<s)})"], {"s=1}^{S}\\mathbb{E}_{x_{1": "T} \\sim q} [log p_\\theta(z_0^{(s)}|z_1^{(s)}, z_0^{(<s)})) - KL(q(z_T | z_0^{(<s)}) || p_\\theta (z_T | z_0^{(<s)}))\n- KL(q(z^{(s)} | z, x)||p_\\theta (z_T | z_0^{(<s)}))] \nThis produces a similar form to our loss function, with the exception being that h is not volume- preserving. However, this bound is much looser than our proposed bound. We denote as C-VDM a cascading diffusion model (Kingma et al., 2021) designed with this standard non-volume-preserving hierarchical representation (Ho et al., 2022) (i.e., Section 4.1). All model architecture and hyper- parameters are retained from W-PCDM and LP-PCDM, except for the choice of z(1), . . ., z(S). In Tables 4, 5, and 6, we see that the removing volume-preservation property of h significantly degrades likelihood modeling performance. We theorize that this is due to the looser bound obtained by"}]