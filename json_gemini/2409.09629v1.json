{"title": "CONFIDENCE ESTIMATION FOR LLM-BASED DIALOGUE STATE TRACKING", "authors": ["Yi-Jyun Sun", "Suvodip Dey", "Dilek Hakkani-T\u00fcr", "Gokhan Tur"], "abstract": "Estimation of a model's confidence on its outputs is critical for Conversational AI systems based on large language models (LLMs), especially for reducing hallucination and preventing over-reliance. In this work, we provide an exhaustive exploration of methods, including approaches proposed for open- and closed-weight LLMs, aimed at quantifying and leveraging model uncertainty to improve the reliability of LLM-generated responses, specifically focusing on dialogue state tracking (DST) in task-oriented dialogue systems (TODS). Regardless of the model type, well-calibrated confidence scores are essential to handle uncertainties, thereby improving model performance. We evaluate four methods for estimating confidence scores based on softmax, raw token scores, verbalized confidences, and a combination of these methods, using the area under the curve (AUC) metric to assess calibration, with higher AUC indicating better calibration. We also enhance these with a self-probing mech- anism, proposed for closed models. Furthermore, we assess these methods using an open-weight model fine-tuned for the task of DST, achieving superior joint goal accuracy (JGA). Our findings also suggest that fine-tuning open-weight LLMs can result in enhanced AUC performance, indicating better confidence score calibration.\nIndex Terms- Task-oriented dialogue systems, dialogue state tracking, model uncertainty, confidence scores.", "sections": [{"title": "1. INTRODUCTION", "content": "As the adoption of dialogue systems grows, a critical challenge has emerged: ensuring the reliability of the responses of these systems and preventing the generation of model responses that are inaccurate or fabricated. To mitigate this problem, recent studies [1, 2, 3, 4] have focused on measuring model uncertainty to quantify the re- liability of their outputs. Reliability in dialogue systems refers to the system's ability to consistently understand user inputs, retrieve relevant results or information when needed, generate appropriate responses, and handle uncertainties or errors effectively. A promis- ing approach to improving reliability is through the estimation of confidence scores, which aim to provide a quantitative measure of the system's uncertainty in its outputs. By incorporating confidence scores, dialogue systems can better manage uncertainties, identify potential errors, and make more informed decisions. For instance, a system with high confidence in its response can proceed smoothly, while one with low confidence can seek clarification from the user or escalate the query to a human operator. This dynamic adjustment based on confidence scores not only enhances the system's reliability but also improves its overall performance and user satisfaction.\nTo ensure that confidence scores can be applied reasonably, they must be well-calibrated. A well-calibrated confidence score means that the predicted probability accurately reflects the true likelihood of correctness, aligning the system's uncertainty with actual accu- racy and making it possible to trust and utilize these scores. There are various methods for achieving well-calibrated confidence scores, including open-box and closed-box approaches that are proposed for open-weight and closed-weight models. Open-box approaches ac- cess internal model information such as model logits [5, 3] and in-ternal states [6, 7, 8, 9], making them feasible only for open-weight models, such as Meta's Llama [10]. Closed-box methods, on the other hand, measure confidence on model outputs using verbalized or linguistic cues [4, 1], directly instructing a closed-weight model, such as OpenAI GPT-4 [11], to express its confidence.\nIn this work, we focus on LLM-based DST for TODs. By in-structing the LLM with slot descriptions and in-context examples, we predict slot-value pairs for an ongoing interaction. Instead of predicting complete outputs, we generate scores for individual slot-value pairs, offering finer-grained information for the dialoo policy as shown in Fig. 1. This allows the system to confirm only uncertain slots with the user, reducing redundancy.\nWe explored four confidence estimation methods, applicable to both open-box and closed-box models, ensuring adaptability across various TODs architectures. Additionally, a self-probing prompting strategy improves the calibration of confidence scores, enabling sys- tems to handle uncertainties more effectively. This approach lays the groundwork for more reliable dialogue systems that leverage en- hanced confidence estimation.\nThe contributions of our work can be summarized as follows:\n\u2022 We investigate various prompting strategies for dialogue state tracking and discuss their run time requirements and accu- racy.\n\u2022 We experiment with a set of open-box and closed-box meth-ods for confidence estimation of dialogue state tracking.\n\u2022 We demonstrate that a combination of open-box and closed-box methods with open-weight LLMs results in the most re- liable outcome."}, {"title": "2. RELATED WORK", "content": "2.1. Dialogue State Tracking\nDialogue State Tracking is crucial in TODs, aiming to capture the user's goals and intent during conversations. DST takes the user's utterance and conversation history as input and outputs the dia- logue belief state in a slot-value format, based on a domain-specific schema. This belief state guides the system's next action, as shown in Fig. 1.\nThe nature of DST requires a predefined domain-specific schema, and training a DST model necessitates annotated domain-specific dialogues as training data. However, data collection is notoriously challenging and labor-intensive. Consequently, the abil- ity to handle unseen domains in a zero-shot manner becomes a crucial capability for DST systems [12, 13, 14].\n2.2. Model Uncertainty\nIn machine learning, there are two types of uncertainty: epis-temic [15] and aleatoric uncertainty. Epistemic uncertainty arises from limited knowledge and reflects uncertainty in the model param-eters. By measuring it, one can assess the model's reliability-lower epistemic uncertainty typically indicates more accurate predictions with higher confidence. In contrast, aleatoric uncertainty stems from inherent randomness or noise in the data, representing the variability within the data itself.\nTo quantify these uncertainties effectively, we categorize exist-ing uncertainty measurement methods into two categories. The first is open-box methods, which access the model's internal informa-tion. Logits-based approaches, like [5, 3], estimate model uncer-tainty by utilizing model weights during inference. Ensemble-based approaches, like [16, 17, 18], estimate model uncertainty by calcu-lating the consistency between multiple responses or extracting from the predictive distribution of those ensemble models. Furthermore, [18] trains a single model by adopting ensemble distillation. The second category is methods for closed models, which do not ac-cess the internal model. Instead, these methods estimate uncertainty mostly by prompting the model [19, 20, 2, 1] in different strategies and using the responses as cues to estimate model uncertainty.\nOur method differs from traditional ensemble-based approaches by combining generation, non-generation, and verbalized insights to estimate uncertainty using a single LLM output, rather than multi-ple model runs. Although slot-level self-probing increases computa-tional cost slightly, our approach remains more efficient than ensem-bles, which require multiple predictions. This makes our method both computationally efficient and broadly applicable, offering ro- bust uncertainty estimation without the heavy overhead of ensemble methods.\n2.3. Confidence Calibration\nRecent studies have explored various methods to produce well-calibrated confidence scores. Authors of [21] employ mathematical methods to calibrate the model's output probabilities. It quanti- fies the model's bias towards certain answers using content-free input, adjusting the probabilities to better reflect true confidence levels. Another promising approach is the use of prompting strate-gies [2]. This study found that verbalized confidence, where the model expresses its confidence in natural language, is typically better calibrated than the model's raw conditional probabilities."}, {"title": "3. APPROACH", "content": "In order to present confidence estimation approaches, we first present our dialogue state tracking method inspired from the recent state-of-the-art work.\n3.1. Dialogue State Tracking\nOur dialogue state tracking approach is based on the LLM-based method proposed in [22]. Basically, an LLM is prompted twice for each turn, once for detecting the current domain (such as restaurant or train) and then the follow-up for slot filling, i.e., assigning slot values (such as restaurant name or cuisine) for that turn for the se-lected domain. This strategy greatly reduces the number of candidate slots and allows writing more targeted prompts. The prompts con-tain a description of the task along with examples. All prompts used in this paper are provided as supplementary material. A simplified version of the prompt used for domain classification is shown below.\nThe dialogue history is given as input to the LLM. Since the model is focused on the current turn, slot carryover is performed manually following the MinTL approach [23], as done in the previous work. This is especially critical for the dialogues spanning multiple do-mains, such as first finding an attraction place followed by booking a taxi to go there.\nWe explored multiple prompt designs, varying along two dimen-sions: zero-shot vs. few-shot and predicting all slots at once vs. one slot at a time. Few-shot prompts include DST-related exam-ples, while asking for all slots allows a single LLM call, compared to multiple calls for individual slots. These four prompt variants are detailed in the supplementary material. Like previous work, we instructed the LLM to output in JSON format, using in-context ex-amples. A simplified slot-filling prompt for a single slot is shown below.\n3.2. Confidence Score Estimation\nIn this study, we experiment with both open- and closed-weight models. Open-weight models serve two purposes, (i) comparing the confidence measurements directly estimated from the model with the closed-weight model approaches, including self probing, and (ii) assessing the effect of task specific fine-tuning on confidence estimation. However, note that, as the current open-weight models are less powerful than the closed-weight models, we report both DST accuracy related metrics and confidence score metrics. Our goal is devising an approach that maximizes metrics for both DST accuracy and confidence score quality."}, {"title": "3.2.1. Methods for Open-Weight Models", "content": "We adopt two methods to estimate the confidence score for open-weight models, described as follows.\nA. Scores derived from Token Probabilities of the LLM: Given the model in a generative fashion, the logits and softmax outputs reflect the token probability distribution of the model conditioned on dialogue context C. Logits (1), are crucial because they contain the raw prediction information, which can be transformed into con- fidences by applying the softmax activation function as,\n$Confi = \\sigma(1)\u2081 = \\frac{exp(l_i)}{\\Sigma_j exp(l_j)}$\nwhere $Confi$ and $l_i$ denotes the confidence and logits of token i, re- spectively. Beam search then uses these probability-form logits to calculate the scores for the sequences and extends the current candi- date sequences to generate new candidate sequences as follows,\n$S(Ci) = S(C) + log P(i | C)$\nwhere $S(Ci)$ represents the score for the concatenation of the context and token i. We calculate it by adding the score of the exist- ing sequence/context S(C) to the log-probability of the new token i given the existing sequence, i.e., log P(i | C).\nIn practice, a single word of the slot name or value may be com-posed of multiple tokens. To obtain the confidence score for a word with N tokens, we combine the logits ($l_i$) of each token in the word sequence, defined as follows,\n$Confw = \\Pi_{j=1}^N \\sigma(l)_j$\nwhere $Conf_w$ represents the confidence score of the word w with N tokens. This formula calculates the word confidence by taking the product of the softmax probabilities of each token, effectively combining the individual token confidences into a single measure of confidence for the entire word.\nAdditionally, the confidence score for a slot-value pair is derived from the individual confidence of the slot and the value tokens. We denote it as $Conf_{(Softmax)}$ as it is computed using the softmax scores. This score is calculated as follows:\n$Conf_{slot-value}^{(Softmax)} = Conf_{slot} \\times Conf_{value}$\nwhere $Conf_{slot}$ and $Conf_{value}$ represent the confidence scores of the slot and the value, respectively. By multiplying these two confidence scores, we obtain a pair confidence score that reflects the confidence in the accuracy of both the slot and its corresponding value. In our preliminary experimentation, we tested whether using just the score of the slot or the value may result in better performance for the slot-value pair and converged to using the multiplied confidence in the equation above. From the task completion perspective, both the slot and the value should be accurate for retrieving accurate information from knowledge sources.\nB. Scores derived using The Surface Form: This is a comple- mentary approach, showing the logits in a non-generative fashion, so no beam search is necessary. We just compute the score of each given token in a response string using the LLM:\n$Conf_i^{(LLM)} = log P(i|C_{<i}; \\theta)$\nwhere $Conf_i^{(LLM)}$ denotes the confidence score of token i with con- text $C_{<i}$, using the LLM with weights $\\theta$ following the Llama/GPT-style architecture [24]. We have used the minicons library\u00b2 to com-pute these raw token-level scores. For slot-value pairs with multiple tokens, we aggregated the token-level scores by taking an average. For a given slot-value pair, we denote the minicons confidence score $Conf_{slot-value}^{(Minicons)}$ as follows,\n$Conf_{slot-value}^{(Minicons)} = \\frac{1}{N} \\Sigma_{j=1}^N Conf_j^{(LLM)}$\nwhere the slot-value pair is composed of N tokens."}, {"title": "3.2.2. Methods for Open- and Closed-Weight Models", "content": "The following two methods can be applied to both open- and closed-weight models. These are also helpful in comparing their perfor-mances for confidence estimation. Furthermore, an open model, like Llama, fine-tuned with the target DST task, can also be used with these approaches.\nA. Verbalized Confidence Estimation: In addition to utilizing open-weight models, we also implement a concept inspired by hu-man conversation. We prompt the model to adhere to a predefined output format and directly extract the verbalized confidence score for each slot-value pair from the text generated by the LLM. For a given slot-value pair, this score is denoted as $Conf^{(Verbalized)}$. This approach leverages the model's ability to articulate its confidence levels in a human-like manner, providing a more transparent means of assessing the reliability of its predictions. A simplified prompt used for this approach is shown as follows.\nB. Self-Probing Confidence Estimation: Humans frequently no-tice that it is often simpler to spot errors in others' responses than in their own. Inspired by the approach in [1], where the model is queried with \"how likely is the above answer to be correct?\" to investigate model uncertainty improvements, we integrated this self- probing prompting strategy into our confidence estimation process. Specifically, we employed self-probing in three distinct scenarios: i) no self-probing, ii) self-probing at the turn-level, and iii) self- probing at the slot-level.\nIn our method, after predicting the belief state, we provided the predicted response from the previous state to the self-probing"}, {"title": "3.2.3. Combined Confidence Estimation", "content": "In the previous sections, we introduced three types of confidence score: i) softmax confidence $Conf^{(Softmax)}$, ii) minicons confidence $Conf^{(Minicons)}$, and iii) verbalized confidence $Conf^{(Verbalized)}$ The first two scores are applicable to open-weighted models, while the third one works for both open and closed-weighted models. The reason for estimating multiple types of confidence scores is to pro-vide more insights and generate calibrated confidence scores. Fig. 2 depicts an example of the confidence score combination for DST. To achieve this, for each experimental setting with open models, we train a linear regression model to produce a combined confidence score for a given slot value pair as follows:\n$Conf^{(Combined)} = Conf^{(Softmax)}+BConf^{(Minicons)} + Conf^{(Verbalized)}$\nwhere $\\alpha, \\beta, \\gamma\\in R$ are the learnable parameters of the linear regres-sion model and denote the weightage for each confidence score."}, {"title": "4. EXPERIMENT SETUP", "content": "4.1. Dataset\nOur experiments use MultiWOZ [25], a multi-domain task-oriented dialogue dataset. It is a human-human written dialogue dataset that contains turn-level annotations and descriptions of slot labels. We use the MultiWOZ 2.2 [26] version, which has refined belief state and user action annotations. We use the training subset of Multi-WOZ to fine-tune the open-weight models and the validation set to train regression for combining the confidence scores from multiple methods. The basic statistics of the dataset are shown in Table 1.\n4.2. Models\nWe evaluate our method on three models: the closed-weight GPT-4, the open-weight Llama3-8B, and the open-weight Mistral-7B. For both Llama3-8B and Mistral-7B, we also use versions that are fine- tuned with the training sets of both the MultiWOZ and SGD datasets. Additionally, 8-bit quantization is applied to both models to optimize performance and reduce memory usage."}, {"title": "4.2.1. Fine-tuning Details", "content": "To enable the model to generate responses that include verbalized confidence scores, we built ground truth confidence scores for use during the fine-tuning phase. For each dialogue turn, the model as-sesses the difficulty of predicting the dialogue state based on the given user utterance and dialogue history. Below is the simplified prompt we have used to assess the difficulty:\nIn this work, we use four difficulty levels - High, Medium, Easy, and Other. The difficulty level is mapped into a confidence score by introducing a degree of randomness appropriate to each level, described as follows:\n\u2022 Easy: mapped to a range between 0.9 and 1.0.\n\u2022 Medium: mapped to a range between 0.8 and 0.9.\n\u2022 Hard: mapped to a range between 0.7 and 0.8.\n\u2022 Other: A default confidence score of 0.5 is assigned.\nThis mapping process adds variability to the confidence scores, better reflecting real-world uncertainty. During the fine-tuning pro-cess, we provide the model with both the ground truth state and the corresponding ground truth confidence score for each slot-value pair. This dual-training approach aims to enhance the model's abil-ity to accurately predict dialogue states and verbalize its confidence, thereby improving the overall performance and reliability of the di-alogue state tracking system.\nDuring fine-tuning, we provide the model with both the ground truth state and the corresponding ground truth confidence score for each turn. This enhances the model's ability to predict verbalized confidence and improves the overall performance. Additionally, we employed low-rank adapters (LoRA) [27] to fine-tune the model."}, {"title": "4.3. Evaluation Metrics", "content": "We evaluated our method in two aspects: i) the performance of our approach on the DST task and ii) the calibration level of the confi- dence scores.\n4.3.1. DST Evaluation Metrics\nTo evaluate the quality of dialogue state tracking, we use two met-rics: joint goal accuracy (JGA) and slot-level F-measure (Slot-F). JGA is the commonly used DST evaluation metric that requires all the slot and value pairs in the predicted state to exactly match the slot and value pairs in the ground truth state, for that turn to be con- sidered accurate. Previous work discussed that JGA overly punishes DST performance [28, 29]. Hence, we also present slot F-measure, which computes the slot-value pair level F-scores between the pre- dicted and ground truth dialogue states."}, {"title": "5. RESULTS", "content": "In our experiments, we use the MultiWOZ 2.2 test data for assessing the quality of the dialogue state tracking and associated confidence score estimations.\n5.1. Dialogue State Tracking Performance\nTable 2 shows the JGA performance of various prompting strate-gies for DST. We use four prompting strategies using descriptions of All/One slot(s) with examples covering All/One slot(s). For exam-ple, in the All/All strategy, we provide descriptions of all slots with examples covering all slots. In the All/One strategy, we provide de-scriptions of all slots with the example covering a single slot. The other two strategies can be defined in a similar fashion.\nWe observe that the All/All strategy achieves the best JGA score for different models. This is because it covers all the slots along with examples, resulting in superior performance. Moreover, the All/All strategy is also the most computationally efficient method. As it covers both descriptions and examples of all the slots, this strategy requires calling the LLM only once for every dialogue turn. In con-trast, the other three strategies necessitate multiple calls to the LLM, significantly increasing the computational overhead. Given the ad-vantages in both performance and efficiency, we adopt the All/All strategy to report the results for the remainder of the paper.\nTable 3 presents the results for the dialogue state tracking and confidence score prediction experiments using two models - i) closed-weight GPT-4 and ii) open-weight Llama3-8B. The left table presents results with no DST examples in the instruction context, whereas the right table presents results with few-shot examples in-cluded in the context of the instruction to the model. For few-shot, we used three examples in these experiments. The examples are selected by utilizing FAISS DB\u00b3 to find similar samples to the dia-logue that is being tested from the training subset based on the last two utterances. We also present results after fine-tuning with the training subset of the MultiWOZ 2.2."}, {"title": "5.2. Quality of the Confidence Scores", "content": "Besides JGA, Table 3 also presents the performance of confidence scores with respect to ROC-AUC (shown as AUC in Table 3) and ECE. While the JGA of GPT-4 with in-context examples (few-shot) is high, the quality of the verbalized confidence scores is lower than other models, even with self-probing. The best AUC obtained with GPT-4 is around 0.54 for both zero-shot and few-shot scenar-ios. Amongst the individual methods, the softmax confidence score achieves the best results for both metrics. Fine-tuning Llama3 for the task also results in an improvement of the confidence score qual-ity, similar to the JGA performance. This result is aligned with the previous works [30]. The combination of the three confidence scores leads to small improvements for AUC. However, this improvement is much higher for ECE due to better calibration of the score values after the regression. We also observe performance gain after ap-plying self-probing in some instances. For example, Llama3, with few-shot examples and a combined confidence score, achieves the highest AUC. In terms of the two targets of high DST accuracy and best confidence scores, we get the best outcome with the fine-tuned Llama3 model using the combined confidence scores, resulting in a JGA of 44.6%, AUC of 0.725 and ECE of 0.018."}, {"title": "5.3. Correlation Analysis", "content": "We analyze the correlation of confidence scores using the Pearson correlation coefficient, based on the MultiWOZ dataset with Llama3 experiments. A label list is created, where 1 indicates the presence of a slot-value pair in the ground truth, and 0 otherwise. Table 4 shows the correlations of the four confidence scores with these la-bels. Softmax scores show moderate correlation, while minicons and verbalized scores have weak correlations in both zero-shot and few-shot scenarios. This also justifies the superior performance of the combined confidence score in Table 3. Although the improvement in the correlation for the combined score is marginal with respect to the softmax confidence score, it results in better calibration."}, {"title": "5.4. Computational Costs", "content": "While slot-level self-probing incurs higher computational costs due to the need for multiple LLM inferences per slot, our results show that turn-level self-probing, which requires only a single LLM in-ference per turn, offers comparable performance. As demonstrated in Table 3, the differences in evaluation metrics between slot-level and turn-level are minimal, with variations of less than 0.05. There-fore, turn-level self-probing provides a much more computationally"}, {"title": "6. CONCLUSION AND FUTURE WORK", "content": "In summary, we presented a comprehensive study exploring various methods for confidence score estimation for dialogue state tracking (DST) in a Conversational AI system. In this work, we explored four different confidence scores based on softmax, raw token scores, verbalized confidences, and a combination of these methods. We observed that all methods are sub-optimal with closed-weight mod-els, and it is possible to get well-calibrated confidence scores by fine-tuning an open-weight model. Regarding how to use the con-fidence score we get, existing work such as [31] incorporate uncer-tainty in the database query vector in the form of confidence thresh-olds, or [18] use confidence score at data selection components and label-validation mechanism. In our work, we found that incorpo-rating confidence scores into fine-tuning procedure significantly im-proves the DST performance. In addition, a well-calibrated confi-dence score is generated by the combined method. We also showed that the combined confidence score is moderately correlated with the ground truth labels, justifying its superior performance.\nOur future work involves using these confidence scores to im-prove the goal completion rate by extending the dialogue policy model accordingly. For example, if the system has low confidence about a slot value, it can perform implicit or explicit confirmation, or utilize this in a chain of thought reasoning process. Furthermore, we plan to conduct experiments on a second dataset like SGD to val-idate the generalizability of our approach and ensure its robustness across different datasets."}, {"title": "A. ZERO-SHOT PROMPT FORAMT", "content": "A. ZERO-SHOT PROMPT FORAMT\n<|begin_of_text | >\n<|start_header_id|>system<|end_header_id|>\nCapture entity values from the LAST UTTERANCE of the conversation.\nFOCUS ONLY ON THE VALUES MENTIONED IN THE LAST UTTERANCE.\nFormat the output as a valid JSON object for each entity-value pair.\nFormat: {\"state\": { \"_entity_\":\"_value_\" }, \"confidence\": \"X\"}\nWhere X is the Confidence of the answer.\nFill the actual entity value into the placeholder encapsulated with underscores.\nPut  as EOS token at the end of response.\nValues that should be captured are:\nIn the DOMAIN of \"train\", the values that should be captured are:\n\"arriveby\" that specifies what time the train should arrive\n\"leaveat\" that specifies what time the train should leave\n\"day\" that specifies what day the train should leave\n(monday/tuesday/wednesday/thursday/friday/saturday/sunday)\n\"departure\" that specifies the departure station\n\"destination\" that specifies the destination station\n\"bookpeople\" that specifies how many people the booking is for\nDo not capture any other values!\nIf not specified, do not respond to that slot-value.\nMAKE SURE TO SEPARATE EACH SLOT-VALUE PAIR, AND ALONG WITH EACH OF THEIR CONFIDENCE (0-1).\nFormat the output as:\n[\n````json\n]\n````\n{\"state\": { \"_entity1_\":\"_value1_\"}, \"confidence\": \"_X1_\"},\n{\"state\": { \"_entity2_\":\"_value2_\"}, \"confidence\": \"_X2_\"},\n{\"state\": {\"_entity3_\":\"_value3_\" }, \"confidence\": \"_X3_\"},\nNow complete the following example, AND PROVIDE CONFIDENCE THAT IT'S CORRECT:\ninput: <leot_id|>\n<|start_header_id|>user<|end_header_id |>\nCustomer: I need train reservations from norwich to cambridge"}, {"title": "B. FEW-SHOT PROMPT FORAMT", "content": "B. FEW-SHOT PROMPT FORAMT\n<|begin_of_text | >\n<|start_header_id|>system<|end_header_id|>\nCapture entity values from the LAST UTTERANCE of the conversation.\nFOCUS ONLY ON THE VALUES MENTIONED IN THE LAST UTTERANCE.\nFormat the output as a valid JSON object for each entity-value pair.\nFormat: {\"state\": {\"_entity_\":\"_value_\"}, \"confidence\": \"X\"}\nWhere X is the Confidence of the answer.\nFill the actual entity value into the placeholder encapsulated with underscores.\nPut  as EOS token at the end of response.\nValues that should be captured are:\nIn the DOMAIN of \"train\", the values that should be captured are:\n\"arriveby\" that specifies what time the train should arrive\n\"leaveat\" that specifies what time the train should leave\n\"day\" that specifies what day the train should leave\n(monday/tuesday/wednesday/thursday/friday/saturday/sunday)\n\"departure\" that specifies the departure station\n\"destination\" that specifies the destination station\n\"bookpeople\" that specifies how many people the booking is for\nDo not capture any other values!\nIf not specified, do not respond to that slot-value.\n---Example 0:\ncontext: Customer: I need to get a train from cambridge to Norwich.\nstate: ````json{{\"state\": {departure: cambridge}, \"confidence\": \"0.9517114799962094\"},\n{\"state\": { destination: norwich}, \"confidence\": \"0.9517114799962094\"},\n}````\nExample 1:\ncontext: Customer: I need a train from Cambridge to Norwich please.\nstate: ````json{{\"state\": {departure: cambridge}, \"confidence\": \"0.909169572100446\"},\n{\"state\": { destination: norwich}, \"confidence\": \"0.909169572100446\"},\n}````\nMAKE SURE TO SEPARATE EACH SLOT-VALUE PAIR, AND ALONG WITH EACH OF THEIR CONFIDENCE (0-1).\nFormat the output as:\n[\n````json\n]\n````\n{\"state\": { \"_entity1_\":\"_value1_\"}, \"confidence\": \"_X1_\"},\n{\"state\": { \"_entity2_\":\"_value2_\"}, \"confidence\": \"_X2_\"},\n{\"state\": {\"_entity3_\":\"_value3_\" }, \"confidence\": \"_X3_\"},\nNow complete the following example, AND PROVIDE CONFIDENCE THAT IT'S CORRECT:\ninput: <leot_id|>\n<|start_header_id|>user<|end_header_id |>\nCustomer: I need train reservations from norwich to cambridge"}, {"title": "C. SELF-PROBING PROMPT", "content": "C.1. Turn-level\nConversation:\n['Customer: I need train reservations from norwich to cambridge']\nState:\n{'departure': 'norwich', 'destination': 'cambridge', 'bookpeople': '2'}\nHow likely is the above state to be correct?\nAnalyze the state, provide 1 brief reason, and give confidence (0-1).\nFormat: confidence: \"X\", reason: \"brief reason\"\nThink carefully and step by step.\nOutput:\nC.2. Slot-level\nConversation:\n['Customer: I need train reservations from norwich to cambridge']\nState:\n{'arriveby': '-1'}\nHow likely is the above state to be correct?\nAnalyze the state, provide 1 brief reason, and give confidence (0-1).\nFormat: confidence: \"X\", reason: \"brief reason\"\nThink carefully and step by step.\nOutput:"}, {"title": "D. GROUND TRUTH CONFIDENCE SCORE PROMPT", "content": "D. GROUND TRUTH CONFIDENCE SCORE PROMPT\n<|begin_of_text | >\n<|start_header_id|>system<|end_header_id | >\nYou are a helpful AI assistant for evaluating the hardness of dialogue state tracking\nfrom last user utterance given dialogue history <|eot_id|>\n<|start_header_id|>user<|end_header_id|>\nHow difficult would it be for a Language Model to predict the dialogue state from:\nutterance: ('Customer: I need train reservations from norwich to cambridge']\ngiven dialogue history\nhistory:\n['Customer: I need train reservations from norwich to cambridge']\nChoose the level of hardness from (Easy/Medium/Hard).\nAnswer:"}, {"title": "E. PROMPT VARIATIONS", "content": "E.1. All Slot Description + All Slot Examples\nE. PROMPT VARIATIONS\n<|begin_of_text | >\n<|start_header_id|>system<|end_header_id|>\nCapture entity values from the LAST UTTERANCE of the conversation.\nFOCUS ONLY ON THE VALUES MENTIONED IN THE LAST UTTERANCE.\nFormat the output as a valid JSON object for each entity-value pair.\nFormat: {\"state\": {\"_entity_\":\"_value_\""}, "confidence\": \"X\"}\nWhere X is the Confidence of the answer.\nFill the actual entity value into the placeholder encapsulated with underscores.\nPut  as EOS token at the end of response.\nValues that should be captured are:\nIn the DOMAIN of \"train\", the values that should be captured are:\n\"arriveby\" that specifies what time the train should arrive\n\"leaveat\" that specifies what time the train should leave\n\"day\" that specifies what day the train should leave\n(monday/tuesday/wednesday/thursday/friday/saturday/sunday)\n\"departure\" that specifies the departure station\n\"destination\" that specifies the destination station\n\"bookpeople\" that specifies how many people the booking is for\nDo not capture any other values!\nIf not specified, do not respond to that slot-value.\n---Example 0:\ncontext: Customer: I need to get a train from cambridge to Norwich.\nstate: ````json{{\"state\": {departure: cambridge}, \"confidence\": \"0.9517114799962094\"},\n{\"state\": { destination: norwich}, \"confidence\": \"0.9517114799962094\"},\n}````\nExample 1:\ncontext: Customer: I need a train from Cambridge to Norwich please.\nstate: ````json{{\"state\": {departure: cambridge}, \"confidence\": \"0.909169572100446\"},\n{\"state\": { destination: norwich}, \"confidence\": \"0.909169572100446\"},\n}````\nMAKE SURE TO SEPARATE EACH SLOT-VALUE PAIR, AND ALONG WITH EACH OF THEIR CONFIDENCE (0-1).\nFormat the output as:\n[\n````json"], "n````\n{\"state\"": ""}