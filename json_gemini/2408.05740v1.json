{"title": "MTSCI: A Conditional Diffusion Model for Multivariate Time Series Consistent Imputation", "authors": ["Jianping Zhou", "Junhao Li", "Guanjie Zheng", "Xinbing Wang", "Chenghu Zhou"], "abstract": "Missing values are prevalent in multivariate time series, compromising the integrity of analyses and degrading the performance of downstream tasks. Consequently, research has focused on multivariate time series imputation, aiming to accurately impute the missing values based on available observations. A key research question is how to ensure imputation consistency, i.e., intra-consistency between observed and imputed values, and inter-consistency between adjacent windows after imputation. However, previous methods rely solely on the inductive bias of the imputation targets to guide the learning process, ignoring imputation consistency and ultimately resulting in poor performance. Diffusion models, known for their powerful generative abilities, prefer to generate consistent results based on available observations. Therefore, we propose a conditional diffusion model for Multivariate Time Series Consistent Imputation (MTSCI). Specifically, MTSCI employs a contrastive complementary mask to generate dual views during the forward noising process. Then, the intra contrastive loss is calculated to ensure intra-consistency between the imputed and observed values. Meanwhile, MTSCI utilizes a mixup mechanism to incorporate conditional information from adjacent windows during the denoising process, facilitating the inter-consistency between imputed samples. Extensive experiments on multiple real-world datasets demonstrate that our method achieves the state-of-the-art performance on multivariate time series imputation task under different missing scenarios. Code is available at https://github.com/JeremyChou28/MTSCI.", "sections": [{"title": "1 INTRODUCTION", "content": "Multivariate time series data widely exists in various real-world applications, e.g., transportation [29, 36], meteorology [1, 17], healthcare [37], energy [2], etc. The integrity of time series plays a crucial role on tasks such as forecasting [26, 48] and classification [22]. However, missing data is a common issue in real-world datasets due to device failures, communication interruptions, and human errors [16, 34], which impairs the downstream task performance and renders the integrity analysis approaches inapplicable. Consequently, multivariate time series imputation, which aims to accurately impute missing values using available observations, arises as an important research question.\nEarly studies are based on statistical learning [4, 13] and machine learning methods [20, 40, 46]. Subsequently, numerous deep learning-based methods have been proposed. Some approaches [5, 6, 9, 33] treat the imputation task as a deterministic point estimation problem, while others [3, 8, 12, 35, 45] view it as a probabilistic generative problem. However, these methods rely solely on the inductive bias of artificially simulated imputation targets to guide the learning process, as illustrated in Figure 1 (a), neglecting the crucial aspect of imputation consistency.\nThe imputation consistency can be divided into two categories: (i)intra-consistency and (ii)inter-consistency. As shown in Figure 1 (b), consider two samples of adjacent windows in incomplete time series. Intra-consistency implies that the imputed values, guided by observed values, should facilitate the reconstruction of observed values, ensuring consistency between imputed and observed values, thereby reducing imputation bias. Inter-consistency means that when imputing a single sample, the sample from the adjacent window should be considered to ensure that the complete sample maintains temporal consistency with the adjacent window."}, {"title": "2 RELATED WORK", "content": "Existing multivariate time series imputation methods can be divided into four categories: statistical-based methods, machine learning-based methods, deterministic deep models and probabilistic generative models. (i) Statistical methods, such as Mean, Median [13] and KNN [4], utilize the statistical indicators to impute missing values. (ii) Machine learning methods like linear imputation, state-space models [10] and MICE [40] impute missing values based on linear dynamics assumptions. Other machine learning methods like low-rank matrix factorization, e.g., NMF [20], TRMF [46], TIDER [25], factorize incomplete data into low-rank matrices and impute missing values using the product of these matrices. However, these methods struggle with capturing the nonlinear dynamics and handling large datasets. (iii) Deterministic deep models treat imputation as a deterministic point estimation problem. For instance, GRUD [6] uses the last observation and mean of observations to represent missing patterns. BRITS [5] employs bidirectional recurrent neural networks to capture temporal features. mTAN [33] and SAITS [9] design attention mechanisms to capture temporal dependencies. TimesNet [42] transforms 1D time series to 2D space to capture complex temporal variations, achieving state-of-the-art performance in multiple time series tasks, including imputation. However, deterministic methods fall short of modeling imputation uncertainties. (iv) Probabilistic generative models view imputation as a missing values generation problem. For example, GP-VAE [19] combines the VAE [19] and Gaussian process to model incomplete time series. GAIN [45] uses a GAN [15] with a hint mechanism to aid imputation. McFlow [30] leverages normalizing flow generative models and Monte Carlo sampling for imputation. CSDI [35] and MIDM [39] utilize conditional diffusion models to generate missing values by treating the observed values as conditional information. CSBI [8] employs the Schr\u00f6dinger bridge algorithm for imputation. PriSTI [24] extracts conditional information for spatio-temporal imputation using geographic data. However, these methods only use the inductive bias on imputation targets to guide the learning process, which is insufficient to maintain intra-consistency between observed and imputed values within a sequence, as well as inter-consistency between imputed sequences.\nAlthough there are no imputation methods considering the imputation consistency, other studies have explored consistency strategies in time series analysis. Time series representation methods [37, 44] propose a consistency strategy in the sampling process, where time segments within adjacent windows exhibit high pattern consistency, such as consistent trends, periods and amplitudes."}, {"title": "3 PRELIMINARIES", "content": "In this section, we define the problem of multivariate time series imputation, and then introduce the background of diffusion models and conditional diffusion models."}, {"title": "3.1 Problem Definition", "content": "DEFINITION 1 (MULTIVARIATE TIME SERIES.). The multivariate time series denoted as $X \\in \\mathbb{R}^{T\\times C}$ contains $C$ features with length $T$. The mask matrix denoted as $M \\in \\{0,1\\}^{T\\times C}$ indicates whether the values is observed or missing, where $M_{i,j} = 1$ indicates $X_{i,j}$ is observed, and $M_{i,j} = 0$ indicates $X_{i,j}$is missing. Then, the observed values in $X$ is denoted as $X^o = X \\odot M$, the missing values in $X$ is denoted as $X^m = X \\odot (1 - M)$.\nPROBLEM 1 (MULTIVARIATE TIME SERIES IMPUTATION.). Given the multivariate time series $X$ and the mask matrix $M \\in \\{0,1\\}^{T\\times C}$ over $T$ time slices, our task of multivariate time series imputation is to estimate the missing values or corresponding distributions in $X$. The problem can be formulated as learning a probabilistic imputation function $p_\\theta$:\n$\\max_\\theta p_\\theta(X^m|X^o),$\nwhere the goal is to approximate the real the conditional distribution $p_\\theta(X^m|X^o)$ or minimize the estimation error on missing positions."}, {"title": "3.2 Diffusion Models", "content": "A well-known diffusion model is the denoising diffusion probabilistic model (DDPM) [18], which contains the forward noising process and the backward denoising process. During the forward noising process, an input $x_0$ is gradually corrupted to a Gaussian noise vector, which can be defined by the following Markov chain:\n$q(x_{T}|x_0) := \\prod_{k=1}^{T} q(x_k|x_{k-1}), q(x_k|x_{k-1}) := N(\\sqrt{1 - \\beta_k}x_{k-1}, \\beta_kI),$\nwhere $\\beta_k \\in [0, 1]$ represents the noise level. Then, sampling of $x_k$ can be written as $q(x_k|x_0) = N(x_k; \\sqrt{\\bar{\\alpha}_k}x_0, (1 - \\bar{\\alpha}_k)I)$, where $\\bar{\\alpha}_k = \\prod_{i=1}^{k} \\alpha_i$, and $\\alpha_k = 1 - \\beta_k$. Thus, $x_k$ can be simply obtained as:\n$x_k = \\sqrt{\\bar{\\alpha}_k}x_0 + \\sqrt{1 - \\bar{\\alpha}_k}\\epsilon,$\nwhere $\\epsilon$ is a Gaussian noise. During the backward denoising process, DDPM considers the following specific parameterization of $p_\\theta(x_{k-1}|x_k)$:\n$p_\\theta(x_{k-1}|x_k) = N(x_{k-1}; \\mu_\\theta(x_k, k), \\sigma_\\theta(x_k, k)),$\nwhere the variance $\\sigma_\\theta(x_k, k)$ is usually fixed as $\\sigma^2I$ and the mean $\\mu_\\theta(x_k, k)$ is defined by a denoising network $(\\epsilon_\\theta \\text{or} x_\\theta)$. For noising estimation, the denoising network $\\epsilon_\\theta$ predicts the noise, and then obtains the mean $\\mu_\\theta(x_k, k)$:\n$\\mu_\\theta(x_k, k) = \\frac{1}{\\sqrt{\\alpha_k}} (x_k - \\frac{1 - \\alpha_k}{\\sqrt{1 - \\bar{\\alpha}_k}}\\epsilon_\\theta(x_k, k)).$\nThe denoising network $\\epsilon_\\theta$ is trained by minimizing the loss $L_\\epsilon$:\n$L_\\epsilon = E_{k,x_0,\\epsilon}||\\epsilon - \\epsilon_\\theta(x_k, k) ||^2.$\nFor $x_0$ estimation, the denoising network $x_\\theta$ predicts the value $x_0$, and then obtains the mean $\\mu_\\theta(x_k, k)$:\n$\\mu_\\theta(x_k, k) = \\frac{\\sqrt{\\alpha_k} (1 - \\bar{\\alpha}_{k-1})}{1 - \\bar{\\alpha}_k}x_k + \\frac{\\sqrt{\\bar{\\alpha}_{k-1}}\\beta_k}{1 - \\bar{\\alpha}_k}x_\\theta(x_k, k).$\nThe denoising network $x_\\theta$ is trained by minimizing the loss $L_x$:\n$L_x = E_{k,x_0,\\epsilon}||x_0 - x_\\theta(x_k, k) ||^2.$"}, {"title": "3.3 Conditional Diffusion Models", "content": "The multivariate time series imputation task aims to impute missing values $X^m \\in \\mathbb{R}^{T\\times C}$ based on the conditional information of observed values $X^o \\in \\mathbb{R}^{T\\times C}$. The forward noising process and backward denoising process of conditional diffusion model to impute missing values are defined as follows:\n$p_\\theta(x_{T}|x) := p(x) \\prod_{k=1}^{T} p_\\theta(x_{k-1}|x_k, c), x_T \\in N(0, I),$\n$p_\\theta(x_{k-1}|x_k, c) := N(x_{k-1}; \\mu_\\theta(x_k, k|c), \\sigma_\\theta(x_k, k|c)),$\nwhere $c = F(x^o)$ is the conditional information output of the conditioning network $F$. By repeatedly running the denoising step in (10) till k = 1, the imputed value $\\hat{x}^m$ is obtained."}, {"title": "4 METHODOLOGY", "content": "In this section, we elaborate on our model, MTSCI, which is designed to tackle the multivariate time series imputation task. We start with an overview of MTSCI in Section 4.1. Then, we introduce the contrastive consistency on forward noising process in Section 4.2. Following that, we explain the consistency-assured denoising process in Section 4.3. Finally, we outline the algorithm procedures for both the training and inference stages."}, {"title": "4.1 Overview", "content": "MTSCI is a conditional diffusion model for multivariate time series consistent imputation. As shown in Figure 2, the \"sampled\" window $X_{t-L:t}$ is sampled for imputation. First, we use contrastive complementary mask to generate two views for contrastive consistency on noising process. Then, we utilize the intra contrastive module and inter-consistency condition network for consistency-assured denoising process, where the intra contrastive module is to calculate contrastive loss between two views, ensuring the intra-consistency between imputed and observed values. The inter-consistency condition network is to incorporate the conditional information from \"context\" window for consistency between adjacent windows."}, {"title": "4.2 Contrastive Consistency on Noising Process", "content": "Goal: Previous works directly use a random mask strategy to generate imputation targets and utilize the remaining observations to impute them. However, this approach does not guarantee consistency between the imputed and the observed values, meaning the imputed values cannot guide the network to reconstruct the observed values. This results in overfitting some outliers and then leads to deviations between the imputed values and their ground-truth. To address this, we utilize a contrastive complementary mask strategy for the \"sampled\" window of the incomplete time series, generating a pair of samples where the imputation targets and observations are complementary. This approach ensures that MTSCI learns to impute missing values consistent with the observed values during training.\n4.2.1 Contrastive Complementary Mask. We refer to the input into the MTSCI as the \"sampled\" window, denoted as $x_{t-L:t}$, and the \"context\" window is denoted as $x_{t:t+L}$. Using the complementary mask strategy, we generate two views of the \"sampled\" window. Based on a random mask matrix m from self-supervised mask modeling process, we consider $x_{1,t-L:t} = m \\odot x_{t-L:t}$ and $x_{2,t-L:t} = (1-m) \\odot x_{t-L:t}$ as the two views, where $\\odot$ denotes element-wise production. The selection of mask patterns and mask ratios in the self-supervised mask modeling process is discussed in Section 5.1.\n4.2.2 Noising. Take the $x_{1,t-L:t}$ as an example, the imputation target and conditional observation are denoted as $\\hat{x}_{1,t-L:t}^a$, $x_{1,t-L:t}^o = \\hat{x}_{1,t-L:t}$, respectively. Based on (3), we obtain the noised $\\hat{x}_k^a$:\n$\\hat{x}_k^a = \\sqrt{\\bar{\\alpha}_k}\\hat{x}_{1,t-L:t}^a + \\sqrt{1 - \\bar{\\alpha}_k}\\epsilon,$\nwhere $\\epsilon$ is sampled from N(0, I) with the same size as $\\hat{x}_{1,t-L:t}^a$."}, {"title": "4.3 Consistency-Assured Denoising Process", "content": "Goal: The goal is to introduce how consistency strategies can be ensured in the backward denoising process to enhance imputation performance. We first introduce the intra contrastive loss to constrain the two views generated in Section 4.2, enabling the imputed and observed values to reconstruct each other(intra consistency). Subsequently, we utilize the \"context\" window to provide supplemental conditional information, combined with the observed conditional information in the \"sampled\" window via a mixup mechanism, to teach the model to impute the \"sampled\" window while considering the \"context\" window(inter-consistency).\n4.3.1 Intra Contrastive Module(Intra-consistency). The noised contrastive pairs obtained from Section 4.2.2 are input into the denoising network, producing the embedding $z_1$and $z_2$ after the encoder. Then, we calculate the representation similarity between $z_1$ and $z_2$. To be specific, suppose that the number of samples in one batch is N, then after applying the complementary mask, we have 2N views of samples. Inspired by the cross-entropy formulation of contrastive loss in previous works [11, 21], we calculate the intra contrastive loss as follows:\n$L_{CL} = \\frac{1}{N} \\sum_{i=1}^{N} -log \\frac{exp(sim(z_i, z_i)/\\tau)}{\\sum_{m=1}^{2N}1_{[m\\neq i]}exp(sim(z_i, z_m)/\\tau)},$\nwhere $sim(u, v) = \\frac{uv}{||u||||v||}$ is the cosine similarity, $1_{m\\neq i} \\in \\{0, 1\\}$ is an indicator function, $\\tau$ is a temperature parameter. By constraining the representation similarity between two views generated by complementary mask strategy, the model learns to ensure that the imputed values can also reconstruct the observed values during the imputation process.\n4.3.2 Inter-consistency Condition Network(Inter-consistency). Existing methods only use the observed values to impute imputation targets in \"sampled\" window, ignoring the conditional information of the \"context\" window actually. To address this problem, we utilize the \"context\" window to provide supplemental conditional information, teaching the model to impute the \"sampled\" window while maintaining the contextual consistency on adjacent windows. Although the \"context\" window is accessible during training, it is not available during inference. Therefore, we incorporate a mixup mechanism [32, 49] to combine the conditional information in the"}, {"title": "4.3.3 Denoising Network", "content": "The denoising network accepts the noised \"sampled\" window and the conditional information representation c to predict the noise at missing positions, and then generate the imputed values.\nEmbedding: First, the noised \"sampled\" window $\\hat{x}_k^a$ is embed on a linear layer, plus with the diffusion step representation $p^k$ to acquire the hidden representation h:\n$h = Linear(\\hat{x}_k^a) + p^k,$\n$p^k = FeedForward(kembedding(k)),$\n$kembedding(k) = [sin(10^{\\frac{0}{d}-1}k),........., sin(10^{\\frac{d}{d}-1}k), cos(10^{\\frac{0}{d}-1}k),..., cos(10^{\\frac{d}{d}-k})],$\nwhere FeedForward() is a two fully-conncted layers with the SiLU activation function, kembedding is d-dimension vectors, w = $\\frac{\\pi}{4}$. Then, the h is sent to the encoder to learn the temporal and variable dependencies.\nEncoder: First, a linear layer is utilized to fuse the input, and then, we utilize the single-layer vanilla transformer block [38] to capture the temporal dependency. Inspired by the recent work iTransformer [26], we introduce another single-layer invert transformer block to capture the variable dependency. The encoder is stacked by multiple encoder-layers. For a single encoder-layer, the dependency extraction process is defined as follows:\n$H = Trm(Linear(h) + TE(h)),$\n$H_{inv} = iTrm(Transpose(H) + FE(H))),$\nwhere H is the output of transformer, $H_{inv}$ is the output of inverted transformer, Trm represents the transformer block, iTrm represents the itransformer block, TE() represents the temporal position embedding, FE() represents the feature position embedding.\nDecoder: The decoder is to merge multiple output from the encoder to acquire $H_{out}$, and then generate the predicted noise $\\epsilon_\\theta$ through a feed-forward network implemented by two fully-connected layers with ReLU activation function:\n$H_{out} = LN(Concat(H_{inv}, H)),$\n$\\epsilon_\\theta(\\hat{x}_k^a, k|c) = FeedForward(H_{out}),$\n$\\epsilon_{k-1} = \\frac{1}{\\sqrt{\\alpha_k}} (\\frac{\\hat{x}_k^a}{\\sqrt{1 - \\bar{\\alpha}_k}} - \\epsilon_\\theta(\\hat{x}_k^a, k|c)) + \\sigma_k\\epsilon,$\nwhere $H_{inv}$ represents the output of l-th layer, L is the number of encoder layers."}, {"title": "4.4 Training", "content": "During the training process, for each predicted noise $\\hat{\\epsilon}$, we calculate the denoising loss $L_\\epsilon$ as follows:\n$L_\\epsilon = E_{\\hat{x}_k^a,\\epsilon\\sim N(0,1),k}L_\\epsilon(k),$\n$L_\\epsilon(k) = ||(1 - M) \\odot (\\epsilon - \\epsilon_\\theta(\\hat{x}_k^a, k|c))||^2.$\nThen, the total loss L is obtained by adding the denoising loss $L_\\epsilon$ and the intra contrastive loss $L_{CL}$ with weighted coefficient $\\lambda$:\n$L = L_\\epsilon + \\lambda L_{CL}.$\nThe complete training procedure is shown in Algorithm 1."}, {"title": "4.5 Inference", "content": "During the inference process, all elements in the mixing coefficient matrix is set to 1. By repeatedly running the denoising step till k equals 1, we obtain the $\\hat{\\epsilon}$ as the final predicted noise. The inference procedure is shown in Algorithm 2."}, {"title": "5 EXPERIMENTS", "content": "In this section, we first introduce the experimental setups, including the datasets, baselines, evaluation metrics and implementation details. Then, we evaluate our model, MTSCI, with extensive experiments to answer the following research questions:\n\u2022 RQ1: How does MTSCI perform against other baselines in the multivariate time series imputation task?"}, {"title": "5.1 Experimental Setup", "content": "Datasets. We evaluate our proprosed model on three commonly used public datasets: an electricity dataset ETT [9], a climate dataset Weather [43], and a traffic speed dataset METR-LA [23]. The statistical details of these datasets are listed in Table 1.\nBaselines. We select 13 baselines to evaluate the performance of our proposed method on multivariate time series imputation task. These baselines include statistical methods (Mean, KNN), typical machine learning methods (MICE, TRMF), deterministic imputation methods (BRITS, mTAN, SAITS, TimesNet, Non-stationary Transformer) and deep generative imputation models (GP-VAE, rGAIN, CSBI, CSDI). We briefly introduce the baseline methods as follows: (1)Mean: directly use the average value of observed values to impute. (2)KNN: use the average value of similar samples to missing sample, as implemented by fancyimpute. (3)MICE [40]: multiple imputation method by chained equations. (4)TRMF [46]: a temporal regularized matrix factorization method. (5)GP-VAE [12]: combine VAE [19] with Guassion process for time series probabilistic imputation. (6)rGAIN [45]: a GAN-based method with a"}, {"title": "5.2 Overall Performance(RQ1)", "content": "The overall performance is shown in Table 2 and Table 3. We make the following observations: (1) MTSCI achieves the state-of-the-art performance across multiple datasets, both in point missing and block missing patterns, with an average improvement of 17.88% in MAE, 15.09% in RMSE and 13.64% in MAPE, respectively. Notably, in the case of block missing pattern with continuous missing scenarios, our method demonstrates greater superiority, highlighting the effectiveness of the imputation consistency strategy adopted in MTSCI. (2) The statistical methods and classical machine learning methods perform poor on all datasets due to the strong non-linearity of incomplete multivariate time series. These methods impute missing values based on assumptions such as stability or linear dynamics, which fail to capture the complex temporal correlations in real-world datasets. (3) Compared to deterministic imputation models, MTSCI achieves performance improvements of 42.07% in MAE, 24.15% in RMSE and 39.76% in MAPE respectively on average. The difference between these methods and our model is that we employ the conditional diffusion process to model the incomplete time"}, {"title": "5.3 Ablation Study(RQ2)", "content": "We conduct an ablation study to evaluate the effectiveness of the complementary mask strategy to generate intra contrastive loss and the inter-consistency condition network with mixup mechanism for utilizing conditional information from adjacent windows. We"}, {"title": "5.4 Case Study(RQ2)", "content": "In order to intuitively understand how MTSCI imputes the incomplete time series, we visualize the imputation results of MTSCI and the sub-optimal method CSDI. Specifically, we randomly select three snapshots of incomplete time series with block missing pattern from three datasets. As shown in Figure 4, MTSCI demonstrates better imputation performance compared to the CSDI, which is attributed to intra contrastive consistency and inter-consistency condition network. In addition, compared to three variants of MTSCI, our model exhibits consistent trend between imputed and observed values, alleviating the imputation error. This improvement is due to MTSCI's consideration of intra-consistency within a single window that observed and imputed values can reconstruct each other, as well as the inter-consistency between adjacent windows that can provide supplemental condition information to guide imputation. Additionally, we use the CRPS metric [28, 35] to measure the imputation consistency between the imputed results and the observed values at the whole dataset. As shown in Table 5, our method outperforms the sub-optimal method CSDI."}, {"title": "5.5 Sensitivity Analysis(RQ3)", "content": "To evaluate the generalization ability of MTSCI, we carry out an assessment of performance w.r.t. different missing ratios on Weather dataset (due to space limitations). To comprehensively account for the sufficiency and sparsity of missing data, we use missing ratios of 10%,30%,50%,70% in testing set. We compare MTSCI with two other conditional diffusion models: CSBI and CSDI. The results are shown in Table 6. MTSCI outperforms the baselines across different missing ratios both on two different mask patterns. However, we observe that the imputation performance does not consistently decrease with increasing missing ratios, indicating the presence of distribution shift in incomplete time series. Notably, our consistency strategy helps alleviate this issue to some extent, maintaining good generalization ability. To further verify the generalization of our model, we also evaluate it under different missing patterns for the training and testing sets. Specifically, we use two settings: Point->Block (Point missing pattern in training set, Block missing pattern in testing set.) and Block->Point (Block missing pattern in training set, Point missing pattern in testing set.) As shown in Table 7, our method achieves relatively better performance even when the missing patterns in the training and testing sets differ. This indicates that our trained model can handle imputation tasks in testing environments with missing patterns that are different from those in the training environment."}, {"title": "6 CONCLUSION", "content": "In this paper, we systematically summarize the imputation consistency for improving imputation performance, including the intra-consistency and inter-consistency. We propose a conditional diffusion model for multivariate time series consistent imputation, MTSCI. Specifically, we adopt a complementary mask strategy to introduce intra contrastive loss, ensuring the mutual consistency between the imputed and observed values. Moreover, we utilize an inter-consistency condition network with a mixup mechanism to incorporate the conditional information from adjacent windows to facilitate imputation. Extensive experiments demonstrate the effectiveness of our method, achieving state-of-the-art performance on multiple real-world datasets under various experimental settings. In the future, we will extend our method to apply to more complex missing data scenarios."}]}