{"title": "MTSCI: A Conditional Diffusion Model for Multivariate Time Series Consistent Imputation", "authors": ["Jianping Zhou", "Junhao Li", "Guanjie Zheng", "Xinbing Wang", "Chenghu Zhou"], "abstract": "Missing values are prevalent in multivariate time series, compromising the integrity of analyses and degrading the performance of downstream tasks. Consequently, research has focused on multivariate time series imputation, aiming to accurately impute the missing values based on available observations. A key research question is how to ensure imputation consistency, i.e., intra-consistency between observed and imputed values, and inter-consistency between adjacent windows after imputation. However, previous methods rely solely on the inductive bias of the imputation targets to guide the learning process, ignoring imputation consistency and ultimately resulting in poor performance. Diffusion models, known for their powerful generative abilities, prefer to generate consistent results based on available observations. Therefore, we propose a conditional diffusion model for Multivariate Time Series Consistent Imputation (MTSCI). Specifically, MTSCI employs a contrastive complementary mask to generate dual views during the forward noising process. Then, the intra contrastive loss is calculated to ensure intra-consistency between the imputed and observed values. Meanwhile, MTSCI utilizes a mixup mechanism to incorporate conditional information from adjacent windows during the denoising process, facilitating the inter-consistency between imputed samples. Extensive experiments on multiple real-world datasets demonstrate that our method achieves the state-of-the-art performance on multivariate time series imputation task under different missing scenarios.", "sections": [{"title": "1 INTRODUCTION", "content": "Multivariate time series data widely exists in various real-world applications, e.g., transportation [29, 36], meteorology [1, 17], healthcare [37], energy [2], etc. The integrity of time series plays a crucial role on tasks such as forecasting [26, 48] and classification [22]. However, missing data is a common issue in real-world datasets due to device failures, communication interruptions, and human errors [16, 34], which impairs the downstream task performance and renders the integrity analysis approaches inapplicable. Consequently, multivariate time series imputation, which aims to accurately impute missing values using available observations, arises as an important research question.\nEarly studies are based on statistical learning [4, 13] and machine learning methods [20, 40, 46]. Subsequently, numerous deep learning-based methods have been proposed. Some approaches [5, 6, 9, 33] treat the imputation task as a deterministic point estimation problem, while others [3, 8, 12, 35, 45] view it as a probabilistic generative problem. However, these methods rely solely on the inductive bias of artificially simulated imputation targets to guide the learning process, as illustrated in Figure 1 (a), neglecting the crucial aspect of imputation consistency.\nThe imputation consistency can be divided into two categories: (i)intra-consistency and (ii)inter-consistency. As shown in Figure 1 (b), consider two samples of adjacent windows in incomplete time series. Intra-consistency implies that the imputed values, guided by observed values, should facilitate the reconstruction of observed values, ensuring consistency between imputed and observed values, thereby reducing imputation bias. Inter-consistency means that when imputing a single sample, the sample from the adjacent window should be considered to ensure that the complete sample maintains temporal consistency with the adjacent window.\nThis concept coincides with the idea of good continuity in the temporal samples of adjacent windows [37]. However, no existing imputation method addresses the issue of imputation consistency in multivariate time series imputation task.\nNowadays, diffusion models exhibit powerful generative abilities in synthesizing images [31], audio [7], text [14], and time series [35]. Compared to other models, diffusion models are more inclined to generate data consistent with the distribution of observed data, which aligns well with the concept of imputation consistency [18]. Therefore, to address the aforementioned issues, we propose MTSCI, a conditional diffusion model for Multivariate Time Series Consistent Imputation. MTSCI employs a contrastive complementary mask strategy during the noising process and a mixup mechanism that incorporates conditional information from adjacent windows, effectively guiding the model to maintain consistency and improving imputation performance. Specifically, (i) the complementary mask strategy, which self-supervises the generation of complementary pairs of samples when simulating the imputation targets, imposes contrastive loss to teach the model to maintain intra-consistency between the observed and imputed values. (ii) The mixup mechanism incorporates the temporal characteristics of the adjacent window as conditional information to assist in the imputation of current window during the training stage, thus preserving inter-consistency between adjacent windows. Experimental results on three real-world datasets demonstrate that our method achieves state-of-the-art performance with average improvements of 17.88% in MAE, 15.09% in RMSE, and 13.64% in MAPE compared to baseline methods.\nIn summary, the main contributions of our work are as follows:\n\u2022 Motivated by the neglect of imputation consistency in existing time series imputation methods, we systematically summarize the concept of imputation consistency in multivariate time series imputation task as intra-consistency and inter-consistency.\n\u2022 We propose MTSCI, a conditional diffusion model for multivariate time series consistent imputation, incorporating a complementary mask strategy and a mixup mechanism to realize intra-consistency and inter-consistency.\n\u2022 We conduct extensive experiments on multiple real-world datasets to demonstrate the effectiveness of MTSCI. The results indicate that our method achieves the state-of-the-art performance on multivariate time series imputation task under different missing data scenarios."}, {"title": "2 RELATED WORK", "content": "Existing multivariate time series imputation methods can be divided into four categories: statistical-based methods, machine learning-based methods, deterministic deep models and probabilistic generative models. (i) Statistical methods, such as Mean, Median [13] and KNN [4], utilize the statistical indicators to impute missing values. (ii) Machine learning methods like linear imputation, state-space models [10] and MICE [40] impute missing values based on linear dynamics assumptions. Other machine learning methods like low-rank matrix factorization, e.g., NMF [20], TRMF [46], TIDER [25], factorize incomplete data into low-rank matrices and impute missing values using the product of these matrices. However, these methods struggle with capturing the nonlinear dynamics and handling large datasets. (iii) Deterministic deep models treat imputation as a deterministic point estimation problem. For instance, GRUD [6] uses the last observation and mean of observations to represent missing patterns. BRITS [5] employs bidirectional recurrent neural networks to capture temporal features. mTAN [33] and SAITS [9] design attention mechanisms to capture temporal dependencies. TimesNet [42] transforms 1D time series to 2D space to capture complex temporal variations, achieving state-of-the-art performance in multiple time series tasks, including imputation. However, deterministic methods fall short of modeling imputation uncertainties. (iv) Probabilistic generative models view imputation as a missing values generation problem. For example, GP-VAE [19] combines the VAE [19] and Gaussian process to model incomplete time series. GAIN [45] uses a GAN [15] with a hint mechanism to aid imputation. McFlow [30] leverages normalizing flow generative models and Monte Carlo sampling for imputation. CSDI [35] and MIDM [39] utilize conditional diffusion models to generate missing values by treating the observed values as conditional information. CSBI [8] employs the Schr\u00f6dinger bridge algorithm for imputation. PriSTI [24] extracts conditional information for spatio-temporal imputation using geographic data. However, these methods only use the inductive bias on imputation targets to guide the learning process, which is insufficient to maintain intra-consistency between observed and imputed values within a sequence, as well as inter-consistency between imputed sequences.\nAlthough there are no imputation methods considering the imputation consistency, other studies have explored consistency strategies in time series analysis. Time series representation methods [37, 44] propose a consistency strategy in the sampling process, where time segments within adjacent windows exhibit high pattern consistency, such as consistent trends, periods and amplitudes."}, {"title": "3 PRELIMINARIES", "content": "In this section, we define the problem of multivariate time series imputation, and then introduce the background of diffusion models and conditional diffusion models."}, {"title": "3.1 Problem Definition", "content": "DEFINITION 1 (MULTIVARIATE TIME SERIES.). The multivariate time series denoted as \\(X \\in \\mathbb{R}^{T \\times C}\\) contains C features with length T. The mask matrix denoted as \\(M \\in \\{0,1\\}^{T \\times C}\\) indicates whether the values is observed or missing, where \\(M_{i,j} = 1\\) indicates \\(X_{i,j}\\) is observed, and \\(M_{i,j} = 0\\) indicates \\(X_{i,j}\\) is missing. Then, the observed values in X is denoted as \\(X^o = X \\odot M\\), the missing values in X is denoted as \\(X^m = X \\odot (1 - M)\\).\nPROBLEM 1 (MULTIVARIATE TIME SERIES IMPUTATION.). Given the multivariate time series X and the mask matrix \\(M \\in \\{0,1\\}^{T \\times C}\\) over T time slices, our task of multivariate time series imputation is to estimate the missing values or corresponding distributions in X. The problem can be formulated as learning a probabilistic imputation function \\(p_\\theta\\):\n\\(max_\\theta p_\\theta(X^m | X^o),\\)   (1)\nwhere the goal is to approximate the real the conditional distribution \\(p_\\theta(X^m | X^o)\\) or minimize the estimation error on missing positions."}, {"title": "3.2 Diffusion Models", "content": "A well-known diffusion model is the denoising diffusion probabilistic model (DDPM) [18], which contains the forward noising process and the backward denoising process. During the forward noising process, an input \\(x_0\\) is gradually corrupted to a Gaussian noise vector, which can be defined by the following Markov chain:\n\\(q(x_T|x_0) := \\prod_{k=1}^T q(x_k|x_{k-1}),  q(x_k|x_{k-1}) := \\mathcal{N}(\\sqrt{1 - \\beta_k}x_{k-1}, \\beta_k I),\\)   (2)\nwhere \\(\\beta_k \\in [0, 1]\\) represents the noise level. Then, sampling of \\(x_k\\) can be written as \\(q(x_k|x_0) = \\mathcal{N}(x_k; \\sqrt{\\bar{\\alpha}_k}x_0, (1 - \\bar{\\alpha}_k)I)\\), where \\(\\bar{\\alpha}_k = \\prod_{i=1}^k \\alpha_i\\), and \\(\\alpha_k = 1 - \\beta_k\\). Thus, \\(x_k\\) can be simply obtained as:\n\\(x_k = \\sqrt{\\bar{\\alpha}_k}x_0 + \\sqrt{1 - \\bar{\\alpha}_k} \\epsilon,\\)   (3)\nwhere \\(\\epsilon\\) is a Gaussian noise. During the backward denoising process, DDPM considers the following specific parameterization of \\(p_\\theta(x_{k-1}|x_k)\\):\n\\(p_\\theta(x_{k-1}|x_k) = \\mathcal{N}(x_{k-1}; \\mu_\\theta(x_k, k), \\sigma_\\theta(x_k, k)),\\)   (4)\nwhere the variance \\(\\sigma_\\theta(x_k, k)\\) is usually fixed as \\(\\sigma^2I\\) and the mean \\(\\mu_\\theta(x_k, k)\\) is defined by a denoising network (either \\(x_\\epsilon\\) or \\(\\epsilon_\\theta\\)). For noising estimation, the denoising network \\(\\epsilon_\\theta\\) predicts the noise, and then obtains the mean \\(\\mu_\\theta(x_k, k)\\):\n\\(\\mu_\\theta(x_k, k) = \\frac{1}{\\sqrt{\\alpha_k}} (x_k - \\frac{1 - \\alpha_k}{\\sqrt{1 - \\bar{\\alpha}_k}} \\epsilon_\\theta(x_k, k)).\\)   (5)\nThe denoising network \\(\\epsilon_\\theta\\) is trained by minimizing the loss \\(L_\\epsilon\\):\n\\(L_\\epsilon = \\mathbb{E}_{k, x_0, \\epsilon} [| | \\epsilon - \\epsilon_\\theta(x_k, k) | |_2^2].\\)   (6)\nFor \\(x_0\\) estimation, the denoising network \\(x_\\theta\\) predicts the value \\(x_0\\), and then obtains the mean \\(\\mu_\\theta(x_k, k)\\):\n\\(\\mu_\\theta(x_k, k) = \\frac{\\sqrt{\\alpha_k} (1 - \\bar{\\alpha}_{k-1})}{1 - \\bar{\\alpha}_k} x_k + \\frac{\\sqrt{\\alpha_{k-1}} \\beta_k}{1 - \\bar{\\alpha}_k} x_\\theta(x_k, k).\\)   (7)\nThe denoising network \\(x_\\theta\\) is trained by minimizing the loss \\(L_x\\):\n\\(L_x = \\mathbb{E}_{k, x_0, \\epsilon} [| | x_0 - x_\\theta(x_k, k) | |_2^2].\\)   (8)"}, {"title": "3.3 Conditional Diffusion Models", "content": "The multivariate time series imputation task aims to impute missing values \\(X^m \\in \\mathbb{R}^{T \\times C}\\) based on the conditional information of observed values \\(X^o \\in \\mathbb{R}^{T \\times C}\\). The forward noising process and backward denoising process of conditional diffusion model to impute missing values are defined as follows:\n\\(p_\\theta(x_T|x_0) := p(x_T) \\prod_{k=1}^T p_\\theta(x_k|x_{k-1}, c),  x_T \\in \\mathcal{N}(0, I),\\)   (9)\n\\(p_\\theta(x_{k-1}^m|x_k, x^o) := \\mathcal{N}(x_{k-1}^m; \\mu_\\theta(x_k, k|c), \\sigma_\\theta(x_k, k|c)),\\)   (10)\nwhere \\(c = F(x^o)\\) is the conditional information output of the conditioning network F. By repeatedly running the denoising step in (10) till k = 1, the imputed value \\(\\hat{x}^m\\) is obtained."}, {"title": "4 METHODOLOGY", "content": "In this section, we elaborate on our model, MTSCI, which is designed to tackle the multivariate time series imputation task. We start with an overview of MTSCI in Section 4.1. Then, we introduce the contrastive consistency on forward noising process in Section 4.2. Following that, we explain the consistency-assured denoising process in Section 4.3. Finally, we outline the algorithm procedures for both the training and inference stages."}, {"title": "4.1 Overview", "content": "MTSCI is a conditional diffusion model for multivariate time series consistent imputation. As shown in Figure 2, the \"sampled\" window \\(X_{t-L:t}\\) is sampled for imputation. First, we use contrastive complementary mask to generate two views for contrastive consistency on noising process. Then, we utilize the intra contrastive module and inter-consistency condition network for consistency-assured denoising process, where the intra contrastive module is to calculate contrastive loss between two views, ensuring the intra-consistency between imputed and observed values. The inter-consistency condition network is to incorporate the conditional information from \"context\" window for consistency between adjacent windows."}, {"title": "4.2 Contrastive Consistency on Noising Process", "content": "Goal: Previous works directly use a random mask strategy to generate imputation targets and utilize the remaining observations to impute them. However, this approach does not guarantee consistency between the imputed and the observed values, meaning the imputed values cannot guide the network to reconstruct the observed values. This results in overfitting some outliers and then leads to deviations between the imputed values and their ground-truth. To address this, we utilize a contrastive complementary mask strategy for the \"sampled\" window of the incomplete time series, generating a pair of samples where the imputation targets and observations are complementary. This approach ensures that MTSCI learns to impute missing values consistent with the observed values during training."}, {"title": "4.2.1 Contrastive Complementary Mask", "content": "We refer to the input into the MTSCI as the \"sampled\" window, denoted as \\(x_{t-L:t}\\), and the \"context\" window is denoted as \\(x_{t:t+L}\\). Using the complementary mask strategy, we generate two views of the \"sampled\" window. Based on a random mask matrix m from self-supervised mask modeling process, we consider \\(X_{1,t-L:t} = m \\odot x_{t-L:t}\\) and \\(x_{2,t-L:t} = (1-m) \\odot x_{t-L:t}\\) as the two views, where \\(\\odot\\) denotes element-wise production. The selection of mask patterns and mask ratios in the self-supervised mask modeling process is discussed in Section 5.1."}, {"title": "4.2.2 Noising", "content": "Take the \\(x_{1,t-L:t}\\) as an example, the imputation target and conditional observation are denoted as \\(x_\\text{t}^a = x_{1,t-L:t}^m, x^o = x_{1,t-L:t}^o\\), respectively. Based on (3), we obtain the noised \\(x_k^a\\):\n\\(x_k^a = \\sqrt{\\bar{\\alpha}_k} x_t^a + \\sqrt{1 - \\bar{\\alpha}_k} \\epsilon,\\)   (11)\nwhere \\(\\epsilon\\) is sampled from \\(\\mathcal{N}(0, I)\\) with the same size as \\(x_t^a\\)."}, {"title": "4.3 Consistency-Assured Denoising Process", "content": "Goal: The goal is to introduce how consistency strategies can be ensured in the backward denoising process to enhance imputation performance. We first introduce the intra contrastive loss to constrain the two views generated in Section 4.2, enabling the imputed and observed values to reconstruct each other(intra consistency). Subsequently, we utilize the \"context\" window to provide supplemental conditional information, combined with the observed conditional information in the \"sampled\" window via a mixup mechanism, to teach the model to impute the \"sampled\" window while considering the \"context\" window(inter-consistency)."}, {"title": "4.3.1 Intra Contrastive Module(Intra-consistency)", "content": "The noised contrastive pairs obtained from Section 4.2.2 are input into the denoising network, producing the embedding \\(z_1\\)and \\(z_2\\) after the encoder. Then, we calculate the representation similarity between \\(z_1\\) and \\(z_2\\). To be specific, suppose that the number of samples in one batch is N, then after applying the complementary mask, we have 2N views of samples. Inspired by the cross-entropy formulation of contrastive loss in previous works [11, 21], we calculate the intra contrastive loss as follows:\n\\(L_{CL} = - \\frac{1}{N} \\sum_{i=1}^N log \\frac{exp(sim(z_i, z_i)/\\tau)}{\\sum_{m=1}^{2N} 1_{[m\\neq i]} exp(sim(z_i, z_m)/\\tau)},\\)   (12)\nwhere \\(sim(u, v) = uv/||u||||v||\\) is the cosine similarity, \\(1_{[m\\neq i]} \\in \\{0, 1\\}\\) is an indicator function, \\tau is a temperature parameter. By constraining the representation similarity between two views generated by complementary mask strategy, the model learns to ensure that the imputed values can also reconstruct the observed values during the imputation process."}, {"title": "4.3.2 Inter-consistency Condition Network(Inter-consistency)", "content": "Existing methods only use the observed values to impute imputation targets in \"sampled\" window, ignoring the conditional information of the \"context\" window actually. To address this problem, we utilize the \"context\" window to provide supplemental conditional information, teaching the model to impute the \"sampled\" window while maintaining the contextual consistency on adjacent windows. Although the \"context\" window is accessible during training, it is not available during inference. Therefore, we incorporate a mixup mechanism [32, 49] to combine the conditional information in the \"sampled\" window with that in the \"context\" window as follows:\n\\(x^{\\text{mix}} = m_k \\odot x_{t-L:t} + (1 - m_k^2) \\odot F(x_{t:t+L}),\\)   (13)\nwhere \\(x^{\\text{mix}}\\) represents the mixed conditional information, \\(m_k\\) is a mixing coefficient matrix sampled from the uniform distribution \\(\\mathcal{U}(0, 1)\\), and F is a convolution function with 1 x 1 kernel size. Notably, we only utilize \"context\" windows during the training stage, while during the inference stage, we set all elements in \\(m_k\\) to 1. Then, \\(x^{\\text{mix}}\\) is passed through a linear layer to obtain the hidden representations c:\nc = Linear\\((x^{\\text{mix}})\\).   (14)\nSubsequently, the conditional information representation c, which incorporates information from adjacent windows, is used in the encoder of the denoising network to capture dependencies between observed and missing values."}, {"title": "4.3.3 Denoising Network", "content": "The denoising network accepts the noised \"sampled\" window and the conditional information representation c to predict the noise at missing positions, and then generate the imputed values.\nEmbedding: First, the noised \"sampled\" window \\(x_k^a\\) is embed on a linear layer, plus with the diffusion step representation \\(p^k\\) to acquire the hidden representation h:\nh = Linear\\((x_k^a)\\) + p^k,   (15)\np^k = FeedForward\\((kembedding(k))\\),   (16)\nkembedding\\((k)\\) = [\\(\\sin(10^{0 \\times 4} k)\\),........, \\(\\sin(10^{(d-1) \\times 4} k)\\), cos\\((10^{0 \\times 4} k)\\),..., \\(\\cos(10^{(d-1) \\times 4} k)\\)],   (17)\nwhere FeedForward() is a two fully-conncted layers with the SiLU activation function, kembedding is d-dimension vectors, w = 4. Then, the h is sent to the encoder to learn the temporal and variable dependencies.\nEncoder: First, a linear layer is utilized to fuse the input, and then, we utilize the single-layer vanilla transformer block [38] to capture the temporal dependency. Inspired by the recent work iTransformer [26], we introduce another single-layer invert transformer block to capture the variable dependency. The encoder is stacked by multiple encoder-layers. For a single encoder-layer, the dependency extraction process is defined as follows:\nH = Trm(Linear(h) + TE(h)),\nH^{\\text{inv}} = iTrm(Transpose(H) + FE(H))),   (18)\nwhere H is the output of transformer, \\(H^{\\text{inv}}\\) is the output of inverted transformer, Trm represents the transformer block, iTrm represents the itransformer block, TE() represents the temporal position embedding, FE() represents the feature position embedding.\nDecoder: The decoder is to merge multiple output from the encoder to acquire \\(H^{\\text{out}}\\), and then generate the predicted noise \\(\\hat{\\epsilon}\\) through a feed-forward network implemented by two fully-connected layers with ReLU activation function:\nH^{\\text{out}} = LN(Concat\\((H^{\\text{inv}}, H)\\),\n\\(\\epsilon_\\theta(x_k^a, k|c) = FeedForward(H^{\\text{out}}),\\)\n\\(\\epsilon_{k-1} = \\frac{1}{\\sqrt{\\alpha_k}} (\\frac{x_k^a}{\\sqrt{1 - \\bar{\\alpha}_k}} - \\epsilon_\\theta(x_k^a, k|c)) + \\sigma \\epsilon,\\)   (20)\nwhere \\(H^{\\text{inv}}\\) represents the output of l-th layer, L is the number of encoder layers."}, {"title": "4.4 Training", "content": "During the training process, for each predicted noise \\(\\hat{\\epsilon}\\), we calculate the denoising loss \\(L_\\epsilon\\) as follows:\n\\(L_\\epsilon = \\mathbb{E}_{x_t^a, \\epsilon \\sim \\mathcal{N}(0,1), k} L_\\epsilon (k),\\)\n\\(L_\\epsilon(k) = ||(1 - M) \\odot (\\epsilon - \\epsilon_\\theta (x_k^a, k|c))||_2^2.\\)   (21)\nThen, the total loss L is obtained by adding the denoising loss \\(L_\\epsilon\\) and the intra contrastive loss \\(L_{CL}\\) with weighted coefficient \\(\\lambda\\):\n\\(L = L_\\epsilon + \\lambda L_{CL}.\\)   (22)\nThe complete training procedure is shown in Algorithm 1."}, {"title": "4.5 Inference", "content": "During the inference process, all elements in the mixing coefficient matrix is set to 1. By repeatedly running the denoising step till k equals 1, we obtain the \\(\\hat{\\epsilon}\\) as the final predicted noise. The inference procedure is shown in Algorithm 2."}, {"title": "5 EXPERIMENTS", "content": "In this section, we first introduce the experimental setups, including the datasets, baselines, evaluation metrics and implementation details. Then, we evaluate our model, MTSCI, with extensive experiments to answer the following research questions:\n\u2022 RQ1: How does MTSCI perform against other baselines in the multivariate time series imputation task?\n\u2022 RQ2: How does the imputation consistency of MTSCI contribute to its imputation performance?\n\u2022 RQ3: How does the imputation performance for MTSCI perform about different missing scenarios, including different missing ratios and diverse mask patterns?\n\u2022 RQ4: What is the impact of weighted coefficient \\(\\lambda\\) and major hyperparameters of MTSCI on the imputation performance?"}, {"title": "5.1 Experimental Setup", "content": "Datasets. We evaluate our proprosed model on three commonly used public datasets: an electricity dataset ETT [9], a climate dataset Weather [43], and a traffic speed dataset METR-LA [23]. The statistical details of these datasets are listed in Table 1.\nBaselines. We select 13 baselines to evaluate the performance of our proposed method on multivariate time series imputation task. These baselines include statistical methods (Mean, KNN), typical machine learning methods (MICE, TRMF), deterministic imputation methods (BRITS, mTAN, SAITS, TimesNet, Non-stationary Transformer) and deep generative imputation models (GP-VAE, rGAIN, CSBI, CSDI). We briefly introduce the baseline methods as follows: (1)Mean: directly use the average value of observed values to impute. (2)KNN: use the average value of similar samples to missing sample, as implemented by fancyimpute. (3)MICE [40]: multiple imputation method by chained equations. (4)TRMF [46]: a temporal regularized matrix factorization method. (5)GP-VAE [12]: combine VAE [19] with Guassion process for time series probabilistic imputation. (6)rGAIN [45]: a GAN-based method with a bidirectional recurrent encoder-decoder. (7)BRITS [5]: use bidirectional RNN for multivariate time series imputation. (8)mTAN [33]: use multi-time attention network to impute missing values, which is a transformer-based method. (9)SAITS [9]: use joint-optimization training strategy (masked imputation task and observed reconstruction task) to impute missing values, which is also a tranformer-based method. (10)Non-stationary Transformer [27]: a tranformer-based method to attenuate time series non-staionary. (11)TimesNet [42]: transform the 1D time series into 2D space and capture the temporal 2D-variations dependencies. Both Non-stationary Transformer and TimesNet are the state-of-the-art multivariate time series imputation methods implemented in TSlib. (12)CSBI [8]: use the Schr\u00f6dinger bridge algorithm to probabilistic time series imputation. (13)CSDI [35]: a score-based conditional diffusion model for probabilistic time series imputation method.\nEvaluation metrics. Three commonly used metrics in multivariate time series imputation task are used to evaluate the performance of all methods, including the Mean Absolute Error(MAE), Root Mean Squared Error(RMSE), Mean Absolute Percentage Error(MAPE).\nImplementation. We divide the training/validation/testing set following the settings of previous works [9, 24, 43]. For ETT, we select the first four-month data(2016/07-2016/10) as the testing set, the following four-month data(2016/11-2017/02) as the validation set, and the left sixteen months(2017/03-2018/06) as the training set. For Weather and METR-LA, we split the training/validation/testing set by 70%/10%/20%. We divide the samples by a window size of 24 steps without overlapping. We consider two different missing patterns to artificially simulate the missing values for evaluation: (1)Point missing: we randomly mask 20% data points in the datasets. (2)Block missing: based on randomly masking 5% of the observed data, mask observations ranging from [L/2, 2L] (L is the window size) with 0.15% probability. For training strategies, we utilize point and block strategies for self-supervised learning. Specifically, for point strategy, we randomly chooser r (r\u2208 [0%, 100%]) of observed values as imputation targets. For block strategy, we randomly choose a sequence with a length in the range [L/2,L] with a probability r (r\u2208 [0%, 15%]) as imputation targets."}, {"title": "5.2 Overall Performance(RQ1)", "content": "The overall performance is shown in Table 2 and Table 3. We make the following observations: (1) MTSCI achieves the state-of-the-art performance across multiple datasets, both in point missing and block missing patterns, with an average improvement of 17.88% in MAE, 15.09% in RMSE and 13.64% in MAPE, respectively. Notably, in the case of block missing pattern with continuous missing scenarios, our method demonstrates greater superiority, highlighting the effectiveness of the imputation consistency strategy adopted in MTSCI. (2) The statistical methods and classical machine learning methods perform poor on all datasets due to the strong non-linearity of incomplete multivariate time series. These methods impute missing values based on assumptions such as stability or linear dynamics, which fail to capture the complex temporal correlations in real-world datasets. (3) Compared to deterministic imputation models, MTSCI achieves performance improvements of 42.07% in MAE, 24.15% in RMSE and 39.76% in MAPE respectively on average. The difference between these methods and our model is that we employ the conditional diffusion process to model the incomplete time series imputation task, which refines the imputed values through multiple steps instead of non-autoregressive single step imputation as deterministic methods do. (4) Compared with deep generative imputation methods, MTSCI consistently outperforms on several datasets. This indicates that our imputation consistency strategy effectively enhances imputation performance. Previous generative models, including the conditional diffusion models like CSBI and CSDI, still exhibit significant errors because they rely solely on self-supervised masking strategy to generate imputation targets and directly guide the denoising network through the inductive bias at the imputation targets. In contrast, MTSCI utilizes a complementary mask strategy to generate dual views for intra contrastive loss and a mixup mechanism to combine conditional information from adjacent windows, facilitating more accurate and consistent imputation performance."}, {"title": "5.3 Ablation Study(RQ2)", "content": "We conduct an ablation study to evaluate the effectiveness of the complementary mask strategy to generate intra contrastive loss and the inter-consistency condition network with mixup mechanism for utilizing conditional information from adjacent windows. We compare three variants of MTSCI with and without these components: (1) We remove the complementary mask strategy on the forward noising process along with the intra-consistency loss. This variant is denoted as w/o intra. (2) We remove the mixup mechanism in inter-consistency condition network, which is denoted as w/o inter. (3) We use only the conditional information of observed values from single window and the denoising network of MTSCI, without the complementary mask strategy and mixup mechanism. This variant is denoted as w/o cons.\nFigure 3 shows the performance comparison of these three variants and MTSCI. First, we observe that without the complementary mask strategy, the performance of this variant deteriorates. This indicates that generating contrastive views to facilitate the mutual reconstruction of observed and imputed values, improves the imputation performance. Second, using adjacent windows to provide supplemental conditional information also enhances the imputation performance. This suggests that the adjacent windows can bring contextual consistency constraints to the missing values of the current window, alleviating the estimation error between the imputation results and the ground-truth. Finally, the collaboration of these modules jointly improves the imputation performance, further confirming the necessity of utilizing both simultaneously. We also conduct experiments to compare the performance using the same denoising network architecture but with different objectives: predict noise \\(\\epsilon\\) or predict missing values \\(x_k^a\\). As shown in Table 4, performance is better when the denoising network's objective is to predict noise. This is likely because noise follows a Gaussian distribution, which aids the complementary mask views in predicting noise and reconstructing each other, thereby maintaining the intra-consistency. However, the distributions of observed values and missing values may not be the same."}, {"title": "5.4 Case Study(RQ2)", "content": "In order to intuitively understand how MTSCI imputes the incomplete time series, we visualize the imputation results of MTSCI and the sub-optimal method CSDI. Specifically, we randomly select three snapshots of incomplete time series with block missing pattern from three datasets. As shown in Figure 4, MTSCI demonstrates better imputation performance compared to the CSDI, which is attributed to intra contrastive consistency and inter-consistency condition network. In addition, compared to three variants of MTSCI, our model exhibits consistent trend between imputed and observed values, alleviating the imputation error. This improvement is due to MTSCI's consideration of intra-consistency within a single window that observed and imputed values can reconstruct each other, as well as the inter-consistency between adjacent windows that can provide supplemental condition information to guide imputation. Additionally, we use the CRPS metric [28, 35] to measure the imputation consistency between the imputed results and the observed values at the whole dataset. As shown in Table 5, our method outperforms the sub-optimal method CSDI."}]}