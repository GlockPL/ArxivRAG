{"title": "SPARKLE: Enhancing SPARQL Generation with Direct KG Integration in Decoding", "authors": ["Jaebok Lee", "Hyeonjeong Shin"], "abstract": "Existing KBQA methods have traditionally relied on multi-stage methodologies, involving tasks such as entity linking, subgraph retrieval and query structure generation. However, multi-stage approaches are dependent on the accuracy of preceding steps, leading to cascading errors and increased inference time. Although a few studies have explored the use of end-to-end models, they often suffer from lower accuracy and generate inoperative query that is not supported by the underlying data. Furthermore, most prior approaches are limited to the static training data, potentially overlooking the evolving nature of knowledge bases over time. To address these challenges, we present a novel end-to-end natural language to SPARQL framework, SPARKLE. Notably SPARKLE leverages the structure of knowledge base directly during the decoding, effectively integrating knowledge into the query generation. Our study reveals that simply referencing knowledge base during inference significantly reduces the occurrence of inexecutable query generations. SPARKLE achieves new state-of-the-art results on SimpleQuestions-Wiki and highest F1 score on LCQUAD 1.0 (among models not using gold entities), while getting slightly lower result on the WebQSP dataset. Finally, we demonstrate SPARKLE's fast inference speed and its ability to adapt when the knowledge base differs between the training and inference stages.", "sections": [{"title": "1 Introduction", "content": "Knowledge Base Question Answering (KBQA) is a task which aims to answer user queries by extracting relevant information from structured knowledge bases, such as DBPedia (Auer et al., 2007), Freebase (Bollacker et al., 2008) and Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014). KBQA systems enable users to interact with abundant information in knowledge bases without requiring an in-depth understanding of query languages. Existing KBQA approaches have often adopted multi-stage pipelines, which involve entity linking frameworks such as DPR (Karpukhin et al., 2020), BLINK (Wu et al., 2020), and ELQ (Li et al., 2020) or subgraph retrieval mechanisms like KNN and Personalized PageRank (Haveliwala, 2002) to handle the complexity of large-scale data sources. While these strategies have shown promise in improving accuracy, they introduce undesirable side effects, especially an increase in inference time and a dependency on the performance of preceding modules (Yu et al., 2023).\nIn contrast to multi-stage methods, some studies have explored alternative approaches to KBQA. For instance, end-to-end pre-trained language models (PLMs) have been employed to generate query language (Yin et al., 2021; Rony et al., 2022) or direct answers among entities (Saffari et al., 2021; McKenna and Sen, 2023). While end-to-end semantic parsing models offer simplicity, they may produce invalid queries, as they lack an understanding of valid facts and connections in the knowledge base. Moreover, these models are often tightly bound to their training data, struggling to adapt to the evolving nature of knowledge.\nIn this paper, we present a novel solution to address the aforementioned challenges in KBQA. We propose a straightforward yet effective end-to-end natural language to SPARQL model for KBQA. Building upon the concept of entity and relation retrieval within a generation model (De Cao et al., 2021; Rossiello et al., 2021), we extend this approach to SPARQL query generation. Our model not only generates entities and relations within a SPARQL query, but also directly leverages the structural information in the knowledge base to generate valid triple patterns. This is achieved through the constrained decoding within a single sequence-to-sequence model. Such an integration is natural and seamless, allowing left-to-right decoding process to accurately reflect the semantic structure of the knowledge base.\nWe empirically evaluate SPARKLE on three benchmark datasets: WebQSP (Yih et al., 2016), SimpleQuestions-Wiki (Diefenbach et al., 2017), and LCQUAD 1.0 (Trivedi et al., 2017). Our model achieves new state-of-the-art result on SimpleQuestions-Wiki (+3.5 F1 score), and competitive result on LCQuAD 1.0 (the highest F1 score among models that do not use gold entities). While its performance on WebQSP slightly lags behind, it still achieves the highest Hits@1 score among end-to-end methods, a point we will discuss in Section 5.2. SPARKLE not only delivers fast inference times (under 1 second), suitable for real-world scenarios, but also supports batch processing of multiple questions simultaneously. Additionally, we demonstrate that SPARKLE enables seamless adaptation to updated knowledge bases during inference without retraining. Our model, initially trained with 2016-04 DBPedia, successfully retrieves newly added facts about events occurring between April and October 2016, by simply switching the knowledge base for inference to the 2016-10 DBPedia. The contributions of this paper are summarized as follows:\n\u2022 We propose a novel end-to-end method for natural language to SPARQL translation through the constrained decoding in a single sequence-to-sequence model. To the best of our knowledge, no existing method uses the structure of knowledge base directly within decoding.\n\u2022 SPARKLE is evaluated on three distinct KBQA datasets, each linked to a different knowledge base. Experimental results indicate that our approach achieves new state-of-the-art or competitive performance on SimpleQuestions-Wiki and LCQuAD 1.0, while showing fast inference speed and enabling batch processing.\n\u2022 Further analysis demonstrates our model's capability to adapt to evolving knowledge base during inference without additional training."}, {"title": "2 Related Work", "content": "2.1 Multi-Stage Approaches for KBQA\nIn the field of Knowledge Base Question Answering (KBQA), numerous researchers have concentrated on multi-stage approaches. These methods decompose the KBQA process into several stages, including entity linking, relation prediction, subgraph retrieval and query structure generation. Such segmentation allows for handling the complexity of large-scale data sources more effectively. PullNet (Sun et al., 2019) iteratively identifies question-specific entities, constructs a subgraph, and then finds answers using a Graph Convolutional Network. EDGQA (Hu et al., 2021) decomposes the input question into an entity description graph using rules, from which subqueries are generated. DECAF (Yu et al., 2023) proposes text-based retrieval and generates logical form through Fusion-in-Decoder (Izacard and Grave, 2021). Many studies utilize pre-trained language models (PLMs) for query structure generation (Das et al., 2021; Hirigoyen et al., 2022; Banerjee et al., 2022; Ravishankar et al., 2022; Ye et al., 2022; Chen et al., 2022; Banerjee et al., 2023; Huang et al., 2023; Wang et al., 2023; Omar et al., 2023), combined with independent entity linking modules. Li et al. (2023a) generates query drafts through Large Language Model (LLM) and refine them by linking entities with BM25 and FACC1 (Gabrilovich et al., 2013). FlexKBQA (Li et al., 2023b) also utilizes an LLM, but for generating synthetic data. It then trains a teacher model, incorporating entity linking results from Li et al. (2020).\nThe performance of such multi-stage approaches is intrinsically tied to the outcomes of preceding stages. Longer inference time also happens as a consequence. The key distinction between multi-stage studies and SPARKLE is attributed to its end-to-end framework. This ensures that the performance is influenced solely by a single model, thereby reducing inference times.\n2.2 End-to-End Approaches for KBQA\nEnd-to-end methods in KBQA leverage a single neural network model, providing an advantage in terms of simplicity. Recent studies have incorporated PLMs to generate queries or retrieve direct answers. Rigel-E2E (Saffari et al., 2021) jointly performs entity resolution and inference using a differentiable knowledge graph construction suggested by Cohen et al. (2020). KG-Flex (McKenna and Sen, 2023) decodes into a continuous embedding space where relations are expressed in natural language, enabling use of new relations at test time without retraining. Similar to our approach, some studies (Soru et al., 2018; Rony et al., 2022) uses PLMs to generate full SPARQL queries with various linguistic features to overcome the complexity in multi-stage approaches. However, these methods can generate invalid triple patterns and is inherently bound to the knowledge base used during training when conducting inference. On the other hand, SPARKLE utilizes the structural information embedded in the knowledge base in real-time, enabling adaptive inference on the evolving knowledge base.\n2.3 Semantic Parsing with Constraints\nSemantic parsing methods often integrates constraints with a language model to ensure the output is grammatically correct. PICARD (Scholak et al., 2021) provides multiple levels of constraints, from lexical to grammatical, for text-to-SQL generation. Pangu (Gu et al., 2023), on the other hand, utilizes an external symbolic agent that extends S-expressions iteratively. TIARA (Shu et al., 2022) focuses on constrained decoding to generate valid KB classes and relations. ArcaneQA (Gu and Su, 2022) employs constrained decoding to narrow down the search space following pre-defined expansion rules.\nNonetheless, these KBQA approaches (Gu et al., 2023; Gu and Su, 2022) that apply constraints on S-expressions still encounter difficulties in converting S-expressions to SPARQL queries (Hu et al., 2022). SPARKLE focuses on generating a valid triple pattern by directly integrating knowledge base in the model without complex pre-defined rules or external components. This method ensures SPARKLE naturally captures semantic structure of knowledge base during ongoing left-to-right query generation since SPARQL arranges a triple pattern in the order of (h, r, t): \"h performs r on t\"."}, {"title": "3 Preliminaries", "content": "A Knowledge Graph (KG), denoted as G is a collection of triples where each triple consists of three elements: a subject entity denoted as h, a relation denoted as r, and an object entity denoted as t. Formally, it is represented as {(h,r,t)|h,t \u2208 E,r \u2208 R}, where E represents the set of entities and R denotes the set of relations. Each triple signifies the existence of a relational connection between the head entity h and the tail entity t.\nThe term SPARQL is an acronym for SPARQL Protocol and RDF Query Language. This query language plays an important role in a knowledge graph systems, enabling users to retrieve and manipulate data stored in RDF (Resource Description Framework) format. SPARQL consists of query forms (e.g., ASK, SELECT, CONSTRUCT, DESCRIBE), modifiers (e.g., ORDER, PROJECTION, DISTINCT etc), and triple patterns.\nIt is important to note that the validity of a SPARQL query hinges on the existence of the specified triple patterns in the underlying RDF data. For instance, consider a user asking, \"What Michael Bay work has nominated for Academy Awards?\" in a knowledge graph like Figure 1. A typical sequence-to-sequence model for SPARQL generation may include a triple pattern (Michael_Bay, write, ?var) in a query, interpreting 'write' as synonymous with 'make'. However, this pattern would be invalid because Michael Bay has no record of writing any movies in the underlying graph. This example highlights the significance of incorporating real linkage information from the knowledge graph when generating SPARQL queries."}, {"title": "4 Method", "content": "We address the natural language to SPARQL problem using a single sequence-to-sequence model with constrained decoding during inference. As depicted in Figure 2, our decoding involves two types of constraints. We first focus on generation of entity and relation. Additionally, we leverage the interconnected structure of the knowledge base when decoding relations after the head entity and, correspondingly, tail entities after the relation in triple patterns. As highlighted by De Cao et al. (2021), making entity and relation identifiers meaningful is crucial for the success of a sequence-to-sequence model to retrieve them. Considering that knowledge bases such as Wikidata and Freebase use arbitrary identifiers, we convert these into more intuitive, human-readable identifiers using their names and types. For instance, we transform Quentin Tarantino's Freebase ID, m.06931, into a human-readable format \"[quentin tarantino (film director)]\" (details in Appendix A). SPARKLE is trained using a standard sequence-to-sequence objective, which aims to maximize the likelihood of the output sequence.\n4.1 Entity and Relation Generation\nFor the generation of entity and relation, SPARKLE employs an autoregressive formulation that assigns a score to each entity \\(e \u2208 E\\) and relation \\(r\u2208 R\\), denoted as \\(p(z|x)\\).\n\\(p(z|x) = \\prod_{i=1}^{n}P_\\theta(y_i | x, y_1, ..., y_{i-1})\\)  (1)\nHere, y represents the set of tokens in the identifiers of \\(e \u2208 E\\) and \\(r \u2208 R\\), x is the input, \u03b8 stands for the model parameters, and z denotes either e or r. To navigate the search space effectively, we make use of Beam Search (Sutskever et al., 2014) decoding strategies. Currently, many multi-stage KBQA methods (Ye et al., 2022; Hu et al., 2022; Ravishankar et al., 2022) exploit a dense retriever such as BLINK (Wu et al., 2020) and ELQ (Li et al., 2020). This approach, however, leads to retrieval costs increasing linearly with the growth of the knowledge base, as each input sentence must be compared against all entities or relations. Instead, we simply rank multiple SPARQL queries that contain various entities and relations using Beam Search. This significantly reduces the cost associated with the retrieval. The time required for this process is now dependent on the size of beams and the length of identifiers, making it more manageable.\nTo enforce SPARKLE to generate only valid identifiers for entities and relations, we define identifier tries T as described by De Cao et al. (2021). An example of such a trie is depicted in Figure 2a. Each node within T is annotated with tokens from the vocabulary. As the model traverses the nodes in the trie starting from the root, it generates a next token based on the previous ones. Therefore, each child node represents all the possible continuous tokens required to construct valid identifiers.\n4.2 Pruning Invalid Triple Patterns\nWe additionally extend constrained decoding to prevent the model from generating invalid triple patterns within SPARQL queries. There are two scenarios where we can exploit the structural information of the knowledge base. The first case occurs when generating a relation after the head entity has been generated. In this case, we constrain the decoding process to consider only relations that are linked to the head entity. The probability formulation for this scenario is as follows:\n\\(p(r = N(e_{head})|x) = \\prod_{i=1}^{n}P_\\theta(w_i | x, w_{<s}, e_{head})\\)  (2)\nwhere w denotes the set of tokens available, \\(e_{head}\\) represents the head entity, and N indicates the set of neighbors. The subscript s...i is used to denote the indices of the most recently generated identifier tokens because each identifier in the output of language model is represented by a series of tokens rather than a single token. For instance, if the model has generated \"SELECT ?var { [ Michael_Jordan]\" up to this point, it's understood that the head entity identifier comprises multiple tokens (e.g., [, Michael, , Jo, rdan, ]). In this notation, s is used to mark the index of the first token of the head entity, enabling the model to accurately identify the entity within the ongoing query generation.\nThe second case concerns the generation of a tail entity after the relation has been generated. Here, we restrict the tail entity decoding process to consider only entities that have a connection to the specified relation in the knowledge base. The probability formulation for this scenario is as follows:\n\\(p(e_{tail} \u2208 N(r)|x) = \\prod_{i=1}^{n}P_\\theta(w_i | x, w_{<s}, r_{s,...,i})\\)  (3)\nThe applicability and effectiveness of these strategies are rooted in the nature of SPARQL, which organizes triple patterns in the order of (h,r,t). This structure is inherently compatible with the left-to-right decoding of sequence-to-sequence models, making it feasible to implement these constraints efficiently. We enforce these constraints by masking the log probabilities of tokens (setting their score to \u2013 inf) for invalid entities, relations, and connectivities. Our approach not only ensures the syntactical accuracy of the generated triple pattern but also aligns them with the semantic structure of the underlying knowledge base."}, {"title": "5 Experiments", "content": "5.1 Experimental Setting\n5.1.1 Dataset\nIn our experiments, SPARKLE is evaluated across three benchmark datasets, each sourced from different knowledge bases: LCQuAD 1.0 (Trivedi et al., 2017), WebQSP (Yih et al., 2016) and SimpleQuestions-Wiki (Diefenbach et al., 2017). SimpleQuestions-Wiki, a large-scale KBQA dataset, provides 14,184 train questions, 2,111 dev questions and 4,116 test questions, annotated on Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014). We use Wikidata dump from December 2017 and filter out questions whose triples are not supported by the dump. LCQuAD 1.0 comprises 5,000 questions accompanied by corresponding SPARQL queries, each of which can be answered using DBpedia (Auer et al., 2007) 2016-04. We allocate 200 questions from the 4,000 in the training dataset to a dev dataset. WebQSP consists of 4,937 questions designed for semantic parsing on Freebase (Bollacker et al., 2008). We adopt the same train and dev splits employed by Yu et al. (2023).\n5.1.2 Evaluation Metrics\nFollowing previous works (Ravishankar et al., 2022; Ye et al., 2022; Yu et al., 2023), our model is evaluated using Hits@1 and F1 score. Both metrics function as a comprehensive gauge of our model's ability to retrieve answer sets in KBQA.\n5.1.3 Implementation Details\nWe first build identifier tries using Marisa trie, a memory-efficient trie, designed by Yata (2011). This is important because a knowledge base can have millions of components. To make sure that looking up connections in the knowledge base does not slow down the decoding, we store this information in a hash table. To ensure that the model can effectively handle SPARQL expressions, we tokenize each SPARQL terms (e.g., \u2018?var', 'SELECT' and 'ORDER BY') as individual tokens. When generating entities, we insert variables to the candidates so that the model can choose to generate them. For training, we fine-tune the pre-trained BART model (Lewis et al., 2020) with a sequence-to-sequence objective, maximizing log p\u03b8(y|x) with respect to model's parameters \u03b8, which is commonly used in neural machine translation. In the inference phase, we use beam search to generate the top-k SPARQL queries. These queries are executed in turn until a non-empty query result is obtained, following the previous works (Hu et al., 2022; Shu et al., 2022). For more training details, refer to Appendix B.\n5.2 Overall Performance\nWe evaluate SPARKLE with a variety of KBQA methods, including but not limited to natural language to SPARQL models. In Table 1, we compare SPARKLE with multi-stage methods on two benchmark datasets. Our findings highlight that SPARKLE excels on SimpleQuestions-Wiki and LCQUAD 1.0, setting a new state-of-the-art on Simple Questions-Wiki. SPARKLE exhibits competitive results on LCQuAD 1.0, aligning closely\n5.3 Analysis\n5.3.1 Impact of Constrained Decoding\nTo assess the effect of the proposed constrained decoding, we conduct ablation studies in Table 1, 2. SPARKLE without pruning refers to our model only applying constraint on retrieving entities and relations in a generative way, without utilizing structural information for pruning. SPARKLE without constraints describes our model operating entirely unconstrained, identical to a BART model during inference. The results clearly demonstrate a gradual decrease in performance when constraints are removed in sequence. Specifically, the absence of any constraints leads to a significant performance decline, with a reduction of up to 14.9 F1 scores observed in the LCQUAD 1.0 dataset.\nMoreover, there is a noticeable increase in the proportion of queries that cannot be executed. Without any constraints, the ratio of inexecutable queries rises to 27.9%, 12.4%, and 17.5% for SimpleQuestions-Wiki, LCQuAD 1.0, and WebQSP, respectively. This indicates that a standalone sequence-to-sequence model struggles to accurately generate valid identifiers of entity and relation, as well as triple patterns. Therefore it becomes clear that constrained decoding is crucial for the generation of valid query components.\n5.3.2 Impact of Beam Size\nIn Figure 3, we present a comprehensive overview of how beam size influences the performance of our model across three benchmark datasets. On LCQuAD 1.0 and WebQSP, we observe a significant improvement in the model's performance as beam size increases. This improvement appears to reach a saturation point when the beam size approaches 7. Beyond this point, further increases in beam size yield diminishing returns in terms of performance enhancement.\nWhen the beam size is set to 1, representing a greedy decoding, the model is prone to making incorrect predictions for entities and relations. This problem arises due to the abundance of entities that share similar label and type in the knowledge base, leading to similar identifiers. Such similarities pose challenges for the model in accurately retrieving the correct entity. This initial retrieval failure has a cascading effect on the model's ability to prune invalid triple patterns since subsequent retrievals are based on the preceding, potentially erroneous choices. Consequently, increasing beam size acts as a safeguard against such erroneous selections, ultimately reducing the likelihood of generating inaccurate triple patterns within the query.\nNonetheless, increasing the beam size does not yield similar improvements on SimpleQuestions-Wiki. SPARQL queries in SimpleQuestions-Wiki involve only a single triple pattern, and many questions explicitly include the surface forms of entity and relation. It seems increasing beam size rather degrades the model performance (Cohen and Beck, 2019) on such straightforward questions.\nAnother noteworthy observation is that the rate of inexecutable queries remains consistently near zero, irrespective of the beam size, indicating that simple constrained decoding is effective in ensuring the generation of executable queries.\n5.3.3 Adaptive Inference\nSPARKLE, in its operation, dynamically uses the structure of knowledge base at runtime. This straightforward approach empowers SPARKLE to make adaptive inferences based on a knowledge base that differs from the one used during its training. We initially trained SPARKLE using LCQuAD 1.0 with DBPedia 2016-04 dump. Subsequently, we put SPARKLE to the test by performing inferences using DBPedia 2016-10 dump, representing a knowledge base that evolved over time. To assess the model's adaptability, we manually create two questions that are related to the events occurred between April 2016 and October 2016. These questions include newly introduced entity and newly linked relation. The results of theses evaluations are presented in Table 3.\nIn Case I, SPARKLE successfully retrieves the newly registered entity <http://dbpedia.org/resource/AlphaGo_versus_Lee_Sedol> which was not a part of the knowledge base during the model's training. In Case II, when SPARKLE performs inference using DBPedia 2016-04, it is unable to generate a pattern involving <http://dbpedia.org/resource/Bob_Dylan> and <http://dbpedia.org/ontology/award>. This is due to the fact that the relation <http://dbpedia.org/ontology/award> was added to the entity <http://dbpedia.org/resource/Bob_Dylan> after he received the Nobel Prize in Literature in October 2016. These examples vividly illustrate SPARKLE's ability to perform adaptive inference when confronted with newly introduced entities and relations without retraining.\nIn Table 4, we additionally evaluate SPARKLE's ability to adapt with queries involving partially or entirely unseen entities or relations. Each test set is split into two categories: Seen and Unseen. Unseen refers to queries where at least one entity or relation is not encountered during training. Despite new entities and relations, SPARKLE shows robust performance on such questions.\n5.3.4 Inference time\nWe assess the efficiency of SPARKLE by measuring the average inference time per question on the test split of each dataset. The distinctive feature of SPARKLE lies in its single sequence-to-sequence model architecture, which results in fast inference speed as shown in Table 5. The experiments used an NVIDIA V100 GPU and a beam size of 10 during decoding.\nSince many KBQA approaches employ multi-stage methods, they often suffer from longer inference time (Gu et al., 2021; Ye et al., 2022; Shu et al., 2022). Multi-stage approaches involve the loading and unloading of data to and from the GPU when processing questions in a sequential manner. A direct comparison of inference times between SPARKLE and other KBQA models is challenging for several reasons. Different models often employ varying datasets and some send queries via SPARQL endpoint (e.g., Virtuoso server) for intermediate computations. Given these complexities, our analysis focuses on the marginal increase in latency by SPARKLE over a naive sequence-to-sequence model. This slight increase stems from the computational overhead of masking probabilities for invalid tokens during auto-regressive generation. Nonetheless, employing a Trie structure for retrieving identifiers and a hash table for managing connectivity significantly mitigates complexity. Moreover, as discussed in Section 5.2, the model's performance stabilizes with a beam size of 7, allowing faster inference without significant performance loss.\nAdditionally, one of the key strengths of SPARKLE is its ability to perform batch processing. As an end-to-end system, it can handle multiple queries simultaneously, potentially improving throughput in practical applications. As shown in Table 5, employing batch processing with a batch size of 8 reduces the average inference time per query by up to 41%. This feature is especially beneficial in real-world scenarios where handling large volumes of queries is essential."}, {"title": "6 Conclusion", "content": "In this work, we present SPARKLE, a novel end-to-end approach that directly use the structural information of knowledge base to enhance SPARQL query generation. SPARKLE employs a straightforward yet effective strategy of constrained decoding in two contexts: retrieving entities and relations in a generative way, and pruning invalid triple patterns based on knowledge base structure. The experimental results show that our approach helps sequence-to-sequence models generate executable queries, resulting in strong performance across benchmark datasets: SimpleQuestions-Wiki, LCQuAD 1.0 and WebQSP. Moreover, SPARKLE's adaptability is demonstrated as it can accommodate new entities and relations without retraining, simply by switching the underlying knowledge base during inference. We additionally show that SPARKLE offers faster inference time and supports batch processing, allowing simultaneous handling of multiple questions."}, {"title": "Limitations", "content": "Although our approach shows good performance with its simple architecture, there remains scope for further enhancements. Our model requires substantial memory resources to utilize structural information of knowledge base during decoding. As the size of the knowledge base grows, these memory requirements increase linearly. Considering that large knowledge bases often contain over millions of entities, managing the connectivity information for such knowledge base becomes challenging. However, for large platforms providing KBQA services, the bigger challenge is not memory resources; rather, it is delivering a real-time service to their users.\nMoreover, our use of constrained decoding is currently restricted to the generation of triple patterns. While triple pattern is the most important component of SPARQL queries, SPARQL itself comprises more advanced expressions. For instance, a bottom-up constrained parsing for nested queries, such as those involving UNION clause, could improve our model. A comprehensive SPARQL query generation requires addressing these additional components. Looking ahead, we plan to enhance our models by incorporating such grammatical analysis of SPARQL to support more sophisticated query constructs."}, {"title": "A Human-readable identifiers", "content": "Table 6 provides examples of how we convert entity IRIs into human-readable identifiers. While DBPedia (Auer et al., 2007) includes the entity's label and type in its IRIs, rendering them already meaningful, Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) and Freebase (Bollacker et al., 2008) use random characters for their IRIs. To make these IRIs more interpretable, we extract the label and type of an entity from the respective knowledge bases and format the identifiers as label(type). For Wikidata entities, we determine their type using the P31 (instance of) relation. In Freebase, common.topic.notable.types is used for this purpose. When an entity is associated with multiple types, we randomly select two of these for construction of the identifier. Additionally, if an entity shares its label and type with others, we append its IRI to the end of the identifier to ensure uniqueness. To clearly differentiate entities and relations from other text elements, we enclose the identifier with square brackets at its beginning and end."}, {"title": "B Training Details", "content": "SPARKLE is developed using PyTorch (Paszke et al., 2019) and HuggingFace library (Wolf et al., 2019) for both training and inference. Throughout the training process, Adam optimizer (Kingma and Ba, 2015) is utilized. The model's learning rate is determined through experiments and searched from [5e-4, 5e-5, 5e-6]. Our training objective is a sequence-to-sequence cross-entropy loss without label smoothing. The models are trained using a batch size of 32. The experiments are conducted on 4 to 8 NVIDIA V100 GPUs. For the output generation, the maximum token length is set to 128 for all datasets."}]}