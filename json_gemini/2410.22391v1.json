{"title": "A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks", "authors": ["Thomas Schmied", "Thomas Adler", "Vihang Patil", "Maximilian Beck", "Korbinian P\u00f6ppel", "Johannes Brandstetter", "G\u00fcnter Klambauer", "Razvan Pascanu", "Sepp Hochreiter"], "abstract": "In recent years, there has been a trend in the field of Reinforcement Learning (RL) towards large action models trained offline on large-scale datasets via sequence modeling. Existing models are primarily based on the Transformer architecture, which result in powerful agents. However, due to slow inference times, Transformer-based approaches are impractical for real-time applications, such as robotics. Recently, modern recurrent architectures, such as xLSTM and Mamba, have been proposed that exhibit parallelization benefits during training similar to the Transformer architecture while offering fast inference. In this work, we study the aptitude of these modern recurrent architectures for large action models. Consequently, we propose a Large Recurrent Action Model (LRAM) with an XLSTM at its core that comes with linear-time inference complexity and natural sequence length extrapolation abilities. Experiments on 432 tasks from 6 domains show that LRAM compares favorably to Transformers in terms of performance and speed.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) has been responsible for impressive success stories such as game-playing, plasma control for fusion, or navigation of stratospheric balloons. While these successes were based on classical RL approaches, in which agents have been trained online with RL objectives, recently there has been a trend towards offline RL settings and sequence models trained via behavior cloning. Such approaches, in which agents are trained on large-scale offline datasets with causal sequence modeling objectives, have been driven by the proliferation of Transformer-based architectures and gave rise to what we refer to as Large Action Models (LAMs) to highlight their similarity to large language models (LLMs). LAM approaches can also be used in multi-task settings to develop generalist agents such as Gato. Existing LAMs are primarily based on the Transformer architecture. Because of their powerful predictive performance, robotics has become an emergent application area for large models and a number of large multi-task datasets were collected. This development bears the potential to produce robotics agents that learn to master complex tasks in a wide range of environments and even different embodiments. For example, recently it has been demonstrated, albeit in restricted settings, that sequence models trained on multi-episodic contexts can perform in-context learning (ICL). One potential application of ICL can be to learn new related tasks in robotics without the need for re-training or fine-tuning."}, {"title": "2 Related work", "content": "Sequence Models in RL. LSTM is the dominant backbone architecture for partially observable online RL problems and has been behind achievements such as mastering Starcraft II, Dota 2, and Atari. After the success of the Transformer in NLP, computer vision and speech recognition, the architecture has found its way into RL. Chen et al. [2021] proposed the Decision Transformer (DT) a GPT-style model, that learns to predict actions from offline trajectories via behavior cloning. Trajectory Transformer predicts action tokens along with states and rewards, which allows for dynamics modeling. A number of follow-up works build on the DT-architecture. Furthermore, sequence models trained on offline data were found to exhibit in-context learning if conditioned on previous trajectories , albeit only in limited scenarios.\nLarge Action Models (LAMs). LAMs, such as the Decision Transformer, are well suited for multi-task settings. Lee et al. [2022] found that a multi-game DT can learn to play 46 Atari games. Reed et al. [2022] introduced a generalist agent trained on over 600 tasks from different domains, ranging from Atari to manipulation of a robot arm. Jiang et al. [2022] a Transformer for robot manipulation based on multi-modal prompts, that allow to steer the model to perform new tasks. Recently, Raad et al. [2024] introduced an agent instructable via language to play a variety of commercial video games. Since then, robotics has become an emergent area for developing LAMs , also due to the availability of large-scale robotics datasets.\nNext-generation Sequence Modeling Architectures. Linear recurrent models, such as state-space models (SSM) have challenged the dominance of the Transformer architecture on long-range tasks. The key insight of those linear RNNs was to diagonalize the recurrent state matrix and enforce stable training via an exponential parameterization. Since then, there have been efforts to include features such as gating from RNNs . Non-linear gates are believed to have higher expressivity, but are harder to train. Griffin mixes gated linear recurrences with local attention to achieve more training data efficiency than Llama-2 and better sequence extrapolation. Mamba introduces a selection mechanism similar to gating into SSMs, which makes its state and input matrix time dependent. This is similar to the gating mechanism of RNNs but also bears resemblance to approaches like fast weights and Linear Attention. Mamba-2 highlight the connection between SSMs with input dependent state and input matrices and (Gated) Linear attention variants. Most recently, the xLSTM was proposed as an improvement over the classic LSTM that combines gating, linear recurrences and recurrent weights into a single architecture for language modeling. First, xLSTM leverages exponential gating with stabilization to RNNs for stronger emphasis on important inputs. Second, XLSTM is composed of two variants, the mLSTM variant with an emphasis on memory that proves important in language modeling and the sLSTM variant that keeps the non-diagonalized recurrent matrix to enable state-tracking . State tracking is important in logic tasks and cannot be modeled fundamentally by linearized recurrent or state-space models like Mamba, Griffin or Transformers."}, {"title": "3 Large Recurrent Action Models", "content": ""}, {"title": "3.1 Background", "content": "Reinforcement Learning. We assume the standard RL formulation via a Markov Decision Process (MDP) represented by a tuple of (S, A, P, R), where S and A denote state and action spaces, respectively. At every timestep t the agent observes state $s_t \\in S$, predicts action $a_t \\in A$, and receives a scalar reward $r_t$. The reward is determined by the reward function R(rt | st, at). P(St+1 | St, at) defines the transition dynamics and constitutes a probability distribution over next states st+1 when executing action at in state st. The goal of RL is to learn a policy \u03c0(at | st) that predicts an action at in state st that maximizes rt.\nDecision Transformer casts the RL problem setting as next action prediction task via causal sequence modeling. At training time, DT aims to learn a policy \u03c0\u03bf that maps future rewards to actions, which is often referred to as upside-down RL . At inference time, the DT is conditioned via a target return to emit high-reward actions. Consequently, we assume access to a dataset $D = \\{T_i\\}_{i=1}^N$ containing N trajectories ti consisting of quadruplets $(s_1, R_1, a_1, r_1,..., s_T, R_T, a_T, r_T)$ of state st, return-to-go (RTG) $R_t = \\sum_{t'=t}^T r_{t'}$, action at, and reward rt. Here, T refers to the length of the trajectory. The DT \u03c0\u03b8 is trained to predict the ground-truth action at conditioned on sub-trajectories from the dataset:\n$\\hat{a}_t \\sim \\pi_\\theta(\\hat{a}_t \\mid s_{t-C}, R_{t-C},a_{t-C},r_{t-C},..., s_{t-1}, R_{t-1}, a_{t-1}, r_{t-1}, s_{t}, R_{t}).$  (1)\nwhere C < T is the size of the context window. In fact, Equation 1 describes the setting of the multi-game DT , which also includes rewards in the sequence representation."}, {"title": "3.2 Large Recurrent Action Models (LRAMS)", "content": "Our LRAM has a modern recurrent architecture at its core (see Figure 1), which comes with a parallel training and a recurrent inference mode. We instantiate LRAM with three different variants, two different XLSTM configurations and Mamba. Furthermore, we use a training protocol similar to that of  with some differences.\nMulti-modal sequence representation. To encode input from different environments with varying state and action spaces, we use separate encoders per modality that are shared across tasks and domains. For encoding images we use a CNN similar to , whereas for low-dimensional inputs we use a fully connected network. We refrain from patchifying images and tokenizing continuous states to avoid unnecessarily long sequences. Similarly, we use linear layers to encode rewards and RTGs. We omit actions in our sequence formulation, as we found that this can be detrimental to performance, in particular for continuous control tasks (see Section 4.3). Consequently, our trajectories have the form t\u2081 = (81, R1, r1,..., st, R\u012b, rr) and we train our policy \u03c0\u03c1 to predict the ground-truth action at as:\n$\\hat{a}_t \\sim \\pi_\\theta(\\hat{a}_t \\mid s_{t-C}, R_{t-C},r_{t-C},..., s_{t-1}, R_{t-1}, r_{t-1}, s_{t}, R_{t}).$ (2)\nShared action head. Action spaces in RL typically vary across environments. For example, in the environments we consider, there are 18 discrete actions and a maximum of 8 continuous dimensions for continuous control environments. Therefore, we employ discretization of continuous action dimensions into 256 uniformly-spaced bins, similar to . Unlike prior work, we leverage a shared action head to predict all discrete actions or continuous action dimensions jointly. We found this setup significantly reduces inference time compared to using autoregressive action prediction of continuous actions.\nRecurrent inference mode. At inference time, we leverage the recurrent backbone and maintain the hidden states of the last timestep. This enables fast inference with linear-time complexity along the sequence length. In addition, the recurrent-style inference is well suited for online fine-tuning via RL objectives, similar to LSTM-based policies in online RL. To further speed-up inference, we leverage custom kernels for the xLSTM backbone (see Appendix 22)."}, {"title": "4 Experiments", "content": "We study the aptitude of modern recurrent architectures as LAMs on 432 tasks from 6 domains: Atari , Composuite , DMControl , Meta-World , Mimicgen , and Procgen . To this end, we compile a large-scale dataset containing 894 million transitions (see Section 4.1).\nAcross all experiments, we compare four backbone variants: xLSTM [7:1], xLSTM [1:0] , Mamba , and the GPT-2 style Transformer employed in the DT . Following , we use the bracket notation for xLSTM, which indicates the ratio of mLSTM to sLSTM blocks. For example, xLSTM [1:0] contains only mLSTM blocks.\nIn Section 4.2, we conduct a scaling comparison for four model sizes ranging from 16M to 208M parameters that shows that modern recurrent architectures achieve performance comparable or favorable to the Transformer baseline across different model sizes. In Section 4.3, we study the impact of the recurrent backbones on fine-tuning performance and ICL abilities, and further analyze our trained recurrent backbones. Finally, in Section 4.4, we empirically examine the differences at inference time in terms of latency and throughput between xLSTM-based and Transformer-based agents, which indicate a clear advantage for the recurrent backbone."}, {"title": "4.1 Datasets & Environments", "content": "Datasets. We compile a large-scale dataset comprising 432 tasks from six domains. We leverage datasets from prior works. For Atari, we extract 5M transitions per task from the DQN-Replay dataset released by. For Composuite, we leverage the datasets released by . For Meta-World, we use 2M transitions per task released by . For DMControl, we generate 10M transitions per task using task-specific RL agents. For Mimicgen, we use the datasets for the 21 tasks released by  and generate trajectories for the remaining 62 tasks. Finally, for Procgen, we extract 20M transitions from the datasets released by . Our final dataset contains 3.4M trajectories and in total 894M transitions (see Table 4.1). We reserve an additional 37 tasks from the same domains for zero-shot evaluation. To foster future research, we release our data-preparation pipeline and generated data.\nEnvironments. Atari and Procgen come with image observations and discrete actions. In contrast, the remaining four domains exhibit state-based observations and continuous actions. Consequently, our experiments involve a mixture of state and action spaces as well as varying episode lengths (see Table 4.1). Periodically evaluating the trained agents on all 432 tasks sequentially is time-consuming and we, therefore, distributed the evaluation across GPUs and parallel processes (see Appendix B).\nAdditional details on our datasets, environments are available in Appendix A."}, {"title": "4.2 Scaling comparison", "content": "To conduct our main comparisons, we train our four backbone variants on the full training task mixture of 432 tasks. For each architecture backbone, we report performance scores for four model sizes: 16M, 48M, 108M, and 206M parameters. We train all models for 200K updates with a batch size of 128 and context length of 50 timesteps. All domains are represented with approximately equal proportion, resulting in 33K updates per domain. Additional implementation details and hyperparameters for every backbone variant and model size are available in Appendix B.\nSequence prediction performance. In Figure 2a, we report the validation set perplexity for all backbones and model sizes averaged over the individual scores from all domains. To achieve this, we maintain a hold-out set of trajectories for each training task (2.5%) and compute the perplexities after every 50K steps. Both recurrent backbones outperform the Transformer baseline considerably, especially as the model sizes increase. We provide the perplexities on the training set in Figure 13.\nEvaluation performance. During training, we evaluate our agents after every 50K step in all 432 training environments. In Figure 2b, we report the resulting normalized performances averaged across all six domains. The recurrent backbones outperform the Transformer one across model sizes. While xLSTM and Mamba performs similarly at smaller scales, xLSTM tends to outperform Mamba at larger scales (206M). This is an important advantage of xLSTM, as LRAM agents can strongly benefit from more data and consequently larger models. Note, that Mamba has a significantly higher number of parameters than competitors. For the zero-shot evaluation performances on the 37 hold-out tasks, we refer to Figure 15 in Appendix C.2."}, {"title": "Performance per domain", "content": "In Figure 3, we report the normalized scores for the 206M parameter models attained on all six domains. For Meta-World, DMControl, Mimicgen, Composuite, and Procgen we use data-normalized scores, as suggested by [Levine et al., 2020]. For Atari, we report human-normalized scores. Overall, we observe that the xLSTM backbone outperforms competitors on three of the six domains, while all methods perform similarly on the remaining 3 domains.\nThese experiments suggest that modern recurrent backbones can be attractive alternatives to the Transformer architecture for building LAMs in terms of final performance."}, {"title": "4.3 Analyses & Ablations", "content": "Fine-tuning. To assess the effect of the recurrent backbones on fine-tuning performance, we fine-tune our models on 37 held-out environments from all 6 domains. We evaluate the fine-tuning performance of the xLSTM architecture for both the 16M parameter pretrained models and compared it against an XLSTM trained from scratch. The pretrained LRAM outperforms the randomly initialized XLSTM model in most domains. For detailed results, see Appendix C.3. This suggests that fine-tuning performance is not negatively affected by switching the backbone.\nIn-context Learning. Next, we study the ICL abilities of our recurrent backbones on the Dark-Room environment considered in prior work on in-context RL. To study ICL in isolation, we train models from scratch with a multi-episodic context, which results in a large context length (we refer to Appendix C.4 for details on the experiment setup). In particular, we adopt the Algorithm Distillation (AD, Laskin et al., 2022) framework and exchange the Transformer backbone architecture with modern recurrent architectures. In Figure 17, we report the ICL performance on (a) 80 train and (b) 20 hold-out tasks. We find that xLSTM [7:1] attains the highest overall scores both on training and hold-out tasks, which we attribute to the state-tracking abilities of sLSTM blocks.\nEmbedding space analysis. In Figure 5, we analyze the representations learned by our model. To this end, we sample 32 sub-trajectories from every task, extract the sequence representation at the last layer, cluster them using UMAP , and color every point by its domain. Appendix E describes the setup in greater detail. We find that tasks from the same domain cluster together. Furthermore, xLSTM exhibits a more refined domain separation compared to DT, which may contribute to the better down-stream performance.\nRemoving Actions & Effect of Context Length. We found that removing actions from the context results in better performance across backbones. While context lengths beyond 1 hurt performance on Meta-World when training with actions, the reverse is true when training without actions (see Figure 23). This is in contrast to recent works, which did not benefit from longer contexts. While removing actions improves performance on the robotics domains, it does not affect performance on discrete control. For robotics, we observed that the models become overly confident (high action logits), which is problematic if poor initial actions are produced. We assume this is because in robotics actions change smoothly and by observing previous actions the agent learns shortcuts. Removing actions from the input prevents the agent from using shortcuts. Importantly, the evaluation performance improves as the sequence length increases, which indicates that the history helps to predict the next action (e.g., by observing mistakes made in the recent past).\nWe present additional ablations on the effect of reducing the number of layers in xLSTM and disabling Dropout on DT in Appendix D.3 and D.2, respectively."}, {"title": "4.4 Inference Time Comparison", "content": "Finally, we empirically examine the difference between xLSTM-based and Transformer-based agents at inference time. Similar to , we report both latency and throughput. We focus our analysis on latency, as it is the more important dimension for real-time applications."}, {"title": "5 Conclusion", "content": "In this work, we study the aptitude of modern recurrent architectures as alternatives to Transformers for building LAMs. We found that our LRAM with an XLSTM or Mamba at its core compare favorably to the Transformer in terms of evaluation performance across different model scales. Moreover, we demonstrated that xLSTM-based LRAMs exhibit higher inference speeds, especially at large context sizes. Thus, the empirical evidence suggests, that recurrent backbones such as the xLSTM can be attractive alternatives for LAMs. Notably, the linear-time inference complexity of xLSTM may enable applications that require long context lengths, such as in-context RL, and facilitate the application of large-scale agents for real-time applications, such as robotics.\nLimitations. The primary target application of LAMs is robotics. While the majority of our experiments involve robotic simulations, we do not yet provide empirical evidence for real robots. We do, however, believe that our findings translate to real-world scenarios and aim to provide further evidence in future work. Moreover, the fine-tuning experiments in this work are limited to offline RL. We envision that an agent pre-trained by behavioral cloning on large-scale offline RL datasets may be successfully fine-tuned in an online RL setting to explore new strategies that do not appear in the training data. Modern recurrent architectures offer both parallel and recurrent training mode, which might be the key to success for such applications. While we provide initial evidence of improved ICL abilities of modern recurrent architectures, we only consider a limited grid-world setting. Consequently, we aim to further investigate the in-context RL abilities of recurrent backbones on more complex environments in future work."}, {"title": "6 Ethics Statement", "content": "While we conduct all our experiments in simulated environments, the primary target application of our method is robotics. We believe that our work can positively impact applications in the near future, which require efficient inference, on-device processing, or have real-time constraints. However, robotics applications in the real world are not without risks. In particular, in areas where humans are involved, such as factory settings, special care is required. LAMs are trained via next-action prediction similar to LLMs. Consequently, LAMs may also suffer from hallucinations in unknown scenarios. We therefore strongly discourage users from blindly following the predictions made by real-world LAMs without appropriate safeguards regarding safety and robustness. It is essential to ensure responsible deployment of such future technologies, and we believe that more research on the robustness of LAMs is necessary."}, {"title": "7 Reproducibility", "content": "Our code-base used for our experiments and the datasets we generated are publicly available at: https://github.com/ml-jku/LRAM. We describe the environments we use for our experiments and provide dataset statistics in Appendix A. Furthermore, in Appendix B, we provide implementation details for all methods and a list of hyperparameters used for our experiments. In Appendix C, we present additional figures that accompany our results in the main text (e.g., all model sizes). Finally, in Appendices D and E, we provide further details on the conducted ablation studies and the embedding space analysis, respectively."}, {"title": "A Environments & Datasets", "content": ""}, {"title": "A.1 General", "content": "We compile a large-scale dataset comprising 432 tasks from six domains, 3.4M trajectories, and 894M transitions in total (see Table 4.1). To enable fast and targeted data-loading, every trajectory is stored in a separate hdf5 file. We trade off some data-loading speed for disk space efficiency, by compressing trajectories that contain image-based observations."}, {"title": "A.2 Atari", "content": "The Arcade Learning Environment (ALE) is the standard benchmark for evaluating RL agents and consists of 57 Atari games. Input observations in Atari are RGB images, but as is standard practice we gray-scale and crop frames (|S| = 1 \u00d7 64 \u00d7 64). There are 18 discrete action across all 57 Atari games (|A| = 18), but individual games may use only use a subset of these actions. Furthermore, we adopt the standard Atari recipe as used in prior works, including a frame skip of 4, maximum number of no-ops of 30, resetting on life loss, and reward clipping to [-1,1] .\nTasks. Similar to Lee et al. [2022], we assign 41 games to the training set, and 5 additional tasks to the hold-out set. The 41 training tasks include:\namidar, assault, asterix, atlantis, bank-heist, battle-zone, beam-rider, boxing, breakout,\ncarnival, centipede, chopper-command, crazy-climber, demon-attack, double-dunk, enduro,\nfishing-derby, freeway, frostbite, gopher, gravitar, hero, ice hockey, jamesbond, kangaroo,\nkrull, kung-fu-master, name-this-game, phoenix, pooyan, qbert, riverraid, road-runner,\nrobotank, seaquest, time-pilot, up-n-down, video-pinball, wizard-of-wor, yars-revenge,\nzaxxon"}, {"title": "A.3 Meta-World", "content": "The Meta-World benchmark consists of 50 manipulations tasks using a Sawyer robotic arm, ranging from opening or closing windows, to pressing buttons. Meta-World is based on the MuJoCo physics engine. Observations in Meta-World are 39-dimensional continuous vectors (|S| = 1 \u00d7 64 \u00d7 39), and actions are represented by 6 continuous dimensions (|A| = 18) in range [-1, 1]. All tasks share a common action and state space. Following , we limit the episode lengths to 200 interactions.\nTasks. We follow Yu et al. [2020a] and split the 50 Meta-World tasks into 45 training tasks (MT45) and 5 evaluation tasks (MT5).\nThe 45 training tasks are:\nreach, push, pick-place, door-open, drawer-open, drawer-close, button-press-topdown,\npeg-insert-side, window-open, window-close, door-close, reach-wall, pick-place-wall,\npush-wall, button-press, button-press-topdown-wall, button-press-wall,\npeg-unplug-side, disassemble, hammer, plate-slide, plate-slide-side, plate-slide-back,\nplate-slide-back-side, handle-press, handle-pull, handle-press-side, handle-pull-side,\nstick-push, stick-pull, basketball, soccer, faucet-open, faucet-close, coffee-push,\ncoffee-pull, coffee-button, sweep, sweep-into, pick-out-of-hole, assembly, shelf-place,\npush-back, lever-pull, dial-turn\nThe 5 evaluation tasks are: bin-picking, box-close, door-lock, door-unlock, hand-insert\nDataset. For Meta-World, we use the datasets released by , which contain 2M transitions per tasks and consequently 90M transitions in total for the training set. All episodes last for 200 environment interaction steps, and consequently there are 10K episodes for every task. For detailed dataset statistics per task, we refer to their publication."}, {"title": "A.4 DMControl", "content": "The DMControl benchmark consists of 30 different robotic tasks. Unlike Meta-World, the benchmark contains robots with different morphologies instead of a single common Sawyer arm. Due to the different robot morphologies, the state, and action spaces vary across tasks (3 \u2264 |S| \u2264 24, 1 \u2264 |A| \u2264 6), with all actions in range [-1, 1].\nTasks. We do not use all 30 tasks contained in the DMControl benchmark, but select 16 of the 30 tasks that have been used in prior works , which we refer to as DMC11 and DMC5 respectively.\nThe 11 training tasks are:\nfinger-turn_easy, fish-upright, hopper-stand, point_mass-easy, walker-stand, walker-run,\nball_in_cup-catch, cartpole-swingup, cheetah-run, finger-spin, reacher-easy\nThe 5 evaluation tasks are:\ncartpole-balance, finger-turn_hard, pendulum-swingup, reacher-hard, walker-walk\nDataset. For DMControl, we generate 10M transitions per task by training task-specific SAC agents, using the same setup as . Episodes in all DMControl tasks last for 1000 environment steps and per time-step a maximum reward of +1 can be achieved, which results in a maximum reward of 1000 per episode. Consequently, our training set contains 10K episodes per tasks, amounting to 110K episodes and 110M transitions in total across all tasks. We list the dataset statistics for all 11 tasks in Table 3."}, {"title": "A.5 Composuite", "content": "The Composuite benchmark , is a robotics benchmark for grasping and object manipulation. The benchmark is implemented on top of robotsuite , which in turn leverages the MuJoCo simulator under the hood [Todorov et al., 2012b]. Composuite contains a mix of 4 simulated robot arms: IIWA,\nJaco, Gen3, and Panda (see Figure 9). All arms share a common state and action space containing 93 continuous\nstate dimensions and 8 continuous action dimensions, respectively (|S| = 93, |A| = 8).\nTasks. CompoSuite is designed as a compositional multi-task benchmark for RL, in which a particular robot manipulates a particular object given an objective, while avoiding obstacles. Overall, there are 4 robots arms,\n4 objects, 4 obstacles, and 4 task objectives. This results in 256 possible robot/object/objective/obstacles combinations. For our experiments, we assign 240 tasks to the training set and use the remaining 16 tasks as hold-out set (Panda and Object_Wall) combinations. For a list of all 256 tasks, we refer to Mendez et al.\n[2022].\nDataset. For Composuite, we leverage the datasets released by Hussing et al. [2023]. For every task, we\nselect 2000 episodes, which last on average for 500 steps. This amounts to 1M transitions per task, and 240M\ntransitions across all 240 training tasks. For dataset statistics, we refer to Hussing et al. [2023]."}, {"title": "A.6 Mimicgen", "content": "Similar to Composuite, Mimicgen is based on robosuite and the MuJoCo simulator. Mimicgen is designed for automatically synthesizing large-scale datasets from only a handful of human demonstrations. Observations in Mimicgen can be represented as images (from multiple cameras) or low dimensional continuous states. For our experiments, we opt for the low-dimensional state representation to simplify learning. Therefore, observations and actions are represented by 37-dimensional and 7-dimensional continuous vectors, respectively (|S| = 37, |A| = 7). Similar to Composuite, Mimicgen supports 4 different robot arms: Panda, IIWA, Sawyer, and UR5e (see Figure 10).\nTasks. Mimicgen consists of 24 diverse tasks, including stacking blocks, re-assembling objects, and even long-horizon tasks like coffee preparation. These 24 tasks can be performed with the four supported robot arms, amounting to 96 tasks in total.\nDataset. Mandlekar et al. [2023] released dataset for the 24 tasks using the default robot arm Panda. To increase the dataset diversity, we additionally generated data for the remaining 3 robot arms. However, not all data generation runs produce successful trajectories, and we discard with too few successful trajectories. Our final"}, {"title": "A.7 Procgen", "content": "Procgen benchmark consists of 16 procedurally-generated video games. Observations in Procgen are RGB images of dimension 3 \u00d7 64 \u00d7 64. However, for training efficiency, we apply gray-scaling to image observations (S = 1\u00d764 \u00d7 64). All 16 environments share a common action space of 15 discrete actions (A = 16). Procgen is designed to test the generalization abilities of RL agents. Consequently, procedural generation is employed to randomize background and colors, while retaining the game dynamics.\nTasks. Following prior works , we assign 12 and 4 tasks to training and hold-out set, respectively. The 12 training tasks are:\nbigfish, bossfight, caveflyer, chaser, coinrun, dodgeball,\nfruitbot, heist, leaper, maze, miner, starpilot\nThe 4 hold-out tasks are: climber, ninja, plunder, jumper\nDataset. We leverage the datasets released by Schmied et al. [2024b], which contain 20M transitions per task. The datasets were generated by recording all transitions observed by training RL agents for 25M steps, followed by uniform subsampling to 20M transitions. Consequently, the dataset contains mixed quality trajectories ranging from random (beginning of training) to expert (end of training). We list the dataset statistics for all 16 tasks in Table 4."}, {"title": "B Experimental & Implementation Details", "content": ""}, {"title": "B.1 Training & Evaluation.", "content": "In our experiments, we compare two variants of xLSTM, Mamba and DT. For our main experiments in Section 4.2, we train all models for 200K updates, and evaluate after every 50K update steps. We report the mean and 95% confidence intervals over three seeds in our experiments, as suggested by Agarwal et al. [2021]. For every evaluation tasks, we take the average of 3 evaluation seeds.\nWe train our agents with a batch size of 128 and gradient accumulation across the 6 domains, such that every domain is represented with the same proportion. Consequently, the effective batch size is 768. We use a learning rate of 1e-4, 4000 linear warm-up steps followed by a cosine decay to le-6, and train using the AdamW optimizer . In addition, we employ gradient clipping of 0.25, weight decay of 0.01 for all models. We do not employ Dropout, as is standard practice in DTs, as we found that it negatively affects performance (see Section 4.3). We use separate reward scales of 200, 100 and 20 for Meta-World, DMControl and Atari, respectively. Furthermore, for all domains, we set the target return to the maximum return achieved for a particular task in the training datasets. This is particularly useful for domains, where the maximum returns differ heavily across tasks (e.g., Atari). We list all hyperparameters in Table 5."}, {"title": "B.2 Context Lengths.", "content": "By default, we train all models with a context length C = 50 timesteps. For every timestep there are three tokens (s/rt/r) and consequently, the effective context length is 150. We found that performance improves for longer context lengths (see Section D.1), but limit our experiments to C = 50 to reduce the computational cost."}, {"title": "B.3 Model Architectures.", "content": "We train models across 4 models sizes: 16M", "2022": "in selecting the number of layers and hidden dimensions. For XLSTM and Mamba", "7": 1, "2024": "."}, {"7": 1, "2024": ".", "1": [1, 3], "3": "and [1, 3, 5", "2021": ".", "2018": "and"}]}