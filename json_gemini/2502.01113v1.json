{"title": "GFM-RAG: Graph Foundation Model for Retrieval Augmented Generation", "authors": ["Linhao Luo", "Zicheng Zhao", "Gholamreza Haffari", "Dinh Phung", "Chen Gong", "Shirui Pan"], "abstract": "Retrieval-augmented generation (RAG) has proven effective in integrating knowledge into large language models (LLMs). However, conventional RAGs struggle to capture complex relationships between pieces of knowledge, limiting their performance in intricate reasoning that requires integrating knowledge from multiple sources. Recently, graph-enhanced retrieval augmented generation (GraphRAG) builds graph structure to explicitly model these relationships, enabling more effective and efficient retrievers. Nevertheless, its performance is still hindered by the noise and incompleteness within the graph structure. To address this, we introduce GFM-RAG, a novel graph foundation model (GFM) for retrieval augmented generation. GFM-RAG is powered by an innovative graph neural network that reasons over graph structure to capture complex query-knowledge relationships. The GFM with 8M parameters undergoes a two-stage training process on large-scale datasets, comprising 60 knowledge graphs with over 14M triples and 700k documents. This results in impressive performance and generalizability for GFM-RAG, making it the first graph foundation model applicable to unseen datasets for retrieval without any fine-tuning required. Extensive experiments on three multi-hop QA datasets and seven domain-specific RAG datasets demonstrate that GFM-RAG achieves state-of-the-art performance while maintaining efficiency and alignment with neural scaling laws, highlighting its potential for further improvement\u00b9.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in large language models (LLMs) (OpenAI, 2024a; Meta, 2024; Yang et al., 2024) have greatly propelled the evolution of natural language processing, positioning them as foundational models for artificial general intelligence (AGI). Despite the remarkable reasoning ability (OpenAI, 2024b), LLMs are still limited in accessing real-time information and lack of domain-specific knowledge, which is outside the pre-training corpus. To address these limitations, retrieval-augmented generation (RAG) (Gao et al., 2023) has become a popular paradigm in adding new knowledge to the static LLMs by retrieving relevant documents into the context of LLM generation.\nExisting RAG methods typically retrieve documents independently, making it difficult to capture complex relationships between pieces of knowledge (Karpukhin et al., 2020; Chen et al., 2023; Moreira et al., 2024). This limitation hampers the performance of LLMs in integrating knowledge across document boundaries, particularly in multi-hop reasoning tasks (Yang et al., 2018; Trivedi et al., 2022) and real-world applications like legal judgment (Kang et al., 2024) and medical diagnoses (Jin et al., 2019), which require reasoning over multiple sources. Although recent methods have expanded the retrieval process into multiple steps and incorporate LLM reasoning, they still encounter high computational costs due to iterative retrieval and reasoning with LLMs (Trivedi et al., 2023; Sun et al., 2024; Joshi et al., 2024).\nRecently, graph-enhanced retrieval augmented generation (GraphRAG) (Peng et al., 2024; Han et al., 2024) has emerged as a novel solution that builds a graph structure to explicitly model the intricate relationships between knowledge. This enables the development of a graph-enhanced retriever to identify relevant information using graphs. The structural nature of graphs allows GraphRAG to capture"}, {"title": "2. Related Work", "content": "Retrieval-augmented generation (RAG) (Gao et al., 2023) provides an effective way to integrate external knowledge into large language models (LLMs) by retrieving relevant documents to facilitate LLM generation. Early works adopt the pre-trained dense embedding model to encode documents as separate vectors (Karpukhin et al., 2020; Chen et al., 2023; Li et al., 2023b; Moreira et al., 2024), which are then retrieved by calculating the similarity to the query. Despite efficiency and generalizability, these methods struggle to capture complex document relationships. Subsequent studies have explored multi-step retrieval, where LLMs guide an iterative process to retrieve and reason over multiple documents (Trivedi et al., 2023; Jiang et al., 2023; Su et al., 2024). However, this approach is computationally expensive.\nGraph-enhanced retrieval augmented generation (GraphRAG) (Peng et al., 2024; Han et al., 2024) is a novel approach that builds graphs to explicitly model the complex relationships between knowledge, facilitating comprehensive retrieval and reasoning. Early research focuses on retrieving information from existing knowledge graphs (KGs), such as WikiData (Vrande\u010di\u0107 & Kr\u00f6tzsch, 2014) and Freebase (Bollacker et al., 2008), by identifying relevant facts or reasoning paths (Li et al., 2023a; LUO"}, {"title": "3. Approach", "content": "The proposed GFM-RAG essentially implements a GraphRAG paradigm by constructing graphs from documents and using a graph-enhanced retriever to retrieve relevant documents.\nGFM-RAG Overview. Given a set of documents $D = {D_1, D_2, ..., D_{|D|}}$, we construct a knowledge graph $G \\{(e,r,e') \\in E \\times R \\times E\\}$, where $e, e' \\in E$ and $r \\in R$ denote the set of entities and relations extracted from $D$, respectively. For a user query $q$, we aim to design a graph-enhanced retriever to obtain relevant documents from $D$ by leveraging the knowledge graph $G$. The whole GFM-RAG process can be formulated as:\n$G = KG-index(D)$  (1)\n$D^K = GFM-Retriever(q, D, G)$  (2)\n$a = LLM(q, D^K)$.  (3)\nIn the first step, KG-index() constructs a knowledge graph index $G$ from the document corpus $D$, followed by our proposed graph foundation model retriever (GFM-Retriever), which is pre-trained on large-scale datasets. It retrieves top-K documents based on any user query $q$ and knowledge graph index $G$. The retrieved documents $D^K$, along with the query $q$, are then input into a large language model (LLM) to generate the final answer $a$. These three main components in GFM-RAG are illustrated in Figure 2 and will be detailed next."}, {"title": "3.1. KG-index Construction", "content": "Conventional embedding-based index methods encode documents as separate vectors (Karpukhin et al., 2020; Chen et al., 2023; Moreira et al., 2024), which are limited in modeling the relationships between them. Knowledge graphs (KGs), on the other hand, explicitly capturing the relationships between millions of facts, can provide a structural index of knowledge across multiple documents (Edge et al., 2024; Guti\u00e9rrez et al., 2024). The structural nature of the KG-index aligns well with the human hippocampal memory indexing theory (Teyler & DiScenna, 1986), where the KG-index functions like an artificial hippocampus to store associations between knowledge memories, enhancing the integration of diverse knowledge for complex reasoning tasks (Guti\u00e9rrez et al., 2024).\nTo construct the KG-index, given a set of documents $D$, we first extract entities $E$ and relations $R$ to form triples $T$ from documents. Then, the entity to document inverted index $M \\in \\{0,1\\}^{|E| \\times |D|}$ is constructed to record the entities mentioned in each document. Such a process can be achieved by existing open information extraction (OpenIE) tools (Angeli et al., 2015; Zhou et al., 2022; Pai et al., 2024). To better capture the connection between knowledge, we further conduct the entity resolution (Gillick et al., 2019; Zeakis et al., 2023) to add additional edges $T^+$ between entities with similar semantics, e.g., (USA, equivalent, United States of America). Therefore, the final KG-index $G$ is constructed as $G = \\{(e,r, e') \\in T \\cup T^+\\}$. In implementation, we leverage an LLM (OpenAI, 2024a) as the OpenIE tool and a pre-trained dense embedding model (Santhanam et al., 2022) for entity resolution."}, {"title": "3.2. Graph Foundation Model (GFM) Retriever", "content": "The GFM retriever is designed to retrieve relevant documents based on any user query and the constructed KG-index. While the KG-index offers a structured representation of knowledge, it still suffers from incompleteness and noise, resulting in suboptimal retrieval performance when solely relying on its structure (Guti\u00e9rrez et al., 2024). Recently, graph neural networks (GNNs) (Wu et al., 2020) have shown impressive graph reasoning ability by capturing the complex relationships between knowledge for retrieval or question answering (Mavromatis & Karypis, 2024; He et al., 2024). However, existing GNNs are limited in generalizability, as they are usually trained on specific graphs (Mao et al., 2024; Liu et al., 2023), which limits their application to unseen corpora and KGs. Therefore, there is still a need for a graph foundation model that can be directly applied to unseen datasets and KGs without additional training.\nTo address these issues, we propose the first graph foundation model-powered retriever (GFM retriever), which harnesses the graph reasoning ability of GNNs to capture the complex relationships between queries, documents, and knowledge graphs in a unified and transferable space. The GFM retriever employs a query-dependent GNN to identify relevant entities in graphs that will aid in locating pertinent documents. After pre-training on large-scale datasets, the GFM retriever can be directly applied to new corpora and KGs without further training."}, {"title": "3.2.1. QUERY-DEPENDENT GNN", "content": "Conventional GNNs (Gilmer et al., 2017) follow the message passing paradigm, which iteratively aggregates information from neighbors to update entity representations. Such a paradigm is not suitable for the GFM retriever as it is graph-specific and neglects the relevance of queries. Recent query-dependent GNNs (Zhu et al., 2021; Galkin et al., 2024) have shown promising results in capturing query-specific information and generalizability to unseen graphs, which is essential for the GFM retriever and can be formulated as:\n$H^l = GNN_q(q, G, H^{l-1}),$ (4)\nwhere $H^0 \\in R^{|E| \\times d}$ denotes initial entity features, and $H^l$ denotes the updated entity representations conditioned on query $q$ after $L$ layers of query-dependent message passing."}, {"title": "3.2.2. TRAINING PROCESS", "content": "Training Objective. The training objective of the GFM retriever is to maximize the likelihood of the relevant entities to the query, which can be optimized by minimizing the binary cross-entropy (BCE) loss:\n$L_{BCE} = \\frac{1}{|A_q|} \\sum_{e \\in A_q} log P_q(e) - \\frac{1}{|E|} \\sum_{e \\in E} log(1 - P_q(e)),$ (11)\nwhere $A_q$ denotes the set of target relevant entities to the query $q$, and $E \\subseteq E \\setminus A_q$ denotes the set of negative entities sampled from the KG. However, due to the sparsity of the target entities, the BCE loss may suffer from the gradient vanishing problem (Lin et al., 2024). To address this issue, we further introduce the ranking loss (Bai et al., 2023) to maximize the margin between the positive and negative entities:\n$L_{RANK} = \\frac{1}{|A_q|} \\sum_{e \\in A_q} \\frac{1}{\\sum_{e' \\in E} P_q(e')},$ (12)\nThe final training objective is the weighted combination of the BCE loss and ranking loss:\n$L = \\alpha L_{BCE} + (1 - \\alpha)L_{RANK}.$ (13)\nUnsupervised KG Completion Pre-training. To enhance the graph reasoning capability of the GFM retriever, we first pre-train it on a large-scale knowledge graph (KG) completion task. We sample a set of triples from the KG index and mask either the head or tail entity to unsupervisedly create synthetic queries in the form $q = (e, r, ?)$ or $(?, r, e')$, with the masked entity serving as the target entity $A_q = \\{e\\}$ or $\\{e'\\}$. The GFM retriever is then trained to predict the masked entity using both the query and the KG, as outlined in equation 13.\nSupervised Document Retrieval Fine-tuning. After unsupervised pre-training, we fine-tune the GFM retriever on a supervised document retrieval task. In this task, queries $q$ are natural language questions, and target entities $A_q$ are extracted from labeled supporting documents $D_q$. The GFM retriever is trained to retrieve relevant entities from the KG index using the same training objective as in equation 13."}, {"title": "3.3. Documents Ranking and Answer Generation", "content": "Given the entity relevance scores $P_q \\in R^{|E| \\times 1}$ predicted by the GFM retriever, we first retrieve the top-T entities $E^\\top$ with the highest relevance scores as:\n$E^\\top = arg \\top-T(P_q), E^\\top = \\{e_1,..., e_T\\}$. (14)\nThese retrieved entities are then used by the document ranker to obtain the final documents. To diminish the influence of popular entities, we weigh the entities by the inverse of their frequency as entities mentioned in the document inverted index $M \\in \\{0,1\\}^{|E| \\times |D|}$ and calculate the final document relevance scores by summing the weights of"}, {"title": "4. Experiment", "content": "In experiments, we aim to address the following research questions: (1) How does GFM-RAG perform in multi-hop retrieval and QA tasks? (Sections 4.2 and 4.3); (2) What are the efficiency and effectiveness of GFM-RAG in multi-hop retrieval? (Section 4.4); (3) How well does GFM-RAG generalize to unseen datasets as a foundation model? (Section 4.6); (4) How does the performance of GFM-RAG scale with training as a foundation model? (Section 4.7); (5) How to interpret GFM-RAG in multi-hop reasoning? (Section 4.8)."}, {"title": "4.1. Experimental Setup", "content": "Datasets. We first evaluate the effectiveness of GFM-RAG on three widely-used multi-hop QA datasets, including HotpotQA (Yang et al., 2018), MuSiQue (Trivedi et al., 2022), and 2WikiMultiHopQA (2Wiki) (Ho et al., 2020). For a fair comparison (Trivedi et al., 2023; Guti\u00e9rrez et al., 2024), we use 1,000 samples from each validation set for testing. We merge the candidate passages into the document corpus and extract 20,000 samples from each training set for GFM training. We also evaluate the performance of GFM-RAG on seven RAG datasets from three domains, including biomedical (Jin et al., 2019), custom support (Sadat et al., 2023; Nandy et al., 2021; Malaviya et al., 2023; Castelli et al., 2020), and general knowledge (Nguyen et al., 2016; Kamalloo et al., 2023), to demonstrate the generalizability of GFM-RAG as the foundation model. The detailed statistics of the test datasets are shown in the Appendix A.\nBaselines. We compare against several widely used retrieval methods under three categories: (1) single-step naive methods: BM25 (Robertson & Walker, 1994), Contriever (Izacard et al., 2022), GTR (Ni et al., 2022), ColBERTv2 (Santhanam et al., 2022), RAPTOR (Sarthi et al., 2024), Proposition (Chen et al., 2024); (2) state-of-the-art graph-enhanced methods: LightRAG (Guo et al., 2024), HippoRAG (Guti\u00e9rrez et al., 2024); (3) multi-step methods: IRCOT (Trivedi et al., 2023), which can be integrated with"}, {"title": "4.2. Retrieval Performance", "content": "We first evaluate the retrieval performance of GFM-RAG against the baselines on three multi-hop QA datasets. As shown in Table 2, GFM-RAG achieves the best performance on all datasets, outperforming the SOTA IRCOT + HippoRAG by 16.8%, 8.3%, 19.8% in R@2 on HotpotQA, MuSiQue, and 2Wiki, respectively. The results demonstrate the effectiveness of GFM-RAG in multi-hop retrieval. From the result, we can observe that the naive single-step retrievers (e.g., BM25, RAPTOR) are outperformed by graph-enhanced HippoRAG, which highlights the significance of graph structure in multi-hop retrieval. Although LightRAG"}, {"title": "4.3. Question Answering Performance", "content": "We then evaluate the QA performance of GFM-RAG, as it is directly influenced by retrieval quality. We adopt the GPT-40-mini (OpenAI, 2024a) as LLM and use the top-5 retrieved documents for generating answers. From the results shown in Table 3, the single-step GFM-RAG has already achieved state-of-the-art performance against all other baselines. Meanwhile, we also integrate GFM-RAG with IRCOT to conduct multi-step retrieval and reasoning, which further improves the performance by 8.5%, 21.2%, 3.9% in EM on three datasets, respectively. The results demonstrate the effectiveness and great compatibility of GFM-RAG with an arbitrary multi-step framework in multi-hop reasoning tasks."}, {"title": "4.4. Efficiency Analysis", "content": "GFM-RAG achieves great efficiency in performing multi-step reasoning in a single step. As shown in Table 4, while the naive single-step methods get the best efficiency whose performance is not satisfying. Admittedly, the multi-step framework IRCOT could improve the performance, but it suffers from high computational costs due to the iterative"}, {"title": "4.5. Ablation Study", "content": "We conduct ablation studies to investigate the effectiveness of different components in GFM-RAG, including different sentence embedding models (Appendix D.1), pre-training strategies (Appendix D.2), and loss weighting strategies (Appendix D.3). The results show that GFM-RAG is not sensitive to different sentence embedding models, and the pre-training strategy, as well as the loss weighting strategy, are both crucial for the performance of GFM-RAG."}, {"title": "4.6. Model Generalizability", "content": "To demonstrate the generalizability of GFM-RAG as a foundation model, we test the performance (R@5) of GFM-RAG on seven domain-specific RAG datasets without any fine-tuning. Specifically, we first build the KG-index from the documents in each dataset. Then, given the query, we use the pre-trained GFM retriever to retrieve the top-K documents with the help of the corresponding KG-index. As shown in Figure 3, GFM-RAG achieves the best performance on all datasets, outperforming the SOTA HippoRAG by 18.9% on average. The results demonstrate the generalizability of GFM-RAG as the foundation model which can be directly applied to various unseen datasets without any fine-tuning. Additionally, results in Appendix D.4 demonstrate GFM-RAG's strong transferability for further performance improvement when fine-tuned on domain-specific datasets."}, {"title": "4.7. Model Neural Scaling Law", "content": "We further investigate the neural scaling law of GFM-RAG, which quantifies how model performance grows with the scale of training data and model parameter size. It has been validated in the recent foundation models (Kaplan et al., 2020; Dehghani et al., 2023). As shown in Figure 4, the performance of GFM-RAG (MRR: z) scales well with the"}, {"title": "4.8. Path Interpretations", "content": "GFM-RAG exhibits the multi-hop reasoning ability powered by the multi-layer GFM. We provide path interpretations of GFM-RAG for multi-hop reasoning in Table 5. Inspired by NBFNet (Zhu et al., 2021), the paths' importance to the final prediction can be quantified by the partial derivative of the prediction score with respect to the triples at each layer (hop), defined as:\n$S_1, S_2,...,S_L = arg top-k \\frac{\\partial p_{q}(y)}{\\partial e_l}$ (19)\nThe top-k path interpretations can be obtained by the top-k longest paths with beam search. We illustrate the path interpretations in Table 5. In the first example, GFM-RAG successfully deduces that the singer of the song has a football club named after him and that he owned it. In the second example, GFM-RAG identifies two paths related to the murder case and the judge presiding over the trial. These interpretations show that GFM-RAG exhibits the ability of multi-hop reasoning within single-step retrieval. We also illustrate the distribution the multi-hop prediction in Appendix D.6."}, {"title": "5. Conclusion", "content": "In this paper, we introduce the first graph foundation model for retrieval augmented generation. By leveraging the knowledge graph index, GFM-RAG explicitly models the complex relationships between knowledge and documents, facilitating a more effective and efficient retrieval process. Powered by a query-dependent GNN pre-trained on large-scale datasets, GFM-RAG can effectively perform multi-hop reasoning over the graph structure to find relevant knowledge in a single step. Extensive experiments across three benchmark datasets and seven domain-specific datasets demonstrate that GFM-RAG significantly outperforms state-of-the-art methods in effectiveness, efficiency, and generalizability. Its alignment with scaling laws also suggests the potential for scaling to even larger datasets. In the future, we plan to conduct larger-scale training and further explore GFM-RAG's capabilities in other challenging scenarios such as knowledge graph completion and question answering."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Datasets", "content": "A.1. Multi-hop QA Datasets\nThree multi-hop QA datasets are used in our experiments: HotpotQA (Yang et al., 2018), MuSiQue (Trivedi et al., 2022), and 2WikiMultiHopQA (2Wiki) (Ho et al., 2020). We provide a brief overview of these datasets below.\n\u2022 HotpotQA (Yang et al., 2018) is a multi-hop QA dataset that requires reasoning over multiple documents to answer questions. The dataset consists of 97k question-answer pairs, where each question is associated with up to 2 supporting and several distracting documents. The questions are designed to be answerable using multiple pieces of information from the supporting documents.\n\u2022 MuSiQue (Trivedi et al., 2022) is a challenging multi-hop QA dataset with 25k 2-4 hop questions. It requires coherent multi-step reasoning to answer questions that span multiple documents.\n\u2022 2WikiMultiHopQA (2Wiki) (Ho et al., 2020) is a multi-hop QA dataset that requires reasoning over multiple Wikipedia articles to answer questions. The dataset consists of 192k questions, which are designed to be answerable using information from 2 or 4 articles.\nIn experiments, we follow existing methods (Trivedi et al., 2023; Guti\u00e9rrez et al., 2024) to use the same 1,000 samples from each validation set and merge the candidate passages as the document corpus for KG-index construction, whose statistics are presented in Table 6."}, {"title": "A.2. Domain-specific RAG Datasets", "content": "To test the generalizability of GFM-RAG, we evaluate it on seven domain-specific RAG datasets (Friel et al., 2024) including, (1) biomedical: PubMedQA (Jin et al., 2019); (2) customer support: DelucionQA (Sadat et al., 2023), TechQA (Castelli et al., 2020), ExpertQA (Malaviya et al., 2023), EManual (Nandy et al., 2021); (3) general knowledge: MS Marco (Nguyen et al., 2016), HAGRID (Kamalloo et al., 2023). We provide a brief overview of these datasets below.\n\u2022 PubMedQA (Jin et al., 2019) is a collection of PubMed research abstracts with corresponding questions paired with 4 abstract chunks.\n\u2022 DelucionQA (Sadat et al., 2023) is a domain-specific RAG dataset leveraging Jeep's 2023 Gladiator model manual as the source of knowledge, where each question is associated with 4 context documents and only 1 relevant passage.\n\u2022 TechQA (Castelli et al., 2020) is a collection of real-world user questions posted on IBMDeveloper and Developer Works forums, along with 10 technical support documents relating to each question.\n\u2022 ExpertQA (Malaviya et al., 2023) is a collection of curated questions from domain experts in various fields of science, arts, and law. The dataset also contains expert-curated passages relevant to each question.\n\u2022 EManual (Nandy et al., 2021) is a question-answering dataset comprising consumer electronic device manuals and realistic questions about them composed by human annotators, where each question is related with up to 3 context documents.\n\u2022 MS Marco (Nguyen et al., 2016) is an open-domain question-answering dataset sourced from Bing search engine user query logs. Each question is associated with 10 context passages retrieved via Bing web search.\n\u2022 HAGRID (Kamalloo et al., 2023) is a multi-lingual information retrieval dataset with questions and passages from MIRACL (Zhang et al., 2022).\nIn experiments, we use test sets constructed by RAGBench (Friel et al., 2024) and merge all the candidate passages as document corpus for KG-index construction. The statistics of the test dataset are detailed in Table 6."}, {"title": "B. Baselines", "content": "In experiments, we compare with several widely used retrieval methods under three categories: (1) single-step naive methods: BM25 (Robertson & Walker, 1994), Contriever (Izacard et al., 2022), GTR (Ni et al., 2022), ColBERTv2 (Santhanam et al., 2022), RAPTOR (Sarthi et al., 2024), Proposition (Chen et al., 2024); (2) graph-enhanced methods: LightRAG (Guo et al., 2024), HippoRAG (Guti\u00e9rrez et al., 2024); (3) multi-step methods: IRCOT (Trivedi et al., 2023). The detailed introduction of the baselines is as follows.\nSingle-step Naive Methods are widely adopted in real-world applications due to their great efficiency and generalizability."}, {"title": "C. Implementations and Training Details", "content": "C.1. Training Data Construction\nWe extract 60,000 samples from the training set of HotpotQA, MuSiQu, and 2Wiki to construct KG-indexes and conduct large-scale training. Specifically, we merge the candidate passages as the document corpus. In the KG-index construction, we use the GPT-40-mini (OpenAI, 2024a) with the OpenIE prompts described in HippoRAG (Guti\u00e9rrez et al., 2024) to extract the entities, relations, and triples from the document corpus. Then, we use the ColBERTv2 (Santhanam et al., 2022) to conduct the entity resolution by computing the similarity between entities as\n$s(e_i, e_j) = Emb.(e_i) \\cdot Emb.(e_j),$ (20)\nwhere a new triple $(e_i, equivalent, e_j)$ is generated if $s(e_i, e_j) > \\tau$ and $e_i \\ne e_j$. We set the threshold $\\tau$ as 0.8 in our experiments. We divide the samples into groups of approximately 1k questions and 10k documents each to control the constructed KG-index size. In the end, we obtain 60 different KG-indexes and associated question-document pairs for model training.\nC.2. Model Settings\nIn GFM-RAG, the GFM is implemented as a 6-layer query-dependent GNN with the hidden dimension of 512, DistMult message function, and sum aggregation. The relation update function $g^l(\\cdot)$ is implemented as a 2-layer MLP. We use the all-mpnet-v2 as the sentence embedding model with a dimension of 768. The total training parameters of the GFM is 8M. In the retrieval stage, we select top $T = 20$ entities for the document ranker."}, {"title": "C.3. Training Settings", "content": "In the unsupervised KG completion pre-training, the GFM is trained on the mixture of 60 constructed KG-indexes for 30,000 steps. Then, we conduct the supervised document retrieval fine-tuning on the labeled question-document pairs for 5 epochs. The weight $\\alpha$ between losses is set to 0.3. We use AdamW optimizer, learning rate of 5e-4 with batch sizes of both training stages set to 4. Each batch contains only one KG-index and training samples associated to it, where we randomly sample from different KG-indexes during training. The model is trained on 8 NVIDIA A100s (80G) with 14 hours pre-training and 5 hours fine-tuning. The detailed settings are summarized in Table 7."}, {"title": "D. Additional Experiments", "content": "D.1. Effectiveness of Different Sentence Embeddings\nIn this section, we study the effectiveness of different sentence embeddings in the GFM. We compare the all-mpnet-v2 (SBERT, 2021), bge-large-en (Xiao et al., 2023), gte-Qwen2-1.5B-instruct and gte-Qwen2-7B-instruct (Li et al., 2023b) as"}, {"title": "3.2.2. TRAINING PROCESS", "content": "Training Objective. The training objective of the GFM retriever is to maximize the likelihood of the relevant entities to the query, which can be optimized by minimizing the binary cross-entropy (BCE) loss:\n$L_{BCE} = \\frac{1}{|A_q|} \\sum_{e \\in A_q} log P_q(e) - \\frac{1}{|E|} \\sum_{e \\in E} log(1 - P_q(e)),$ (11)\nwhere $A_q$ denotes the set of target relevant entities to the query $q$, and $E \\subseteq E \\setminus A_q$ denotes the set of negative entities sampled from the KG. However, due to the sparsity of the target entities, the BCE loss may suffer from the gradient vanishing problem (Lin et al., 2024). To address this issue, we further introduce the ranking loss (Bai et al., 2023) to maximize the margin between the positive and negative entities:\n$L_{RANK} = \\frac{1}{|A_q|} \\sum_{e \\in A_q} \\frac{1}{\\sum_{e' \\in E} P_q(e')},$ (12)\nThe final training objective is the weighted combination of the BCE loss and ranking loss:\n$L = \\alpha L_{BCE} + (1 - \\alpha)L_{RANK}.$ (13)\nUnsupervised KG Completion Pre-training. To enhance the graph reasoning capability of the GFM retriever, we first pre-train it on a large-scale knowledge graph (KG) completion task. We sample a set of triples from the KG index and mask either the head or tail entity to unsupervisedly create synthetic queries in the form $q = (e, r, ?)$ or $(?, r, e')$, with the masked entity serving as the target entity $A_q = \\{e\\}$ or $\\{e'\\}$. The GFM retriever is then trained to predict the masked entity using both the query and the KG, as outlined in equation 13."}, {"title": "3.3. Documents Ranking and Answer Generation", "content": "Given the entity relevance scores $P_q \\in R^{|E| \\times 1}$ predicted by the GFM retriever, we first retrieve the top-T entities $E^\\top$ with the highest relevance scores as:\n$E^\\top = arg \\top-T(P_q), E^\\top = \\{e_1,..., e_T\\}$. (14)\nThese retrieved entities are then used by the document ranker to obtain the final documents. To diminish the influence of popular entities, we weigh the entities by the inverse of their frequency as entities mentioned in the document inverted index $M \\in \\{0,1\\}^{|E| \\times |D|}$ and calculate the final document relevance scores by summing the weights of"}, {"title": "D.2. Effectiveness of Different Training Strategies", "content": "In this section, we study the effectiveness of the two training tasks used in GFM-RAG."}]}