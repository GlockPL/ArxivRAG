{"title": "ROBUST GYMNASIUM: A UNIFIED MODULAR BENCHMARK FOR ROBUST REINFORCEMENT LEARNING", "authors": ["Shangding Gu", "Laixi Shi", "Muning Wen", "Ming Jin", "Eric Mazumdar", "Yuejie Chi", "Adam Wierman", "Costas Spanos"], "abstract": "Driven by inherent uncertainty and the sim-to-real gap, robust reinforcement learning (RL) seeks to improve resilience against the complexity and variability in agent-environment sequential interactions. Despite the existence of a large number of RL benchmarks, there is a lack of standardized benchmarks for robust RL. Current robust RL policies often focus on a specific type of uncertainty and are evaluated in distinct, one-off environments. In this work, we introduce Robust-Gymnasium, a unified modular benchmark designed for robust RL that supports a wide variety of disruptions across all key RL components-agents' observed state and reward, agents' actions, and the environment. Offering over sixty diverse task environments spanning control and robotics, safe RL, and multi-agent RL, it provides an open-source and user-friendly tool for the community to assess current methods and foster the development of robust RL algorithms. In addition, we benchmark existing standard and robust RL algorithms within this framework, uncovering significant deficiencies in each and offering new insights. The code is available at this website\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "Reinforcement learning (RL) is a popular learning framework for sequential decision-making based on trial-and-error interactions with an unknown environment, achieving success in a variety of applications, such as games (Mnih et al., 2015; Vinyals et al., 2019), energy systems (Chen et al., 2022), finance and trading (Park & Van Roy, 2015; Davenport & Romberg, 2016), and large language model alignment (OpenAI, 2023; Ziegler et al., 2019).\nDespite recent advances in standard RL, its practical application remains limited due to concerns over robustness and safety. Specifically, policies learned in idealized training environments often fail catastrophically in real-world scenarios due to various factors such as the sim-to-real gap (Pinto et al., 2017), uncertainty (Bertsimas et al., 2019), noise, and even malicious attacks (Zhang et al., 2020; Klopp et al., 2017; Mahmood et al., 2018). Robustness is key to deploying RL in real-world applications, especially in high-stakes or high-cost fields such as autonomous driving (Ding et al., 2023b), clinical trials (Liu et al., 2015), robotics (Li et al., 2021), and semiconductor manufacturing (Kozak et al., 2023). Towards this, Robust RL seeks to ensure resilience in the face of the complexity and variability of both the physical world (Bertsimas et al., 2019) and human behavior (Tversky & Kahneman, 1974; Arthur, 1991).\nRobust RL policies currently fall short of the requirement for broad deployment. Disruptions or interventions can occur at various stages of the agent-environment interaction, affecting the agent's observed state (Zhang et al., 2020; 2021b; Han et al., 2022; Sun et al., 2021; Xiong et al., 2022), observed reward (Xu & Mannor, 2006), action (Huang et al., 2017), and the environment (transition kernel) (Iyengar, 2005; Pinto et al., 2017) and existing robust RL policies are vulnerable to such real-world failures (Mandlekar et al., 2017). This vulnerability is, in part, a result of the fact that policies are designed to address only one specific type of disruption (e.g., over the observed state), among other technical limitations (Ding et al., 2024). More critically, robust RL policies are often evaluated in distinct, one-off environments that can be narrow or over-fitted to the proposed algorithms. The absence of standardized benchmarks is a key bottleneck to progress in robust RL. Ideally, a benchmark should offer a wide range of diverse tasks for comprehensive evaluation and account for uncertainty and disruptions over multiple stages throughout the interaction process."}, {"title": "2 A UNIFIED ROBUST REINFORCEMENT LEARNING FRAMEWORK", "content": "We begin by presenting a robust RL framework that unifies various robust RL tasks explored in the literature, including combinations of these paradigms. We outline the framework in the context of single-agent RL and then extend it to encompass broader classes of RL tasks, such as safe RL and multi-agent RL.\nBackground: Markov decision process (MDP). A single-agent RL problem is formulated as a finite-horizon Markov decision process (MDP), represented by the tuple $M = (S, A, T, P^o, r^o)$, where $S$ and $A$ denote the (possibly infinite) state and action spaces, and $T$ is the horizon length. The nominal transition kernel $P^o = \\{P_t\\}_{1 \\leq t \\leq T}$ defines the environmental dynamics: $P(s'|s, a)$ gives the probability of transitioning from state $s$ to state $s'$ given action $a$ at time step $t$. The reward function $r^o = \\{r_t\\}_{1 \\leq t \\leq T}$ represents the immediate reward at time step $t$, given the current state $s$ and action $a$."}, {"title": "2.1 A UNIFIED ROBUST RL FRAMEWORK: MDPS WITH DISRUPTION", "content": "To proceed, we introduce an additional disruption module that represents potential uncertainties or disturbances that impact different stages of the agent-environment interaction process (MDP). This module provides a categorized summary of the types of uncertainty addressed in prior robust RL studies.\nDisruptors. We introduce each type in detail as follows:\n\u2022 Observation-disruptor. An agent's observations may not perfectly reflect the true status of the environment due to factors like sensor noise and time delays. To model this sensing inaccuracy, we introduce an additional module-the observation-disruptor-which determines the agent's observations from the environment: Agents' observed state $S_t$: The observation-disruptor takes the true current state $s_t$ as input and outputs a perturbed state $S_t = D_s(s_t)$. The agent uses $S_t$ as input to its policy to select an action; Agents' observed reward $r_t$: The observation-disruptor takes the real immediate reward $r_t$ as input and outputs a perturbed reward $\\hat{r_t} = D_r(r_t)$. The agent observes $\\hat{r_t}$ and updates its policy accordingly.\n\u2022 Action-disruptor. The real action $a_t$ chosen by the agent may be altered before or during execution in the environment due to implementation inaccuracies or system malfunctions. The action-disruptor models this perturbation, outputting a perturbed action $\\tilde{a_t} = D_a(a_t)$, which is then executed in the environment for the next step.\n\u2022 Environment-disruptor. Recall that a task environment consists of both the internal dynamic model and the external workspace it interacts with, characterized by its transition dynamics $P$ and reward function $r$. The environment during training can differ from the real-world environment due to factors such as the sim-to-real gap, human and natural variability, external disturbances, and more. We attribute this potential nonstationarity to an environment-disruptor, which determines the actual environment $(\\mathbb{P}, r)$ the agent is interacting with at any given moment. These dynamics may differ from the nominal environment $(P^o, r^o)$ that the agent was originally expected to interact with.\nMDPs with Disruption. As shown in Fig. 2, a robust RL problem can be formulated as a finite-horizon MDP with an additional disruption module $M_{dis} = (S,A,T, P,r, D_s(\\cdot), D_r(\\cdot), D_a(\\cdot))$, abbreviated as Disrupted-MDP. It consists of three potential disruptors introduced above. Specifically, the interaction process between an agent and an MDP with disruption (Fig. 2) unfolds as follows: at each time step $t \\in [T]$, the (possibly perturbed) environment outputs the current state $s_t$ and reward $r_t$. The observation-disruptor then perturbs these, sending the modified state $S_t = D_s(s_t)$ and reward $\\hat{r_t} = D_r(r_t)$ to the agent. Based on these, the agent selects an action $a_t \\sim \\pi_t(S_t)$, according to its policy $\\pi = \\{\\pi_t\\}_{1 \\leq t \\leq T}$, where $\\pi_t : S \\rightarrow \\Delta(A)$ defines the probability distribution over actions in $A$ given the observed state $S_t$. The action-disruptor then perturbs this action to $\\tilde{a_t} = D_a(a_t)$, which is then sent to a perturbed environment governed by the environment-disruptor, based on the reference nominal environment $(P^o, r^o)$. The environment then transitions to the next state $s_{t+1} \\sim P_t(S_t, \\tilde{a_t})$ and provides the reward $r_{t+1}(s_t, \\tilde{a_t})$, which becomes the input for the observation-disruptor in the next step $t + 1$."}, {"title": "3 Robust-Gymnasium: A UNIFIED ROBUST RL BENCHMARK", "content": "We now introduce our main contribution, a modular benchmark (Robust-Gymnasium) designed for evaluating Robust RL policies in robotics and control tasks. Each task is constructed from three main components: an agent model (the robot object), an environment (the agent's workspace), and a task objective (such as navigation or manipulation). Robust-Gymnasium offers robust RL tasks by integrating various disruptors of different types, modes, and frequencies with these task bases. Not all task bases support every type of disruption. A detailed list of the robust RL tasks implemented in this benchmark is available in Figure 17. In the following sections, we introduce over 60 task bases from eleven sets, outline the design of the disruptors, and describe the construction of a Disrupted-MDP- robust RL tasks."}, {"title": "3.1 TASK AND ENVIRONMENT BASES", "content": "Gymnasium-Box2D (three relative simple control tasks in games).\nThese tasks are from Gymnasium (Towers et al., 2024), including three robot models from different games, such as the Bipedal Walker \u2014 a 4-joint walking robot designed to move forward and Car Racing - navigating a track by learning from pixel inputs (Parberry, 2017; Brockman et al., 2016).\nGymnisium-MuJoCo (eleven control tasks).\nIt includes various robot models, such as bipedal and quadrupedal robots. This benchmark is widely used in various RL problems, including standard online and offline RL, with representative examples like Hopper, Ant, and HalfCheetah (Todorov et al., 2012; Brockman et al., 2016).\nMaze (two navigation environments).\nMaze comprises environments where an agent must reach a specified goal within a maze (Gupta et al., 2020). Two types of agents are available: a 2-degrees of freedom (DoF) ball (Point-Maze) and a more complex 8-DoF quadruped robot (Ant-Maze) from Gymnasium-MuJoCo. Various goals and maze configurations can be generated to create tasks of varying difficulty.\nFetch (four tasks for Fetch Mobile Manipulator robot arm).\nFetch features a 7-degrees of freedom (DoF) Fetch Mobile Manipulator arm with a two-fingered parallel gripper (Plappert et al., 2018). The environment consists of a table with various objectives, resulting in four tasks: Reach, Push, Slide, and PickAndPlace, which involve picking up or moving the objects to specified locations.\nFranka Kitchen (tasks need long-horizon, multi-task planning for a robot arm).\nThis environment is based on a 9-degrees of freedom (DoF) Franka robot situated in a kitchen containing common household items like a microwave and cabinets (Gupta et al., 2020). The task goal is to achieve a specified configuration, which may involve planning and completing multiple sub-tasks. For example, a goal state could have the microwave open, a kettle inside, and the light over the burners turned on.\nDexterous Hand (five dexterous hand manipulation tasks).\nIt is based on the Shadow Dexterous Hand\u2014an anthropomorphic 24-DoF robotic hand with 92 touch sensors at palm and phalanges of the fingers (Plappert et al., 2018; Melnik et al., 2021). The tasks involve manipulating various objects, such as a pen, egg, or blocks.\nAdroit (four manipulation tasks for a dexterous hand attached to a free arm).\nThis environment features a free arm equipped with a Shadow Dexterous Hand, providing up to 30-DoF (Rajeswaran et al., 2018). The high degree of freedom enables the robot to perform more complex tasks, such as opening a door with a latch (AdroitHandDoor).\nHumanoidBench (four tasks for a high-dimensional humanoid).\nWe incorporate four tasks from the recent HumanoidBench (Sferrazza et al., 2024) designed mainly for a Unitree H1 humanoid robot, which is equipped with two dexterous Shadow Hands. Specifically, we include two manipulation tasks (push, truck) and two locomotion tasks (reach, slide), all of which require sophisticated coordination among various body parts.\nRobosuite (twelve tasks for various modular robot platforms).\nRobosuite is a popular modular benchmark (Zhu et al., 2020) that supports seven robot arms, eight grippers, and six controller modes. The manipulation tasks are conducted in environments with doors, tables, and multiple robot arms, with goals such as wiping tables or coordinating to transfer a hammer. Additionally, we introduce a new task-MultiRobustDoor-featuring an adversarial arm that impedes another arm's success to test robustness.\nSafety MuJoCo (nine control tasks with additional safety constraints).\nBuilt on standard robot models in Gymnasium-MuJoCo, the Safety MuJoCo tasks are designed for scenarios that prioritize both long-term returns and safety. These tasks incorporate safety constraints, such as limiting velocity and preventing robots from falling (Gu et al., 2024b).\nMAMuJoCo (twelve multi-agent cooperation tasks).\nMAMuJoCo is based on a multi-agent platform from the factorizations of Gymnisium-MuJoCo robot models (Peng et al., 2021). The tasks need to be solved by cooperations of multiple agents. This set of tasks are vulnerable to disturbance like one leg of a quadruped robot is malfunctioning, or all dynamics of legs are contaminated by system noise."}, {"title": "3.2 DISRUPTOR DESIGN: MODES AND FREQUENCIES", "content": "In a Disrupted-MDP, disruptors affecting various stages of the agent-environment interaction can operate in different modes. We typically consider four common modes found in the robust RL literature, each driven by specific real-world scenarios and robustness requirements. These modes allow the construction of tasks with varying levels of difficulty:\n\u2022 Random disturbance: for all disruptors. Stochastic noise is ubiquitous in sensors, mechanical hardware, and random events, often modeled as random noise added to nominal components in the interaction process (Duan et al., 2016). The noise typically follows a distribution such as Gaussian or uniform. This mode can be applied to all disruptors, affecting the agent's observed state, observed reward, action, and environment.\nWe offer Gaussian distribution $\\mathcal{N}(\\cdot,\\cdot)$ (Zhang et al., 2018) and bounded uniform distribution $\\mathcal{U}(\\cdot,\\cdot)$ (Zouitine et al., 2024) as default options. For instance, the environment-disruptor can introduce noise to robot dynamics (e.g., mass, torso length) or external factors (e.g., gravity, wind), as shown in Fig. 4. The observation-disruptor can add noise to the observed state and/or reward, namely, $S_t = s_t + \\mathcal{N}(\\mu_s,\\sigma_s)$ (${\\mu_s}$ and ${\\sigma_s}$ are the mean and variance) or $S_t = s_t +\\mathcal{U}(a_s, b_s)$ ($a_s, b_s$ are the min and max thresholds). The action-disruptor can also introduce noise to the action sent to the environment.\n\u2022 Adversarial disturbance: for all disruptors. In real-world applications, adversarial disturbances occur when external forces deliberately attempt to degrade the agent's performance. This mode is also relevant when prioritizing safety, ensuring the agent can perform well in worst-case scenarios within certain predefined sets. It can be applied to all three disruptors. This mode can be viewed as a two-player zero sum game between the agent and an adversarial player (Tanabe et al., 2022). Any algorithms can acts as the adversarial player through this interface to adversarially attack the process. This mode is applicable to all disruptors; for instance, the observation-disruptor generates a fake state that falls within the prescribed set around the true state, or the environment-disruptor adjusts parameters within a neighborhood of the nominal values;\nNotably, in our benchmark, we implement and feature an algorithm leveraging LLM to determine the disturbance. In particular, the LLM is told of the task and uses the current state and reward signal as the input. It directly outputs the disturbed results like a fake state for the agent. See more details in the code in Appendix C.1.\n\u2022 Internal dynamic shift: for the environment-disruptor. This mode captures variations in the agent's internal model between training and testing, caused by factors such as the sim-to-real gap, measurement noise, or accidental malfunctions. The environment-disruptor introduces biases to dynamic parameters within a prescribed uncertainty set. For example, the torso length (Fig. 4 (c)) might shift from 0.3 to 0.5.\nFor tasks in control and robotics, the environment disruptor can alter the robot model, changing the system's internal dynamics (Zhang et al., 2020; Zouitine et al., 2024). Using Gymnasium-MuJoCo as an example, Fig. 3(b)-(c) depict the consequences of such disruption by altering the Ant robot's head and legs around its original model (Fig. 3(a)).\n\u2022 External disturbance: for the environment-disruptor. Nonstationarity in the external workspace can result from variability in the physical world or human behavior, such as changes in wind, friction, or human intervention. The environment-disruptor uses this mode to modify the external task environment by altering properties and configurations within the robot's workspace or by introducing abrupt external interventions (Luo et al., 2024; Pinto et al., 2017; Ding et al., 2024).\nFor example, in robosuite, Fig. 3(e)-(f) illustrate disrupted tasks compared to the original reference in Fig. 3(d). In these tasks, the environment disruptor changes the distance between the table and the arm, or even introduces an additional arm to actively interfere with the yellow-black robot's ability to accomplish its goal.\nTiming of operations for disruptors. We support perturbations occurring at any stage of the process and at different frequencies. Users can choose to apply perturbations at any time step or episode during the training process, or exclusively during testing."}, {"title": "3.3 CONSTRUCTING ROBUST RL TASKS", "content": "Robust-Gymnasium is a modular benchmark that offers flexible methods for constructing robust RL tasks through three main steps. First, we select a task base from the eleven options outlined in Sec. 3.1. Second, we choose a disruptor from the observation, action, and environment categories introduced in Sec. 3.2), and specify its operation modes (random disturbance, adversarial disturbance, internal dynamic shift, and external disturbance, as detailed in Sec. 3.2). Finally, we determine the interaction process and frequencies between the disruptor, agent, and environment.\nIn addition to these basic construction methods, our benchmark supports advanced modes: A combination of disruptors allows users to select multiple disruptors, such as an observation-disruptor and an environment-disruptor, to simulate conditions where perception sensors have system noise and external disturbances from human occur; Varying operation frequencies enables disruptors to operate intermittently during interactions, either at fixed intervals or in a random pattern to characterize accidental events and uncertainties."}, {"title": "4 EXPERIMENTS AND ANALYSIS", "content": "Robust-Gymnasium offers a variety of tasks for comprehensively evaluating the robustness of different RL paradigms. We demonstrate its flexibility by constructing robust RL tasks based on various task bases, incorporating disruptions with different types, modes, and frequencies, and evaluating several SOTA algorithms on these tasks. In addition to benchmarking existing algorithms, we also highlight an adversarial disruption mode that leverages LLMs. Examples of robust RL tasks are shown in Figure 4. More details about the experiments can be found in Appendix E.\nBenchmark RL algorithms. Specifically, we benchmark several SOTA algorithms in their corresponding robust RL tasks: Standard RL: Proximal Policy Optimization (PPO) (Schulman et al., 2017), Soft Actor-Critic (SAC) (Haarnoja et al., 2018); Robust RL: Occupancy-Matching Policy Optimization (OMPO) (Luo et al., 2024), Robust State-Confounded SAC (RSC) (Ding et al., 2024), Alternating Training with Learned Adversaries (ATLA) (Zhang et al., 2021b), and Deep Bisimulation for Control (DBC) (Zhang et al., 2021a); Safe RL: Projection Constraint-Rectified Policy Optimization (PCRPO) (Gu et al., 2024b), Constraint-Rectified Policy Optimization (CRPO) (Xu et al., 2021); Multi-Agent RL: Multi-Agent PPO (MAPPO) (Yu et al., 2022), Independent PPO (IPPO) (De Witt et al., 2020).\nEvaluation processes. We mainly focus on two evaluation settings: In-training: the disruptor simultaneously affects the agent and environment during both training and testing at each time step. This process is typically used in robotics to address sim-to-real gaps by introducing potential noise during training; 2) Post-training: the disruptor only impacts the agent and environment during testing, mimicking scenarios where learning algorithms are unaware of testing variability.\nRobust metrics. In this work, we usually use the performance in the original (deployment) environment as the robust metric for evaluations. While there are many different formulations of the robust RL objective (robust metrics), such as risk-sensitive metrics (e.g., CVaR) (Chan et al., 2019), and the worst-case or average performance when the environment shifts (Zouitine et al., 2024)."}, {"title": "4.1 EVALUATION OF STANDARD RL BASELINES", "content": "To begin, we evaluate two types of robust RL tasks: one with an observation disruptor (affecting the agent's observed state) and the other with an action disruptor (affecting the action), both subjected to random disturbances at varying levels. We benchmark the performance of standard RL baselines-PPO (Schulman et al., 2017) and SAC (Haarnoja et al., 2018)\u2014on robust RL tasks based on the representative HalfCheetah-v4 task from Gymnasium-MuJoCo, as partially shown in Figure 5. Here, S=0.1 indicates that the random disturbance over the state follows a Gaussian distribution with a mean of 0 and a standard deviation of 0.1 (resp.0.15). The same applies for A=0.1 or A=0.15. Figures 5 (a)-(b) and Figure 5 (c)-(d) present the results from two different evaluation processes-In-training and Post-training, respectively. The results show that as the disturbance level increases, the performance of the baselines degrades quickly, particularly when the training process is unaware of potential disturbances (as seen in the Post-training results). More experiments, including those using disturbances over reward or the results for SAC, can be found in Appendix B.1."}, {"title": "4.2 EVALUATION OF ROBUST RL BASELINES", "content": "In this section, we evaluate robust RL tasks using an environment disruptor under two representative modes: internal dynamic shift and external disturbance. The robust RL tasks are based on various task bases, including Ant-v5 and Hopper-v5 from Gymnasium-MuJoCo, as well as DoorCausal-v1 and LiftCausal-v1 from Robosuite, utilizing the In-training evaluation process.\nSpecifically, Figure 6(a-b) displays the performance of the robust RL baseline OMPO across two tasks with internal dynamic shifts: (a) Ant-v5 with varying gravity and wind strength, and (b) Hopper-v5 with changes to the robot model's shape, including torso and foot length. Experimental settings can be found in (4) and (6) in Appendix C.2. The results indicate that OMPO's performance significantly declines in non-stationary environments compared to stationary conditions without disturbances.\nFigures 6(c-d) illustrate the performance of three robust RL baselines (RSC, ATLA, DBC) in two tasks from Robosuite involving disruptions on the environment with external semantic disturbances. In the DoorCausal task, the initial distance of the door from the robot and the height of the door handle are varied in a correlated manner. In the CausalLift task, both the position and color of the object to be lifted are changed together according to specific patterns. RSC demonstrates greater robustness than ATLA and DBC, maintaining stable reward trajectories throughout the training process. However, RSC's training efficiency may need further improvement, as it generates augmentation data during policy learning."}, {"title": "4.3 EVALUATION OF SAFE RL BASELINES", "content": "Two safe RL baselines, PCRPO (Gu et al., 2024b) and CRPO (Xu et al., 2021), are benchmarked on robust safety-critical tasks using the In-training evaluation process. Specifically, we assess two types of robust RL tasks based on Walker2d from Gymnasium-MuJoCo: (a) an action-disruption attacks the agent's action with different levels; (b) the agent's observe immediate safety cost is disturbed in different levels. These attacks follow a Gaussian distribution with a mean of 0 and standard deviations of 0.15 or 0.3 for both the action and the observed cost. The outcomes and safety costs for these tasks are presented in Figures 12(a-b) and Figures 12(c-d), respectively. The performance of CRPO quickly degrades when disruptions occur, while PCRPO demonstrates greater robustness against disturbances in either action or observed cost. Notably, PCRPO's performance under disturbance surpasses its performance without disturbance, suggesting that introducing appropriate disturbances during training may enhance overall performance. Due to space limitations, additional results can be found in Appendix B.2."}, {"title": "4.4 EVALUATION OF MULTI-AGENT RL BASELINES", "content": "We evaluate two MARL baselines: Multi-Agent PPO (MAPPO) (Yu et al., 2022) and Independent PPO (IPPO) (De Witt et al., 2020) on MA-HalfCheetah-v4 from MAMoJoCo under various disruption settings affecting the agents' observed states, actions, and rewards. Using the In-training evaluation process, as shown in Figure 8, we apply disruptions to all agents. The results indicate that the performance of both MAPPO and IPPO degrades accordingly as the disruptions occur. Additionally, we conduct experiments involving partial disruptions on a subset of agents within the multi-agent system; further details can be found in Appendix B.3."}, {"title": "4.5 ADVERSARIAL DISTURBANCE THROUGH LLMS", "content": "In addition to benchmarking various existing RL algorithms, this section demonstrates the adversarial disturbance mode by leveraging a featured approach with LLMs. As shown in Figure 9, we evaluate the performance of PPO on Ant-v4 with adversarial disruptions to the agent's observed state. Different attack configurations are employed, including comparisons to uniform noise and testing varying frequencies. Here, \u201cC[0.2-0.8]\" indicates that the noise level from the LLM is constrained within the [0.2, 0.8] range; \u201c100F\u201d (resp. \u201c500F\") signifies that the agent is attacked every 100 (resp. 500) steps; and \"U[0.2-0.8]\" represents noise drawn from a uniform distribution $\\mathcal{U}(0.2, 0.8)$. The results show that LLM-based attacks lead to a more significant performance drop for PPO compared to that using uniform distribution (Figure 9(a)). Figure (b) examines how varying attack frequencies affect performance, revealing that higher-frequency attacks (PPO-S-100F) result in greater performance degradation. Due to space constraints, additional frequency experiments on other robust tasks based on Gymnasium-MuJoCo using PPO are provided in Appendix B.4."}, {"title": "5 CONCLUSION", "content": "In this work, we introduce Robust-Gymnasium, a unified modular benchmark explicitly designed for robust RL. Unlike existing RL benchmarks, Robust-Gymnasium aims to evaluate the resilience of RL algorithms across a wide range of disruptions. These disruptions include perturbations at every stage of the entire agent-environment interaction process, affecting agent observations, actions, rewards, and environmental dynamics. Robust-Gymnasium provides a comprehensive platform for benchmarking RL algorithms, featuring over 60 diverse task environments across domains such as robotics, multi-agent systems, and safe RL. Additionally, we benchmark various SOTA RL algorithms, including PPO, MAPPO, OMPO, RSC, and IPPO, across a wide array of robust RL tasks in Robust-Gymnasium. The results highlight the deficiencies of current algorithms and motivate the development of new ones. This work represents a significant step forward in standardizing and advancing the field of robust RL, promoting the creation of more reliable, generalizable, and robust learning algorithms."}, {"title": "C OTHER SETTINGS OF THE FRAMEWORK", "content": "C.1 BENCHMARK FEATURES\nThe features of the benchmark are as follows:\nHigh Modularity: It is designed for flexible adaptation to a variety of research needs, featuring high modularity to support a wide range of experiments.\nWide Coverage of: It provides a comprehensive set of tasks to evaluate robustness across different RL scenarios. An overview of the task list is shown in Figure 17.\nHigh Compatibility: It can be seamless and compatible with a wide range of existing environments. An example is shown in Listing 1. Moreover, this benchmark supports vectorized environments, which means it can be useful to enable parallel processing of multiple environments for efficient experimentation.\nC.2 ROBUST NON-STATIONARY TASKS:\nInspired by OMPO (Luo et al., 2024), we provide various task settings to evaluate policy robustness, as illustrated in Figure 16. During policy learning, we introduce adversarial attacks during walking or running tasks by altering robot dynamics and environmental conditions. For instance, we stochastically adjust the robot's gravity and the environment's wind speed, introducing uncertain disturbances during policy learning. Additionally, we stochastically modify the robot's physical shape throughout the learning process to test and enhance policy robustness.\nSpecifically, in non-stationary Ant-v5 Tasks, during each step, we introduce noise into the agent's dynamics by attacking factors like the Ant robot's gravity and the wind speed in the robot's environment. As demonstrated in Equation (2) for attacks at initial and training steps, we introduce deterministic perturbations to the Ant robot, such as variations in gravity and environmental wind speed, the pseudo code is shown in Listing 3. Furthermore, Equation (3) is for initial noise, and Equation (4) is for noise during training we use these Equarions to consider the incorporation of stochastic disturbances into the Ant robot model, again including factors like gravity fluctuations and wind speed variations, the pseudo code is shown in Listing 4. Apart from wind and gravity disturbances, we also investigate the robot shape disturbances during policy learning, as shown in Equations (5)-(8), and an example of pseudo code is shown in Listing 5."}, {"title": "D REPRESENTATIVE EXAMPLES OF USING Robust-Gymnasium", "content": "In this section, we present an overview of the task environments, as illustrated in Figure 17. Additionally, we show some robustness-focused tasks, detailed in Tables 1-8.\nMoreover, inspired by (Yu et al., 2020), to illustrate the standardized usage of our benchmark, we propose the following framework for evaluation settings. These align with the principles of benchmarking, including standardized performance metrics and evaluation protocols:\n\u2022 Random attack (Easy) \u2192 Adversarial attack (Hard). Random Attack (Easy): Random noise, drawn from distributions such as Gaussian or uniform, is added to the nominal variables. This mode is applicable to all sources of perturbation and allows for testing robustness under stochastic disturbances, e.g., see Figure 5 (a) and (b). Adversarial Attack (Hard): An adversarial attacker selects perturbations to adversely degrade the agent's performance. This mode can be applied to observation or action perturbations and represents the most challenging scenario, e.g., see Figure 9 (a) and (b).\n\u2022 Low state-action dimensions (Easy) \u2192 High state-action dimensions (Hard) As the state and action space dimensions increase, the tasks become significantly more challenging. The difficulty level of tasks typically progresses from Box2D, Mujoco tasks, robot manipulation, and safe tasks to multi-agent and humanoid tasks. For instance, the Humanoid task, with a 51-dimensional action space and a 151-dimensional state space, is substantially more challenging than the Mujoco Hopper task, which has a 3-dimensional action space and an 11-dimensional state space."}]}