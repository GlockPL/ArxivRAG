{"title": "Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment Revealing Hidden Fault Lines in Large Language Models", "authors": ["Rubing Li", "Jo\u00e3o Sedoc", "Arun Sundararajan"], "abstract": "When encountering increasingly frequent performance improvements or cost reductions from a new large language model (LLM), developers of applications leveraging LLMs must decide whether to take advantage of these improvements or stay with older tried-and-tested models. Low perceived switching frictions can lead to choices that do not consider more subtle behavior changes that the transition may induce. Our experiments use a popular game-theoretic behavioral economics model of trust to show stark differences in the trusting behavior of OpenAI's and DeepSeek's models. We highlight a collapse in the economic trust behavior of the o1-mini and 03-mini models as they reconcile profit-maximizing and risk-seeking with future returns from trust, and contrast it with DeepSeek's more sophisticated and profitable trusting behavior that stems from an ability to incorporate deeper concepts like forward planning and theory-of-mind. As LLMs form the basis for high-stakes commercial systems, our results highlight the perils of relying on LLM performance benchmarks that are too narrowly defined and suggest that careful analysis of their hidden fault lines should be part of any organization's AI strategy.", "sections": [{"title": "Introduction", "content": "As large language models (LLMs) become the foundation for a variety of business applications, choosing appropriate benchmarks and surrogates that capture benefits and costs of post-deployment behavior becomes increasingly central to an organization's returns from their AI investments. Granted, multiple academic and commercial benchmarks (Wang et al., 2019; Chiang et al., 2024; White et al., 2024; AI, 2024; Xia et al., 2024; Chang et al., 2024) are available to assess baseline \"raw\" LLM intelligence, supplemented by metrics of bias, safety, personality, alignment, and task-specific performance (Ferrara, 2023; Hagendorff et al., 2023; Giorgi et al., 2023; Cao et al., 2023). However, as increasingly autonomous AI agents participate on behalf of humans in the economy, more subtle aspects of their economic personality will be important determinants of their reliability and success. We posit that trusting behavior is a central but frequently ignored aspect of this kind. Trust is fundamental to business and social interaction, and AI systems, however sophisticated, will fail unless trusted by users.\nHere, we examine conditions under which trusting behaviors that are human-like emerge in LLMs. We use the trust game (Berg et al., 1995) that has been widely used for economic experiments involving human subjects as the basis to assess trusting behavior. We show how LLM trusting behaviors across models vary with how trustworthy the counterparty actually is, the preferences assigned to the LLM and the reasoning strategies used by the LLM, uncovering sharp contrasts between OpenAI's and DeepSeek's models.\nOur contributions are summarized below.\n(1) We highlight the importance of expanding LLM performance metrics beyond raw intelligence or compute cost to include other aspects of human behavior central to eventual success.\n(2) We provide the first analysis of the interplay between trusting behavior and reasoning strategies increasingly central to LLM performance improvements, and the first evidence of DeepSeek's superior trusting behavior in complex settings.\n(3) We provide a new, standardized implementation for LLM-based agents playing repeated games that reflects the underlying economic structure of deterministic finite-horizon games and instrumentation that enables differentiation between models, demonstrating importance through our results."}, {"title": "Background & Related Work", "content": "LLMs are increasingly used to simulate human behavior and interaction in real-world settings. Prior research examining the economic behavior of LLMs in laboratory-like experimental settings has shed new light on the role of endowments, information, revealed preference, and rationality (Chen et al., 2023; Horton, 2023; Gui and Toubia, 2023; Leng, 2024; Goli and Singh, 2024).\nTrust Game The trust game (Berg et al., 1995)\u2014a classic model of behavioral economics-is designed as follows. There are two players, a sender and a receiver, both endowed with ten dollars. The sender moves first, choosing a fraction of their endowment to send to the receiver. According to the design of the game, the amount sent to the receiver is inflated (typically tripled). The receiver then chooses what amount to transfer back to the sender, which measures the level to which they \"reciprocate.\" The amount the sender chooses measures trust, since a sender who places a higher trust that the receiver will reciprocate will send a higher amount, while one who expects the receiver to simply keep their gains will send nothing. (See Appendix A.1)\nOver the last thirty years, thousands of experiments using the trust game have deepened understanding of human trusting and reciprocity behavior. The unique subgame perfect Nash equilibrium in a single-shot or finitely-repeated trust game involves the sender sending $0. Strikingly, however, human subjects who have played the trust game have consistently chosen a more \"trusting\u201d behaviors in both single-shot and multi-round experiments, typically sending about 50% of their endowment and obtaining better outcomes than the Nash equilibrium would predict (Johnson and Mislin, 2011).\nThe trust game has revealed key insights about human trusting behavior. The trust game has revealed that human trusting behavior differs across regions, with variations observed between North America, Europe, and Africa. It is also distinct from a person's risk attitudes (Houser et al., 2010) and is shaped by other-regarding preferences such as altruism (Barclay, 2004). Additionally, a greater aversion to ambiguity (Li et al., 2019) or fear of betrayal (Bohnet and Zeckhauser, 2004) lowers trusting behavior. Finally, research shows that humans dosed with oxytocin tend to be more trusting (Fehr et al., 2005).\nThe widespread acceptance of the trust game in behavioral economics underscores its validity and motivates our use of it. Our experiments contribute to a nascent literature (for example, Xie et al. (2024) and Gao et al. (2025)) that uses the trust game to study the trusting behavior of non-human agents. Our paper is the first we are aware of to examine the interplay between trusting behavior and the reasoning strategy the LLM uses, and to compare GPT with DeepSeek. The contrast we reveal between standardized performance benchmarks and trusting behavior (the \"collapse\" of trust that we encounter in newer GPT models, for example) underscores the importance of a broader view on LLM evaluation. We also add to the recent literature on benchmarks for LLMs playing economic games (Duan et al., 2024) and repeated games (Abdelnabi et al., 2023)."}, {"title": "Methods", "content": "We study LLM behavior as the \u201csender\u201d to a rule-based receiver in the play of a ten-round repeated trust game. We simulate a lab-like environment through structured prompts that capture the trust game, designing a modular prompt structure that clearly distinguishes between fixed and variable components, allowing us to alter different parts of the prompt based on experimental parameters. These parameters vary the objective of the sender, its reasoning strategy, information about the game, the behavior of the counterparty, and information about this behavior, as explained in what follows.\nObjective: The objective defines the preferences of the LLM agent (the sender), including the role the LLM should assume and the behavior it should display. We use three different objectives: helpful, profit-maximizing, and risk-seeking.\nLLM Versions: We contrast five LLM senders: GPT-40-mini (Hurst et al., 2024), 01-mini (Jaech et al., 2024), 03-mini (OpenAI, 2025), DeepSeek-R1 and DeepSeek-R1-Distill-Llama-70b (Guo et al., 2025). We also include the performance of GPT-3.5-turbo (OpenAI, 2023) from an earlier set of experiments.\nReasoning Strategies: Our baseline is direct prompting, which we compare with the infusion of two reasoning strategies: zero-shot chain-of-thought or CoT (Wei et al., 2023; Kojima et al., 2023) and self-consistency (Wang et al., 2023) into"}, {"title": "Results and Discussion", "content": "In experiments that preceded those whose results we report here, we validate our hypothesis that for GPT-3.5-turbo, the infusion of CoT and self-consistency reasoning alters trusting behavior.\nUnsurprisingly, LLMs that integrate CoT reasoning into their inference (Jaech et al., 2024; Guo et al., 2025, 01-mini, 03-mini, DeepSeek-R1, DeepSeek-R1-Distill-Llama-70b) do not alter their trusting behavior after infusing either CoT or self-consistency reasoning. Surprisingly, we also find that we are unable to reject the null hypothesis that infusing GPT-40-mini with either CoT or self-consistency reasoning alters its trusting behavior. (See Appendix A.3 for an illustrative comparison.)\nWe therefore report the results involving our baseline treatment that introduces no additional reasoning strategies via prompting. We base our results on two measures: (1) the amount the LLM ended up with at the end of the ten-round game as a fraction of the theoretical maximum it could"}, {"title": "Conclusion", "content": "Our experiments use a popular game-theoretic behavioral economics model to reveal stark and statistically significant differences in the trusting behavior and performance of DeepSeek and OpenAI's LLMs. DeepSeek's models display more sophisticated and profitable trusting behavior stemming from an ability to incorporate deeper concepts like forward planning and theory-of-mind, balancing short-term profit objectives with longer-term goals when facing a trustworthy adversary, and correspondingly tempering how much risk it takes when facing an untrustworthy adversary even if promoted to be risk-seeking. Our experiments found no evidence that OpenAI's models were able to similarly reconcile short-run objectives with the longer-term considerations essential for building trust. Our study also contributes structured methodologies for understanding the currently underexplored problem of trusting behavior in LLMs which today form the basis for a growing number of high-stakes commercial systems. Our findings suggest that careful analysis of their hidden fault lines that move beyond merely relying on benchmarks of intelligence and compute cost should be part of any organization's AI strategy."}, {"title": "Limitations", "content": "Trust games, while widely used, are only one of many possible proxies for economic trusting behavior. While our work shows striking contrasts in trusting behavior across AI systems it is difficult to assess the full impact of these differences in more general settings. We ran our experiments with default temperature and settings. Our examination of LLMs uses a limited set of prompts and our experiments were conducted only in English."}, {"title": "Appendix", "content": "A.1 Illustration of the Trust Game\nThe image below illustrates the trust game."}, {"title": "Prompt Structure", "content": "A.2\nStructure\nDescription\nPremise\nYou are a [helpful/profit-maximizing/risk-seeking/etc] assistant.\nInstruction\nThis task is about an exchange between yourself and another person to\nwhom you are randomly matched. You do not know this other person\nand you will not knowingly meet him/her.\nYou have been assigned the role of the \"sender.\" The other person is in\nthe role of the \"receiver.\" You and the receiver are both endowed with\n10 dollars. You first decide how much of your 10-dollar endowment to\ntransfer to the receiver. You can choose any amount between 0 dollars\nand 10 dollars. The amount you transfer is tripled before being received\nby the receiver. The amount you keep is not tripled and simply remains\nin your possession.\nThe receiver then decides how much of the tripled transfer to return to\nyou. The receiver can choose any amount between 0 dollars and this\ntripled amount to return to you. The amount the receiver returns to you is\nnot tripled.\nYour final payment is the sum of the amount of your 10-dollar endowment\nyou keep + the amount the receiver returns to you.\nThe receiver's final payment is the sum of their 10-dollar endowment +\nthe amount the receiver keeps from your tripled transfer.\nAction + Reasoning\nDirect Prompting / Zero-shot Chain of Thought / Self-Consistency\nObservation\n[Round Prompt] You will engage in xx rounds of this kind of exchange\nwith the same receiver, including this round.\n[Info Player Prompt] In each future round, you will be assigned the\nrole of \"sender\" and the same other person will be assigned the role of\n\"receiver.\"\n[Info Prev Prompt] you sent an average of yy to the receiver and the\nreceiver sent back an average of zz. You CANNOT send more than your\nendowment of 10 dollars.\n[Infer Other Prompt] Think about what amount the receiver could possi-\nbly send in future rounds.\n[Obfuscating language] There are almost xx rounds left.\n[Obfuscating language (Game termination possibility)] There is a 10\npercentage of chance of this game being terminated."}]}