{"title": "A New Perspective on Time Series Anomaly Detection: Faster Patch-based Broad Learning System", "authors": ["Pengyu Li", "Zhijie Zhong", "Tong Zhang", "Zhiwen Yu", "C.L.Philip Chen", "Kaixiang Yang"], "abstract": "Time series anomaly detection (TSAD) has been a research hotspot in both academia and industry in recent years. Deep learning methods have become the mainstream research direction due to their excellent performance. However, new viewpoints have emerged in recent TSAD research. Deep learning is not required for TSAD due to limitations such as slow deep learning speed. The Broad Learning System (BLS) is a shallow network framework that benefits from its ease of optimization and speed. It has been shown to outperform machine learning approaches while remaining competitive with deep learning. Based on the current situation of TSAD, we propose the Contrastive Patch-based Broad Learning System (CPatchBLS). This is a new exploration of patching technique and BLS, providing a new perspective for TSAD. We construct Dual-PatchBLS as a base through patching and Simple Kernel Perturbation (SKP) and utilize contrastive learning to capture the differences between normal and abnormal data under different representations. To compensate for the temporal semantic loss caused by various patching, we propose CPatchBLS with model-level integration, which takes advantage of BLS's fast feature to build model-level integration and improve model detection. Using five real-world series anomaly detection datasets, we confirmed the method's efficacy, outperforming previous deep learning and machine learning methods while retaining a high level of computing efficiency.", "sections": [{"title": "I. INTRODUCTION", "content": "Time series anomaly detection (TSAD) is a key task in time series analysis, playing a crucial role in various fields such as finance, personal health, and industry. [1] The economic value inherent in various fields has led to the widespread development of TSAD in recent years. Researchers initially explored the use of traditional machine learning methods that are intuitive and convenient, such as LOF and Isolation Forest. Subsequently, deep learning models are delved due to strong performance and greater generalization capabilities, such as TCN and Transformer [2, 3, 4, 5, 6]. More recently, the deep learning route based on MLP [7, 8] has also gained attention. More effective and powerful deep learning detection models are explored and developed. However, is the deep learning architecture a necessity in all practical application scenarios for implementing TSAD? Are there other perspectives to implementing TSAD?\nIn the realm of time series prediction, Zeng et al. [9] demonstrated that the multi-head attention mechanism in Transformers does not adeptly adapt to temporal data. Conversely, they proposed a simple model, DLinear, that exhibits greater robustness and is less susceptible to noise interference. Currently, there is still skepticism within the community regarding whether the accomplishments of Transformer architectures in NLP can be readily extended to diverse time series tasks. Moreover, Sarfraz et al. [10] have raised concerns regarding deep learning, emphasizing that while deep learning has shown promising performance in the domain of TSAD, existing studies have not provided sufficient evidence regarding the indispensability of deep learning. Therefore, the necessity of deep learning also requires further investigation.\nCurrently, the TSAD community faces a dilemma where traditional machine learning methods struggle to capture temporal information and anomaly semantics, while deep learning methods are slow in speed, and their necessity is being questioned. This is particularly critical in industrial scenarios, where there is a pressing need for swift algorithms. Is there a more balanced option that preserves the robustness and speed advantages of machine learning while also demonstrating the powerful representational capacity akin to deep learning?\nTo address the aforementioned issue, we grounded in the Broad Learning System (BLS) and proposed a potential alternative solution called Contrastive Patch-based Broad Learning System (CPatchBLS), which is designed for unsupervised TSAD. CPatchBLS is a BLS architecture that deviates from deep learning frameworks by updating parameters via pseudo-inverse computation, eliminating away with the need for backpropagation. CPatchBLS achieves accurate time series anomaly detection with accelerated training and testing speeds while also offering a simpler model structure.\nIn specific terms, the contributions of this paper can be delineated into the following three points:\n1) To model temporal data, we introduce the concept of patching into BLS for the first time, proposing PatchBLS to enhance the capability of BLS in extracting temporal semantic information.\n2) The dissimilarity is utilized between normal and anomalous views for TSAD, and Simple Kernel Perturbation (SKP)-PatchBLS is incorporated to construct Dual-PatchBLS for anomaly identification.\n3) Harnessing the rapid advantages of BLS and integrating multi-scalele features for robust representatio, CPatch-BLS maintains the benefits of both machine learning and deep learning, surpassing previous deep learning methods across multiple datasets."}, {"title": "II. RELATED WORK", "content": ""}, {"title": "A. Traditional TSAD", "content": "Traditional TSAD methods are improved from classical machine learning methods by extending them to time series scenarios. Autoregressive Integrated Moving Average (ARIMA) is a forecasting method using historical data to predict future patterns, with the assumption that higher prediction errors indicate anomalies. One-Class Support Vector Machine (OCSVM) [11] employs a classification-based TSAD approach, utilizing support vector machine principles. Isolation Forest (IForest) [12] constructs tree structures and computes the average path length across all trees for each sample, serving the path length as the anomaly score to complete anomaly detection. Local Outlier Factor (LOF) [13] assesses the ratio of local and neighboring densities, utilizing them for detecting anomalies. K-Nearest Neighbors (KNN) [14] assesses the degree of outlierness by measuring the proximity to the k-th nearest neighbor.\nTraditional classical methods like prediction, classification, tree partitioning, density, etc., can effectively detect anomalies in time series. Nevertheless, as industrial application automation rises and control systems become more complex, traditional TSAD methods encounter challenges. The scarcity of fault patterns results in label shortages, and the increasing complexity of time series data leads to the curse of dimensionality, causing performance degradation of classical algorithms [15]."}, {"title": "B. Broad Learning System TSAD", "content": "The Broad Learning System (BLS) is an advanced model founded on Random Vector Functional Link Neural Networks (RVFLNN) [16]. BLS employs randomly initialized feature nodes and enhancement nodes for feature extraction. It learns the data distribution by solely updating the parameters of the output layer through pseudo-inverse calculation. Unlike traditional deep learning, BLS addresses issues such as parameter tuning difficulties, slow training speeds, and the complexity of finding the optimal solution space.\nAdaMemBLS [1] combines the fast inference capabilities of BLS with memory prototypes to effectively distinguish between normal and anomalous data. AdaMemBLS enhances the ability to learn time series features through various data augmentation techniques and incremental learning algorithms. Additionally, it employs diverse ensemble methods and discriminative anomaly scores to improve detection performance. And a theoretical analysis of the algorithm's time complexity and convergence demonstrates the feasibility of AdaMemBLS.\nDespite the introduction of AdaMemBLS, its efficacy is constrained by the incapacity of the proposed temporal slices to adequately model temporal data. Our research suggests that the untapped potential of BLS in TSAD tasks warrants further exploration. In our study, we introduce PatchBLS and Contrastive BLS as extensions to the foundational BLS framework, aiming to augment the performance of BLS in TSAD and facilitate rapid anomaly detection within the BLS paradigm."}, {"title": "C. Deep Learning TSAD", "content": "1) Patch Technique: PatchTST [17] utilizes a patch-based approach to model temporal sequences, emphasizing sequence-based modeling for time series data. It introduces patching and channel-independence mechanisms to capture sequential information, resulting in reduced computational complexity and improved performance over previous Transformer-based models. PatchTST demonstrates strong capabilities in multi-variable time series forecasting, with the effectiveness of the patching mechanism validated through ablation studies.\nIn time series forecasting, PatchMixer [8] introduces a novel convolutional structure that replaces the computationally intensive self-attention in Transformer architectures with a single-scale depthwise separable convolutional block. This design optimizes patch representation and effectively captures temporal semantic information within and across patches. As a result, PatchMixer enhances the extraction of comprehensive semantic features, showcasing superior performance. This shows the potential of the patch technique for integrating with base models. It also demonstrates its adaptability to various network structures in time series analysis tasks.\nIn time series representation learning, PITS [18] focuses on patch independence by ignoring inter-patch correlations. Instead of predicting masked patches based on unmasked data, it reconstructs unmasked patches independently. Utilizing contrastive learning, PITS generates complementary augmented views of original samples. By integrating reconstruction with hierarchical contrastive loss, it establishes an effective self-supervised learning framework, achieving strong performance across various time series tasks, including forecasting and classification."}, {"title": "III. PROPOSED APPROACH", "content": ""}, {"title": "A. Problem Statement", "content": "In the TSAD task, a time series $X = [x_1,x_2,..., x_n]$ is given $X \\in \\mathbb{R}^{C\\times n}$, with C denoting the number of channels and n denoting the time series length. The model with parameters $\\Theta$ is trained on $X_{train}$ and anticipated to represent normal patterns. The model on the testing dataset $X_{test}$ predicts the associated labels $y = [y_1, y_2,..., y_n]$, where $y \\in \\mathbb{R}^n$. An abnormality at a time point i is indicated by a value of $y_i = 1$."}, {"title": "B. Framework of CPatchBLS", "content": "The fundamental concept behind our approach is utilizing a more streamlined BLS architecture with accelerated training and inference speeds to address the limitations of deep structures in the TSAD. The approach aims to overcome the time-consuming nature and intricate model structure associated with deep learning approaches. Through the integration of time series patching and contrastive learning, the proposed CPatchBLS significantly bolstered its representational capacity for better TSAD performance.\nInitially, CPatchBLS incorporates concepts from cutting-edge research and introduces the Patching module into BLS tailored for TSAD, thereby enhancing the representation capabilities of BLS for time series data. Subsequently, following the processing of the original time series data by the patching module, it undergoes a dual-branch contrastive learning framework comprising SKP-PatchBLS and Basic-PatchBLS. This framework, by examining different feature representations from diverse perspectives, facilitates the efficient detection of anomalies. Finally, by capitalizing on the time-efficient attributes of BLS and leveraging Multi-Scale Patches, CPatch-BLS harnesses semantic information from multi-scale time series, thereby boosting the overall resilience and performance of the model. A detailed exposition of CPatchBLS is provided sequentially in this section."}, {"title": "C. PatchBLS", "content": "The temporal patterns inherent in time series data are often highly intricate, making it difficult for simple model architectures to effectively capture and handle these complexities. This challenge has long posed limitations on traditional classical methods, resulting in performance bottlenecks. To overcome this challenge and make BLS perceive temporal information, we integrate the patching operation into the BLS framework, resulting in the development of a new paradigm called PatchBLS. In preprocessing the original time series data for model input, PatchBLS undergoes segmentation into equidistant and non-overlapping time series segments, with each segment referred to as a patch, as shown in Fig. 3. By treating these time series patches as separate entities, the temporal representation capacity is improved due to the fact that local semantic information is preserved inside the time series. As the basis for PatchBLS to learn sequential semantic information, patches convey information that is not merely a simple individual timestamp. Intuitively, PatchBLS is more advantageous than simply examining timestamps alone because it can identify trends and other varied temporal patterns by concentrating on these patches.\nThe multivariate time series data $X \\in \\mathbb{R}^{C\\times n}$ first be divided into C univariate time series $x' \\in \\mathbb{R}^{1\\times n}$ in order to comply with the channel independence as described in PatchTST [17]. Subsequently, uniform patching is applied to each individual time series sequence, reshaping $x'$ to $x \\in \\mathbb{R}^{N_{patch} \\times S_{patch}}$, where $N_{patch}$ signifies the number of patches and $S_{patch}$ signifies the length of patch.\nThe subsequent PatchBLS performs the time series patches through PatchBLS's encoder, which is composed of arrays of feature nodes and enhancement nodes. Let $Z_i$ denote the hidden state of i-th feature node, which is represented by the equation:\n$Z_i = \\Phi (xW_i + \\beta_i), \\forall i \\in \\{1,2,......,m\\}.$       (1)\nwhere $\\Phi (\\cdot)$ represents the activation function, such as ReLU or Sigmoid, x represents the input data, $W_i \\in \\mathbb{R}^{S_{patch} \\times D_{ft}}$ represents the weight matrix, and $\\beta_i \\in \\mathbb{R}^{S_{patch} \\times D_{ft}}$ represents the bias matrix, $D_{ft}$ signifying the dimension of feature node. The aforementioned weight and bias matrices are all randomly generated and then adapted to the data using the sparse autoencoder method (SAE) [19] to optimize these weights. The weight matrices are then forced to be orthogonal by the applied Schmid orthogonalization, which lessens the redundancy of various feature node groups.\nIn order to improve the representation and mining capabilities for time series data, PatchBLS incorporates cascade operations in feature nodes. The feature nodes are concatenated sequentially, with the output of the previous layer becoming the input of the next layer. This process continues through k cascade feature layers to delve deeper into the semantic aspects of temporal data. The k-th cascade feature layer is represented as:\n$H_i = \\begin{cases} \\Phi (xW_i + \\beta_i), & \\forall i \\in \\{1,2,..., m\\}, k = 1, \\\\ \\Phi (H_{i}^{(k-1)}W_i + \\beta_i), & \\forall i \\in \\{1,2,..., m\\}, \\forall k \\in \\{2,..., q\\}. \\end{cases}$  (2)\nAdditionally, empirical validation suggests that the hidden states of m feature node groups and the k cascade layers of feature nodes are commonly concatenated to obtain a more complex representation:\n$Z = [Z_1^1,..., Z_1^k, ..., Z_m^1,..., Z_m^k].$ (3)\nThe enhancement nodes are derived from the feature nodes by undergoing the additional linear mappings. The following is the equation when n groups of enhancement nodes are created:\n$H_j = \\xi (ZW_j + \\beta_j), \\forall j \\in \\{1,2,..., n\\}.$ (4)\nThe meanings of the different parameters in the equation are akin to those in Equation (1), where $\\xi(\\cdot)$ also represents the activation function, $Z \\in \\mathbb{R}^{N_{patch} \\times (D_{ft} \\times C_{ft} \\times G_{ft})}$ represents the input feature layer data, $W_j \\in \\mathbb{R}^{(D_{ft}\\times C_{ft}\\times G_{ft}) \\times D_{enh}}$ represents the weight matrix, and $\\beta_j \\in \\mathbb{R}^{(D_{ft}\\times C_{ft}\\times G_{ft}) \\times D_{enh}}$ represents the bias matrix, with $G_{ft}$ signifying the group numbers of the feature layers, $C_{ft}$ signifying the numbers of the cascade feature layers, and $D_{enh}$ signifying the dimension of the enhancement layer.\nSimilarly, we also introduce cascading within the enhancement layer. The v-th enhancement layer is represented as:\n$H_j = \\begin{cases} \\xi (ZW_j + \\beta_j), & \\forall j \\in \\{1,2,..., n\\}, v = 1, \\\\ \\xi (H_{j}^{(v-1)}W_j + \\beta_j), & \\forall j \\in \\{1,2,..., n\\}, \\forall v \\in \\{2, ...,p\\}. \\end{cases}$ (5)\nThere are a total of n enhancement node groups and v cascade enhancement layers. Similarly, these enhancement nodes are concatenated, obtaining $H_n \\in \\mathbb{R}^{N_{patch} \\times(D_{enh} \\times C_{enh} \\times G_{enh})}$, where $C_{enh}$ signifying the numbers of the cascade enhancement layer, $G_{enh}$ signifying the groups of the enhancement layer, and the equation is as follows:\n$H = [H_1^1,..., H_1^v, ..., H_n^1,..., H_n^v].$ (6)\nThe next step in PatchBLS is to concatenate all the obtained feature nodes and the enhancement nodes to form the latent features $A \\in \\mathbb{R}^{N_{patch}\\times(T_{ft}+T_{enh})}$, where $T_{ft} = D_{ft} \\times C_{ft} \\times G_{ft}$ signifying the dimension of the feature node groups, $T_{enh} = D_{enh} \\times C_{enh} \\times G_{enh}$ signifying the dimension of the enhancement node groups, which can be expressed by the following:\n$A = [Z, H].$ (7)\nDeep neural networks rely on backpropagation to iteratively update weights, and they are sensitive to learning rate when network depth increases. PatchBLS, however, uses the pseudo-inverse to quickly compute hidden-to-output weight, ensuring efficient training without issues like the local optima problem [20]. At this point, the output of this problem can be modeled as:\n$Y = AW_o.$ (8)\nwhere $W_o$ represents the weight matrix of the output block. Since the generation process of the PatchBLS encoder's parameters and the latent feature A are fixed and known and $Y \\in \\mathbb{R}^{N_{patch} \\times S_{patch}}$ are also known as the final target of output, the only parameters that need to be updated in PatchBLS are the weight matrix $W_o \\in \\mathbb{R}^{(T_{ft}+T_{enh}) \\times S_{patch}}$. Therefore, the objective function of PatchBLS can be expressed as follows:\n$L = ||Y - \\hat{Y} ||_2^2 + \\lambda||W_o||_2.$ (9)\nwhere Y represents the actual values, $\\hat{Y}$ represents the reconstructed values, the weight matrix that requires training is $W_o$, and $\\lambda$ is the regularization coefficient preventing overfitting. The pseudo-inverse approach is used to update $W_o$ in the manner described below in order to minimize L:\n$W_o = (A^T A + \\lambda I)^{-1}A^TY.$ (10)\nin which I is the identity matrix, and the meanings of other parameters are the same in Equations (8) and (9).\nIn order to identify anomalies, PatchBLS uses the anomaly score $Score = ||Y - \\hat{Y} ||_2$ during inference. The model generates anomaly predictions for a specific detection threshold, denoted as $\\delta$. This occurs when the score at the time point i exceeds $\\delta$.\nAs shown in Fig. 3, the encoding layer with feature nodes and enhancement nodes in the PatchBLS is utilized as the PatchBLS encoder, while the corresponding output layer is employed as the PatchBLS decoder to generate the representation of time series data through a reconstruction-based approach. However, the reconstruction-based method's limited sensitivity to normal patterns hampers its ability to differentiate between normal and anomalous data [21]. Inspired by [4], we propose Dual-PatchBLS to address these difficulties. We utilize contrastive learning with Dual-PatchBLS to learn a more discriminative perspective, which successfully distinguishes between normal and anomalous data and contributes to better TSAD."}, {"title": "D. Dual-PatchBLS", "content": "Although PatchBLS demonstrates the ability to construct consistent representations of normal patterns, to thoroughly capture temporal features, it is essential to introduce additional view representations and the differences in anomalous data under a contrastive view. Therefore, we propose a dual-branch contrastive framework, Dual-PatchBLS.\nTo construct the Dual-PatchBLS, we incorporate random feature approximation transformations within the feature layer of the PatchBLS framework, thereby constructing an abnormal perspective, referred to as the Simple Kernel Perturbation (SKP)-PatchBLS branch. Through an approximation of the Gaussian kernel, more uncertainty is injected into the original PatchBLS while also enhancing the ability of this branch to process nonlinear time series data, capturing more complex temporal data patterns and differing significantly from the original representation. The specific equation of SKP is as follows:\n$SKP(Z) = \\frac{1}{\\sqrt{d_k}} [\\sqrt{2}cos(\\omega_1 Z + b_1), ..., \\sqrt{2}cos(\\omega_{d_k} Z+b_{d_k})].$ (11)\nwhere $d_k$ denotes the dimension of the random feature space, the variable Z represents time series data following processing by the feature layer, $\\omega_i \\sim N(0, \\sigma^2)$ represents the Gaussian random vector, and $b_i \\sim U[0, 2\\pi]$ represents the random variable.\nThe preliminary time series anomaly detection result is obtained by comparing the representation results of the above two types of BLS branches with different perspectives and calculating the difference score between $PatchBLS_{SKP}(\\cdot)$ and $PatchBLS_{Basic}(\\cdot)$ as follows:\n$Score_{diff} = D(PatchBLS_{SKP}(X), PatchBLS_{Basic}(X)).$ (12)\nWhere $Score_{diff}$ represents the difference score between two branches, X represents the identical time series data, and D() represents the function calculating the feature difference utilizing KL divergence to measure the distinction in our implementation:\n$Score_{diff} = \\frac{1}{2}KL(PatchBLS_{SKP}(X), PatchBLS_{Basic}(X)) + \\frac{1}{2}KL(PatchBLS_{Basic}(X), PatchBLS_{SKP}(X)).$ (13)"}, {"title": "E. Multi Scale Patches", "content": "The performance of CPatchBLS may vary with different patch size settings when utilizing fixed-length patches for extracting local semantic information from time series data, potentially resulting in reduced model robustness. This assertion is substantiated through subsequent ablation experiments in this study. Moreover, within the TSAD scenario, the manifestation patterns of time series anomalies exhibit notable distinctions, encompassing both point-wise anomalies and more intricate pattern-wise anomalies. Failure to extract and comprehend time series patches' information across multiple scales leads to incomplete exploration of the time series data. This phenomenon is investigated in the experiment.\nGiven the aforementioned issues, a model-level Multi Scale Patches (MSP) integration method is proposed. This approach leverages temporal information more effectively while preserving the computational efficiency and fast processing capabilities of PatchBLS. By incorporating temporal features from multi-scale patches, the proposed method showcases resilience against the fragility and instability found in individual models. It also covers a broader range of temporal pattern features, effectively tackling significant challenges in time series analysis without adding substantial overhead to the model's inherent strengths. The corresponding equation is:\n$Score_{Anom} = Avg (Score_{diff_1},..., Score_{diff_i}), \\forall i \\in \\{1, 2, ..., n\\}.$ (14)\nHere $Score_{Anom}$ represents the anomaly score of time series and Avg(\u00b7) represents the aggregation method for the difference scores from models with different n multi-scale patch sizes. The variation in i signifies a specific index corresponding to different patch sizes, resulting in different Dual-PatchBLS to achieve varying levels of $Score_{diff}$."}, {"title": "Algorithm 2 CPatchBLS's Pytorch-like pseudo-code.", "content": "INPUT: The time series data $X \\in \\mathbb{R}^{C\\times n}$; the list of patches $L_{patches}$ and the various parameters related to Basic-PatchBLS and SKP-PatchBLS in Algorithm (1);\nOUTPUT: The anomaly score $Score_{Anom}$ obtained from CPatchBLS;\nfor Patch in $L_{patches}$ do\nObtain $PatchBLS_{Basic} (X)$ and $PatchBLS_{SKP}(X)$ using Algorithm (1);\nObtain the difference score $Score_{diff}$ by (13);\nObtain the anomaly score $Score_{Anom}$ by (14);\nreturn $Score_{Anom}$;"}, {"title": "IV. EXPERIMENTAL SETTING & RESULTS", "content": ""}, {"title": "A. Datasets", "content": "Our models and other baselines are evaluated utilizing five widely used real-world datasets: (1) MSL (Mars Science Laboratory dataset) [22], (2) SMAP (Soil Moisture Active Passive dataset) [23], (3) SWaT (Secure Water Treatment testbed) [24], (4) WADI (WAter DIstribution testbed) [25], (5) PSM (Pooled Server Metrics) [26].\nThe characteristics of these datasets are presented in Table I. Entities indicate that the dataset comprises multiple subsets. Continuing from the previous setup [4], we combine them for model training. Train and Test indicate the number of data time points in the final training and test sets."}, {"title": "Algorithm 1 Basic-PatchBLS's and SKP-PatchBLS's Pytorch-like pseudo-codes.", "content": "INPUT: The time series data $X \\in \\mathbb{R}^{C\\times n}$; the identical parameters of Basic-PatchBLS and SKP-PatchBLS as follows: the length of the patch $S_{patch}$, the $D_{ft}$, $C_{ft}$, $G_{ft}$, $D_{enh}$, $C_{enh}$, $G_{enh}$, the shrink coefficient s of the enhancement layer, and the output regularization r of the output layer; the kernel dimension $d_k$ and the sigma $\\sigma$ of SKP-PatchBLS;\nOUTPUT: The reconstructed representation O obtained from Basic-PatchBLS or SKP-PatchBLS;\nfor $x' \\in \\mathbb{R}^{1\\times n}$ in $X \\in \\mathbb{R}^{C\\times n}$ do\nReshape $x' \\in \\mathbb{R}^{1\\times n}$ into $x' \\in \\mathbb{R}^{N_{patch} \\times S_{patch}}$;\nRandomly generate the parameters of $G_{ft}$ groups and $C_{ft}$ cascade layers of feature nodes;\nfor i = 1 to m and k = 1 to q do\nCalculate the output of the feature nodes using (2);\nif it's the SKP-PatchBLS branch\nProcess the output of the feature nodes using (11);\nend if\nend for\nConnect the feature nodes as (3);\nRandomly generate the parameters of $G_{enh}$ groups and $C_{enh}$ cascade layers of enhancement nodes;\nfor j = 1 to n and v = 1 to p do\nCalculate the output of the enhancement nodes using (5);\nend for\nConnect the enhancement nodes as (6);\nConnect the feature nodes and the enhancement nodes to obtain A, as in (7);\nFeed A to the decoder and calculate the weight W by (10);\nObtain the reconstructed representation O by (8);\nreturn 0;"}, {"title": "B. Metrics", "content": "In our experiments, we utilize AUC-ROC, AUC-PR, and PA-F1 for a comprehensive evaluation, aligning with recent state-of-the-art works like [27], [28], [29], and [30]. AUC-ROC and AUC-PR assess the performance of an anomaly detection model across all thresholds. The ROC curve illustrates a binary classifier's diagnostic ability by plotting TPR against FPR. In contrast, the precision-recall curve is beneficial for imbalanced datasets, graphing precision against recall at different thresholds. PA-F1 is suitable for point anomalies and shows robust performance, even with imprecise labels. These multifaceted metrics ensure the accuracy and robustness of the evaluation."}, {"title": "C. Baselines", "content": "In our experiments, we introduced twelve classic and advanced TSAD methods and compared our proposed CPatch-BLS in terms of anomaly detection capability, training and testing efficiency, and robustness. From the perspective of the core motivation, we compared classical machine learning methods\u00b9 such as LOF [13], IForest [12], and ECOD [31], and the machine learning enhanced method Deep SVDD [32], reconstruction-based methods like USAD [33] and FITS [34], Transformer-based methods such as Anomaly Transformer (AnomTrans) 2 [2] and TranAD [28], 2D-variation model like TimesNet [35], LLM-based method like OFA [36], contrastive learning-based models like COUTA [37], and prediction-based method such as LSTMAD [38]. The implementations of Deep SVDD, TranAD, TimesNet and COUTA are publicly available in DeepOD Library\u00b3, and we use the implementations of USAD, FITS, OFA and LSTMAD in TSB-AD repository4 [39]."}, {"title": "D. Performance Comparisons", "content": "Table II presents a performance comparison between the CPatchBLS and multiple baselines using five real-world datasets. CPatchBLS performs superior to other models in most evaluation metrics. Specifically, compared to the best baseline models on each dataset, CPatchBLS exhibits absolute improvements of 0.77% in ROC-AUC (99.05% to 99.81%), 5.08% in ROC-PR (93.50% to 98.25%), and 3.00% in F1 score (94.05% to 96.87%)."}, {"title": "E. Time Cost Studies", "content": "To validate the high efficiency of CPatchBLS, we roughly conducted a comprehensive multi-trial experiment to compare the training and testing time of twelve algorithms with our Ours-Parallel Execution (PE) and Ours-Sequatial Execution (SE) models across five real-world datasets. Ours-PE involves training and testing the Dual-PatchBLS sub-models independently and in parallel within CPatchBLS. The time taken by Ours-PE is measured based on the training and testing duration of the longest Dual-PatchBLS sub-model. Similarly, Ours-SE involves sequentially training and testing the Dual-PatchBLS sub-models within CPatchBLS. Each sub-model is trained and tested one after the other, continuing until all sub-models have been processed.\nThe average training and testing times of Ours-PE and Ours-SE outperform all deep learning methods in the comparative experiments, strongly confirming the superiority of BLS architecture in shorter time consumption compared to deep learning architectures in the TSAD domain. The average training time (33.6s) and testing time (2.2s) of Ours-SE surpasses the average training and testing time of LOF (55.0s/44.1s). The theoretically optimal model Ours-PE demonstrates even better performance, with average training time (12.6s) and testing time (1.1s) approaching the performance time of ECOD (10.4s/9.4s). Overall time consumption is only lower than IForest (1.7s/0.6s), showing that our proposed method has the potential to match or even surpass some traditional machine learning methods in time consumption."}, {"title": "F. Complexity Analysises", "content": "To further demonstrate the time efficiency of our models", "CPatchBLS": "nThe primary computation of PatchBLS is the generation of feature nodes and enhancement nodes", "as": "n$T_{FE"}, "T_F + T_E $\n$= O(N_{patch} \\cdot S_{patch} \\cdot n_1 \\cdot d_1) + O(N_{patch} \\cdot n_1 \\cdot d_1 \\cdot n_2 \\cdot d_2) $\n$= O(N_{patch} \\cdot max\\{S_{patch}, n_2 \\cdot d_2\\} \\cdot n_1 \\cdot d_1).$       (15)\nAccording to 10, the complexity of the pseudo-inverse calculation is:\n$T_W = O(max\\{N_{patch}, 2d\\} \\cdot min\\{N_{patch}, 2d\\}^2)$\n$= O(N_{patch} \\cdot (2d)^2)$\n$= O(N_{patch} \\cdot d^2).$         (16)\nwhere $d = n_1d_1+n_2 \\cdot d_2$. Collectively, considering the overall complexity of PatchBLS performing C channels is:\n$T_{PatchBLS} = C (T_{FE} + T_W)$\n$= O(C \\cdot N_{patch} \\cdot max\\{S_{patch}, n_2 \\cdot d_2\\} \\cdot n_1 \\cdot d_1) + O(C \\cdot N_{patch} \\cdot d^2).$ (17)\nThe introduction of the random feature approximation operation for minor data perturbation in the SKP-PatchBLS structure, which maps the data post-feature layer to a nonlinear space of $d_k$ dimensions, implies that, based on the available information, its time complexity can be determined as:\n$T_{SKP-PatchBLS} = O(C \\cdot N_{patch} \\cdot max\\{S_{patch}, n_2 \\cdot d_2\\} \\cdot n_1 \\cdot d_1 \\cdot d_k) + O(C \\cdot N_{patch} \\cdot (n_1 \\cdot d_1 \\cdot d_k + n_2 \\cdot d_2)^2).$ (18)\nWhen $d^2$ is sufficiently large, $T_{PatchBLS}$ can be considered as $O(C \\cdot N_{patch} \\cdot d^2)$. Similarly, $T_{SKP-PatchBLS}$ can be approximated as $O(C \\cdot N_{patch} \\cdot (n_1 \\cdot d_1 \\cdot d_k + n_2 \\cdot d_2)^2)$.\nDual-PatchBLS operates by running Basic-PatchBLS and SKP-PatchBLS independently and then combining their results to determine difference scores. Therefore, its time cost is associated with the execution of these two branches as follows:\n$T_{Dual-PatchBLS} = T_{PatchBLS} + T_{SKP-PatchBLS}$\n$= O(C \\cdot N_{patch} \\cdot d^2)$\n$+ O(C \\cdot N_{patch} \\cdot (n_1 \\cdot d_1 \\cdot d_k + n_2 \\cdot d_2)^2)$\n$= O(C \\cdot N_{patch} \\"]}