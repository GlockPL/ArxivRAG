{"title": "Unsupervised Translation of Emergent Communication", "authors": ["Ido Levy", "Orr Paradise", "Boaz Carmeli", "Ron Meir", "Shafi Goldwasser", "Yonatan Belinkov"], "abstract": "Emergent Communication (EC) provides a unique window into the language systems that emerge autonomously when agents are trained to jointly achieve shared goals. However, it is difficult to interpret EC and evaluate its relationship with natural languages (NL). This study employs unsupervised neural machine translation (UNMT) techniques to decipher ECs formed during referential games with varying task complexities, influenced by the semantic diversity of the environment. Our findings demonstrate UNMT's potential to translate EC, illustrating that task complexity characterized by semantic diversity enhances EC translatability, while higher task complexity with constrained semantic variability exhibits pragmatic EC, which, although challenging to interpret, remains suitable for translation. This research marks the first attempt, to our knowledge, to translate EC without the aid of parallel data.", "sections": [{"title": "Introduction", "content": "Emergent communication (EC) describes the phenomenon in which Al agents develop communication protocols to achieve shared goals (Giles and Jim 2003; Kasai, Tenmoto, and Kamiya 2008; Boldt and Mortensen 2024; Brandizzi 2023). This capacity has garnered attention due to its significant potential for understanding the complexities of language formation and evolution within multi-agent systems. Yet, ECs remain largely opaque, difficult to interpret and translate into human-readable forms.\nAlthough efforts to understand EC have been made, interpretability remains elusive. Traditional approaches, such as topographic similarity (Brighton and Kirby 2006), measure the correlation between message and input distances, provide a coarse measure of EC structures, but do not capture the full nuances of EC compositionality. Recent advancements aim to use natural language (NL) to analyze EC (Xu, Niethammer, and Raffel 2022; Carmeli, Belinkov, and Meir 2024; Chaabouni et al. 2020a). Another line of research aims to translate EC into NL using parallel data (EC-NL pairs) to gain a more intuitive understanding (Andreas, Dragan, and Klein 2017; Yao et al. 2022). However, this approach may introduce certain biases in the translation process.\nOur approach is to utilize unsupervised neural machine translation (UNMT) to translate the emergent languages developed during referential games of varying complexity. UNMT is particularly suited for this task as it does not require parallel data (Lample et al. 2018a; Artetxe, Labaka, and Agirre 2018a; Conneau et al. 2017; Lample et al. 2018b; Xu et al. 2018), which is typically unavailable for AI-generated languages. To facilitate translation, we generate an EC corpus from each game by compiling the messages exchanged between agents. Concurrently, we employ a separate English caption dataset as a linguistic prior, which provides a resource of image descriptions for images provided to the agents during the game (Naturally, the images provided to the agents contains no caption). By integrating these distinct datasets, we can adapt existing UNMT techniques (Chronopoulou, Stojanovski, and Fraser 2020) to translate the emergent communications into English, showcasing their translatability across varying task complexities.\nLeveraging the capabilities of UNMT, our study dives into the intricacies of structured referential games of varying complexity (Lewis 2008; Lazaridou, Peysakhovich, and Baroni 2016; Choi, Lazaridou, and De Freitas 2018; Guo et al. 2019), including Random, Inter-category, Supercategory, and Category image discrimination tasks. In these games, agents must identify a target image, such as a giraffe, from a set of distractors. Distractors are non-target images selected based on the game type, adding complexity to the task (Figure 1a). For example, in a Supercategory game, a distractor for a giraffe target can be cow, whereas in a Random game, the distractors could be as unrelated as a fire hydrant (Figure 2), for the same giraffe target. These games are deliberately designed to simulate diverse communication environments, aimed at constructing ECs that increasingly resemble human languages, and will eventually be useful for interpretability and usability of AI-generated languages in real-world scenarios. We hypothesize that, akin to the evolution of human languages that adapt in response to social and environmental pressures (Kuhl 2004; Kottur et al. 2017), the complexity inherent in these games will catalyze similar transformations in the EC.\nThrough our research, we have established a new benchmark for translating EC into NL. This benchmark leverages a comprehensive suite of metrics designed to capture the intricacy and diversity of AI-generated languages. These"}, {"title": "Related Work", "content": "Research in EC has developed various methodologies to understand and evaluate the structure and sophistication of languages that emerge among AI agents. Early efforts to quantify communication among agents focused on traditional metrics like topographic similarity, which assesses the correlation between the distances of messages and their corresponding inputs (Brighton and Kirby 2006). To understand EC, Lazaridou et al. (2018) explored the input space effect on EC by demonstrating that AI agents can develop EC that solves the shared task using both symbolic and pixel inputs, while Lee et al. (2018) highlighted how factors like model capacity and communicative bandwidth foster systematic compositional structures in EC. To understand NL properties in EC, several studies have explored aspects such as Zipf's Law of Abbreviation (Ueda and Washio 2021) and Harris's Articulation Scheme (Ueda, Ishii, and Miyao 2022). Additional work focused on compositionality, examining disentanglement-based measures (Chaabouni et al. 2020a), Adjusted Mutual Information (AMI) (Mu and Goodman 2021), and rigorous methods to verify compositional structures (Vani et al. 2021; Andreas 2019). More recent advances have introduced more nuanced metrics that aim to understand EC using NL. For instance, Xu, Niethammer, and Raffel (2022) evaluated learned EC on unseen test data to assess generalization, providing insights into NL aspects. Carmeli, Belinkov, and Meir (2024) proposed mapping EC symbols to NL concepts, assessing EC's compositionality. However, their approach forms a global mapping of atomic symbols, rather than full translation of individual messages. Most closely related to our approach is work translating EC into human-understandable language. Andreas, Dragan, and Klein (2017) translated continuous communication channels into NL by collecting agents' messages and corresponding NL strings within the same games. Yao et al. (2022) trained image discrimination referential game and translated the EC to NL captions grounded on the same images. While promising, these studies crucially used parallel EC-NL pairs for training the translation system. In contrast, our approach does not rely on any parallel data. Several works leverage EC as a tool to enhance NL translation in unsupervised settings (Downey et al. 2023; Lee et al. 2018). However, unlike our focus on directly translating EC, these studies utilized EC to facilitate future NL translation tasks. Guo et al. (2021) termed EC expressivity as the amount of input information encoded in EC, defining theoretically and demonstrating empirically the impact of unpredictability and contextual similarity on EC expressivity. In this paper, we extend the concept of contextual complexity by introducing a broader spectrum of complexity, including low, moderate, and high complexity levels. This allows us to systematically explore how varying degrees of similarity among distractors influence the translatability of EC."}, {"title": "Complexity of Referential Games", "content": "Complexity refers to the degree of similarity between distractors and the target in referential games. This property can range from being semantically related to completely unrelated. In this section, we define four levels of complexities that represent a wide range of game complexity levels."}, {"title": "Preliminaries", "content": "We focus on a two-agent setup, consisting of a Sender and a Receiver, operating under predefined rules within simulated environments. These agents interact through a discrete communication channel (Lazaridou and Baroni 2020; Denamgana\u00ef and Walker 2020) to successfully complete shared tasks by conveying information about objects or concepts represented in the environment.\nIn the framework of referential games, the Sender observes an image $i \\in D$ and sends a message $m\\in M$ to describe the image to the Receiver. The Receiver uses this message to identify the correct image from a set of candidates, where the target's position is randomized. This setup is defined by the tuple (A, M, D, C, S), where:\n\u2022 A = {AS, AR} are the Sender and Receiver agents.\n\u2022 M represents the message space.\n\u2022 D is the dataset used to sample images.\n\u2022 C is the set of categories in the dataset.\n\u2022 S encompasses the set of supercategories, grouping similar categories together."}, {"title": "Game Complexity Levels", "content": "We craft the environment for EC games around a target image $i_r$ classified under a category $c_r$ and a supercategory $S_T$. The distractor environment is composed of $d$ images chosen based on the following criteria:\n1. Random: In this simplest form of the game, agents operate in a completely random environment, where each distractor image $i$ is sampled uniformly from the dataset: $i \\sim D$. For example, the task could involve distinguishing between a car and a zebra. This basic level challenges the agents to develop fundamental communication protocols from scratch, testing their ability to generate and understand rudimentary language. In Lazaridou et al. (2018), it is described as a uniform game.\n2. Category Discrimination: The most contextually complex level involves agents discriminating between images containing objects of the same category, such as distinguishing between different images of giraffes. Here each distractor $i$ is sampled from the set of images that share a category with that of the target images: $i \\sim D_{CT}$, where"}, {"title": "Methodology", "content": "The training regime of our methodology consists of two phases. First, we train the agents on image discrimination games (Figure 1a) of varying complexity (Section 3) and record the exchanged messages. Each game serves as a unique setup to provoke distinct communication strategies among agents, depending on task complexity. Our next phase is to use EC messages from each game as a monolingual EC corpus for training UNMT system (Figure 1b).\nUNMT Architecture For details on UNMT techniques foundational to our study, see Appendix B. Our work employed the UNMT system by Chronopoulou, Stojanovski, and Fraser (2020) that is implemented in three steps:\n1. Pre-training: Utilizing a high-resource English corpus to train our model, ensuring a rich linguistic foundation.\n2. Fine-tuning: Adapting the pre-trained model using EC data collected from AI agents, enhancing its ability to handle the specific linguistic features of EC. This phase creates a shared embedding space with the EC.\n3. Back-translation and Denoising: Employing back-translation and denoising techniques to refine the model's output and improve translation between EC and English."}, {"title": "Experimental Setup", "content": "For this study, we employed the MSCOCO dataset (Lin et al. 2014), a diverse collection of 117K complex images that are annotated with various NL concepts. Each image in the dataset is paired with five distinct captions, providing high-quality captions to inject prior knowledge of image descriptions while training our UNMT models, and to serve as valuable reference points for evaluating translation performance. More information about the dataset appears in Appendix A."}, {"title": "AI Agents Architecture", "content": "AI agents were configured with a hybrid architecture combining elements of LSTMs (Hochreiter and Schmidhuber 1997) and ResNets (Koonce and Koonce 2021), to process images and to generate ECs effectively, representing a typical architectural standard in the EC domain. Each agent was initialized with different seeds to ensure the robustness and generalizability of the results. The Sender and the Receiver are sharing the ResNet weights, to ensure that both agents process visual information consistently, supporting coherent communication across the system by minimizing discrepancies in visual perception. The ResNet was initialized with pre-trained weights from ImageNet (Deng et al. 2009). The joint training objective is infoNCE (van den Oord, Li, and Vinyals 2018) which is often used in discrimination setups."}, {"title": "Implementation Details", "content": "We used the EGG framework (Kharitonov et al. 2019) to train the models with a batch size of 1024 and an initial learning rate of 0.001, using the Adam optimizer. Training epochs were set to 50, with early stopping based on validation loss. In each game, nine distractors are sampled based on the complexity policy. The same target images were used for each complexity's test, but a new set of distractors was sampled according to the complexity of the game. The agents' communication channel is quantized (Carmeli, Meir, and Belinkov 2023), consisting of 64 symbols, represented as binary vectors of length 6, with each message composed of 6 symbols and EOS symbol. To ensure robustness, each referential game was run with 5 different random seeds.\nThe UNMT model utilized a pre-trained XLM, which was fine-tuned on both the EC corpus and MSCOCO captions. More details on the EN corpus are provided in Appendix A. See Appendix G for full hyperparameters."}, {"title": "Evaluation Metrics", "content": "To comprehensively assess the effectiveness of EC games and the UNMT, we employed a diverse set of evaluation metrics. See Appendix E for more details on the metrics.\nEC Games Metrics We used the following metrics to evaluate the performance and sophistication of the EC emerged by AI agents during the referential games:\n\u2022 Game Accuracy: Measures the percentage of correct identifications made by the agents (Lewis 2008).\n\u2022 Vocabulary Usage (VU): Assesses vocabulary diversity by measuring the range of unique symbols utilized during game sessions (Graesser et al. 2003).\n\u2022 Message Entropy: Calculates the uncertainty in the agents' messages (Shannon 1948)."}, {"title": "Results", "content": "Emergent Communication Games The baseline performs marginally above random, which is suggestive of Buzaglo et al. (2024)'s findings on limited generalization in randomly sampled networks. As shown in Table 1, it significantly underperforms across all metrics, with a VU of 100%, indicating an arbitrary use of the EC vocabulary without solving the task. These results demonstrate the effectiveness of the EC strategies developed in the games.\nAs expected, in the ten-candidates (ACC 10) scenario, the game accuracy scores degrade with increasing task difficulty, with Category game complexity presenting significantly"}, {"title": "Unsupervised Machine Translation", "content": "Benchmark UNMT system's performance We conducted two experiments to understand the UNMT capabilities. The first experiment aimed to calibrate the performance of our UNMT approach on a real, high-resource language by translating Chinese captions to English. For this experiment, we used MSCOCO-CN (Li et al. 2019) composed of ~ 30K annotated images. The results, detailed in Appendix F, showed fluent English translations with an overall BLEU score of 5.65 points. The second experiment, serves as a baseline, evaluates the performance of the UNMT on EC generated by random agents that did not undergo the training phase designed to optimize EC. The results, presented in Table 2, indicate significantly lower performance metrics, with BLEU scores near zero. This confirmed that random agent communication did not form meaningful linguistic patterns, strengthening our translation reliability on EC that emerged from AI agents' collaboration towards a shared goal.\nUNMT of EC Our findings demonstrate the potential of UNMT for translating EC into English without the use of parallel data. This is reflected in all translation metrics: While modest compared to full-fledged supervised translation of natural languages, these metrics are significantly higher than the baseline, indicating the formation of"}, {"title": "Game complexity vs. Translatability", "content": "Several key insights emerge from our analysis. Translation performance metrics, including BLEU and ROUGE-L, are highest for Inter-category. This indicates that partial supervision and candidates from broad categories facilitate better translation, making ECs in these contexts more translatable. In contrast, Supercategory, with high contextual complexity, presents the highest novelty score (70.00%) while having the lowest BLEU (6.08) and ROUGE-L (0.352). This highlights the difficulty in translating ECs that were developed based on semantically similar image discrimination. As illustrated by Table 1, this complexity type is associated with the highest Entropy and VU, indicating that agents communicated diverse messages, which directly affected translation performance and encouraged more novel translations. Conversely, the Category complexity, designed for fine-grained distinctions, achieved moderate translation metrics with BLEU (7.41) and ROUGE-L (0.361), significantly surpassing Supercategory and Random, despite the lower scores among the EC metrics. We attribute this result to the high specificity leading to more simplistic communication strategies, characterized by a more limited and predictable set of symbols, making it easier to translate."}, {"title": "Compositionality and Translatability", "content": "Figure 5 illustrates the relationships between various EC compositionality measures and translation quality. Notably, AMI shows positive correlations with BLEU and METEOR but negative associations with Novelty, suggesting that messages with a close conceptual mapping tend to be easier to translate. Meanwhile, translation metrics compare generated translations to ground-truth captions; thus, a more \u201cnovel\u201d expression naturally scores lower. In contrast, BosDis and PosDis focus on how strongly each symbol (or position) encodes a single attribute, offering a view of per-token \"disentanglement\". These metrics correlate negatively with standard translation scores but positively with Novelty. One possibility is that strictly mapping attributes to individual symbols results in messages with rich details and emphases, making translation more challenging, while simultaneously driving higher Novelty by introducing more unique expressions. Additionally, TopSim is positively correlated with METEOR, suggesting that images with similar representations produce messages closer in structure, leading to greater translation predictability. Overall, these findings suggest a connection between compositionality and translatability."}, {"title": "Conclusion", "content": "This research introduces a novel application of UNMT to translate AI agents' EC emerged in referential games into natural language. Our study demonstrates UNMT's potential to translate EC without parallel data and examines the impact of game complexity on EC translatability. Our findings indicate that different complexities produce distinct communication patterns. For instance, the Inter-Category game generated EC that achieved significantly higher translation accuracy than the Supercategory game, which often had lower scores despite its complexity. This suggests that not all complexities foster equally translatable communications.\nContrary to the initial hypothesis that more complex setups result in more detailed communications that are easier to translate, our empirical evidence shows otherwise. EC emerging from the Category game, characterized by high contextual complexity, resulted in more pragmatic communication with significantly lower VU and Entropy while exhibiting high game accuracy. In contrast, the Supercategory game, with slightly less contextual complexity, produced richer communication. We attribute this phenomenon to the agents' optimization for efficiency, where fine-grained distinctions encourage the use of low-level features. However, in terms of translatability, the Category game achieved higher translation scores than the Supercategory game, suggesting that a more pragmatic protocol facilitates translation.\nMoreover, our analysis of compositionality and translation quality leads to a key hypothesis: while EC messages that closely reflect underlying concepts are more straightforward to translate, greater symbol-level detail increases novelty but also makes translation more challenging."}, {"title": "Limitations and Future Work", "content": "This study provides valuable insights into translating EC using UNMT. However, several limitations should be acknowledged. First, the number of distractors adds a layer of complexity that was not fully explored. Second, sharing a pre-trained visual module may limit generality. Future work could explore separate or scratch-trained modules to assess their impact on EC translatability. Third, the communication channel, vocabulary size, and messages length were fixed based on preliminary experiments. While these configurations were chosen to optimize performance, they may not capture the full range of possible communication strategies. Before translating, we analyzed many such configurations, ultimately selecting the setups with the best results.\nBeyond limitations, the study suggests several future research avenues. EC is a rich field, characterized by a variety of techniques and strategies that form unique ECs. These range from multi-agent collaboration (Michel et al. 2023), to bidirectional communication (Nikolaus 2023), and reconstruction objectives (Chaabouni et al. 2021). Future research should enhance UNMT techniques to more effectively capture the subtleties of these ECs (Chauhan et al. 2022; Amani et al. 2024) and assess faithfulness by having the Receiver evaluate how well the back-translation (EC\u2192EN\u2192EC) captures the original message's nuanced details. Finally, success in unsupervised translation of EC can further motivate similar attempts to translate other communication systems, such as animal communication (Goldwasser et al. 2024)."}, {"title": "Chinese UNMT", "content": "Before translating EC, we calibrated our expectations by evaluating how UNMT performs on a clear, interpretable and more complex language than EC. For this experiment, we collected a corpus of 120K English image captions (Lin et al. 2014) and a 30K Chinese subset of it (Li et al. 2019) and trained the UNMT system with the same model and parameters used in other experiments. Table 2, in the Chinese column, presents the metrics for the Chinese-to-English UNMT. Figure 8 provides examples of translated captions. The Chinese UNMT results are summarized as follows:\n\u2022 The UNMT system's ability to capture contextual meaning of source text, despite not always producing verbatim translations.\n\u2022 Similar performance patterns between Chinese and EC translation strengthen our confidence in applying UNMT to emergent languages."}, {"title": "Hyperparameters", "content": "The experiment can be divided into two main components, each with its own set of hyperparameters.\nEC Game For each game type, referential games were trained with the following seeds: 31, 42, 123, 555, 999. Hyperparameters are reported in Section 5. Regarding communication channel, the LSTM for both agents is with hidden sizes of 20 and embeddings of 50.\nUNMT: During Phase 2, the model was fine-tuned using a transformer architecture with 6 layers and 8 heads, embedding dimension of 1024, and Adam optimizer with a learning rate of 0.0001. The dropout rates for both standard and attention mechanisms were maintained at 0.1. For Phase 3, training objectives were autoencoding and backtranslation with additional word manipulation parameters-shuffle, dropout, and blanking, all set at 0.1, and translation sampling performed with a greedy decoder."}, {"title": "EN to EC Translation", "content": "Given the iterative back-translation process at the core of our UNMT training, the model inherently learns to translate in both directions-EC \u2192 EN and EN \u2192 EC. The EN \u2192 EC translation performance presented in Table 4 reinforces our core assertion that emergent protocols optimized for pragmatic clarity are inherently more translatable. In particular, the Category game yields significantly higher BLEU (39.31) compared to other complexities, indicating a strong alignment between the emergent messages and their NL counterparts. Conversely, the Supercategory setting, characterized by richer but more variable communication, shows substantially lower scores (e.g., BLEU: 14.55; METEOR: 0.278), suggesting that increased structural diversity\u2014while expressive-compromises direct translatability. The intermediate performance observed in the Random and Inter-category games further underscores the delicate balance between expressiveness and translatability. We excluded the EOS token from our calculations, to ensure that these metrics precisely capture translation quality without any bias."}]}