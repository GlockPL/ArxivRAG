{"title": "ChemEval: A Comprehensive Multi-Level Chemical Evalution for Large Language Models", "authors": ["Yuqing Huang", "Rongyang Zhang", "Xuesong He", "Xuyang zhi", "Hao Wang", "Xin Li", "Feiyang Xu", "Deguang Liu", "Huadong Liang", "Yi Li", "Jian Cui", "Zimu Liu", "Shijin Wang", "Guoping Hu", "Guiquan Liu", "Qi Liu", "Defu Lian", "Enhong Chen"], "abstract": "There is a growing interest in the role that LLMs play in chemistry which lead to an increased focus on the development of LLMs benchmarks tailored to chemical domains to assess the performance of LLMs across a spectrum of chemical tasks varying in type and complexity. However, existing benchmarks in this domain fail to adequately meet the specific requirements of chemical research professionals. To this end, we propose ChemEval, which provides a comprehensive assessment of the capabilities of LLMs across a wide range of chemical domain tasks. Specifically, ChemEval identified 4 crucial progressive levels in chemistry, assessing 12 dimensions of LLMs across 42 distinct chemical tasks which are informed by open-source data and the data meticulously crafted by chemical experts, ensuring that the tasks have practical value and can effectively evaluate the capabilities of LLMs. In the experiment, we evaluate 12 mainstream LLMs on ChemEval under zero-shot and few-shot learning contexts, which included carefully selected demonstration examples and carefully designed prompts. The results show that while general LLMs like GPT-4 and Claude-3.5 excel in literature understanding and instruction following, they fall short in tasks demanding advanced chemical knowledge. Conversely, specialized LLMs exhibit enhanced chemical competencies, albeit with reduced literary comprehension. This suggests that LLMs have significant potential for enhancement when tackling sophisticated tasks in the field of chemistry. We believe our work will facilitate the exploration of their potential to drive progress in chemistry.", "sections": [{"title": "1 Introduction", "content": "The advent of large language models has ushered in a transforma- tive era in artificial intelligence, particularly within the domain of natural language processing. The expansive capabilities of these models have not only redefined the boundaries of text genera- tion and understanding but have also opened new av- enues for barious domains, such as recommendation , social and scientific exploration . Researchers have adeptly employed LLMs to accelerate the pace of scientific research and instigate a transformative shift in scientific research paradigms. The field of chemistry has notably profited from the integration and advancement of LLMs , becoming a key area where these sophisticated technologies have delivered substantial advantages. The intricate nature of chemical research, involving complex molecular interactions and reactions, presents a unique challenge that LLMs are ready to address through advanced pattern recognition and predictive analytics.\nIn order to systematically assess the capabilities of LLMs across various domains and identify areas for their potential enhancement, numerous benchmarking initiatives have been introduced. For in- stance, the MMLU covers 57 tasks spanning basic mathematics, American history, computer science, law, and other fields. The XieZhi benchmark includes three major academic categories with 516 specific subjects. However, general benchmarks often overlook detailed assessment of chemical knowledge. Although Sun et al. introduce SciEVAL as a framework for assessing the competencies of LLMs within the scientific domain, the chemistry- related tasks are overly simplistic and do not adequately capture the depth required. Regarding chemistry domain-specialized bench- marks, Guo et al. propose eight chemical tasks aimed at as- sessing understanding, reasoning, and explanation abilities, but it consists of tasks derived from existing public datasets, which may be insufficient to capture the full spectrum of competencies needed for thorough chemical research. Other studies like  have similar problems. This limitation prevents them from tackling key issues of interest to chemistry researchers and has not fully met the specialized needs of chemistry."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Large Language Models", "content": "The advent of LLMs has marked a significant milestone in the field of Natural Language Processing (NLP). Over the past few years, there has been a surge in the development of proprietary models such as GPT-4 and Claude-3.5 , which have demonstrated remarkable capabilities in various NLP tasks. These models, through their ex- tensive training on diverse datasets, have achieved unprecedented levels of performance, often outperforming human benchmarks in tasks like language translation, summarization, and quest-ion-answering. Concurrently, open-source models like LlaMA and ChatGLM series have emerged as viable alternatives, provid- ing the research community with accessible tools to explore and innovate within the NLP domain. The success of these models is attributed to their massive scale, the vast amount of diverse data they have been trained on, and the architectural advancements that make them able to capture complex linguistic patterns and generate highly human-like text."}, {"title": "2.2 Large Language Models for Chemistry", "content": "As the ability of general LLMs has gained widespread recognition, researchers have endeavored to harness their power to assist in scientific research tasks. However, the application of these mod- els in specialized domains such as chemistry meets with some challenges. The lack of domain-specific knowledge often leads to inadequate performance, particularly when dealing with tasks that involve technical jargon and numerical calculations. To address this gap, several approaches have been proposed. For instance, Galactica was developed through extensive pre-training on sci- entific datasets, while SciGLM employed fine-tuning techniques using relevant datasets to enhance its performance in scientific tasks and ChemCrow augmented the LLM performance in chemistry by integrating 18 expert-designed tools. In the chemical domain, models like ChemDFM , LlaSMol , and ChemLLM have been introduced, each with tailored training regimes to imbue the models with chemical knowledge. Additionally, Drugchat and Drugassist have been specifically trained to understand molec- ular structures and chemical properties. Despite these efforts, the comprehensive understanding of the chemical domain by LLMs remains an area ripe for further exploration and development."}, {"title": "2.3 Large Language Model Evaluations", "content": "The progress made in the field of LLMs is tightly linked to the establishment of robust evaluation frameworks. For general tasks, benchmarks such as MMLU and GLUE have become stan- dard tools for assessing model capabilities. In the scientific domain, recent initiatives like SciEval, SceMQA , and SciAssess have been introduced to evaluate scientific reasoning and knowl- edge. In the domain of chemistry, however, there are few com- pressive benchmarks available. For instance, ChemLLMbench , focusing on chemical task evaluation, closely resembles our work."}, {"title": "3 ChemEval", "content": "While the evaluation of large language models has been extensively conducted across domains such as law , finance , healthcare , and sciences , the domain-specific as- sessment within the field of chemistry remains notably sparse. Thus, we introduce a refined benchmark named ChemEval specifically designed to evaluate the capabilities of LLMs within the chemical domain to fill the absence of a holistic benchmark that encompasses"}, {"title": "3.1 Advanced Knowledge Question Answering", "content": "This segment is pivotal in assessing the models' proficiency in un- derstanding and applying fundamental chemical concepts, which include Objective Question dimension and Subjective Question di- mension, totally 5 different tasks. Through a blend of objective and subjective tasks, the Advanced Knowledge Question Answering challenges the models to demonstrate their insight in areas rang- ing from chemical terminology to quantitative analysis. The tasks within this section are designed to be both comprehensive and diagnostic, providing a clear measure of the models' readiness to tackle more advanced chemical inquiries."}, {"title": "3.1.1 Objective Questions", "content": "The first dimension is objective ques- tion answering, which primarily assesses the model's grasp of fundamental chemical knowledge and its capability to apply this knowledge in straightforward scenarios. Objective question an- swering encompasses the following tasks: Multiple Choice Task, Fill-in-the-Blank Task, and True/False Task. By incorporating these tasks, Chem-Eval can more effectively gauge the model's overall proficiency in understanding and applying chemical knowledge across various contexts and formats."}, {"title": "3.1.2 Subjective Questions", "content": "The second dimension is subjec- tive question answering, which includes Short Answer Task and Calculation Task, both aiming to evaluate the depth of the model's comprehension and its ability to apply chemical knowledge effec- tively. Because on the basis of the previous task, the model also requires providing a detailed solution or reason, which involves the understanding of the chemical principles and concepts in the question, and applying these principles and concepts to construct logically clear and organized answers, which intuitively reflects the model's understanding of basic chemical knowledge."}, {"title": "3.2 Literature Understanding", "content": "Advanced Knowledge Question Answering is designed to assess the model's comprehension and mastery of chemical knowledge, while Literature Understanding evaluates the model's capacity to inter- pret and assimilate information from chemical literature, which is foundational for subsequent inductive generation tasks. Literature Understanding, including Inductive Generation dimension and In- formation Extraction dimension, totally 15 tasks, delves into tasks"}, {"title": "3.2.1 Information Extraction", "content": "This is the first step to read a paper and also the foundation for the next inductive generation task. It involves the extraction of various elements related to chem- istry, such as named entities, reaction substrates, and catalyst types, encompassing a total of 11 tasks. These tasks aim to decompose and organize chemical information found in text, covering entities, relationships, and various aspects of chemical reactions."}, {"title": "3.2.2 Inductive Generation", "content": "Based on Information Extraction, Inductive Generation involves creating new, coherent, and con- textually relevant content based on existing data and knowledge. This process incorporates Chemical Paper Abstract Generation, Re- search Outline Generation, Chemical Literature Topic Classification, and Reaction Type Recognition and Induction, all geared towards synthesizing and organizing chemical information in meaningful ways."}, {"title": "3.3 Molecular Understanding", "content": "This section builds upon the previous foundation to assess the mod- el's understanding and generative capabilities at the molecular level. It includes 4 dimensions: Molecular Name Generation, Molecular Name Translation, Molecular Property Prediction and Molecular De- scription, totally 9 tasks. Molecular Understanding explores tasks essential for molecular understanding, evaluating the LLMs' ability to generate, translate, and describe molecular names and properties. These tasks assess the models' proficiency in interpreting and gen- erating chemical information accurately. The following subsections detail various specific tasks within this broader objective."}, {"title": "3.3.1 Molecular Name Generation", "content": "Molecular Name Genera- tion is the basis of Molecular Understanding and only contains one task, Molecular Name Generation from Text Description. This task is purposed to evaluate the capacity of LLMs in generating valid chemical structure representations. It necessitates that the models, based on intricate textual descriptions encompassing molec- ular structures, properties, and classifications, synthesize SMILES molecular formulas effectively."}, {"title": "3.3.2 Molecular Name Translation", "content": "Furthermore, Molecular Na-me Translation aims to enable a deep understanding of molecu- lar structures and representations, which should serve as the fun- damental knowledge for chemistry LLMs. It focuses on converting molecular names between different formats, requiring LLMs to output a specified alternative format based on a given molecular representation. It involves the conversion between representations of molecules such as IUPAC names and SMILES molecular for- mulas, encompassing a total of five tasks, each focusing on distinct aspects of molecular notation conversion."}, {"title": "3.3.3 Molecular Property Prediction", "content": "Apart from molecular na-me understanding, the ability to predict molecular property is also important. Molecular Property Prediction targets the forecast"}, {"title": "3.3.4 Molecular Description", "content": "To further understand molecular, the Molecular Description task has been designed to evaluate LLMs' capability in understanding and describing molecular structures. This task consists of a single subtask: Physicochemical Property Prediction from Molecular Structure."}, {"title": "3.4 Scientific Knowledge Deduction", "content": "Having established a solid grasp of basic chemical knowledge, the skill to interpret scientific literature, and the capacity to understand molecular structures, we expect that the model will proceed to conduct deeper chemical reasoning and deduction. So the part of Scientific Knowledge Deduction encompasses four key dimensions: Retrosynthetic Analysis, Reaction Condition Recommendation, Reac- tion Outcome Prediction and Reaction Mechanism Analysis, totally 13 tasks, which are essential for effective chemical synthesis. This part evaluates the LLMs' capabilities in retrosynthetic analysis, rec- ommending reaction conditions, predicting reaction outcomes, and analyzing reaction mechanisms. These tasks provide a comprehen- sive assessment of the models' performance in these critical areas of chemical synthesis."}, {"title": "3.4.1 Retrosynthetic Analysis", "content": "Retrosynthetic Analysis is a cru- cial technique in the field of chemical synthesis, particularly in organic synthesis. It starts from the target product and analyzes possible synthesis pathways and reactant substrates, demonstrating the reverse reasoning ability of the large model in the field of chem- ical synthesis. It comprises Substrate Recommendation, Synthetic Pathway Recommendation and Synthetic Difficulty Evaluation."}, {"title": "3.4.2 Reaction Condition Recommendation", "content": "Based on the re- sults of Retrosynthetic Analysis, LLMs can recommend suitable reaction conditions. Reaction condition recommendation is a key task in chemical synthesis, involving selecting the most suitable conditions for specific chemical reactions to ensure maximum effi- ciency, selectivity, and yield. This task integrates recommendations for conditions such as ligands, reagents, and catalysts, encompass- ing a total of six tasks, each targeting a specific component of the reaction condition optimization."}, {"title": "3.4.3 Reaction Outcome Prediction", "content": "After determining the re- action pathway and reaction conditions, the large model can predict possible reaction outcomes. Reaction outcome prediction is a core technology in chemical synthesis aimed at predicting possible re- sults of a reaction before it is actually carried out. This encompasses Reaction Product Prediction, Product Yield Prediction, Reaction Rate Prediction."}, {"title": "3.4.4 Reaction Mechanism Analysis", "content": "Reaction Mechanism Anal- ysis is a critical area in the study of chemical reactions, aiming to explain the detailed steps involved in the transformation from re- actants to products. This is the final step in the field of chemical synthesis, including identifying various intermediates, transition states, as well as the kinetic and thermodynamic parameters of each step in the reaction. Intermediate Derivation is the sole subtask in this phase."}, {"title": "3.5 Evaluation", "content": "3.5.1 Data Collection. Data plays an indispensable role in the realm of LLMs . Our data collection is comprised of two compo- nents: Open-source Data and Domain-Experts data. Open-source Data is based on keywords such as chemistry, large models, knowl- edge question answering, and information extraction, retrieve and download relevant papers on chemical large models from aca- demic websites. Then, extract and code the downstream tasks and their datasets within the chemical evaluation system from the papers . Next, download the official datasets for the different downstream tasks, using the presence of an official test set as the main criterion for selection. Nevertheless, the scope of open- source data is inadequate, which is why we collect expert datasets to enhance the evaluation's rigor and breadth. Domain-experts data is from scientific literature in the field, professional textbooks and supplementary materials, and laboratory chemical experiment data, manually construct question-answer pairs according to the task type.\n3.5.2 Data Processing. Through our data collection endeavors, we get a vast array of raw data in the chemical domain. However, to harness this data for our benchmarking work, it necessitates a subsequent phase of meticulous selection and filtration aligned with the diverse tasks.\nOur data processing for different levels: 1). Advanced knowledge question-answering. We meticulously compile question-answer pai- rs derived from undergraduate and postgraduate level textbooks, as well as ancillary educational materials. These pairs encompass a broad spectrum of seven distinct categories: organic chemistry, inor- ganic chemistry, materials chemistry, analytical chemistry, biochem- istry, physical chemistry, and polymer chemistry. This comprehen- sive selection ensures a diverse representation of chemical concepts and principles. 2). Literature understanding component. We extract relevant fragments and questions from scientific literature, combin- ing them with task-specific answers to create question-answer test sets for various downstream tasks. 3). Molecular understanding and scientific knowledge deduction. Our approach leverages a combina- tion of open datasets and proprietary laboratory data sourced from our collaborating universities. We engage in the thoughtful design and construction of test sets meticulously aligned with the unique content requirements of downstream tasks.\nIt is important to highlight that when integrating multiple open- source datasets for downstream tasks, we adopt a methodical ap- proach to constructing the corresponding test sets. This involves employing proportional sampling techniques that take into account the varying scales of the different data sources. This strategy en- sures that the test sets accurately reflect the broader dataset while maintaining a balanced distribution of question and answer types.\n3.5.3 Data Statistics. For each downstream task, a test set of 20 question-answer pairs and a few-shot set of 3 task introduction examples were constructed, all described in natural language text. Additionally, the molecular property classification task includes 100 items, while the molecular property regression task includes 140 items. Through our data collection endeavors, we get a vast array of raw data in the chemical domain. Notably, the test sets for different downstream tasks were cross-checked to remove dupli- cates with the training sets of corresponding tasks in open-source domain models, ensuring that there is no risk of data leakage in the evaluation of different downstream tasks.\n3.5.4 Instruction Creation. To evaluate the effectiveness of the model, in this paper, we constructed five sets of instruction sets for different downstream tasks: system-only instructions, task-specific prompts, and task-specific prompts with 1 to 3 example sets added respectively . For downstream tasks with open-source datasets, to facilitate evaluation, the evaluation system in this paper strength- ens the format of the output data based on its instructions. For the domain expert-built part, the evaluation system in this paper will design instructions for task introduction and formatted output ac- cording to the task type, and continuously adjust the instructions based on the return results of GPT-4, thereby strengthening the instructions for different self-constructed downstream tasks."}, {"title": "4 Experiment", "content": "4.1 Setup\nTo comprehensively evaluate the chemical capabilities of LLMs, our evaluation framework includes assessments of most of the current general large models, as well as some recently fine-tuned models with a focus on chemical knowledge. As a representative of the general large model, GPT-4 is the best model from OpenAI that has undergone pretraining, instruction-tuning, and reinforcement learning. Claude-3.5, developed by Anthropic, is the latest iteration of the Claude model family and is often regarded as surpassing GPT in terms of performance. Claude-3.5-Sonnet , the first release in this series, sets a new industry standard for intelligence. Baidu's ERNIE offers significant advancements in Al-driven content creation, while Kimi by Moonshot AI can provide accurate responses in both English and Chinese. Meta AI's LLaMA is probably the best open-weight foundation model so far. We evaluate LLaMA3-8B and LLaMA3-70B here. GLM-4 by ZhipuAI outper- forms LLaMA3-8B in various evaluations, and DeepSeek-V2 by DeepSeek is a robust Mixture-of-Experts (MoE) open-source Chinese language model comparable to GPT-4-turbo.\nIn the field of chemistry, specialized LLMs have demonstrated significant advancements. ChemDFM , based on LLaMA-13B, can surpass GPT-4 on a great portion of chemical tasks, despite the significant size difference. LlaSMol advances LLMs for chem- istry through instruction fine-tuning of pre-trained models, with Mistral being the best base model. LlaSMol significantly outper- forms Claude-3.5-Sonnet on most chemical tasks. ChemLLM by AI4Chem interprets and predicts chemical properties and reac- tions based on molecular structures, effectively analyzing complex chemical data to provide insights into molecular behavior and inter- actions. ChemSpark is trained through full-parameter fine-tunin-g based on the Spark 13B* foundation model by the dataset mixed of general domain Q&A and chemical domain-specific Q&A.\nTo illustrate the capability of LLMs in solving various chemical tasks, we present the average performance of LLMs across four levels under zero-shot condition, along with a detailed account of the zero-shot results. Due to the constraints on space, the average result of zero-shot results of all the models are shown in table 2 and detailed results of all subtasks representing different types of tasks are shown in Appendix A.1. Additionally, to investigate the adaptability and in-context learning abilities of LLMs for chemical tasks, we report the average performance across the same four levels under three-shot conditions. We provide the detailed result of few-shot setting in Appendix A.2."}, {"title": "4.2 Performance Results", "content": "We evaluate the model's performance by averaging the metrics for each task across four assessment dimensions. Certain models are unable to address specific tasks entirely. For example, ChemLLM demonstrates particularly poor instruction-following capabilities, which significantly impairs its ability to generate responses based on task prompts. Consequently, we are unable to provide numerical results for the tasks affected by this limitation. We discuss the key findings from our benchmark and analyze them to explore how different settings related to LLMs affect performance and provide valuable insights into Chemical benchmarks."}, {"title": "4.2.1 The models' performance across four levels", "content": "Observing the models' performance across four levels, we have the follow- ing findings: 1). Basic Knowledge: The results indicate that gen- eral large models like DeepSeek-V2 and LLaMA3-70B excel in ad- vanced knowledge answering, chemical literature comprehension, and scientific knowledge deduction tasks due to their extensive pre- training and document comprehension capabilities. DeepSeek-V2, in particular, shows strong performance across various tasks, while models like GPT-4 and Claude-3.5-Sonnet also perform well in cer- tain areas. However, models like LlaSMol and ChemLLM struggled, highlighting challenges in instruction fine-tuning. 2). Chemical expertise: Chemistry-specific models like ChemSpark stand out in tasks requiring deep chemical knowledge, such as molecular"}, {"title": "4.2.2 The benefits and drawbacks of specialized-LLMs", "content": "Compared with general-LLMs, we notice that the specialized-LLMs per- form differently. 1). The drawbacks of specialized-LLMs: In advanced knowledge answering and literature comprehension tasks, chemical models perform significantly worse than general models. Although specialized models acquire domain-specific knowledge thr-ough fine-tuning, their foundational natural language processing capa- bilities are compromised. This suggests that these models may encounter challenges related to catastrophic forgetting during the fine-tuning process. 2). The benefits of specialized-LLMs: In specific tasks that require specialized terminology and molecular properties, chemical models tend to have an advantage. The limited proportion of specialized chemical data in the pre-training datasets of gen- eral models allows them to perform adequately on simpler tasks. However, when faced with more complex scenarios, their ability to process and infer specialized chemical knowledge is notably defi- cient. 3). Instruction-following ability: During the evaluation process, different models exhibited varying levels of instruction-following capability. The instruction-following ability of chemistry-specific LLMs was significantly lower than that of general LLMs. These models, while possessing deep knowledge in chemistry, may not have been as widely exposed to the variety of tasks and diverse data present in the benchmark, leading to difficulties in adapting to new or varied instructions."}, {"title": "4.2.3 The influence of few-shot", "content": "We find that few-shot prompt- ing has a great impact on the model. 1). Text processing capabil- ity: Comparing results among 0-shot and few-shot, GPT-4 and ERNIE-4.0 get great performance enhancement in objective ques- tion answering while other models almost remain the same. When it comes to chemical literature comprehension and objective question answering, we find that few-shot prompting helps many models achi-eve better results, which indicates that few-shot helps improve the model's text comprehension, processing, and generalization abilities. But few-shot setting hurts the performance of ChemDFM, LlaSMol and ChemLLM. This is maybe because these models have not appropriately incorporated few-shot demonstrations into the in- struction tuning stage, thus sacrificing few-shot in-context learning performance to obtain enhanced zero-shot instruction-following abilities. 2). Reasoning ability: As for Molecular Understanding and Scientific Knowledge Deduction, the improvement of all models is not particularly significant, and many indicators even have a decrease. It may stem from the intrinsic complexities of the tasks, potential mismatches between training data and task requirements, inadequate fine-tuning processes, and the limitations of current LLMs in capturing expert-level cognitive reasoning in chemistry."}, {"title": "4.2.4 The impact of model scaling", "content": "In our evaluation of two model sizes of LlaMA3, we found that LlaMA3-70B consistently outperforms LlaMA3-8B. The enhancement in performance on che- mical tasks with the increase in model parameters can be attributed to the augmented memory capacity and reasoning abilities of larger models. This allows for improved comprehension and detailed anal- ysis of complex molecular structures and chemical phenomena, leading to notably superior performance in literature comprehen- sion and scientific knowledge inference."}, {"title": "5 Discussion", "content": "5.1 Impact\nChemEval, introduced in this paper, aims to address the absence of benchmark in the domain of chemistry for LLMs by providing a comprehensive benchmark that encompasses a wide array of chemical tasks. Its strength lies in the inclusion of expert-reviewed data, which ensures a high level of authenticity and quality. The meticulous construction of ChemEval, supported by rigorous qual- ity control measures and expert curation, positions it as a valuable platform for driving innovation and enhancement in chemical in- formatics and LLM evaluation. Its impact on the field is significant, offering a reliable and valid assessment of LLMs in the chemi- cal domain. By providing a comparative analysis of model per- formance, ChemEval aids in the selection of suitable models for scientific research, thereby promoting the advancement of chemical science. Researchers can select large models based on the needs of actual scientific research, leveraging the models' strengths to extract knowledge from scientific literature and experimental data, thereby promoting the advancement of scientific research."}, {"title": "5.2 Limitations", "content": "In our construction of chemical tasks, we observed several key find- ings: 1). Prediction Challenges: For tasks involving the prediction of spectral numerical features based on molecular structure descrip- tions, simulation of molecular dynamics behavior, and optimization of molecular 3D coordinates, all evaluated LLMs either produced refusals to respond or provided indiscriminate answers. 2). Tex- tual Description Limitation: Despite their impressive capabilities in handling natural language tasks, the application of LLMs in the chemical domain is hindered by the current evaluation frameworks' reliance on textual descriptions. 3). Integration Shortcomings: This limitation is exacerbated by the lack of integration with profes- sional molecular simulation tools, which are essential for accurate computational optimization and analysis. 4). Domain-Specific Ad- vantages: General LLMs and chemistry-specific LLMs demonstrate distinct advantages across different tasks, underscoring the signifi- cance of domain-specific data and the challenge of catastrop-hic forgetting. 5). Toxic Generation: LLMs may generate content that is toxic, harmful or illegal, underscoring the necessity for stringent supervision of their generative processes."}, {"title": "5.3 Future Work", "content": "The future refinement of ChemEval, including the incorporation of multimodal tasks and advanced functionalities, will enhance its utility and applicability in the evolving landscape of AI and chemistry. We will invite experts to manually evaluate the results"}, {"title": "6 Conclusion", "content": "In this paper, we developed a comprehensive chemical evaluation system to assess the performance of popular LLMs across four levels of chemical tasks. The findings indicate that LLMs exhibit relatively poor performance on tasks requiring the understanding of molecular structures and scientific knowledge inference, whereas they perform better on tasks involving literature comprehension. This suggests both the potential for improvement and the need for further advancements in the application of LLMs to chemical tasks. Through this extensive evaluation, we demonstrate that there remains significant room for enhancement in the capabilities of LLMs across various chemical tasks. We hope our work will inspire future research to further explore and leverage the potential of LLMs in the field of chemistry. This has the potential to contribute to the transformation of scientific research paradigms and holds significant implications for the advancement of both the scientific community and artificial intelligence. Future work on ChemEval will integrate multimodal tasks and more sophisticated tasks and expert manual evaluations will be conducted to validate the result of ChemEval and other benchmarks to improve the evaluation system's dependability for practical and scientific applications."}, {"title": "A Supplementary Experimental Results", "content": "A.1 The experimental results for each level\nA.1.1 The result of Advance Knowledge Answering. Objective ques- tion answering encompasses the following tasks: Multiple Choice task (MCTask), Fill-in-the-Blank task (FBTask), and True/False Task (TFTask). For all the above tasks, only the answers need to be pro- vided without any explanation, and the accuracy of the correct answers should be used as the model evaluation criterion. Subjec- tive Questions include Short Answer Task (SATask) and Calculation Task (CalcTask). The BLEU metric is employed to reflect the model's accuracy in generating relevant and precise molecular names for this task.\nFrom the results, DeepSeek-V2 appears to be the best model for Advance Knowledge Answering tasks. It demonstrates significantly superior performance in subjective tasks and achieves exceeding average performance in objective tasks. But other LLMs like GPT-4 and LLaMA3-70B demonstrate good performance in objective tasks and poor performance in subjective tasks. For True/Fal-se Task (TF-Task), Claude-3.5 Sonnet achieves the best results. In Short Answer Task (SATask) and Calculation Task (CalcTask), DeepS-eek-V2 con- sistently outperforms the others. We hypothesize that general LLMs have an advantage in this task because their pre-training encom- passes extensive knowledge bases and they have acquired more complex reasoning and associative capabilities. Furthermore, ad- vanced knowledge question answering does not require in-depth knowledge of molecular formulas and structures, allowing these models to perform better in this domain.\nA.1.2 The result of Literature Understanding. Information Extrac- tion is subdivided into the following areas: Chemical Named En- tity Recognition (CNER), Chemical Entity Relationship Classification (CERC), Synthetic Reaction Substrate Extraction (SubE), Synthetic Reaction Additive Extraction (AddE), Synthetic Reaction Solvent Ex- traction (SolvE), Reaction Temperature Extraction (TempE), Reaction"}, {"title": "B Prompt Examples", "content": "The following section presents a set of ChemEval prompts de- signed to evaluate a chemical language model. These prompts are categorized under four primary indicators: Advanced Knowledge Questions, Literature Comprehension, Molecular Understanding, and Scientific Reasoning. Each primary indicator is further divided into secondary and tertiary indicators. The examples provided here are zero-shot examples, where the model is required to generate responses without any prior examples or training on similar tasks.\nPay attention that some of the tasks below are in Chinese: all sub-tasks of Advanced Knowledge Question Answering, Spectral Feature Prediction from Molecular Structure, Molecular 3D Coordi- nate Optimization and Reaction Rate Prediction. Other tasks are all in English. To facilitate understanding for non-Chinese speaking audiences, we provide the English translations of these prompts, which are marked with (Translation). Note that the actual input used was in Chinese, and the following English text is provided for reference only."}, {"title": "B.1 Advanced Knowledge Questions", "content": null}, {"title": "B.1.1 Objective Question-Answering", "content": "Multiple Choice task(MCTask): In this assessment method, partic- ipants are tasked with selecting the correct answer from a provided list of options. Commonly used to gauge knowledge retention and comprehension, the MCT requires respondents to indicate their choice using the designated letter format (e.g., A, B, C, D) without additional commentary. The focus is on accuracy within the strict confines of the given format.\nPrompt Example\nQuery(Translation): Given a chemistry problem and mul- tiple choices, please select the correct answer. Your answer should be one of 'A', 'B', 'C', 'D', etc. You only need to output the correct answer's letter without any additional explanation. The problem and options are as follows:\nWhich of the following methods can be used to prepare a cis-alkene from an alkyne?\nA. Sodium borohydride reduction B. Hydroboration- reduction C. Lithium aluminum hydride reduction D. Alkali metal and liquid ammonia reduction\nYou must output your judgment and strictly follow the output format, {\"answer\": \"your answer\"}. I don't need any explanations, you only need to output your judgment in the specified format.\nAnswer: \"B\",\nFill-in-the-Blank task(FBTask): This task assesses the LLMs' recall of specific chemical terms or concepts by requiring participants to complete statements or sentences with the appropriate term or phrase. Indicated by underscores ('_____'), these blanks must be filled in correctly according to the specified guidelines.\nPrompt Example\nQuery(Translation): Given a chemistry fill-in-the-blank question, the '_' underline area is where you need to fill in the answer. You only need to output the correct answer for each '' underline area. The requirements are as fol- lows:\n1) Do not repeat the content of the question;\n2) Do not output any analysis process;\n3) If there are multiple underline areas, please use the fol- lowing format:\n1, 2, ...\nThe question is as follows:\n of the\nThe selection of a distillation flask is based on the crite- rion that the liquid volume should occupy flask volume. When the boiling point of the distillate is below 80\u00b0C, use heating. When the boiling point is between 80 - 200\u00b0C, use heating. Do not use for direct heating.\nYou must output your judgment and strictly follow the output format, {\"answer\": \"your answer\"}. I don't need any explanations, you only need to output your judgment in the specified format.\nAnswer(Translation): \"1. 1/3 - 2/3; 2. water bath; 3. oil bath; 4. electric heating mantle\"\nTrue/False Task(TFTask): In this task, the LLMs' ability to assess the accuracy of statements is tested through binary judgments. Participants evaluate given statements, responding with either \"True\" if the statement is correct, or \"False\" if it is incorrect."}, {"title": "B.1.2 Subjective Question-Answering", "content": "Short Answer Task(SATask): In this task", "Example\nQuery(Translation)": "Given a chemistry short answer question", "follows": "nWhat should be noted during the fractional distillation op- eration?\nYou must output your judgment and strictly follow the output format", "answer\"": "your answer\""}, ".", "I don't need any explanations, you only need to output your judgment in the specified format.\nAnswer(Translation): \"(1) When assembling the appa- ratus, the fractionating column should be as vertical as possible to ensure that the condensate from above and the rising vapor from below can fully exchange heat and mass, thus improving separation efficiency. (2) Select a suit- able heating bath based on the"]}