{"title": "DATASET OWNERSHIP VERIFICATION IN CONTRASTIVE PRE-TRAINED MODELS", "authors": ["Yuechen Xie", "Jie Song", "Mengqi Xue", "Haofei Zhang", "Xingen Wang", "Bingde Hu", "Genlang Chen", "Mingli Song"], "abstract": "High-quality open-source datasets, which necessitate substantial efforts for curation, has become the primary catalyst for the swift progress of deep learning. Concurrently, protecting these datasets is paramount for the well-being of the data owner. Dataset ownership verification emerges as a crucial method in this domain, but existing approaches are often limited to supervised models and cannot be directly extended to increasingly popular unsupervised pre-trained models. In this work, we propose the first dataset ownership verification method tailored specifically for self-supervised pre-trained models by contrastive learning. Its primary objective is to ascertain whether a suspicious black-box backbone has been pre-trained on a specific unlabeled dataset, aiding dataset owners in upholding their rights. The proposed approach is motivated by our empirical insights that when models are trained with the target dataset, the unary and binary instance relationships within the embedding space exhibit significant variations compared to models trained without the target dataset. We validate the efficacy of this approach across multiple contrastive pre-trained models including SimCLR, BYOL, SimSiam, MOCO v3, and DINO. The results demonstrate that our method rejects the null hypothesis with a p-value markedly below 0.05, surpassing all previous methodologies.", "sections": [{"title": "1 INTRODUCTION", "content": "The success of deep learning is greatly dependent on the the availability of high-quality open-source datasets, which empower researchers and developers to train and test their models and algorithms. Presently, the majority of public datasets Deng et al. (2009); Krizhevsky et al. (2009); Netzer et al. (2011) are designated exclusively for academic purposes, with commercial use prohibited without explicit permission. Therefore, preventing the stealing of public datasets holds significant importance for the benefit of the data owners.\nNumerous traditional techniques exist for data security, including encryption Boneh & Franklin (2001); Khamitkar, differential privacy Dwork (2006); Abadi et al. (2016), and digital watermarking Cox et al. (2002); Podilchuk & Delp (2001); Kadian et al. (2021). However, these methods fall short in protecting the copyrights of open-source datasets, as they either impede dataset accessibility or necessitate the knowledge of the training process of potentially suspicious models. Recently, dataset ownership verification (DOV) Guo et al. (2023); Li et al. (2022; 2023b) emerges as a novel defense measure to deter dataset theft. It allows defenders, i.e., dataset owners, to demonstrate whether suspects have infringed upon their rights by ascertaining whether a suspicious black-box backbone has been pre-trained on their datasets. However, as most existing DOV techniques are designed solely for supervised models where verification relies on distances between data points and decision boundaries Li et al. (2018); Karimi et al. (2019); Karimi & Tang (2020), they are not directly applicable to recently increasing popular self-supervised pre-trained models Chen et al. (2020); Chen & He (2021); Chen et al. (2021a) due to the absence of the well-defined decision boundaries."}, {"title": "2 RELATED WORK", "content": "Data Protection. Dataset ownership verification is an emerging field in data security. Typically, it involves embedding watermarks into the original dataset (Guo et al., 2023; Li et al., 2022; 2023b; Tang et al., 2023). Models trained on the watermarked dataset will incorporate a pre-designed backdoor, allowing defenders to verify data ownership simply by triggering the model's backdoor. However, current DOV methods primarily target supervised models and require altering the original dataset's distribution to inject watermarks, which makes it susceptible to various watermark removal mechanisms (Chen et al., 2021b; Liu et al., 2021b; Sun et al., 2023; Kwon, 2021; Hayase et al., 2021).\nThe proposed method demonstrates that, for contrastive learning, dataset ownership can be efficiently verified without modifying the original dataset.\nDataset inference Maini et al. (2021) is a state-of-the-art defense against model stealing (Sha et al., 2023; Sanyal et al., 2022; Shen et al., 2022). It does not require retraining the model or embedding watermarks within the dataset, which reduces the time cost significantly while preserving the original distribution of the data. The latest dataset inference method Dziedzic et al. (2022) has expanded its application to self-supervised learning. Although it's primarily aimed at encoder theft, it can also be directly used for dataset ownership verification. However, it necessitates inferring the entire training set to model the output features of all data from both the training and testing sets. It is prohibitively time-consuming for large datasets, such as ImageNet (Deng et al., 2009). In contrast, our method achieves accurate verification using only a small fraction of the dataset. For instance, on ImageNet, we use only 0.1% of the training set for verification.\nMembership inference Shokri et al. (2017); Choquette-Choo et al. (2021); Carlini et al. (2022); Hu et al. (2022) aims to determine whether an input was part of the model's training dataset. EncoderMI Liu et al. (2021a) is a powerful method specifically designed for membership inference on encoders pre-trained via contrastive learning, which takes advantage of the overfitting tendencies of the image encoder. However, it directly trains the inferencer on high-dimensional representations that contain a large amount of redundant information, which leads to a heavy computational cost and increased training difficulty. In contrast, our method extracts the most critical information for verification from the representations, namely contrastive relationship gap, achieving effective verification without the need to train an inferencer.\nInspired by Proof of Learning (PoL) Jia et al. (2021); Fang et al. (2023); Zhao et al. (2024), Proof of Training Data (PoTD) Choi et al. (2024) is proposed to assist third-party auditor in validating which data were used to train models. It helps develop practical and robust tools for accountability in the large-scale development of artificial intelligence models. However, it entails substantial verification costs, as the model trainer (suspect) is required to disclose detailed training records to the verifier, including training data, training code, and intermediate checkpoints. In practical scenarios, if the models trained by the suspect possess significant commercial value, the suspect is seldom willing to comply with such disclosures. Our setup is more reflective of real-world scenarios, where the model is a black box, and the defender can only access its API.\nContrastive Learning. Contrastive learning Chen et al. (2020); Chen & He (2021); Chen et al. (2021a); Caron et al. (2020); Albelwi (2022); He et al. (2020) aims to pre-train image encoders on unlabeled data by leveraging the supervisory signals inherent in the data itself, with these pre-trained encoders being applicable to numerous downstream tasks. The central idea of contrastive learning is to enable the encoder to produce similar feature vectors for a pair of augmentations derived from the same input image (positive samples), and distinct feature vectors for augmentations derived from different input images (negative samples). Classical approaches like SimCLR Chen et al. (2020), MoCo He et al. (2020), SwAV Caron et al. (2020), utilize both positive samples (for feature alignment) and negative samples (for feature uniformity). Surprisingly, researchers notice that contrastive learning can also work well by only aligning positive samples, such as BYOL Grill et al. (2020) and DINO Caron et al. (2021). We follow some literatures Albelwi (2022); Gao et al. (2022) to coin these methods as a special type of contrastive learning, or contrastive learning without negatives. We make no strict distinction between these concepts here due to the clear context in this work. Our method is designed to protect the unlabeled datasets used in contrastive learning, thereby securing and fostering healthy development in this field."}, {"title": "3 THE PROPOSED METHOD", "content": "In this study, we focus on the dataset ownership verification task in black-box scenarios. The problem involves two key player: the defender and the suspect. The defender, assuming the role of the dataset provider, endeavors to ascertain whether the suspect model, $M_{sus}$, has been unlawfully trained on his public dataset $D_{pub}$. $M_{sus}$ can be classified into four scenarios based on its training datasets:"}, {"title": "3.1 PROBLEM FORMULATION", "content": "\u2460 $M_{sus}$ is exclusively trained on the public dataset $D_{pub}$ of the defender, indicating the occurrence of dataset misappropriation; \u2461 $M_{sus}$ is trained on a dataset that encompasses the designated public dataset $D_{pub}$ along with an additional dataset $D_{alt}$, signifying dataset misappropriation, albeit posing a more challenging DOV task than case \u2460 due to the presence of $D_{alt}$; \u2462 $M_{sus}$ is trained on an unrelated dataset $D_{unre}$ outside the scope of the defender's public dataset, indicating the innocence of the suspect; \u2463 $M_{sus}$ is trained on an alternative dataset $D_{alt}$ that bears significant resemblance yet doesn't overlap with the public dataset $D_{pub}$, suggesting the innocence of the suspect, albeit posing a more arduous DOV challenge than case \u2462. These four scenarios encompass nearly every conceivable real-world circumstance."}, {"title": "3.2 CONTRASTIVE RELATIONSHIP GAP", "content": "In contrastive learning, a pivotal training objective for encoders is to maximize the similarity between the representations of positive samples, which are different augmentations of the same training image. This training approach leverages the neural network's memory capacity, prompting the encoder to retain the features of the training data. As a result, we derive the following two significant insights:"}, {"title": "3.2.1 OBSERVATIONS AND DEFINITIONS", "content": "Observation 1 (Unary Relationship) Contrastive pre-trained encoders can produce more alike representations for the same seen samples' augmentations during pre-training than unseen samples.\nObservation 2 (Binary Relationship) The pairwise similarity between the seen samples' representations hardly change after augmentations, unlike with unseen samples during pre-training.\nWe characterize the disparity between familiar and unfamiliar data encountered during the training phase as the encoder's contrastive relationship gap, a metric that can aid defenders in discerning whether the queried encoder has been pre-trained on their dataset. The precise definition is as follows:\nDefinition 1 (Contrastive Relationship Gap) Given a contrastive pre-trained encoder $M$ and a dataset $D$, the contrastive relationship gap of $M$ is defined as:\n$d(D,\\hat{D},M,T) = \\{s_i - \\hat{s_i} \\mid i \\in [1, |S|], s_i \\in S(D, M,T), \\hat{s_i} \\in S(\\hat{D},M,T)\\}$\nwhere $\\hat{D}$ is a dataset that $M$ has not been pre-trained on. $T(\\cdot)$ denotes an augmentation function. $S(\\cdot, \\cdot, \\cdot)$ is a similarity set. $|S|$ is the total number of samples in $S(D, M,T)$.\nA larger mean of contrastive relationship gap suggests that $M$ is more likely to have been pre-trained on $D$. According to Observation 1 and Observation 2, $S$ consists of unary relationship similarity set $S_u$ and binary relationship similarity set $S_B$."}, {"title": "3.2.2 THE CALCULATION OF $S_U$ AND $S_B$", "content": "Random cropping is a commonly used data augmentation technique in contrastive learning Chen et al. (2020); Chen & He (2021); Chen et al. (2021a), which can enhance the model's generalization ability significantly. In this paper, we use multi-scale random cropping to capture both global and local features of objects. Specifically, we design the $T$ in Eq.(1) as a multi-scale augmentation function $T_{ms} \\{T^g, T^l\\}$, hoping to activate the encoder's contrastive relationship gap from various dimensions. $T^g$ is the global augmentation function responsible for larger regions, while $T^l$ is the local augmentation function focusing on smaller regions. Through $T_{ms}$, we calculate $S_u$ and $S_B$ at multi-scale. Their definitions are as follows:\nDefinition 2 (Unary Relationship Similarity Set) Given an encoder $M$ and a dataset $D$, the unary relationship similarity set is defined as:\n$S_u (D, M, T^{ms}) = \\{S^{gg}_u, S^{ll}_u, S^{gl}_u\\}$\nwhere $S^{gg}_u$, $S^{ll}_u$, and $S^{gl}_u$ respectively denote the unary relationship similarity between global and global views, local and local views, and global and local views.\nThe specific formulas are as follows:\n$S^{gg}_u = \\frac{2}{|D|M(M - 1)}\\sum_{i=1}^{|D|}\\sum_{m=1}^{M}\\sum_{n=m+1}^{M} sim(M(T^g_m(x_i)), M(T^g_n(x_i)))$\n$S^{ll}_u = \\frac{2}{|D|N(N - 1)}\\sum_{i=1}^{|D|}\\sum_{m=1}^{N}\\sum_{n=m+1}^{N} sim(M(T^l_m(x_i)), M(T^l_n(x_i)))$\n$S^{gl}_u = \\frac{1}{|D|MN}\\sum_{i=1}^{|D|}\\sum_{m=1}^{M}\\sum_{n=1}^{N} sim(M(T^g_m(x_i)), M(T^l_n(x_i)))$\nwhere $x_i \\in D$, and $|D|$ is the total number of samples in dataset $D$. $M$ and $N$ are the execution number for $T^g$ and $T^l$, respectively. $T^g_m(x_i)$ denotes the m-th augmentation of $x_i$ by $T^g$, similarly for $T^l_n(x_i)$, $T^g_m(x_i)$, $T^l_n(x_i)$. $sim(\\cdot, \\cdot)$ represents the cosine similarity function.\nDefinition 3 (Binary Relationship Similarity Set) Similar to $S_u$, given an encoder $M$ and a dataset $D$, the binary relationship similarity set is defined as:\n$S_B(D,M,T^{ms}) = \\{S^{gg}_B, S^{ll}_B, S^{gl}_B\\}$\nwhere $S^{gg}_B$, $S^{ll}_B$, and $S^{gl}_B$ is the binary relationship similarity between global and global views, local and local views, and global and local views respectively.\nWe first introduce the binary relationship set G. It includes the pairwise similarity between the augmented images' representations, denoted as:\n$G(D,M,T) = \\{sim (M(T(x_i)),M(T(x_j))) \\mid i \\in [1, |D|], j \\in (i, |D|]\\}$\nwhere $sim(\\cdot, \\cdot)$ denotes the cosine similarity function, with $x_i, x_j \\in D$, $|D|$ is the total number of samples in dataset $D$, and $T(\\cdot)$ represents the augmentation function. By substituting the augmentation functions $T^g$ and $T^l$ into Eq.(7), we obtain the binary relationship set $G^g$ and $G^l$ at respective scales. Below, we formally present the specific formulas for $S^{gg}_B$, $S^{ll}_B$, and $S^{gl}_B$:\n$S^{gg}_B = \\frac{2}{M(M - 1)}\\sum_{m=1}^{M}\\sum_{n=m+1}^{M} f(G^g_m, G^g_n)$\n$S^{ll}_B = \\frac{2}{N(N - 1)}\\sum_{m=1}^{N}\\sum_{n=m+1}^{N} f(G^l_m, G^l_n)$\n$S^{gl}_B = \\frac{1}{MN}\\sum_{m=1}^{M}\\sum_{n=1}^{N} f(G^g_m, G^l_n)$\nwhere $f(\\cdot,\\cdot)$ is a distance measurement function, which is implemented as the mean absolute error in this paper. $M$ and $N$ are the execution number for $T^g$ and $T^l$, respectively. $G^g_m$ represents the m-th binary relationship set based on $T^g$, similarly for $G^l_n, G^g_m$ and $G^l_n$.\nUsing unary relationship similarity set $S_u$ and binary relationship similarity set $S_B$, we can determine the contrastive relationship gap $d$ of the encoder $M$ as follows:\n$d = \\{\\sum_\\ast (S_\\ast - \\hat{S_\\ast}), \\sum_\\ast (I(S_B \\geq \\hat{S_B}) - I(\\hat{S_B} \\geq S_B))\\}$\nwhere $\\ast \\in \\{gg,ll, gl\\}$, $S$ and $\\hat{S}$ come from $S(D, M,T)$ and $S(\\hat{D}, M,T)$ in Eq.(1), respectively. $I(\\cdot)$ is the function returning a if the input statement is true or returning 1 if the input statement is false. $a$ is a hyperparameter with a default value of 1. As $a$ increases, the contrastive relationship gap of encoder $M$ between $D$ and $\\hat{D}$ becomes larger."}, {"title": "3.2.3 THE COMPLETE PROCESS", "content": "We propose a method of dataset ownership verification by contrastive relationship gap.  displays the entire process of our method, divided into three stages:\n(1) pre-training a shadow encoder $M_{sdw}$ on a shadow dataset $D_{sdw}$ to compare with $M_{sus}$;\n(2) performing $K$ samplings on $D_{pub}$ and $D_{pvt}$ (a defender's private dataset which isn't publicly available, and $M_{sus}$ has not been trained on it), that represent $D$ and $\\hat{D}$ in Eq.(1), respectively. The sampling sizes are $k_{pub}$ and $k_{pvt}$ respectively, resulting in the subsets $\\{D^{k_{pub}}_1, ..., D^{k_{pub}}_K\\}$ and $\\{D^{k_{pvt}}_1, ..., D^{k_{pvt}}_K\\}$. Then using these subsets calculate the contrastive relationship gaps $d_{sus} = d^{k_{pub}}_1 \\cup ... \\cup d^{k_{pub}}_K$ and $d_{sdw} = d^{k_{sdw}}_1 \\cup ... \\cup d^{k_{sdw}}_K$ of $M_{sus}$ and $M_{sdw}$, respectively;\n(3) One-tailed pair-wise T-test Hogg et al. (2013) is conducted on $d_{sus}$ and $d_{sdw}$. The null hypothesis, $H_0$, posits that the mean of $d_{sus}$ is less than or equal to that of $d_{sdw}$, while the alternative hypothesis, denoted as $H_1$, posits that the mean of $d_{sus}$ is greater than the mean of $d_{sdw}$. If the p-value $p$ is less than 0.05, we can reject the null hypothesis and conclude that $D_{pub}$ has been stolen. On the other hand, if the null hypothesis can't be rejected, we think the suspect is innocent."}, {"title": "4 EXPERIMENTS", "content": "We evaluate our method using six visual datasets (CIFAR10 Krizhevsky et al. (2009), CIFAR100 Krizhevsky et al. (2009), SVHN Netzer et al. (2011), ImageNette Howard (2019), ImageWoof Howard (2019) and ImageNet Deng et al. (2009)) and five contrastive learning algorithms (SimCLR, BYOL, SimSiam, MOCO v3, and DINO). ImageNette and ImageWoof are two non-overlapping subsets of ImageNet, each containing 10 classes.\nThe specific experimental setup is introduced in Section 4.1, results and analyses are presented in Section 4.2, the application of our method on the ImageNet pre-trained models are demonstrated in Section 4.3, ablation studies are conducted in Section 4.4 and Appendix A. Specifically, Appendix A.7 presents the ablation study of sampling size, the ablation study of global and local augmentation number is shown in Appendix A.8, and the ablation study of shadow dataset and hyperparameter $a$ are featured in Appendix A.9. The impact of shadow model's training hyperparameters is shown in Appendix A.5. The anti-interference capability of our method is conducted in Section 4.5, Appendix A.11 introduces the comparison with the method based on watermark. Appendix A.10 introduces the impact of early stopping. Appendix A.13 presents some visualization results of our method."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "For SimCLR, BYOL, SimSiam, and MoCo v3, we use VGG16 Simonyan & Zisserman (2014), and Resnet18 He et al. (2016) as encoder architectures. Additionally, we use ViT-T, ViT-S, and ViT-B Dosovitskiy et al. (2020) for DINO. For $M_{sdw}$, we default to using ResNet18 and SimCLR as its encoder architecture and training algorithm.\nTo simulate $D_{alt}$, a dataset similar to $D_{pub}$ but without overlapping data (as described in Section 3.1), we randomly divide a dataset into two subsets of equal size representing $D_{pub}$ and $D_{alt}$, respectively. For $D_{pvt}$, we set it as the testing set of the undivided dataset for convenience. Specific settings are as follows:\n\u2022 Experiment 1: $D_{pub}$ is random half of CIFAR10 training set and $D_{alt}$ is the other half. $D_{unre}$, $D_{sdw}$ and $D_{pvt}$ are SVHN, CIFAR100 and CIFAR10 testing set respectively.\n\u2022 Experiment 2: $D_{pub}$ is random half of ImageNette training set and $D_{alt}$ is the other half. $D_{unre}$, $D_{sdw}$ and $D_{pvt}$ are ImageWoof, SVHN and ImageNette testing set respectively.\nThe settings for the remaining parameters are provided in Appendix A.2. To simulate adversarial behavior, we pre-train $M_{sus}$ using $D_{pub}$, $D_{pub} \\cup D_{alt}$, $D_{unre}$, and $D_{alt}$, respectively, which is corresponds to the four cases in Section 3.1."}, {"title": "4.2 EXPERIMENTAL RESULTS", "content": "Our approach is proven effective as illustrated in Figure 3 (refer to Appendix A.3 and A.4 for specific p-values), which display the experimental results of baselines and our method on CIFAR10 and ImageNette. Note that when $D_{sus}$ is CIFAR10-1 (ImageNette-1) or CIFAR10 (ImageNette), $D_{sus}$ includes $D_{pub}$ ($D_{pub}$ is CIFAR10-1 and ImageNette-1 in two cases respectively), which implies the suspect is illegal, and $p$ should be less than 0.05. However, when $D_{sus}$ is CIFAR10-2 (ImageNette-2) or SVHN (ImageWoof), the suspect did not use $D_{pub}$ and is legal, so $p$ should be greater than 0.05.\nThe two baselines struggle to accurately distinguish the legality of various scenarios. There are a large number of false positive or false negative samples in all cases. In contrast, our method consistently produces correct results in all cases. Unlike the baselines, which model high-dimensional representations containing a large amount of redundant information directly, our method refines the most valuable information from these representations.\nThis crucial information, contrastive relationship gap, is extracted based on the characteristics of contrastive learning. Therefore, our method is not constrained by the encoder architecture and training algorithm, achieving desirable outcomes in various scenarios.\nSensitivity and specificity reflect the algorithm's ability to identify positive and negative samples."}, {"title": "4.3 THE APPLICATION OF OUR METHOD ON IMAGENET", "content": "To validate the efficacy of our method in real-world scenarios, we conduct dataset ownership verification on ImageNet, a large-scale visual dataset containing over 14 million images across 1000 classes, using ten pre-trained encoders. The architecture of these encoders includes CNN and ViT, and they are pre-trained using the six popular contrastive learning methods currently. Among these, the pre-trained model for DINO is obtained from the official repository\u00b9 , while the models for the other contrastive learning methods are sourced from MMSelfSup\u00b2 . In our experiments, we designate $D_{pvt}$ as the validation set of ImageNet and $D_{sdw}$ as SVHN. The architecture and training algorithm of $M_{sdw}$ are ResNet18 and SimCLR, respectively. Parameter settings are provided in Appendix A.2. As shown in Table 2, the experimental outcomes demonstrate that our method is well-suited for pre-trained models on ImageNet, even when using only 0.1% of ImageNet data for dataset ownership verification. Conversely, the performances of baselines are unsatisfactory."}, {"title": "4.4 ABLATION STUDIES", "content": ""}, {"title": "4.4.1 THE IMPACT OF MULTI-SCALE AUGMENTATION IN UNARY AND BINARY RELATIONSHIP", "content": "We use pre-trained models on ImageNet to verify the effectiveness and robustness of unary and binary relationship's multi-scale augmentations. Specifically, the models are ResNet50 and ViT-B/16 pre-trained by DINO. Both $D_{pub}$ and $D_{sus}$ are ImageNet. As shown in Table 3, The combined use of unary and binary relationship's multi-scale augmentations outperform other choices. This superiority is attributed to its attempts to activate the encoder's contrastive relationship gap from various angles, thereby endowing it with strong generalization capabilities to adapt to different encoders."}, {"title": "4.4.2 THE IMPACT OF SAMPLE NUMBER OF $D_{pub}$ AND $D_{alt}$", "content": "We study the impact of the sample number of $D_{pub}$ and $D_{alt}$ on our method. Specifically, we denote the proportion of $D_{pub}$ in $D_{pub} \\cup D_{alt}$ as $r$. And $D_{pub} \\cup D_{alt}$ is always CIFAR10. For example, when $r = 0.1$, $D_{pub}$ is 10% of the CIFAR10 training set randomly sampled, while $D_{alt}$ consists of the remaining 90%. Similarly, when $r = 0.2$, $D_{pub}$ is 20% of the CIFAR10 training set randomly sampled, and $D_{alt}$ is the remaining 80%."}, {"title": "4.5 THE ANTI-INTERFERENCE CAPABILITY OF OUR METHOD", "content": ""}, {"title": "4.5.1 THE IMPACT OF PRIVACY TRAINING METHOD", "content": "Private training methods Abadi et al. (2016); Papernot et al. (2018) are typically used to protect private, non-open-source datasets. In our scenario, the suspect might employ private training methods to obscure their illegal activities and interfere with the defender's dataset ownership verification, even if it reduces the encoder's normal performance. Therefore, we chose the classic private training method DP-SGD Abadi et al. (2016) and conducted the following experiments. Specifically, we trained the suspicious encoder on ImageNette using DP-SGD or not. The $\\epsilon$ for DP-SGD is 50, and the maximum norm for gradient clipping is 1.2. The results are shown in Table 4 and Table 5, indicating that our method remains effective in this more arduous scenario."}, {"title": "4.5.2 THE APPLICATION OF OUR METHOD ON FINE-TUNED ENCODERS", "content": "We also challenge the scenario where $M_{sus}$ is applied to downstream tasks. Specifically, we train the entire classifier on CIFAR10 and CIFAR100 respectively, whose backbone is a ResNet50 pre-trained on ImageNet using SimCLR. Similarly, in the black-box environment, we can only use the predicted probability vectors of the input samples. The results are shown in Table 6 and Table 7. \u2018$D_{downstream}$\u2019 is the dataset of downstream tasks. 'Acc' represents the accuracy on downstream tasks. $D_{sus}$ and $D_{pub}$ are both ImageNet. Note that in this scenario, $D_{sus}$ includes $D_{pub}$, making the suspect's behavior illegal, and the p-values should be less than 0.05. Moreover, we set the hyperparameter $a$ to 5 to amplify the contrastive relationship gap. Excitingly, even after fine-tuning, we are still able to identify the suspect's theft. For details on fine-tuning, please refer to Appendix A.6."}, {"title": "4.6 THE TIME COST OF OUR METHOD", "content": "We calculated the time required for our method and DI4SSL to perform a single verification on ImageNet. The experiments were conducted using an NVIDIA GeForce RTX 4090. The encoder is a ResNet50 pre-trained on ImageNet using SimCLR. As shown in Table 8, the time consumption of our method is significantly less than that of DI4SSL.\nThis is because our method only requires inferring on a small subset of ImageNet (depending on $k_{pub}$, $k_{pvt}$, M and N), whereas DI4SSL needs to infer the entire dataset. Additionally, our method was properly validated (p < 0.05), further demonstrating its superiority."}, {"title": "4.7 LIMITATIONS", "content": "Not all encoders are pre-trained using contrastive learning. Masked Image Modeling (MIM) Girdhar et al. (2023); He et al. (2022) is also a significant method for pre-training encoders. However, as shown in Appendix A.12, our method doesn't effectively apply to encoders pre-trained via MIM. This is because that the representations learned through MIM are harder to distinguish compared to those from contrastive learning Zhou et al. (2022), although MIM-based pre-training methods demonstrate superior performance in downstream tasks. This results in less pronounced unary and binary relational gaps in the representations. We plan to refine this aspect in our future work."}, {"title": "5 CONCLUSION", "content": "High-quality open-source datasets are essential for the rapid development of deep learning. We propose a method for verifying dataset ownership in contrastive learning to protect the legitimate right of dataset owners. Specifically, we propose the concept of contrastive relationship gap based on the unary and binary relationship of contrastive pre-trained models. The experiment proves that it can effectively verify dataset ownership. Promising future work includes (1) extending our method to other self-supervised learning approaches; (2) adapting our method to protect other types of data (e.g., text); (3) exploring other privacy risks associated with encoders."}, {"title": "APPENDIX", "content": ""}, {"title": "A THE DETAILS AND ADDITIONAL SUPPLEMENTS OF EXPERIMENTS", "content": ""}, {"title": "A.1 DATASETS USED", "content": "CIFAR10 Krizhevsky et al. (2009): The CIFAR10 dataset consists of 32x32 colored images with 10 classes. There are 50000 training images and 10000 test images.\nCIFAR100 Krizhevsky et al. (2009): The CIFAR100 dataset consists of 32x32 coloured images with 100 classes. There are 50000 training images and 10000 test images.\nSVHN Netzer et al. (2011): The SVHN dataset contains 32x32 coloured images with 10 classes. There are roughly 73000 training images, 26000 test images and 530000 \"extra\" images.\nImageNette Howard (2019): ImageNette is a subset of 10 easily classified classes from Imagenet. It includes the following categories: tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball and parachute. There are roughly 10000 training images and 4000 test images.\nImageWoof Howard (2019): ImageWoof is a subset of 10 classes from Imagenet that aren't so easy to classify. It includes the following categories: Australian terrier, Border terrier, Samoyed, Beagle, Shih-Tzu, English foxhound, Rhodesian ridgeback, Dingo, Golden retriever, Old English sheepdog. There are approximately 9000 training images and 4000 test images.\nImageNet Deng et al. (2009): Larger sized coloured images with 1000 classes. There are approximately 1 million training images and 50000 test images. As is commonly done, we resize all images to be of size 224x224."}, {"title": "A.2 EXPERIMENTAL DETAILS", "content": "The ResNet18 trained on CIFAR10/CIFAR100 uses a convolutional kernel size of 3x3 with a stride of 1, instead of the default 7x7, and doesn't use a max pooling layer.\nOn CIFAR10/CIFAR100/SVHN, we pre-train the encoder for 800 epochs with a batch size of 512. On ImageNette/ImageWoof, the encoder with non-ViT-S/16 architecture is pre-trained"}]}