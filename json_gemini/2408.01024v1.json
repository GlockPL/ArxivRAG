{"title": "Semantic Skill Grounding for Embodied Instruction-Following in\nCross-Domain Environments", "authors": ["Sangwoo Shin", "Seunghyun Kim", "Youngsoo Jang", "Moontae Lee", "Honguk Woo"], "abstract": "In embodied instruction-following (EIF), the in-\ntegration of pretrained language models (LMS)\nas task planners emerges as a significant branch,where tasks are planned at the skill level by\nprompting LMs with pretrained skills and user\ninstructions. However, grounding these pre-\ntrained skills in different domains remains chal-\nlenging due to their intricate entanglement with\nthe domain-specific knowledge. To address this\nchallenge, we present a semantic skill ground-\ning (SemGro) framework that leverages the hi-\nerarchical nature of semantic skills. SemGro\nrecognizes the broad spectrum of these skills,\nranging from short-horizon low-semantic skills\nthat are universally applicable across domains\nto long-horizon rich-semantic skills that are\nhighly specialized and tailored for particular\ndomains. The framework employs an iterative\nskill decomposition approach, starting from the\nhigher levels of semantic skill hierarchy and\nthen moving downwards, so as to ground each\nplanned skill to an executable level within the\ntarget domain. To do so, we use the reasoning\ncapabilities of LMs for composing and decom-\nposing semantic skills, as well as their multi-\nmodal extension for assessing the skill feasibil-\nity in the target domain. Our experiments in the\nVirtualHome benchmark show the efficacy of\nSemGro in 300 cross-domain EIF scenarios.", "sections": [{"title": "Introduction", "content": "In the area of embodied instruction-following\n(EIF), the quest to develop systems capable of un-\nderstanding user instructions and achieving tasks\nin a physical environment has been a pivotal aspi-\nration for general artificial intelligence. Recent ad-\nvancements in language models (LMs) have demon-\nstrated their extensive and impressive application\nin EIF, where LMs are employed as task planners,\ndrawing on their commonsense knowledge of the\nphysical world (Yang et al., 2023; Padmakumar\net al., 2023; Pantazopoulos et al., 2023; Yun et al.,\n2023; Logeswaran et al., 2022).\nThe process of LM-based task planning in EIF\nunfolds via a two-stage pipeline: (i) Interpretable\nskills (i.e., semantic skills) are acquired, and (ii)\ngiven a user instruction, the LM is prompted with\nthe instruction and the skill semantics to identify\nappropriate skills to perform. To ensure that such\nplanned skill can be executed within the domain\ncontext the agent is placed in, several works have\nfocused on incorporating environmental observa-\ntions into task planning (Singh et al., 2023; Song\net al., 2023; Xiang et al., 2023; Lin et al., 2023b,a;\nShinn et al., 2023; Carta et al., 2023). Yet, EIF in\ncross-domain environments has not been fully ex-\nplored. In cross-domain settings, the complexity\ngoes beyond logical task planning by LMs; it ne-\ncessitates the adaptation of pretrained skills to align\nwith the specific characteristics and limitations of\nthe target physical environment. This adaptation\nchallenge stems from the inherent domain speci-\nficity of semantic skill acquisition, calling for a\nstrategy that disentangles these learned behaviors\nfrom their original domain contexts and reapplies\nthem in new domains.\nIn response to this challenge, we present a se-\nmantic skill grounding framework SemGro. We\nobserve a trade-off between semantic depth and\ndomain-agnostic executability within the skill hier-\narchy. High-level skills, such as \u201cprepare a meal\u201d,\nembody a richer semantic content and long-horizon\nbehavior compared to low-level skills, such as\n\u201cgrab a cup\u201d. However, these high-level skills tend\nto be effective only within specific domains, as\ntheir execution heavily depends on a multitude of\nenvironmental factors, unlike their low-level coun-\nterparts which can be universally applicable across\ndomains. SemGro capitalizes on this insight by em-\nploying an iterative skill grounding scheme, de-\nsigned to progressively decompose pretrained skills"}, {"title": "Preliminaries", "content": "For complex embodied tasks, prior works have\nleveraged the notion of semantic skills, which\nrepresent interpretable patterns of expert behav-\nior. These skills have a wide spectrum, ranging\nfrom short-horizon basic low-level skills (e.g.,\n\"stir the drink\u201d) to long-horizon high-level skills"}, {"title": "Language Models for Task Planning", "content": "For complex embodied tasks, prior works have\nleveraged the notion of semantic skills, which\nrepresent interpretable patterns of expert behav-\nior. These skills have a wide spectrum, ranging\nfrom short-horizon basic low-level skills (e.g.,\n\"stir the drink\u201d) to long-horizon high-level skills (e.g., \"prepare a meal\"). These diverse skills \u03a0\u2081 =\n{\u03c0\u03b9|l : skill semantic} can be efficiently estab-\nlished within a training environment, through deter-\nministic algorithms, reinforcement learning (RL),\nor imitation learning. The integration of semantic\nskills and the reasoning capabilities of LMs enables\nthe zero-shot adaptation of EIF agents, without ad-\nditional environment interactions or data collection\nefforts (Song et al., 2023; Singh et al., 2023; Bro-\nhan et al., 2023c). In these approaches, an LM is\nprompted with the semantics of pretrained skills\nL\u2081 = {l : \u03c0\u03b9 \u2208 \u03a0} along with a specific in-\nstruction i from the user. Subsequently, the LM\nproduces a series of semantic skills. The execu-\ntion of these skills facilitates the decomposition of\nthe complex task associated with instruction i into\nsmaller, achievable skill-level subtasks."}, {"title": "Cross-Domain Skill Grounding", "content": "While several works have explored the use of LMs\nas task planners, they primarily focus on single-\ndomain agent deployment, wherein skills are both\nacquired and used within the same environment set-\nting. In contrast, the cross-domain setting presents\nunique challenges for embodied task planning, as\nthe feasibility of skill execution in the environment\ndiffers, necessitating the adaptation of pretrained\nskills to the specific domain contexts of the tar-\nget environment. Consider the skill \"Make a cup\nof coffee\", as illustrated in Figure 1, learned in\na high-tech kitchen environment equipped with\nvoice-activated appliances, a digital inventory sys-\ntem, and automated utensil handlers. In this envi-\nronment, the skill entails complex interactions with"}, {"title": "Approaches", "content": "We utilize the intrinsic semantic hierarchy of skills,\na structure derived from their compositional nature;\nhigh-level semantic skills are established by chain-\ning short-horizon low-level skills. High-level skills,\ncharacterized by rich semantics, closely align with\nthe abstract nature of user instructions. This align-\nment in abstraction levels facilitates the task plan-\nning process, providing the LM with skills that\nare directly beneficial for interpreting and accom-\nplishing the instructions. However, the execution\nof these long-horizon high-level skills is contingent\nupon the precise alignment of various environmen-\ntal and domain-specific conditions. This precludes\ntheir adaptability in cross-domain settings. In con-\ntrast, low-level skills exhibit a robust capacity for\nexecution that is not as tightly bound to specific do-\nmain contexts, offering greater adaptability across\ndiverse domains. Nonetheless, the sheer volume of\nlow-level skills and their limited semantic depths\npose additional challenges for task planning.\nThe core of SemGro lies in the hierarchical task\nplanning scheme via iterative skill grounding. The\nframework initiates task planning with high-level\nrich-semantic skills, and then progressively decom-\nposes each of these planned high-level skills into\na series of low-level skills executable within the\ndomain of the target environment. This can be seen"}, {"title": "Overall Framework", "content": "As illustrated in Figure 2, SemGro is structured\nwith (a) a task planner and (b) a multi-modal skill\ncritic, which collaboratively ground skills for user\ninstructions in cross-domains. At each skill-step,\nthe task planner produces an appropriate seman-\ntic skill by prompting an LM with information\nretrieved from a skill database. The skill critic\nthen evaluates each skill from the skill planner by\nprompting a multi-modal LM with visual obser-\nvations from the environment. When evaluated as\nunsuitable, the task planner decomposes the skill\ninto smaller, more general subtasks using another\nretrieval from the database. Through this iterative\nprocess, the framework systematically assembles a\nsequence of semantic skills, executable within the\ndomain context of the target environment."}, {"title": "LM-based Task Planner", "content": "For LM-based task planners, the development of\neffective prompts is essential to fully harness the\nreasoning capabilities of LMs. We consider two\naspects: (i) selecting skill candidates that facilitate\nthe skill-level task decomposition and (ii) incorpo-\nrating in-context examples applied to similar in-\nstructions. To capture these aspects, we construct\na hierarchical semantic skill database, leveraging\nthe reasoning prowess of LMs. Integrated with a\nkNN retriever, this database not only assists in iden-\ntifying suitable high-level skill candidates for task\nplanning but also facilitating the retrieval of perti-\nnent in-context examples that can further aid the\nplanning process. The task planner, enriched with\nthis selection of skill candidates and in-context ex-\namples, ensures the application of suitable skill\nsemantics in cross-domains.\nHierarchical semantic skill database. For the hi-\nerarchical semantic skill database D, we use a struc-\ntured approach to semantic skill training, similar\nto (Zhang et al., 2023). The skill acquisition ini-\ntiates with a set of short-horizon basic low-level\nskills I} = {\u03c0{1,1),..., \u03c0(1,1)} in the training\nenvironment, which can be acquired via RL or imi-\ntation learning. Under the LM guidance, the agent\nproduces skill chains from II}, achieving high-level\nlong-horizon skills II?. This bootstrapping process\niteratively expands to form a comprehensive set of\nskill levels I\u2081 = {I},\u2026\u2026, \u03a0^^ }. This collection,\nencompassing skills from low-level to high-level,\nis used to establish the skill database D:\nIm = {\u03c0(m,1), ..., \u03c0(m,Nm) }\nD := {e(m,n) : 1 < m < M, 1 \u2264 n \u2264 Nm}\ne(m,n) := (1(m,n), dn(e(m,n)), p(e(m,n)))\n[(m,n) := Skill semantic of \u03c0(m,n)\np(e(m,n)) := (e(m-1,nj\u2081),...).                                                       (1)\nThe entry e(m,n) of D includes the skill semantic\n1(m,n) of \u03c0\u03af (m,n), detected object names dn(e(m,n))\ncollected during training of (m,n), and a one-\nstep low-level semantic skill planning p(e(m,n)) =\n(e(m-1,nj\u2081), ...), in that the skill \u03c0(m,n) \u03c0 is acquired\nby chaining of (\u03c0\u03b9 (m-1,nj\u2081), ...). To train the skill\ncritic (Section 3.3), we also establish a dataset:\nDo = {o(TR), dn(o(TR)), ds(o(TR)) : t}                               (2)\nof visual observations of o(TR) in the training environ-\nment, detected object names dn(o(TR)) and object\nphysical states ds(o(TR), at each timestep t dur-\ning the establishment of \u03a0\u03b9. \u03a4o obtain such object\nnames (e.g., \"microwave\") and physical states (e.g.,\n\"microwave: closed\"), we use an open-vocabulary\ndetector tailored for the training environment.\nSkill generator. For a given user instruction i\nand target environment observation ot, the LM-\nbased skill generator \u00d8G integrates skill candidates\nc(i, ot), in-context examples x(i, ot), and the skill\nexecution history h to generate the skill semantic l\nwhich is most likely to be executed at the present:\n$G : (i, h, x(i, ot), c(i, ot)) \u2194 \u012a\u2208 c(i, ot).                                   (3)\nTo derive these in-context examples and skill can-\ndidates, we use a kNN retriever that selects en-\ntries e(i, ot) = {e(m1,n1), ..., e(mk,nk)} in D. The\nretrieval process calculates similarity by combin-\ning the instruction i and the visual context of\nOt into a single query. It evaluates (i, 1(m,n)) +\n(dn(e(m,n)), ot) to determine the k entries with the\nhighest similarity scores, where (,) denotes the\ncosine similarity. Here, l(m,n) is the skill semantic"}, {"title": "Multi-Modal Skill Critic", "content": "For observation ot and skill semantic l derived from\nthe skill generator \u00d8G (in Eq (3)), the skill critic \u03c8\nassesses the execution feasibility of skill \u03c0\u03c4:\n\u03c8 : (ot, 1) \u2194 c \u2208 {E, (NE, f)}.                                    (6)\nHere, E specifies that the skill is executable, while\nNE denotes inexecutability with language feedback\nf. The critic & includes the vision-language model\n(VLM)-part VLM and LM-part \u03a8LM. Here, \u03c8VLM\nidentifies object names dn(ot) and states ds(ot)\nfrom ot, and VLM utilizes this information to deter-\nmine the executability of the skill \u03c0\u03c4:\n\u03c8 = \u03c8LM \u00b0 \u03c8VLM : (ot, \u012a) \u2192 {E, (NE, f)}\nVLM : ot \u2192 (dn(ot), ds(ot))\nLM: (1, dn(ot), ds(ot)) \u2192 {E, (NE, f)}.                               (7)\nWe use the dataset Do (in Eq (2)) to fine-tune the\nphysically grounded VLM (Gao et al., 2023). Our\nablation study (Section 4.3) specifies the benefits"}, {"title": "Experiments", "content": "Environment. We use VirtualHome, a realistic\nhousehold activity simulator and benchmark. For\nEIF agents, we define 3949 low-level skills (i.e.,\n\u03a0\u2081 in Section 3.2) based on the combination of\nbehaviors (e.g., \u201cgrasp", "apple": ".", "types": "OL involves changes in the placements\nand locations of objects; PA presents changes in the\nphysical attributes of objects; RS is characterized\nby different room structures and visual attributes.\nThese cross-domain contexts necessitate the refine-\nment of certain pretrained high-level skills into\nlow-level skills for successful execution. Consider\na high-level skill \u201ccook salmon"}, {"title": "SemGro Implementation", "content": "For the KNN retriever, we configure k to be 10\nand utilize the sentence-transformer (Reimers and\nGurevych, 2019) to compute the similarity between\nthe instruction and skill semantics in the database.\nThe task planner & and the LM-part VLM of the\nskill critic are both implemented using GPT-3.5\nAPI with the temperature hyperparameter set to 0.\nFor fair comparisons, the same GPT-3.5 configura-\ntion is used across all baseline task planners. More\ndetails, including the baseline implementation, are\nin the Appendix."}, {"title": "Performance", "content": "Overall performance. Table 1 compares the cross-\ndomain performance, achieved by our SemGro\nand the baselines (LLM-Planner, SayCan, Prog-\nPrompt). SemGro outperforms the baselines in all\ncross-domain scenarios, achieving average perfor-\nmance gains of 25.02%, 26.83%, and 27.65% over\nthe most competitive baseline SayCan in SR and\nCGC, and LLM-Planner in Plan, respectively. The\nexceptional performance of SemGro highlights its\nsuperiority in hierarchical task planning via itera-\ntive skill grounding across unseen domains.\nInterestingly, for long-horizon instruction sce-\nnarios, there are several instances where SR and\nCGC metrics exceed Plan. In the PA cross-domain\ncontext, SemGro achieves Plan of 61.11%, while\nSR and CGC reach 62.96% and 74.07%, respec-\ntively. This phenomenon, reminiscent of solving"}, {"title": "Executable skill identification", "content": "We investigate the\nefficacy of SemGro in generating executable skills.\nTo this end, we introduce the Exec metric, which\nquantifies the percentage of the planned semantic\nskills that are executable in a given domain, regard-\nless of their impact on task completion. In Table 2,\nSemGro surpasses the most competitive baseline\nProgPrompt by 17.99% in Exec at average. This\nspecifies our framework's proficiency in discerning\nthe varying conditions of cross-domain environ-\nments for skill grounding. This ability is attributed\nto the skill critic which uses the domain-agnostic\nrepresentation capabilities of the adopted VLM."}, {"title": "Ablation Studies", "content": "Skill hierarchy for planning. To verify the effec-\ntiveness of the iterative skill grounding scheme, we\nconduct task planning with the fixed abstraction"}, {"title": "Experiment Details", "content": "In this section, we describe the experiment settings\nin detail. We also specify the cross-domain sce-\nnarios and environment details used in the exper-\niments, as well as the data collection process for\nour hierarchical skill database and the training pro-\ncedure of our multi-modal critic.\nCross-domain scenarios. We used 5 EIF tasks,\n4 types of language instructions, and 15 environ-\nmental scenes to test across 300 cross-domain sce-\nnarios. The variation in instructions, akin to the\napproach used in SayCan, is crafted to assess the\nextent to which our framework can deconstruct ab-\nstract instructions and ground skills in the target\ndomain. We utilize 4 instruction types such as Ab-\nstract Noun, Abstract Verb, Structured, and Long\nHorizon, comprising a total of 137 instructions.\nThe Abstract Noun and Abstract Verb cate-\ngories are created to examine whether SemGro can\nexecute the appropriate sequences when given ab-\nstractions are in nouns and verbs. Structured Lan-\nguage, paralleling Abstract Verbs, aims to evaluate\nSemGro's performance with well-structured lan-\nguages. Long Horizon, necessitating multi-step\nreasoning, is designed to evaluate SemGro's ca-\npability to process and reason through temporally\nextended instructions.\nVirtualHome simulation. We use Virtual-\nHome (Puig et al., 2018), an interactive simulation\nplatform for complex household tasks. The skills\nprovided in the environment include manipulation\nactions such as picking up objects and toggling\nappliances on or off. We impose restrictions on\nthese actions within the environment to align with\nour settings; the skills performed are implemented\nto be feasible only with the objects currently\nvisible. For example, \"grab apple\" is executable\nwhen the apple is closed enough and is visible to\nthe agent. The action space of the VirtualHome\nexperiments is provided in Table 8."}, {"title": "LMs for task planner and skill critic", "content": "To imple-\nment the task planner \u00a2 = ($G, \u03a6R) and the LM-\npart VLM of the skill critic, we test a range of LMs\nfrom open-source LLaMA-2-70B to proprietary ca-\npable LMs such as PaLM, GPT-3.5, and GPT-4.\nIn Table 7, while LLaMa-2-70B, with significantly\nfewer parameters, does not achieve comparable\nperformance, SemGro demonstrates robust perfor-\nmance consistently with capable LM backbones."}, {"title": "Related Works", "content": "Language models for embodied control. The use\nof pretrained LMs for embodied control tasks has\nbeen increasingly prevalent. A significant focus of\nthis research is on bridging the gap between LMs'\nextensive knowledge and their lack of inherent\nunderstanding of embodied experiences. Several\nworks tackle the challenge by directly incorporat-\ning environmental contexts to LMs through textual\nprompts (Xiang et al., 2023; Lin et al., 2023b,a;\nShinn et al., 2023; Carta et al., 2023), based on\nthe premise that observations can be described in\ntext form. For example, (Lin et al., 2023b) utilizes\na text-based table to display objects' location and\ntheir relation. Expanding beyond the mere textual\nenvironment, other works have pivoted towards vi-\nsual tasks and EIF (Wang et al., 2023; Song et al.,\n2023; Driess et al., 2023; Wu et al., 2023; Singh\net al., 2023). Particularly, (Brohan et al., 2023c)\nintroduces a method to learn skill affordance from\nvisual observations using an offline RL dataset\ncollected from the specific environment. This ap-\nproach integrates skill logits from an LM with iden-\ntified skill affordance to select the most valuable\nskill for execution. Meanwhile, (Wu et al., 2023;\nSong et al., 2023) use detected object classes in\nthe observation to refine the selection of actionable\nskills.\nOur SemGro distinguishes itself in EIF by be-\ning the first to harness the capabilities of LMs for\ngrounding pretrained skills in cross-domains.\nLanguage conditioned policy. A number of\nworks have concentrated on the generalization\nof language-conditioned policies or skills for un-\nseen language instructions (Guhur et al., 2022;\nGarg et al., 2022), visual attributes (Brohan et al.,\n2023b,a; Stone et al., 2023; Shridhar et al., 2022,\n2021), or environment dynamics (Shin et al.,\n2023), with some extending to multi-modal instruc-\ntions (Li et al., 2023b; Shah et al., 2023; Shin et al.,\n2024; Jiang et al., 2022; Jang et al., 2022). For in-\nstance, (Brohan et al., 2023a) adapts a pretrained\nfoundation model to robot trajectory datasets in an"}, {"title": "Conclusion", "content": "In this paper, we presented a semantic skill ground-\ning framework SemGro, which supports the adap-\ntation of a broad range of skills in cross-domain en-\nvironments. SemGro harnesses the reasoning capa-\nbilities of LMs not only to compose semantic skills\nfor establishing a hierarchical skill database but\nalso to decompose semantic skills into executable\nlow-level skills. This novel approach uniquely ex-\nplores the trade-off between semantic richness and\ndomain-agnostic executability of semantic skills.\nWe hope our work provides valuable insights into\nthe development and application of real-world EIF\nagents."}, {"title": "Limitations", "content": "Despite the strong performance of SemGro, we\nidentify several limitations for improvement. (i)\nSingle-step visual observation for skill feasibil-\nity: Currently, the skill feasibility is assessed based\non a single observation. This approach can be im-\nproved by integrating a VLM into our framework\nthat is capable of generating imaginary observa-\ntions, at the expense of increased resource usage by\nthe embodied agent. By employing Monte Carlo\nTree Search, the feasibility of skills could then be\ncomputed across multiple planning paths. Address-\ning this enhancement remains for our future work.\n(ii) Unidirectional decomposition strategy:\nThe strategy for semantic skill decomposition is\nunidirectional, proceeding only in a downward di-\nrection. This may often restrict the framework as\nit cannot revert to initiating planning from a high-level skill at an appropriate juncture. Introducing\nbidirectionality in task planning could improve the\nversatility of SemGro. In scenarios with substantial\ndomain shifts, such bidirectionality would allow\nfor completely novel planning that has not been"}, {"title": "Ethics Statement", "content": "This work introduces a technology that can impact\nthe application of robotics in household tasks. This\ntechnology signals a major shift in the human ex-\nperience, as household tasks increasingly become\nautomated by robotic agents. The deployment of\nrobotic agents in household tasks presents both sig-\nnificant benefits and challenges. On the positive\nside, this technology can save time and effort for\nusers, leading to increased convenience and effi-\nciency in managing household chores. However, it\nalso raises important ethical considerations related\nto labor displacement. As robotic agents take over\ntasks traditionally performed by humans, there is a\npotential impact on the livelihoods of current ser-\nvice providers who may need to seek alternative\nsources of income. This transition necessitates care-\nful consideration and planning to mitigate adverse\neffects on workers and to ensure that the benefits\nof technological advancement are equitably dis-\ntributed. By acknowledging and addressing these\nsocietal impacts, our research aims to contribute\npositively to the field of robotics while fostering\nan inclusive approach that considers the broader\nimplications for society."}, {"title": "Acknowledgements", "content": "We would like to thank anonymous reviewers for\ntheir valuable comments and suggestions. This\nwork was supported by Institute of Information\n& communications Technology Planning & Eval-\nuation (IITP) grant funded by the Korea govern-\nment (MSIT) (No. 2022-0-01045, 2022-0-00043,\n2019-0-00421, 2020-0-01821), and by the National\nResearch Foundation of Korea (NRF) grant funded\nby MSIT (RS-2023-00213118)."}, {"title": "Implementation Details", "content": "In this section, we present the implementation de-\ntails of each baseline method and our framework\nSemGro."}, {"title": "LLM-Planner", "content": "The LLM-Planner (Song et al., 2023) employs tem-\nplatized semantic skills, k-nearest neighbors (kNN)\nretrievers, and a Language Model(LM) planner to\ngenerate plans for embodied tasks. The semantic\nskills are defined through templates which then\nare matched with the objects visible in the en-\nvironment; this allows for instant skill construc-\ntion options based on the specific situation ob-\nserved. The LLM-Planner initially retrieves in-\ncontext examples from the training data, utilizing\nk-nearest neighbors (kNN) retrievers, which are\nthen prompted in the LM planner. Subsequently,\nthe planner merges these skill templates with cur-\nrently visible objects to determine the skill that is\nboth achievable and capable of completing the task.\nWe use the open-source code from the Ilm-\nplanner official github. Due to the lack of exist-\ning training datasets in VirtualHome, we exploit\ndata from the primitive level of our skill database\n(in SemGro) as the training data for the retriever\nto generate in-context examples. We use GPT-3.5\nturbo for the planner and a BERT-base-uncased\nmodel from the Huggingface Transformers library\nfor the KNN retriever."}, {"title": "SayCan", "content": "SayCan (Brohan et al., 2023c) employs an LM plan-\nner, pre-defined semantic skills, and an affordance\nvalue function to generate a feasible skill plan for\nuser instructions. The LM planner is responsible\nfor finding the suitable skill from the skill set to\nachieve the given task using the LM log probability\nbetween the human instruction and the pre-defined\nset of skills. Then, the affordance score of each skill\nin the skill set is calculated using the pre-trained\naffordance value function. Saycan combines two\nscores and uses them to choose the appropriate\nfeasible skill to achieve the task.\nWe implement SayCan in a similar way as LLM-\nPlanner. First, in-context examples are generated\nusing kNN retrieval from our framework. The re-\ntrieved in-context examples are then prompted to\nthe LM planner. We found that the skill generation\nis dependent on the in-context prompts. To facil-\nitate the LM Planner in producing skills that are"}, {"title": "ProgPrompt", "content": "ProgPrompt (Singh et al., 2023) employs a human-\ndesigned programming assertion syntax to check\nthe pre-conditions for executing skills and ad-\ndresses failures by initiating predefined recovery\nskills. In detail, ProgPrompt uses a plan structure\nconsisting of comments, actions, and assertions\nin the style of programming language. Comments\nare used to cluster high-level skills together and\nactions are expressed as imported function calls.\nAssertions are used to check conditions right be-\nfore actions start and execute the recovery skills if\nneeded.\nWe use the code from the ProgPrompt official\ngithub. We use the same plan templates as em-\nployed by ProgPrompt, featuring a Pythonic style\nwhere the task name is designated as a function,\navailable actions are included through headers, ac-\ncessible objects are specified in variables, and each\naction is delineated as a line of code within the\nfunction. Similar to SayCan implementation, we\nalso use knn retrieval to generate in-context exam-\nples for the LM Planner. We use GPT-3.5 turbo for"}, {"title": "SemGro", "content": "Our SemGro framework comprises two primary\ncomponents: the task planner $, which is com-\nposed of the skill generator \u00d8G and task retriever\nOR, and the multi-modal skill critic 4. We employ\nthe KNN retriever to extract k in-context examples\nfrom the hierarchical skill database using the in-\nstruction i and current visual observations Ot. These\nk in-context examples' low-level skill plans serve\nas candidates for the skill generator G.\nThe execution feasibility of the planned skill\nis then assessed by the multi-modal skill critic \u03c8\nbased on the current observation. Within the critic,\nthe Vision-Language Model (VLM)-part utilizes\nvisual observations from the environment to infer\ndetected objects and their states. Leveraging this\ninferred information, the LM-part of the critic as-\nsesses the execution feasibility of the planned skill.\nIf the skill is assessed as executable, the skill is\nperformed as it is; otherwise, we prompt the feed-\nback from the critic along with the one-step further\nlower-level skill candidates to generate alternative,\nmore fine-grained instructions. Utilizing the revised\ninstructions, the database is queried for skills at a\nlower abstraction level to reinitiate the skill ground-\ning process. An example of detailed process of our\nframework is in Table 10.\nThis iterative feedback mechanism ensures that\nthe agent can adaptively refine its actions to better\nalign with the dynamic conditions of its target phys-\nical environment, thereby enhancing the overall ef-\nfectiveness and precision of skill execution. In the\ndevelopment of our skill generator, we utilize GPT-\n3.5 Turbo. For the multi-modal critic, we employ\nInstructBLIP as the VLM-part, and GPT-3.5 as the"}]}