{"title": "Semantic Skill Grounding for Embodied Instruction-Following in Cross-Domain Environments", "authors": ["Sangwoo Shin", "Seunghyun Kim", "Youngsoo Jang", "Moontae Lee", "Honguk Woo"], "abstract": "In embodied instruction-following (EIF), the integration of pretrained language models (LMS) as task planners emerges as a significant branch, where tasks are planned at the skill level by prompting LMs with pretrained skills and user instructions. However, grounding these pretrained skills in different domains remains challenging due to their intricate entanglement with the domain-specific knowledge. To address this challenge, we present a semantic skill grounding (SemGro) framework that leverages the hierarchical nature of semantic skills. SemGro recognizes the broad spectrum of these skills, ranging from short-horizon low-semantic skills that are universally applicable across domains to long-horizon rich-semantic skills that are highly specialized and tailored for particular domains. The framework employs an iterative skill decomposition approach, starting from the higher levels of semantic skill hierarchy and then moving downwards, so as to ground each planned skill to an executable level within the target domain. To do so, we use the reasoning capabilities of LMs for composing and decomposing semantic skills, as well as their multi-modal extension for assessing the skill feasibility in the target domain. Our experiments in the VirtualHome benchmark show the efficacy of SemGro in 300 cross-domain EIF scenarios.", "sections": [{"title": "1 Introduction", "content": "In the area of embodied instruction-following (EIF), the quest to develop systems capable of understanding user instructions and achieving tasks in a physical environment has been a pivotal aspiration for general artificial intelligence. Recent advancements in language models (LMs) have demonstrated their extensive and impressive application in EIF, where LMs are employed as task planners, drawing on their commonsense knowledge of the physical world (Yang et al., 2023; Padmakumar et al., 2023; Pantazopoulos et al., 2023; Yun et al., 2023; Logeswaran et al., 2022).\nThe process of LM-based task planning in EIF unfolds via a two-stage pipeline: (i) Interpretable skills (i.e., semantic skills) are acquired, and (ii) given a user instruction, the LM is prompted with the instruction and the skill semantics to identify appropriate skills to perform. To ensure that such planned skill can be executed within the domain context the agent is placed in, several works have focused on incorporating environmental observations into task planning (Singh et al., 2023; Song et al., 2023; Xiang et al., 2023; Lin et al., 2023b,a; Shinn et al., 2023; Carta et al., 2023). Yet, EIF in cross-domain environments has not been fully explored. In cross-domain settings, the complexity goes beyond logical task planning by LMs; it necessitates the adaptation of pretrained skills to align with the specific characteristics and limitations of the target physical environment. This adaptation challenge stems from the inherent domain specificity of semantic skill acquisition, calling for a strategy that disentangles these learned behaviors from their original domain contexts and reapplies them in new domains.\nIn response to this challenge, we present a semantic skill grounding framework SemGro. We observe a trade-off between semantic depth and domain-agnostic executability within the skill hierarchy. High-level skills, such as \u201cprepare a meal\u201d, embody a richer semantic content and long-horizon behavior compared to low-level skills, such as \u201cgrab a cup\u201d. However, these high-level skills tend to be effective only within specific domains, as their execution heavily depends on a multitude of environmental factors, unlike their low-level counterparts which can be universally applicable across domains. SemGro capitalizes on this insight by employing an iterative skill grounding scheme, designed to progressively decompose pretrained skills to fit in the cross-domain EIF environment as well as to fully utilize the reasoning strengths of LMs.\nTo be specific, the iterative mechanism of SemGro involves two procedures: (i) task planning with skills that align with a given instruction (in Section 3.2) and (ii) assessing the executability of planned skills (in Section 3.3). In SemGro, a task planner employs in-context learning of LMs to interpret a given instruction and generate a series of semantic skills for the instruction through iterative skill grounding and decomposition. Concurrently, a skill critic employs the multi-modal extension of LMs to evaluate the applicability and execution potential of these skills within target domains. When the critic deems a planned skill as unsuitable for execution, the planner responds by generating an alternative, detailed instruction. This instruction is intended to be carried out through a sequence of lower-level skills that align with the previously planned skill. Through the collaborative interaction between the task planner and the skill critic, SemGro navigates the semantic hierarchy of skills to establish the best trade-off between the logical relevance of skills to a given instruction and the skill feasibility in cross-domain environments.\nUsing the VirtualHome benchmark (Puig et al., 2018) to evaluate EIF agents (Singh et al., 2023; Hazra et al., 2023; Chen et al., 2023; Xiang et al., 2023), we demonstrate the advantages of SemGro for cross-domain EIF scenarios. For instance, SemGro outperforms the state-of-the-art task planning framework LLM-Planner (Song et al., 2023) by 23.97% in task completion performance.\nOur contributions are three-fold: (i) We present a novel framework SemGro to address the issue of cross-domain skill grounding in EIF. (ii) We devise an iterative skill grounding algorithm, leveraging the reasoning capabilities of LMs and their multi-modal extension. (iii) We conduct an intensive evaluation of SemGro in 300 cross-domain scenarios of the VirtualHome benchmark, complemented by ablation studies to explore the efficacy of SemGro."}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Language Models for Task Planning", "content": "For complex embodied tasks, prior works have leveraged the notion of semantic skills, which represent interpretable patterns of expert behavior. These skills have a wide spectrum, ranging from short-horizon basic low-level skills (e.g., \u201cstir the drink\u201d) to long-horizon high-level skills (e.g., \u201cprepare a meal\u201d). These diverse skills $\\Pi_l = {\\pi_l | l : \\text{skill semantic}}$ can be efficiently established within a training environment, through deterministic algorithms, reinforcement learning (RL), or imitation learning. The integration of semantic skills and the reasoning capabilities of LMs enables the zero-shot adaptation of EIF agents, without additional environment interactions or data collection efforts (Song et al., 2023; Singh et al., 2023; Brohan et al., 2023c). In these approaches, an LM is prompted with the semantics of pretrained skills $L_l = {l : \\pi_l \\in \\Pi}$ along with a specific instruction $i$ from the user. Subsequently, the LM produces a series of semantic skills. The execution of these skills facilitates the decomposition of the complex task associated with instruction $i$ into smaller, achievable skill-level subtasks."}, {"title": "2.2 Cross-Domain Skill Grounding", "content": "While several works have explored the use of LMs as task planners, they primarily focus on single-domain agent deployment, wherein skills are both acquired and used within the same environment setting. In contrast, the cross-domain setting presents unique challenges for embodied task planning, as the feasibility of skill execution in the environment differs, necessitating the adaptation of pretrained skills to the specific domain contexts of the target environment. Consider the skill \"Make a cup of coffee\", as illustrated in Figure 1, learned in a high-tech kitchen environment equipped with voice-activated appliances, a digital inventory system, and automated utensil handlers. In this environment, the skill entails complex interactions with technology to efficiently prepare a cup of coffee. However, when a robot is deployed in a traditional kitchen devoid of such advanced features, it encounters a significantly different domain context. The skill \"Make a cup of coffee\", initially tailored for the environment with high-tech gadgets, now faces significant execution challenges due to the unavailability of similar technological supports.\nTo address this gap, we introduce cross-domain skill grounding, a process aimed at adapting pretrained skills $\\pi_l \\in \\Pi$ for diverse domains. This process involves not only reinterpreting the original skill but also deconstructing it into a series of achievable lower-level subtasks, such as \u201cBoil water\u201d, \u201cAdd coffee to cup\u201d, \u201cPour water\u201d, and \u201cStir the drink\u201d. Through cross-domain skill grounding, we ensure that embodied agents can perform a vast set of skills within domains that differ significantly from those in which the skills were initially learned."}, {"title": "3 Approaches", "content": "We utilize the intrinsic semantic hierarchy of skills, a structure derived from their compositional nature; high-level semantic skills are established by chaining short-horizon low-level skills. High-level skills, characterized by rich semantics, closely align with the abstract nature of user instructions. This alignment in abstraction levels facilitates the task planning process, providing the LM with skills that are directly beneficial for interpreting and accomplishing the instructions. However, the execution of these long-horizon high-level skills is contingent upon the precise alignment of various environmental and domain-specific conditions. This precludes their adaptability in cross-domain settings. In contrast, low-level skills exhibit a robust capacity for execution that is not as tightly bound to specific domain contexts, offering greater adaptability across diverse domains. Nonetheless, the sheer volume of low-level skills and their limited semantic depths pose additional challenges for task planning.\nThe core of SemGro lies in the hierarchical task planning scheme via iterative skill grounding. The framework initiates task planning with high-level rich-semantic skills, and then progressively decomposes each of these planned high-level skills into a series of low-level skills executable within the domain of the target environment. This can be seen as finding an optimal intermediate semantic representation for skills pertinent to a given instruction, exploring a balance between their logical relevance and actual feasibility in the target environment. Such a balance ensures that the planned skills not only match the conceptual demands of the given instruction but are also effective within the limitations and resources of the target environment."}, {"title": "3.1 Overall Framework", "content": "As illustrated in Figure 2, SemGro is structured with (a) a task planner and (b) a multi-modal skill critic, which collaboratively ground skills for user instructions in cross-domains. At each skill-step, the task planner produces an appropriate semantic skill by prompting an LM with information retrieved from a skill database. The skill critic then evaluates each skill from the skill planner by prompting a multi-modal LM with visual observations from the environment. When evaluated as unsuitable, the task planner decomposes the skill into smaller, more general subtasks using another retrieval from the database. Through this iterative process, the framework systematically assembles a sequence of semantic skills, executable within the domain context of the target environment."}, {"title": "3.2 LM-based Task Planner", "content": "For LM-based task planners, the development of effective prompts is essential to fully harness the reasoning capabilities of LMs. We consider two aspects: (i) selecting skill candidates that facilitate the skill-level task decomposition and (ii) incorporating in-context examples applied to similar instructions. To capture these aspects, we construct a hierarchical semantic skill database, leveraging the reasoning prowess of LMs. Integrated with a kNN retriever, this database not only assists in identifying suitable high-level skill candidates for task planning but also facilitating the retrieval of pertinent in-context examples that can further aid the planning process. The task planner, enriched with this selection of skill candidates and in-context examples, ensures the application of suitable skill semantics in cross-domains.\nHierarchical semantic skill database. For the hierarchical semantic skill database D, we use a structured approach to semantic skill training, similar to (Zhang et al., 2023). The skill acquisition initiates with a set of short-horizon basic low-level skills $\\Pi_1 = {\\pi_1^1,..., \\pi_1^{N_1}}$ in the training environment, which can be acquired via RL or imitation learning. Under the LM guidance, the agent produces skill chains from $\\Pi_1$, achieving high-level long-horizon skills $\\Pi_M$. This bootstrapping process iteratively expands to form a comprehensive set of skill levels $\\Pi_l = {\\Pi_1,..., \\Pi_M}$. This collection, encompassing skills from low-level to high-level, is used to establish the skill database D:\n$\\Pi^m = {\\pi^{m,1}, ..., \\pi^{m,N_m}}$\nD := {e^{m,n} : 1 < m < M, 1 \\le n \\le N_m}\ne^{m,n} := (l^{m,n}, d_n(e^{m,n}), p(e^{m,n}))\nl^{m,n} := \\text{Skill semantic of } \\pi^{m,n}\np(e^{m,n}) := (e^{m-1,n_{j1}},...).$\n(1)\nThe entry $e^{m,n}$ of D includes the skill semantic $l^{m,n}$ of $\\pi^{m,n}$, detected object names $d_n(e^{m,n})$ collected during training of $\\pi^{m,n}$, and a one-step low-level semantic skill planning $p(e^{m,n}) = (e^{m-1,n_{j1}}, ...)$, in that the skill $\\pi^{m,n}$ is acquired by chaining of $(\\pi^{m-1,n_{j1}}, ...)$. To train the skill critic (Section 3.3), we also establish a dataset:\n$D_0 = {o_t^{(TR)}, d_n(o_t^{(TR)}), d_s(o_t^{(TR)}) : t}$ (2)\nof visual observations of $o_t^{(TR)}$ in the training environment, detected object names $d_n(o_t^{(TR)})$ and object physical states $d_s(o_t^{(TR)})$ at each timestep $t$ during the establishment of $\\Pi_1$. To obtain such object names (e.g., \"microwave\") and physical states (e.g., \"microwave: closed\"), we use an open-vocabulary detector tailored for the training environment.\nSkill generator. For a given user instruction $i$ and target environment observation $o_t$, the LM-based skill generator $\\phi_G$ integrates skill candidates $c(i, o_t)$, in-context examples $x(i, o_t)$, and the skill execution history $h$ to generate the skill semantic $\\bar{l}$ which is most likely to be executed at the present:\n$\\phi_G : (i, h, x(i, o_t), c(i, o_t)) \\leftrightarrow \\bar{l} \\in c(i, o_t).$ (3)\nTo derive these in-context examples and skill candidates, we use a kNN retriever that selects entries $e(i, o_t) = {e^{m_1,n_1}, ..., e^{m_k,n_k}}$ in D. The retrieval process calculates similarity by combining the instruction $i$ and the visual context of $o_t$ into a single query. It evaluates $(i, l^{m,n}) + (d_n(e^{m,n}), o_t)$ to determine the k entries with the highest similarity scores, where $(\u00b7,\u00b7)$ denotes the cosine similarity. Here, $l^{m,n}$ is the skill semantic"}, {"title": "Algorithm 1 Iterative Skill Grounding for EIF", "content": "of LM's multi-modal extension with VLMs in cross-domains.\nThe iterative skill grounding scheme of SemGro is illustrated in Algorithm 1."}, {"title": "4 Experiments", "content": "Environment. We use VirtualHome, a realistic household activity simulator and benchmark. For EIF agents, we define 3949 low-level skills (i.e., $\\Pi_1$ in Section 3.2) based on the combination of behaviors (e.g., \u201cgrasp\u201d) and objects (e.g., \"apple\"). For cross-domain evaluations, we use 15 distinct environment settings, categorized by one of domain shift types: OL involves changes in the placements and locations of objects; PA presents changes in the physical attributes of objects; RS is characterized by different room structures and visual attributes. These cross-domain contexts necessitate the refinement of certain pretrained high-level skills into low-level skills for successful execution. Consider a high-level skill \u201ccook salmon\u201d intended to fulfill the user instruction \u201cprepare delicious food for me\u201d. If this skill was trained in a setup where salmon is always prepared on a cutting board, it might not work in OL cross-domain scenarios where salmon is instead found inside a refrigerator. In this case, it becomes necessary to decompose the skill into a sequence of lower-level executable skills such as \u201cgo to the refrigerator\u201d, \u201cgrasp the refrigerator handle\u201d, among others, ensuring the skill be adapted to the new domain context.\nInstructions. Given 5 evaluation tasks, we use 4 different instruction types per task: Abstract Nouns and Abstract Verbs focus on instruction queries involving abstract nouns and verbs, respectively; Structured language queries include precise verbs; Long-Horizon queries involve multiple reasoning steps. These instruction types are used to test various linguistic aspects of task interpretation and execution, similar to (Brohan et al., 2023c). By combining 15 environment settings, 5 tasks, and 4 types of instructions, we establish a comprehensive set of 300 distinct unseen cross-domain scenarios.\nEvaluation metrics. We use several evaluation metrics, consistent with previous works (Singh et al., 2023; Song et al., 2023). SR measures the percentage of successfully completed tasks, where a task is considered to be completed if its all goal conditions are completed; CGC measures the percentage of completed goal conditions; Plan measures the percentage of the planned skill sequence that continuously aligns with the ground-truth sequence from the beginning.\nBaselines. We compare our SemGro with three baselines, each leveraging an LM as a task planner but with distinct approaches to filter executable skill candidates, including SayCan (Brohan et al., 2023c) which employs an offline RL dataset collected from the training environment to learn an affordance function; LLM-Planner (Song et al., 2023) which leverages templatized semantic skills; ProgPrompt (Singh et al., 2023) which uses human-crafted programmatic assertion syntax to verify the pre-conditions of skill execution and respond to the failed cases with recovery."}, {"title": "4.1 SemGro Implementation", "content": "For the KNN retriever, we configure k to be 10 and utilize the sentence-transformer (Reimers and Gurevych, 2019) to compute the similarity between the instruction and skill semantics in the database. The task planner $\\phi$ and the LM-part $\\psi_{LM}$ of the skill critic are both implemented using GPT-3.5 API with the temperature hyperparameter set to 0. For fair comparisons, the same GPT-3.5 configuration is used across all baseline task planners. More details, including the baseline implementation, are in the Appendix."}, {"title": "4.2 Performance", "content": "Overall performance. Table 1 compares the cross-domain performance, achieved by our SemGro and the baselines (LLM-Planner, SayCan, Prog-Prompt). SemGro outperforms the baselines in all cross-domain scenarios, achieving average performance gains of 25.02%, 26.83%, and 27.65% over the most competitive baseline SayCan in SR and CGC, and LLM-Planner in Plan, respectively. The exceptional performance of SemGro highlights its superiority in hierarchical task planning via iterative skill grounding across unseen domains.\nInterestingly, for long-horizon instruction scenarios, there are several instances where SR and CGC metrics exceed Plan. In the PA cross-domain context, SemGro achieves Plan of 61.11%, while SR and CGC reach 62.96% and 74.07%, respectively. This phenomenon, reminiscent of solving math problems in an NLP context (Lightman et al., 2023; Fu et al., 2023; Imani et al., 2023), suggests that tasks requiring multiple reasoning steps can be accomplished through diverse skill sequences.\nExecutable skill identification. We investigate the efficacy of SemGro in generating executable skills. To this end, we introduce the Exec metric, which quantifies the percentage of the planned semantic skills that are executable in a given domain, regardless of their impact on task completion. In Table 2, SemGro surpasses the most competitive baseline ProgPrompt by 17.99% in Exec at average. This specifies our framework's proficiency in discerning the varying conditions of cross-domain environments for skill grounding. This ability is attributed to the skill critic which uses the domain-agnostic representation capabilities of the adopted VLM.\nIterations for cross-domain skill grounding. We investigate the number of iterations used for skill grounding in the target domain across varying degrees of domain shift. To quantify the degree of shift, we calculate the number of high-level skills that must be decomposed into different series of low-level skills for execution in the target domain, compared to the training environment. The degree of shift is categorized into 4 levels. 'None' indicates no domain shift, while 'Small,' 'Medium,' and 'Large' denote increasing degrees of shift. The detailed criterion for quantifying domain shift degrees can be found in Appendix B. To accurately measure the iterations induced by domain shift, distinctions are made between skills rendered inexecutable due to the immediate observational state (Obs. & Dom. in the row name) and those affected by domain disparities (Dom.). Table 3 reveals a positive correlation between the domain shift degrees and the number of iterations. This specifies the benefits of SemGro in handling cross-domain settings with varying degrees of domain shift, progressively searching the appropriate abstraction level of skills."}, {"title": "4.3 Ablation Studies", "content": "Skill hierarchy for planning. To verify the effectiveness of the iterative skill grounding scheme, we conduct task planning with the fixed abstraction level of skills, bypassing the executability check of the skill critic. In Table 4, SG-L and SG-H employ the lowest-level skills $\\Pi^1$ and highest-level skills $\\Pi^M$ for task planning, respectively, while SG-M employs middle-tier skills {$\\Pi_m : 1 < m < M$}. As shown, SG-H exhibits the most accurate task planning (high Plan), while its planned skills are rarely executable (low Exec) due to domain shift. Conversely, SG-L, equipped with potentially executable skills, shows unreliable task planning due to the abstraction level mismatch with the instruction. Although SG-M employs skills with moderate semantic content and executability by adhering to a fixed abstraction level, it falls short in performance. SemGro adaptively identifies skill semantics at the intermediate level, through iterative downward decomposition within the semantic hierarchy of skills. This strategy not only facilitates task planning but also achieves robust executions in new domains.\nIn-context example retrieval. In Table 5, we conduct an ablation study on the kNN retriever by varying the number k of retrieved entries in Eq (4). Furthermore, we test a random-selection retriever that selects 10 examples at random (Random in the column name) as well as another variant that calculates the similarity based solely on instructions without observation information (k = 10 \\O in the column name). For the random-selection retriever, skill candidates are determined using the kNN retriever to precisely assess the impact of in-context examples. As observed, selecting an appropriate number of examples (k = 10) leads to performance gain in Plan by 30.89% and 7.87% compared to the random retriever and k = 10 \\O, respectively. Furthermore, we observe an increase in planning performance when k ranges from 5 to 10, but beyond this range, larger values do not yield further enhancements. This implies that including skill candidates unrelated to the task may deteriorate the task planning.\nThe dual-model structure for skill critic. As explained in Section 3.3, we structure the skill critic $\\psi$ with two models VLM and VLM. To validate this dual-model structure, we conduct an ablation study using a single VLM for the skill critic. Specifically, we use InstructBLIP (Dai et al., 2023), a state-of-the-art general-purpose VLM, alongside its fine-tuned counterpart PG-InstructBLIP (Gao et al., 2023), which is optimized for object-centric physical understanding. We fine-tune these VLMs on the VirtualHome dataset. We provide the dataset collection process in the Appendix. In Table 6, the dual-model structure for $\\psi$ improves the performance in Exec by 9.62% over the singular VLM. This specifies that while VLMs are adept at assessing physical attributes, their capacity to discern the executability of semantic skills in diverse physical environments can be more effectively augmented by the inclusion of LMs.\nLMs for task planner and skill critic. To implement the task planner $\\phi = (\\phi_G, \\phi_R)$ and the LM-part $\\psi_{LM}$ of the skill critic, we test a range of LMs from open-source LLaMA-2-70B to proprietary capable LMs such as PaLM, GPT-3.5, and GPT-4. In Table 7, while LLaMa-2-70B, with significantly fewer parameters, does not achieve comparable performance, SemGro demonstrates robust performance consistently with capable LM backbones."}, {"title": "5 Related Works", "content": "Language models for embodied control. The use of pretrained LMs for embodied control tasks has been increasingly prevalent. A significant focus of this research is on bridging the gap between LMs' extensive knowledge and their lack of inherent understanding of embodied experiences. Several works tackle the challenge by directly incorporating environmental contexts to LMs through textual prompts (Xiang et al., 2023; Lin et al., 2023b,a; Shinn et al., 2023; Carta et al., 2023), based on the premise that observations can be described in text form. For example, (Lin et al., 2023b) utilizes a text-based table to display objects' location and their relation. Expanding beyond the mere textual environment, other works have pivoted towards visual tasks and EIF (Wang et al., 2023; Song et al., 2023; Driess et al., 2023; Wu et al., 2023; Singh et al., 2023). Particularly, (Brohan et al., 2023c) introduces a method to learn skill affordance from visual observations using an offline RL dataset collected from the specific environment. This approach integrates skill logits from an LM with identified skill affordance to select the most valuable skill for execution. Meanwhile, (Wu et al., 2023; Song et al., 2023) use detected object classes in the observation to refine the selection of actionable skills.\nOur SemGro distinguishes itself in EIF by being the first to harness the capabilities of LMs for grounding pretrained skills in cross-domains.\nLanguage conditioned policy. A number of works have concentrated on the generalization of language-conditioned policies or skills for unseen language instructions (Guhur et al., 2022; Garg et al., 2022), visual attributes (Brohan et al., 2023b,a; Stone et al., 2023; Shridhar et al., 2022, 2021), or environment dynamics (Shin et al., 2023), with some extending to multi-modal instructions (Li et al., 2023b; Shah et al., 2023; Shin et al., 2024; Jiang et al., 2022; Jang et al., 2022). For instance, (Brohan et al., 2023a) adapts a pretrained foundation model to robot trajectory datasets in an end-to-end manner, by representing robot actions as text tokens. (Li et al., 2023b; Jiang et al., 2022) offer a unified framework capable of achieving interleaved vision-language instructions, employing a set of pretrained models to encode them.\nWhile prior works emphasize the robust policy structure, SemGro leverages an LM-based iterative reasoning mechanism, aiming for balanced skill grounding across diverse domain shifts. Moreover, SemGro addresses realistic EIF tasks with the VirtualHome benchmark, unlike the prior works that often focus on straightforward manipulation tasks."}, {"title": "6 Conclusion", "content": "In this paper, we presented a semantic skill grounding framework SemGro, which supports the adaptation of a broad range of skills in cross-domain environments. SemGro harnesses the reasoning capabilities of LMs not only to compose semantic skills for establishing a hierarchical skill database but also to decompose semantic skills into executable low-level skills. This novel approach uniquely explores the trade-off between semantic richness and domain-agnostic executability of semantic skills. We hope our work provides valuable insights into the development and application of real-world EIF agents."}, {"title": "7 Limitations", "content": "Despite the strong performance of SemGro, we identify several limitations for improvement. (i) Single-step visual observation for skill feasibility: Currently, the skill feasibility is assessed based on a single observation. This approach can be improved by integrating a VLM into our framework that is capable of generating imaginary observations, at the expense of increased resource usage by the embodied agent. By employing Monte Carlo Tree Search, the feasibility of skills could then be computed across multiple planning paths. Addressing this enhancement remains for our future work.\n(ii) Unidirectional decomposition strategy: The strategy for semantic skill decomposition is unidirectional, proceeding only in a downward direction. This may often restrict the framework as it cannot revert to initiating planning from a high-level skill at an appropriate juncture. Introducing bidirectionality in task planning could improve the versatility of SemGro. In scenarios with substantial domain shifts, such bidirectionality would allow for completely novel planning that has not been trained to achieve complex, unseen tasks. Enhancing our framework to incorporate this bidirectional planning presents a challenge but crucial direction for future development."}, {"title": "8 Ethics Statement", "content": "This work introduces a technology that can impact the application of robotics in household tasks. This technology signals a major shift in the human experience, as household tasks increasingly become automated by robotic agents. The deployment of robotic agents in household tasks presents both significant benefits and challenges. On the positive side, this technology can save time and effort for users, leading to increased convenience and efficiency in managing household chores. However, it also raises important ethical considerations related to labor displacement. As robotic agents take over tasks traditionally performed by humans, there is a potential impact on the livelihoods of current service providers who may need to seek alternative sources of income. This transition necessitates careful consideration and planning to mitigate adverse effects on workers and to ensure that the benefits of technological advancement are equitably distributed. By acknowledging and addressing these societal impacts, our research aims to contribute positively to the field of robotics while fostering an inclusive approach that considers the broader implications for society."}, {"title": "A Implementation Details", "content": "In this section, we present the implementation details of each baseline method and our framework SemGro."}, {"title": "A.1 LLM-Planner", "content": "The LLM-Planner (Song et al., 2023) employs templatized semantic skills, k-nearest neighbors (kNN) retrievers, and a Language Model(LM) planner to generate plans for embodied tasks. The semantic skills are defined through templates which then are matched with the objects visible in the environment; this allows for instant skill construction options based on the specific situation observed. The LLM-Planner initially retrieves in-context examples from the training data, utilizing k-nearest neighbors (kNN) retrievers, which are then prompted in the LM planner. Subsequently, the planner merges these skill templates with currently visible objects to determine the skill that is both achievable and capable of completing the task.\nWe use the open-source code from the Ilm-planner official github. Due to the lack of existing training datasets in VirtualHome, we exploit data from the primitive level of our skill database (in SemGro) as the training data for the retriever to generate in-context examples. We use GPT-3.5 turbo for the planner and a BERT-base-uncased model from the Huggingface Transformers library for the KNN retriever."}, {"title": "A.2 SayCan", "content": "SayCan (Brohan et al., 2023c) employs an LM planner, pre-defined semantic skills, and an affordance value function to generate a feasible skill plan for user instructions. The LM planner is responsible for finding the suitable skill from the skill set to achieve the given task using the LM log probability between the human instruction and the pre-defined set of skills. Then, the affordance score of each skill in the skill set is calculated using the pre-trained affordance value function. Saycan combines two scores and uses them to choose the appropriate feasible skill to achieve the task.\nWe implement SayCan in a similar way as LLM-Planner. First, in-context examples are generated using kNN retrieval from our framework. The retrieved in-context examples are then prompted to the LM planner. We found that the skill generation is dependent on the in-context prompts. To facilitate the LM Planner in producing skills that are achievable in the current observations, we provide additional task examples that are attainable within the present context. Then, the planner chooses a suitable skill from the skill candidates to accomplish the task. We use GPT-3.5 turbo for the planner.\nWe use the formulation of skill and value functions similar to LLM-Planner. In SayCan, each skill is formulated as actions that can perform a short-horizon task, such as picking up a particular object. Similarly, we use the atomic actions provided in VirtualHome, such as grab, walk, switchon, and also combine them with individual objects to formulate the set of low-level skills. Subsequently, the value function emerges as another critical element in SayCan, evaluating the feasibility of each skill in light of the present observation. Due to the challenges associated with training low-level policies in VirtualHome, we adopt the approach used by LLM-Planner, providing SayCan with Oracle object data to establish the Oracle value function. This provides SayCan with preemptive access to comprehensive knowledge about all objects and their affordance in the current observation; thus, this shrinks the list of potential skills for the LLM to consider. Consequently, this simplifies the decision-making process for the planner, facilitating the selection of a suitable skill to complete the task effectively."}, {"title": "A.3 ProgPrompt", "content": "ProgPrompt (Singh et al., 2023) employs a human-designed programming assertion syntax to check the pre-conditions for executing skills and addresses failures by initiating predefined recovery skills. In detail, ProgPrompt uses a plan structure consisting of comments, actions, and assertions in the style of programming language. Comments are used to cluster high-level skills together and actions are expressed as imported function calls. Assertions are used to check conditions right before actions start and execute the recovery skills if needed.\nWe use the code from the ProgPrompt official github. We use the same plan templates as employed by ProgPrompt, featuring a Pythonic style where the task name is designated as a function, available actions are included through headers, accessible objects are specified in variables, and each action is delineated as a line of code within the function. Similar to SayCan implementation, we also use knn retrieval to generate in-context examples for the LM Planner. We use GPT-3.5 turbo for the planner.\nUtilizing assertions to verify the pre-conditions of each action is a pivotal concept in ProgPrompt, enabling the correction of incorrect behaviors to accomplish tasks. However, generating pre-conditions for every action involves substantial effort and resources; furthermore, given the assumption that an innumerable set of skills could be integrated, manually annotating pre-conditions for each skill becomes practically infeasible. Similar to SayCan implementation, we endow ProgPrompt with Oracle pre-conditions for each action. By implementing Oracle assertions before each skill, the system automatically corrects behaviors, thus allowing the agent to perform only feasible skills to complete the task efficiently."}, {"title": "A.4 SemGro", "content": "Our SemGro framework comprises two primary components: the task planner $\\phi$, which is composed of the skill generator"}]}