{"title": "A Racing Dataset and Baseline Model for Track Detection in Autonomous Racing", "authors": ["Shreya Ghosh", "Yi-Huan Chen", "Ching-Hsiang Huang", "Abu Shafin Mohammad Mahdee Jameel", "Chien Chou Ho", "Aly El Gamal", "Samuel Labi"], "abstract": "A significant challenge in racing-related research is the lack of publicly available datasets containing raw images with corresponding annotations for the downstream task. In this paper, we introduce RoRaTrack, a novel dataset that contains annotated multi-camera image data from racing scenarios for track detection. The data is collected on a Dallara AV-21 at a racing circuit in Indiana, in collaboration with the Indy Autonomous Challenge (IAC). RoRaTrack addresses common problems such as blurriness due to high speed, color inversion from the camera, and absence of lane markings on the track. Consequently, we propose RaceGAN, a baseline model based on a Generative Adversarial Network (GAN) that effectively addresses these challenges. The proposed model demonstrates superior per-formance compared to current state-of-the-art machine learning models in track detection. The dataset and code for this work are available at github.com/RaceGAN.", "sections": [{"title": "I. INTRODUCTION", "content": "Modern vehicles are increasingly equipped with a range of computer vision technologies to assist drivers and improve road safety. A critical application of these technologies, par-ticularly for autonomous and self-driving vehicles, is lane detection, which ensures that vehicles remain within desig-nated lanes [1]. Lane detection systems not only help maintain proper lane alignment, but also provide visual cues to drivers about lane boundaries.\nSimilarly, autonomous technologies are being integrated into race cars, giving rise to the emerging field of autonomous racing. In this domain, vehicles operate entirely without human intervention, relying solely on artificial intelligence and com-puter vision algorithms [2]. In such high-stakes environments, precise and timely execution of tasks such as track detection is crucial, as there is no human driver to correct potential errors.\nAutonomous racecars are equipped with a range of sensors that deliver real-time data, enabling the vehicle to understand its environment effectively. Some common sensors include LiDAR, RADAR, GPS/GNSS, and cameras, among others [3]. While LiDAR is effective for track detection on race tracks with bounding walls, it struggles on road courses that lack such clear boundaries [2]. Road courses, often surrounded by grass or gravel, mimic real-world roads and present unique challenges for LiDAR-based systems. In addition, all sensors have inherent error margins and could malfunction, potentially disrupting navigation. These limitations highlight the need for reliable camera-based methods to detect track boundaries, which serve as affordable alternatives and augmentations of existing systems.\nDespite growing interest in autonomous racing, there is a notable lack of datasets specifically designed for racing environments. Existing traffic datasets, while useful for urban scenarios, fail to address the unique challenges of racing, such as high speeds, blurred images, and the absence of clear lane markings. These factors make it difficult for models trained on traffic data to generalize effectively to racing conditions.\nIn this paper, we focus on track detection as a key task and address these challenges through the following contributions:\n1) We introduce the Road Racing Track Dataset (Ro-RaTrack), the first open source dataset specifically designed for autonomous racing on road courses. This dataset includes image data paired with instance-level annotations in the form of segmentation masks, collected using cameras mounted at various angles on an autonomous race car. RoRaTrack captures common racing challenges, including single-lane tracks, lack of lane markings, high-speed data, and image distortion.\n2) We propose RaceGAN, a GAN-based method for track detection tailored to the unique challenges of racing environments. Through a comprehensive evaluation of eight state-of-the-art track detection methods, we demon-strate that RaceGAN significantly outperforms existing approaches, setting a new benchmark for track detection in autonomous racing."}, {"title": "II. LITERATURE REVIEW", "content": "In this section, we focus on existing work on track detection methods for autonomous racing vehicles, which is a special-ized application of segmentation in road scenes. We first re-view state-of-the-art segmentation methods, then segmentation in road scenes, and finally, track detection methods.\nSegmentation for General Tasks: YOLOv8 [12] excels in both object detection and segmentation, combining CSPDark-net for feature extraction, PANet for feature aggregation, and a U-Net inspired decoder. With cIoU and DiceLoss, it achieves state-of-the-art segmentation. However, its requirement of a large memory limits its deployment on resource-constrained devices. Hetnet [13] integrates low-level features (e.g., inten-"}, {"title": "III. RORATRACK DATASET", "content": "In this section, we provide a detailed description of the construction, characteristics, and annotations included in the RoRaTrack dataset."}, {"title": "A. Data Collection and Split", "content": "This dataset was collected using the Dallara AV-21 race car on Putnam Park Road Course, Putnam County, Indiana. The Dallara AV-21 has a Carbon Chassis with a race weight of 640-649 Kg, 335kW/450 Horsepower, Ricardo 6-speed semi-automatic gearbox, rear-wheel drive. The sensors onboard include 6 mono cameras (2 front, 2 stereo, 2 rear), 4 Radars, 3 LiDARs, and a TRK GPS. The computing platform is an Intel Xeon CPU equipped with an NVIDIA Quadro RTX 8000 GPU.\nData collected on a road course is considerably different from that collected on a motor speedway. The track layout in a motor speedway is usually oval and generally at the same elevation throughout the track, often containing lane markings for multi-vehicle racing. On the other hand, road courses like Putnam Park have multiple winding turns with some elevation differences over the course.\nThe data was collected from cameras on one run of the Dallara AV-21 vehicle on the track that took approximately 12 minutes. The vehicle is equipped with 4 video cameras -front right, front left, rear right, and rear left. The data in the RoRaTrack dataset has been collected from the front left and front right cameras. The sampling rate of the camera is 30 frames per second (fps). We down-sample the 30 fps videos to 1 fps for both left and right videos to yield 607 front left images and 791 front right images. The total number of images in the dataset is 1398 which is divided into a training and testing split of 80-20.\nWe utilize perception-based track detection to identify and map the unobstructed area in front of the vehicle, enabling safe and efficient navigation. This becomes particularly crucial during curves and turns, where the vehicle must accurately determine the available space that it can occupy and adjust its speed accordingly. For this purpose, we used data from the two front cameras to train our model."}, {"title": "B. Dataset Characteristics", "content": "In Table II, the distribution of the data is presented in the different racing scenarios. Examples of these diverse scenarios are shown in Figure 1. As a purpose-built dataset for au-tonomous racing on road courses, RaceGAN incorporates the unique characteristics present in this challenging environment. Frames for videos collected at high speed and in different weather conditions suffer from blurriness, color imbalance (over- or under-exposed), and dazzle light. Images also have saturation-related artifacts, such as a green hue. Due to these factors, only 146 images out of 1398 images can be classified as relatively normal (straight road, no color imbalances or blurriness). The data was collected on a road course that are known for their windy turns, and 378 images captured curved roads.\nTraffic datasets typically collect data from a single front-facing camera, which offers a clear view of road and lane markings. In contrast, our dataset -collected from a race car -includes views from both the right and left, providing a perspective more suited for track detection in a racing environment. Since our dataset is intended to perform track detection for the explicit purpose of racing, it does not contain any lane markings or lane boundaries. Additionally, the chassis"}, {"title": "C. Annotation", "content": "We employed a two-stage annotation approach for our dataset. Initially, 20% of the RoRaTrack dataset was manually annotated with segmentation masks. A pre-trained YOLOv8n-seg model, initially trained on the COCO dataset, was sub-sequently fine-tuned using the annotated data. In the second stage, the fine-tuned YOLOv8n-seg model was utilized to generate segmentation masks for the remaining 80% of the dataset, which served as our ground truth annotations. To further validate the accuracy of the annotations, we performed random manual inspections on the segmentation masks pro-duced by the YOLOv8n-seg model."}, {"title": "IV. PROPOSED METHOD", "content": "Our proposed RaceGAN model, illustrated in Figure 2, leverages a Deep Convolutional Generative Adversarial Net-work (DCGAN) architecture, which consists of both deep con-volutional generator and discriminator networks. This model integrates key elements of CycleGAN [28] and Wasserstein GAN [29], combining their strengths to improve performance. Specifically, the design choices are carefully tailored to opti-mize track recognition, enabling more effective learning and generation of track patterns.\nThe Generative Adversarial Network (GAN), utilized in RaceGAN, is a type of deep learning network that generates new data samples based on a target training dataset. GANs are made up of two neural networks: a generator and a discriminator. The two networks are trained simultaneously as adversaries. The goal of the generator network is to gen-erate realistic synthetic images that make it difficult for the discriminator to distinguish between real and fake images. In parallel, the discriminator is trained to distinguish real data from fake data. This adversarial process leads the generator to improve over time and ultimately generate realistic data that closely resembles the training dataset."}, {"title": "A. Generator", "content": "The generator architecture is based on the WeBACNN model (Figure 3) which has shown strong performance in track detection tasks [2]. We improve the model by augmenting it with additional computational blocks designed for deep feature extraction, as well as residual connections to enhance the generation of realistic images. Figure 2 shows the detailed construction of these two blocks.\nThe first block of the generator is tasked with making an initial prediction regarding the pixels, determining whether they belong to a region containing lane. This process requires an understanding of the composition of the image. Typically, in an image, the road surface exhibits a relatively uniform color, while non-lane regions are characterized by multiple colors. This block is designed to classify regions with diverse pixel values as non-lane areas and regions with more uniform (monochromatic) pixel values as lane regions. To achieve this, the image is first converted to grayscale. Then, two downsampling layers, each using kernels of different sizes, are applied to capture both global statistics and local details in a region. To further refine the pixel classification, a custom classification algorithm is introduced, which enhances the distinction between dark pixels (likely to be part of the lane) and bright pixels (typically representing the background).\nThe second block focuses on deep feature extraction from the initial guesses. This block consists of consecutive convolu-tion layers with different kernel sizes to capture different levels of detail. The smaller kernels facilitate the extraction of fine-grained details within localized regions of the image, whereas the larger kernels provide more contextual information about the region. Each convolutional block also contains normaliza-tion layers for better generalizability and stable training.\nThe result is then passed into the WeBACNN backbone, which makes up the last block and generates the final pre-diction. This model contains two branches for two different layers of detail. During aggregation each branch is weighted according to the section in which it is predicting the mask. For sections that require more fine details, the branch containing more localized features is assigned a greater weight to ensure a smooth reconstruction, whereas for sections that require more global context, the branch containing higher-level features is assigned a greater weight. Together, they utilize regional weights in different parts of the image to give a smoother prediction."}, {"title": "B. Discriminator", "content": "The discriminator is composed of convolutional layers, max-pooling layers, normalization, and transpose convolutions, which collectively enable deep feature extraction, similar to the second block of the generator architecture. The discrimi-nator's role is to classify each pixel as real or fake, with the average classification of all pixels being used to determine the authenticity of the image. Instead of explicitly classifying the entire image, the discriminator operates more like a 'critic,' evaluating each pixel independently. This approach leads to more refined and accurate outputs."}, {"title": "C. GAN Training", "content": "Diverging from traditional GAN architectures, RaceGAN takes an image of the track as input, rather than a random noise vector. This design choice enables the generator to produce domain-specific images. To reinforce this domain specificity, we introduce a domain transfer loss and an adversarial loss, as formulated in Equation 1, to guide the GAN training process.\n$L_{total} = L_{adv}(G, D) + \\lambda L_{domain}(G, F)$ (1)\n* $L_{adv}(G, D)$: The adversarial loss, where G is the gen-erator and D is the discriminator. This term helps the generator create realistic outputs by trying to mislead the discriminator. This loss is used to train the discriminator.\n* $\\lambda$: A hyperparameter that controls the balance between the adversarial loss and the domain transfer loss.\n* $L_{domain}(G, F)$: The domain transfer loss where G is the generator.\nThe domain transfer loss function assesses the alignment of generated data with the target domain, using an L2 norm to measure the difference between generated and real tar-get domain data. During generator training, the overall loss is a weighted combination of the adversarial loss, which incentivizes the generator to deceive the discriminator, and the domain transfer loss, which enforces the desired domain adaptation. By fixing the discriminator, the generator learns to produce realistic and domain-specific images simultaneously. In contrast, during discriminator training, the generator is kept constant, and the discriminator learns only through adversarial loss. Throughout training, the accuracy of the discriminator re-mains near 50%, indicating its inability to reliably distinguish real from fake images, characteristic of GANs in equilibrium."}, {"title": "D. Post Processing", "content": "To improve output quality, a postprocessing step is intro-duced to reduce noise in the generated results. This process involves analyzing each pixel and its neighbors, evaluating the connectivity of the pixels to remove small scattered groups,"}, {"title": "V. RESULTS", "content": "We provide the results of qualitative and quantitative anal-ysis of our model's performance compared against eight competing methods for track detection. To the best of our knowledge, there are currently no track detection models specifically trained on road racing data. Therefore, a direct comparison with specialized track detection methods for road racing data is not possible. However, WeBACNN has demon-strated success in track detection tasks for racing circuits. Additionally, several state-of-the-art methods for traffic lane detection are readily available. To establish benchmark results, we select seven traffic lane detection methods, and one track detection method (WeBACNN), and train and test them on the RoRaTrack dataset. Subsequently, we compare the results with those obtained using RaceGAN. Some methods were augmented with a post-processing thresholding method to enhance the accuracy of the detections.\nAll experiments were conducted on a computer with the following specifications: Intel Xeon Gold 6142 CPU with 64 cores, 256GB of RAM, and a Nvidia Tesla P100 GPU with 16GB of memory."}, {"title": "A. Evaluation Metrics", "content": "For the quantitative analysis, we use the following metrics to evaluate the models-Mean Intersection over Union (mIoU), Accuracy, Precision, Recall, F1 score and Specificity. For each metric, we assume that each image pixel can be classified into two classes, lane or background.\nWe define a few useful terms that are used in the computa-tion of each metric:\n1) True Positives (TP) Number of pixels correctly classified as the positive class. This corresponds to the number of pixels correctly classified as lane.\n2) True Negatives (TN) Number of pixels correctly classified as the negative class. This corresponds to the number of pixels correctly classified as background.\n3) False Positives (FP) Number of pixels wrongly classified as the positive class. This number represents the number of background pixels classified as lane pixels."}, {"title": "B. Performance Comparison", "content": "The quantitative performance of our model and the compet-ing methods are presented in Table III. It can be observed that RaceGAN outperforms competing methods in mIoU, accuracy, and F1 score, with values of 0.8691, 0.9580, and 0.8738, exceeding the best method in each category by 2%, 0.35%, and 3.8%. Among the benchmark methods, the second-best method is Hetnet in terms of mIoU, accuracy, and F1 Score. Hetnet also achieves the highest specificity value of 0.9908."}, {"title": "C. Track Detection Examples under Different Scenarios", "content": "Next, in Figure 4, we present an example of (a) the original input image to our model, (b) the predicted lane mask, and (c) a superimposed version of the predicted mask on the original image, with the red section highlighting the detected lane. In our qualitative assessment of the performance of both our model and the competing models, we use figures similar to Figure 4 (c) to visually assess their accuracy and effectiveness."}, {"title": "D. Computational Cost", "content": "In track detection algorithms, the speed of the algorithm is a critical consideration, as even the best-performing track detection algorithm can be ineffective in a real-time scenario if it cannot process frames quickly enough. Another important point to consider is the computational power needed to run these algorithms. Given that modern racing cars are equipped with highly powerful computing units, there is an increas-ing need to balance performance with efficiency. Since our track detection algorithm is meant to be run on a resource-constrained processor, our goal is to develop a model that has low floating point operations per second (FLOPs), low memory requirement, and fast inference time.\nTo this end, in Table IV, we present three values -FLOPs, the number of parameters in the deep learning model, and in-"}, {"title": "VI. CONCLUSION", "content": "In this work, we address the limited availability of road racing datasets by introducing the RoRaTrack dataset, an open source resource designed for research purposes. The RoRaTrack dataset encompasses a wide variety of common racing scenarios and challenges, providing a comprehensive and valuable tool for researchers to develop models specif-ically tailored to the complexities of road racing. This is particularly significant, as existing models trained on traffic lane data often struggle when applied to racing environments, underscoring the need for specialized solutions. Building on this foundation, we propose RaceGAN, a novel GAN-based approach for track detection. RaceGAN outperforms current state-of-the-art methods, effectively overcoming the unique challenges of racing data and setting a new benchmark for track detection in autonomous racing."}]}