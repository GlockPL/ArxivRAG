{"title": "Uniform Discretized Integrated Gradients: An effective attribution based method for explaining large language models", "authors": ["Swarnava Sinha Roy", "Ayan Kundu"], "abstract": "Integrated Gradients is a well-known technique for explaining deep learning models. It calculates feature importance scores by employing a gradient based approach computing gradients of the model output with respect to input features and accumulating them along a linear path. While this works well for continuous features spaces, it may not be the most optimal way to deal with discrete spaces like word embeddings. For interpreting LLMs (Large Language Models), there exists a need for a non-linear path where intermediate points, whose gradients are to be computed, lie close to actual words in the embedding space. In this paper, we propose a method called Uniform Discretized Integrated Gradients (UDIG) based on a new interpolation strategy where we choose a favorable non-linear path for computing attribution scores suitable for predictive language models. We evaluate our method on two types of NLP tasks- Sentiment Classification and Question Answering against three metrics viz Log odds, Comprehensiveness and Sufficiency. For sentiment classification, we have used the SST2, IMDb and Rotten Tomatoes datasets for benchmarking and for Question Answering, we have used the fine-tuned BERT model on SQUAD dataset. Our approach outperforms the existing methods in almost all the metrics.", "sections": [{"title": "Introduction", "content": "In recent times, the machine learning paradigm has seen incredible advancement in the field of large language models [1] [2], owing to their accurate predictions and natural language generating capabilities. While the complexity of machine learning models has itself increased, the demand for model interpretability has grown significantly especially in domains like finance, legal and healthcare, where having a transparent model is critical. Models such as BERT and its variants are being used in various NLP tasks like classification, Question-Answering, machine translation etc. Although some of these language models achieve state of the art performance on most benchmark datasets, explainability of their predictions is equally important if not more. Some of the industrywide accepted explainability techniques like LIME [11] and SHAP [3] are post-hoc in nature and may not capture the true features contributing to the model outcome. Gradient based approach is one way to explain the model predictions where the gradient of the model output is calculated with respect to the inputs (words or phrases in a text). One such method is the Integrated Gradients [4], an attribution technique commonly known for satisfying two axioms favorable for gradient based methods: Sensitivity and Implementation invariance. In the IG method, attribution of a word is calculated by accumulating gradients along a linear path from a baseline input (black image for image networks, zero embedding vector for textual models) to the target input. Although the IG method can be used for generating attribution scores of features in both continuous and discrete spaces, it has certain limitations for textual data as the points interpolated along this straight line are not good representations of the actual words or tokens due to the inherent discreteness of the underlying word embedding space. The interpolated points can be extremely far off from any word in the embedding space and computing their gradients may lead to sub-optimal attribution scores. To mitigate this, [5] proposed a technique called Discretized Integrated Gradients (DIG), where they have relaxed the constraint of choosing the intermediate points along the straight line between the baseline input and the target input. They select points close to words (anchors) in the vocabulary itself to justify correct attribution computations. This resulted in more faithful and better attribution score generations than IG.\nHowever, DIG has a few limitations. It fails to ensure the uniformity and boundedness of the interpolated points. Based on the nature of the embedding space, these intermediate points can lie far away from the target or the reference word. It is also likely that they will not be uniformly spread between the reference token and the input word. This can create a highly non-linear path which could even violate the monotonic conditions required for computing the attribution scores defined in [5]. Consequently, the gradients computed along this path may yield erroneous attribution scores. We verify this in a subsequent section by comparing the delta approximation error of DIG and our method.\nIn this paper, we propose an improved approach called Uniform Discretized Integrated Gradients (UDIG) by creating a new interpolation strategy and using a new baseline token (\"MASK\"). The intermediate points are chosen uniformly along the straight line joining the baseline and the target input. An anchor word is then selected around each point. This ensures the uniformity and boundedness of the interpolated points required for more accurate attribution computation. We also assert monotonicity between these words to apply Riemann's summation for approximating the UDIG Integral. The choice of the baseline or reference token also plays a significant role here. DIG uses the \u201cPAD\u201d token as its baseline in the attribution formula. In a later section, we will show, how in our case, using the \u201cMASK\u201d token resulted in better performance metrics and word attributions.\nWe applied UDIG to generate attributions for two different tasks: Sentiment Classification and Question-Answering. For sentiment classification task we tested our interpolation algorithm to generate attributions for two large language models - BERT [1] and DistilBERT [6] each fine-tuned on three sentiment classification datasets - SST2 dataset [7], IMDB dataset [12] and Rotten Tomatoes dataset [13]. For Question-Answering task we applied UDIG on BERT model fine-tuned on the SQUAD dataset [14]. We evaluated the performance on three automated metrics viz. log odds, comprehensiveness, and sufficiency against the existing attribution methods.\nOur contributions in this paper\u00b9 are as follows:\n\u2022\tWe have proposed a new interpolation algorithm for selecting points along a non-linear path from the input word to the baseline word."}, {"title": "Methodology", "content": "In this section, we provide the methodology of computing the attribution score for each word in a sentence. We also explain the strategy of interpolating points along the nonlinear path between the baseline and the target word."}, {"title": "Attribution score", "content": "Let \\(F: \\mathbb{R}^{m \\times n} \\rightarrow \\mathbb{R}\\) be a language model, which takes a sequence of m words as input; n is the word embedding dimension.\nLet \\(x_{ij}\\) be the jth dimension of the ith word in a sentence.\nThe attribution score of \\(x_{ij}\\) is given by,\n\\[\nUDIG_{ij}(x) = \\int_{x'=x_0}^{x_k} \\frac{\\partial F(x)}{\\partial x_{ij}} dx_{ij} \\tag{1},\n\\]\nwhere \\(x'_0\\) is jth dimension of the baseline word and \\(x^k_i\\) is the kth interpolated point along the path from the baseline to the ith word in the sentence.\nThe final attribution score of a word is obtained by summing UDIG\u00a1\u00a1(x) over j and dividing it with its norm.\n\\[\nUDIG_{i}(x) = \\frac{\\sum_{j=1}^n UDIG_{ij}(x)}{||UDIG||}\n\\]\nThe above integral can be approximated using the Riemann Sum approximation of integrals based on the constraint that the interpolated points are monotonic.\n\\[\nUDIG_{ij}(x) = \\sum_{k=1}^K \\frac{\\partial F(x)}{\\partial x_{ij}} *(x^{k+1}_{ij} - x^{k}_{ij}) \\tag{2},\n\\]\nwhere K is the number of steps between the baseline and the input word.\nFor any given i, let us assume WLOG \\(x_0^j \\leq x_k^j \\leq x_j\\) if \\(x_0^j \\leq x_j\\),\n\\(x_0^j \\geq x_k^j \\geq x_j\\) otherwise, where \\(k\\)e {1,2, ... K}"}, {"title": "Interpolation algorithm", "content": "Here we provide the details of the process of selecting the intermediate words between the input word and the baseline word. First, given an initial input word embedding w and a baseline embedding wo, we fix points along the linear path connecting wo and w in a uniform manner. Next, we look for words in the vicinity of every such point on the straight line. Finally, we ensure monotonicity along the path from the baseline to the input word."}, {"title": "Setting path uniformity", "content": "In this step we set equidistant points in the word embedding space that lie along the linear path between the input word embedding w and the baseline word embedding wo. These points will be random high dimensional floating-point values in the embedding space and may lie far away from any real word in the vocabulary. The Integrated gradients algorithm accumulates gradients along these points for computing the attribution score. The number of steps in the interpolation process is specified in this step."}, {"title": "Nearest word", "content": "Once we have the embeddings of the sampled points along the linear path, we search for the nearest word in the vocabulary. This can be achieved by considering a neighborhood around each point and choosing k-nearest words based on a similarity score defined between word embeddings. Out of these k neighbors, we select the anchor word based on one of two approaches viz. Greedy and Max-Count introduced in [5]. This is where non-linearity is introduced in the path. The Greedy approach computes the monotonic representation of every word in the neighborhood and picks that word which is closest to its monotonic form. The Max-Count approach chooses that word with the maximum number of monotonic dimensions. We explain what monotonicity exactly means in the next section. The two approaches are illustrated in Figure 1."}, {"title": "Monotonicity", "content": "This is to ensure that we can apply Riemann Summation to approximate the integral in (1). We have kept this step identical to [5]. A word embedding 'a' is defined to be monotonic w.r.t. w and wo, if all the dimensions of a are individually monotonic with the corresponding dimensions of w and wo. The monotonic dimensions of a is given by:\n\\(M_a = {j|w_0j \\leq a_j \\leq w_j} \\cup {j|w_j \\geq a_j \\geq w_0j} je{1,2, ... D}\\) where D is the embedding dimension. The remaining non-monotonic dimensions are perturbed in order to achieve monotonicity entirely."}, {"title": "Choice of baseline", "content": "According to the authors of the Integrated Gradients paper, the baseline should be chosen such that the output of the model at the baseline is \u201cnear zero\u201d. Although in [5], authors have used the \u201cPAD\u201d token as the baseline, the \"MASK\" token satisfies this criterion better (Table 1). This could be due to the \"PAD\u201d token embedding being updated during pre-training and/or fine-tuning phase of the language model training. Whereas the \"MASK\" token embedding learns to fill missing words which are removed randomly from a sentence during its pre-training phase, making it equally unbiased for all the words in the vocabulary. Hence it serves as a better choice for the baseline."}, {"title": "Axioms satisfied by UDIG", "content": "There are a couple of fundamental axioms that need to be satisfied by attribution methods as pointed out in [4]. They are i) Sensitivity and ii) Implementation Invariance. Sensitivity axiom states that if any two inputs, that differ in only one feature, have different outcomes, then that differing feature should have non-zero attribution score. Implementation invariance axiom states that the attribution scores from two functionally equivalent models must be equal even if they differ in implementation. Our proposed method satisfies both these axioms as it is a path integral based method. Moreover, UDIG also satisfies the Completeness axiom which follows from the fundamental theorem of calculus that the sum of the attributions of an input x should be equal to the difference of the model output at x and the baseline x' i.e., \\(\\sum UDIG(x) = F(x) - F(x')\\)"}, {"title": "Experiment Design", "content": "In this section, we describe the datasets and the models used to evaluate our method against other explainability methods. We have followed the experimental approach used by Sanyal and Ren."}, {"title": "Sentiment Classification", "content": "Dataset: Our method is evaluated on three different datasets. The Stanford Sentiment Treebank (SST2) classification test dataset which consists of 1821 examples, the IMDB test dataset which consists of 25000 sentence-label pairs and the Rotten Tomatoes (RT) datasets which has 1066 labelled examples. Each dataset has binary labels for positive and negative sentiment and are available in the Huggingface library\u00b2. For SST2 and Rotten Tomatoes, we have used the complete test dataset, and for IMDB, we have sampled around 650 examples for our analysis.\nLanguage Models: We have used the finetuned variants of the BERT and DistilBERT classification models on the SST2, IMDB and RT datasets. All the finetuned models can be downloaded from the Huggingface website.\nBaselines: The following feature attribution methods are used for benchmarking against our method: Integrated Gradients (IG) and Discretized IG (DIG)."}, {"title": "Question-answering", "content": "For Question-answering task, we need two logits one for the start position of the answer and one for the end position. Based on these two logits, the span of the answer is determined. In Classification task, we have a global representation of the input from the model which is used to get the predictions. On the other hand, in Question-Answering to predict the answer span, we need to consider all the tokens to calculate the probability distributions over each word.\nDataset: The Question-Answering task is evaluated on the Stanford Question Answering Dataset (SQUAD). This dataset has 87599 examples in the train set and 10570 examples in the validation set. The data contains questions, contexts, and answers along with some other metadata. From the Stanford Question Answering dataset, we have sampled 950 question answer pairs based on the length of the context available for each pair.\nLanguage Models: We have used BERT Question-Answering model which is fine-tuned on the SQUAD dataset. The finetuned model is available in the Huggingface library."}, {"title": "Results and Analysis", "content": ""}, {"title": "Performance comparison", "content": "In Tables 2, 3, 4 and 5 we have presented the comparison of IG, DIG and UDIG in terms of the metrics stated in the previous section. We have observed that UDIG outperforms other methods consistently for both Sentiment Analysis and Question Answering classification tasks."}, {"title": "Path density effect on Delta", "content": "Path integral based attribution methods satisfy the Completeness axiom as stated in Sec 2.4._This property can be used to estimate the approximation error of integral in equation (2) and is denoted by Delta % error. It is necessary that this error rate be within a threshold for accurate computation of the attribution scores. In IG, as we keep on increasing the number of steps, delta keeps on decreasing. Similarly, for UDIG, if we insert the mean of two consecutive points between all the words in the path, we observe that the delta percent error drops. This process is referred to as up-sampling in [5]. For all our experiments we have suitably chosen f=1 for a low delta error percentage. In tables 6, 7, 8 and 9, we have shown that UDIG has significantly lesser delta error than DIG for all the experiments. This supports our claim that DIG tends to follow non-linear paths which can generate suboptimal attribution scores."}, {"title": "Time Complexity", "content": "One of the limitations of UDIG, is that it is computationally more expensive than IG and DIG. This is due to the interpolation algorithm involving an extra step of specifying points along the straight line and then searching for the nearest actual word. It then follows a similar mechanism to DIG where it chooses the closest monotonic word in its neighborhood. More details on time complexity are covered in the Appendix."}, {"title": "Conclusion", "content": "In this paper, we have suggested a new method Uniform Discretized Integrated Gradients (UDIG) for computing attribution scores for textual data at a word/token level. UDIG is a hybrid of IG and DIG as it includes both their steps. It is better than IG at calculating attribution scores because it aggregates meaningful gradients of words by following a non-linear path. It is also better than DIG because it keeps the non-linearity under control by choosing words uniformly and close to the straight-line connecting the baseline and the target word. We have evaluated this approach on two types of NLP classification problems viz Sentiment Analysis and Question Answering and compared the performances with the other methods. Our method performs better consistently for both the streams of problems. It suggests that keeping the interpolation points uniform and bounded yields better results while calculating the attribution scores. We also recommend using the MASK token as the baseline when explaining models that contain this special token. For language models where the MASK token is not available e.g., GPT, the zero embedding vector or the PAD token can be a suitable alternative for the baseline. Although we validate our method on only two language tasks, gradient based approaches like UDIG can be used to derive attribution scores for any type of predictive language model. In the future, we wish to apply this model explainability technique to a wider range of NLP tasks."}]}