{"title": "SCALABLE NESTED OPTIMIZATION FOR DEEP LEARNING", "authors": ["Jonathan Peter Lorraine"], "abstract": "Gradient-based optimization has been critical to the success of machine learning, updating a single\nset of parameters to minimize a single loss. A growing number of applications rely on a generalization\nof this, where we have a bilevel or nested optimization of which subsets of parameters update on\ndifferent objectives nested inside each other. We focus on motivating examples of hyperparameter\noptimization and generative adversarial networks. However, na\u00efvely applying classical methods often\nfails when we look at solving these nested problems on a large scale. In this thesis, we build tools for\nnested optimization that scale to deep learning setups.\n\u2022 In Chapter 2, we provide an explicit, differentiable approximation to how neural network\nweights best-respond or optimize for their loss. We train a hypernetwork that takes\nin hyperparameters and outputs neural network weights. We use this hypernetwork for\nhyperparameter optimization.\n\u2022 In Chapter 3, we explore an algorithm that implicitly approximates the neural network\nweights best-response, using the implicit function theorem. We apply this to hyperparameter\noptimization, showing we can tune as many hyperparameters as neural network weights.\n\u2022 In Chapter 4, we augment simultaneous gradient descent to mitigate rotational dynamics. We\ndo this by generalizing gradient descent with momentum to have a complex-valued momentum\nwhile retaining real-valued parameter updates.\n\u2022 In Chapter 5, we generalize strategies for finding diverse solutions in single-objective optimization\nto finding diverse solutions in setups where multiple agents optimize for their own objective.\nWe do this by taking the Ridge-rider algorithm and generalizing their branching criteria to\noccur at bifurcations using connections to Lyapunov exponents.", "sections": [{"title": "Introduction", "content": "Motivating single-objective optimization in machine learning: In recent years, machine\nlearning has emerged as a transformative force in numerous scientific and industrial domains, pro-\nfoundly impacting everything from computer-vision, to natural language processing, to personalized\nmedicine. Large neural networks have become a foundational workhorse for using machine learning\nto advance these fields. Central to this revolution has been the advancement of gradient-based\noptimization techniques, which have effectively trained increasingly large and complex neural network\narchitectures. However, the traditional focus on optimizing a single set of parameters for a singular\nobjective increasingly gives way to more nuanced paradigms.\nMotivating the generalized nested optimization paradigm in machine learning: A growing\nnumber of applications require learning with subsets of parameters updating on different objectives.\nImportant examples are hyperparameter optimization (Maclaurin et al., 2015a; Andrychowicz et al.,\n2016; Fu et al., 2016; Shaban et al., 2019), GANS (Goodfellow et al., 2014), actor-critic models (Pfau\nand Vinyals, 2016), curriculum learning (Baker et al., 2019; Balduzzi et al., 2019; Sukhbaatar et al.,\n2018), adversarial examples (Bose et al., 2020; Yuan et al., 2019), learning models (Rajeswaran\net al., 2020; Abachi et al., 2020; Nikishin et al., 2021), domain adversarial adaptation (Acuna et al.,\n2021), neural architecture search (Elsken et al., 2019), and meta-learning (Finn et al., 2017; Ren\net al., 2018a, 2020). This thesis focuses on motivating examples of hyperparameter optimization and\nGANs. Furthermore, we will call a setup with more than one objective a game, call optimization here\nlearning in games, and each subset of parameters with their respective losses will be called a player.\nIntroducing nested optimization more formally: Consider 2-player games\u00b9 with players\ndenoted by A and B where each player minimizes their loss $L_A, L_B$ with their parameters $\u0398_A, \u03b8_B$.\nWe work with setups where the objectives are nested inside of each other - so-called Stackelberg\ngames or bilevel optimization (Von Stackelberg, 1952) - whose solutions can be defined as:\n$\u0398_A^* = \\underset{\u0398_A}{argmin} L_A (\u0398_A, \u03b8^*_B (\u0398_A))$,\n$\u03b8^*_B (\u0398_A) = \\underset{\u0398_B}{argmin} L_B (\u0398_A, \u0398_B)$\n(1.1)\nHere, $\u03b8^*_B (\u0398_A)$ denotes player B's best-response function, which says how their optimal parameters\nchange as a function of the other players' parameters. In general, the inner and outer optimization\ncan have multiple solutions \u2013 see Vicol et al. (2022a) \u2013 giving rise to a best-response set instead of a\nfunction. However, for most of the thesis, we focus on the setup with unique solutions, except in\nChapter 5, where we look at methods to find multiple valid solutions.\nExisting nested optimization approaches and their limitations: The overarching goal of this\nthesis is to efficiently find solutions $(\u0398^*_A,\u03b8^*_B)$ when our parameters $\u0398_A$ and $\u03b8_B$ are approaching the\nsize of modern neural network parameters. We may be able to approximately find $\u0398^*_A$ efficiently if we\ncan do gradient-based optimization on:\n$L^* (\u0398_A) = L_A (\u0398_A, \u03b8^*_B (\u0398_A))$\n(1.2)\nOptimizing this objective would require computing the hypergradient:\n$\\frac{\u2202L^* (\u0398_A)}{\u2202\u0398_A} = \\frac{\u2202L_A}{\u2202\u0398_A} + \\frac{\u2202\u03b8^*_B}{\u2202\u0398_A} \\frac{\u2202L_A}{\u2202\u03b8^*_B} $\n$\\text{hypergradient} = \\frac{\u2202L_A(\u0398_A,\u03b8^*_B (\u0398_A))}{\u2202\u0398_A} + \\frac{\u2202\u03b8^*_B (\u0398_A)}{\u2202\u0398_A} \\frac{\u2202L_A(\u0398_A,\u03b8^*_B (\u0398_A))}{\u2202\u03b8^*_B}$  $\\text{indirect gradient} = \\frac{\u2202L_A}{\u2202\u0398_A} + \\frac{\u2202\u03b8^*_B}{\u2202\u0398_A} \\frac{\u2202L_A}{\u2202\u03b8^*_B}$  $\\frac{\u2202\u03b8^*_B (\u0398_A)}{\u2202\u0398_A}$ $\\text{direct gradient}$  $\\text{best-response Jacobian}$\n(1.3)\nUnfortunately, this often requires $ \\frac{\u2202\u03b8^*_B (\u0398_A)}{\u2202\u0398_A}$, but $\u03b8^*_B (\u0398_A)$ and its Jacobian are typically intractable, as they\nrequire exact evaluation and differentiation through optimization. An alternative strategy is simulta-\nneous/alternating gradient descent, where each player does gradient descent on their own objectives -\n$L_A$ and $L_B$ - $\\frac{\u2202L_A}{\u2202\u0398_A}$ and $ \\frac{\u2202L_B}{\u2202\u03b8_B}$ respectively. However, strategies like this can fail. For example, because a gradient is\nidentically zero as in hyperparameter optimization or because of rotational dynamics (Berard et al.,\n2019) as in GANs. Black-box methods like random-search or Bayesian Optimization (Mo\u010dkus, 1975;\nSnoek et al., 2012) circumvent gradient calculations but are ineffective for high-dimensional problems\n- e.g., greater than 100 dimensions. Specifically, we want the ability to optimize high-dimensional\nproblems prevalent in machine learning."}, {"title": "Thesis Outline", "content": "In the Chapters 2 and 3, we explore approximations for the best-response $\u03b8^*_B (\u0398_A)$ and its Jacobian\n$\\frac{\u2202\u03b8^*_B (\u0398_A)}{\u2202\u0398_A}$, which are applied to hyperparameter optimization. In Chapter 4, we instead look at augmenting\nthe gradient dynamics to avoid rotational dynamics, without approximating $\\frac{\u2202\u03b8^*_B (\u0398_A)}{\u2202\u0398_A}$, which is applied\nto GANs. Finally, we look at generalizing a branching single-objective optimization algorithm that\nfinds multiple solutions to set-ups with multiple objectives.\n\u2022 In Chapter 2, we explicitly approximate the best-response function $\u03b8^*_B (\u0398_A)$ with a hypernetwork\nwhich we differentiate through for optimization. Our method collapses the nested optimization\nof model weights and hyperparameters into a joint stochastic optimization We do this via\namortized optimization with hypernetworks, which output approximately optimal weights as a\nfunction of hyperparameters. We compare this with standard hyperparameter optimization\nand demonstrate its use in tuning hundreds to thousands of hyperparameters."}, {"title": "Hyperparameter Optimization Through Hypernetworks", "content": "Machine learning models often nest optimization of model weights within the optimization of\nhyperparameters. We give a method to collapse this nested optimization into joint stochastic\noptimization of weights and hyperparameters. We train a neural (hyper)network to output optimized\nweights as a function of hyperparameters. We compare this method to standard hyperparameter\noptimization methods and demonstrate its effectiveness in tuning thousands of hyperparameters.\nModel selection and hyperparameter tuning are significant bottlenecks in the design of predictive\nmodels. Hyperparameter optimization is a nested optimization, which can be viewed as a special case\nof Equation 1.1. Here, the inner optimization finds the neural network's weights w that minimize the\ntraining loss $L_T$ given hyperparameters $\u03bb$. The outer optimization chooses $\u03bb$ to reduce the validation\nloss $L_v$ with the best-responding weights $w^*$.\n$\\lambda^* = argmin_\u03bb L(\u03bb) \\text{where}$\n(2.1)\n$L(\u03bb) = L_v(\u03bb, w^*(\u03bb))$ and $w^*(\u03bb) = argmin_w L_T(\u03bb, w)$(2.2)\nStandard practice in machine learning solves Equation 2.1 by gradient-free optimization of hyperpa-\nrameters, such as grid or random search. Each set of hyperparameters is evaluated by reinitializing\nthe weights and training the model to completion. Retraining a model from scratch is wasteful if the\nhyperparameters change by a small amount. Some approaches, such as Hyperband (Li et al., 2016)\nand freeze-thaw Bayesian optimization (Swersky et al., 2014), resume model training but often scale\npoorly beyond 10 to 20 dimensions. How can we avoid retraining from scratch each time? Well, the\noptimal parameters $w^*$ are a function of the hyperparameters $\u03bb$:\n$w^*(\u03bb) = argmin_w L_T(w, \u03bb)$\n(2.3)\nWe propose to learn this function.\u00b9 Specifically, we train a neural network that takes hyperparameters\nas input and outputs an approximately optimal set of weights. This formulation provides two main\nbenefits. First, we can train the hypernetwork to convergence using stochastic gradient descent without\ntraining any particular model to completion. Second, differentiating through the hypernetwork allows\nus to optimize hyperparameters with stochastic gradient-based optimization."}, {"title": "Training a network to output optimal weights", "content": "Given some hyperparameters, how can we teach a hypernetwork (Ha et al., 2016) to produce\napproximately optimal weights for another neural network? At each iteration, we ask a hypernetwork\nwith parameters $\u03a6$ to input hyperparameters $\u03bb$ and to output a set of weights: $w_\u03a6 (\u03bb)$. Instead\nof updating the weights w using the training loss gradient $\u2202L_T (w)/\u2202w$, we update the hypernetwork\nweights using the chain rule: $\u2202L_T(w)/\u2202\u03a6 \u2202w/ \u2202\u03a6$. At convergence, we want our hypernetwork to match\nthe best response function closely: $w_\u03a6 (\u03bb) \u2248 w^*(\u03bb)$. This formulation allows us to optimize the\nhyperparameters $\u03bb$ using our hypernetwork-learned function as a surrogate best-response. We call\nthis method hyper-training and contrast it with standard training methods."}, {"title": "Advantages of hypernetwork-based optimization", "content": "Hyper-training is a method to learn a mapping from hyperparameters to validation loss that is\ndifferentiable and inexpensive to evaluate. Alternatively, model-based hyperparameter schemes, like\nBayesian optimization (e.g., Snoek et al. (2012)) build a model of the validation loss as a function of\nhyperparameters. This approach has several disadvantages compared to hyper-training.\nFirst, obtaining data for standard Bayesian optimization requires optimizing models from initial-\nization for each set of hyperparameters. In contrast, hyper-training never needs to optimize any one\nmodel fully, removing choices like how many models to train and for how long. Second, standard\nBayesian optimization treats the best-responding validation loss as a black-box function. In contrast,\nhyper-training takes advantage of the fact that validation loss is a known, differentiable function,\nwhich can be evaluated stochastically by sampling points from the validation set. This can have\na better generalization for learning hyperparameter to validation loss than directly fitting the loss\nusing a Gaussian process (e.g., Rasmussen and Williams (2006)), as in Figure 2.6."}, {"title": "Limitations of hypernetwork-based optimization", "content": "We apply this method to unconstrained continuous bilevel optimization problems with a differentiable\ninner and outer loss function. But what kind of parameters can be optimized by our approach?\nHyp        yperparameters typically fall into two broad categories: (1) parameters that change the set of\nlocally optimal points, like regularization parameters or architectural choices, and (2) parameters\nthat affect the choice of locally optimal point and the rate we converge to them, like optimization\nhyperparameters such as learning rates. Hyper-training does not have inner optimization parameters\nbecause there is no internal training loop, so we cannot optimize these. We must still choose the\noptimization parameters for the fused optimization loop. In principle, hyper-training can handle\ncontinuous relaxations of discrete hyperparameters, which we further explore in non-thesis research\nof MacKay et al. (2019a).\nA clear difficulty of this approach is that hypernetworks can require several times as many\nparameters as the original model. For example, training a fully connected hypernetwork with 1\nhidden layer of H units to output D parameters requires at least D \u00d7 H hypernetwork parameters.\nTo partially address this problem, in Section 2.2.3, we propose an algorithm that trains only a\nlinear model that maps hyperparameters to model weights. We further explore better alternatives in\nnon-thesis research of MacKay et al. (2019a)."}, {"title": "Related Work", "content": "Here, we briefly review the gradient-free and gradient-based methods we compare to in our experiments.\nOur work complements the SMASH algorithm of Brock et al. (2017), with Section 2.2 discussing\nour differences. We are both special cases of amortized optimization methods, which use learning to\npredict solutions when we repeatedly solve instances of the same problem (Amos, 2022). The idea\nof weight generation given a context vector has been used in multiple areas (Requeima et al., 2019;\nRatzlaff and Fuxin, 2019; Pilault et al., 2020; Tay et al., 2020; Rusu et al., 2018).\nGradient-free Hyperparameter Optimization: Various gradient-free hyperparameter opti-\nmization methods \u2013 not scalable to this regime are detailed in Appendix Section A.2. Model-free\napproaches use only trial and error to explore the hyperparameter space. Simple model-free ap-\nproaches applied to hyperparameter optimization include grid and random search (Bergstra and\nBengio, 2012), or more sophisticated methods like Hyperband (Li et al., 2016) combine bandit ap-\nproaches with modeling the learning procedure. Model-based approaches build a surrogate function,\nlike Bayesian optimization (Mo\u010dkus, 1975; Snoek et al., 2012), which we use as a baseline in our\nexperiments, implemented by Snoek et al. (2019). Evolutionary methods (Alexandropoulos et al.,\n2019) are similar to our method, such as population-based training (Jaderberg et al., 2017), which\nmaintains a set of networks instead of using a hypernetwork.\nGradient-based Hyperparameter Optimization: Notably, we compare to differentiating\nthrough optimization, which Domke (2012) proposes, Maclaurin et al. (2015a) scales to differentiating\nthrough learning procedures in deep learning, and is implemented by Franceschi et al. (2017). More\ngradient-based methods are discussed in Chapter 3, Section 3.3 where we compare with them."}, {"title": "Experiments", "content": "Our experiments examine the standard example of stochastic gradient-based optimization of neural\nnetworks with a weight regularization penalty. In all experiments, Algorithms 2 or 3 are used to\noptimize the weights on MNIST (LeCun et al., 1998) with a 12 weight decay penalty weighted by\nexp(x). Unless otherwise specified, all hidden units in the hypernetwork have ReLU activation (Nair\nand Hinton, 2010). Autograd (Maclaurin et al., 2015b) was used to compute all derivatives. The\nmini-batch samples 2 sets of hyperparameters and up to 1000 training data points for each experiment.\nWe use Adam (Kingma and Ba, 2014) to train the hypernetwork and hyperparameters with a step\nsize of 0.0001, and (\u03b21, \u03b22) = (.9, .999)."}, {"title": "Learning a global best-response", "content": "Our first experiment, shown in Figure 2.2, demonstrates learning a global approximation to a best-\nresponse function using Algorithm 2. To simplify visualization of the regularization loss, we use 10\ntraining data points to exacerbate overfitting. We compare the performance of the weights output by\nthe hypernetwork with those trained by standard cross-validation (Algorithm 1). The elementary\nweights were randomly initialized for each hyperparameter choice. When training the hypernetwork,\nthe hyperparameters were sampled from a broad Gaussian distribution: $p(\u03bb) = N(0,1.5)$. The\nhypernetwork has 50 hidden units. Takeaway: The minimum of the best-response in Figure 2.2 is\nclose to the real minimum of the validation loss, which shows that a hypernetwork can approximate\na global best-response function in small problems."}, {"title": "Learning a local best-response", "content": "Figure 2.4 shows the same experiment as Figure 2.2, except using Algorithm 3. The fused updates\nresult in finding a best-response approximation whose minimum is the actual minimum faster\nthan in the prior experiment. The conditional hyperparameter distribution is given by $p(\u03bb) =$\n$N(\u03bb, 0.00001I)$. Here, the hypernetwork is a linear model. Again, the minimum of the best-response\nat the end of training minimizes the validation loss. Takeaway: Using only a locally trained linear\nbest-response function can give sufficient gradient information to optimize hyperparameters while\nbeing less computationally expensive than learning a global best-response."}, {"title": "Hyper-training and unrolled optimization", "content": "We train models with a separate 12 weight decay applied to each weight in a one-layer model to\ncompare hyper-training with other gradient-based hyperparameter optimization methods. The\nconditional hyperparameter distribution and optimizer for the hypernetwork and hyperparameters\nare the same as in the prior experiments. Here, we use a hypernetwork with 10 hidden units.\nFigure 2.5, top, shows that Algorithm 3 converges more quickly than the unrolled reverse-mode\noptimization implemented by Franceschi et al. (2017). Hyper-training overfits validation data less\nthan unrolling but reaches sub-optimal solutions - perhaps because of limitations on how many\nhyperparameters can be sampled for each update. Standard Bayesian optimization cannot be scaled\nto this many hyperparameters. Takeaway: Algorithm 3 can efficiently partially optimize thousands\nof hyperparameters. But, unrolled optimization performed better asymptotically."}, {"title": "Estimating weights versus estimating loss", "content": "Our approach differs from Bayesian optimization, which attempts to model the validation loss of\noptimized weights directly, where we try to learn to predict optimal weights. In this experiment,\nwe begin to unravel the reason for the better performance of our method. Is it because of a better\ninductive bias or because our way can see more hyperparameter settings during optimization?\nFirst, we constructed a hyper-training set: We optimized 25 sets of weights to completion, given\nrandomly sampled hyperparameters. We chose 25 samples because that is the regime in which we\nexpect Gaussian process-based approaches to have the largest advantage. We constructed an unseen\nset of 10215 (optimized weight, hyperparameter) tuples generated similarly. We then fit a Gaussian\nprocess (GP) regression model with an RBF kernel from sklearn on the validation loss data. A\nhypernetwork is fit to the same set of hyperparameters and data. Finally, we optimize another\nhypernetwork using Algorithm 2 for the same amount of time as building the GP training set. The\ntwo hypernetworks are linear models with the same optimizer parameters as prior experiments.\nFigure 2.6 shows the distribution of prediction errors for these three models. The Gaussian\nprocess tends to underestimate the loss. The hypernetwork trained with the same small fixed set of\nexamples tends to overestimate loss. We conjecture that this is due to the hypernetwork producing\nbad weights in regions without enough training data. Because the hypernetwork must provide actual\nweights to predict the validation loss, poorly fit regions can overestimate the loss $L_v$. Finally, the\nhypernetwork trained with Algorithm 2 produces errors tightly centered around 0. Takeaway: A\nhypernetwork can learn more accurate surrogate functions than a GP for equal compute budgets\nbecause it views (noisy) evaluations of many more points."}, {"title": "Conclusions and Future Work", "content": "This chapter addressed the tuning of hyperparameters using gradient-based optimization by replacing\nthe training optimization loop with a differentiable hypernetwork. We also presented a simple and\nmore scalable method that jointly optimizes hyperparameters and hypernetwork weights, allowing our\nmethod to work with manageable-sized hypernetworks. Experimentally, we showed that hypernetworks\ncould provide a better inductive bias for hyperparameter optimization than Gaussian processes that\nfit the validation loss. There are many ways to extend the proposed methods, with more examples\nin Section 6.2. For example, the hypernetwork could consist of several optimization iterations as\nan easily differentiated fine-tuning step. Hypernetworks could be incorporated into meta-learning\nschemes, such as MAML (Finn et al., 2017), which finds weights that perform various tasks after\nunrolling gradient descent. We also note that optimizing thousands of hyperparameters raises the\nquestion of hyper-regularization, or regularization of hyperparameters.\nA key limitation of this work is that it does not scale to ultra-large hyperparameter regimes,\nwhere we have as many hyperparameters as neural network parameters for large networks. In the\nfollowing chapter, we look at methods that scale to this regime."}, {"title": "My Contributions Towards this Paper As it Pertains to the Thesis", "content": "This was my first paper and was shepherded into a submission under the close guidance of David\nDuvenaud, for which I will forever be grateful. I proposed the idea while David helped code up initial\nversions of this in Autograd, which I fleshed out into our experiments. I also wrote most of the paper\nwith much assistance from David. This paper has an updated arXiv version (Lorraine and Duvenaud,\n2018) and an older workshop version (Lorraine and Duvenaud, 2017)."}, {"title": "Optimizing Millions of Hyperparameters by Implicit Differentiation", "content": "We propose an inexpensive, gradient-based hyperparameter optimization algorithm that combines\nthe implicit function theorem (IFT) with efficient inverse Hessian approximations. We present results\non the relationship between IFT and differentiation through optimization, motivating our algorithm.\nWe use our approach to train modern network architectures with millions of weights and millions\nof hyperparameters. Specifically, we learn a data-augmentation network-where every weight is a\nhyperparameter tuned for validation performance that outputs augmented training examples; we\nlearn a distilled dataset where every feature in each data point is a hyperparameter; and we tune\nmillions of regularization hyperparameters. Jointly tuning weights and hyperparameters with our\napproach is only a few times more costly in memory and compute than standard training.\nNeural network generalization to unseen data crucially depends on hyperparameter choice. Hyperpa-\nrameter optimization (HO) has a rich history (Schmidhuber, 1987; Bengio, 2000), and has recently\nachieved successful scaling due to gradient-based optimizers (Domke, 2012; Maclaurin et al., 2015a;\nFranceschi et al., 2017, 2018; Shaban et al., 2019; Finn et al., 2017; Rajeswaran et al., 2019; Liu\net al., 2018; Grefenstette et al., 2019; Mehra and Hamm, 2019). There are dozens of regularization\ntechniques to combine in deep learning, and each may have multiple hyperparameters (Kuka\u010dka et al.,\n2017). If we can scale hyperparameter optimization to have as many\u2014or more hyperparameters\nas parameters, various exciting regularization strategies exist to investigate. For example, we could\nlearn a distilled dataset with a hyperparameter for every feature of each input (Maclaurin et al.,\n2015a; Wang et al., 2018), weights on each loss term (Ren et al., 2018b; Kim and Choi, 2018; Zhang\net al., 2019a), or augmentation on each input (Cubuk et al., 2018; Xie et al., 2019)."}, {"title": "Overview of Proposed Algorithm", "content": "There are four essential components to understanding our proposed algorithm. Further background\nis provided in Appendix A.1, and the notation is shown in Table A.1.\n1. Hyperparameter optimization is nested optimization: $L_T$ and $L_v$ denote the training\nand validation losses, w the neural network weights, and $\u03bb$ the hyperparameters. We aim to find\noptimal hyperparameters $\u03bb^\u2217$ such that the neural network minimizes the validation loss after training:\n$\u03bb^\u2217 = argmin_\u03bb L^\u2217(\u03bb) where$\n(3.1)\n$L^\u2217(\u03bb) = L_v(\u03bb, w^\u2217(\u03bb)) and w^\u2217(\u03bb) = argmin_w L_T(\u03bb, w)$\n(3.2)\nAs in Chapter 2, our implicit function $w^\u2217(\u03bb)$ is the best-response of the weights to the hyperparame-\nters, and our desired objective is the best-responding validation loss $L^\u2217$ in red. Again, for simplicity,\nwe assume unique solutions to argmin, and refer to non-thesis research of Vicol et al. (2022a) for\nanalysis in the non-unique setup.\n2. Hypergradients have two terms: For gradient-based hyperparameter optimization we\nwant the hypergradient $\\frac{\u2202L^\u2217}{\u2202\u03bb}(\u03bb)$, which decomposes into:\n$\\frac{\u2202L^\u2217}{\u2202\u03bb}(\u03bb)$    hypergradient  $= (\\frac{\u2202L_v(\u03bb, w^\u2217(\u03bb))}{\u2202\u03bb}) + (\\frac{\u2202L_v}{\u2202w} + \\frac{\u2202w^\u2217}{\u2202\u03bb}))$   $\\frac{\u2202w^\u2217}{\u2202\u03bb}$    hyperparam indirect grad  $= \\frac{\u2202L_v(\u03bb, w^\u2217(\u03bb))}{\u2202\u03bb} +  \\frac{\u2202L_v(\u03bb, w^\u2217(\u03bb))}{\u2202w}  \\frac{\u2202w^\u2217(\u03bb)}{\u2202\u03bb}  \\frac{\u2202w^\u2217(\u03bb)}{\u2202\u03bb}   \\frac{\u2202w^\u2217(\u03bb)}{\u2202\u03bb}$ $\\text{hyperparam direct grad.}$ $\\text{parameter direct grad.} \\text{best-response Jacobian}$ \n(3.3)\nThe direct gradient is easy to compute. However, the indirect gradient is difficult to compute because\nwe must account for how the optimal weights change for the hyperparameters (i.e., $\\frac{\u2202w^\u2217(\u03bb)}{\u2202\u03bb}$). In\nhyperparameter optimization the direct gradient is often identically 0, necessitating an approximation\nof the indirect gradient to make any progress (visualized in Figure 3.1). In Chapter 4, we look at\nalgorithms that do not require response Jacobians for setups where the direct gradient is non-zero."}, {"title": "Related Work", "content": "Implicit Function Theorem. The IFT has been used for optimization in nested optimization\nproblems (Ochs et al., 2015; Wang et al., 2019a; Lee et al., 2019), backpropagating through arbitrarily\nlong RNNs (Liao et al., 2018), k-fold cross-validation (Beirami et al., 2017), and influence func-\ntions (Koh and Liang, 2017). Early work applied the IFT to regularization by explicitly computing\nthe Hessian (or Gauss-Newton) inverse (Larsen et al., 1996; Bengio, 2000). In Luketina et al. (2016),\nthe identity matrix approximates the IFT's inverse Hessian. HOAG (Pedregosa, 2016) uses conjugate\ngradient (CG) to approximately invert the Hessian and provides convergence results given tolerances\non the optimal parameter and inverse. In iMAML (Rajeswaran et al., 2019), a center to the weights\nis fit to perform on multiple tasks, where we fit to perform on the validation loss. In DEQ (Bai et al.,\n2019), implicit differentiation is used to add differentiable fixed-point methods to neural network\narchitectures. We use a Neumann approximation for the inverse Hessian instead of CG (Pedregosa,\n2016; Rajeswaran et al., 2019) or the identity."}, {"title": "Method", "content": "In this section, we discuss how hyperparameter optimization is a uniquely challenging nested\noptimization problem and how to combine the benefits of the IFT and unrolled differentiation.\nEquation 3.3 shows that the hypergradient decomposes into a direct and indirect gradient. The\nbottleneck in hypergradient computation is usually finding the indirect gradient because we must\nconsider how the optimized parameters vary for the hyperparameters. A simple optimization approach\nis to neglect the indirect gradient and only use the direct gradient. This can be useful in zero-sum\ngames like GANs (Goodfellow et al., 2014) because they always have a non-zero direct term.\nHowever, using only the direct gradient does not work in general games (Balduzzi et al., 2018). In\nparticular, it does not work for hyperparameter optimization because the direct gradient is identically\n0 when the hyperparameters $\u03bb$ only influence the validation loss by changing the optimized weights\n$w^\u2217(\u03bb)$. For example, if we use regularization like weight decay when computing the training loss but\nnot the validation loss, then the direct gradient is always 0.\nIf the direct gradient is identically 0, we call the game pure-response. Pure-response games are\ndifficult nested optimization problems for gradient-based methods because we cannot simply use the\ndirect gradient like in simultaneous SGD. So, we must approximate the indirect gradient."}, {"title": "The Relationship Between Unrolled Optimization and the IFT", "content": "Here, we (a) introduce the recurrence relation that arises when we unroll SGD optimization, (b)\ngive a formula for the derivative of the recurrence, and (c) establish conditions for the recurrence to\nconverge. Notably, we show that the fixed points of the recurrence recover the IFT solution. We\nuse these results to motivate a computationally tractable approximation scheme to the IFT solution.\nProofs of all the results are in Appendix A.4. Unrolling SGD optimization at an initialization $w_0$\ngives us the recurrence:\n$w_{i+1}(\u03bb) = F(\u03bb, w_i) = w_i(\u03bb) - \u03b1\\frac{\u2202L_T (\u03bb, w_i(\u03bb))}{\u2202w}$\n(3.6)\nWe provide a formula for the derivative of the recurrence, to show that it converges to the IFT under\nsome conditions.\nLemma. Given the recurrence from unrolling SGD optimization in Equation 3.6, we have:\n$\\frac{\u2202w_{i+1}}{\u2202\u03bb} = -\u03b1 \\sum_j (I - \u03b1 \\frac{\u2202^2L_T}{\u2202w\u2202w^T})^{-1}  \\frac{\u2202^2L_T}{\u2202w\u2202\u03bb}$ \nThis recurrence converges to a fixed point if the Jacobian of the fixed-point operator F$\\frac{\u2202F}{\u2202w} =I-\u03b1\\frac{\u2202^2L_T}{\u2202w\u2202w^T}$ is contractive by the Banach Fixed-Point Theorem (Banach, 1922). Theorem 2 shows\nthat the recurrence converges to the IFT if we start with locally optimal weights $w_0 = w^\u2217(\u03bb)$, and\nthe Jacobian of the fixed-point operator $\\frac{\u2202F}{\u2202w}$ is contractive. We leverage that if an operator U is\ncontractive, then the Neumann series $\\sum_{i=0}^\u221e U^k = (I-U)^{-1}$.\nTheorem 2 (Neumann-SGD). Given the recurrence from unrolling SGD optimization in Equation 3.6,\nif $w_0 = w^\u2217(\u03bb)$:\n$\\frac{\u2202w_{i+1}}{\u2202\u03bb} = \u03b1 \\sum[I-\u03b1  \\frac{\u2202L_T}{\u2202w\u2202w^T}]^j     \\frac{\u2202^2L_T}{\u2202w\u2202\u03bb} \\frac{\u2202w_{i+1}}{\u2202\u03bb}    w^*(\u03bb)$\nand if $I-\u03b1  \\frac{\u2202^2L_T}{\u2202w\u2202w^T}$ is contractive:\n$\\lim_{i \\to \u221e}\\frac{\u2202w_{i+1}}{\u2202\u03bb}=\u03b1[  \\frac{\u2202^2L_T}{\u2202w\u2202w^T}]^{-1}  \\frac{\u2202^2L_T}{\u2202w\u2202\u03bb} w^*(\u03bb)$ \nThis result is also shown in Shaban et al. (2019), but they use a different approximation for\ncomputing the hypergradient-see Table 3.1. Instead, we use the following best-response Jacobian\napproximation, where i controls the trade-off between computation and error bounds:\n$\\frac{\u2202w^\u2217}{\u2202\u03bb}  \\approx  \u03b1 \\sum[I-\u03b1  \\frac{\u2202^2L_T}{\u2202w\u2202w^T}]^j   \\frac{\u2202^2L_T}{\u2202w\u2202\u03bb}$     $w^*(\u03bb)$\nShaban et al. (2019) use an approximation that scales memory linearly in i, while ours is constant.\nWe save memory because we reuse last w i times, while Shaban et al. (2019) needs the last i w's.\nScaling the Hessian by the learning rate a is key for convergence. Our algorithm has the following\nmain advantages relative to other approaches:"}, {"title": "Scope and Limitations", "content": "The assumptions necessary to apply the IFT in our setup are as follows: (1) $L_v: \u039b \u00d7 W \u2192 R$ is\ndifferentiable, (2) $L_T: \u039b \u00d7 W \u2192 R$ is twice differentiable with an invertible Hessian at $w^\u2217 (\u03bb)$, and\n(3) $w^\u2217: \u039b \u2192 W$ is differentiable.\nWe need continuous hyperparameters to use gradient-based optimization, but many discrete\nhyperparameters (e.g., number of hidden units) have continuous relaxations (Maddison et al., 2017;\nJang et al., 2016). Also, we can only optimize hyperparameters that change the loss manifold, so our\napproach is not straightforwardly applicable to optimizer hyperparameters.\nTo exactly compute hypergradients, we must find $(\u03bb' , w')$ such that $\\frac{\u2202L_T}{\u2202w}|_{\u03bb' ,w'} = 0$, which we can\nonly solve to a tolerance with an approximate solution denoted $w^\u2217 (\u03bb)$. Pedregosa (2016) shows\nresults for how an error in the approximate solution affects the inversion."}, {"title": "Experiments", "content": "We first compare the properties of Neumann inverse approximations and conjugate gradient with\nexperiments similar to Liao et al. (2018); Maclaurin et al. (2015a); Shaban et al. (2019); Pedregosa\n(2016). Then", "tasks": 1}]}