{"title": "SCALABLE NESTED OPTIMIZATION FOR DEEP LEARNING", "authors": ["Jonathan Peter Lorraine"], "abstract": "Gradient-based optimization has been critical to the success of machine learning, updating a single set of parameters to minimize a single loss. A growing number of applications rely on a generalization of this, where we have a bilevel or nested optimization of which subsets of parameters update on different objectives nested inside each other. We focus on motivating examples of hyperparameter optimization and generative adversarial networks. However, na\u00efvely applying classical methods often fails when we look at solving these nested problems on a large scale. In this thesis, we build tools for nested optimization that scale to deep learning setups.\n\u2022 In Chapter 2, we provide an explicit, differentiable approximation to how neural network weights best-respond or optimize for their loss. We train a hypernetwork that takes in hyperparameters and outputs neural network weights. We use this hypernetwork for hyperparameter optimization.\n\u2022 In Chapter 3, we explore an algorithm that implicitly approximates the neural network weights best-response, using the implicit function theorem. We apply this to hyperparameter optimization, showing we can tune as many hyperparameters as neural network weights.\n\u2022 In Chapter 4, we augment simultaneous gradient descent to mitigate rotational dynamics. We do this by generalizing gradient descent with momentum to have a complex-valued momentum while retaining real-valued parameter updates.\n\u2022 In Chapter 5, we generalize strategies for finding diverse solutions in single-objective optimization to finding diverse solutions in setups where multiple agents optimize for their own objective. We do this by taking the Ridge-rider algorithm and generalizing their branching criteria to occur at bifurcations using connections to Lyapunov exponents.", "sections": [{"title": "Chapter 1", "content": "Motivating single-objective optimization in machine learning: In recent years, machine learning has emerged as a transformative force in numerous scientific and industrial domains, profoundly impacting everything from computer-vision, to natural language processing, to personalized medicine. Large neural networks have become a foundational workhorse for using machine learning to advance these fields. Central to this revolution has been the advancement of gradient-based optimization techniques, which have effectively trained increasingly large and complex neural network architectures. However, the traditional focus on optimizing a single set of parameters for a singular objective increasingly gives way to more nuanced paradigms.\nMotivating the generalized nested optimization paradigm in machine learning: A growing number of applications require learning with subsets of parameters updating on different objectives. Important examples are hyperparameter optimization, GANS, actor-critic models, curriculum learning, adversarial examples, learning models, domain adversarial adaptation, neural architecture search and meta-learning. This thesis focuses on motivating examples of hyperparameter optimization and GANs. Furthermore, we will call a setup with more than one objective a game, call optimization here learning in games, and each subset of parameters with their respective losses will be called a player.\nIntroducing nested optimization more formally: Consider 2-player games\u00b9 with players denoted by A and B where each player minimizes their loss LA, LB with their parameters \u0398\u0391, \u03b8\u0392. We work with setups where the objectives are nested inside of each other - so-called Stackelberg games or bilevel optimization  whose solutions can be defined as:\n0 = argmin LA (\u03b8\u03b1, \u03b8\u0463 (\u03b8\u03b1)),\nA\n\u0398\u0391\n\u03b8 (04) = argmin LB (0A, 0B)                                                                                                                                (1.1)\n\u04e8\u0432\nHere, 0 (0) denotes player B's best-response function, which says how their optimal parameters change as a function of the other players' parameters. In general, the inner and outer optimization can have multiple solutions giving rise to a best-response set instead of a function. However, for most of the thesis, we focus on the setup with unique solutions, except in Chapter 5, where we look at methods to find multiple valid solutions.\nExisting nested optimization approaches and their limitations: The overarching goal of this thesis is to efficiently find solutions (0,0) when our parameters A and B are approaching the size of modern neural network parameters. We may be able to approximately find efficiently if we can do gradient-based optimization on:\nL* (0\u2084) = La (04, 0(0))                                                                                                              (1.2)\nOptimizing this objective would require computing the hypergradient:\nJL*(04)\nJLALA\n00(04)  =  20 A  +  X 20 A  200 B \u0434\u0430     =\n204,0(04)\n\u0434 \u0435 \u0432 \u0434\u0430\n\u043404  \u04340(04) \u043404\nhypergradient\nindirect gradient\n* =      direct gradient                                                                                                                  (1.3)\nUnfortunately, this often requires 20%, but 0%(04) and its Jacobian 200 are typically intractable, as they require exact evaluation and differentiation through optimization. An alternative strategy is simulta-neous/alternating gradient descent, where each player does gradient descent on their own objectives -\nLa and LB respectively. However, strategies like this can fail. For example, because a gradient is\n20%                                                      20 A\nidentically zero as in hyperparameter optimization or because of rotational dynamics as in GANs. Black-box methods like random-search or Bayesian Optimization circumvent gradient calculations but are ineffective for high-dimensional problems - e.g., greater than 100 dimensions. Specifically, we want the ability to optimize high-dimensional problems prevalent in machine learning.\n1.1 Thesis Outline\nIn the Chapters 2 and 3, we explore approximations for the best-response %(0\u2084) and its Jacobian 20\n20,\n\uc694, which are applied to hyperparameter optimization. In Chapter 4, we instead look at augmenting\nthe gradient dynamics to avoid rotational dynamics, without approximating %, which is applied\nto GANs. Finally, we look at generalizing a branching single-objective optimization algorithm that finds multiple solutions to set-ups with multiple objectives.\n\u2022 In Chapter 2, we explicitly approximate the best-response function %(0) with a hypernetwork which we differentiate through for optimization. Our method collapses the nested optimization of model weights and hyperparameters into a joint stochastic optimization We do this via amortized optimization with hypernetworks, which output approximately optimal weights as a function of hyperparameters. We compare this with standard hyperparameter optimization and demonstrate its use in tuning hundreds to thousands of hyperparameters."}, {"title": "Chapter 2", "content": "Machine learning models often nest optimization of model weights within the optimization of hyperparameters. We give a method to collapse this nested optimization into joint stochastic optimization of weights and hyperparameters. We train a neural (hyper)network to output optimized weights as a function of hyperparameters. We compare this method to standard hyperparameter optimization methods and demonstrate its effectiveness in tuning thousands of hyperparameters.\nContext for this Work\nThis was my first paper, guided to a workshop submission and then a technical report by David. All material in this chapter comes from Lorraine and Duvenaud (2017). We had a follow-up paper, Self-Tuning Networks led by Matthew MacKay and Paul Vicol, which improves many aspects of this work. There, we build more scalable hypernetwork architectures, a better parameterization of the distribution of hyperparameters for training the hypernetwork, tuning hyperparameters of larger-scale networks in varying modalities, a wide variety of hyperparameters are optimized, and we demonstrate practical benefits from the method. Further improvements are in the follow-up of A-STN .\n2.1 Introduction\nModel selection and hyperparameter tuning are significant bottlenecks in the design of predictive models. Hyperparameter optimization is a nested optimization, which can be viewed as a special case of Equation 1.1. Here, the inner optimization finds the neural network's weights w that minimize the training loss Ly given hyperparameters \u5165. The outer optimization chooses to reduce the validation loss Ly:\nW* = argmin Cv (1) where                                                                                                                                                                                                                       (2.1)\n\u03bb\nLv(X) = Lv (l, w* (A)) and w* (A) = argmin LT (l, w)                                                                                                                                                                                                        (2.2)"}, {"title": "Chapter 3", "content": "We propose an inexpensive, gradient-based hyperparameter optimization algorithm that combines the implicit function theorem (IFT) with efficient inverse Hessian approximations. We present results on the relationship between IFT and differentiation through optimization, motivating our algorithm. We use our approach to train modern network architectures with millions of weights and millions of hyperparameters. Specifically, we learn a data-augmentation network\u2014where every weight is a hyperparameter tuned for validation performance\u2014that outputs augmented training examples; we learn a distilled dataset where every feature in each data point is a hyperparameter; and we tune millions of regularization hyperparameters. Jointly tuning weights and hyperparameters with our approach is only a few times more costly in memory and compute than standard training.\n3.1 Introduction\nNeural network generalization to unseen data crucially depends on hyperparameter choice. Hyperparameter optimization (HO) has a rich history , and has recently achieved successful scaling due to gradient-based optimizers. There are dozens of regularization techniques to combine in deep learning, and each may have multiple hyperparameters. If we can scale hyperparameter optimization to have as many\u2014or more\u2014hyperparameters as parameters, various exciting regularization strategies exist to investigate. For example, we could learn a distilled dataset with a hyperparameter for every feature of each input, weights on each loss term , or augmentation on each input.\nWhen the hyperparameters are low-dimensional simple methods, like random search, work; however, these break down for medium-dimensional hyperparameter optimization - e.g., 5-100 dimensions. We may use more scalable algorithms like Bayesian optimization, but this often breaks down for high-dimensional hyperparameter optimization\u2014e.g., >100 dimensions. Hypernetwork-based approaches, as in Chapter 2, scale further (~ 1000 dimensions), but break down when approaching the scale of modern neural networks. We can solve high-dimensional hyperparameter optimization problems locally with gradient-based optimizers, but this is difficult because we must differentiate through the optimized weights as a function of the hyperparameters. Formally, we must approximate the Jacobian of the best-response function of the parameters to the hyperparameters.\nWe leverage the Implicit Function Theorem (IFT) to compute the optimized validation loss gradient with respect to the hyperparameters, hereafter denoted the hypergradient. The IFT requires inverting the training Hessian for the neural network weights, which is infeasible for modern, deep networks. Thus, we propose an approximate inverse, motivated by a link to unrolled differentiation that scales to Hessians of large neural networks, is more stable than conjugate gradient, and only requires a constant amount of memory.\nFinally, when fitting many parameters, the amount of data can limit generalization. There are ad hoc rules for partitioning data into training and validation sets - e.g., using 10% for validation. Practitioners often re-train their models from scratch on the combined training and validation partitions with optimized hyperparameters, which can provide marginal test-time performance increases. We verify empirically that standard partitioning and retraining procedures perform well when fitting a few hyperparameters but break down when fitting many. When fitting many hyperparameters, we may need a large validation partition, which makes retraining our model with optimized hyperparameters vital for strong test performance.\n3.2 Overview of Proposed Algorithm\nThere are four essential components to understanding our proposed algorithm. Further background is provided in Appendix A.1, and the notation is shown in Table A.1.\n1. Hyperparameter optimization is nested optimization: Ly and Ly denote the training and validation losses, w the neural network weights, and the hyperparameters. We aim to find optimal hyperparameters \u5165* such that the neural network minimizes the validation loss after training:\n\u5165* = argmin C (A) where                                                                                                                               (3.1)\n\u03bb\nL*(x) = Lv (A, w* (1)) and w* (\u5165) = argmin LT(1, w)                                                                                                                                                           (3.2)\nW\nAs in Chapter 2, our implicit function w* (\u5165) is the best-response of the weights to the hyperparameters, and our desired objective is the best-responding validation loss C in red. Again, for simplicity, we assume unique solutions to argmin, and refer to non-thesis research of Vicol et al. (2022a) for analysis in the non-unique setup.\n2. Hypergradients have two terms: For gradient-based hyperparameter optimization we want the hypergradient(1), which decomposes into:\n* =      (    Lv (1,w*(x))  +  Lv + dLv dw*  )     \u03bb,w*(x)  hypergradient                                                                                                                                                                                                                                                (3.3)\n\u0434\u0445                                                                                                       dw d                                                                                                                                 \u03bb          hyperparam indirect grad.\nLv (\u03bb,w*(x))\n+  dw*(x)        x  dw*(x)\nhyperparam direct grad. parameter direct grad. best-response Jacobian\nThe direct gradient is easy to compute. However, the indirect gradient is difficult to compute because we must account for how the optimal weights change for the hyperparameters (i.e., w*(x)). In hyperparameter optimization the direct gradient is often identically 0, necessitating an approximation of the indirect gradient to make any progress (visualized in Figure 3.1). In Chapter 4, we look at algorithms that do not require response Jacobians for setups where the direct gradient is non-zero.\nCHAPTER 3. OPTIMIZING MILLIONS OF HYPERPARAMETERS BY IMPLICIT DIFFERENTIATION\n\u2022 We motivate existing inverse Hessian approximation algorithms by connecting them to iterative optimization algorithms.\n\u2022 We scale IFT-based hyperparameter optimization to large neural architectures, including AlexNet and LSTM-based language models.\n\u2022 We demonstrate several uses for fitting hyperparameters almost as easily as weights, including per-parameter regularization, data distillation, and learned data augmentation methods.\n\u2022 We explore how training-validation splits should change when tuning many hyperparameters.\n3.2 Overview of Proposed Algorithm\nThere are four essential components to understanding our proposed algorithm. Further background is provided in Appendix A.1, and the notation is shown in Table A.1.\n1. Hyperparameter optimization is nested optimization: Ly and Ly denote the training and validation losses, w the neural network weights, and the hyperparameters. We aim to find optimal hyperparameters \u5165* such that the neural network minimizes the validation loss after training:\n\u5165* = argmin C (A) where                                                                                                                               (3.1)\n\u03bb\nL*(x) = Lv (A, w* (1)) and w* (\u5165) = argmin LT(1, w)                                                                                                                                                           (3.2)\nW\nAs in Chapter 2, our implicit function w* (\u5165) is the best-response of the weights to the hyperparameters, and our desired objective is the best-responding validation loss C in red. Again, for simplicity, we assume unique solutions to argmin, and refer to non-thesis research of Vicol et al. (2022a) for analysis in the non-unique setup.\n2. Hypergradients have two terms: For gradient-based hyperparameter optimization we want the hypergradient(), which decomposes into:\n* =      (    Lv (1,w*(x))  +  Lv + dLv dw*  )     \u03bb,w*(x)  hypergradient                                                                                                                                                                                                                                                (3.3)\n\u0434\u0445                                                                                                       dw d                                                                                                                                 \u03bb          hyperparam indirect grad.\nLv (\u03bb,w*(x))\n+  dw*(x)        x  dw*(x)\nhyperparam direct grad. parameter direct grad. best-response Jacobian\nThe direct gradient is easy to compute. However, the indirect gradient is difficult to compute because we must account for how the optimal weights change for the hyperparameters (i.e., w*(x)). In hyperparameter optimization the direct gradient is often identically 0, necessitating an approximation of the indirect gradient to make any progress (visualized in Figure 3.1). In Chapter 4, we look at algorithms that do not require response Jacobians for setups where the direct gradient is non-zero."}, {"title": "4 Complex Momentum for Optimization in Games", "content": "We generalize gradient descent with momentum for optimization in differentiable games to have complex-valued momentum. We give theoretical motivation for our method by proving convergence on bilinear zero-sum games for simultaneous and alternating updates. Our method gives real-valued parameter updates, making it a drop-in replacement for standard optimizers. We empirically demonstrate that complex-valued momentum can improve convergence in realistic adversarial games-like generative adversarial networks-by showing that we find better solutions with an almost identical computational cost. We also show a practical complex-valued Adam variant, which we use to train BigGAN to improve inception scores on CIFAR-10.\n4.1 Introduction\nGradient-based optimization has been critical for the success of machine learning, updating a single set of parameters to minimize a single loss. A growing number of applications require learning with multiple players, each with parameters and objectives. Common examples are GANs hyperparameter optimization, actor-critic models, curriculum learning, adversarial examples, learning models domain adversarial adaptation, neural architecture search, and meta-learning.\nWe often want solutions where no player gains from changing their strategy unilaterally, e.g., Nash equilibria or Stackelberg equilibria. Classical gradient-based learning often fails to find these equilibria due to rotational dynamics, or identically zero direct gradients as in Chapters 2 and 3. There are numerous saddle point finding algorithms for zero-sum games.\nCHAPTER 4. COMPLEX MOMENTUM FOR OPTIMIZATION IN GAMES\nGidel et al. generalize GD with momentum to games, showing that we can use a negative momentum to converge if the eigenvalues of the Jacobian of the gradient vector field have a large imaginary part. We use terminology in Gidel et al. and say (purely) cooperative or adversarial games for games with (purely) real or imaginary eigenvalues. Setups like GANs are not purely adversarial but have both purely cooperative and adversarial eigenspaces - i.e., eigenspaces with purely real or imaginary eigenvalues. More generally, we say that an eigenspace is cooperative when the real part of the eigenvalue is larger than the imaginary part and is otherwise adversarial. In cooperative eigenspaces, classical optimization methods perform best, while in adversarial eigenspaces, methods customized for games work best. We want a method that converges with simultaneous and alternating updates in purely adversarial games a setup where existing momentum methods fail. Also, we want a method that robustly converges with different mixtures of adversarial and cooperative eigenspaces because finding all eigenspaces depends on an eigendecomposition that can be intractable. To solve this, we unify and generalize existing momentum methods to recurrently linked momentum a setup with multiple recurrently linked momentum buffers with potentially negative coefficients. Selecting two of these recurrently linked buffers with appropriate momentum coefficients can be interpreted as the real and imaginary parts of a single complex buffer and complex momentum coefficient see Appendix Figure B.ld. This setup (a) allows us to converge in adversarial games with simultaneous updates, (b) only introduces one new optimizer parameter - the phase or arg of our momentum, (c) allows us to gain intuitions via complex analysis, (d) is trivial to implement in libraries supporting complex arithmetic, and (e) robustly converges for different eigenspace mixtures.\nIntuitively, our complex buffer stores historical gradient information, oscillating between adding or subtracting at a frequency dictated by the momentum coefficient. Classical momentum only adds gradients, and negative momentum changes between adding or subtracting each iteration while we oscillate at an arbitrary (fixed) frequency see Figure 4.2. This reduces rotational dynamics during training by canceling out opposing updates.\nOur contributions include:\n\u2022 Providing generalizations and variants of classical (Polyak, 1964), negative and aggregated momentum for learning in differentiable games.\n\u2022 Showing our method converges on adversarial games, including bilinear zero-sum games and a Dirac-GAN, with simultaneous and alternating updates.\n\u2022 Illustrating a robustness during optimization, converging faster and over a larger range of mixtures of cooperative and adversarial games than existing first-order methods.\n\u2022 Giving a practical extension of our method to a complex-valued Adam variant, which we use to train a BigGAN to improve inception scores."}, {"title": "5 Lyapunov Exponents for Diversity in Differentiable Games", "content": "Ridge Rider (RR) is a method to find diverse solutions in optimization problems by following eigenvectors of the Hessian (\"ridges\"). RR is designed for conservative gradient systems (i.e., settings with a single loss/potential function), where it branches at saddles easy-to-find bifurcations. We generalize RR to nonconservative, multi-agent gradient systems by proposing the Generalized Ridge Rider (GRR) method to find arbitrary bifurcations. We give theoretical motivation for our method by leveraging tools from the field of dynamical systems. We construct novel toy problems where we visualize new phenomena while giving insight into high-dimensional problems of interest. Finally, we empirically evaluate our method by finding diverse solutions to the iterated prisoners' dilemma and machine learning problems, including generative adversarial networks.\n5.1 Introduction\nSelecting solutions with desirable properties that an arbitrary (global or local) minimum might not have is often useful. For example, finding solutions in image classification using shapes that generalize more effectively than textures. Important examples of this in single-objective minimization are seeking solutions that generalize to unseen data in supervised learning, in policy optimization, and generative models. Many real-world systems are not so simple and instead involve multiple agents, each using a different subset of parameters to minimize their own objective. Some examples are generative adversarial networks, actor-critic models, curriculum learning hyperparameter optimization, adversarial examples learning models, domain adversarial adaptation, neural architecture search, multi-agent settings and meta-learning. In these settings, the aim is to find one equilibrium - of potentially many equilibria - where the agents exhibit some desired behavior.\nCHAPTER 5. LYAPUNOV EXPONENTS FOR DIVERSITY IN DIFFERENTIABLE GAMES\nFor example, in the iterated prisoners' dilemma, solutions favoring reciprocity over unconditional defection result in higher returns for all agents. In GANs, solutions often generate a subset of the modes of the target distribution and in Hanabi, some solutions coordinate better with humans. Existing methods often find solutions in small subspaces even after many random restarts, as shown in Table 5.1. By finding a diverse set of equilibria, we may be able to (a) find solutions with a better joint outcome, (b) develop stronger generative models in adversarial learning, or (c) find solutions that coordinate better with humans.\nRecently, Ridge Rider (RR) proposed a method to find diverse solutions in single-objective optimization. RR is a branching tree search, which starts at a stationary point and then follows different eigenvectors of the Hessian (\"ridges\") with negative eigenvalues, moving downhill from saddle point to saddle point. In settings where multiple agents each have their own objective (i.e., games), the relevant generalization of the Hessian the game Hessian in Equation 5.6 is not symmetric. Thus, the game Hessian generally has complex eigenvalues (EVals) with associated complex eigenvectors, making RR not directly applicable.\nIn this chapter, we generalize RR to multiagent settings using machinery from dynamical systems. We connect RR with methods for finding bifurcation points, i.e., points where small changes in the initial parameters lead to different optimization dynamics and learning outcomes. We propose novel metrics inspired by Lyapunov exponents that measure the speed with which learning trajectories separate. These metrics generalize the branching criterion from RR, allowing us to locate a broad class of potential bifurcations and find more diverse behavior. Starting points with rapid trajectory separation can be found via gradient-based optimization by differentiating through the exponent calculation, which is implemented efficiently for large models with Jacobian-vector products. Our contributions include:\n\u2022 Connections between finding diverse solutions and Lyapunov exponents, allowing us to leverage a broad body of work in dynamical systems.\n\u2022 Proposing Generalized Ridge Rider (GRR), scaling Ridge Rider (RR) to differentiable games.\n\u2022 Presenting novel diagnostic problem settings based on high dimensional games like the iterated prisoners dilemma (IPD) and GANs to study various bifurcation types.\n\u2022 Compared to existing methods, GRR finds diverse solutions in the IPD, spanning cooperation, defection, and reciprocity.\n\u2022 Lastly, we present larger-scale experiments on GANS a model class of interest to the machine learning community."}, {"title": "Chapter 6", "content": "This thesis introduces new methods for scalable nested optimization in various relevant problems within machine learning. First, we give methods based on hypernetworks and implicit differentiation for computing the best-response Jacobian, which we used to calculate approximate hypergradients for large-scale hyperparameter optimization. Next, we augmented classical first-order optimization methods to reduce rotational dynamics when applied to nested optimization problems. Finally, we generalize an algorithm that branches gradient-based optimization at bifurcations to work with nonconservative dynamics often present in nested optimization.\n6.1 Summary of Chapters\nIn Chapter 2, we address tuning hyperparameters using gradient-based optimization by replacing the training optimization loop with a differentiable hypernetwork. We presented a simple and scalable method that jointly optimizes both hyperparameters and hypernetwork weights.\nIn Chapter 3, we present a gradient-based hyperparameter optimization algorithm that scales to high-dimensional hyperparameters the size of modern, deep neural networks. We use the implicit function theorem to formulate the hypergradient whose bottleneck is inverting the training loss Hessian with respect to the neural network parameters. We approximate inverse-Hessian-vector products using a relationship with unrolled differentiation.\nIn Chapter 4, we generalize existing momentum methods for learning in differentiable games by allowing complex momentum with real-valued updates. Our method robustly converges in games with a different range of eigenspace mixtures than existing methods. We also presented a practical generalization of our method to the Adam optimizer, which we used to improve BigGAN training.\nIn Chapter 5, we introduce Generalized Ridge Rider, an extension of the Ridge Rider algorithm to settings with multiple losses. In these settings, we showed that a broader class of bifurcation points needs to be considered and that GRR indeed discovers them in various problems. We also provide an empirical justification for our method using tools from the dynamical systems literature, allowing us to find arbitrary bifurcations. Experimentally, GRR obtains various solutions in multi-agent settings, such as the iterated prisoner's dilemma.\n6.2 Limitations and Future Directions\n6.2.1 Hyperparameter Optimization Through Hypernetworks\nLimitations\nThe most scalable architecture proposed in this work is a linear hypernetwork, which will not scale when the output weights are the size of modern neural networks. We look at smarter factorized networks in non-thesis research MacKay et al. that scale to large modern networks. Another limitation is that it is difficult to set the scale of the hyperparameters sampled during training, which is again partially resolved by adaptive scales in non-thesis research MacKay et al. (2019a). Unlike Bayesian optimization, this method does not facilitate uncertainty-aware exploration and requires a differentiable validation loss. A further limitation is that this method relies on sampling hyperparameters, which will not scale for ultra-large hyperparameter regimes, i.e., hundreds of thousands to millions of hyperparameters.\nFuture directions\nFuture directions could include combining this amortized optimization framework into other nested optimization setups besides hyperparameter optimization \u2013 ex., meta-learning schemes like MAML or (small-scale) GANs. Alternatively, we could combine these methods with other strategies for approximating the best-response, like unrolled or implicit differentiation. Furthermore, optimizing thousands of hyperparameters raises the question of hyper-regularization, or regularization of hyperparameters. We could also look for improved ways to formulate hypernetwork training, as in Bae and Grosse (2020).\n6.2.2 Optimizing Millions of Hyperparameters by Implicit Differentiation\nLimitations\nFor implicit differentiation, we can only optimize hyperparameters that change the loss manifold, so our approach is not straightforwardly applicable to optimizer hyperparameters. Also, we need continuous hyperparameters to use gradient-based optimization, but many discrete hyperparameters (e.g., number of hidden units) have continuous relaxations. We cannot exactly compute the hypergradients since we must find (X',w') such that, which we can only solve to a tolerance with an approximate solution denoted w*(\u5165). Some hyperparameters fail when we retrain the model with the final fixed value. Furthermore, this method does not facilitate uncertainty-aware exploration. Unlike Bayesian optimization, we also require a differentiable validation loss, which can directly optimize validation objectives such as accuracy. We also observed that the introduction of millions of hyperparameters can easily overfit the validation dataset. Some noisy hyperparameters, such as the continuous relaxation of discrete parameters as in MacKay et al.(2019a), did not work robustly with this method \u2013 perhaps due to a higher sensitivity to noise. It would be better to investigate the effects of stochasticity in our results, for example, when we should compare to CG or re-sample batches in our hypergradient evaluation. As such, this does not scale to settings such as generating the code for our model as a hyperparameter as in Zhang et al. (2023a).\nFuture directions\nWe believe algorithms of this nature provide a path for practical nested optimization, where we have Hessians with a known structure. We seek to understand why some hyperparameters fail to retrain, which is partially solved in the non-thesis work of Vicol et al. (2022a). Since na\u00efvely introducing hyperparameters causes validation overfitting, we should find useful ways to introduce many hyperparameters. Further, we should investigate useful cross-validation data splits when using many hyperparameters and when we want to retrain with the final hyperparameters. Investigating other matrix-inversion methods besides the Neumann series could also be fruitful.\n6.2.3 Complex Momentum for Optimization in Games\nLimitations\nOur method stores real and imaginary components in the momentum buffer \u03bc, so we use more memory than standard momentum. Our method is more challenging to analyze, as our augmented dynamics requires wielding the roots of a cubic instead of a quadratic. Our method introduces a new hyperparameter - the imaginary components or arg of \u1e9e \u2013 that needs to be tuned. For some games, we need higher-order information than first-order to converge - ex., pure-response games because the first-order information for a player is identically zero. So, momentum methods that only use first-order info, like the direct gradient, will not generally converge.\nFuture directions\nWe should consider which kind of spectrum gradient optimization induces in nested optimization setups of interest, such as GANs, to guide our algorithm design. Furthermore, we should find useful approximations of these empirical spectrums that are tractable to analyze yet still provide useful results. We should also look to get a closed-form solution for the optimal learning rate and momentum as a function of the spectrum so that we can tightly bound the convergence rates for our method. Additionally, we should explore other, more general recurrently linked momentum setups to see which achieves the best convergence rates on spectrums of interest.\n6.2.4 Lyapunov Exponents for Diversity in Differentiable Games\nLimitations\nOur method does not have guarantees if our optimization process does not converge ex., when the learning rate is too high, when there is stochasticity, or when we have solutions as the parameters tend to infinity. Furthermore, our approaches to estimating the exponent involve simulating trajectories for a finite horizon, which we must tune. If the horizon is too long, our gradients become too sharp and have a limited signal, while if the horizon is too short, we may not find bifurcations. Further, instead of approximating the maximum eigenvalue of the sum of Jacobians in optimization trajectories, we tractably approximate the maximum eigenvalue for each Jacobian. We approximate the maximum eigenvalue via a power iteration. We also do gradient descent on our approximate Lyapunov exponent, which finds approximate solutions, and it is unclear how far we have to move in each direction to move across the bifurcation. Further, scaling to large-scale setups like GANs proves challenging, especially when optimizing the exponent. We should find strong use cases for which we want to find diverse solutions in nested optimization setups."}]}