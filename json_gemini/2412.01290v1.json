{"title": "Learning Smooth Distance Functions via Queries", "authors": ["Akash Kumar", "Sanjoy Dasgupta"], "abstract": "In this work, we investigate the problem of learning distance functions within the query-based learning\nframework, where a learner is able to pose triplet queries of the form: \u201cIs xi closer to xj or xk?\" We establish\nformal guarantees on the query complexity required to learn smooth, but otherwise general, distance functions\nunder two notions of approximation: w-additive approximation and (1 + w)-multiplicative approximation.\nFor the additive approximation, we propose a global method whose query complexity is quadratic in the size\nof a finite cover of the sample space. For the (stronger) multiplicative approximation, we introduce a method\nthat combines global and local approaches, utilizing multiple Mahalanobis distance functions to capture local\ngeometry. This method has a query complexity that scales quadratically with both the size of the cover and\nthe ambient space dimension of the sample space.", "sections": [{"title": "1 Introduction", "content": "In personalized retrieval or, more generally, personalized prediction, a machine learning system needs to\nadapt to a specific user's perceptions of similarity and dissimilarity over a space X of objects (products,\ndocuments, movies, etc). Suppose the relevant structure is captured by a distance function d : X \u00d7 X \u2192 R.\nHow can this be learned through interaction with the user?\nThis is one of the basic questions underlying the field of metric learning (Kulis, 2013). A key design\nchoice in the interactive learning of distance functions is the nature of user feedback. One line of work\nassumes that the user is able to provide numerical dissimilarity values d(x, x') for pairs of objects x, x' (Cox\nand Cox, 2008; Stewart et al., 2005); the goal is then to generalize from the provided examples to an entire\ndistance function. Another option, arguably more natural from a human standpoint, is for the user to provide\ncomparative information about distances (Schultz and Joachims, 2003; Jamieson and Nowak, 2011). For\ninstance, given a triplet of objects, x, x', x\", the user can specify which of d(x, x') and d(x, x\") is larger.\nThis is the model we consider.\nEarlier work in metric learning has focused primarily on learning Mahalanobis (linear transformations)\ndistance functions (Weinberger and Saul, 2009; Xing et al., 2002; Davis et al., 2007; Goldberger et al., 2004).\nWe look at the more general case of smooth but otherwise arbitrary distance functions d over X C RP. We\nshow a variety of results about the learnability of such distances using triplet queries.\nThe learning protocol operates as follows: on each round of interaction,"}, {"title": "2 Related Work", "content": "There is a rich literature, spanning many decades, on methods for inferring the geometry of an unknown\nspace, such as the internal semantic or preference space of a user. Early work dating back to the 1960s\nincludes the psychometric literature on multidimensional scaling, which showed how to fit Euclidean distance\nfunctions to user-supplied dissimilarity values. More recently, it has become standard practice to train neural\nnet embeddings using pairs of dissimilar and similar data points.\nThe geometry that is learned can be of three forms:\n\u2022 A distance function, typically Mahalanobis distance.\n\u2022 A similarity function, typically a kernel function.\n\u2022 An embedding, nowadays typically a neural net.\nThe information used to learn these can consist of exact distance or similarity values, as in classical multidi-\nmensional scaling, or comparisons such as triplets and quadruples.\nLearning a distance function Distance learning has been extensively studied in two main categories: (1)\nexact measurement information, where precise pairwise distances are provided (e.g., in metric multidimen-\nsional scaling) (Cox and Cox, 2008; Stewart et al., 2005), and (2) pairwise constraints informed by class\nlabels, where the goal is to learn a distance function under the assumption that similar objects share the\nsame class or cluster (Weinberger and Saul, 2009; Xing et al., 2002; Davis et al., 2007; Goldberger et al.,\n2004). Further work has explored learning with ordinal or triplet constraints, where only relative comparisons\nbetween samples are provided (Schultz and Joachims, 2003; Kleindessner and von Luxburg, 2016), and\nqueries involving ordinal constraints (Jamieson and Nowak, 2011).\nOne specific class of distance functions, the Mahalanobis distance, defined through a linear transformation,\nhas received considerable attention in metric learning. In supervised contexts, studies such as Weinberger and\nSaul (2009), Xing et al. (2002), Davis et al. (2007), and Goldberger et al. (2004) have focused on designing\nMahalanobis distance functions that minimize intra-class distances while maximizing inter-class distances.\nThis has been shown to improve prediction performance when integrated with hypothesis classes (Shaw\net al., 2011; McFee and Lanckriet, 2010). However, unlike these works, which rely on class supervision, our\napproach seeks to learn a target Mahalanobis or general distance function without any side information. In\nunsupervised metric learning, classical methods like linear discriminant analysis (LDA) (Fisher, 1936) and\nprincipal component analysis (PCA) (Jolliffe, 1986) solve the problem by approximating a low-dimensional\nstructure without requiring labeled data. Our work diverges from these methods, as we assume the learner\nqueries triplets to obtain qualitative feedback, rather than having direct access to linearly transformed data.\nGeneralization bounds for metric learning in i.i.d. settings have also been explored for various loss\nfunctions (Verma and Branson, 2015; Mason et al., 2017; Ye et al., 2019; Lei et al., 2020; Cao et al., 2016).\nOf particular relevance to our work are the studies by Schultz and Joachims (2003) and Mason et al. (2017),\nwhich use triplet comparisons to learn Mahalanobis distance functions. Our approach differs from Mason\net al. (2017), as we allow the learner to select triplets and receive exact labels, while they use i.i.d. triplets\nwhere the oracle may provide noisy labels. Additionally, Shalev-Shwartz et al. (2004) addressed learning\nMahalanobis distances in an online setting. For comprehensive surveys on Mahalanobis and general distance\nfunctions, refer to Bellet et al. (2015) and Kulis (2013).\nLearning an embedding Learning appropriate representations or embeddings for a given dataset has been\na central problem in machine learning, studied under various frameworks such as metric and non-metric\nmultidimensional scaling, isometric embedding, and dimensionality reduction, among others. Classical\napproaches have addressed this problem in both linear (Jolliffe, 1986; Cox and Cox, 2008) and nonlinear\nsettings (Belkin and Niyogi, 2003; Tenenbaum et al., 2000; Roweis and Saul, 2000), with the primary\nobjective of uncovering a suitable low-dimensional representation of the data's inherent structure. In contrast,\nour work focuses on using an embedding of the data space in the form of a cover that globally approximates\nthe underlying latent distance function."}, {"title": "3 Preliminaries", "content": "We denote by X a space of objects/inputs. We consider a general notion of distance on this space.\nDefinition 1 (distance function) A bivariate function d : X x X R is a distance function if d(x, x') \u2265 0\nand d(x,x) = 0 for all x, x' \u2208 X.\nIn this work, we are interested in various families of distance functions: distances on finite spaces; Maha-\nlanobis distance functions; and smooth distances. For the latter two cases we will assume X C RP.\nMahalanobis distance function Assume that X CRP. A Mahalanobis distance function is characterized\nby a symmetric, positive semidefinite matrix (written M > 0, or alternatively M \u2208 Sym+(RP\u00d7P)), and is\ndenoted by dm, where for all x, x' \u2208 X,\n$d_{M}(x, x') = \\sqrt{(x - x')^\\top M (x - x')}.$ (3.1)\nWe denote the family of Mahalanobis distance functions by\n$M_{maha} = \\{d_M : X \\times X \\to \\mathbb{R}_{\\geq 0} \\mid M \\in Sym_{+}(\\mathbb{R}^{p \\times p})\\}$\nComparisons of distances In this work, we obtain information about a target distance function on X via\ntriplet comparisons. For any triplet x, y, z \u2208 X and distance function d, we define the label l((x, y, z); d) \u2208\n{-1,0, 1} to be the sign of d(x, y) \u2013 d(x, z).\nThe label of a triplet corresponds to the three possibilities d(x, y) < d(x, z), d(x,y) = d(x, z), and\nd(x,y) > d(x, z). From a human feedback standpoint, it would also be reasonable to allow the labels to\nbe either < or >. However, these relaxed labels would not be sufficiently informative for our purposes; for\ninstance, they are always satisfied by the trivial distance function d(x, x') = 1(x \u2260 x').\nQuery learning a distance function: Given a family of distance functions M, the learner seeks to learn a\ntarget distance function d within this space by querying the user/oracle. The learner is allowed rounds of\ninteraction in which it adaptively pick triplets (x, y, z) \u2208 X\u00b3 and receives their labels. We use the notation\n(x, y, z)e to denote a triplet (x, y, z) along with its label l((x, y, z); d). We call this setup learning a distance\nfunction via queries.\nGiven the family M we are interested in understanding the minimal number of triplet queries that the\nlearner needs so that it can learn d either exactly or approximately. To make this precise, we start by defining\ntriplet equivalence between two distance functions."}, {"title": "4 Additive approximation of a smooth distance function", "content": "In this section, we show how to learn a smooth distance function d up to approximate triplet equivalence in\nthe w-additive sense.\nThe learner's strategy is to take an e-cover C of the instance space X (with respect to l2 distance) and to\nlearn a distance function that is exactly triplet-equivalent to d on C. This distance function is then extended to\nall of X using l2 nearest neighbor."}, {"title": "4.1 Learning a distance function on a finite space, upto triplet equivalence", "content": "In Theorem 5, we demonstrate how any distance function on a finite space (e.g., d on C) can efficiently be\nlearned upto exact triplet-equivalence. The full proof is deferred to Appendix B.\nTheorem 5 Given any distance function d : X \u00d7 X \u2192 R>0 on a sample space X, and a finite subset\nX\u3002CX, a learner can find a distance function d : X \u00d7 X \u2192 R>0 that is triplet-equivalent to d on X\u3002\nusing |X|2 log |X\u3002| triplet queries.\nProof outline: There are only O(|X|\u00b3) triplets involving points in X, so at most this many queries are\nneeded. To get a better query complexity, we use a reduction to comparison-based sorting.\nFix any x\uff61 \u2208 X. In order to correctly answer all triplet queries of the form (x,, x', x\") for x', x\" \u2208 X,,\nit is both necessary and sufficient to know the ordering of the distances {d(x,, x) : x \u2208 X}. In other words,"}, {"title": "4.2 A notion of smooth distance function", "content": "Let C be an e-cover of the instance space X, with respect to l2 distance. Theorem 5 shows how to learn a\ndistance function d on C that is exactly triplet-equivalent to the target distance function d on C. To extend\nd to all of X, we use l2 nearest neighbor. For any x \u2208 X, let c(x) be its nearest neighbor in C, that is,\narg minc\u2208c ||x \u2212 c||. We will treat c(x) as a stand-in for x.\nThis can be problematic if d(x, c(x)) is large even though ||x - c(x)|| \u2264 \u20ac. We now introduce a\nsmoothness condition that precludes this possibility.\nDefinition 6 ((\u03b1, L, \u03b4)-smooth) Suppose X C RP. We say a distance function d : X \u00d7 X \u2192 R>o is\n(\u03b1, L, \u03b4)-smooth for a, L, \u03b4 > 0 if for any x, x' \u2208 X such that ||x \u2212 x\u2032||2 \u2264 d we have\n$d(x, x') \\leq L \\cdot ||x - x'||_2$\nWe'll see that this smoothness condition holds for distance functions with bounded third derivatives.\nAssumption 1 (local smoothness) Suppose X C RP. For x \u2208 X, define fx : X \u2192 R by fx(x') = d(x,x').\nAssume that for all x \u2208 X, the function fx is C\u00b3 in an open ball around x and that its third partial derivatives\nare bounded in absolute value by a constant M > 0. Let H be the Hessian of fx at x.\nThe following result follows from the Taylor expansion of fr. We will make heavy use of it."}, {"title": "Lemma 7 (Taylor's theorem)", "content": "Fix X C RP. If distance function d : X \u00d7 X \u2192 R>0 satisfies Assumption 1,\nthen for any x, x' \u2208 X, we have\n$|d(x, x') - \\frac{1}{2} (x' - x)^\\top H_x(x' - x)| \\leq \\frac{Mp^\\frac{3}{2}}{6} ||x - x'||_2^3,$\nwhere H denotes the Hessian of the function d(x,\u00b7) at x. Furthermore, He is a symmetric, positive\nsemidefinite matrix, i.e. H \u2265 0.\nThis lemma suggests that for a point x \u2208 X, the distance function in the vicinity of x is well-approximated\nby the Mahalanobis distance\n$d(x, x') \\approx \\frac{1}{2} (x' -x)^T H^* (x' - x).$\nLet's take this a step further, by considering\n$(x' \u2013 x)^\\top H_x(x' \u2013 x) \\leq \\lambda_{max} (H_x) \\cdot ||x \u2013 x'||^2$\nwhere Amax(H\u2082) denotes the largest eigenvalue of H. If these eigenvalues are uniformly upper-bounded by\na constant y > 0, then it follows from Lemma 7 that the distance function d is (a, L, \u03b4)-smooth for a = 2,\n\u03b4 = 1, and L = (\u03b3/2) + (Mp3/2/6)."}, {"title": "4.3 Learning a smooth distance upto additive approximation", "content": "Under (\u03b1, L, \u03b4)-smoothness of the target distance d, it is enough to learn a distance function d that is triplet-\nequivalent to d on a finite cover C of X, and then extend it to the rest of the space using l\u2082 nearest neighbor.\nSee Algorithm 1.\nDistance Model: Consider a distance function model Mnn = {(d, (C, dc), l2)} where every distance\nfunction d \u2208 MNN is parameterized by a finite subset C C X and a distance function de : C \u00d7 C \u2192 R>o.\nThen d : X \u00d7 X \u2192 R>0 has the following form:\n$d(x,y) = d_C(c(x), c(y)) \\text{ where } c(x) = \\arg \\min_{c \\in C} ||x - c||_2.$\nFor this family of distance functions, we show that a cua-cover of the space is sufficient to learn an\n(\u03b1, L, \u03b4)-smooth d, where c depends on the smoothness parameters of the distance function. To achieve\nthe formal guarantee, we also require a directional form of the triangle inequality in addition to (a, L, \u03b4)-\nsmoothness, which is formalized in the following.\nAssumption 2 (triangle inequality) Suppose X C RP. We say a distance function d defined on X satisfies\ntriangle inequality\u00b9 over X if for all x, y, z \u2208 X, d(x, y) + d(y, z) \u2265 d(x, z).\nRemark 8 The requirement of this directional triangle inequality is for the ease of analysis. With a minor\nmodification to the triangle inequality, such as:\n$\\forall x, y, y' \\in X, |d(x, y) \u2013 d(x,y')| < C \\cdot |||y \u2013 y'|||_2,$\nfor constants C, a > 0, we can lift the requirement of Assumption 2. As discussed in the later sections, this\nallows this work to include KL divergence in the analysis."}, {"title": "Theorem 9", "content": "Consider a compact subset X C RP. Consider a distance function d : X \u00d7 X \u2192 R> that is\n(a, L, 1)-smooth for a, L > 0 and satisfies Assumption 2. Then, Algorithm 1 outputs a distance function\nd'\u2208 Mnn triplet equivalent to d up to w-additive approximation. The query complexity to find such a\nd' is (N(X, \u20ac, l2)2 log N(X, \u20ac, l2)) triplet queries where e = min(1, (w/4L)1/a) and N(X, \u20ac, l2) is the\ncovering number of the space X with e-balls in l2 norm.\nProof outline: As outlined in Algorithm 1, the query strategy involves constructing an e-cover of the space,\ndenoted by CC X. For any samples x, y, z \u2208 X, the nearest neighbors c(x), c(y), c(z) in C are determined.\nLeveraging the smoothness property, the distances of the form d(x, c(x)) and d(c(x), x) (even though d need\nnot be symmetric, local smoothness guarantees a bound) can be bounded by L \u00b7 \u0454\u00ba, since ||x \u2212 c(x)||2 \u2264 \u20ac.\nWith the chosen cover radius and the triangle inequality, it follows that |d(x, y) \u2013 d(c(x), c(y))| \u2264 2L \u00b7 \u0454\u00ba.\nFinally, we need to ensure that the error bound of 2Le\u00ba is a constant factor smaller than w, which is\nguaranteed by the choice of e specified in the theorem.\nThe results in Theorem 9 can be specialized to the case of distances that satisfy Assumption 1, using the\nremarks that follow that assumption."}, {"title": "Corollary 10", "content": "Consider a compact subset X C RP and a distance function d : X \u00d7 X \u2192 R>0 that satisfies\nAssumption 1 and Assumption 2. Furthermore, suppose that for all x \u2208 X, Amax(H) is upper bounded\nby a constant \u03b3 > 0. Then, Algorithm 1 outputs a distance function d' \u2208 Mnn that is triplet-equivalent\nto d up to an w-additive approximation, using at most N(X, \u20ac, l2)\u00b2 log N(X, e, l2) triplet queries, where\n$ \\epsilon < (\\frac{\\omega'}{2\\gamma + 4K})^\\frac{2}{\\alpha}$ for $\\omega' := \\min\\{1,\\omega\\}$ and $K := \\frac{M p^{1.5}}{6}$"}, {"title": "5 Multiplicative approximation of a smooth distance function", "content": "Here, we aim to learn a target distance d up to multiplicative approximation. This is a stronger notion\nthan additive approximation, where the learner only needs to correctly distinguish distances between points\nx,x',x\" \u2208 X such that the distances differ by at least w; in particular, small distances can be ignored. In\ncontrast, in the multiplicative approximation case, the learner must learn to distinguish distances that are\narbitrarily small, if they differ by a multiplicative factor of (1 + w), for any arbitrary but fixed choice of\n\u03c9 > 0.\nAn intuition from the previous section is that if the distances are indeed large, then a global approximation\nbased on a cover with a suitable choice of l2 norm radius should suffice. However, the challenge remains\nin how to approximate small local distances for arbitrary distance functions. A natural approach is to\napproximate using local Mahalanobis distance functions. Indeed, Lemma 7 indicates that under mild\nconditions, locally, d behaves as a Mahalanobis distance function up to some additive error. We show that\nthis local linear approximation can be used to obtain w-multiplicative approximation.\nIn this pursuit, we first demonstrate how to query learn local Mahalanobis distances with quadratic\ndependence on the ambient space dimension."}, {"title": "5.1 Learning a Mahalanobis distance function via queries", "content": "Although prior work has studied the problem of learning Mahalanobis distances with triplet and pairwise\ncomparisons in the i.i.d setting (Wang et al., 2024; Mason et al., 2017; Schultz and Joachims, 2003) and\nadaptive embedding learning with pairwise comparisons (Jamieson and Nowak, 2011), the query complexity\nin the query framework is underexplored. We show that a binary search-based method, along a predefined set\nof directions in RP, can statistically efficiently approximates a target matrix in Mmaha up to a linear scaling,\ndiscussed concretely in Algorithm 2.\nFirst note that positive semidefinite matrices M and cM, for c > 0, yield Mahalanobis distance functions\ndm and dem that are triplet-equivalent. Thus we can only hope to recover a target matrix M* upto a\nscaling factor. Moreover, since each query yields a constant number of bits of information, the recovery will\nnecessarily be approximate. We achieve approximation in Frobenius norm: for any desired \u20ac > 0, we use\ntriplet queries to find a matrix M \u2265 0 such that ||M \u2013 \u0442\u041c*|| \u2264 6, where the constant 7 is chosen so that the\nlargest diagonal element of TM* is 1.\nIn particular, we first identify the coordinate vector \u2022 y \u2208 {e1, ..., ep} for which yT M*y is maximized;\nthis requires p triplet queries of the form (0, ei, ej). We then use queries to obtain p(p + 1)/2 1 other\n(approximate) linear constraints to return a matrix M such that y My = 1 and solve the resulting system to\nget the distance matrix.\nWe state the claim of the result in Theorem 11 with a detailed proof deferred to Appendix D. In what\nfollows, \u043a(M*) is the condition number of matrix M* and we assume it has been normalized so that its\nlargest diagonal entry is 1.\nTheorem 11 Fix an input space X C RP, an error threshold e > 0, and a target matrix M* \u2208 Mmaha.\nThen, Algorithm 2 run with \u20acalg = 2p2 outputs a distance function dm \u2208 Mmaha such that\n$||\\tau M^* \u2013 M||_F < \\epsilon$\nusing $p(\\frac{p+1}{2}) \\log \\frac{2p^2 \\kappa(M^*)^2}{\\epsilon^2} + p$ triplet queries, where t is a scaling factor so that $\\max_i (\\tau M^*)_{ii} = 1$.\nProof outline: There are degrees of freedom for a given symmetric matrix. If these degrees of freedom\nare correctly approximated, it becomes possible to learn the matrix within a controlled error. This is the\ncentral idea of the proof of the theorem, where we construct a basis B of rank-1 matrices {uzu\u0142} for the space\nSym(RP\u00d7P). One of these ui's is the coordinate vector y described above. For each remaining ui, we use\nbinary search and triplet queries to approximately find the value c = (u7M*ui)/(yTM*y); in particular,\nwe find \u0109i = c + O(\u20ac) using log \u03ba(M*)2/e queries. We then solve the linear system u Mu\u2081 = \u0109\u017cy\u0142 My to\nget M and project the result to the semidefinite cone. To bound the Frobenius norm between M* and M,\nwe analyze the condition number of the linear system of equations."}, {"title": "Query Learning a Local Mahalanobis Distance with a Smooth Distance Function", "content": "Theorem 11 assumes\nthat triplet queries are answered with respect to a target Mahalanobis distance dM*. However, we will be\nusing Mahalanobis distance to locally approximate a smooth distance function in the vicinity of particular"}, {"title": "5.2 (1 + w)-multiplicative approximation via local Mahalanobis distance functions", "content": "For a (1 + w)-multiplicative approximation, we integrate global and local strategies: a cover of the space\ncombined with local Mahalanobis distance functions at the centers of the cover. Algorithm 1 describes\nlearning a smooth distance function up to triplet equivalence using a finite distance function, while Algorithm 2\napproximates local Mahalanobis distances, as shown in Theorem 12. These approximations are combined\nthrough a switch determined by a threshold that is a function of w. The query strategy is outlined in\nAlgorithm 3. For any pair x, y \u2208 X, a local Mahalanobis distance (y \u2013 x)THc(x) (y \u2013 x) is computed to\ndecide whether to use the global or local distance function, with the threshold depending on w. This achieves\nthe (1 + w)-multiplicative approximation of a smooth distance function, as formally proven in Theorem 14.\nIn this section, we consider a distance model that combines local and global distances as follows:\nDistance Model: Consider a distance function model defined for a threshold 0, denoted as M(0) =\n{(d, (C, dc), (Uc, dMc)\u2208c, l2)}, where each distance function d \u2208 M(0) is parameterized by a finite set\nof centers CCX, along with a distance function de defined on C. For each center c \u2208 C, there is a local\nMahalanobis distance function dM, defined on its neighborhood U C X, with Mc \u2265 0. The overall distance\nfunction d takes the form:\n$d(x,y) = \\begin{cases}\n  d_C(c(x), c(y)) + \\theta, & \\text{if } d_{M_{c(x)}}(x, y) > 0, \\\\\n  d_{M_{c(x)}} (x, y), & \\text{otherwise,}\n\\end{cases}$,\nwhere c(x) is the center such that x \u2208 U (ties are broken with smallest l2 distance). The goal is to\napproximate a smooth distance function up to a (1 + w)-multiplicative factor within M(0).\nBefore stating the main result, we discuss two regularity conditions on the curvature and arbitrarily small\ndistances of a distance function.\nAssumption 4 (Hessian continuity) Consider a distance function d : X \u00d7 X \u2192 R> that is C2 in its second\nargument. We say d is Hessian continuous if there exists a scalar L > 0 such that\n$\\forall x,x'\\in X, ||H_x - H_{x'}||_F \\leq L \\cdot ||x - x'||_2.$"}]}