{"title": "How to do impactful research in artificial intelligence for chemistry and materials science", "authors": ["Austin H. Cheng", "Cher Tian Ser", "Marta Skreta", "Andr\u00e9s Guzm\u00e1n-Cordero", "Luca Thiede", "Andreas Burger", "Abdulrahman Aldossary", "Shi Xuan Leong", "Sergio Pablo-Garc\u00eda", "Felix Strieth-Kalthoff", "Al\u00e1n Aspuru-Guzik"], "abstract": "Machine learning has been pervasively touching many fields of science. Chemistry and materials science are no exception. While machine learning has been making a great impact, it is still not reaching its full potential or maturity. In this perspective, we first outline current applications across a diversity of problems in chemistry. Then, we discuss how machine learning researchers view and approach problems in the field. Finally, we provide our considerations for maximizing impact when researching machine learning for chemistry.", "sections": [{"title": "Introduction", "content": "Machine learning (ML) has been applied in many facets of chemistry, and its use is rapidly growing. We argue in this perspective that despite this dramatic growth and impact, ML could be employed better and more extensively. Current work is still far from exhausting the potential of ML to advance theory and application in chemistry in terms of breadth, depth, and scale. In addition, the actual types of problems that ML could tackle, such as hypothesis generation or enabling internalized scientific understanding, are still areas of active research or open problems."}, {"title": "Chemistry meets data: A taxonomy of problems", "content": "To color a picture of the field, we begin by outlining a taxonomy of the chemical problems to which ML has been applied, ranging from prediction, generation, synthesis, force fields, spectroscopy, reaction optimization, and foundation models. Shifting gears, we then introduce types of problems in ML and show how chemical problems can be reformulated as instances of ML problems. These standard problems help organize the toolbox of algorithms and theory provided by ML. Digging further into this perspective, we examine differences in practices and values between the ML and chemistry communities and highlight where collaboration and cross-pollinating perspectives can advance both fields. Armed with the above, we can then discuss how to select impactful applications of ML in chemistry and recommend our suggested good practices for research in this area.\nChemistry, and science in general, involves data in one form or another. Not surprisingly, then, data science is integral to chemistry. Machine learning, a sub-field of data science, has become an integral tool in our domain science's arsenal. Therefore, it is crucial to begin cataloguing and organizing critical efforts to date.\nWe suggest a taxonomy of the chemical problems to which machine learning has been applied. As shown in Figure 1, ML has been applied to solve various chemical problems by encoding and decoding to and from chemical structure, properties, 3D structure and dynamics, and experimental data. For reasons of"}, {"title": "Structure to property: property prediction", "content": "Chemistry has leveraged data to predict properties from a chemical structure long before the everyday use of the term \u201cmachine learning\". This field has been originally identified initially as cheminformatics. These tools sought to store, retrieve, and model chemical structures. Early examples began in 1957 with substructure searches in a database, followed by simple multivariate regression for learning quantitative structure-activity relationships (QSAR) between molecular descriptors like Hammett constants and partition coefficients, and biological activity. These were mostly property-activity relationships \u2013 the first structure-activity relationships involved local explanations analyzing how substituents on a ring affected activity, which could be generalized to many scaffolds via substructural analysis. Eventually, computers automatically encoded molecular structures as fingerprints - bit-vectors that store the presence or absence of many substructures found in the molecule. These fingerprints were useful in encoding molecular structures to predict molecular activity in simple models such as support vector machines.\nWhile chemists have a conceptual understanding of the effects of functional groups on the properties of a molecule, communicating this information to a model is critical to ensure that the model is predictive. Expert descriptors infuse chemical knowledge derived from experiments or conceptual knowledge into the features provided to a model and have achieved good predictive performance, especially in low-data regimes. These expert descriptors also generalize well outside the model's training set, as chemical knowledge is baked into these features. As early as 1937, Hammett fitted sigma parameters for predicting the influence of chemical substituents on reactivity. Additionally, group contribution methods, which assume that structural components or functionalizations behave the same way across many different molecules, parameterize these components into numerical features that can be used to predict molecular properties. The discipline has since grown to involve molecular fingerprinting techniques and the incorporation of 2D and even 3D information for use in prediction. In more recent times, as the properties of a homogeneous transition metal catalyst are strongly influenced by the ligands attached to it, parameterizing the structural and electronic features of these phosphine ligands has also been successful in predicting the properties of a catalyst. Looping back to historical models, recent work has also been able to leverage density functional theory (DFT) and machine learning to successfully machine learn Hammett parameters.\nModels have become more complex with advances in computational hardware, moving from simple linear regression models to complex architectures like auto-encoders, generative adversarial networks, graph neural networks or transformers. Instead of relying on"}, {"title": "Limits and open problems", "content": "Despite the great strides made in molecular machine learning, the ability of machine learning models to extrapolate beyond the data it is trained on is still limited, posing barriers for application to novel chemistries. Several approaches can potentially bridge these gaps. For example, by using physics-informed models that can contain fundamental representations that help in generalizing the representation itself to satisfy some symmetries or properties related to the physical laws of nature. Active learning is also a powerful tool for expanding datasets on the fly by capturing computational or experimental data for extrapolation. Additionally, while models have progressively performed better on property prediction benchmark tasks, these benchmarks represent only a tiny subset of chemical tasks, making their performance on various other tasks unknown. While we have attempted to create benchmarks more representative of typical tasks, this is still not a central focus of the community.\nStructure-to-property models have been widely employed in screening projects, leading to experimentally verified predictions. We will discuss a few selected case studies in Sec. 2.2.1."}, {"title": "Property to structure: designing molecules in chemical space", "content": "While the rational design paradigm analyzes the relationship between structure and properties to design promising molecules, another paradigm asks: what are all the possible molecules that satisfy a given property? Solving this question is known as the inverse design problem.\nChemical space is the set of all synthesizable molecules and is often cited for having an astronomical size of at least 1033 to 1060 molecules. Within this vast space are potential drugs that could cure current diseases and putative materials that could enable a sustainable future."}, {"title": "Virtual screening", "content": "A simple approach to navigating chemical space is to enumerate a feasible set of possible options and then narrow them down to the best solution. This shift in perspective has its experimental implementation employing strategies such as high-throughput screening of chemical libraries and combinatorial chemistry to synthesize these libraries. Given the astronomical size of chemical space, it became clear that arbitrarily searching through compounds would produce few promising hits, making this approach inefficient as the cost of extensive chemical synthesis campaigns is often taxing or prohibitive. This motivated virtual screening and computational search funnels as a way to filter out unpromising compounds, leaving only the best candidates for synthesis and testing. In drug discovery, molecules are filtered out with computationally lean checks such as high molecular weight or problematic functional groups, followed by more computationally intensive docking for estimating binding affinity, ultimately narrowing down to a handful of lead compounds. Scaling the size of virtual libraries increases the likelihood of promising hits, which has motivated ever-larger screening campaigns requiring increasing computational resources. One example was the Harvard Clean Energy Project, in which we searched through 107 candidates with quantum chemistry calculations on distributed volunteer computing to search for efficient organic photovoltaics.\nSimilarly, VirtualFlow docked over 10\u00ba molecules by efficiently using thousands of CPU cores. As the size of chemical libraries grows, with the required computational resources scaling linearly, hierarchical approaches to evaluate the fitness of individual synthetic building blocks offer a way past linear scaling."}, {"title": "Generative models for inverse design", "content": "As the size of chemical libraries surpasses 1015 molecules and becomes computationally prohibitive to screen, ML offers ways to consider large search spaces without simulating all molecules. For example, in a chemical library, many molecules should have similar structures and properties, so running simulations on every molecule is redundant. A formal way to handle this is to simulate a portion of the library and then train property prediction models on this subset, which should be generalized across the library. Since these property prediction models are computationally cheaper than simulations, they can be evaluated for the entire library and used to prioritize candidates for simulation. We leveraged this approach to design organic light-emitting diodes that were verified experimentally.\nHowever, another arm of ML offers a way to consider all (or a vast subset) of the chemical space. Given a dataset of molecules in a representation such as SMILES strings, generative models learn to generate strings which resemble the"}, {"title": "Limits and open problems", "content": "While candidates can be generated easily with such models, the quality of the candidates depends on the ability to develop a properly performing and scalable cost function for conditioning the generative models. Additionally, these models are trained on approximate metrics, which means that their real-life performance still has to be evaluated. Thus, evaluating the synthesizability of a candidate or providing steps to make candidates is of paramount importance (see next section).\nMost generative models have been developed with simple benchmarks in mind, such as predicting simple properties like logP. However, developing using proper benchmarks (such as Tartarus) or restricting them to feasible sets of molecules, such as those synthesizable with self-driving labs (see Sec. 2.7), remains a challenge."}, {"title": "Structure to structure: synthesis planning and reaction condition prediction", "content": "Synthesis planning - i.e. finding synthetic pathways that give rise to a desirable target molecule \u2013 is an open challenge that chemists have faced for over a century, particularly in the \u201cmolecular world\" of drug discovery, agricultural chemistry or molecular materials chemistry. This problem is complex in two respects: First, predicting the outcome of a specific unseen reaction, given all reactants, reagents, and reaction conditions, is effectively an unsolved problem to date. Second, even with such a \"reaction prediction\" tool at hand, finding feasible multi-step sequences of reactions that eventually enable the synthesis of the target molecule from cheap and commercially available precursors requires searching a massive network of possible pathways. Additional challenges arise from practical demands to the synthesis planning problem: efficiency, cost, waste production, sustainability, safety, or toxicity are practical concerns, especially in an industrial setting."}, {"title": "Synthesis planning", "content": "Synthesis planning is classically addressed through the formalism of retrosynthesis, as pioneered by Nobel Prize winner E. J. Corey: Using knowledge of chemical reactivity, the target molecule is gradually disconnected into progressively simpler precursors, which eventually yields commercially available starting materials. Formally, this corresponds to a tree search problem. As early as in the 1960s, Corey realized that this approach is ideally suited to be tackled in a computational manner. Since then, a number of expert systems have been developed to guide this tree search.\nThe past decade has seen significant progress in addressing this challenge using the toolbox of ML. In this context, the key \"decision policy\" has often been treated as a multi-task regression problem: Given the structure of a target molecule, a ML model is trained to predict an applicable reaction out of a catalog of reactions. This symbolic approach, however, requires a pre-defined catalogue of all reaction types, often referred to as reaction \"rules\" or \"templates\", which itself presents new obstacles. There is neither a generally accepted definition of the term \u201creaction rule\u201d nor an unambiguous procedure to perform reaction rule extraction from data. Alternatively, \u201ctemplate-free\u201d approaches to the one-step reaction prediction problem, predict reactions as graph edits in the starting material graph, or solve a sequence-to-sequence \u201cproduct-"}, {"title": "to-starting-material\u201d translation task", "content": "Notably, these models (template and template-free) can be similarly trained in the forward direction, predicting reaction products from starting materials.\nThese single-step prediction models have been used to build tree search models, which aim to solve the full synthesis planning problem. In this context, a Monte-Carlo tree search is usually the method of choice. Following the pioneering works from Segler et al. and Coley et al., a number of mostly open-source systems have been released."}, {"title": "Prediction and optimization of reaction conditions", "content": "What is often overlooked in synthesis planning is that knowing a possibly suitable reaction type alone does not guarantee that the envisioned intermediate or target product can be prepared from the proposed starting materials. The question if the product can be obtained (ideally in high yield), crucially depends on what is often referred to as the reaction conditions: the choice of reagent(s), catalyst(s), additive(s) and solvent, the values of continuous parameters such as stoichiometries, temperature and reaction time, as well as the practical details of running the reaction in the laboratory. In an ideal scenario, an AI-assisted tool would take in a new \"starting-material-to-product\" transformation, and spit out the required reaction conditions for this transformation. However, this is yet to be achieved, particularly because reaction conditions cover a vast combinatorial parameter space and are frequently governed by underlying physical principles that are difficult to simulate. In practice, reaction conditions are often selected by employing \"nearest-neighbor\" reasoning based on literature precedents, either automatically or through human expertise.\nMachine learning approaches to reaction condition optimization have thus mainly focused on regression modelling of reaction yields as a function of reaction conditions. In this context, data-driven approaches have intersected with regression techniques from physical organic chemistry, which attempt to model reaction outcomes based on mechanistic considerations. In highly constrained condition spaces, purely data-driven, supervised learning of product yields on systematically generated data from high-throughput experimentation has shown promising results. For example, our work on optimizing the E/Z ratio of a reaction relevant to pharmaceutical process chemistry showed that only with \u2248 100 experiments we were able to outperform what had been the state-of-the-art for this process by human-only reaction optimization. Meanwhile, the use of literature data for the same purpose is highly flawed, usually necessitating individual, case-by-case reaction optimization (see below for a more detailed discussion). Black-box optimization algorithms, particularly Bayesian Optimization (BO), have become increasingly prominent over the past decade. In BO, probabilistic models for predicting reaction yields are built through Bayesian inference with existing data. These models then iteratively guide decision-making throughout the optimization process. The idea of iterative, closed-loop optimization with ML-based surrogate models is discussed further in Sec. 2.7. For condition optimization, these iterative approaches have demonstrated remarkable success in increasingly complex synthetic reaction scenarios. At the same time, chemistry-specific challenges, such as the identification of conditions which are \"generally applicable\" to a wide variety of substrates, as opposed to just one or a few model substrates, have inspired algorithmic advances in the field. No-"}, {"title": "Limits and open problems", "content": "While the field of ML-based synthesis planning has seen significant algorithmic advances during the past ten years, its practical utility has remained limited to the development of relatively simple target molecules and short synthetic routes. In fact, as of today, expert systems, which involve manually coding reaction types and applicability rules, represent the state of the art in computer-aided synthesis planning. In particular, Grzy- bowski's Chematica system (now commercialized as Synthia) has had impressive experimental applications, even in complex natural product synthesis, or supply-chain-aware synthesis planning. In principle, while ML-based al- gorithms should be capable of providing similar or superior synthetic routes com- pared to these expert systems, the current shortcomings can mainly be attributed to deficiencies in the quality and quantity of available synthesis data and algorithmic limitations in extracting structured knowledge from the data. We and others have extensively discussed these factors recently.\nSimilar data limitations have also been discussed in the context of reaction outcome and reaction condition prediction. Patent data and even commercial databases are highly problematic not only because of erroneous, inconsistent or unstructured data reporting: Human biases in the reported experiments, particularly the accumulation of prominent conditions and the lack of low-yielding records, have prevented predictive modelling of reaction yields from literature data. Community-driven, open source data repositories such as the Open Reaction Database represent an essential step towards less biased and more holistic data collection \u2013 but such initiatives require a more digitized mindset in the way data is generated, collected and reported in synthetic organic chemistry laboratories.\nA further consequence of this data deficiency is the lack of representative benchmark problem sets. This applies to multi-step synthesis planning, where benchmarks are urgently needed for a more quantitative evaluation of synthesis planning performance. Similarly, optimization algorithms for chemical reactiv- ity would benefit from representative benchmarks to evaluate how standard BO algorithms translate to the intricacies of chemical reactivity. Most importantly, such benchmarks must reflect real-life problems, as identified by expert chemists, in order to inspire and motivate algorithmic ML advances to tackle the challenges in computer-aided organic synthesis."}, {"title": "Structure to physics: simulation and 3D structure", "content": "Machine learning has enabled data-driven solutions to both experimental prob- lems and computational problems. Whereas organic chemistry emphasizes molecules' 2D molecular graph structure, molecules are also grounded in 3D physical reality by the Schr\u00f6dinger equation, providing a rich theory of quantum mechanics and statistical mechanics for predicting molecular properties and interactions. Simu- lation methods such as density functional theory (DFT) and molecular dynamics (MD) then use this theory to computationally predict molecular properties and in- teractions. However, despite continual increases in computing power, these sim- ulations remain computationally costly, which has restricted simulation to small"}, {"title": "Neural network potentials", "content": "A significant shift came with the introduction of neural force fields. By train- ing neural networks on DFT data to predict energy and forces directly from 3D nuclear coordinates, molecular dynamics could now be propagated at ab initio ac- curacy at a much lower computational cost. Since forces must be equivariant to the molecule's rotation - i.e. if the molecule is rotated, the molecular forces must \"rotate along with it\" \u2013 this motivated the development of equivariant neural architectures to respect this symmetry. Neural force fields have been com- petitively benchmarked in ML, continually comparing different architectures and methods on several benchmarks. A detailed timeline of development of these equivariant architectures is given in Duval et al. As datasets of energy and forces have grown, such as the Open Catalyst Benchmark, neural force fields have started striving for universal applicability."}, {"title": "Predicting wavefunctions and electron densities", "content": "An alternative to predicting energies with force fields is to predict the wavefunction or electron density itself. The advantage is that these objects contain energy and the rest of the system's physical observables. For example, neural networks can be trained to predict the Hamiltonian matrix directly from the nuclear coordinates. Diagonalizing the Hamiltonian matrix gives the molecular orbitals, which com- prise the wavefunction. Furthermore, self-consistent field iteration can be initial- ized using the predicted wavefunction, allowing faster convergence of the quan- tum chemistry. Recently, it was shown that neural networks can be trained so that their output satisfies the self-consistency equation, bypassing the need for labels of Hamiltonian matrices.\nFurthermore, neural networks can be used as ans\u00e4tze to represent the wave- function itself directly. In this case, the network takes as input electron coordi- nates, and outputs wavefunction amplitude. Using the same stochastic optimiza- tion algorithms, neural wavefunctions can be trained to minimize the variational energy and satisfy the Schr\u00f6dinger equation. This approach has recently"}, {"title": "Predicting and generating 3D structure", "content": "Even if fast and accurate force fields were available, many problems rely on finding energetically preferred conformations of molecules. However, conformational space remains huge and cannot be practically enumerated, especially for large systems like proteins. Sim- ilarly, when modelling chemical reactions, the sizeable conformational search space makes it challenging to identify transition states. To solve these problems, ML approaches can predict and generate 3D structure directly.\nThe large conformational search space motivates generative models to nav- igate this space. Unconditional generative models such as equivariant diffusion models can generate 3D atomic positions and atom types simultaneously. For the problem of conformer search, which seeks stable 3D configurations for a given molecule, atom types can held constant while generation is conditioned on the 2D molecular graph. Some approaches generate atom positions freely, while other approaches generate torsion angles of rotatable bonds. Recent work has shown that forgoing both torsional and rotational symmetry constraints can yield better results, but at a higher cost. A related task known as docking performs conformer search of a ligand inside a protein pocket, as an estimate of binding affinity. This has also been approached with diffusion models.\nIn the problem of crystal structure prediction, the goal is to find the most sta- ble periodic arrangement of atoms for a given composition. While traditional approaches search through all stable configurations of coordinates and lattice vectors to find the lowest energy structure, equivariant diffusion models have found a natural fit for this problem, diffusing both coordinates and lattice pa- rameters simultaneously, while also enforcing space group constraints to enhance performance further. Indeed, scaling this diffusion approach to large datasets enabled inverse design to satisfy multiple desired properties simultane- ously.\nIn the fields related to the simulation of biomolecules, 3D structure predic- tion problems are abundant. The longstanding problem of predicting folded 3D protein structure from protein sequence has, to a certain extent, been solved by AlphaFold2. and related models. Building on this approach, diffusion mod- els have generated protein backbones represented as sequences of rigid bodies of residues. These models have been so successful that they have been used to design proteins satisfying structural constraints, which have been exper- imentally validated. The scope of these diffusion models has expanded to all biomolecules, with methods predicting how proteins, RNA, DNA, and ligands assemble in 3D atomistic detail, subsuming the task of docking, and hence, promising to become a de-facto conditioning function for drug discovery in the future."}, {"title": "Enhanced sampling and coarse-grained simulation", "content": "While finding the most stable geometry is useful, truly modelling the thermodynamic interac- tions between molecules requires sampling the equilibrium distribution of 3D structures. Equilibrium states follow a Boltzmann distribution with respect to"}, {"title": "Limits and open problems", "content": "While neural force fields can achieve great accuracy, they still require enough training data to cover the entire phase space. Without complete coverage, neural force fields can stumble into unstable dynamics. One benchmark emphasizes that force fields should be judged by their dynamics, not their force errors.\nHowever, these issues may begin to go away as neural forces are trained on ever larger datasets in the quest for universal force fields. Though ML models are limited by the quality of their data, the fact that new data can be generated by simulation paints a promising picture for data availability and large models.\nAt the same time, much work remains to reach simulation at large length and time scales. The most significant challenges of proper equilibrium sampling under metastable conditions and the related problem of rare-event sampling also"}, {"title": "Structure and analysis: spectroscopy and elucidation", "content": "One natural yet underexplored area of ML application in chemistry is structure elucidation, which aims to predict 2D or 3D molecular structures from spectro- scopic or other analytical data. Just as computer vision enables computers to perceive the natural world, computational spectroscopy could allow machines to perceive the molecular world through analytical instruments. The anticipated in- crease in the synthesis of de novo and unknown compounds through advances in experimentation automation drives the need for faster yet accurate structure elucidation to fully support these autonomous molecular and reaction discovery platforms."}, {"title": "Forward spectral prediction", "content": "The most straightforward approach to data-driven structure elucidation is to store a library of spectra, search for a match in the library for a given spectrum, and then retrieve the corresponding struc- ture. To increase the coverage of the library, forward spectral prediction can be used to predict spectra given chemical structure. While physical simulation offers a grounded way to predict spectra, it can be difficult and computation- ally expensive. An alternative approach leverages machine learning to predict spectrum from structure, for a variety of types of spectra, including mass spec- trometry (MS), nuclear magnetic resonance (NMR), and ultraviolet- visible spectroscopy (UV-vis). Some frame the forward prediction problem as formula prediction, employing either autoregressive models or a fixed vo- cabulary of formulas ; while others focus on subgraph prediction, utilizing recursive fragmentation, autoregressive generation, and deep probabilistic mod- els, or incorporate 3D structural information. In the context of mass spectra, some methods approximate the spectrum as a sequence of discrete bins with corresponding peak intensities, reducing the problem to a task of re- gressing the mass spectrum directly from structure. In addition to structure- to-spectrum prediction, another approach involves predicting structure-property relationships by estimating various molecular descriptors \u2013 ranging from scalars (e.g., energy, partial charges) to vectors (e.g., electric dipoles, atomic forces), and higher-order tensors (e.g., Hessian matrix, polarizability, octupole moment) and then using these descriptors to predict different spectra, including IR, Ra- man, UV-Vis, and NMR."}, {"title": "Structure elucidation", "content": "On the other side is the inverse problem of di- rectly predicting chemical structure from a given spectrum. DENDRAL was the first expert system for inferring chemical structure from mass spectra in 1969. Chemists also used ML to analyze infrared (IR), nuclear magnetic resonance (NMR), and mass spectra for identifying limited sets of functional groups. While these methods provide helpful structural insights, they are insufficient for fully elucidating molecular structures.\nCombining information of many inferred functional groups has enabled struc- ture elucidation. For NMR data, the molecular structure can be elucidated by first identifying molecular substructures and functional groups, which are then"}, {"title": "Limits and open problems", "content": "As with all data-hungry approaches, one key issue remains universal: While simulated spectra can be obtained in large quantities, it is crucial to consider if the model performs well on experimental spectra, which often exhibit more significant variability and inconsistencies. A"}, {"title": "Leveraging scale with foundational models for chemistry", "content": "With increasing computational power, machine learning models have been trained on progressively larger datasets. At scale, ML offers qualitatively different ca- pabilities. Foundation models are large-scale models that have been trained on a broad spectrum of data and can be applied to a variety of downstream tasks. Sev- eral general-purpose foundation models \u2013 such as ChatGPT, Gemini, and Llama \u2013 are typically utilized for language and image generation; many of these are language-only models or models trained on multiple modalities. However, using these models in the chemical domain presents unique challenges, and so many have trained their models from scratch on chemical data, but this is not trivial either. In this section, we will describe the current state of foundation models in chemistry and give our perspective on remaining open questions."}, {"title": "Transforming knowledge with large language models and agents", "content": "Some of the earliest applications of generative models to chemistry have been via language, which was enabled by the fact that molecules can be represented with strings using SMILES notation. Preliminary chemistry language mod- els were trained in an unsupervised manner on SMILES representations, which learned dependencies between molecular subfragments. More recently, models have also been concurrently trained on other molecular modalities rep- resented by text tokens, such as textual descriptions, scientific papers, synthesis procedures, commonly with autoregressive losses to be able to generate molecule descriptions or structures at inference time. Ramos et al. wrote a com- prehensive review detailing 80 chemistry/biochemistry language models to date for further reading. One motivation behind incorporating textual descriptions is that they contain information about functional properties of molecules, which can be useful for improving the embedding representations of molecules that are structurally similar but functionally different, or vice versa. They also enable in- teraction with models using natural language, which is a more intuitive interface for many users than rigid queries. Additionally, LLMs have been utilized"}, {"title": "Limits and open problems", "content": "In the case of the domain sciences, we are not as privileged as in the domain of natural language or images, which already has internet-scale data available. Scientific data is scarce; every data point must be an experiment or a high-quality simulation. If simulations are employed, the model must find a way to translate their results to specific experimental condi- tions. We suspect that universal models across chemistry are still a decade away and will perhaps be a moving target as humans continue to demand more of them. This is analogous to the problem of widening highways where many analysts have shown that as soon as a road is widened, the additional created demand due to its availability makes the highway full of traffic immediately."}, {"title": "Closed-loop optimization and self-driving labs", "content": "As ML applications continue to evolve, the necessity and scarcity of high-quality data become increasingly apparent. The advent of chemical digitization and advances in ML have laid the groundwork for combining ML with automated data generation through robotic experimentation. This synergy has given rise to the concept of the self-driving laboratory (SDL). SDLs are primarily composed of two critical components: automated laboratory equipment and experimental planners, both of which lever- age ML techniques to enhance their functionality. The ultimate goal is to au- tonomously execute the scientific method, encompassing hypothesis generation (ML), hypothesis testing (experimentation), and hypothesis refinement (ML), po- tentially allowing for the exploration of vast design spaces in a data-efficient manner.\nSignificant advancements in automated laboratory equipment have been achieved by integrating ML with computer vision, leading to the concept of \u201cgeneral chemistry robots.\u201d These ML-trained robots can make decisions based on ex- ternal feedback, enabling the dynamic automation of chemical operations tradi- tionally performed by human chemists. Given the inherent challenges in training robotic equipment for active decision-making based on external feed- back, a notable innovation in this area is the use of digital twins-virtual repli- cas of laboratory setups-that provide a robust framework for accelerating the training of robotic ML models. These digital twins simulate chemical scenar- ios with high fidelity, creating a realistic feedback loop that accelerates the model's learning process.\nOn the experimental planning side, heuristic techniques are being pro- gressively replaced by ML optimization algorithms. When combined with chem- ical digitization, these optimization techniques can identify target chemicals and optimize reaction conditions while significantly reducing the number of ex- perimental steps required. Among the various ML optimization techniques, Bayesian optimization has gained particular prominence in experimental chemistry due to its success in chemical applications. Machine-learning-based surrogate models, which predict the properties of chemicals and reactions, have been instrumental in this success, with documented examples in both pro- cess optimization and materials discovery.\nMoreover, the rise of LLMs has further enhanced the auxiliary components of SDLs. LLMs have been effectively used to create human-machine interfaces that bypass traditional coding, enabling more natural communication between chemists and laboratory systems a significant advantage for users who may not be well-versed in coding or data processing."}, {"title": "Limits and open problems", "content": "As discussed by us recently, the chal- lenges facing SDLs can be broadly categorized into two areas: motor (hardware- related) and cognitive (AI-related).\nMotor challenges. The primary hardware challenges stem from the human- centric design of chemical instruments and the lack of seamless interconnection between existing automated modules. As a result, most SDLs operate semi- automatically, requiring human intervention for tasks such as sample transfer, maintenance, and troubleshooting. Various solutions have been proposed to ad-"}, {"title": "Problems meet methods: a machine learning perspective on solving chemical problems", "content": "There is already a wealth of resources on how to apply the specifics of machine learning in several books, reviews, and internet resources. In this section, we provide a high-level perspective of how ML researchers and communities view and tackle problems. To start, we reclassify the diverse chemical problems introduced above as instances of well-established ML problems. To elaborate the ML perspective, we gather common themes and practices in the ML community and examine them in light of application to chemistry, highlighting points to consider related to benchmarking, the role of domain knowledge, and community values."}, {"title": "The toolbox of machine learning", "content": "ML provides a toolbox of algorithms and theory for solving problems using data. ML has formalized a set of well-defined problems to solve diverse tasks in lan- guage, vision, audio, video, tabular data, scientific data, and other domains. Each problem establishes a set of input requirements and a desired goal, which has"}, {"title": "The benefits of a toolbox", "content": "A shared problem interface enables clear and broad benchmarking of many different algorithms. One example can be seen in Table 1 of Song et al., who propose a new class of generative models and extensively compares their method to 27 different generative models of different"}, {"title": "Tools can be stacked on top of each other", "content": "ML problems are also in- terwined with each other. Generative models, like diffusion models, use neural networks trained to regress denoising steps. Agents are built on top of generative text models, while the core of the generative model itself is a neural network pre- dicting the next token. All these networks are trained using stochastic optimiza- tion methods like Adam, while black-box optimization is used to choose network hyperparameters. Sampling algorithms, black-box optimization, and agents can also incorporate generative models trained on previous data, improving the data generation quality.\nThe problems enumerated in Table 1 are not an exhaustive list. Other prob- lems include uncertainty quantification, which is helpful in Bayesian optimiza- tion and active learning, federated learning for combining industrial phar- maceutical data while preserving privacy, representation learning for gener- ally applicable molecular descriptors, causal learning, retrieval, and compres- sion."}, {"title": "Themes and practices in the ML community", "content": "Solving chemical problems can be aided by both high-level perspectives and community practices. To contextualize ML perspectives on algorithm develop- ment, we describe common themes and practices in the ML community, such as benchmarking, extreme interdisciplinarity, and the bitter lesson of deep learning. All of these are expanded below."}, {"title": "The role of benchmarking", "content": "Benchmarking plays a crucial role in the ML development process, driving the continuous improvement of models and methods. The ML community highly values methods that improve on the state of the art. With at least three major computer science conferences annu- ally (NeurIPS, ICML, and ICLR), incremental advances are frequent. These mi- nor, iterative improvements on established benchmarks often accumulate to gain significant performance gains over time. For researchers, benchmarks provide a clear metric for assessing which components of a model most affect performance, enabling more focused and impactful developments.\nA prominent feature of ML research is the use of leaderboards, where pro- posed methods are ranked based on their performance against established bench- marks. Papers must either advance or be competitive with the state of the art to be accepted at major conferences. This process has driven notable progress in various domains, from image classification and machine translation to image generation, and even solving Olympiad math problems. Leveraging this mechanism, the Open Catalyst Project, set a benchmark for neural network potentials to relax organic adsorbates on metal surfaces. This project provided a dataset much larger than encountered before, which motivated the continual development of more powerful equivariant architectures. From 2020 to 2023, the success rate of predicting adsorption energy grew from 1% to 14%, with current models now becoming useful in predicting adsorption. An- other benchmark called Matbench Discovery has initiated an arms race of neural force fields on the industry level.\nHowever, while benchmarking is a powerful tool, it is essential to be crit- ical of its applicability to chemistry. Domain experts are uniquely positioned to define practical benchmarks that can translate to real-world outcomes in the lab. Too often, ML literature presents problem settings that, while optimized for computational performance, may be unrealistic for experimental validation. This misalignment can lead to a scenario where the focus shifts from solving the actual problem to merely advancing ML techniques. As methods mature and benchmarks become saturated, new, more relevant benchmarks must arise.\nUltimately, defining and framing problems for ML researchers is a critical task. It involves proposing important questions and calls to action in a way that is accessible to the broader ML community. By doing so, chemists can guide the development of ML tools more likely to have practical applications in exper- imental research. While creating datasets and benchmarks can be seen as rote work, it can spur progress on difficult problems by leveraging community ef- forts of the ML community. Suppose a chemical problem can be crystallized and packaged into a clearly and appropriately benchmarked ML problem. Chemists can now wonder"}]}