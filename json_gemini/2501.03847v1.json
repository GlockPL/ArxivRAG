{"title": "Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control", "authors": ["ZEKAI GU", "RUI YAN", "JIAHAO LU", "PENG LI", "ZHIYANG DOU", "CHENYANG SI", "ZHEN DONG", "QIFENG LIU", "CHENG LIN", "ZIWEI LIU", "WENPING WANG", "YUAN LIU"], "abstract": "Diffusion models have demonstrated impressive performance in generating high-quality videos from text prompts or images. However, precise control over the video generation process-such as camera manipulation or content editing-remains a significant challenge. Existing methods for controlled video generation are typically limited to a single control type, lacking the flexibility to handle diverse control demands. In this paper, we introduce Diffusion as Shader (DaS), a novel approach that supports multiple video control tasks within a unified architecture. Our key insight is that achieving versatile video control necessitates leveraging 3D control signals, as videos are fundamentally 2D renderings of dynamic 3D content. Unlike prior methods limited to 2D control signals, DaS leverages 3D tracking videos as control inputs, making the video diffusion process inherently 3D-aware. This innovation allows DaS to achieve a wide range of video controls by simply manipulating the 3D tracking videos. A further advantage of using 3D tracking videos is their ability to effectively link frames, significantly enhancing the temporal consistency of the generated videos. With just 3 days of fine-tuning on 8 H800 GPUs using less than 10k videos, DaS demonstrates strong control capabilities across diverse tasks, including mesh-to-video generation, camera control, motion transfer, and object manipulation. Codes and more results are available at https://igl-hkust.github.io/das/.", "sections": [{"title": "1 INTRODUCTION", "content": "The development of diffusion generative models [Blattmann et al. 2023; Brooks et al. 2024; Ho et al. 2020; Lin et al. 2024; Rombach et al. 2022; Zheng et al. 2024b] enables high-quality video generation from text prompts or a starting image. Recent emerging models, e.g. Sora [Brooks et al. 2024], CogVideo-X [Yang et al. 2024b], Keling [Kuaishou 2024], and Hunyuan [Kong et al. 2024], have shown impressive video generation ability with strong temporal consistency and appealing visual effects, which becomes a promising tool for artists to create stunning videos using just few images or text prompts. These advancements show strong potential to revolutionize the advertising, film, robotics, and game industries, becoming fundamental elements for various generative AI-based applications.\nA major challenge in video generation lies in achieving versatile and precise control to align seamlessly with users' creative visions. While recent methods have introduced strategies to integrate control into the video generation process [Guo et al. 2024; He et al. 2024b,a; Huang et al. 2023; Ma et al. 2024b,a; Namekata et al. 2024; Polyak et al. 2024; Wang et al. 2024f,c; Yuan et al. 2024], they predominantly focus on specific control types, relying on specialized architectures that lack adaptability to emerging control requirements. Furthermore, these approaches are generally limited to high-level adjustments-such as camera movements or maintaining identity-falling short when it comes to enabling fine-grained modifications, like precisely raising an avatar's left hand.\nWe argue that achieving versatile and precise video generation control fundamentally requires 3D control signals in the diffusion model. Videos are 2D renderings of dynamic 3D content. In a traditional Computer Graphics (CG)- based video-making pipeline, we can effectively control all aspects of a video in detail by manipulating the underlying 3D representations, such as meshes or particles. However, existing video control methods solely apply 2D control signals on rendered pixels, lacking the 3D awareness in the video generation process and thus struggling to achieve versatile and fine-grained controls. Thus, to this end, we present a novel 3D-aware video diffusion method, called Diffusion as Shader (DaS) in this paper, which utilizes 3D control signals to enable diverse and precise control tasks within a unified architecture.\nSpecifically, as shown in Figure 1 (a), DaS is an image-to-video diffusion model that takes a 3D tracking video as the 3D control signals for various control tasks. The 3D tracking video contains the motion trajectories of 3D points whose colors are defined by their coordinates in the camera coordinate system of the first frame. In this way, the 3D tracking video represents the underlying 3D motion of this video. The video diffusion model acts like a shader to compute shaded appearances on the dynamic 3D points to generate the video. Thus, we call our model Diffusion as Shader.\nUsing 3D tracking videos as control signals offers a significant advantage over depth videos with enhanced temporal consistency. While a straightforward approach to incorporating 3D control into video diffusion models involves using depth maps as control signals, depth maps only define the structural properties of the underlying 3D content without explicitly linking frames across time. In contrast, 3D tracking videos provide a consistent association between frames, as identical 3D points maintain the same colors across the video. These color anchors ensure consistent appearances for the same 3D points, thereby significantly improving temporal coherence in the generated videos. Our experiments demonstrate that even when a 3D region temporarily disappears and later reappears, DaS effectively preserves the appearance consistency of that region, thanks to the temporal consistency enabled by the tracking video.\nBy leveraging 3D tracking videos, DaS enables versatile video generation controls, encompassing but not limited to the following video control tasks."}, {"title": "(1) Animating meshes to videos.", "content": "Using advanced 3D tools like Blender, we can design animated 3D meshes based on predefined templates. These animated meshes are transformed into 3D tracking videos to guide high-quality video generation (Figure 1 (b))."}, {"title": "(2) Motion transfer.", "content": "Starting with an input video, we employ a 3D tracker [Xiao et al. 2024b] to generate a corresponding 3D tracking video. Next, the depth-to-image Flux model [Labs 2024] is used to modify the style or content of the first frame. Based on the updated first frame and the 3D tracking video, DaS generates a new video that replicates the motion patterns of the original while reflecting the new style or content (Figure 1 (c))."}, {"title": "(3) Camera control.", "content": "To enable precise camera control, depth maps are estimated to extract 3D points [Bochkovskii et al. 2024]. These 3D points are then projected onto a specified camera path to create a 3D tracking video, which guides the generation of videos with customized camera movements (Figure 1 (d))."}, {"title": "(4) Object manipulation.", "content": "By integrating object segmentation techniques [Kirillov et al. 2023] with a monocular depth estimator [Bochkovskii et al. 2024], the 3D points of specific objects can be extracted and manipulated. These modified 3D points are used to construct a 3D tracking video, which guides the creation of videos for object manipulation (Figure 1 (e)).\nDue to the 3D awareness of DaS, DaS is data-efficient. Finetuning with less than 10k videos on 8 H800 GPUs for 3 days already gives the powerful control ability to DaS, which is demonstrated by various control tasks. We compare DaS with baseline methods on camera control [He et al. 2024b; Wang et al. 2024c] and motion transfer [Geyer et al. 2023a], which demonstrates that DaS achieves significantly improved performances in these two controlling tasks than baselines. For the remaining two tasks, i.e. mesh-to-video and object manipulation, we provide extensive qualitative results to show the superior generation quality of our method."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Video diffusion", "content": "In recent years, the success of diffusion models in image generation [Ho et al. 2020; Peebles and Xie 2023a; Rombach et al. 2022] has sparked interest in exploring video generation [Blattmann et al. 2023; Brooks et al. 2024; Chen et al. 2023b, 2024b; Guo et al. 2023; He et al. 2022; Ho et al. 2022; Kong et al. 2024; Kuaishou 2024; Lin et al. 2024; Xing et al. 2024; Yang et al. 2024b; Zheng et al. 2024b]. VDM [Ho et al. 2022] is the first work to explore the feasibility of diffusion in the field of video generation. SVD [Blattmann et al. 2023] introduces a unified strategy for training a robust video generation model. Sora [Brooks et al. 2024], through training on extensive video data, suggests that scaling video generation models is a promising path towards building general-purpose simulators of the physical world. CogVideo-X [Yang et al. 2024b], VideoCrafter [Chen et al. 2023b, 2024b], DynamiCrafter [Xing et al. 2024], Keling [Kuaishou 2024], and Hunyuan [Kong et al. 2024] have demonstrated impressive video generation performance with strong temporal consistency.\nControllable video generation. Existing works still lack an effective way to control the generation process. There are many works [Guo et al. 2024; He et al. 2024b,a; Huang et al. 2023; Ma et al. 2024b,a,a; Namekata et al. 2024; Polyak et al. 2024; Qiu et al."}, {"title": "2.2 Controlled video generation", "content": "We review the following 4 types of controlled video generation.\nAnimating meshes to videos. Animating meshes to videos aims to texture meshes. Several works [Cai et al. 2024; Cao et al. 2023; Richardson et al. 2023; Wang et al. 2023] have demonstrated the feasibility of mesh texturization using powerful diffusion models. TexFusion [Cao et al. 2023] applies the diffusion model's denoiser on a set of 2D renders of the 3D object, optimizing an intermediate neural color field to output final RGB textures. TEXTure [Richardson et al. 2023] introduces a dynamic trimap representation and a novel diffusion sampling process, leveraging this trimap to generate seamless textures from various views. G-Rendering [Cai et al. 2024] takes a dynamic mesh as input. To preserve consistency, G-Rendering employs UV-guided noise initialization and correspondence-aware blending of both pre- and post-attention features. Following G-Rendering, our method also targets dynamic meshes, utilizing a diffusion model as a shader to incorporate realistic texture information. Unlike G-Rendering, which preserves consistency at the noise and attention levels, our approach leverages 3D tracking videos as supplementary information, integrating them into the diffusion model to ensure both temporal and spatial consistency.\nCamera control. Camera control [Bahmani et al. 2024; Geng et al. 2024; He et al. 2024b; Wang et al. 2024e,c; Xiao et al. 2024a; Yang et al. 2024a; Yu et al. 2024; Zheng et al. 2024a] is an important capability for enhancing the realism of generated videos and increasing user engagement by allowing customized viewpoints. Recently, many efforts have been made to introduce camera control in video generation. MotionCtrl [Wang et al. 2024c] incorporates a flexible motion controller for video generation, which can independently or jointly control camera motion and object motion in generated videos. CameraCtrl [He et al. 2024b] adopts Pl\u00fccker embeddings [Sitzmann et al. 2021] as the primary form of camera parameters, enabling the ViewCrafter [Yu et al. 2024] employs a point-based representation for free-view rendering, enabling precise camera control. AC3D [Bahmani et al. 2024] optimizes pose conditioning schedules during training and testing to accelerate convergence and restricts the injection of camera conditioning to specific positions, reducing interference with other meaningful video features. CPA [Wang et al. 2024e] incorporates a Sparse Motion Encoding Module to embed the camera pose information and integrating the embedded motion information via temporal attention. Our method aims to use 3D tracking videos as an intermediary to achieve precise and consistent camera control.\nMotion transfer. Motion transfer [Esser et al. 2023; Geng et al. 2024; Geyer et al. 2023a; Meral et al. 2024; Park et al. 2024; Pondaven et al. 2024; Wang et al. 2024d,c; Yatim et al. 2024] aims to synthesize novel videos by following the motion of the original one. Gen-1 [Esser et al. 2023] employs depth estimation results [Bochkovskii"}, {"title": "3 METHOD", "content": ""}, {"title": "3.1 Overview", "content": "DaS is an image-to-video (I2V) diffusion generative model, which applies both an input image and a 3D tracking video as conditions"}, {"title": "3.2 Backend video diffusion model", "content": "DaS is finetuned from the CogVideoX [Yang et al. 2024b] model that is a transformer-based video diffusion model [Peebles and Xie 2023a] operating on a latent space. Specifically, as shown in Figure 2 (d), we adopt the I2V CogVideoX model as the base model, which takes an image $I \\in \\mathbb{R}^{H\\times W \\times 3}$ as input and generate a video $V \\in \\mathbb{R}^{T\\times H \\times W \\times 3}$.\nThe generated video V has T frames with the same image size of width W height H as the input image. The input image I is first padded with zeros to get an input condition video with the same size $T \\times H \\times W \\times 3$ as the target video. Then, a VAE encoder is applied to the padded condition video to get a latent vector of size $\\frac{T}{8}\\times\\frac{H}{8}\\times\\frac{W}{8}\\times 16$, which is concatenated with a noise of the same size. A diffusion transformer (DiT) [Peebles and Xie 2023b] is iteratively used to denoise the noise latent for a predefined number of steps and the output denoised latent is processed by a VAE decoder to get the video V. In the following, we discuss how to add a 3D tracking video as an additional condition on this base model."}, {"title": "3.3 Finetuning with 3D tracking videos", "content": "We add a 3D tracking video as an additional condition to our video diffusion model. As shown in Figure 2 (a, b), the 3D tracking video is rendered from a set of moving 3D points {$p_i(t) \\in \\mathbb{R}^3$}, where t = 1, ..., T means the frame index in the video. The colors of these points are determined by their coordinates in the first frame, where we normalize the coordinates into $[0, 1]^3$ and convert the coordinates into RGB colors {$c_i$}. Note we adopt the reciprocal of z-coordinate in the normalization. These colors remain the same for different timesteps t. Then, to get a specific t-th frame of the tracking video, we project these 3D points onto the t-th camera to render this frame. In Sec. 3.4, we will discuss how to get these moving 3D points and the camera poses of different frames for different control tasks. Next, we first introduce the architecture to utilize the 3D tracking video as a condition for video generation.\nInjecting 3D tracking control. We follow a similar design as the ControlNet [Chen et al. 2024a; Zhang et al. 2023] in DaS to add the 3D tracking video as the additional condition. As shown in Figure 2 (d), we apply the pretrained VAE encoder to encode the 3D tracking video to get the latent vector. Then, we make a trainable copy of the pretrained denoising DiT, called condition DiT, to process the latent vector of the 3D tracking video. The denoising DiT contains 42 blocks and we copy the first 18 blocks as the condition DiT. In the condition DiT, we extract the output feature of each DiT block, process it with a zero-initialized linear layer, and add the feature to the corresponding feature map of the denoising DiT. We finetune the condition DiT with the diffusion losses while freezing the pretrained denoising DiT.\nFinetuning details. To train the DaS model, we construct a training dataset containing both real-world videos and synthetic rendered videos. The real-world videos are from MiraData [Ju et al. 2024] while we use the meshes and motion sequences from Mixamo to render synthetic videos. All videos are center-cropped and resized to 720 \u00d7 480 resolution with 49 frames. We only finetune the copied condition DiT while freezing all the original denoising DiT. To construct the 3D tracking video for the rendered videos, since we have access to the ground-truth 3D meshes and camera poses for the synthetic videos, we construct our 3D tracking videos directly using these dense ground-truth 3D points, which results in dense 3D point tracking. For real-world videos, we adopt SpatialTracker [Xiao et al. 2024b] to detect 3D points and their trajectories in the 3D space. Specifically, for each real-world video, we detect 4,900 3D evenly distributed points and track their trajectories. For training, we employ a learning rate of $1 \\times 10^{-4}$ using the AdamW optimizer. We train the model for 2000 steps using the gradient accumulation"}, {"title": "3.4 Video generation control", "content": "In this section, we describe how to utilize DaS for the following controllable video generation."}, {"title": "3.4.1 Object manipulation.", "content": "DaS can generate a video to manipulate a specific object. As shown in Figure 3 (a), given an image, we estimate the depth map using Depth Pro [Bochkovskii et al. 2024] or MoGE [Wang et al. 2024b] and segment out the object using SAM [Kirillov et al. 2023]. Then, we are able to manipulate the point cloud of the object to construct a 3D tracking video for object manipulation video generation."}, {"title": "3.4.2 Animating meshes to videos.", "content": "DaS enables the creation of visually appealing, high-quality videos from simple animated meshes. While many Computer Graphics (CG) software tools provide basic 3D models and motion templates to generate animated meshes, these outputs are often simplistic and lack the detailed appearance and geometry needed for high-quality animations. Starting with these simple animated meshes, as shown in Figure 3 (b), we generate an initial visually appealing frame using a depth-to-image FLUX model [Labs 2024]. We then produce 3D tracking videos from the animated meshes, which, when combined with the generated first frame, guide DaS to transform the basic meshes into visually rich and appealing videos."}, {"title": "3.4.3 Camera control.", "content": "Previous approaches [He et al. 2024b; Wang et al. 2024c] rely on camera or ray embeddings as conditions to control the camera trajectory in video generation. However, these embeddings lack true 3D awareness, leaving the diffusion models to infer the scene's 3D structure and simulate camera movement. In contrast, DaS significantly enhances 3D awareness by incorporating 3D tracking videos for precise camera control. To generate videos with a specific camera trajectory, as shown in Figure 3 (c), we first estimate the depth map of the initial frame using Depth Pro [Bochkovskii et al. 2024] and convert it into colored 3D points. These points are then projected onto the given camera trajectory, constructing a 3D tracking video that enables DaS to control camera movements with high 3D accuracy."}, {"title": "3.4.4 Motion transfer.", "content": "As shown in Figure 3 (d), DaS also facilitates creating a new video by transferring motion from an existing source video. First, we estimate the depth map of the source video's first frame and apply the depth-to-image FLUX model [Labs 2024] to repaint the frame into a target appearance guided by text prompts. Then, using SpatialTracker [Xiao et al. 2024b], we generate a 3D tracking video from the source video to serve as control signals. Finally, the DaS model generates the target video by combining the edited first frame with the 3D tracking video."}, {"title": "4 EXPERIMENTS", "content": "We conduct experiments on five tasks, including camera control, motion transfer, mesh-to-video generation, and object manipulation to demonstrate the versatility of DaS in controlling the video generation process."}, {"title": "4.1 Camera control", "content": "Baseline methods. To evaluate the ability to control camera motions of generated videos, we select two representative methodologies, MotionCtrl [Wang et al. 2024c] and CameraCtrl [He et al. 2024b] as baseline methods, both of which allow camera trajectories as input and use camera or ray embeddings for camera control.\nMetrics. To measure the accuracy of the camera trajectories of generated videos, we evaluate the consistency between the estimated"}, {"title": "4.2 Motion transfer", "content": "Baseline methods. We compare DaS with two famous motion transfer methods, TokenFlow [Geyer et al. 2023b] and CCEdit [Feng et al. 2024b]. TokenFlow represents video motions with the feature consistency across different timesteps extracted by a diffusion model. Then, the feature consistency is propagated to several keyframes generated by a text prompt for video generation. For TokenFlow, we adopt the Stable Diffusion 2.1 [Rombach et al. 2022] model for the motion transfer task. CCEdit adopts depth maps as conditions to control the video motion and transfers the motion using a new repainted frame to generate a video.\nMetrics. Since all methods generate the transferred videos based on text prompts, we aim to evaluate the alignment between the generated videos and the text prompts, as well as the video coherence, using the CLIP [Radford et al. 2021]. Specifically, for video-text alignment, we extract multiple frames from the video and compare them with the corresponding text prompts by calculating the CLIP score [Hessel et al. 2022] for each frame. This score reflects"}, {"title": "4.3 Animating meshes to videos", "content": "Qualitative comparison. We compare our method against a state-of-the-art human image animation method CHAMP [Zhu et al. 2024] on the mesh-to-video task. Champ takes a human image and a motion sequence as input and generates a corresponding human video. The motion sequence is represented by an animated SMPL [Loper"}, {"title": "4.4 Object manipulation", "content": "Qualitative results. For the object manipulation, we adopt the SAM [Kirillov et al. 2023] and depth estimation models [Bochkovskii et al. 2024; Wang et al. 2024b] to get the object points. Then, we evaluate two kinds of manipulation, i.e. translation and rotation. The results are shown in Figure 9, which demonstrate that DaS achieves accurate object manipulation to produce photorealistic videos with strong multiview consistency for these objects."}, {"title": "4.5 Analysis", "content": "We conduct analysis on the choice of 3D control signals, i.e. depth maps or 3D tracking videos, and the number of 3D tracking points. To achieve this, we randomly selected 50 videos from the validation split of the DAVIS [Pont-Tuset et al. 2017] and MiraData [Ju et al. 2024] video dataset. We extract the first-frame images as the input image and apply different models to re-generate these videos. To evaluate the quality of the generated videos, we compute PSNR, SSIM [Wang et al. 2004], LPIPS [Zhang et al. 2018], and FVD [Unterthiner et al. 2019] between the generated videos and the ground-truth videos."}, {"title": "4.5.1 Depth maps vs. 3D tracking videos.", "content": "To illustrate the effectiveness of our 3D tracking videos, we compare DaS with a baseline using depth maps as conditions instead of 3D tracking videos. Specifically, the baseline adopts the same architecture as DaS but replaces the 3D tracking video with a depth map video. We adopt the Depth Pro [Bochkovskii et al. 2024] to generate the video depth video for this baseline method. As shown in Table 3, our model outperforms this baseline in all metrics, demonstrating that the 3D tracking videos provide a better signal for the diffusion model to recover groud-truth videos than the depth map conditions. Figure 10 shows the generated videos, which demonstrate that our method produces more consistent videos with the ground truth. The main reason is that the 3D tracking videos effectively associate different frames of a video while the depth maps only provide some cues of the scene structures without constraining the motion of the video."}, {"title": "4.5.2 Point density.", "content": "In Table 3, we further present an ablation study with varying numbers of 3D tracking points as control signals. The number of 3D tracking points ranges from 900 (30\u00d730) to 8100 (90\u00d790). Though the generated videos with 4900 tracking points perform slightly better than the other ones, the visual qualities of 2500, 4900, and 8100 tracking points are very similar to each other. Since tracking too many points with SpatialTracker [Xiao et al. 2024b] would be slow, we choose 4900 as our default setting in all our other experiments using 3D point tracking."}, {"title": "4.5.3 Runtime.", "content": "In the inference stage, we employ the DDIM [Song et al. 2020] sampler with 50 steps, classifier-free guidance of magnitude 7.0, which costs about 2.5 minutes to generate 49 frames on a H800 GPU at a resolution of 480\u00d7720."}, {"title": "5 LIMITATIONS AND CONCLUSIONS", "content": "Limitations and future works. Though DaS achieves control over the video generation process in most cases, it still suffers from multiple failure cases mainly caused by incorrect 3Dtracking videos. The first failure case is that the input image should be compatible with the 3D tracking videos. Otherwise, the generated videos would be implausible as shown in Figure 11 (top). Another failure case is that for regions without 3D tracking points, the generated contents may be out-of-control and produce some unnatural results (Figure 11 (bottom)). For future works, we currently rely on provided animated meshes or existing videos to get high-quality 3D tracking videos and a promising direction is to learn to generate these 3D tracking videos with a new diffusion model.\nConclusions. In this paper, we introduce Diffusion as Shader (DaS) for controllable video generation. The key idea of DaS is to adopt the 3D tracking videos as 3D control signals for video generation. The 3D tracking videos are constructed from colored dynamic 3D points which represent the underlying 3D motion of the video. Then, diffusion models are applied to generate a video following the motion of the 3D tracking video. We demonstrate that the 3D tracking videos not only improve the temporal consistency of the generated videos but also enable versatile control of the video content, including mesh-to-video generation, camera control, motion transfer, and object manipulation."}]}