{"title": "DMPA: Model Poisoning Attacks on Decentralized Federated Learning for Model Differences", "authors": ["Chao Feng", "Yunlong Li", "Yuanzhe Gao", "Alberto Huertas Celdr\u00e1n", "Jan von der Assen", "G\u00e9r\u00f4me Bovet", "Burkhard Stiller"], "abstract": "Federated learning (FL) has garnered significant attention as a prominent privacy-preserving Machine Learning (ML) paradigm. Decentralized FL (DFL) eschews traditional FL's centralized server architecture, enhancing the system's robustness and scalability. However, these advantages of DFL also create new vulnerabilities for malicious participants to execute adversarial attacks, especially model poisoning attacks. In model poisoning attacks, malicious participants aim to diminish the performance of benign models by creating and disseminating the compromised model. Existing research on model poisoning attacks has predominantly concentrated on undermining global models within the Centralized FL (CFL) paradigm, while there needs to be more research in DFL. To fill the research gap, this paper proposes an innovative model poisoning attack called DMPA. This attack calculates the differential characteristics of multiple malicious client models and obtains the most effective poisoning strategy, thereby orchestrating a collusive attack by multiple participants. The effectiveness of this attack is validated across multiple datasets, with results indicating that the DMPA approach consistently surpasses existing state-of-the-art FL model poisoning attack strategies.", "sections": [{"title": "Introduction", "content": "The advent of diverse privacy regulations has significantly increased the need for privacy preservation in Machine Learning (ML). Federated Learning (FL) emerges as a novel collaborative and privacy-preserving ML paradigm that has attracted substantial attention from both academic and industrial communities (McMahan et al. 2017). FL enables participants to share model parameters instead of raw data, thereby ensuring data privacy (Yang et al. 2019). Traditional FL architectures rely on a central server to distribute, called Centralized FL (CFL), receive and aggregate model parameters from participants (Alazab et al. 2021). However, this client-server architecture suffers from significant drawbacks, including susceptibility to a single point of failure and server-side bottleneck (Mart\u00ednez Beltr\u00e1n et al. 2023). To address these challenges, Decentralized FL (DFL) has been introduced.\nIn DFL, each client independently manages the processes of establishing communication, sharing, and aggregating model parameters with other clients (Lian et al. 2017). During each iteration, participants execute several tasks: training their local models, transmitting model parameters to directly connected peers, aggregating received parameters, and updating their local models accordingly. Unlike CFL, clients in DFL have the autonomy to independently select any other participant to establish bidirectional communication connections, allowing for customizable overlay network topology, such as fully connected, ring, star, and even dynamic configurations (Yuan et al. 2024). This adaptability enables DFL to effectively address the single point of failure issue inherent in CFL, thereby enhancing the system's robustness and scalability. Nevertheless, this decentralized nature introduces new security vulnerabilities, making DFL more susceptible to various threats, including model poisoning attacks.\nModel poisoning attacks involve malicious clients directly altering their local model parameters to negatively influence the global model training by sending harmful updates (Feng et al. 2023). In FL, since participants can directly share and modify model parameters, model poisoning attacks are relatively more straightforward to execute and pose significant threats. Current model poisoning attacks primarily target CFL models. These adversarial clients transmit meticulously crafted local model updates to the central server during the training phase, leading to issues such as reduced accuracy of the global model (Cao and Gong 2022). These attacks presuppose that the attacker controls a substantial number of compromised real clients, which may include either hijacked clients or fabricated ones.\nNevertheless, the architectural distinctions between DFL and CFL, particularly in overlay network topology, imply that existing attacks targeting CFL can not be directly transferable to DFL. In DFL, the communication links are significantly longer compared to CFL, making it challenging for a malicious participant to impact the entire federation (Mart\u00ednez Beltr\u00e1n et al. 2023). Additionally, in DFL, each node could decide whether to disseminate and receive models from neighboring nodes. This capability enables the potential blockage of malicious attacks at intermediate nodes, thereby diminishing the effectiveness of such attacks. Consequently, from an adversarial perspective, it is imperative to develop model poisoning attacks that are effective in DFL.\nTo this end, this paper investigates model poisoning at-"}, {"title": "Background and Related Work", "content": "This section provides an introduction to the security problem in DFL and a summary of the current research on model poisoning attacks in FL."}, {"title": "Security Problem in DFL", "content": "Differing from traditional CFL, neighboring clients in DFL exchange local model parameters or gradients over a P2P network and construct consensus models independently. DFL solves the problem of risk associated with a single point of failure, enhances its scalability, and is well suited for applications in the Industrial Internet of Things (IIoT) (Tan et al. 2023). However, the decentralized nature of DFLS rather increases their risk of being exposed to malicious attacks, especially poisoning attacks (Feng et al. 2024). Poisoning attacks, which aim to diminish the resilience of FL models, can be classified into data poisoning and model poisoning attacks. Data poisoning attacks typically involve malicious clients interfering with the training process by introducing harmful data (such as backdoor attacks or label flipping) into the local training dataset, thereby inducing biased learning outcomes (Hallaji, Razavi-Far, and Saif 2022). Conversely, model poisoning attacks are characterized by malicious clients directly altering their local model parameters and affecting the global model training via harmful model updates. To improve the model robustness of FL and defend against the poisoning attacks, (McMahan et al. 2017) proposed the use of averaging model parameters to improve training efficiency. (Blanchard et al. 2017) introduced the Krum method, which excludes malicious updates by calculating the Euclidean distance of a multi-node model and selecting the model with the smallest distance. This method reduces the impact of malicious attacks and is now widely used for robust aggregation in DFL. (Yin et al. 2018) developed two robust distributed learning algorithms for Byzantine errors and potential adversarial behavior, a robust distributed gradient descent algorithm based on Median and Trimmed Mean operations, respectively. These two methods are widely used in both CFL and DFL because they can be implemented with only one communication round and achieve optimal model robustness."}, {"title": "Model Poisoning Attack", "content": "Compared with data poisoning attacks, model poisoning attacks are easier to execute as they directly modify the shared model. Therefore, there has been research suggesting how to optimize the attack method to enhance the attack effectiveness. Existing model poisoning attacks mainly target CFL. The attack assumes that the attacker has access to a large number of compromised clients. During the training process, malicious clients send carefully designed local model updates to the server, resulting in problems such as a decline in the accuracy of the global model (Cao and Gong 2022). The MIN-MAX and MIN-SUM methods proposed by (Shejwalkar and Houmansadr 2021) respectively minimize the maximum and sum of the distance between toxic updates and all benign updates in CFL. These methods assume that malicious model updates are the sum of aggregated model updates and a fixed disturbance vector scaled by factors before the attack, and implement the attack by maximizing the distance between toxic updates and benign updates in the inverse direction of the global model. The \"a little is enough\" (LIE) attack (Baruch, Baruch, and Goldberg 2019) directly modifies the model parameters, which generates malicious model updates in each training round by calculating the average of the real model updates of the malicious client and perturbing them.\nMost model poisoning attacks rely on additional information from the central server, such as aggregation methods, global models, or even the training data utilized by nodes. This dependency renders external attacks impractical. Nevertheless, some attacks are engineered to maintain efficacy even in the absence of such additional knowledge. For instance, (Zhang et al. 2023) employs global historical data to build an estimator that forecasts the subsequent round of global models as a benign reference. Despite this, the approach necessitates substantial resources and encounters difficulties in accessing the global model within DFL systems, thereby limiting its practical application in DFL.\nTo conclude, while a substantial amount of research has been dedicated to optimizing model poisoning attacks against the FL, these efforts predominantly concentrate on"}, {"title": "Problem Setup", "content": "In the DFL framework, consider a network of K clients, each denoted as k \u2208 {1, 2, . . ., K}. In each communication round, each client k maintains and updates a local model $w_t$ according to an aggregation algorithm A(\u00b7) defined by this framework. The communication between clients can be represented as a graph G = (V, E), where V is the set of nodes (clients) and & is the set of edges (communication links) between the nodes. The neighborhood of a client k, denoted as Nk \u2286 V, contains all clients that are directly connected to k via an edge in E. Each client updates its model by training it locally and aggregating models from its neighbors. The network topology, represented by G, plays a key role in determining how information is shared and how global knowledge is propagated through the network.\nA general process of DFL can be divided into the following stages: First, each client k initializes its local model parameters $w_0$. Then, each client trains its local model to further optimize these parameters. Gradient descent is generally used as the core optimization strategy during local training. Specifically, client k performs several epochs of local training using its private dataset Pk to minimize its local loss function $L(w, Pk)$ as defined in Equation 1.\n$w \\leftarrow w - \\eta \\nabla L(w, P_k)$\nwhere the gradient $\\nabla l(w, P_k)$ reflects the change in the loss function l with respect to the model parameters w. The learning rate \u03b7 controls the step size of the parameter updates at each iteration. By iteratively reducing the loss function, gradient descent effectively guides the model parameters toward a more optimal direction.\nAfter each client k completes the training of its local model, it interacts with its neighbors Nk and transmits the parameters of the current round of local model training to them, as defined by the network topology G. Simultaneously, client k also receives model updates $w_j^t$ from its neighbors j\u2208 Nk and follows an aggregation algorithm A(.) to form a new model $w_k^{t+1}$. The aggeration process can be defined in Equation 2.\n$w_k^{t+1} = A({w_k^t} \\cup {w_j^t : j \\in N_k})$\nBy continuously repeating the above processes of local model training and aggregation until the predefined num-"}, {"title": "Threat Model", "content": "Adversary Objective. The primary goal of the attacker is to minimize the performance of the global model in the DFL system by manipulating the updates to the client models under its control. Specifically, the attacker hopes to mislead the learning process of the global model by introducing carefully crafted malicious updates to the model parameters, ultimately leading to significant degradation in model accuracy or deviation in behavior.\nAdversary Capability. The attacker has the ability to control a certain percentage of clients and has full access to and modify the local model updates of these clients. The attacker is able to send maliciously modified model parameters to neighboring nodes and participate in model aggregation after each round of training. In addition, the attacker can continuously observe and adjust the attack strategy during the training process to improve the stealth and effectiveness of the attack.\nAdversary Knowledge. The attacker only has model information of all malicious participants and has no knowledge of the internal information of other benign clients. This knowledge level assumption is very strict, but it is consistent with the conditions of a realistic attack, which reflects the harmfulness of the designed model poisoning attack discussed in next section."}, {"title": "DMPA Approach", "content": "In this section, the general framework for applying the DMPA method in DFL is described, followed by a discussion of the research issues concerning Model Poisoning Attacks in DFL. The section then presents solutions to the problems outlined in the first section."}, {"title": "Method Overview", "content": "This workd proposes DMPA, an advanced model poisoning attack specifically designed to target DFL models. As illustrated in Figure 2, DMPA employs a tripartite attack strategy. In contrast to existing model poisoning attacks in FL (e.g., (Shejwalkar and Houmansadr 2021; Baruch, Baruch, and Goldberg 2019; Zhang et al. 2023)), which determine the attack direction based on model similarity, DMPA leverages the maximum eigenvalue of the correlation matrix to identify the optimal poisoning direction. The primary objective is to maximize the training loss of benign models after model aggregation."}, {"title": "Attack Strategy", "content": "Algorithm 1 provides the attack pseudocode of DMPA, which explains how malicious clients execute the DPMA attack in DFL. This work defines the model of the received malicious client as U. All malicious client models are transformed into column vectors, represented as U = [41, 42, ...]. During the attack phase, a decentralized approach is employed, leading to the computation of a new deviation, denoted as vij, using Equation 3. These resulting relative offsets collectively constitute the matrix V.\n$v_{ij} = u_{ij} - \\overline{u_j}$\nwhere \u016b\u00a1 is the average of the j th model parameter.\nTo ensure the rigor of this analysis, it is essential to compute the overall standard deviation and provide an unbiased estimate. This necessitates the derivation of a correlation matrix. As demonstrated in Equation 4, the correlation matrix V is determined using the matrix C.\n$C = \\frac{1}{n-1}VTV$\nThe matrix C can be regarded as a covariance matrix, in which each element represents the correlation between subscript corresponding vector groups. However, in this paper,"}, {"title": "Experimental Setup", "content": "Datasets. The experiments use CIFAR-10 (Krizhevsky, Hinton et al. 2009), MNIST (Deng 2012) and Fashion-MNIST (Xiao, Rasul, and Vollgraf 2017) as validation datasets, which are widely used in validating model performance. The experiments adopt an independent identically distributed (IID) data partitioning method, which ensures that the data have the same statistical properties. The a parameter is defaulted to 100 as set in ((Shejwalkar and Houmansadr 2021), (Tan et al. 2023), (Li et al. 2024)).\nMachine learning models. Three different model architectures are chosen to correspond to different datasets. The batch size is set to 64 for all clients, and the random seed for each client is set to its corresponding ID value. A simple convolutional neural network (CNN) model with Conv2d, BatchNorm2d, 5 Depthwise Conv2d, and fully connected (linear) layers is used for the CIFAR-10 dataset, a model with three fully connected (linear) layers is used for the MNIST dataset, and a simple CNN model with 2 Conv2d and 2 fully connected layers is used for the Fashion-MNIST dataset.\nMeasurement metrics. The model F1 score is used to assess the attack performance. A lower F1 score indicates a better attack method. All results are the average of the F1 scores of all benign client models after last round of training, which enables a clear assessment of the impact of the MPA attack on the clients in the whole DFL network.\nBaseline defenses and attacks. Three of the most effective model poisoning attacks in FL are chosen to attack DFL,"}, {"title": "Experimental Results", "content": "Compare DMPA with state-of-art model poisoning attacks. In this section, the attack method proposed in this study is compared with the current state-of-the-art model poisoning attack methods, including Lie(Baruch, Baruch, and Goldberg 2019) and Min-Max and Min-Sum (Shejwalkar and Houmansadr 2021). Three topologies (Fully, Star, and Ring) are used for the comparison experiments, and IID data is used for the experiments. The experimental results are based on a configuration of 40% malicious clients and 60% benign clients, and calculate the average F1 scores in the final round (smaller F1 scores represent more effective attacks). The experimental results are shown in Table I. Bolded data indicates the best results.\nFirstly, from the experimental results, when the proportion of malicious participants is 40%. The results show that DMPA has a significant attack effect in DFL with different topologies, different aggregation methods, and different datasets, and its F1 scores are all the lowest, indicating that DMPA has the best attack effect, which fully verifies the effectiveness of the method on attack.\nImpact of overlay network topology. In fully connected networks, DMPA shows extreme attackability in several experiments. For example, in experiments on the MNIST dataset corresponding to the Krum aggregation method, the CIFAR-10 dataset corresponding to the Median, Trimmed Mean and FedAvg aggregation methods, and the Fashion-MNIST dataset corresponding to the Krum aggregation method, DMPA achieves a score of less than 0.1. This shows that DMPA effectively influences the gradient descent process of 60% benign clients and successfully fills the difference in gradient descent by increasing the loss. This shows that DMPA has been validated for its reasonable design, which can invalidate the gradient descent by increasing and filling in the gradient difference. In contrast, although the other three attack methods achieve the attack by influencing the direction and magnitude of the model update, most of the average F1 scores among the 60% benign clients are greater than 0.8, indicating that their attack effects are more limited.\nThe overall F1 score of DMPA in the ring network is low and close to the lowest of the other attack methods. The F1 scores of DMPA are all less than 0.8, showing high stability. This may be due to the fact that in ring networks, 60% of the benign clients are buffered to some extent due to the longer chain, mitigating the impact of the attack, which is a major advantage of DFL. As for the other methods, half of them have F1 scores above 0.8, indicating that these methods are not stable enough for model poisoning attacks in the ring structure and are prone to only changing the direction of the model and not effectively affecting other benign clients. This further demonstrates that DMPA exhibits stable and effective attacks that can generally affect benign clients in the DFL ring network. In the Star network, DMPA also shows its advantages, further proving the effectiveness of the method in this paper. Since the results of CIFAR-10 are the most representative for studying the attack ratio among the three datasets, the CIFAR-10 dataset will be selected in the following to further investigate the impact of the malicious client ratio on the overall experimental results.\nImpact of increasing the percentage of malicious Participants. Figure 3a to Figure 3c shows the impact of MPA on the F1 score of benign clients after the last round of aggregation in datasets, as the rate of malicious nodes increases from 10% to 60% in the DFL environment. The orange of the dotted line is no attack in each aggregation.\nAs can be seen, the DMPA method shows the best attack effect in many results to achieve effective attacks. Compared with other methods on different data sets, DMPA has the influence of attacks. However, in different datasets, this paper finds that the more complex the network is, the more effective its influence is. When more network layers are used (for example, CIFAR-10 uses more complex networks), its attack effectiveness is higher than other methods. Obviously, the higher the complexity of the network, the more influence it has. Overall, the method in this paper is more effective than other attack methods at present."}, {"title": "Conclusion and Future Work", "content": "This study proposes DMPA, a general framework designed to execute model poisoning attacks within DFL systems. The DMPA framework leverages feature angle deviations to ascertain the most effective attack strategy. In contrast to prior attack methodologies that rely on imprecise or heuristic approaches, DMPA employs the model's numerical properties for its computations, thereby maintaining the model's numerical integrity across iterations. Empirical results indicate that DMPA achieves superior attack efficacy compared to existing state-of-the-art model poisoning techniques, underscoring its substantial practical relevance.\nFuture research is planned to explore both offensive and defensive dimensions. From an offensive standpoint, existing studies predominantly utilize data distributed in an IID manner, thereby simplifying the attack process. However, the complexity of attacks escalates when data distribution is non-IID. Consequently, future research will focus on devising strategies for effective assaults under non-IID conditions. From a defensive perspective, the proposed DPMA underscores the efficacy of using feature angle deviations as a potent attack vector, providing valuable insights for the development of robust mechanisms to protect against potential threats."}]}