{"title": "ANPMI: Assessing the True Comprehension Capabilities of LLMs for Multiple Choice Questions", "authors": ["Gyeongje Cho", "Yeonkyoung So", "Jaejin Lee"], "abstract": "Multiple-choice benchmarks, consisting of various prompts and choices, are among the most widely used methods to assess a language model's natural language understanding capability. Given a specific prompt, we typically compute $P(Choice | Prompt)$ to evaluate how likely a language model is to generate the correct choice compared to incorrect ones. However, we observe that performance measured using this approach reflects not only the model's comprehension of the prompt but also its inherent biases for certain choices regardless of the prompt. This issue makes it challenging to accurately measure a model's natural language understanding, as models may select the answer without fully understanding the prompt. To address this limitation, we propose a novel metric called ANPMI, which normalizes Pointwise Mutual Information (PMI) by $- log P(Choice)$. ANPMI provides a more accurate assessment of the model's natural language understanding by ensuring that it is challenging to answer a question without properly understanding the prompt.", "sections": [{"title": "1 Introduction", "content": "Suppose that a man/woman answers a multiple-choice question, and the answer is correct. Could he truly solve the problem if he only looked at the options and guessed? It would not accurately reflect his ability or understanding that was intended to be assessed by the question.\nA similar issue arises when we evaluate a language model. Currently, the natural language understanding capability of the model is often assessed using multiple choice questions (Achiam et al., 2023; Team et al., 2023; Jiang et al., 2024; Dubey et al., 2024). The performance of the model is measured by how frequently it selects the correct answer, based on the probability $P(Choice | Prompt)$ - the likelihood that the model will generate a given choice in response to the prompt. However, this method overlooks whether the decision is based on a genuine understanding of the prompt and only considers which choice the model ultimately selects. As a result, the model may sometimes end up selecting an answer as if it were only looking at the choices without seeing the question.\nThe options in multiple-choice questions consist of diverse sentences, and the language model is not trained to generate these sentences with equal probabilities without a given prompt. It is a natural phenomenon for the language model, but it may lead to performance measurements that do not accurately reflect the model's understanding of the prompt. For example, the model might select the correct choice $c$ because $P(c)$ is much higher than others, leading to overestimating the model's actual performance. Conversely, it might choose an incorrect option if the correct choice has a lower probability, leading to an underestimation of its performance.\nTo assess the model's actual ability to understand the given multiple-choice question and answer it correctly, it is important to equalize the generation probabilities of each answer choice. However, modifying the language model to deal with this problem is not only complicated but also sabotages the process of assessing the model's performance. Adjusting the answer choices in benchmarks is not a practical solution either, as finding suitable alternatives is challenging and could limit the diversity of the tasks, restricting the evaluation of the model's ability.\nInstead of relying on $P(Choice | Prompt)$, alternative methods are often used to determine the selection by the model. For example, in benchmarks like Hellaswag (Zellers et al., 2019), the model's performance is usually measured by normalizing $P(Choice | Prompt)$ based on the length of the choice (Gao et al., 2024; Zhang et al., 2024), addressing the probability imbalance caused by the varying lengths of the choices. Another approach involves calculating mutual information to measure the dependence between the choice and the prompt (Gao et al., 2024). However, these methods do not completely solve the issue stated above regarding the probability imbalance between choices.\nThis paper analyzes the impact of the imbalance in $P(Choice)$ on language model performance and confirms the importance of addressing the issue. We propose a method to measure model performance by normalizing the Pointwise Mutual Information (PMI) between the prompt and choice using $- log P(Choice)$ to assess the model's actual understanding of the prompt. Our approach is theoretically less affected by the imbalance in $P(Choice)$. Using various pre-trained models and benchmarks, we show that the proposed method more accurately evaluates the understanding of prompts by the model than existing approaches."}, {"title": "2 Related Work", "content": "In natural language processing (NLP), multiple-choice questions (MCQs) are widely used as a benchmarking method for model evaluation. The most common approach to evaluate generative language models involves measuring the likelihood of generating correct answers based on specific prompts. However, this method is sensitive to the choice of prompts, which can lead to substantial outcome variations and heavily affect measured performance. As a result, many studies have investigated techniques to identify prompts that most accurately reflect a model's language understanding capabilities (Webson and Pavlick, 2021; Wei et al., 2022; Leidinger et al., 2023).\nRecent studies analyze language models' true comprehension by testing whether they can answer correctly with partial prompt information. Some studies have shown that model performance can be assessed using partial information from the prompt (Belinkov et al., 2019; Gururangan et al., 2018; Poliak et al., 2018; Feng et al., 2019; Srikanth and Rudinger, 2022). This demonstrates that problems can often be solved without fully understanding the prompt. Furthermore, Balepur et al. (2024) have revealed that models can solve problems even when the original prompt information is excluded, relying solely on answer choices. These findings highlight the challenges of accurately measuring a model's understanding of prompts.\nIn contrast to previous studies, which mainly focus on prompts, our study examines how answer choice design influences the evaluation of models. By removing all information about other answer choices and additional prompt details, we isolate each choice to assess its impact. This approach reveals that a model's inherent preference for specific choices significantly affects its decisions, even when prompts are provided. This paper explores the role of answer choice probability biases in language model assessment and proposes effective methods to address these biases, improving the reliability of evaluations."}, {"title": "3 Impact of the Prior Probability", "content": "Multiple-choice questions are standard for evaluating a language model's natural language understanding. The model solves each question based on the probability $P(Choice | Prompt)$ the likelihood of generating a particular choice Choice given the prompt Prompt. The predicted answer is the choice with the highest probability, and the number of correctly predicted answers determines accuracy. This section explores how $P(Choice)$, the prior probability, affects model performance when calculating $P(Choice | Prompt)$.\nWe decompose $P(Choice | Prompt)$ by multiplying and dividing by $P(Choice)$, leading to two components: $P(Choice)$ determined independently of the prompt and $P(Choice Prompt) / P(Choice)$ influenced by the prompt. This allows us to express $P(Choice | Prompt)$ as a product of the two components:\n$P(Choice | Prompt) = P(Choice) \\cdot \\frac{P(Choice | Prompt)}{P(Choice)} \\qquad (1)$\n$P(Choice)$ represents the probability of generating a choice Choice without any prompt, which we refer to as prior probability. On the other hand, $\\frac{P(Choice | Prompt)}{P(Choice)}$ indicates how much the prompt Prompt affects the probability of generating the choice Choice. It is equivalent to the exponential of the Pointwise Mutual Information (PMI), $log \\frac{P(Choice | Prompt)}{P(Choice)}$ (Fano and Hawkins, 1961). We analyze the two components for each choice across various benchmarks to understand how the choices influence the model's final decision."}, {"title": "4 Existing Metrics", "content": "Due to the limitations of evaluating model performance based solely on $P(Choice | Prompt)$, some benchmarks employ additional metrics. This section explores several alternative metrics commonly used in such evaluations. We explain how these metrics address the limitations of $P(Choice | Prompt)$ and discuss their constraints."}, {"title": "4.1 Length-Normalized Accuracy", "content": "Language models generally assign higher probabilities to shorter sentences than longer ones. It means that when there are significant differences in the lengths of the choice options, the model's decision can be biased, favoring shorter options. This results in an imbalance in $P(Choice)$ based on the length of a choice option Choice. To address it, length-normalized accuracy is used, which normalizes $log P(Choice | Prompt)$ based on the text length of Choice. For example, the Language Model Evaluation Harness uses length normalization by dividing each choice option's log-likelihood by its length in characters (Gao et al., 2024). It is particularly effective for datasets, such as Hellaswag, where there are significant differences in choice lengths.\nWhile the length-normalized accuracy addresses the problem of length imbalance and its impact on the model performance, $P(Choice)$ is not always inversely proportional to the length in characters."}, {"title": "4.2 Pointwise Mutual Information (PMI)", "content": "Using mutual information (Shannon, 1948) in language modeling has a different motivation. Its goal is to measure how much the presence of a prompt increases the likelihood of a particular choice Choice relative to its prior probability $P(Choice)$. Specifically, the model selects a choice option based on the Pointwise Mutual Information (PMI) value (Fano and Hawkins, 1961), $log \\frac{P(Choice | Prompt)}{P(Choice)}$. This approach counteracts the tendency of high prior probability choices to dominate the selection. When $P(Choice)$ is high, indicating that the model is likely to select Choice regardless of Prompt, PMI normalizes $P(Choice | Prompt)$ using the prior probability of Choice, allowing selection of responses with low prior probability but high contextual relevance more often. Thus, PMI focuses on enhancing contextual relevance over raw likelihood. While less common than metrics, such as accuracy and length-normalized accuracy, PMI has been used selectively in some studies (Askell et al., 2021; Biderman et al., 2024).\nThe PMI value is always zero when no prompt is given, regardless of the choice. It implies that in the absence of a prompt, each choice option has an equal probability of being chosen by the model. However, when a prompt is provided, the maximum possible PMI value is $- log P(Choice)$, as PMI reaches its peak when $P(Choice | Prompt) = 1$. As a result, each choice has a different maximum possible value based on its prior probability. When $P(Choice)$ is high, the maximum PMI value decreases, resulting in an unintended issue: choices with high $P(Choice)$ values are penalized by PMI, even if they are not inherently incorrect nor intentionally boosted. It becomes problematic when a correct Choice has both a meaningfully high $P(Choice | Prompt)$, indicating relevance to the prompt, and a naturally high $P(Choice)$. This case prevents the model from selecting the correct answer simply because the answer's prior probability happens to be high."}, {"title": "4.3 Normalized PMI (NPMI)", "content": "PMI yields different maximum values depending on the choice. Due to this property, PMI is unsuitable for comparing different choices. To address this limitation, Normalized PMI (NPMI) (Bouma, 2009) was introduced, with PMI divided by $- log P(Choice, Prompt)$. NPMI normalizes PMI so that it falls within [-1,1] under the assumption that $P(Choice, Prompt) = P(Prompt, Choice)$ to allow a fair comparison.\nIf $P(Choice, Prompt) = P(Prompt, Choice)$, PMI satisfies the following relationship:\n$PMI(Choice, Prompt) = log \\frac{P(Choice, Prompt)}{P(Choice)P(Prompt)} = log \\frac{P(Choice | Prompt)}{P(Choice)} = log \\frac{P(Prompt | Choice)}{P(Prompt)} \\qquad (2)$\nSince probability values are between 0 and 1, we have\n$log \\frac{P(Choice | Prompt)}{P(Choice)} < - log P(Choice). \\qquad (3)$\nand\n$log \\frac{P(Prompt | Choice)}{P(Prompt)} < - log P(Prompt). \\qquad (4)$\nThis means,\n$max(PMI(Choice, Prompt)) = min(-log P(Choice), - log P(Prompt)) < - log P(Choice, Prompt)$.\nHowever, in the case of language models, $P(Prompt, Choice)$ represents the probability of generating a full sentence where the prompt is followed by the choice, Prompt + Choice. Since changing the order of words results in a fundamentally different event in language models, this results in $P(Prompt, Choice) \\neq P(Choice, Prompt)$. Thus, $PMI_{LM}$ in language models satisfies the following relationship where x is Choice, and y is Prompt:\n$PMI_{LM}(x, y) = log \\frac{P(xy)}{P(x)P(y)} \\neq log \\frac{P(yx)}{P(x)} \\qquad (5)$\nThus, NPMI is not an appropriate normalization method for PMI in language models. As a result, NPMI is treated as nonstandard in language model evaluation and is not commonly used."}, {"title": "5 The Proposed Metric, ANPMI", "content": "We observed that due to the imbalance in $P(Choice)$, accurately assessing a model's language comprehension ability in multiple-choice tasks is challenging. While carefully constructing answer choices could address this issue, designing choices that prevent $P(Choice)$ imbalance across all language models is impractical. Thus, we propose a normalized PMI metric, Asymmetric NPMI (ANPMI) to evaluate the model performance in multiple-choice tasks. It is defined as follows:\n$ANPMI(Choice, Prompt) = \\frac{PMI(Choice, Prompt)}{- log P(Choice)} \\qquad (6)$\nIt mitigates the influence of the $P(Choice)$ imbalance, offering a more reliable indicator of a model's understanding of the prompt.\nIdeally, the following requirements should be met by an assessment metric to measure a model's true language comprehension capability:\n*   In the absence of a prompt, the model should assign equal probabilities to each choice option of a question, indicating that the prompt is essential for answering it."}, {"title": "6 Experiments", "content": "In this section, we evaluate the performance of the models using ANPMI, while comparing it with the existing metrics. Specifically, we conduct experiments using instruction-tuned language models, such as Mistral-7B(version 0.3) (Jiang et al., 2024), Gemma-7B (Team et al., 2024), and LLaMA3.1-8B (Dubey et al., 2024), along with seven widely used multiple-choice benchmarks (Zellers et al., 2019; Bisk et al., 2020; Clark et al., 2018; Hendrycks et al., 2020; Welbl et al., 2017; Liu et al., 2021; Lai et al., 2017). We aim to highlight the differences between ANPMI and other popular existing metrics, demonstrating both their benefits and limitations through empirical analysis. The model performance is denoted as Acc, AccNorm, AccPMI, and AccANPMI when measured using P(Choice|Prompt), length-normalized P(Choice|Prompt), PMI, and ANPMI. Random represents the baseline performance, reflecting the probability of selecting the correct label between labels 0, 1, 2, and 3 by chance, based solely on the label distribution. We exclude NPMI because it is not standard in language models, and it is impossible to compute P(Choice, Prompt) if Prompt + Choice is larger than the maximum sequence length."}, {"title": "6.1 Performance When No Prompt Provided", "content": "To verify that ANPMI can evaluate performance while reducing bias from differences in P(Choice), we measure the language model performance on the various benchmarks without providing prompts. The results of these evaluations are summarized in Table 2.\nFor MMLU, we observe identical performance across all models, regardless of the metric used. This is because the same four choices A, B, C, and D are given throughout examples. However, for other datasets, such as Hellaswag and ARC, which have a set of different answer choices for each example, model performance varies when evaluated using Acc or AccNorm. For each benchmark, we observe a difference of up to 30% in performance between models when evaluated using these metrics. Section 3 demonstrates that variations in P(Choice) significantly influence a model's final decisions. Thus, these performance differences observed without prompts, which highlight the impact of prior probabilities, may complicate accurately ranking models. Moreover, the measured performance for Hellaswag, PiQA, and ARC-easy is significantly higher than that of random guessing. This indicates that when using Acc or AccNorm, models may achieve high scores on these benchmarks without understanding the prompts, complicating the evaluation of their language comprehension capability.\nIn contrast, PMI and ANPMI have identical performance across all models when prompts are absent. These metrics always assign a zero value when prompts are not provided, resulting in consistent performance measurements by always choosing the same choice. Consequently, PMI and ANPMI effectively mitigate the influence of P(Choice) on performance, making them reliable metrics for accurately assessing a model's understanding of prompts to answer questions."}, {"title": "6.2 Comparison of the Metrics", "content": "The results of evaluating the model performance using various metrics are summarized in Table 3. Some examples where ANPMI differs from other metrics can be found in Appendix C. The experiments are conducted using the Language Model Evaluation Harness (Gao et al., 2024) under a zero-shot setting.\nBenchmarks where the final decision of the model depends heavily on P(Choice) show a larger performance gap when measured using metrics other than P(Choice|Prompt). For instance, when evaluating HellaSwag using LLaMA3.1-8B, about 65% of decisions are influenced by the differences in P(Choice) as seen in Table 1, resulting in a 14.68% performance gap between Acc and AccANMPI. Conversely, in MMLU, where only 13% to 16% of decisions of each model depend on the P(Choice) difference according to Table 1, the maximum performance discrepancy is merely up to 0.19% comparing Acc and AccANMPI.\nThe difference between Length-normalized $log P(Choice | Prompt)$ and ANPMI can be observed on MMLU. Since all choices in MMLU have the same length in characters (1 char), AccNorm is identical to Acc, with no performance change occurring due to length normalization. In contrast, ANPMI theoretically addresses the impact of the imbalance in P(Choice) on model performance measurement. As a result, differences between Acc and AccANMPI are consistently observed across all models.\nThe difference in model performance measured by PMI and ANPMI is caused by the fact that PMI does not perform any normalization. Table 4 shows how the lack of normalization affects the model's final choices in MMLU. PMI tends to assign smaller maximum values to choices with higher $log P(Choice)$, making the model less likely to select options with large P(Choice) values. As demonstrated in Table 4, under PMI, choice A (A has the highest $log P(Choice)$) is the least frequently chosen, whereas choice C (C has the lowest $log P(Choice)$) is the most frequently chosen. In contrast, this tendency is less evident when using ANPMI.\nThe experimental results indicate that when model performance is evaluated using a metric that fails to account for the P(Choice) imbalance, the model's performance does not accurately reflect its natural language understanding capability. As a result, ANPMI, which theoretically addresses the P(Choice) imbalance, is identified as the most appropriate metric for assessing a language model's natural language understanding capability."}, {"title": "7 Conclusion", "content": "This paper introduces ANPMI, a novel metric for assessing natural language understanding in language models for multiple-choice tasks. It ensures that the model performance reflects the true comprehension capability of the model rather than unrelated choice preferences. ANPMI is defined by normalizing PMI with $- log P(Choice)$. All choices yield an identical score without a prompt under ANPMI, requiring the model to understand the prompt to solve the task. Unlike PMI, ANPMI maintains the same maximum and minimum values across all choices, mitigating bias towards any specific choice and focusing solely on the relationship between the prompt and choices. Through evaluations using diverse language models and benchmarks, we demonstrate that ANPMI effectively addresses the issue of inaccurate performance measurement caused by imbalances in $P(Choice)$."}, {"title": "Limitations", "content": "For benchmarks where the model's choice, driven by P(Choice), is as critical as understanding the prompt, traditional metrics like P(Choice|Prompt) might be more suitable than ANPMI. For example, in tasks such as ToxiGen (Hartvigsen et al., 2022), which evaluates whether a model avoids harmful responses regardless of the prompt, it is essential to ensure that the model does not generate toxic outputs independently of the prompt. Thus, selecting the right metric based on a clear understanding of the task is crucial for accurate performance evaluation of language models.\nWhile some benchmarks for evaluating language model performance, such as HumanEval (Chen et al., 2021) and IFEval (Zhou et al., 2023), are not in multiple-choice format, this study focuses exclusively on multiple-choice benchmarks. Additionally, although the structure of prompts used in evaluations significantly impacts model performance, our analysis is limited to the effects of choice construction. In the future, we plan to address cases not covered in this study to ensure accurate performance measurement and fair comparisons across models.\nANPMI ensures an accurate measurement of a model's understanding of a prompt by satisfying two key properties: (1) when no prompt is given, all choices have the same metric value, and (2) the choices have the same minimum and maximum metric values. Normalizing PMI with $log P(Choice)$ is based on the assumption that PMI should increase significantly if the model properly understands the prompt. However, when $P(Choice)$ is extremely high, even if the model fully understands the prompt, $P(Choice|Prompt) \\approx P(Choice)$ may hold, leading to minimal PMI changes. ANPMI does not account for this effect, and addressing this limitation remains an important direction for future work."}, {"title": "Ethics Statement", "content": "Our research adheres to rigorous ethical standards while contributing to the advancement of NLP."}, {"title": "A.1 Hellaswag", "content": "HellaSwag (Zellers et al., 2019) is a benchmark for evaluating commonsense natural language inference (NLI). The task involves selecting the most appropriate continuation of a given sentence. We use the validation set, which consists of 10,042 examples, for our experiment.\nPrompt: activity_label: ctx_a ctx_b\nChoices: [endings1, endings2, endings3, endings4]\nPrompt: Clean and jerk: A lady walks to a barbell. She bends down and grabs the pole. The lady\nChoices: [swings and lands in her arms.,pulls the barbell forward.,pulls a rope attached to the barbell.,stands and lifts the weight over her head.]"}, {"title": "A.2 PiQA", "content": "Physical Interaction: Question Answering(PiQA) (Bisk et al., 2020) is a benchmark to evaluate whether a model can answer questions based on physical commonsense knowledge. PiQA focuses on everyday situations with a preference for atypical solutions, and each question has two options. The validation set used for our evaluation consists of 1,838 questions.\nPrompt: Question: question\nAnswer:\nChoices: [sol1, sol2]\nPrompt: Question: To fight Ivan Drago in Rocky for sega master system.\nAnswer:\nChoices: [Drago isn't in this game because it was released before Rocky IV.,You have to defeat Apollo Creed and Clubber Lang first.]"}, {"title": "A.3 ARC", "content": "The AI2 Reasoning Challenge(ARC) (Clark et al., 2018) comprises science questions and answers targeted at students from grade 3 to grade 9. It is divided into two difficulty levels: easy and challenge. For model evaluation, we use the test sets for both difficulty levels. The ARC-Easy test set includes 2,376 questions, while the ARC-Challenge test set contains 1,172 questions.\nPrompt: Question: question\nAnswer:\nChoices: [choices1, choices2, choices3, choices4]\nPrompt: Question: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\nAnswer:\nChoices: [safety goggles, breathing mask, rubber gloves, lead apron]"}, {"title": "A.4 LogiQA", "content": "Logical Reasoning Question Answering(LogiQA) (Liu et al., 2021) is a benchmark designed to assess a model's logical reasoning abilities. It consists of expert-written questions that cover multiple types of deductive reasoning. In our experiments, we use a test set of 651 problems.\nPrompt: Passage: context\nQuestion: question\nChoices:\nA: option1\nB: option2\nC: option3\nD: option4\nAnswer:\nChoices: [option1, option2, option3, option4]\nPrompt: Passage: There are five teams participating in the game. The audience had the following comments on the results? (1) The champion is either the Shannan team or the Jiangbei team. (2) The champion is neither Shanbei nor Jiangnan. (3) The champion is Jiangnan Team. (4) The champion is not the Shannan team.\nQuestion: The result of the match showed that only one argument was correct, so who won the championship?\nChoices:\nA. Shannan\nB. Jiangnan\nC. Shanbei\nD. Jiangbei\nAnswer:\nChoices: [Shannan, Jiangnan, Shanbei, Jiangbei]"}, {"title": "A.5 RACE", "content": "ReAding Comprehension dataset from Examinations(RACE) (Lai et al., 2017) is an English reading comprehension dataset derived from China's middle and high school English exam questions. Each question comprises an article followed by several questions and answer choices. For our evaluation, the test set contains 1,045 questions. We include all but the final question from each set in the prompt, ensuring that most of the context is part of the model's input.\nPrompt: Article: article\nQuestion: problem1\nAnswer: answer1\nQuestion: problem_last\nChoices: [option1, option2, option3, option4]\nPrompt: Article: A girl with blue eyes is a blue-eyed girl. There are sound-proof rooms in all broadcasting stations.\nQuestion: The clothes which you buy from the supermarket are called clothes.\nAnswer: ready-made\nWhat do you think is the best title for the article?\nChoices: [The Forms of Compound Words.,Compound Words in Everyday Life,How to Use Compound Words.,Water-proof Cloth in the Best.]"}, {"title": "A.6 SciQ", "content": "Scientific Question Answering(SciQ) (Welbl et al., 2017) is a dataset of science exam questions crowdsourced across domains such as Physics, Chemistry, and Biology. Each question includes a question, answer choices, and a paragraph of supporting information to assist reasoning. For our evaluation, we use a test set comprising 1,000 questions.\nPrompt: support\nQuestion: question\nAnswer:\nChoices: [distractor1, distractor2, distractor3, correct_answer]\nPrompt: Tree rings, ice cores, and varves indicate the environmental conditions at the time they were made.\nQuestion: Ice cores, varves and what else indicate the environmental conditions at the time of their creation?\nAnswer:\nChoices: [mountain ranges, fossils, magma, tree rings]"}, {"title": "A.7 MMLU", "content": "Massive Multitask Language Understanding(MMLU) (Hendrycks et al., 2020) evaluates a model's breadth and depth of knowledge across various domains. The dataset covers 57 topics, including STEM, humanities, and social sciences. Our experiments use the comprehensive test set, which contains 14,042 questions. Each multiple-choice question assesses the model's ability to integrate diverse knowledge.\nPrompt: question\nA. choice1\nB. choice2\nC. choice3\nD. choice4\nAnswer:\nChoices: [A, B, C, D]\nPrompt: The following are multiple choice questions (with answers) about astronomy. What is the second most common element in the solar system?\nA. Iron\nB. Hydrogen\nC. Methane\nD. Helium\nAnswer:\nChoices: [A, B, C, D]"}, {"title": "B Effects of Altering Choices", "content": "To further investigate the impact of P(Choice) on the model performance, we modify the choices for each problem and examine how these changes affect the model performance. We append the sentence \"Hi\" as the answer choice in each question. The sentence \"Hi\" appears frequently in various text data, resulting in a high prior probability P(\"Hi\"). However, since choices like \"Hi\" are unrelated to the prompt, the model's performance should remain stable if it truly relies on prompt understanding rather than P(Choice) alone. If such a choice affects the model's decisions, this would indicate that P(Choice) plays a significant role in the model's decision-making. We expect that appending a choice with a high prior probability, such as \"Hi,\" will lead to cases where the model incorrectly selects this option over the correct one. To verify it, we perform an experiment using three instruction-tuned language models: Mistral-7B(version 0.3), Gemma-7B, and LLaMA3.1-8B, with three downstream tasks: Hellaswag, Arc-easy, and SciQ. The results are summarized in Table 5.\nTable 5 shows that the model's final choice decisions are affected across all benchmarks when the additional choice is introduced. For SciQ, the effect is relatively minor, with the rate of answer change in the model's final decision ranging from 0.8% to 2.2%. The log prior probability difference has less impact on performance than the PMI difference. However, in the case of Hellaswag, 94.88% to 95.10% of the model's decisions are changed due to the addition of the choice, demonstrating how significantly the prior probability difference can influence the model's decisions. The results demonstrate that P(Choice) substantially affects model performance depending on the benchmark."}, {"title": "C Examples where ANPMI differs from Other Metrics", "content": "The following examples illustrate cases where ANPMI produces different outcomes than other metrics. These examples are derived from evaluations of HellaSwag (Zellers et al., 2019) using Instruct-tuned LLaMA 3.1-8B (Dubey et al., 2024).\nThe first case is a scenario where the correct choice has a long sequence length and a low prior probability (P(Choice)) value. In this case, selecting an answer based on $P(Choice)$ or log P(Choice Prompt) often favors the shortest available option, such as Choice 2, which could lead to incorrect predictions. However, metrics that incorporate normalization based on sequence length or P(Choice), such as length-normalized log P(Choice Prompt), PMI, and ANPMI, can identify the correct answer: Choice 4.\nThe second case is a scenario where the correct choice has a short sequence length and a high P(Choice) value. In this scenario, length-normalized log P(Choice|Prompt) and PMI fail to select the correct answer, Choice 2. When these methods are applied to candidates with much longer sequence lengths or higher P(Choice) values, they tend to overcompensate for these factors, leading to unintended biases that prevent the correct answer from being selected. In contrast, ANPMI maintains the maximum value for each choice at 1, thereby accurately selecting Choice 2.\nIt is important to note that the advantage of ANPMI lies in its ability to evaluate model performance with a stronger emphasis on understanding the Prompt rather than merely yielding higher performance measurements. In the two cases presented, certain biases inherent to other metrics prevent the model from selecting the correct answer. However, these same biases could also, in some cases, help the model select the correct answer even when it has misinterpreted the Prompt. Thus, ANPMI's role is not necessarily to enhance measured performance but to provide a more reliable evaluation framework."}]}