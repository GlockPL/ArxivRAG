{"title": "Generative Dataset Distillation Based on Diffusion Model", "authors": ["Duo Su", "Junjie Hou", "Guang Li", "Ren Togo", "Rui Song", "Takahiro Ogawa", "Miki Haseyama"], "abstract": "This paper presents our method for the generative track of The First Dataset Distillation Challenge at ECCV 2024. Since the diffusion model has become the mainstay of generative models because of its high-quality generative effects, we focus on distillation methods based on the diffusion model. Considering that the track can only generate a fixed number of images in 10 minutes using a generative model for CIFAR-100 and Tiny-ImageNet datasets, we need to use a generative model that can generate images at high speed. In this study, we proposed a novel generative dataset distillation method based on Stable Diffusion. Specifically, we use the SDXL-Turbo model which can generate images at high speed and quality. Compared to other diffusion models that can only generate images per class (IPC) = 1, our method can achieve an IPC = 10 for Tiny-ImageNet and an IPC = 20 for CIFAR-100, respectively. Additionally, to generate high-quality distilled datasets for CIFAR-100 and Tiny-ImageNet, we use the class information as text prompts and post data augmentation for the SDXL-Turbo model. Experimental results show the effectiveness of the proposed method, and we achieved third place in the generative track of the ECCV 2024 DD Challenge. Codes are available at https://github.com/Guang000/BANKO.", "sections": [{"title": "1 Background", "content": "Deep learning has achieved remarkable success, driven by advancements in powerful computational resources. Particularly since the rise of Transformers [2, 3,"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Dataset Distillation", "content": "Dataset distillation was initially proposed by [52]. It aims to synthesize a smaller dataset based on the original dataset while ensuring that the results of model training on the synthetic data are similar to those trained on the original dataset [25]. Existing dataset distillation algorithms mainly include kernel-based and matching-based methods [20,39,58]."}, {"title": "2.2 Diffusion Models", "content": "Generative models have experienced rapid advancements in recent years, leading to their successful application in various industries, with prominent examples including Imagen [41], DALLE 2 [37], Stable Diffusion [38], and Adobe Firefly. Compared to other generative approaches like GANs [11] or VAEs [17], diffusion models often demonstrate more stable and versatile generative capabilities. These models operate by gradually adding random noise to real data and then learning to reverse this process, thereby recovering the original data distribution. Particularly in the field of computer vision, diffusion models have been successfully applied to a wide range of tasks [55]. For instance, SR3 [42] and CDM [14] leverage diffusion models to enhance image resolution. RePaint [30] and Palette [40] utilize denoising techniques for image restoration. Additionally, RVD [56] employs diffusion models for future frame prediction, thereby improving video compression performance, and DDPM-CD [1] uses them for anomaly detection in remote sensing images. Unlike these applications, our approach aims"}, {"title": "2.3 Generative Dataset Distillation", "content": "A few studies have already begun exploring the potential integration of generative models in dataset distillation, with examples such as KFS [18], IT-GAN [62] and DiM [26,50]. The inherent ability of generative models to create new data aligns well with the concept of synthesizing datasets in dataset distillation. KFS improves training efficiency by encoding features in the latent space and using multiple decoders to generate additional new samples. IT-GAN, on the other hand, generates more informative training samples through a GAN model, enabling faster and more effective model training, and thereby addressing the challenges faced in dataset distillation. DiM stores the information of target images into a generative model, enabling the synthesis of new datasets. In parallel with recent works in [12, 33, 47], our approach also integrates an effective variant of the Stable Diffusion model into the dataset distillation framework, resulting in a more efficient method for distilled image synthesis."}, {"title": "3 Methods", "content": "Our goal is to distill the dataset with limited time and resources. We leverage the prior knowledge and controllability of the generative models to develop a dataset distillation method, which does not require any parameter optimization. The method is capable of distilling a large number of images in a short period while maintaining high sampling fidelity. Theoretically, the method is general as it can distill datasets with arbitrary sizes. The core of our approach lies in the use of SDXL-Turbo, a diffusion model trained with adversarial diffusion distillation (ADD) strategy. SDXL-Turbo synthesizes high-fidelity images with only 1-4 steps during the inference time."}, {"title": "3.1 Overall Procedure", "content": "As illustrated in Fig. 1, we employ the Text2Image (T2I) pipeline within SDXL-Turbo, where the category labels from the original dataset are formatted as the prompts. All of the distilled images are obtained after one-step sampling."}, {"title": "3.2 Fast and High-Fidelity Dataset Distillation", "content": "The real-time sampling of the SDXL-Turbo benefits from its ADD training strategy, which includes two objectives: (1) the adversarial loss involving images synthesized by the student model and the original images (xo), and (2) distillation loss compares the teacher-student outputs. Adversarial training ensures the generated images align with the original manifolds, mitigating the blurriness and artifacts inherent in traditional distillation methods. Distillation training"}, {"title": "3.3 T2I Dataset Distillation", "content": "Considering the dataset, category labels provide crucial information that should not be overlooked. These labels can be embedded as conditions in diffusion models to guide the image generation process. During training, the discriminator of SDXL-Turbo employs a projector to extract such conditional information. Consequently, the trained diffusion model can leverage text labels as prompt information."}, {"title": "3.4 Post Data Augmentation", "content": "Since the Dataset Distillation challenge uses a trivial ConvNet classifier for evaluation, we aim to enhance the information richness of the distilled images without increasing computational complexity. Post data augmentation (PDA) is an effective approach, as it only introduces marginal overhead. Specifically, after synthesizing images with the diffusion model, we directly apply several augmentation methods such as image cropping, rotation, or flipping to these images. Please refer to Section 4.1 for detailed PDA settings."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Implementation Details", "content": "The proposed method is based on SDXL-Turbo, an effective variant of the Stable Diffusion model. The pretrained checkpoints come from the official Huggingface repository. For boosting the generation of more high-fidelity images, the model utilizes half-precision (float16) tensors. The prompts consist of the corresponding category labels as mentioned. These labels are derived from the ordered training datasets and saved in the corresponding lists. The hyper-parameters are set to num_inference_steps=1 and guidance_scale=0. To maximize the diversity of the distilled images, Post Data Augmentation is applied after the sampling process. The specific parameters used for augmentations are as follows:\nRandomCrop(width=size, height=size)\nHorizontalFlip(p=0.8)\nVerticalFlip(p=0.8)\nRandomBrightnessContrast(p=0.5)\nRotate(limit=60, p=0.8)\nRandomGamma(p=0.5)\nConsidering the generation speed along with the time required for model loading and image saving, the final IPC is set to 50 for Tiny-ImageNet and 100 for CIFAR-100. The environmental requirements of our experiments are listed as follows:\nPython \u2265 3.9\nPytorch > 1.12.1\nTorchvision \u2265 0.13.1\nDiffusers == 0.29.2\nWe follow the official repository for evaluation. The evaluation network, ConvNetD3-W128, will be trained for 1000 epochs. The learning rate, momentum, and weight decay are set to 0.01, 0.9, and 0.0005 respectively. For assessing the robustness of the method, each dataset is evaluated three times."}, {"title": "4.2 Experimental Results", "content": "According to Table 1, PDA directly increases the number of IPC and significantly improves the distilled performance. The Accuracy increased by 0.0041 on Tiny-ImageNet, from 0.0437 to 0.0478, while in CIFAR-100, it increased by 0.0040. The top-10 category visualization results are displayed in Fig. 2. The first row features images generated by the diffusion model, noted for their high fidelity and realism The remaining four rows illustrate the results of applying data augmentation to these images, which increases their diversity and improves the quality of the distilled dataset."}, {"title": "5 Discussion", "content": null}, {"title": "5.1 Distribution Discrepancy", "content": "The matching strategies in traditional dataset distillation methods ensure that the distribution of the distilled dataset aligns with the original dataset. However, when it comes to the generative models, significant discrepancies may arise between their distributions. Some recent works indicate that images generated by the diffusion model perform well on the large-scale ImageNet-1K. This success may be attributed to two factors: (1) the images closely resemble the real-world scenes and (2) the resolution of ImageNet-1K is similar to the generative model. In other words, ImageNet-1K has a resolution of 224 \u00d7 224 and the diffusion model generates images at 256 \u00d7 256."}, {"title": "6 Conclusion", "content": "We have proposed a novel generative dataset distillation method based on diffusion model. Specifically, we use the SDXL-Turbo model which can generate images at high speed and quality. As a result, our method can generate distilled datasets with large IPC. Furthermore, to obtain high-quality distilled CIFAR-100 and Tiny-ImageNet datasets, we utilize the class information as text prompts and post-augmentation for the proposed method. The top performance during the challenge shows the superiority of our method. In the future, we want to develop more effective distillation techniques with generative models across different datasets."}]}