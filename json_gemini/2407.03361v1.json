{"title": "PianoBART: Symbolic Piano Music Generation and Understanding with Large-Scale Pre-Training", "authors": ["Xiao Liang", "Zijian Zhao", "Weichao Zeng", "Yutong He", "Fupeng He", "Yiyi Wang", "Chengying Gao"], "abstract": "Learning musical structures and composition patterns is necessary for both music generation and understanding, but current methods do not make uniform use of learned features to generate and comprehend music simultaneously. In this paper, we propose PianoBART, a pre-trained model that uses BART for both symbolic piano music generation and understanding. We devise a multi-level object selection strategy for different pre-training tasks of PianoBART, which can prevent information leakage or loss and enhance learning ability. The musical semantics captured in pre-training are fine-tuned for music generation and understanding tasks. Experiments demonstrate that PianoBART efficiently learns musical patterns and achieves outstanding performance in generating high-quality coherent pieces and comprehending music. Our code and supplementary material are available at https://github.com/RS2002/PianoBart.", "sections": [{"title": "I. INTRODUCTION", "content": "Music generation and understanding are interrelated topics in the music community. Understanding the melody, rhythm, and structure [1], [2] greatly benefits effective music genera- tion. Music generation [3], [4] is also helpful in investigating how well machines understand musical structure and com- position patterns. Hence there is great and urgent interest in exploring automatic music generation and understanding.\nGiven the sequential similarity between text and symbolic music, language-based methods have been applied to sym- bolic music generation and understanding [1]\u2013[6]. However, directly using language-based methods is challenging due to the inherent differences between text and music. First, music has a more complex semantic and hierarchical structure than natural language, which means that symbolic music requires much longer sequences to represent. Second, symbolic music involves various musical elements like melody, rhythm, and harmony. Nevertheless, the lack of sufficient specialized la- beled data hinders representation learning.\nAlthough recent works on music generation have proposed some encoding methods to represent symbolic music [5], [7], these encodings produce very long sequences up to thousands or more tokens for a full song of several minutes. To address this, we adopt the compact Octuple encoding [1]. It can efficiently and comprehensively represent music and greatly reduce the sequence length, thus supporting long-term music generation and full-song-level understanding.\nIn many fields including Music Information Retrieval (MIR), there is often insufficient labeled data due to factors like expensive cost and copyright issues. To overcome the scarcity of labeled data, pre-training on available unlabeled data has become the most promising method [8]\u2013[10], as it enables models to learn common features of data structure. In MIR, there have been several works [1], [2], [6] using the pre-training models in NLP such as BERT [11]. However, the problem of information leakage or loss is caused by these models selecting too few or too many objects for pre- training transformations, which degrades the ability to capture underlying musical patterns. To prevent information leakage or loss, based on the octuple encoding and pre-training tasks, we propose a multi-level object selection strategy including a designed n-bar level method. The method can dynamically determine the selection range of pre-training objects and is applicable to different pre-training transformations.\nBoth music generation and music understanding are based on learning musical structure and semantics, while existing methods address these tasks separately, preventing the reuse of learned features in one task for the other. To this end, we propose PianoBART, a novel BART-based pre-trained model that addresses both symbolic piano music generation and understanding in a unified framework. Unlike BERT-based pre-trained models [1], [2], [6], PianoBART uses an encoder- decoder structure that allows it to be applied for sequence- to-sequence task, thus handling both generation and compre- hension. In addition, PianoBART includes more transforma- tions than BERT to enhance pre-training effectively. More importantly, for each transformation, we design corresponding object selection methods to largely avoid information leakage or loss. PianoBART is pre-trained in a self-supervised way to overcome the lack of labeled data. Experiments show that PianoBART successfully captures music domain knowledge and excels in both music generation and comprehension.\nIn summary, the main contributions of this work are:\n(1) We propose PianoBART, the first large-scale pre-trained model that uses BART for both symbolic piano music gener- ation and understanding.\n(2) A novel multi-level object selection strategy for pre- training is designed to avoid information leakage and loss while improving the quality of downstream tasks.\n(3) Experiments show that PianoBART achieves excellent performance on realistic and coherent music generation and a number of music understanding tasks."}, {"title": "II. RELATED WORK", "content": "A. Symbolic Music Generation\nSignificant progress has been made in automatic symbolic music generation [3]\u2013[5]. Inspired by the similarity between text and symbolic music, sequence models like attention-based Transformer [12] have been increasingly applied to capture the long-term dependency of music and produce coherent music samples [5], [7]. However, existing symbolic music encoding [5], [13] produces too lengthy sequences, making the Transformer computationally difficult. In this paper, we introduce Octuple representation [1] into music generation, effectively reducing the sequence length. Besides, this paper introduces the BART pre-training model [14] and proposes PianoBART for symbolic music generation.\nB. Symbolic Music Understanding\nIn the symbolic music domain, it's difficult and time- consuming to obtain professional labels, hence existing la- beled datasets remain small size [15]\u2013[17]. To overcome the lack of labeled data, recent works use pre-trained language models like BERT [11], [18] to learn the long-term musical structure in an unsupervised way [1], [2], [6]. However, due to the repetitive characteristics of music, simply using pre- training methods in NLP (e.g., mask language model [11]) for symbolic music may lead to information leakage or loss [1], affecting the performance of downstream tasks. To address this, we design a multi-level object selection strategy for pre- training that is able to enhance the model's robustness."}, {"title": "III. APPROACH", "content": "A. PianoBART Framework\nThe proposed PianoBART is a BART-based model for piano music generation and understanding. As shown in Fig. 1, PianoBART adopts the standard Transformer encoder and decoder [12] architecture as the backbone. The encoder bidirectionally encodes sequences of symbolic music tokens through multi-head self-attention. The decoder autoregres- sively generates outputs from left to right, which is suitable for sequence generation tasks.\nWe employ the Octuple representation [1] to encode sym- bolic music. An example is demonstrated in Fig. 1, where the input MIDI is converted to a sequence of octuple tokens. Each octuple token corresponds to a note and contains 8 mu- sical elements, including time signature (TS), tempo (BPM), bar, position, instrument, pitch, duration, and velocity. The embeddings of the 8 elements in each token are concatenated together and then linear projected to the embedding token, which is fed to the BART. As for the hidden state produced by the Transformer decoder, octuple elements in each token can be predicted simultaneously with different linear layers.\nPianoBART is pre-trained by (1) corrupting octuple token sequences, and (2) learning a model to reconstruct the original sequences [14]. More diverse transformations than BERT that destroy the sequence structure are employed to enhance the model's ability to learn the musical pattern. We design a multi- level object selection strategy based on the pre-training trans- formations and the employed Octuple encoding. This strategy can effectively prevent the information leakage and loss that may occur during the pre-training. PianoBART is fine-tuned on a variety of downstream tasks in music generation and understanding, which will be described in experiments.\nB. Multi-level Object Selection Strategy\nPianoBART is trained in a self-supervised way that maps the corrupted octuple token sequences into the original ones. The initial operation is to select specific objects and apply certain transformations to corrupt the original structure. Then the model is trained to reconstruct the modified objects.\nTo choose the objects to be corrupted, we design a multi- level object selection strategy from two dimensions of the octuple encoding, i.e., attributes and time span. For simplicity, we refer to each octuple object as a \"token\" and each attribute within an octuple object as an \u201celement\". In terms of attributes, we consider the Element Level and the Token Level, with element and token as a single entity, respectively. In terms of time span, we consider the Octuple Level, the Bar Level, and the n-Bar Level, each covering a different period. The combination of attributes and time span leads to 6 selection methods.\nOctuple Level. Figure 1 shows an example of the Octuple Level selection method, which is inspired by the naive selec- tion method in the mask language modeling (MLM) of BERT [11]. In the Octuple-Element Level and the Octuple-Token Level, we randomly choose independent elements or tokens as target objects, respectively.\nHowever, the octuple level method may cause informa- tion leakage. In specific, music is repetitive, some musical attributes (e.g., bar, position, and pitch) may be identical in successive segments. Considering the case of simply masking a single note, the missing attributes can be easily inferred by directly replicating neighboring notes. The model can therefore achieve relatively high accuracy without learning the music context. However, the underlying musical structures and patterns cannot be fully captured, which limits the performance of downstream tasks.\nBar Level. To address the information leakage problem, the Bar-Element Level method is proposed by [1], where elements of the same type in the same bar are regarded as a unit and selected simultaneously. Moreover, we further present the Bar- Token Level method, which chooses all the complete tokens within the same bar at the same time.\nHowever, in musical compositions, some elements do not strictly repeat within a whole bar. For example, as shown in Fig. 1, the \"Pitch A4\" only repeats for half a bar. Although the information leakage is mitigated at the bar level, it's more likely to cause information loss, when a whole bar that may contain dozens of notes or tokens is masked [6].\nn-Bar Level. To enhance the model's generalization capacity, we further design a novel n-Bar Level Method, where the time span n is randomly chosen and defined as follows:\n$n = inf\\{n : \\sum_{i=p}^{p+n} dur(T_i) \\geq \\frac{m}{64}\\}.$ (1)\nwhere $p$ is the number of an initially selected token $T_p$, $T_i$ is the $i^{th}$ token in the octuple sequence, $dur()$ represents the duration of a token, and $m$ is a random integer in $[1,128]$. The n-Bar-Element Level or the n-Bar-Token Level is to select n consecutive elements or tokens at a time, respectively. For example, assuming there are 4 quarter-notes in a measure, only the first note is selected ($n = 1$) if $m \\in [1,16]$, the first two notes are selected ($n = 2$) if $m \\in [17,32]$, and so on.\nAccording to Equation (1), the minimum selected time span is a hemidemisemiquaver (1/64) and the maximum is two whole notes (2 bars in 4/4 time signature), which is the common range of duration for a note. Compared to the Bar Level method (equivalent to the 1-Bar-Level), the selection range of n-Bar Level is optional, not limited to a single bar, which is more flexible. This way can effectively prevent information loss and avoid information leakage. Furthermore, the dynamic selection range helps the model to learn more structural relations in music like intra-bar and inter-bar con- nections. Therefore, high-level semantic information of music, such as chords (a fixed combination of adjacent pitches), is more likely to be captured by the n-Bar Level method.\nC. Pre-training PianoBART\nTo train PinaoBART, we utilize the noising approaches of BART [14], which consists of five transformations. By combining these transformations, PinaoBART enables any type of corruption to the original music sequence, forcing the model to reason more about the musical context. For each transformation, we apply different object selection methods.\n(1) Token Masking. Elements or tokens are randomly sampled and replaced with the [MASK] token. For this task, we use four object selection methods, including the Octuple-Element Level, the Octuple-Token Level, the n-Bar-Element Level, and the n-Bar-Token Level.\n(2) Token Deletion. Some of the objects are randomly deleted with a probability of 15%. It is evident that deleting the element-level object would cause alignment issues. Therefore, we only allow the Octuple-Token Level and the n-Bar-Token Level selection methods for this task.\n(3) Text Infilling. Spans of objects are replaced with a single [MASK] token. Similar to the Token Deletion task, we also employ the Octuple-Token Level and the n-Bar-Token Level selection methods for this task.\n(4) Sentence Permutation. For text, sentence permutation refers to randomly shuffling the sentence order [14]. In music, however, there are few concepts corresponding to sentences. Obtaining such concepts from MIDI is not feasible, and manual annotation requires specialized knowledge. To this end, we treat each bar as a \"simplified sentence\". We adopt the Octuple-Token Level and the Bar-Token Level method to randomly split the music and shuffle.\n(5) Document Rotations. A token is selected, and then the sequence is rotated around the chosen token. Since we only need to choose one token at a time, we only utilize the Octuple-Token Level choosing method for this task.\nDuring pre-training, PinaoBART is optimized with a recon- struction loss\u2014the cross-entropy between the decoder's output and the original token sequence."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "In this section, we first present the experimental setup in this study. Then we conduct a series of evaluations and analyze the results to verify the performance of PianoBART.\nA. Experimental Setup\nWe pre-train PianoBART with 8 layers of 8 attention heads in each of the encoder and decoder, and a hidden size of 1024, resulting in 225M parameters. The batch size is 16 sequences, each with a maximum length of 1024 octuple tokens. The training is conducted on two NVIDIA V100 GPUs for 3 days. We use Adam optimizer and set the learning rate to 2e-5, and L2 weight decay to le-2. We clip the gradient with the maximum norm of 3. The training is early stopped when the loss has not decreased for 30 consecutive epochs. During pre-training, PianoBART adopts the proposed multi- level object selection strategy and randomly selects one of the 5 transformations to dynamically corrupt the data for each batch.\nB. Pre-training\nWe collect five public available piano MIDI datasets (Pop1K7 [7], ASAP [16], POP909 [15], Pianist8 [2], and \u0395\u039c\u039f\u03a1\u0399\u0391 [17]) to train PianoBART. These datasets contain Western classical music as well as piano covers of pop music, including 4166 pieces in total. We convert MIDI files into Octuple sequences and split them into segments with 1024 tokens, which results in 8393 segments for pre-training. MusicBERT [1] and MidiBERT [2] are used as baselines and are pre-trained with the same data and resources. Each model was trained five times and the average performance is shown in Table II. Among the baselines, PianoBART achieves the shortest pre-training time with a speed faster than baselines, reaching the best reconstruction accuracy of over 96%.\nC. Fine-tuning\nWe fine-tune PianoBART on two types of downstream tasks: music generation and understanding. In this work, music generation involves conditioning the model with a fragment of piano performance as the prompt and producing a continua- tion. The encoder takes the prompt as input and the decoder autoregressively generates the target music. We leverage the stochastic temperature-controlled sampling method [7] to im- prove the diversity of generated samples.\nThe music understanding consists of two token-level tasks (velocity prediction and melody extraction), and two sequence- level tasks (emotion classification and composer classifica- tion). The same token sequence is fed into the encoder and decoder. For token-level tasks, each input token corre- sponds to an output label. The hidden state of the top-layer decoder is used to classify each token. For the sequence-level tasks, the final hidden state of the decoder is fed into an additional Attention-based Weight Average Layer [2] to map the decoder's output sequence to a single label. We compare PianoBART with previous works on symbolic music understanding, including MusicBERT [1] and MidiBERT [2].\n(1) Music Continuation. Since most music-generative mod- els are auto-regressive sequence-to-sequence models [3], [5], it's appropriate to evaluate with a continuation task. We fine- tune PianoBART on two MIDI datasets: MAESTRO [19] includes 1276 recordings, and GiantMIDI-Piano [20] contains 7236 pieces of classical music, from which we select 1383 songs. Given our resource constraints, we randomly crop segments with 1024 octuple tokens as the prompt and the following 1024 tokens as the continuation.\nWe compare PianoBART with Music Transformer [3] and Pop Music Transformer [5] on Pitch Fr\u00e9chet Similarity (PFS), Pitch Class Histogram Entropy (PCHE), and Grooving pattern Similarity (GS). PFS measures the pitch distance of generated results from the ground truth (GT) [21]. We also test the PFS between generated results and the given prompt to measure repeated patterns. PCHE reflects the stability of the pitch distribution and GS measures the coherence of rhythm [22]. Both PCHE and GS concern the music itself and we compute their absolute difference with GT. Results are shown in Table IV and Table III. We observe that PianoBART outperforms baselines by a large margin, which shows the effectiveness of PianoBART. We also see that PianoBART supports generating music of arbitrary length, while [3], [5] can only produce sequences of limited length.\n(2) Velocity Prediction. The velocity in music indicates the level of dynamics and corresponds to the perceptual loudness of notes. Learning the velocity is helpful in modeling expressive piano performance [23], [24]. Following [2], in our experiment, we quantize MIDI velocity values (0-127) into 6 levels. Velocity prediction is regarded as a 6-class classification task. We adopt the GiantMidi dataset [20] and train the model to predict the velocity level for each note. Table V shows the classification accuracy. It's apparent that the performance is generally not high, which may be because velocity is a rather subjective factor related to the dynamics of performers.\n(3) Melody Extraction. Melody is the most intuitive and important element of a musical composition. In the symbolic domain, melody extraction is to identify the melody notes in a MIDI file of polyphonic piano music [25], [26], which is a crit- ical token-level understanding task. We fine-tune PianoBART on the POP909 dataset [15] that contains piano covers of 909 pop songs, with labels of melody, bridge, and accompaniment for each note. The performance is evaluated by classification accuracy and the results are shown in Table V. There is little performance difference between the compared methods, with PianoBART achieving slightly better accuracy.\n(4) Emotion Recognition. Music is a natural carrier to express and convey emotion. Understanding the overall emo- tion of music [27], [28] is of great significance for topics such as music recommendation and personalized music gen- eration. In this paper, emotion recognition is treated as a 4-class classification problem. We adopt the EMOPIA [17], an emotion-labeled symbolic music dataset for this task. The emotional annotation of each clip is labeled using the 4-class taxonomy (HVHA, HVLA, LVHA, LVLA). Table V shows that PianoBART outperforms compared baselines, showing its ability in symbolic-domain emotion recognition.\n(5) Composer Classification. Recognizing a composer of a piece [29], [30] used to be reserved for experts in music theory, which is a fine-grained discriminative task compared to genre or style classification. We use the Pianist8 dataset [2] and the ASAP dataset [16] for this task. Pianist8 consists of 411 original piano pieces performed by eight composers. ASAP contains 1068 MIDI pieces of Western classical piano music from 15 composers. Table V demonstrates the strengths of PianoBART on this professional sequence-level music com- prehension task.\nD. Ablation Study\nWe design two variants (PianoBART (w/o pretraining) and PianoBART-simple) to validate the effects of pre-training and the proposed multi-level object selection strategy. (1) Piano- BART (w/o pretraining) does not initialize the model with the pre-trained parameters, and only uses the data of downstream tasks to train PianoBART from scratch. (2) PianoBART-simple only uses the Octuple-Token Level method to select the object to be corrupted for any pre-training transformation.\nTable IV and Table III reflect the results of ablated vari- ants on music continuation. Figure 2 shows the piano-roll visualization of MIDI produced by ablated variants. We can see that the lack of pre-training performs worse on PFS and struggles in continuous music generation. PianoBART-simple achieves better PFS scores but lacks clear and coherent musical patterns. Notably, PianoBART generates harmonious long- term music with coherent rhythm, showing the effectiveness of pre-training and the proposed multi-level selection strategy. We provide MIDI demos in the supplementary material and recommend readers to listen for an intuitive experience.\nTable V shows the ablation results on four music under- standing tasks. It's obvious that the absence of pre-training significantly affects the accuracy, while using the pre-trained model to initialize and fine-tune can improve the performance. The results can be further improved by using PianoBART'S multi-level object selection strategy, which demonstrates the effectiveness of our method."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose PianoBART, a comprehensive pre- trained model designed for symbolic music understanding and generation. By introducing the BART framework and devising a multi-level object selection strategy, PianoBART exhibits remarkable performance in generating coherent music and un- derstanding musical patterns. The ablation results demonstrate the effectiveness of the pre-training and the proposed multi- level object selection strategy. PianoBART holds significant potential for advancing music study and creation. Future works involve further enhancements in the model's performance and the incorporation of expert knowledge."}]}