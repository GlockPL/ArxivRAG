{"title": "PianoBART: Symbolic Piano Music Generation and Understanding with Large-Scale Pre-Training", "authors": ["Xiao Liang", "Zijian Zhao", "Weichao Zeng", "Yutong He", "Fupeng He", "Yiyi Wang", "Chengying Gao"], "abstract": "Learning musical structures and composition patterns is necessary for both music generation and understanding, but current methods do not make uniform use of learned features to generate and comprehend music simultaneously. In this paper, we propose PianoBART, a pre-trained model that uses BART for both symbolic piano music generation and understanding. We devise a multi-level object selection strategy for different pre-training tasks of PianoBART, which can prevent information leakage or loss and enhance learning ability. The musical semantics captured in pre-training are fine-tuned for music generation and understanding tasks. Experiments demonstrate that PianoBART efficiently learns musical patterns and achieves outstanding performance in generating high-quality coherent pieces and comprehending music.", "sections": [{"title": "I. INTRODUCTION", "content": "Music generation and understanding are interrelated topics in the music community. Understanding the melody, rhythm, and structure [1], [2] greatly benefits effective music generation. Music generation [3], [4] is also helpful in investigating how well machines understand musical structure and composition patterns. Hence there is great and urgent interest in exploring automatic music generation and understanding.\nGiven the sequential similarity between text and symbolic music, language-based methods have been applied to symbolic music generation and understanding [1]\u2013[6]. However, directly using language-based methods is challenging due to the inherent differences between text and music. First, music has a more complex semantic and hierarchical structure than natural language, which means that symbolic music requires much longer sequences to represent. Second, symbolic music involves various musical elements like melody, rhythm, and harmony. Nevertheless, the lack of sufficient specialized labeled data hinders representation learning.\nAlthough recent works on music generation have proposed some encoding methods to represent symbolic music [5], [7], these encodings produce very long sequences up to thousands or more tokens for a full song of several minutes. To address this, we adopt the compact Octuple encoding [1]. It can efficiently and comprehensively represent music and greatly reduce the sequence length, thus supporting long-term music generation and full-song-level understanding."}, {"title": "II. RELATED WORK", "content": "Significant progress has been made in automatic symbolic music generation [3]\u2013[5]. Inspired by the similarity between text and symbolic music, sequence models like attention-based Transformer [12] have been increasingly applied to capture the long-term dependency of music and produce coherent music samples [5], [7]. However, existing symbolic music encoding [5], [13] produces too lengthy sequences, making the Transformer computationally difficult. In this paper, we introduce Octuple representation [1] into music generation, effectively reducing the sequence length. Besides, this paper introduces the BART pre-training model [14] and proposes PianoBART for symbolic piano music generation."}, {"title": "A. Symbolic Music Understanding", "content": "In the symbolic music domain, it's difficult and time-consuming to obtain professional labels, hence existing labeled datasets remain small size [15]\u2013[17]. To overcome the lack of labeled data, recent works use pre-trained language models like BERT [11], [18] to learn the long-term musical structure in an unsupervised way [1], [2], [6]. However, due to the repetitive characteristics of music, simply using pre-training methods in NLP (e.g., mask language model [11]) for symbolic music may lead to information leakage or loss [1], affecting the performance of downstream tasks. To address this, we design a multi-level object selection strategy for pre-training that is able to enhance the model's robustness."}, {"title": "B. PianoBART Framework", "content": "The proposed PianoBART is a BART-based model for piano music generation and understanding. As shown in Fig. 1, PianoBART adopts the standard Transformer encoder and decoder [12] architecture as the backbone. The encoder bidirectionally encodes sequences of symbolic music tokens through multi-head self-attention. The decoder autoregressively generates outputs from left to right, which is suitable for sequence generation tasks.\nWe employ the Octuple representation [1] to encode symbolic music. An example is demonstrated in Fig. 1, where the input MIDI is converted to a sequence of octuple tokens. Each octuple token corresponds to a note and contains 8 musical elements, including time signature (TS), tempo (BPM), bar, position, instrument, pitch, duration, and velocity. The embeddings of the 8 elements in each token are concatenated together and then linear projected to the embedding token, which is fed to the BART. As for the hidden state produced by the Transformer decoder, octuple elements in each token can be predicted simultaneously with different linear layers.\nPianoBART is pre-trained by (1) corrupting octuple token sequences, and (2) learning a model to reconstruct the original sequences [14]. More diverse transformations than BERT that destroy the sequence structure are employed to enhance the model's ability to learn the musical pattern. We design a multi-level object selection strategy based on the pre-training transformations and the employed Octuple encoding. This strategy can effectively prevent the information leakage and loss that may occur during the pre-training. PianoBART is fine-tuned on a variety of downstream tasks in music generation and understanding, which will be described in experiments."}, {"title": "C. Multi-level Object Selection Strategy", "content": "PianoBART is trained in a self-supervised way that maps the corrupted octuple token sequences into the original ones. The initial operation is to select specific objects and apply certain transformations to corrupt the original structure. Then the model is trained to reconstruct the modified objects.\nTo choose the objects to be corrupted, we design a multi-level object selection strategy from two dimensions of the octuple encoding, i.e., attributes and time span. For simplicity, we refer to each octuple object as a \"token\" and each attribute within an octuple object as an \u201celement\u201d. In terms of attributes, we consider the Element Level and the Token Level, with element and token as a single entity, respectively. In terms of time span, we consider the Octuple Level, the Bar Level, and the n-Bar Level, each covering a different period. The combination of attributes and time span leads to 6 selection methods, as shown in Table I.\nOctuple Level. Figure 1 shows an example of the Octuple Level selection method, which is inspired by the naive selection method in the mask language modeling (MLM) of BERT [11]. In the Octuple-Element Level and the Octuple-Token Level, we randomly choose independent elements or tokens as target objects, respectively.\nHowever, the octuple level method may cause information leakage. In specific, music is repetitive, some musical attributes (e.g., bar, position, and pitch) may be identical in successive segments. Considering the case of simply masking a single note, the missing attributes can be easily inferred by directly replicating neighboring notes. The model can therefore achieve relatively high accuracy without learning the music context. However, the underlying musical structures and patterns cannot be fully captured, which limits the performance of downstream tasks.\nBar Level. To address the information leakage problem, the Bar-Element Level method is proposed by [1], where elements of the same type in the same bar are regarded as a unit and selected simultaneously. Moreover, we further present the Bar-Token Level method, which chooses all the complete tokens within the same bar at the same time.\nHowever, in musical compositions, some elements do not strictly repeat within a whole bar. For example, as shown in Fig. 1, the \"Pitch A4\" only repeats for half a bar. Although the information leakage is mitigated at the bar level, it's more likely to cause information loss, when a whole bar that may contain dozens of notes or tokens is masked [6].\nTo enhance the model's generalization capacity, we further design a novel n-Bar Level Method, where the time span n is randomly chosen and defined as follows:\n$n = \\inf\\left\\{n : \\sum_{i=p}^{p+n} d u r\\left(T_{i}\\right) \\geq \\frac{m}{64}\\right\\}.$\nwhere p is the number of an initially selected token $T_{p}, T_{i}$ is the $i^{t h}$ token in the octuple sequence, $d u r()$ represents the duration of a token, and m is a random integer in [1,128]. The n-Bar-Element Level or the n-Bar-Token Level is to select n consecutive elements or tokens at a time, respectively. For example, assuming there are 4 quarter-notes in a measure, only the first note is selected (n = 1) if $m \\in[1,16]$, the first two notes are selected (n = 2) if $m \\in[17,32]$, and so on.\nAccording to Equation (1), the minimum selected time span is a hemidemisemiquaver (1/64) and the maximum is two whole notes (2 bars in 4/4 time signature), which is the common range of duration for a note. Compared to the Bar Level method (equivalent to the 1-Bar-Level), the selection range of n-Bar Level is optional, not limited to a single bar, which is more flexible. This way can effectively prevent information loss and avoid information leakage. Furthermore, the dynamic selection range helps the model to learn more structural relations in music like intra-bar and inter-bar connections. Therefore, high-level semantic information of music, such as chords (a fixed combination of adjacent pitches), is more likely to be captured by the n-Bar Level method."}, {"title": "D. Pre-training PianoBART", "content": "To train PinaoBART, we utilize the noising approaches of BART [14], which consists of five transformations. By combining these transformations, PinaoBART enables any type of corruption to the original music sequence, forcing the model to reason more about the musical context. For each transformation, we apply different object selection methods.\n(1) Token Masking. Elements or tokens are randomly sampled and replaced with the [MASK] token. For this task, we use four object selection methods, including the Octuple-Element Level, the Octuple-Token Level, the n-Bar-Element Level, and the n-Bar-Token Level.\n(2) Token Deletion. Some of the objects are randomly deleted with a probability of 15%. It is evident that deleting the element-level object would cause alignment issues. Therefore, we only allow the Octuple-Token Level and the n-Bar-Token Level selection methods for this task.\n(3) Text Infilling. Spans of objects are replaced with a single [MASK] token. Similar to the Token Deletion task, we also employ the Octuple-Token Level and the n-Bar-Token Level selection methods for this task.\n(4) Sentence Permutation. For text, sentence permutation refers to randomly shuffling the sentence order [14]. In music, however, there are few concepts corresponding to sentences. Obtaining such concepts from MIDI is not feasible, and manual annotation requires specialized knowledge. To this end, we treat each bar as a \u201csimplified sentence\u201d. We adopt the Octuple-Token Level and the Bar-Token Level method to randomly split the music and shuffle.\n(5) Document Rotations. A token is selected, and then the sequence is rotated around the chosen token. Since we only need to choose one token at a time, we only utilize the Octuple-Token Level choosing method for this task.\nDuring pre-training, PinaoBART is optimized with a reconstruction loss\u2014the cross-entropy between the decoder's output and the original token sequence."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "In this section, we first present the experimental setup in this study. Then we conduct a series of evaluations and analyze the results to verify the performance of PianoBART."}, {"title": "A. Experimental Setup", "content": "We pre-train PianoBART with 8 layers of 8 attention heads in each of the encoder and decoder, and a hidden size of 1024, resulting in 225M parameters. The batch size is 16 sequences, each with a maximum length of 1024 octuple tokens. The training is conducted on two NVIDIA V100 GPUs for 3 days. We use Adam optimizer and set the learning rate to 2e-5, and L2 weight decay to le-2. We clip the gradient with the maximum norm of 3. The training is early stopped when the loss has not decreased for 30 consecutive epochs. During pre-training, PianoBART adopts the proposed multi-level object selection strategy and randomly selects one of the 5 transformations to dynamically corrupt the data for each batch."}, {"title": "B. Pre-training", "content": "We collect five public available piano MIDI datasets (Pop1K7 [7], ASAP [16], POP909 [15], Pianist8 [2], and \u0395\u039c\u039f\u03a1\u0399\u0391 [17]) to train PianoBART. These datasets contain Western classical music as well as piano covers of pop music, including 4166 pieces in total. We convert MIDI files into Octuple sequences and split them into segments with 1024 tokens, which results in 8393 segments for pre-training. MusicBERT [1] and MidiBERT [2] are used as baselines and are pre-trained with the same data and resources. Each model was trained five times and the average performance is shown in Table II. Among the baselines, PianoBART achieves the shortest pre-training time with a speed faster than baselines, reaching the best reconstruction accuracy of over 96%."}, {"title": "C. Fine-tuning", "content": "We fine-tune PianoBART on two types of downstream tasks: music generation and understanding. In this work, music generation involves conditioning the model with a fragment of piano performance as the prompt and producing a continuation. The encoder takes the prompt as input and the decoder autoregressively generates the target music. We leverage the stochastic temperature-controlled sampling method [7] to improve the diversity of generated samples."}, {"title": "D. Ablation Study", "content": "We design two variants (PianoBART (w/o pretraining) and PianoBART-simple) to validate the effects of pre-training and the proposed multi-level object selection strategy. (1) Piano-BART (w/o pretraining) does not initialize the model with the pre-trained parameters, and only uses the data of downstream tasks to train PianoBART from scratch. (2) PianoBART-simple only uses the Octuple-Token Level method to select the object to be corrupted for any pre-training transformation."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose PianoBART, a comprehensive pre-trained model designed for symbolic music understanding and generation. By introducing the BART framework and devising a multi-level object selection strategy, PianoBART exhibits remarkable performance in generating coherent music and understanding musical patterns. The ablation results demonstrate the effectiveness of the pre-training and the proposed multi-level object selection strategy. PianoBART holds significant potential for advancing music study and creation. Future works involve further enhancements in the model's performance and the incorporation of expert knowledge."}]}