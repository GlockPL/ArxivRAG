{"title": "Refining Wikidata Taxonomy using Large Language Models", "authors": ["Yiwen Peng", "Thomas Bonald", "Mehwish Alam"], "abstract": "Due to its collaborative nature, Wikidata is known to have a complex taxonomy, with recurrent issues like the ambiguity between instances and classes, the inaccuracy of some taxonomic paths, the presence of cycles, and the high level of redundancy across classes. Manual efforts to clean up this taxonomy are time-consuming and prone to errors or subjective decisions. We present WiKC, a new version of Wikidata taxonomy cleaned automatically using a combination of Large Language Models (LLMs) and graph mining techniques. Operations on the taxonomy, such as cutting links or merging classes, are performed with the help of zero-shot prompting on an open-source LLM. The quality of the refined taxonomy is evaluated from both intrinsic and extrinsic perspectives, on a task of entity typing for the latter, showing the practical interest of WiKC.", "sections": [{"title": "1 Introduction", "content": "Wikidata is a general-purpose Knowledge Base (KB) maintained by a large community of contributors. As a collaborative project, Wikidata faces several challenges, including the ambiguity, inconsistency, redundancy, and complexity of its taxonomy. Ambiguity arises from the confusion between instances and classes. For example, scientist (Q901) is both an instance of profession (Q28640) and a subclass of person (Q215627). Inconsistency here refers to the inaccuracy of some taxonomic paths. For instance, city (Q515) is inaccurately classified as a subclass of mathematical object (Q246672) through the following taxonomic path: city (Q515) \u2192 spatial entity (Q58416391) \u2192 geometric object (Q123410745) \u2192 mathematical object (Q246672). Redundancy is also prevalent with classes like human (Q5) and person (Q215627) coexisting, where one would suffice. The complexity of the taxonomy is another major issue. The taxonomy of Wikidata has a depth of 20, contains many cycles, like axiom (Q17736) \u2192 first principle (Q536351) \u2192 principle (Q211364) \u2192 axiom (Q17736), and transitive links, like airport (Q1248784) \u2192 aerodrome (Q62447) \u2192 station (Q12819564) and airport (Q1248784) \u2192 station (Q12819564). Additionally, only 4% of the 4 million classes are instantiated, with many lacking labels and descriptions."}, {"title": "2 Related Work", "content": "General-purpose knowledge bases. Wikidata [21] is the largest open general-purpose KB, maintained by a large community of contributors. Due to its collaborative nature, Wikidata is known to have a complex taxonomy, including errors, redundancies and inconsistencies [18,6]. Cleaning this taxonomy is the main objective of our work. Other general-purpose KBs include DBpedia [3] and YAGO 4.5 [19]. DBpedia is a multilingual KB automatically extracted from Wikipedia and more recently, Wikidata. Its ontology covers a wide range of concepts but suffers from inconsistencies [17,1] due to its reliance on Wikipedia and the prioritization of coverage over precision. YAGO 4.5 is based on Wikidata, with a manual mapping of the upper taxonomy to Schema.org, providing a clean upper-level taxonomy designed by human experts. In this paper, we propose an automatic approach for refining the Wikidata taxonomy, without requiring any human expertise or subjective decision.\nTaxonomy refinement, taxonomy induction. Taxonomy refinement is the task of updating an existing taxonomy while maintaining its structure. Previous methods are either domain-specific [15] or depend on lexical structures of existing hierarchies [16]. Recently, more advanced approaches have incorporated word embeddings into taxonomy refinement. For instance, hyperbolic embeddings are used in [2] to detect outliers in a domain-specific taxonomy. In [14], a hierarchical semantic similarity metric is used to select better embeddings and then refine a taxonomy."}, {"title": "3 Approach", "content": "In our work, we use the truthy version of Wikidata\u00b3, which contains the best non-deprecated rank for each property."}, {"title": "3.1 Taxonomy Extraction", "content": "In principle, the taxonomy of Wikidata is defined by the subclassOf (P279) property. In practice, this property is often confused with the instance Of (P31) property by contributors to Wikidata, requiring some work to extract the actual taxonomy.\nInstance or class? We extract instances and classes using the instance Of and subclass Of properties, respectively, giving priority to the instance Of property if both appear. For example, the entity hydrogen (Q556), which is both an instance of chemical element (Q11344) and a subclass of energetic material (Q5376832), is considered as an instance, not a class. However, some exceptions must be taken into account. For example, the entity company (Q783794), which is an instance of type of organisation (Q17197366) and a subclass of organization (Q43229), should be considered as a class. The difference with the previous example is that the entity type of organisation is, in fact, a metaclass (Q19478619), i.e., a class which has instances that are all themselves classes. Given that, the entity company should indeed be considered as a class, not an instance.\nIn our case, we consider that an entity is a meta-class if it is an instance of either metaclass (Q19478619) or second-order class (Q24017414) that meets the following criteria: (1) Its label contains a keyword like type, class, style, genre, form, occupation, profession, category, classification, (2) Its label does not contain a preposition, which corresponds to very specific classes, nor the keyword property, which refers to classes of properties.\nWe also exclude BFO class (Q124711104) from the meta-classes to avoid external ontologies. We finally obtain 434 meta-classes and approximately 1.7M classes (either an instance of a meta-class or an entity that has the subclass Of"}, {"title": "3.2 Taxonomy Refinement", "content": "We refine the taxonomy to address issues like redundancy and inconsistency. For instance, the class city or town (Q27676416) is a subclass of city or town (Q7930989) (redundancy) and a transitive subclass of mathematical object (Q246672) (inconsistency). For this, we prompt an LLM (see details in \u00a73.3) to analyze each link of the graph and predict the correct semantic relation from the following ones: subclass Of, superclass Of, equivalent, irrelevant, and none. The superclass Of prediction is used to potentially reverse the link direction. Given these results, obtained for each link of the graph, we apply the following steps sequentially: (1) Cut irrelevant links; (2) Resolve reversed links; (3) Reduce transitive links; (4) Merge equivalent classes; (5) Rewire links upon confirmation by LLM and (6) Filter out non-informative and rare classes. The evolution of the subgraph corresponding to the paths from city or town (Q7930989) to entity (Q35120) is shown in Figure 1, after each refining step. The details for each step are described below.\nCut. Any link that is predicted as irrelevant or none is cut if the corresponding classes remain connected to the root class entity after the cut, or if the"}, {"title": "3.3 Large Language Models", "content": "For the sake of reproducibility, we use the open-source LLM Mixtral-8x7B-Instruct-v0.1\u2077, with a temperature set to zero to get deterministic results. We formulate a prompt to enable the LLM to generate answers from the context, including class labels and descriptions. Inspired by the chain-of-thought [22], which bridges the reasoning gap between input and answer, we add an explanation part before the answer to ensure careful analysis before predicting the semantic relation. The corresponding prompt is shown below."}, {"title": "4 Evaluation", "content": "We assess the quality of WiKC from intrinsic and extrinsic perspectives. Data, code, prompts, and resources are all available online\u2078.\nIntrinsic evaluation. We verified the inclusion of the 40 upper-level classes of YAGO 4.5\u2079 in WiKC. There are only two exceptions: yago:Gender, which can be represented by the property sex or gender (P21), and schema:Taxon, which is a specific class of the biological domain. Following YAGO 4.5 [19], we evaluate WiKC in terms of three key criteria: complexity, conciseness, and understand-ability (fraction of classes having labels and descriptions). The statistics are given in Table 1. As expected, WiKC is much simpler and much more concise than Wikidata taxonomy. Compared to WiKC, Wikidata taxonomy has a factor higher than 200 in the number of classes, and a factor higher than 10 in the average number of paths from an instance to the root class entity (Q35120).\nExtrinsic evaluation. We further evaluate WiKC on a task of entity typing, i.e., the prediction of the classes of an entity. This is a crucial task for various downstream tasks, like entity alignment [13] or entity linking [12]. Our evaluation includes the direct classes of each instance as well as their ancestors in the taxonomy, in order to assess the inconsistency of some taxonomic paths.\nWe collect instances from the Wikidata dump based on the instance Of (P31) or occupation (P106) relations, excluding scholarly articles (described in Section \u00a73.1), ensuring each instance has a label, a description, and an English Wikipedia page (resulting in 7M instances). We retype these instances using WiKC by assigning instances to their nearest classes in the taxonomy. To avoid class distribution imbalance (e.g., the class person can have 2.6M cumulative instances), we limit each class to 1000 instances and randomly sample 100k samples overall, resulting in nearly 1M type statements per taxonomy. We design a judge LLM\u00b9\u2070 to verify the accuracy of type statements based on the context provided by an instance. For example, given the context: *Paris* is described as the capital of France, the LLM judges if the statement *Paris* is a [city or town], which means 'large human settlement' is True or False. In this case, the class within brackets can be any ancestor of city (Q515), the direct class of Paris (Q90).\nTable 2 demonstrates the accuracy of entity typing across different depths of the taxonomy on Wikidata and WiKC, where depth refers to the shortest distance from the root class entity. The results show that WiKC consistently outperforms Wikidata across all depth ranges. WiKC shows significant accuracy gains at deeper levels (depth 10 or more), suggesting that WiKC has resolved many inconsistency issues in the lower levels of the Wikidata taxonomy. The fact that accuracy is higher at a deeper level (depth 5 or more) compared to a shallow level on WiKC can be explained by the fact that more specific types are easier for LLMs to judge. For example, it is easier to classify Motokazu Mori (Q75688679) as a poet (Q49757) (depth 9) than as a corporate body (Q106668099) (depth 2).\nDiscussion. We here discuss some limits of our work.\n(1) Problems with the LLM. The LLM might hallucinate by producing re-sponses in conflict with the input prompt. For example, when checking the link"}, {"title": "5 Conclusion", "content": "In this paper, we propose WiKC, a cleaned version of Wikidata taxonomy, generated by an automated process combining zero-shot prompting on an open-source LLM and graph mining approaches. The objective is to address several known limits of Wikidata taxonomy, such as inaccurate taxonomic paths, redundancy across classes, complexity, and ambiguity between instances and classes. Our approach consists of cutting irrelevant links, resolving reversed links, reducing transitive links, merging equivalent classes, rewiring links upon reconfirmation of LLM, and filtering out non-informative or rare classes. The experimental results show the improved accuracy and conciseness of WiKC compared to the original taxonomy of Wikidata. In addition, we provide a mapping file from WiKC to Wikidata, encouraging the reuse of WiKC in various downstream tasks, such as entity recognition [10], entity linking [5] and entity summarization [9], to further validate its reliability and its coverage of general knowledge.\nFor future work, we consider directions for exploring other open-source LLMs to clean and evaluate taxonomies based on our proposed pipeline, and investigating the trustworthiness of these LLMs in the taxonomy refinement task. It is also valuable to share this approach with the Wikidata community to further check its feasibility and help alleviate the burden of manual taxonomy cleaning\u00b9\u00b9."}]}