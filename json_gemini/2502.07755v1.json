{"title": "An Advanced NLP Framework for Automated Medical Diagnosis with DeBERTa and Dynamic Contextual Positional Gating", "authors": ["Mohammad Ali Labbaf Khaniki", "Sahabeh Saadati", "Mohammad Manthouri"], "abstract": "This paper presents a novel Natural Language Processing (NLP) framework for enhancing medical diagnosis through the integration of advanced techniques in data augmentation, feature extraction, and classification. The proposed approach employs back-translation to generate diverse paraphrased datasets, improving robustness and mitigating overfitting in classification tasks. Leveraging Decoding-enhanced BERT with Disentangled Attention (DeBERTa) with Dynamic Contextual Positional Gating (DCPG), the model captures fine-grained contextual and positional relationships, dynamically adjusting the influence of positional information based on semantic context to produce high-quality text embeddings. For classification, an Attention-Based Feedforward Neural Network (ABFNN) is utilized, effectively focusing on the most relevant features to improve decision-making accuracy. Applied to the classification of symptoms, clinical notes, and other medical texts, this architecture demonstrates its ability to address the complexities of medical data. The combination of data augmentation, contextual embedding generation, and advanced classification mechanisms offers a robust and accurate diagnostic tool, with potential applications in automated medical diagnosis and clinical decision support. This method demonstrates the effectiveness of the proposed NLP framework for medical diagnosis, achieving remarkable results with an accuracy of 99.78%, recall of 99.72%, precision of 99.79%, and an F1-score of 99.75%. These metrics not only underscore the model's robust performance in classifying medical texts with exceptional precision and reliability but also highlight its superiority over existing methods, making it a highly promising tool for automated diagnostic systems.", "sections": [{"title": "1. Introduction", "content": "Accurate and timely medical diagnosis is essential for effective healthcare, directly impacting patient outcomes and treatment quality. Delays or errors in diagnosis, especially for critical conditions like sepsis, cancer, or myocardial infarction, can worsen prognoses and lead to unnecessary treatments or fatalities. Traditional diagnostic methods remain labor-intensive, reliant on clinician expertise, and subject to variability due to subjective interpretations, cognitive biases, and human error [1]. Additionally, challenges such as time-consuming processes, resource limitations, and misinterpretations contribute to diagnostic inefficiencies. These issues are further exacerbated in regions with limited healthcare resources, highlighting the need for more reliable and efficient diagnostic solutions [2]. Recent advancements in Natural Language Processing (NLP) and deep learning have opened new possibilities for automating medical diagnosis, overcoming the limitations of traditional methods. Al-powered systems enhance scalability and accessibility, providing diagnostic support in underserved areas through mobile apps and telemedicine platforms. These models also help reduce diagnostic disparities by incorporating diverse data and serve as decision-support tools for clinicians, improving accuracy and reducing cognitive load without replacing human expertise [3].\nThe emergence of transformer architectures and attention mechanisms has revolutionized the field of NLP, offering significant improvements in model performance across a range of tasks. Transformers, introduced by Vaswani et al. (2017), leverage self-attention mechanisms to process input sequences in parallel rather than sequentially, enabling more efficient learning of long-range dependencies [4]. This parallelism not only accelerates training but also enhances the model's ability to capture complex relationships in data, overcoming the limitations of traditional sequential models such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory"}, {"title": "(LSTM) networks.", "content": "The scalability of transformers also allows them to handle large datasets more effectively, making them suitable for a wide array of real-world applications, including large-scale language models and complex multi-task learning scenarios [5].\nThe attention mechanism, a core component of transformers, allows models to weigh the importance of different tokens in a sequence when making predictions. This capability enables more nuanced understanding and contextualization of language, as the model can focus on relevant parts of the input while disregarding less important information. Specifically, self-attention computes a weighted sum of all token representations in a given sequence, dynamically adjusting the focus on each token based on its relevance to other tokens. This mechanism is particularly effective in handling variable-length sequences, capturing intricate dependencies, and improving interpretability by allowing models to highlight which tokens influence their predictions [6]. The integration of transformers with attention mechanisms has led to substantial advancements in tasks such as machine translation, sentiment analysis, question answering, and text classification, making them the foundation for state-of-the-art NLP models such as BERT, GPT, and DeBERTa [7], [8].\nData augmentation plays a crucial role in sentiment analysis and text classification by enhancing the diversity and volume of training data, which helps improve model generalization and performance. In these tasks, the availability of labeled data is often limited, and augmenting the dataset through techniques such as paraphrasing, back-translation, and noise injection allows models to learn from a broader range of linguistic variations [9]. By simulating different ways in which sentiments or categories can be expressed, data augmentation mitigates the risk of overfitting and enables the model to become more robust in handling unseen or rare expressions. This process is particularly important for dealing with imbalanced datasets, where it can help"}, {"title": "prevent bias", "content": "toward the majority class and ensure that the model is equally sensitive to all categories or sentiments. Ultimately, data augmentation is essential for building high-performing text classification models, especially in real-world applications where data diversity is critical. In medical diagnosis, data augmentation techniques such as back-translation and paraphrasing are essential for increasing the diversity of clinical text data, enabling models to better generalize across varied symptom descriptions and improving diagnostic accuracy [10].\nThis paper introduces a novel approach for medical diagnosis using a combination of advanced techniques in data augmentation, feature extraction, and classification. The methodology can be summarized as follows:\n1. Back-Translation for Data Augmentation:\nThe model utilizes a back-translation technique to augment data diversity. This process involves translating text from English to German and subsequently re-translating it back to English, thereby generating paraphrased versions of the original data. This approach aids in mitigating overfitting and enhances the robustness of the classifier.\n2. DeBERTa with Dynamic Contextual Positional Gating (DCPG):\nThe DeBERTa model serves as the backbone for feature extraction, leveraging its disentangled attention mechanism and enhanced decoding to capture fine-grained contextual and positional relationships in the input text. This allows the model to generate high-quality embeddings that are crucial for downstream classification tasks. Building on DeBERTa's foundation, the extended version, DeBERTa with DCPG, introduces a novel gating mechanism that adaptively adjusts the interaction between content and positional information based on the semantic context. Unlike the original DeBERTa, which relies on"}, {"title": "fixed interactions", "content": "to separate content and position, DCPG dynamically modulates the positional influence in a context-aware manner. This enhancement improves the model's flexibility and allows the attention mechanism to focus more effectively on relevant features. By combining relative positional embeddings with dynamic gating, DCPG achieves a balanced integration of positional and semantic information, making it highly effective for tasks that demand a detailed understanding of both content and structure.\n3. Attention-Based Feedforward Neural Network (ABFNN) for Classification:\nThe ABFNN serves as the classifier within the proposed pipeline. This innovative architecture integrates attention mechanisms with a feedforward neural network, enabling the model to selectively focus on the most pertinent features of the input data. By effectively leveraging the hierarchical representations provided by DeBERTa, the ABFNN enhances the overall classification accuracy.\nThis architecture is applied to medical diagnosis by classifying symptoms, clinical notes, and related data into predefined categories or conditions. The back-translation augmentation enhances data resilience, while DeBERTa generates robust text representations. The ABFNN component ensures precise decision-making. Collectively, these elements address the complexities inherent in medical text data, providing a robust and accurate diagnostic tool. Comparative results demonstrate the superiority of the proposed method over Medical Concept Normalization\u2014Bidirectional Encoder Representations from Transformers (MCN-BERT) [11]."}, {"title": "2. Background and Related Work", "content": "This section provides an overview of the medical diagnosis dataset, detailing its structure, features, and relevance to the task of disease classification. Additionally, a comprehensive"}, {"title": "literature review is", "content": "presented, summarizing key studies and methodologies in the field of medical diagnosis using NLP techniques.\n2.1. Medical Diagnosis Dataset Overview\nThe Symptom2disease dataset is a comprehensive collection of symptom-disease associations designed for medical text analysis and disease prediction tasks. It comprises 1200 datapoints, each containing two key elements: a disease label and a corresponding natural language description of symptoms. The dataset covers 24 distinct diseases, with each disease represented by 50 unique symptom descriptions. This balanced distribution results in a total of 1200 entries. The diseases included in the dataset span a wide range of medical conditions, including dermatological disorders (e.g., Psoriasis, Acne), infectious diseases (e.g., Typhoid, Chicken pox, Dengue), respiratory conditions (e.g., Common Cold, Pneumonia, Bronchial Asthma), and chronic illnesses (e.g., Hypertension, Diabetes) [11].\n\u2022 label: This column contains the disease labels, representing the 24 different medical conditions.\n\u2022 text: This column includes natural language descriptions of symptoms associated with each disease label.\nThe Symptom2disease dataset is publicly available on Kaggle in https://www.kaggle.com/datasets/niyarrbarman/symptom2disease/, making it accessible to researchers and data scientists interested in exploring the relationship between symptoms and diseases in a structured format.\n2.2. Literature review"}, {"title": "Machine learning", "content": "has found widespread applications across various domains, including healthcare, finance, and autonomous systems, where it enables data-driven decision-making and automation. Esmaeili et al.'s study examines recent advancements in AI-driven customer service technologies, focusing on chatbots and virtual assistants that provide personalized support, analyze consumer data for predictive insights, and enhance overall customer experiences [12]. Li et al. developed and evaluated machine learning models to predict hospital mortality in mechanically ventilated ICU patients, using 32 selected features from the MIMIC-III database, with the CatBoost model achieving the highest performance [13]. Sepanloo et al. propose a multi-sensor augmented reality system for nursing education that integrates various sensors with AR technology to provide precise analysis and feedback during training simulations, enhancing the learning experience for nursing students [14]. Khodayari Gharanchaei's study compares various machine learning methods for predicting credit card defaults using a dataset of 30,000 Taiwanese clients, evaluating different approaches to classify defaulters and non-defaulters while considering the impact of including or excluding borderline cases [15]. This study by Mohammadjafari et al. explores using eye tracking to detect cognitive load in complex virtual reality training environments, employing machine learning models to predict cognitive load from eye movement data and demonstrating the potential for developing adaptive VR training systems [16]. This study proposes a novel approach for safe robot navigation in unmapped environments using composite control barrier functions, addressing both state and input constraints while leveraging real-time perception feedback to construct local CBFs, enabling optimal control generation for obstacle avoidance and constraint satisfaction [17].\nDeep learning has revolutionized numerous fields, such as image recognition, natural language processing, and speech synthesis, by enabling highly accurate models through neural networks"}, {"title": "with multiple layers.", "content": "Shafee et al. evaluate the performance of various LLM chatbots in OSINT-based cybersecurity tasks, finding that while some models like GPT-4 show promise in binary classification, all tested chatbots have limitations in named entity recognition compared to specialized models [18]. Benchari and Totaro's study presents an integrated U-Net and ResNet50 architecture for MRI brain cancer image detection, combining the strengths of both models to improve segmentation and classification performance in identifying brain tumors from MRI scans [19]. Mohammadjafari et al. provide a comprehensive review of LLM-based text-to-SQL systems, tracing their evolution from early rule-based models to advanced approaches using large language models and retrieval augmented generation (RAG), while examining key aspects like benchmarks, evaluation methods, and challenges in the field [20]. Merikhipour et al. propose a novel approach for transportation mode detection that combines spatial attention-based transductive LSTM with off-policy feature selection, enhancing the model's ability to capture spatial-temporal dependencies and select relevant features from GPS trajectories [21].\nNLP and LLMs are revolutionizing healthcare by enhancing various aspects of patient care, clinical workflows, and medical research. These AI technologies are being applied to analyze vast amounts of medical literature, clinical records, and scientific papers, providing valuable insights to aid in accurate diagnoses, treatment planning, and informed decision-making. This study compares the performance of WE-LSTM networks and a WizardLM-powered chatbot called DiabeTalk for diagnosing diabetes from medical texts, likely evaluating their accuracy, efficiency, and potential applications in clinical settings [22]. This study proposes a Graph Attention Network (GAT) model combined with curriculum learning for classifying depression from online media, utilizing graph-based techniques to handle increasingly challenging training samples and improve mental healthcare classification performance [23]. This study proposes a medical specialty"}, {"title": "prediction model using", "content": "a domain-specific pre-trained BERT to analyze patient-side medical question texts. The researchers fine-tuned the model to predict medical specialty labels from a dataset of medical questions, demonstrating improved performance compared to competitive models and showcasing its potential benefits for hospital patient management and specialty recommend [24]. This study explores using the BERT language model for multi-criteria classification of scientific articles, specifically employing SciBERT to classify MEDLINE abstracts based on multiple selection criteria. The researchers compared different ensemble architectures, including a novel cascade ensemble, to a single integrated model, finding that the cascade ensemble achieved higher precision and F-measure, while the single model performed better for tasks requiring high fixed recall like systematic reviews [25]. This study proposes a role-distinguishing BERT model for medical dialogue systems in smart cities, aiming to improve the accuracy of intent recognition by differentiating between patient and doctor roles in conversations [26]. This study proposes a role-distinguishing BERT model for medical dialogue systems in smart cities, aiming to improve the accuracy of intent recognition by differentiating between patient and doctor roles in conversations, contributing to the development of sustainable smart city healthcare services [27]. This study explores the application of BERT-based models for biomedical natural language inference on clinical trials, likely aiming to improve the accuracy and efficiency of processing and understanding clinical trial reports and related medical texts [28]. This study proposes DeBERTa-BiLSTM, a deep learning model combining DeBERTa and BiLSTM networks for multi-label classification of Arabic COVID-19 questions. The model demonstrates promising performance in categorizing medical questions into multiple categories, achieving high accuracy and precision, which could significantly benefit telehealth services and automated medical question-answering systems [29]. This study compares the performance of RoBERTa,"}, {"title": "CNN, and ChatGPT models", "content": "in automatically detecting unexpected findings in radiology reports, demonstrating that a fine-tuned RoBERTa model outperforms both CNN and ChatGPT for this specific task [30]. This study proposes a novel cross-attention approach called PSAT that incorporates clinical practice guidelines for depression into transformer models, aiming to enhance explainability and generate clinician-understandable explanations for depression diagnosis from unstructured text data [31]. This study proposes a novel automatic medical report generation model that combines a Detector Attention Module to fuse visual and location features with a GPT-based Word LSTM for improved report generation from medical images [32].\n3. Methodology\nIn this section, we outline the methodologies employed in the proposed framework for enhancing medical diagnosis using NLP. The approach integrates three key techniques: back-translation for data augmentation, DeBERTa with DCPG for feature extraction, and an ABFNN for classification. Each of these components plays a crucial role in improving the model's accuracy, robustness, and contextual understanding in the medical domain."}, {"title": "3.1. Back-Translation for Data Augmentation", "content": "To enhance the generalizability and robustness of the proposed model, a back-translation technique was employed for data augmentation. This approach involves translating input text into an intermediate language and subsequently translating it back into the original language. By rephrasing the text through the translation pipeline, back-translation introduces lexical diversity into the dataset, enriching it with variations in word choice and sentence structure. Importantly, this technique ensures semantic preservation, maintaining the original meaning of the text-a"}, {"title": "critical factor for tasks", "content": "such as medical diagnosis. The added variability in the dataset reduces the model's reliance on specific patterns in the training data, thereby mitigating overfitting and improving its ability to generalize effectively to unseen data.\nThe back-translation process begins with an initial translation of the input text from English into German using the MarianMT model (Helsinki-NLP/opus-mt-en-de) [33]. This neural machine translation system, built on the Transformer architecture, has been fine-tuned on extensive parallel corpora from the OPUS project. By leveraging diverse open-domain texts, the model achieves high-quality translations suited for general-purpose applications. Subsequently, the German-translated text undergoes reverse translation back into English using the complementary MarianMT model (Helsinki-NLP/opus-mt-de-en). This second translation pass ensures further diversification of the text while preserving its semantic integrity, enriching the dataset with varied lexical and syntactic structures [34].\n3.2. DeBERTa with DCPG\nDisentangled attention in BERT refers to separating content and positional information within the attention mechanism to improve how the model understands relationships between tokens. Instead of combining token embeddings with positional embeddings directly, as in standard transformers, disentangled attention treats semantic content and relative positional information independently, enabling the model to learn distinct interactions for content-to-content, content-to-position, and position-to-content relationships. This separation enhances the model's flexibility in handling tasks where positional dependencies vary in importance, such as long-sequence understanding or tasks with complex contextual relationships. By explicitly modeling these interactions, disentangled attention provides a more nuanced representation of token relationships,"}, {"title": "improving performance on", "content": "a variety of NLP tasks."}, {"title": "3.2.1. Input Representation", "content": "Let an input sequence contain n tokens. The model utilizes two embedding matrices to represent semantic content and relative positional relationships:\n\u2022 Content Embeddings:\nC = [C1, C2, ..., Cn ]T \u2208 R(nxd) (1)\nwhere c\u2081 \u2208 R(d) represents the semantic content of the i-th token. The content embeddings matrix C encodes the semantic meaning of tokens in the input sequence. These embeddings are typically obtained from a token embedding layer that maps vocabulary tokens into d dimensional dense vectors\n\u2022 Relative Positional Embeddings:\nP = [P1, P2, \u2026\u2026\u2026, Pn ]T \u2208 R(kmaxxd) (2)\nwhere Pli\u2212j| \u2208 R(d) encodes the clipped relative distance |i - j| between tokens i and j, with kmax denoting the maximum allowed distance. To ensure manageable computational complexity and memory usage, the relative distance |i - j| is clipped to a maximum allowable distancekmax, meaning all distances greater than kmax are treated as kmax. This clipping mechanism balances model expressiveness with computational efficiency while enabling the model to capture meaningful relative positional relationships."}, {"title": "3.2.2. Query and Key Projections", "content": "The model learns separate projection matrices to compute queries (Q) and keys (K):\n\u2022 Content-Specific Projections:"}, {"title": "The content embeddings", "content": "matrix C is used to compute content-based queries (Qc) and keys (Kc) through linear projections:\nQc = CW, Kc = CW\u00a3 (3)\nwhere WC, W\u2208 R(dxd) are learnable weight matrices of shape. These projections enable the model to extract semantic relationships between tokens in the sequence.\n\u2022 Position-Specific Projections:\nThe same content embeddings matrix C is also used to compute position-based queries (Qp) and keys (Kp) through separate projection matrices:\nQp = CW, Kp = CW\nP (4)\nwhere W, W\u2208 R(d\u00d7d) are independent trainable weight matrices. These projections allow the model to focus on relative positional relationships between tokens, leveraging the positional embedding information indirectly."}, {"title": "3.2.3. Disentangled Attention Components", "content": "In disentangled attention mechanisms, the attention score for tokens i and j is decomposed into three distinct components, each capturing a specific interaction type between the content and positional information of the tokens:\n\u2022 Content-to-Content Interaction A\nThis component represents the semantic relationships between the content of tokens i and j. The attention score is computed as:"}, {"title": "A(cc)", "content": "i,j\n=\nQc\u2081 Kcj\nva (5)\nHere, Qc\u2081 and Kc; are the content-based query and key projections for tokens i and j, respectively.\nThis term measures the alignment between their semantic content. capturing semantic relationships between tokens.\n\u2022 Content-to-Position Interaction A\nThis component models how the semantic content of token iii relates to the positional embedding of token j. The score is given by:\nA(cp)\ni,j\n=\nQc\u2081 Kpi-j\n\u221aa (6)\nHere, Qc\u2081 is the content-based query for token i, and Kp\u2081i-j| is the key projection for the relative positional embedding |i \u2013 j]. This term captures how a token's semantic content interacts with the positional context of another token.\n\u2022 Position-to-Content Interaction A\nThis component models how the positional embedding of token i interacts with the semantic content of token j. The attention score is computed as:\nA(pc)\n=\nQPi-jKcj\n\u221aa (7)\nHere, QPi-jl is the query projection for the relative positional embedding |i - j], and K content-based key for token j. This term captures how positional information influences the understanding of semantic content. These three components collectively disentangle the roles of semantic content and positional information in the attention mechanism. By explicitly separating"}, {"title": "semantic content and positional", "content": "interactions, the model gains finer control over how it interprets token relationships, leading to better performance and interpretability in tasks where both semantic and positional relationships are crucial."}, {"title": "3.2.4. Dynamic Contextual Gating Mechanism", "content": "To dynamically modulate positional influence based on contextual relevance, we introduce a learnable gating scalar.\n\u2022 Gating Scaler:\nThe gating scalar a\u2081\u2081j is a learnable value that modulates the influence of positional relationships between tokens i and j based on their semantic context. It is computed as:\nai,j = 0 (QcWgKP-1) (8)\nWhere Wg \u2208 R(dxd) is trainable matrix, and \u03c3(.) is the sigmoid function, ensuring the gating scalar ai,j \u2208 [0,1]. This formula dynamically adjusts the influence of positional relationships based on the semantic context, making the model more flexible in capturing complex token dependencies.\n\u2022 Gated Content-to-Position Interaction:\nThe Gated Content-to-Position Interaction is the process where the content-to-position interaction matrix Acp) is modulated by the gating scalar a\u2081\u2081j. It is defined as:\nAi,j\n=\nAi,j\nA(cp) = dij. A(cp) (9)"}, {"title": "This operation allows the", "content": "model to adaptively adjust the strength of token interactions based on their contextual meaning and positional distance, making the model more flexible and context-aware."}, {"title": "3.2.5. Total Attention Score and Output", "content": "The final attention score combines the three components with gradient-stabilizing scaling:\nAi,j =\nA(cc) + A(cp)\n+ A\n\u221a3d (10)\nTo convert the raw attention scores into probability-like values, softmax normalization is applied across all tokens for each i. This step ensures that the sum of the attention weights for each token equals 1, making them interpretable as probabilities\nAttention Weightsi,j = softmax(Ai,j) (11)\nThe softmax function transforms the attention scores into normalized attention weights that highlight the relative importance of each token when computing the final output. The attention output for token i is calculated as the weighted sum of value projections for all tokens:\nn\nOutputi = \u2211Attention Weightsi,j. Vj (12)\nj=1\nWhere Vj = C;Wy represents the value projection of token j, and Wy \u2208 R(dxd) is a learnable weight matrix.\nDeBERTa's with DCPG significantly enhances the capabilities of DeBERTa's disentangled attention by introducing a dynamic mechanism that adaptively modulates the influence of positional interactions based on semantic context. Unlike DeBERTa, which statically weights"}, {"title": "content-to-position (C\u2192P) and", "content": "position-to-content (P\u2192C) interactions uniformly across all token pairs, DCPG employs a learnable gating scalar ai,j, conditioned on the semantic embeddings of tokens i and j. This gate dynamically suppresses or amplifies positional dependencies when semantic context dominates (e.g., resolving lexical ambiguities like \"bank\" near \"river\") or when syntactic structure is critical (e.g., clause boundaries), respectively. By integrating this context-aware flexibility, DCPG achieves finer-grained control over disentangled interactions, reducing over-reliance on positional biases and improving performance on tasks requiring nuanced semantic reasoning, such as coreference resolution or topic modeling. The addition of a lightweight gate matrix Wg ensures computational efficiency while enabling the model to outperform DeBERTa in adaptability and accuracy across diverse linguistic scenarios."}, {"title": "3.3. ABFNN", "content": "The ABFNN is a novel neural architecture designed to enhance feature integration and classification performance through the combination of hierarchical feature extraction, multi-head self-attention, and multi-scale integration mechanisms. The ABFNN incorporates multiple processing branches to extract diverse representations from input features, followed by a multi-head self-attention mechanism that dynamically identifies and emphasizes important features. To further refine the representations, the architecture integrates residual connections, promoting efficient learning and mitigating gradient vanishing issues in deeper layers. A multi-scale feature integration module combines attention-enhanced and hierarchical features to capture both global and local information effectively. By leveraging attention mechanisms and hierarchical processing, ABFNN achieves improved performance and interpretability compared to traditional feedforward networks. The key components of ABFNN are:\n1. Hierarchical Feature Extraction: Two branches extract diverse features:\nhbranch\u2081 = ReLU(W\u2081x + b\u2081), hbranch\u2082 = ReLU(W2x + b2) (13)\nhcombined = hbranch\u2082 + hbranch\u2081 (14)\n2. Self-Attention Mechanism: Focuses on important features:\nAttention(Q, K, V) = Softmax\n\u221ad (15)"}, {"title": "with Q, K, V = Wqhcombined,Wxhcombined,", "content": "Wvhcombined\n3. Residual Connection and Multi-Scale Integration:\nhresidual = ReLU(W,Attention + br) + Attention (16)\nhmulti-scale = ReLU(Wm[hresidual, hcombined] + bm) (17)\n4. Output Layer:\ny = softmax(Wouthmulti-scale + bout) (18)\nThis design enhances feature representation and classification, making it suitable for complex datasets, such as outputs from models like BERT."}, {"title": "3.4. The proposed Medical Diagnosis Method", "content": "This subsection provides an overview of the proposed method, which consists of five key stages:\n1. Symptom Descriptions: Patients' symptoms are provided in textual form, serving as the primary input for the diagnosis model."}, {"title": "2. Back-Translation for Data", "content": "Augmentation: To enhance the diversity and robustness of the dataset, the input texts are translated to German and back to English using the MarianMT model. This introduces lexical variety while preserving semantic meaning, improving the model's ability to generalize.\n3. Data Collection and Preprocessing: The collected symptom descriptions are cleaned, tokenized, and prepared for input into the model, ensuring high-quality and standardized data for training.\n4. DeBERTa with DCPG: The input data is processed using the DeBERTa model, which employs disentangled attention and positional gating to capture contextual relationships and subtle nuances in the text for accurate feature representation.\n5. ABFNN: The output embeddings from DeBERTa are fed into the ABFNN, which uses an integrated attention mechanism to focus on the most relevant features and improve classification accuracy for medical diagnosis."}, {"title": "4. Simulations", "content": "This section presents a comprehensive evaluation of the performance of several deep learning models, including the BERT, DeBERTa, and the methods which is presented in [11]. we utilized the model with the following hyper parameters:\n\u2022 Number of Layers: 12 layers in the transformer encoder."}, {"title": "Hidden Size:", "content": "The hidden size of each layer was set to 768, determining the dimensionality of the token embeddings.\n\u2022 Number of Attention Heads: The model employed 12 attention heads in each self-attention layer, allowing it to focus on different aspects of the input sequence simultaneously.\n\u2022 Feedforward Size: The intermediate size in the feedforward layers was set to 1024, providing the network with sufficient capacity to learn complex transformations of the hidden states.\nAdditionally, all the feedforward layers in the model employed the ReLU activation function, which introduces non-linearity and enables the network to capture more complex patterns in the data. This configuration was chosen to strike a balance between computational efficiency and model performance, leveraging the strengths of the transformer architecture while ensuring sufficient expressiveness for the task at hand."}, {"title": "4.1. Accuracy", "content": "This subsection utilizes accuracy as a key evaluation metric to measure the performance of the models. The formula for accuracy is expressed as follows:\nAccuracy =\nTP + TN\nTP + TN + FP + FN (17)\nIn the context of binary classification, True Positives (TP) represent the count of positive instances that are correctly identified as positive, while True Negatives (TN) denote the count of negative instances accurately classified as negative. Conversely, False Positives (FP) refer to negative"}, {"title": "This high accuracy stems", "content": "from the synergy between advanced techniques addressing different challenges in medical diagnosis. Back-Translation enhances data diversity by paraphrasing symptom descriptions, improving model generalization and reducing overfitting. DeBERTa, combined with DCPG, disentangles content and positional information, dynamically adjusting their influence to better interpret complex symptom contexts. Meanwhile, ABFNN employs attention mechanisms to prioritize diagnostically significant features, ensuring accurate predictions. Together, these components form a robust framework that handles variability, context, and feature importance, achieving better accuracy compared to MCN-BERT and other models [11]."}, {"title": "4.2. Confusion Matrix", "content": "The confusion matrix in illustrates the classification performance of the proposed DeBERTa with DCPG model on a medical diagnosis task."}, {"title": "skin lead to errors.", "content": "Despite these, the high counts along the diagonal underscore the model's robustness and accuracy for most conditions.\nKey strengths of the model include its dynamic positional adaptation, which adjusts to time-sensitive symptom contexts (e.g., \"fever for 5 days\"), and contextual precision, enabling differentiation of semantically and positionally complex diseases (e.g., chronic Arthritis vs. acute Dimorphic Hemorrhoids). However, limitations emerge in conditions with sparse training data, such as Cervical Spondylosis, and diseases with symptom overlap, like Typhoid vs. Urinary Tract Infection, pointing to opportunities for improvement. Integrating richer clinical data (e.g., laboratory results) or refining feature extraction techniques could further enhance performance. Overall, the DeBERTa with DCPG model validates its 99.8% accuracy, demonstrating its capacity to advance automated medical diagnostic systems with precision and adaptability."}, {"title": "4.3. Attention Map", "content": "In this subsection, we examine the explainability of the proposed method using an attention heatmap. For this purpose, the following sentence is utilized, and the corresponding attention heatmap is shown in Figure 5:\n\"I'm currently experiencing a high fever, along with red spots and rashes covering my entire body. This has left me feeling extremely fatigued, with a significant loss of appetite, leading to weakness and a general sense of lethargy. I'm quite concerned about my condition.\""}, {"title": "The attention heatmap in", "content": "Figure 5 demonstrates how the DeBERTa with DCPG model identifies and prioritizes key symptoms from the input text to classify the condition as chickenpox. High attention weights are assigned to phrases like \"high fever\" and \"red spots and rashes covering my entire body,\" as these are hallmark features of chickenpox. The model captures the widespread and vesicular nature of the rash, along with systemic symptoms like \"extreme fatigue\" and \"weakness,\" to differentiate chickenpox from other illnesses. By dynamically suppressing positional bias for generic terms (e.g., \"loss of appetite\") and amplifying attention for time-sensitive descriptors (e.g., \"rash covering my entire body\"), the DCPG mechanism ensures contextually precise symptom prioritization. This disentangled attention approach separates symptom content (e.g., \"rash\") from positional relevance (e.g., \"entire body\"), effectively capturing the progression and severity of chickenpox symptoms.\nThe model's performance aligns with clinical diagnostic guidelines for chickenpox, which include generalized vesicular rash and fever as key criteria. The heatmap further highlights its ability to distinguish chickenpox from other conditions like measles, allergy, or fungal infections by"}, {"title": "focusing on specific symptom", "content": "patterns. For example, it rules out measles due to the absence of respiratory symptoms and eliminates allergy or fungal infections by ignoring localized descriptors like \"swelling\" or \"scaly patches.\" This ability to adaptively emphasize critical symptoms, while maintaining explainability, enhances both diagnostic precision and clinical interpretability, making the DeBERTa-DCPG framework a valuable tool for automated medical diagnosis."}, {"title": "4.4. Quantitative Evaluation of the Classification Methods", "content": "This subsection provides a quantitative evaluation of the classification methods. Recall, Precision, and F1-score are crucial metrics for assessing classification models, particularly when dealing with imbalanced datasets. Recall, also known as sensitivity, measures the model's ability to correctly identify all relevant positive instances, emphasizing its effectiveness in capturing true positives. Precision, on the other hand, focuses on the accuracy of positive predictions, ensuring that the model minimizes false positives. The F1-score combines Recall and Precision into a single metric, offering a balanced evaluation of the model's performance in scenarios where both false positives and false negatives are important. Additionally, the Area Under the Curve (AUC) provides insight into the model's discrimination capability between positive and negative classes. A higher AUC value indicates better classification performance, with 1.0 representing a perfect classifier and 0.5 indicating random guessing. The following metrics represent the performance indices used to evaluate the models, with their corresponding results presented in Table 2.\nFPR =\nFP\nFP + TN (19)\nRecall = TPR =\nTP\nTP + FN (20)"}, {"title": "ROC - AUC", "content": "=\n1\n\u222b TPR(FPR) dFPR (21)\n0\nPrecision =\nTP\nTP + FP (22)\nF1 \u2212 score = 2 Precision \u00d7 Recall (23)\nPrecision + Recall"}, {"title": "The performance metrics highlight", "content": "the superiority of the BT + DeBERTa - DCPG + ABFNN model in medical diagnosis tasks compared to other models. With an accuracy of 99.78%, recall of 99.72%, precision of 99.79%, F1-score of 99.75%, and AUC-ROC of 99.88%, it outperforms all the other approaches, including MCN-BERT + AdamP and BT + DeBERTa + ABFNN. The inclusion of the DCPG mechanism improves the model"}]}