{"title": "Generative Data Mining with Longtail-Guided Diffusion", "authors": ["David S. Hayden", "Mao Ye", "Timur Garipov", "Gregory P. Meyer", "Carl Vondrick", "Zhao Chen", "Yuning Chai", "Eric Wolff", "Siddhartha S. Srinivasa"], "abstract": "It is difficult to anticipate the myriad challenges that a predictive model will encounter once deployed. Common practice entails a reactive, cyclical approach: model deployment, data mining, and retraining. We instead develop a proactive longtail discovery process by imagining additional data during training. In particular, we develop general model-based longtail signals, including a differentiable, single forward pass formulation of epistemic uncertainty that does not impact model parameters or predictive performance but can flag rare or hard inputs. We leverage these signals as guidance to generate additional training data from a latent diffusion model in a process we call Longtail Guidance (LTG). Crucially, we can perform LTG without retraining the diffusion model or the predictive model, and we do not need to expose the predictive model to intermediate diffusion states. Data generated by LTG exhibit semantically meaningful variation, yield significant generalization improvements on image classification benchmarks, and can be analyzed to proactively discover, explain, and address conceptual gaps in a predictive model.", "sections": [{"title": "1. Introduction", "content": "Longtail encounters are common in production ML systems but difficult to anticipate and be robust towards. Recently, the answer has been: add more training data and more compute (Kaplan et al., 2020). However, the time and cost to acquire additional data, especially longtail data, can be prohibitive (e.g. train instances occurs 1 time for every 104 car instances in Berkeley DeepDrive (Yu et al., 2020)). Additional compute can be traded for improved performance; see distillation (Yu et al., 2023b; Gou et al., 2021) and synthetic data generation (Du et al., 2024; Zhang et al., 2023b) for"}, {"title": "2. Model-Based Longtail Signals", "content": "Longtail is a suitcase word (Minsky, 2006) \u2013 it does not have a single definition. Two reasonable definitions include:\n\u2022 Rare: An input is longtail if instances similar to it are rare in training data. This definition naturally captures a data-centric view in which events are longtail if their overall occurrence is rare (independent of any model)."}, {"title": "2.1. Epistemic Head", "content": "Traditional measures of predictive uncertainty, like entropy, do not distinguish what is rare from what is ambiguous or hard. To address longtail scenarios, we would ideally account for both. To do so, we decompose predictive uncertainty into two components: epistemic and aleatoric (Figure 4). For test input x with unknown label y and predictive model parameters & drawn from distribution \u03a6, epistemic uncertainty is defined as (Depeweg et al., 2018),\n\\begin{equation}\n\u0415(\u0443, \u0424) = U(y) \u2013 E\u2084 [U(y | \u00a2)]\n\\end{equation}\nwhere we implicitly condition on x in each term, and where U is an uncertainty measure, such as entropy for discrete y or variance for continuous y. If entropy, Eqn 2 is equivalent to measuring the mutual information between parameter distribution and unknown target y, which informs us about how much knowing about one tells us about the other.\nIn principle, epistemic uncertainty can be computed by sampling from the posterior predictive distribution:\n\\begin{equation}\np(y | x) = \\int p(y | \u0444, \u0445) \u0440(\u0444 | Xtrain, Ytrain) d.\n\\end{equation}\nThe first term is a likelihood for test label y and the second term is a posterior distribution for model parameters 4, conditioned on all previous training data (Xtrain, Ytrain).\nPerforming the inference required to sample from Eqn 3 is intractable for modern neural networks. Nevertheless, it can be approximated. One approach is variational inference, typified by Monte Carlo dropout (Gal & Ghahramani, 2016), where the same input is passed through a network many times (often 50 or more forward passes), with test time variation in each pass enabled by random Bernoulli dropout. Unfortunately, this would be expensive to compute and is not conveniently differentiable due to discrete sampling (Jang et al., 2016). It is also bested in practical terms by a model ensemble, where a small number of models, often 3 - 5, are trained (independently (Lakshminarayanan et al., 2017) or otherwise (Maddox et al., 2019)). However, computing a gradient (which we will need to generate long-tail synthetic data) across K instances of a model introduces substantial memory overhead and compute latency.\nIn Figure 3 (left), we introduce a lightweight ensembling technique called the Epistemic Head, which provides a superior longtail signal with no impact on model performance and negligible changes to model parameter count, training time, and inference time (see Supplement A.6). Inspired by LORA (Hu et al., 2021) and the Hydra architecture (Tran et al., 2020), we duplicate the head of an existing prediction model K times and jointly train with the same loss as the base model but with diversity encouraged through an oracle loss (Guzman-Rivera et al., 2012) that only propagates per-example loss through the best-performing head. Head outputs act as fixed-point samples from the posterior predictive (Eqn 3) under a prior determined by weight initialization and permit differentiable computation of longtail signals in a single forward pass, including E(y, \u0424),\n\\begin{equation}\nf(x) \u2248 U\\left(\\frac{1}{K}\u03a3\\limits_k p(y | \u03a6\u03ba)\\right) \u2248 -\u03a3\\limits_k U(p(y | \u03a6k)).\n\\end{equation}"}, {"title": "3. Longtail Guidance", "content": "To motivate Longtail Guidance, we briefly review diffusion. Diffusion models learn a continuous data distribution Po(xo) = \u222b Po(x0:T)dx1:T from a finite set of data samples by defining a forward noising process q(x1:Tx0) over latent states X1:T, and learning a reverse denoising process Po (xo:T). In the DDPM (Ho et al., 2020) and DDIM (Song et al., 2020a) formulations, the forward process is a Markov chain that iteratively add noise according to schedule 1:T that decreases over steps 1,...,T,\n\\begin{equation}\nq(xt | Xt-1) = N(\\sqrt{at}xt\u22121, (1 - at)I)\n\\end{equation}\nHowever, they differ in the learned reverse process: DDPM models it as a Markov chain,\n\\begin{equation}\nPo(Xt |Xt-1) = N(\u03bc\u0473(xt,t), \u03a3\u00f8(xt,t))\n\\end{equation}\nwhere, by reparameterization for numerical stability, the diffusion network e\u0473(xt, t) learns to predict the previously-sampled noise at each step rather than the process mean. In DDIM, the reverse process is instead modeled as non-Markovian. At each step, it predicts,\n\\begin{equation}\nXt-1 = \\sqrt{at-1}\\widehat{x}(t) + vt + otet\n\\end{equation}\nfor terminal state estimate (t), directional vector vt pointing towards xt, and sampled noise et ~ N(0, I),\n\\begin{equation}\n\\widehat{x}(t) = a^{-0.5} (xt - \\sqrt{1 - At}e\u0473(xt, t))\n\\end{equation}\n\\begin{equation}\nvt = (1 - A_{t-1} \\sigma_t)^{1/2}\u20ac_0(x, t).\n\\end{equation}\nSDE formulations of diffusion (Song et al., 2020b) generalize DDPM and DDIM. Denoising models can be learned in one framework (DDPM) and sampled in another (DDIM).\nAs stated, diffusion models sampled according to Eqns 6, 7 are unconditional; they will sample instances that are distributed approximately according to the data. This can be changed with guidance. In classifier-free guidance (Ho & Salimans, 2022), the diffusion model is trained on pairs (x, c) for data x and conditioning vector c (for example, class labels or CLIP-encoded text). In contrast, classifier guidance (Dhariwal & Nichol, 2021) can be used during the sampling process even when no conditioning information was available at diffusion training time. It operates by biasing the denoising estimate in the direction of the gradient of a differentiable signal, \u2207 f (xt, t),\n\\begin{equation}\n\u00eat = \u20ac_0(xt, t) \u2013 w\u2207_{xt} f (xt, t)ot.\n\\end{equation}\nCommonly, f (xt, t) = log p(Yi | Xt, t), the log probability of class i under trained classifier f\u00f8r. But this only works if classifier for is trained on intermediate noisy diffusion states 11:T and accepts denoising step t (thus, it trains on triples of (noisy data xt, step t, and label y) instead of a standard formulation of (clean data x and label y)). Training fo' on intermediate diffusion states is required because the distribution of xt differ from the original data x. Classifier-free guidance can be combined with classifier guidance.\nUsing classifier guidance to generate synthetic data in the longtail of an existing production model f$, as defined by model-based longtail signal flf(x) is appealing because it can be applied to existing diffusion models without retraining. However, as stated, classifier guidance presents a dilemma: we must either fine-tune a noise-aware model f'(xt, t) from the original model f(x) on noisy, intermediate diffusion states, at which point for no longer reflects the production model performance of f$, or we must deploy a production model for that wastes capacity on intermediate diffusion states that it will not encounter in production.\nThere is an additional challenge with using classifier guidance for longtail data generation: SOTA diffusion models perform training and sampling in a lower dimensional latent space Z by first encoding the original data using a pretrained VAE, noising (at training) and denoising (at inference) among latent states 20:T, then decoding the final result back to data space X. How can we pass latent zt through production model fo when it operates in data space X?\nWith Longtail Guidance, we find a simple diffusion guidance approach that couples longtail signals from an existing"}, {"title": "4. Experiments", "content": "In Section 4.1, we compare predictive model generalization improvements when training data is augmented with synthetic data generated by Longtail Guidance, when training data is augmented with existing synthetic data generation approaches, and when training data is augmented with traditional data augmentations approaches. In Section 4.2, we reduce synthetic data generated with Longtail Guidance to a set of text descriptions that describe attributes of a predictive model's longtail. We demonstrate that these descriptions are meaningful by showing that they produce higher-value synthetic data than manually prompt-tuned diffusion. In Supplement A.1, we additionally demonstrate that LTG improves the ImageNet-LT generalization performance of SOTA VIT models that compensate for class-imbalance. In Section 4.3, we examine why Longtail Guidance outperforms existing synthetic data generation approaches."}, {"title": "4.1. LTG Improves Predictive Model Generalization", "content": "We compare Longtail Guidance with three measures (Epistemic, Entropy, Energy) to the recent Guided Imagination Framework (GIF) (Zhang et al., 2023b) and Dream the Impossible (Dream-ID) (Du et al., 2024) synthetic data generation approaches. For baselines, we compare to prompt-tuned Stable Diffusion 1.4 (SD), prompt-tuned DALL-E2, and masked autoencoder (MAE) data generation (He et al., 2022). We also compare to data augmentation baselines: Cutout (DeVries, 2017), GridMask (Chen et al., 2020), RandAugment (Cubuk et al., 2020), AutoAugment (Cubuk et al., 2019), CutMix (Yun et al., 2019), AugMix (Hendrycks et al., 2019) and adversarial robustness approaches DeepAugment (Hendrycks et al., 2021), and MEMO (Zhang et al., 2022a)."}, {"title": "4.2. LTG Produces Meaningful Longtail Data", "content": "We examine synthetic data generated by LTG to determine whether they exhibit meaningful variation from baseline synthetic data generated by prompt-tuned diffusion. This is a difficult and fundamentally qualitative task to manually perform at scale. To make it quantitative, we ask: do text descriptions of LTG-generated data lead to predictive model generalization improvements when they are used to generate additional synthetic data (without LTG)?\nStarting with the strongest baseline model f trained on Flowers (Original from Table 1), we ask a VLM (Liu et al., 2024) to caption real training instances Xreal and also synthetic data x1tg generated by LTG as guided by ft. For each synthetic instance, N novel keywords are found by computing the token embeddings that are furthest in cosine"}, {"title": "4.3. Discussion", "content": "Longtail Guidance significantly outperforms leading synthetic data generation baselines including GIF and Dream-ID even though it requires no prompt tuning. To explain this, we note that GIF and Dream-ID reason in latent CLIP space to create new embedding vectors with which to prompt a diffusion model. For each real data instance, GIF creates K additional synthetic instances by jointly optimizing K embedding vectors (initialized by CLIP image encoding) such that they are difficult for CLIP zero-shot classification (high entropy), remain close to the target class (high log prob of target class), and diverse (high KL divergence to a mean embedding). In Dream-ID, a separate model is trained to predict class embedding vectors from (real data, CLIP-based class text embedding vector) pairs. From this, a manifold in CLIP space can be sampled along class boundaries to generate new diffusion conditioning vectors.\nWe posit that synthesizing data that is difficult or rare for a foundation model, like CLIP, is different from synthesizing data that is difficult or rare for a specific, deployed predictive"}, {"title": "5. Related Work", "content": "Diffusion has seen wide success, particularly in image generation, where it outperform GANs in image quality and diversity without suffering from unstable training or mode collapse (Dhariwal & Nichol, 2021). Recent work has seen progress in handling large data dimensions with latent spaces (Chen et al., 2025; Podell et al., 2023; Rombach et al., 2022) or hourglass networks (Crowson et al., 2024), improved sampling (Lu et al., 2022; Karras et al., 2022; Ho et al., 2020; Song et al., 2020a), additional data domains (Ran et al., 2024; Pronovost et al., 2023; Zhong et al., 2023), and personalization (Ruiz et al., 2023; Kumari et al., 2023). Much work has also been done on new forms of guidance (Wallace et al., 2023b; Yu et al., 2023a; Wallace et al., 2023a; Zhang et al., 2023a) beyond just classifier guidance and classifier-free guidance (Dhariwal & Nichol, 2021; Ho & Salimans, 2022). Universal Guidance (Bansal et al., 2023) is most relevant, and is discussed in Section 3.\nSynthetic training data from generative models has been"}, {"title": "Conclusion", "content": "We developed model-based longtail signals that do not impact model weights or performance and are leveraged by Longtail Guidance, a synthetic data generation approach that explicitly conditions on an existing predictive model to generate examples that are rare or hard from that model's perspective. We show that training on LTG-generated data provides stronger generalization improvements than synthetic data generation and data augmentation baselines over eight datasets. We further demonstrate that longtail synthetic data generations can be rendered into meaningful text descriptions that can aid future (real or synthetic) data collection priorities.\nPredictive models are being deployed more than ever before. Increasingly, they will encounter longtail scenarios that human operators cannot predict in advance. Foundation models can be used to mitigate some risk, but we cannot shoehorn the entirety of Internet-scale knowledge into every deployed model due to capacity and compute constraints.\nBy letting an existing predictive model speak for itself, we can expend offline compute to generatively mine high-value synthetic training data. Further reducing these synthetic examples to human- and machine-readable text suggests a future where we can move away from slow, reactive longtail mitigation towards fast, proactive longtail discovery."}, {"title": "A. Appendix", "content": "A.1. LTG Improves Longtail Performance\nIn Table 4, we train SOTA ViT-based models (LiVT) from scratch on ImageNet-LT according to the longtail-compensation approach of (Xu et al., 2023). In summary, training includes MAE pretraining followed by 100 epochs of BCE loss with a logit adjustment to account for class imbalance. Initial LiVT training is on real data only and uses many data augmentations including RandAugment, Mixup, and CutMix. Our baseline predictive model, LiVT (Reproduced), nearly matches the generalization performance (top-1 accuracy) of what is reported in (Xu et al., 2023) (LiVT (Reproduced): 60.6 vs LiVT (Reported): 60.9). We then fine-tune LiVT on 24000 additional synthetic data evenly distributed across all 1000 classes for 100 epochs with le - 4 learning rate. We use the same Stable Diffusion 1.4 baseline and sampling details as in the main paper (LiVT SD), and also compare Longtail Guidance with three types of model-based long-tail signals (LiVT LTG {Energy, Entropy, Epistemic}). Overall generalization improvements on the balanced validation set are mild but significant (+1.3 top-1 accuracy). However, performance for longtail classes (Few) improves by four points for baseline diffusion (+10%) but a marked 10 points for Longtail Guidance (+24%)!\nSynthetic data always improves Medium and Few performance, which are most impacted by a fixed number of data generations per class. This experiment demonstrates that Longtail Guidance works with multiple architectures (ResNet, ViT), multiple losses (CE, logit-adjusted BCE), and that it disproportionately improves longtail performance. It also demonstrates that LTG can be composed with finely-crafted training recipes to further improve predictive model generalization performance.\nA.2. Comparison to GIF and Dream-ID"}, {"title": "A.3. Longtail Introspection", "content": "We construct refined prompts for each class by prompting LLaVA-1.6 7B (Liu et al., 2024) to generate a description for two sets of synthetic images: 100 generated by Longtail Guidance and 100 generated by baseline diffusion. VLM prompts are of the form: This is an image of <Class>. Describe it in detail. For each output VLM description, we compute per-token BERT embeddings (Devlin, 2018). This provides us with two token embedding distributions: longtail and baseline.\nFor each token embedding in the longtail distribution, we compute cosine similarity to the nearest token embedding in the baseline distribution. For each longtail image, we define the K tokens that are furthest from the the baseline distribution as longtail keywords. We retain the (description, keyword) pairs of the P = 40 longtail examples per class whose keyword tokens are furthest from the base distribution.\nFollowing, we create P = 40 refined prompts per class by prompting GPT-40 (OpenAI, 2023) with: <VLM description of the image> The following keywords describe the key features of the description above: <Keyword 1>, <Keyword 2> ... Use a complete sentence to summarize the key features. The sentence should start with: A photo of <Class> that...."}, {"title": "A.5. Can We Continuously Mine for Additional High-Value Synthetic Data?", "content": "In Table 8 we ask: if Longtail Guidance generates high-value training data for predictive model f4, can we continuously iterate between fine-tuning f and generating synthetic data for additional generalization improvements? In this experiment, we generate synthetic data according the original schedules defined in Table 5 for the first 100 epochs. We then continue fine-tuning and generating an additional 1\u00d7 expansion every 5 epochs for a total of 300 epochs and report generalization performance. We observe that in all cases, we can improve performance, but that gains eventually saturate. It remains a question for future work whether higher-capacity generative models could be mined longer periods of time before predictive performance saturates. If so, it suggests an exciting future possibility of continuously exchanging unused offline compute for improved predictive model performance."}, {"title": "A.6. Computational Costs", "content": "EPISTEMIC HEAD\nThe Epistemic Head with K heads has a parameter count equal to K \u00d7 dmodel \u00d7 C, for model embedding dimension dmodel and number of output logits C. For many predictive models, this leads to negligible parameter count increase, as displayed\nA.7. Longtail Signals"}, {"title": "A.8. Why Does Longtail Guidance Work without Training the Predictive Model on Intermediate Diffusion States?", "content": "A key finding and contribution of this work is that an existing predictive model f\u00f8 does not need to be retrained on intermediate, noisy diffusion states to effectively guide diffusion model e\u0189 towards high-value synthetic training examples that are rare or hard from f's perspective. This realization frees us from the dilemma of having to decide whether to waste predictive model capacity training on intermediate diffusion data it will never see in production or risking divergence from the production model by fine-tuning on intermediate noisy states. But why does this work?\nIn Figure 13, we visualize the intermediate, noisy diffusion states of two quantities: the decoded, predicted terminal state D(P(x)) (top row, what LTG uses as guidance input to predictive model f$) and the decoded data state, D(xt) (bottom row, what classifier guidance would traditionally performs guidance if not in a latent space). We observe that the terminal state predictions much more readily resemble natural image data at a much earlier time in the diffusion process (within the first 10% of denoising steps) than do the data states. In fact, decoded data states have off-distribution noise artifacts up through the first 90% of the diffusion denoising steps.\nWe make this quantitative with the Frechet Inception Distance (FID) (Heusel et al., 2017), defined as\n\\begin{equation}\nFID = ||\u03bc\u03b3 - \u03bcg||2 + Tr(\u03a3r + \u03a3g \u2013 2(\u03a3\u03a3\u338f))\n\\end{equation}\nwhere (\u03bc\u03b3, \u03a3\u03b3), (\u03bcg, \u03a3g) are the mean and covariance of Inception-V3's pool3 features for real and synthetic data, respectively. Lower FID indicates that the generated data more closely match and cover the real data. We measure FID for two conditions:\n1. FID between the decoded, predicted terminal state x = D(P(xt)) and real ImageNet-LT training data, and\n2. FID between the decoded data state xt and real ImageNet-LT training data.\nResults are plotted in Figure 15. Observe that the decoded, predicted terminal state D(P(x)) are dramatically closer (in distribution) to real training data than are the naively decoded data states, xt for nearly all denoising iterations.\nFigure 14 demonstrates that, because the terminal state estimates D(P(\u00ee)) are closer real training data, the predictive model is able to effectively guide data generations towards higher longtail signals (model entropy in this case), with clear longtail signal separation between different longtail guidance weights occurring within the first 25% of denoising iterations. In contrast, naively guiding based on data states xt causes the predictive model to be unable to effectively guide data generation until the last 5% of denoising iterations, when there are no longer many degrees of freedom for the diffusion model to meaningfully change image content.\nA.9. Ablations"}, {"title": "A.10. Additional Examples", "content": "In Figure 18, we show high-resolution examples of synthetic data generated by baseline diffusion and by Longtail Guidance. In order, classes are: jellyfish, flamingo, lion, monarch butterfly, ant, candle, jeep, forklift, and ambulance. Guidance is performed by a SOTA ViT-based ImageNet-LT classifier described in Supplement A.1."}]}