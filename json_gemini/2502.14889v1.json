{"title": "NARROWING INFORMATION BOTTLENECK THEORY FOR MULTIMODAL IMAGE-TEXT REPRESENTATIONS INTERPRETABILITY", "authors": ["Zhiyu Zhu", "Zhibo Jin", "Jiayu Zhang", "Nan Yang", "Jiahao Huang", "Jianlong Zhou", "Fang Chen"], "abstract": "The task of identifying multimodal image-text representations has garnered increasing attention, particularly with models such as CLIP (Contrastive Language-Image Pretraining), which demonstrate exceptional performance in learning complex associations between images and text. Despite these advancements, ensuring the interpretability of such models is paramount for their safe deployment in real-world applications, such as healthcare. While numerous interpretability methods have been developed for unimodal tasks, these approaches often fail to transfer effectively to multimodal contexts due to inherent differences in the representation structures. Bottleneck methods, well-established in information theory, have been applied to enhance CLIP's interpretability. However, they are often hindered by strong assumptions or intrinsic randomness. To overcome these challenges, we propose the Narrowing Information Bottleneck Theory, a novel framework that fundamentally redefines the traditional bottleneck approach. This theory is specifically designed to satisfy contemporary attribution axioms, providing a more robust and reliable solution for improving the interpretability of multimodal models. In our experiments, compared to state-of-the-art methods, our approach enhances image interpretability by an average of 9%, text interpretability by an average of 58.83%, and accelerates processing speed by 63.95%. Our code is publicly accessible at https://github.com/LMBTough/NIB.", "sections": [{"title": "1 INTRODUCTION", "content": "CLIP (Contrastive Language-Image Pretraining) has rapidly become a pivotal model in the field of multimodal learning, especially excelling in its ability to connect the visual and textual modalities (Lin et al., 2023). By training on large-scale image-text pairs collected from the internet, CLIP is capable of performing zero-shot classification and image-text retrieval tasks, making it an indispensable component of modern generative artificial intelligence (Novack et al., 2023; Jiang & Ye, 2023). With its strong visual-textual understanding capabilities, CLIP can generate, classify, and explain content without the need for fine-tuning, providing robust support for various generative AI applications. As one of the most representative Multimodal Image-Text Representation (MITR) methods, CLIP's core strength lies in mapping images and texts into a shared embedding space, significantly enhancing the performance of multimodal tasks.\nDespite its outstanding performance in MITR tasks, developing effective interpretability methods to reveal CLIP's decision-making mechanisms has become increasingly important. The black-box nature of CLIP's multimodal embeddings presents significant challenges in high-risk applications such as medical diagnosis and content moderation, where transparency and reliability are crucial (Eslami et al., 2021; Tong et al., 2024; Yuan et al., 2024; Zhu et al., 2024b). A deeper understanding of how CLIP establishes associations between visual and textual representations is essential to ensure the transparency and trustworthiness of its outputs.\nThere have been numerous interpretability methods focused on unimodal tasks Ribeiro et al. (2016); Sundararajan et al. (2017); Zhu et al. (2024a), but these methods are not designed for the unique"}, {"title": "2 RELATED WORK", "content": "characteristics of MITR tasks, resulting in suboptimal performance when directly applied to such tasks. However, there are existing interpretability methods specifically developed for MITR tasks. Despite their development, these methods often suffer from randomness issues, requiring additional sampling or loss information (Wang et al., 2023), which leads to a crisis of trust in the interpretability method itself. These issues will be further analyzed in the related work section.\nGiven that CLIP can generate unique image and text representations without the need for additional samples and can directly establish their correlations, it is possible to design an interpretability algorithm that unveils the mechanisms behind these correlations without requiring extra sampling. M2IB (Wang et al., 2023), based on the Information Bottleneck Principle (IBP), proposes an interpretability method that does not require additional samples. This method controls the amount of feature information through a Bottleneck layer and optimizes the parameters of this layer to maximize the mutual information between the representations and the task target while minimizing the correlation between the representations and the original sample. Although IBP has a solid theoretical foundation in information theory, in practice, its reliance on hyperparameters and random sampling often introduces bias into the interpretation results. We will discuss this issue in detail in Section 3.2.\nTo address the aforementioned challenges, we propose a novel Narrowing Information Bottleneck Theory (NIBT). Through rigorous theoretical derivation, NIBT effectively eliminates the randomness and hyperparameter dependency in IBP, resulting in more deterministic interpretability outcomes. Additionally, we introduce a new concept of negative property, which identifies feature dimensions that negatively impact the model's predictions, further enhancing the model's interpretability. Our Contributions as follows:\n\u2022 We systematically summarize existing MITR interpretability methods and highlight the limitations.\n\u2022 We propose and derive the novel Narrowing Information Bottleneck Theory, which enables interpretation of MITR tasks without randomness, while preserving the advantages of the IBP.\n\u2022 Our research significantly improves the interpretability of the CLIP model, and we release our method as open-source for further research and application."}, {"title": "2.1 CONTRASTIVE LANGUAGE-IMAGE PRETRAINING (CLIP)", "content": "Radford et al. (2021) introduced CLIP, which learns multimodal embeddings of images and text by training image and text encoders on large-scale image-text paired data. This enables CLIP to establish connections between the two modalities within a unified embedding space, facilitating zero-shot transfer, where the model can make predictions based on natural language descriptions without relying on task-specific labeled data. However, the complexity of these multimodal tasks necessitates a focus on interpretability to ensure that the model's decisions are grounded in meaningful features. Studying the interpretability of CLIP helps verify whether the model genuinely understands the relationship between vision and language, as opposed to relying on spurious correlations in the data."}, {"title": "2.2 TRADITIONAL INTERPRETABILITY METHODS", "content": "Traditional interpretability methods for deep learning models were initially designed for unimodal tasks. Early methods, such as Saliency Maps, generate fine-grained heatmaps by computing the gradient of the model's output with respect to input pixels. However, these methods are sensitive to noise and often yield coarse explanations. Grad-CAM (Selvaraju et al., 2017), by computing the gradient of activation maps in convolutional layers, produces class-specific heatmaps, making the explanations more intuitive, particularly for convolutional neural networks (CNNs). RISE (Petsiuk et al., 2018) further advances the field by introducing a black-box method that applies random masks to different regions of the input image, observes changes in the model's output, and generates heatmaps that account for both global and local interpretability. RISE's advantage lies in its model-agnostic nature, making it applicable to any architecture. However, due to its reliance on"}, {"title": "2.3 INTERPRETABILITY METHODS FOR MULTIMODAL TASKS", "content": "Currently, existing interpretability methods for multimodal tasks still exhibit several limitations that require improvement, as shown in Table 1. In the following sections, we will provide a detailed explanation of the causes and effects of these limitations.\nNo Extra Example indicates that no additional samples are required during the interpretation process, which is crucial because in real-world scenarios, we do not know what samples to select, nor can we explain why a particular pair of samples are correlated. For instance, if we aim to explain which parts of an image depict a cat, the image in the CLIP model already exhibits high activation with respect to the text cat. Therefore, we should not need to reference 100 additional images of cats and 100 images without cats. A well-trained model that already understands the semantics should not require such sampling. No Randomness means that the calculation process involves no randomness, as randomness reduces trust in the interpretability method. No Specific Structure means that the method does not depend on a particular model structure. No Info Loss ensures that no information is lost during the interpretation process, such as interpreting only a subset of the model's output. Current Model indicates that the method explains the model as it currently exists, without constructing a new model for interpretation. No Downstream Task means that no downstream tasks are required for the explanation process.\nM2IB (Wang et al., 2023) introduced a multimodal information bottleneck method aimed at explaining the decision-making process of vision-language pre-trained models by compressing task-irrelevant information to highlight key predictive features. However, this approach introduces additional complexity, which will be analyzed further when discussing the IBP theory. Similarly,\nCOCOA (Lin et al., 2022) extended Integrated Gradients (IG) to multimodal tasks by incorporating positive and negative sample pairs in its loss function, but this requires sampling additional relevant examples, introducing extraneous information that may not be directly relevant to explaining the current sample."}, {"title": "3 PRELIMINARY", "content": ""}, {"title": "3.1 PROBLEM DEFINITION", "content": "Following the setup of CLIP (Radford et al., 2021), a trained MITR model can be defined as follows: let $f_\\mathrm{I}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^d$ denote the image encoder, which transforms an input image $x_\\mathrm{I} \\in \\mathbb{R}^n$ into a $d$-dimensional image representation; $f_\\mathrm{T}: \\mathbb{R}^m \\rightarrow \\mathbb{R}^d$ denote the text encoder, which transforms an input text $x_\\mathrm{T} \\in \\mathbb{R}^m$ into a $d$-dimensional text representation. We can use $\\cos (f_\\mathrm{I}(x_\\mathrm{I}), f_\\mathrm{T}(x_\\mathrm{T}))$ to evaluate the matching performance between the visual and textual modalities. Additionally, the representations can be directly applied to downstream tasks (Sanghi et al., 2022; Zhou et al., 2023). In the following, we use $f$ to represent either $f_\\mathrm{I}$ or $f_\\mathrm{T}$. By substituting $f$ with $f_\\mathrm{I}$, we obtain the results associated with the image modality, and similarly, we can derive the results for the text modality.\nFor an $L$-layer neural network, we can decompose it into the concatenation of two neural networks $f_{1:l-1} \\circ f_{l:L}(x)$ at the $l$-th layer. For ease of expression, we use $z = f_{1:l-1}(x)$ to represent the latent feature of the intermediate layer.\nOur goal is to construct an interpretability method $A$ that yields $A(x) \\in \\mathbb{R}^{\\vert z\\vert}$. The larger the value of $A$, the more important that dimension is for the representation."}, {"title": "3.2 THE INFORMATION BOTTLENECK PRINCIPLE", "content": "The information bottleneck principle (Tishby et al., 2000), based on information theory, introduces the bottleneck to control the amount of information passing through it, aiming to find the minimal feature encoding that retains the least amount of information from the original sample while preserving the necessary information for a given task. For ease of reading and understanding, we provide a brief explanation and simplify the notation, with more detailed analysis provided in Appendix A.\nThe goal of the information bottleneck principle is to construct an optimization function and find the optimal parameter $\\lambda$:\n$\\lambda^* = \\max I(\\tilde{z}, Y) - \\beta I(\\tilde{z}, x; \\lambda)$   (1)\nwhere $I(x, Y) = H(x) - H(x|Y) = H(Y) - H(Y|x)$ represents the mutual information between events $x$ and $Y$, which can be interpreted as the reduction in uncertainty about event $x$ after observing event $Y$. Intuitively, the stronger the correlation between the two, the greater the reduction in uncertainty, and thus the larger the mutual information. Here, $x$ represents the input sample, $\\tilde{z}$ represents the encoding of $x$, which can be understood as the extracted features, and $Y$ represents the given task. $\\lambda$ controls the size of the bottleneck. We emphasize the Key Point 1: $\\lambda^*$ represents the value of $\\lambda$ when the mutual information between the encoding and the task is maximized while minimizing the correlation with the original sample (i.e., extracting as few features as possible). The optimization process follows (Schulz et al., 2020)."}, {"title": "4 METHOD", "content": "In this section, we first deconstruct how the Information Bottleneck Principle extracts the importance distribution of sample features and analyze the shortcomings of applying this theory to the"}, {"title": "4.1 ANALYSIS OF THE INFORMATION BOTTLENECK PRINCIPLE (IBP)", "content": "Several works (Wang et al., 2023; Schulz et al., 2020) have applied the IBP to the interpretability of neural networks. Their approach typically involves inserting a Bottleneck layer at the $l$-th layer of the neural network, with $\\lambda \\in \\mathbb{R}^{|z|}$ controlling the amount of information in the $l$-th layer feature $z = f_{1:l-1}(x)$. The optimal solution for $\\lambda$, based on Equation 1, is found through gradient descent iterations, and the control is achieved as follows:\n$\\tilde{z}_{ic}(x) = \\lambda_{ic} \\cdot z_{ic} + (1 - \\lambda_{ic}) \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(\\mu, \\sigma^2)$  (2)\nHere, following the Grad-CAM approach (Selvaraju et al., 2017), the dimension of $z$ is split into two parts for discussion: $i$ represents the spatial encoding, and $c$ represents the channel encoding (for instance, $z \\in \\mathbb{R}^{w \\times h \\times c}$, where $w$, $h$, and $c$ represent width, height, and channels, respectively. The $w \\times h$ portion is simplified as $i$). The noise distribution follows $\\epsilon \\sim \\mathcal{N}(\\mu, \\sigma^2)$, and any distribution independent of $z$ can be used, but a normal distribution is chosen for computational simplicity. The parameters $\\sigma^2$ and $\\mu$ can be arbitrarily specified. When $\\lambda = \\lambda^*$, the importance of $z_i$ is given by $D_{KL}(P(\\tilde{z}_{ic}|x)||\\mathcal{N}(\\mu, \\sigma^2))$. Intuitively, this measures the uncertainty allowed in the $i$-th feature dimension under the condition of the Key Point 1. If this dimension is very close to an independent noise distribution of sample $x$, a small KL divergence implies irrelevance to $x$, i.e., unimportance, and vice versa.\nWhile the IBP theory itself is solid, its application faces several limitations as following:\n1. the optimization process introduces randomness, as the calculation of $I(\\tilde{z}, Y)$ depends on $\\tilde{z}_{ic}(x)$, which involves sampling noise $\\lambda$. This results in numerous local optima in the optimization of $\\lambda_{ic}$, leading to variations between runs.\n2. the hyperparameter $\\beta$ in the optimization objective significantly influences the interpretability results (Wang et al., 2023). $\\beta$ controls the trade-off between the two mutual information terms, allowing different task information to result in entirely different explanations.\n3. the KL divergence is always positive, preventing the explanation from reflecting negative properties (As shown in Figure 1, our proposed method successfully distinguishes and excludes negative properties from the explanation. In Figure 1d, the M2IB method continues to highlight irrelevant negative features, such as the cat's face, even when the subject is a dog. However, in Figure 1b, our method correctly ignores these negative properties, focusing on more relevant, positive features, showcasing its improved attribution performance.)\n4. the explanation results do not directly reflect the association between feature dimensions and $I(z, Y)$ (the explanation is derived by optimizing Equation 1 to obtain $\\lambda$, followed by computation).\nTo address the three above issues, we propose the Narrowing Bottleneck Theory."}, {"title": "4.2 THE NARROWING INFORMATION BOTTLENECK THEORY", "content": "In this section, we introduce three core theorems of the Narrowing Bottleneck Theory and propose our Narrowing Information Bottleneck Method (NIB) algorithm based on them.\nWe continue to introduce a Bottleneck layer at the $l$-th layer and use $\\lambda$ to control the information flow, while the scalar $\\lambda$ serves as a universal update parameter for each layer, the flow of information for each feature dimension is determined independently. More Details of $\\lambda$ please refer to Appendix G\n$\\tilde{z}_{ic}(x) = \\lambda \\cdot z_{ic} + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ (3)\nHowever, unlike Equation 3, we use the same scalar $\\lambda \\in \\mathbb{R}$ to control all dimensions of $z$. Additionally, we assume that the noise follows a distribution with zero mean and variance $\\sigma^2$, and we eliminate the noise weighting factor $1 - \\lambda$ (as long as the noise distribution is independent of $z$, this modification does not affect the bottleneck's properties). $\\tilde{z}_{ic}$ is deterministically obtained by the model, ensuring there is no inherent randomness in its computation.\nFor simplicity, we present an equivalent form, where $I$ represents the identity matrix:\n$\\tilde{z}(x) = \\lambda \\cdot z + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2\\mathbb{I})$ (4)\nTheorem 1 (Narrowing Information Bottleneck). Given $0 < \\lambda_1 < \\lambda_2 \\le 1$, we have $\\sup I(\\tilde{z}(\\lambda_1), x) <\\sup I(\\tilde{z}(\\lambda_2), x)$, and when $\\lambda = 0$, we have $I(\\tilde{z}(0), x) = 0$.\nProof. We start by expressing the mutual information $I(\\tilde{z}, x)$ as:\n$I(\\tilde{z}, x) = \\mathbb{E}_{x} [D_{KL} [P(\\tilde{z} | x)||Q(\\tilde{z})] - D_{KL} [P(\\tilde{z})||Q(\\tilde{z})]]$   \n$\\le \\mathbb{E}_{x} [D_{KL} [P(\\tilde{z} | x)||Q(\\tilde{z})]]$  (5)\nGiven that $P(\\tilde{z}(\\lambda) | x) = \\mathcal{N}(\\lambda z, \\sigma^2\\mathbb{I})$, we can compute the difference between the mutual information at two values of $\\lambda$ as follows:\n$\\sup I(\\tilde{z}(\\lambda_1), x) - \\sup I(\\tilde{z}(\\lambda_2), x) = \\mathbb{E}_{x} [\\frac{1}{2 \\sigma^2} ||\\lambda_1 z||^2 - \\frac{1}{2 \\sigma^2} ||\\lambda_2 z||^2]$ (6)\nSince $(\\lambda_1^2 - \\lambda_2^2) < 0$, it follows that:\n$\\sup I(\\tilde{z}(\\lambda_1), x) < \\sup I(\\tilde{z}(\\lambda_2), x)$ (7)\nThis completes the key proof. Further details can be found in Appendix B.\nTheorem 1 shows that by decreasing the value of $\\lambda$, we can reduce the mutual information between $x$ and its encoding, and when $\\lambda = 1$, $I(\\tilde{z}(\\lambda), x)$ reaches its maximum value. Since the computation involves noise sampling, we aim to remove the randomness caused by Theorem 1, which leads us to Theorem 2.\nTheorem 2. When $\\sigma^2 \\rightarrow 0$, given $0 < \\lambda_1 < \\lambda_2 \\le 1$, we have:\n$\\sup I(\\tilde{z}(\\lambda_1), x) < \\sup I(\\tilde{z}(\\lambda_2), x)$ (8)\n$\\sigma^2 \\rightarrow 0$ $\\sigma^2 \\rightarrow 0$\nIn Theorem 2, we demonstrate that the conclusion of Theorem 1 holds as $\\sigma^2$ tends to zero. Specifically, as $\\sigma^2 \\rightarrow 0$, $\\tilde{z}(x)$ converges to $\\lambda z$. In practical scenarios, due to precision limitations, the two expressions will become indistinguishable, effectively eliminating any inherent randomness.\nTheorem 3. Given the function $I(\\tilde{z}, Y)$, the following holds:\n$\\sum_i \\sum_c [\\int \\frac{\\partial I(\\tilde{z}(\\lambda), Y)}{\\partial \\tilde{z}_{ic}(\\lambda)} \\frac{\\partial \\tilde{z}_{ic}(x)}{\\partial \\lambda} dx] = I(\\tilde{z}(1), Y) - I(\\tilde{z}(0), Y)$ (9)\nBuilding on Theorem 2, we can adjust the value of $\\lambda$ to control the size of $I(\\tilde{z}(\\lambda), x)$. When $\\lambda = 0$, $I(\\tilde{z}(\\lambda), x)$ is minimized, and when $\\lambda = 1$, $I(\\tilde{z}(\\lambda), x)$ can be"}, {"title": "5 EXPERIMENTS", "content": "viewed as a function of $\\lambda$, where the process from 1 to 0 corresponds to the bottleneck transitioning from fully open to completely closed. The importance $A(z_i)$ of $z_i$ can be expressed as:\n$A(z_i) = \\sum_i \\sum_c [\\int_0^1 \\frac{\\partial I(\\tilde{z}(\\lambda), Y)}{\\partial \\tilde{z}_{ic}(\\lambda)} \\frac{\\partial \\tilde{z}_{ic}(x)}{\\partial \\lambda} dx]$ (10)\nThe total importance across all dimensions of $z$ equals the loss in $I(\\tilde{z}(\\lambda), x)$ caused by narrowing the bottleneck from fully open to closed. Negative values are also allowed, as some features may reduce $I(\\tilde{z}(\\lambda), x)$. This process eliminates the need for balancing two mutual information terms, thus avoiding the introduction of the $\\beta$ hyperparameter and preventing instability in the interpretability results.\nFor the design of $I(\\tilde{z}(\\lambda), x)$, we follow the work of Wang et al. (2023), using $\\cos (f_\\mathrm{I}(x_\\mathrm{I}), f_\\mathrm{T}(x_\\mathrm{T}))$ as an equivalent replacement. It is worth noting that if we aim to obtain the attribution result for the image modality, $I(\\tilde{z}(\\lambda), x)$ becomes $\\cos (f_{l:L}(\\tilde{z}(f_{1:l-1}(x_\\mathrm{I}))), f(x_\\mathrm{T}))$. Additionally, since $i$ in $z_i$ represents the spatial encoding corresponding to the original encoding, the importance distribution of the original sample features $A(x)$ can be obtained by performing linear interpolation on $A$ from 0 to 1, as described in (Wang et al., 2023; Schulz et al., 2020). Theorem 2, Theorem 3, and the proofs of the Sensitivity and Implementation Invariance axioms are provided in the Appendix."}, {"title": "5.1 MODELS AND DATASETS", "content": "In this study, we follow the experimental setup of M2IB (Wang et al., 2023), utilizing the pre-trained CLIP model with a Vision Transformer (ViT-B/32) (Dosovitskiy, 2020) as the visual encoder. CLIP's joint optimization of image and text alignment has demonstrated outstanding performance in multimodal tasks. We conduct experiments on three different datasets: Conceptual Captions (Sharma et al., 2018), ImageNet (Deng et al., 2009), and Flickr8k (Hodosh et al., 2013). Each of these datasets has unique characteristics, providing diverse visual and textual inputs for the model. Conceptual Captions is a large-scale image-text alignment dataset containing automatically generated image-text pairs, helping the model learn a shared feature space between vision and language. ImageNet, a classic image classification dataset, contains a large number of annotated images and a wide range of class labels, making it a standard dataset for training and evaluating visual models. Flickr8k is a relatively small image-text alignment dataset consisting of 8,000 images and their corresponding natural language descriptions, commonly used to assess multimodal alignment in image captioning and text generation tasks."}, {"title": "5.2 PARAMETER SETTINGS", "content": "We reduced the number of parameters required by the IBP-based method while retaining the core hyperparameters used during the generation of saliency maps, including the number of iterations (num_steps) and the layer number.\nnum_steps: Number of Iterations The num_steps parameter refers to the number of iterations used during gradient optimization, and it primarily affects the precision. It determines how many updates are made to the feature maps in each layer during gradient backpropagation. A larger num_steps generally leads to higher precision, as the model is given more iterations to accumulate gradients and refine attribution results. However, as the number of iterations increases, so does the computational cost, necessitating a balance between accuracy and efficiency in practical applications. In our experiments, num_steps is set to 10, which has been experimentally verified to provide a higher precision result while maintaining relatively low computational overhead.\nlayer number: Layer Number The layer number refers to the identifier of the specific layer chosen from the neural network model as the bottleneck layer. In this study, we selected the 9th layer (layer number = 9), indicating that we extract the hidden states from the 9th layer for generating saliency maps. The choice of this layer is motivated by the fact that intermediate layers typically contain rich contextual information, reflecting both low-level features and some high-level abstract representations. Specifically, using the hidden states from the 9th layer allows us to capture the model's intermediate features, avoiding the low-level signals from early layers or the overly abstract"}, {"title": "5.3 EVALUATION METRICS", "content": "In the evaluation of attribution algorithms, traditional insertion score and deletion score metrics rely on task-specific confidence outputs. However, as our experiments do not include task labels or confidence information, incorporating downstream task outputs would weaken the generality of the interpretability methods. Additionally, the metric ROAR+ (an Extension of ROAR (Hooker et al., 2019)), which requires retraining the model after removing key features, incurs a high computational cost, particularly when dealing with complex models and large-scale datasets, significantly increasing time and resource consumption. For these reasons, we reference two model-output-based evaluation methods proposed by Wang et al. (2023): Confidence Drop and Confidence Increase (Chattopadhyay et al., 2018), to evaluate the performance of attribution algorithms.\nThe Confidence Drop and Confidence Increase are evaluation metrics used to assess the effectiveness of attribution methods. The former measures whether model performance decreases when less important features are removed, with the ideal scenario being that only high attribution scores are retained and the removal of other features does not significantly impact performance. A lower value of Confidence Drop indicates better performance of the attribution method. The latter evaluates whether removing noisy information from the input enhances the model's confidence, with the expectation that the removal of irrelevant features should increase the model's confidence. A higher value of Confidence Increase indicates better performance of the attribution method. Both metrics serve to gauge whether the attribution method effectively identifies and preserves important features while mitigating the impact of noise."}, {"title": "5.4 BASELINE", "content": "We compare our proposed Narrowing Information Bottleneck (NIB) method against several well-established attribution techniques to evaluate its effectiveness. The baseline methods include M2IB (Wang et al., 2023), RISE (Petsiuk et al., 2018), Grad-CAM (Selvaraju et al., 2017), the method by Chefer et al. (2021), Saliency Maps (Simonyan, 2013), MFABA (Zhu et al., 2024c), and FastIG (Hesse et al., 2021)."}, {"title": "5.5 RESULT", "content": "achieves superior performance in both accuracy and efficiency across all datasets."}, {"title": "6 ABLATION RESULT", "content": ""}, {"title": "6.1 ABLATION STUDY OF num_steps", "content": "achieves the highest FPS of 1.5817, providing a substantial performance boost over RISE and other methods.\nIn the ImageNet dataset, NIB maintains its leading position, with the lowest Image Confidence Drop (0.9012), outperforming M2IB by 0.2603 units and Grad-CAM by 1.6471 units. Additionally, NIB achieves the highest Image Confidence Increase of 53.1, showing a 19.2 unit improvement over Grad-CAM and a 3.7 unit improvement over M2IB. For text metrics, NIB continues to excel with the lowest Text Confidence Drop (0.4193), representing a 2.1825 unit gap over M2IB and a 2.2231 unit gap over Grad-CAM. The FPS score for NIB is also competitive at 2.4481, showing high efficiency in real-time applications.\nOn the Flickr8k dataset, NIB achieves the lowest Image Confidence Drop (1.4495), only slightly better than M2IB by 0.0236 units, but significantly outperforms Grad-CAM by 3.7374 units. In terms of Image Confidence Increase, NIB ties with M2IB at 28.1, while exceeding Grad-CAM by 14.5 units. NIB also leads in Text Confidence Drop, with a score of 0.4562, outperforming M2IB by 1.6221 units and Grad-CAM by 1.7261 units. The computational efficiency of NIB remains high, with an FPS of 2.1995, reflecting its ability to maintain high-speed performance compared to slower methods like RISE.\nIn summary, the proposed NIB method consistently outperforms existing attribution techniques across all datasets, providing better attribution accuracy and computational efficiency. The improvements in both Confidence Drop and Confidence Increase metrics demonstrate NIB's capability to identify key features and remove irrelevant ones, enhancing the interpretability and robustness of multimodal models. Please see the attribution results images in the GitHub repository."}, {"title": "6.2 ABLATION STUDY OF target_layer", "content": "provides an optimal balance between accuracy and efficiency.\nIn the ablation study of the target_layer parameter, we explore the impact of selecting different layers for generating saliency maps. Specifically, we evaluate the performance of layers 3, 6, and 9 across the Conceptual Captions, ImageNet, and Flickr8k datasets, with num_steps fixed at 10.\nThe results in Table 4 reveal that layer 9 generally yields the best performance across all datasets. For Conceptual Captions, layer 9 achieves the lowest Text Confidence Drop (0.2705) and the highest Text Confidence Increase (43.95), indicating that the saliency maps generated from this layer provide the most accurate and interpretable attributions. Similarly, in the ImageNet dataset, layer 9 performs well, with a moderate Image Confidence Drop (0.9012) and the highest Text Confidence Increase (56.1), demonstrating that it effectively captures important features for both image and text alignment.\nIn contrast, selecting earlier layers (3 and 6) results in higher Confidence Drop scores, particularly in the Text Confidence Drop metric, suggesting that these layers lack the necessary high-level semantic information. Therefore, the results indicate that layer 9 strikes an optimal balance between capturing rich feature representations and providing interpretable attributions, making it the most effective choice for generating saliency maps in the NIB method."}, {"title": "7 CONCLUSION", "content": "This paper introduces the Narrowing Information Bottleneck Theory (NIBT) to address the challenges of randomness and hyperparameter sensitivity in explaining multimodal models like CLIP. By re-engineering the traditional Bottleneck method, NIBT improves interpretability for both image and text representations. The proposed method demonstrates superior performance in terms of both attribution accuracy and computational efficiency across multiple datasets. These advancements contribute to a more transparent and reliable interpretation of complex multimodal tasks, paving the way for broader applications of explainable AI in high-stakes environments."}, {"title": "CODE OF ETHICS AND ETHICS STATEMENT", "content": "All authors of this paper have read and adhered to the ICLR Code of Ethics\u00b9during the research, development, and writing of this work. We affirm that this paper complies with all ethical guidelines outlined in the code. No part of our research involved human subjects, and there are no significant concerns regarding privacy, fairness, or potential conflicts of interest. All datasets and methodologies used are publicly available, and no sponsorship influenced the content or findings of this work.\nAdditionally, the interpretability methods proposed in this paper aim to improve model transparency and are intended for enhancing the trustworthiness of AI systems, especially in sensitive domains such as healthcare. We believe that our contributions will support ethical AI deployment by making models more interpretable and accountable."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "We have taken multiple steps to ensure the reproducibility of our results. The detailed descriptions of the models, datasets, and training protocols used in our experiments are provided in the main text. Specific hyperparameters, including the number of iterations and selected layers for saliency map generation, are also reported in Section 5.2. Furthermore, the code for implementing our proposed Narrowing Information Bottleneck Theory (NIBT) and the associated datasets are available in the Anonymous Repository\u00b2. These resources should enable the community to reproduce our findings and apply the methods to their own work."}, {"title": "A PRINCIPLES OF INFORMATION THEORY", "content": ""}, {"title": "A.1 PROPERTIES OF MUTUAL INFORMATION", "content": "1. Non-negativity\n$I(X; Y) \\geq 0$\n2. Symmetry\n$I(X; Y) = I(Y; X)$\n3. Relationship with Conditional Entropy and Joint Entropy\n$I(X; Y) = H(X) - H(X|Y)$\n$= H(Y) - H(Y|X)$\n$= H(X) + H(Y) - H(X,Y)$\n$= H(X,Y) - H(X|Y) - H(Y|X)$\n4. Relationship with Kullback-Leibler (KL) Divergence\n$I(X; Y) = \\sum_y p(y) \\sum_x p(x|y) \\log_2 \\frac{p(x|y)}{p(x)}$\n$= \\sum_y p(y) D_{KL}(p(x|y)||p(x))$\n$= \\mathbb{E}_y [D_{KL}(p(x|y)||p(x))", "Z": "mathbb{E}_{x} [D"}]}