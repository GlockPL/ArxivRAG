{"title": "Democratizing Reward Design for Personal and Representative Value-Alignment", "authors": ["Carter Blair", "Kate Larson", "Edith Law"], "abstract": "Aligning AI agents with human values is challenging due to diverse and subjective notions of values. Standard alignment methods often aggregate crowd feedback, which can result in the suppression of unique or minority preferences. We introduce Interactive-Reflective Dialogue Alignment, a method that iteratively engages users in reflecting on and specifying their subjective value definitions. This system learns individual value definitions through language-model-based preference elicitation and constructs personalized reward models that can be used to align AI behaviour. We evaluated our system through two studies with 30 participants, one focusing on \"respect\" and the other on ethical decision-making in autonomous vehicles. Our findings demonstrate diverse definitions of value-aligned behaviour and show that our system can accurately capture each person's unique understanding. This approach enables personalized alignment and can inform more representative and interpretable collective alignment strategies.", "sections": [{"title": "1 INTRODUCTION", "content": "As AI agents take on more tasks and affect personal parts of our lives, it is increasingly important to align their behaviour with human values at both individual and collective levels. This alignment challenge spans from personalized interactions, like an AI agent's respectful behaviour in a household, to broader societal collective decisions.\nCurrent AI alignment approaches, such as reinforcement learning from human feedback (RLHF), often rely on feedback aggregated from many users [6, 15]. This aggregation implicitly makes collective decisions about AI behaviour, potentially marginalizing minority viewpoints or unique preferences [68]. By \"averaging out\u201d personal differences, these methods risk creating agents that ignore the values of minority groups or those with uncommon preferences. Accurate individual reward models offer a solution to this problem. These models enable personalization by tailoring AI behaviour to each user's unique values and beliefs when appropriate while simultaneously providing a foundation for more representative and interpretable collective decision-making. By understanding individual stakeholder preferences, we can aggregate diverse viewpoints more fairly and make informed trade-offs in group contexts. This approach addresses both the need for personalized AI interactions and the challenge of making ethical collective decisions.\nHowever, creating personalized reward models presents several challenges. First, finding the most effective way for humans to convey their goals and desires to Al systems remains an open question [15]. Second, end users of AI systems may lack the technical skills or necessary training to provide feedback to an agent [7]. Third, it is impractical to expect individuals to teach an agent how to behave when many examples may be required.\nIn this work, we present Interactive-Reflective Dialogue Align-ment, an interactive system for aligning AI agents to individual values that is novice-friendly and sample-efficient. The user-facing side is a simple chat interface that prompts users to explain their desired behavioural patterns and selectively sends examples of the agent's behaviour to solicit feedback using active learning tech-niques [49]. The system's textual messages are designed to encour-age users to reflect deeply on their value definitions, inspired by prior work on designing for reflection [9, 25, 34, 73]. Using the feedback gathered from the dialogue, we create a language-based reward model that leverages the in-context learning abilities of large language models [14].\nWe evaluated Interactive-Reflective Dialogue Alignment through two studies involving a total of 30 participants. In the first study, 21 participants used the system to build a reward model for their personal definitions of respectful behaviour. The second study involved 9 participants and focused on ethical decision-making in autonomous vehicles. We found that participants had widely vary-ing definitions of value-aligned behaviour across both studies and that our system could capture these subjective and personal defini-tions significantly more accurately than baseline systems.\nOur contributions are as follows:\n\u2022 A novel, accessible, and theoretically grounded pipeline for aligning Al agents to individual values and preferences, draw-ing upon insights from AI, HCI, and social science research.\n\u2022 A comprehensive evaluation of the system across two dis-tinct domains, demonstrating its ability to capture individual human values and ethical preferences.\n\u2022 Empirical evidence highlighting the diversity of individual interpretations of value-aligned behaviour.\n\u2022 Insights for future work on enabling end users to interac-tively align Al agents with their personal values, including potential applications in both individual and collective align-ment contexts."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "Our work intersects human-computer interaction, reinforcement learning, and value alignment. We draw on insights from AI, HCI, and social science research to develop a system that accurately captures individual values in a reward model. Below, we outline the related work most relevant to our system and provide the necessary background to understand it.\n2.1 Human Values and Fuzzy Preferences\nValues are principles that guide human behaviour and ethical judg-ments [66]. Friedman et al. describe values as \"what a person or group of people consider important in life\" [27]. In the context of Al systems, values can manifest in various ways. For example, in a household setting, the value of respect might manifest as an AI assistant using appropriate language or maintaining privacy. In the context of autonomous vehicles, values might include prioritizing passenger safety while also considering the welfare of pedestri-ans and other road users. Importantly, values vary across cultures, individuals, and contexts [40].\nWhile values themselves may be clearly stated, their interpreta-tion and application often become fuzzy when considered in specific contexts [70]. This ambiguity is further compounded when attempt-ing to translate these human values into computational terms for AI alignment [64]. Users may struggle to articulate their values or determine which AI behaviours align with their values.\nThe concept of fuzzy preferences [62] provides insight into this challenge. Fuzzy decision-making acknowledges that preferences, attributes, and objectives in decision processes can be imprecise. In Al alignment, this fuzziness manifests in the difficulty of precisely defining and measuring value alignment. Users may not have clear, a priori knowledge of what constitutes aligned behaviour, and the importance of different behavioural features may be unclear or change based on context.\nOur work addresses these challenges by developing a system that helps users articulate and refine their fuzzy value definitions, enabling more precise alignment of Al systems with individual human values.\n2.2 Designing for Reflection\nReflection has several notable benefits that can help users clarify their understanding of their preferences and values. For example, it has been found that prompting medical practitioners to reflect can increase diagnostic accuracy [19, 23, 45]. Moreover, in consumer research, it has been found that prompting consumers to engage in preference self-reflection can lead to more accurate reporting of preferences and that engaging consumers in realistic decisions can increase preference elicitation accuracy [31].\nDesigning for reflection has been of interest to HCI researchers for some time [28, 42, 56, 63, 76]. Fleck & Fitzpatrick propose a framework for designing for reflection that includes definitions of various levels of reflection and techniques for supporting reflection [25]. They define five levels of reflection, ranging from simply revisiting past events to critical reflection that considers wider social or moral contexts. Levels 1-3 are most relevant to our work as they inspired the design of our system: Level 1 involves explaining or justifying actions/events. Level 2 involves exploring relationships and considering different perspectives and hypotheses. Level 3 involves fundamentally challenging assumptions and transforming one's understanding or practice.\nLLMs present an opportunity for more tailored and dynamic dialogue to engage users in reflection. Some work has explored the use of LLMs for generating reflective prompts in a design template [75], while others have used language prompts to engage users in reflection through single questions or sentence starters [24, 32, 61, 72]. Further extensions of this approach have included the use of pre-scripted, multi-message dialogues to facilitate reflection [35, 74].\nWe extend this body of work by integrating reflective dialogue techniques with large language models, creating a dynamic and personalized reflection process that helps users clarify their values for Al alignment."}, {"title": "2.3 Reinforcement Learning and Its Variants", "content": "Reinforcement Learning (RL) is a branch of machine learning in which an agent learns a policy, i.e., a strategy for selecting actions in an environment, to maximize the rewards it receives. Traditionally, the agent designer crafts a reward function to incentivize desirable behaviour. However, reward design is notoriously challenging and can result in reward hacking where an agent learns behaviour that gets high reward, but that runs counter to the designers intent [4, 20, 22].\nInverse reinforcement learning (IRL) was developed in response to difficulties with reward design. In IRL, the goal is to learn a reward function from expert demonstrations [1, 50, 51, 59, 77]. This can be useful when a desired behaviour is easy to demonstrate but difficult to design a reward function for. However, since multiple reward functions can often explain the observed behaviour, the reward function lacks interpretability and is inherently ambiguous [46].\nReinforcement learning from human feedback (RLHF) instead learns a reward model from human feedback [6, 18, 78]. This ap-proach is especially useful when aligning agents to human prefer-ences as the agent can optimize for getting \"good\" feedback. There are three main parts of RLHF: Feedback collection where humans provide feedback, reward modelling where the feedback is turned into a reward model, and policy optimization where the reward model is used to train a policy/agent with reinforcement learning [15].\nOur approach bridges the gap between RL and HCI by combin-ing RLHF techniques with user-friendly interfaces and reflective dialogue, making the process of aligning AI agents with individual values more accessible to non-expert users."}, {"title": "2.4 Preference Elicitation and Active Learning", "content": "When using language feedback for RLHF, it is natural to implement dialogue-based preference elicitation as the mechanism for collect-ing feedback. In dialogue-based preference elicitation, the goal is to understand a user's preferences through the use of dialogue [57, 58]. There are two main approaches to preference elicitation: item-based preference elicitation, where the user is asked about their preferences regarding specific things [2, 17, 30, 44, 65] and feature-based preference elicitation where the user is asked about general features or attributes [43, 69, 71]. Some methods use both item- and feature-based methods [11], similar to our approach. However, previous work has focused on content recommendation, not agent alignment.\nWhen performing item-based preference elicitation, we must select items to query the user about. Active learning is a subfield of machine learning where the learner is responsible for choosing which examples to request labels for [60, 67]. Being selective about which examples to query the user about is important in cases where labels are expensive to obtain [54]. There are generally three strate-gies for sampling items to query the user about, namely random-, diversity-, and uncertainty-based sampling [48]. Random sampling selects items arbitrarily, while diversity-based sampling chooses items that are most different from each other. Uncertainty-based sampling, in contrast, focuses on items the model is least confident about [49].\nOur system integrates these approaches into an interactive dia-logue framework, combining item- and feature-based preference elicitation with active learning and reflective dialogue to capture individual value definitions efficiently."}, {"title": "3 INTERACTIVE-REFLECTIVE DIALOGUE ALIGNMENT (IRDA) SYSTEM", "content": "We introduce an interactive system, named Interactive-Reflective Dialogue Alignment (IRDA), which aims to enable end users with no particular expertise in machine learning to define agent behaviour that is aligned with their subjective definitions of a human value and generate a reward model that can be later used to train an agent based on this definition. We will begin by outlining our design aims, providing a high-level overview of the system and user flow, and then describe each component in more detail.\n3.1 Design Aims\nOur system is intended for the end-users of Al agents, and we do not assume that the users of our system will have any technical expertise. As such, the design aims for our system are centred around the idea of making the system easy and quick to use while maintaining good performance. In particular, we require:\n(1) The system should not require any special technical knowl-edge (e.g., how to program or how to design a \"good\" reward function).\n(2) The system should be sample-efficient (i.e., strategically se-lecting the most informative examples to ask users for feed-back on).\n(3) The system should be able to capture individual differences and unique conceptions of the appropriate behaviour associ-ated with a value.\n3.2 System and User Flow\nUpon entering our system, users are presented with a greeting message outlining the purpose of the system as well as a preview of the environment. To better illustrate our system, we will use one of the environments used in our evaluation, the Multi-Agent Apple Farming Environment, as the running example. The environment is a 6\u00d76 grid where the agents are rewarded for picking apples and receive no reward for collecting garbage. There is one blue \"main\" agent in the environment and three grey \"background\" agents as shown in Figure 1. Each one of the agents owns one of the four 3\u00d73 quadrants, which each represent an orchard. Each agent is free to move around the whole grid; however, two of the three background agents are programmed to be stationary. The main agent is the agent whose behaviour users are asked to monitor and give feedback on. Users give feedback on whether the behaviour aligns with a particular value (e.g., picking apples but being \"respectful\u201d to neighbours).\nThe input to our system is a value that the user wants the agent to adopt. For example, a user could specify to the system \"I would like the agent to act respectfully\" through a chat interface, as shown in Figure 3. Given this value specification, we collect a large pool of trajectories (sequences of actions the agent took in the environ-ment). This pool can be collected by taking a portion of an existing dataset or by running random rollouts in a simulator. The system then selectively samples a small number of trajectories where the agent behaves in diverse ways to present to the user for feedback (trajectory assessment). Based on this feedback, the system hy-pothesizes about the rationale behind the user's assessments (e.g., the user thinks the agent's behaviour is disrespectful because it wan-dered into another agent's yard). This hypothesis is presented to the user along with alternative perspectives the user could consider to prompt them to reflect on their current value definition (ratio-nale reflection). Upon reflection, the user can opt to re-explain the trajectories they initially saw with their new perspective. This iterative process is called the preference clarification loop (shown in Figure 1).\nAll of the information collected from the user through this itera-tive feedback loop is then used to build an initial language-based reward model. The initial reward model is refined by selectively picking trajectories that the model is most uncertain about and querying the user for feedback (uncertainty reduction). Each time the user explains one of the trajectories the model is uncertain about, this information is added to the reward model, thus making it less uncertain about similar trajectories. This process is repeated until the overall uncertainty is below a certain threshold. In the end, the system generates a language-based reward model that can effectively administer rewards to any agent when it acts in accordance with the user's value definitions (reward modelling). Depending on the complexity of the desired behaviour and envi-ronment, this reward model can then be used to either train a more efficient reward model or to train an agent directly.\nHaving described, at a high level, the system and user flow, we will now describe various user flow steps (those in bold) in more de-tail, namely trajectory assessment, rationale reflection, uncertainty reduction and reward modelling."}, {"title": "3.2.1 Trajectory Assessment", "content": "Our system initially lacks informa-tion about the user's preferences, and thus, we employ diversity-based sampling [52] to find a set of trajectories where the agent exhibits a diverse set of behaviours in varying situations. To do so, we begin by sampling a large pool of over 1000 trajectories. We turn each trajectory into a numerical array that gives a full rep-resentation of the state of the environment at each time step. For example, in a 30-step trajectory in the multi-agent apple farming environment, we can encode each step as multiple arrays of the same shape as the grid. In each array, for a step, we can encode the position of a certain type of entity within the environment. The numerical encoding for a single timestep is shown in Figure 4.\nWith the pool of numerically encoded trajectories in hand, we perform k-means clustering to split the trajectories into k distinct clusters. Since we want to query the user about a diverse set of tra-jectories in this initial phase, we select one trajectory from each of the k clusters. In particular, for each group, we select the trajectory closest to the arithmetic mean, or the centroid, of all the trajectories in the cluster.\nThe system implements item-based preference elicitation by ask-ing the user to explain whether the agent is aligned with their preferences in each of the k trajectories, one at a time, using 1-3 sentences. For instance, if a user were trying to train an agent to behave respectfully, the system would ask, \"Does the agent act respectfully? Please explain your reasoning using 1-3 sentences.\""}, {"title": "3.2.2 Rationale Reflection", "content": "During the trajectory assessment pro-cess, the system intermittently interjects a dialogue to engage the users to reflect on their value definition. To do this, we use a large language model (GPT-4 Turbo) to make a hypothesis about what features the user is basing their decisions on and other features the user could consider. To do so, we parse each trajectory into an ASCII representation (see Figure 5 for an example) and pass this along with the user's explanation to the LLM. In the prompt, we ask the LLM to make a hypothesis about what features the user is basing their decisions on and to offer alternative features the user could consider.\nFor example, in the multi-agent apple farming environment, where an agent owns one of four orchards in the environment, the user may base their definition of \"respectful\" behaviour on whether the agent stays in its own orchard. A simplified example of a response from the system could be:\nBased on your explanations, it seems as though a key factor in determining whether the agent's behaviour is respectful or not is whether the agent stays in its own orchard. You could also consider:\n(1) Whether the agent helps pick up garbage\n(2) Whether the agent steals apples from other agents\nThe user is then prompted to explain if the hypothesis the system made is correct and if the other features should be considered. This feature-based preference elicitation step is important for a number of reasons. First, understanding the features on which the user bases their decisions allows for improved generalization of the reward model. The system can use these features, or patterns of behaviour, as decision criteria when assessing whether the agent's behaviour in a trajectory aligns with the user's value definition. Second, this step encourages the user to reflect on their definition and consider alternative perspectives. This reflection can assist users in gaining a clearer understanding of the specific behaviour they want the agent to demonstrate. Third, if the diversity-based sampling in the initial item-based preference elicitation phase does not select any trajectories where a certain behavioural feature is present that would be important to the user, the alternative or additional features proposed by the system can help uncover these. This can help the system gain a more holistic understanding of the user's preferences, even when only querying the user about a handful of items.\nIf the user updates the features they think are important based on the system's proposed alternatives, they can optionally re-assess the initial k trajectories in the preference clarification loop. This allows users to iteratively reflect on and specify what features or behavioural patterns they would like the agent to embody. This refinement process simultaneously helps the user better understand their own preferences and helps the system gain a clearer under-standing of the user's true intent."}, {"title": "3.2.3 Uncertainty Reduction", "content": "After the user has provided feedback through the alternating processes of trajectory assessment and rationale reflection, we implement another phase of item-based preference elicitation to create and refine the reward model. This approach focuses on querying the user about specific items (trajectories) to improve the model's understanding of user preferences. To begin with, we use all of the information collected in the trajectory assessment and rationale reflection phases to create an initial reward model. The reward model can be thought of as a classifier the input is an ASCII representation of a trajectory, and the output should be a 1 (reward) if the agent's behaviour aligns with the user's expressed intent and 0 (no reward) otherwise.\nWe use this initial model to assign rewards to a held out pool of trajectories. Our reward model, based on an autoregressive trans-former (LLM), allows us to identify trajectories where the model has high uncertainty about the appropriate reward. We do this by comparing token probabilities for positive (reward) and negative (no reward) classifications. For example, if the user is training the agent to be respectful, we compare the probability of the reward model outputting the \"respectful\" token to the probability of the model outputting the \"disrespectful\" token. If the probabilities are close to one another, then the model is less certain, and if they are very different (e.g. 0.99 and 0.01), then the model has high certainty about whether the agent acts according to the user's intent. The \"confidence\" of the model can be thought of as the absolute value of the difference between the two token probabilities.\nThe item-based preference elicitation process for refining the reward model proceeds as follows: (1) Identify the trajectory in the pool where the model has the lowest confidence. (2) Query the user to explain this trajectory. (3) Add the ASCII representation of the trajectory and the user's explanation to the reward model. (4) Repeat steps 1-3 until the model's confidence for each trajectory in the uncertainty pool exceeds a minimum confidence threshold, \u0454.\nThis iterative process reduces the model's uncertainty about the user's value definition by focusing on specific items (trajectories) where additional user input is most beneficial."}, {"title": "3.2.4 Reward Modelling", "content": "Once the feedback from the user has been collected, the last step is to create a reward model that can give feedback to the learning agent on behalf of the human. Creating a reward model that can issue rewards on behalf of humans is important as, depending on the complexity of the environment and the desired behaviour, it can take millions [55] or even billions [3] of timesteps for an agent to learn the desired behaviour. The reward model can act on behalf of the user, thus relieving the user of giving feedback during agent training.\nOur in-context, language-based reward model functions as a classifier, evaluating agent behaviour based on user-defined values. The model receives as input a trajectory encoded in ASCII format, a process detailed in Appendix B. This encoding preserves essential spatial and temporal information while enabling efficient process-ing by language models (see Figure 5 for an example). The model leverages two key elements to classify a trajectory: (1) The encoded trajectory and (2) the user feedback collected during dialogue-based preference elicitation.\nUsing this information, we prompt the LLM to assess whether the agent's behaviour in the given encoded trajectory aligns with the user's expressed intent. The LLM's output is then parsed into a binary classification: 1 if the behaviour aligns with user intent, 0 otherwise.\nTo achieve this, we create a prompt that includes\n(1) a description of the environment and the ASCII characters used in the encoding,\n(2) the information gained from the user during the dialogue-based preference elicitation,\n(3) a description informing the language model that its goal is to predict whether the agent's behaviour is aligned with the user's value definition,\n(4) an ASCII representation of the trajectory that the model is to label,\n(5) text to encourage the model to engage in chain-of-thought reasoning,\n(6) and instructions about how the LLM is to format its response so that the output can be programmatically parsed.\nTwo major findings about LLM capabilities inspired the design of this prompt. First, LLMs are effective in-context, few-shot learn-ers [14]. This means that a trained LLM, with fixed parameters, can learn new patterns by including the beginning of a pattern as context and asking the model to continue the pattern. In settings similar to ours, this has been shown to be far more sample efficient than traditional supervised learning [37]. The pattern of interest, in our case, is what behaviour the user deems to be aligned with their value definition. The second finding is that prompting LLMs to engage in chain-of-thought reasoning can greatly increase their performance [36]. The idea is that the model forms an argument first and then, due to the auto-regressive nature of language models, uses the argument it made to determine a final answer.\nSince our LLM-based reward model receives a full trajectory as input and outputs a reward, it can deal with non-Markovian rewards. This means that the reward model can evaluate the agent's behaviour over multiple time steps when determining the reward. We hypothesized that this would be important for capturing users' preferred agent behaviour, and this was supported by our empirical findings discussed in Section 7.4.1."}, {"title": "4 STUDY DESIGN & METHODOLOGY", "content": "The goal of our system, Interactive-Reflective Dialogue Alignment, is to learn individual value definitions. We evaluated our system in two studies by comparing to another language-based reward modelling pipeline and to supervised learning. Study 1 investigates the utility of our system for learning about participants' definition of respectful agent behaviour. Study 2 investigates the utility of our system for learning about participants' decision-making in moral dilemmas involving an agent (autonomous vehicle). Our studies employ a within-subject design, collecting data from each participant to train various language-based reward models, supervised learning baselines, and for a test set to evaluate these methods.\n4.1 Environments\nIn Study 1, the multi-agent apple farming grid world, which was described in the beginning of subsection 3.2, was used. Participants were asked to evaluate if the agent was acting respectfully.\nIn Study 2, we used the Moral Machine environment [5]. The Moral Machine dataset is based on a simulated environment that presents ethical dilemmas faced by autonomous vehicles. In each scenario, a self-driving car encounters an unavoidable accident and must choose between two outcomes: staying on course or swerving. Each decision results in different consequences for the individuals involved. The environment populates these scenarios with characters chosen from 20 predefined types, including pedestri-ans, passengers, and various demographic groups such as children, adults, elderly, and animals. Each outcome features between one and five characters. The scenarios explore nine key ethical dimen-sions: the nature of the vehicle's intervention, the relationship of individuals to the vehicle (pedestrians or passengers), the legality of pedestrians' actions, and various attributes of potential victims, including age, gender, social status, physical fitness, number, and species (human or pet). Participants were asked to choose whether the vehicle should stay or swerve in each scenario presented to them.\n4.2 Participants\nIn Study 1, we recruited 21 participants from our institution (18 to 39 age range, M=23.86, 7 self-identified as male and 14 as female)."}, {"title": "4.3 Procedure", "content": "Participants used our interface to specify how they would like the agent to act via our dialogue-based preference elicitation process. Participants then labelled 50 scenarios and were interviewed.\nIntroduction (~5min) - After completing the consent form and the demographic questions, the mechanics of the environment were fully explained to the user. We explained the environment mechanics as thoroughly as possible so that differences observed between participants stemmed from their opinions, not hidden assumptions about the environment.\nDialogue - The participant began by conversing with the system about the agent's behaviour following the dialogue structure pre-sented in Section 3.2. To control the amount of time users spent on the activity, we limited the user to one preference clarification loop and one uncertainty reduction loop.\nLabelling - Following the participants' dialogue interaction with the system, participants labelled 50 scenarios. Each participant labelled the same scenarios, which allowed us to assess how much the participants agreed on the labels.\nSemi-structured Interview (~10min) - After completing the la-belling task, each participant was asked whether they felt they were able to give the system a good understanding of their decision-making if they were always able to articulate why they chose a label, if it was ever difficult to decide, and if they thought their labelling behaviour changed over time. The interview aimed to determine two things. First, it sought to determine if participants could verbally explain their definition of aligned behaviour. Second, it attempted to discover whether these definitions changed over time and what factors caused any changes."}, {"title": "4.4 Baseline Comparisons", "content": "We compared our system, Interactive-Reflective Dialogue Alignment, to several baselines to evaluate its effectiveness. These baselines include another language-based system and various supervised learning approaches.\n4.4.1 Language-Based Baseline (LB). Kwon et al. [37] proposed a reward modelling pipeline for text-based environments where the user selects multiple examples from a palette of examples of the agent behaving as they would desire, accompanied by explanations. We slightly modify their pipeline in the following way: Instead of asking the user to select examples from a handcrafted palette, we choose the examples the user sees with the diversity-based sampling procedure described in Stage 1 of Section 3.2.1.\nSince the baseline pipeline LB is a subprocess of our full system, Interactive-Reflective Dialogue Alignment, the user only interacts with our system. However, when forming the reward model for LB, we only include the information collected from the user during the trajectory assessment phase (described in Section 3.2.1) before they have done rationale reflection, mirroring the pipeline proposed in [37].\n4.4.2 Supervised Learning Baselines. We compared our approach to supervised learning methods using neural networks. These base-lines include both individual models trained separately for each participant and collective models trained on aggregated data from all participants. For more details on the architecture and training of these models, see Appendix C.\nIn Study 1, we employed multi-layer perceptron (MLP) models: individual models (MLPind) for each participant i, and a collective model (MLPcol) using aggregated data from all participants. These models used input based on the grid map encoding (see Figure 4).\nFor Study 2, we compared to both MLP and convolutional neural network (CNN) models. We used the same MLP architecture as in Study 1, but with a 26-dimension vector input representing Moral Machine scenarios described in Appendix B. We also introduced CNN models, both individual (CNNind) and collective (CNNcol), which use image representations of scenarios as input.\nAcross all supervised learning models, we utilized 30 scenarios per participant for training, selected from the 50 scenarios anno-tated during the labelling phase. By comparing Interactive-Reflective Dialogue Alignment to these baselines (LB, MLPind, MLPcol, CNNind, and CNNcol), we aim to evaluate the efficacy of our language-based reward modeling approach and the value added by our reflective dialogue process."}, {"title": "4.5 Analysis", "content": "Our analysis aims to answer the following three questions:\nRQ1: Do value definitions significantly vary between participants?\nRQ2: Does structured reflection enhance language-based reward modelling?\nRQ3: When is individualized language-based reward modelling effective?\nTo address these questions, we employ a mixed-methods ap-proach, combining quantitative analyses of model performance and inter-annotator agreement with qualitative analyses of partic-ipant decision-making processes and experiences. The following subsections detail our analytical methods, each designed to provide insights into one or more of our research questions.\n4.5.1 Inter-Annotator Agreement. We assess the inter-annotator agreement between participants on the test set of scenarios they labelled in each study. Since each participant labelled the same test scenarios, we can use Fleiss' kappa value to quantify the inter-annotator agreement between the participants [39]. Generally, kappa statistics below 0 indicate \"poor\" agreement and kappa statistics above 0.8 indicate \"nearly perfect\" agreement [39]. This analysis di-rectly addresses RQ1 by quantifying how much participants agree when labelling examples. Low agreement suggests diverse value definitions, while high agreement indicates more uniform values across participants. It also informs RQ3 by indicating when in-dividualized approaches might be more beneficial than collective ones.\n4.5.2 Evaluation of Language-Based Reward Model Performance. Our evaluation compares our system's performance against the baseline language-based reward modelling system that does not leverage dialogic (level 3) reflection. We use a performance metric, P, calculated for each participant for both systems. We denote $P_{i}^{IRDA}$ and $P_{i}^{LB}$ as the performance metrics for participant i on our system and the baseline system, respectively. The choice of performance metric varied between studies: Study 1 used balanced accuracy due to high class imbalance, while Study 2 used accuracy.\nFor each participant i, we generate rewards (labels) using both systems for the 20 scenarios participants labelled that were not used for training the SL baselines. We then calculate P for each system per participant. This process yields n pairs of P values, where n is the total number of participants. We conducted three statistical tests on the P values:\n(1) We bootstrapped 95% confidence intervals for the mean by resampling 10,000 times with replacement.\n(2) For each participant, we calculated the difference $\\Delta P_{i} = P_{i}^{IRDA} - P_{i}^{LB}$ and bootstrapped these differences in the same way.\n(3) We compared the P values for each system using the Wilcoxon signed-rank test. This non-parametric test was chosen over parametric alternatives as it is less prone to false positives and more robust when the data distribution is unknown or skewed [13].\nThis analysis primarily addresses RQ2 by comparing the perfor-mance of systems with and without dialogic reflection, allowing us to assess if structured reflection enhances language-based reward modelling.\n4.5.3 Comparison to Supervised Learning. We compared our language-based systems to traditional supervised learning approaches. Both the individual models (MLPind and CNNind) and the collective mod-els (MLPcol and CNNcol) were trained incrementally, gradually in-creasing the number of samples used per participant. This method-ology allowed us to analyze how model performance evolved with increasing data availability. For each increment, we calculated $p_{i}^{ind}$ and $p_{i}^{col}$ for each participant i, representing the performance of the individual and collective models, respectively. To ensure robust statistical analysis, we bootstrapped these values with replacement using 10,000 resamples.\nThis comparison helps answer RQ3 by revealing when language-based methods outperform supervised learning approaches under various conditions. It also informs RQ1 by showing if individual-ized language-based models consistently capture personal value definitions better than collective supervised models."}, {"title": "4.5.4 Qualitative Analysis of Participant Decision Making", "content": "We con-ducted a detailed analysis of the message exchanges between par-ticipants and the system to gain insight into participants' decision-making processes. We employed an inductive coding approach, systematically reviewing the messages to identify key features and criteria that participants used in their decision-making. Our coding process involved multiple passes through the data, with iterative refinement of the codebook to ensure it captured the full range of decision-making strategies observed.\nThis analysis directly addresses RQ1 by identifying different features and criteria used by participants, providing rich evidence of how value definitions differ.\n4.5.5 Analysis of Feature Similarity Between Participants. To quan-tify how similar participants were in their use of decision-making features, we employed the Jaccard similarity coefficient. This mea-sure calculates the overlap between two sets of items which, in our case, are features the two participants used to make decisions [33"}]}