{"title": "Generative Distribution Prediction: A Unified Approach to Multimodal Learning", "authors": ["Xinyu Tian", "Xiaotong Shen"], "abstract": "Accurate prediction with multimodal data encompassing tabular, textual, and visual inputs or outputs is fundamental to advancing analytics in diverse application domains. Traditional approaches often struggle to integrate heterogeneous data types while maintaining high predictive accuracy. We introduce Generative Distribution Prediction (GDP), a novel framework that leverages multimodal synthetic data generation such as conditional diffusion models-to enhance predictive performance across structured and unstructured modalities. GDP is model-agnostic, compatible with any high-fidelity generative model, and supports transfer learning for domain adaptation. We establish a rigorous theoretical foundation for GDP, providing statistical guarantees on its predictive accuracy when using diffusion models as the generative backbone. By estimating the data-generating distribution and adapting to various loss functions for risk minimization, GDP enables accurate point predictions across multimodal settings. We empirically validate GDP on four supervised learning tasks-tabular data prediction, question answering, image captioning, and adaptive quantile regression-demonstrating its versatility and effectiveness across diverse domains.", "sections": [{"title": "Introduction", "content": "The advent of big data and the proliferation of diverse data sources have catalyzed a growing interest in multimodal learning. Multimodal data combines information from multiple sources or modalities. Each modality offers unique insights into the underlying phenomena, and integrating these heterogeneous data types holds the potential for more comprehensive models and enhanced predictive performance [29, 4].\nIn contexts such as healthcare, the integration of medical records, imaging, and genomics has shown potential in enhancing outcomes [16]. For applications like autonomous driving, combining data from cameras, lidar, and radar promotes superior decision-making [11, 9]. Additionally, in domains such as social media analysis and emotion recognition, merging text, images, and physiological signals offers substantial benefits [46, 12]. These applications underscore the necessity for models capable of effectively processing multimodal data. However, the integration of heterogeneous modalities poses significant challenges due to variations in structural characteristics, representation, and statistical properties [34, 4]. For instance, images are characterized by high dimensionality, text is inherently sequential, and tabular data is structured, which complicates the fusion process.\nTraditional methods often utilize prediction models designed for single-modal or tabular data. The complexity of unstructured data, including challenges like high dimensionality and the absence of natural structure in embeddings, further obstructs multimodal learning [14]. Thus, these methods find it difficult to effectively manage heterogeneous data types, limiting their applicability in multimodal analysis.\nTo address these difficulties, we propose Generative Distribution Prediction (GDP), a framework that facilitates the integration of multimodal data through conditional synthetic data generation. GDP exploits the core concept that generative models, such as diffusion models [39, 47, 19, 50, 26, 49, 24, 51, 21] and normalizing flows [22], capture more expressive features by identifying patterns and dependencies typically overlooked by supervised models, which are often limited to specific distribution characteristics like conditional means and quantiles. The central thesis of this work is that by accurately modeling the data generation process, generative models not only improve the generalizability of supervised learning but also enhance the effectiveness and efficiency of the learning process.\nIn contrast to conventional models that are focused on specific tasks or modalities, GDP enables the integration of tabular data with unstructured modalities into a unified representation, employing a generative model such as diffusion, augmented with shared embedding mechanisms. By implementing transfer learning, we fine-tune the target model with a pre-trained source model, optimizing embedding sizes to boost accuracy for downstream tasks. This strategy harnesses knowledge from large-scale models trained on extensive datasets while being tailored to the specific requirements of multimodal integration.\nGDP presents several distinct advantages. First, it facilitates multimodal data prediction within a single generative modeling framework, enabling the generation of synthetic data that mirrors the original distribution while providing point predictions through risk minimization, adaptable to any loss function. Second, it accommodates mixed data types for supervised tasks by modeling the conditional distribution of variables of interest. This enables GDP to capture complex relationships across different modalities, significantly improving predictive performance and generalization. Third, it enhances robustness and generalizability by incorporating beneficial variability through synthetic data generation, empowering the model to handle new data more effectively. By integrating these features, GDP provides an effective solution for multimodal data integration and predictive modeling.\nThe contributions of this article are as follows:\n1. Generative Distribution Prediction for multimodal supervised learning: This paper introduces the Generative Distribution Prediction framework, an innovative approach to multimodal supervised learning that leverages advanced generative"}, {"title": "Generative distribution prediction", "content": "In multimodal supervised learning, the objective is to predict the outcomes of response variables (outputs) based on predictors (inputs) for new, unseen data by learning the underlying relationships from paired input-output data. A key challenge in this framework is domain adaptation, where a supervised model trained on the source domain must adapt to perform well in a different but related target domain. For example, a credit scoring model trained on data from a high-risk population adapts to a low-risk population where defaults are rare. This scenario, known as response shift, occurs when the conditional relationship between input features (covariates) and the response variable remains consistent across domains. Still, the distribution of the response variable differs between the source and target domains. Similarly, covariate shift arises when the marginal distribution of the covariates changes between domains, while the conditional distribution of the response given the covariates remains unchanged [43]. Domain adaptation also encompasses the standard case of supervised learning when the data distribution is consistent across both domains.\nTo predict the outcomes of response variables $Y_t$ given a new predictor value $X_t = x_{new}$, we consider the predictive probability $P_{Y_t|X_t=x_{new}}$, the conditional probability of $Y_t$ given $X_t = x_{new}$, with subscript t denoting the target task. Here $Y_t$ and $X_t$ can be tabular, unstructured such as text, or both. This probability, supported by decision theory [5], captures the direct relationship between $X_t$ and $Y_t$, ensuring accurate predictions by accounting for how different values of $X_t$ affect the likelihood of $Y_t$."}, {"title": "Generative distribution prediction", "content": "Generative Distribution Prediction (GDP) generates predictive distributions and corresponding point predictions by synthesizing data to replicate the conditional distribution of the response given the predictors. Although GDP is capable of providing uncertainty measures from these predictive distributions, this article focuses exclusively on point prediction. The GDP methodology involves two key steps:\nStep 1: Constructing a conditional generator for domain adaptation. Using transfer learning, we fine-tune a target generative model based on a pre-trained source model and the target training sample $D = \\{(x_i, y_i)\\}_{i=1}^n$, leveraging dual-level shared embeddings (DSE) as discussed in Section 3.1. In scenarios where transfer learning is not applicable, the conditional generator may be trained directly on $D$.\nStep 2: Using synthetic data for point prediction. The conditional generator produces a synthetic sample $D_{x_{new}} = \\{(\\tilde{y}_i, x_{new})\\}_{i=1}^m$, representing the responses $Y_t \\sim P_{Y_t|X_t}(Y_t|x_{new})$ corresponding to a given predictor value $x_{new}$, where $P_{Y_t|X_t}$ represents the generation distribution obtained in Step 1. To obtain a point prediction, we first construct an empirical prediction loss $m^{-1}\\sum_{i=1}^m l(\\theta(x_{new}), \\tilde{y}_i)$, where the loss function $l$ (for example, the squared loss) evaluates the prediction performance"}, {"title": "Theory: GDP's predictive risk", "content": "This subsection establishes a theoretical foundation linking generation error in synthetic data replication\u2014or estimation error in the global data distribution\u2014to local point predictions within the GDP methodology. It highlights the essential role of precise distribution estimation, achieved through high-fidelity data generation, in minimizing downstream risk within the GDP framework. By ensuring accurate synthetic data generation, the GDP approach facilitates robust predictions across diverse loss functions through risk minimization, as defined in (1). In essence, effectively estimating the data-generating distribution enhances performance across downstream prediction tasks. For instance, high-fidelity synthetic data replication leads to strong predictive performance under various loss functions, including absolute loss, hinge loss, and squared error loss, demonstrating the flexibility and effectiveness of the proposed approach.\nTo formalize this, the excessive risk $R(\\theta_0(x_t), \\theta(x_t))$ quantifies the risk associated with loss $l$ in (1):\n$R(\\theta_0(x_t), \\theta(x_t)) = \\mathbb{E}_{Y_t|x_t} [l(\\theta(x_t), Y_t) - l(\\theta_0(x_t), Y_t)],$   (2)\nwhere $\\mathbb{E}_{Y_t|x_t}$ is taken concerning the true conditional distribution of $Y_t$ given $x_t$. The Wasserstein-1 distance is used as a metric to assess the accuracy of global distribution estimation, defined as:\n$W(P,Q) = \\inf_{\\Upsilon \\in \\Gamma(P,Q)} \\int_{X \\times Y} ||x - y|| d\\gamma(x, y),$   (3)\nwhere $|| \\cdot ||$ is the Euclidean distance, and $\\Gamma(P, Q)$ is the set of all joint distributions (couplings) with marginals $P$ and $Q$.\nWe introduce the following regularity conditions on the loss function $l$ to connect global distribution estimation accuracy with local point predictions:\nAssumption 1 (Lipschitz over bounded domain). The loss $l$ satisfies the Lipschitz condition:\n$\\sup_{(x_t,Y_t)} |l(\\theta_1(x_t), Y_t) - l(\\theta_2(x_t), y_t)| \\leq \\beta||\\theta_1 - \\theta_2 ||,$   (4)\nand $\\sup_{x_t} \\sup_{\\theta(x_t) \\in \\Theta(x_t)} || \\theta(x_t) ||_{\\infty}\\leq c_b$, where $|| \\cdot ||_{\\infty}$ is the vector sup-norm, and $c_b$ and $\\beta$ are positive constants."}, {"title": "Multimodal learning and domain adaptation", "content": "This section focuses on diffusion models, though the GDP framework broadly applies to other generative models."}, {"title": "Generation via transfer learning with dual-level embedding", "content": "To enhance target generation, we leverage a source generation task, transferring knowledge from a pre-trained model to improve performance. The transfer process involves aligning the conditional probability distributions of the source $P(Y_s|X_s)$ and the target $P(Y_t|X_t)$ using the dual-level shared embeddings (DSE) framework. This alignment allows effective transfer even when the distributions of $(Y_s, X_s)$ and $(Y_t, X_t)$ differ, facilitating efficient generation with limited target domain data.\nThe concept of dual-level shared embeddings (DSE) between source and target tasks extends the shared embedding concept for transfer learning in [44].\nDual-level shared embeddings. Consider an encoder-decoder system, where an encoder $f$ maps a random vector $Y_i$ into a latent embedding vector $U_i$, potentially of different dimensionality. A decoder $g$ then reconstructs the original data from the embedding $U_i$, ensuring that $Y_i = g(U_i)$ for $j = s, t. The encoder and decoder act as transformations between the semantic latent space and the observed data space, which is especially important for unstructured data. In an image captioning task, the encoder and decoder translate captions to and from a numerical embedding space. Depending on the styles of captions and photos, the semantic representations of the images may vary in distribution.\nThe DSE model assumes the conditional distribution of $U_j$ given $X_j$, $P_{U_j|X_j}(*|X_j)$ to follow a shared structure across tasks through an embedding function $h$:\n$P_{U_j|X_j}(\\cdot|x_j) = P(\\cdot, h(x_j)), j = s,t.$   (7)\nThe DSE model in (7) captures the shared structure between source and target tasks. Based on (7), given the pre-estimated $(f, \\hat{g}, h)$ from the source task, we can learn the distribution of $U_t$ in the latent space. Let $\\theta_t$ parametrize the conditional generator and $P_t(u_t, h(x_t))$, We estimate the target-specific latent parameter by minimizing the target generation loss using the target samples $\\{(y_i, x_i)\\}_{i=1}^n$:\n$\\theta_t = \\arg \\min_{\\theta_t \\in \\Theta_t} \\sum_{i=1}^n l_t (f(y_i), h(x_i); \\theta_t),$   (8)\nwhere $\\Theta_t$ is the parameter space for $\\theta_t$, while $l_t$ is the generation loss such as the diffusion score matching loss defined in Section C of the appendix.\nGiven the learned parameters $(\\theta_t, h)$, the generator produces synthetic samples $(\\tilde{Y}, X_t)_{i=1}^m$ given $X_t$ via the decoder $\\hat{g}$. Specifically, it completes the generation process by computing $\\tilde{Y} = \\hat{g}(U)$. Here, we derive $\\hat{g}$ from the source task through shared structures and draw the samples $(\\tilde{U}, X_t)_{i=1}^m$ from the distribution $P_t(\\cdot; h(x_t), \\theta_t)$ given $X_t$. The distribution $P_t(\\cdot; \\hat{h}(X_t), \\hat{\\theta_t})$ denotes the generator's output distribution parameterized by t."}, {"title": "Unified diffusion generation for tabular and unstructured data", "content": "This subsection introduces a diffusion model for synthesizing multimodal samples by integrating structured tabular data with unstructured data (e.g., text and images) for conditional generation. While diffusion models have demonstrated remarkable performance in generating unstructured data [19, 41] and tabular data [24]- the literature has thus far treated these domains separately. To our knowledge, no prior work has combined these modalities under a unified diffusion framework. Our approach fills this gap by employing a conditional diffusion framework with shared covariate embeddings and shared response encoder-decoder (as discussed in Section 3.1), facilitating transfer learning and enabling smooth adaptation between source and target domains. Moreover, to effectively merge structured and unstructured data, we incorporate multiple embedding layers within the embedding function, with adjustable embedding sizes optimized for performance across various downstream tasks."}, {"title": "Numerical examples", "content": "This section rigorously evaluates GDP's performance in tabular and text prediction tasks by assessing its predictive accuracy and benchmarking it against state-of-the-art methods on benchmark datasets and simulated examples. We examine several variants of GDP, including diffusion-GDP (which incorporates diffusion generation with a statistical guarantee), GDP integrated with large pre-trained models, and transfer-GDP enhanced through transfer learning.\nOur evaluation encompasses four supervised tasks that yield tabular and text outputs using a combination of tabular, text, and image predictors. Specifically, Subsections 4.1-4.4 detail experiments: (i) domain adaptation using diffusion models for predicting Yelp reviews across source and target domains; (ii) image captioning with diffusion models; (iii) question answering utilizing a large language model; and (iv) adaptive quantile regression across multiple quantiles using diffusion models."}, {"title": "Applying diffusion-GDP for domain adaptation in rating prediction", "content": "This subsection evaluates GDP's predictive performance by leveraging a combination of tabular and unstructured predictors and compares it against several popular supervised methods. These include classical approaches such as logistic regression [13], Naive Bayes [36], Random Forest [6], and SVM [7], as well as the large language model BERT [15]. The evaluation considers two scenarios: source and target learning, which rely solely on source and target data, and domain adaptation. While BERT incorporates external data for domain adaptation during source and target learning, GDP enables effective domain adaptation for target learning by leveraging source data.\nTo assess these models, we use the Yelp reviews dataset (www.yelp.com/dataset), a widely recognized benchmark for sentiment analysis and star rating prediction. The"}, {"title": "Applying GDP with diffusion and large pre-trained models to image captioning", "content": "This subsection evaluates the performance of the GDP method within a multimodal framework for image captioning. Specifically, we integrate GDP with two distinct multimodal generators a diffusion model and the Bootstrapping Language-Image Pre-training (BLIP) model [25]-yielding the diffusion-GDP and BLIP-GDP variants, respectively. Notably, the diffusion-GDP approach is supported by the theoretical results presented in Theorems 1 and 2, whereas no corresponding theory currently exists for BLIP-GDP.\nModern image captioning typically employs an encoder-decoder framework. A Convolutional Neural Network (CNN) extracts image features then passes to a decoder-originally an LSTM model [45] and more recently a Transformer-to generate captions such as BLIP involving attention mechanisms.\nFor our experiments, we employ the captions_val2014 subset of the widely recognized COCO Caption dataset (available at http://coco-dataset.org).\nWe use the first 35,000 samples for training and reserve the remaining 5,504 samples for testing. The COCO Caption dataset is a standard benchmark for image captioning, featuring richly annotated images paired with human-generated captions. Specifically, we train our diffusion model on the designated training set, while we do not fine-tune BLIP since it was pre-trained on the entire COCO Caption dataset. We evaluate both methods on the 5,504 test samples.\nIn our multimodal diffusion model, as described in Section 3.2, we use Sentence-BERT (Sentence-Bidirectional Encoder Representations from Transformers) [35] to extract sentence embeddings and CLIP (Contrastive Language-Image Pretraining) [32] to align textual and visual modalities, together forming the encoder. We then apply a Gaussian diffusion model in the embedding space, leveraging a residual network [18] to enhance feature representation and maintain model stability. Finally, we use a pre-trained Vec2Text model [28] to decode the diffused vector embeddings to generate image captions.\nTo assess alignments between the generated and reference captions, we compute"}, {"title": "Applying large language model-GDP to Q&A", "content": "This subsection examines the GDP's performance in question-answering (Q&A) tasks, where both questions and their corresponding answers are textual. The analysis evaluates the accuracy of responses generated by three methods using the LLaMA-3.1-8B-Instruct model [1] within a Q&A framework, leveraging the widely recognized WikiQA dataset [48]. The WikiQA corpus is a benchmark dataset for open-domain Q&A tasks and consists of 3,047 questions sourced from user queries submitted to Bing, paired with 29,258 candidate answers extracted from Wikipedia sentences. Among these, 1,473 question-answer pairs are labeled as correct, indicating an alignment between the questions and their respective answers.\nIn this experiment, we randomly sample 100 question-answer pairs from the subset of pairs labeled as correct to serve as the test set. This sampling ensures that the evaluation focuses on instances with human-validated answers based on dataset annotations.\nTo evaluate the semantic similarity between two text answers, we compute the cosine similarity score between their sentence embeddings, following the same approach described in Section 4.2.\nWe evaluate three methods using the LLaMA-3.1-8B-Instruct model [1] to answer questions, with the temperature parameter controlling the randomness or diversity of the model's outputs. The default temperature is set to 0.7, balancing diversity and determinism. The first method is the GDP approach, which employs a cosine dissimilarity loss (1 minus the cosine similarity score) applied to answers generated by the LLaMA model, as defined in (1) with m = 50. This generative approach selects the answer that minimizes the dissimilarity loss averaged over m = 50 generated answers for the same question, prioritizing semantic similarity to the correct answer. The second method is the deterministic approach, which uses the LLaMA model with a temperature setting of 0. This configuration ensures that the model always returns the same answer for a given question, emphasizing consistency and accuracy in output. The third method is another generative approach using the default temperature setting of 0.7 without minimizing a loss function over m = 50 candidates. Unlike the GDP approach, it generates a single answer (m = 1) for each question, which can vary slightly in semantic meaning across iterations, introducing diversity in the outputs.\nAs shown in Figure 4, the mean similarity scores for the deterministic, generative, and GDP methods on the test set are 0.7670, 0.7590, and 0.7846, respectively, while the corresponding median values are 0.7811, 0.7781, and 0.8118. These results indicate that the GDP method enhances the performance of the generative model.\nThis systematic comparison underscores the impact of the synthetic sample size m (m = 50 vs m = 1) and the inherent trade-offs between accuracy and diversity. When generating a single answer, the deterministic approach often surpasses the generative approach (using the default temperature) by prioritizing accuracy. In contrast, GDP effectively manages this trade-off by optimizing the final output from a diverse yet comparable set of candidate answers. This approach balances accuracy and diversity, aligning with the discussion on the impacts of synthetic sample size for GDP in Theorem 1."}, {"title": "Diffusion-GDP for adaptive quantile regression", "content": "This experiment considers nonparametric quantile regression via simulations to evaluate the performance of diffusion-GDP for a prediction task, where the true data-generating mechanism is known. Specifically, let $X = (X_1, ..., X_p)^T$ be a vector of predictors, and define the response $Y$ as\n$Y = sin(X^T \\beta) + log(1+|X_1|) + \\epsilon (1 + |X_2|),$   (9)\nwhere $X^T$ denotes the transpose of $X$, and $\\beta$ is the vector of regression coefficients generated uniformly in (-1, 1). To induce heteroscedasticity, the noise term $\\epsilon$ follows a normal distribution with mean 0 and variance $|X_2|$, that is, $\\epsilon \\sim N (0, |X_2|)$. Hence, the variance is modulated by the magnitude of $X_2$. This setup naturally creates a challenging scenario for regression, incorporating both nonlinear and heteroscedastic behavior. Two marginal distributions for $X$ are considered:\n* Case I: $X \\sim N(0, I)$, where $I$ is the identity matrix.\n* Case II: $X \\sim N(0, \\Sigma)$, where the covariance matrix $\\Sigma$ is defined by $\\Sigma_{ij} = \\rho^{|i-j|}$ with $\\rho = -0.5$. Here $\\Sigma = (\\sigma_{ij})_{pxp}$ follows an AR(1) dependence structure and 1 is the common variance of each $X_i$ and $\\rho$ ($|\\rho| < 1$) is the lag-1 correlation between two observations $X_i$ and $X_{i+1}$.\nIn this study, we focus on adapting to multiple quantiles by evaluating the predictive"}, {"title": "Discussion", "content": "The proposed Generative Distribution Prediction (GDP) framework offers a unified approach to multimodal learning by integrating diverse data modalities-tabular, textual, and image-through generative models and shared embeddings. By leveraging transfer learning and conditional generation techniques, GDP directly models the conditional distributions of the response given predictors, enhancing both predictive and generative accuracy. This approach ensures that the generated outputs closely approximate the true data distributions, which improves uncertainty quantification and addresses challenges in domain adaptation and heterogeneous data integration.\nExperimental results across various tasks\u2014including Yelp review predictions, image captioning, question answering, and adaptive quantile regression\u2014demonstrate that GDP outperforms traditional models and state-of-the-art methods while exhibiting remarkable robustness and adaptability. Its high generative accuracy is evident in its ability to capture fine-grained data structures and produce outputs that closely mirror real-world distributions, ultimately reducing prediction errors.\nTheoretical foundations of the GDP method, established for diffusion models, provide statistical guarantees and scalability for various applications. Notably, recent work [44] suggests that these theoretical results may be extendable to normalizing flows. However, establishing statistical guarantees for other generative models such as BLIP remains an open question. This limitation underscores the need for further theoretical exploration to understand GDP's performance benefits across different generative approaches.\nFuture research could focus on improving computational efficiency and expanding the applicability of GDP, further solidifying its role as a valuable tool for advancing multimodal analytics and deepening our understanding of generative modeling in complex data environments.\nGDP's theoretical underpinnings on diffusion models validate its effectiveness, offering statistical guarantees and scalability for diverse applications. Its versatility underscores significant real-world potential. Future efforts could improve efficiency and applicability, positioning GDP as a valuable tool for advancing multimodal analytics across fields, particularly for statistical theory when GDP working with other generative models."}, {"title": "Appendix", "content": ""}, {"title": "Proofs", "content": "Proof of Theorem 1. Let $Y_t$ denote the synthetic sample from $P_{Y_t|x_t}$ and define the corresponding excessive risk based on $Y_t$ as $R(\\theta_0, \\theta)$. By Assumption 1 and the duality theorem of Kantorovich and Rubinstein [20], the following bounds hold:\n$|R(\\theta_0,\\theta) - \\hat{R}(\\theta_0, \\theta)| \\leq \\beta W(P_{Y_t|x_t}, P_{\\tilde{y}_t|x_t}),$\n$| \\text{Var}_{Y_t|x_t} (l(\\theta, Y_t) - l(\\theta_0, Y_t)) - \\text{Var}_{\\tilde{y}_t|x_t} (l(\\theta, \\tilde{Y}_t) - l(\\theta_0, \\tilde{Y}_t))| \\leq 4c_v\\beta W(P_{Y_t|x_t}, P_{\\tilde{y}_t|x_t}).$\nBy Assumption 2,\n$\\text{Var}_{\\tilde{y}_t, x_t}(l(\\theta, \\tilde{Y}_t) - l(\\theta_0, \\tilde{Y}_t)) \\leq \\text{Var}_{Y_t|x_t} (l(\\theta, Y_t) - l(\\theta_0, Y_t)) + 4c_v\\beta W,$\n$\\leq c_vR(\\theta_0, \\theta) + 4c_v\\beta W,$\n$\\leq c_v\\hat{R}(\\theta_0, \\theta) + 2c_v(c_v + 2\\beta)W.$\nSetting $c_e = 1 + \\beta$ yields that $\\{R(\\theta_0, \\theta) \\geq \\epsilon_m + c_eW\\} \\subset \\{\\hat{R}(\\theta_0, \\theta) \\geq \\epsilon_m + W\\}.$\nHence,\n$P_{D_n} (R(\\theta_0, \\theta) \\geq \\epsilon_m + c_eW) \\leq P_{D_n} (\\hat{R}(\\theta_0, \\theta) \\geq \\epsilon_m + W)$\n$\\leq P_{D_n} (\\sup_{\\theta \\in \\Theta} |\\mathbb{E}_m(L_m(\\theta) - L_m(\\theta_0))| > 2^j (\\epsilon_m + W)) := \\sum_{j=0}^{\\infty} I_j, \\text{(11)}$\nwhere $L_m(\\theta) = m^{-1} \\sum_{k=1}^m l(\\theta,\\tilde{y}_k; x_t)$ and $A_j = \\{\\theta : 2^j (\\epsilon_m + W) < \\hat{R}(\\theta_0, \\theta) \\leq 2^{j+1}(\\epsilon_m + W)\\}$.\nNext, we bound each term $I_j$ ($j = 0, \\cdot\\cdot\\cdot ,$) in (11) by Lemma 1 separately. Toward"}, {"title": "Experiment details", "content": ""}, {"title": "Experiment settings for Section 4.1", "content": "For the machine learning methods used in this experiment, including logistic regression, Naive Bayes, Random Forest, and SVM, we utilize the standard implementation provided by [31] to expand to text predictors. The text data is preprocessed using scikit-learn's TfidfVectorizer, which converts the text into numerical features by analyzing words and bigrams, removing stop words, and filtering out rare terms with a minimum document frequency of 5. The machine learning models follow the configurations outlined in [27].\nFor the BERT-based classification model, we employ the simpletransformers"}, {"title": "Experiment settings for Section 4.2", "content": "In the image-captioning experiment, we employ a conditional diffusion model to encode multimodal data\u2014including images and text data-into a shared latent embedding space for caption generation, as described in Section 3.2.\nOur approach deviates from standard implementations in several key ways: (1) it uniquely integrates CLIP and SentenceBERT for multimodal alignment, (2) it employs a residual network for reverse diffusion instead of the more commonly used U-Net [37], and (3) it utilizes vec2text to decode denoised embeddings into captions. While building on existing principles, this method makes it a distinctive contribution to multimodal text generation.\nTo represent image-caption pairs in this unified space, we use SentenceBERT and CLIP: SentenceBERT (GTR-T5-Base, huggingface.co/sentence-transformers/gtr-t5-base) captures rich linguistic semantics from textual descriptions, while CLIP (CLIP-ViT-B-32, huggingface.co/sentence-transformers/clip-ViT-B-32) extracts aligned image and text embeddings. All embeddings are standardized to a common dimensionality using trainable projection layers for alignment and then fused through concatenation. By combining SentenceBERT and CLIP, this approach enhances caption quality by refining textual representations beyond what CLIP alone can achieve.\nBuilding on these joint embeddings, we apply a Gaussian diffusion process for conditional caption generation, iteratively injecting and refining noise in the text embeddings based on the given image. We model the reverse diffusion process by a residual network, progressively removing noise while conditioning the image, ensuring the generated caption remains visually relevant. Specifically, the diffusion model inputs a noisy text embedding from GTR-T5-Base, a condition embedding from CLIP-ViT-B-32, and a timestep indicator. These inputs are projected into a unified hidden space of 2048 dimensions, with the timestep further encoded into a time embedding through a two-layer MLP, also with a 2048-dimensional hidden layer."}, {"title": "Experiment settings for Section 4.3", "content": "The details have been provided in Section 4.3 and are thus omitted."}, {"title": "Experiment settings for Section 4.4", "content": "For quantile regression, we employ XGBoost with the quantile loss objective. Hyperparameter tuning is conducted using GridSearchCV with 3-fold cross-validation. The search space includes the following parameters: learning rate (0.01, 0.04, 0.1), maximum tree depth (3, 5, 7), and number of estimators (50, 100, 150). The best hyperparameters are selected based on the pinball loss metric. Once tuning is complete, the optimal model is used to generate predictions on the test set.\nFor the DQR model, we utilize a ReLU network, with 3-fold cross-validation to determine the optimal network width (64, 128) and network depth (3, 5, 10). During training, the Adam optimizer is used with a fixed learning rate of 1 \u00d7 10-4 for a maximum of 200 epochs. The model is trained with a batch size of 512, and early stopping is applied using the validation set, with a maximum patience of 20 epochs.\nFor the GDP method, the neural network architecture for the conditional diffusion models consists of two components: a ReLU network for condition embedding and a ReLU network for score matching, both set to the same network width. Additionally, we employ a sinusoidal time embedding to project the timestep into a vector. The diffusion model operates over 1000 timesteps, ranging from 1 \u00d7 10-4 to 0.02. Hyperparameter tuning is performed using 3-fold cross-validation, optimizing the condition embedding dimension (64, 128), time embedding dimension (16, 32, 64), network width (64, 128, 256), and network depth (3, 5). The training strategy follows the same procedure as the DQR model."}, {"title": "Diffusion models and generation accuracy theory", "content": ""}, {"title": "Diffusion modeling", "content": "Recent advancements in generative modeling provide compelling solutions to challenges in supervised learning by harnessing their robust data synthesis capabilities. In particular", "U(0)$": "n$dU(\\tau) = -b_{\\tau"}, "U(\\tau)d\\tau + \\sqrt{2b_{\\tau}}dW(\\tau), \\quad \\tau \\geq 0,$   (15)\nwhere $U(\\tau)$ has a probability density $p_{u(\\tau)}$, $\\{W(\\tau)\\}_{\\tau>0}$ represents a standard Wiener process and $b_\\tau$ is a non-decreasing weight function. Under (15), $U(\\tau)$ given $U(0)$ follows $\\mathcal{N}(\\mu_\\tau U(0), \\sigma_\\tau^2 I)$, where $\\mu_\\tau = \\exp(- \\int_0^\\tau b_s ds)$ and $\\sigma_\\tau^2 = 1 - \\mu_\\tau^2$. Here, we set $b_\\tau = 1$, which results in $\\mu_\\tau = \\exp(-\\tau)$ and $\\sigma_\\tau^2 = 1 - \\exp(-2\\tau)$. Practically, the process terminates at a sufficiently large $\\tau$, ensuring the distribution of $U(\\tau)$, a mixture of $U(0)$ and white noise, resembles the standard Gaussian vector.\nBackward process. Given $U(\\tau)$ in (15), a backward process is employed for sample generation for $U(0)$. Assuming (15) satisfies certain conditions [2"], "as": "n$dV"}