{"title": "DutyTTE: Deciphering Uncertainty in Origin-Destination Travel Time Estimation", "authors": ["Xiaowei Mao", "Yan Lin", "Shengnan Guo", "Yubin Chen", "Xingyu Xian", "Haomin Wen", "Qisen Xu", "Youfang Lin", "Huaiyu Wan"], "abstract": "Uncertainty quantification in travel time estimation (TTE) aims to estimate the confidence interval for travel time, given the origin (O), destination (D), and departure time (T). Accurately quantifying this uncertainty requires generating the most likely path and assessing travel time uncertainty along the path. This involves two main challenges: 1) Predicting a path that aligns with the ground truth, and 2) modeling the impact of travel time in each segment on overall uncertainty under varying conditions. We propose DutyTTE to address these challenges. For the first challenge, we introduce a deep reinforcement learning method to improve alignment between the predicted path and the ground truth, providing more accurate travel time information from road segments to improve TTE. For the second challenge, we propose a mixture of experts guided uncertainty quantification mechanism to better capture travel time uncertainty for each segment under varying contexts. Additionally, we calibrate our results using Hoeffding's upper-confidence bound to provide statistical guarantees for the estimated confidence intervals. Extensive experiments on two real-world datasets demonstrate the superiority of our proposed method.", "sections": [{"title": "Introduction", "content": "Origin-destination (OD) travel time estimation (TTE) refers to estimating the travel time \\( \\Delta t \\) from the origin (O) to the destination (D) starting from the departure time (T). In many real-world applications, providing just an average estimate of travel time is inadequate. A more reliable and informative approach includes quantifying travel time uncertainty. Specifically, we aim to estimate the confidence interval for travel time with a specified confidence level to quantify uncertainty, which is beneficial in many scenarios. For example, ride-hailing services can benefit from providing customers with lower and upper confidence bounds of travel time, allowing them to better plan their schedules (Liu et al. 2023). Moreover, understanding travel time uncertainty can help ride-hailing and logistics platforms improve decision-making effectiveness, such as in order dispatching and vehicle routing (Xu et al. 2018; Liu et al. 2023; Fu et al. 2020). Existing studies for OD TTE (Yuan et al. 2020a; Lin et al. 2023; Wang et al. 2023) primarily focus on point estimates of travel time and do not address uncertainty quantification.\nQuantifying origin-destination travel time uncertainty involves two key tasks. First, a path between OD that matches the ground truth needs to be predicted, as the accuracy of the path is closely related to the accuracy of the quantified uncertainty in TTE. Second, the travel time uncertainty in each segment along the predicted path needs to be quantified so that the travel time for the whole trip can be aggregated (Lin et al. 2023; Wang et al. 2023; Shi et al. 2024). Effectively accomplishing these tasks faces several challenges.\nFirst, predicting a path that aligns with the ground truth is challenging. Given an OD input, incorrect predictions at multiple steps during path prediction can lead to significant divergence between the predicted and actual paths, especially for long paths with distant OD pairs. Such errors can accumulate, making the predicted path diverge from the actual path, which greatly hurts the accuracy of the quantified uncertainty in TTE. Traditional methods for path prediction typically involve modeling the weights of road segments, which are then integrated into search-based algorithms (Jain et al. 2021) to generate a path step by step. More recent approaches (Shi et al. 2024) use deep neural networks to learn transition patterns among roads and generate paths autoregressively. These methods face challenges in predicting paths that closely match the ground truth due to limited solutions for mitigating error accumulation during the path prediction process.\nSecond, effectively modeling the impact of travel time in each road segment on overall travel time uncertainty under varying conditions is challenging. Travel time in a segment varies across different trips and affects overall travel time uncertainty differently. As illustrated in Figure 1, the travel time of \\( e_6 \\) in trip 1, which starts at 8:00, is relatively certain since the statistical times during 7:50-8:00 are concentrated around 30 seconds. However, during 8:10-8:20, travel times of \\( e_6 \\) become more varied, increasing the uncertainty for trip 2 which starts at 8:20. The varying travel time uncertainty in each segment, influenced by complex factors like traffic conditions and departure times, makes it difficult to accurately quantify overall travel time uncertainty.\nTo address these challenges, we propose DutyTTE for Deciphering Uncertainty in origin-destination Travel Time Estimation. First, to reduce error accumulation in path prediction, we optimize path prediction by enhancing the overall alignment between the predicted path and the ground truth, rather than just maximizing the likelihood of each segment independently. We introduce a Deep Reinforcement Learning (DRL) approach to optimize objectives that measure the overall similarity between the predicted path and the ground truth. Second, we propose a Mixture of Experts guided Uncertainty Quantification mechanism (MoEUQ) to model the impact of travel time in each road segment on overall travel time uncertainty. This mechanism adaptively selects the most representative experts to process each road segment, handling its travel time uncertainty and its impact in different contexts. This allows for better discrimination and capture of each road segment's impact on overall travel time uncertainty. Finally, to provide statistical guarantees for the estimated intervals, we calibrate these intervals by computing a calibration parameter using Hoeffding's upper-confidence bound.\nOur contributions are summarized as follows:\n\u2022 We propose an OD travel time uncertainty quantification method that accurately predicts paths and provides confidence intervals with statistical guarantees.\n\u2022 We introduce a DRL method to enhance the alignment between the predicted path and the ground truth, facilitating accurate travel time uncertainty quantification.\n\u2022 We present a mixture of experts guided uncertainty quantification mechanism to model the impact of travel time in each road segment on overall travel time uncertainty. The estimated confidence intervals are calibrated to ensure statistical guarantees.\n\u2022 Extensive experiments on two real-world datasets show that our proposed method outperforms other solutions."}, {"title": "Related Work", "content": "Travel Time Estimation. TTE solutions are classified into two types: for paths and for OD input.\nPath-based TTE methods like WDR (Wang, Fu, and Ye 2018a), DeepTTE (Wang et al. 2018), TADNM (Xu et al. 2020), and ProbTTE (Liu et al. 2023) use deep learning to model spatio-temporal correlations in traveled segments. DeepGTT (Li et al. 2019) uses a variational model for TTE. GNNs and attention mechanism are used to capture spatial-temporal dependencies in road networks for TTE (Fang et al. 2020; Hong et al. 2020; Derrow-Pinion et al. 2021; Fu et al. 2020; Jin et al. 2022; Chen et al. 2022; Yuan, Li, and Bao 2022). ProbTTE (Xu, Wang, and Sun 2024) models trip-level link travel time with a Gaussian hierarchical model.\nOD-based TTE methods, like TEMP (Wang et al. 2019), ST-NN (Jindal et al. 2017), MURAT (Li et al. 2018), DeepOD (Yuan et al. 2020b) mainly take ODT as input to model the correlations between ODT and travel times. DOT (Lin et al. 2023) develops a diffusion model for TTE using pixelated trajectories. MWSL-TTE (Wang et al. 2023) proposes to search a potential path first, and sums the estimated travel times of segments. However, these methods face limitations in modeling the complex correlations between OD pairs and travel time under varying traffic conditions, and they lack the capability to effectively model travel time uncertainty in road segments.\nUncertainty Quantification in Deep Learning. Methods for quantifying uncertainty in deep learning can be broadly classified into Bayesian and Frequentist approaches (Wu et al. 2021). Bayesian methods (Izmailov et al. 2021; Kendall and Gal 2017) capture uncertainty by assigning probabilistic distributions to model parameters, considering variations in these parameters as measures of uncertainty. Non-Bayesian methods, such as ensemble-based approaches (Lakshminarayanan, Pritzel, and Blundell 2017; Liu et al. 2020) and Brownian Motion-based methods (Kong, Sun, and Zhang 2020), model the randomness of the learning process in various tasks. MIS-Regression (Gneiting and Raftery 2007) and Quantile Regression (Gasthaus et al. 2019) estimate uncertainty intervals by modifying the output layers without relying on likelihood functions. These uncertainty quantification methods could be combined with TTE methods for OD input to quantify uncertainty in travel time. However, such combinations fall short of effectively modeling the intricate correlations between OD pairs and travel time uncertainty under complex conditions, and cannot offer statistical guarantees for the estimated uncertainty of travel time."}, {"title": "Preliminaries", "content": "In this section, we provide the relevant definitions and formalize the problem of uncertainty quantification in origin-destination travel time estimation.\nDefinition 1. Road Network. A road network is defined as the aggregate of all road segments in a studied area or a city. It is modeled as a directed graph \\( G = (V,E) \\), where \\( V \\) is a set of nodes \\( v_i \\) representing road intersections or segment ends, and \\( E \\) is a set of edges \\( e_i \\) representing road segments. Each node in the network has a unique index.\nDefinition 2. Trips and Paths. A trajectory \\( T \\) is a sequence of GPS points with timestamps:\n\\[T = \\{(g_1, c_1),..., (g_{|T|},c_{|T|})\\}, \\text{where} g_i = (lng_i, lat_i), i = 1,...,T \\text{denotes i-th GPS point, and } |T| \\text{ denotes the total number of GPS points in the trajectory.} \\]\nAfter map-matching using OpenStreetMap, the trajectory of a trip is\n\\[x_T = \\{(v_1, c_1), \\cdots, (v_k, c_k)\\}, \\text{with the time index } c \\text{ monotonically increasing. The total travel time of the trip is } y = c_k - c_1. \\]\nNote that multiple GPS data points can be located on the same road segment. A path is defined as a sequence of nodes \\( x = (v_1, ..., v_n) \\), where each pair of nodes is adjacent, i.e., \\( \\forall i = 0, 1,\u00b7\u00b7\u00b7, |X|, (v_i, v_{i+1}) \\in E \\).\nProblem Statement. We aim to learn a mapping function \\( f_{\\theta} \\) to estimate the upper and lower confidence bounds for travel time based on an ODT input, denoted as \\( \\hat{l} \\) and \\( \\hat{u} \\), which quantifies the uncertainty of the travel time. Such that \\( [\\hat{l}, \\hat{u}] \\) is a confidence interval that covers the actual travel time \\( \\tau \\) with a confidence level of \\( 1 - p \\). We denote the ODT input as \\( q = (g_o, g_d, c_o) \\), where \\( g_o, g_d \\), and \\( c_o \\) are the GPS points of the origin, destination, and departure time, respectively."}, {"title": "Methodology", "content": "The framework of the proposed method is shown in Figure 2. In this section, we first formulate path prediction from the DRL perspective. Second, we elaborate on enhancing the overall alignment between the predicted path and the ground truth using policy-gradient optimization. Third, we introduce the MoE guided uncertainty quantification to estimate confidence intervals of travel time. Finally, we elaborate the calibration of the estimated intervals."}, {"title": "Path prediction from the DRL perspective", "content": "To mitigate error accumulation in path prediction, we aim to optimize overall alignment objectives between the predicted path \\( \\hat{x} \\) and the ground truth \\( x \\). However, overall alignment measures like Longest Common Subsequence (LCS) and Dynamic Time Warping (DTW) are non-differentiable when used as loss functions due to the arg max operation involved in obtaining the predicted path. To address this, we introduce a deep reinforcement learning (DRL) method to enhance path prediction by improving overall alignment.\nPath prediction is modeled as a finite-horizon discounted Markov Decision Process (MDP), where an agent, over T discrete time steps, engages with the environment by making actions for path generation. An MDP is defined as \\( M = (S, A, P, R, s_0, \\gamma) \\), with S being the set of states and A being the set of possible actions. Next, \\( P : S \\times A \\times S \\rightarrow \\mathbb{R}^+ \\) is a transition probability function and \\( R : S \\times A \\rightarrow \\mathbb{R} \\) is a reward function. Finally, \\( s_0 : S \\rightarrow \\mathbb{R}^+ \\) is the initial state distribution and \\( \\gamma \\in [0, 1] \\) is a discount factor.\nGiven a state \\( s_t \\) at time t, the agent uses the policy \\( \\pi_{\\theta} \\) to make sequential decisions regarding the next node to generate. The agent then receives a reward \\( r_t \\). Next, the reward is used to update the policy network. The objective is to learn the optimal parameters \\( \\theta^* \\) for the policy network to maximize the expected cumulative reward:\n\\[\\theta^* = \\arg \\max_{\\pi_{\\theta}} \\mathbb{E}_{\\pi_{\\theta}} [\\sum_{t=1}^{T} \\gamma^t r_t], \\text{where } \\gamma \\text{ controls the tradeoffs between the importance of immediate and future rewards.} \\]\nNext, we describe agent, state, action, reward, and state transition in detail.\nAgent. The agent takes actions according to the policy based on the states and receives rewards to learn an optimal policy that maximizes the cumulative reward. The actions are determined by a policy network. The policy network is responsible for learning the mapping function \\( f_{ep} \\) from an ODT to a path. In our formulation, the policy network is mainly composed of a transformer and a prediction head. The transformer generates the representation of the predicted path. Next, the prediction head takes the representation of the predicted path, traffic conditions, distance and direction to the destination as input, then it outputs the distribution of generating the next node.\nState. The state at time step t, denoted as \\( s_t \\in S \\), encodes context information necessary to derive a policy at each time step and is given by \\( s_t = (h_t, x_{1:t-1}, w_t, dist, dir_t) \\). Here, \\( h_t \\) denotes the representation of the predicted path, \\( x_{1:t-1} \\) is the previously predicted path, \\( w_t \\) denotes traffic conditions, dist is the distance from the current node to the destination, and \\( dir_t \\) is the direction from the current node to the destination at the t-th time step.\nAction. An action \\( a_t \\in A_t \\) represents choosing the next node to generate in the path. If there are T nodes in a path, the sequence \\( (a_1, a_2\u2026\u2026, a_T) \\in A_1 \\times A_2 \\times \\cdots \\times A_T \\) represents a predicted path.\nState transition probability. The transition probability function \\( P(s_{t+1}|s_t,a_t) \\): \\( S \\times A \\times S \\rightarrow \\mathbb{R}^+ \\) captures the probability of transitioning from \\( s_t \\) to state \\( s_{t+1} \\) when action \\( a_t \\) is taken. In our problem, the environment is deterministic, meaning there is no uncertainty in the state transition. Therefore, state \\( s_{t+1} \\) is deterministic.\nReward. The reward is defined by the objectives of enhancing the overall alignment between the predicted path and the ground truth and is detailed in the following section."}, {"title": "Policy-gradient optimization for path prediction", "content": "Maximize the expected cumulative reward can be converted to a loss function defined as the negative expected reward of the predicted path: \\( L_e = -\\mathbb{E}_{\\pi_{\\theta}}[r(a_1,\\cdots,a_T)] \\), where \\( \\{a_1,\\cdots,a_T\\} \\sim \\pi_{\\theta} \\) are sampled from the policy \\( \\pi_{\\theta} \\) and \\( r(a_1,\\cdots, a_T) \\) represents the reward. This reward is defined by overall alignment objectives, utilizing metrics such as DTW and LCS. DTW calculates the cost to align nodes of the predicted path with the ground truth in a way that minimizes the total distance. While LCS identifies the longest subsequence common to both the predicted path and the ground truth. Generating paths with lower DTW and higher LCS values indicates greater similarity to the ground truth. Formally, the reward function calculated using the predicted path \\( \\hat{x} \\) and the ground truth \\( x \\) is formulated as follows:\n\\[r(a_1,\\ldots, a_T) = (\\omega * LCS(\\hat{x}, x) - \\beta * DTW(\\hat{x}, x)),\\tag{1}\\]\nwhere \\( \\omega, \\) and \\( \\beta \\) are hyper-parameters that control the scale of the reward. We can approximate the expected reward of the predicted path using N sampled paths from the policy \\( \\pi_{\\theta} \\). The derivative of loss function \\( L_e \\) is formulated as follows:\n\\[\\nabla_{\\pi_{\\theta}} L_e \\approx \\frac{1}{N} \\sum_{i=1}^{N} [\\nabla_{\\pi_{\\theta}}\\log\\pi_{\\theta} (a_{i,1},\\ldots,a_{i,T})r(a_{i,1},\\cdots, a_{i,T})]\tag{2}\\]\n\\[\\pi_{\\theta} \\sum_{\\pi_{\\theta}} \\frac{1}{N} \\sum_{i=1}^{N} [\\nabla_{\\pi_{\\theta}}\\log \\pi_{\\theta}(a_{i,1},\\ldots,a_{i,T})\\times (r(a_{i,1},\\ldots, a_{i,T}) - r(\\bar{a}_{i,1},\\ldots,\\bar{a}_{i,T}))]\tag{3}\\]\nwhere \\( a_{i,t} \\) is the arg max value of the output distribution for the next node from the t-th step of the i-th sample. If a sampled sequence is better than the model's output at testing time, the probability of observing that sample is increased; if the sample is worse, the probability is decreased.\nInspired by (Hughes, Chang, and Zhang 2019; Paulus, Xiong, and Socher 2017), we also use Maximum-Likelihood Estimation (MLE) objective in path prediction to assist the policy learning algorithm to achieve better prediction. Formally, the joint loss function is formulated as follows:\n\\[\\mathcal{L}_{o} = \\gamma \\mathcal{L}_{\\pi_{\\theta}} + \\mathcal{L}_{CE},\\tag{4}\\]\nwhere \\( \\gamma \\) is a hyper-parameter. \\( \\mathcal{L}_{CE} \\) is defined by the MLE objective in path prediction (Shi et al. 2024)."}, {"title": "MoE guided uncertainty quantification", "content": "We propose a MoEUQ (Mixture of Experts for Uncertainty Quantification) model to capture the varying degrees of travel time uncertainty in each segment under different conditions. This segment-level uncertainty is then aggregated to estimate confidence intervals of overall travel time.\nFirst, we take the discretized travel time distribution of segments in the predicted path as input, such as\n\\[u = \\{(d_1, f_1),\\ldots,(d_m, f_m)\\} \\text{for segment } e_j.\\]\nWe select the top-m largest possible travel times in a period before departure time \\( c_o \\). Here, \\( d_m \\) is the m-th largest travel time and \\( f_m \\) is the associated probability. We concatenate u with the embeddings of road segments initialized by\nNode2Vec (Grover and Leskovec 2016) and the time slice of the departure time, then encode them to obtain the embedding \\( r_j \\) of the j-th segment. Next, we utilize a LSTM (Hochreiter and Schmidhuber 1997) to model the mutual correlations among segments. The j-th encoded segment in the predicted path is denoted as \\( r'_j \\), which contains travel time information and aggregated spatio-temporal information of other segments in a path.\nTo better capture travel time uncertainty in each segment, we introduce the Mixture of Experts (MoE) approach (Shazeer et al. 2017). This method utilizes different combinations of experts to represent each segment based on its aggregated spatio-temporal information, effectively handling varying degrees of travel time uncertainty for the segment under different conditions. Specifically, given the j-th encoded segment \\( r'_j \\), the output of the MoE layer is:\n\\[r^{'}_j = \\sum_{i=1}^{n} G(r^{'})_iE_i(r^{'}_j),\\tag{5}\\]\nwhere \\( E_i(r^{'}_j) \\) denotes the output of the i-th expert network, and \\( G(r^{'})_i \\) denotes the gating network given \\( r^{'} \\). There are a total n expert networks with separate parameters, and the output of the gating network is a n-dimensional vector.\nTo prevent segments from being represented by the same set of experts and to explore better combinations of experts under varying conditions, we implement noisy top-k gating before applying the softmax function in the gating network. Each segment is routed to the most representative experts, guided by the gating network, to distinguish travel time uncertainty in different conditions. The noisy top-k gating for the j-th segment in a predicted path is formulated as:\n\\[G(r^{'}_j) = \\text{Softmax(TopK(H(}r^{'}_j), k))\\]\\[H(r^{'}_j) = (r_jW_g) + \\mathcal{N}(0, 1). Softplus((r^{'}_j . W_{noise}))\\]\\[\\text{TopK}(z, k)_i = \\begin{cases} z_i \\quad \\text{if } z_i \\text{ is in the top k elements of z} \\\\ -\\infty \\quad \\text{otherwise} \\end{cases}\\]\nNext, to estimate the confidence intervals of travel time in the path, we sum along the sequence length dimension based on the output of the MoE layer. Then, we use separate prediction heads to estimate the lower and upper bounds of travel time, supervised by the Mean Interval Score\n(MIS) (Gneiting and Raftery 2007). For a confidence level of 1 - \\( \\rho \\), the predicted upper and lower bounds of the travel time for the i-th sample given an ODT query q are defined by \\( \\hat{u}_i = \\hat{y}_i + \\delta_i \\), and \\( \\hat{l}_i = \\hat{y}_i - \\delta_i \\), where \\( \\hat{u}_i \\) and \\( \\hat{l}_i \\) are the (1-\\( \\rho \\)) and quantiles for the 1 - \\( \\rho \\) confidence interval and \\( \\hat{y}_i \\) is the point estimation of travel time, respectively. The MIS loss is formulated as follows:\n\\[L_{MIS} = \\frac{1}{N} \\sum_{i=1}^{N} [(\\hat{u}_i - \\hat{l}_i) + \\frac{2}{\\rho} (y_i - \\hat{u}_i)I\\{y_i > \\hat{u}_i\\} + \\frac{2}{\\rho} (\\hat{l}_i - y_i)I\\{y_i < \\hat{l}_i\\} + \\frac{4}{\\rho} |y_i - \\hat{y}_i|^2]\tag{9}\\]"}, {"title": "Calibrating estimated confidence intervals", "content": "We have formulated the method to obtain the estimated lower and upper bounds for travel time. However, it remains uncertain whether the estimated interval will contain the ground truth with the desired probability during testing. To address this, we further calibrate the estimated intervals by scaling them on a held-out dataset until they encompass the desired fraction of the ground truth travel times, as inspired by (Angelopoulos et al. 2022). Specifically, Given a held-out set of calibration data, \\( \\{(q_i, y_i)\\}_{i=1}^{M} \\), we build intervals that contain at least 1 - \\( \\alpha \\) of the ground truth values with probability 1 - \\( \\delta \\). Formally, with probability at least 1 - \\( \\delta \\),\n\\[\\mathbb{E}[\\frac{1}{M} |\\{ (i): y_{test}^{(i)} \\in \\mathcal{T}(q_{test})^{(i)} \\}|] \\geq 1 - \\alpha,\tag{10}\\]\nwhere \\( \\alpha \\in (0,1) \\), and \\( \\delta \\in (0,1) \\) represent the desired risk level and error level, respectively. \\( q_{test}^{test}, y_{test}^{test} \\) denotes test data sampled from the same distribution as the calibration data, \\( \\mathcal{T}(q_{test})^{(i)} \\) is an interval-valued function for the i-th sample in the test set, which is formulated as: \\( \\mathcal{T}(q)^{(i)} = [\\hat{y}_i - \\delta, \\hat{y}_i + \\delta] \\).\nTo realize the statistical guarantee defined in Equation 10, the interval-valued function \\( \\mathcal{T}(q) \\) needs to satisfy:\n\\[\\mathbb{P} \\left(\\mathbb{E} [\\mathcal{L}(\\mathcal{T}(q), y)] > \\alpha \\right) \\leq \\delta,\\tag{11}\\]\nwhere\n\\[\\mathcal{L}(\\mathcal{T}(q), y) = 1 - \\frac{|\\{ (i): y_{(i)} \\in \\mathcal{T}(q)^{(i)}\\}|}{M}\\tag{12}\\]\nThe inner expectation in Equation 11 is calculated using data from the test set. The outer probability is calculated using data from the calibration set.\nNext, we elaborate how to derive \\( \\mathcal{T}(s) \\) to satisfy Inequality 11 for statistical guarantee of the estimated intervals. Specifically, we introduce a calibrator \\( \\lambda \\) and formulate the calibrated interval as follows:\n\\[\\mathcal{T}_{\\lambda}(q)^{(i)} = [\\hat{y}_i - \\lambda \\delta, \\hat{y}_i + \\lambda \\delta]\tag{13}\\]\nIn Equation 13, when \\( \\lambda \\) increases, the intervals expand. If \\( \\lambda \\) is sufficiently large, the intervals will encompass all the ground truth travel time values. We aim to calculate \\( \\lambda \\) to be the smallest value that such that \\( \\mathcal{T}_{\\lambda}(q) \\) satisfies the Inequality 11. To achieve this, we introduce the Hoeffding bound (Hoeffding 1994), which holds the property that:\n\\( \\mathbb{P} [\\mathcal{R}^{+}(\\lambda) < \\mathcal{R}(\\lambda)] < \\delta \\), and can be transformed to the form of Inequality 11. Specifically, let \\( \\mathcal{R}(\\lambda) = \\mathbb{E} [\\mathcal{L}(\\mathcal{T}_{\\lambda}(q), y)] \\), and the Hoeffding's upper-confidence bound \\( \\mathcal{R}^{+}(\\lambda) \\) is formulated as:\n\\[\\mathcal{R}^{+}(\\lambda) = \\frac{1}{M} \\sum_{i=1}^{M} \\mathcal{L}(\\mathcal{T}_{\\lambda}(q_i), y_i) + \\sqrt{\\frac{1}{2M} \\log \\frac{1}{\\delta}}\\tag{14}\\]\nNext, we use \\( \\mathcal{R}^{+}(\\lambda) \\) to calculate the smallest \\( \\lambda \\) that satisfies Inequality 11 based on the held-out dataset. The closed-form expression for this process is formulated as:\n\\[\\hat{\\lambda} = \\min \\{ \\lambda : \\mathcal{R}^{+}(\\lambda^{'}) \\leq \\alpha, \\lambda^{'} \\geq \\lambda \\}\tag{15}\\]\nWith \\( \\lambda \\) calculated in Euqation 15, we can ensure that \\( \\mathcal{T} \\) satisfies Inequality 11 (Bates et al. 2021). Thus, the calibrated intervals are statistically guaranteed to contain the ground truth with the desired probability. Details about the calibration algorithm are illustrated in the Appendix."}, {"title": "Experiments", "content": "Datasets\nIn our experiments, we utilize two real-world taxi trajectory datasets collected from from Didi Chuxing\u00b9."}, {"title": "MoE guided uncertainty quantification", "content": "We propose a MoEUQ (Mixture of Experts for Uncertainty Quantification) model to capture the varying degrees of travel time uncertainty in each segment under different conditions. This segment-level uncertainty is then aggregated to estimate confidence intervals of overall travel time."}, {"title": "Efficiency Study", "content": "We evaluate the average execution time per batch, consisting of 128 samples from City A and City B, for efficiency validation, as shown in Figure 6. DutyTTE uses a DRL method for training; during prediction, the policy network allows for fast inferences. Given DutyTTE's good performance, we see this as a promising trade-off between efficiency and effectiveness."}, {"title": "Conclusion", "content": "We introduce DutyTTE for quantifying uncertainty in OD travel times. Using a DRL approach, we optimize path prediction to align closely with the ground truth and enhance uncertainty quantification. To capture travel time uncertainty across road segments, we propose a mixture of experts mechanism that models the impact of segments in complex contexts. We also calibrate confidence intervals to ensure statistical guarantees. Experiments on two datasets validate DutyTTE's effectiveness in path prediction, travel time uncertainty quantification."}]}