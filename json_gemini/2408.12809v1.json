{"title": "DutyTTE: Deciphering Uncertainty in Origin-Destination Travel Time Estimation", "authors": ["Xiaowei Mao", "Yan Lin", "Shengnan Guo", "Yubin Chen", "Xingyu Xian", "Haomin Wen", "Qisen Xu", "Youfang Lin", "Huaiyu Wan"], "abstract": "Uncertainty quantification in travel time estimation (TTE) aims to estimate the confidence interval for travel time, given the origin (O), destination (D), and departure time (T). Accurately quantifying this uncertainty requires generating the most likely path and assessing travel time uncertainty along the path. This involves two main challenges: 1) Predicting a path that aligns with the ground truth, and 2) modeling the impact of travel time in each segment on overall uncertainty under varying conditions. We propose DutyTTE to address these challenges. For the first challenge, we introduce a deep reinforcement learning method to improve alignment between the predicted path and the ground truth, providing more accurate travel time information from road segments to improve TTE. For the second challenge, we propose a mixture of experts guided uncertainty quantification mechanism to better capture travel time uncertainty for each segment under varying contexts. Additionally, we calibrate our results using Hoeffding's upper-confidence bound to provide statistical guarantees for the estimated confidence intervals. Extensive experiments on two real-world datasets demonstrate the superiority of our proposed method.", "sections": [{"title": "Introduction", "content": "Origin-destination (OD) travel time estimation (TTE) refers to estimating the travel time \\( \\Delta t \\) from the origin (O) to the destination (D) starting from the departure time (T). In many real-world applications, providing just an average estimate of travel time is inadequate. A more reliable and informative approach includes quantifying travel time uncertainty. Specifically, we aim to estimate the confidence interval for travel time with a specified confidence level to quantify uncertainty, which is beneficial in many scenarios. For example, ride-hailing services can benefit from providing customers with lower and upper confidence bounds of travel time, allowing them to better plan their schedules (Liu et al. 2023). Moreover, understanding travel time uncertainty can help ride-hailing and logistics platforms improve decision-making effectiveness, such as in order dispatching and vehicle routing (Xu et al. 2018; Liu et al. 2023; Fu et al. 2020). Existing studies for OD TTE (Yuan et al. 2020a; Lin et al. 2023; Wang et al. 2023) primarily focus on point estimates of travel time and do not address uncertainty quantification.\nQuantifying origin-destination travel time uncertainty involves two key tasks. First, a path between OD that matches the ground truth needs to be predicted, as the accuracy of the path is closely related to the accuracy of the quantified uncertainty in TTE. Second, the travel time uncertainty in each segment along the predicted path needs to be quantified so that the travel time for the whole trip can be aggregated (Lin et al. 2023; Wang et al. 2023; Shi et al. 2024). Effectively accomplishing these tasks faces several challenges.\nFirst, predicting a path that aligns with the ground truth is challenging. Given an OD input, incorrect predictions at multiple steps during path prediction can lead to significant divergence between the predicted and actual paths, especially for long paths with distant OD pairs. Such errors can accumulate, making the predicted path diverge from the actual path, which greatly hurts the accuracy of the quantified uncertainty in TTE. Traditional methods for path prediction typically involve modeling the weights of road segments, which are then integrated into search-based algorithms (Jain et al. 2021) to generate a path step by step. More recent approaches (Shi et al. 2024) use deep neural networks to learn transition patterns among roads and generate paths autoregressively. These methods face challenges in predicting paths that closely match the ground truth due to limited solutions for mitigating error accumulation during"}, {"title": "Path prediction from the DRL perspective", "content": "To mitigate error accumulation in path prediction, we aim to optimize overall alignment objectives between the predicted path \\( \\hat{x} \\) and the ground truth \\( x \\). However, overall alignment measures like Longest Common Subsequence (LCS) and Dynamic Time Warping (DTW) are non-differentiable when used as loss functions due to the arg max operation involved in obtaining the predicted path. To address this, we introduce a deep reinforcement learning (DRL) method to enhance path prediction by improving overall alignment.\nPath prediction is modeled as a finite-horizon discounted Markov Decision Process (MDP), where an agent, over T discrete time steps, engages with the environment by making actions for path generation. An MDP is defined as \\( M = (S, A, P, R, s_0, \\gamma) \\), with S being the set of states and A being the set of possible actions. Next, \\( P : S \\times A \\times S \\rightarrow R^+ \\) is a transition probability function and \\( R : S \\times A \\rightarrow R \\) is a reward function. Finally, \\( s_0 : S \\rightarrow R^+ \\) is the initial state distribution and \\( \\gamma \\in [0, 1] \\) is a discount factor.\nGiven a state \\( s_t \\) at time t, the agent uses the policy \\( \\pi_\\theta \\) to make sequential decisions regarding the next node to generate. The agent then receives a reward \\( r_t \\). Next, the reward is used to update the policy network. The objective is to learn the optimal parameters \\( \\theta^* \\) for the policy network to maximize the expected cumulative reward:\n\\( \\theta^* = \\arg \\max_\\theta E_{\\pi_\\theta} [\\sum_{t=1}^T \\gamma^t r_t] \\), where \\( \\gamma \\) controls the tradeoffs between the importance of immediate and future rewards. Next, we describe agent, state, action, reward, and state transition in detail.\nAgent. The agent takes actions according to the policy based on the states and receives rewards to learn an optimal policy that maximizes the cumulative reward. The actions are determined by a policy network. The policy network is responsible for learning the mapping function \\( f_{ep} \\) from an ODT to a path. In our formulation, the policy network is mainly composed of a transformer and a prediction head. The transformer generates the representation of the predicted path. Next, the prediction head takes the representation of the predicted path, traffic conditions, distance and direction to the destination as input, then it outputs the distribution of generating the next node.\nState. The state at time step t, denoted as \\( s_t \\in S \\), encodes context information necessary to derive a policy at each time step and is given by \\( s_t = (h_t, x_{1:t-1}, w_t, dist, dir_t) \\). Here, \\( h_t \\) denotes the representation of the predicted path, \\( x_{1:t-1} \\) is the previously predicted path, \\( w_t \\) denotes traffic conditions, \\( dist \\) is the distance from the current node to the destination, and \\( dir_t \\) is the direction from the current node to the destination at the t-th time step.\nAction. An action \\( a_t \\in A_t \\) represents choosing the next node to generate in the path. If there are T nodes in a path, the sequence \\( (a_1, a_2 \\ldots, a_T) \\in A_1 \\times A_2 \\times \\cdots \\times A_T \\) represents a predicted path.\nState transition probability. The transition probability function \\( P(s_{t+1} | s_t, a_t) : S \\times A \\times S \\rightarrow R^+ \\) captures the probability of transitioning from \\( s_t \\) to state \\( s_{t+1} \\) when action \\( a_t \\) is taken. In our problem, the environment is deterministic, meaning there is no uncertainty in the state transition. Therefore, state \\( s_{t+1} \\) is deterministic.\nReward. The reward is defined by the objectives of enhancing the overall alignment between the predicted path and the ground truth and is detailed in the following section."}, {"title": "Policy-gradient optimization for path prediction", "content": "Maximize the expected cumulative reward can be converted to a loss function defined as the negative expected reward of the predicted path: \\( L_e = -E_{\\pi_\\theta}[r(a_1, \\ldots, a_T)] \\), where \\( \\{a_1, \\ldots, a_T\\} \\sim \\pi_\\theta \\) are sampled from the policy \\( \\pi_\\theta \\) and \\( r(a_1, \\ldots, a_T) \\) represents the reward. This reward is defined by overall alignment objectives, utilizing metrics such as DTW and LCS. DTW calculates the cost to align nodes of the predicted path with the ground truth in a way that minimizes the total distance. While LCS identifies the longest subsequence common to both the predicted path and the ground truth. Generating paths with lower DTW and higher LCS values indicates greater similarity to the ground truth. Formally, the reward function calculated using the predicted path \\( \\hat{x} \\) and the ground truth \\( x \\) is formulated as follows:\n\\( r(a_1, \\ldots, a_T) = (\\omega * LCS(\\hat{x}, x) - \\beta * DTW(\\hat{x}, x)), \\)\nwhere \\( \\omega, \\) and \\( \\beta \\) are hyper-parameters that control the scale of the reward. We can approximate the expected reward of the predicted path using N sampled paths from the policy \\( \\pi_\\theta \\). The derivative of loss function \\( L_e \\) is formulated as follows:\n\\( \\nabla_{\\pi_\\theta} L_e = \\frac{1}{N} \\sum_{i=1}^N [\\nabla_{\\pi_\\theta} \\log \\pi_\\theta(a_{i,1}, \\ldots, a_{i,T}) r(a_{i,1}, \\ldots, a_{i,T})] \\)"}, {"title": "MoE guided uncertainty quantification", "content": "We propose a MoEUQ (Mixture of Experts for Uncertainty Quantification) model to capture the varying degrees of travel time uncertainty in each segment under different conditions. This segment-level uncertainty is then aggregated to estimate confidence intervals of overall travel time.\nFirst, we take the discretized travel time distribution of segments in the predicted path as input, such as \\( u = (d_1, f_1, \\ldots, d_m, f_m) \\) for segment \\( e_j \\). We select the top-m largest possible travel times in a period before departure time \\( c_0 \\). Here, \\( d_m \\) is the m-th largest travel time and \\( f_m \\) is the associated probability. We concatenate u with the embeddings of road segments initialized by Node2Vec (Grover and Leskovec 2016) and the time slice of the departure time, then encode them to obtain the embedding \\( r_j \\) of the j-th segment. Next, we utilize a LSTM (Hochreiter and Schmidhuber 1997) to model the mutual correlations among segments. The j-th encoded segment in the predicted path is denoted as \\( r'_j \\), which contains travel time information and aggregated spatio-temporal information of other segments in a path.\nTo better capture travel time uncertainty in each segment, we introduce the Mixture of Experts (MoE) approach (Shazeer et al. 2017). This method utilizes different combinations of experts to represent each segment based on its aggregated spatio-temporal information, effectively handling varying degrees of travel time uncertainty for the segment under different conditions. Specifically, given the j-th encoded segment \\( r'_j \\), the output of the MoE layer is:\n\\( r^*_j = \\sum_{i=1}^n G_i(r'_j) E_i(r'_j), \\)"}, {"title": "Calibrating estimated confidence intervals", "content": "We have formulated the method to obtain the estimated lower and upper bounds for travel time. However, it remains uncertain whether the estimated interval will contain the ground truth with the desired probability during testing. To address this, we further calibrate the estimated intervals by scaling them on a held-out dataset until they encompass the desired fraction of the ground truth travel times, as inspired by (Angelopoulos et al. 2022). Specifically, Given a held-out set of calibration data, \\( \\{(q_i, y_i)\\}_{i=1}^M \\), we build intervals that contain at least \\( 1-\\alpha \\) of the ground truth values with probability \\( 1-\\delta \\). Formally, with probability at least \\( 1-\\delta \\),\n\\( P \\left\\{ \\frac{1}{M} \\sum_{i=1}^M \\mathbb{I} \\left[ y^{(i)}_{test} \\in T(q^{(i)}_{test}) \\right] \\geq 1 - \\alpha \\right\\}, \\)\nwhere \\( \\alpha \\in (0,1) \\), and \\( \\delta \\in (0,1) \\) represent the desired risk level and error level, respectively. \\( q^{(i)}_{test}, y^{(i)}_{test} \\) denotes test data sampled from the same distribution as the calibration data, \\( T(q^{(i)}_{test}) \\) is an interval-valued function for the i-th sample in the test set, which is formulated as: \\( T(q^{(i)}) = [\\hat{y}_i - \\lambda \\hat{\\sigma}_l, \\hat{y}_i + \\lambda \\hat{\\sigma}_u] \\).\nTo realize the statistical guarantee defined in Equation 10, the interval-valued function T(q) needs to satisfy:\n\\( P \\left( E[L(T(q), y)] > \\alpha \\right) < \\delta, \\)\nwhere\n\\( L(T(q), y) = 1 - \\frac{\\{i: y_{(i)} \\in T(q)_{(i)} \\}}{M}. \\)\nThe inner expectation in Equation 11 is calculated using data from the test set. The outer probability is calculated using data from the calibration set.\nNext, we elaborate how to derive T(s) to satisfy Inequality 11 for statistical guarantee of the estimated intervals. Specifically, we introduce a calibrator \\( \\lambda \\) and formulate the calibrated interval as follows:\n\\( T_{\\lambda}(q)_{(i)} = [\\hat{y}_i - \\lambda \\hat{\\sigma}_l, \\hat{y}_i + \\lambda \\hat{\\sigma}_u] \\)\nIn Equation 13, when \\( \\lambda \\) increases, the intervals expand. If \\( \\lambda \\) is sufficiently large, the intervals will encompass all the ground truth travel time values. We aim to calculate \\( \\lambda \\) to be the smallest value that such that \\( T_{\\lambda}(q) \\) satisfies the Inequality 11. To achieve this, we introduce the Hoeffding bound (Hoeffding 1994), which holds the property that:\n\\( P [\\hat{R}^{+}(\\lambda) < \\hat{R}(\\lambda)] < \\delta \\), and can be transformed to the form of Inequality 11. Specifically, let \\( \\hat{R}(\\lambda) = E[L(T_{\\lambda}(q), y)] \\), and the Hoeffding's upper-confidence bound \\( \\hat{R}^{+}(\\lambda) \\) is formulated as:\n\\( \\hat{R}^{+}(\\lambda) = \\frac{1}{M} \\sum_{i=1}^M L(T_{\\lambda}(q_i), y_i) + \\sqrt{\\frac{1}{2M} \\log \\frac{1}{\\delta}}. \\)\nNext, we use \\( \\hat{R}^{+}(\\lambda) \\) to calculate the smallest \\( \\hat{\\lambda} \\) that satisfies Inequality 11 based on the held-out dataset. The closed-form expression for this process is formulated as:\n\\( \\hat{\\lambda} = \\min \\{ \\lambda: \\hat{R}^{+}(\\lambda') \\leq \\alpha, \\lambda' \\geq 1 \\} \\)\nWith \\( \\lambda \\) calculated in Euqation 15, we can ensure that T satisfies Inequality 11 (Bates et al. 2021). Thus, the calibrated intervals are statistically guaranteed to contain the ground truth with the desired probability. Details about the calibration algorithm are illustrated in the Appendix."}, {"title": "Experiments", "content": "In our experiments, we utilize two real-world taxi trajectory datasets collected from from Didi Chuxing."}, {"title": "Ablation Study", "content": "To verify the effectiveness of components, we conduct an ablation study with the following DutyTTE variants: 1) w/o-M: replacing the mixture of experts guided uncertainty quantification mechanism with a LSTM. 2) w/o-P: removing the policy loss and using only cross-entropy loss in path prediction. 3) w/o-C: eliminating the calibration of predicted confidence intervals."}, {"title": "Efficiency Study", "content": "We evaluate the average execution time per batch, consisting of 128 samples from City A and City B, for efficiency validation, as shown in Figure 6. DutyTTE uses a DRL method for training; during prediction, the policy network allows for fast inferences. Given DutyTTE's good performance, we see this as a promising trade-off between efficiency and effectiveness."}, {"title": "Conclusion", "content": "We introduce DutyTTE for quantifying uncertainty in OD travel times. Using a DRL approach, we optimize path prediction to align closely with the ground truth and enhance uncertainty quantification. To capture travel time uncertainty across road segments, we propose a mixture of experts mechanism that models the impact of segments in complex contexts. We also calibrate confidence intervals to ensure statistical guarantees. Experiments on two datasets validate DutyTTE's effectiveness in path prediction, travel time uncertainty quantification."}]}