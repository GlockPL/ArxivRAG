{"title": "Incorporation of Verifier Functionality in the Software for Operations and Network Attack Results\nReview and the Autonomous Penetration Testing System", "authors": ["Jordan Milbrath", "Jeremy Straub"], "abstract": "The software for operations and network attack results review (SONARR) and the autonomous\npenetration testing system (APTS) use facts and common properties in digital twin networks to\nrepresent real-world entities. However, in some cases fact values will change regularly, making it\ndifficult for objects in SONARR and APTS to consistently and accurately represent their real-world\ncounterparts. This paper proposes and evaluates the addition of verifiers, which check real-world\nconditions and update network facts, to SONARR. This inclusion allows SONARR to retrieve fact values\nfrom its executing environment and update its network, providing a consistent method of ensuring that\nthe operations and, therefore, the results align with the real-world systems being assessed. Verifiers\nallow arbitrary scripts and dynamic arguments to be added to normal SONARR operations. This provides\na layer of flexibility and consistency that results in more reliable output from the software.", "sections": [{"title": "1. Introduction", "content": "The software for operations and network attack results review (SONARR) is designed to perform security\nassessment on mission critical systems, which cannot be taken offline or risk damage from conventional\npenetration testing [1]. The autonomous penetration testing system (APTS) builds on the SONARR\narchitecture to actually conduct automatic penetration testing [2].\n\nVerifiers are used within the SONARR system to update networks' fact values to ensure that they\ncorrespond with the real-world entities that they represent. As is typical with the concept of digital\ntwins [3], the purpose of SONARR network facts is to mirror the real-world entities that they represent.\nBeing able to update facts in real-time means that the network will be able to provide relevant data and\ninteract with live systems in a way that is both accurate and effective. Verifiers are associated with facts\nto verify single facts, or they can be associated with common properties to provide a more general\nverification capability.\n\nVerifiers work by running a script that retrieves the real-world value that corresponds with a given fact.\nFor example, if there is a fact that represents whether or not a computer is connected to the internet,\nthen a script would be run that checks this and then returns true or false, accordingly. The result of this\nscript is returned to the software, the facts is updated, and network processing operations continue as\nnormal. Verifiers prevent the network from significantly diverging from the real-world entities it was\ndesigned to represent and draw conclusions about."}, {"title": null, "content": "Without verifiers, SONARR has no way to determine if its representation of facts is accurate, as\ncompared to the real-world entities that they are designed to represent. SONARR does not have the\nability to retrieve any information from its environment, which may be changing, or may start in a state\nthat was not anticipated. Verifiers cause SONARR to adapt to its environment and allow it to provide\ncorrect results that account for unforeseen changes and dynamic environments.\n\nThis paper introduces and evaluates the addition of verifiers to SONARR and APTS. This paper continues\nwith Section 2, which discusses prior work on which the current work builds. Then, Section 3 describes\nthe changes that were made to SONARR to support verifiers in their current state and then talks about\nhow they function conceptually. Following this, Section 4 provides more details on how verifiers work in\nthe current implementation of SONARR, using an example to explain. Section 5 describes and analyzes\ntesting that was completed to evaluate the efficacy of verifiers. It also provides detail about the practical\nbenefits of including verifiers in SONARR. Finally, the paper concludes with Section 6 which describes the\nsteps needed to further integrate verifiers into the SONARR infrastructure and some areas that could\nmake the use of verifiers even more helpful for users."}, {"title": "2. Background", "content": "This section discusses areas of prior work related to the addition of verifiers to SONARR. First, the\nBlackboard Architecture is reviewed. Then, work related to discovery in automated penetration testing\nis discussed."}, {"title": "2.1 Blackboard Architecture", "content": "In the 1960s and 1970s, expert systems were introduced with the creation of Dendral [4] and Mycin [5],\ntwo systems that used rule-fact structures for decision making. The Blackboard Architecture was first\nestablished in 1985 by Hayes-Roth [6]. Notably it contributed new ideas to the control flow of previous\nexpert systems. The Blackboard Architecture, more recently, has added the concept of actions, which\nallow the system to directly influence its environment [7]. In addition to this, key concepts related to\ngeneric-ification and reuse of rules and separating logical and physical / organizational relationships\n(including common properties, containers, and links) have been added to the functionality of the\nBlackboard Architecture [8], [9].\n\nThe Blackboard Architecture's adaptability allows it to be used in many application areas, and its expert-\nsystem roots makes it a highly explainable form of artificial intelligence (AI). The technology has been\nused for work with unmanned system failure detection [10], medical image interpretation [11],\nterrorism countering [12], and legal decision making [13], where detailed reporting of the Al's inner\nworkings is required. Other areas that have made use of the Blackboard Architecture include software\ntesting [14], cybersecurity attack modeling [15], recognizing handwriting [16], sound identification [17],\ncreating mathematical proofs [18], and even generating poetry [19]."}, {"title": "2.2 Discovery in Penetration Testing", "content": "Penetration testing involves approaching a system from the perspective of an attacker in an attempt to\nfind vulnerabilities in that system that could be exploited [20]. The National Institute of Standards and\nTechnology (NIST) has defined a four-stage penetration testing methodology [21]. The steps involved in\nthis methodology are planning, discovery, attack, and reporting. During the discovery step, penetration\ntesters collect information about the system they are testing. This step is crucial because it provides"}, {"title": null, "content": "information that can be used in the attack and reporting steps. One key part of this discovery phase is\nscanning. With passive scanning, data is collected without system interaction. Active scanning, on the\nother hand, requires the system to be acted upon and information to be collected from it and returned\nto the penetration tester [22]. The information that is retrieved from a scan will then be used during the\nattack phase to decide on attack techniques that will be used and to supply arguments for attacks that\nare run [21].\n\nIn the context of automated penetration testing tools, the discovery phase uses automated scanning\ntools to collect information and return it to the tool, similar to how the penetration tester would\ntypically collect data [23]. Having this data is critical for the operations of the automated penetration\ntesting tool for the same reasons that it is critical for manual penetration testers: it allows them to\nconfigure their attacks to the environment that they operate in. Penetration testers (and the automated\nsystem) can use this information to specifically create attacks and respond to the environment as it is\ninfluenced [24].\n\nThe current implementation of APTS does not have any discovery capabilities, so it skips this step during\nits operations. Verifiers work to fill this gap and allow the system adapt to its environment, thus filling\nthe role that is required for the tool to be comprehensive and meet the standards set by NIST and other\norganizations that have worked with automated penetration testing tools in the past."}, {"title": "3. Additions Made to SONARR to Support Verifiers", "content": "In order for verifiers to be added to SONARR, several modifications were required. First, a verifier class\nneeded to be added. This class includes several properties that specify how verifiers are executed. In\norder for a verifier process to execute, a file name must be provided and arguments may be provided. A\nverifier stores the executable path, which is provided as the process' file name. This value should\ncorrespond with the script that it runs. For example, if the verifier works through a PowerShell script,\nthen the executable path should be \u2018PowerShell'. If it uses a Python script, it should be 'Python'. This\nvalue is plugged directly into the ProcessStartInfo object, so it is necessary for the system to be able to\nfind the executable through the specified value. Thus, one should be able to open up a terminal, type in\nthe executable path, and have the process execute.\n\nThe next property addition is the format string field. The format string specifies the format in which the\narguments should be presented in the verifier command. In many cases, it is necessary for a specific\nscript to be run. This script name will be specified as one of the arguments. How it is presented to the\nmain executable as an argument may different depending on that executable. Some formats may need\n\"-File\" or \"-Script\" before the script name, while others may not need to specify an argument name at all\nand simply place the path of the script in that location. Having the format string as a property allows the\nformat to be configurable for each individual verifier and specified specifically for each executable that\nwill need to parse it.\n\nAn array of format arguments is used with each format string. These arguments specify the values that\nwill be retrieved from a fact that is being verified. These arguments are based on the specific SONARR\nimplementation and can be set to any arbitrary value. The verifier will look for a custom property whose\ncommon property has a description identical to the specified format argument. The description of that\ncustom property will be the string that is used in the arguments for the executable. A concrete example\nis now described to clarify and describe this logic in more detail."}, {"title": null, "content": "For example, a container object could be used to represent a computer. That container has a fact that\nindicates whether or not the computer is connected to the internet. In order to verify that it is\nconnected to the internet, the computer pings an IP address. If the ping is successful, it concludes that it\nis connected to the internet: however, if the ping is unsuccessful, it concludes that it is not connected to\nthe internet. A potential argument for this situation is the IP address that the computer pings, or its\n\"ping target\". Figure 1 shows the relationships and necessary information for a verifier to generate\narguments for this ping request."}, {"title": null, "content": "If a verifier has a format argument of \"Ping Target\" and is given the \"Fact 1\", as shown in Figure 1, then\nthe verifier will look at the fact's custom properties. In this case, there is only one. It finds the custom\nproperty whose common property has a description of \u201cPing Target\" and retrieves the description of\nthat custom property, which in this example is \u201c8.8.8.8\u201d. When the verifier generates its argument\nstring, it will use the value of \"8.8.8.8\u201d where the \"Ping Target\" should be placed.\n\nThere is one special case where the format argument is not retrieved from a fact in this way. This is\nwhen the format argument is \u201cDescription\u201d. When that is the case, instead of looking at the custom\nproperties, the description of the fact is returned. This is relevant in scenarios where the description of a\nfact is necessary for the script that is retrieving its value.\n\nA small test network of three contains was generated to demonstrate verifiers. It is shown in Figure 2."}, {"title": null, "content": "A single verifier script was created; however, depending on the environment and what that script\nreturns, the results of the run may be different. As shown in the diagram above, container 1's \"Has\nIntermediate Key\" fact is set to true. A rule exists indicating that if this fact is true, traversal may\nproceed to container 2. The same logic applies to container 2 and its goal key. Initially, however, the\n\"Has Goal Key\u201d fact value is set to false. This relationship is similar to needing a key to move on to the\nnext room in a hallway. If one has a key, they may proceed; otherwise, they must stop."}, {"title": "4. Current Implementation", "content": "The current implementation of verifiers was designed as a proof of concept to display their functionality.\nIt only supports a fixed set of verifiers, but it does so in a way that allows the easy interchanging of these\nverifiers for testing purposes. The executable path and format string are hardcoded and cannot be\nreadily changed by the user, but they can easily be changed by a testing developer. The fact, custom\nproperty, and common property values can be readily changed by users through the SONARR interface,\nand these changes may affect network traversal.\n\nAn example container with verifier arguments is shown in Figure 3. The example verifier's executable\npath is \"PowerShell\", and its format string is:\n\n-ExecutionPolicy Bypass -File \u201cVerifier1.ps1\u201d \u201c{0}\u201d \u201c{1}\u201d"}, {"title": null, "content": "The format arguments, for this verifier, are \u201cDescription\u201d and \u201cTarget\u201d. Using the format string above,\nthe value retrieved for \u201cDescription\" will go in place of {0}, and \u201cTarget\u201d will go in place of {1}. Fact 5's\ndescription is used as the value for \"Description\u201d as described previously. To determine the value for\n\"Target\", the custom properties for Fact 5 are examined. There is a custom property that has a common\nproperty with the description \u201cTarget\u201d, so the description of that custom property is used for the second\nargument. In this case, that value is \"Correct Goal Key\". Given this, the command that will be run, as a\nnew process is:\n\nPowerShell -ExecutionPolicy Bypass -File \u201cVerifier1.ps1\u201d \u201cHas Goal Key\u201d \u201cCorrect Goal Key\"\n\nWhatever is returned from this process run is parsed as a bool and the value of Fact 5 is set to this. The\nexample verifier script is shown in Listing 1."}, {"title": null, "content": "param([Parameter (Mandatory=$true)] $folderName,\n[Parameter (Mandatory=$true)] $targetKeyValue)\n\n$targetFolderFullName = Join-Path -Path $PSScriptRoot -ChildPath $folderName\n$childItems = Get-ChildItem -Path $targetFolderFullName\n$targetFileContent = Get-Content -Path $childItems[0].FullName\n\nreturn $targetFileContent -ceq $targetKeyValue\n\nFor testing purposes, this verifier script checks the content of the first file within the specified folder. If\nthe contents of that file match the target value, it returns true; otherwise, it returns false.\n\nThe file that the verifier script checks is on the system that runs the script, and it is not influenced by\nSONARR traversal processing. It is only read from. To test the efficacy of the implementation, traversals\nwere run.\n\nThe first run was done with the key file's contents set to \"Correct Goal Key\", which allows the traversal\nto continue to the goal node. During this run, the verifier PowerShell script was run and returned \u201cTrue\u201d,\nallowing a path to be generated from container 1 to container 3.\n\nFor the second run, the key file's contents were set to \"Not Correct Goal Key\u201d. This traversal did not\nresult in any paths being found, and debug information indicates that the PowerShell script returned a\nvalue of \"False\", thus stopping traversal.\n\nIn addition to demonstrating the correct functionality of the verifier software implementation, this\nshows that using verifiers, traversal may verify fact values that are dependent on environmental or\nexternal variables. This will affect the results of traversal, according to the external state and values\nretrieved."}, {"title": "5. Testing, Analysis, and Impacts", "content": "This section describes the testing process that was used to analyze the performance of verifiers and\ntheir impact on the overall function of SONARR. The testing process and speed impact is described and\nanalyzed in Section 5.1. Then, the implications of varying verifier timing are assessed in Section 5.2.\nFinally, the practicality of verifiers and some potential use cases are described in Section 5.3."}, {"title": "5.1. Performance Impact", "content": "Testing was conducted to determine the impact of verifiers on the performance of the SONARR and\nAPTS systems. The system was tested under three different scenarios. First, the system was run with no\nverifiers. Then, it was run using the PowerShell script verifier described in Section 4. Finally, he system\nwas run using a Windows batch script verifier. There were 1,000 testing iteration completed for each\nscenario, with 3,000 iterations completed total. A single testing iteration involved a full APTS traversal of\nthe network as described in Section 4. Each testing iteration was timed, and the average time elapsed\nfor each experimental condition is shown in Figure 5. This testing demonstrates the impact of a very\nbasic verifier to demonstrate the computational performance it would have. Most actual verifiers would\nhave a more pronounced impact on speed, due to their comparatively greater functionality and, thus,\ngreater processing and other requirements."}, {"title": null, "content": "The testing indicated that including verifiers in APTS traversal processing increases the time that it takes\nto complete that traversal. In the case of a batch file, which is a relatively low-level type of script, it took\nan average of 1,538,582 ticks to complete, which is 1,124,490 more than the average when no verifier\nwas used. This means that including a batch verifier increased the overall traversal time by\napproximately 271%. While batch verifiers are useful in many scenarios for simple operations,\nPowerShell or similarly more complex scripts may be more practical to use to verify fact values. During\nthe tests, it was found that, on average, runs that included PowerShell verifiers took 872% longer to\ncomplete than those without any verifiers.\n\nPart of the reason for the large increase in time when running verifiers is due to the need to create a\nnew process to run the verifier. Creating a new process takes a notable amount of time, relative to the\nrest of the operations that are completed during traversal. As shown by the difference between the\nbatch verifier and the PowerShell verifier's times, the method of verification also impacts the time\nelapsed. PowerShell scripts offer more capabilities under many scenarios, and they provide easier ways\nto complete these complex tasks; however, they take notably longer than batch files to run.\n\nThis speed impact is important to consider when creating verifiers for networks. Depending on the\ndesired speed of traversal, processing the verifier scripts and the executables that those scripts use to\noperate may need to be optimized. Optimization of verifier scripts may help to ensure that APTS\ntraversals run smoothly and in as little time as possible. Keeping verifiers as light-weight as possible will\nhelp to ensure that they complete their processing in as little time as possible."}, {"title": "5.2. Accuracy", "content": "The APTS system, using the verifiers, verified the desired fact values accurately 100% of the time, during\ntesting. This was to be expected, due to the nature of the verifiers used.\n\nThe impact that using verifiers has on traversal should be taken into consideration while developing a\nnetwork and scenario for that network to operate in. While not using verifiers significantly reduces the\ntime spent, even for the lightweight scripts such as batch scripts, in scenarios where the environment is\nnot completely controlled, fact values may not always be in the state that the system expects them to\nbe in. Including verifiers in traversals allows operators to ensure that the system is producing correct"}, {"title": null, "content": "results (that parallel the state of the target system). Verifiers also help the system to be more adaptable\nin scenarios where the environment may change frequently. Ensuring that the APTS network is updated\nconsistently allows the results of each run to be accurate at the time of traversal.\n\nSince the operating environment may change, APTS may generate different results when run with the\nsame arguments at different points during runtime. Since verifiers change the values of facts in the APTS\nnetwork, this may cause different rules to be triggered, resulting in different actions firing. This may\ncascade, in some scenarios, into very different end states being reached within that environment. An\nend state reached without verifiers is, thus, not indicative of the state that will be reached with verifiers.\nBecause of changing information, it is possible that a state completely different is reached, when\nverifiers are used.\n\nRuntimes of verifiers may differ, quite significantly in some scenarios. In networks that have facts that\nchange values over time, the speed of the verifier that is used may impact the fact values that are\nretrieved. For example, consider a scenario where there are two verifiers, verifier A and verifier B, and\nverifier A takes one second to retrieve a fact's value, while verifier B takes two seconds to retrieve that\nsame fact's value. This could result in different outcomes. If the fact's value changes after 1 second and\nbefore 2 seconds, between when Verifier A would capture the fact value and Verifier B would, then the\ntwo verifiers, which are both designed to retrieve the same fact value, would return different results for\nthe same fact due to the speed at which they get that fact value. Considerations like this, related to\nverifiers, must be analyzed when constructing networks, creating verifiers, and analyzing results of APTS\nruns. Ensuring that all aspects of verifiers and the implications of using them are understood is key to\nusing them effectively."}, {"title": "5.3. Practicality Impacts", "content": "The practical benefits of verifiers are described in this section. Section 5.3.1 details how verifiers reduce\nthe likelihood of results that inaccurately represent their real-world counterparts. Section 5.3.2\ndescribes how verifier scripts can greatly extend the functionality of SONARR and APTS and how their\ndynamic argument support allows complex scripts to be used during traversal."}, {"title": "5.3.1. Fixing Inaccurate Results", "content": "Prior to verifiers, APTS had the ability to run actions and influence the environment in which it is\nrunning; however, it lacked the ability to update or receive input back from its environment. This limited\nits ability to provide up to date output for its user. In many cases, users of APTS will not have all of the\ninformation necessary to fully predict the outcome of actions in the execution environment. Because of\nthis, when actions are run, they may result in different results then were predicted. Verifiers fill this\nneed and are thus crucial to the robustness of APTS. This is demonstrated by the results presented in\nSection 4.\n\nTo illustrate the impact of adding verifiers to APTS, the execution environment was set up to make a\nreal-world traversal from the entity represented by container 2 to the entity represented by the end\ncontainer (container 3), shown in Figure 2, impossible. This change was not, however, mirrored in the\nfact configuration of the imported network. This represents a scenario when the user does not know\neverything about the execution environment or the environment changes after a network is created."}, {"title": null, "content": "Figure 6 shows the result of running the network without the inclusion of verifiers. It returns one path,\nwhich asserts that APTS was able to make it from the start container to the end container. This path was\nreturned because there were not any verifiers to look at the execution environment and recognize that\nthat the fact values that were previously loaded do not correctly represent the execution environment.\nThus, this path is inaccurate, since the execution environment made this traversal impossible. This\nmeans that APTS returned a path that could not truly be completed in the real world. Without verifiers,\nit is implicitly assumed that all of the information APTS was provided is accurate and that the\nenvironment did not change, which was not the case for this particular run."}, {"title": null, "content": "There are several reasons that providing inaccurate results could be problematic. First, showing results\nthat are inaccurate will be confusing for the end user. For example, the end container may not have\nbeen reached in the executing environment, and the fact values displayed in the APTS interface may not\nmatch the real world. This may result in networks in APTS that are substantially different from what they\nare intended to represent. Figure 7 shows the correct representation of the network after an APTS run\nwith verifiers for \"Has Intermediate Key\" (Fact 4) and \u201cHas Goal Key\u201d (Fact 5), where the end container\nwas not reached. Figure 8 shows the network run without verifiers. It is shown that Facts 2, 3, and 5 all\nhave different values in the network run without verifiers than the network that represents the\nenvironment correctly. As networks get larger, the impact of inaccurate fact values has the potential to\ngrow tremendously."}, {"title": null, "content": "In addition to providing an inaccurate representation to the end user, making assumptions during\ntraversal could create unintended consequences in the executing environment. While this test used\npassive actions that did not notably impact the environment, scenarios that use more powerful tools\ncould result in unintended damage to the system and place it in a state that inhibits or alters its\nfunctionality, deletes necessary files, or creates other unintended issues. For larger networks, the level\nof divergence and negative impact may be proportionally larger.\n\nIt is vital that the APTS system knows when the environment is not responding to actions as expected,\nand that it responds to that. Without this ability, APTS would simply generate a fixed set of actions that\nit would run, based solely off of its input data without any consideration of the current state of the\nenvironment that it resides in. Verifiers connect APTS to its environment, allowing APTS to generate"}, {"title": null, "content": "results that correctly represent that environment at present. Figure 9 shows the result of an APTS run in\nthe impossible testing environment with the inclusion of verifiers.\n\nFigure 9 shows that no complete paths are generated, which mirrors the real-world environment. In an\nenvironment such as this, where verifiers stop a traversal that would otherwise execute without their\ninclusion, actions that may alter or harm the system (due, for example, to the APTS representation of\nthe network not matching the real-world) are not executed. This prevents issues from arising due to\ninaccuracy. It also provides the end user with results that correctly represent the state of the\nenvironment."}, {"title": "5.3.2. Dynamic Argument Support and Complex Scripting", "content": "Since verifiers pull arguments for their executables from custom properties, facts may have many\narguments that are specific to them. Pairing this idea with customized verifiers allows a wide range of\ncapabilities.\n\nThis flexible structure allows for complex verifiers to be developed. For example, if a fact is created to\nrepresent whether the executing environment has access to a computer, that the fact could have a\ncommon property that references the computer. How that computer is referenced can be changed\ndepending on what information is available and a verifier script can be created to match. If a computer\nname is known, a script can be created to ping computers by name. If an IP address is known, a script\ncan be created to ping computers by IP address. If there is a file containing the computer's name on a\nserver that the executing computer has access to, a script may be written that retrieves that file, reads\nand parses the text, and then pings the computer based on that. A single verifier script could also be\ncreated that takes any of these arguments as an input, checks which type of argument was provided,\nand pings the computer using the appropriate technique.\n\nTo enable this, any parameters within verifier scripts that are fact-specific should be placed into\narguments that are stored within custom properties specific to each fact that uses the verifier. This\nallows several facts to use the same verifier. The number of verifiers necessary for a given set of facts is\ndependent on the flexibility of the verifier script itself, as scripts can be made to account for different\nformats of arguments.\n\nWhen SONARR networks are created to represent real-world computer networks, verifiers can be used\nto run scripts that remotely assess or which are automatically loaded on computers that are not the\noriginal executing computer. For example, a script could be created that takes a target computer\nreference (such as an IP address) as an input. A second input could be the filename of a script that will\nbe sent to and run on the target computer. The first script may gain access to the computer with the\ngiven IP address and send over the contents of the second script to run. The details of this execution\nwould be dependent on the script itself. Languages, such as Python or PowerShell, are capable of tasks"}, {"title": null, "content": "such as this. Verifiers provide a dynamic foundation that supports complex scripting, allowing for\ncomplex procedures that can be completed in a script to be triggered, with arguments."}, {"title": "6. Conclusions and Future Work", "content": "This paper has described and evaluated a functional implementation of verifiers. It has demonstrated\nthe capabilities that verifiers provide and characterized their impact on SONARR and APTS system\nperformance. Additional work is needed to allow users to easily work with verifiers; however, the work\ndescribed herein provides the foundation to build upon.\n\nThe limitations of the current implementation of verifiers are described in this section along with a\ndiscussion regarding the future work needed. Section 6.1 describes the addition of the verifier files and\nfile support to match other network object types that currently exist within SONARR. Then, Section 6.2\ndescribes the addition of verifier information to the user interface, allowing users to see the results of\nverifier scripts and how they are affecting traversal."}, {"title": "6.1. Extending Verifier Type Support", "content": "Several additions will be needed for the current infrastructure of SONARR and APTS to allow verifiers to\nbe used seamlessly. First, verifiers will need to be imported with networks. This means that they will\nneed their own files that include an ID, executable path, format string, and format arguments. This is\nnecessary for the verifiers to determine which facts they should pull data from to execute their\ncommands.\n\nThe current implementation allows users to supply arbitrary custom property values and therefore\narbitrary arguments for the verifier processes that were used for testing; however, as a proof of\nconcept, it does not allow custom verifiers to be imported themselves. Thus, the system could be\nfurther expanded by having files that store information about verifiers, allowing custom ones to be user-\ndefined, and allowing those files to be imported and associated with facts and common properties. This\nwould facilitate a greater integration of verifiers into networks. Users could create their own verification\nscripts using any language that can be run from a terminal. Format strings and arguments also could be\nconfigured, for any scenario, allowing for prospectively any type of fact values to be collected.\n\nAdditionally, common properties will need a \"verifiers\" column added to their file. This will allow the\nverifier with a given ID to be associated with a common property. This connection allows verifiers to be\nexecuted during traversal. These changes will require alteration of the SONARR importing functions;\nhowever, the methods used to implement this new type of network object will be similar to those\ncurrently used."}, {"title": "6.2. Verifier Result Display", "content": "In many cases, especially where actions do not produce their intended outcomes, it may be beneficial\nfor the end user to know which actions executed successfully and which did not. Verifiers validate their\nresults against the intended values for each fact, so this result data could be made available to the user\nthrough the SONARR user interface. This would provide the user with a better understanding of what\noccurred during a given APTS run. This may allow the user to make changes to the network\nrepresentation or alter the actions to produce the desired results."}]}