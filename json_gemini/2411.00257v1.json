{"title": "Understanding Graphical Perception in Data Visualization through Zero-shot Prompting of Vision-Language Models", "authors": ["Grace Guo", "Jenna Jiayi Kang", "Raj Sanjay Shah", "Hanspeter Pfister", "Sashank Varma"], "abstract": "Vision Language Models (VLMs) have been successful at many chart comprehension tasks that require attending to both the images of charts and their accompanying textual descriptions. However, it is not well established how VLM performance profiles map to human-like behaviors. If VLMs can be shown to have human-like chart comprehension abilities, they can then be applied to a broader range of tasks, such as designing and evaluating visualizations for human readers. This paper lays the foundations for such applications by evaluating the accuracy of zero-shot prompting of VLMs on graphical perception tasks with established human performance profiles. Our findings reveal that VLMs perform similarly to humans under specific task and style combinations, suggesting that they have the potential to be used for modeling human performance. Additionally, variations to the input stimuli show that VLM accuracy is sensitive to stylistic changes such as fill color and chart contiguity, even when the underlying data and data mappings are the same.", "sections": [{"title": "Introduction and Related Work", "content": "Vision Language Models (VLMs) are capable of synthesizing information in both the vision and language input modalities, leading to their application in healthcare diagnostics (19), autonomous vehicles (16), interactive robotic applications (24), and other domains. In our domain of interest, data visualization, VLMs have also been used for a range of tasks that require attending to both the images of charts and graphs and their accompanying textual descriptions (5, 12, 13, 14, 23), from simple tasks such as data extraction (14) and question answering (2, 7, 8, 9, 10, 11, 12, 13, 14, 15, 18, 20, 22) to more complex tasks such as chart generation and refinement (5).\nRecent research has evaluated whether VLMs show human-like visualization comprehension abilities using visualization literacy tests (1). Such tests consist of questions that measure the ability of humans to comprehend and extract information from visualizations. Studies with GPT-4 show that it can reason about visualizations, identify trends, and suggest best design practices. Yet, the model struggles with simple tasks like value retrieval and color distinctions in charts. If VLMs show human-like visualization comprehension abilities, they can be used to design and evaluate visualizations, e.g., identifying potential sources of cognitive processing (over)load. However, doing so requires establishing that VLM performance profiles map to human-like behaviors. Here, we lay the foundations for such applications by evaluating the accuracy of VLMs when performing graphical perception tasks."}, {"title": "2 Method", "content": "Our work adapts the stimuli and tasks from two prior human studies to evaluate the behavioral alignment of the graphical perception abilities of VLMs. To ensure the comparability of results across studies, we recreate the stimuli and prompt the VLM with the same probes in a zero-shot manner. Information for stimulus generation was taken from both (3, 6), whereas the text of the prompts was referenced from the experimental materials of (6). Our study included seven tasks from these studies plus two new variants. Each task consisted of 45 distinct trials.\nStimuli and Tasks. To create the stimuli (i.e., visualizations), we first generated ten numerical values using the formula from Cleveland and McGill (3):\n$S_i = 10 \\times 10^{(i-1)/12}, i = 1, 2, ..., 10$ (1)\nWe then constructed all 45 possible unique pairs of these values. The ratios of these pairs ranged from 0.18 to 0.83. For each of the seven tasks from the original studies (3, 6), we generated 45 visualizations corresponding to these pairs. In each visualization, the segments encoding the values being compared were colored blue and yellow and also labeled \u201cA\u201d and \u201cB\u201d."}, {"title": "3 Experiments 1 and 2", "content": "Experiment 1. For each trial, the VLM was given a visualization and asked to respond to the probes:\n1. Which of the two, blue (A) or yellow (B), shapes is smaller?\n2. What percentage is the SMALLER marked shape of the LARGER? Enter a % between 0 and 100.\nWe vary the framing of the prompts in two ways. The first is the explicit mention of color in the probe (no color/has color). The \"has color\" prompt contains references to the colors of the two labeled segments. The second is requesting explanations in the model response (no explanation/has explanation). The \"has explanation\" prompt asks the model not just to provide an answer (such as identifying which visual element is smaller or the proportion between two segments), but also to generate the reasoning behind its decision. \nFinally, since VLMs may exhibit bias towards left/right layouts and A/B labels, we added three stimuli variations that inverted the order of colors and A/B labels.\nExperiment 2. In addition to the stimuli used in prior studies , we created two new variations of all (7 \u00d7 45 =) 315 visualizations and evaluated VLM comprehension of these stimuli using the same probes and prompt framings. These variations are, first, no colored bars, and second, all colored bars; Figure 2b). Other than fill color changes, the stimuli in the variations were identical."}, {"title": "3.1 Results", "content": ""}, {"title": "4 Experiment 3", "content": "Experiment 3. To disentangle the effect of contiguous segments on model performance, we created variations of Tasks 5 and 6 (henceforth 5B and 6B) that change whether the segments used for comparison are contiguous with one another. In Task 5, the segments being compared are always contiguous, whereas in Task 5B, they are always separated by another segment. In Task 6, the segments are always separated by other segments, whereas in Task 6B, they are always contiguous.\nSince Experiment 1 demonstrates that model accuracy is highest when the prompt framing includes color and explanation, we use this framing here for Experiment 3 as well. Similarly, based on Experiment 2 results, we apply the best-performing default variant in this experiment ."}, {"title": "4.1 Results", "content": "Experiment 3 - Segment Contiguity: There was an effect of segment contiguity on model performance. For the default condition, the model was less accurate when the segments being compared were contiguous than when they were well-separated (VLM performance Task 5B > Task 5; Task 6B < Task 6). However, inverting segment colors also inverts this relationship, causing contiguous segments to perform better than separate segments."}, {"title": "5 Discussion", "content": "Comparison to human performance: To evaluate the relationship between VLM performance and human performance, we conducted a rank-order correlation analysis . We ordered"}, {"title": "6 Conclusion", "content": "This paper reports the initial findings of evaluating GPT-40-mini in a zero-shot manner on graphical perception tasks with established human performance profiles (3, 6). The study assesses the model's ability to extract and compare data from segments in a visualization. Our results show that VLMs perform similarly to humans when 1) both color and explanations are present in the prompt template, 2) segments are colored in the visualization, and 3) segments are non-contiguous. This suggests that, for certain combinations of task and visualization type, VLMs have the potential to design and evaluate visualizations by modeling human performance.\nLooking ahead, the findings here may be useful for predicting and explaining VLM performance on more complex chart types, as seen in real-world applications. For instance, the effect of segment contiguity, documented here in the novel comparisons between Task 5 and 5B and Task 6 and 6B, may result in lower accuracies on ChartQA tasks (10) for stacked bar charts and pie charts overall. Future work can also evaluate human performance on the Task 5B and 6B variations introduced here to establish whether VLMs can generate new predictions about human performance on novel chart comprehension tasks."}, {"title": "7 Acknowledgements", "content": "The authors would like to thank the anonymous reviewers for their thoughtful and detailed feedback on the paper. This work is supported in part by NIH grant 1U01CA284207."}, {"title": "8 Appendix", "content": "Input prompts to VLM\nHere are the different kinds of prompts used in the study.\nPrompt - No Color, No Explanation\nWhich of the two, (A) or (B), shapes is smaller?\nWhen the inputs are bar charts or wedges compare the lengths/ height.\nWhen the inputs are pie charts or circles compare the area.\nSelect one of the following:\nA. The marked shape (A) is smaller.\nB. The marked shape (B) is smaller.\nWhat percentage is the SMALLER marked shape of the LARGER?\nEnter a percentage between 0 and 100.\nOutput in JSON format:\n{\n\"Is A smaller than B\": true/false,\n\"percentage\": \"XX%\"\n}\nPrompt - Has Color, No Explanation\nWhich of the two, blue (A) or yellow (B), shapes is smaller?\nWhen the inputs are bar charts or wedges compare the lengths/ height.\nWhen the inputs are pie charts or circles compare the area.\nSelect one of the following:\nA. The marked blue shape (A) is smaller.\nB. The marked yellow shape (B) is smaller.\nWhat percentage is the SMALLER marked shape of the LARGER?\nEnter a percentage between 0 and 100.\nOutput in JSON format:\n{\n\"Is A smaller than B\": true/false,\n\"percentage\": \"XX%\"\n}\nPrompt - No Color, Has Explanation\nWhich of the two, (A) or (B), shapes is smaller?\nWhen the inputs are bar charts or wedges compare the lengths/ height.\nWhen the inputs are pie charts or circles compare the area.\nSelect one of the following:\nA. The marked shape (A) is smaller.\nB. The marked shape (B) is smaller.\nWhat percentage is the SMALLER marked shape of the LARGER?\nEnter a percentage between 0 and 100.\nOutput in JSON format:\n{\n\"explanation for smaller or bigger\": \"...\",\n\"Is A smaller than B\": true/false,\n\"explanation for percentage\": \"...\",\n\"percentage\": \"XX%\"\n}\nPrompt - Has Color, Has Explanation\nWhich of the two, blue (A) or yellow (B), shapes is smaller?\nWhen the inputs are bar charts or wedges compare the lengths/ height.\nWhen the inputs are pie charts or circles compare the area.\nSelect one of the following:\nA. The marked blue shape (A) is smaller.\nB. The marked yellow shape (B) is smaller."}, {"title": "Limitations and Future Work", "content": "We acknowledge a few limitations of our work. Our analysis used only one VLM, limiting the findings' generalizability, as different VLMs may exhibit varying performance characteristics. In particular, we expect that VLMs fine-tuned for chart comprehension or chart question-answering tasks will outperform general-purpose models like GPT-40-mini. Future work should thus consider testing multiple VLMs to create a more comprehensive evaluation. We also observed significant uncertainty in the model's performance on tasks involving percentage judgments, which indicates lower model performance on these proportion-type judgments. Further testing would be useful to better understand and potentially mitigate this uncertainty.\nAnother limitation is that the input stimuli used for these experiments may not resemble the types of visualizations found in the training data of GPT-40-mini. This mismatch could have contributed to suboptimal performance in specific tasks. Future studies can modify the input stimuli to match visualization styles in the training data better to evaluate model accuracy and reliability more precisely."}]}