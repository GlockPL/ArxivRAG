{"title": "Evidence of Cognitive Deficits and Developmental Advances in Generative AI: A Clock Drawing Test Analysis", "authors": ["Isaac R. Galatzer-Levy", "Jed McGiffin", "David Munday", "Xin Liu", "Danny Karmon", "Ilia Labzovsky", "Rivka Moroshko", "Amir Zait", "Daniel McDuff"], "abstract": "The rapid advancement of generative artificial intelligence (GenAI) has ignited significant interest in the cognitive capabilities of the underlying models. This surge in interest stems from the unprecedented ability of these models to perform tasks previously considered exclusive to human cognition, such as natural language understanding, creative writing, and code generation. Interrogating the performance of such models on standardized cognitive tasks can provide insights into how they directly compare to healthy and abnormal human performance. This study explores the performance of multiple recent GenAI models on the Clock Drawing Test (CDT), a widely used neuropsychological assessment for evaluating aspects of executive functioning such as visuospatial planning and organization. Our findings demonstrate that while models can produce clock-like representations, they exhibit fundamental deficits in the ability to reason and produce the correct time, consistent with mild to severe cognitive deficits using a standardized scoring scheme (Wechsler, 2009). Specifically, AI-generated drawings frequently demonstrate errors in numerical sequencing (omissions, repetitions, misordering), and numerical reasoning (clock time errors), and intrusions (addition of irrelevant or hallucinated information) while models consistently perform well in rendering features (numbers, hands, contour), together indicating crystallized knowledge but an inability to demonstrate visual-spatial reasoning. Of those evaluated, only GPT 4 Turbo and Gemini Pro 1.5 successfully produced the correct time and demonstrated scores consistent with healthy cognitive functioning (Weighted score = 4/4). A follow-up test in which multimodal models were asked to read a clock revealed that only one model (Sonnet 3.5) read the clock, correctly indicating that the deficits in drawing are due to an inability to comprehend, attend, or manipulate numeric concepts. The observed findings can reflect deficits in visual-spatial understanding, working memory, and/or calculation. Together, this research identifies consistent strengths in crystallized knowledge but deficits in reasoning capabilities. This underscores the value of benchmarks that facilitate comparison across human and machine performance. This translation allows us to understand the cognitive capabilities of artificial intelligence and to guide further development towards generalized artificial cognitive functions akin to those of neurological healthy humans.", "sections": [{"title": "1. Introduction", "content": "Generative artificial intelligence (GenAI) models possess the ability to perform tasks that resemble human cognitive functions, such as reasoning and information retrieval. This capacity stems from their training on massive data sets of text and code, enabling them to learn complex patterns and relationships. Unlike the biologically constrained architecture of the human brain, GenAI models operate on computational principles that are still being deciphered. Comparing their performance on cognitive tasks with human benchmarks allows us to probe these underlying mechanisms and elucidate the computational underpinnings of intelligence. Although generative models can produce remarkably human-like text and images, their underlying cognitive architecture differs significantly from ours and remains poorly understood. Just as human cognitive abilities are rooted in neuroanatomy yet emerge as complex, measurable behaviors, understanding the cognitive capabilities of these models is essential. Only then can we grasp their potential and limitations as simulations of human cognition and their capacity for everyday reasoning.\nThe Clock Drawing Test (CDT, (Agrell and Dehlin, 1998)) is a classic performance-based neu-ropsychological screening tool, offering a concise yet multifaceted evaluation of cognitive domains frequently compromised in neurodegenerative disease and other brain disorders. Its sensitivity to a range of cognitive functions makes it an ideal instrument for probing the capabilities of GenAI models. By examining their performance on this test, we can gain insights into their strengths and weaknesses in areas such as visuospatial processing, planning, and numerical reasoning. The deceptively simple task of drawing a clock face and setting the hands to a specified time, engages a complex interplay of cognitive processes, including visuospatial skills (the ability to accurately represent spatial relationships; (Agrell and Dehlin, 2012; Fukui and Lee, 2009), executive functioning, (ability to plan, sequence, organize, and reason; (Dubois et al., 2008; Jones and Graff-Radford, 2021), working memory, (critical for parsing instructions and maintaining a mental image of a clock; (Bondi et al., 1996), and sustained attention (ability to focus on the task and resist distractions; (Lezak, 1995)). Performance on the CDT is commonly used by psychologists and physicians to screen for cognitive dysfunction in the above domains, and is particularly sensitive to pathology associated with neurodegenerative disorders, including Alzheimer's disease and other dementia subtypes 6\u20138. Given that such tasks can be translated for administration to generative models, such methods serve as useful benchmarks of cognition that translate across humans and machines.\nError analysis in CDT extends beyond a simple numerical total score, offering insights into specific cognitive deficits and potential underlying neuroanatomical mechanisms involved. For example, number omissions may suggest attentional impairment or executive dysfunction (e.g., poor error monitoring), whereas sequencing errors in number placement or hand setting often reflect executive dysfunction and frontal lobe involvement (Lezak, 1995). Errors in number generation, such as repeated or incorrect numbers, can indicate difficulties with numerical sequence comprehension, working memory limitations, or visuospatial organization deficits. A distorted clock contour, or otherwise visually fragmented clock lacking overall \u201cgestalt\u201d is indicative of visuospatial dysfunction and potential parietal lobe involvement (Fukui and Lee, 2009). A misplaced center, where the hands of the clock converge, suggests difficulty with visuospatial planning and organization. Finally, intrusions, such as extraneous drawings or words, point to executive dysfunction and poor response inhibition, commonly seen in frontal lobe disorders and sometimes associated with psychosis (Jones and Graff-Radford, 2021; Royall et al., 1999).\nPerformance on the CDT is not itself diagnostic of pathology as multiple factors influence cognitive performance. Moreover, impaired performance on any individual subtest should be considered amongst data from a broader neuropsychological assessment for questions relevant to diagnosis. Limitations notwithstanding, various CDT scoring systems are available (for a review, see: (Spenciere et al., 2017)) that have been normed against the general population and provide performance brackets indicating likely levels of cognitive impairment. As such, population norming for tests refers to the percentile ranking of the cognitively healthy population that would attain a score at or below that score. This provides a reference for the chances that the individual being tested is cognitively healthy, or conversely, at risk requiring further focused evaluation. Referring to norms developed as part of the Brief Cognitive Status Exam (BCSE) of the Wechsler Memory Scale - Fourth Edition ((Wechsler, 2009) scores on the CDT (as elaborated in the methods section) can be used to rank examinees' CDT total scores as 'Very Low (corresponding to smaller than 2% chance of being cognitively healthy),' \u2018Low (2-4%),' 'Borderline (9%),' \u2018Low Average,' and 'Average'. Scores in the very low range have a high probability of being considered abnormal while scores in the Low range have moderate probability of being abnormal. Scores in the Borderline range and higher have less evidence that scores indicate"}, {"title": "2. Methods", "content": "In the current investigation, the CDT was administered to multiple generative models independently. This included large language models (LLMs) that are capable of language understanding and genera-tion and multimodal models that are capable of image generation from language understanding.\nThe language models we tested (as of July 2024) were Google's Gemini Nano, Gemini Pro, Gemini Ultra and Gemini Advanced, OpenAI's PT-3.5 Turbo, GPT-4 Turbo, and Anthropic's Claude 3 Opus. The multimodal models were Google Deepmind's Image Gen 2, Stable Diffusion XL Base 1.0, Stable Diffusion 3 Medium, OpenAI's GPT-40. These represent a range of foundation models with different numbers of underlying parameters, training data and regimes, and architectures. We did not assess any models in the Llama family developed by Meta because licensing requires direct permission from Meta to use such models for research purposes.\nLanguage only models Google Gemini Family: We utilized four models from Google's Gemini family: Gemini Nano, Gemini Pro, Gemini Ultra, and Gemini Advanced. These models are designed for different computational constraints and offer varying levels of capability. However, Google has not publicly released details about the specific architectures, training data, or number of parameters for these models. OpenAI GPT Family: We incorporated two models from OpenAI's GPT family: GPT-3.5 Turbo and GPT-4 Turbo. GPT-3.5 Turbo, a text-only model. GPT-4 Turbo, is a larger more advanced model although OpenAI has not disclosed specifics regarding its architecture, training dataset size, or number of parameters. Anthropic: Claude 3 Opus by Anthropic is also an open source large language model known for its focus on safety and helpfulness.\nMultimodal Models: Google Gemini Family: Google Deepmind ImageGen 2: We employed Google Deepmind's ImageGen 2, a text-to-image diffusion model. ImageGen 2 demonstrates significant advancements in generating high-quality images from text prompts. Stability AI Stable Diffusion XL Base 1.0 & Stable Diffusion 3 Medium: We incorporated two models from Stability Al's Stable Diffusion suite: Stable Diffusion XL Base 1.0 and Stable Diffusion 3 Medium. These open-source latent text-to-image diffusion models are known for their ability to generate high-resolution images. OpenAI GPT-40: We utilized GPT-40, OpenAI's multimodal model capable of processing and generating both text and images."}, {"title": "2.1. Models", "content": null}, {"title": "2.2. Administration and Scoring", "content": "All models were similar in that specific parameter counts and training approaches are not disclosed publically. Across multimodal and LLM models, the same prompt was administered to the model directly (once per model) with no previous prompting or fine tuning. The prompt for the CDT, as incorporated within the WMS-IV BCSE (Wechsler, 2009), is to \u201cdraw the face of a clock, put in the numbers, and set the hands to 10 minutes after nine\" (Wechsler, 2009). For LLMs, additional instructions were provided to render code to produce the clock using SVG (text added to prompt: \"Provide Scalable Vector Graphics (SVG) code to draw the image.\").\nThe CDT involves three distinct components: drawing the clock face, putting in the numbers, and"}, {"title": "3. Results", "content": "Variability in performance was observed across the 12 models which were each tested only once (Figure 1, Table 1). Only the most advanced and highest parameter models (GPT 4 Turbo, Gemini Pro 1.5 demonstrated the capability to place both hands of the clock at the correct time. Larger parameter models were the only ones to demonstrate scores consistent with the population of individuals with healthy cognitive functioning [GPT 3.5 Turbo (Raw Score = 12; Weighted Score = 12; GPT 4 Turbo (Raw Score = 12; Weighted Score = 4; Gemini Pro 1.5 (Raw Score = 14; Weighted Score = 4)] while the smallest parameter model scored the poorest (Gemini Nano, Raw Score = 0; Weighted Score = 0). All models, with the exception of Gemini Nano which was unable to render a representation of an analogue clock or correct elements, demonstrated relatively minor errors in clock contour, number generation and sequencing, proportional hands and center alignment. Interestingly, GPT-40 was the only tested model able to perform both direct-visual and SVG-generated renderings, although these demonstrated a wide divergence in scores and neither was able to accurately render the correct time. Together these results indicate that, over a floor threshold of small parameterized models, current state-of-the-art multimodal and large language generative models are able to produce images grossly consistent with the concept of a clock and its constituent elements; however, only larger models, possibly with more advanced approaches to training, are currently able to produce clock drawings that also simulate the correct use of a clock to accurately represent human-level conceptualizations of time.\nAdditionally, models either consistently omitted information with seven models failing to produce all 12 numbers. Conversely, multiple models demonstrated intrusions including novel shapes in the place of numbers and additional objects like human hands in the case of multimodal models, and additional numbers (GPT 40 language only). Additionally, Miro 4 and Miro 5 produced elaborate intrusions beyond simple changes to existing clock elements.\nFinally, when selected models (Gemini Pro 1.5, GPT 4 Turbo, GPT 40, Claude Opus 3, Claude Sonnet 3.5) were shown a clock and asked to read the time, all failed with the exception of Claude Sonnet 3.5. Specifically, when presented with a clock that displays the time \"5:45\", two models demonstrated that they primarily attended to the long hand, misinterpreted its meaning, and ignored the short hand by stating that the time is \"9:00\"(Gemini Pro 1.5, GPT 4 Turbo). Other models (GPT-40) confused the long and short hand to interpret the time as \"9:30\", as did Gemini XL reporting \"9:25\". Claude Opus 3 demonstrated a similar conceptual confusion about the long and short hands by interpreting the time as \"4:50\". Claude Sonnet 3.5 demonstrated the correct answer and demonstrating both the correct time and the underlying reasoning by stating \"The clock in the image is showing 5:45. The hour hand is positioned between 5 and 6, but closer to 5, indicating it's 5 o'clock. The minute hand is pointing directly at 9, which represents 45 minutes past the hour. Therefore, the time shown on this analog clock is 5:45.\""}, {"title": "4. Discussion", "content": "Results indicate that generative AI models consistently demonstrate performance impairments (errors, omissions, and intrusions) on the Clock Drawing Test, a common screening tool sensitive to abnormal cognitive functioning in humans. While all models successfully generated basic clock components like the contour, hands, and center, suggesting intact basic visuospatial processing and motor planning (Agrell and Dehlin, 2012), significant deficits emerged in higher-order cognitive functions. The consistent misplacement of hands across all models, regardless of image or SVG generation, implies a shared difficulty in accurately representing spatial and temporal concepts, a cognitive domain reliant on intact executive functioning and working memory (Bondi et al., 1996; Libon et al., 1993). Other models exhibited errors mirroring human cognitive deficits, such as potential working memory limitations reflected in number omissions or sequencing errors (Bondi et al., 1996; Libon et al., 1993).This finding aligns with previous research demonstrating challenges in AI models' grasp of abstract concepts like time (Dasgupta and et al., 2022; Xu et al., 2023). The superior performance of larger, more recent models suggests that increasing model scale and refining training methodologies contribute to advancements in higher-order cognitive abilities in GenAI. Of note, larger parameter more current models performed the best, both able to generate the concept of a clock and able to represent a clock functionally. This indicates that, though generative models are overall unreliable in tasks that involve reasoning, there is progress across generations indicating that generative models may soon reliably perform higher order cognitive tasks.\nAdditional analysis of model's ability to read a clock revealed that the deficits observed in on the CDT were not simply due to the inability to produce an image correctly. Indeed, the models demonstrated a deficit in the ability to understand the underlying numeric concepts of an analog clock and to manipulate them for their purposes. Some models simply reported an unrelated time while others demonstrated an inability to differentiate the hands of a clock to read the clock correctly. Only Claude Sonnet demonstrated the ability to read the clock correctly. While it is unknown why this model performed better, the output was accompanied by logical steps and their individual answers indicating that the model had accompanying prompts to guide its reasoning. This indicates that some internal mechanisms to break down and logically solve parts of the problem may be a successful strategy for generative models to perform higher order reasoning tasks such as this. This highlights the importance of incorporating explicit reasoning mechanisms into AI models and indicates that some internal mechanisms to break down and logically solve parts of the problem is a successful strategy for generative models to perform higher order reasoning tasks.\nThe presence of intrusions in the form of extraneous images or numbers suggests potential weaknesses in response inhibition and attention, key components of executive function often associated with frontal lobe function in humans (Agrell and Dehlin, 2012; Royall et al., 1999) regardless of method of producing an output. Clinically such a pattern of behavior can indicate either a lack of focused attention on the primary task or over-fixation of some small unimportant aspect at the expense of the primary goal which are both behaviors that are common in attentional disorders like Attention Deficit Hyperactive Disorder(ADHD) or early forms of dementia. Notably, while these intrusions resemble patterns observed in individuals with frontal lobe disorders, it's crucial to avoid direct comparisons given the inherent differences between Al models and the human brain.\nIt is important to acknowledge the limitations of this study. Primarily, the small sample size of models tested limits the generalizability of these findings to the broader landscape of generative AI. Second, we elected a single CDT administration for each AI model to stay consistent with the traditional testing format that underlies the population norming, although future replications of these results with iterative administrations per model are warranted to accurately assess the range and stability of error profiles associated with each model. Additionally, while the CDT is a sensitive cognitive screening tool, it provides a limited representation of human cognitive processes that are otherwise complex and multimodal. Future research should expand on these findings by incorporating a larger, more diverse set of models with iterative administrations, and utilizing a more comprehensive array of neuropsychological assessments to evaluate the strengths and weaknesses of AI across human cognitive domains. Finally, a significant limitation is the lack of publicly available information about the models themselves, limiting the ability to understand the relationship between performance and key parameters such as training data or the size of the model. While we can observe that smaller models perform more poorly and newer model perform better, we can not directly compare parameter count or other key features to cognitive ability.\nImportantly, while poor performance in humans reflects underlying biological deficits, limitations observed in generative models can only be interpreted as a snapshot of the current capabilities along a developmental trajectory. Indeed, we observe that as models advance in size and versioning, there is a developmental trend towards improved performance without any known specific training to solve the problem of clock drawing or recognition. In this context, poor performance can be interpreted as as limited development rather then an insurmountable deficit.\nDespite these limitations, this study offers valuable insights into the evolving capabilities and limitations of generative AI models. The observed patterns of performance on the CDT, a task sensitive to various cognitive domains, provide a unique lens through which to understand how these models process and integrate information as well as the targets for development that will improve general cognitive performance. As AI technology continues to advance, such investigations will be essential in guiding development, refining our understanding of AI's cognitive abilities, and ultimately bridging the gap between artificial and human intelligence. Importantly, the Clock Drawing Task indicates broad meta-cognitive capabilities and deficits. Further research should investigate specific substrates of cognition including executive functioning, working memory, visual reasoning, and crystallized knowledge, all of which can affect performance on this task."}]}