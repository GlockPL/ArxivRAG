{"title": "UTMath: Math Evaluation with Unit Test\nvia Reasoning-to-Coding Thoughts", "authors": ["Bo Yang", "Qingping Yang", "Runtao Liu"], "abstract": "The evaluation of mathematical reasoning ca-\npabilities is essential for advancing Artificial\nGeneral Intelligence (AGI). While Large Lan-\nguage Models (LLMs) have shown impres-\nsive performance in solving mathematical prob-\nlems, existing benchmarks such as GSM8K\nand MATH present limitations, including nar-\nrow problem definitions with specific numbers\nand reliance on predetermined rules that hinder\naccurate assessments of reasoning and adapt-\nability. This paper introduces the UTMath\nBenchmark, which robustly evaluates the mod-\nels through extensive unit tests. It consists\nof 1,053 problems across 9 mathematical do-\nmains, with over 68 test cases per problem. We\npropose an innovative evaluation framework\ninspired by unit testing in software develop-\nment, focusing on both accuracy and reliabil-\nity of results. Furthermore, we introduce the\nReasoning-to-Coding of Thoughts (RCoT) ap-\nproach, which encourages LLMs to perform\nexplicit reasoning before generating code, lead-\ning to generating more advanced solution and\nimproved performance. Furthermore, we are\nreleasing not only the UTMath benchmark but\nalso the UTMath-Train training dataset (more\nthan 70k samples), to support the community\nin further exploring mathematical reasoning.", "sections": [{"title": "1 Introduction", "content": "The pursuit of AGI necessitates strong mathemat-\nical reasoning capabilities, making the evaluation\nof such abilities a crucial area of research (Zhou\net al., 2024a). Recent advancements in LLMs\nhave demonstrated remarkable proficiency in solv-\ning complex mathematical problems, achieving\namazing performance on various datasets of Math\nWord Problems (MWPs), such as GSM8K (Cobbe\net al., 2021), MATH (Hendrycks et al., 2021), The-\noremQA (Chen et al., 2023).\nHowever, classic benchmarks exhibit several lim-\nitations that impede the accurate and comprehen-\nsive assessment of these models' capabilities (Ahn\net al., 2024). First, these benchmarks test models\non narrowly defined problems with some specific\nnumbers, which do not adequately assess adaptabil-\nity to similar but varied scenarios as shown in Fig. 1.\nSecond, their evaluation relies on predetermined\nrules or the method of LLM-as-a-Judge( (Dubois\net al., 2024; Zheng et al., 2023)) that usually failed\nwith capricious responses of LLMs. For exam-\nple, an accurate answer need to be extracted to ex-\nactly match the fianl answer in the dataset GSM8K,\nTheoremQA, and MATH dataset. Furthermore,\nthese benchmarks focus more on final answers than\non the underlying reasoning steps. While recent\nwork has made great progress in developing new\nbenchmarks for evaluating the mathematical rea-\nsoning of LLMs, many of these approaches still\nfall short of addressing the fundamental limitations\nof earlier datasets. For instance, benchmarks like\nGSM-HARD (Gao et al., 2023), GSM-IC (Shi et al.,"}, {"title": "2 Related Work", "content": "With the rapid development of LLMs, evaluating\nand exploring the intelligence and limitations of\nthese models has emerged as an urgent issue to\naddress (Chang et al., 2024). Reasoning abil-\nity, as a crucial component of general intelligence,\nhas garnered widespread attention since the advent\nof LLMs (Patel et al., 2021; Cobbe et al., 2021;\nValmeekam et al., 2022; Perez et al., 2022; Gupta\net al., 2022; Shakarian et al., 2023). Mathemat-\nical reasoning, due to its complex mathematical"}, {"title": "3 UTMath Benchmark", "content": ""}, {"title": "3.1 Introduction for OEIS.", "content": "The OEIS was established to document integer se-"}, {"title": "3.2 Benchmark Construction", "content": "The sequences that meet our expectations and can\nbe used to test reasoning abilities should satisfy the\nfollowing conditions:\n1. The sequence should require reasoning to de-\nrive its recurrence or formula, not just simple\ncalculations.\n2. The sequence must be derivable with infinitely\nmany terms, allowing deduction of a(n) for\nany positive integer n.\n3. The sequence must be accurately described to\ndeduce it entirely from the description.\n4. The sequence should belong to mathematics,\npreventing reasoning errors from lack of do-\nmain knowledge.\nData Crawling OEIS provides users with a list\nof principal sequences 1, which are representative\nsequences in the OEIS. To help users quickly find\nthe main sequences within their areas of interest,\nOEIS categorizes these sequences into 118 sections\nbased on the first 2-3 letters of their content themes.\nBy scraping the category tags within each section\nand the AIDs of their subordinate sequences, we\nobtained 569 categories and 23,238 principal se-\nquences' AIDs. OEIS provides an interface to re-\nquest the JSON data of the HTML page for each\nsequence using its AID 2. By passing the sequence\nAIDs to this interface, we acquired the JSON data\nfor these 23,238 sequences.\nData Cleaning We found that some of the se-\nquences we collected did not meet our criteria and\nshould be removed. Here, we present only a por-\ntion, with further details provided in the appendix\nB.\n\u2022 Hard to solve, few terms are discoverable\nA portion of the sequences retrieved are marked\nas \"hard\" in the keyword field of their entries in\nOEIS. According to OEIS, \u201cAny sequence which"}, {"title": "3.3 Evaluation Metrics", "content": "We adopt the metric pass@k to evaluate the perfor-\nmance of LLMs. The metric pass@k is a classic\nmetric in code generation, where k code samples\nare generated per problem, a problem is consid-\nered solved if any sample passes the unit tests, and\nthe total fraction of problems solved is reported.\nWe use the stable method of calculation proposed\nby (Chen et al., 2021):\npass@k := $\\frac{c}{problems} \\left[1-\\left(\\begin{array}{c}n \\\\ k\\end{array}\\right)\\left(1-\\frac{c}{n}\\right)^k\\right]$"}, {"title": "3.4 Dataset Statistics", "content": "To gain a deeper understanding of the composition\nof the UTMath Benchmark, we analyzed the po-\ntential fields each problem might pertain to and\nidentified nine mathematical fields: Number The-\nory, Graph Theory, Group Theory, Game Theory,\nDiscrete Mathematics, Combinatorial Mathematics,\nGeometry and Topology, Polynomial and Series\nExpansions, Special Numbers, Formal Languages.\nUsing GPT-40, we allocated each problem to these\nfields and obtained the distribution shown in Table\n1."}, {"title": "4 Reasoning-to-Coding of Thoughts", "content": "Compared to evaluation methods that solely rely\non checking if the results generated by LLMs are\nidentical, without assessing the reasoning process,\nwe propose a method based on evaluating mathe-\nmatical reasoning processes through code imple-\nmentation."}, {"title": "5 Experiment", "content": ""}, {"title": "5.1 Experimental Setup", "content": "Here, we consider the closed-source models,\ni.e., GPT-3.5-Turbo/GPT-40 from OpenAI (Ope-\nnAI, 2024), Claude-3.5-Sonnet (Claude, 2024),\nGemini-1.5-Pro (Reid et al., 2024), as well as\nthe open-source models, i.e., LLaMA-3.1 (Dubey\net al., 2024), Qwen2.5 (Qwen, 2024a), Qwen2.5-\nMath (Qwen, 2024b), DeepSeek-V2.5 (Bi et al.,\n2024). The metric pass@1 is calculated as the aver-\nage result over 5 run times. We run all evaluations\nin a laptop with CPU Intel(R) Core(TM) i7-10750H\nCPU @ 2.60GHz."}, {"title": "5.2 Evaluation on UTMath", "content": "Here we evaluate both open-source and closed-\nsource models using RCoT and PoT in Tab. 2. The\nexperimental results shows that all tested models\nperformed poorly on our benchmark. The best\nmodel, GPT-40, only solves 26.93% problem in\nour benchmark. Since our problems are sourced\nfrom the OEIS, they consist of sequences and so-\nlutions proposed by various mathematicians in the\ncontext of cutting-edge research. This suggests that\nour benchmark is challenging enough to help guide\nfuture directions for improving LLMs.\nCompared to PoT, our method RCoT demon-\nstrates superiority in two aspects. First, prompting\nwith RCoT achieves higher pass@5 performance\nacross 6 LLMs, with the best results observed on\nGPT-40. Second, the solutions generated by RCOT\ndemonstrate more efficient performance, partic-\nularly Qwen2.5-72B, where the RCoT approach"}, {"title": "5.3 The Effectiveness of Hard Test Cases", "content": "As we mentioned in Sec. 3.2, each sequence in the\nOEIS lists only the initial terms, which we refer\nto as \"easy test cases\u201d. To investigate the model's\nability to handle challenging cases, we evaluated\nwhether it could predict values that appear later\n(i.e., 106) in a sequence. These later values are\ntypically underrepresented in pre-training data and\noften require more computation time and a more\nprecise implementation to retrieve accurately. The\nexperimental results, which depicted in Tab. 3, re-\nveal that the model's performance drops signifi-\ncantly when handling these hard cases. This in-\ndicates that introducing these cases can prevent\nsimple solutions from passing all the easy cases,\nthereby filtering for more advanced solutions. Our\nbenchmark provides a better measure of the reason-"}, {"title": "5.4 Scaling of the Inference Times", "content": "We compared the performance difference between\nrunning the LLMs five times and reported the met-\nric of pass@k. As shown in Fig. 4, all models im-\nproved their performance with an increasing num-\nber of inference times. For Qwen2.5-72B, RCOT\nwas slightly weaker than PoT in the first infer-\nence but quickly approached and surpassed PoT\nin subsequent run times. For GPT-3.5-Turbo and\nGemini-1.5-Pro, PoT consistently outperformed\nRCoT. We observed that with an increasing number\nof inference time, RCoT consistently demonstrated\na growing advantage in performance across almost\nall models, except for GPT-3.5. This suggests that\nRCoT may perform better in models with stronger\nreasoning capabilities."}, {"title": "5.5 Importance of the Reasoning Step", "content": "GPT-40 has the best performance, while we are\nunclear whether this was due to its superior rea-\nsoning or coding ability. To investigate further, we\ntested using GPT-40 for the reasoning step while\nother models perform the coding step based on\nthe reasoning result from GPT-40. As depicted in\nFig.5, the results showed that the performance of\nmodels, except for Qwen2.5-72B, increased signif-\nicantly when implementing coding based on GPT-\n40's reasoning output, suggesting that the reason-\ning quality is important and GPT-40 does produce\nhigher-quality reasoning results."}, {"title": "5.6 Performance on Different Categories", "content": "Our benchmark comprehensively evaluates the\nLLMs' ability across various categories of math\nproblems. We analyzed the differences in perfor-\nmance in terms of categories. GPT-40 continued\nto demonstrate a clear advantage across most tasks,"}, {"title": "B Rulers for Data Cleaning", "content": "1. Problem Description: The sequence is too dif-\nficult, requiring extensive background knowl-\nedge, or only a limited number of terms are\nfound.\nFiltering Method: Remove sequences with\nkeywords containing 'hard', 'fin' (finite)."}]}