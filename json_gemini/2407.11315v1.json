{"title": "COMET : \"Cone of experience\u201d enhanced\nlarge multimodal model for mathematical problem generation", "authors": ["Sannyuya Liu", "Jintian Feng", "Zongkai Yang", "Yawei Luo", "Qian Wan", "Xiaoxuan Shen", "Jianwen Sun"], "abstract": "The automatic generation of high-quality mathematical problems is practically valuable in many educational scenarios. Large multimodal model provides a novel technical approach for the mathematical problem generation because of its wide success in cross-modal data scenarios. However, the traditional method of separating problem solving from problem generation and the mainstream fine-tuning framework of monotonous data structure with homogeneous training objectives limit the application of large multimodal model in mathematical problem generation. Addressing these challenges, this paper proposes COMET, a \"Cone of Experience\" enhanced large multimodal model for mathematical problem generation. Firstly, from the perspective of mutual ability promotion and application logic, we unify stem generation and problem solving into mathematical problem generation. Secondly, a three-stage fine-turning framework guided by the \"Cone of Experience\" is proposed. The framework divides the fine-tuning data into symbolic experience, iconic experience, and direct experience to draw parallels with experiences in the career growth of teachers. Several fine-grained data construction and injection methods are designed in this framework. Finally, we construct a Chinese multimodal mathematical problem dataset to fill the vacancy of Chinese multimodal data in this field. Combined with objective and subjective indicators, experiments on multiple datasets fully verify the effectiveness of the proposed framework and model.", "sections": [{"title": "1 Introduction", "content": "As a vital driving force leading the revolution of technological and industrial development, generative artificial intelligence (GenAI) is restructuring various industries. For education, the impact of GenAI is unprecedented (Wang et al., 2023a). The Large Language Model (LLM), as one of the most representative technologies of GenAI, displays excellent capabilities in text generation and processing (Achiam et al., 2023; Ouyang et al., 2022; Sun et al., 2023). The Large Multimodal Model (LMM) further expands the data boundaries of the LLM and has achieved widespread success in cross-modal tasks (including image captioning and visual question answering) (Liu et al., 2024a; Li et al., 2023; Chen et al., 2023). The integration and application of LMM has become a key approach to promote the digital transformation of education, since most of the teaching resources and records in the educational scenario are multimodal data.\nIn recent years, many researchers have been exploring the possibilities of combining LMM with education, such as assisted writing(Liu et al., 2024b) and emotional support(Lissak et al., 2024). However, there is still a lack of relevant research in the generation of educational resources, especially in the field of mathematical problem generation. The shortage of high-quality educational resources is one of the main contradictions in the digitization of education. As shown in Figure 1, a high-quality mathematical problem needs to be carefully designed by domain experts and meet multiple requirements. First of all, completeness. During the teaching process, the mathematical problem is for teachers, students, and parents concurrently. Therefore it should contain four parts: mind of design,"}, {"title": null, "content": "stem, mind of solution, and answer, all with fluent language and correct logic. Secondly, precision. The mathematical problem should accurately reflect the objectives of the curriculum, be highly related to given knowledge points, and provide the function of exercises and tests. Lastly, differentiation. For certain key knowledge points under investigation, the mathematical problem should differentiate in theme, problem type, difficulty level, etc., to better serve complex and diverse learning needs.\nIn summary, constructing high-quality mathematical problems requires the ability to generate both stems and solutions to form a complete closed loop. Traditionally, the studies of mathematical problem generation are divided into two independent subfields, namely stem generation(Polozov et al., 2015) (some works simply record as problem generation) and problem solving(Kushman et al., 2014). These studies mostly design rules or deep neural networks to achieve reasoning, but are ineffective due to limitations in feature engineering and model capabilities, and the research paradigm that separates stem generation and problem solving does not meet the application requirement in educational scenarios. LLM, which provides a novel approach for mathematical problem generation, can not only generate coherent and logical replies against cross-modality data, but also respond to diverse demands because of its ability to in-context learning and instruction following. However, there are still challenges when directly applying the existing LMM to mathematical problem generation. Firstly, current work mostly focuses only on enhancing one aspect of the abilities of LLM in stem generation or problem solving, with little research proposing methods to simultaneously enhance both aspects of the model on the scale of multimodality. Secondly, general LMMs have learned abundant general concepts from a massive amount of pre-training data, but lack specialized knowledge needed for mathematical problem generation. Thirdly, implementing domain fine-tuning based on general LMM is currently the basic paradigm to execute domain task transfer. Previous data structure, as well as construction methods, are simple, and the training objectives are not diverse enough, thus it's hard to fully adapt to the application requirements of the target domain.\nTo address the above issues, this paper proposes a \"Cone of Experience\" enhanced large multi-modal model for mathematical problem generation (COMET). Firstly, stem generation and problem solving are unified into mathematical problem generation tasks. Intuitively, the professional knowledge and practical experience required for stem generation and problem solving share commonalities. Integrating the two abilities into a single model can benefit the promotion of each other, and is more practically logical in educational scenarios. Secondly, inspired by the \"Cone of Experience\" theory proposed by American educator Edgar Dale (Dale, 1947), we propose a three-stage fine-turning framework. The \"Cone of Experience\" divides human learning experience into three layers: symbolic experience, iconic experience, and direct experience. The experiences of different layers are interconnected and only by fully integrating all three layers can high-quality learning be achieved. From the perspective of Data-centric AI (DCAI) (Zha et al., 2023), we believe that the depth and breadth of transfer training are key to domain transfer. Accordingly, for the specific task of mathematical problem generation, we design multiple fine-grained data production methods for the three types of experiences, establish multi-level experience data injection methods, and form a complete fine-tuning framework. Finally, a Chinese multimodal mathematical problem dataset (CMM12K) is formulated, filling the gap in Chinese multimodal corpus in this field. The effectiveness of the framework and model is comprehensively validated with both objective and subjective indicators on multiple datasets.\nThe main contributions of this paper can be summarized as follows:\n\u2022 From the perspective of DCAI, we propose COMET, a \u201cCone of Experience\" enhanced large multimodal model for mathematical problem generation. To the best of our knowledge, this is the first work to systematically enhance mathematical problem generation on a single LMM.\n\u2022 The formal definition of a three-stage fine-tuning framework based on the \u201cCone of Experience\" is provided, together with the data flow production methods for symbolic experience, iconic experience, and direct experience. The corresponding knowledge infusion methods are empirically demonstrated.\n\u2022 A Chinese multimodal mathematical problem"}, {"title": "2 Related Work", "content": "As mentioned above, the complete closed-loop of mathematical problem generation involves two dimensions of stem generation (previous works simply record as problem generation) and problem solving. This section introduces related work from the perspective of technological development.\nEarly studies focus on the design of generation rules and reason templates through summarizing the characteristics and patterns of mathematical problems(Singh et al., 2012). These works accomplish stem generation or mathematical reasoning by combining the concepts, formulas, and theorems (Polozov et al., 2015). Nandhini et al. (Nandhini and Balasundaram, 2011) proposed two stem generation methods based on templates and context-free grammar, generating stem with more diversified structures and semantics. Moura et al. (De Moura et al., 2015) built a knowledge base with a large number of mathematical theorems embedded, providing interactive theorem proving based on rule reasoning. These methods have a certain degree of controllability and a high accuracy rate, but lack problem adaptability and creativity.\nWith the development of machine learning, methods such as decision trees and support vector machine have been widely used to address mathematical stem generation or reasoning, recognizing the structure and patterns of problems by models which are trained based on large amounts of labeled data. Heilman et al. (Heilman, 2011) used a syntactic parser to convert input text into tree representations and designed templates to achieve automatic transformation of problem forms. Roy et al. (Roy et al., 2015; Roy and Roth, 2016) mapped unstructured text to a more easily reasoned representation space to eliminate text ambiguity, which can reason multi-step arithmetic problems. Deep learning further provides more powerful models, including seq2seq model, attention mechanism, graph network, etc., providing new approaches for the generation and reasoning of a wide range of mathematical problems such as arithmetic, algebra, and geometry (Wu et al., 2022; Liu et al., 2019; Cao et al., 2021; Wang et al., 2018; Chen et al., 2021). Zhou et al. (Zhou and Huang, 2019) first proposed a seq2seq model based on the attention mechanism, which generated the stem of applied problems given equations and mathematical topics, significantly improving the quality and diversity of generation. Wang et al. (Wang et al., 2017) used recurrent neural network to transform math word problems into equations and based on similarity retrieval to improve reasoning performance. Group-ATT (Li et al., 2019) applied multi-head attention to extract the global feature, numerical feature, and quantity pair feature of math word problems, achieving significant performance improvement on the Math23K dataset.\nThe mathematical problem generation has entered a new stage because of the sharply developed and applied LLM (Christ et al., 2024; Drori et al., 2022; Yue et al., 2023; Zhou et al., 2023), since by training on large amounts of corpus LLMs can understand complex language structures including mathematical problems. In terms of stem generation, Droria et al. (Drori et al., 2022) use OpenAI Codex (LLM of code data fine-tuning) to approach human-level in generating college-level mathematical stems. Zong et al. (Zong and Krishnamachari,\n2023) based on the few-shot learning prompt GPT-3 to achieve the generation of related topics. In terms of problem solving, WizardMath (Luo et al., 2023) proposed a reinforcement learning from the evol-instruct feedback method to construct more complex instruction datasets, enhancing the mathematical reasoning ability of LLaMA-2. MathGLM (Yang et al., 2023) demonstrates proficient multi-digit arithmetic ability with only 2B parameters. MathPrompter (Imani et al., 2023) increases the credibility of the output result by generating multiple algebraic expressions or Python functions to solve the same problem based on zero-shot chain of thought. TORA (Gou et al., 2023) integrates computational libraries and symbolic solvers to solve complex mathematical problems."}, {"title": "3 Methodology", "content": "Figure 2 is a schematic diagram of the three-stage fine-tuning framework. The entire fine-tuning process is guided by the \u201cCone of Experience\u201d, injecting symbolic experience, iconic experience, and direct experience. This section first defines the global fine-tuning goals and notations, decomposing the application requirements of the target domain into three subtasks for reinforcement. Then, the three-stage fine-tuning framework is expanded according to the type of injected experience, elaborating on the definitions, construction methods, and training methods."}, {"title": "3.1 Problem Formulation", "content": "To effectively apply LMM in teaching scenarios, this work mainly enhances three capabilities of LMM during the domain fine-tuning process: controllable generation (CG), analogy generation (AG), and fine-grained solving (FS) for mathematical problems. Both CG and AG reflect the ability of LMM to generate problems, the difference being that the former generates the mind of design and original problem according to given requirements (such as problem type, knowledge point, difficulty level, etc.), while the latter understands and transforms the seed problems (such as changing topic and type, expanding knowledge point or adjusting difficulty level). The FS reflects the problem-solving capacity of LMM, emphasizing the importance of producing detailed solution steps similar to textbook references.\nFor LMM, the instructions for the above three tasks can be formally defined as follows:\n1. Given the problem type $t$, knowledge point $c$, difficulty-level $d$ and grade level $g$, the CG prompt is constructed as $q_c = F_c(t, c, d, g)$.\n2. Given the seed problem $s \\in S$, the AG prompt is constructed as $q_a = F_a(s)$.\n3. Suppose a math problem is $p$, the prompt identifier of FS is $q_s = F_s(p)$.\nThe $F_c, F_a, F_s$ can be flexibly designed according to the scene, and the settings in this work can be seen in Section 3.4. Please note that in this paper, $\u00e6$ represents a vector or a string, $x$ represents a scalar or a single character, $X$ represents a set, and $X$ represents a function.\nThe task requirements are defined as $q_{in} \\in {q_c, q_a, q_s}$. This work can be defined as performing three-stage fine-tuning based on the general LMM $F_{lmm}^{(0)}$, combined with the \u201cCone of Experience\" theory, to obtain an LMM $F_{lmm}^{(3)}$ that meets the application requirements of mathematical problem generation in the teaching scene, so as to maximize the following conditional probability:\n$$P_{lmm}(m | q_{in}; \\theta^{(3)}) =\\prod_{m}^{N_m} P_{lmm}(w_i | q_{in}, w_{k}; \\theta^{(3)}).$$\n$$P_{lmm}(a | q_{in}, m; \\theta^{(3)}) =\\prod_{k=1}^{N_a} P_{lmm}(w_k | q_{in}, m, w_{k}; \\theta^{(3)}).$$\nwhere $\\theta^{(3)}$ is the parameters of LMM $F_{lmm}^{(3)}$, \u2225 represents the string concatenation operation. $m = {w_1, w_2, ..., w_{N_m}}$ represents the mind of design or problem-solving steps generated by LMM, and $a = {w_1, w_2, ..., w_{N_a}}$ represents the original problem or final answer generated by the LMM."}, {"title": "3.2 Symbolic Experience: Learning through Abstractions", "content": "This paper defines symbolic experience as the background knowledge related to the target domain, or the prerequisite knowledge required to carry out the target task. Symbolic experience does not directly help the model solve specific tasks, but it provides strong support. For mathematical problem generation, we summarize symbolic experience into four types for production: book knowledge, graph knowledge, arithmetic knowledge, and general knowledge.\nThe data sources of book knowledge include textbooks, lecture notes, teachers' books, pedagogy, and psychology books, aiming to build teaching concepts and supplement subject knowledge. Through methods such as web crawling, OCR, and manual annotation, we complete data collection and pre-processing (de-duplication, noise reduction, etc.) via both online and offline channels. The number of book knowledge tokens sorted out in this work is approximately 140M.\nWe construct a large heterogeneous subject knowledge graph, where the node types include grade, knowledge points, concept descriptions, and example problems. This graph encompasses 1,225 knowledge points and related concepts from elementary to junior high school, providing approximately 18,000 example problems. To train LMM using structured data, we design a graph sampling method based on random walk to extract diversified and differentiated disciplinary information. Then GPT4(V) is used to transform the edge information into a concatenated text, thereby generating graph knowledge for symbolic experiences. Specifically, the heterogeneous subject knowledge graph is represented as $G =< {N_c, N_g, N_d, N_p}, E >$, where $N_c, N_g, N_d, N_p$ represent the node sets of knowledge points, grade, concept descriptions, and related example problems. $E$ is the set of edges between all nodes. The generation process of graph knowledge can be seen in Algorithm 1, which generates two types of training samples: a whole link learning sample (Sample_1) is formed as a four-tuple {grade, knowledge point, concept description, example problem}, and a relationship learning sample (Sample_2) formed by the concatenation of multiple adjacent knowledge points, totaling 220M tokens.\nThe function of arithmetic knowledge is to compensate for the shortcomings of LMM in arithmetic, to reduce the probability of numerical errors occurring in the mathematical reasoning process. It is an equation consisting of pure numbers and mathematical operators. We directly use the arithmetic dataset proposed by Yang et al. (Yang et al., 2023). This dataset is carefully designed, containing not only operations such as addition, subtraction, multiplication, division, and exponentiation, but also various numerical formats such as integers, decimals, percentages, fractions, and negative numbers. In this work, approximately 200M tokens are extracted as fine-tuning data for arithmetic knowledge.\nWe extracted approximately 220M tokens of generic data (including plain text, single-turn, and multi-turn Q&A) from open-source corpora, such as Wikipedia, SkyPile-150B(Wei et al., 2023), MOSS(Sun et al., 2023) and BELLE(Ji et al., 2023),"}, {"title": "3.3 Iconic Experience: Learning through Observation", "content": "The iconic experience is defined as the data generated by the subject in the process of performing the target task, which includes not only human experts proficient in the target task but also the large model. Injecting the iconic experience aims to allow LMM to learn mathematical problem generation from humans and improve upon the failed reasoning data produced by other LMMs. This paper summarizes the iconic experience into three types of production: the experience of stem generation, problem solving, and failure.\nTo construct stem generation experience, we first collect exercises and test items covering all grades from elementary to junior high school. Next, based on manual annotation methods, we extracted key information from math problems in several dimensions including educational grade, problem type, knowledge points, and difficulty, and deduced problem requirements in reverse. Finally, we constructed a query-problem pair, with manual writing examples of mind of design, and used GPT4(V) for bulk supplementation of question making ideas in a few-shot manner. The final data form is {problem requirement, mind of design, original problem}.\nTo construct problem solving experience, we hire normal school students to write analyses and answers for the collected mathematical problems. However, due to differences in cognitive levels and writing styles between individuals, it is difficult to align the granularity of the analyses. To generate fine-grained analyses, we use GPT4(V) to generate high-quality analyses with consistent writing styles based on manually parsed data. Three generation methods are proposed:\n1. The task requires GPT4(V) to directly solve the problem: {q} \u2192 {s}.\n2. The task requires GPT4(V) to fill in the middle process when both the problem and answer are given: {q, a} \u2192 {s}.\n3. When the complete problem, analyses, and answer are given, GPT4(V) is required to rewrite the analyses: {q, s, a} \u2192 {s'}.\nWe chose the second method as the data production method for this stage due to its stability. The final data form is {mathematical problem, mind of solution, final answer}.\nFailure experience is mainly generated by LMMs that have not been domain-adapted. First, a collaborative environment consisting only of LMMs is built, among which GPT4(V) plays the role of the discriminator, and multiple LMMs (such as Qwen-VL-Chat, Yi-VL-6/34B, etc.) play the role of generators. Secondly, two generators are randomly assigned to complete the task of mathematical problem generation, and then the discriminator guides and evaluates the degree of completion. Finally, the summarized procedural data forms a sample in the format {task instruction, collaboration information, guidance feedback}.\nIn this stage, the data pertaining to the iconic experience is learned by the LMM in the form of instruction Tuning. All data is arranged in a query-"}, {"title": "3.4 Direct Experience: Learning by Doing", "content": "The direct experience is defined as the procedural data generated when the fine-tuned object carries out the target task with results feedback. Such experience aims to correct the inference preference of the LMM with higher-order domain values, allowing it to embodied evolve during the practice.\nFirstly, we design a set of task instructions for three subtasks (CG, AG, and FS). For CG, the focus of the prompt design is to highlight the controllable elements in the generation process. This paper mainly considers four controllable factors in the problem generation process: grade, problem type, knowledge point, and difficulty level, and requires giving out the mind of design. For AG, the prompt design focuses on asking the model to first understand the seed problem to initially judge the important elements such as problem type and knowledge points, and then give the mind of design and rewrite the problem in the form of chain of thought. For FS, the core concept of the prompt design is to clearly require the model to generate a detailed analysis process rather than just outputting an answer. All prompts designed in this work for the three tasks are shown in Figure 3.\nSecondly, the LMM can produce multiple different responses to the same query due to the randomness of its reasoning. We order the preferences of multiple responses corresponding to the same instruction. This paper utilizes human preferences (manual annotation) and model preferences (GPT4(V) generation) during the preference ranking process. The final data form is {task instruction, high preference response, low preference response}.\nThe fine-tuning stage uses the direct preference optimization (Rafailov et al., 2024) (DPO) algorithm to infuse direct experience into LMM, the loss function is as follows:\n$$Loss_{3st}(\\theta^{(2)}, \\theta^{(3)}) = -E_{x,y_w,y_l \\sim D}[log \\sigma(\\beta log \\frac{P(y_w | x; \\theta^{(3)})}{P(y_w | x; \\theta^{(2)})} - \\beta log \\frac{P(y_l | x; \\theta^{(3)})}{P(y_l | x; \\theta^{(2)})} )].$$ \nthe $\\theta^{(3)}$ uses $\\theta^{(2)}$ as the initial solution, for the same input $x$, $y_w$ and $y_l$ represent the preferred solution and the non-preferred solution."}, {"title": "4 Experiments", "content": "4.1 Implementation\nWe conduct the \"Cone of Experience\" enhanced three-stage fine-tuning based on the well-trained Qwen-VL-Chat(Bai et al., 2023) provided by Alibaba Cloud. For all fine-tuning stages, Adam with different learning rates is used as the optimizer, and the gradient truncation threshold is set to 0.5. We incorporate a warmup ratio of 0.05 and employ the batch size of 64. To control overfitting, we apply a weight decay of 0.1. The Max token is uniformly set to 2,048, and the data is spliced or truncated in the pre-processing stage to improve training efficiency or reduce information loss. In addition, we employ deepspeed with ZeRO-2 stage(Rajbhandari et al., 2020) to improve parallel efficiency for speed up training.\nIn the first and second stages, we use LoRA (Hu et al., 2021) to perform parameter-efficient fine-tuning, then set rank, alpha and dropout to 16, 32 and 0.05. All linear layers (including the image encoder) of LMM except the head layer are designated to apply the LoRA adapter. Among them, the learning rate of the first stage is set to 2 \u00d7 10\u22125, and halved in the second stage. We use the DPO(Rafailov et al., 2024) algorithm to inject direct experience for learning reasoning preferences in the third stage. The learning rate is 5 \u00d7 10\u22125, the DPO smoothing value is 0.1. To ensure reproducibility, the random seed is set to 42 during the whole experiment. The three-stage fine-tuning is performed on 8 NVIDIA A800-80G. For one epoch, the three-stage fine-tuning takes about 200, 50, and 20 GPU hours. In the test stage, the inference parameters of LMM are uniformly set top_k to 20, top_p to 0.7, repetition_penalty to 1, and temperature to 0.3."}, {"title": "4.2 Dataset and Baseline", "content": "The datasets used are shown in Table 1. GSM8K(Cobbe et al., 2021) is an English single-"}, {"title": "4.3 Metrics", "content": "This study designs three types of evaluation criteria for different capabilities of LMM.\nScoring mode based on GPT4(V). Multiple scoring dimensions are designed for controllable generation (CG), analogy generation (AG), and fine-grained solving (FS). The scoring dimensions of CG include language fluency (LF) (both mathematical terms and formulas), logical correctness (LC), content completeness (CC) (both ideas and stems), knowledge point relevance (KR), difficulty appropriateness (DA) and type adaptability (TA). The scoring dimensions of AG include language fluency (LF), logical correctness (LC), content completeness (CC), reasoning rationality(RR), and seed relevance (SR). The scoring dimensions of FS include language fluency (LF), logical correctness (LC), analytical completeness (AC), and answer accuracy (AA). The GPT4(V) is required to give a score and reason in the range of 1 to 10 according to the dimension.\nArena mode based on GPT4(V). Considering the subjectivity of mathematical problem generation, GPT4(V) is introduced as a referee to comprehensively rule on different responses of the same query from aspects such as accuracy, fluency, and values. Specifically, we calculate a rating value for each LMM to represent the ability on a certain task. During the judging process by GPT4(V), the ELO rating algorithm(Elo, 1967; Zheng et al., 2024) is used to update the rating value of the participating LMM. Assume the initial ELO rating value is 1,000. In M rounds of competition, two LMMs (called LMM-x and LMM-y) are randomly selected to reply to the same query each time. According to the ruling results of GPT4(V), the rating value is calculated as follows:\n$$E_x = \\frac{1}{1 + 10^{(R_x - R_y)/400}},$$  $$R_i = R_i + K \\cdot (P_K(i) - E_i).$$  where $R_x$ and $R_y$ respectively represent the rating values of LMM-x and LMM-y in the previous round, $E_x$ and $E_y = 1 - E_x$ represent the current expected rating value. $R_i (i \\in {x,y})$ represents the updated rating value of LMM-i, $P_K(i) \\in {0, 1}$ is a boolean function that identi-"}, {"title": "5 Result and Discussion", "content": "5.1 Performance of Controllable Generation\nScoring mode. We employed GPT4(V) to evaluate the responses of LMMs to CG tasks on the test set and subsequently reported the average scores across six dimensions. As illustrated in Figure 4(a), our model outperforms all other baselines in 4 of 6 dimensions (LF, LC, KR, and DA).\nFigure 4(b) shows that our model performs comparably to Yi-VL-34B across the six dimensions of CG tasks. Models with comparable parameter levels, such as Qwen-VL-Chat (7B), LLaVA-1.6 (7B), and Yi-VL-6B, demonstrate slightly inferior performance, while the larger parameter-scale model, CogVLM with 17B parameters, exhibits relatively lower performance.\nAlthough our model slightly lags behind Yi-VL-34B in terms of content completeness and problem type adaptability, it's important to note that our parameter count is approximately five times smaller than that of Yi-VL-34B. Therefore, this discrepancy may arise from limitations imposed by the scale of parameters, which could hinder the comprehension and processing of contextual information.\nThrough a three-stage fine-tuning process guided by the \"Cone of Experience\", our model can effectively focus on generating responses that prioritize controllable factors such as language fluency, logical correctness, knowledge point relevance, and difficulty appropriateness.\nArena mode. We conducted approximately 3, 600 competitions, where two LMMs were ran-"}, {"title": "5.3 Performance of Fine-grained Sovling", "content": "Scoring mode. We report the scores for 4 dimensions in the FS task of all LMMs on the test set. The average scores across various evaluation dimensions are presented in Figure 4(a), where our model achieved the state-of-the-art (SOTA) in 3 of 4 evaluation dimensions(LC, AC, and AA), falling slightly short only in language fluency (LF) compared to Yi-VL-34B. Figure 4(b) depicts the ranking across all evaluation dimensions, further validating the superiority of our model, which maintains an absolute lead in most dimensions with a relatively smaller parameter size (7B). Moreover, by comparing with the backbone model Qwen-VL-Chat, we can conclude that the three-stage fine-tuning guided by the \"Cone of Experience\" significantly enhances its FS capabilities.\nArena mode. As illustrated in Figure 5(c), after 3, 600 competition rounds, our model triumphs over all baselines with the highest ELO Rating. The average ELO rating leads the backbone model Qwen-VL-Chat by approximately 20%, and also maintains an advantage against Yi-VL-34B. However, analyzing from the perspective of win rates reflected in the heatmap, our model lags behind Yi-VL-34B with a slight disadvantage, as it has a win rate of only 49%.\nBased on the scoring results of GPT4(V) and the arena outcomes, it can be concluded that, in the FS task, our model demonstrates significant performance improvement compared to the backbone model Qwen-VL-Chat, surpassing baselines of the same parameter scale comprehensively, and even shows certain advantages compared to baselines approximately five times its size."}, {"title": "5.4 Result of Objective Evaluation", "content": "Performance of Objective Problems. As shown in Table 2, our model dramatically outperforms the backbone model Qwen-VL-Chat in terms of accuracy on GSM8K and TAL-SCQ5K-CN (+10.62%, +5.85%), achieving state-of-the-art (SOTA) on TAL-SCQ5K-CN. Although Yi-VL-34B leads on GSM8K, its parameter size, which is 5 times larger than ours, implies greater training cost and time.\nOn CMM12K, our model's overall score of 33.84% remarkably exceeds all baselines, with approximately an 8% performance advantage over the second-place Yi-VL-34B. Specifically, we conducted statistics on two modalities and three problem types, totaling 2\u00d73 = 6 categories. The results show that our model achieved SOTA in 5 of 6 categories, only slightly lagging behind Yi-VL-34B"}, {"title": "5.5 Ablation Study", "content": "To validate the effectiveness of the three-stage fine-tuning framework proposed in this paper, we designed the following ablation experiments. We refer to the model after the ith stage of fine-tuning as Si, where i \u2208 {1,2,3}. We preserved all checkpoints of the fine-tuning stages: SO - the original backbone model Qwen-VL-Chat without any fine-tuning. S1 - the model after the first stage of fine-tuning, injected with symbolic experience. S2 - based on S1, the model after the second stage of fine-tuning, infused with iconic experience. S3 - based on S2, the model after the third stage of fine-tuning, incorporating direct experience.\nWe first obtained these four models' responses on the test set regarding the CG, AG, and FS tasks, and scored them based on GPT4(V) in 15 fine-grained dimensions. Figure 6 shows the score"}, {"title": "5.6 Case Study", "content": "To demonstrate the strength of our model, we have selected some examples of CG, AG, and FS tasks respectively. Previous results, such as Figure 4, have already proven that Yi-VL-34B - a model with more than five times the number of parameters as ours, is a comparable competitor. Thus, for each task, we show the difference in response quality between our model, the backbone model Qwen-VL-Chat, and Yi-VL-34B when using the same prompt, the case details refer to Figure 7.\nCase of CG In the example shown in Figure 7(a), LMM was asked to generate one problem based on the given planar geometric picture. Our model accurately captured the geometric elements in the picture and expressed the test problem using the correct mathematical language. In contrast, Qwen-VL-Chat failed to comprehend the content of the given picture, erroneously providing the condition (\u25b3ADC ~ \u25b3AEC, but both ADC and AEC are not triangles). For Yi-VL-34B, the problem it constructed was not based on the given image, hence not aligning with requirements.\nCase of FS Given this problem, Qwen-VL-Chat correctly understood the elements in the picture, but its erroneous reasoning steps (AD=AC=3cm is given, but actually AD=2AC=6cm) led to an incorrect final result. Yi-VL-34B made similar mistakes as Qwen-VL-Chat. However, our model first parsed the problem requirements, and then extracted the geometric elements of the given picture, and finally correctly reasoned step by step according to the problem to arrive at the correct answer (DB=4cm).\nCase of AG We require LMMs to simulate the seed problem and construct a new problem. Each LMM first analyzes the ideas for the construction of the seed problem and then constructs a problem, and they are also asked to explain the thought process behind the constructed problem.\nOur model first understands the meaning of the problem, parses the content of the knowledge"}, {"title": "6 Conclusion", "content": "In this work, we propose COMET, a \u201cCone of Experience\" enhanced large multimodal model for mathematical problem generation. Inspired by the \"Cone of Experience\u201d theory, we follow the growth process of teachers to define the experience as symbolic, iconic, and direct. Based on this, we design a three-stage fine-tuning framework to enhance the capabilities of problem generation and problem solving within a single LMM to meet the requirements of educational applications. Moreover, a Chinese multimodal mathematics problem dataset (CMM12K) is built to alleviate the scarcity of Chinese multimodal corpora in this field. Extensive experiments have demonstrated the advancement and effectiveness of the proposed model. In the future, we will explore retrieval-enhanced generation methods based on model recall behavior, since the proposed direct experience can potentially serve as LMM historical memory."}]}