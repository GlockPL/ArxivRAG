{"title": "COMET : \"Cone of experience\u201d enhanced\nlarge multimodal model for mathematical problem generation", "authors": ["Sannyuya Liu", "Jintian Feng", "Zongkai Yang", "Yawei Luo", "Qian Wan", "Xiaoxuan Shen", "Jianwen Sun"], "abstract": "The automatic generation of high-quality math-\nematical problems is practically valuable in\nmany educational scenarios. Large multimodal\nmodel provides a novel technical approach for\nthe mathematical problem generation because\nof its wide success in cross-modal data scenar-\nios. However, the traditional method of separat-\ning problem solving from problem generation\nand the mainstream fine-tuning framework of\nmonotonous data structure with homogeneous\ntraining objectives limit the application of large\nmultimodal model in mathematical problem\ngeneration. Addressing these challenges, this\npaper proposes COMET, a \"Cone of Experi-\nence\" enhanced large multimodal model for\nmathematical problem generation. Firstly, from\nthe perspective of mutual ability promotion\nand application logic, we unify stem genera-\ntion and problem solving into mathematical\nproblem generation. Secondly, a three-stage\nfine-turning framework guided by the \"Cone\nof Experience\" is proposed. The framework\ndivides the fine-tuning data into symbolic ex-\nperience, iconic experience, and direct experi-\nence to draw parallels with experiences in the\ncareer growth of teachers. Several fine-grained\ndata construction and injection methods are de-\nsigned in this framework. Finally, we construct\na Chinese multimodal mathematical problem\ndataset to fill the vacancy of Chinese multi-\nmodal data in this field. Combined with objec-\ntive and subjective indicators, experiments on\nmultiple datasets fully verify the effectiveness\nof the proposed framework and model.", "sections": [{"title": "1 Introduction", "content": "As a vital driving force leading the revolution of\ntechnological and industrial development, gener-\native artificial intelligence (GenAI) is restructur-\ning various industries. For education, the impact\nof GenAI is unprecedented (Wang et al., 2023a).\nThe Large Language Model (LLM), as one of the\nmost representative technologies of GenAI, dis-\nplays excellent capabilities in text generation and\nprocessing (Achiam et al., 2023; Ouyang et al.,\n2022; Sun et al., 2023). The Large Multimodal\nModel (LMM) further expands the data boundaries\nof the LLM and has achieved widespread success in\ncross-modal tasks (including image captioning and\nvisual question answering) (Liu et al., 2024a; Li\net al., 2023; Chen et al., 2023). The integration and\napplication of LMM has become a key approach\nto promote the digital transformation of education,\nsince most of the teaching resources and records in\nthe educational scenario are multimodal data.\nIn recent years, many researchers have been\nexploring the possibilities of combining LMM\nwith education, such as assisted writing(Liu et al.,\n2024b) and emotional support(Lissak et al., 2024).\nHowever, there is still a lack of relevant research in\nthe generation of educational resources, especially\nin the field of mathematical problem generation.\nThe shortage of high-quality educational resources\nis one of the main contradictions in the digitiza-\ntion of education. As shown in Figure 1, a high-\nquality mathematical problem needs to be carefully\ndesigned by domain experts and meet multiple re-\nquirements. First of all, completeness. During the\nteaching process, the mathematical problem is for\nteachers, students, and parents concurrently. There-\nfore it should contain four parts: mind of design,"}, {"title": "2 Related Work", "content": "As mentioned above, the complete closed-loop of\nmathematical problem generation involves two di-\nmensions of stem generation (previous works sim-\nply record as problem generation) and problem\nsolving. This section introduces related work from\nthe perspective of technological development.\nEarly studies focus on the design of generation\nrules and reason templates through summarizing\nthe characteristics and patterns of mathematical\nproblems(Singh et al., 2012). These works accom-\nplish stem generation or mathematical reasoning by\ncombining the concepts, formulas, and theorems\n(Polozov et al., 2015). Nandhini et al. (Nandhini\nand Balasundaram, 2011) proposed two stem gener-\nation methods based on templates and context-free\ngrammar, generating stem with more diversified\nstructures and semantics. Moura et al. (De Moura\net al., 2015) built a knowledge base with a large\nnumber of mathematical theorems embedded, pro-\nviding interactive theorem proving based on rule\nreasoning. These methods have a certain degree of\ncontrollability and a high accuracy rate, but lack\nproblem adaptability and creativity.\nWith the development of machine learning, meth-\nods such as decision trees and support vector ma-\nchine have been widely used to address mathemati-\ncal stem generation or reasoning, recognizing the"}, {"title": "3 Methodology", "content": "Figure 2 is a schematic diagram of the three-stage\nfine-tuning framework. The entire fine-tuning pro-\ncess is guided by the \u201cCone of Experience\u201d, inject-\ning symbolic experience, iconic experience, and di-\nrect experience. This section first defines the global\nfine-tuning goals and notations, decomposing the\napplication requirements of the target domain into\nthree subtasks for reinforcement. Then, the three-\nstage fine-tuning framework is expanded according\nto the type of injected experience, elaborating on\nthe definitions, construction methods, and training\nmethods."}, {"title": "3.1 Problem Formulation", "content": "To effectively apply LMM in teaching scenarios,\nthis work mainly enhances three capabilities of\nLMM during the domain fine-tuning process: con-\ntrollable generation (CG), analogy generation (AG),\nand fine-grained solving (FS) for mathematical\nproblems. Both CG and AG reflect the ability of\nLMM to generate problems, the difference being\nthat the former generates the mind of design and\noriginal problem according to given requirements\n(such as problem type, knowledge point, difficulty\nlevel, etc.), while the latter understands and trans-\nforms the seed problems (such as changing topic\nand type, expanding knowledge point or adjust-\ning difficulty level). The FS reflects the problem-\nsolving capacity of LMM, emphasizing the impor-\ntance of producing detailed solution steps similar\nto textbook references.\nFor LMM, the instructions for the above three\ntasks can be formally defined as follows:\n1. Given the problem type t, knowledge point c,\ndifficulty-level d and grade level g, the CG\nprompt is constructed as \\(q_{c} = F_{c}(t, c, d, g)\\).\n2. Given the seed problem \\(s \\in S\\), the AG prompt\nis constructed as \\(q_{a} = F_{a}(s)\\).\n3. Suppose a math problem is p, the prompt iden-\ntifier of FS is \\(q_{s} = F_{s}(p)\\).\nThe \\(F_{c}, F_{a}, F_{s}\\) can be flexibly designed accord-\ning to the scene, and the settings in this work can\nbe seen in Section 3.4. Please note that in this pa-\nper, \u00e6 represents a vector or a string, x represents a\nscalar or a single character, X represents a set, and\nX represents a function.\nThe task requirements are defined as \\(q_{in} \\in\n{q_{c}, q_{a}, q_{s}}\\). This work can be defined as perform-\ning three-stage fine-tuning based on the general\n\\(F^{(0)}_{lmm}\\), combined with the \u201cCone of Experi-\nence\" theory, to obtain an LMM \\(F^{(3)}_{lmm}\\) that meets\nthe application requirements of mathematical prob-\nlem generation in the teaching scene, so as to max-\nimize the following conditional probability:\n\\(P_{lmm} (m/q_{in} ; \u03b8^{(3)} ) = \n \n \\prod_{k=1}^{N_{m}} P_{lmm} (w_{k} |q_{in} w_{k} ; \u03b8^{(3)} ).\\)   (1)\n\\(P_{lmm} (a|q_{in} , m; \u03b8^{(3)}) = \n  \\prod_{k=1}^{N_{a}} P_{lmm} (w_{k} |q_{in} +m w_{k} ; \u03b8^{(3)} ).\\)   (2)\nwhere \\(\u03b8^{(3)}\\) is the parameters of LMM \\(F^{(3)}_{lmm}\\), +\nrepresents the string concatenation operation.\nm = {\\(w_{1}, w_{2}, ..., w_{N_{m}}\\)} represents the mind\nof design or problem-solving steps generated by\nLMM, and a = {\\(w_{1}, w_{2}, ..., w_{N_{a}}\\)} represents the\noriginal problem or final answer generated by the\nLMM."}, {"title": "3.2 Symbolic Experience: Learning through Abstractions", "content": "This paper defines symbolic experience as the back-\nground knowledge related to the target domain, or\nthe prerequisite knowledge required to carry out\nthe target task. Symbolic experience does not di-\nrectly help the model solve specific tasks, but it"}, {"title": "3.3 Iconic Experience: Learning through Observation", "content": "The iconic experience is defined as the data gener-\nated by the subject in the process of performing the\ntarget task, which includes not only human experts\nproficient in the target task but also the large model.\nInjecting the iconic experience aims to allow LMM\nto learn mathematical problem generation from hu-\nmans and improve upon the failed reasoning data\nproduced by other LMMs. This paper summarizes\nthe iconic experience into three types of produc-\ntion: the experience of stem generation, problem\nsolving, and failure."}, {"title": "3.4 Direct Experience: Learning by Doing", "content": "The direct experience is defined as the procedural\ndata generated when the fine-tuned object carries\nout the target task with results feedback. Such ex-\nperience aims to correct the inference preference\nof the LMM with higher-order domain values, al-\nlowing it to embodied evolve during the practice.\nFirstly, we design a set of task instructions for\nthree subtasks (CG, AG, and FS). For CG, the fo-\ncus of the prompt design is to highlight the con-\ntrollable elements in the generation process. This\npaper mainly considers four controllable factors in\nthe problem generation process: grade, problem\ntype, knowledge point, and difficulty level, and\nrequires giving out the mind of design. For AG,\nthe prompt design focuses on asking the model to\nfirst understand the seed problem to initially judge\nthe important elements such as problem type and\nknowledge points, and then give the mind of de-\nsign and rewrite the problem in the form of chain\nof thought. For FS, the core concept of the prompt\ndesign is to clearly require the model to generate a\ndetailed analysis process rather than just outputting\nan answer. All prompts designed in this work for\nthe three tasks are shown in Figure 3.\nSecondly, the LMM can produce multiple dif-\nferent responses to the same query due to the ran-\ndomness of its reasoning. We order the prefer-\nences of multiple responses corresponding to the\nsame instruction. This paper utilizes human prefer-\nences (manual annotation) and model preferences\n(GPT4(V) generation) during the preference rank-\ning process. The final data form is {task instruc-\ntion, high preference response, low preference re-\nsponse}.\nThe fine-tuning stage uses the direct preference\noptimization (Rafailov et al., 2024) (DPO) algo-\nrithm to infuse direct experience into LMM, the\nloss function is as follows:\n\\(Loss_{3st} (\u03b8^{(2)}, \u03b8^{(3)}) = \u2212\\mathbb{E}_{x, y_{w}, y_{l}\u223cD} [log \u03c3(\n\u03b2log  \\frac{P(y_{w} |x; \u03b8^{(3)})}{P(y_{w} |x; \u03b8^{(2)})}  \n\u03b2log  \\frac{P(y_{l} |x; \u03b8^{(3)})}{P(y_{l} |x; \u03b8^{(2)})} )].\\)(5)\nthe \\(\u03b8^{(3)}\\) uses \\(\u03b8^{(2)}\\) as the initial solution, for the\nsame input x, \\(y_{w}\\) and \\(y_{l}\\) represent the preferred\nsolution and the non-preferred solution."}, {"title": "4 Experiments", "content": "We conduct the \"Cone of Experience\" enhanced\nthree-stage fine-tuning based on the well-trained\nQwen-VL-Chat(Bai et al., 2023) provided by Al-\nibaba Cloud. For all fine-tuning stages, Adam with\ndifferent learning rates is used as the optimizer, and\nthe gradient truncation threshold is set to 0.5. We\nincorporate a warmup ratio of 0.05 and employ the\nbatch size of 64. To control overfitting, we apply a\nweight decay of 0.1. The Max token is uniformly\nset to 2,048, and the data is spliced or truncated in\nthe pre-processing stage to improve training effi-\nciency or reduce information loss. In addition, we\nemploy deepspeed with ZeRO-2 stage(Rajbhandari\net al., 2020) to improve parallel efficiency for speed\nup training.\nIn the first and second stages, we use LoRA\n(Hu et al., 2021) to perform parameter-efficient\nfine-tuning, then set rank, alpha and dropout to 16,\n32 and 0.05. All linear layers (including the im-\nage encoder) of LMM except the head layer are\ndesignated to apply the LoRA adapter. Among\nthem, the learning rate of the first stage is set to\n2 \u00d7 10\u22125, and halved in the second stage. We\nuse the DPO(Rafailov et al., 2024) algorithm to\ninject direct experience for learning reasoning pref-\nerences in the third stage. The learning rate is\n5 \u00d7 10\u22125, the DPO smoothing value is 0.1. To\nensure reproducibility, the random seed is set to\n42 during the whole experiment. The three-stage\nfine-tuning is performed on 8 NVIDIA A800-80G.\nFor one epoch, the three-stage fine-tuning takes\nabout 200, 50, and 20 GPU hours. In the test stage,\nthe inference parameters of LMM are uniformly\nset top_k to 20, top_p to 0.7, repetition_penalty to\n1, and temperature to 0.3."}, {"title": "4.2 Dataset and Baseline", "content": "The datasets used are shown in Table 1.\nGSM8K(Cobbe et al., 2021) is an English single-"}, {"title": "5 Result and Discussion", "content": "We employed GPT4(V) to evaluate\nthe responses of LMMs to CG tasks on the test\nset and subsequently reported the average scores\nacross six dimensions. As illustrated in Figure 4(a),\nour model outperforms all other baselines in 4 of 6\ndimensions (LF, LC, KR, and DA).\nFigure 4(b) shows that our model performs com-\nparably to Yi-VL-34B across the six dimensions of\nCG tasks. Models with comparable parameter lev-\nels, such as Qwen-VL-Chat (7B), LLaVA-1.6 (7B),\nand Yi-VL-6B, demonstrate slightly inferior per-\nformance, while the larger parameter-scale model,\nCogVLM with 17B parameters, exhibits relatively\nlower performance.\nAlthough our model slightly lags behind Yi-VL-\n34B in terms of content completeness and prob-\nlem type adaptability, it's important to note that\nour parameter count is approximately five times\nsmaller than that of Yi-VL-34B. Therefore, this\ndiscrepancy may arise from limitations imposed\nby the scale of parameters, which could hinder the\ncomprehension and processing of contextual infor-\nmation.\nThrough a three-stage fine-tuning process guided\nby the \"Cone of Experience\", our model can effec-\ntively focus on generating responses that prioritize\ncontrollable factors such as language fluency, log-\nical correctness, knowledge point relevance, and\ndifficulty appropriateness.\nWe conducted approximately\n3, 600 competitions, where two LMMs were ran-"}, {"title": "5.2 Performance of Analogy Generation", "content": "In the AG task, our model demon-\nstrates robust capabilities and absolute superior-\nity. As depicted in Figure 4(a) and (b), firstly,\nour model comprehensively outperforms all base-\nlines in various evaluation dimensions of AG, par-\nticularly surpassing the strong baseline Yi-VL-\n34B with a parameter size five times larger than\nours. Secondly, our backbone model Qwen-VL-\nChat ranks relatively lower among the six mod-\nels, around the 4th to 5th position, trailing behind\nmodels with similar parameter scales such as Yi-\nVL-6B and LLaVA-1.6. However, following the\nthree-stage fine-tuning guided by the \u201cCone of Ex-\nperience\", its AG capabilities have significantly\nimproved, with an average score increase of ap-\nproximately 60% across all dimensions."}, {"title": "5.3 Performance of Fine-grained Sovling", "content": "We report the scores for 4 dimen-\nsions in the FS task of all LMMs on the test set. The\naverage scores across various evaluation dimen-\nsions are presented in Figure 4(a), where our model\nachieved the state-of-the-art (SOTA) in 3 of 4 evalu-\nation dimensions(LC, AC, and AA), falling slightly\nshort only in language fluency (LF) compared to\nYi-VL-34B. Figure 4(b) depicts the ranking across\nall evaluation dimensions, further validating the su-\nperiority of our model, which maintains an absolute\nlead in most dimensions with a relatively smaller\nparameter size (7B). Moreover, by comparing with\nthe backbone model Qwen-VL-Chat, we can con-"}, {"title": "5.4 Result of Objective Evaluation", "content": "As shown\nin Table 2, our model dramatically outperforms"}, {"title": "5.5 Ablation Study", "content": "To validate the effectiveness of the three-stage fine-\ntuning framework proposed in this paper, we de-\nsigned the following ablation experiments. We\nrefer to the model after the ith stage of fine-tuning\nas Si, where i \u2208 {1,2,3}. We preserved all check-\npoints of the fine-tuning stages: SO - the original\nbackbone model Qwen-VL-Chat without any fine-\ntuning. S1 - the model after the first stage of fine-\ntuning, injected with symbolic experience. S2 -\nbased on S1, the model after the second stage of\nfine-tuning, infused with iconic experience. S3 -\nbased on S2, the model after the third stage of\nfine-tuning, incorporating direct experience.\nWe first obtained these four models' responses\non the test set regarding the CG, AG, and FS tasks,\nand scored them based on GPT4(V) in 15 fine-\ngrained dimensions. Figure 6 shows the score"}, {"title": "5.6 Case Study", "content": "To demonstrate the strength of our model, we have\nselected some examples of CG, AG, and FS tasks\nrespectively. Previous results, such as Figure 4,\nhave already proven that Yi-VL-34B - a model with\nmore than five times the number of parameters as\nours, is a comparable competitor. Thus, for each\ntask, we show the difference in response quality"}, {"title": "6 Conclusion", "content": "In this work, we propose COMET, a \u201cCone of Ex-\nperience\" enhanced large multimodal model for\nmathematical problem generation. Inspired by the\n\"Cone of Experience\u201d theory, we follow the growth\nprocess of teachers to define the experience as sym-"}]}