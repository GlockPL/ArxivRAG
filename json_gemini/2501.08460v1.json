{"title": "Towards Zero-Shot & Explainable Video Description by Reasoning over Graphs of Events in Space and Time", "authors": ["Mihai Masala", "Marius Leordeanu"], "abstract": "In the current era of Machine Learning, Transformers have become the de facto approach across a variety of domains, such as computer vision and natural language processing. Transformer-based solutions are the backbone of current state-of-the-art methods for language generation, image and video classification, segmentation, action and object recognition, among many others. Interestingly enough, while these state-of-the-art methods produce impressive results in their respective domains, the problem of understanding the relationship between vision and language is still beyond our reach. In this work, we propose a common ground between vision and language based on events in space and time in an explainable and programmatic way, to connect learning-based vision and language state of the art models and provide a solution to the long standing problem of describing videos in natural language. We validate that our algorithmic approach is able to generate coherent, rich and relevant textual descriptions on videos collected from a variety of datasets, using both standard metrics (e.g. Bleu, ROUGE) and the modern LLM-as-a-Jury approach.", "sections": [{"title": "1. Introduction", "content": "The task of describing the visual content of a given video in natural language, video captioning [1, 5, 18-20, 32], represents a challenge for both the computer vision and natural language processing communities.\nAlthough there is a plethora of methods both from the field of video understanding (object detection and tracking [33], semantic segmentation [8, 42] and action recognition [30, 35]) and that of natural processing (LLM such as ChatGPT [14]), we are still far from understanding how to best bridge the two fields and are still not able to describe in rich natural language the content of videos.\nBefore the recent rise of Visual Large Language Models (VLLMs), existing deep learning methods trained for video description are only able to produce very short captions of videos, being rather close to video classification (where a video could belong to a finite number of classes) than to that of describing in natural language such videos, with rich textual descriptions that could have infinitely many forms. Moreover, such models suffer from overfitting such that once given a video from an unseen context or distribution the quality and accuracy of the description drops, as our evaluations prove. On the other hand, VLLMs have shown impressive results, being capable of generating long, rich descriptions of videos. Unfortunately VLLMs still share some of the same weaknesses as previous methods: they are largely unexplainable and they still rely on sampling frames to process a video. Moreover, top-performing models such as GPT, Claude or Gemini are not open and are only accessible via an paid API.\nWe argue that one of the main reasons why this interdisciplinary cross-domain task is still far from being solved is that we still lack an explainable way to bridge this apparently insurmountable gap. Explainability could provide a more analytical and stage-wise way to make the transition from vision to language that is both trustworthy and makes sense. It is clear that language is grounded in vision, as it describes events happening in the real world and being connected spatially, temporally and semantically, in which objects perform actions and interact in physical or semantic context that could be captured by vision and described by language.\nIn some sense, language \"speaks\" about what vision \"sees\" and it makes sense to think that vision comes first and then is followed by language, an observation that is in agreement with neuroscience studies about human brain development in infants. Given that today's learning models in both vision and language are so impressive, we believe that it is time to fully exploit such existing methods and create procedural methods that can offer the explainable bridge currently so much needed between vision and language. While learning novel vision-language models from data is both important and powerful, the direct path from vision to language by building procedures from the existing state of the art in both fields is left unexplored. Our proposed approach harnesses existing strong pre-trained vision models for a variety of tasks (i.e., action detection, object detection and tracking, semantic segmentation and depth estimation) to build an explicit, grounded representation in the form of a Graph of Events in Space and Time - GEST [22]. Furthermore, this representation is used to build an intermediate textual description (proto-language) that is then converted into a fully fledged rich textual description using text-only LLMs. An overview of our proposed approach is presented in Figure 1."}, {"title": "2. Related Work", "content": "Up until recently, most video captioning models were based on the encoder decoder architecture, using mostly CNNs for encoding the video frames and LSTMs to generate the textual description [20, 32]. Research [18] has been focused on probing different video representations such as ResNet [13], C3D [12] and S3D [23] or CLIP-ViT [27], for improving video captioning quality.\nDosovitskiy [10] showed that the Transformer architecture, which has been initially developed for machine translation, can also be applied in computer vision tasks, out-performing CNNs in image classification tasks. From then on, Transformers have been successfully applied in a broad range of Computer Vision tasks including tasks performed on videos: action recognition [21], video captioning [19] or even multi-modal (vision and language) learning [5, 6, 11]. VALOR [5] uses three separate encoders for video, audio and text modalities and a single decoder for multi-modal conditional text generation. This architecture is pretrained on 1M audible videos with human annotated audiovisual captions, using multi-modal alignment and multi-modal captioning tasks. PDVC [37] frame the dense caption generation as a set prediction tasks with competitive results, compared to previous approaches based on the two-stage \"localize-then-describe\" framework.\nUnified vision and language foundational models are either trained using both images and videos simultaneously [2] or use a two-stage approach [38, 41] in which the first stage contains image-text pairs, followed by a second stage in which video-text pairs are added. This two-stage approach has the advantage of faster training, models can be scaled up easier, and data is more freely available. VAST [7] is a unified foundational model across three modalities: video, audio and text. To alleviate the limited scale and quality of video-text training data, COSA [6] converts existing image-text data into long-form video data. Then an architecture based on ViT [10] and BERT [9] is trained on this new long-form data. GIT [34] is a unified vision-language model with a very simple architecture consisting of a single image encoder and a text decoder, trained with the standard language modeling task. mPLUG-2 [40] builds multi-modal foundational models using separate modules including video encoder, text encoder, image encoder followed by universal layers, a multi-modal fusion module and finally a decoder module.\nWhat all these methods lack is the explainability factor, as the inner representation is opaque. Methods to obtain some of explainability include adding Reasoning Module Networks (RMNs) to guide the text generation process (e.g. for video captioning), including Explainable modules based on objects detected in saliency maps [28] or applying model agnostic techniques such as LIME [24].\nGraph of Events in Space and Time - GEST [22] provides an explicit spatio-temporal representation of stories as they naturally appear in any median (e.g., videos, texts). GEST was previously shown to be a meaningful representation, providing a unified (vision and text) and explainable space in which semantic similarities can be effectively computed [22]. The main elements of GEST are events, represented as nodes and their interactions, in the form of edges. The nodes represent events, ranging from simple to more complex actions, constrained to a specific time and space. GEST edges relate two events and can define any kind of interaction, from temporal to semantic to logical. While previously used for generating videos in this work we implement the GEST concept the other way, starting from real videos towards GEST and finally a rich textual description."}, {"title": "3. Method", "content": "To properly describe all kinds of videos ranging from simple to more complex (e.g., longer, with more actions and actors) you first have to analyze and understand what happens in a video. Furthermore, to ensure this process is explainable we decide to stray away from the current paradigm of sampling frames, processing, and feeding them into a model that builds an inner obfuscated numerical representation. Instead, we aim to harness the power and expressivity of Graphs of Events in Space and Time (GESTs) [22]. Therefore, our first goal is to understand the video, to build a pipeline that given an input video it automatically builds an associated GEST. Then, by reasoning over GEST we build an intermediate textual description in the form of a proto-language that is then converted to natural language description.\nIn summary, our framework consists of two main steps: I. building the Graph of Events in Space and Time by processing and understanding frame level information, followed by reasoning to get an integrated, global view and II. translating this understanding in a rich natural language description by reasoning over GEST via a two-step process. For a complete example, starting from a video, building the GEST followed by the two-step process that generated the final description, see Figure 2"}, {"title": "3.1. Understanding the video - Building the GEST", "content": "In order to build an explicit representation of a video, we exploit existing sources of high-level image and video information: action detection, object detection and tracking, semantic segmentation and depth information. For each frame in a given video, we first extract this information followed by a matching and aggregation step. The output of the action detector includes, for every action a bounding box of the person performing the action together with the name of the action and a confidence score. Starting from this bounding box, we aim to gather all the objects in the vicinity of the person, objects that the actor could interact. First, the original bounding box is slightly enlarged to better capture the surroundings of the person, followed by finding all the objects that touch or intersect the new bounding box, based on information from the object detector and semantic segmentation. The list of objects is further filtered based on the intersection over union of the object and person bounding box with a fixed threshold, followed by depth-based filtering: we compute an average pixel-level depth for the person and the object, and if the depth difference between the person and the objects is between a set threshold, we consider the object close enough (both in \"2D\" based on intersection of bounding boxes and in \"3D\" based on depth) and we keep it in the list. All the objects that are not in the proximity of the person are discarded. Using this process, at this step we save for each action at each frame, information that includes the frame number, the person id (given at this point by the tracking model), the action name and confidence score as given the action detector, possibly involved objects and the bounding box of the person.\nThe next step is aggregating and processing frame-level information into global, video-level data. The first thing we noticed was that the model used for tracking had slight inconsistencies (e.g., changing the assigned id for a person from one frame to another even though the person in question did not move) or certain blind spots (e.g., losing sight of a person for a couple of frames). We noticed that a lot of the time the tracker would lose sight of a person for 5 to 10 consecutive frames. Upon detecting the person again it assigns a new id, as if it was a different person. We solve these short-term inconsistencies by unifying two person ids if they appear close in time (less than 10 frames) and they overlap enough (higher than 0.4 intersection over union). Note that these thresholds were set empirically by manually verifying around 20-25 examples.\nThe lack of consistency of the tracker manifests itself both in short-term and long-term inconsistencies. An example of long-form inconsistency is when a person exists the frame, either due to camera movement or the person moving, and then re-enters the frame at a later time and in a different position. The previous solution can not work for this long-term inconsistency. Instead we are looking for semantic-based solution that is powerful enough for person re-identification while being very fast. For each person detected, in each frame we compute a feature vector based on the HSV histogram. For each pixel in the segmentation/mask of a person we bin the hue, saturation and value and linearize the resulting 3 dimensional space into a vector. We further compare such representations using cosine similarity. Finally, when a person appears in a frame, we compute its representation and compare it to previously seen persons. If the highest similarity exceeds a set threshold, we unify them into a single entity.\nThe next step after person unification, is frame-based action filtering: based on empirically set thresholds, we filter our actions with confidence lower than 0.75 and for each frame keep only the two most confident actions. Then, to ensure a certain robustness, we implement a voting mechanism as follows: for each action in a frame we consider the previous five and the next five frames and if an action appears less than five times in this window of 11 frames, we discard it. This voting mechanism alleviates some of the inconsistencies and ensures a smoother action space.\nArmed with this rich frame-level information we proceed to build the video-level representation. The first step is aggregating actions that appear in consecutive frames in events by saving the start and end frame ids, possible objects involved (union over objects at each frame, keeping objects that appear at least in 10% of frames between start and end frame) and bounding boxes. Finally, we perform an additional unification step in which we aim to detect cases in which we find events with the same actors and the same action that are close in time (e.g. one starts at frame 10 and ends at frame 120 while the second starts at frame 130 and ends at frame 250) but are considered two different events. As such, we unify such events, again to make the final event-space less fragmented and more coherent.\nAt this moment in time we have a list of events and for each event have actors, objects, timeframe (start and end frame ids) and location (bounding boxes). The last step in this entire pipeline in building spatio-temporal relationships between events. As both temporal and spatial information is readily available for each event, this is a rather straightforward process: we build pairs of events and if they meet certain criteria we link them in space or time. For spatial relations between two events that have an overlap in time, for each such frame, we are interested in the two actions being close in space. Therefore we compute the ratio between the Euclidean distance of the centroids and the sum of the diagonals and if this ratio is lower than a certain threshold we consider that the two actions are related (i.e., close) in space. If this happens for more than 75% of the overlapping frames we consider the events to be close in space and mark them accordingly (i.e. build an edge, a spatial relation in the graph between the two events). For temporal relations we follow a similar approach, we are checking pair of events, characterize three types of temporal relations: next, same time and meanwhile.\nThis leaves us with an over-complete graph, as it contains an over-complete set of possible objects for each event. Better grounding and obtaining a concrete GEST can be obtained in a variety of ways including picking objects based on proximity to the person or by the \"temporal\u201d size (number of frames in which is close to the person). We solve this at a later stage in the pipeline, by allowing an LLM to pick the most probable object. For more details see the following section.\nFor action detection, we use VideoMAE [30] finetuned on AVA-Kinetics [17]. Object detection and tracking are performed using the YOLO pipeline [15], while semantic segmentation is performed using Mask2Former [8]. Finally, Marigold [16] is used to compute depth estimation."}, {"title": "3.2. Generating a natural language description", "content": "Translating a GEST into a cohere, rich and natural language description is not a straight-forward task with multiple possibilities. In this work we adopt a two-stage approach that harnesses the power of existing text-based LLMs to build natural descriptions. The goal of the first step in our approach in to convert the graph into sound but maybe a rough around the edges textual form, an initial description that we call proto-Language. While this representation is sound and accurately depicts the information encoded in the graph, it lacks a certain naturalness, as it may sound too robotic, lacking a more nuanced touch. Therefore, to obtain a more human-like description we use existing LLMs by feeding them with this proto-Language and prompting with the goal of rewriting the text to make it sound more natural.\nThe visual information is already converted and integrated into the GEST, but the question of how this graph can be effectively converted to an input to be consumed by an LLM still remains. The first step in this process involves a temporal sorting of the graph (by the start frame of each event; akin to a topological sort). If at each moment in time a single actor performs a single action, this is a rather straightforward process, with the results being a tree in space and time. With multiple actors and/or actions, this becomes more complex, with more than one possible representation. Our approach aggregates chronologically sorted actions into higher-level groups of actions by actors. Each such group is then described in text, by describing each event using a simple grammar and taking into account the intra-group and inter-group spatial and temporal relations. A high-level example of this algorithm is presented in Figure 3. Describing a single event involves describing the actor or actors (including objects) involved, the action performed, and spatial and temporal information if available.\nCrucially, we decide to not make a hard decision when selecting the possible objects involved in an event and to double down on the power of LLMs, feeding them with special instructions for selecting the most probable object in the given context. Therefore, when describing an event, we list all possible objects (as computed earlier) and let the LLM pick the objects that are most probable to appear in the given context, with the power to pick a new object that is not present in the list or not pick an object at all. Furthermore, we allow the LLM to change the name of an action or delete an action and its associated entities entirely if it does not fit the context. The prompt and instructions used to generate the final text description are depicted in Figure 4.\nFinally, to get a better understanding of the context we prompt a small vision language model with the following instruction: \"In what scene does the action take place? Simply name the scene with no further explanations. Use very few words, just like a classification task, e.g., classroom, park, football field, mountain trail, living room, street.\" and prepend the answer to the proto language. This allows the LLM to better understand the context of the actions and objects and thus better ground the description in the real world."}, {"title": "4. Experiments and Evaluation", "content": "In this section, we describe the experimental settings, ranging from datasets to methods used and the selected evaluation methodology."}, {"title": "4.1. Datasets", "content": "To validate our approach, we employ five different datasets: Videos-to-Paragraphs [4], COIN [29], WebVid [3], VidOR [26] and ImageNet-VidVRD [25].\nVideos-to-Paragraphs [4] consists of 510 videos of actions performed by actors in a school-like environment, filmed with both moving and fixed cameras. All the videos contain a multitude of actions including interactions between two or more actors. The complexity of the Videos-to-Paragraphs videos stems rather from the multitude of actors and actions rather than from the complexity of individual actions. The COIN dataset [29] consists of over 11k videos of people solving 180 different everyday tasks in 12 domains (e.g., automotive, home repairs). All videos were collected from Youtube\u00b2, with an average duration of 2.36 minutes. We chose this data set for its rather long and complex nature. VidVRD [25] and VidOR [26] consist of 1k and 10k video annotated with visual relations. VidVRD contains 35 unique subject/object categories with a total of 132 predicate categories. Similarly, in VidOR 80 categories of objects and 50 categories of relations are annotated. We select video from both sources for their rich visual relations, often containing multiple actors performing a multitude of complex intertwined actions. WebVid [3] contains 10 million rich and diverse web-scraped videos with short text descriptions. We pick videos from this dataset mainly for the diverse base that it offers (e.g. a wide range of possible actions and environments). For each dataset, video duration statistics are presented in Figure 5.\nAt this point, it is important to make a clear distinction between the types of videos in the five datasets and why we consider the Videos-to-Paragraphs dataset the most relevant for the task at hand (i.e., rich video description). The reason is two fold: on one side Videos-to-Paragraphs dataset contains rich, two-level human annotated descriptions (i.e., SVO and video level descriptions) so it represents a strong benchmark. Secondly, the dataset was built in such a way that each video has a clear context, there are a lot of interactions with objects and between persons, and crucially, there is no single encompassing action that could properly describe the actions performed in the video. While Videos-to-Paragraphs videos are not the longest, they are in this sense the most complex. Instructional videos, such as COIN videos, that are significantly longer by definition"}, {"title": "4.2. Methods", "content": "We compare our approach (GEST) against a suite of existing open models: VidIL [39], VALOR [5], COSA [6], VAST [7], GIT2 [34], mPLUG-2 [40] and PDVC [37]. Upon careful inspection of generated texts, we found that VidIL generated texts tend to be rich, but contain a high degree of hallucinations, while descriptions generated by our method tend to miss certain relevant aspects; see Section 5.3 for more details. Grounding VidIL and vice versa, adding more details to our approach should increase the overall quality of the descriptions. Therefore we add the output of our method to the input used by VidIL (e.g. frame captions, events) and re-run GPT 4o to generate a textual description. Thus, the only changes we apply to VidIL are simply adding the textual description generated by our approach to the set of inputs already used by VidIL and minimally tweaking the generation prompt."}, {"title": "4.3. Evaluation", "content": "To evaluate our approach and compare it with existing models, we use two evaluation protocols. On one hand, we turn to a text-based evaluation based on standard text similarity metrics (akin to how captioning methods are evaluated), while on the other hand, we perform a study to obtain qualitative ranking of the generated texts.\nWhile for videos in Videos-to-Paragraphs [4] we have access to a rich, narrative-like ground truth, for the other datasets this is not readily available. Therefore, we use GPT 4o to generate pseudo-ground truth rich descriptions. For the ranking part, we harness strong Vision Large Language Models (VLLMs) and faced with a video and six automatically generated texts, their goal is to rank the videos from best to worst based on richness and factual correctness. We selected the methods used for this evaluation using the quantitative results already obtained and by running a very small-scale initial experiment with a human annotator. In the end, the survey includes texts generated by the following methods: GEST (own), VidIL, GIT2, mPLUG-2, PDVC and GEST (own) + VidIL. We implement the LLM-as-a-Jury [31] approach, with Claude 3.53, GPT 404 [14], and Qwen25 [36] prompted with 10 uniformly sampled frames from each video, the six generated descriptions and the set of instructions. Beyond the ranking, we prompt the VLLMs to also provide a score between 1 and 10, to better understand the differences between the methods."}, {"title": "5. Results", "content": "Finally, we present some qualitative examples and highlight some patterns observed throughout a multitude of videos and generated descriptions."}, {"title": "5.1. Quantitative Evaluation - Captioning metrics", "content": "For the Videos-to-Paragraphs dataset, where rich ground truth is available, we present results for both levels of annotations available (i.e., caption and SVO-level) in Table 1 and Table 2. Note that in both cases, our proposed method performs the best. For this dataset the combination of our method with VidIL is underperforming, when compared with captions it is performing worse on average than the two methods individually, while for the SVO-level description it slightly improves over VidIL but trails behind our method.\nThe results for the five considered datasets are aggregated in Table 3 while per dataset averages are presented in Table 4. For datasets besides Videos-to-Paragraphs, VidIL performs significantly better than other considered methods. This is in part due to the nature of the videos and the nature of the GPT generated pseudo-ground truth that contain a lot of details, details that are captured by the input extraction methods used by VidIL. Other methods tend to focus more on describing the action and less on describing details of the scene (e.g., how people are dressed). Combining these rich details about the scene with a rich description of the actions performed in the video leads to more qualitative descriptions, as the results prove: the combination of our method with VidIL obtains top scores on all datasets with the exception of VidVRD where it obtains a competitive results, very close to the top performer."}, {"title": "5.2. Qualitative Evaluation - Method Ranking", "content": "The results are presented in Tables 5 and 6. Again, we note the very strong performance of our method on Videos-to-Paragraphs dataset, with the lowest rank and highest grade at a significant distance from other methods. Out of the considered methods, GIT2 and mPLUG-2 have by far generated the shortest and \"simplest\" descriptions (akin to video captioning) and their similarity is clearly seen in the results: they are very close both when considered the rank and the grades. This is in somewhat contrast to the quantitative results, where the differences between the two methods are clearly visible. PDVC, a competitive method if judged by text similarity metrics, is clearly underperforming if qualitatively judged. Combining our method with VidIL tends to increase the overall quality of the generated texts, obtaining a better ranking on 3 out of the 5 datasets, with small differences on the other 2."}, {"title": "5.3. Qualitative Examples and Observations", "content": "We present a sample video together with all the generated descriptions and ground truth in Figure 6. By manually investigating more than 200 videos with their associated descriptions we noticed some strong patterns: both GIT2 and mPLUG-2 method generated very short descriptions in the form of one sentence, mentioning a single entity (that could include more than one actor e.g., \"two men\") and a single action. These descriptions are very simple, trivially true (a sentence that only describes the surroundings, or a sentence that states that a person is somewhere) and most of the time completely miss actions and actors. This makes them suitable for videos which have a single overarching action.\nWhile arguably competitive based on qualitative metrics, PDVC-generated descriptions are too scriptic and contain way too little information to be relevant in real-world scenario. This proves yet again that automatic evaluation based on text similarity metrics is not the be-all end-all solution for evaluating video descriptions as our analysis casts a serious doubt on the effectiveness of such an approach.\nOn the other side, descriptions generated by VidIL are far richer, in some cases too rich, containing a lot of hallucinations and untrue facts. For example, in most Videos-to-Paragraph samples, as it sees a person in a room with a chalkboard it automatically infers that that person is a teacher, even if the person is sitting alone in the room, at a desk, doing something completely unrelated to teaching. It even hallucinates non-existing students (for some reason always six students) that are attentive to this imagined teacher, even if in the entire video there is only one person. Also, if a person is holding or writing on a laptop, that person \"becomes\" a computer scientist and all of the subsequent actions are described through this new persona (e.g. writing on laptop becomes coding).\nAs our method is based on an action recognizer that has a rather small and fixed set of possible actions, our generated descriptions lack flexibility and sometimes exhibit a limited understanding of the world. They tend to describe lower-level actions, for example, mopping the flooring might be described by holding an object while walking around. This also explains the strong performance of our method on Videos-to-Paragraphs dataset as the videos complexity stems from the multitude actions and interactions between multiple actors, rather than from individual action complexity.\nCombining our method with VidIL yields mixed results: in some cases the generated description is more grounded, containing fewer hallucinations, while in other cases our input seems to be irrelevant. This seems to happen more often where the exists a strong disagreement between the two"}, {"title": "6. Conclusions", "content": "We have proposed a novel method that combines state-of-the-art models from both computer vision and natural language processing domains with a procedural module to generate explainable video descriptions. It uses object and action detectors, semantic segmentation and depth estimation to automatically extract frame-level information, that is further aggregated into video-level events, ordered in space and time. Using a relatively simple algorithm, events and their spatio-temporal relations are further converted into a proto-language that is rich in information, but lacks fluency and grammatical complexity. Using LLMs, this simple language is finally converted into a fluent, coherent story that describes the events in natural language. To our best knowledge, we are the first to explore such a procedural approach, that bridges existing state of the art learning models from vision and language in order to provide an explainable solution to the long-standing vision to language translation problem. Our experiments on videos from several current datasets, show that our zero-shot approach can outperform the current state of the art open models that are heavily trained for video captioning.\nFurthermore, our method greatly outperforms existing methods on Videos-to-Paragraphs dataset, a grounded and complex dataset with multiple actions and actors. Our approach is especially suited for this kind of videos, for example surveillance and security videos, accurately describing the actors and actions performed."}]}