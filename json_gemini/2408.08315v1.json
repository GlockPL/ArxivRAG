{"title": "Segment Anything for Videos: A Systematic Survey", "authors": ["Chunhui Zhang", "Yawen Cui", "Weilin Lin", "Guanjie Huang", "Yan Rong", "Li Liu", "Shiguang Shan"], "abstract": "The recent wave of foundation models has witnessed tremendous success in computer vision (CV) and beyond, with the segment anything model (SAM) having sparked a passion for exploring task-agnostic visual foundation models. Empowered by its remarkable zero-shot generalization, SAM is currently challenging numerous traditional paradigms in CV, delivering extraordinary performance not only in various image segmentation and multi-modal segmentation (e.g., text-to-mask) tasks, but also in the video domain. Additionally, the latest released SAM 2 is once again sparking research enthusiasm in the realm of promptable visual segmentation for both images and videos. However, existing surveys mainly focus on SAM in various image processing tasks, a comprehensive and in-depth review in the video domain is notably absent. To address this gap, this work conducts a systematic review on SAM for videos in the era of foundation models. As the first to review the progress of SAM for videos, this work focuses on its applications to various tasks by discussing its recent advances, and innovation opportunities of developing foundation models on broad applications. We begin with a brief introduction to the background of SAM and video-related research domains. Subsequently, we present a systematic taxonomy that categorizes existing methods into three key areas: video understanding, video generation, and video editing, analyzing and summarizing their advantages and limitations. Furthermore, comparative results of SAM-based and current state-of-the-art methods on representative benchmarks, as well as insightful analysis are offered. Finally, we discuss the challenges faced by current research and envision several future research directions in the field of SAM for video and beyond.", "sections": [{"title": "I. INTRODUCTION", "content": "FOUNDATION models [1]\u2013[3] have become a significant area of research in recent years, revolutionizing various fields such as natural language processing (NLP), computer vision (CV), and machine learning. These models are typically pre-trained on massive datasets, enabling them to learn general representations of the input data and extract meaningful fea-tures that can be further fine-tuned for specific applications.\nWhile foundation models have primarily garnered extensive attention in NLP, their utility extends beyond that domain. In CV, researchers have been exploring the application of foundation models to enhance imaging understanding [4]\u2013[6], object detection [7], [8], image segmentation [9], [10], and other vision-related tasks [11], [12].\nOne prominent example is the segment anything model (SAM) [13], which has achieved remarkable progress in ex-ploring general and task-agnostic foundation models in the CV community. By training on over 1 billion masks on 11 million images, SAM can deliver high-quality segmentation masks based on multiple prompts (e.g., points, box, and text). More importantly, SAM exhibits powerful zero-shot generalization in various segmentation tasks (e.g., interactive segmentation, semantic segmentation, and panoptic segmentation), without the retraining or finetuning previously required [14]. Therefore, the emergence of SAM has led many researchers to believe that this is \u201cthe GPT-3 moment for CV, as SAM has learned the general concept of what an object is, even for unknown objects, unfamiliar scenes (e.g., underwater and cell microscopy and ambiguous cases)\" [15]. A large number of researchers have extended SAM to different fields [16]\u2013[20].\nThe segment anything model 2 (SAM 2) [21] enhances its predecessor, SAM, by integrating a transformer framework with streaming memory, facilitating superior real-time video segmentation capabilities. Trained on the extensive and diverse segment anything video (SA-V) dataset, SAM 2 demonstrates heightened accuracy and efficiency over SAM, particularly in video tasks, and offers a robust solution for promptable visual segmentation across varied spatio-temporal contexts.\nIncorporating SAM into Video Tasks. Video is an incredibly important medium in today's digital age [22]. Compared to"}, {"title": "Unique Challenges in Videos", "content": "Compared with other tasks, e.g., image and text processing, video tasks present the follow-ing unique challenges [3], [22]\u2013[25]. 1) Temporal information processing: video data encompasses not only spatial informa-tion but also temporal dynamics. Thus, handling video data requires considering the temporal relationships and dynamic changes. 2) High-dimensional data: each frame of a video consists of high-dimensional data with a large number of pixels, leading to a massive amount of data that demands more computational resources and storage space. 3) Continuity and stability: videos are generally continuous, and processing them involves considering the coherence and stability between frames to achieve reliable results in analysis and applications. 4) Time cost: due to the substantial volume of video data, the time cost for processing video tasks is usually higher, posing greater demands on computational resources and algorithm efficiency. 5) Action and event recognition: compared to static images, video tasks often involve recognizing actions and events, requiring models to understand and learn dynamic changes in temporal sequences. The above challenges fore-shadow the extreme complexity of video tasks and enormous research opportunities [16], [22], [27]."}, {"title": "Comparisons with Previous Surveys", "content": "Although three sur-veys [3], [9], [31] have been proposed for SAM, the differ-ences between our survey and existing ones are mainly in three aspects. 1) Previous SAM-based surveys only focus on medical image segmentation tasks [9] or roughly cover video tasks [3], [31], however, SAM for videos is a challenging and promising research topic with many innovation opportunities and potential applications [22]. This inspires us to conduct a systematic survey dedicated to this specific field (i.e., SAM for videos) to benefit relevant researchers and practitioners. 2) This survey provides an understandable and highly structured taxonomy of SAM for videos, dividing existing methods into three major categories (i.e., video understanding, video gener-ation, and video editing), which is significantly different from previous ones. 3) A comprehensive performance evaluation, together with many new insights on SAM for videos are offered to help readers track recent advances. Additionally, the proposed research directions are deliberate and can pave new avenues for developing foundation models in the video domain and beyond. For a comprehensive understanding of foundation models, we also refer readers to other excellent surveys for language [32]-[34], vision [22], [35], and multi-modality [1], [2]."}, {"title": "Contributions", "content": "The main contributions of this survey are threefold:\n\u2022 We thoroughly review the development of SAM for videos in the foundation models era and provide a systematic survey of the latest progress in this field, which can be grouped into three major categories: video understanding, video generation, and video editing. To the best of our knowledge, this is the first systematic survey that focuses on this specific domain.\n\u2022 We comprehensively compare SAM-based methods with current state-of-the-art (SOTA) methods on representative datasets for various video tasks. Importantly, our in-depth analysis about the pros and cons of these leading-edge methods can help readers choose appropriate baselines for their specific applications while delivering valuable insights on improving existing methods.\n\u2022 Based on the systematic literature review and comprehen-sive performance evaluation, we highlight some potential future developmental trends."}, {"title": "Organization", "content": "The remainder of this survey is organized as follows. Section II summarizes the background knowledge, including the workflows of SAM and SAM 2, research routes, and relevant research domains. In Section III, we primarily present an overview of methods in the field of video understanding with SAM. In Section IV, we delve into the principal studies concerning video generation with SAM. In Section V, we elucidate the methods for video editing with SAM. Section VI introduces the benchmark datasets and evaluation. In Sec-tion VII, we conclude this article and highlight the potential avenues for future research."}, {"title": "II. PRELIMINARIES", "content": "In this section, we first briefly introduce SAM, then review three video-related research domains, including video under-standing, video generation, and video editing."}, {"title": "A. Segment Anything Models", "content": "SAM is the segment foundation model proposed by Meta [13], as illustrated in Fig. 2(a). The pathway of SAM consists of three steps, namely task, model, and data. Inspired by large language models, tasks in SAM are usually introduced using prompt engineering [36], where a prompt is to indicate what to segment. A unique characteristic of the promptable task is that it can return a valid segmentation mask when given any segmentation prompt. The structure of SAM consists of three parts: a powerful image encoder (i.e., ViT [37]); a prompt encoder, dense input, and a mask decoder (prompt-image bidi-rectional Transformer decoder using self-attention and cross-attention). The model is trained with focal loss [38] and dice loss [39]. Due to the insufficiency of public training data for segmentation tasks, the training-annotation iterative process is conducted in SAM by constructing a data engine to achieve"}, {"title": "B. Research Routes of SAM", "content": "Research on SAM mainly adapts the following routes: model compression [28], ensuring model robustness [29], advancing efficient finetuning techniques [4], and developing"}, {"title": "C. Related Tasks", "content": "Video Understanding. Video understanding aims to recognize and localize different actions or events appearing in the video, including (1) video recognition and (2) video localization.\n(1) Video recognition aims to classify the video clip or snippet into one of action or event categories. Frameworks of current works are mainly divided into two series: two-stream networks [40], [41] and single-stream RGB networks [42], [43]. This work [44] proposes a two-stream ConvNet archi-tecture which incorporates spatial and temporal networks and demonstrates that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. SlowFast networks [43], a one-stream framework, consists of a fast pathway operating at high frame rate and a slow pathway operating at low frame rate.\n(2) Video localization targets to detect and classify actions in untrimmed long videos. There are two widely used detection paradigms. The two-stage paradigm [45], [46] first localizes class-agnostic action proposal, then classifies and refines each proposal. Another one-stage paradigm [47], [48] combines localization and classification, which densely classifies each frame into actions or backgrounds.\nVideo Generation. Video generation aims to generate new videos from (1) the text (i.e., text-to-video generation) or from (2) a single video. (1) Text-to-video generation. Early works [48]\u2013[50] primarily generate videos in simple domains, such as moving digits or specific human actions. Recently, a series of works [51], [52] conduct VAE-based methods"}, {"title": "III. VIDEO UNDERSTANDING WITH SAM", "content": "In this section, we primarily introduce various video under-standing tasks using SAM, as shown in Fig. 3."}, {"title": "A. Video Object Segmentation", "content": "Video object segmentation (VOS) is a crucial task in CV for segmenting primary objects in a video. By combining with the pre-trained segmentation model SAM, recent works present great potential in VOS. We briefly summarize them into semantic, instance, panoptic, and entity levels (see Fig. 4).\n1) Video Semantic Segmentation: Zhang et al. [20] was the first to adopt SAM for unsupervised VOS, which performs segmentation without manual annotations. Specifically, they remove the mask prediction branch in IDOL [96] to adapt it as a novel video salient object tracking method, which is to discover the salient object and spatial-temporal trajectories. Then, they adopt SAM with the generated trajectories as the prompt to obtain mask results frame by frame.\nBesides, one-shot object segmentation customizing the great segmentation ability of SAM also works well in image seg-mentation and video segmentation. Liu et al. [14] presents a training-free framework, Matcher, with one-shot object seg-mentation. They integrate an all-purpose feature extraction model (e.g., DINOv2 [10], CLIP [97], and MAE [98]) and a class-agnostic segmentation model (i.e., SAM) with three oper-ations to realize the controllable masks generation. Following it, Zhang et al. [19] introduce a training-free personalization SAM, named PerSAM, to segment only the user-provided object with SAM. Specifically, they first obtain a location con-fidence map for the target object with the user-provided image and mask. Then, based on the confidence scores, they proposed"}, {"title": "B. Video Object Tracking", "content": "Video object tracking (VOT) is a fundamental task in CV. We divide VOT methods using SAM into four groups: (1) general object tracking, (2) open-vocabulary tracking, (3) point tracking, and (4) nighttime unmanned aerial vehicle (UAV) tracking. (1) Recently, the strong segmentation ability of SAM enhances the perception of objects and allows better development in general object tracking. Yang et al. [23] propose the training-free track anything model (TAM) based on SAM to achieve high-performance interactive tracking and segmentation in videos. Specifically, they first use SAM to get an initial mask of objects, where the user can choose the target object by a click or modify the mask. Then, they adopt XMem to perform VOS on the following frames with the user-selected mask. To avoid the issue that XMem segments more coarsely over time, they use SAM again to refine it. Cheng et al. [27] propose SAM-Track to segment and track any object in a video. They incorporate SAM to obtain segments, Grounding-DINO to understand natural language, and DeAOT [102] for tracking. In the VOTS2023 challenge, Zhu et al. [68] won 2nd place to achieve high-quality VOT with their proposed HQTrack. Specifically, the framework implements the improved variants of DeAOT and SAM (i.e., HQ-SAM [16]) for multi-object segmentation and mask refining, respectively. A similar idea of combining SAM and DeAOT is seen in the 1st place solution of TREK-150 object tracking challenge [103]. They introduce MSDEAOT as an improved variant of DeAOT by replacing the bounding box with masks in the reference frame and feeding the mask and frames into the VOS model. (2) Chu et al. [69] utilize SAM as the segmenter along with an open-vocabulary object detector and an optical flow estimation to build a zero-shot open-vocabulary visual tracking framework OVTracktor. (3) SAM-PT [70] was proposed to utilize the sparse point propagation of VOS. Taking a video with point annotations in the first frame as input, SAM-PT can achieve strong zero-shot performance with a point tracker to generate the trajectories as prompts and SAM to output predicted masks. The predicted masks are also used to reinitialize and get rid of the unreliable points. (4) Yao et al. [26] utilize SAM for the field of real-time nighttime UAV tracking to accurately locate the potential object and determine high-quality target domain training samples from the night-time images."}, {"title": "C. Deepfake Detection", "content": "In a recent investigation, Lai et al. [30] delved into evaluat-ing the performance of SAM and its variants in the context of deepfake detection and localization, marking the first attempt to assess these methods for this specific task. The researchers noted that existing approaches, which utilize LoRA [104], SAM adapter [4], and learnable prompt [105] to fine-tune SAM on downstream tasks, often yielded unsatisfactory re-sults, particularly in terms of face forgery localization. This inadequacy was attributed to their limited capacity in modeling both local and global contexts for forgery.\nTo tackle these challenges, Lai et al. [30] proposed an innovative framework, named detect any deepfakes (DADF),"}, {"title": "D. Video Shadow Detection", "content": "The detection of video shadows plays a crucial role in various applications, including object detection [106], image segmentation [107], and virtual reality scene generation [108]. However, the challenge lies in the limited availability of training data, posing difficulties for the generalization capa-bility of existing deep neural network based methods. These limitations can lead to prediction errors accumulating during video propagation [109].\nIn particular, when applying SAM to single-frame shadow detection, SAM tends to categorize shadows as part of the background [18]. This introduces a nontrivial challenge in using SAM for shadow detection, as it requires bridging the gap between natural objects and complex shadows. To address this challenge, Wang et al. [18] introduced ShadowSAM, a straightforward yet effective framework designed for finetun-ing SAM specifically for shadow detection. Additionally, by adopting a long and short-term attention mechanism, they extended its capabilities for efficient video shadow detection."}, {"title": "E. Miscellaneous", "content": "1) Audio-Visual Segmentation: Recently, SAM was applied in audio-visual localization and segmentation [17], [71]. Both studies focus on overcoming challenges associated with audio-visual localization and segmentation, particularly addressing the inherent misalignment between audio and various objects in the video.\nIn [17], the authors address this challenge by introducing AV-SAM, a method that learns audio-aligned visual features for each mask prompt from the video. This facilitates the guidance of mask generation in SAM through pixel-wise audio-visual fusion. The approach utilizes audio features and visual features from the pre-trained image encoder in SAM to aggregate cross-modal representations. Conversely, Wang et al. [71] present an encoder-prompt-decoder paradigm to tackle issues related to data scarcity and varying data distribution. Leveraging abundant knowledge from pre-trained models, they introduce a semantic-aware audio prompt to assist the visual foundation model in focusing on sounding objects. Simultaneously, this approach encourages the reduction of the semantic gap between visual and audio modalities. Further-more, Bhosale et al. [72] propose CMSF, a method leveraging audio cues to generate audio tags and subsequently proposing segmentation masks. These recent advancements underscore the versatility of SAM in addressing intricate tasks related to audio-visual processing.\n2) Referring Video Object Segmentation: Despite SAM gaining widespread attention for its impressive performance in image segmentation, a study discussed in [73] highlights"}, {"title": "IV. VIDEO GENERATION WITH SAM", "content": "In this section, we divide video generation with SAM into four groups and provide detailed reviews for each: video synthesis (e.g., dance generation) [84], [85], video super-resolution (i.e., generating more detailed and visually appeal-ing videos from low-resolution versions) [25], 3D reconstruc-tion (e.g., reconstruction and segmentation of 3D objects and producing point-level semantic labels for 3D point cloud) [86], [88], and video dataset annotation generation (e.g., bounding boxes and masks generation) [24], [89]\u2013[92]."}, {"title": "A. Video Synthesis", "content": "SAM has been recently employed in two works focusing on dance video synthesis [84], [85]. In the Dancing Avatar project, SAM is employed to generate contextually appropriate background images for human motion videos, following tex-tual specifications and using image inpainting techniques. This method leads to the creation of distinct pose-guided images for various poses, resulting in the generation of human area masks. In the research presented in [84], SAM ensures a consistent background throughout the human motion image sequence, effectively separating the human foreground from the background [85]. SAM's outstanding performance in these projects, along with its contributions to other modules, has played a pivotal role in achieving an impressive result."}, {"title": "B. Video Super-Resolution", "content": "The main challenge in video super-resolution (VSR) lies in handling large motions in input frames, making it challenging to accurately aggregate information from multiple frames. However, according to literature [25], existing methods over-look valuable semantic information that could significantly enhance results, and flow-based approaches heavily depend on the accuracy of flow estimates, which may be imprecise for two low-resolution frames.\nIn [25], a robust and semantic-aware prior for improved VSR was investigated by leveraging the SAM. To incorporate the SAM-based prior, the study proposed the SAM-guidEd refinEment Module (SEEM), a simple yet effective module enhancing both alignment and fusion procedures through the utilization of semantic information. This lightweight plug-in module is designed not only to leverage attention mecha-nisms for generating semantic-aware features but also to be easily integrated into existing methods. Specifically, SEEM was applied to two representative methods, EDVR and Ba-sicVSR, resulting in consistently improved performance with minimal implementation effort across three widely used VSR datasets: REDS, Vid4, and Vimeo-90K [114]. Importantly, SEEM was found to enhance existing methods efficiently, providing increased flexibility in adjusting the balance between performance and the number of training parameters."}, {"title": "C. 3D Reconstruction", "content": "Recent research has explored leveraging SAM's robust generalization and transfer capabilities to extend its application from 2D image segmentation to tasks related to 3D reconstruc-tion and segmentation. These efforts bring forth a fresh per-spective and methodology for understanding and reconstruct-ing 3D scenes. Based on the approaches of converting the 2D segmentation masks into 3D masks, existing methodologies can be broadly classified into two categories: tracking-based approaches [88] and projection-based approaches [86], [87].\nTracking-based Approaches. Xu et al. [88] introduced OS-TRA, an open-source one stop 3D target reconstruction and multilevel segmentation framework. Within this framework, SAM is employed to segment the first frame of a video. Subsequently, the authors use VOT algorithms to generate continuous masks for video frames. In this process, SAM is also used to complementarily correct tracking errors. Fi-nally, 3D reconstruction methods are applied to reconstruct"}, {"title": "V. VIDEO EDITING WITH SAM", "content": "In this section, we detail the video editing algorithms using SAM that are divided into three groups: generic video editing, text guided video editing, and object removing. \nThe taxonomy relations of video editing with SAM are illustrated in Fig. 3.\nGeneric Video Editing. Make-A-Protagonist [93] proposes a framework for generic video editing with both visual and textual clues. It leverages multiple pre-trained experts to process source video and target visual/textual clues. Then, all the information is put into the proposed visual-textual-based video generation model with mask-guided denoising sampling to generate the desired output. SAM plays a key role in segmenting the protagonist based on the text description and masking out the background in the reference image.\nText Guided Video Editing. Wu et al. [94] introduces a new dataset (TGVE) that contains 76 videos with 4 prompts each for text-guided video editing (see Fig. 5(b)). Based on the TGVE dataset, the competition workshop was held at CVPR 2023. The winning method Two-Stage Video Editing (2SVE) incorporates many pre-trained models such as SAM, OpenCLIP [116] and ControlNet [117]. The target segment process is based on the SAM and OpenCLIP models. It works as follows: SAM predicts the masks of the input frame automatically. Then, OpenCLIP converts the masks into embeddings and calculates similarity with the text embeddings to select the target mask for the next steps. The 2SVE method consists of two stages, as its name indicates. The first stage uses ControlNet to edit the foreground, background, and structure of the input video. The second stage uses a diffusion model trained on the MSVD [118] dataset to edit the style and appearance of the output video. ControlNet and diffusion model take the target masks from the target segment process as guidance in both stages.\nObject Removing. Researchers also leverage the strong prompt segmentation ability of SAM to remove objects in 3D scenes. OR-NeRF [95] proposes a novel object-removing pipeline using either points or text prompts on a single view and ensuring multiview consistency and plausible completion after deletion. OR-NeRF consists of two stages: multiview segmentation and scene object removal. For the input in the first stage, the model either uses point prompts directly or converts the input text information into point prompts via Ground-SAM. The model uses SAM to predict the mask of images from all viewing angles based on prompts and uses LaMa to obtain color and depth priors. Then, using NeRF to reconstruct the scene after removal. Based on their one-step multiview segmentation method, which leverages SAM's strong power, it achieves better removal quality and requires less time than previous methods."}, {"title": "VI. PERFORMANCE EVALUATION", "content": "In this section, we introduce the benchmark datasets, eval-uation metrics, and comparative results of current SOTA and SAM-based methods across different video tasks."}, {"title": "VII. CONCLUSION AND FUTURE DIRECTIONS", "content": "This survey offered an in-depth look at the latest devel-opments in the era of foundation models with a focus on SAM for videos. To the best of our knowledge, this is the first systematic and comprehensive survey that concentrate on this specific and promising research field. We commenced by summarizing the unique challenges in the video domain, highlighting the extreme complexity of video tasks and the urgent need for a systematic review of SAM models for videos. This was followed by an overview of SAM and SAM 2,"}, {"title": "B. Future Directions", "content": "Through our investigation and in-depth evaluation, we have found that although the SAM models (including SAM 2) has made or is making significant breakthroughs in various image and video tasks, there still exist numerous opportunities and challenges. We provide several future research directions in the area of SAM for videos and beyond in the following.\n\u2022 Constructing Large-Scale Video Datasets. The substan-tial achievements of visual foundation models are mainly attributed to the availability of billions of high-quality im-age data. Nevertheless, considering the huge cost of data collection and annotation, current video tasks are usually limited to relatively small-scale datasets. For instance, the VOT dataset TrackingNet [159] contains 30,643 videos and 14.43 million frames, but its significant drawback is sparse annotation. Leveraging SAM to automatically generate dense mask annotations from videos is a potential solution to achieve data scalability [24].\n\u2022 Building Large-Scale Video Foundation Models. Most current visual foundation models primarily concentrate on pre-training and adaptation at the image level, which are evidently constrained in complex and dynamic video-level understand-ing tasks. Due to the increasingly convenient collection and storage, videos are emerging as a domain force on the edge devices and Internet [22]. Therefore, the development of video foundation models, e.g., medical video foundation models, for broad video applications becomes an urgent requirement.\n\u2022 Parameter-Efficient Training and Fast Inference. Training video foundation models with billions of parameters from scratch inevitably faces significant challenges due to high data dimension and the high computational overhead. While some efforts to explore new technologies, e.g., adapter [4] and prompt learning [160], by utilizing pre-trained models to promote efficient transfer learning, there remains a press-ing need to mitigate training and inference expenses. More efficient training strategies and model compression methods may unlock more power in video foundation models on edge devices, e.g., automobile and surgical robots, with limited computational resources.\n\u2022 Incorporating More Modalities. Although current foun-dation models have achieved significant advances in single modality and two modalities (e.g., vision and text, vision and audio), the integration of more modalities is far from being explored. A core reason is the lack of extensive aligned multi-modal data [2]. On one hand, collecting multi-modal data, e.g., visual images, text, audio, point cloud, infrared images, depth images, and event streams, is crucial for researching multi-modal foundation models. On the other hand, developing a unified model [12] for multi-modal perception without requiring paired multi-modal data is a promising direction.\n\u2022 Credible and Interpretable Video Foundation Models. The security of artificial intelligence has attracted significant concerns as it may lead to privacy breaches and security risks in practical applications such as face recognition and autonomous driving. However, the capability of video foun-dation models to resist various attacks [29] is still far from being explored. In addition, due to the high complexity and rapidly increasing deployment of video foundation models [3], improving their interpretability and enhancing people's trust in decision-making is a valuable avenue for future research.\n\u2022 More Innovative Opportunities in SAM for Videos. As SAM for videos is a rapid-evolving research field, we might not cover all the latest advancements in this review. Actually,"}]}