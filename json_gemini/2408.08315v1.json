{"title": "Segment Anything for Videos: A Systematic Survey", "authors": ["Chunhui Zhang", "Yawen Cui", "Weilin Lin", "Guanjie Huang", "Yan Rong", "Li Liu", "Shiguang Shan"], "abstract": "The recent wave of foundation models has witnessed tremendous success in computer vision (CV) and beyond, with the segment anything model (SAM) having sparked a passion for exploring task-agnostic visual foundation models. Empowered by its remarkable zero-shot generalization, SAM is currently challenging numerous traditional paradigms in CV, delivering extraordinary performance not only in various image segmentation and multi-modal segmentation (e.g., text-to-mask) tasks, but also in the video domain. Additionally, the latest released SAM 2 is once again sparking research enthusiasm in the realm of promptable visual segmentation for both images and videos. However, existing surveys mainly focus on SAM in various image processing tasks, a comprehensive and in-depth review in the video domain is notably absent. To address this gap, this work conducts a systematic review on SAM for videos in the era of foundation models. As the first to review the progress of SAM for videos, this work focuses on its applications to various tasks by discussing its recent advances, and innovation opportunities of developing foundation models on broad applications. We begin with a brief introduction to the background of SAM and video-related research domains. Subsequently, we present a systematic taxonomy that categorizes existing methods into three key areas: video understanding, video generation, and video editing, analyzing and summarizing their advantages and limitations. Furthermore, comparative results of SAM-based and current state-of-the-art methods on representative benchmarks, as well as insightful analysis are offered. Finally, we discuss the challenges faced by current research and envision several future research directions in the field of SAM for video and beyond.", "sections": [{"title": "I. INTRODUCTION", "content": "FOUNDATION models [1]\u2013[3] have become a significant area of research in recent years, revolutionizing various fields such as natural language processing (NLP), computer vision (CV), and machine learning. These models are typically pre-trained on massive datasets, enabling them to learn general representations of the input data and extract meaningful fea-tures that can be further fine-tuned for specific applications.\nWhile foundation models have primarily garnered extensive attention in NLP, their utility extends beyond that domain. In CV, researchers have been exploring the application of foundation models to enhance imaging understanding [4]\u2013[6], object detection [7], [8], image segmentation [9], [10], and other vision-related tasks [11], [12].\nOne prominent example is the segment anything model (SAM) [13], which has achieved remarkable progress in ex-ploring general and task-agnostic foundation models in the CV community. By training on over 1 billion masks on 11 million images, SAM can deliver high-quality segmentation masks based on multiple prompts (e.g., points, box, and text). More importantly, SAM exhibits powerful zero-shot generalization in various segmentation tasks (e.g., interactive segmentation, semantic segmentation, and panoptic segmentation), without the retraining or finetuning previously required [14]. Therefore, the emergence of SAM has led many researchers to believe that this is \u201cthe GPT-3 moment for CV, as SAM has learned the general concept of what an object is, even for unknown objects, unfamiliar scenes (e.g., underwater and cell microscopy and ambiguous cases)\u201d [15]. A large number of researchers have extended SAM to different fields [16]\u2013[20]. As shown in Fig. 1(a), the number of SAM-related research works has increased significantly since April 20231. The segment anything model 2 (SAM 2) [21] enhances its predecessor, SAM, by integrating a transformer framework with streaming memory, facilitating superior real-time video segmentation capabilities. Trained on the extensive and diverse segment anything video (SA-V) dataset, SAM 2 demonstrates heightened accuracy and efficiency over SAM, particularly in video tasks, and offers a robust solution for promptable visual segmentation across varied spatio-temporal contexts.\nIncorporating SAM into Video Tasks. Video is an incredibly important medium in today's digital age [22]. Compared to"}, {"title": "Unique Challenges in Videos", "content": "static image and pure text, video offers strong visual represen-tation, enhanced perception and memory, powerful storytelling capabilities, and rich interactivity, making it a more effective medium for communication and entertainment [2], [22]. The exploration of SAM in video tasks is quickly becoming a booming area of research [3], [23]\u2013[25]. Although SAM has shown great potential in various image tasks, it still faces numerous challenges in video tasks, such as ensuring SAM's ability to consistently and coherently generate masks across lengthy video frames [16], [19] and enhancing its scalability and efficiency for handling large-scale video data [26], [27]. Most current works on video-related tasks usually employ SAM directly to achieve remarkable results of innovative ap-plications. For a comprehensive understanding of this cutting-edge research field, as illustrated in Fig. 1(b), we conducted this survey and categorized existing works of innovative ap-plications with SAM into three major categories (i.e., video understanding, video generation, and video editing).\nUnique Challenges in Videos. Compared with other tasks, e.g., image and text processing, video tasks present the follow-ing unique challenges [3], [22]\u2013[25]. 1) Temporal information processing: video data encompasses not only spatial informa-tion but also temporal dynamics. Thus, handling video data requires considering the temporal relationships and dynamic changes. 2) High-dimensional data: each frame of a video consists of high-dimensional data with a large number of pixels, leading to a massive amount of data that demands more computational resources and storage space. 3) Continuity and stability: videos are generally continuous, and processing them involves considering the coherence and stability between frames to achieve reliable results in analysis and applications. 4) Time cost: due to the substantial volume of video data, the time cost for processing video tasks is usually higher, posing greater demands on computational resources and algorithm efficiency. 5) Action and event recognition: compared to static images, video tasks often involve recognizing actions and events, requiring models to understand and learn dynamic changes in temporal sequences. The above challenges fore-shadow the extreme complexity of video tasks and enormous research opportunities [16], [22], [27]."}, {"title": "Comparisons with Previous Surveys", "content": "Comparisons with Previous Surveys. Although three sur-veys [3], [9], [31] have been proposed for SAM, the differ-ences between our survey and existing ones are mainly in three aspects. 1) Previous SAM-based surveys only focus on medical image segmentation tasks [9] or roughly cover video tasks [3], [31], however, SAM for videos is a challenging and promising research topic with many innovation opportunities and potential applications [22]. This inspires us to conduct a systematic survey dedicated to this specific field (i.e., SAM for videos) to benefit relevant researchers and practitioners. 2) This survey provides an understandable and highly structured taxonomy of SAM for videos, dividing existing methods into three major categories (i.e., video understanding, video gener-ation, and video editing), which is significantly different from previous ones. 3) A comprehensive performance evaluation, together with many new insights on SAM for videos are offered to help readers track recent advances. Additionally, the proposed research directions are deliberate and can pave new avenues for developing foundation models in the video domain and beyond. For a comprehensive understanding of foundation models, we also refer readers to other excellent surveys for language [32]\u2013[34], vision [22], [35], and multi-modality [1], [2].\nThe main contributions of this survey are threefold:\n\u2022\n\u2022\n\u2022\nWe thoroughly review the development of SAM for videos in the foundation models era and provide a systematic survey of the latest progress in this field, which can be grouped into three major categories: video understanding, video generation, and video editing. To the best of our knowledge, this is the first systematic survey that focuses on this specific domain.\nWe comprehensively compare SAM-based methods with current state-of-the-art (SOTA) methods on representative datasets for various video tasks. Importantly, our in-depth analysis about the pros and cons of these leading-edge methods can help readers choose appropriate baselines for their specific applications while delivering valuable insights on improving existing methods.\nBased on the systematic literature review and comprehen-sive performance evaluation, we highlight some potential future developmental trends.\nThe remainder of this survey is organized as follows. Section II summarizes the background knowledge, including the workflows of SAM and SAM 2, research routes, and relevant research domains. In Section III, we primarily present an overview of methods in the field of video understanding with SAM. In Section IV, we delve into the principal studies concerning video generation with SAM. In Section V, we elucidate the methods for video editing with SAM. Section VI introduces the benchmark datasets and evaluation. In Sec-tion VII, we conclude this article and highlight the potential avenues for future research."}, {"title": "II. PRELIMINARIES", "content": "In this section, we first briefly introduce SAM, then review three video-related research domains, including video under-standing, video generation, and video editing."}, {"title": "A. Segment Anything Models", "content": "SAM is the segment foundation model proposed by Meta [13], as illustrated in Fig. 2(a). The pathway of SAM consists of three steps, namely task, model, and data. Inspired by large language models, tasks in SAM are usually introduced using prompt engineering [36], where a prompt is to indicate what to segment. A unique characteristic of the promptable task is that it can return a valid segmentation mask when given any segmentation prompt. The structure of SAM consists of three parts: a powerful image encoder (i.e., ViT [37]); a prompt encoder, dense input, and a mask decoder (prompt-image bidi-rectional Transformer decoder using self-attention and cross-attention). The model is trained with focal loss [38] and dice loss [39]. Due to the insufficiency of public training data for segmentation tasks, the training-annotation iterative process is conducted in SAM by constructing a data engine to achieve model training and dataset construction simultaneously. Ben-efiting from well-designed tasks, the model structure, and an extensive repository of high-quality training data, experiments demonstrate that the SAM model excels in zero-shot transfer capabilities. It has shown remarkable performance in tasks such as single-cue point segmentation, edge detection, object proposal, instance segmentation, interactive segmentation, and multi-modal segmentation (text-to-mask). Notably, the SAM model even surpasses supervised models in certain aspects.\nThe latest SAM 2 [21] (see Fig. 2(b)) introduces a signifi-cant evolution over its predecessor by extending its capabilities to the domain of video segmentation. SAM 2 incorporates a transformer-based architecture with a streaming memory component, enabling real-time processing of video frames. It refines the segmentation process through interactive user prompts and leverages a memory attention mechanism to retain and utilize information about the target object across frames. The SAM 2 model demonstrates improved accuracy and efficiency, requiring fewer interactions for video segmentation and outperforming SAM in both speed and accuracy for image segmentation tasks. Furthermore, the SAM 2 model is trained on the SA-V dataset, which is a substantial expansion from SAM's training data. The SA-V dataset, comprising 50.9K videos with 642.6K masklets, is not only larger but also more diverse, covering a wider range of objects and scenarios. This extensive and varied dataset has been instrumental in enhancing SAM 2's ability to segment objects in complex, real-world video content, thereby setting a new benchmark for visual segmentation tasks. The improvements in SAM 2 reflect a concerted effort to address the dynamic challenges present in video data, such as motion, deformation, and occlusion, and to provide a more generalized solution for promptable visual segmentation."}, {"title": "B. Research Routes of SAM", "content": "Research on SAM mainly adapts the following routes: model compression [28], ensuring model robustness [29], advancing efficient finetuning techniques [4], and developing innovative applications [9], [30] (as illustrated in Fig. 2(c)) from the perspective of methodology. In the realm of video processing, the majority of SAM research falls under the cate-gory of innovative applications, where SAM is directly applied to achieve significant outcomes. Meanwhile, a portion of the research is dedicated to refining finetuning approaches tailored for individual video tasks. Consequently, the taxonomy in this paper is based on innovative applications that SAM enables across diverse video-related challenges."}, {"title": "C. Related Tasks", "content": "Video Understanding. Video understanding aims to recognize and localize different actions or events appearing in the video, including (1) video recognition and (2) video localization.\n(1) Video recognition aims to classify the video clip or snippet into one of action or event categories. Frameworks of current works are mainly divided into two series: two-stream networks [40], [41] and single-stream RGB networks [42], [43]. This work [44] proposes a two-stream ConvNet archi-tecture which incorporates spatial and temporal networks and demonstrates that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. SlowFast networks [43], a one-stream framework, consists of a fast pathway operating at high frame rate and a slow pathway operating at low frame rate.\n(2) Video localization targets to detect and classify actions in untrimmed long videos. There are two widely used detection paradigms. The two-stage paradigm [45], [46] first localizes class-agnostic action proposal, then classifies and refines each proposal. Another one-stage paradigm [47], [48] combines localization and classification, which densely classifies each frame into actions or backgrounds.\nVideo Generation. Video generation aims to generate new videos from (1) the text (i.e., text-to-video generation) or from (2) a single video. (1) Text-to-video generation. Early works [48]\u2013[50] primarily generate videos in simple domains, such as moving digits or specific human actions. Recently, a series of works [51], [52] conduct VAE-based methods for more realistic scenes. Inspired by text-to-image diffusion models, Video Diffusion Models (VDM) [53] are proposed with a space-time factorized U-Net with joint image and video data training. Make-A-Video [54] and MagicVideo [55] aim to generate videos by transferring progress from text-to-image generation. (2) Video generation from a single video. The methods on this task are divided into GAN-based meth-ods [56], [57] and Patch nearest-neighbour methods [58]. Sinfusion [59] is the first work to utilize the capabilities of diffusion models to learn the appearance and dynamics of a single video for generating new videos.\nVideo Editing. Video editing usually refers to editing a video according to the textual information or an example. Video stylization is a specific type of editing task where the style provided by an example frame is propagated to the video. Existing methods can be roughly divided into (1) propagation-based methods and (2) video layering-based methods. (1) Propagation-based methods use keyframes [60], [61] to prop-agate edits throughout the video. This work [60] proposes a new type of guidance for SOTA patch-based synthesis, which can be applied to any type of video content. (2) Layer-based methods [62], [63] usually decompose the video into layers that are then edited. Layered neural atlases [62] map the foreground and background of a video to a canonical space, which is then operated for video editing."}, {"title": "III. VIDEO UNDERSTANDING WITH SAM", "content": "In this section, we primarily introduce various video under-standing tasks using SAM, as shown in Fig. 3."}, {"title": "A. Video Object Segmentation", "content": "Video object segmentation (VOS) is a crucial task in CV for segmenting primary objects in a video. By combining with the pre-trained segmentation model SAM, recent works present great potential in VOS. We briefly summarize them into semantic, instance, panoptic, and entity levels (see Fig. 4).\n1) Video Semantic Segmentation: Zhang et al. [20] was the first to adopt SAM for unsupervised VOS, which performs segmentation without manual annotations. Specifically, they remove the mask prediction branch in IDOL [96] to adapt it as a novel video salient object tracking method, which is to discover the salient object and spatial-temporal trajectories. Then, they adopt SAM with the generated trajectories as the prompt to obtain mask results frame by frame.\nBesides, one-shot object segmentation customizing the great segmentation ability of SAM also works well in image seg-mentation and video segmentation. Liu et al. [14] presents a training-free framework, Matcher, with one-shot object seg-mentation. They integrate an all-purpose feature extraction model (e.g., DINOv2 [10], CLIP [97], and MAE [98]) and a class-agnostic segmentation model (i.e., SAM) with three oper-ations to realize the controllable masks generation. Following it, Zhang et al. [19] introduce a training-free personalization SAM, named PerSAM, to segment only the user-provided object with SAM. Specifically, they first obtain a location con-fidence map for the target object with the user-provided image and mask. Then, based on the confidence scores, they proposed target-guided attention and target-semantic prompting to aid SAM's decoder for personalized segmentation. Additionally, they provide a finetuning variant PerSAM-F with only 2 parameters within 10 seconds to alleviate the mask ambiguity issue. Both of the methods can be used in image and video object segmentation in the frame-by-frame setting.\nExcept for them, Chang et al. [64] adopting SAM as a post-processing technique for semantic segmentation in the PVUW2023 VSS track. Zhou et al. [65] propose a novel mov-ing object segmentation (MOS) dataset, named DSEC-MOS, with high temporal resolution and low-latency information on the changes of scenes to promote the research on MOS.\n2) Video Instance Segmentation: To solve the problems of coarse mask boundaries and incorrect predictions on SAM, Ke et al. [16] propose HQ-SAM, which equips SAM with the ability to segment any object more accurately. To be specific, they introduce a lightweight High-quality Output Token to replace the original SAM's output token and a Global-local Feature Fusion to fuse the global semantic context and the local boundary details. They fix the pre-trained model pa-rameters to keep the original performance of SAM and only train a few parameters of the introduced components on their composed dataset with 44K fine-grained masks.\n3) Video Panoptic Segmentation: One essential challenge for the end-to-end video segmentation model is poor perfor-mance in large-vocabulary settings. In the large-vocabulary dataset VIPSeg [99], a recent work [100] achieves only 26.1 in terms of video panoptic quality score. Cheng et al. [66] state that the increasing number of classes and scenarios makes it difficult to conduct end-to-end training with good performance. Therefore, they propose a decoupled video seg-mentation approach (DEVA) with task-specific image-level segmentation and class/task-agnostic bi-directional temporal propagation. Specifically, SAM is used for image-level seg-mentation with universal data training containing outside-the-target-domain data. With the first segmented frame, they denoise the error with a few frames in the near future to reach a consensus as the output segmentation. Then, XMem [101] is adapted as the temporal propagation model to propagate the segmentation to subsequent frames. The extensive experiments on VIPSeg validate its effectiveness on large-scale video panoptic segmentation.\n4) Video Entity Segmentation: The in-the-wild setting of the image/video segmentation task is a big challenge for the existing methods, where no restriction is set on domains, classes, image resolution, and quality [67]. Although entity segmentation is designed to segment unseen categories in the training set, the lack of entity segmentation datasets makes it difficult to develop well on this task. To fill this gap, Qi et al. [67] construct a high-quality large-scale entity segmentation dataset, named EntitySeg. The dataset contains 33,227 images with high-quality annotated masks on multiple domains and diverse resolutions, allowing the evaluation of the models' generalization and robustness. They benchmark the existing models and find that they cannot well accommodate the proposed dataset. Therefore, they further propose the CropFormer [67] framework to solve the problem."}, {"title": "B. Video Object Tracking", "content": "Video object tracking (VOT) is a fundamental task in CV. We divide VOT methods using SAM into four groups: (1) general object tracking, (2) open-vocabulary tracking, (3) point tracking, and (4) nighttime unmanned aerial vehicle (UAV) tracking. (1) Recently, the strong segmentation ability of SAM enhances the perception of objects and allows better development in general object tracking. Yang et al. [23] propose the training-free track anything model (TAM) based on SAM to achieve high-performance interactive tracking and segmentation in videos. Specifically, they first use SAM to get an initial mask of objects, where the user can choose the target object by a click or modify the mask. Then, they adopt XMem to perform VOS on the following frames with the user-selected mask. To avoid the issue that XMem segments more coarsely over time, they use SAM again to refine it. Cheng et al. [27] propose SAM-Track to segment and track any object in a video. They incorporate SAM to obtain segments, Grounding-DINO to understand natural language, and DeAOT [102] for tracking. In the VOTS2023 challenge, Zhu et al. [68] won 2nd place to achieve high-quality VOT with their proposed HQTrack. Specifically, the framework implements the improved variants of DeAOT and SAM (i.e., HQ-SAM [16]) for multi-object segmentation and mask refining, respectively. A similar idea of combining SAM and DeAOT is seen in the 1st place solution of TREK-150 object tracking challenge [103]. They introduce MSDEAOT as an improved variant of DeAOT by replacing the bounding box with masks in the reference frame and feeding the mask and frames into the VOS model. (2) Chu et al. [69] utilize SAM as the segmenter along with an open-vocabulary object detector and an optical flow estimation to build a zero-shot open-vocabulary visual tracking framework OVTracktor. (3) SAM-PT [70] was proposed to utilize the sparse point propagation of VOS. Taking a video with point annotations in the first frame as input, SAM-PT can achieve strong zero-shot performance with a point tracker to generate the trajectories as prompts and SAM to output predicted masks. The predicted masks are also used to reinitialize and get rid of the unreliable points. (4) Yao et al. [26] utilize SAM for the field of real-time nighttime UAV tracking to accurately locate the potential object and determine high-quality target domain training samples from the night-time images."}, {"title": "C. Deepfake Detection", "content": "In a recent investigation, Lai et al. [30] delved into evaluat-ing the performance of SAM and its variants in the context of deepfake detection and localization, marking the first attempt to assess these methods for this specific task. The researchers noted that existing approaches, which utilize LoRA [104], SAM adapter [4], and learnable prompt [105] to fine-tune SAM on downstream tasks, often yielded unsatisfactory re-sults, particularly in terms of face forgery localization. This inadequacy was attributed to their limited capacity in modeling both local and global contexts for forgery.\nTo tackle these challenges, Lai et al. [30] proposed an innovative framework, named detect any deepfakes (DADF), building upon SAM. Specifically, they introduced a multi-scale adapter within SAM designed to capture short- and long-range forgery contexts, facilitating efficient finetuning. Additionally, a reconstruction guided attention module was introduced to enhance forged traces and boost the model's sensitivity to-ward forgery regions. The proposed method exhibited SOTA performance in both forgery detection and localization."}, {"title": "D. Video Shadow Detection", "content": "The detection of video shadows plays a crucial role in various applications, including object detection [106], image segmentation [107], and virtual reality scene generation [108]. However, the challenge lies in the limited availability of training data, posing difficulties for the generalization capa-bility of existing deep neural network based methods. These limitations can lead to prediction errors accumulating during video propagation [109].\nIn particular, when applying SAM to single-frame shadow detection, SAM tends to categorize shadows as part of the background [18]. This introduces a nontrivial challenge in using SAM for shadow detection, as it requires bridging the gap between natural objects and complex shadows. To address this challenge, Wang et al. [18] introduced ShadowSAM, a straightforward yet effective framework designed for finetun-ing SAM specifically for shadow detection. Additionally, by adopting a long and short-term attention mechanism, they extended its capabilities for efficient video shadow detection."}, {"title": "E. Miscellaneous", "content": "1) Audio-Visual Segmentation: Recently, SAM was applied in audio-visual localization and segmentation [17], [71]. Both studies focus on overcoming challenges associated with audio-visual localization and segmentation, particularly addressing the inherent misalignment between audio and various objects in the video.\nIn [17], the authors address this challenge by introducing AV-SAM, a method that learns audio-aligned visual features for each mask prompt from the video. This facilitates the guidance of mask generation in SAM through pixel-wise audio-visual fusion. The approach utilizes audio features and visual features from the pre-trained image encoder in SAM to aggregate cross-modal representations. Conversely, Wang et al. [71] present an encoder-prompt-decoder paradigm to tackle issues related to data scarcity and varying data distribution. Leveraging abundant knowledge from pre-trained models, they introduce a semantic-aware audio prompt to assist the visual foundation model in focusing on sounding objects. Simultaneously, this approach encourages the reduction of the semantic gap between visual and audio modalities. Further-more, Bhosale et al. [72] propose CMSF, a method leveraging audio cues to generate audio tags and subsequently proposing segmentation masks. These recent advancements underscore the versatility of SAM in addressing intricate tasks related to audio-visual processing.\n2) Referring Video Object Segmentation: Despite SAM gaining widespread attention for its impressive performance in image segmentation, a study discussed in [73] highlights SAM's limitations in the realm of referring video object segmentation (RVOS). This limitation stems from the need for precise user interactive prompts and a constrained under-standing of different modalities, such as language and vision. In a concerted effort to effectively tailor SAM for RVOS in an end-to-end manner and fully unleash its potential for video segmentation and multi-modal fusion, Li et al. conducted a groundbreaking study [73]. They delved into SAM's potential for RVOS by integrating multi-view information from diverse modalities and successive frames at different timestamps. The authors introduced RefSAM, a novel approach that utilizes lightweight modules and an efficient finetuning strategy to align and fuse language and vision features in an end-to-end learning fashion. Additionally, they designed a hierarchical dense attention module to exploit diverse levels of visual and textual features, thereby facilitating effective cross-modal segmentation of objects with varying sizes."}, {"title": "F. Domain Specific", "content": "1) Medical Videos: SAM also contributes to the analysis of medical videos. Regarding the two problems with naive pipeline of SAM (i.e., the domain gap and the dependency on precise point or box locations), SurgicalSAM [74] introduces a novel end-to-end efficient finetuning approach for SAM, and the objective is to seamlessly incorporate surgical-specific information with SAM's pre-trained knowledge for enhancing overall generalization capabilities. This work [110] compre-hensively explores different scenarios of robotic surgery and evaluates SAM's robustness and zero-shot generalizability. SAMSNERF [75] combines SAM and neural radiance field (NeRF) techniques, which generates accurate segmentation masks of surgical tools using SAM and then guides the refinement of the dynamic surgical scene reconstruction by NeRF. Fillioux et al. [111] evaluate SAM's performance on processing patient-derived organoids microscopy frames. MediViSTA-SAM [5] is the first study on adapting SAM to video segmentation. SuPerPM [76] is a large deformation-robust surgical perception framework, which utilizes SAM to segment tissue regions from the background.\n2) Domain Adaptation: Recently, researchers utilized SAM to enhance the generalization ability of the model on target domain, especially in situations where the quality and quantity of data in the target domain are less than ideal. Bonani et al. [77] utilized SAM to provide a regularization signal for real data and introduced an invariance-variance loss structure. This structure is defined for self-supervised learning on unlabeled target domain data, facilitating the robustness of domain adap-tation ability for semantic segmentation networks. Yao et al. [26] proposed SAM-DA, a SAM-powered domain adaptation framework designed for real-time nighttime UAV tracking. They introduced an innovative SAM-driven method to expand target domain training samples, which generates a substantial quantity of high-quality training samples for the target domain from each nighttime image, enabling one-to-many sample generation. This approach significantly augments both the quantity and quality of target domain training samples, thereby providing improved data support for domain adaptation.\n3) Tool Software: Hsieh et al. [78] explored the possibility of leveraging tool documentation, as opposed to demonstra-tions, for instructing large language models (LLMs) on the utilization of new tools. The article [78] demonstrated that the use of tool documentation empowered LLMs to employ SAM in a zero-shot manner, eliminating the need for training or finetuning. Of equal significance, the article showcased the potential of employing tool documentation to enable novel applications. One such illustration involved the amalgamation of GroundingDino [7] and SAM, resulting in the creation of Grounded-SAM [112]\u2014a model proficient in generating text grounded in visual content, showcasing its capabilities to derive meaningful textual information from images.\n4) More Directions: Several studies have applied SAM in various applications, spanning optical flow estimation [79], robotics [80], [83], [113], reinforcement learning (RL) for video games [82], and semantic communication [81].\nTo address the challenge of \u201cfragmentation\u201d in optical flow estimation, Zhou et al. [79] employed SAM as an image encoder, providing optical flow estimation with richer and higher-level contextual features. This strategy mitigates the model's tendency to focus exclusively on local and low-level cues. In the work by Yang et al. [80], SAM was used to generate segmentation masks for objects, providing the model with rich semantic, geometric, and shape priors. This, in turn, assists robots in perceiving object poses and determining grasp points. Similar ideas are also evident in [83], [113]. In [82], the authors enhanced the original pixel input using SAM, aiming to improve the performance of RL agents in Atari video games. Despite the observed improvement in the game-playing performance of the RL agent, finding a suitable balance between performance enhancement and computational cost remains an ongoing exploration. Additionally, Raha et al. [81] proposed a novel semantic communication framework based on SAM, efficiently transmitting sequential images or videos while preserving the original content unchanged."}, {"title": "IV. VIDEO GENERATION WITH SAM", "content": "In this section, we divide video generation with SAM into four groups and provide detailed reviews for each: video synthesis (e.g., dance generation) [84], [85], video super-resolution (i.e., generating more detailed and visually appeal-ing videos from low-resolution versions) [25], 3D reconstruc-tion (e.g., reconstruction and segmentation of 3D objects and producing point-level semantic labels for 3D point cloud) [86], [88], and video dataset annotation generation (e.g., bounding boxes and masks generation) [24], [89]\u2013[92]. The taxonomy details of video generation with SAM is illustrated in Fig. 3."}, {"title": "A. Video Synthesis", "content": "SAM has been recently employed in two works focusing on dance video synthesis [84], [85]. In the Dancing Avatar project, SAM is employed to generate contextually appropriate background images for human motion videos, following tex-tual specifications and using image inpainting techniques. This method leads to the creation of distinct pose-guided images for various poses, resulting in the generation of human area masks. In the research presented in [84], SAM ensures a consistent background throughout the human motion image sequence, effectively separating the human foreground from the background [85]. SAM's outstanding performance in these projects, along with its contributions to other modules, has played a pivotal role in achieving an impressive result."}, {"title": "B. Video Super-Resolution", "content": "The main challenge in video super-resolution (VSR) lies in handling large motions in input frames, making it challenging to accurately aggregate information from multiple frames. However, according to literature [25], existing methods over-look valuable semantic information that could significantly enhance results, and flow-based approaches heavily depend on the accuracy of flow estimates, which may be imprecise for two low-resolution frames.\nIn [25], a robust and semantic-aware prior for improved VSR was investigated by leveraging the SAM. To incorporate the SAM-based prior, the study proposed the SAM-guidEd refinEment Module (SEEM), a simple yet effective module enhancing both alignment and fusion procedures through the utilization of semantic information. This lightweight plug-in module is designed not only to leverage attention mecha-nisms for generating semantic-aware features but also to be easily integrated into existing methods. Specifically, SEEM was applied to two representative methods, EDVR and Ba-sicVSR, resulting in consistently improved performance with minimal implementation effort across three widely used VSR datasets: REDS, Vid4, and Vimeo-90K [114]. Importantly, SEEM was found to enhance existing methods efficiently, providing increased flexibility in adjusting the balance between performance and the number of training parameters."}, {"title": "C. 3D Reconstruction", "content": "Recent research has explored leveraging SAM's robust generalization and transfer capabilities to extend its application from 2D image segmentation to tasks related to 3D reconstruc-tion and segmentation. These efforts bring forth a fresh per-spective and methodology for understanding and reconstruct-ing 3D scenes. Based on the approaches of converting the 2D segmentation masks into 3D masks", "categories": "tracking-based approaches [88", "86": [87], "88": "introduced OS-TRA", "steps": "firstly"}, {"86": "introduced SAM3D"}, {"86": "Dong et al. [87", "strategies": "a CLIP-based LSeg [115"}]}