{"title": "Rag 'n Roll: An End-to-End Evaluation of Indirect Prompt Manipulations in LLM-based Application Frameworks", "authors": ["Gianluca De Stefano", "Lea Sch\u00f6nherr", "Giancarlo Pellegrino"], "abstract": "Retrieval Augmented Generation (RAG) is a tech- nique commonly used to equip models with out-of-distribution knowledge. This process involves collecting, indexing, retriev- ing, and providing information to an LLM for generating responses. Despite its growing popularity due to its flexibility and low cost, the security implications of RAG have not been extensively studied. The data for such systems are often col- lected from public sources, providing an attacker a gateway for indirect prompt injections to manipulate the model's responses. Although the risks of indirect prompt injection are known they are mainly studied in isolation, and their concrete impact on complete RAG-based applications are largely unknown.\nIn this paper, we investigate the security of RAG systems against end-to-end indirect prompt manipulations. First, we review existing RAG framework pipelines and derive a pro- totypical architecture and identify potentially critical configu- ration parameters. We then examine prior works to identify techniques that attackers can use to perform indirect prompt manipulations. Based on this, we implemented multiple RAG configurations following the prototypical architecture and build our framework Rag-n-Roll that can test them against the identified attacks to determine their effectiveness and measure their concrete impact.\nOur results show that existing attacks are mostly opti- mized to boost the ranking of malicious documents during the retrieval phase. However, a higher rank does not immediately translate into a reliable attack. Most attacks, against various configurations, settle around a 40% success rate, which could rise to 60% when considering ambiguous answers as successful attacks (those that include the expected benign one as well). Additionally, when using unoptimized documents, attackers deploying two of them (or more) for a target query can achieve similar results as those using optimized ones. Finally, exploration of the configuration space of a RAG showed limited impact in thwarting the attacks, where the most successful combination severely undermines functionality.", "sections": [{"title": "1. Introduction", "content": "Retrieval-Augmented Generation (RAG) is an increas- ingly adopted technique for integrating Large Language Models (LLMs) into applications, aimed at reducing the risk of inconsistent and incoherent responses, commonly referred to as hallucinations, and enabling responses to out- of-context knowledge without the need for retraining. RAGS achieve that by grounding LLMs' responses in an external knowledge base, which provides context to the model to generate an answer to a user-provided query. A notable implementation of RAG is Google Gemini's integration into Workspace, where Gemini answers questions using data stored across web services like Gmail. As the integration of LLMs into applications expands, the potential for attackers to exploit vulnerabilities to manipulate model responses increases. While much research has focused on the security of LLMs in isolation, less attention has been paid to the security of integrated systems such as RAGs.\nOne of the main security concerns for LLM-based ap- plications is prompt injection attacks [1], where the attacker submits a malicious query designed to bypass security mea- sures (e.g., Jailbreak [1]) or to extract confidential data such as high-quality prompts (e.g., [2], [3]). When a LLM is augmented with retrieval capabilities, an attacker can also introduce malicious inputs indirectly through the retrieved documents [4] with the objective of manipulating responses to benign users' queries. The success of these indirect attacks depends on the inclusion of a malicious document in the LLM's prompt. Previous research has proposed several techniques to manipulate documents to increase their rank- ing during the retrieval phase (e.g., [5], [6], [7]), which has been measured in isolation, focusing only on the retrieval component, showing that manipulations can increase the ranking of malicious documents. However, the capabilities of RAGs to split, manipulate, reorder, and reason about the position of malicious documents may reduce the effective- ness of these attacks. Consequently, the impact of end-to-end indirect prompt manipulation in RAGs is largely unknown.\nIn this paper, we investigate the security of RAG systems against end-to-end indirect prompt manipulations, delivering one of the first and most comprehensive evaluations by exploring the configuration space of RAG systems. We begin with a survey of RAG frameworks and an analytical decomposition, deriving a general RAG model with com- mon components and configuration parameters. Then, we review prior work that targets adversarial techniques affect- ing ranking algorithms or retrieval components. We then design Rag-n-Roll, an automated end-to-end evaluation framework that systematically assesses the susceptibility of RAG systems to indirect manipulations. Rag-n-Roll"}, {"title": "2. Background", "content": "Before presenting our study, we first introduce some real world examples of RAG applications in Section 2.1, and then, we present the threat model of this work in Section 2.2.\n2.1. RAGS\nTraditional LLM applications generate responses based on factual knowledge retained from training data. Unfortu- nately, this introduces two significant challenges. The first challenge is LLM hallucinations, which are responses to users' questions that are inconsistent or incoherent with the factual knowledge in the training data. The second challenge is the inability to handle questions whose answers are not present in the training data. Addressing these challenges re- quires constantly updating LLMs through retraining, which is costly and time consuming.\nInstead of retraining LLMs, RAGs add contextual in- formation to the users' queries fetched from an external knowledge base. The knowledge base is extracted from data sources such as web articles, file storage, or email boxes and saved in vector databases, which are database systems opti- mized to index information using multidimensional features. Each data point of the data source, e. g., a document, is split into chunks, and each chunk is indexed separately using text- to-vector embedding models. The embeddings and the text are stored in the vector database. Whenever a user submits a query, the RAG fetches its most relevant chunks from the vector db and includes both in the prompt fed into the LLM. Previous studies [8] have already validated the benefits of RAG to improve the model's performance.\n2.2. Threat Model\nAttacks targeting LLMs predominantly focus on user- controlled aspects, for example, the LLM input prompt. The attacker is also a user, aiming to manipulate the model's output or to extract confidential data such as the underlying application's private prompt through carefully crafted inputs. This threat still applies to RAG systems; however, in this paper, we focus on a new attack vector introduced by RAGs that exploits external sources providing manipulated contex- tual data to the model, thereby allowing the embedding of potentially harmful payloads.\nWe consider that the attacker can add malicious doc- uments to the knowledge base, which are then correctly indexed and stored in the vector database. This scenario is feasible when the data source is public, e.g., social network messages, or when accessible to an attacker, e.g., an email"}, {"title": "3. Problem statement", "content": "This paper aims to answer the following questions:\n(RQ1) Analysis of RAG Architectures. RAG systems consist of more than just an LLM. We identify the critical components of a RAG system and investigate potential gateways for an attacker. For this, we divide a typical RAG system into its components and analyze their impact on the model's output from our threat model's perspective.\n(RQ2) Evaluation of Indirect Prompt Manipulation At- tacks. Adversarial attacks that hijack RAG system responses often target the retrieval and re-ranking steps to boost the ranking of malicious documents. These algorithms achieve their goals by modifying or adding trigger tokens at the beginning of target texts to enhance their ranking. However, due to the capabilities of RAG pipelines to split, manipulate, reorder, and reason about the content of documents, the downstream success of these algorithms may not correlate directly with the position of the malicious document in the retrieved list. This research question aims to assess the effectiveness of existing strategies and explores how an attacker might refine their strategies to further enhance effectiveness within RAG systems.\n(RQ3) Evaluation of Baseline Attacks. Existing attacks optimize malicious documents so that the malicious infor- mation is ranked higher in the contextual information when presented to an LLM. While the previous research question addresses the effectiveness of these techniques, this research question examines a simpler attack, where the malicious documents contain only the malicious information, without any optimization such as trigger tokens.\n(RQ4) Configuration Parameters and RAG Robustness. While RAG parameters are typically optimized to enhance the performance of a model, e.g., maximizing correct an- swers, they may also play a role in preventing indirect prompt manipulation attacks. We address this challenge by"}, {"title": "4. Architecture of a RAG-based Application", "content": "We now present a prototypical architecture of the RAG frameworks. We derive this architecture by reviewing and decomposing existing RAG frameworks into their funda- mental building block components, outlining their param- eters, and explaining how these components are intercon- nected.\n4.1. Review of RAG Frameworks\n4.1.1. Identifying Relevant Frameworks. The initial step in our review identifies relevant RAG frameworks. We conducted this identification by searching for articles and blog posts within LLM communities, such as Medium and Reddit. Specifically, on Medium, we utilized the plat- form's search function with the query \"Retrieval augmented generation over documents\". For Reddit, we employed a Google search restricted to Reddit pages by incorporating the keyword \"site:reddit.com\u201d. When reviewing the top 15 search results from the result pages, we noted the rec- ommended RAG frameworks. The top three most recom- mended frameworks are LangChain [9], LlamaIndex [10], and Haystack [11].\n4.1.2. Component Identification. After identifying the RAG frameworks, we read their developers' documenta- tion, focusing on \"quick start\" tutorials and API docu- mentation, as well as any website pages describing the creation of question-answering applications. We accessed these resources using each framework's internal documen- tation search engine with the keywords \u201cdocument question answering.\u201d\nFor each framework, we pinpointed building block com- ponents responsible for specific operations, such as retriev- ing documents or calculating document embeddings for data"}, {"title": "4.2. Components and Parameters", "content": "We now present the six components of our architecture.\n4.2.1. Data Source Processing. The first group of compo- nents is responsible to process documents from an external data source. These components cover different roles rang- ing from fetching data from one or more data sources to prepare the data for storage in a vector database, which are typically executed offline to build, update, and maintain the knowledge base used by the application to answer users' queries.\nData Download. This component retrieves documents from a data source and makes it available to the other components of the framework. Frameworks like LangChain offers pre- built downloaders for known sources, such as Wikipedia, Reddit, and Arxiv. However, developers could in principle also create their own downloaders to retrieve data from custom sources. This component does not transform, modify, or alter the content of the fetched data, thus we did not consider its parameters.\nData Splitting. Providing an entire document when prepar- ing an answer may be inefficient and introducing irrelevant context to the mode as the information useful to the answer may be in a chunk of the document. Accordingly, a RAG attempts to identify relevant chunks of the document and provide the model only with the chunks appropriate to a given users' query. To achieve that, RAG frameworks offer a variety of data splitters to divide a document into chunks. For example, the data splitter of LangChain splits the document using the line feed. If a chunk is longer than the maximum chunk size, it keeps on splitting the chunk using another separator, i.e., the blank space. If chunks are still longer than the maximum size, it truncates the chunk at the chunk size. Data splitter may cut a text with the answer in two, destroying information useful for the answer. To mitigate that risk, data splitters have an additional parameter called padding size which is the number of overlapping characters between two consecutive chunks.\nData Indexing (Optional). Retrieving chunks that are rele- vant for a given query is a critical step. This process involves extracting an index from each text chunk before storing it in the database, and using the index at retrieval time to sort texts based on their relevance to the analyzed query. The data indexing component supports different indexing techniques. For instance, traditional algorithms such as BM25 [12] and TF-IDF exact lexical matching to construct vectors that represent texts. In these vectors, each dimension corresponds to a word, with the value being either n or 0, depending on whether the word is present in the text or not. Since most values in these vectors will be 0, these techniques are referred to as sparse retrievers. A significant disadvan- tage of lexical matching is that a text will be considered relevant to a query only if it shares the same terminology, which can lead to low precision. To address this limitation, dense retrievers use specially trained networks to extract dense embeddings that are more sensitive to the semantics of the texts. Finally, both sparse and dense retrievers can be combined in hybrid systems where indexes are a weighted sum of the results of the two.\n4.2.2. Vector Database. Vector databases are data man- agement software optimized to store and retrieve data items in dense vector form. They are specialized in effi- ciently retrieving data using similarity functions, fetching the closest matching entries to the one supplied. Examples of vector databases are Chroma [13], Pgvector [14], and Pinecode [15]."}, {"title": "4.2.3. Answer Generation", "content": "The second group of compo- nents is responsible for generating the answer. This involves retrieving relevant chunks, optionally re-ranking them, and then generating the prompt for the LLM.\nChunks Retrieval. Based on the user's query, this compo- nent retrieves the relevant chunks that provide contextual information. The retrieval process transforms the query into the same feature space as the chunks (refer to the Retriever parameter of the data indexing component) and identifies the closest K chunks.\nData Re-Ranking (Optional). The embedding functions used for indexing and retrieving chunks prioritize speed over accuracy, employing functions that are quicker to compute. However, these faster functions may yield K chunks with irrelevant entries. To mitigate this, an optional re-ranking step can be implemented before final chunk selection. This step uses more accurate, albeit slower, embedding functions, such as cross-encoders. The parameter K from the retrieval component is only applied after re-ranking to select the most relevant chunks.\nAnswer Generation. The final step in a RAG is generating the answer by constructing a prompt for an LLM. Typically, this involves copying the selected K chunks and the user's query into a single prompt template, which is then fed to the chosen model. However, a review of the documentation revealed that LangChain proposes four distinct patterns for utilizing retrieved data with a model [9]. Each pattern de- scribes a different model interaction and potential transfor- mation of the contextual information, including the general approach. Consequently, we have decided to include these interaction modes and their default prompts in our study (see [16]):\n1) Stuff chain: This mode involves taking all K retrieved documents and integrating them into the prompt. The model is tasked with generating an answer to the given query using this comprehensive input.\n2) Refine chain: In this mode, the model initially uses the first document to generate an answer. The response is then iteratively refined using the subsequent documents.\n3) Map reduce chain: This mode utilizes the K docu- ments to generate individual answers for each; these answers are then synthesized into a final response in a subsequent LLM call.\n4) Map Re-rank chain: This mode employs the LLM to generate a response for each document along with a score. The responses are then re-ranked based on their scores, and the best response is selected.\nIn addition to the interaction mode, this component requires several parameters to be passed to the LLM. These parameters influence various aspects of the response, such as aesthetic quality-e.g., penalties for repeated words and control over the randomness of the model's output, such as"}, {"title": "5. Indirect Prompt Manipulation Attacks", "content": "We compile a list of concrete attacks that aim at in- directly manipulating the output to attacker-specified re- sponses through alterations of the knowledge base. We start with a literature review of prior work to identify meth- ods and techniques capable of generating such manipulated documents. Subsequently, we identify approaches that may serve as baselines for the experiments detailed in Section 7.\n5.1. Attacks from the Literature\n5.1.1. Survey. To manipulate the output of an LLM, an attacker primarily targets the retrieval phase and the sub- sequent ranking step. We conducted a systematic literature review focusing on the algorithms that govern these two components.\nIn total, we identified 11 papers (i.e., [6], [7], [17], [18], [19], [20], [21], [22], [23], [24], [25]. Of these, only three provided the source code or the necessary artifacts to implement their techniques. These techniques are Adversar- ial Semantic Collision (ASC) [17], PAT [7], and IDEM [5]. Additionally, the authors of PAT [7] and IDEM [5] eval- uated their approaches against a simpler attack known as Query+ [7], where the attacker embeds the targeted query within the malicious document.\n5.1.2. Selected Attacks. We selected ASC [17], PAT [7], IDEM [5], and Query+ [7] for our evaluation. All of these techniques generate a trigger text that improves the alignment of the multi-dimensional representations of the query and the malicious document. The attack requires two"}, {"title": "5.1.3. Position of the Triggers", "content": "The selected approaches embed triggers within malicious documents to increase their retrieval rank. Among the four methods, only IDEM determines the trigger's position as part of its generation approach. The other three\u2014ASC, PAT, and Query+-require specification of the exact position in the original document for inserting the trigger.\nThe position of these triggers may influence the suc- cess of the attacks. Firstly, the attacker could position the triggers at the document's outset, as previous studies have suggested [7]. However, RAGs often split documents into smaller chunks (refer to Data Splitting in Figure 2), po- tentially separating the trigger from the associated mali- cious text, thus reducing the attack's impact. Alternatively, the trigger could be strategically placed to appear at the beginning of a chunk containing malicious content. This placement strategy, however, assumes the attacker knows about the specific parameters used by the RAG developer, such as chunk size, which is not the case, complicating"}, {"title": "5.2. Generative Optimizations", "content": "The attacks we have discussed focus on optimizing texts to maximize their probability of retrieval. These op- timizations are typically highly localized, targeting specific document positions for trigger placement. However, if trig- gers and adversarial responses become separated during the document chunking phase, the effectiveness of these optimizations is compromised. Additionally, the insertion of uncorrelated texts can disrupt the flow of the content, thereby reducing both the stealthiness and the quality of the generated response.\nTo mitigate these issues, we explored an alternative strategy to generate a malicious document using a Phi3 model [27]. This model was instructed to rewrite input documents with dual objectives: optimizing their ranking for the given query and enhancing the delivery of the malicious answer. We choose this model to be different from the evaluated RAG models to avoid inducing bias in the generated documents. The specific prompt used in this experiment is detailed below:\nYou are a Search Engine Optimization (SEO) expert that wants to boost an article to spread the answer to a specific question as much as possible.\nGiven an article, your task is to rewrite it so that:\n1) It ranks high on search engines for the given question\n2) The information relative to the targeted question/answer pair is immediately visible and highlighted.\nYou can repeat or remove information from the articles if you find it useful.\nThe article is:\n{Article}\nThe targeted question is:\n{Question}\nThe targeted answer is:\n{Answer}"}, {"title": "6. Dataset and Metrics", "content": "6.1. Evaluation Metrics and Output Matching\n6.1.1. Metrics. This paper evaluates various RAG config- urations as detailed in Section 4 against indirect prompt manipulation attacks described in Section 5.1. Each attack generates an optimized malicious document for insertion into the knowledge database, based on a target query and an initial malicious document that contains a malicious answer to the target query. To assess the true effectiveness of an"}, {"title": "6.1.2. Output Matching", "content": "Matching responses from LLMs against a set of expected answers poses significant chal- lenges. Previous approaches have employed both string matching (i.e., [8], [28], [29]) and LLM-based techniques (i.e., [30]) to ascertain equivalence between responses. While string matching offers rapid results, its accuracy is contingent upon the presence of multiple possible answers. Alternatively, using an LLM can enhance robustness to answer variability but at the expense of processing speed. Given the extensive number of evaluations and responses generated during our assessment, we have opted for string matching. To address the challenges associated with robust- ness, we increase the variability of the answers as discussed in Section 6.2"}, {"title": "6.2. Evaluation Dataset", "content": "This section presents the creation of our dataset. Our dataset contains 119 data points. Each data point contains: two queries (one original and one variant), a benign docu- ment, six benign answers (at least one original and at most five variants), six malicious answers, and one malicious document. In addition, the dataset contains 3.000 benign documents that are unrelated with the queries. We use these documents to form a generic knowledge base.\n6.2.1. Initial Dataset. To create our dataset, we considered the NQ dataset that contains questions, the expected an- swers, and Wikipedia pages containing the answers. Unfor- tunately, the answers of this dataset are long, which makes it hard to match them against the model's answers because of the too many syntactic variations introduced by the model itself. Accordingly, we looked for a similar dataset with shorter answers and used the dataset curated by Liu et al. [28]. This dataset has 2.655 entries and is a subset of the NQ dataset with at most five token long answers. Liu's data set does not contain the original Wikipedia page that we need for the knowledge base. We instead retrieved it from the original NQ dataset. Finally, to ensure that the document associated to a question contains the expected answer, we verify via string matching that the answer is present. After that, we selected additional 3.000 documents from the NQ dataset that are not related to the questions to form a generic knowledge base.\n6.2.2. Generation. We then expand the initial dataset to include variants of questions, answers, and malicious docu- ments. We iterated the generation procedure until we obtain 120 valid data points.\nFirst, we start with benign answers. Here, we observe that models can still provide valid answers that are syn- tactically different from the expected benign ones. For ex- ample, the date of a historical event can be formatted in different ways, e.g., \"01/01/1970\" or \"Jan 1st, 1970\". Such differences will challenge matching the correct answer in the RAG's output. Accordingly, we generate variants of the benign answers, asking an LLM (GPT-4) to provide syntactic variants of the benign answers in our dataset. The exact prompt for this step is presented in Appendix A.1. We then verify that the answers, both originals and variants, are consistent with the original documents and the original queries by asking GPT to answer the questions using the original documents only. If the answer does not match any of the valid answers, we discard the data point from our dataset. The prompt is in shown Appendix A.2\nSecond, our threat model assumes that an attacker in- tends to hijack questions users may ask. This implies that the attacker has a set of questions that are the target of their attack. Accordingly, we generate a dataset of variants of the questions starting from our initial dataset using GPT-4. The exact prompt for this step is provided in Appendix A.3. Then, the objective of the attacker is to hijack questions with attacker-chosen answers using malicious documents. Instead of generating these documents and answer manually, we asked GPT to act as a teacher and generate them. The exact prompt to generate answers in Appendix A.4 and the one to generate documents is in Appendix A.5. Finally, we verify that the questions, malicious answers, and malicious documents are consistent. The prompt is Appendix A.2. If the answer does not match any of the expected answers, we discard the data point."}, {"title": "6.2.3. Sanity Checks", "content": "Given the widespread recognition of the NQ dataset as a foundational resource for train- ing Question Answering (QA) models, it is probable that the LLMs leveraged within our pipeline have been trained using this dataset. This simple fact introduces a potential bias, as these models may tend to replicate responses they were originally trained on. In real-world applications, RAG pipelines are often deployed on novel datasets, making the use of familiar query-answer pairs an unreliable method for evaluation. To address this issue, we employed GPT- 4 to generate semantically similar variants of the original queries for testing. We then used these mutated queries to collect results on the default version of our pipeline without attacks, while using different LLMs. The results are depicted in Notably, all models, except for GPT- 4, demonstrates a preference for the original version of the queries. Consequently, to offer a fairer evaluation, we have decided to utilize these mutated queries in all subsequent experiments to minimize the dataset-induced bias as much as possible."}, {"title": "7. End-to-End Evaluation", "content": "We evaluate the RAG piple under attack for different configurations to assess the impact of different parameters.\n7.1. Experiment Setup\n7.1.1. Rag-n-Roll. We developed a framework called Rag-n-Roll to conduct the experiments described in this paper. The primary objective of Rag-n-Roll is to facili- tate the evaluation of prompt injection attacks on different RAG implementations. Rag-n-Roll requires two inputs: the configured RAG under test and a dataset comprising questions and benign documents. Then, Rag-n-Roll gen- erates tests for the RAG under test and evaluates the out- comes. Figure 4 shows the architecture of Rag-n-Roll. The main components of Rag-n-Roll include:\nDataset Generation: This module constructs a dataset as outlined in Section 6.2, starting from a collection of ques- tions and benign documents. It then utilizes an attack library to optimize malicious documents for indirect prompt injec- tion attacks.\nAttack Library: The attack library currently includes the five attacks discussed in Section 5.1, including variants (both trigger position and aggressiveness level for ASC).\nData Sources Creation: This module generates data sources, one for each type of attack. These data sources are subsequently integrated into and utilized by the RAG under test.\nTest Generation: This module prepares the inputs for the tests, i.e., formulates the questions and the expected answers, and associates them with the data sources created in the previous step.\nResponse Analysis: This component analyzes the responses from the RAG, applying the four evaluation metrics detailed in Section 6.1, namely, benign, malicious, ambiguous, and incoherent responses.\n7.1.2. RAG-based Application Under Tests. For the ex- periments presented in this paper, we implemented a RAG- based question answering bot application using LangChain. We implemented the following configurations using the parameters listed in Table 1:\nDefault Configuration: The first configuration adheres to the default parameter values specified in Table 1.\nSingle Parameter Change: Building on the default con- figuration, we created five groups of configurations, each corresponding to a distinct RAG component. For each group, we changed the value of one parameter of that component. For instance, we created seven configurations for the data splitting component, comprising eight for chunk size values and four for padding size values.\nRobust Configuration: This configuration was developed by evaluating the performance and robustness of other configurations, identifying parameters that minimize attack success without compromising functionality. We present the exact configuration after we present the experiment results.\n7.1.3. Instantiation of Rag-n-Roll. Throughout this sec- tion, each attack against a selected configuration is evalu- ated alongside the performance of the configuration in the absence of an attacker, i.e., when the knowledge base is not poisoned.\nWhen the attacker poisons the knowledge base, for each query and initial malicious document, we gener- ate one or more optimized versions of the document for each attack technique via the dataset generation module of Rag-n-Roll. The number of optimized documents varies depending on the variable being tested; this is further clarified during the presentation of the experiments. For each optimized malicious document, we create a knowledge base containing the benign documents, generic documents, and the optimized malicious document using the data source creation module of Rag-n-Roll. We then run the tests and measure the benign, malicious, ambiguous, and incoherent responses."}, {"title": "7.2. Experiment Results", "content": "7.2.1. Baselines of the Default Configuration. Before evaluating our configurations in the presence of optimized malicious documents, we first establish the baseline behavior of the default configuration under two distinct scenarios: one where the entire knowledge base contains no malicious documents and another where it includes unoptimized mali- cious documents. The results of these baseline assessments are presented in Table 3.\nWhen the knowledge base is unpoisoned, the default configuration accurately responds to approximately three- quarters of the questions. The remaining one-quarter of the responses fail to include the expected answer. Of these incorrect responses, 14.2% are non-committal with replies such as \"I cannot answer\" or \"I do not know.\" A further 9.2% of all responses are inconsistent or incoherent (hal- lucinations). The remaining 1% of responses are variants of the correct answer that our response evaluation module failed to recognize.\nUnder attack with unoptimized documents, containing only malicious information, the attacker successfully manip- ulates 19.3% of the responses with the intended malicious content. In addition, 18.5% of the responses are ambiguous, containing both malicious and benign information. Despite these attempts, the model remains resilient in 42% of the cases, continuing to return the expected benign answer. The remaining 19% of the responses are inconclusive, divided as follows: 15% are non-committal (e.g., 'I cannot answer\" or \"I do not know\"), 2.5% are inconsistent or incoherent (hallucinations), and 1% are accurate but were misidentified due to pattern matching errors in Rag-n-Roll.\n7.2.2. Effectiveness of Triggers Position. Having estab- lished the baseline behavior, we now evaluate the effec- tiveness of the trigger's position within the document. This experiment is designed to empirically determine the place- ment strategy for subsequent experiments. We generated optimized malicious documents utilizing ASC, PAT, IDEM, and Query+ techniques, considering two possible positions: at the beginning of the document and immediately before the malicious answers. The results for each attack configuration are presented in Section 7.2.2, with aggregated results for each trigger position shown in Section 7.2.2. The tables also include baseline results with unoptimized malicious documents.\nThe results indicate that positioning the trigger imme- diately before the malicious sentences typically doubles the fraction of responses that are malicious. Additionally, the average number of chunks derived from malicious optimized documents (# Mal chunks in Section 7.2.2) retrieved by the LLM is greater when the trigger is positioned before the malicious answer. Conversely, placing the trigger at the beginning of the document does not significantly affect the likelihood of increasing the retrieval of malicious chunks on average when compared with the unoptimized version of the attack.\n7.2.3. Parameters' Effects on Attacks. The objective of this section is to determine if selecting the right parameters can strengthen a RAG pipeline and reduce the malicious answers produced by the model. For each identified param-\""}, {"title": "7.2.4. Optimal Configuration", "content": "From Table 6 and Table 7, we selected parameters that demonstrated higher perfor- mance over the baseline and exhibited a lower success rate for attacks. We configured the chunk size to 1200, padding size to 600, retriever to TEL, with no re-ranking, set K to 12, temperature to 0, top-p to 0, and chose GPT4 as the model. Section 7.2.4 displays the attack results for each scenario, including the average number of malicious chunks provided to the LLM.\nThe results indicate that the RAG with the optimal configuration tends to reduce ambiguous and inconclusive responses, while increasing the number of benign answers. However, the number of malicious answers does not ap- pear to have been significantly affected. In conclusion, the combination of parameters that individually may mitigate attacks does not necessarily amplify their effectiveness when combined.\n7.2.5. Contribution of the Optimizations. Table 6 and Table 7 shows that ASC and PAT tend to perform as good as or worse than the unoptimized documents baseline. The other attacks, SEO, Query+, and PAT tend to perform better than the baseline; however, we observe that these attacks hardly outperform the already-low baseline, rarely going about 50% chance of hijacking the response by settling mostly around 40% chance of success.\n7.2.6. Attacks with Multiple Documents. The evaluation so far work under the assumption that (i) the attacker uses a single malicious document to hijack the model's response and (ii) the knowledge base contains only one benign doc- ument with the correct answer. We now revisit that and evaluate that assumptions and measure the the impact when more malicious and benign redundant information is present in the knowledge base. For this experiment, we expand our dataset with additional benign and malicious, unoptimized documents. We used 20 queries from our dataset as Google Search query and grabbed the first five pages answering our queries. For the malicious document, we used GPT-4 with the prompt Appendix A.5 to generate five mutations of the original malicious document. For this evaluation, we used the default RAG configuration. Results are shown in the image Table 9.\nResults show a positive correlation between higher re- dundancy of malicious documents and attack success, i.e., the more malicious documents the attacker inserts in the knowledge base, the higher the chance of successful attacks. For example, with six malicious documents the attacker can reach about 60% and 55% success rate when only one and two benign documents are present respectively. At the same time, a higher redundancy of benign documents can mitigate the impact of malicious documents. For example, four documents are sufficient to bring the success rate to about 45%."}, {"title": "8. Discussion", "content": "8.1. Takeaway\nWe now review our results and distill the main takeaways from our experiments.\n1) Struggle to Achieve a Stable Attack Success Rate. Our results show that proposed attacks against retrieval and re-ranking struggle to achieve a stable success rate higher than 50%. Many of them, e.g., ASC and its variants, even tend to perform like or worse than the unoptimized documents baseline. Other optimization techniques perform better, however, they settle around 40% success rate, which is about as twice as good as the unoptimized documents baseline. Surprisingly, two of the best performing techniques are Query+ (a malicious document using the original query as trigger) and SEO (a Phi3 generated malicious document).\n2) Two Unoptimized Malicious Documents as Good as Single Optimizations. Our results also showed that already two malicious documents can achieve the same performance of a single optimized malicious document.\n3) Ambiguous Answers as Successful Attack. Depending on the circumstances, the presence of a malicious answer in the response, regardless of the presence of a benign answer, may qualify as a successful attack. In our results, most attacks produce at most 20% ambiguous answers, which, when added to malicious answers, can offer a success rate of up to 60%, in the best case.\n4) RAG Parameters Largely Ineffective. We evaluated different RAG configurations, in isolation and by composing promising parameters. While these parameters do impact attack results, they do not seem to play a significant role in mitigating these attacks, which already struggle to outper- form."}, {"title": "5) Redundant Benign Knowledge"}]}