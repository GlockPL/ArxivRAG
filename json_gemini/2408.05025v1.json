{"title": "Rag 'n Roll: An End-to-End Evaluation of Indirect Prompt Manipulations in LLM-based Application Frameworks", "authors": ["Gianluca De Stefano", "Lea Sch\u00f6nherr", "Giancarlo Pellegrino"], "abstract": "Retrieval Augmented Generation (RAG) is a technique commonly used to equip models with out-of-distribution knowledge. This process involves collecting, indexing, retrieving, and providing information to an LLM for generating responses. Despite its growing popularity due to its flexibility and low cost, the security implications of RAG have not been extensively studied. The data for such systems are often collected from public sources, providing an attacker a gateway for indirect prompt injections to manipulate the model's responses. Although the risks of indirect prompt injection are known they are mainly studied in isolation, and their concrete impact on complete RAG-based applications are largely unknown.\nIn this paper, we investigate the security of RAG systems against end-to-end indirect prompt manipulations. First, we review existing RAG framework pipelines and derive a prototypical architecture and identify potentially critical configuration parameters. We then examine prior works to identify techniques that attackers can use to perform indirect prompt manipulations. Based on this, we implemented multiple RAG configurations following the prototypical architecture and build our framework Rag-n-Roll that can test them against the identified attacks to determine their effectiveness and measure their concrete impact.\nOur results show that existing attacks are mostly optimized to boost the ranking of malicious documents during the retrieval phase. However, a higher rank does not immediately translate into a reliable attack. Most attacks, against various configurations, settle around a 40% success rate, which could rise to 60% when considering ambiguous answers as successful attacks (those that include the expected benign one as well). Additionally, when using unoptimized documents, attackers deploying two of them (or more) for a target query can achieve similar results as those using optimized ones. Finally, exploration of the configuration space of a RAG showed limited impact in thwarting the attacks, where the most successful combination severely undermines functionality.", "sections": [{"title": "1. Introduction", "content": "Retrieval-Augmented Generation (RAG) is an increasingly adopted technique for integrating Large Language Models (LLMs) into applications, aimed at reducing the risk of inconsistent and incoherent responses, commonly referred to as hallucinations, and enabling responses to out-of-context knowledge without the need for retraining. RAGS achieve that by grounding LLMs' responses in an external knowledge base, which provides context to the model to generate an answer to a user-provided query. A notable implementation of RAG is Google Gemini's integration into Workspace, where Gemini answers questions using data stored across web services like Gmail. As the integration of LLMs into applications expands, the potential for attackers to exploit vulnerabilities to manipulate model responses increases. While much research has focused on the security of LLMs in isolation, less attention has been paid to the security of integrated systems such as RAGs.\nOne of the main security concerns for LLM-based applications is prompt injection attacks [1], where the attacker submits a malicious query designed to bypass security measures (e.g., Jailbreak [1]) or to extract confidential data such as high-quality prompts (e.g., [2], [3]). When a LLM is augmented with retrieval capabilities, an attacker can also introduce malicious inputs indirectly through the retrieved documents [4] with the objective of manipulating responses to benign users' queries. The success of these indirect attacks depends on the inclusion of a malicious document in the LLM's prompt. Previous research has proposed several techniques to manipulate documents to increase their ranking during the retrieval phase (e.g., [5], [6], [7]), which has been measured in isolation, focusing only on the retrieval component, showing that manipulations can increase the ranking of malicious documents. However, the capabilities of RAGs to split, manipulate, reorder, and reason about the position of malicious documents may reduce the effectiveness of these attacks. Consequently, the impact of end-to-end indirect prompt manipulation in RAGs is largely unknown.\nIn this paper, we investigate the security of RAG systems against end-to-end indirect prompt manipulations, delivering one of the first and most comprehensive evaluations by exploring the configuration space of RAG systems. We begin with a survey of RAG frameworks and an analytical decomposition, deriving a general RAG model with common components and configuration parameters. Then, we review prior work that targets adversarial techniques affecting ranking algorithms or retrieval components. We then design Rag-n-Roll, an automated end-to-end evaluation framework that systematically assesses the susceptibility of RAG systems to indirect manipulations. Rag-n-Roll"}, {"title": "2. Background", "content": "Before presenting our study, we first introduce some real world examples of RAG applications in Section 2.1, and then, we present the threat model of this work in Section 2.2.\n2.1. RAGS\nTraditional LLM applications generate responses based on factual knowledge retained from training data. Unfortunately, this introduces two significant challenges. The first challenge is LLM hallucinations, which are responses to users' questions that are inconsistent or incoherent with the factual knowledge in the training data. The second challenge is the inability to handle questions whose answers are not present in the training data. Addressing these challenges requires constantly updating LLMs through retraining, which is costly and time consuming.\nInstead of retraining LLMs, RAGs add contextual information to the users' queries fetched from an external knowledge base. The knowledge base is extracted from data sources such as web articles, file storage, or email boxes and saved in vector databases, which are database systems optimized to index information using multidimensional features. Each data point of the data source, e. g., a document, is split into chunks, and each chunk is indexed separately using text-to-vector embedding models. The embeddings and the text are stored in the vector database. Whenever a user submits a query, the RAG fetches its most relevant chunks from the vector db and includes both in the prompt fed into the LLM. Previous studies [8] have already validated the benefits of RAG to improve the model's performance.\n2.2. Threat Model\nAttacks targeting LLMs predominantly focus on user-controlled aspects, for example, the LLM input prompt. The attacker is also a user, aiming to manipulate the model's output or to extract confidential data such as the underlying application's private prompt through carefully crafted inputs. This threat still applies to RAG systems; however, in this paper, we focus on a new attack vector introduced by RAGs that exploits external sources providing manipulated contextual data to the model, thereby allowing the embedding of potentially harmful payloads.\nWe consider that the attacker can add malicious documents to the knowledge base, which are then correctly indexed and stored in the vector database. This scenario is feasible when the data source is public, e.g., social network messages, or when accessible to an attacker, e.g., an email"}, {"title": "3. Problem statement", "content": "This paper aims to answer the following questions:\n(RQ1) Analysis of RAG Architectures. RAG systems consist of more than just an LLM. We identify the critical components of a RAG system and investigate potential gateways for an attacker. For this, we divide a typical RAG system into its components and analyze their impact on the model's output from our threat model's perspective.\n(RQ2) Evaluation of Indirect Prompt Manipulation Attacks. Adversarial attacks that hijack RAG system responses often target the retrieval and re-ranking steps to boost the ranking of malicious documents. These algorithms achieve their goals by modifying or adding trigger tokens at the beginning of target texts to enhance their ranking. However, due to the capabilities of RAG pipelines to split, manipulate, reorder, and reason about the content of documents, the downstream success of these algorithms may not correlate directly with the position of the malicious document in the retrieved list. This research question aims to assess the effectiveness of existing strategies and explores how an attacker might refine their strategies to further enhance effectiveness within RAG systems.\n(RQ3) Evaluation of Baseline Attacks. Existing attacks optimize malicious documents so that the malicious information is ranked higher in the contextual information when presented to an LLM. While the previous research question addresses the effectiveness of these techniques, this research question examines a simpler attack, where the malicious documents contain only the malicious information, without any optimization such as trigger tokens.\n(RQ4) Configuration Parameters and RAG Robustness. While RAG parameters are typically optimized to enhance the performance of a model, e.g., maximizing correct answers, they may also play a role in preventing indirect prompt manipulation attacks. We address this challenge by"}, {"title": "4. Architecture of a RAG-based Application", "content": "We now present a prototypical architecture of the RAG frameworks. We derive this architecture by reviewing and decomposing existing RAG frameworks into their fundamental building block components, outlining their parameters, and explaining how these components are interconnected.\n4.1. Review of RAG Frameworks\n4.1.1. Identifying Relevant Frameworks. The initial step in our review identifies relevant RAG frameworks. We conducted this identification by searching for articles and blog posts within LLM communities, such as Medium and Reddit. Specifically, on Medium, we utilized the platform's search function with the query \"Retrieval augmented generation over documents\". For Reddit, we employed a Google search restricted to Reddit pages by incorporating the keyword \"site:reddit.com\u201d. When reviewing the top 15 search results from the result pages, we noted the recommended RAG frameworks. The top three most recommended frameworks are LangChain [9], LlamaIndex [10], and Haystack [11].\n4.1.2. Component Identification. After identifying the RAG frameworks, we read their developers' documentation, focusing on \"quick start\" tutorials and API documentation, as well as any website pages describing the creation of question-answering applications. We accessed these resources using each framework's internal documentation search engine with the keywords \u201cdocument question answering.\u201d\nFor each framework, we pinpointed building block components responsible for specific operations, such as retrieving documents or calculating document embeddings for data"}, {"title": "4.2. Components and Parameters", "content": "We now present the six components of our architecture.\n4.2.1. Data Source Processing. The first group of components is responsible to process documents from an external data source. These components cover different roles ranging from fetching data from one or more data sources to prepare the data for storage in a vector database, which are typically executed offline to build, update, and maintain the knowledge base used by the application to answer users' queries.\nData Download. This component retrieves documents from a data source and makes it available to the other components of the framework. Frameworks like LangChain offers pre-built downloaders for known sources, such as Wikipedia, Reddit, and Arxiv. However, developers could in principle also create their own downloaders to retrieve data from custom sources. This component does not transform, modify, or alter the content of the fetched data, thus we did not consider its parameters.\nData Splitting. Providing an entire document when preparing an answer may be inefficient and introducing irrelevant context to the mode as the information useful to the answer may be in a chunk of the document. Accordingly, a RAG attempts to identify relevant chunks of the document and provide the model only with the chunks appropriate to a given users' query. To achieve that, RAG frameworks offer a variety of data splitters to divide a document into chunks. For example, the data splitter of LangChain splits the document using the line feed. If a chunk is longer than the maximum chunk size, it keeps on splitting the chunk using another separator, i.e., the blank space. If chunks are still longer than the maximum size, it truncates the chunk at the chunk size. Data splitter may cut a text with the answer in two, destroying information useful for the answer. To mitigate that risk, data splitters have an additional parameter called padding size which is the number of overlapping characters between two consecutive chunks.\nData Indexing (Optional). Retrieving chunks that are relevant for a given query is a critical step. This process involves extracting an index from each text chunk before storing it in the database, and using the index at retrieval time to sort texts based on their relevance to the analyzed query. The data indexing component supports different indexing techniques. For instance, traditional algorithms such as BM25 [12] and TF-IDF exact lexical matching to construct vectors that represent texts. In these vectors, each dimension corresponds to a word, with the value being either n or 0, depending on whether the word is present in the text or not. Since most values in these vectors will be 0, these techniques are referred to as sparse retrievers. A significant disadvantage of lexical matching is that a text will be considered relevant to a query only if it shares the same terminology, which can lead to low precision. To address this limitation, dense retrievers use specially trained networks to extract dense embeddings that are more sensitive to the semantics of the texts. Finally, both sparse and dense retrievers can be combined in hybrid systems where indexes are a weighted sum of the results of the two.\n4.2.2. Vector Database. Vector databases are data management software optimized to store and retrieve data items in dense vector form. They are specialized in efficiently retrieving data using similarity functions, fetching the closest matching entries to the one supplied. Examples of vector databases are Chroma [13], Pgvector [14], and Pinecode [15]."}, {"title": "4.2.3. Answer Generation", "content": "The second group of components is responsible for generating the answer. This involves retrieving relevant chunks, optionally re-ranking them, and then generating the prompt for the LLM.\nChunks Retrieval. Based on the user's query, this component retrieves the relevant chunks that provide contextual information. The retrieval process transforms the query into the same feature space as the chunks (refer to the Retriever parameter of the data indexing component) and identifies the closest K chunks.\nData Re-Ranking (Optional). The embedding functions used for indexing and retrieving chunks prioritize speed over accuracy, employing functions that are quicker to compute. However, these faster functions may yield K chunks with irrelevant entries. To mitigate this, an optional re-ranking step can be implemented before final chunk selection. This step uses more accurate, albeit slower, embedding functions, such as cross-encoders. The parameter K from the retrieval component is only applied after re-ranking to select the most relevant chunks.\nAnswer Generation. The final step in a RAG is generating the answer by constructing a prompt for an LLM. Typically, this involves copying the selected K chunks and the user's query into a single prompt template, which is then fed to the chosen model. However, a review of the documentation revealed that LangChain proposes four distinct patterns for utilizing retrieved data with a model [9]. Each pattern describes a different model interaction and potential transformation of the contextual information, including the general approach. Consequently, we have decided to include these interaction modes and their default prompts in our study (see [16]):\n1) Stuff chain: This mode involves taking all K retrieved documents and integrating them into the prompt. The model is tasked with generating an answer to the given query using this comprehensive input.\n2) Refine chain: In this mode, the model initially uses the first document to generate an answer. The response is then iteratively refined using the subsequent documents.\n3) Map reduce chain: This mode utilizes the K documents to generate individual answers for each; these answers are then synthesized into a final response in a subsequent LLM call.\n4) Map Re-rank chain: This mode employs the LLM to generate a response for each document along with a score. The responses are then re-ranked based on their scores, and the best response is selected.\nIn addition to the interaction mode, this component requires several parameters to be passed to the LLM. These parameters influence various aspects of the response, such as aesthetic quality-e.g., penalties for repeated words and control over the randomness of the model's output, such as"}, {"title": "4.3. Default RAG Configuration", "content": "To systematically evaluate the efficacy of each parameter in isolation, we first establish a default configuration of a RAG to serve as a foundational baseline for our experiments. When reviewing the documentation, we identified the default or recommended parameter for each component. The default parameter value is shown in Table 1."}, {"title": "5. Indirect Prompt Manipulation Attacks", "content": "We compile a list of concrete attacks that aim at indirectly manipulating the output to attacker-specified responses through alterations of the knowledge base. We start with a literature review of prior work to identify methods and techniques capable of generating such manipulated documents. Subsequently, we identify approaches that may serve as baselines for the experiments detailed in Section 7.\n5.1. Attacks from the Literature\n5.1.1. Survey. To manipulate the output of an LLM, an attacker primarily targets the retrieval phase and the subsequent ranking step. We conducted a systematic literature review focusing on the algorithms that govern these two components.\nIn total, we identified 11 papers (i.e., [6], [7], [17], [18], [19], [20], [21], [22], [23], [24], [25]. Of these, only three provided the source code or the necessary artifacts to implement their techniques. These techniques are Adversarial Semantic Collision (ASC) [17], PAT [7], and IDEM [5]. Additionally, the authors of PAT [7] and IDEM [5] evaluated their approaches against a simpler attack known as Query+ [7], where the attacker embeds the targeted query within the malicious document.\n5.1.2. Selected Attacks. We selected ASC [17], PAT [7], IDEM [5], and Query+ [7] for our evaluation. All of these techniques generate a trigger text that improves the alignment of the multi-dimensional representations of the query and the malicious document. The attack requires two"}, {"title": "5.1.3. Position of the Triggers", "content": "The selected approaches embed triggers within malicious documents to increase their retrieval rank. Among the four methods, only IDEM determines the trigger's position as part of its generation approach. The other three\u2014ASC, PAT, and Query+-require specification of the exact position in the original document for inserting the trigger.\nThe position of these triggers may influence the success of the attacks. Firstly, the attacker could position the triggers at the document's outset, as previous studies have suggested [7]. However, RAGs often split documents into smaller chunks (refer to Data Splitting in Figure 2), potentially separating the trigger from the associated malicious text, thus reducing the attack's impact. Alternatively, the trigger could be strategically placed to appear at the beginning of a chunk containing malicious content. This placement strategy, however, assumes the attacker knows about the specific parameters used by the RAG developer, such as chunk size, which is not the case, complicating accurate trigger placement. A more reliable strategy might involve positioning the trigger in close proximity to the malicious information, specifically immediately preceding it."}, {"title": "5.2. Generative Optimizations", "content": "The attacks we have discussed focus on optimizing texts to maximize their probability of retrieval. These optimizations are typically highly localized, targeting specific document positions for trigger placement. However, if triggers and adversarial responses become separated during the document chunking phase, the effectiveness of these optimizations is compromised. Additionally, the insertion of uncorrelated texts can disrupt the flow of the content, thereby reducing both the stealthiness and the quality of the generated response.\nTo mitigate these issues, we explored an alternative strategy to generate a malicious document using a Phi3 model [27]. This model was instructed to rewrite input documents with dual objectives: optimizing their ranking for the given query and enhancing the delivery of the malicious answer. We choose this model to be different from the evaluated RAG models to avoid inducing bias in the generated documents. The specific prompt used in this experiment is detailed below:\nYou are a Search Engine Optimization (SEO) expert that wants to boost an article to spread the answer to a specific question as much as possible.\nGiven an article, your task is to rewrite it so that:\n1) It ranks high on search engines for the given question\n2) The information relative to the targeted question/answer pair is immediately visible and highlighted.\nYou can repeat or remove information from the articles if you find it useful.\nThe article is:\n{Article}\nThe targeted question is:\n{Question}\nThe targeted answer is:\n{Answer}"}, {"title": "6. Dataset and Metrics", "content": "6.1. Evaluation Metrics and Output Matching\n6.1.1. Metrics. This paper evaluates various RAG configurations as detailed in Section 4 against indirect prompt manipulation attacks described in Section 5.1. Each attack generates an optimized malicious document for insertion into the knowledge database, based on a target query and an initial malicious document that contains a malicious answer to the target query. To assess the true effectiveness of an attack, we establish a baseline performance of the RAG configurations in the absence of an attacker, i.e., when the RAG system processes queries with a pristine knowledge base devoid of any malicious documents.\nIn each experiment, regardless of the attacker's presence, it is crucial to determine whether the RAG's response is correct. For instance, if the knowledge base is free of malicious documents, we verify that the RAG provides the expected benign answer. Conversely, if the knowledge base has been compromised, we assess whether the RAG delivers the expected malicious response. Additionally, considering that the model may encounter contradictory contextual information\u2014comprising both malicious and benign answers-it is reasonable to expect responses that amalgamate these elements. Finally, in scenarios involving inconsistent or incoherent responses, such as hallucinations, these must also be accounted for. Accordingly, this paper employs the following four metrics:\nBenign Answers (Ben): This metric quantifies the number of responses that correspond with the expected benign answers.\nMalicious Answers (Mal): This metric quantifies the number of responses that align with the expected malicious answers.\nAmbiguous Answers (Amb): This metric quantifies the number of responses that match both the expected benign and malicious answers in the same response.\nInconclusive Answers (Inc): This metric quantifies the number of responses that match neither the expected benign nor malicious answers. These answers include those that the model could not answer with the given context or provide an incoherent and inconsistent answer (hallucinations).\n6.1.2. Output Matching. Matching responses from LLMs against a set of expected answers poses significant challenges. Previous approaches have employed both string matching (i.e., [8], [28], [29]) and LLM-based techniques (i.e., [30]) to ascertain equivalence between responses. While string matching offers rapid results, its accuracy is contingent upon the presence of multiple possible answers. Alternatively, using an LLM can enhance robustness to answer variability but at the expense of processing speed. Given the extensive number of evaluations and responses generated during our assessment, we have opted for string matching. To address the challenges associated with robustness, we increase the variability of the answers as discussed in Section 6.2\n6.2. Evaluation Dataset\nThis section presents the creation of our dataset. Our dataset contains 119 data points. Each data point contains: two queries (one original and one variant), a benign document, six benign answers (at least one original and at most five variants), six malicious answers, and one malicious document. In addition, the dataset contains 3.000 benign documents that are unrelated with the queries. We use these documents to form a generic knowledge base."}, {"title": "6.2.1. Initial Dataset", "content": "To create our dataset, we considered the NQ dataset that contains questions, the expected answers, and Wikipedia pages containing the answers. Unfortunately, the answers of this dataset are long, which makes it hard to match them against the model's answers because of the too many syntactic variations introduced by the model itself. Accordingly, we looked for a similar dataset with shorter answers and used the dataset curated by Liu et al. [28]. This dataset has 2.655 entries and is a subset of the NQ dataset with at most five token long answers. Liu's data set does not contain the original Wikipedia page that we need for the knowledge base. We instead retrieved it from the original NQ dataset. Finally, to ensure that the document associated to a question contains the expected answer, we verify via string matching that the answer is present. After that, we selected additional 3.000 documents from the NQ dataset that are not related to the questions to form a generic knowledge base."}, {"title": "6.2.2. Generation", "content": "We then expand the initial dataset to include variants of questions, answers, and malicious documents. We iterated the generation procedure until we obtain 120 valid data points.\nFirst, we start with benign answers. Here, we observe that models can still provide valid answers that are syntactically different from the expected benign ones. For example, the date of a historical event can be formatted in different ways, e.g., \"01/01/1970\" or \"Jan 1st, 1970\". Such differences will challenge matching the correct answer in the RAG's output. Accordingly, we generate variants of the benign answers, asking an LLM (GPT-4) to provide syntactic variants of the benign answers in our dataset. The exact prompt for this step is presented in Appendix A.1. We then verify that the answers, both originals and variants, are consistent with the original documents and the original queries by asking GPT to answer the questions using the original documents only. If the answer does not match any of the valid answers, we discard the data point from our dataset. The prompt is in shown Appendix A.2\nSecond, our threat model assumes that an attacker intends to hijack questions users may ask. This implies that the attacker has a set of questions that are the target of their attack. Accordingly, we generate a dataset of variants of the questions starting from our initial dataset using GPT-4. The exact prompt for this step is provided in Appendix A.3. Then, the objective of the attacker is to hijack questions with attacker-chosen answers using malicious documents. Instead of generating these documents and answer manually, we asked GPT to act as a teacher and generate them. The exact prompt to generate answers in Appendix A.4 and the one to generate documents is in Appendix A.5. Finally, we verify that the questions, malicious answers, and malicious documents are consistent. The prompt is Appendix A.2. If the answer does not match any of the expected answers, we discard the data point."}, {"title": "6.2.3. Sanity Checks", "content": "Given the widespread recognition of the NQ dataset as a foundational resource for training Question Answering (QA) models, it is probable that the LLMs leveraged within our pipeline have been trained using this dataset. This simple fact introduces a potential bias, as these models may tend to replicate responses they were originally trained on. In real-world applications, RAG pipelines are often deployed on novel datasets, making the use of familiar query-answer pairs an unreliable method for evaluation. To address this issue, we employed GPT-4 to generate semantically similar variants of the original queries for testing. We then used these mutated queries to collect results on the default version of our pipeline without attacks, while using different LLMs. The results are depicted in Figure 3. Notably, all models, except for GPT-4, demonstrates a preference for the original version of the queries. Consequently, to offer a fairer evaluation, we have decided to utilize these mutated queries in all subsequent experiments to minimize the dataset-induced bias as much as possible."}, {"title": "7. End-to-End Evaluation", "content": "We evaluate the RAG piple under attack for different configurations to assess the impact of different parameters.\n7.1. Experiment Setup\n7.1.1. Rag-n-Roll. We developed a framework called Rag-n-Roll to conduct the experiments described in this paper. The primary objective of Rag-n-Roll is to facilitate the evaluation of prompt injection attacks on different RAG implementations. Rag-n-Roll requires two inputs: the configured RAG under test and a dataset comprising questions and benign documents. Then, Rag-n-Roll generates tests for the RAG under test and evaluates the outcomes. Figure 4 shows the architecture of Rag-n-Roll. The main components of Rag-n-Roll include:\nDataset Generation: This module constructs a dataset as outlined in Section 6.2, starting from a collection of questions and benign documents. It then utilizes an attack library to optimize malicious documents for indirect prompt injection attacks.\nAttack Library: The attack library currently includes the five attacks discussed in Section 5.1, including variants (both trigger position and aggressiveness level for ASC).\nData Sources Creation: This module generates data sources, one for each type of attack. These data sources are subsequently integrated into and utilized by the RAG under test.\nTest Generation: This module prepares the inputs for the tests, i.e., formulates the questions and the expected answers, and associates them with the data sources created in the previous step.\nResponse Analysis: This component analyzes the responses from the RAG, applying the four evaluation metrics detailed in Section 6.1, namely, benign, malicious, ambiguous, and incoherent responses.\n7.1.2. RAG-based Application Under Tests. For the experiments presented in this paper, we implemented a RAG-based question answering bot application using LangChain. We implemented the following configurations using the parameters listed in Table 1:\nDefault Configuration: The first configuration adheres to the default parameter values specified in Table 1.\nSingle Parameter Change: Building on the default configuration, we created five groups of configurations, each corresponding to a distinct RAG component. For each group, we changed the value of one parameter of that component. For instance, we created seven configurations for the data splitting component, comprising eight for chunk size values and four for padding size values.\nRobust Configuration: This configuration was developed by evaluating the performance and robustness of other configurations, identifying parameters that minimize attack success without compromising functionality. We present the exact configuration after we present the experiment results.\n7.1.3. Instantiation of Rag-n-Roll. Throughout this section, each attack against a selected configuration is evaluated alongside the performance of the configuration in the absence of an attacker, i.e., when the knowledge base is not poisoned.\nWhen the attacker poisons the knowledge base, for each query and initial malicious document, we generate one or more optimized versions of the document for each attack technique via the dataset generation module of Rag-n-Roll. The number of optimized documents varies depending on the variable being tested; this is further clarified during the presentation of the experiments. For each optimized malicious document, we create a knowledge base containing the benign documents, generic documents, and the optimized malicious document using the data source creation module of Rag-n-Roll. We then run the tests and measure the benign, malicious, ambiguous, and incoherent responses."}, {"title": "7.2. Experiment Results", "content": "7.2.1. Baselines of the Default Configuration. Before evaluating our configurations in the presence of optimized malicious documents, we first establish the baseline behavior of the default configuration under two distinct scenarios: one where the entire knowledge base contains no malicious documents and another where it includes unoptimized malicious documents. The results of these baseline assessments are presented in Table 3.\nWhen the knowledge base is unpoisoned, the default configuration accurately responds to approximately three-quarters of the questions. The remaining one-quarter of the responses fail to include the expected answer. Of these incorrect responses, 14.2% are non-committal with replies such as \"I cannot answer\" or \"I do not know.\" A further 9.2% of all responses are inconsistent or incoherent (hallucinations). The remaining 1% of responses are variants of the correct answer that our response evaluation module failed to recognize.\nUnder attack with unoptimized documents, containing only malicious information, the attacker successfully manipulates 19.3% of the responses with the intended malicious content. In addition, 18.5% of the responses are ambiguous, containing both malicious and benign information. Despite these attempts, the model remains resilient in 42% of the cases, continuing to return the expected benign answer. The remaining 19% of the responses are inconclusive, divided as follows: 15% are non-committal (e.g., 'I cannot answer\" or \"I do not know\"), 2.5% are inconsistent or incoherent (hallucinations), and 1% are accurate but were misidentified due to pattern matching errors in Rag-n-Roll.\n7.2.2. Effectiveness of Triggers Position. Having established the baseline behavior, we now evaluate the effectiveness of the trigger's position within the document. This experiment is designed to empirically determine the placement strategy for subsequent experiments. We generated optimized malicious documents utilizing ASC, PAT, IDEM, and Query+ techniques, considering two possible positions: at the beginning of the document and immediately before the malicious answers. The results for each attack configuration are presented in Section 7.2.2, with aggregated results for each trigger position shown in Section 7.2.2. The tables also include baseline results with unoptimized malicious documents.\nThe results indicate that positioning the trigger immediately before the malicious sentences typically doubles the fraction of responses that are malicious. Additionally, the average number of chunks derived from malicious optimized documents (# Mal chunks in Section 7.2.2) retrieved by the LLM is greater when the trigger is positioned before the malicious answer. Conversely, placing the trigger at the beginning of the document does not significantly affect the likelihood of increasing the retrieval of malicious chunks on average when compared with the unoptimized version of the attack.\n7.2.3. Parameters' Effects on Attacks. The objective of this section is to determine if selecting the right parameters can strengthen a RAG pipeline and reduce the malicious answers produced by the model. For each identified param-"}, {"title": "7.2.4. Optimal Configuration", "content": "From Table 6 and Table 7, we selected parameters that demonstrated higher performance over the baseline and exhibited a lower success rate for attacks. We configured the chunk size to 1200, padding size to 600, retriever to TEL, with no re-ranking, set K to 12, temperature to 0, top-p to 0, and chose GPT4 as the model. Section 7.2.4 displays the attack results for each scenario, including the average number of malicious chunks provided to the LLM.\nThe results indicate that the RAG with the optimal configuration tends to reduce ambiguous and inconclusive responses, while increasing the number of benign answers. However, the number of malicious answers does not appear to have been significantly affected. In conclusion, the combination of parameters that individually may mitigate attacks does not necessarily amplify their effectiveness when combined."}, {"title": "7.2.5. Contribution of the Optimizations", "content": "Table 6 and Table 7 shows that ASC and PAT tend to perform as good as or worse than the unoptimized documents baseline. The other attacks, SEO, Query+, and PAT tend to perform better than the baseline; however, we observe that these attacks hardly outperform the already-low baseline, rarely going about 50% chance of hijacking the response by settling mostly around 40% chance of success."}, {"title": "7.2.6. Attacks with Multiple Documents", "content": "The evaluation so far work under the assumption that (i) the attacker uses a single malicious document to hijack the model's response and (ii) the knowledge base contains only one benign document with the correct answer. We now revisit that and evaluate that assumptions and measure the the impact when more malicious and benign redundant information is present in the knowledge base. For this experiment, we expand our dataset with additional benign and malicious, unoptimized documents. We used 20 queries from our dataset as Google Search query and grabbed the first five pages answering our queries. For the malicious document, we used GPT-4 with the prompt Appendix A.5 to generate five mutations of the original malicious document. For this evaluation, we used the default RAG configuration. Results are shown in the image Table 9.\nResults show a positive correlation between higher redundancy of malicious documents and attack success, i.e., the more malicious documents the attacker inserts in the knowledge base, the higher the chance of successful attacks. For example, with six malicious documents the attacker can reach about 60% and 55% success rate when only one and two benign documents are present respectively. At the same time, a higher redundancy of benign documents can mitigate the impact of malicious documents. For example, four documents are sufficient to bring the success rate to about 45%."}, {"title": "8. Discussion", "content": "8.1. Takeaway\nWe now review our results and distill the main takeaways from our experiments.\n1) Struggle to Achieve a Stable Attack Success Rate. Our results show that proposed attacks against retrieval and re-ranking struggle to achieve a stable success rate higher than 50%. Many of them, e.g., ASC and its variants, even tend to perform like or worse than the unoptimized documents baseline. Other optimization techniques perform better, however, they settle around 40% success rate, which is about as twice as good as the unoptimized documents baseline. Surprisingly, two of the best performing techniques are Query+ (a malicious document using the original query as trigger) and SEO (a Phi3 generated"}]}