{"title": "Long-Range Vision-Based UAV-assisted Localization for Unmanned Surface Vehicles", "authors": ["Waseem Akram", "Siyuan Yang", "Hailiang Kuang", "Xiaoyu He", "Muhayy Ud Din", "Yihao Dong", "Defu Lin", "Lakmal Seneviratne", "Shaoming He", "Irfan Hussain"], "abstract": "The global positioning system (GPS) has become an indispensable navigation method for field operations with unmanned surface vehicles (USVs) in marine environments. However, GPS may not always be available outdoors because it is vulnerable to natural interference and malicious jamming attacks. Thus, an alternative navigation system is required when the use of GPS is restricted or prohibited. To this end, we present a novel method that utilizes an Unmanned Aerial Vehicle (UAV) to assist in localizing USVs in GNSS-restricted marine environments. In our approach, the UAV flies along the shoreline at a consistent altitude, continuously tracking and detecting the USV using a deep learning-based approach on camera images. Subsequently, triangulation techniques are applied to estimate the USV's position relative to the UAV, utilizing geometric information and datalink range from the UAV. We propose adjusting the UAV's camera angle based on the pixel error between the USV and the image center throughout the localization process to enhance accuracy. Additionally, visual measurements are integrated into an Extended Kalman Filter (EKF) for robust state estimation. To validate our proposed method, we utilize a USV equipped with onboard sensors and a UAV equipped with a camera. A heterogeneous robotic interface is established to facilitate communication between the USV and UAV. We demonstrate the efficacy of our approach through a series of experiments conducted during the \"Muhammad Bin Zayed International Robotic Challenge (MBZIRC-2024)\" in real marine environments, incorporating noisy measurements and ocean disturbances. The successful outcomes indicate the potential of our method to complement GPS for USV navigation.", "sections": [{"title": "I. INTRODUCTION", "content": "The maritime sector is increasingly focusing on Unmanned Surface Vehicles (USVs), which possess the ability to autonomously carry out navigation missions [1]. These vehicles have gained significant attention due to their diverse and impactful applications, including the exploration of marine resources [2], oceanographic mapping [3], [4], and the inspection and monitoring of coastal and offshore structures [5], [6], ports [7], and more [8], [9]. In marine dynamic environments, accurate localization of USVs (e.g., to estimate one's position and orientation with respect to surrounding environments) is crucial [10]. The USVs require location information to make decisions related to control, navigation, collision avoidance, and path planning [11]. Therefore, localization is considered a fundamental capability for USVs engaged in tracking, exploration, or monitoring of the marine environment [12], [13].\nDue to technological advancements, an integrated navigation system, combining an Inertial Navigation System (INS) with the Global Positioning System (GPS), has been employed for navigation and"}, {"title": "II. RELATED WORK", "content": "Currently, a range of methodologies and technologies employed in USV localization, emphasizing sensor fusion [30], [31], vision-based techniques [32], radar integration [33], and deep learning [34], [35] applications to enhance accuracy, robustness, and reliability in challenging maritime environments. Each approach contributes unique insights and advancements towards achieving effective and efficient USV localization capabilities. However, these studies exhibit notable limitations in the context of USV localization. Firstly, they are constrained by range, operating within limited geographical areas. Many of these methods rely on vision-based techniques using USV onboard camera imagery, which becomes impractical when the USV must navigate the broader region. Additionally, achieving feature-based localization becomes challenging due to poor visibility, low light conditions, and inadequate texture information in imaging data. Hence, we propose a UAV-assisted vision-based localization for USVs to enhance navigation capabilities in GPS-restricted environments.\nIn contrast to existing approaches, our work aims to address these challenges by integrating UAV and USV equipped with diverse sensors for localization, a combination rarely experimentally demonstrated in real-world marine settings under GNSS-denied environments. In the following, we briefly review different approaches, such as Radar-based, LiDAR-based, and Vision-based, for USV localization in GNSS-denied environments."}, {"title": "A. Radar-based Approaches", "content": "Radar-based localization techniques for USVs have emerged as practical solutions, particularly in GPS-denied or challenging maritime environments. Radar systems offer unique capabilities for detecting and mapping surrounding obstacles and features, which can be used for accurate USV positioning and navigation. Several notable studies have explored radar-based USV localization methods. For example, Han et al. [1] proposed a radar-centric approach for USV localization and navigation, adopting the SLAM paradigm. Their method involves extracting coastline contours from radar-acquired images using image processing techniques. These extracted contours serve as landmark features for localization, and an EKF-based algorithm is employed for estimating vehicle positions based on these features. The experimental results highlight this approach's computational efficiency and effectiveness compared to traditional point-cloud methods. Ma et al. [20] introduced a technique that leverages the fusion of radar and satellite imagery for USV localization. Their approach uses computer vision methodologies to extract coastline features from radar and satellite images. By developing an image registration technique that accounts for horizontal and vertical perspectives captured in the input images, they demonstrated the viability of this approach with an average error of 9.77 meters in USV positioning. Dagdilelis et al. [36] presented a novel radar-based method for localizing USVs using sea chart information. They proposed utilizing radar detection to identify underwater buoys and matching these buoys with entries from electronic navigation charts. Subsequently, triangulation and trilateration methods are applied for precise pose estimation. The simulation results in the Great Belt region in Denmark showed promising reductions in uncertainty regarding pose and heading estimation. However, further research in this area may focus on improving radar sensor technologies, optimizing algorithms for real-time processing, and integrating radar systems with other sensor modalities to enhance further USV localization performance and reliability."}, {"title": "B. LiDAR-based Approaches", "content": "LiDAR-based localization methods for USVs offer promising solutions for navigating in GPS-denied environments by utilizing laser scanning technology to generate high-resolution 3D maps of the surroundings. Several studies have explored LiDAR-based USV localization techniques, demonstrating their effectiveness and applicability in challenging maritime scenarios. Shen et al. [37] introduced a novel approach for USV localization by integrating LiDAR SLAM with GNSS/INS systems. Their method employs a dynamic switching strategy to transition to LiDAR SLAM positioning when GPS signals are unavailable or unreliable. Position and heading estimates are refined using the EKF algorithm. Experimental results showed a significant reduction (55.4%) in position error compared to traditional Kalman filter algorithms, highlighting the potential of LiDAR-based localization for USVs.\nThe study in [38] discussed an autonomous SLAM navigation, path planning, and collision avoidance system for the USV, equipped with a Velodyne 3D VLP16 lidar sensor and Axis PTZ camera. Using the Robot Operating System (ROS) navigation stack, the USV demonstrates successful autonomous navigation, path planning, and obstacle avoidance in marine environments, generating detailed maps for pipeline inspection. Similarly, the studies in [39], [39], [40] also proposed using LiDAR technology for the USV localization in marine environments. Although, LiDAR-based USV localization methods offer advantages such as high accuracy, independence from external signals (like GPS), and suitability for mapping complex environments with obstacles. However, many challenges exist, including the cost and complexity of LiDAR systems and the need for robust algorithms to handle real-time processing of dense point cloud data in dynamic maritime environments."}, {"title": "C. Vision-based Approaches", "content": "Over the past few years, significant efforts have been made regarding USV localization and navigation in marine complex environments. One approach that has gained popularity in this context is the implementation of vision-based localization. For instance, Liu et al. [16] proposed a visual-inertial odometry (VIO) technique for USV localization in GPS-restricted environments. They utilized cameras to capture point and line features along bridge walls, integrating this visual data with inertial measurements for real-time position estimation. While effective, the method's computational complexity is a noted challenge. Roedele et al. [14] introduced a monocular camera imaging method within USV operating zones. By matching camera features with synthetic images from a digital elevation model (DEM), they derived 3-dimensional position estimates. However, practical deployment feasibility remains a concern. Volden et al. [8] explored a stereo-vision approach for USV localization during docking maneuvers. They utilized stereo cameras to detect and triangulate ArUco tags at docking stations, integrating deep learning and feature-matching techniques. While practical, further testing across diverse weather conditions is required for robustness validation. Hu et al. [41] leveraged lidar semantic and geometric data alongside deep learning models for USV localization during docking and departing scenarios. Their approach identified and mitigated the influence of dynamic objects on localization accuracy, achieving superior performance compared to traditional GPS systems."}, {"title": "III. PROPOSED APPROACH", "content": null}, {"title": "A. Solution overview", "content": "The proposed framework for USV localization consists of two main components: the UAV and the USV. The UAV is equipped with a high-definition movable camera, which it uses to scan the open sea environment and identify target locations, such as vessels. Once a target is identified, the UAV records its position information, establishing it as the reference point. The UAV then maintains a fixed altitude and continuously scans the USV, accurately estimating its relative position. This is achieved using a state-of-the-art deep learning algorithm that enables the UAV to track and locate the USV. An Extended Kalman Filter (EKF) is employed to determine the USV's position relative to the UAV's location and altitude based on detection and geometric measurements integrated with the radio range (datalink range) measurements. With the target position provided by the UAV, the algorithm generates the USV's state matrix in the NED (North-East-Down) frame using the EKF. This data is subsequently used by conventional closed-loop control to guide the USV toward the target. An"}, {"title": "B. USV detection", "content": "This section discusses the essential steps in obtaining the USV detection model using data-driven methods. Fig. 3 gives an overview of the process. This illustrates the essential steps involved in refining an object recognition model. Initially, images are prepared and annotated to establish ground truth for the supervised Convolutional Neural Network (CNN) learning process. The annotated data is then introduced into the data-driven detection model, and pre-trained model weights are used to refine the model. A validation set is employed for model selection, determining the appropriate stopping point for training. Subsequently, the refined model tests unseen data to assess its accuracy, often measured by mean average precision (mAP) metrics. The final model weights are deployed for object recognition tasks during the prediction phase if the outcomes are satisfactory. Next, we discuss the preparation of the USV dataset, model training, and model testing.\n1) Step 1- Data preparation: The dataset plays a pivotal role in training CNN models in supervised learning. This data serves as the ground truth for the specific class the model aims to learn. During this phase, a custom dataset is collected, comprising examples of the class and their associated features, which the model is expected to learn. In our study, we conducted data collection to gather imaging data of USV from various perspectives and angles within a marine environment. A comprehensive custom dataset containing images of USVs captured in real marine settings was curated. This dataset encompasses color image data captured by a UAV while operating over the sea surface, as depicted in Fig. 4. Subsequently, the dataset was randomized and divided, allocating 80% for training purposes and 10% each for validation and testing, respectively.\nGround truth labels guide the supervised model towards the correct answer. We use the annotation program Yolo Mark to create ground truth"}, {"title": "C. USV localization", "content": "In this section, we present the approach used for localizing USVs. This localization process is executed by a heterogeneous robotic system consisting of both UAV and USV in a marine environment where GNSS signals are restricted. The UAV acquires visual data using its camera from a fixed altitude near the coastal area, scanning the surroundings. Initially, the UAV employs a search method to locate the USV within its field of view as shown in Fig. ??. This entails controlling the camera's angle and position to scan the area effectively. Once the USV is detected within the camera's field of view, the detector identifies and extracts the bounding box of the USV object within the image frame. The results of this bounding box and camera information are used for USV pose estimation to allow smooth USV navigation along the predefined target positions.\nVision-based pose estimation relies on the orientation and position of the camera. The UAV's camera needs to maintain the detected USV centrally within the image frame. This task is accomplished through the process involves calculating the pixel error and sending control input to the UAV's camera, allowing it to make adjustments to minimize the pixel error between the image center and the bounding box showing the detected USV.\n1) Coordinate transformations: In the context of USV localization and navigation, it's common to represent the USV position in the NED (North-East-Down) inertial frame. In the NED frame, the x-axis points north, the y-axis points east, and the z-axis points downward towards the Earth's center. This is a local coordinate system and is often preferred in marine navigation [44]. As such, we process the transformation of the camera measurement to the USV inertial frame.\nThis coordinate transformation aims to estimate the USV's current position and heading angle, denoted as $p = {x,y,z,yaw}$, within the inertial frame as the USV navigates toward the predefined target. Estimating the USV pose involves integrating data collected from various reference frames. The following different frames are used in the coordinate transformations.\nAs depicted in Table I, the \u201cInertial\u201d denotes the East-North-Up (ENU) frame, with the UAV takeoff position on the ground serving as the origin. In the ENU frame, East is represented by the $x$ axis, North by the $y$ axis, and Up by the $z$ axis. The \"Body\" means the Front-Left-Up (FLU) frame, originating at the UAV camera. In the FLU frame, the front is denoted by the $x$ axis, the left side by the $y$ axis, and the upward direction by the $z$ axis. The \u201cPod/Camera\u201d illustrates the Front-Left-Up (FLU) frame, centered at the origin of the UAV's camera. In this FLU frame, the front of the camera corresponds to the $x$ axis, the left side corresponds to the $y$ axis, and the upward direction corresponds to the $z$ axis.\nLet $\\{r_I, p_I, q_I\\}$ represent the Euler angles denoting roll, pitch, and yaw, respectively, in the I frame of the UAV. Moreover, the camera's Euler angles in the B frame, denoted by $\\{r_B, p_B, y_B\\}$, indicate the camera's roll, pitch, and yaw angles. The camera's field of view, both horizontally and vertically, is expressed as $\\theta_p$ and $\\phi_p$, respectively. Additionally, we use pixel error between the detected position of the USV in the image, considering both horizontal and vertical directions relative to the center of the image. For this purpose, normalized pixel errors for horizontal and vertical directions are represented as $(u, v)\\in [-1,1]$. Moreover, the datalink range is denoted by $r$, indicating the range between the UAV and the USV. Lastly, the UAV's position is denoted by $P_{UAV} = [x_{uav}, y_{uav}, z_{uav}]^T = [x_{uav}, y_{uav}, h]^T$, where the $x_{uav}$ and $y_{uav}$ positions are assumed to be 0, and $h$"}, {"title": "D. USV positioning via EKF", "content": "We base our measurements solely on geometric principles when we obtain the state variables through basic geometric relationships. In this approach, each frame of measurement provides outputs that are independent of each other, lacking any intrinsic relationship. Consequently, this independence between measurement frames leads to significant output variations. The lack of correlation between measurements from different frames can introduce uncertainty and inconsistency in the estimation process [45].\nWe utilize the EKF to address this issue. The EKF is a powerful tool in estimation theory that enables the estimation of state variables in systems with nonlinear dynamics [46]. Unlike traditional linear estimation techniques, the EKF can effectively handle nonlinear relationships between variables, making it particularly suitable for scenarios where geometric relationships alone may not suffice to capture the complexity of the system dynamics [47].\nBy employing the EKF, we aim to integrate the information from multiple measurement frames and exploit the correlations between them to improve the accuracy and reliability of our state variable estimates. The EKF achieves this by iteratively updating the state estimates based on the latest measurements while considering the system's nonlinear dynamics [48]. Through this iterative process, the EKF refines the forecast, reducing the impact of measurement variations and enhancing the overall robustness of the estimation process [49].\nThe most critical part of using EKF is model creation, based upon estimation theory principles, to derive a nonlinear transition function that accommodates unknown variables within each estimation state [45]. Within the framework of EKF, there are two general essential models: the state model and the measurement model, expressed as follows:\nState model: $x_{k+1} = f(x_k, u_k + w_k)$\nMeasurement model: $z_k = h(x_k + v_k)$        (7)\nhere, $x$ represents the state model comprising parameters utilized for state estimation, where $x_{k+1}$ denotes the predicted subsequent state of the model. The term $u_k$ signifies the control input, while $w_k$ pertains to noise inherent in the system. In the measurement model, $z_k$ encapsulates data from various sensors, while $v_k$ denotes Gaussian white noise.\nThe state variable is set as position of target in I frame, a 3 \u00d7 1 vector,\n$x = [x, y, z]^T$        (8)\nFor the process model, we assume it as stationary target, i.e.,\n$F =\n\\begin{bmatrix}\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 1\n\\end{bmatrix}$         (9)\nMeasurements are ranger $r$, camera azimuth $\\alpha$, camera elevation $\\epsilon$, and height of UAV $h$, i.e.,\n$h(x) = z = [r, \\alpha, \\epsilon, h]^T$        (10)\nMeasurement equation is\n$z=\\begin{bmatrix}\nr\\\\\n\\alpha\\\\\n\\epsilon\\\\\nh\n\\end{bmatrix} = \\begin{bmatrix}\n\\sqrt{x^2 + y^2 + z^2}\\\\\narctan(y_c/x_c)\\\\\narctan(z_c/x_c)\\\\\nz + z_{uav}\n\\end{bmatrix}$        (11)\nOne key aspect of EKF is the use of the Jacobian matrix, which is essentially the first-order derivative, to linearize the non-linear functions $f$ and $h$. Jacobi matrix is:\n$H = \\frac{\\partial h(x)}{\\partial (x)} |_x =\\begin{bmatrix}\n\\frac{\\partial r}{\\partial x} & \\frac{\\partial r}{\\partial y} & \\frac{\\partial r}{\\partial z}\\\\\n\\frac{\\partial \\alpha}{\\partial x} & \\frac{\\partial \\alpha}{\\partial y} & \\frac{\\partial \\alpha}{\\partial z}\\\\\n\\frac{\\partial \\epsilon}{\\partial x} & \\frac{\\partial \\epsilon}{\\partial y} & \\frac{\\partial \\epsilon}{\\partial z}\\\\\n\\frac{\\partial h}{\\partial x} & \\frac{\\partial h}{\\partial y} & \\frac{\\partial h}{\\partial z}\n\\end{bmatrix}$         (12)\n$H = \\frac{\\partial h(x)}{\\partial (x)} |_x =\\begin{bmatrix}\n\\frac{\\partial r}{\\partial x} & \\frac{\\partial r}{\\partial y} & \\frac{\\partial r}{\\partial z}\\\\\n\\frac{\\partial \\alpha}{\\partial x} & \\frac{\\partial \\alpha}{\\partial y} & \\frac{\\partial \\alpha}{\\partial z}\\\\\n\\frac{\\partial \\epsilon}{\\partial x} & \\frac{\\partial \\epsilon}{\\partial y} & \\frac{\\partial \\epsilon}{\\partial z}\\\\\n0 & 0 & 1\n\\end{bmatrix}$        (13)\n$\\begin{bmatrix}\n\\frac{\\partial p_c}{\\partial x} & \\frac{\\partial p_c}{\\partial y} &  \\frac{\\partial p_c}{\\partial z}\\\\\n\\frac{\\partial p_c}{\\partial x} & \\frac{\\partial p_c}{\\partial y} &  \\frac{\\partial p_c}{\\partial z}\\\\\n\\frac{\\partial p_c}{\\partial x} & \\frac{\\partial p_c}{\\partial y} &  \\frac{\\partial p_c}{\\partial z}\\\\\n0 & 0 & 1\n\\end{bmatrix}$\n$H = [0:3,0:3]=\\begin{bmatrix}\n\\frac{\\partial r}{\\partial p_c} & \\frac{\\partial r}{\\partial p_c} &  \\frac{\\partial r}{\\partial p_c}\\\\\n\\frac{\\partial \\alpha}{\\partial p_c} & \\frac{\\partial \\alpha}{\\partial p_c} &  \\frac{\\partial \\alpha}{\\partial p_c}\\\\\n\\frac{\\partial \\epsilon}{\\partial p_c}\\\\\n\\frac{\\partial p_c}{\\partial x} & \\frac{\\partial p_c}{\\partial y} &  \\frac{\\partial p_c}{\\partial z}\\\\\n\\frac{\\partial h}{\\partial x} & \\frac{\\partial h}{\\partial y} &  \\frac{\\partial h}{\\partial z}\n\\end{bmatrix}$      (14)\n$H [0:3,0:3]=\\begin{bmatrix}\n\\frac{\\partial r}{\\partial p_c} & \\frac{\\partial r}{\\partial p_c} &  \\frac{\\partial r}{\\partial p_c}\\\\\n\\frac{\\partial \\alpha}{\\partial p_c} & \\frac{\\partial \\alpha}{\\partial p_c} &  \\frac{\\partial \\alpha}{\\partial p_c}\\\\\n\\frac{\\partial \\epsilon}{\\partial p_c}\\\\\n\\frac{\\partial h}{\\partial x} & \\frac{\\partial h}{\\partial y} &  \\frac{\\partial h}{\\partial z}\n\\end{bmatrix}$  (15)\n$\\{\n\\frac{Xc}{r} &\\frac{Yc}{r} & \\frac{Zc}{r}\\\\\n\\frac{-Yc}{x^2+ y^2} & \\frac{Xc}{x^2+ y^2} & 0\\\\\n\\frac{-Zc}{x^2+ y^2} & \\frac{Xc}{x^2+ y^2} & 0\\\\\n\\\\\n\\}:=\\begin{bmatrix}\nR & R & R\n\\end{bmatrix}$         (16)\nThen we can concatenate with the last row as\n$\\begin{bmatrix}\n\\frac{\\partial z}{\\partial x}\\\\\nRORBR RORBR & RORBR\\\\\n\\\\\n\\frac{\\partial z}{\\partial z} & 0 & 1\\\\\n\n\\end{bmatrix}$\n        (17)\nA typical EKF model with linear F and nolinear h(x) is:\n$\\{\n\\hat{x} = F\\hat{x}\\\\\nP= FPFT + Q\n\\\\\ny=z - h(\\hat{x})\\\\\nK = PH^T (HPH^T + R)^{-1}\\\\\n\\hat{x} = \\hat{x}+ Ky\\\\\nP=(I - KH)P\n\\}Prediction, \nMeasurement update.\n     (18)\nEKF is a recursive algorithm working in two steps e.g. prediction and measurement. At the measurement state, using $Xk$ and $P_k$ obtained from prediction state, the Kalman Gain K is calculated representing the trustable value of state model and measurement variable. We set measurement noise R and process noise Q as:\nThe EKF works recursively in two stages: prediction and measurement. During the measurement phase, the Kalman Gain K is computed using $Xk$ and $P_k$ obtained from the prediction stage, which signifies the reliability of both the state model and the measurement variable. We define the measurement noise R and process noise Q as follows:\n$R = [1,0.5, 0.5, 5]^T$\n$Q =\\begin{bmatrix}100\\\\010\\\\001 \\end{bmatrix} \\frac{\\sigma a^4}{3}$        (19)\nwhere $\\sigma a = 1$, and set initial value of P as identity matrix. If R tends towards zero, it indicates higher reliability in the measurement variable compared to the state model. Conversely, if $P_k$ tends towards zero, the opposite holds true. Subsequently, the"}, {"title": "IV. RESULTS AND DISCUSSION", "content": null}, {"title": "A. Experimental setup", "content": "In our experiment, we employ a USV equipped with different sensors and navigation systems. The USV, developed by Spin Italia S.r.l, is specifically designed for the \"Mohamed Bin Zayed International Robotics Challenge 2024\". It has dual thrusters, a DVL, LiDAR, and an onboard camera. Additionally, our experimental setup incorporates a customized UAV equipped with a Pod camera boasting a field of view ranging from 2.3 to 63.7 degrees, with a distance range of approximately 4 $km^2$. The communication interface is facilitated through Nvidia NX on the USV, which is equipped with 16 GB memory and Ubuntu (20.04) with ROS (Neotic) installed. This setup enables seamless data transmission between the USV, UAV, and our control station, enabling real-time monitoring and control. With its adaptability and reliability, the USV stands as an invaluable asset in our pursuit of advancing marine research and exploration. Fig. 7 shows the USV system's components and Fig. ?? shows the USV in operating mode in real marine environment."}, {"title": "B. Results", "content": "In this section, we present the experimental results of the proposed vision-based positioning technique. These experiments were carried out at two distinct sites: 1) Shisanling Lake, Changping, Beijing, and 2) Yas Island, Abu Dhabi, UAE (see Fig. 8. The primary aim of these experiments was to investigate the effectiveness of the proposed method in estimating the position of the USV when compared to a GPS. For experiments, we discuss how it was conducted and the obtained results. Lastly, we discuss some remarks on the system's performance.\nIn experiment 1, we evaluate the efficacy of the proposed method within a lake environment. Initially, we execute a series of experiments employing an open-loop control approach, wherein the USV is manually navigated along predefined x and y directions for a certain period. The position of the USV was recorded via the proposed scheme and compared with the onboard GPS. The proposed scheme produces relatively similar results to those of GPS. In the subsequent scenario, the USV is directed to navigate toward predetermined x and y locations employing a closed-loop control algorithm, where the vehicle is operating at a certain surge speed for some time. We plot the USV position and yaw angle for all scenarios and compare them with the GPS measurements.\nExperiment 2 extends the scope to assess how the proposed approach performs under extreme weather conditions in a real marine environment. These tests were conducted at sea with ocean disturbances, wind, and light reflections. The test area spanned approximately 4 $km^2$ at Yas Island, Abu Dhabi, UAE. We explore how the UAV detects the USV when located too far from the coastal area where no GPS signals are available. Subsequently, we show how our proposed method eliminates drift and provides a smooth position measurement of the USV during the tracking process.\nThe results from scenario 1 in experiment 1 are shown in Figs. 9 and 10. Fig. 9 shows the trajectory of the USV as it navigates in the lake. The x-axis represents time, while the y-axis indicates the position of the USV in meters. The plot illustrates the USV's movement over a specified duration, showcasing its traveled distance in x and y direction. The results demonstrated that the vehicle position extracted through the proposed vision-based method is similar to the USV's onboard GPS. Similarly, Fig. 10 shows the x and y position of the USV compared with the GPS measurement. The orange line shows the GPS data, and the green line shows the proposed EKF measurements. The results show that the proposed scheme has demonstrated better performance for the USV localization in the lake setup.\nThe results from scenario 2 in experiment 1 are shown in Figs. 11-15. The USV estimated position in the ENU frame is plotted in Fig. 11. Here, we plotted measurements of three sources: EKF, Mean Filter, and GPS. The GPS measurements of X, Y, Z, and R are considered ground truth for comparison purposes. The grey vertical lines means how many times we got valid measurements. In addition to EKF, we also used the Mean Filter technique for position estimation. The results demonstrated that the proposed EKF method successfully tracked and followed the ground truth. However, the results from the Mean filter showed deviation in the USV positions compared with GPS measurement. The results indicate that the proposed method can supplement the USV localization in a GNSS-denied environment.\nFigs. 12 and 13 show the estimated relative USV ENU position to UAV with a) proposed method, b) mean filter estimated, and c) real GPS data as ground truth. The red point at the origin shows the UAV position and is considered a starting point. The trajectory of GPS data indicates that the USV first went towards the northeast and then turned towards the southeast. The output trajectory of the proposed method converged towards the actual trajectory, while the mean filter didn't eliminate estimation error. From the results, it is noticed that the trajectory obtained by the proposed method closely aligns with the actual trajectory, highlighting the method's capability to estimate the relative position of the USV accurately relative to the UAV. The results demonstrated that the proposed method could provide reliable guidance and tracking information for the USV, facilitating tasks such as surveillance, monitoring, or coordination in marine operations.\nFig. 14 shows the estimated velocity of the USV throughout the experiment. The x-axis depicts the time in seconds, while the y-axis represents the vehicle's velocity in meters per second (m/s). The plot provides insights into the dynamic behavior of the USV as it maneuvers in the lake during the experiment. Although there is some fluctuation in the surge and sway speed of the vehicle, the proposed method can still eliminate the error and produce accurate localization results comparable with the ground truth measurements.\nFig. 15 shows the absolute trajectory error obtained by the proposed method and the mean filter method. The results reveal a notable difference in error between the two approaches, with the proposed method exhibiting significantly lower error than the mean filter method. Specifically, the proposed method demonstrates an average localization error of 2.76 meters and a maximum localization error of 4.76 meters. In contrast, the mean filter method fails to mitigate drift, resulting in a persistent deviation from the ground truth trajectory. A comparison with the ground truth highlights the enhanced localization accuracy achieved by the proposed method. These findings demonstrate the efficacy of the proposed approach in meeting localization requirements and providing robust and precise USV positioning in GNSS-denied marine environments.\nIn Experiment 2, we implemented the proposed method in a real marine environment at Yas Island, Abu Dhabi, UAE, where GPS signals were restricted. The results of Experiment 2 are depicted in Figs. 16-19. These results are extracted from two different runs conducted as part of the MBZIRC-2024 Challenge, which focused on developing and evaluating solutions requiring coordinated efforts between UAVs and USVs to execute complex navigation and manipulation tasks in GNSS-denied marine environments. The testing area spanned 4x4 km\u00b2. The challenge entailed a series of tasks: first, locating a target vessel within the marine environment, followed by guiding a USV to approach and dock at this vessel. Subsequently, UAVs stationed on the USV flew out to inspect small suspicious boxes aboard the target vessel. Collaboratively, the UAVs retrieved these boxes and transported them back to the USV. Finally, the last task involved using the manipulator attached to one side of the USV to grasp and transfer a large box from the target vessel onto the USV.\nThe proposed vision-based and UAV-assisted localization method was utilized during the operation. From a constant height of around 7.5m, the UAV was allowed to search and scan the marine area for the target vessel. Once the vessel's position was identified, the UAV switched to the USV's localization mode. Then, as long as the USV was approaching the target location, the UAV continuously and synchronously performed the localization tasks of the USV. Fig. 16 shows the position $x, y, z$, and angle yaw in ENU frame during the run 1 from experiment 2. The origin of the USV was considered the current docking position. From the results, it can be noted that the proposed method produces smooth position curves along $x, y$ directions. It can be more clearly observed in Fig. 17 where $x,y$ position are plotted. Similarly, Figs. 18 and 19 show the results obtained during run 2 from experiment 2. In this case, the USV traveled a very long distance, up to 1km\u00b2, in a forward direction toward the target vessel. The proposed method produces smooth and robust position information of the USV even in the presence of ocean waves, wind, and light reflection. During the operation, the vehicle accelerates significantly to reach the target position. As a result, the position curves show some rapid drift for a short period, as shown in Fig. 18 after"}, {"title": "C. Discussion", "content": "In this work, we presented a UAV-assisted vision-based localization method for the USV in the marine environment in a GNSS-denied environment. We conducted experiments to test and validate the method's performance in different test locations under various conditions. The results obtained from experiment 1 and experiment 2 verify the method's applicability for USV localization in the real-time marine environment.\nIn Experiment 1, we showed the performance of our localization method by comparing it with ground truth GPS measurements, illustrated in Figs. 9-15. Multiple test runs revealed that our method accurately tracked the GPS-recorded positions. To further validate our approach, we compared it with a mean filter and observed that it failed to accurately follow the ground truth measurements during operation. Additionally, we demonstrated that the positioning measurements obtained from our method exhibit smoothness and robustness under the velocity profile used for USV maneuvering. Our process also exhibited lower position errors compared to the mean filter approach.\nIn Experiment 2, we further extend the scope and showcase the performance of our localization method for inspection and intervention tasks using a heterogeneous collaborative system in a GNSS-denied environment. The results obtained (refer to Figs. 16-19) demonstrated the ability of our method to accurately estimate the USV position in the marine environment using visual information. Particularly noteworthy was the observation that our method enabled drift-free USV localization tasks\u2014using a UAV equipped with a camera significantly extended operational time and range. The UAV's camera could scan marine areas up to 4km\u00b2, facilitating localization of the US"}]}