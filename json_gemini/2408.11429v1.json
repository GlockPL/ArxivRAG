{"title": "Long-Range Vision-Based UAV-assisted Localization for Unmanned Surface Vehicles", "authors": ["Waseem Akram", "Siyuan Yang", "Hailiang Kuang", "Xiaoyu He", "Muhayy Ud Din", "Yihao Dong", "Defu Lin", "Lakmal Seneviratne", "Shaoming He", "Irfan Hussain"], "abstract": "The global positioning system (GPS) has become an indispensable navigation method for field operations with unmanned surface vehicles (USVs) in marine environments. However, GPS may not always be available outdoors because it is vulnerable to natural interference and malicious jamming attacks. Thus, an alternative navigation system is required when the use of GPS is restricted or prohibited. To this end, we present a novel method that utilizes an Unmanned Aerial Vehicle (UAV) to assist in localizing USVs in GNSS-restricted marine environments. In our approach, the UAV flies along the shoreline at a consistent altitude, continuously tracking and detecting the USV using a deep learning-based approach on camera images. Subsequently, triangulation techniques are applied to estimate the USV's position relative to the UAV, utilizing geometric information and datalink range from the UAV. We propose adjusting the UAV's camera angle based on the pixel error between the USV and the image center throughout the localization process to enhance accuracy. Additionally, visual measurements are integrated into an Extended Kalman Filter (EKF) for robust state estimation. To validate our proposed method, we utilize a USV equipped with onboard sensors and a UAV equipped with a camera. A heterogeneous robotic interface is established to facilitate communication between the USV and UAV. We demonstrate the efficacy of our approach through a series of experiments conducted during the \"Muhammad Bin Zayed International Robotic Challenge (MBZIRC-2024)\" in real marine environments, incorporating noisy measurements and ocean disturbances. The successful outcomes indicate the potential of our method to complement GPS for USV navigation.", "sections": [{"title": "I. INTRODUCTION", "content": "The maritime sector is increasingly focusing on Unmanned Surface Vehicles (USVs), which possess the ability to autonomously carry out navigation missions [1]. These vehicles have gained significant attention due to their diverse and impactful applications, including the exploration of marine resources [2], oceanographic mapping [3], [4], and the inspection and monitoring of coastal and offshore structures [5], [6], ports [7], and more [8], [9]. In marine dynamic environments, accurate localization of USVs (e.g., to estimate one's position and orientation with respect to surrounding environments) is crucial [10]. The USVs require location information to make decisions related to control, navigation, collision avoidance, and path planning [11]. Therefore, localization is considered a fundamental capability for USVs engaged in tracking, exploration, or monitoring of the marine environment [12], [13].\nDue to technological advancements, an integrated navigation system, combining an Inertial Navigation System (INS) with the Global Positioning System (GPS), has been employed for navigation and localization for USVs in the marine environment [14]. However, the instability of GPS signals in the dynamic marine environment, due to many reasons such as signal blockages, multipath reflection, and jamming, often leads to weak or missing signals, resulting in reduced position accuracy [15]. To enhance the navigation system, integrating additional sensors such as Droplet Velocity Log (DVL) and Radar for position measurements instead of relying solely on GPS becomes effective. Nonetheless, this approach inevitably increases the overall navigation cost [16].\nThe integration of DVL and radar systems for USV navigation offers enhanced capabilities in challenging marine environments [17]. DVL provides real-time velocity measurements by using acoustic signals aiding in precise navigation, especially in GNSS-denied environments. However, drift problems in DVL systems can arise due to cumulative errors in velocity measurements over time, leading to inaccurate position estimates [18]. Factors such as sensor misalignment, calibration errors, and variations in water properties can contribute to drift, impacting the overall navigation accuracy. On the other hand, radar systems offer the capability of obstacle detection and navigation in low visibility conditions [19]. They can identify other vessels, landmasses, or structures, enabling the USV to navigate safely. However, radars also have poor performance in close proximity and in adverse weather conditions [20], [21].\nRecently, there has been an increased interest in developing and using vision-based localization systems because they are more robust, reliable, and cheaper than other sensor-based localization systems, e.g., acoustic or laser-based systems [8], [16]. In literature, many studies focus on feature-based localization methods such as visual odometry or simultaneous localization and mapping (SLAM) systems because they are flexible and require no additional infrastructure in the environment [22]. However, localization algorithms based on visual information may fail in some challenging environments, such as low-resolution features, low visibility conditions, or moving objects [23]. In addition, sensor fusion has been used in marine environments [24]. The sensor fusion approach combines measurements or observations from multiple sensors such as IMU, camera, LiDAR, etc. [25]. Fusing these diverse sensor inputs eliminates individual sensor limitations and enhances the USV's ability to navigate autonomously in dynamic and challenging marine conditions [26], [27].\nOne significant drawback of using the traditional methods for USV localization in long-range navigation and target tracking is the limited field of view and range. Onboard sensors, such as cameras, LiDAR, or radar, are constrained by the curvature of the Earth, environmental obstacles, and their inherent range limitations. This can result in blind spots, reduced accuracy, and delayed response times when tracking distant targets or navigating in a long range. The USV can drift with the ocean waves, which means it might still miss the target even if it's following the correct heading angle. As we know, UAVs offer an aerial viewpoint, providing a wide field of vision that is impossible to achieve from the surface level. This aerial advantage motivates the integration of UAVs to enhance USV localization, particularly in long-range marine environments [28].\nIn this paper, we employ a heterogeneous robotic setup comprising both a USV and a UAV to localize the USV with assistance from a UAV using vision-based techniques. The UAV's camera identifies the USV from a fixed altitude along the shoreline, observing the USV within a broader marine setting. Subsequently, triangulation and geometric information are employed to determine the USV\u2019s position relative to the UAV using visual observations and the range of datalinks on UAV and USV as the distance between them. Moreover, we propose a method for controlling the UAV's camera orientation to center the USV within its field of view, thereby obtaining precise pose information. We integrate USV positions with EKF to enhance localization accuracy, resulting in a more robust solution based on visual measurements. A conceptual framework of the work is shown in Fig. 1. This work was developed as a part of the \"Muhammad Bin Zayed International Robotics Challenge (MBZIRC-2024)\", held in Abu Dhabi, United Arab Emirates [29]. The main goal of this challenge was to develop a heterogeneous robotic solution consisting of both UAVs and USVs for tasks involving maritime monitoring and intervention in a GNSS-denied environment. The proposed method for localizing USVs was implemented and validated during the competition. Additionally, we illustrated how camera measurements can complement localization solutions in a long-range marine environment. Consequently,"}, {"title": "our work contributes to advancing and validating GNSS-independent localization systems for USVs in marine operations.\nThe key contributions of this work are as follows:", "content": "\u2022 To overcome the limitations of sometimes not having a GPS signal, we propose a heterogeneous framework consisting of a USV and a UAV for position estimation of the USV.\n\u2022 Integration of multiple sensors, including cameras and datalink mounted on UAV for precise positioning and orientation determination of USV employing vision-based localization in order to reduce the limitation of onboard sensors-based localization.\n\u2022 Demonstration of the effectiveness of the innovative approach, combining UAV and USV technologies, in accurately localizing USV even in GNSS-denied marine environments.\n\u2022 Conducting a series of experiments to validate the proposed method for USV localization in real marine experiments."}, {"title": "II. RELATED WORK", "content": "Currently, a range of methodologies and technologies employed in USV localization, emphasizing sensor fusion [30], [31], vision-based techniques [32], radar integration [33], and deep learning [34], [35] applications to enhance accuracy, robustness, and reliability in challenging maritime environments. Each approach contributes unique insights and advancements towards achieving effective and efficient USV localization capabilities. However, these studies exhibit notable limitations in the context of USV localization. Firstly, they are constrained by range, operating within limited geographical areas. Many of these methods rely on vision-based techniques using USV onboard camera imagery, which becomes impractical when the USV must navigate the broader region. Additionally, achieving feature-based localization becomes challenging due to poor visibility, low light conditions, and inadequate texture information in imaging data. Hence, we propose a UAV-assisted vision-based localization for USVs to enhance navigation capabilities in GPS-restricted environments. In contrast to existing approaches, our work aims to address these challenges by integrating UAV and USV equipped with diverse sensors for localization, a combination rarely experimentally demonstrated in real-world marine settings under GNSS-denied environments. In the following, we briefly review different approaches, such as Radar-based, LiDAR-based, and Vision-based, for USV localization in GNSS-denied environments."}, {"title": "A. Radar-based Approaches", "content": "Radar-based localization techniques for USVs have emerged as practical solutions, particularly in GPS-denied or challenging maritime environments. Radar systems offer unique capabilities for detecting and mapping surrounding obstacles and features, which can be used for accurate USV positioning and navigation. Several notable studies have explored radar-based USV localization methods. For example, Han et al. [1] proposed a radar-centric approach for USV localization and navigation, adopting the SLAM paradigm. Their method involves extracting coastline contours from radar-acquired images using image processing techniques. These extracted contours serve as landmark features for localization, and an EKF-based algorithm is employed for estimating vehicle positions based on these features. The experimental results highlight this approach's computational efficiency and effectiveness compared to traditional point-cloud methods. Ma et al. [20] introduced a technique that leverages the fusion of radar and satellite imagery for USV localization. Their approach uses computer vision methodologies to extract coastline features from radar and satellite images. By developing an image registration technique that accounts for horizontal and vertical perspectives captured in the input images, they demonstrated the viability of this approach with an average error of 9.77 meters in USV positioning. Dagdilelis et al. [36] presented a novel radar-based method for localizing USVs using sea chart information. They proposed utilizing radar detection to identify underwater buoys and matching these buoys with entries from electronic navigation charts. Subsequently, triangulation and trilateration methods are applied for precise pose estimation. The simulation results in the Great Belt region in Denmark showed promising reductions in uncertainty regarding pose and heading estimation. However, further research in this area may focus on improving radar sensor technologies, optimizing algorithms for real-time processing, and integrating radar systems with other sensor modalities to enhance further USV localization performance and reliability."}, {"title": "B. LiDAR-based Approaches", "content": "LiDAR-based localization methods for USVs offer promising solutions for navigating in GPS-denied environments by utilizing laser scanning technology to generate high-resolution 3D maps of the surroundings. Several studies have explored LiDAR-based USV localization techniques, demonstrating their effectiveness and applicability in challenging maritime scenarios. Shen et al. [37] introduced a novel approach for USV localization by integrating LiDAR SLAM with GNSS/INS systems. Their method employs a dynamic switching strategy to transition to LiDAR SLAM positioning when GPS signals are unavailable or unreliable. Position and heading estimates are refined using the EKF algorithm. Experimental results showed a significant reduction (55.4%) in position error compared to traditional Kalman filter algorithms, highlighting the potential of LiDAR-based localization for USVs.\nThe study in [38] discussed an autonomous SLAM navigation, path planning, and collision avoidance system for the USV, equipped with a Velodyne 3D VLP16 lidar sensor and Axis PTZ camera. Using the Robot Operating System (ROS) navigation stack, the USV demonstrates successful autonomous navigation, path planning, and obstacle avoidance in marine environments, generating detailed maps for pipeline inspection. Similarly, the studies in [39], [39], [40] also proposed using LiDAR technology for the USV localization in marine environments. Although, LiDAR-based USV localization methods offer advantages such as high accuracy, independence from external signals (like GPS), and suitability for mapping complex environments with obstacles. However, many challenges exist, including the cost and complexity of LiDAR systems and the need for robust algorithms to handle real-time processing of dense point cloud data in dynamic maritime environments."}, {"title": "C. Vision-based Approaches", "content": "Over the past few years, significant efforts have been made regarding USV localization and navigation in marine complex environments. One approach that has gained popularity in this context is the implementation of vision-based localization. For instance, Liu et al. [16] proposed a visual-inertial odometry (VIO) technique for USV localization in GPS-restricted environments. They utilized cameras to capture point and line features along bridge walls, integrating this visual data with inertial measurements for real-time position estimation. While effective, the method's computational complexity is a noted challenge. Roedele et al. [14] introduced a monocular camera imaging method within USV operating zones. By matching camera features with synthetic images from a digital elevation model (DEM), they derived 3-dimensional position estimates. However, practical deployment feasibility remains a concern. Volden et al. [8] explored a stereo-vision approach for USV localization during docking maneuvers. They utilized stereo cameras to detect and triangulate ArUco tags at docking stations, integrating deep learning and feature-matching techniques. While practical, further testing across diverse weather conditions is required for robustness validation. Hu et al. [41] leveraged lidar semantic and geometric data alongside deep learning models for USV localization during docking and departing scenarios. Their approach identified and mitigated the influence of dynamic objects on localization accuracy, achieving superior performance compared to traditional GPS systems."}, {"title": "III. PROPOSED APPROACH", "content": "The proposed framework for USV localization consists of two main components: the UAV and the USV. The UAV is equipped with a high-definition movable camera, which it uses to scan the open sea environment and identify target locations, such as vessels. Once a target is identified, the UAV records its position information, establishing it as the reference point. The UAV then maintains a fixed altitude and continuously scans the USV, accurately estimating its relative position. This is achieved using a state-of-the-art deep learning algorithm that enables the UAV to track and locate the USV. An Extended Kalman Filter (EKF) is employed to determine the USV's position relative to the UAV's location and altitude based on detection and geometric measurements integrated with the radio range (datalink range) measurements. With the target position provided by the UAV, the algorithm generates the USV's state matrix in the NED (North-East-Down) frame using the EKF. This data is subsequently used by conventional closed-loop control to guide the USV toward the target. An"}, {"title": "B. USV detection", "content": "This section discusses the essential steps in obtaining the USV detection model using data-driven methods. Fig. 3 gives an overview of the process. This illustrates the essential steps involved in refining an object recognition model. Initially, images are prepared and annotated to establish ground truth for the supervised Convolutional Neural Network (CNN) learning process. The annotated data is then introduced into the data-driven detection model, and pre-trained model weights are used to refine the model. A validation set is employed for model selection, determining the appropriate stopping point for training. Subsequently, the refined model tests unseen data to assess its accuracy, often measured by mean average precision (mAP) metrics. The final model weights are deployed for object recognition tasks during the prediction phase if the outcomes are satisfactory. Next, we discuss the preparation of the USV dataset, model training, and model testing.\n1) Step 1- Data preparation: The dataset plays a pivotal role in training CNN models in supervised learning. This data serves as the ground truth for the specific class the model aims to learn. During this phase, a custom dataset is collected, comprising examples of the class and their associated features, which the model is expected to learn. In our study, we conducted data collection to gather imaging data of USV from various perspectives and angles within a marine environment. A comprehensive custom dataset containing images of USVs captured in real marine settings was curated. This dataset encompasses color image data captured by a UAV while operating over the sea surface, as depicted in Fig. 4. Subsequently, the dataset was randomized and divided, allocating 80% for training purposes and 10% each for validation and testing, respectively.\nGround truth labels guide the supervised model towards the correct answer. We use the annotation program Yolo Mark to create ground truth labels. That is, rectangle-shaped bounding boxes are dragged around the USV in the scene. Consequently, the features that help to fine-tune the model are those that recognize the USV. Notice that precise labeling is essential for the learning process. Unexpected learning often a result of inaccurate labels, e.g., only label parts of the object can be dangerous as the model interprets this as the complete object.\n2) Step 2- Data training: For USV detection, we employ a transfer learning approach, where a pre-trained model serves as the initial point for fine-tuning toward the final detection task. The pre-trained model and its parameters are trained on the ImageNet dataset [42], which comprises 15 million annotated images. We utilize pre-trained weights along with our custom dataset for model training. The original YOLOv5s and YOLOv5s6 models are employed for this custom training. YOLOv5, developed by Ultralytics [43], is a deep learning model designed explicitly for object detection tasks. Its architecture primarily consists of three components: a Backbone, which forms the main network body, utilizing the CSP-Darknet53 structure in YOLOv5's design; a Neck part, connecting the backbone and the model's head, incorporating SPPF and New CSP-PAN structures in YOLOv5; and a Head part, responsible for generating the final output, utilizing the YOLOv3 Head in YOLOv5. YOLOv5 represents a significant advancement in real-time object detection models, surpassing previous iterations of the YOLO family in performance and efficiency by integrating various new features, enhancements, and training strategies. It achieved an mAP value of 72.7% when evaluated on the COCO dataset with a 0.5 Intersection over the Union (IoU) threshold.\nThe hardware environment for training is characterized by Intel Core i7-12700KF CPU 3.6GHz with a single GPU of NVIDIA RTX 3090 24G memory, while Python3.8.10 and Pytorch1.8.0 configure the software environment. We trained two models for USV detection, named yolov5s6 and yolov5s. We used pre-trained models on the COCO dataset and fine-tuned them on a self-made USV dataset. For the yolov5s6 model, the batch size is 32, the image size is 1280, and the learning rate is 0.01. For the yolov5s model, the batch size is 64, the image size is 640, and the learning rate is 0.01. All models are trained with 32 epochs.\n3) Step 3- Data testing and prediction: Following model training, the trained model undergoes testing on previously unseen data, referred to as the test data, to evaluate its performance. We utilize mean Average Precision (mAP) as a metric to check the performance of the trained model. Typically, a higher mAP signifies superior model performance, indicating its suitability for the tasks it was trained on. In our scenario, as the model was specifically trained for USV detection, we expect it to produce a higher mAP value during the testing phase. Upon testing the trained model on a test set extracted from the custom dataset, it achieved a mAP score of 99.05%. Given these satisfactory results, the final trained model is deployed for USV detection in real-world experiments. During the prediction phase, we use a UAV's camera to scan the environment continuously, stream real-time data, and employ the trained model to identify USVs. A few example of the USV detection is given in Fig. 5."}, {"title": "C. USV localization", "content": "In this section, we present the approach used for localizing USVs. This localization process is executed by a heterogeneous robotic system consisting of both UAV and USV in a marine environment where GNSS signals are restricted. The UAV acquires visual data using its camera from a fixed altitude near the coastal area, scanning the surroundings. Initially, the UAV employs a search method to locate the USV within its field of view as shown in Fig. ??. This entails controlling the camera's angle and position to scan the area effectively. Once the USV is detected within the camera's field of view, the detector identifies and extracts the bounding box of the USV object within the image frame. The results of this bounding box and camera information are used for USV pose estimation to allow smooth USV navigation along the predefined target positions.\nVision-based pose estimation relies on the orientation and position of the camera. The UAV's camera needs to maintain the detected USV centrally within the image frame. This task is accomplished through the process involves calculating the pixel error and sending control input to the UAV's camera, allowing it to make adjustments to minimize the pixel error between the image center and the bounding box showing the detected USV.\n1) Coordinate transformations: In the context of USV localization and navigation, it's common to represent the USV position in the NED (North-East-Down) inertial frame. In the NED frame, the x-axis points north, the y-axis points east, and the z-axis points downward towards the Earth's center. This is a local coordinate system and is often preferred in marine navigation [44]. As such, we process the transformation of the camera measurement to the USV inertial frame.\nThis coordinate transformation aims to estimate the USV's current position and heading angle, denoted as $p = \\{x,y,z,yaw\\}$, within the inertial frame as the USV navigates toward the predefined target. Estimating the USV pose involves integrating data collected from various reference frames. The following different frames are used in the coordinate transformations.\nAs depicted in Table I, the \u201cInertial\u201d denotes the East-North-Up (ENU) frame, with the UAV takeoff position on the ground serving as the origin. In the ENU frame, East is represented by the $x$ axis, North by the $y$ axis, and Up by the $z$ axis. The \"Body\" means the Front-Left-Up (FLU) frame, originating at the UAV camera. In the FLU frame, the front is denoted by the $x$ axis, the left side by the $y$ axis, and the upward direction by the $z$ axis. The \u201cPod/Camera\u201d illustrates the Front-Left-Up (FLU) frame, centered at the origin of the UAV's camera. In this FLU frame, the front of the camera corresponds to the $x$ axis, the left side corresponds to the $y$ axis, and the upward direction corresponds to the $z$ axis.\nLet $\\{r_I, p_I, q_I\\}$ represent the Euler angles denoting roll, pitch, and yaw, respectively, in the I frame of the UAV. Moreover, the camera's Euler angles in the B frame, denoted by $\\{r_B, p_B, y_B\\}$, indicate the camera's roll, pitch, and yaw angles. The camera's field of view, both horizontally and vertically, is expressed as $\\theta_p$ and $\\varphi_p$, respectively. Additionally, we use pixel error between the detected position of the USV in the image, considering both horizontal and vertical directions relative to the center of the image. For this purpose, normalized pixel errors for horizontal and vertical directions are represented as $(u, v) \\in [-1,1]$. Moreover, the datalink range is denoted by $r$, indicating the range between the UAV and the USV. Lastly, the UAV's position is denoted by $P_{UAV} = [x_{uav}, y_{uav}, z_{uav}]^T = [x_{uav}, y_{uav}, h]^T$, where the $x_{uav}$ and $y_{uav}$ positions are assumed to be 0, and h"}, {"title": "D. USV positioning via EKF", "content": "We base our measurements solely on geometric principles when we obtain the state variables through basic geometric relationships. In this approach, each frame of measurement provides outputs that are independent of each other, lacking any intrinsic relationship. Consequently, this independence between measurement frames leads to significant output variations. The lack of correlation between measurements from different frames can introduce uncertainty and inconsistency in the estimation process [45].\nWe utilize the EKF to address this issue. The EKF is a powerful tool in estimation theory that enables the estimation of state variables in systems with nonlinear dynamics [46]. Unlike traditional linear estimation techniques, the EKF can effectively handle nonlinear relationships between variables, making it particularly suitable for scenarios where geometric relationships alone may not suffice to capture the complexity of the system dynamics [47].\nBy employing the EKF, we aim to integrate the information from multiple measurement frames and exploit the correlations between them to improve the accuracy and reliability of our state variable estimates. The EKF achieves this by iteratively updating the state estimates based on the latest measurements while considering the system's nonlinear dynamics [48]. Through this iterative process, the EKF refines the forecast, reducing the impact of measurement variations and enhancing the overall robustness of the estimation process [49].\nThe most critical part of using EKF is model creation, based upon estimation theory principles, to derive a nonlinear transition function that accommodates unknown variables within each estimation state [45]. Within the framework of EKF, there are two general essential models: the state model and the measurement model, expressed as follows:\nState model: $x_{k+1} = f(x_k, u_k + w_k)$\nMeasurement model: $z_k = h(x_k + v_k)$\nhere, $x$ represents the state model comprising parameters utilized for state estimation, where $x_{k+1}$ denotes the predicted subsequent state of the model. The term $u_k$ signifies the control input, while $w_k$ pertains to noise inherent in the system. In the measurement model, $z_k$ encapsulates data from various sensors, while $v_k$ denotes Gaussian white noise. The state variable is set as position of target in I frame, a 3 \u00d7 1 vector,\n$x = [x, y, z]^T$\nFor the process model, we assume it as stationary target, i.e.,\n$F = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}$\nMeasurements are ranger, camera azimuth $\\alpha$, camera elevation $\\epsilon$, and height of UAV $h$, i.e.,\n$h(x) = z = [r, \\alpha, \\epsilon, h]^T$\nMeasurement equation is\nz = \\begin{bmatrix}\nr \\\\\n\\alpha \\\\\n\\epsilon \\\\\nh\n\\end{bmatrix} = \\begin{bmatrix}\n\\sqrt{x^2 + y^2 + z^2} \\\\\narctan(y_c/x_c) \\\\\narctan(z_c/x_c) \\\\\nz + z_{uav}\n\\end{bmatrix}\nOne key aspect of EKF is the use of the Jacobian matrix, which is essentially the first-order derivative, to linearize the non-linear functions $f$ and $h$. Jacobi matrix is:\nH = \\frac{\\partial h(x)}{\\partial (x)}|_x = \\begin{bmatrix}\n\\frac{\\partial r}{\\partial x} & \\frac{\\partial r}{\\partial y} & \\frac{\\partial r}{\\partial z} \\\\\n\\frac{\\partial \\alpha}{\\partial x} & \\frac{\\partial \\alpha}{\\partial y} & \\frac{\\partial \\alpha}{\\partial z} \\\\\n\\frac{\\partial \\epsilon}{\\partial x} & \\frac{\\partial \\epsilon}{\\partial y} & \\frac{\\partial \\epsilon}{\\partial z} \\\\\n\\frac{\\partial h}{\\partial x} & \\frac{\\partial h}{\\partial y} & \\frac{\\partial h}{\\partial z}\n\\end{bmatrix}\nH = \\frac{\\partial h(x)}{\\partial (x)}|_x = \\begin{bmatrix}\n\\frac{\\partial r}{\\partial x} & \\frac{\\partial r}{\\partial y} & \\frac{\\partial r}{\\partial z} \\\\\n\\frac{\\partial \\alpha}{\\partial x} & \\frac{\\partial \\alpha}{\\partial y} & \\frac{\\partial \\alpha}{\\partial z} \\\\\n\\frac{\\partial \\epsilon}{\\partial x} & \\frac{\\partial \\epsilon}{\\partial y} & \\frac{\\partial \\epsilon}{\\partial z} \\\\\n0 & 0 & 1\n\\end{bmatrix}\nH = \\begin{bmatrix}\n\\frac{\\partial r}{\\partial p_c} & \\frac{\\partial r}{\\partial p_c} & \\frac{\\partial r}{\\partial p_c} \\\\\n\\frac{\\partial \\alpha}{\\partial p_c} & \\frac{\\partial \\alpha}{\\partial p_c} & \\frac{\\partial \\alpha}{\\partial p_c} \\\\\n\\frac{\\partial \\epsilon}{\\partial p_c} & \\frac{\\partial \\epsilon}{\\partial p_c} & \\frac{\\partial \\epsilon}{\\partial p_c} \\\\\n0 & 0 & 1\n\\end{bmatrix}\nThe first three rows of H is\nH[0:3, 0:3] = \\begin{bmatrix}\n\\frac{\\partial r}{\\partial p_c} & \\frac{\\partial r}{\\partial p_c} & \\frac{\\partial r}{\\partial p_c} \\\\\n\\frac{\\partial \\alpha}{\\partial p_c} & \\frac{\\partial \\alpha}{\\partial p_c} & \\frac{\\partial \\alpha}{\\partial p_c} \\\\\n\\frac{\\partial \\epsilon}{\\partial p_c}\n\\end{bmatrix}\nmeasurements undergo updating based on the K value. The algorithm proceeds through the following steps in the solution.\nA typical EKF model with linear F and nolinear h(x) is:\n$\\$\\begin{aligned}\n&\\hat{\\mathbf{x}}=\\mathbf{F} \\mathbf{x} \\\\\n&\\mathbf{P}=\\mathbf{F} \\mathbf{P} \\mathbf{F}^T+\\mathbf{Q} \\\\\n&\\mathbf{y}=\\mathbf{z}-h(\\hat{\\mathbf{x}}) \\\\\n&\\mathbf{K}=\\mathbf{P} \\mathbf{H}^T(\\mathbf{H} \\mathbf{P} \\mathbf{H}^T+\\mathbf{R})^{-1} \\\\\n&\\mathbf{x}=\\hat{\\mathbf{x}}+\\mathbf{K} \\mathbf{y} \\\\\n&\\mathbf{P}=(\\mathbf{I}-\\mathbf{K} \\mathbf{H}) \\mathbf{P}\n\\end{aligned}\\$\n\nEKF is a recursive algorithm working in two steps e.g. prediction and measurement. At the measurement state, using $X_k$ and $P_k$ obtained from prediction state, the Kalman Gain K is calculated representing the trustable value of state model and measurement variable. We set measurement noise $R$ and process noise $Q$ as:\nThe EKF works recursively in two stages: prediction and measurement. During the measurement phase, the Kalman Gain K is computed using $X_k$ and $P_k$ obtained from the prediction stage, which signifies the reliability of both the state model and the measurement variable. We define the measurement noise R and process noise Q as follows:\n$R = [1,0.5, 0.5, 5]^T$\n$Q = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix} \\sigma_a^4 \\frac{l^4}{3}$\nwhere $\\sigma_a = 1$, and set initial value of $P$ as identity matrix. If $R$ tends towards zero, it indicates higher reliability in the measurement variable compared to the state model. Conversely, if $P_k$ tends towards zero, the opposite holds true. Subsequently, the measurements undergo updating based on the K value. The algorithm proceeds through the following steps in the solution."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "In our experiment, we employ a USV equipped with different sensors and navigation systems. The USV, developed by Spin Italia S.r.l, is specifically designed for the \"Mohamed Bin Zayed International Robotics Challenge 2024\". It has dual thrusters, a DVL, LiDAR, and an onboard camera. Additionally, our experimental setup incorporates a customized UAV equipped with a Pod camera boasting a field of view ranging from 2.3 to 63.7 degrees, with a distance range of approximately 4 km\u00b2. The communication interface is facilitated through Nvidia NX on the USV, which is equipped with 16 GB memory and Ubuntu (20.04) with ROS (Neotic) installed. This setup enables seamless data transmission between the USV, UAV, and our control station, enabling real-time monitoring and control. With its adaptability and reliability, the USV stands as an invaluable asset in our pursuit of advancing marine research and exploration. Fig. 7 shows the USV system's components and Fig. ?? shows the USV in operating mode in real marine environment."}, {"title": "B. Results", "content": "In this section", "sites": 1}]}