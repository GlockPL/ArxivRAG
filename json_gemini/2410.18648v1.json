{"title": "GADT: Enhancing Transferable Adversarial Attacks through Gradient-guided Adversarial Data Transformation", "authors": ["Yating Ma", "Xiaogang Xu", "Liming Fang", "Zhe Liu"], "abstract": "Current Transferable Adversarial Examples (TAE) are primarily generated by adding Adversarial Noise (AN). Recent studies emphasize the importance of optimizing Data Augmentation (DA) parameters along with AN, which poses a greater threat to real-world AI applications. However, existing DA-based strategies often struggle to find optimal solutions due to the challenging DA search procedure without proper guidance. In this work, we propose a novel DA-based attack algorithm, GADT. GADT identifies suitable DA parameters through iterative antagonism and uses posterior estimates to update AN based on these parameters. We uniquely employ a differentiable DA operation library to identify adversarial DA parameters and introduce a new loss function as a metric during DA optimization. This loss term enhances adversarial effects while preserving the original image content, maintaining attack crypticity. Extensive experiments on public datasets with various networks demonstrate that GADT can be integrated with existing transferable attack methods, updating their DA parameters effectively while retaining their AN formulation strategies. Furthermore, GADT can be utilized in other black-box attack scenarios, e.g., query-based attacks, offering a new avenue to enhance attacks on real-world AI applications in both research and industrial contexts.", "sections": [{"title": "Introduction", "content": "Artificial Intelligence (AI) has made tremendous progress with Deep Neural Networks (DNNs) in various high-security tasks, such as face recognition (Meng et al. 2021; Boutros et al. 2022), disease diagnosis (Khan et al. 2021), and recent large-scale vision-language deep models (Radford et al. 2021; Li et al. 2023). However, all existing DNNs still suffer from the safety issue of Adversarial Examples (AEs), which can cause the target model to give incorrect results by adding human-imperceptible noise. Among various attack methods, Transferable Attacks(TAs) have garnered significant attention because they require minimal knowledge of the target model, fitting into the practical category of black-box attacks. In these scenarios, attackers use a white-box surrogate model to generate AEs and evaluate the attack success rate on the target model. By studying TAs, researchers and developers can train more robust and reliable models, enhancing their security and trustworthiness. Therefore, improving the transferability of AEs is an urgent topic that needs to be explored."}, {"title": "Related Work", "content": "Current research on transferable attacks can be broadly categorized into two main approaches: gradient-optimization-based methods (Han et al. 2023; Yang et al. 2023; Wan and Huang 2023; Zhu et al. 2023) and DA-based methods (Dong et al. 2019; Lin et al. 2019; Xie et al. 2019; Lin et al. 2024). We will provide a brief overview of both, with a closer focus on the latter, as it is more closely related to our work.\nGradient-optimization-based methods. These approaches were initially developed for white-box attacks to improve gradients for formulating AN. However, they have also proven effective for transferable attacks in black-box scenarios. MI-FGSM (Dong et al. 2018), based on FGSM (Goodfellow, Shlens, and Szegedy 2014), is a significant method in transferable attacks that introduces momentum during the generation of adversarial examples. This approach accelerates the gradient descent process and enhances the attack success rate. Later, the Nesterov accelerated gradient method (Lin et al. 2019) was introduced as an optimization algorithm for minimizing convex functions, incorporating momentum to accelerate convergence. Subsequently, Wang and He (2021) utilized gradient variance from previous iterations to adjust the current gradient, stabilizing the update direction and avoiding poor local optima. However, these strategies still exhibit limited transferability in black-box settings because the effective attack spaces for different target models are broad, and relying solely on AN is not sufficient to cover them.\nData-augmentation-based methods. DA-based approaches transform clean examples using various combinations of DA parameters and then input them into surrogate models to compute gradients and generate adversarial examples. DI-FGSM (Xie et al. 2019) introduces random perturbations, including color and texture variations, at each iteration to enhance the robustness of adversarial examples. TI-FGSM (Dong et al. 2019) adopts a translation-invariant approach to correct the gradient direction, achieved through predefined kernel convolutions for image translation. SI-NI-FGSM (Lin et al. 2019) utilizes Nesterov momentum to escape local optima during optimization, leveraging the scale-invariance property of DNNs to enhance transferability. Wang et al. (2021) proposed Admix, a novel input transformation that blends the original image with randomly selected images from other classes through linear interpolation, calculating gradients for the blended image while preserving the original label. However, none of these methods have addressed the optimization of data augmentation parameters.\nThus, a series of search-based frameworks have been proposed. ILA-DA (Yan, Cheung, and Yeung 2022) employs three novel augmentation techniques to improve adversarial examples by maximizing their perturbation on an intermediate layer of the surrogate model. It focuses on finding the optimal combination weight for each augmentation operation. ATTA (Wu et al. 2021), which is related to our method, constructs a DNN to simulate data augmentation but only considers color and texture variations. Similar to ILA-DA, it focuses on optimizing the combination weight. However, these methods are limited in finding optimal DA parameters"}, {"title": "Method", "content": "The formulation of adversarial attack. Let's consider a image classification model $f(x)$, where $x$ denotes the input and $y$ is the corresponding ground truth. The attacker generates an adversarial example as $x^{adv} = x + \\delta$, where $\\delta$ is the designed perturbation, and $\\delta$ is restrained by $l_p$-ball. For the adversarial sample $x^{adv}$, its output should satisfy $f(x^{adv}) \\neq y$. The traditional pipeline for generating such adversarial samples involves an standard optimization problem, which can be formulated as follows\n$ \\underset{x^{adv}}{argmax} L(x^{adv}, y), $\n$s.t. ||x^{adv} - x||_{\\infty} \\leq \\epsilon, $\nwhere $L(\\cdot, \\cdot)$ is the loss function, $\\epsilon$ is the maximum perturbation range on the $l_{\\infty}$-ball. To solve the optimization problem in Eq. 1, an iterative method is typically employed. In this approach, adversarial examples are generated based on the gradient direction of the loss function, as follows\n$x_0^{adv} = x,$\n$x_{t+1}^{adv} = x_t^{adv} + \\alpha \\times sign(\\nabla_{x^{adv}} L (f(x_t^{adv}), y)),$\nwhere $t$ denotes the t-th iteration, $\\alpha$ represents the step size, $sign()$ is the sign function.\nAugmentation can help enhance attack effects. Several methods utilize augmentation strategies to enhance attack efficacy. The fundamental theory is that suitable augmentation can expand the solution space of adversarial examples, thereby increasing the attack success rate (as shown in Fig. 1). In contrast to traditional adversarial examples, which are generated by simply adding noise, augmentation incorporates various transformations (e.g., spatial, color changes) to the original clean samples.\nIn this approach, the adversarial sample is first augmented through various operations and then finalized by formulating the adversarial perturbations using the attack pipeline, as in Eq. 2. In our work, we employ a parameterizable augmentation module for the augmentation step. Optimizing the corresponding parameters allows us to obtain aggressive DA parameters that are suitable for the attack. Suppose the augmentation can be formulated as follows\n$x^{trans} = T_{\\theta}(x),$\nwhere $T$ is the augmentation module with $\\theta$ as its corresponding parameter. While $T$ can be a neural network, it typically lacks interpretability and generalization, and its parameter $\\theta$ is often large, making it unsuitable for many applications. In this work, we use a differentiable data augmentation library that provides interpretable augmentation operations and adjustable parameters $\\theta$, which are tiny and more suitable for our needs."}, {"title": "Motivation", "content": "The shortcomings of existing DA-based attack methods. Although there have been several DA-based attack methods, they come with various disadvantages. As discussed in \"Introduction Section\", existing DA-based attacks can be classified into two categories. One approach involves using traditional transformation strategies and exploring combina-"}, {"title": "Algorithm 1: GADT", "content": "Input: A clean image $x$ and its ground-truth label $y$, transformation network $T(\\cdot)$ and its paramethers $\\theta$, loss function $L_{trans}$, number of transformation iterations $K$, surrogate model $f(.)$ Parameter: Perturbation budget $\\epsilon$, number of attack iteration $T$, classify loss function $L$, momentum $\\mu$\nOutput: $\\theta, x^{adv}$ Initialize $x^{trans} = x, \\theta_{k-1}$  for $k = 0$ to $K-1$ do  $x^{trans} = T(x^{trans}; \\theta_k)$  Update $ \\theta_k = \\theta_k - Adam (L_{trans} (x^{trans}, y))$  end for $ \\alpha = \\epsilon /T; g_0 = 0;$  $x_0^{adv} = x_K^{trans}$  for $t = 0$ to $T-1$ do   Input $x^{adv}$ to obtain the gradient $\\nabla_xL(x_t^{adv}, y)$   Update $ g_{t+1} = \\mu \\cdot g_t + \\frac{\\nabla_xL(x_t^{adv}, y)}{||L(x_t^{adv}, y)||_1}$   Update $x_{t+1}^{adv} = x_t^{adv} + \\alpha sign(g_{t+1})$  end for  return $x^{adv}$ gradient-based guidance and the new loss function.\nData Augmentation based on Kornia. Kornia is a comprehensive computer vision library comprising modules with operators designed for seamless integration into neural networks. Built on PyTorch, Kornia enables reverse-mode auto-differentiation to compute gradients of augmentation transformations. This capability optimizes data transformations during training, similar to model training itself. With Kornia, we achieve precise control over augmentation parameters, facilitating gradient computation of the loss function with respect to each transformation's magnitude effortlessly.\nThe new loss function to guide the DA parameters' optimization. To determine the optimal DA parameters, we iteratively apply data augmentation to each clean sample, adjusting transformation parameters based on gradient ascent. A critical aspect is selecting a suitable loss function to guide this process. While a straightforward approach involves using task-oriented losses like Cross-Entropy (CE) for classification tasks, we must also consider adversarial stealthiness. Excessive augmentation can not only enhance attack efficacy but also risks detection by intelligent systems. Our goal is to devise a new loss function that serves as the metric, balancing the maximization of attack efficacy with the preservation of original image content, thereby achieving both objectives simultaneously.\nSpecifically, for the attack target, we employ the CE loss, denoted as $L_{CE}$. Additionally, we utilize the Mean Squared Error (MSE) loss, $L_{MSE}$, to enforce fidelity at the pixel level between adversarial examples and clean samples. To integrate these objectives, we combine both loss functions with a balancing parameter $\\lambda$. The overall loss function is formulated as follows\n$L_{trans} = -L_{CE}(f(x^{trans}), y) + \\lambda \\cdot L_{MSE}(x, x^{trans}).$  Despite the simplicity of this loss function, we have found"}, {"title": "Experiments", "content": "Dataset. We conducted experiments on a dataset of 1,000 images extracted from an ImageNet-compatible dataset, used in the NIPS 2017 adversarial competition\u00b9. This dataset is widely utilized for evaluating transferable attack methods (Kurakin et al. 2018b).\nModels. We selected five commonly used undefended models as surrogate models: VGG16 (Simonyan and Zisserman 2014), ResNet-101 (RN101) (He et al. 2016), Inception v3 (Inc-v3) (Szegedy et al. 2016), and DenseNet-121 (DN121) (Huang et al. 2017). These models were also employed as target models for evaluation. Additionally, to comprehensively assess attack effects in the black-box setting, we included ResNet-50 (RN50) (He et al. 2016), Inception-ResNet v2 (IncRes-v2) (Szegedy et al. 2017), and CLIP (ResNet-101 & ViT-B/32 version) (Radford et al. 2021), which is a prominent vision-language model, alongside the aforementioned models.\nFor evaluating adversarial defense methods, we consider the adversarially trained Inception-v3 model (Inc-v3adv) (Kurakin et al. 2018a), as well as two methods: AT (Tram\u00e8r et al. 2017) and HGD (Liao et al. 2018). All these models are pretrained on the ImageNet's valuation set.\nBaselines. We compare our method with various state-of-the-art transferable transformation-based attack methods mentioned in related work: Momentum Iterative Fast Gradient Sign Method (MIM) (Dong et al. 2018), Diverse Input Method (DIM) (Xie et al. 2019), Translation-Invariant Method (TIM) (Dong et al. 2019), ScaleInvariant Method (SIM) (Lin et al. 2019), and Admix (Wang et al. 2021). Specifically, our strategy focuses on optimizing the DA parameters of these methods to evaluate improvements in attack effectiveness. Among them, DIM, TIM, and SIM are all transfer attack methods that rely on input transformation. In our experiments, \u201cGADT-X\" means applying our GADT strategy on the baseline of X.\nAttack details. For all attack methods, we followed the parameter settings used in the original papers. We set $ \\alpha = 1.6$,"}, {"title": "The effectiveness of our DA optimization strategy", "content": "The effectiveness of our DA optimization strategy. To validate the effectiveness of our DA optimization strategy, we conducted ablation experiments by removing our gradient-guided DA optimization procedure. Similar to our full strategy, we combined the same Kornia-based DA transformation operations with MIM but without the DA optimization process, denoted as MIM-k. The results are listed in Table 3. We observed that attacks using our original GADT strategy generally outperformed those without the corresponding DA optimization. Specifically, when attacking IncRes-v2, the attack success rate increased by 10% when comparing GADT-MIM and MIM-k. Moreover, for attacks on CLIPRN101 and"}, {"title": "The effect of our loss function $L_{trans}$ on fidelity", "content": "The effect of our loss function $L_{trans}$ on fidelity. Our method specifically enhances the fidelity of adversarial examples by designing a loss function, $L_{trans}$. To verify its effectiveness, we selected two image quality assessment metrics: PSNR and SSIM, to assess the similarity between adversarial examples generated by MIM/GADT-MIM and"}, {"title": "Conclusion", "content": "In this paper, we introduce a novel DA optimization strategy aimed at generating effective and transferable adversarial examples, termed GADT. Unlike existing approaches, we compute gradients of the loss with respect to DA parameters, leveraging the differentiable DA operations provided by Kornia. Additionally, we design a new loss function to guide the optimization of DA parameters, balancing attack effectiveness and stealthiness. Our approach is compatible with all existing transferable attack strategies, and extensive experiments validate the improvements achieved by incorporating GADT. GADT can be extended to other black-box attack strategies, offering new insights for attack algorithms."}]}