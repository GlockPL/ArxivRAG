{"title": "Heterogeneous Space Fusion and Dual-Dimension Attention: A New Paradigm for Speech Enhancement*", "authors": ["Tao Zheng", "Liejun Wang", "Yinfeng Yu"], "abstract": "Self-supervised learning has demonstrated impressive performance in speech tasks, yet there remains ample opportunity for advancement in the realm of speech enhancement research. In addressing speech tasks, confining the attention mechanism solely to the temporal dimension poses limitations in effectively focusing on critical speech features. Considering the aforementioned issues, our study introduces a novel speech enhancement framework, HFSDA, which skillfully integrates heterogeneous spatial features and incorporates a dual-dimension attention mechanism to significantly enhance speech clarity and quality in noisy environments. By leveraging self-supervised learning embeddings in tandem with Short-Time Fourier Transform (STFT) spectrogram features, our model excels at capturing both high-level semantic information and detailed spectral data, enabling a more thorough analysis and refinement of speech signals. Furthermore, we employ the innovative Omni-dimensional Dynamic Convolution (ODConv) technology within the spectrogram input branch, enabling enhanced extraction and integration of crucial information across multiple dimensions. Additionally, we refine the Conformer model by enhancing its feature extraction capabilities not only in the temporal dimension but also across the spectral domain. Extensive experiments on the VCTK-DEMAND dataset show that HFSDA is comparable to existing state-of-the-art models, confirming the validity of our approach.", "sections": [{"title": "I. INTRODUCTION", "content": "Speech communication is a fundamental mode of human interaction. However, in everyday speech communications, environmental noise, background noise, and other interfering factors frequently \"pollute\" the speech data, significantly diminishing its quality and clarity. Speech enhancement(SE) technology seeks to address this issue by minimizing the impact of noise while preserving the integrity of the original clear signal. Given its relevance across various practical scenarios, this paper primarily focuses on the study of single-channel speech enhancement.\nAs technology advances, deep learning methods have gradually become the mainstream strategy in various fields of artificial intelligence. Numerous studies [1-6], have shown that deep learning models, with their excellent feature extraction and information modeling capabilities, exhibit significant potential across various domains. In the field of speech enhancement, the application of deep learning has led to notable advancements. Depending on the input method, speech enhancement techniques can be categorized into two types: one involves directly inputting the time-domain signal into the deep learning model, which facilitates rapid implementation but may require a complex network structure to effectively process the raw signal [7, 8]; the other method first processes the signal through Short-Time Fourier Transform (STFT), converting it into a time-frequency domain representation, before inputting it into the model. Within the time-frequency domain approaches, there are mainly two methods: mapping-based methods [9, 10] and masking-based methods [11, 12]. In this paper, we focus on the study of masking-based SE methods.\nSelf-supervised learning (SSL) models, due to their outstanding performance in various speech tasks, are considered a significant future direction for speech enhancement technology. However, the application of these models in the field of SE is still in its initial stages. Huang et al. [13] have evaluated the performance of thirteen self-supervised models in SE tasks and proposed the strategy of directly applying self-supervised models to SE. Notably, when applied to SE tasks, SSL models often exhibit characteristics different from those in other speech-processing tasks. Huang and colleagues observed that detailed information might be lost in deeper layers of the network. Hung et al. [14] confirm that using cascaded features as input significantly improves model performance in SE tasks.\nAttention mechanisms are more effective in handling sequential data and are, therefore, widely applied in speech tasks. Transformer utilizes multi-headed self-attention(MHSA) to process the sequence data [15]. While this model performs well in capturing global information, its effectiveness in extracting local information is limited. On the other hand, Convolutional Neural Networks(CNN) are proficient in modeling local data. The Conformer model [16] combines the strengths of both by integrating CNN and Transformers. We observe that the one-dimensional convolutional module in the Conformer block not only extracts local information along the time dimension but also pays attention to relevant information along the frequency dimension, thereby enhancing the model's performance. We believe that the ability to focus on frequency-domain information also affects the model's performance, which may be one reason for this model's performance improvement.\nThis study introduces a novel speech enhancement network that integrates heterogeneous spatial features (HSF) and"}, {"title": "II. RELATED WORK", "content": "Self-supervised models have shown significant progress in speech tasks. The earliest methods, such as Contrastive Predictive Coding (CPC) [18], and Autoregressive Predictive Coding (APC) [19], first introduced unsupervised learning to audio pre-training. Building on these works, the wav2vec [20] series further enhanced the performance of automatic speech recognition (ASR). HuBERT [21] and WavLM [22] improved the performance and generalization ability of audio ASR. When applying pre-trained self-supervised models to downstream tasks, significant performance improvements have been observed through task-specific fine-tuning. For example, in the wav2vec 2.0 project, an SSL model, after being fine-tuned with labeled data using a CTC loss function, was used to enhance speech recognition tasks [23]. Research has shown that applying SSL models to speech-processing tasks extends beyond ASR. Specifically, studies in speech emotion recognition (SER) and SE have demonstrated improvements in model performance. For SER, Khare et al. [24] utilized a transformer-based SSL model, enhancing performance by fine-tuning an initially trained transformer. In the field of SE, Lee et al. [25] explored strategically designed ensemble mapping processes within the SSL feature space, aiming to improve speech enhancement through adaptation strategies. Similarly, Song et al. [26] designed a regression-based variant of the WavLM objective, optimizing within an unsupervised learning framework to predict continuous outputs from masked regions of the input signals."}, {"title": "B. Conformer", "content": "When processing speech data, considering the context-related information of sequence data is crucial. Unlike traditional Recurrent Neural Networks (RNNs) or CNN, the Transformer model captures global context effectively by processing all positions in the input sequence simultaneously through its self-attention mechanism. Yu et al. employed a Transformer in the domain of speech enhancement and utilized LocalLSTM instead of positional embeddings to represent the local structure of speech signals [27]. Conformer enhances model performance by combining the advantages of CNN and Transformer. Kim et al. proposed a novel time-domain speech enhancement method named SE-Conformer [28]. This method adopts the Conformer architecture and integrates it into the convolutional encoder-decoder (CED) framework to improve speech quality and clarity. Abdulatif et al. [29] explore the efficacy of two-stage Conformer blocks in capturing temporal and spectral dependencies while maintaining a relatively low computational burden."}, {"title": "III. METHOD", "content": "Fig. 1 depicts the overall architecture of the model we propose. Speech data is fed using two branches of different spatial features. Specifically, one branch first transforms speech waveform data into a spectrogram via STFT and then uses ODConv technology to extract key information from the spectrogram. The other branch feeds the speech data into a self-supervised model to extract high-level semantic information. The two types of features are merged in the time dimension through a concatenation operation, which is then fed into a DDA module. This module is capable of extracting features across both the time and frequency dimensions. Finally, the data is processed through a Feedforward layer before output. The model employs an L1 smooth loss function to compute loss and optimize performance."}, {"title": "A. ODConv Module", "content": "CNNs are frequently employed in SE tasks. Traditional CNNs use static convolution kernels, which cannot flexibly adapt to the diverse characteristics of input samples. Consequently, the exploration of dynamic convolution kernels represents a novel direction in CNN kernel research. Li et al. introduced the Omni-dimensional Dynamic Convolution (ODConv) [17] and demonstrated its efficacy in object detection tasks. Motivated by this study, we have adopted ODConv for the processing of spectrograms. The ODConv mechanism, illustrated in Fig. 2, dynamically adjusts the weights of convolution kernels in response to the input data, thereby enhancing the model's ability to capture features effectively.\nSpecifically, ODConv is applied to STFT spectrograms after they undergo a time-frequency transformation. This is achieved by a parallel processing strategy that intricately tunes the convolution kernels across four dimensions. For a set of convolution kernels \\(W_i\\) in \\(W = {W_1,...,W_n}\\), we assign attention weights to the time dimension (\\(a_{si}\\)), frequency dimension (\\(a_{ci}\\)), output channels (\\(a_{fi}\\)), and the overall convolution kernel (\\(a_{wi}\\)) for each kernel \\(W_i\\).\nThe application of these attention weights occurs in a sequential manner, following the hierarchy of time, frequency, output channel, and convolution kernel scales. This sequential multiplication of the four attention types to the respective convolution kernel \\(W_i\\) is what grants the ODConv its distinctive flexibility and adaptability to multidimensional changes. Consequently, this enhances the network's proficiency in feature extraction and information integration within complex scenarios.\nThe formal representation of this operation is given by the"}, {"title": "equation:", "content": "equation:\n\\(y = (a_{w1}a_{f1} a_{c1} a_{s1}W_1+\n+a_{wn}a_{fn} a_{cn} a_{sn} W_2) * x,\\)   (1)\nwhere \\(\\odot\\) denotes element-wise multiplication, and * represents the convolution operation applied to the input x. This equation encapsulates the core principle of OD-Conv-weighting convolution kernels by paying attention to different dimensions before applying them to the input, thereby adapting the process to the data's unique features."}, {"title": "B. DDA MODULE", "content": "The conformer architecture is a commonly used architecture for speech tasks, and we observe that the one-dimensional convolutional module in the conformer block can play a role in extracting localized information in the time dimension while also paying some attention to information related to the frequency dimension, thus improving the performance of the model, and hence it can be seen that the ability of the model to pay attention to information in the frequency dimension also affects the performance of the model. We believe that attention in the frequency dimension may be more competent for such a task. At the same time, based on the consideration of lightweight, we designed the FreqLite Attention (FA) module for feature extraction of frequency dimension speech information.\nFig.3 depicts the FA module, which is a lightweight attention mechanism designed to focus on the frequency dimension. For the input features \\(X \\in R^F\\), the FA module re-computes a relevance score \\(U_t\\) for each element \\(X_t\\), Here"}, {"title": "is the formula for the attention fraction Ut:", "content": "is the formula for the attention fraction \\(U_t\\):\n\\(U_t = Attention(X)\n= \\sigma[\\omega_1 AvgPool(X) + \\omega_2 MaxPool(X)],\\)  (2)\n\\(\\sigma\\)acts as an activation function, while \\(\\omega_1\\) and \\(\\omega_2\\) represent the learned weight matrices. The operations AvgPool and MaxPool perform average pooling and max pooling on the input X, respectively, aiding in capturing frequency information at various levels. Specifically, average pooling uniformly considers all frequency components, which helps in capturing the overall frequency distribution. Conversely, max pooling emphasizes the most prominent frequency component, thus focusing on key frequencies relevant to the task. The outputs from these two pooling strategies are integrated through a dynamic weighting system, allowing the model to adjust its focus on frequency features according to the specific requirements of the task. This adaptive structure not only enhances the model's sensitivity to frequency details but also improves computational efficiency, making the FA module a powerful tool for managing frequency complexity in various applications."}, {"title": "Subsequently, \\(U_t\\) is expanded to match the dimensions of X, and then element-wise multiplied with the original input \\(X_t\\) to yield the adjusted \\(X_t\\).", "content": "Subsequently, \\(U_t\\) is expanded to match the dimensions of X, and then element-wise multiplied with the original input \\(X_t\\) to yield the adjusted \\(X_t\\).\n\\(X_t = expand(U_t) * X,\\) (3)"}, {"title": "IV. EXPERIMENTAL SETUP", "content": "Our study utilized the commonly employed VCTK-DEMAND dataset, which comprises a mix of mixed noise and clean speech, to evaluate the denoising performance of our model. The clean speech recordings were sourced from the VoiceBank corpus. The training set consists of 11,572 audio recordings, while the test set includes 872 recordings. The noise data were obtained from the DEMAND database, with the training dataset featuring ten different types of noise, such as babble, cafeteria, and kitchen noise, with Signal-to-Noise Ratios (SNRs) of 0, 5, 10, and 15 dB. The test dataset contained five types of noise, with SNRs of 2.5, 7.5, 12.5, and 17.5 dB. There is no overlap of noise data or noise conditions between the training and test sets. Our evaluation metrics include Perceptual Evaluation of Speech Quality (PESQ), CSIG, CBAK, COVL, and the Short-Time Objective Intelligibility (STOI) measure."}, {"title": "B. Experimental setup", "content": "During the data preprocessing stage, speech from the training set was segmented into 1.5-second slices to facilitate model processing; meanwhile, speech in the test set was kept at its original length to evaluate the model's performance on data of varying lengths. For spectral feature extraction, we employed a 25-millisecond window length (corresponding to a 400-point Fast Fourier Transform (FFT) and a 10-millisecond step size using a Hamming window. Consequently, the resultant spectrograms have 200 frequency bins in the frequency dimension, while the length in the time dimension depends on the duration of each audio track."}, {"title": "C. Performance Comparison", "content": "In this study, we first compared our proposed model with the current advanced enhancement models that have applied self-supervised learning to the VCTK dataset. The results show that our model outperforms all reference baselines in terms of performance. Subsequently, we further compared our model with models that employ Transformer and Conformer architectures. The analysis indicates that our model also exhibits superior performance. Considering that most current models do not incorporate self-supervised learning or the Conformer architecture, we also compared our model with other models that use different network architectures. The comparative results also demonstrate the competitiveness and superiority of our model."}, {"title": "D. Ablation Analysis", "content": "As shown in Table II, ablation studies were meticulously executed to substantiate the functional significance of the constituent modules in our architecture. The initial substitution of the DDA module with a conformer module led to a decrease in the PESQ score by 0.08, accompanied by declines in other evaluative metrics. These findings suggest the potential superior suitability of the lightweight FrquLite Attention module over traditional convolutional modules in speech enhancement tasks. Subsequently, the FA module was integrated between the MHSA and CNN components within the conformer architecture; however, this modification did not yield a significant improvement in model performance. This indicates that the frequency information, once weighted and reshaped by the FA module, may no longer be conducive for processing by the CNN. Concurrently, differential self-supervised models were employed in our experimentation; the substitution of WavLM with Wav2Vec led to a moderate decrement in model performance, thereby attesting to the comparative advantage of WavLM in the realm of speech enhancement. To assess the impact of heterogeneous space fusion features on model prowess, inputs were confined to discrete spatial features. Sole reliance on SSL output manifested a decrease of 0.24 in PESQ, whereas exclusive dependence on STFT output entailed a PESQ reduction of 0.18, corroborating the substantial influence of heterogeneous space fusion features on model competence. Lastly, the removal of the ODConv component from a singular branch model precipitated a PESQ decrease of 0.30, underscoring the indispensability of the initial processing of the STFT spectrogram."}, {"title": "V. CONCLUSIONS", "content": "In this paper, we introduce HFSDA, a novel speech enhancement model that addresses the complexities associated with noisy communication environments, utilizing heterogeneous spatial features and a dual-dimension attention mechanism. Our model effectively combines self-supervised embeddings with spectrogram features derived from STFT, enabling the subtle integration of semantic and detailed acoustic information. This holistic approach significantly enhances the clarity and quality of speech signals. Overall, our extensive evaluations of the VCTK-DEMAND dataset firmly demonstrate the effectiveness and superiority of our proposed method across several critical metrics, including PESQ and STOI. Spatial feature fusion in the field of speech enhancement has not yet seen significant development. Our research may pave the way for novel approaches to spatial feature fusion in the future."}]}