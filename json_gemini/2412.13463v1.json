{"title": "FlexPose: Pose Distribution Adaptation with Limited Guidance", "authors": ["Zixiao Wang", "Junwu Weng", "Mengyuan Liu", "Bei Yu"], "abstract": "Numerous well-annotated human key-point datasets are publicly available to date. However, annotating human poses for newly collected images is still a costly and time-consuming progress. Pose distributions from different datasets share similar pose hinge-structure priors with different geometric transformations, such as pivot orientation, joint rotation, and bone length ratio. The difference between Pose distributions is essentially the difference between the transformation distributions. Inspired by this fact, we propose a method to calibrate a pre-trained pose generator in which the pose prior has already been learned to an adapted one following a new pose distribution. We treat the representation of human pose joint coordinates as skeleton image and transfer a pre-trained pose annotation generator with only a few annotation guidance. By fine-tuning a limited number of linear layers that closely related to the pose transformation, the adapted generator is able to produce any number of pose annotations that are similar to the target poses. We evaluate our proposed method, FlexPose, on several cross-dataset settings both qualitatively and quantitatively, which demonstrates that our approach achieves state-of-the-art performance compared to the existing generative-model-based transfer learning methods when given limited annotation guidance.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks are data-hungry and rely on large-scale datasets with high-quality human annotations for training. However, the process of annotating these datasets can be expensive and time-consuming, particularly when dense annotation are required, as is often the case in pose estimation tasks (Wang and Zhang 2022; He et al. 2017). To overcome this challenge, AI-aided labeling methods have become increasingly popular, where a pre-trained model's prediction serves as a reference to reduce human workload. However, when there is a domain shift (Luo et al. 2019), where the distribution of the training dataset and test dataset are not aligned not only on the input image domain but on the pose annotation domain as well, the accuracy of the model can significantly decline.\nConsiderable efforts have been devoted to tackling this issue. Among them, domain adaptation (DA) (Daum\u00e9 III 2009; Csurka 2017) introduces knowledge from existing annotated datasets to a target dataset and is verified effective on several computer vision tasks (Cao et al. 2019; Inoue et al. 2018). However, things become different in the human-related dataset, e.g., human pose (Ionescu et al. 2014) and human face (Wu et al. 2018). As the source human appearance is usually required in DA-related methods for input image domain adaptation, they may import unexpected data distribution bias (Buolamwini and Gebru 2018), e.g., gender or color, from the source. Besides, the direct exposure of private portraits may raise the privacy issue.\nOn the other hand, it is commonly observed that different human poses share a similar hinge-structure prior. Typically, poses in a target dataset can be transferred from poses of a pre-collected source set by applying geometric transformations on for example pivot orientation, joint rotation, and bone length ratio. Therefore, adapting the pose distribution only can be a viable option in the case of human-related datasets. Pose Domain Adaptation (PDA) avoids the direct use of human appearance images, effectively addressing the aforementioned issues. Motivated by this observation, we propose FlexPose (shown in Figure 1), a method that transfers the source pose distribution to a target distribution with limited pose annotation guidance. After pose distribution transfer by FlexPose, each input image can be matched with the most related generated pose in estimated pose distribution by utilizing a matching algorithm (Jakab et al. 2020) for weakly supervised pose estimation and pose annotation. Besides, the generated poses can also be utilized in plenty of downstream tasks such as pose-conditioned image generation (Zhang and Agrawala 2023).\nIn FlexPose, we treat pose annotations as skeleton images to well align the annotations with their RGB appearance correspondences, and to improve the learnability of pose prior as the skeleton images well preserve the spatial structure of joint connection on image plane. We first learn the pose prior and fit the empirical distribution from a source human pose dataset by a multi-layer generative model. Thereafter, specific layers of the generative model are calibrated by inserting learnable lightweight linear modules to transfer the source distribution to the target domain. Considering that only a limited number of poses are given, we introduce three regularizations to avoid the collapse of the transfer solution. By generating credible pose interpolations with Pose-mixup regularization and by strictly limiting the complexity of the transfer module with linear and sparse regularization, we minimize the requirement of sample amount but maximize the sample diversity in FlexPose. FlexPose is computation-efficient. It operates on the pose domain, and hence the training convergence is much faster than methods in domain adaptation, which focuses adaptation on both the pose and image domains together. FlexPose is also data-efficient. We only need limited pose annotations from the target dataset to fine-tune the transfer modules. Extensive experiments on three pose-based tasks, i.e., human pose annotation, human face landmarks annotation, and pose-conditional human image generation, demonstrate that FlexPose outperforms baselines by a large margin both quantitatively and qualitatively. Our contributions can be summarized as follows:\n\u2022 We propose to treat the task of Pose Domain Adaptation as the transfer of skeleton image generator and demonstrate that a target pose distribution can be well approximated from a well-learned pose prior.\n\u2022 We introduce FlexPose, a PDA framework that employs three regularizations to efficiently transfer a pose distribution to a target one by utilizing a limited number of guiding poses with low computation and storage costs.\n\u2022 Extensive experimental results on three pose-related tasks show that FlexPose achieves remarkable improvement over existing methods."}, {"title": "2 Related Works", "content": "Deep Generative Model for Image Generation. Deep generative models such as GAN, Variational AutoEncoder (VAE), and Diffusion models achieve great success in realistic/artificial image generating and natural image distribution modeling. Recently proposed generative models such as StyleGAN (Karras, Laine, and Aila 2019), DDPM (Ho, Jain, and Abbeel 2020), NVAE (Vahdat and Kautz 2020) introduce new mechanisms, new architecture, and new regularizations into image generation. VAEs (Kingma and Welling 2014) learn to maximize the variational lower bound of likelihood. Diffusion probabilistic models (Sohl-Dickstein et al. 2015) synthesize images by a denoising procedure. GANs (Goodfellow et al. 2014) are trained in an adversarial manner to learn how to generate realistic images. Among them, Karras et al. (Karras, Laine, and Aila 2019) proposed an architecture StyleGAN that can learn a hierarchical decoupled style code and controls image synthesis. Our method is based on generators with multi-layer architecture and leverages StyleGAN as the backbone. We are inspired by the recent works (Zhu et al. 2016; Yin et al. 2022), which manipulate the latent code in the generative model to edit the output images. These works motivate us to transfer the pose distribution to the target domain by transferring style codes with few-shot guidance.\nTransfer Learning for Generative Models. The literature on transfer learning has been extensively studied in recent years (Oquab et al. 2014; Long et al. 2015; Ganin and Lempitsky 2015). Transfer learning learns to transfer the knowledge from a large-scale source dataset to a small target dataset to enhance model performance on the target dataset. The methodology of transfer learning is also treated as a pre-training technique. It is utilized to accelerate the learning on the target dataset. (Wang et al. 2018) finetunes a pre-trained GAN on a target dataset to get better performance. (Noguchi and Harada 2019) transfers knowledge from a large dataset to a small dataset by re-computing batch statistics. Existing methods focus on either the image domain or the neural language processing domain (Shin, Hwang, and Sung 2016). For these methods, hundreds of training samples are still required. Compared with these approaches, we focus on pose domain adaptation, and our method only requires few-shot guidance for transferring. After LoRA (Hu et al. 2021) is widely used in Large Language Model finetuning, the researchers in Content Generation are inspired to introduce or extend this technique in generative model (Mou et al. 2024). Compared with the light-weight but global model finetuning in LoRA, FlexPose only focuses on calibrating specific layers with semantics of pose geometric transformation locally to satisfy the linear and sparse finetuning requirements.\nHuman Pose Estimation. 2D Human pose estimation is a task that predicts the 2D pose from a single image. Fully-supervised methods (Andriluka, Roth, and Schiele 2009; Bai and Wang 2019; Belagiannis and Zisserman 2017) utilize large-scale annotated datasets such as COCO (Lin et al. 2014), Human3.6M (Ionescu et al. 2014) and 3DHP (Mehta et al. 2017) for model training. Weakly-supervised (Kanazawa et al. 2018; Gecer et al. 2019; Geng, Cao, and Tulyakov 2019; Wang et al. 2023) and unsupervised (Shu et al. 2018; Jakab et al. 2018) methods such as KeypointGAN (Jakab et al. 2020) have been proposed to reduce the dependence on the expensive pose annotation. These methods require supervised post-training or additional prior knowledge to generate meaningful landmarks, which can serve as a distance measurement between the provided prior knowledge and the target distribution. To match poses generated by FlexPose with unlabelled images in the target dataset, we employ an unsupervised method (Jakab et al. 2020) in addition to supervision from adversarial training. This matching procedure serves as an evaluation method for FlexPose and is further detailed in Section 4. Recently, test-time adaptation (Li et al. 2021; Cui et al. 2023; Hu et al. 2024) has proven to be an effective way"}, {"title": "3 Method", "content": "Given a limited number of 2D pose annotations set $T = \\{Y_t | Y_t \\in \\mathbb{R}^{M \\times 2}\\}$ of a newly collected human pose images, FlexPose aims to estimate the whole distribution $D_t$ which pose annotation T belongs to, and generate any number of new pose annotations that follow the distribution. M here is the number of joints in each pose. This task setting is challenging. However, we believe that with the prior from sufficient off-the-shelf annotations $S = \\{y_s | Y_s \\in \\mathbb{R}^{M \\times 2}\\}$, the distribution $D_t$ can be estimated and well shaped. In this paper, we transfer the distribution $D_s$ estimated from S to the target pose domain to estimate the target distribution $D_t$ by considering the guidance from 2D pose annotations set T."}, {"title": "3.1 Overview", "content": "As illustrated in Figure 2, our framework consists of three phases: \u2460 Generic Pose Distribution Estimation. We learn a generator $g_s(\\cdot)$ on the pose set S to estimate the pose distribution $D_s$. The generator takes a latent code z as input and outputs a skeleton $\\hat{x_s}$, i.e. $\\hat{x_s} = g_s(z)$. We take the distribution of generated $\\hat{x_s}$ to mimic that of the generic pose $x_s$. Here, $x_s$ is the corresponding skeleton image of an annotation $y_s$ as shown in the left part of Figure 2. \u2461 Pose Distribution Adaptation. Given the limited target annotation set T, we transfer $g_s(\\cdot)$ to fit the pose distribution $D_t$ and learn a new generator $g_t(\\cdot)$ of the target pose domain. Considering the limited knowledge acquired from target pose annotation T, we introduce three regularizations, Linear, Sparse and Pose-mixup, to avoid reaching a collapse solution. \u2462 Target Pose Sampling. The transferred generator $g_t(\\cdot)$ can flexibly synthesize any number of fake pose annotations by randomly sampling in the latent space. This generated annotation set $\\hat{T}$ will be treated as an extension of given annotations set T in the downstream tasks, e.g., Keypoints Annotation and Pose-conditional Human Image Generation, since poses within both of them follow the distribution $D_t$."}, {"title": "3.2 Generic Pose Distribution Estimation", "content": "Deep generative models have been widely verified that they have a rich capacity to well approximate image distributions when given sufficient training data. Motivated by the success of these generative models (Karras, Laine, and Aila 2019) on natural/artificial image generation, we treat 2D pose annotations $y_s, Y_t \\in \\mathbb{R}^{M \\times 2}$ as skeleton images $x_s, x_t \\in \\mathbb{R}^{C \\times W \\times H}$ and extend an image generator to generate 2D pose annotations by synthesizing corresponding skeleton images. As shown in the left part of Figure 2, the transformation from the 2D keypoints to the skeleton images can be implemented by functions $\\alpha(\\cdot)$, namely $x = \\alpha(y)$, where $\\alpha(\\cdot)$ simply draws keypoints from y and connects them with straight lines on a blank figure. The visual effect is similar to the stick man. To achieve precise semantic alignment with the appearance correspondence, each bone in the skeleton image is assigned a unique color. Therefore, C of each skeleton image is set as three (RGB channels). Compared with Black&White, the colorful embedding brings marginal improvement in the quality of generated skeletons.\nA generator can be formulated as a mapping function g(\u00b7), which gets a latent code z and outputs a skeleton image x. The probability distribution of skeleton images hence is estimated by $p(x) = p(z) p_{g}(x|z)$. We assume that the pose distributions of different datasets share similar pose prior, and their distributions can transfer to one another by geometric transformations. Based on this assumption, we"}, {"title": "3.3 Pose Distribution Adaptation", "content": "As illustrated in Figure 3, to transfer $p_{\\delta}(h_s|z)$ to $p_{\\delta}(h_t|z)$, we adjust the style code by introducing a transfer function $\\tau(\\cdot)$ at the output of $\\delta(\\cdot)$, and therefore the target domain generator is defined as\n$g_t = \\phi \\circ \\delta_t = \\phi \\circ (\\tau \\circ \\delta_s)$.\nTo learn the transfer function \u03c4, we first randomly sample |T| latent codes z (one for each pose in T) from the latent space, and require the generator maps each code to corresponding skeletons in T. This transferring procedure can be achieved by minimizing the following perceptual loss,\n$\\min_{\\theta_\\tau} \\Gamma = \\min_{\\theta_\\tau} \\sum_{i=1}^{|T|} || \\Gamma(g_t(z);\\theta_\\tau) - \\Gamma(x) ||_2^2$,\nwhere $\\theta_\\tau$ is the parameter of $\\tau(\\cdot)$, \u0393 is a pre-trained feature extractor, z is from the set of sampled latent codes, and $x$ is the skeleton image drawn from the pose annotation set T.\nHowever, the problem is we only have few-shot guidance T from the target domain distribution. Given a data-starving deep learning model, the guidance is insufficient to reach a satisfactory solution. For that reason, we introduce three regularizations to alleviate the data-insufficient issue.\nLinear & Sparse Regularization. Compared with finetuning the whole transformation function $\\delta_s$ to reach $\\delta_t$, only adjusting the affine transformation from $A_s$ to $A_t$, i.e. $A_t = \\tau \\circ A_s$, can efficiently shrink the searching space of transfer solution, and therefore avoid overfitting. Meanwhile, the recent GAN inversion technique shows that the layer-wise style code in StyleGAN leads to the hierarchical disentanglement of local and global attributes, which aligns well with our motivation of adapting pose distribution by considering the global geometric transformation between poses. We thus adjust the source affine transformation $A_s$ from the perspective of layer level, and limit the number of to-be-adjusted layers as small as possible. Considering the form of the affine transformation A and the layer decoupling characteristics of StyleGAN, we empirically define the transfer function $\\tau(\\cdot)$ as a block diagonal matrix,\n$\\tau \\triangleq diag(I, ..., I, U_{l_0}, I, ..., I, U_{l_1}, I, ..., I)$,\nwhere only a limited number of block is defined by U, i.e. $l_0$ and $l_1$ in this case, to follow the sparse regularization. We experimentally find that the earlier layers are most related to the geometric transformation. And we only learn those layers in our experiments. Meanwhile, other blocks are set as identity matrix I. We investigated how the choice of layer l affects the transformation procedure in Section 4.4.\nPosa-mixup Regularization. Most poses interpolated between two real poses physically exist, and their convex combinations build the real-world pose distribution. Inspired by the mixup regularization (Zhang et al. 2017) on images, we therefore extend it to 2D pose annotations and propose the Pose-mixup to enrich the guidance set. The main difference between mixup and Pose-mixup is that the mixup works on image space and the Pose-mixup works on keypoint space. Given that the mixup on skeleton space may lead to unreasonable results, Pose-mixup regularizes the neural network to learn the simple linear behavior in-between 2D poses and thus prevents the model from generating unrealistic human pose annotations. By mixing up the corresponding joints of any two 2D poses with mixup ratio $\\lambda \\in [0, 1]$, the extended annotation set T* from T is then defined as,\n$T^* = \\{y^* | y^* = \\lambda \\cdot y_i + (1 - \\lambda) \\cdot y_j, y_i, y_j \\in T \\} .$"}, {"title": "3.4 Target Pose Sampling", "content": "Once the transferred generator $g_t(\\cdot)$ is obtained, we can generate theoretically as many target skeleton images $x_t$ as possible by randomly sampling latent codes z in the estimated target distribution $D_t$. Unfortunately, the generated target skeleton images are not perfect and may bring in artifacts which mislead the training of a neural network. To address this issue, we utilize $\\beta(\\cdot)$ to filter out the random noise. $\\beta(\\cdot)$ is a neural network regressor pre-trained on S and acts as a tight information bottleneck that preserves skeleton information and ignores random noise. Following the generation of the fake skeleton images $\\hat{x_t}$ from $g_t()$, we extract the coordinates of interpretable 2D keypoints $\\hat{T} = \\{\\hat{y_t}\\}$ from it, e.g., hands, by applying $\\hat{y_t} = \\beta(x_t)$. Thereafter, we can get a clean generated skeleton $\\hat{x_t}$ by a re-render process,\n$\\hat{x_t} = \\alpha(\\hat{y_t}) = \\alpha(\\beta(x_t)).$\nThese generated skeleton images and the corresponding generated 2D pose annotations can be further utilized to assist any pose-related down-stream tasks."}, {"title": "4 Experiments", "content": "In this section, we first evaluate the distribution similarity between transferred distribution and target distribution via two standard metrics. Then, we show how FlexPose can improve the performance of existing unsupervised landmark detection algorithms and benefit unlabelled human pose dataset annotation. At last, we extensively discussed how each part of FlexPose works."}, {"title": "4.1 Pose Distribution Transformation", "content": "In this subsection, we conduct a transformation experiment between COCO (Lin et al. 2014) and Human3.6M (Ionescu et al. 2014) to show how FlexPose works.\nExperiment Setting. We train a StyleGAN (Karras, Laine, and Aila 2019) using the skeleton images from the source datasets to estimate the distribution of source human pose. And then we transform the estimated distribution to the target one according to several samples from target dataset. For source dataset COCO, we only keep the annotated people instances with full pose annotations to construct a training set of 32k samples. The training of StyleGAN follows standard protocol in the original work. In the transformation phase, we only use 30 annotations from the target dataset Human3.6M (two for each class). The size of interpolated pose set (|T*|) is set as 1000. We experimented with changing different layers and found that setting l=3, i.e., transferring the third coarsest layer, usually gets the lowest reconstruction loss in Equation (5) as shown in Figure 4. So, we set l=3 in all experiments. A detailed setting and deeper analysis can be found in appendix. When adaptation phase ends, we sample new poses from generator and treat them as Adapted COCO.\nEvaluation & Visualization. Qualitatively, we show the visual result of pose transformation in Figure 5. For each row, we show one skeleton (Left) that was randomly sampled from the generator before transformation, one skeleton (Middle) that was sampled from the transformed generator by using the same latent noise as the Left, and one skeleton (Right) in the few-shot annotation set T from target dataset. We can see that the Left and the Middle generated from the same random noise are visually quite different, and the Middle is more similar to the Right.\nIn Figure 6, we plot the t-SNE embedding of the poses generated by FlexPose (Adapted COCO), comparing it with the embedding of poses from the source (COCO) and target (Human3.6M) dataset. As can be seen, the embedding of poses from the source and target dataset are separated, and the distribution of generated poses significantly overlaps with the target ones. We also noted that the pose distributions are 'mismatched' in the upper right region. Considering that only two shots are utilized as guidance for each class during the transformation, such a mismatch is reasonable.\nQuantitatively, we measure the similarity between the transferred distribution and target distribution using the Fr\u00e9chet distance (FD), which follows from the Wasserstein-"}, {"title": "4.2 Unlabelled Human Pose Dataset Annotation", "content": "To further evaluate the quality of transformed pose quantitatively and show potential downstream application of FlexPose, we show how can we annotated unlabelled human-related dataset with the help of FlexPose.\nPose-Image Matching Algorithm. In dataset annotation tasks, our goal is to assign each image in the target dataset to the most closely related pose in the estimated target distribution $D_t$. Existing self-supervised human pose detection methods (Jakab et al. 2018; Lorenz et al. 2019; Thewlis, Bilen, and Vedaldi 2017) are usually constrained by the high relevance between model prediction and input image. Among them, KeypointGAN (Jakab et al. 2020) can match unpaired images and annotations by forcing the distribution of detector predictions to align with the existing poses. We train KeypointGAN by using human images from target dataset and generated pose set $\\hat{T}$. Once the training process is completed, the model prediction on samples can be treated as the best-matched annotations in given distribution.\nSource Datasets. Apart from COCO, we also use MPI-INF-3DHP (3DHP) (Mehta et al. 2017) which contains more than 1.8 million human pose annotations from eight subjects and covers eight complex exercise activities. SURREAL (Varol et al. 2017) is a synthetic dataset containing more than six million frames of people in motion.\nTarget Datasets. The large-scale dataset Human3.6M (Ionescu et al. 2014) has 3.6 million samples. The Simplified Human3.6M dataset (Zhang et al. 2018) contains 800k training and around 90k testing images. We use all human images in the target dataset and randomly select several guide poses.\nEvaluation Metrics. Since dataset annotation shares similar targets with 2D landmark detection. We report 2D landmark detection performance for evaluation. Two standard evaluation metrics are considered to compare our method with baselines. The MSE column reports a mean square error in pixels overall pre-defined common joints. The Percentage of Correct Key-points (PCK-p) is used as an accuracy metric that measures if the distance between the predicted keypoint and the true joint is within a certain threshold p.\nPerformance Comparisons. We feed the generated skeleton images $\\hat{x_t}$ and RGB human images from target dataset into KeypointGAN to evaluate the effectiveness of each pose transformation algorithm. As a baseline, we train the detector on each target dataset by directly using the pose annotations set S from the source dataset, which we denote as Baseline in the comparison and can be roughly treated as the worst case. We also employ three strong competitors, AdaGAN,"}, {"title": "4.3 Unlabelled Human Face Dataset Annotation", "content": "Introducing FlexPose to human face landmarks transfer is straightforward since both the human pose and the human face consist of a set of pre-determined keypoints.\nDatasets. WFLW (Wu et al. 2018) has 10 thousand samples with 98 facial landmarks, where 7.5 thousand for training and 2.5 thousand for testing. 300-VW (Sagonas et al. 2013) consists of 300 Videos in the wild and contains ~95 thousand annotated human faces in the training set. We treat 300-VW as the source dataset and only use its annotations for training StyleGAN. And few-shot annotations in the target dataset WFLW are utilized for transformation. We only keep the shared 68 facial landmarks in two datasets.\nExperiments Settings and Results. The evaluation metrics and the experiment protocols are the same as that in the human pose. The size of the few-shot guidance set from target dataset is set as 30. We report the evaluation results on the validation set of WFLW in Table 3. FlexPose still outperforms the baseline by a large margin (MSE 18.78 \u2192 11.64 and PCK 0.679 \u2192 0.766). Given that the human face"}, {"title": "4.4 Ablation Study & Parameter Sensitivity", "content": "In Table 4, ablation studies are conducted:\nEffect of Regularization. We proposed three kinds of regularization in Section 3.3 to alleviate the extreme data-insufficient issue. We remove part of them from our FlexPose, and the results are #1 to #7. From #1 to #4, we gradually relax the sparsity regularization by allowing more blocks in the diagonal matrix \u03c4 not to be an identity matrix I. The performance only drops by an acceptable level thanks to the Linear and Mixup regularization. Furthermore, in #5, #6, and #7, we further relax the Mixup and Linear regularization, which significantly hurt the quality of generated images and lower the model accuracy in downstream tasks.\nNumber of Shots from Target Dataset. Under the setting of COCO-S-H3.6M, we increase the number of shots from 12 to 48 and found that the performance of the pose detector has no obvious difference. The results can be found in #8 to #10. An explanation is that the increment of few-shot samples from the target dataset brings a limited gain of information compared with the strong prior trained on large-scale datasets. Few samples are enough for target distribution localization.\nChoice of Layers l. In previous experiments, we empirically choose l=3 in Equation (4) for all experiments and get significant improvement. We found that the choice of l is not strictly fixed. We have also tried a composition of multi-layer, and the results can be found in #2, #3, and #4. The result in #4 shows the necessity of sparse regularization. We leave the best choice of l to future work.\nMulti-source Datasets. To study the effect of the setting where the source annotations are from different datasets, we conduct two additional experiments (#12 and #13) in Table 4 and compare them with existing experiments (#11). In #12 and #13, we use the union of different source datasets to train the generic generator. The result indicates that the increasing diversity on the source dataset (#11 #12 #13) brings better results on the target dataset. By utilizing FlexPose, the performance of downstream task models can benefit from collecting a more diverse pose dataset, which is much easier compared with collecting a realistic human dataset with accurate landmarks. However, the result of #13 is still worse than that of #1, which indicates the trade-off between diversity and similarity to the target dataset when choosing the source."}, {"title": "5 Conclusion", "content": "We aim to transfer knowledge in the pose domain and propose an effective method named FlexPose. Our approach allows us to adapt an existing pose distribution to a different target one by using a few poses from the target dataset and generating theoretically infinite poses following the target distribution. FlexPose can be used on several pose-related works. In future work, we hope to extend our method to a more generic pose domain adaptation approach."}, {"title": "6 Training Details", "content": "Pre-training Setting of StyleGAN. We follow the settings in StyleGAN (Karras, Laine, and Aila 2019) to learn the source generator. All the hyper-parameters are kept the same, and the generator is trained until convergence. The training set is re-rendered from the pose annotation of the Source Dataset S by the function \u03b1(\u00b7). Different methods share the common pre-training weights for a fair comparison.\nTraining Setting of FlexPose. We freeze the weights of the pre-trained source generator and only finetune on the linear block-diagonal matrix \u03c4(\u00b7). \u03c4(\u00b7) is initialized with an identity matrix I, and only a few selected blocks (3rd, for example) of it can be adjusted due to the sparse regularization. The training set only includes the skeletons re-rendered from the target pose set T and the extended annotation set T*. The parameter betas of the Adam optimizer is set as (0.9, 0.999). The learning rate is set to 0.1. The batch size is 128 with a training length of 1000 iterations. It takes less than 5 minutes on a single V100 GPU to finetune the model.\nTraining Setting of Methods for Comparison. We compare our FlexPose with three similar methods, AdaGAN (Noguchi and Harada 2019), FreezeD (Mo, Cho, and Shin 2020) and LoRA(Hu et al. 2021), in our experiments. We re-implement AdaGAN under our settings according to the official open-source code. FreezeD has an official implementation on StyleGAN and is utilized for the comparison. Similar to our method, the low-rank modules are conducted aside the linear-block-diagonal matrix \u03c4(\u00b7) with the default rank setting (r=8). To have a fair comparison, all the hyper-parameters and the settings including training length, learning rate, batch size, optimizer are well aligned with the competitors."}, {"title": "7 Implementation of \u03b1(\u00b7) and \u03b2(\u00b7)", "content": "\u03b1(\u00b7) is a rule-based function. Given an annotation y, we draw each keypoint on an empty black figure, and then connect them with fixed color lines. The choice of color is random and is pre-defined before the experiment. All methods share the same choice of colors. The visual effect is similar to a stick man.\nAs a reverse function of \u03b1(\u00b7), \u03b2(\u00b7) is a pre-trained neural network. The input is a skeleton image x and the output is M heatmaps. The locations of keypoints are further obtained by the method introduced in (Jakab et al. 2018) by converting each heatmap into a 2D probability distribution. The training of \u03b2(\u00b7) is offline. The training procedure is achieved by minimizing the reconstruction loss on the given annotation y from the source dataset,\n$L_{rec} = || y - \\beta(\\alpha(y)) ||^2.$"}, {"title": "8 Skeleton-guided Applications", "content": "Recently, there have been studies on generating human images with given 2D poses. A large amount of reasonable human poses in a certain style or distribution may be needed to evaluate their performance. FlexPose is born for this task and can generate infinite suitable 2D human poses with few-shot human poses in the needed style. Figure 8 gives some"}, {"title": "9 Extended Discussion", "content": "We expanded our discussion on the fine-tuned layers selection presented in Figure 4 of the main text. The discussion investigates the pose geometric semantics of each layer in the StyleGAN (Karras, Laine, and Aila 2019).\nGeometric Rotation Transformation.. Two types of geometric rotation transformation were applied to the poses from the target dataset. The global transformation, $Aug_g(\\cdot)$, rotates the target skeleton image x by a given angle \u03b8. In contrast, the local transformation, $Aug_l(\\cdot)$, rotates a single leg of the target skeleton image x by an angle \u03b3. The angle \u03b8 is randomly selected from [-45\u00b0, 45\u00b0], while \u03b3 is randomly chosen from [135\u00b0, 225\u00b0]. An illustration of these two transformations is provided in Figure 10. The augmented skeleton images can differ significantly from the samples in the source distribution. Our investigation centers on identifying which layer calibration can minimize the reconstruction loss during the transformation phase when the target skeleton images are individually transformed by these two methods."}]}