{"title": "GPTON \u2013 Generative Pre-trained Transformers enhanced with Ontology Narration for accurate annotation of biological data", "authors": ["Rongbin Li", "Wenbo Chen", "Jinbo Li", "Hanwen Xing", "Zhao Li", "W. Jim Zheng"], "abstract": "By leveraging GPT-4 for ontology narration, we developed GPTON to infuse structured knowledge into LLMs through verbalized ontology terms, achieving accurate text and ontology annotations for over 68% of gene sets in the top five predictions. Manual evaluations confirm GPTON's robustness, highlighting its potential to harness LLMs and structured knowledge to significantly advance biomedical research beyond gene set annotation.", "sections": [{"title": "Main", "content": "Generative Pre-trained Large Language Models (LLMs), trained on vast datasets (e.g., over 15 trillion tokens for Llama 3 models\u00b9), can process and infer complex information relationships beyond traditional methods. This ability is particularly useful in biology for tasks like annotating gene sets, overcoming conventional analysis limits\u00b2. However, current applications of large language models (LLMs) in gene set analysis often fall short because they typically generate ontology labels directly, resulting in suboptimal outcomes.3-5 This poor performance stems from the fact that ontologies are neither widely nor precisely used in the biomedical literature-a major component of the training data for LLMs.4, 5 Consequently, while LLMs excel at generating natural language, they struggle with structured, specialized data like ontologies due to the lack of context and understanding in their training data.\nTo address this challenge, we developed GPTON, Generative Pre-trained Transformers enhanced with Ontology Narration, for accurate annotation of biological data. Ontology narration, or ontology verbalization, converts ontology terms into natural language without losing their meaning. Fine-tuning LLMs with verbalized ontology terms has the potential to enhance their performance in accurately annotating gene sets\u2014a pivotal task in analyzing single-cell RNA sequencing and other high-throughput 'omics' experiments. GPTON overcomes limitations of traditional gene set analysis methods, such as Gene Set Enrichment Analysis (GSEA), which rely on existing gene set databases and single-gene annotations that are often incomplete and limited in scope7. Additionally, these traditional methods cannot generate summary text to describe gene set functions, hindering the discovery of novel gene interactions and pathways.\nGPTON uses an innovative three-step method for gene set annotation with LLMs: 1) Ontology Narration: Utilizing GPT-4 to verbalize GO terms from the Biological Process branch into narratives; 2) LLM Fine-tuning: Using these narratives to label gene sets from MSigDB and fine-tune LLMs like Llama 3 for generating aligned narrative summaries; and 3) Mapping Narrative Summaries: Mapping narrative summaries to verbalized GO terms using cosine similarity with an encoding module, offering GO term annotations for each gene set. We extensively evaluated GPTON using the Llama 3 8B and 70B models for annotating human and mouse gene sets. For comparison, we included GeneAgent\u00b2 and GPT-43, which are current state-of-the-art LLM-based approaches for identifying GO terms for gene set annotation. Hyperparameter optimization was performed using grid search to determine the optimal learning rate, LoRA size, and batch size. We assessed performance using widely recognized metrics, such as ROUGE scores\u00ba for word-level similarity and BERTScore10 for semantic similarity between generated gene set labels and ground truth"}, {"title": "labels (Methods). Additionally, domain experts conducted manual evaluation to ensure the accuracy of predicted GO terms and verbalized narratives.", "content": null}, {"title": "labels (Methods). Additionally, domain experts conducted manual evaluation to ensure the accuracy of predicted GO terms and verbalized narratives.", "content": "GPTON achieved 14% and 20% improvements in ROUGE-1 compared to GeneAgent and GPT-4, respectively, for human gene sets. The GPTON model based on Llama 3 70B demonstrated the best performance across all four metrics for both human and mouse gene sets. While directly fine-tuning Llama 3 with GO labels outperformed the base model, it remained suboptimal compared to models fine-tuned with verbalized ontology.\nWe selected GPTON based on the Llama 3 8B model for further testing and analysis due to its relatively compact size and strong performance. The stability of GPTON is demonstrated by the consistent ROUGE-1 scores across 5-fold cross-validation. This stability remains true across all four metrics.\nGPTON demonstrated high consistency and accuracy. Two bioinformaticians manually evaluated the agreement between the original GO terms assigned to each gene set (A) and those predicted by GPTON (B) (Methods). For the top 50 gene sets, 92% of the predicted GO terms exactly matched or were highly relevant to the original ones, compared to 69% for the bottom 50 gene sets. The alignment between GO terms and their GPT-4 verbalized narratives (C) was also evaluated, with 98% of the verbalized narratives for the top 50 gene sets"}, {"title": "and 92% for the bottom 50 sets accurately aligning with the original GO terms in meaning. The assessment was objective, based on whether a GO term for a gene set could be mapped back to its ground truth annotation.", "content": "We further evaluated GPTON for gene set annotation using the topology of the GO graph. Our results show that GPTON can identify exact or highly relevant GO terms in over 68% of mouse gene sets among the top five predictions. A highly relevant GO term is defined as being within three or fewer edges from the ground truth GO term in the GO graph. This quantitative evaluation, free from potential human subjectivity, further underscores GPTON's utility in real-world gene set analysis tasks, demonstrating its ability to generate highly informative results suitable for manual confirmation."}, {"title": "To examine GPTON's generalizability, we predicted annotations for external sets of 1,670 human and 1,247 mouse gene sets from Reactome (Methods). GPTON, based on two fine- tuned Llama 3 models, achieved ROUGE-1 scores of 0.33 and 0.31 on the mouse dataset, respectively, significantly surpassing the baseline GPT-4 model's score of 0.16.", "content": "On the human dataset, the improvement reaches 19% compared to GPT-4. These results demonstrate GPTON's strong generalizability across datasets of different origins, even those not seen during training. This is crucial for annotating gene sets with few or no existing ontology labels and for providing accurate annotations for novel gene sets from experimental investigations.\nFurthermore, this capability can be further improved by additional training samples. When the training set was at 1\u00bc, \u00bd, and \u00bc of its original size, there is a consistent increase in performance in generating narrative summaries with larger training sets.\nGPTON pioneers a flexible and informative approach to integrating structured knowledge into LLMs, delivering reliable annotations for gene sets even without existing ontology labels or from novel experiments. Mapping generated summaries back to structured knowledge minimizes hallucination, a common LLM limitation. Expanding training data with additional resources and structured knowledge could enhance GPTON's performance, extending its applications beyond gene set annotation to human disease research and drug discovery."}, {"title": "Methodology", "content": "We selected two species, human (Homo Sapiens) and mouse (Mus Musculus), and obtained a list of gene sets with their corresponding GO terms from MSigDB\u00b9\u00b9 database for both species to conduct our experiments. We also conducted filtering on the two lists of gene sets with the following criteria:\n\u2022\tThe collection category has to be a GO Biological Process (GO:BP);\n\u2022\tThe gene sets must have an identified exact source;\n\u2022\tThe number of genes contained in each gene set must be less than 500.\nThe filtered dataset contains 7,423 and 7,292 gene sets for mouse and human, respectively. For the collected gene sets, we also obtained the description of each gene from the NCBI gene info table12. For the human dataset, we used the same evaluating gene sets in GeneAgent as the test set. To prevent potential data leakage, we filtered out gene sets in the human training dataset that had the same GO term labels or genes as those in the test set. The mouse dataset was separated into training and test sets by an 8:2 ratio. Our study then revolved around fine- tuning LLMs with the collected gene sets and their labels, such that the LLMs are deployed to the task of generating the gene set annotations.\nA second dataset was also constructed from the Reactome subset of the MSigDB database with the collection category being: CP:REACTOME11, which contains 1,670 human gene sets and"}, {"title": "1,247 mouse gene sets after we use a similar filtering criteria to obtain labels of the Reactome gene sets.", "content": null}, {"title": "Gene Ontology Verbalization", "content": "Prior to the LLM fine-tuning, we first converted the collected GO terms from short phrases to natural language sentences, extending their length and context. This was done by querying GPT-4 (GPT4-0125) to generate verbalized narratives for all GO terms. During this process, we only changed the maximum number of tokens to 1,024 and kept all other parameters as default for GPT-4."}, {"title": "LLM Selection and Prompting Pipeline", "content": "To evaluate the ability of generative LLMs in predicting gene set annotation labels after fine- tuning and find the model that is most efficient and effective, we selected two different versions of the state-of-the-art LLMs hosted on Hugging Face: Llama 3 8B, and Llama 3 70B\u00b9.\nWe then composed a prompt template to fine-tune the Llama 3 models while using the verbalized GO terms generated by GPT-4 as ground truth."}, {"title": "Model Training", "content": "Training data containing the proposed prompts as input and verbalized GO terms as ground truth labels were generated and utilized to fine-tune the LLMs. During model training, 1/8 of the training samples were further separated as the validation set, while the remaining 7/8 of data were learned upon.\nThe Transformer Reinforcement Learning (TRL) library from Hugging Face13 was used to accommodate our usage of the NVIDIA H100 GPUs. Since full fine-tuning on larger models such as Llama 3 70B is not feasible with our hardware resources, we employed two fine-tuning strategies: full-model fine-tuning (Llama 3 8B) and LoRA with Quantization (QLoRA) (Llama 3 8B and 70B)14. To draw out the most optimal performance from LLMs, we also conducted hyperparameter searching using Llama 3 8B model by designating learning rate, LoRA parameters (rank and alpha values), and batch size to tweak, and the values we decided upon are 0.00001, [128,512], 16, respectively. The Llama 3 70B model adopted the same hyperparameters as those optimized for the 8B model."}, {"title": "Identify GO terms using generated summary by embedding similarity", "content": "The target GO term for each gene set was identified by comparing the semantic embeddings of the verbalized narrative for all GO term candidates with the narrative summary generated by the fine-tuned LLM for the given gene set. We adapted MedCPT, a Transformer-based encoder model pre-trained on PubMed query-article pairs for zero-shot semantic information retrieval in biomedicine, to embed both the generated gene set description and the verbalized narratives of all GO terms in the biological process namespace. The cosine similarity between the gene set summary and each candidate GO term was then calculated as the dot product of the normalized vectors. The GO terms corresponding to the highest-ranked verbalized narratives were retrieved as the predicted target GO terms for the gene set."}, {"title": "Benchmarking Methods", "content": "GeneAgent\u00b2 is an LLM-based approach designed to enhance gene set analysis by minimizing the common issue of hallucinations in LLMs. Built upon GPT-4, GeneAgent operates through a structured workflow of generation, self-verification, modification, and summarization. Initially, it generates process names and analytical texts for input gene sets. It then employs a selfVeri-"}, {"title": "Agent to autonomously verify these outputs against curated knowledge from domain-specific databases through Web APIs. This verification involves extracting claims and comparing them with database information, iteratively refining the process names and analytical narratives to ensure accuracy. GeneAgent's self-verification capability allows it to discern and correct hallucinations, improving reliability. Comparative studies reveal that GeneAgent outperforms standard GPT-4 in generating accurate biological process names and provides more informative gene set annotations.", "content": null}, {"title": "GPT-4", "content": "Hu et al.\u00b3 evaluated the ability of various LLMs to uncover the functions of gene sets derived from functional genomics experiments. Their method involves querying a panel of LLMs, including GPT-4, Gemini-Pro, Mixtral-Instruct, and Llama 2 70B, to directly generate GO term names, supporting analysis texts, and confidence scores for the given gene set. The process includes benchmarking LLMs against gene sets from the Gene Ontology to assess their accuracy and exploring their ability to propose novel functions for gene sets derived from 'omics data. Notably, GPT-4 demonstrated high accuracy in recovering curated gene set names and identifying novel gene functions not reported by traditional functional enrichment methods. Therefore, the GPT-4-based approach described in this paper will serve as one of the baselines for comparative analysis in our study.\nSpecifically, we used the following prompt to query the GPT-4 model to directly predict GO terms. This prompt template includes a one-shot example."}, {"title": "Evaluation Metrics", "content": "The performance of the LLMs was evaluated by comparing the generated response with the ground truth in the testing set and calculating four widely used metrics: ROUGE-1 score, ROUGE-2 score, ROUGE-L score\u00ba, and BERTscore10. We utilized the Python implementation \u201cROUGE\u201d (version 1.0.1) to calculate the ROUGE-1, ROUGE-2, and ROUGE-L scores. Similarly, the Python implementation \u201cbert_score\" (version 0.3.13) was imported to obtain the BERTScore.\nROUGE 1 and 2 scores represent the 1-gram and 2-gram recall value, respectively, between the two texts at comparison. For each gene set, it is calculated by the following equation (1):\nROUGE n =  \\frac{\\sum_{s \\in references} \\sum_{gram_n \\in s} Count_{match}(gram_n)}{\\sum_{s \\in references} \\sum_{gram_n \\in s} Count(gram_n)} (1)\nWhere S is a reference summary, gramn is a n-gram in S, Countmatch(gramn) is the number of n-grams that are found both in the reference summary and the generated summary, and Count(gramn) is the count of all n-grams in the reference summary\u00b2. In our case, n can be 1 or 2.\nROUGE L score is an F1-based metric calculated from the longest common subsequence (LCS) between the two summaries. Let X and Y represent the reference summary and the generated summary, and m and n are the length of X and Y, respectively. To obtain ROUGE L score, the precision (P) and recall (R) with respect to LCS are first calculated by the following equations (2) and (3):\nRLCS = \\frac{LCS(X,Y)}{m} (2)\nPLCS = \\frac{LCS(X,Y)}{n} (3)\nWhere LCS(X, Y) is the length of the longest common subsequence between X and Y. ROUGE L score is then calculated by:\nROUGE L = \\frac{(1+\\beta^2)R_{LCS} \\cdot P_{LCS}}{R_{LCS}+\\beta^2 \\cdot P_{LCS}} (4)\nWhere \u03b2 is a constant hyperparameter value.\nBERTScore is also a F1-based metric, where the P and R are calculated by the matrix similarity between the tokens in two summaries\u00b9\u00ba. Let x and y be the tokens in X and Y, where X and Y are defined as above, the following equations calculate the R, P and BERTScore:\nRBERT = \\frac{1}{|x|} \\sum_{x_i \\in x} max_{y_j \\in y} \\Pi(x_i ;y_j) (5)\nPBERT = \\frac{1}{|y|} \\sum_{yj \\in y} max_{xi \\in x} \\Pi(x_i;y_j) (6)\nBERTScore = 2 \\frac{P_{BERT} R_{BERT}}{P_{BERT}+R_{BERT}} (7)"}, {"title": "Reduced Sample Size Training", "content": "To observe the effect that the size of the training set has on the model's performance, we tried reducing the training set by sampling multiples of quarters before fine-tuning the model and calculating its performance in generating narrative summaries in the separated test set. As the sample size partition increased from 1 to 1, the mean accuracy of the Llama 3 70B and 8B predictions increased as well."}, {"title": "Human Evaluation", "content": "To further assess the quality of the responses gained from LLMs in our method, we sampled 10 predictions in every 0.1 interval in terms of the ROUGE 1 score made by the Llama 3 70B model on the mouse test set. Specifically, we extracted the original GO term label (A), final predicted GO term (B), and verbalized narratives (C). We then deployed two bioinformatics experts to evaluate the semantic relevance between both the true GO term-predicted GO term pair (A vs. B consistency), and the GO term-verbalized narratives pair (A vs. C consistency). We designated three levels of consistency: irrelevant, relevant, and exact match. For a pair to be considered relevant, they must describe the same biological process, while they may slightly differ in scales and details. For example, in an A vs. C pair where the ground truth GO term is \"regulation of mast cell chemotaxis\u201d, a verbalized narrative such as \u201cThis process involves the orchestration of mast cell movement...\u201d would be considered an exact match with the GO term, while a summary such as \u201cThis process involves the enhancement of mast cell movement...\" would be labeled as relevant, since although the original GO term does not specify positive or negative regulation, they are describing the same biological process at different levels of detail. Similarly, in the case of the A vs. B pair, if the reference GO term (A) is \u201cmodification of synaptic structure\", then a GO term such as \u201cregulation of synaptic structure\u201d would be considered an exact match, while \u201cregulation of dendritic spine morphogenesis\u201d would be considered relevant, since dendritic spine morphogenesis is only a subset of synaptic structure.\""}, {"title": "Data Availability", "content": "All data used in this manuscript were downloaded from publicly available sources. The gene sets and their annotation information for human gene sets was obtained from the MSigDB website (https://www.gsea-msigdb.org/gsea/msigdb/human/genesets.jsp?collection=GO:BP). The corresponding annotations for mouse gene sets are also available on the MSigDB website (https://www.gsea-msigdb.org/gsea/msigdb/mouse/genesets.jsp?collection=GO:BP). Detailed information for each gene (gene_info) was sourced from the NCBI Gene FTP database (https://ftp.ncbi.nih.gov/gene/DATA/). The basic version of the Gene Ontology OBO file which includes all Gene Ontology terms was downloaded from the Gene Ontology website (https://geneontology.org/docs/download-ontology/). The human Reactome dataset was downloaded from the MSigDB database under the collection category CP (https://www.gsea-msigdb.org/gsea/msigdb/human/genesets.jsp?collection=CP:REACTOME). Similarly, the mouse Reactome dataset was downloaded from (https://www.gsea-"}]}