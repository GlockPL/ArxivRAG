{"title": "Your Learned Constraint is Secretly a Backward Reachable Tube", "authors": ["Mohamad Qadri", "Gokul Swamy", "Jonathan Francis", "Michael Kaess", "Andrea Bajcsy"], "abstract": "Inverse Constraint Learning (ICL) is the problem of inferring constraints from safe (i.e., constraint-satisfying) demonstrations. The hope is that these inferred constraints can then be used downstream to search for safe policies for new tasks and, potentially, under different dynamics. Our paper explores the question of what mathematical entity ICL recovers. Somewhat surprisingly, we show that both in theory and in practice, ICL recovers the set of states where failure is inevitable, rather than the set of states where failure has already happened. In the language of safe control, this means we recover a backwards reachable tube (BRT) rather than a failure set. In contrast to the failure set, the BRT depends on the dynamics of the data collection system. We discuss the implications of the dynamics-conditionedness of the recovered constraint on both the sample-efficiency of policy search and the transferability of learned constraints.", "sections": [{"title": "1 Introduction", "content": "Constraints are fundamental for safe robot decision-making (Stooke et al., 2020; Qadri et al., 2022; Howell et al., 2022). However, manually specifying safety constraints can be challenging for complex problems, paralleling the reward design problem in reinforcement learning (Hadfield-Menell et al., 2017). For example, consider the example of an off-road vehicle that needs to traverse unknown terrains. Successful completion of this task requires satisfying constraints such as \u201cavoid terrains that, when traversed, will cause the vehicle to flip over\" which can be difficult to specify precisely via a hand-designed function. Hence, there has been a growing interest in applying techniques analogous to Inverse Reinforcement Learning (IRL) \u2014 where the goal is to learn hard-to-specify reward functions to learning constraints (Liu et al., 2024). This is called Inverse Constraint Learning (ICL): given safe expert robot trajectories and a nominal reward function, we aim to extract the implicit constraints that the expert demonstrator is satisfying. Intuitively, these constraints forbid highly rewarding behavior that the expert nevertheless chose not to take (Kim et al., 2023). However, as we now explore, the question of what object we're actually inferring in ICL has a nuanced answer that has several implications for downstream usage of the inferred constraint.\nConsider Fig. 1a, in which an expert (e.g., a human driver) drives a car through a forest from a starting position to an end goal, without colliding with any trees. Assume that the expert has an internal representation of the true constraint, $c^*$, which they use during their planning process to generate demonstrations (Fig. 1b). Here, $c^*$ encodes the location of the trees or, equivalently, the failure set: the set of states which encode having already failed the task. Given expert demonstrations that satisfy $c^*$, we can run an ICL algorithm to obtain an inferred constraint, $\\hat{c}$. Our key question is whether"}, {"title": "2 Problem Setup", "content": "Dynamical System Model. We consider continuous-time dynamical systems described by the ordinary differential equation $\\dot{s} = f(s, a, d, t)$, where $t$ is time, $s \\in S$ is the state, $a \\in A$ is the control input, and $d \\in D$ is the disturbance that accounts for unmodeled dynamics (e.g., wind or friction).\nEnvironment and Task Definition. A task $k$ is defined as a specific objective that our robot needs to complete. For example, in Fig. 1, a mobile robot might be tasked with reaching a specific target pose from a starting position while avoiding environmental obstacles. In this work, we assume this task objective to be implicitly defined using a reward function $r_k : S \\times A \\rightarrow R$. Let $K$ be a set of tasks $\\{k\\}$ with a shared implicit constraint $c^*$ which can be a function of the state and action or of"}, {"title": "3 Background on Inverse Constraint Learning and Safe Control", "content": ""}, {"title": "3.1 Prior Work on Inverse Constraint Learning", "content": "One can think of inverse constraint learning (ICL) as analogous to inverse reinforcement learning (IRL). In IRL, one attempts to learn a reward function that explains the expert agent's behavior Ziebart et al. (2008a;b); Ho & Ermon (2016); Swamy et al. (2021; 2022; 2023); Sapora et al. (2024); Ren et al. (2024); Wu et al. (2024). Similarly, in ICL, one attempts to learn the constraints that an expert agent implicitly satisfies. The main differentiating factors between prior ICL works come from how the problem is formulated (e.g., tabular vs. continuous states), assumptions on the dynamical system (e.g., stochastic or deterministic), and solution algorithms (Liu et al., 2024). Liu et al. (2024) also note that a wide variety of ICL algorithms can be viewed as solving the underlying game multi-task ICL game (MT-ICL) formalized by Kim et al. (2023), which we therefore adopt in for our theoretical analysis. Kim et al. (2023)'s formulation of ICL readily scales to modern deep learning architectures with provable policy performance and safety guarantees, broadening the practical relevance of our theoretical findings. We note that our primary focus is not the development of a new algorithm to solve the ICL problem, but on what these methods actually recover.\nWe now briefly discuss a few notable other prior ICL works. Chou et al. (2020) formulate ICL as an inverse feasibility problem where the state space is discretized and a safe/unsafe label is assigned to each cell in attempt to recover a constraint that is uniformly correct (which can be impractical for settings with high-dimensional state spaces). Scobee & Sastry (2019) adapt the Maximum Entropy IRL (MaxEnt) framework by selecting the constraints which maximize the likelihood of expert demonstrations. This approach was later extended to stochastic models by McPherson et al. (2021) and to continuous dynamics by Stocking et al. (2022). Lindner et al. (2024) define a constraint set through convex combinations of feature expectations from safe demonstrations, each originating from different tasks. This set is utilized to compute a safe policy for a new task by enforcing the policy to lie in the convex hull of the demonstrations. Hugessen et al. (2024) note that, for certain classes of constraint functions, single-task ICL simplifies to IRL, enabling simpler implementation."}, {"title": "3.2 A Game-Theoretic Formulation of Multi-Task Inverse Constraint Learning", "content": "Kim et al. (2023)'s MT-ICL formulates the constraint inference problem as a zero-sum game between a policy player and a constraint player and is based on the observation that we want to recover constraints that forbid highly rewarding behavior that the expert could have taken but chose not to."}, {"title": "3.3 A Brief Overview of Safety-Critical Control", "content": "Safety-critical control (SCC) provides us with a mathematic framework for reasoning about failure in sequential problems. Most critically for our purposes, SCC differentiates between a failure set (the set of states for which failure has already happened) and a backward reachable tube (BRT) (the set of states for which failure is inevitable as we have made a mistake we cannot recover from). Connecting back to ICL, observe that the safe expert demonstrations can never pass through their BRT, as it is impossible to avoid violating the true constraint under their own dynamics. Formally understanding BRTs will help us precisely understand why the constraint we infer with ICL does not generally equal the true constraint $c^*$. In particular, we will show in Section 4 that in the best-case, $\\hat{c}$ approximates the BRT rather than the true failure set. We now provide an overview of BRTS.\nBackward Reachable Tube (BRT). In safe control, the set defined by the true constraint $c^*$ and denoted by $C^* = \\{s \\in S | 1[c^*(s) = \\infty]\\}$ is generally referred to as the failure set and is often denoted by $F$ in the literature. If we know the failure set a priori, $F \\subseteq S$, we can characterize and solve for the safe set, $S^{safe} \\subseteq S$: a subset of states from which if the robot starts, there exists a control action $u$ it can take that guarantees it can avoid states in $F$ despite a worst-case disturbance $d$. Let the maximal safe set and the corresponding minimal unsafe set be:\n$S^{safe} := \\{s_0 \\in S | \\exists \\pi_u; \\forall \\pi_d | \\forall t \\geq 0, \\phi_{u,d}(t) \\notin F\\} $  (2)\n$S^{unsafe} := (S^{safe})^c = BRT(F)$  (3)\nwhere $S$ is the state space, $\\pi_u$ and $\\pi_d$ are respectively the control and disturbance policies, and \"$(.)^c$\" indicates that the set complement of $S^{safe}$. In the safe control community, the unsafe set is often called the Backward Reachable Tube (BRT) of the failure set (i.e., the true constraint) $F$ (Mitchell et al., 2005). In general, obtaining the BRT is computationally challenging but has been studied extensively by the control barrier functions (CBFs) (Ames et al., 2019; Xiao & Belta, 2021) and Hamilton-Jacobi (HJ) reachability (Mitchell et al., 2005; Margellos & Lygeros, 2011) communities. We ground this work in the language of HJ reachability for a few reasons. First, HJ reachability is guaranteed to return the minimal unsafe set \u2013 when studying the best constraint that ICL could ever recover, the BRT obtained via HJ reachability gives us the tightest"}, {"title": "4 What Are We Learning in ICL?", "content": "One might naturally assume that an ICL algorithm would recover the true constraint $c^*$ (e.g. the exact location of the tree, illustrated in Fig. 1b) that the expert optimizes under. However, we now prove that the set $\\hat{C}$, induced by the inferred constraint $\\hat{c}$, is equivalent to the BRT of the failure set, $BRT(F)$, where $F = C^*$. In other words, we prove that constraint inference ultimately learns a dynamics-conditioned unsafe set instead of the dynamics-independent true constraint.\nThroughout this section, we assume we are in the single-task setting ($K = 1$) for simplicity and drop the associated subscript. Let $P(\\cdot) : \\Pi \\rightarrow \\mathbb{R}$ be a function which maps a policy $\\pi$ to some performance measure. For example, in our preceding formulation of multi-task ICL, we had set $P_k(\\pi_k) = J(\\pi_k, r_k)$. We begin by proving that relaxing the failure set to its BRT does not change the set of solutions to a safe control problem. This implies that, from safe expert demonstrations alone, we cannot differentiate between the true failure set and its BRT.\nLemma 4.1. Consider an expert who attempts to avoid the ground-truth failure set $F$ under dynamics $\\dot{s} = f(s, u, d, t)$ while maximizing performance objective $P : \\Pi \\rightarrow \\mathbb{R}$:\n$\\pi^* = \\underset{\\pi \\in \\Pi}{\\operatorname{argmax}} P(\\pi)$   (6)\ns.t. $J(\\pi, 1[\\cdot \\in F]) = 0$.\nAlso consider the relaxed problem below, where the expert avoids the BRT of the failure set $F$:\n$\\pi^\\circ = \\underset{\\pi \\in \\Pi}{\\operatorname{argmax}} P(\\pi)$   (7)\ns.t. $J(\\pi, 1[\\cdot \\in BRT(F)]) = 0$.\nWhere $1[ \\in F]$ and $1[\\cdot \\in BRT(F)]$ are indicator functions that assign the value 1 to states $s \\in F$ and $s \\in BRT(F)$ respectively and the value 0 otherwise. Then, the two problems 6 and 7 have equivalent sets of solutions, i.e.\n$\\pi^* = \\pi^\\circ$.  (8)"}, {"title": "5 Experimental Validation of BRT Recovery", "content": "Our theoretical statements assumed access to a perfect ICL solver. We now empirically demonstrate that even when this assumption is relaxed, we see that $\\hat{c}$ approximates the BRT."}, {"title": "5.1 Dynamical System", "content": "In our experiments, we select a low-dimensional but dynamically-nontrivial system that enables us to effectively validate our theoretical analysis through empirical observation. Specifically, we"}, {"title": "5.2 Constraint Inference Setup", "content": "We use the MT-ICL algorithm developed by Kim et al. (2023). In our setup, task k consists of navigating the robot from a specific start $s_k$ to a goal state $g_k$ without hitting a circular obstacle with a radius of 1, centered at the origin of the environment. This circular obstacle will be the true constraint in the expert demonstrator's mind, $c^*$. We assume the constraint to be a function of only the state $\\hat{c} : s \\rightarrow [-\\infty, \\infty]$. Note that in practice, the output of $\\hat{c}$ is constrained to be in the range [-1,1]. Let $\\mathcal{C}$ be the function class of 3-layer MLPs while $\\Pi$ is the set of actor-critic policies where both actor and critic are 2-layer MLPs. The inner constrained RL loop is solved using a penalty-based constraint handling method where a high negative reward is assigned upon violation of the constraint function $\\hat{c}$. For each model, we train an expert policy using PPO (Schulman et al., 2017) implemented in the Tianshou library (Weng et al., 2022) given perfect knowledge of the environment (i.e., the obstacle location). Note that PPO uses entropy regularization as assumed in section 4. We collect approximately 150 expert demonstrations with different start and target poses to form two training sets, $(\\mathcal{D}_{agile} and \\mathcal{D}_{non-agile})$. Each dataset is then used to train MT-ICL (equation 1) with only access to these demonstrations. All models were trained using a single NVIDIA RTX 4090 GPU."}, {"title": "5.3 BRT Computation", "content": "We solve for the infinite-time avoid BRT using an off-the-shelf solver of the HJI-VI PDE (eq. 4) implemented in JAX (Stanford ASL, 2021). We encode the true circular constraint via the signed distance function to the obstacle: $h(s) := \\{s: ||\\frac{s_x}{s_y}|| \\leq r^2\\}$. We initialize our value function with this signed distance function $V(0, s) = h(s)$ and discretize full the state space $(x, y, \\theta)$ into a grid of size 200 \u00d7 200 \u00d7 200. We run the solver until convergence."}, {"title": "5.4 Results", "content": "We now discuss several sets of experimental results that echo our preceding theory."}, {"title": "5.4.1 ICL Recovers an Approximation of the BRT", "content": "First, we compute the ground truth BRTs for each model by solving the HJB PDE in Eq. 4. Figures 2a and 2b show how each model induces a different BRT, with the BRT growing larger as the"}, {"title": "5.4.2 ICL Can \"Hide\" the BRT When the System is Agile", "content": "Agile systems are commonly used in the existing ICL literature, leading to the impression that the set inferred from constraint $\\hat{c}$ (the set $\\hat{C} = \\{s \\in S | 1[\\hat{c}(s) = \\infty]\\}$ is always equal to the failure set $F = C^*$. However, we note that this equivalence holds only when $BRT(F) \\approx F$\u2014i.e., when the system possesses sufficient control authority to consistently steer away from the unsafe set (e.g. model 1 in Fig. 2). For general dynamics (e.g. model 2 in Fig. 2), $\\hat{C} \\neq F$ when $BRT(F) \\neq F$."}, {"title": "5.4.3 The Constraint Inferred via ICL Doesn't Neccesarily Generalize Across Dynamics", "content": "The fact that ICL approximates the BRT leads to another key observation: the inferred constraint may not generalize effectively across different dynamics. For instance, in our setting, we cannot"}, {"title": "6 Conclusion, Implications, and Future Work", "content": "In this work, we have identified that inverse constrained learning (ICL), in fact, approximates the backward reachable tube (BRT) using expert demonstrations, rather than the true failure set. We now argue that this observation has a positive impact from a computational perspective and a negative impact from a transferability perspective.\nImplications. First, we note that we can add ICL algorithms to the set of computational tools available to us to calculate BRTs, given a dataset of safe demonstrations, without requiring prior knowledge of the true failure set. Computing a BRT is the first step in many downstream safe control synthesis procedures of popular interest. We also note that having access to a BRT approximator can help speed up policy search, as the set of policies that do not violate the constraint is a subset of the full policy space. Thus, a statistical method should take fewer samples to learn the (safe) optimal policy with this knowledge. However, any BRT (inferred by ICL or otherwise) is dependent on the dynamics of the system and hence cannot be easily used to learn policies on different systems without care. In this sense, learning a BRT rather than the failure set is a double-edged sword.\nWe also note that the above observations are somewhat surprising from the perspective of inverse reinforcement learning, where one of the key arguments for learning a reward function is transfer-ability across problems (Ng et al., 2000; Swamy et al., 2023; Sapora et al., 2024). However, such transfer arguments often implicitly assume access to a set of higher-level features which are independent of the system's dynamics on top of which rewards are learned, rather than the raw state space as used in the preceding experiments for learning constraints. Thus, another approach to explore is whether the transferability of constraints would increase if we learn constraints on top of a set of features which are 1) designed to be dynamics-agnostic and 2) for which the target system is able to match the behavior of the expert system, as is common in IRL practice (Ziebart et al., 2008a).\nFuture Work. Regardless, an interesting direction for future research involves recovering the true constraint (i.e., the failure set $F$) using constraints that were learned for different systems with varying dynamics. This process is synonymous to removing the dependence of the constraint on the dynamics by integrating over (i.e., marginalizing) the dynamical variables. This could allow disentangling the dynamics and semantics parts of the constraint, allowing better generalization and faster policy search independent of system dynamics. A potential approach to doing so would be to collect expert demonstrations under a variety of dynamics, learn a constraint for each, and then return an aggregate constraint that is the minimum of the learned constraints, implicitly computing an intersection of the BRTs. Such an intersection would approximate the true failure set."}]}