{"title": "AGENTPEERTALK: Empowering Students through Agentic-AI-Driven Discernment of Bullying and Joking in Peer Interactions in Schools", "authors": ["Aditya Paul", "Chi Lok Yu", "Eva Adelina Susanto", "Nicholas Wai Long Lau", "Gwenyth Isobel Meadows"], "abstract": "Addressing school bullying effectively and promptly is crucial for the mental health of students. This study examined the potential of large language models (LLMs) to empower students by discerning between bullying and joking in school peer interactions. We employed ChatGPT-4, Gemini 1.5 Pro, and Claude 3 Opus, evaluating their effectiveness through human review. Our results revealed that not all LLMs were suitable for an agentic approach, with ChatGPT-4 showing the most promise. We observed variations in LLM outputs, possibly influenced by political overcorrectness, context window limitations, and pre-existing bias in their training data. ChatGPT-4 excelled in context-specific accuracy after implementing the agentic approach, highlighting its potential to provide continuous, real-time support to vulnerable students. This study underlines the significant social impact of using agentic AI in educational settings, offering a new avenue for reducing the negative consequences of bullying and enhancing student well-being.", "sections": [{"title": "1 Introduction", "content": "Regardless of each school's and parents' efforts to address bullying, bullying among students is inevitable, and bullying can extend beyond the perimeter of campuses to intrude into students' private lives [1,2]. Despite many resources and support programs for students, the effectiveness of reaching students and resonating with the developing psychological status of teenage students during high school may vary [1]. While students have the option to report to teachers or parents, they do not always resort to such methods, but instead, sometimes internalize the psychological stress of bullying due to fear of the consequences of such reporting, the unavailability of support persons or resources, and the often blurred nature of bullying versus joking in students' lives [3,4]. Furthermore, the quality and communication skills of support persons and resources vary, often focusing on de-escalating situations without necessarily addressing the legal and ethical domains of the issues [5].\nAdvanced LLMs that are multimodal have been engaging in meaningful conversations with many people [6-11], prompting our exploration of whether LLMs can act as a 24/7 support peer for vulnerable students. However, most LLMs are trained on general data, which does not consider the localized legislative, cultural, and personal context of school bullying, potentially resulting in inaccurate or impersonal advice. Therefore, the aim of this study is to determine whether simulating the agentic approach of LLMs can provide better, more legally and ethically sound advice, instead"}, {"title": "2 Theoretical Foundation", "content": "Evaluating the effectiveness of anti-bullying measures necessitates a multi-layered approach that incorporates legislative, cultural, and personal perspectives. A comprehensive evaluation framework should integrate the following three layers, to ensure that interventions are legally sound, culturally appropriate, and personally impactful:\n1. Legislative Layer: Ensures that any intervention aligns with existing laws and regulations, safeguarding the legal rights of all parties involved. Compliance with local, national, and international laws is crucial to maintain the integrity and legality of anti-bullying efforts.\n2. Cultural Layer: Addresses the social norms and values that influence the perception and impact of bullying. Different cultures have varied definitions and thresholds for what constitutes bullying, making it essential to consider cultural contexts to ensure the relevance and acceptability of interventions.\n3. Personal Layer: Focuses on the individual experiences and psychological states of the students involved. Personal factors such as past experiences, mental health status, and personal relationships play a significant role in how bullying is perceived and experienced."}, {"title": "2.2 Agentic AI Solutions", "content": "The agentic AI approach leverages the proactive capabilities of advanced AI systems to autonomously provide support and guidance, addressing the inadequacies in traditional anti-bullying measures, by offering continuous, real-time support to students, bridging the gap created by the unavailability of human support resources (Fig. 1). LLMs adapt their responses based on legislative, cultural, and personal contexts, ensuring that the advice provided is relevant and appropriate. The proactive nature of agentic AI enables it to identify and address issues before they escalate, offering timely interventions to prevent the negative consequences of bullying. Additionally, the ability of LLMs to integrate ethical considerations into their responses enhances their reliability and trustworthiness, making them valuable tools for students seeking confidential and non-judgmental support. The agentic AI approach provides a robust framework for addressing the complexities of bullying,"}, {"title": "3 Results", "content": "The comparison of the commercial LLMs revealed significant differences in their performance across various scenarios. ChatGPT-4 exhibited a noticeable improvement in accuracy after implementing the agentic approach. For instance, in the first scenario (bullying related to body image), its score improved by 0.4 points. In contrast, Gemini 1.5 Pro and Claude 3 Opus showed mixed results, with some scores decreasing. In some cases, LLMs failed to produce any response after the agentic intervention, which we labeled as 0 (FAIL), because either the context window was too short for legislation we tried to import, or LLMs thought we violated their acceptable usage policies by discussing topics they disliked. The evaluation scores, independently graded by three human reviewers (similar to the NeurIPS peer review process), provided detailed insights into each LLM's performance, as summarized in Table 3. The results indicated that only ChatGPT-4 benefited from our simulated agentic approach, while the other two models (Gemini 1.5 Pro and Claude 3 Opus) experienced a decline in performance or failed to produce results."}, {"title": "3.2 Statistical Analysis", "content": "The statistical analysis involved applying various statistical tests to evaluate the significance of the differences observed in the performance of the LLMs. An ANOVA test was performed to determine if there were statistically significant differences between the models' scores across the five scenarios. The results indicated a p-value of 0.0041, confirming significant differences in performance. Post-hoc analyses using Tukey's HSD test revealed that ChatGPT-4 and Google Gemini 1.5 differed"}, {"title": "4 Discussion", "content": "The study provided critical insights into the capabilities and limitations of LLMs in discerning bullying from joking in school environments. One significant finding was the ability of agentic AI to offer real-time, contextually relevant support to students, demonstrating that advanced LLMs could bridge the gap left by traditional support mechanisms. The comparison of ChatGPT-4, Google Gemini 1.5, and Anthropic Claude 3 highlighted the varying strengths of each model, emphasizing that not all LLMs are suitable to extend with agentic abilities or may require more customized agentic approaches. Apart from the expected LLM refusal to change stance on some topics, our study identified three common reasons for LLM failure during text generation (i.e., failure to generate any text): (1) political overcorrectness, resulting in refusal to even comment on certain topics regardless of our intentions; (2) limitations of the context window against very long, comprehensive, and excessively complicated legislations; and (3) cultural values and differences between trained data and the local environment. Our findings emphasized the importance of continuous fine-tuning and adaptation of LLMs to ensure their responses remain aligned with evolving legal and cultural norms, enhancing their reliability and applicability in real-world scenarios."}, {"title": "4.2 Limitations of the Study", "content": "Despite the valuable insights gained, the study had several limitations. The primary limitation was the reliance on pre-existing datasets for training and evaluation, done by LLM vendors in a blackbox manner, which may not fully capture the diversity and complexity of real-world interactions among students. The curated case studies, while comprehensive, might still fail to encompass all possible scenarios of bullying and joking, potentially limiting the generalizability of the findings. Another limitation was the inherent bias in the external data we supplied to LLMs as part of the prompts, which could influence the accuracy and fairness of the AI's responses. Additionally, the evaluation framework relied on subjective assessments by us (3 human reviewers), a very small sample size, which introduced the possibility of variability in the scoring process. The study did not account for the long-term impact of AI interventions on students, which would require longitudinal studies to fully understand the effectiveness and potential unintended consequences of using LLMs as support tools."}, {"title": "5 Conclusion", "content": "The research highlighted the significant potential of LLMs in addressing the complex issue of bullying within school environments. The comparative analysis of ChatGPT-4, Google Gemini 1.5, and Anthropic Claude 3 illuminated their strengths and limitations in providing contextually relevant and ethically sound support to students. While each LLM exhibited distinct advantages, such as ChatGPT-4's legal accuracy, Google Gemini 1.5's cultural relevance, and Anthropic Claude 3's ethical alignment, there remains room for improvement in integrating legal, cultural, and personal contexts into their responses. Still, the social impact of this study is that it offers a new avenue for providing continuous, real-time support to vulnerable students, potentially reducing the negative consequences of bullying. The potential of agentic AI to offer real-time, contextually appropriate support represents a significant advancement in educational settings. As LLMs and generative AI continue to evolve, their ability to offer personalized, contextually appropriate support will only strengthen. Future developments in generative AI, such as multimodality, Mixture of Experts (MoE), and constitutional AI, hold promise for further improving the outcomes of this research. Multimodality will enable AI to understand and process multiple forms of input, enhancing its ability to interpret complex social interactions. MoE can improve the efficiency and accuracy of AI responses by leveraging specialized expert models. Constitutional AI can ensure that AI systems adhere to ethical guidelines, further enhancing their reliability and trustworthiness. With continued refinement, LLMs can play a transformative role in enhancing student support systems, ultimately contributing to safer, more supportive, and inclusive school environments. The findings lay the groundwork for optimizing the deployment of AI in educational contexts, ensuring all students have access to the support they need to thrive both academically and socially."}, {"title": "A Literature Review", "content": "Agentic AI in knowledge retrieval and distillation has demonstrated significant advancements, particu- larly in its ability to autonomously identify and extract relevant information from vast datasets [8,12]. LLMs, through sophisticated algorithms, have achieved impressive accuracy in understanding and responding to diverse queries, making them valuable tools across various domains [13-15]. The proactive nature of LLMs allows for real-time information filtering, ensuring that users receive contextually relevant and up-to-date responses [9, 16]. Additionally, their capacity to distill complex information into digestible formats has proven beneficial for users requiring quick and accurate insights [17, 18]. The ability of LLMs to adapt their knowledge base in response to new informa- tion enhances their reliability and effectiveness in dynamic environments [19, 20]. Despite such advancements, challenges remain in ensuring that the information retrieved and distilled by LLMs is accurate, unbiased, culturally appropriate, and ethically sound, necessitating ongoing research and development [19, 21, 22]."}, {"title": "A.2 Bias, Hallucination, and Evaluation Benchmarks", "content": "Bias and hallucination in LLM outputs poses significant challenges, impacting the reliability and fairness of AI-driven decisions [23, 24]. LLMs have shown a tendency to reflect and even amplify existing biases present in their training data, leading to ethically problematic outcomes [25-27]. Efforts to mitigate such biases have included the development of more inclusive and representa- tive datasets, as well as algorithmic adjustments aimed at reducing discriminatory outputs [28]. Hallucination, where models generate plausible-sounding but incorrect or nonsensical information, further complicates their deployment in critical applications [21,29]. Establishing rigorous evaluation benchmarks has been crucial in assessing the performance and trustworthiness of LLMs, particularly in sensitive contexts [30]. Comprehensive LLM benchmarks involve not only accuracy metrics but also measures of ethical alignment and bias reduction, to help in identifying specific areas where LLMs require improvements, guiding subsequent refinements [31,32]. The complexity of developing reliable and ethical AI systems is demonstrated by the need to reduce bias, minimize hallucinations, and maintain high performance, ensuring that LLMs provide dependable support in applications involving vulnerable populations such as students facing bullying [33, 34]."}, {"title": "B Curation of Evaluation Case Studies", "content": "Creating evaluation case studies (Algorithm 1) that encompass scenarios of both bullying and joking requires careful consideration of real-world interactions among students. Due to time constraints and the NeurIPS submission deadline, we created five specific cases. The process began with collecting a wide range of documented instances of peer interactions from various sources, including school records, anonymized social media posts, and anecdotal reports. These instances were then reviewed and categorized based on specific criteria, such as the presence of aggressive language, power dynamics, and the context of the interaction. Each case study was designed to represent a plausible situation that students might encounter, ensuring a mix of clear-cut and ambiguous cases to test the AI's discernment capabilities. Scenarios were crafted to reflect different cultural and personal contexts, providing a comprehensive evaluation framework. The curated case studies served as the basis for testing the LLMs, enabling an assessment of their ability to differentiate between bullying and joking accurately."}, {"title": "B.1 Evaluation Criteria (Marking Rubric)", "content": "The evaluation of AI-generated responses involved a detailed marking rubric that assessed the helpfulness of the answers across legal, ethical, and psychological domains. The criteria (Table 2) were designed to ensure that the AI's outputs were not only accurate but also contextually appropriate and beneficial to the users. Legal aspects focused on whether the advice aligned with relevant laws and regulations, ensuring that students were guided in compliance with their rights and responsibilities. Ethical considerations examined the moral implications of the advice, evaluating the fairness and impartiality of the responses. Psychological criteria assessed the potential impact of the advice on the student's mental well-being, ensuring that the AI provided supportive and constructive guidance. Each response generated by the AI was reviewed by three independent evaluators, who rated the quality of the answers based on the marking rubric. The overall average mark for each response was then calculated to provide a comprehensive evaluation of the AI's performance. This rigorous evaluation process mimicked the peer-review standards used in academic conferences such as NeurIPS, ensuring a high level of scrutiny and reliability in the assessment of the AI's capabilities."}]}