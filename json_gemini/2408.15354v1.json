{"title": "What Is Required for Empathic AI? It Depends, and Why That Matters for AI\nDevelopers and Users", "authors": ["Jana Schaich Borg", "Hannah Read"], "abstract": "Interest is growing in artificial empathy, but so is confusion\nabout what artificial empathy is or needs to be. This confu-\nsion makes it challenging to navigate the technical and ethical\nissues that accompany empathic AI development. Here, we\noutline a framework for thinking about empathic AI based on\nthe premise that different constellations of capabilities asso-\nciated with empathy are important for different empathic AI\napplications. We describe distinctions of capabilities that we\nargue belong under the empathy umbrella, and show how\nthree medical empathic AI use cases require different sets of\nthese capabilities. We conclude by discussing why apprecia-\ntion of the diverse capabilities under the empathy umbrella is\nimportant for both AI creators and users.", "sections": [{"title": "Introduction", "content": "According to one author, \u201cIn order to achieve artificial gen-\neral intelligence, an AI must use empathy to make deci-\nsions", "Artificial empathy": "the upgrade AI needs to speak to\nconsumers\u201d (Bhansali 2022) and \"Why We Need Empathy\nIn AI\" (Washington 2022). Some have even suggested em-\npathic AI may offer benefits over human empathy, such as\nbeing less resource-limited and potentially less biased to-\nwards ingroups (Inzlicht et al. 2023). This interest, in turn,\nraises the fundamental question: what capabilities does an\nAl system need to have to be considered empathic? Our goal\nhere is to provide some guidance for how to think about that\nquestion.\nMany different kinds of AI systems have been called \"em-\npathic\". Audi's \u201cempathic mobility partner\u201d, AI:Me, per-\nsonalizes the autonomous driving experience for passengers\nand signals to others on the road what the car \"intends\" to do.\nHumeAI offers \"The empathic AI toolkit for researchers and\ndevelopers\", including tools for measuring emotions from\nsound or videos. Siena, which offers autonomous chat-based\ncustomer service agents, advertises that it provides \"hu-\nman empathy in every interaction\". Researchers, too, have\nto navigate a dizzying number of conceptions of empathy\n(Hall and Schwartz 2022). As one author expressed, \"there\nare perhaps as many definitions as there are authors in the\nfield\" (Cuff et al. 2016). Yet, despite this variety, researchers\nfrequently have very strong and conflicting opinions about\nwhich empathy definition is the \u201ctrue\u201d or \"right\" one, some-\ntimes motivating them to respond to empathic AI efforts\nwith some version of \u201cwell, that's not real empathy"}, {"title": "Three Empathic AI Use Cases in Medicine", "content": "Medical question answerers. The first empathic AI applica-\ntion we consider is AI that responds to medical questions\nvirtually. People often use forums like iCliniq, Lybrate,\nFindaTopDoc, JustAnswer, and others to ask doctors\nmedical questions without having to schedule in-person\nappointments. When doctors provide answers too abruptly\nor dispassionately on these forums, users sometimes be-"}, {"title": "Something is Different about these Empathic AIs,\nbut What?", "content": "We posit that the aforementioned three empathic AI use\ncases require different constellations of abilities. Even so,\nAls functioning successfully in these use cases would be\ndisplaying empathy by at least one of the plethora of defini-\ntions proposed by reputable researchers (Hall and Schwartz\n2019). There are valid research questions related to whether\na specific AI system meets a chosen theoretical definition of\nwhat human empathy is believed to be. For many interested\nin building empathic AI systems, though, the more impor-\ntant task is to clarify what constellation of specific abilities\nfrom within the empathy umbrella the empathic system one\nwants to build needs to have. In the next section, we offer\ndistinctions drawn from a wide variety of disciplines that\nare useful for thinking about these empathic abilities. In all\ndiscussions, we use \u201cempathizer\u201d to refer to the agent who is\nsupposed to have empathy and \"target\" to refer to the agent\nthe empathizer has empathy for."}, {"title": "\u201cFine cuts\u201d of Empathy: Capabilities and\nDistinctions under the Empathy Umbrella", "content": "To explain important distinctions or \u201cfine cuts", "broader cut\" concepts and terminology. Most (but\nnot all) accounts of empathy differentiate between three re-\nlated, but separate, phenomena: cognitive empathy (or un-\nderstanding of another's experience, preferences, or knowl-\nedge), affective empathy (feelings or emotional experiences\nlinked to another's experience, preferences, or knowledge),\nand motivated empathy (other-oriented concern congruent\nwith another's perceived welfare, sometimes called com-\npassion or sympathy). Some of the fodder for distinguish-\ning cognitive empathy from affective empathy comes from\nmedicine, where it has been shown that people can have\ndeficits in understanding what someone knows or believes\nwhile maintaining appropriate emotional responses to some-\none else's emotions, and visa versa (Fletcher-Watson and\nBird 2020; Winter et al. 2017). In addition, neuroscience\nstudies have shown that different brain networks are in-\nvolved in cognitive empathy and affective empathy, support-\ning the idea that these phenomena are distinguishable and\ncan be impacted independently (Stietz et al. 2019). Compas-\nsion and/or sympathy\u2014which are also contested terms, but\ntypically refer to motivations to alleviate others' suffering\u2014\nare sometimes (but not always) separated out from cognitive\nand affective empathy, because the thoughts and emotions\nwe have in response to somebody else's situation don't al-\nways lead us to help that person or want to do so.\nAnother concept incorporated into many accounts of em-\npathy is mirroring. The general idea is that our brains and\nbodies might need to have a representation of another per-\nson's knowledge, thoughts, preferences, or feelings that is\nidentical to what we ourselves would think, perceive, or feel\nin that same situation in order to understand, feel, or re-\nspond acceptably to what that other person is going through.\nSome philosophical and psychological accounts of empathy\npostulated the presence of mirroring mechanisms long ago\n(Zahavi 2010), but enthusiasm for these accounts seemed\nto take off more notably when evidence emerged that so-\ncalled \"mirror neurons\" in a monkey's motor system fire\nboth when monkeys see another monkey reach and when\nmonkeys reach themselves (Iacoboni 2009). Perhaps relat-\nedly, activity increases in overlapping brain regions when we\nobserve, perceive, or believe someone else is having a feel-\ning and have those feelings ourselves, especially when the\nfeelings involve physical pain (Singer and Steinbeis 2009).\nSome accounts (like the perception-action model of empa-\nthy) argue that mirroring is at the core of all empathic phe-\nnomena (Preston and De Waal 2002). Other accounts just\nacknowledge that mirroring is either one of the capabilities\nthat belongs in the empathy umbrella, or a mechanism that\ncontributes to capabilities under the empathy umbrella (e.g.\nYamamoto (2017)).\nThe notions of cognitive empathy, affective empa-\nthy, motivated empathy, and mirroring are prevalent in\nmost empathy accounts, as noted by syntheses of the\nempathy literature (Hall and Schwartz 2019; Eklund\nand Meranius 2021). However, additional distinctions\nand requirements emerge from the plethora of empathy\naccounts as well, even though some are acknowledged\nmuch less consistently across disciplines. These less-\ndiscussed considerations are important to Al creators\nthinking about what their AI systems must achieve. We will\ndiscuss these \"fine cuts\u201d of empathy next (see also Table 1).\nInformation types and how they need to be known. The\nfirst way of distinguishing empathic capabilities is through\nthe type of information the empathizer is supposed to glean\nabout the target. The empathizer might need to know the\ntarget's perceptions (row 1 in Table 1), cognitive phenom-\nena (which include beliefs, thoughts, and knowledge; row\n2), feelings (also referred to as emotions for our purposes;\nrow 3), or some combination of all three. The empathizer\nmay be expected, or even required, to glean this informa-\ntion through specific combinations of their own perceptions,\ncognitive phenomena, or feelings (rows 4, 5, 6).\nConsider, for example, the Sally-Ann test of cognitive em-\npathy, sometimes referred to as \"theory of mind\" (Baron-\nCohen, Leslie, and Frith 1985) that has been incorporated\ninto recent benchmark tests for large language models (Le,\nBoureau, and Nickel 2019) (note that ": "heory of mind"}, {"title": "What Empathic Capabilities Do AIs Need?", "content": "The empathic capabilities needed by question-answerers,\ncare-assistants, and care-providers would likely have both\nsimilarities and differences. All three AI systems, by defini-\ntion, must take appropriate actions on behalf of their target\n(row 14; answering questions in a personalized way counts\nas such an action) and have certain impacts on their target\n(row 15) in order for consumers to continue using them. To\ndirect the appropriate actions, all the applications could ben-\nefit from accurate (row 8) knowledge about the target's feel-\nings (row 3) and beliefs (row 2), even if such knowledge is\nhelpful without being necessary in the case of the question-\nanswerer.\nIn some scenarios it could be useful for the care-assistants\nand care providers to have knowledge of the target's per-\nceptions as well (row 1). If an AI care-assistant needs to\nhelp a disoriented patient find their way home safely while\nwalking around the block, for example, the AI would benefit\nfrom knowing whether the target is noticing stop signs, traf-\nfic lights, and traffic, and whether the target can hear the AI's\ninstructions. Overall, the more any medical AI knows and\nappropriately addresses specific concerns or thoughts targets\nhave, the more useful the AI is likely to be (as long as do-\ning so doesn't creep users out). It is plausible that effective\nAI care-assistants or care-providers could be trained using\nblack-box algorithms that call into question whether the AI\nreally has explicit \u201cknowledge\u201d of targets' feelings, beliefs,\nor perceptions, but we will put that issue aside for now.\nIt seems sufficient for AI question-answerers and care as-\nsistants to obtain knowledge about the targets' beliefs, feel-\nings, and possibly perceptions solely through beliefs (in the\nform of statistical models) about the world (row 5), but many\n(if not all) AI care-assistants may also need to obtain this in-\nformation through their own artificial perceptions (row 4),\nsuch as through video feeds or sensor readings of the target.\nAccording to the criteria established earlier, AI care-takers\nwould have to meet an even higher standard, though. The\nexpectation of care-takers as they have been described pre-\nviously and as we are conceiving of them here is that they\nneed to glean knowledge about the target through their own\n\"real and appropriate feelings\" that are shared, at least to\nsome degree, with the target (row 6).\nThe depth and degree of accuracy (row 8) required by\neach AI application could plausibly differ. It could be suf-\nficient for Al question-answerers to correctly predict the va-\nlence of the question-asker's feelings or beliefs in order to\ncraft acceptable responses for that situation, rather than cor-\nrectly predict exactly what the question-asker is thinking or\nfeeling. Care-assistants, on the other hand, could need to\nhave more detailed and accurate knowledge of a target's be-\nliefs and feelings to help the target with challenges like find-\ning their way home, agreeing to take medications they are\nreluctant to take, or providing entertainment. AI care-takers"}, {"title": "Implications for AI Creators and Users", "content": "Table 1 and the discussion in the last section should make\nit clear that \"Empathic AI\" need not, and likely should not,\nrefer to one thing with a singular set of abilities. Instead,\nwe posit that it will be more fruitful for \"Empathic AI\"\nto be used as a term that refers to agents with different\ncombinations of empathy-related capabilities selected to\naddress the needs of a particular use case. Rather than try to\nbuild a system that matches a specific empathy definition,\nwe recommend that AI creators commit to thinking deeply\nabout which of the components discussed earlier are re-\nquired, optional, or incompatible with the type of empathy\nneeded for their chosen use case, and collect evidence\nwhen necessary to support those conclusions. Doing so will\nhelp both empathic AI creators and users in many ways.\nKnowing what to build. Some groups are trying to create\ngeneral AI that can teach itself how to do anything it needs\nto do, and may hope that empathy is one of the things Als\nlearn or develop along the way. Other empathic AI systems\nare examples of narrow AI that are used to tackle individual\ntasks close to those their creators explicitly and intentionally\ndesigned them to achieve. Within narrow Al systems, it is\ncommon for creators to have to design different approaches\nfor managing each distinct subtask or function the AI needs\nto perform, and then combine those approaches into one in-\ntegrated system. Given that each approach requires signifi-\ncant investment, the systems that are ultimately built are less\nlikely to have specific empathic capabilities or constraints if\nsystem creators don't intentionally try to build them in.\nAs an example, many artificial empathy systems that\nhave been designed to date analyze real-time video feeds\nof a human target, try to identify what emotion the human\ntarget is feeling based on their facial expressions, and\nthen implement a response the AI has learned is most\nappropriate for a target's identified emotion (e.g. Xiao\net al. 2016). These systems, as currently implemented,\ntypically have no ability to predict what a human target\nsees or knows from their own sensory input. Therefore,\nthese AI systems currently have no way of representing the\ncritical perceptual information that is needed to pass the\nSally-Ann test described earlier. They also have no way\nof representing or understanding the thoughts the target is\nhaving that cause the emotion the target is expressing. That\nmeans these empathic AI systems would not be considered\nempathic by accounts that require the target to have beliefs\nabout the target's perceptions and beliefs, and wouldn't\nmeet the requirements of the AI care assistants we discussed\nearlier. Nonetheless, it is likely technically feasible for the\nsystems to be adapted so they could do so. There is nothing\nstopping Al creators from making these adaptations in\nprinciple, other than that they may not have realized these\ncriteria are important for some definitions and applications\nof empathy that they have not yet had reason to work on.\nAvoid wasting time on unnecessary features. Al creators\nmay turn to the empathy literature to design requirements\nfor an empathic AI system, and unknowingly choose an em-\npathy definition that is not well-suited to their use case, lead-\ning them to waste time and resources building unnecessary\nfeatures into their system. For instance, although mirroring\nis likely an important mechanism of some types of human\nempathy and its evolutionary precursors, Als do not need to\nuse the same mirroring mechanisms to be able to accurately"}, {"title": "Conclusion", "content": "Artificial empathy has many exciting uses and has the po-\ntential to overcome many of the disadvantages of human\nempathy by virtue of being free of the computational con-\nstraints and ingroup biases of human empathy (Inzlicht et al.\n2023). It also has the potential to harm individuals and so-\nciety more generally if developed unthoughtfully and with-"}]}