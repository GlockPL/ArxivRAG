{"title": "What Is Required for Empathic AI? It Depends, and Why That Matters for AI Developers and Users", "authors": ["Jana Schaich Borg", "Hannah Read"], "abstract": "Interest is growing in artificial empathy, but so is confusion about what artificial empathy is or needs to be. This confusion makes it challenging to navigate the technical and ethical issues that accompany empathic AI development. Here, we outline a framework for thinking about empathic AI based on the premise that different constellations of capabilities associated with empathy are important for different empathic AI applications. We describe distinctions of capabilities that we argue belong under the empathy umbrella, and show how three medical empathic AI use cases require different sets of these capabilities. We conclude by discussing why appreciation of the diverse capabilities under the empathy umbrella is important for both AI creators and users.", "sections": [{"title": "Introduction", "content": "According to one author, \u201cIn order to achieve artificial general intelligence, an AI must use empathy to make decisions\" (Wu 2019). Indeed, interest in empathic AI systems is growing, as indicated by recent publication titles such as \u201cArtificial empathy: the upgrade AI needs to speak to consumers\u201d (Bhansali 2022) and \"Why We Need Empathy In AI\" (Washington 2022). Some have even suggested empathic AI may offer benefits over human empathy, such as being less resource-limited and potentially less biased towards ingroups (Inzlicht et al. 2023). This interest, in turn, raises the fundamental question: what capabilities does an Al system need to have to be considered empathic? Our goal here is to provide some guidance for how to think about that question.\nMany different kinds of AI systems have been called \"empathic\". Audi's \u201cempathic mobility partner\u201d, AI:Me, personalizes the autonomous driving experience for passengers and signals to others on the road what the car \"intends\" to do. HumeAI offers \"The empathic AI toolkit for researchers and developers\", including tools for measuring emotions from sound or videos. Siena, which offers autonomous chat-based customer service agents, advertises that it provides \"human empathy in every interaction\". Researchers, too, have to navigate a dizzying number of conceptions of empathy (Hall and Schwartz 2022). As one author expressed, \"there are perhaps as many definitions as there are authors in the field\" (Cuff et al. 2016). Yet, despite this variety, researchers frequently have very strong and conflicting opinions about which empathy definition is the \u201ctrue\u201d or \"right\" one, sometimes motivating them to respond to empathic AI efforts with some version of \u201cwell, that's not real empathy\u201d (McStay 2023; Coplan 2011). The result is general confusion on the part of both Al creators and AI users.\nHere, we outline a framework for thinking about empathic AI that we believe will facilitate development of useful empathic systems, make it easier for interdisciplinary researchers to test and audit them, and support practices that minimize ethical harms that occur when the nature of an empathic system is misunderstood. Rather than focus on what \"true\" empathy is, we argue that different constellations of capabilities associated with empathy are important for different applications, and AI creators must think carefully about and perhaps collect evidence about which of these capabilities will be necessary, helpful, or even counterproductive for the specific empathic AI use cases they are working on.\nTo introduce our framework and motivate our recommendations, we begin by discussing three empathic AI use cases in medicine, a domain where enthusiasm for empathic AI has been growing. Next, we describe distinct capabilities that have been recognized under the larger empathy umbrella, draw attention to ways the umbrella is both wider and more fine-grained than previous review papers have emphasized, and use those distinctions to illustrate how the empathic abilities needed for each medical empathic AI use case likely differ. Finally, we discuss why appreciating these \"fine cuts\" of empathy is important for both Al creators and users."}, {"title": "Three Empathic AI Use Cases in Medicine", "content": "Medical question answerers. The first empathic AI application we consider is AI that responds to medical questions virtually. People often use forums like iCliniq, Lybrate, FindaTopDoc, JustAnswer, and others to ask doctors medical questions without having to schedule in-person appointments. When doctors provide answers too abruptly or dispassionately on these forums, users sometimes become too fearful or ashamed to seek additional medical help (Mok and Brueck 2023). Answers conveyed with empathy, on the other hand, help patients feel comfortable continuing to ask questions, and may make it more likely that patients ultimately follow the medical advice they find in the forums. The goal of empathic medical question answerer Als is to give answers to patients in a way that maximizes patients' likelihood of being receptive to the information presented, following relevant advice, and being willing to ask questions again in the future.\nAl care assistants. The second application is an assistant to the elderly, cognitively impaired, or disabled. These AI care assistants are supposed to help patients with things they need to do, provide reminders, give reassurance and encouragement, help patients navigate dangerous situations, and perhaps provide some sorts of company, entertainment, or mental stimulation. For example, ElliQ is a robot that has been used by the state of New York to remind seniors to take their medications, provide wellness suggestions, and provide \"proactive and empathetic\" conversations (Muoio 2022). Users will most likely not use these kinds of AI products if doing so makes them feel worse than they already do or if the Als are not tailored to the specific issues the users need help with. In fact, using Als in such cases could actually be dangerous and lead users to engage in physical harm to themselves or others (as has been alleged in the case of one Belgian man who committed suicide after chatting with an AI chatbot; Xiang 2019) or ignore critical safety advice. Even AIs that listen in an emotionally neutral way can make users feel insignificant (Halpern 2001). So the role of empathy in AI care assistants, as we are considering them here, is to convey sensitivity to users' unique needs, beliefs, and vulnerabilities, and allow the Als to interact with users in a way that feels sufficiently comfortable to the users, given the vulnerable situation many users will be in.\nAl care providers. The third application is an entity that is meant to provide care for patients. This application is different than the AI care assistant, because in this case, it is deemed essential that the patient feels truly cared for, not just aided or assisted. Mental health therapy and long-term treatment for chronic illness are two applications where feeling cared for correlates with better health outcomes and is believed to be critical (Vitinius et al. 2018). In such settings, it has been argued that patients must perceive that the care provider engages with, attends to, understands, and respects their experience, and that the care provider is genuinely motivated to help them because they feel and think the patient's well-being has intrinsic value and that the patient deserves dignity (Montemayor, Halpern, and Fairweather 2022; Perry and Shamay-Tsoory 2013; Portacolone et al. 2020). Some argue a patient must sense that the caretaker has real and appropriate feelings in response to the patient's joy and suffering to achieve the documented medical benefits. Simply being able to label and understand the patient's feelings is not enough because care involves a \u201csharing of emotional feeling and connecting with the patient at an emotional level\" (Jeffrey 2016). The goal of empathic AI care providers is to manifest the same types of unique positive health impacts human medical providers that are perceived to \"care about their patient\u201d have, without manipulating, misleading, or disrespecting patients. Even if it is reasonable to question whether patients truly need to feel cared for to achieve those desired medical benefits, for our purposes here, AI care provider creators do want patients to feel their AI exhibits the qualities described above."}, {"title": "Something is Different about these Empathic AIs, but What?", "content": "We posit that the aforementioned three empathic AI use cases require different constellations of abilities. Even so, Als functioning successfully in these use cases would be displaying empathy by at least one of the plethora of definitions proposed by reputable researchers (Hall and Schwartz 2019). There are valid research questions related to whether a specific AI system meets a chosen theoretical definition of what human empathy is believed to be. For many interested in building empathic AI systems, though, the more important task is to clarify what constellation of specific abilities from within the empathy umbrella the empathic system one wants to build needs to have. In the next section, we offer distinctions drawn from a wide variety of disciplines that are useful for thinking about these empathic abilities. In all discussions, we use \u201cempathizer\u201d to refer to the agent who is supposed to have empathy and \"target\" to refer to the agent the empathizer has empathy for."}, {"title": "\u201cFine cuts\u201d of Empathy: Capabilities and Distinctions under the Empathy Umbrella", "content": "To explain important distinctions or \u201cfine cuts\u201d (Blair 2008) of empathic phenomena efficiently, it is helpful to draw upon some \"broader cut\" concepts and terminology. Most (but not all) accounts of empathy differentiate between three related, but separate, phenomena: cognitive empathy (or understanding of another's experience, preferences, or knowledge), affective empathy (feelings or emotional experiences linked to another's experience, preferences, or knowledge), and motivated empathy (other-oriented concern congruent with another's perceived welfare, sometimes called compassion or sympathy). Some of the fodder for distinguishing cognitive empathy from affective empathy comes from medicine, where it has been shown that people can have deficits in understanding what someone knows or believes while maintaining appropriate emotional responses to someone else's emotions, and visa versa (Fletcher-Watson and Bird 2020; Winter et al. 2017). In addition, neuroscience studies have shown that different brain networks are involved in cognitive empathy and affective empathy, supporting the idea that these phenomena are distinguishable and can be impacted independently (Stietz et al. 2019). Compassion and/or sympathy\u2014which are also contested terms, but typically refer to motivations to alleviate others' suffering\u2014are sometimes (but not always) separated out from cognitive and affective empathy, because the thoughts and emotions we have in response to somebody else's situation don't always lead us to help that person or want to do so.\nAnother concept incorporated into many accounts of empathy is mirroring. The general idea is that our brains and bodies might need to have a representation of another person's knowledge, thoughts, preferences, or feelings that is identical to what we ourselves would think, perceive, or feel in that same situation in order to understand, feel, or respond acceptably to what that other person is going through. Some philosophical and psychological accounts of empathy postulated the presence of mirroring mechanisms long ago (Zahavi 2010), but enthusiasm for these accounts seemed to take off more notably when evidence emerged that so-called \"mirror neurons\" in a monkey's motor system fire both when monkeys see another monkey reach and when monkeys reach themselves (Iacoboni 2009). Perhaps relatedly, activity increases in overlapping brain regions when we observe, perceive, or believe someone else is having a feeling and have those feelings ourselves, especially when the feelings involve physical pain (Singer and Steinbeis 2009). Some accounts (like the perception-action model of empathy) argue that mirroring is at the core of all empathic phenomena (Preston and De Waal 2002). Other accounts just acknowledge that mirroring is either one of the capabilities that belongs in the empathy umbrella, or a mechanism that contributes to capabilities under the empathy umbrella (e.g. Yamamoto (2017)).\nThe notions of cognitive empathy, affective empathy, motivated empathy, and mirroring are prevalent in most empathy accounts, as noted by syntheses of the empathy literature (Hall and Schwartz 2019; Eklund and Meranius 2021). However, additional distinctions and requirements emerge from the plethora of empathy accounts as well, even though some are acknowledged much less consistently across disciplines. These less-discussed considerations are important to Al creators thinking about what their AI systems must achieve. We will discuss these \"fine cuts\u201d of empathy next.\nInformation types and how they need to be known. The first way of distinguishing empathic capabilities is through the type of information the empathizer is supposed to glean about the target. The empathizer might need to know the target's perceptions (row 1 in Table 1), cognitive phenomena (which include beliefs, thoughts, and knowledge; row 2), feelings (also referred to as emotions for our purposes; row 3), or some combination of all three. The empathizer may be expected, or even required, to glean this information through specific combinations of their own perceptions, cognitive phenomena, or feelings (rows 4, 5, 6).\nConsider, for example, the Sally-Ann test of cognitive empathy, sometimes referred to as \"theory of mind\" (Baron-Cohen, Leslie, and Frith 1985) that has been incorporated into recent benchmark tests for large language models (Le, Boureau, and Nickel 2019) (note that \"theory of mind\" is typically considered only one of the functions that contributes to cognitive empathy; Perry and Shamay-Tsoory 2013). In this test, cartoon sequences illustrate Sally putting a marble in a basket and leaving the room. While she is gone, Anne removes the marble from the basket and puts it in a box. Sally comes back into the room, and a potential empathizer is asked where Sally will look for her marble. To arrive at the correct answer of \u201cin the basket\u201d, an empathizer must know that Sally did not see (or visually perceive) that the marble was moved and that, as a consequence, Sally will believe the marble is still in the basket. The empathizer doesn't need to know anything about what Sally feels to pass the task. Thus, if there was a column for the Sally-Ann test in Table 1, rows 1 and 2 would be checked, but row 3 would be blank. Since the empathizer must be able to see the Sally-Ann cartoons or hear their descriptions to pass the test and must form appropriate beliefs based on what they perceive, rows 4 and 5 would be checked as well. However, the empathizer does not need to have any specific feelings to pass the Sally-Ann test.\nBy contrast, consider \"empathic accuracy\" tests that many neuroscience and psychology studies use (Hall and Schmid Mast 2007). In these tasks, empathizers are shown pictures of people expressing different facial expressions and are asked which emotion the person in each picture is feeling. To pass the tests, empathizers need to know what the target feels, but not what they think, know, believe, or perceive. Empathizers must perceive the pictures and form appropriate beliefs about them to pass empathic accuracy tasks, but do not necessarily need to feel anything specific to pass them. So if there was a column for empathic accuracy tests in Table 1, rows 3, 4, and 5 would be checked, but rows 1, 2, and 6 would be blank. That said, some definitions and theories of empathy state or hypothesize that empathizers come to know what a target is experiencing through experiencing similar feelings of their own, and the activation of the empathizer's own feelings is what allows empathy to function efficiently (De Waal and Preston 2017). Under such a conception, row 6 in Table 1 would be checked as well.\nConsciousness. Some conceptions of empathy require that the empathizer either be \u201cconsciously attentive\" to certain types of information or have a certain kind of conscious experience (row 7 in Figure 1). For example, Yu and Chou (2018) say that cognitive empathy involves \"a slow and complex process with efforts, consciousness, and elaborated neural profiles\" (Yu and Chou 2018). Smith (2017) says \u201cA empathizes with B if and only if (1) A is consciously aware that B is $\\phi$, (2) A is consciously aware of what being $\\phi$ feels like, (3) On the basis of (1) and (2), A is consciously aware of how B feels\" (Smith 2017). Montemayor, Halpern, and Fairweather (2022) state that some components of empathy are unconscious, some are conscious, but successful empathic communication that conveys genuine care requires \"consciously practic[ing] empathic attention\u201d (Montemayor, Halpern, and Fairweather 2022). So even if not all aspects of empathy require consciousness, consciousness may be required to realize some of empathy's benefits.\nOther accounts explicitly state that empathy or some of its components can be unconscious. This is often a stipulation for \"emotional contagion\", or \"[t]he tendency to automatically mimic and synchronize expressions, vocalizations, postures, and movements with those of another person's, and, consequently, to converge emotionally\" (Hatfield, Cacioppo, and Rapson 1993), in particular.\nSome theories, especially those from the neuroscience field, postulate that emotional contagion is the core of all other empathic processes, and at minimum a process that should be considered part of empathy (Preston and De Waal 2002). Emphasizing that assumption, neuroscience studies of empathy often focus on automatic, unconscious brain responses while passively viewing pictures of others in pain or distress (Singer and Steinbeis 2009).\nAccuracy. Some conceptions of empathy require that the empathizer not only perceive, believe, and/or emotionally respond to a target's perceptions, beliefs, or feelings, but that they do so with sufficient accuracy (row 8). For example, it may not be enough for an empathizer to try to understand a target's thoughts to have cognitive empathy (Rogers 1975). Instead, empathizers may need to make correct theories and inferences about their target's mental states and behavior, especially if they want their targets to feel understood (Rogers 1975; Spaulding 2017). That said, accuracy may not always be required for empathy to have an impact. If an empathizer believes a target is sad, resonates with that imagined or perceived sadness, and takes actions to try to relieve the target of that believed sadness, many theoretical and folk accounts of empathy would still consider that evidence of affective empathy or motivated empathic concern, even if the target turned out not to feel sad after all (Ickes and Simpson 2001). Evidence of empathic concern, even if misguided, can be sufficient for some purposes.\nSelf-other differentiation. Another characteristic of many empathy definitions is that the empathizer must (or must not) have particular forms of appreciation for the differences between themselves and the target, including differences in what they are thinking, feeling, and why (row 12). For example, Singer and Steinbeis (2009) specify that empathy involves \"a distinction between oneself and others and an awareness that one is vicariously feeling with someone but that this is not one's own emotion\" (Singer and Steinbeis 2009), and Decety, Lamm et al. (2006) specify that empathy is \"the ability to experience and understand what others feel without confusion between oneself and others\" (Decety, Lamm et al. 2006).\nEven so, emotional contagion-one of the aforementioned phenomena under the empathy umbrella-does not require a differentiation between self and other. Further, many non-human empathy tasks-e.g. freezing when another rat freezes (Atsak et al. 2011)\u2014do not require an appreciation of the differences between what oneself is feeling and a target is feeling, and may even manifest only because of this lack of appreciation.\nMotivation and action. Two points of significant divergence among empathy definitions are whether one needs to have a particular kind of other-oriented motivation to have empathy (row 13), or must take a particular kind of other-oriented action (row 14). Zaki (2014) emphasizes motivation, in general: \"empathy is often a motivated phenomenon in which observers are driven either to experience empathy or to avoid it\u201d (Zaki 2014). Definitions of empathy that incorporate notions of \u201cempathic concern\u201d usually go further and specify that an empathizer's motivation must be other-oriented with an ultimate goal of \u201crelieving the valued other's need\" (Batson et al. 2008). Some definitions also require that empathy lead to appropriate actions towards others, such as \u201cresponding with the appropriate prosocial and helpful behaviour", "passive process of information gathering\u201d (Van der Weele 2011). These views are particularly prominent in evolutionary or neuroscience accounts of empathy, which generally postulate that more sophisticated and complex phenomena within the empathy umbrella grew out of, co-opt, or interact with automatic interpersonal perception and automatic mirroring. Evidence used to support these views include the physiological reactions we have when seeing facial expressions of emotion, and the types of automatic copying babies do of their parents and animals do of other animals (Heyes 2018). Even if empathy can theoretically be derived through other more intentional or motivated mechanisms, these views usually see the passive aspects of empathy as critical for typical empathy-mediated behaviors.\nInteractions with targets. A final set of distinctions that vary across empathy definitions pertain to empathy's targets. For example, according to Bagheri et al. (2021), empathy involves \"reactive outcomes\" which \"aim to alter or enhance the target's affective state\u201d (Bagheri et al. 2021). Some therapeutic empathy accounts have similar emphases. Consider McCarthy's axiom that \u201cthe sole judge of empathy is the receiver\u201d (McCarthy 1992). According to these types of views, empathy may not be considered fully intact or functional if it doesn't have the intended effect on the target (row 15).\nAnother way that empathy can involve targets is through communication (row 16). Some empathy accounts, particularly those from medicine or therapy, assert that \"Empathy is not empathy if it is not communicated": "McCarthy 1992). They may require that empathy include an essential \"action\" component during which a physician \u201ccommunicates understanding by checking back with the patient", "tone of voice, facial expressions, body posture, and natural gestures": "Guidi and Traversa 2021). Similar requirements are emphasized in therapeutic contexts, where empathy is frequently treated as a mechanism for helping patients to feel understood and recognized, and therapists and significant others are encouraged to cultivate empathy by verbally acknowledging and validating what patients and partners think, feel, or are experiencing.\nA less obvious way empathy's effects on the target could matter is if one's theory of empathy or empathic learning requires emotional synchrony, motor synchrony, or temporal coordination between the empathizer and target. For Lim and Okuno (2015a), empathy involves \u201csynchronized reactions\u201d between the empathizer and the target (Lim and Okuno 2015a,b), and Hatfield, Cacioppo, and Rapson (1993) define primitive empathy as"}, {"title": "What Empathic Capabilities Do AIs Need?", "content": "The empathic capabilities needed by question-answerers, care-assistants, and care-providers would likely have both similarities and differences. All three AI systems, by definition, must take appropriate actions on behalf of their target (row 14; answering questions in a personalized way counts as such an action) and have certain impacts on their target (row 15) in order for consumers to continue using them. To direct the appropriate actions, all the applications could benefit from accurate (row 8) knowledge about the target's feelings (row 3) and beliefs (row 2), even if such knowledge is helpful without being necessary in the case of the question-answerer.\nIn some scenarios it could be useful for the care-assistants and care providers to have knowledge of the target's perceptions as well (row 1). If an AI care-assistant needs to help a disoriented patient find their way home safely while walking around the block, for example, the AI would benefit from knowing whether the target is noticing stop signs, traffic lights, and traffic, and whether the target can hear the AI's instructions. Overall, the more any medical AI knows and appropriately addresses specific concerns or thoughts targets have, the more useful the AI is likely to be (as long as doing so doesn't creep users out). It is plausible that effective AI care-assistants or care-providers could be trained using black-box algorithms that call into question whether the AI really has explicit \u201cknowledge\u201d of targets' feelings, beliefs, or perceptions, but we will put that issue aside for now.\nIt seems sufficient for AI question-answerers and care assistants to obtain knowledge about the targets' beliefs, feelings, and possibly perceptions solely through beliefs (in the form of statistical models) about the world (row 5), but many (if not all) AI care-assistants may also need to obtain this information through their own artificial perceptions (row 4), such as through video feeds or sensor readings of the target. According to the criteria established earlier, AI care-takers would have to meet an even higher standard, though. The expectation of care-takers as they have been described previously and as we are conceiving of them here is that they need to glean knowledge about the target through their own \"real and appropriate feelings\" that are shared, at least to some degree, with the target (row 6).\nThe depth and degree of accuracy (row 8) required by each AI application could plausibly differ. It could be sufficient for Al question-answerers to correctly predict the valence of the question-asker's feelings or beliefs in order to craft acceptable responses for that situation, rather than correctly predict exactly what the question-asker is thinking or feeling. Care-assistants, on the other hand, could need to have more detailed and accurate knowledge of a target's beliefs and feelings to help the target with challenges like finding their way home, agreeing to take medications they are reluctant to take, or providing entertainment. AI care-takers"}, {"title": "Implications for AI Creators and Users", "content": "Table 1 and the discussion in the last section should make it clear that \"Empathic AI\" need not, and likely should not, refer to one thing with a singular set of abilities. Instead, we posit that it will be more fruitful for \"Empathic AI\" to be used as a term that refers to agents with different combinations of empathy-related capabilities selected to address the needs of a particular use case. Rather than try to build a system that matches a specific empathy definition, we recommend that AI creators commit to thinking deeply about which of the components discussed earlier are required, optional, or incompatible with the type of empathy needed for their chosen use case, and collect evidence when necessary to support those conclusions. Doing so will help both empathic AI creators and users in many ways.\nKnowing what to build. Some groups are trying to create general AI that can teach itself how to do anything it needs to do, and may hope that empathy is one of the things Als learn or develop along the way. Other empathic AI systems are examples of narrow AI that are used to tackle individual tasks close to those their creators explicitly and intentionally designed them to achieve. Within narrow Al systems, it is common for creators to have to design different approaches for managing each distinct subtask or function the AI needs to perform, and then combine those approaches into one integrated system. Given that each approach requires significant investment, the systems that are ultimately built are less likely to have specific empathic capabilities or constraints if system creators don't intentionally try to build them in.\nAs an example, many artificial empathy systems that have been designed to date analyze real-time video feeds of a human target, try to identify what emotion the human target is feeling based on their facial expressions, and then implement a response the AI has learned is most appropriate for a target's identified emotion (e.g. Xiao et al. 2016). These systems, as currently implemented, typically have no ability to predict what a human target sees or knows from their own sensory input. Therefore, these AI systems currently have no way of representing the critical perceptual information that is needed to pass the Sally-Ann test described earlier. They also have no way of representing or understanding the thoughts the target is having that cause the emotion the target is expressing. That means these empathic AI systems would not be considered empathic by accounts that require the target to have beliefs about the target's perceptions and beliefs, and wouldn't meet the requirements of the AI care assistants we discussed earlier. Nonetheless, it is likely technically feasible for the systems to be adapted so they could do so. There is nothing stopping Al creators from making these adaptations in principle, other than that they may not have realized these criteria are important for some definitions and applications of empathy that they have not yet had reason to work on.\nAvoid wasting time on unnecessary features. Al creators may turn to the empathy literature to design requirements for an empathic AI system, and unknowingly choose an empathy definition that is not well-suited to their use case, leading them to waste time and resources building unnecessary features into their system. For instance, although mirroring is likely an important mechanism of some types of human empathy and its evolutionary precursors, Als do not need to use the same mirroring mechanisms to be able to accurately predict what a target is thinking, feeling, or perceiving, or to respond to those thoughts, feelings, or perceptions appropriately. AI designers can of course incorporate mirroring mechanisms into their empathic AI system designs if they wish or find it useful. Other designs may be more technically feasible or efficient, though, and there is no obvious reason why all artificial systems should be required to \"match\" their targets to be considered empathic when some applications would not benefit from such matching.\nAs another example, the fact that we do not yet know whether Al will ever have consciousness does not in principle preclude AI from having at least some types of empathy. Although some conceptions of human empathy require humans to be consciously aware of what targets are feeling, AI that lacks consciousness but meets requirements of other conceptions of empathy might still be extremely useful in appropriate situations. The situations addressed by AI medical question-answerers and care assistants we discussed earlier are examples, and there are many more.\nFacilitating ethical disclosure and use. If users think an AI has empathy according to one definition but the AI really only has empathy according to a different definition, users can be misled, exploited, make poor choices, or even be caused unnecessary emotional harm. In doing so, users would not be treated with the respect many agree is due to all humans, including (if not especially) vulnerable humans (Korsgaard 2021).\nConsider observations that patients are more likely to take their prescribed medication, follow recommended lifestyle changes, share diagnosis-relevant personal information, and experience relief from mental health challenges when they feel their doctors or therapists care for them (Kim, Kaplowitz, and Johnston 2004; Crockett et al. 2010; Eide et al. 2004; Finset and \u00d8rnes 2017). Multiple entities are busy trying to make medical AI care assistants and care providers that humans perceive to be sufficiently empathic to lead to similar positive health outcomes. As we have discussed, it is frequently argued that empathy is as impactful as it is in these contexts because it makes targets feel their welfare genuinely matters to somebody else in a deep and personal way. However, even the most ambitious empathic AI systems that are available today will not have the types of feelings and social motivations required to care about a human in the way humans care about each other. In other words, we currently don't know how to create these types of AI care providers, and we don't know if it will ever be possible to create them. Yet, AI care assistant systems are being built to have the appearance of having those feelings and motivations, and some users incorrectly interpret the Als' behavior as evidence the AI cares about them and has genuine feelings (Brandtzaeg, Skjuve, and F\u00f8lstad 2022; Hu, Mao, and Kim 2023). Empathic AI creators must make clear to users what abilities their AI systems do and do not have in these situations to minimize user deception and exploitation. One approach for doing so would be to leverage adapted model cards or \u201cnutrition labels\u201d that disclose which of the empathy \"fine cuts\" we described here are present or absent in a specific system (Mitchell et al. 2019).\nIt is also critical to appreciate that additional ethical problems will arise, even if Al creators do their best to inform users of their Als' abilities and lack of abilities. Some vulnerable users, such as elderly patients with dementia, may not understand that an AI they are interacting with is not a living system, and may choose to make sacrifices on the AI's behalf the same way they might for another human they had a mutually caring relationship with. Others might know and lament that an AI isn't able to care for them the same way a human can, but voluntarily engage with the AI nonetheless because they feel they have no better alternative. Such interactions can certainly have benefits (Inzlicht et al. 2023), but they could also end up making patients feel even more alone and even more undeserving of human understanding, compassion, and love (Montemayor, Halpern, and Fairweather 2022; Perry 2023). Ubiquity of AI companions could also erode society's commitment to believing all people deserve human versions of these things more generally.\nDifferent ethical concerns arise when targets incorrectly assume an empathic AI's empathy-even if it is unconscious and unfeeling-is designed to function on behalf of the target (rows 11, 13, 14, and 15 in Table 1). Some empathic Als will be designed this way. Others, however, could qualify as empathic by some definitions, even though they are designed to function primarily on behalf of the AI creators. Empathic AI of this kind is often discussed in customer service contexts, such as Siena AI's chatbot that is advertised to have \"human empathy in every interaction\". Targets can benefit from the AI's empathy through having more positive customer experiences, but the point of the empathy in such cases is still primarily to improve metrics the AI creators care about (such as customer retention or conversion). Sometimes what benefits an Al's target and the AI's creators will conflict; AIs that leverage their empathy to create highly personalized experiences for targets may be enjoyable, but may also be used to more effectively nudge or motivate users to do things that are not to their benefit, like buy harmful products, gamble when they cannot afford to do so, or share private information. Users will be more susceptible to empathic AI-driven manipulation if they fall prey to the assumption that all kinds of empathy are good for the target. It's plausible that being transparent about what kind of empathy is being implemented in a given AI will mitigate this ethical risk, but determining what kinds and forms of transparency are sufficient, and how these might differ across various user segments, are open and important questions.\nWe will not attempt to settle these issues here. Our point is that creators of empathic AI will not be able to anticipate or analyze these kinds of ethical concerns adequately if they do not appreciate the distinctions in Figure 1 sufficiently.\nHighlighting potential divergences between artificial and human empathy needs. The overall endeavor of considering what capabilities empathic Als in different contexts need also brings into focus differences between the requirements of human and artificial empathy. One that we have already alluded to is that artificial empathy may need to rely on accurate perception of the target's feelings, beliefs, and perceptions more than many conceptions of human empathy. Some"}, {"title": "Conclusion", "content": "Artificial empathy has many exciting uses and has the potential to overcome many of the disadvantages of human empathy by virtue of being free of the computational constraints and ingroup biases of human empathy (Inzlicht et al. 2023). It also has the potential to harm individuals and society more generally if developed unthoughtfully and without sufficient appreciation for the functions of its different characteristics, like if an empathic AI encourages a user to follow through with their proposed suicide attempts in an misguided effort to validate the user's feelings. To realize the possible advantages of empathic AI while successfully navigating its ethical challenges, we first must be clear about what Als need to do or be able to do to be considered empathic. We have argued that different constellations of phenomena within the empathy umbrella will be needed and desired for different empathic AI applications, and that the criteria used to assess each phenomenon may be different for Als and humans functioning in the same context. More evidence needs to be collected to determine what principles best govern what constellations of empathic phenomena are either necessary or desirable in what situations, and more precise and robust assessments need to be developed to test the presence or absence of those empathic phenomena. As empathic AI applications are pursued, both Al creators and empathy researchers must think deeply about the contingencies between an empathizer and target. All of these considerations and evidence should be incorporated into ongoing discussions about empathic Al's ethical implications. Our aim here has been to amplify and focus future work in this area-work that is poised to impact society at an increasingly large scale."}]}