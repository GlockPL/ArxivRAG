{"title": "CAUSAL DISCOVERY FROM TIME-SERIES DATA WITH SHORT-TERM INVARIANCE-BASED CONVOLUTIONAL NEURAL NETWORKS", "authors": ["Rujia Shen", "Boran Wang", "Chao Zhao", "Yi Guan", "Jingchi Jiang"], "abstract": "Causal discovery from time-series data aims to capture both intra-slice (contemporaneous) and inter-slice (time-lagged) causality between variables within the temporal chain, which is crucial for various scientific disciplines. Compared to causal discovery from non-time-series data, causal discovery from time-series data necessitates more serialized samples with a larger amount of observed time steps. To address the challenges, we propose a novel gradient-based causal discovery approach STIC, which focuses on Short-Term Invariance using Convolutional neural networks to uncover the causal relationships from time-series data. Specifically, STIC leverages both the short-term time and mechanism invariance of causality within each window observation, which possesses the property of independence, to enhance sample efficiency. Furthermore, we construct two causal convolution kernels, which correspond to the short-term time and mechanism invariance respectively, to estimate the window causal graph. To demonstrate the necessity of convolutional neural networks for causal discovery from time-series data, we theoretically derive the equivalence between convolution and the underlying generative principle of time-series data under the assumption that the additive noise model is identifiable. Experimental evaluations conducted on both synthetic and FMRI benchmark datasets demonstrate that our STIC outperforms baselines significantly and achieves the state-of- the-art performance, particularly when the datasets contain a limited number of observed time steps.", "sections": [{"title": "1 Introduction", "content": "Causality behind time-series data plays a significant role in various aspects of everyday life and scientific inquiry. Questions like \"What factors in the past have led to the current rise in blood glucose?\" or \"How long will my headache be alleviated if I take that pill?\" require an understanding of the relationships among observed variables, such as the relation between people's health status and their medical interventions [Cowls and Schroeder, 2015, Pawlowski et al., 2020]. People usually expect to find cyclical and invariant principles in a changing world, which we call causal relationships [Chan et al., 2024, Entner and Hoyer, 2010]. These relationships can be represented as a directed acyclic graph (DAG), where nodes represent observed variables and edges represent causal relationships between variables with time lags. This underlying graph structure forms the factual foundation for causal reasoning and is essential for addressing such queries [Pearl, 2009].\nCurrent causal discovery approaches utilize intra-slice and inter-slice information of time-series data, leveraging techniques such as conditional independence, smooth score functions, and auto-regression. These methods can be broadly classified into three categories: Constraint-based methods [Entner and Hoyer, 2010, Runge et al., 2019, Runge, 2020], Score-based methods [Pamfil et al., 2020], and Granger-based methods [Nauta et al., 2019, Cheng et al., 2022, 2023].Constraint-based methods rely on conditional independence tests to infer causal relationships between variables. These methods perform independence tests between pairs of variables under different conditional sets to determine whether a causal relation exists. However, due to the difficulty of sampling, real-world data often suffers from the limited length of observed time steps, making it challenging for statistical conditional independence tests to fully capture causal relationships [Zhang et al., 2011, Zhang and Suzuki, 2023]. Additionally, these methods often rely on strong yet unrealistic assumptions, such as Gaussian noise, when searching for statistical conditional independence [Spirtes and Zhang, 2016, Wang and Michoel, 2017]. Score-based methods regard causal discovery as a constrained optimization problem using augmented Lagrangian procedures. They assign a score function that captures properties of the causal graph, such as acyclicity, and minimize the score function to identify potential causal graphs. While these methods offer simplicity in optimization, they relying heavily on acyclicity regularization and often lack guarantees for finding the correct causal graph, potentially leading to suboptimal solutions [Varando, 2020, Lippe et al., 2021, Zhang et al., 2023]. Granger-based methods, inspired by [Granger, 1969, Granger and Hatanaka, 2015], offer an intriguing perspective on causal discovery. These methods utilize auto-regression algorithms under the assumption of additive noise to assess if one time series can predict another, thereby identifying causal relationships. However, they tend to exhibit lower precision when working with limited observed time steps.\nTo overcome the limitations of existing approaches, such as low sample efficiency in constraint-based methods, suboptimal solutions from acyclicity regularizers in score-based methods and low precision when limited observed time steps in Granger-based methods, we propose a novel Short-Term Invariance-based Convolutional causal discovery approach (STIC). STIC leverages the properties of short-term invariance to enhance the sample efficiency and accuracy of causal discovery. More concretely, by sliding a window along the entire time-series data, STIC constructs batches of window observations that possess invariant characteristics and improves sample utilization. Unlike existing score-based methods, our model does not rely on predefined acyclicity constraints to avoid local optimization. As the window observations move along the temporal chain, the structure of the window causal graph exhibits periodic patterns, demonstrating short-term time invariance. Simultaneously, the conditional probabilities of causal effects between variables remain unchanged as the window observations slide, indicating short-term mechanism invariance. The contributions of our work can be summarized as follows:\n\u2022 We propose STIC, the Short-Term Invariance-based Convolutional causal discovery approach, which leverages the properties of short-term invariance to enhance the sample efficiency and accuracy of causal discovery.\n\u2022 STIC uses the time-invariance block to capture the causal relationships among variables, while employing the mechanism-invariance block for the transform function.\n\u2022 To dynamically capture the contemporaneous and time-lagged causal structures of the observed variables, we establish the equivalence between the convolution of the space-domain (contemporaneous) and time-domain (time-lagged) components, and the multivariate Fourier transform (the underlying generative mechanism) of time-series data.\n\u2022 We conduct experiments to evaluate the performance of STIC on synthetic and benchmark datasets. The experimental results show that STIC achieves the state-of-the-art results on synthetic time-series datasets, even when dealing with relatively limited observed time steps. Experiments demonstrate that our approach outperforms baseline methods in causal discovery from time-series data."}, {"title": "2 Background", "content": "In this section, we introduce the background of causal discovery from time-series data. Firstly, we show all symbols and their definitions in Section 2.1. Secondy, in Section 2.2, we present the problem definition and formal representation of window causal graph. Thirdly, in Section 2.3, we introduce the concepts of short-term time invariance and mechanism invariance. Building upon these concepts, we derive an independence property specific to window causal graph. Fourthly, in Section 2.4, we delve into the theoretical aspects of our approach. Specifically, we establish the equivalence between the convolution operation and the underlying generative mechanism of the observed time-series data. This theoretical grounding provides a solid basis for the proposed STIC approach. Finally, in Section 2.5, we introduce Granger causality, an auto-regressive approach to causal discovery from time-series data."}, {"title": "2.1 Symbol Summarization", "content": "Firstly, to better represent the symbols used in Section 2, we arrange a table to summarize and show their definitions, as shown in Table 1."}, {"title": "2.2 Problem Definition", "content": "Let an observed dataset denoted as $X = \\{X_1,\\dots, X_d\\} \\in \\mathbb{R}^{d\\times T}$, which consists of $d$ observed continuous time-series variables. Each variable $X_i$ is represented as a time sequence $X_i = \\{X_i^1,\\dots, X_i^T\\}$ with the length of $T$. Here, each $X_i^t$ corresponds to the observed value of the $i$-th variable $X_i$ at the $t$-th time step. Unlike graph embedding algorithms [Cheng et al., 2020, 2021] which aims to learn time series representations, the objective of causal discovery is to uncover the underlying structure within time-series data, which represents boolean relationships between observed variables. Furthermore, following the Consistency Throughout Time assumption [Spirtes et al., 2000, Zhang and Spirtes, 2002, Robins et al., 2003, Kalisch and B\u00fchlman, 2007, Entner and Hoyer, 2010, Assaad et al., 2022], the objective of causal discovery from time-series data is to uncover the underlying window causal graph $\\mathcal{G}$ as an invariant causal structure. The true window causal graph for $X$ encompasses both intra-slice causality with 0 time lags and inter-slice causality with time lags ranging from 1 to $\\tau$. Here, $\\tau$ denotes the maximum time lag. Mathematically, the window causal graph is defined as a finite Directed Acyclic Graph (DAG) denoted by $\\mathcal{G} = (V,E)$. The set $V = \\{X_1, ..., X_d\\}$ represents the nodes within the graph $\\mathcal{G}$, wherein each node corresponds to an observed variable $X_i$. The set $\\mathcal{E}$ represents the"}, {"title": "2.3 Short-Term Causal Invariance", "content": "There has been an assertion that causal relationships typically exhibit short-term time and mechanism invariance across extensive time scales [Entner and Hoyer, 2010, Liu et al., 2023, Zhang et al., 2017]. These two aspects of invariance are commonly regarded as fundamental assumptions of causal invariance in causal discovery from time-series data. In the following, we will present the definitions for these two forms of invariance.\nDefinition 2 (Short-Term Time Invariance) Given $X \\in \\mathbb{R}^{d\\times T}$, for any $X_i, X_j, \\tau > 0$, if $X_i^t \\in Pa_\\tau^t(X_j)$ at time $t$, then there exists $X_i^{t'} \\in Pa_\\tau^{t'}(X_j)$ at time $t' \\neq t$ in a short period of time, where $Pa_\\tau^t(\\cdot)$ denotes the set of parents of a variable with $\\tau$ time lags at time step $t$."}, {"title": "2.4 Necessity of Convolution", "content": "Granger demonstrated, through the Cramer representation and the spectral representation of the covariance sequence [Granger, 1969, Mills and Granger, 2013, Granger and Hatanaka, 2015], that time-series data can be decomposed into a sum of uncorrelated components. Inspired by these representations and the concept of graph Fourier transform [Shuman et al., 2013, Sandryhaila and Moura, 2013, Sardellitti et al., 2017], we propose considering a underlying function $X = f(Pa_{\\mathcal{G}}(X),W) + \\mathcal{E}$, where $Pa_{\\mathcal{G}}(X)$ denotes relationships among $X$ in the window causal graph $\\mathcal{G}$ and $\\mathcal{E}$ is the noise term, to describe the generative process of the observed dataset $X = \\{X_1,\\dots, X_d\\} \\in \\mathbb{R}^{d\\times T}$, with an underlying window causal matrix $W \\in \\mathbb{R}^{d\\times d\\times (\\tau+1)}$. We can then decompose $f(Pa_{\\mathcal{G}}(X), W)$ into Fourier integral forms:\n$$\nX = f(Pa_{\\mathcal{G}}(X),W) + \\mathcal{E}\n= f(s,t) + \\mathcal{E}\n$$\nHere, $s$ and $t$ denote the spatial and temporal projections, respectively, of $f(Pa_{\\mathcal{G}}(X), W)$. Equation 1 is derived from the observation that the contemporaneous part in time-series data corresponds to the spatial domain, while the time-lagged part corresponds to the temporal domain. Therefore, we employ the multivariate Fourier transform,\n$$\n\\mathcal{F}(X) = \\int\\int f(x,y; s, t)e^{-i\\omega (sx+ty)} dxdy = \\int \\int h(s)g(t)e^{-i\\omega (s+t)} d\\hat{s}d\\hat{t}\n$$\nwhere $s$ represents the spatial domain component, $t$ represents the temporal domain component, and $\\omega$ represents the angular frequency along with transform function $f, h$ and $g$. The first line corresponds to applying the Fourier transform"}, {"title": "2.5 Granger Causality", "content": "Granger causality [Granger, 1969, Pavasant et al., 2021, Assaad et al., 2022] is a method that utilizes numerical calculations to assess causality by measuring fitting loss and variance. Formally, we say that a variable $X_i$ Granger-causes another variable $X_j$ when the past values of $X_i$ at time $t$ (i.e., $X_i^{t-\\tau},..., X_i^{t-1}$) enhance the prediction of $X_j$ at time $t$ (i.e., $X_j^t$) compared to considering only the past values of $X_j$. The definition of Granger causality is as follows:\nDefinition 4 (Granger Causality) Let $X = \\{X_1,..., X_d\\} \\in \\mathbb{R}^{d\\times T}$ be a observed dataset containing $d$ variables. If $\\sigma^2(X_j | X) < \\sigma^2(X_j | X - X_i)$, where $\\sigma^2(X_j|X)$ denotes the variance of predicting $X_j$ using $X$ with $\\tau$ time lags, we say that $X_i$ causes $X_j$, which is represented by $W_{i,j}^{\\tau} = 1$.\nIn simpler terms, Granger causality states that $X_i$ Granger-causes $X_j$ if past values of $X_i$ (i.e., $X_i^{t}$) provide unique and statistically significant information for predicting future values of $X_j$ (i.e., $X_j^t$). Therefore, following the definition of Granger causality, we can approach causal discovery as an autoregressive problem."}, {"title": "3 Method", "content": "In this section, we introduce STIC, which involves four components: Window Representation, Time-Invariance Block, Mechanism-Invariance Block, and Parallel Blocks for Joint Training. The process is depicted in Figure 2. Firstly, we transform the observed time series into a window representation format, leveraging Lemma 1. Next, we input the window representation into both the time-invariance block and the mechanism-invariance block ($B_t$ and $B_m$ in Figure 2). Finally, we conduct joint training using the extracted features from two kinds of parallel blocks. In particular, the time-invariance block $B_t$ generates the estimated window causal matrix $\\hat{W}$. To better represent the symbols used in Section 3, we also arrange a table to summarize and show their definitions, as shown in Table 2. The subsequent subsections provide a detailed explanation of the key components of STIC."}, {"title": "3.1 Window Representation", "content": "The observed dataset $X \\in \\mathbb{R}^{d\\times T}$ contains $d$ observed continuous time series (variables) with $T$ time steps. We also define a predefined maximum time lag as $\\tau$. To ensure that the entire causal contemporaneous and time-lagged influence is observed, we calculate the minimum length of the window that can capture this influence as $\\hat{\\tau} = \\tau + 1$. To construct the window observations, we select the observed values from the first $T - 1$ time steps, i.e. $X_{1:T-1} = \\{X_1^{1:T-1},...,X_d^{1:T-1}\\} \\in []\\mathbb{R}^{d\\times (T-1)}$. Using a sliding window approach along the temporal chain of observations, we create window observations of length $\\hat{\\tau}$ and width $d$, with a step size of 1. This process results in $c = T - \\hat{\\tau}$ window"}, {"title": "3.2 Time-Invariance Block", "content": "According to Definition 2, the causal relationships among variables remain unchanged as time progresses. Exploiting this property, we can extract shared information from the window representation $W$ and utilize it to finally obtain the estimated window causal matrix $\\hat{W}$. Inspired by convolutional neural networks used in causal discovery[Nauta et al., 2019], we introduce a invariance-based convolutional network structure denoted as $B_t$ to incorporate temporal information within the window representation $W$. For each window observation $W_{\\psi} \\in \\mathbb{R}^{d\\times \\hat{\\tau}}$, we employ the following formula to aggregate similar information among the time series within the window observations\n$$\n\\hat{W} = f_1(K_t \\odot W_1, ..., K_t \\odot W_c)\n$$\nHere, shared $K_t \\in \\mathbb{R}^{d\\times \\hat{\\tau}}$ represents a learnable extraction kernel utilized to extract information from each window observation. The symbol $\\odot$ denotes the Hadamard product between matrices, and $f_1$ refers to a neural network structure. By applying the Hadamard product with the shared kernel $K_t$, the resulting output exhibits similar characteristics across the time series. Moreover, $K_t$ serves as a time-invariant feature extractor, capturing recurring patterns that appear in the input series and aiding in forecasting short-term future values of the target variable. In Granger causality, these learned patterns reflect causal relationships between time series, which are essential for causal discovery [Nauta, 2018]. To ensure the generality of STIC, we employ a simple feed-forward neural network (FNN) $f_1 : \\mathbb{R}^{c\\times d\\times \\hat{\\tau}} \\rightarrow \\mathbb{R}^{d\\times d\\times \\hat{\\tau}}$ to extract shared information from each $K_t \\odot W_{\\psi}, \\psi = 1, ..., c$. Furthermore, we impose a constraint to prohibit self-loops in the estimated window causal matrix $\\hat{W}$ when the time lag is zero. That is:\n$$\n\\hat{W}_{i,j}^\\tau = \\begin{cases}\n0 & \\text{if } i = j \\text{ and } \\tau = 0 \\\\\n0 & \\text{if } \\hat{W}_{i,j}^\\tau < \\rho\\\\\n1 & \\text{else}\n\\end{cases}\n$$"}, {"title": "3.3 Mechanism-Invariance Block", "content": "As stated in Definition 3, the causal conditional probability relationships among the time series remain unchanged as time varies. Consequently, the causal functions between variables also remain constant over time. With this in mind, our objective in $B_m$ is to find a unified transform function $f_2 : \\mathbb{R}^{d\\times \\hat{\\tau}} \\rightarrow \\mathbb{R}^{d\\times \\hat{\\tau}}$ that accommodates all window observations. To achieve this goal, as depicted in Figure 2, we employ a convolution kernel $K_m \\in \\mathbb{R}^{d\\times \\hat{\\tau}}$ as $f_2$. This kernel performs a Hadamard product operation with each window $W_{\\psi} \\in \\mathbb{R}^{d\\times \\hat{\\tau}}$ in $W$, where $\\psi = 1, ..., c$. Subsequently, we employ the Parametric Rectified Linear Unit (PReLU) activation function [Zhu et al., 2017] to obtain the output $\\hat{W}_{\\psi} \\in \\mathbb{R}^{d\\times \\hat{\\tau}}$,\n$$\n\\overline{W}_{\\psi} = PReLU(K_m \\odot W_{\\psi})\n$$\nEach $\\overline{W}_{\\psi}$ represents the transformed matrix obtained from the window observation $W_{\\psi}$ by a unified transform function $f_2$ implemented with convolution kernel $K_m$. Each $\\overline{W}_{\\psi}$ is finally used to predict $\\hat{X}_{t+\\hat{\\tau}}$. Note that this transform function $f_2$ can also be composed of $N$ different but equal dimensional kernels $K_m^1, ..., K_m^N \\in \\mathbb{R}^{d\\times \\hat{\\tau}}$, which are nested to perform complex nonlinear transformations. After $f_2$, the value inside the window $\\overline{W}_{\\psi} \\in \\mathbb{R}^{d\\times \\hat{\\tau}}$ is then pressed for $\\overline{W}$-selected column summation to predict $\\hat{X}_{t+\\hat{\\tau}} \\in \\mathbb{R}^{d}$."}, {"title": "3.4 Parallel Blocks for Joint Training", "content": "So far, we have obtained the estimated window causal matrix $\\hat{W}$ by using $B_t$. In addition, we also obtained the transformed matrix $\\overline{W}$ with $B_m$. We used convolutional neural networks in both $B_t$ and $B_m$. Their structures are similar, but their functions and purposes are different. In $B_t$, we focus on the shared underlying unified structure of all window observations. Following the Definition 2 of short-term time invariance, we choose a convolutional neural network structure with translation invariance [Kayhan and Gemert, 2020, Singh et al., 2023]. We expect that $f_1$ with $K_t$ as the main component can extract the invariant structure of the window representation $W$. In $B_m$, we focus on the convolution kernel $K_m$, which is expected to serve as a unified transform function $f_2$ to satisfy the Definition 3 of short-term mechanism invariance and perform complex nonlinear transformations."}, {"title": "4 Experiment Results", "content": "In this section, we present a comprehensive series of experiments on both synthetic and benchmark datasets to verify the effectiveness of the proposed STIC. Following the experimental setup of [Runge et al., 2019, Runge, 2020], we compare STIC against the constraint-based approaches such as PCMCI [Runge et al., 2019] and PCMCI+ [Runge, 2020], the score-based approaches such as DYNOTEARS [Pamfil et al., 2020], and the Granger-based approaches TCDF [Nauta et al., 2019], CUTS [Cheng et al., 2022] and CUTS+ [Cheng et al., 2023].\nOur causal discovery algorithm is implemented using PyTorch. The source code for our algorithm is publicly available at the following URL 1. Both the time-invariance block and mechanism-invariance block are implemented using convolutional neural networks.\nFirstly, we conducted experiments on synthetic datasets, encompassing both linear and non-linear cases. The methods of generating synthetic datasets for both linear and non-linear cases will be introduced separately in Section 4.2. Secondly, we proceeded to perform experiments on benchmark datasets to demonstrate the practical value of our model in Section 4.3. Thirdly, to evaluate the sensitivity of hyper-parameters, such as the learning rate (default 1e\u00af5), the predefined\\tau (default 0.4d) and the threshold $p$ (default 0.3), we conducted ablation experiments as detailed in Section 4.4.\nWe employ two kinds of evaluation metrics to assess the quality of the estimated causal matrix: the F1 score and precision. A higher F1 score indicates a more comprehensive estimation of the window causal matrix, while a higher precision indicates the ability to identify a larger number of causal edges. In this paper, we consider causal edges with different time lags for the same pair of variables as distinct causal edges. Specifically, if there exists a causal edge from $X_i$ to $X_j$ with a time lag of $\\tau_1$, and another causal edge from $X_i$ to $X_j$ with the time lags of $\\tau_2$, where $i \\neq j$ and $\\tau_1 \\neq \\tau_2$, we regard these as two separate causal edges. Due to the need to predefine the maximum time lag in STIC, we truncate the estimated $W \\in []\\mathbb{R}^{d\\times d\\times (\\tau+1)}$ to $\\hat{W} \\in \\mathbb{R}^{d\\times d\\times (\\tau+1)}$ and then compute the evaluation metrics. We handle other baselines (such as PCMCI, PCMCI+, DYNOTEARS, CUTS, CUTS+) requiring a predefined maximum time lag parameter in the same manner."}, {"title": "4.1 Baselines", "content": "We select six state-of-the-art causal discovery methods as baselines for comparison:"}, {"title": "4.2 Experiments on Synthetic Datasets", "content": "We generate synthetic datasets in the following manner. Firstly, we consider several typical challenges [Runge et al., 2019, Runge, 2020] with contemporaneous and time-lagged causal dependencies, following an additive noise model. We set the ground truth maximum time lag to 0.4d and initialize the existence of each edge in the true window causal matrix W with a probability of 50%. For each variable $X_i$, its relation to with its parents $Pa_{\\mathcal{G}}(X_i)$ is defined as $X_i = f_i(Pa_{\\mathcal{G}}(X_i)) + \\varepsilon_i$, where $f_i$ represents the ground truth transformation function between $X_i$'s parents $Pa_{\\mathcal{G}}(X_i)$ and $X_i$. If $X_j \\in Pa_{\\mathcal{G}}(X_i)$, then in the ground truth causal matrix $W, W_{i,j}^{\\tau} = 1$. Secondly, for linear datasets, each $f_i$ is defined by a weighted linear function, while for nonlinear datasets, each $f_i$ is defined using a weighted cosine function. We sample the weights from a uniform distribution, such that if a causal edge exists, the corresponding weight in the additive noise model is sampled from the interval $\\mathcal{U}(-2, -0.5] \\cup [0.5, 2)$ to ensure non-zero values. For non-causal edges, the weight is set to 0. The noise term $\\varepsilon_i$ follows either a standard normal distribution $\\mathcal{N}(0, 1)$ or is uniformly sampled from the interval $\\mathcal{U}[0, 1]$. These data-generating procedures are similar to those used by the PCMCI family [Runge et al., 2019, Runge, 2020] and CUTS family [Cheng et al., 2022, 2023].\nIn the following, we present different results on linear Gaussian datasets (Section 4.2.1), nonlinear Gaussian datasets (Section 4.2.2), and linear uniform datasets (Section 4.2.3) to demonstrate the superiority of our model. Specifically, to reduce the impact of random initialization, we conduct 10 experiments for each type of datasets and report the mean and variance of the experimental results."}, {"title": "4.2.1 Linear Gaussian Datasets", "content": "The data generation process for linear Gaussian datasets follows the relationship $X_i = w_i Pa_{\\mathcal{G}}(X_i) + \\varepsilon_i$, where $\\varepsilon_i$ is sampled from a standard normal distribution $\\mathcal{N} (0, 1)$. To demonstrate the capability of our model in causal discovery from time-series data on datasets of varying sizes, we compare STIC with baselines under different conditions, including different numbers of variables ($d = \\{5, 10, 15, 20\\}$) and different lengths of time steps ($T = \\{100, 200, 500, 1000\\}$)."}, {"title": "4.2.2 Nonlinear Gaussian Datasets", "content": "In this section, we perform experiments on nonlinear Gaussian datasets to evaluate the performance of STIC. We set the number of variables ($d = 5$) and the observed time steps ($T = 1000$). For each $X_i$, its relationship with its parents $Pa_{\\mathcal{G}}(X_i)$ is defined using the cosine function, and the noise term $\\varepsilon_i$ follows the standard normal distribution."}, {"title": "4.2.3 Linear Uniform Datasets", "content": "The linear uniform datasets is generated with observed time steps ($T = 1000$) by varied numbers of variables ($d = \\{5, 10, 15, 20\\}$). For each $X_i$, $f_i$ is set as a linear function, while the noise term $\\varepsilon_i$ follows a uniform distribution $\\mathcal{U}[0, 1]$."}, {"title": "4.3 Experiments on Benchmark Datasets", "content": "In this section, we utilize FMRI benchmark datasets, a common neuroscientific benchmark dataset called Functional Magnetic Resonance Imaging [Smith et al., 2011], to explore and discover brain blood flow patterns. The dataset contains 28 different underlying brain networks with the number of observed variables ($d = \\{5, 10, 15\\}$). For each of the 28 brain networks, we observe 200 time steps for causal discovery. The results are reported in Table 3."}, {"title": "4.4 Ablation Study", "content": "We conduct ablation experiments on the linear Gaussian datasets with the number of variables ($d = 5$), to investigate the impact of different hyper-parameters on the experimental results, such as the learning rate (default: 1e-5), the predefined maximum time lag (default: 0.4d = 2), and the threshold $p$ (default: 0.3). Specifically, we vary the learning rate by increasing it to le-4 and decreasing it to le-6. We also increased the predefined maximum lag to $\\tau = 3$ and $\\tau = 4$, respectively, and change the threshold to $p = 0.1$ or $p = 0.5$. The empirical results are summarized in Table 4."}, {"title": "5 Discussion", "content": "This study presents two kinds of short-term invariance-based convolutional neural networks for discovery causality from time-series data. Major findings include: (1) our methods, based on gradients, effectively discover causality from time-series data; (2) convolutional neural network based on short-term invariance improves the sample efficiency of causal discovery. (3) our proposed STIC demonstrates significantly superior performance compared to baseline causal discovery algorithms. In this section, we discuss these results in detail."}, {"title": "5.1 What contributes to the effectiveness of STIC?", "content": ""}, {"title": "5.1.1 Why can STIC find causal relationships", "content": "Numerous gradient-based methods have been developed, such as DYNOTEARS within score-based approaches [Pamfil et al., 2020], and TCDF [Nauta et al., 2019], CUTS[Cheng et al., 2022] and CUTS+[Cheng et al., 2023] within Granger-based approaches. Including our proposed STIC, these gradient-based methods aim to optimize estimated causal matrix by maximizing or minimizing constrained functions. With the rapid advancement and widespread adoption of deep Neural Networks (NNs), researchers have begun employing NNs to infer nonlinear Granger causality, demonstrating the effectiveness of gradient-based methods in causal discovery [Tank et al., 2021, Wu et al., 2021, Khanna and Tan, 2019]. In our approach, we maintain the assumption and the constrained functions of Granger causality so that our method remains effective in discovering causal relationships."}, {"title": "5.1.2 Why can STIC find the true causality", "content": "As time progresses, the values of observed variables change due to statistical shifts in distributions. However, the causal relationships between the variables remain the same. For example, carbohydrate intake may lead to an increase in blood glucose, but the specific magnitude of the increase may vary with covariates such as body weight. The \"lead\" property is used as an indicator of causal relationships, i.e., invariance [Magliacane et al., 2018, Rojas-Carulla et al., 2018, Santos, 2021, Li et al., 2021]. In this paper, we observe that some causal relationships may also vary over time. Therefore, we make a more reasonable assumption, namely short-term time invariance and mechanism invariance [Entner and Hoyer, 2010, Liu et al., 2023, Zhang et al., 2017]. Building on these two forms of short-term invariance, we posit that both the window causal matrix W and the transform functions f remain unchanged in the short term. For example, within a few days (short-term), since covariates affecting blood glucose levels, such as body weight, remain nearly constant, the increase in blood glucose levels due to carbohydrate intake is also essentially constant. The short-term mechanism invariance proposed in this paper is also considered an invariant principle [Liu et al., 2022]. Building on these forms of invariance, a natural extension is the introduction of parallel time-invariance and mechanism-invariance blocks for"}, {"title": "5.2 What contributes to the exceptional performance of STIC?", "content": ""}, {"title": "5.2.1 High F1 scores and precisions", "content": "The experiments conducted on both synthetic and FMRI benchmark datasets in Section 4 demonstrate that our STIC model achieves the state-of-the-art F1 scores and precisions in most cases. We attribute the performance improvement to the incorporation of the window representation, the time-invariance block, and the mechanism-invariance block. The window representation serves as a form of data augmentation and aggregation, providing a macroscopic understanding of common features across multiple window observations, thereby facilitating the learning of more accurate causal structures. The time-invariance block extracts common features from multiple window observations and achieves effective information aggregation, enhancing sample efficiency and enabling the model to achieve high performance. The mechanism-invariance block, with nested convolution kernels, iteratively examines the functional transform within each individual window, enabling complex nonlinear transformations. With improved accuracy in both causal structures and complex nonlinear transformations, STIC demonstrates exceptional performance."}, {"title": "5.2.2 High sample efficiency", "content": "The window representation, introduced in Section 3.1, facilitates the segmentation of the entire observed dataset $X\\in \\mathbb{R}^{d\\times T}$ into $c = T - \\hat{\\tau} - 1$ partitions"}]}