{"title": "CAUSAL DISCOVERY FROM TIME-SERIES DATA WITH SHORT-TERM INVARIANCE-BASED CONVOLUTIONAL NEURAL NETWORKS", "authors": ["Rujia Shen", "Boran Wang", "Chao Zhao", "Yi Guan", "Jingchi Jiang"], "abstract": "Causal discovery from time-series data aims to capture both intra-slice (contemporaneous) and inter-slice (time-lagged) causality between variables within the temporal chain, which is crucial for various scientific disciplines. Compared to causal discovery from non-time-series data, causal discovery from time-series data necessitates more serialized samples with a larger amount of observed time steps. To address the challenges, we propose a novel gradient-based causal discovery approach STIC, which focuses on Short-Term Invariance using Convolutional neural networks to uncover the causal relationships from time-series data. Specifically, STIC leverages both the short-term time and mechanism invariance of causality within each window observation, which possesses the property of independence, to enhance sample efficiency. Furthermore, we construct two causal convolution kernels, which correspond to the short-term time and mechanism invariance respectively, to estimate the window causal graph. To demonstrate the necessity of convolutional neural networks for causal discovery from time-series data, we theoretically derive the equivalence between convolution and the underlying generative principle of time-series data under the assumption that the additive noise model is identifiable. Experimental evaluations conducted on both synthetic and FMRI benchmark datasets demonstrate that our STIC outperforms baselines significantly and achieves the state-of-the-art performance, particularly when the datasets contain a limited number of observed time steps.", "sections": [{"title": "1 Introduction", "content": "Causality behind time-series data plays a significant role in various aspects of everyday life and scientific inquiry. Questions like \"What factors in the past have led to the current rise in blood glucose?\" or \"How long will my headache be alleviated if I take that pill?\" require an understanding of the relationships among observed variables, such as the relation between people's health status and their medical interventions [Cowls and Schroeder, 2015, Pawlowski et al., 2020]. People usually expect to find cyclical and invariant principles in a changing world, which we call causal relationships [Chan et al., 2024, Entner and Hoyer, 2010]. These relationships can be represented as a directed acyclic graph (DAG), where nodes represent observed variables and edges represent causal relationships between variables with time lags. This underlying graph structure forms the factual foundation for causal reasoning and is essential for addressing such queries [Pearl, 2009].\nCurrent causal discovery approaches utilize intra-slice and inter-slice information of time-series data, leveraging techniques such as conditional independence, smooth score functions, and auto-regression. These methods can be broadly classified into three categories: Constraint-based methods [Entner and Hoyer, 2010, Runge et al., 2019, Runge, 2020], Score-based methods [Pamfil et al., 2020], and Granger-based methods [Nauta et al., 2019, Cheng et al., 2022, 2023].Constraint-based methods rely on conditional independence tests to infer causal relationships between variables. These methods perform independence tests between pairs of variables under different conditional sets to determine whether a causal relation exists. However, due to the difficulty of sampling, real-world data often suffers from the limited length of observed time steps, making it challenging for statistical conditional independence tests to fully capture causal relationships [Zhang et al., 2011, Zhang and Suzuki, 2023]. Additionally, these methods often rely on strong yet unrealistic assumptions, such as Gaussian noise, when searching for statistical conditional independence [Spirtes and Zhang, 2016, Wang and Michoel, 2017]. Score-based methods regard causal discovery as a constrained optimization problem using augmented Lagrangian procedures. They assign a score function that captures properties of the causal graph, such as acyclicity, and minimize the score function to identify potential causal graphs. While these methods offer simplicity in optimization, they relying heavily on acyclicity regularization and often lack guarantees for finding the correct causal graph, potentially leading to suboptimal solutions [Varando, 2020, Lippe et al., 2021, Zhang et al., 2023]. Granger-based methods, inspired by [Granger, 1969, Granger and Hatanaka, 2015], offer an intriguing perspective on causal discovery. These methods utilize auto-regression algorithms under the assumption of additive noise to assess if one time series can predict another, thereby identifying causal relationships. However, they tend to exhibit lower precision when working with limited observed time steps.\nTo overcome the limitations of existing approaches, such as low sample efficiency in constraint-based methods, suboptimal solutions from acyclicity regularizers in score-based methods and low precision when limited observed time steps in Granger-based methods, we propose a novel Short-Term Invariance-based Convolutional causal discovery approach (STIC). STIC leverages the properties of short-term invariance to enhance the sample efficiency and accuracy of causal discovery. More concretely, by sliding a window along the entire time-series data, STIC constructs batches of window observations that possess invariant characteristics and improves sample utilization. Unlike existing score-based methods, our model does not rely on predefined acyclicity constraints to avoid local optimization. As the window observations move along the temporal chain, the structure of the window causal graph exhibits periodic patterns, demonstrating short-term time invariance. Simultaneously, the conditional probabilities of causal effects between variables remain unchanged as the window observations slide, indicating short-term mechanism invariance. The contributions of our work can be summarized as follows:\n\u2022 We propose STIC, the Short-Term Invariance-based Convolutional causal discovery approach, which leverages the properties of short-term invariance to enhance the sample efficiency and accuracy of causal discovery.\n\u2022 STIC uses the time-invariance block to capture the causal relationships among variables, while employing the mechanism-invariance block for the transform function.\n\u2022 To dynamically capture the contemporaneous and time-lagged causal structures of the observed variables, we establish the equivalence between the convolution of the space-domain (contemporaneous) and time-domain (time-lagged) components, and the multivariate Fourier transform (the underlying generative mechanism) of time-series data.\n\u2022 We conduct experiments to evaluate the performance of STIC on synthetic and benchmark datasets. The experimental results show that STIC achieves the state-of-the-art results on synthetic time-series datasets, even when dealing with relatively limited observed time steps. Experiments demonstrate that our approach outperforms baseline methods in causal discovery from time-series data."}, {"title": "2 Background", "content": "In this section, we introduce the background of causal discovery from time-series data. Firstly, we show all symbols and their definitions in Section 2.1. Secondy, in Section 2.2, we present the problem definition and formal representation of window causal graph. Thirdly, in Section 2.3, we introduce the concepts of short-term time invariance and mechanism invariance. Building upon these concepts, we derive an independence property specific to window causal graph. Fourthly, in Section 2.4, we delve into the theoretical aspects of our approach. Specifically, we establish the equivalence between the convolution operation and the underlying generative mechanism of the observed time-series data. This theoretical grounding provides a solid basis for the proposed STIC approach. Finally, in Section 2.5, we introduce Granger causality, an auto regressive approach to causal discovery from time-series data."}, {"title": "2.1 Symbol Summarization", "content": "Firstly, to better represent the symbols used in Section 2, we arrange a table to summarize and show their definitions, as shown in Table 1."}, {"title": "2.2 Problem Definition", "content": "Let an observed dataset denoted as $\\mathcal{X} = {X_1,\\dots, X_d} \\in \\mathbb{R}^{d\\times T}$, which consists of $d$ observed continuous time-series variables. Each variable $X_i$ is represented as a time sequence $X_i = {X^i_1,\\dots, X^i_T}$ with the length of $T$. Here, each $X^i_t$ corresponds to the observed value of the $i$-th variable $X_i$ at the $t$-th time step. Unlike graph embedding algorithms [Cheng et al., 2020, 2021] which aims to learn time series representations, the objective of causal discovery is to uncover the underlying structure within time-series data, which represents boolean relationships between observed variables. Furthermore, following the Consistency Throughout Time assumption [Spirtes et al., 2000, Zhang and Spirtes, 2002, Robins et al., 2003, Kalisch and B\u00fchlman, 2007, Entner and Hoyer, 2010, Assaad et al., 2022], the objective of causal discovery from time-series data is to uncover the underlying window causal graph $\\mathcal{G}$ as an invariant causal structure.\nThe true window causal graph for $\\mathcal{X}$ encompasses both intra-slice causality with 0 time lags and inter-slice causality with time lags ranging from 1 to $\\tau$. Here, $\\tau$ denotes the maximum time lag. Mathematically, the window causal graph is defined as a finite Directed Acyclic Graph (DAG) denoted by $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$. The set $\\mathcal{V} = {X_1, ..., X_d}$ represents the nodes within the graph $\\mathcal{G}$, wherein each node corresponds to an observed variable $X_i$. The set $\\mathcal{E}$ represents the"}, {"title": "2.3 Short-Term Causal Invariance", "content": "There has been an assertion that causal relationships typically exhibit short-term time and mechanism invariance across extensive time scales [Entner and Hoyer, 2010, Liu et al., 2023, Zhang et al., 2017]. These two aspects of invariance are commonly regarded as fundamental assumptions of causal invariance in causal discovery from time-series data. In the following, we will present the definitions for these two forms of invariance.\nDefinition 2 (Short-Term Time Invariance) Given $\\mathcal{X} \\in \\mathbb{R}^{d\\times T}$, for any $X_i, X_j, \\tau > 0$, if $X_i \\in Pa^t_\\tau(X_j)$ at time $t$, then there exists $X_i \\in Pa^{t'}_\\tau(X_j)$ at time $t' \\neq t$ in a short period of time, where $Pa^t_\\tau(\\cdot)$ denotes the set of parents of a variable with $\\tau$ time lags at time step $t$."}, {"title": "2.4 Necessity of Convolution", "content": "Granger demonstrated, through the Cramer representation and the spectral representation of the covariance sequence [Granger, 1969, Mills and Granger, 2013, Granger and Hatanaka, 2015], that time-series data can be decomposed into a sum of uncorrelated components. Inspired by these representations and the concept of graph Fourier transform [Shuman et al., 2013, Sandryhaila and Moura, 2013, Sardellitti et al., 2017], we propose considering a underlying function $\\mathcal{X} = f(Pa_\\mathcal{G}(\\mathcal{X}),W) + \\mathcal{E}$, where $Pa_\\mathcal{G}(\\mathcal{X})$ denotes relationships among $\\mathcal{X}$ in the window causal graph $\\mathcal{G}$ and $\\mathcal{E}$ is the noise term, to describe the generative process of the observed dataset $\\mathcal{X} = {X_1,\\dots, X_d} \\in \\mathbb{R}^{d\\times T}$, with an underlying window causal matrix $W \\in \\mathbb{R}^{d\\times d\\times (\\tau+1)}$. We can then decompose $f(Pa_\\mathcal{G}(\\mathcal{X}), W)$ into Fourier integral forms:\n\\begin{equation}\n\\begin{aligned}\n\\mathcal{X} &= f(Pa_\\mathcal{G}(\\mathcal{X}),W) + \\mathcal{E} \\\\\n&= f(s,t) + \\mathcal{E}\n\\end{aligned}\n\\end{equation}\nHere, $s$ and $t$ denote the spatial and temporal projections, respectively, of $f(Pa_\\mathcal{G}(\\mathcal{X}), W)$. Equation 1 is derived from the observation that the contemporaneous part in time-series data corresponds to the spatial domain, while the time-lagged part corresponds to the temporal domain. Therefore, we employ the multivariate Fourier transform,\n\\begin{equation}\n\\mathcal{F}(X) = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f(x,y; s, t)e^{-i\\omega (sx+ty)} dxdy \\\\\n\\alpha \\int \\int h(s)g(t)e^{-i\\omega (s+t)} d\\hat{s}dt\n\\end{equation}\nwhere $s$ represents the spatial domain component, $t$ represents the temporal domain component, and $\\omega$ represents the angular frequency along with transform function $f, h$ and $g$. The first line corresponds to applying the Fourier transform"}, {"title": "2.5 Granger Causality", "content": "Granger causality [Granger, 1969, Pavasant et al., 2021, Assaad et al., 2022] is a method that utilizes numerical calculations to assess causality by measuring fitting loss and variance. Formally, we say that a variable $X_i$ Granger-causes another variable $X_j$ when the past values of $X_i$ at time $t$ (i.e., $X^i_{t-\\tau},\\dots, X^i_{t-1}$) enhance the prediction of $X_j$ at time $t$ (i.e., $X^j_t$) compared to considering only the past values of $X_j$. The definition of Granger causality is as follows:\nDefinition 4 (Granger Causality) Let $\\mathcal{X} = {X_1,\\dots, X_d} \\in \\mathbb{R}^{d\\times T}$ be a observed dataset containing $d$ variables. If $\\sigma^2(X_j|X) < \\sigma^2(X_j|X - X_i)$, where $\\sigma^2(X_j|X)$ denotes the variance of predicting $X_j$ using $X$ with $\\tau$ time lags, we say that $X_i$ causes $X_j$, which is represented by $W^\\tau_{i,j} = 1$.\nIn simpler terms, Granger causality states that $X_i$ Granger-causes $X_j$ if past values of $X_i$ (i.e., $X^i_t$) provide unique and statistically significant information for predicting future values of $X_j$ (i.e., $X^j_t$). Therefore, following the definition of Granger causality, we can approach causal discovery as an autoregressive problem."}, {"title": "3 Method", "content": "In this section, we introduce STIC, which involves four components: Window Representation, Time-Invariance Block, Mechanism-Invariance Block, and Parallel Blocks for Joint Training. The process is depicted in Figure 2. Firstly, we transform the observed time series into a window representation format, leveraging Lemma 1. Next, we input the window representation into both the time-invariance block and the mechanism-invariance block ($B_t$ and $B_m$ in Figure 2). Finally, we conduct joint training using the extracted features from two kinds of parallel blocks. In particular, the time-invariance block $B_t$ generates the estimated window causal matrix $\\hat{W}$. To better represent the symbols used in Section 3, we also arrange a table to summarize and show their definitions, as shown in Table 2. The subsequent subsections provide a detailed explanation of the key components of STIC."}, {"title": "3.1 Window Representation", "content": "The observed dataset $\\mathcal{X} \\in \\mathbb{R}^{d\\times T}$ contains $d$ observed continuous time series (variables) with $T$ time steps. We also define a predefined maximum time lag as $\\tau$. To ensure that the entire causal contemporaneous and time-lagged influence is observed, we calculate the minimum length of the window that can capture this influence as $\\hat{\\tau} = \\tau + 1$. To construct the window observations, we select the observed values from the first $T - 1$ time steps, i.e. $\\mathcal{X}_{1:T-1} = {X_{1:T-1}^1,...,X_{1:T-1}^d} \\in []\\mathbb{R}^{d\\times(T-1)}$. Using a sliding window approach along the temporal chain of observations, we create window observations of length $\\hat{\\tau}$ and width $d$, with a step size of 1. This process results in $c = T - \\hat{\\tau}$ window"}, {"title": "3.2 Time-Invariance Block", "content": "According to Definition 2, the causal relationships among variables remain unchanged as time progresses. Exploiting this property, we can extract shared information from the window representation $W$ and utilize it to finally obtain the estimated window causal matrix $\\hat{W}$. Inspired by convolutional neural networks used in causal discovery[Nauta et al., 2019], we introduce a invariance-based convolutional network structure denoted as $B_t$ to incorporate temporal information within the window representation $W$. For each window observation $W_\\psi \\in \\mathbb{R}^{d\\times\\hat{\\tau}}$, we employ the following formula to aggregate similar information among the time series within the window observations\n\\begin{equation}\n\\hat{\\mathcal{W}} = f_1(K_t \\odot W_1, ..., K_t \\odot W_c)\n\\end{equation}\nHere, shared $K_t \\in \\mathbb{R}^{d\\times\\hat{\\tau}}$ represents a learnable extraction kernel utilized to extract information from each window observation. The symbol $\\odot$ denotes the Hadamard product between matrices, and $f_1$ refers to a neural network structure. By applying the Hadamard product with the shared kernel $K_t$, the resulting output exhibits similar characteristics across the time series. Moreover, $K_t$ serves as a time-invariant feature extractor, capturing recurring patterns that appear in the input series and aiding in forecasting short-term future values of the target variable. In Granger causality, these learned patterns reflect causal relationships between time series, which are essential for causal discovery [Nauta, 2018]. To ensure the generality of STIC, we employ a simple feed-forward neural network (FNN) $f_1 : \\mathbb{R}^{c\\times d\\times\\hat{\\tau}} \\rightarrow \\mathbb{R}^{d\\times d>\\hat{\\tau}}$ to extract shared information from each $K_t\\odot W_\\psi, \\psi = 1, ..., c$. Furthermore, we impose a constraint to prohibit self-loops in the estimated window causal matrix $\\hat{W}$ when the time lag is zero. That is:\n\\begin{equation}\n\\hat{W}^\\tau_{i,j} =\n\\begin{cases}\n0 & \\text{if } i = j \\text{ and } \\tau = 0 \\\\\n0 & \\text{if } \\hat{W}^\\tau_{i,j} < \\rho \\\\\n1 & \\text{else}\n\\end{cases}\n\\end{equation}"}, {"title": "3.3 Mechanism-Invariance Block", "content": "As stated in Definition 3, the causal conditional probability relationships among the time series remain unchanged as time varies. Consequently, the causal functions between variables also remain constant over time. With this in mind, our objective in $B_m$ is to find a unified transform function $f_2 : \\mathbb{R}^{d\\times\\hat{\\tau}} \\rightarrow \\mathbb{R}^{d\\times\\hat{\\tau}}$ that accommodates all window observations. To achieve this goal, as depicted in Figure 2, we employ a convolution kernel $K_m \\in \\mathbb{R}^{d\\times\\hat{\\tau}}$ as $f_2$. This kernel performs a Hadamard product operation with each window $W_\\psi \\in \\mathbb{R}^{d\\times\\hat{\\tau}}$ in $W$, where $\\psi = 1, ..., c$. Subsequently, we employ the Parametric Rectified Linear Unit (PReLU) activation function [Zhu et al., 2017] to obtain the output $W_\\psi \\in \\mathbb{R}^{d\\times\\hat{\\tau}}$,\n\\begin{equation}\n\\widetilde{W}_\\psi = PReLU(K_m \\odot W_\\psi)\n\\end{equation}\nEach $\\widetilde{W}_\\psi$ represents the transformed matrix obtained from the window observation $W_\\psi$ by a unified transform function $f_2$ implemented with convolution kernel $K_m$. Each $\\widetilde{W}_\\psi$ is finally used to predict $\\hat{X}_{t+\\hat{\\tau}}$. Note that this transform function $f_2$ can also be composed of $N$ different but equal dimensional kernels $K^1_m, ..., K^N_m \\in \\mathbb{R}^{d\\times\\hat{\\tau}}$, which are nested to perform complex nonlinear transformations. After $f_2$, the value inside the window $\\widetilde{W}_\\psi \\in \\mathbb{R}^{d\\times\\hat{\\tau}}$ is then pressed for"}, {"title": "3.4 Parallel Blocks for Joint Training", "content": "So far, we have obtained the estimated window causal matrix $\\hat{W}$ by using $B_t$. In addition, we also obtained the transformed matrix $\\widetilde{W}$ with $B_m$. We used convolutional neural networks in both $B_t$ and $B_m$. Their structures are similar, but their functions and purposes are different. In $B_t$, we focus on the shared underlying unified structure of all window observations. Following the Definition 2 of short-term time invariance, we choose a convolutional neural network structure with translation invariance [Kayhan and Gemert, 2020, Singh et al., 2023]. We expect that $f_1$ with $K_t$ as the main component can extract the invariant structure of the window representation $W$. In $B_m$, we focus on the convolution kernel $K_m$, which is expected to serve as a unified transform function $f_2$ to satisfy the Definition 3 of short-term mechanism invariance and perform complex nonlinear transformations."}, {"title": "4 Experiment Results", "content": "In this section, we present a comprehensive series of experiments on both synthetic and benchmark datasets to verify the effectiveness of the proposed STIC. Following the experimental setup of [Runge et al., 2019, Runge, 2020], we compare STIC against the constraint-based approaches such as PCMCI [Runge et al., 2019] and PCMCI+ [Runge, 2020], the score-based approaches such as DYNOTEARS [Pamfil et al., 2020], and the Granger-based approaches TCDF [Nauta et al., 2019], CUTS [Cheng et al., 2022] and CUTS+ [Cheng et al., 2023].\nOur causal discovery algorithm is implemented using PyTorch. The source code for our algorithm is publicly available at the following URL 1. Both the time-invariance block and mechanism-invariance block are implemented using convolutional neural networks.\nFirstly, we conducted experiments on synthetic datasets, encompassing both linear and non-linear cases. The methods of generating synthetic datasets for both linear and non-linear cases will be introduced separately in Section 4.2. Secondly, we proceeded to perform experiments on benchmark datasets to demonstrate the practical value of our model in Section 4.3. Thirdly, to evaluate the sensitivity of hyper-parameters, such as the learning rate (default 1e\u00af5), the predefined$\\tau$ (default 0.4d) and the threshold$\\rho$ (default 0.3), we conducted ablation experiments as detailed in Section 4.4.\nWe employ two kinds of evaluation metrics to assess the quality of the estimated causal matrix: the F1 score and precision. A higher F1 score indicates a more comprehensive estimation of the window causal matrix, while a higher precision indicates the ability to identify a larger number of causal edges. In this paper, we consider causal edges with different time lags for the same pair of variables as distinct causal edges. Specifically, if there exists a causal edge from $X_i$ to $X_j$ with a time lag of $\\tau_1$, and another causal edge from $X_i$ to $X_j$ with the time lags of $\\tau_2$, where $i \\neq j$ and $\\tau_1 \\neq \\tau_2$, we regard these as two separate causal edges. Due to the need to predefine the maximum time lag in STIC, we truncate the estimated $W \\in ]\\mathbb{R}^{d\\times d\\times (\\tau+1)}$ to $\\hat{W} \\in \\mathbb{R}^{d\\times d\\times (\\tau+1)}$ and then compute the evaluation metrics. We handle other baselines (such as PCMCI, PCMCI+, DYNOTEARS, CUTS, CUTS+) requiring a predefined maximum time lag parameter in the same manner."}, {"title": "4.1 Baselines", "content": "We select six state-of-the-art causal discovery methods as baselines for comparison:\n\u2022 PCMCI [Runge et al., 2019] is a notable work that extends the PC algorithm [Kalisch and B\u00fchlman, 2007] for causal discovery from time-series data. The source code for PCMCI is available at https://github."}, {"title": "4.2 Experiments on Synthetic Datasets", "content": "We generate synthetic datasets in the following manner. Firstly, we consider several typical challenges [Runge et al., 2019, Runge, 2020] with contemporaneous and time-lagged causal dependencies, following an additive noise model. We set the ground truth maximum time lag to 0.4d and initialize the existence of each edge in the true window causal matrix $W$ with a probability of 50%. For each variable $X_i$, its relation to with its parents $Pa_\\mathcal{G}(X_i)$ is defined as $X_i = f_i(Pa_\\mathcal{G}(X_i)) + \\varepsilon_i$, where $f_i$ represents the ground truth transformation function between $X_i$'s parents $Pa_\\mathcal{G}(X_i)$ and $X_i$. If $X_j \\in Pa_\\mathcal{G}(X_i)$, then in the ground truth causal matrix $W$, $W^0_{i,j} = 1$. Secondly, for linear datasets, each $f_i$ is defined by a weighted linear function, while for nonlinear datasets, each $f_i$ is defined using a weighted cosine function. We sample the weights from a uniform distribution, such that if a causal edge exists, the corresponding weight in the additive noise model is sampled from the interval $\\mathcal{U}([-2, -0.5] \\cup [0.5, 2))$ to ensure non-zero values. For non-causal edges, the weight is set to 0. The noise term $\\varepsilon_i$ follows either a standard normal distribution $\\mathcal{N}(0, 1)$ or is uniformly sampled from the interval $\\mathcal{U}[0, 1]$. These data-generating procedures are similar to those used by the PCMCI family [Runge et al., 2019, Runge, 2020] and CUTS family [Cheng et al., 2022, 2023].\nIn the following, we present different results on linear Gaussian datasets (Section 4.2.1), nonlinear Gaussian datasets (Section 4.2.2), and linear uniform datasets (Section 4.2.3) to demonstrate the superiority of our model. Specifically, to reduce the impact of random initialization, we conduct 10 experiments for each type of datasets and report the mean and variance of the experimental results."}, {"title": "4.2.1 Linear Gaussian Datasets", "content": "The data generation process for linear Gaussian datasets follows the relationship $X_i = w_iPa_\\mathcal{G}(X_i) + \\varepsilon_i$, where $\\varepsilon_i$ is sampled from a standard normal distribution $\\mathcal{N} (0, 1)$. To demonstrate the capability of our model in causal discovery from time-series data on datasets of varying sizes, we compare STIC with baselines under different conditions, including different numbers of variables ($d = {5, 10, 15, 20}$) and different lengths of time steps ($T = {100, 200, 500, 1000}$).\nThe results are summarized in Figure 4. Figure 4 left presents the variation of F1 score as the number of variables increases, while Figure 4 right shows the variation of precision with the number of variables. A comprehensive analysis of the experiments requires the joint consideration of both Figure 4 left and right. From a macroscopic perspective, our proposed STIC achieves the highest F1 scores on linear Gaussian datasets, while precision reaches the state-of-the-art levels in most cases. We will compare the performance of STIC and the baselines from two aspects of analysis:\nAspect 1: The relationship between the number of variables d and the model when T remains constant. When the observed time steps is fixed at T = 1000, corresponding to the top-left graphs in Figure 4 left and right, we observe that as the number of variables increases, the F1 scores of all causal discovery methods tend to decrease. However, our proposed STIC achieves an average F1 score of 0.86, 0.77, 0.79, 0.77 and an average precision of 0.80, 0.62, 0.74, 0.72 across the four different numbers of variables, surpassing other strong baselines. By comparing the line plots in the corresponding positions of Figure 4 left and right, especially when T = 100, corresponding to the bottom-right graphs in Figure 4 left and right, we find that our proposed STIC achieves an average F1 score of 0.76, 0.76, 0.77, 0.65 and an average precision of 0.66, 0.70, 0.77, 0.65 across the four different numbers of variables, significantly outperforming other strong baselines.\nIn the case of fixed observed time steps, as the number of variables increases, constraint-based approaches such as PCMCI and PCMCI+ suffer from severe performance degradation because they require significant prior knowledge involvement in determining the threshold$\\rho$, which determines the presence of causal edges. For score-based methods, the DYNOTEARS method shows relatively stable performance as the number of variables increases, but it does not achieve the optimal performance among all methods. As for Granger-based methods, CUTS and CUTS+ often suffer from poor performance due to the inability to recognize time lags. Our proposed STIC and the TCDF method achieve competitive results in terms of F1 scores. However, our method exhibits higher precision.\nWe attribute this superior performance to the window representation employed in STIC. By repeatedly extracting features from observed time series in different window observations, such representation acts as a form of data augmentation and aggregation. It enables a macroscopic view of common characteristics among multiple window observations, facilitating the learning of more accurate causal structures. Thus, our STIC model achieves optimal performance when the number of variables d changes.\nAspect 2: The relationship between the observed time steps T and the model when d remains constant. When examining the impact of observed time steps T on the models while keeping the number of variables constant, we observe that our STIC method consistently maintains an F1 score of approximately 0.7 across different values of T. However, PCMCI+ and DYNOTEARS exhibit a significant decline in their F1 scores as T decreases. For instance, at T = 1000, PCMCI+ and DYNOTEARS perform similarly to our STIC method, but at T = 100, their F1 scores drop to half of that achieved by our STIC method. For PCMCI, it consistently falls behind our STIC method, regardless"}, {"title": "4.2.2 Nonlinear Gaussian Datasets", "content": "In this section, we perform experiments on nonlinear Gaussian datasets to evaluate the performance of STIC. We set the number of variables (d = 5) and the observed time steps (T = 1000). For each $X_i$, its relationship with its parents $Pa_\\mathcal{G}(X_i)$ is defined using the cosine function, and the noise term $\\varepsilon_i$ follows the standard normal distribution.\nThe performance of STIC and the baselines is visualized in Figure 5. It can be observed that STIC achieves an F1 score of 0.44, which is higher than all baselines (PCMCI: 0.41, PCMCI+: 0.43, DYNOTEARS: 0.22, TCDF: 0.41, CUTS: 0.24, CUTS+: 0.37). It can be seen that STIC achieves a higher F1 score despite having lower precision compared to the other baselines. For constraint-based methods (PCMCI and PCMCI+), one possible reason for achieving similar F1 scores with our proposed STIC is that the length of observed time steps is set to 1000, which is sufficient for statistical independence tests. Thus, the conditional independence tests can directly operate on the data without being affected by noise. Regarding score-based methods, we believe that DYNOTEARS uses a simple network that may not effectively"}, {"title": "4.2.3 Linear Uniform Datasets", "content": "The linear uniform datasets is generated with observed time steps ($T = 1000$) by varied numbers of variables ($d = {5, 10, 15, 20}$). For each $X_i$, $f_i$ is set as a linear function, while the noise term $\\varepsilon_i$ follows a uniform distribution $\\mathcal{U}[0, 1"}]}