{"title": "Exploring the extent of similarities in software failures across industries using LLMS", "authors": ["Martin Detloff"], "abstract": "The rapid evolution of software development necessitates enhanced safety measures. Extracting information about software failures from companies is becoming increasingly more available through news articles.\nThis research utilizes the Failure Analysis Investigation with LLMs (FAIL) model to extract industry-specific information. Although the FAIL model's database is rich in information, it could benefit from further categorization and industry-specific insights to further assist software engineers.\nIn previous work news articles were collected from reputable sources and categorized by incidents inside a database. Prompt engineering and Large Language Models (LLMs) were then applied to extract relevant information regarding the software failure. This research extends these methods by categorizing articles into specific domains and types of software failures. The results are visually represented through graphs.\nThe analysis shows that throughout the database some software failures occur significantly more often in specific industries. This categorization provides a valuable resource for software engineers and companies to identify and address common failures.\nThis research highlights the synergy between software engineering and Large Language Models (LLMs) to automate and enhance the analysis of software failures. By transforming data from the database into an industry specific model, we provide a valuable resource that can be used to identify common vulnerabilities, predict potential risks, and implement proactive measures for preventing software failures. Leveraging the power of the current FAIL database and data visualization, we aim to provide an avenue for safer and more secure software in the future.", "sections": [{"title": "Introduction", "content": "The software industry's dynamic nature demands continual advancements in safety protocols and failure analysis techniques. Current methodologies for studying software failures often face set backs due to restrictive data access and privacy concerns. The news seems to be the optimal route for collecting information using Large Language Models (LLMs).\nThe FAIL database provides a well established foundation, compiling detailed reports of software failures from various news outlets. However, there exists a significant opportunity to delve deeper into the contextual specifics of these failures, such as the industries most affected and the predominant types of failures in each industry.\nIn response to these needs, this study expanded the use of the FAIL model by categorizing failures not just broadly but within specific industry contexts. This involved enhancing data extraction techniques through prompt engineering with LLMs, aiming to achieve a more nuanced understanding of where and why these failures occur.\nOur analysis demonstrated that some types of software failures are notably more prevalent in certain industries. By identifying patterns, we provide insights that can drive targeted improvements in software practices and policies, potentially leading to substantial reductions in software failures."}, {"title": "Background And Significance", "content": "The Securities and Exchange Commission (SEC) recently made a ruling requiring public companies to disclose cyber-security related failures. This means that any breach in security deemed to have potential significant impact or harm must be reported within 4 business days of the incident. Public companies must also create reports annually of cyber-security risk management strategies, and governance [7]. With this increased transparency requirement in the industry, the FAIL model, and industry specific insights will only become more useful and prevalent in the future."}, {"title": "Related Work", "content": "This work is directly correlated to the work done by FAIL: Analyzing Software Failures from the News Using LLMs [1], materializing a way to automate the process of gathering information on software failures from news articles. In this paper they explored an efficient approach to automate the process of collecting information on recent software failures from the news. They collected information only from articles that the LLMs deemed to be having sufficient enough information to analyze. They then categorized this data into summaries, postmortems, and taxonomy. They collected valuable information, and the information is informative. The information could benefit from further insights into industry-specific failures."}, {"title": "Methodology", "content": "We began by creating a prompt from which we collect data from the database, and categorize the data into industry-specific failures. The initial taxonomy was adopted from [8, 9, 10] which give taxonomies for categorizing software failures. The temperature of the LLM that we used was manually set to 0 to increase consistency, and we used chatgpt-3.5-turbo. Initially the prompt contained general instructions, such as \"categorize these prompts by type of software failure.\u201d However the prompt's performance was sub optimal. Effective prompts need to be clear and concise to guide the Large Language Model (LLM) in producing the desired output [2]. The initial prompt generated results with high variability or \"randomness\". To improve accuracy, we revised the prompt with a specific list of common software failures manually curated based on descriptions in the database, and taxonomies widely used in the industry for categorizing software failures. [10, 9] We ensured the prompt only returned the type of software failure. This adjustment yielded more consistent results by the LLM, but our approach was still producing some incorrect categorizations. After refining the prompt and using few-shot prompting, the prompt performed significantly better. Through the use of a confusion matrix and a small dataset of 90 articles, the overall average accuracy of the prompt was 76 percent (reference Figure 1 for an example of the matrix used). With this accuracy we let the script run through the database, and categorize each software failure by industry. Results are shown visually through bar graphs."}, {"title": "Results", "content": "This section presents the analysis of software failure frequencies across different industries."}, {"title": "Overview", "content": "This section presents the analysis of software failure frequencies across different industries."}, {"title": "Finance Industry", "content": "The graph in figure 2a shows that security-related failures are the most common in the finance industry, with Security vulnerabilities being significantly more frequent than other types of failures."}, {"title": "Healthcare Industry", "content": "As shown in figure 2b shows security vulnerabilities are the most common in the healthcare industry. This could be due to the healthcare industry storing a vast amount of patient data with inadequate security."}, {"title": "Information Industry", "content": "As shown in figure 2c shows security vulnerabilities are most common in the information industry. The vast amount of data within the IT (information Technology) industry increases the risk of security related failures."}, {"title": "Knowledge Industry", "content": "As shown in figure 2d shows security vulnerabilities are most common in the Knowledge industry. This industry includes educational institutions, research organizations, and knowledge-based enterprises. Security related failures could be happening more frequently due to the openness of sharing information in an educational environment."}, {"title": "Transportation Industry", "content": "As shown in figure 2e shows functionality bugs are most common in the Transportation industry. This industry relies heavily on complex logistics and random variables, which could in effect be the reason we are seeing more functionality bugs."}, {"title": "Entertainment Industry", "content": "As shown in figure 2f shows security vulnerabilities are most common in the Entertainment industry. This industry has a large audience, and often uses third party software to manage data, which could lead to increased vulnerabilities."}, {"title": "Government Sector", "content": "As shown in figure 3a shows security vulnerabilities are most common in the Government Sector. With the government sector handling a large range of sensitive information, it's no surprise that security vulnerabilities are at the forefront of software failures for this domain."}, {"title": "Discussion", "content": ""}, {"title": "Interpretation of Results:", "content": "Our results indicate that certain software failures occur significantly more frequently in specific industries. For instance, the analysis revealed that the information industry experiences a higher incidence of security-related failures, while the transportation industry often encounters functionality issues. These findings highlight the distinct challenges faced by different sectors, underscoring the need for tailored approaches to software reliability. We can also see a trend across industries, suggesting that many different industries are facing the same software development challenges.\nOur study dives deeper by quantifying these differences and providing a detailed breakdown of failure types within each industry. This granularity offers new insights into the specific vulnerabilities and common issues faced by different sectors."}, {"title": "Implications of Findings", "content": "Our study provides industry-specific insights into software failures to better cater efforts towards a specific industry.\nThis could allow for more efficient software maintenance and improvements when a lack of resources is present."}, {"title": "Policy and Best Practices", "content": "Given that security vulnerabilities are the most frequent software failure across the large majority of industries, we recommend the following policy and best practices based upon studies such as [3, 4, 5]:\n1. Regular Security Audits: Conduct regular security audits to identify and mitigate vulnerabilities in the software. This includes code reviews, penetration testing,"}, {"title": "Limitations:", "content": "Our study uses only the data found in the FAIL database, which does not capture every failure across every industry and can be seen as limited.\nOur study utilizes Large Language Models to categorize software failures. Large Language models can be inaccurate, and have the possibility of giving false information known as hallucinations.\nThe FAIL database uses news articles as a source of collecting information on software failures. News can have biases, and have only bits of information."}, {"title": "Conclusion", "content": "In conclusion our study highlights the importance of industry-specific software failure distinctions. By identifying the most common failures in each specific sector we can inform more directive preventive measures towards software failures. These findings have significant implications for software engineers and industry best practices, paving the way for a safer software and practices in the future.\nOur research provides a foundation for future research to build upon, with the potential to enhance the FAIL model with new tools, and better informational gathering techniques. Ultimately our work contributes to a deeper understanding of software failures and supports the development of targeted solutions to prevent industry-specific software failures in the future."}]}