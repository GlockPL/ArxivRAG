{"title": "COUNTERFACTUAL EFFECT DECOMPOSITION IN MULTI-AGENT SEQUENTIAL DECISION MAKING", "authors": ["Stelios Triantafyllou", "Aleksa Sukovic", "Yasaman Zolfimoselo", "Goran Radanovic"], "abstract": "We address the challenge of explaining counterfactual outcomes in multi-agent Markov decision processes. In particular, we aim to explain the total counterfactual effect of an agent's action on the outcome of a realized scenario through its influence on the environment dynamics and the agents' behavior. To achieve this, we introduce a novel causal explanation formula that decomposes the counterfactual effect by attributing to each agent and state variable a score reflecting their respective contributions to the effect. First, we show that the total counterfactual effect of an agent's action can be decomposed into two components: one measuring the effect that propagates through all subsequent agents' actions and another related to the effect that propagates through the state transitions. Building on recent advancements in causal contribution analysis, we further decompose these two effects as follows. For the former, we consider agent-specific effects a causal concept that quantifies the counterfactual effect of an agent's action that propagates through a subset of agents. Based on this notion, we use Shapley value to attribute the effect to individual agents. For the latter, we consider the concept of structure-preserving interventions and attribute the effect to state variables based on their \u201cintrinsic\u201d contributions. Through extensive experimentation, we demonstrate the interpretability of our decomposition approach in a Gridworld environment with LLM-assisted agents and a sepsis management simulator.", "sections": [{"title": "1 INTRODUCTION", "content": "Applying counterfactual reasoning to retrospectively analyze the impact of different actions in decision making scenarios is fundamental for accountability. For instance, counterfactual reasoning can be employed to identify actual causes (Halpern, 2016; Triantafyllou et al., 2022), attribute responsibility (Chockler & Halpern, 2004; Friedenberg & Halpern, 2019), generate explanations (Madumal et al., 2020; Tsirtsis et al., 2021), evaluate fairness (Kusner et al., 2017; Huang et al., 2022) and measure harm (Richens et al., 2022; Beckers et al., 2022). To achieve such objectives, many studies often rely on the notion of total counterfactual effects, which quantifies the extent to which an alternative action would have affected the outcome of a realized scenario.\nIn multi-agent sequential decision making, an agent's action typically affects the outcome indirectly. To illustrate this, consider the problem of AI-assisted decision making in healthcare (Lynn, 2019), where a clinician and their AI assistant treat a patient over a period of time. Fig. 1a depicts a specific example, where treatment fails. We estimate that if the clinician had not followed the AI's recommendation at step 10 and administered vasopressors (V) instead of mechanical ventilation (E), the treatment would have been successful with an 82% likelihood. Therefore, the considered alternative action admits a high total counterfactual effect. This effect, however, propagates through all subsequent actions of the clinician and the AI, as well as all the changes in the patient's state. This makes the interpretability of the effect more nuanced, as the change from action to outcome can be transmitted by multiple distinct causal mechanisms. In this work, we ask:\nHow to explain an action's total counterfactual effect in multi-agent sequential decision making?"}, {"title": "2 BACKGROUND AND FORMAL FRAMEWORK", "content": "In this section, we present our formal framework, which is adopted from Triantafyllou et al. (2024) and builds on Multi-Agent Markov Decision Processes (MMDPs) (Boutilier, 1996) and Structural Causal Models (SCMs) (Pearl, 2009).\n2.1 MULTI-AGENT MARKOV DECISION PROCESSES\nAn MMDP is represented as a tuple $(S, \\{1, ..., n\\}, A, T, h, \\sigma)$, where: $S$ is the state space; $\\{1, ..., n\\}$ is the set of agents; $A = \\times_{i=1}^n A_i$ is the joint action space, with $A_i$ being the action space of agent i;\n$T:S\\times A \\times S \\rightarrow [0,1]$ is the transitions probability function; $h$ is the finite time horizon; $\\sigma$ is the initial state distribution.\u00b9 Each agent $i \\in \\{1, ..., n\\}$ has a stationary decision-making policy $\\pi_i$, with the joint policy of all agents represented as $\\pi$. The probability of agents jointly taking action $a_t = (a_{1,t},..., a_{n,t})$ in state $s_t$ at time t is thus given by $\\pi(a_t|s_t) = \\pi_1(A_{1,t}|s_t)\\cdot\\pi_n(a_{n,t}|s_t)$, while the probability of transitioning from state $s_t$ to state $s_{t+1}$ is determined by $T(s_{t+1}|s_t, a_t)$. A sequence of such state-action pairs $\\{(s_t, a_t)\\}_{t\\in\\{0,...,h-1\\}}$ and final state $s_h$ is called a trajectory. With $\\tau(X)$, we denote the value of variable X in trajectory $\\tau$.\n2.2 MMDPS AND STRUCTURAL CAUSAL MODELS\nWe utilize the MMDP-SCM framework (Triantafyllou et al., 2024) to express an MMDP coupled with a joint policy $\\pi$ as an SCM. Specifically, an MMDP-SCM $(V, U, P(u), F)$ consists of\n(i) a tuple $V = (S_0, A_{1,0}, ..., A_{n,0},..., S_h)$ of the observed variables whose causal relations are modelled, i.e., all state and action variables of the MMDP;\n(ii) a tuple $U = (U_{S_0},U_{A_{1,0}},...,U_{A_{n,0}}, ..., U_{S_h})$ of mutually independent unobserved noise variables which capture any underlying stochasticity of the MMDP and agents' policies;\n(iii) A joint probability distribution $P(u) = \\prod_{u^i\\in U} P(u^i)$ over U;\n(iv) A collection F of deterministic functions that determine the values of all observed variables in V via the following structural equations\n$S_0 := f_{S_0}(U_{S_0}); \\; S_t := f_t(S_{t-1}, A_{t-1}, U_{S_t}); \\; A_{i,t} := f_{A_{i,t}}(S_t, U_{A_{i,t}})$. (1)\nNote that any context $u \\sim P(u)$ induces a unique trajectory $\\tau$, such that $\\forall X \\in V$ it holds that $(\\tau(X))$ is the solution of X, for the particular u, in the MMDP-SCM. Furthermore, similar to general SCMs, the MMDP-SCM induces a directed causal graph, which can be found in Appendix C. Appendix B also describes the conditions under which the observational distribution of an MMDP-SCM is consistent with some MMDP and joint policy. In this paper, we focus on categorical MMDP-SCMs.\n2.3 INTERVENTIONS AND COUNTERFACTUALS\nConsider an MMDP-SCM M. An intervention on the action variable $A_{i,t}$ of M corresponds to the process of modifying the structural equation $A_{i,t} := f_{A_{i,t}}(S_t, U_{A_{i,t}})$ from Eq. 1. More specifically, a hard intervention $do(A_{i,t} := a_{i,t})$ fixes the value of $A_{i,t}$ to the constant $a_{i,t}$, resulting in a new MMDP-SCM denoted by $M_{do(A_{i,t}:=a_{i,t})}$. Similar to Correa et al. (2021), when random variables have subscripts we will use square brackets to denote interventions.\nFor ease of notation, rewards are considered part of the states."}, {"title": "3 DECOMPOSING THE TOTAL COUNTERFACTUAL EFFECT", "content": "The total counterfactual effect can inform us about the extent to which an alternative action would have affected the outcome of a trajectory. However, this measure alone does not provide any further insights on why or how that action would have affected the outcome. In this section, we introduce a novel causal explanation formula that decomposes TCFE w.r.t. the two building blocks of an MMDP \u2013 its states and its agents. First, based on prior work we define two causal quantities for measuring how much of the total counterfactual effect of an agent's action on some response variable is mediated by (a) all future agents' actions and (b) the subsequent MMDP's state transitions.\nDefinition 3.1 (tot-ASE). Given an MMDP-SCM M and a trajectory $\\tau$ of M, the total agent-specific effect of intervention $do(A_{i,t} := a_{i,t})$ on $Y \\in V$, relative to reference $\\tau(A_{i,t})$, is defined as\n$ASE_{a_{i,t},\\tau(A_{i,t})}^{(tot)} (Y|\\tau)_M = E[Y|\\tau; M]_{M_{do(I)}} \u2013 E[Y_{\\tau(A_{i,t})}|\\tau]_M = E[Y|\\tau; M]_{M_{do(I)}} - \\tau(Y),$ where $I = \\{A_{i',t'} := A_{i',t'}[a_{i,t}]\\[/i']_{i'\\in\\{1,...,n\\},t'>t}\nDefinition 3.2 (SSE). Given an MMDP-SCM M and a trajectory $\\tau$ of M, the state-specific effect of intervention $do(A_{i,t} := a_{i,t})$ on $Y \\in V$, relative to reference $\\tau(A_{i,t})$, is defined as\n$SSE_{a_{i,t},\\tau(A_{i,t})}(Y|\\tau)_M = E[Y_{a_{i,t}}|\\tau; M]_{M_{do(I)}} \u2013 E[Y_{\\tau(A_{i,t})}|\\tau]_M = E[Y_{a_{i,t}}|\\tau; M]_{M_{do(I)}} - \\tau(Y),$ where $I = \\{A_{i',t'} := A_{i',t'}[\\tau(A_{i',t'})]\\[/i']_{i'\\in\\{1,...,n\\},t'>t}$*\nIn words, Definition 3.1 measures the difference between the factual value of Y, i.e., $\\tau(Y)$, and the (expected) counterfactual value of Y had all agents taken the actions that they would naturally take under intervention $do(A_{i,t} := a_{i,t})$ after time-step t. On the other hand, Definition 3.2 measures the\n\u00b2In this paper, we consider counterfactual effects mostly relative to the factual action ($A_{i,t}$). However, we note that generally any valid action can be used as a reference value."}, {"title": "4 DECOMPOSING THE REVERSE STATE-SPECIFIC EFFECT", "content": "In this section, we focus on further decomposing the reverse state-specific effect. More specifically, our goal is to attribute to each state variable a score reflecting its contribution to r-SSE. Our approach utilizes the notion of intrinsic causal contributions (ICC) introduced by Janzing et al. (2024). For general SCMs, the ICC of an observed variable X to a target variable Y measures the reduction of uncertainty in Y when conditioning on the noise variable UX. In our work, we model uncertainty using the expected conditional variance and modify the ICC definition to quantify the influence of state variables on the variation of r-SSE.\nLet $k \\in \\{0,..., h\\}$. We denote with $U_{S_k}$ the set $(U_{S_k},U_{A_{1,k}}, ...,U_{A_{n,k}})$ for $k < h$, and with $U_{S_h}$ the set $(U_{S_h})$. We also denote with $U^{<S_k}$ the set of noise terms associated with the observed variables preceding (chronologically) $S_k$. Note that r-SSE essentially measures the expected value of the difference $\\Delta Y_{I,a_{i,t}} = Y_I - Y_{a_{i,t}}$ in M, when noise terms U are sampled from the posterior distribution $P(u|\\tau)$. Thus, the ICC can be defined in our context as follows\n$ICC(S_k \\rightarrow \\Delta Y_{I,a_{i,t}}|\\tau) = E[Var(\\Delta Y_{I,a_{i,t}}|\\tau, U^{<S_k})|\\tau] \u2013 E[Var(\\Delta Y_{I,a_{i,t}}|\\tau, U^{<S_k}, U^{S_k})|\\tau]$, (4)\nwhere $I = \\{A_{i',t'} := A_{i',t'}[a_{i,t}]\\[/i']_{i'\\in\\{1,...,n\\},t'>t}$. In words, Eq. 4 measures the reduction of variance in $\\Delta Y_{I,a_{i,t}}$ caused by conditioning on the noise variables associated with state $S_k$ and the agents' actions taken therein. Based on this, we can now define our attribution method for r-SSE."}, {"title": "5 DECOMPOSING THE TOTAL AGENT-SPECIFIC EFFECT", "content": "In this section, we focus on further decomposing the total agent-specific effect. More specifically, our goal is to attribute to each agent a score reflecting its contribution to tot-ASE. Our approach is based on a well-established solution concept in cooperative game theory, the Shapley value (Shapley, 1953), and it utilizes the notion of agent-specific effects introduced by Triantafyllou et al. (2024).\nDefinition 5.1 (ASE). Given an MMDP-SCM M, a non-empty subset of agents N in M and a trajectory $\\tau$ of M, the N-specific effect of intervention $do(A_{i,t} := a_{i,t})$ on $Y \\in V$, relative to reference $\\tau(A_{i,t})$, is defined as\n$ASE_{a_{i,t},\\tau(A_{i,t})}^{N} (Y|\\tau)_M = E[Y|\\tau; M]_{M_{do(I)}} \u2013 E[Y_{\\tau(A_{i,t})}|\\tau]_M = E[Y|\\tau; M]_{M_{do(I)}} - \\tau(Y),$ where $I = \\{A_{i',t'} := \\tau(A_{i',t'})\\[/i']_{i' \\notin N,t'>t}\\cup\\{A_{i',t'} := A_{i',t'}[a_{i,t}]\\[/i']_{i'\\in N,t'>t\\}}$."}, {"title": "6 EXPERIMENTS", "content": "In this section, we empirically evaluate our approach to counterfactual effect decomposition using two environments, Gridworld and Sepsis. We refer the reader to Appendix I for more details on our experimental setup and implementation, and to Appendix J for additional results. Throughout both experiments, we use 100 posterior samples for estimating counterfactual effects. For the r-SSE-ICC method, we use 20 additional samples for the conditioning noise variables.\n6.1 GRIDWORLD EXPERIMENTS WITH LLM-ASSISTED RL AGENTS\nEnvironment. We consider the gridworld depicted in Fig. 2a, where two actors, $A_1$ and $A_2$, are tasked with delivering objects. In the beginning of each trajectory, two randomly sampled objects spawn in each of the boxes located on the rightmost corners of the gridworld. The color of each object determines its value. Colored cells indicate areas of large stochastic penalty, which is significantly reduced when actors carry an object of a matching color. Cells denoted with stars are delivery locations. If an object is delivered to the location with the matching color, then its value is rewarded.\nImplementation. We adopt a Planner-Actor-Reporter system akin to Dasgupta et al. (2023). Planner is implemented using a pre-trained LLM and few-shot learning, to provide actors with instructions. More specifically, Planner can instruct actors to: examine a box, pickup an object and deliver that object to a specific destination. Furthermore, we assume an optimal Reporter whose task is to report to Planner the necessary information about the state of the environment. In particular, Reporter provides information about the boxes' contents and which objects were picked up by the actors. Finally, the two actors are trained with deep RL to follow the Planner's instructions."}, {"title": "7 DISCUSSION", "content": "In this paper, we introduce a causal explanation framework tailored to multi-agent MDPs. Specifically, we decompose the total counterfactual effect of an agent's action by attributing it to the agents' behavior or environment dynamics. Our experimental results demonstrate that our decomposition provides valuable insights into the distinct roles that agents and environment play in influencing the effect. To the best of our knowledge, this is the first work that looks into the problem of counterfactual effect decomposition in the context of multi-agent sequential decision making. While our findings are promising, there are several directions for future exploration, which we outline below.\nComputational complexity. The computational complexity of our decomposition approach depends on the total number of agents and the length of the MMDP's time horizon. In our experiments, we use a relatively small number of agents and a horizon of a few dozen time-steps. We believe that many interesting multi-agent settings belong to this regime, e.g., human-AI collaboration. Nevertheless, there are settings in which computational complexity considerations can be important, and we see this as an interesting future research direction to explore. In Appendix H, we analyze the computational complexity of the ASE-SV and r-SSE-ICC methods, and discuss some potential mitigation strategies for when the number of agents or the time horizon are prohibitively large.\nCausal assumptions. Making causal assumptions in order to enable counterfactual identifiability is quite common in the literature. There is a plethora of works at the intersection of decision making and counterfactual reasoning that assumes exogeneity alongside additional causal properties, such as weak (Triantafyllou et al., 2024) or strong (Tsirtsis & Rodriguez, 2024) noise monotonicity, counterfactual stability (Oberst & Sontag, 2019), or access to the ground-truth causal model (Richens et al., 2022). However, these assumptions are often violated in practice. Thus, extending the applicability of our proposed approach to domains where our theoretical assumptions (exogeneity and weak noise monotonicity) do not hold would be of significant practical importance.\nApplications to accountable decision making. We deem the problem of decomposing counterfactual effects particularly relevant for multi-agent decision-making settings where accountability is paramount. Our approach can be applied in these settings, by integrating it into existing causal tools for retrospectively analyzing decision-making failures. For instance, consider methods for blame attribution in multi-agent systems (Halpern & Kleiman-Weiner, 2018; Friedenberg & Halpern, 2019). Typically, these methods first identify the agents' actions that were critical to the outcome, i.e., those that, had they been different, would have likely prevented failure. Next, they assess the agents' epistemic states, determining to what extent each agent could or should have predicted the consequences of acting differently. Our approach can enhance these methods by offering a more granular notion of blame. In the Sepsis scenario described in Section 1, for example, the clinician may be expected to predict how their actions directly affect the patient's state, but may not be expected to predict the Al's responses, especially if they have never worked with the current version of the model before. According to the output of our decomposition approach (Plot 1b), the clinician would then receive 73.5% of the total blame for their action, rather than bearing full responsibility. We see significant potential in combining our approach with existing works on blame attribution and related concepts in accountable decision making, offering practical benefits across various multi-agent domains."}, {"title": "A LIST OF APPENDICES", "content": "In this section, we provide a brief description of the content provided in the appendices of the paper.\n\u2022 Appendix B provides additional information on MMDP-SCMs.\n\u2022 Appendix C contains the causal graph of the MMDP-SCM from Section 2.2.\n\u2022 Appendix D provides additional information on noise monotonicity.\n\u2022 Appendix E formally states the properties defined in Section 5 for the ASE-SV method.\n\u2022 Appendix F outlines an algorithm for approximating the conditional variance from Eq. 4.\n\u2022 Appendix G contains the proofs of Theorems 3.3 and 4.2.\n\u2022 Appendix H provides a discussion on the computational complexity of the ASE-SV and r-SSE-ICC methods.\n\u2022 Appendix I provides additional information on the experimental setup and implementation details.\n\u2022 Appendix J includes additional experimental results."}, {"title": "B ADDITIONAL INFORMATION ON MMDP-SCMS", "content": "Consider an MMDP-SCM M = (V, U, P(u), F). For the observational distribution of M, P(V), to be consistent with an MMDP (S, {1, ..., n}, A, T, h, o) and a joint policy \u03c0, functions in F and noise distribution P(u) need to satisfy the following conditions for every (s, a, s') triplet:\n$\\int_{u_{S_0}: f_{S_0}(u_{S_0})=s} P(u) = P(S_0 = s|\\sigma);$\n$\\int_{u_{S_t}: f_{S_t}(s,a,u_{S_t})=s'} P(u_{S_t}) = T(s'|s, a);$\n$\\int_{u_{A_{i,t}}: f_{A_{i,t}}(s, u_{A_{i,t}})=a_i} P(u_{A_{i,t}}) = \\pi_i(a_i|s).$ (5)\nThe first two conditions in Eq. 5 guarantee that M induces the initial state distribution and state transition dynamics of the MMDP. The third condition makes sure that the action variables in M agree with the joint policy \u03c0."}, {"title": "D ADDITIONAL INFORMATION ON NOISE MONOTONICITY", "content": "In this section, we define the (weak) noise monotonicity property for categorical SCMs. It has been shown that noise monotonicity enables counterfactual identifiability. For more details on noise monotonicity and its connection to the identifiability problem, we refer the interested reader to Triantafyllou et al. (2024).\nDefinition D.1 (Noise Monotonicity). Given an SCM M with causal graph G, we say that vari- able $V^i\\in V$ is noise-monotonic in M w.r.t. a total ordering $ \\le_i$ on dom\\{$V$\\}, if for any $pa^i \\in dom\\{Pa^i(G)\\}$ and $u_1, u_2 \\sim P(U^i)$ s.t. $u_1 \\le u_2$, it holds that $f^i(pa^i, u_1) \\le_i f^i(pa^i, u_2)$.\nEssentially, noise monotonicity assumes that all observed variables in an SCM, or MMDP-SCM in our paper, are monotonic w.r.t. their corresponding noise variable (for some specified total order- ing). Note that noise monotonicity is not limiting for the MMDPs or agents' policies. In simple words, what noise monotonicity assumption restricts is the expressivity of counterfactual distribu- tions. There can be many MMDP-SCMs whose observational distribution is consistent with the MMDP, but admit different counterfactual distributions. Theorem 4.3 in Triantafyllou et al. (2024) shows that by limiting the class of possible MMDP-SCMs to the ones that satisfy noise monotonic- ity, counterfactual identifiability is guaranteed."}, {"title": "E PROPERTIES FOR ASE-SV", "content": "In this section, we formally state the properties defined in Section 5 for the ASE-SV method.\nEfficiency: The total sum of agents' contribution scores is equal to the total agent-specific effect. Formally,\n$\\sum_{j\\in\\{1,...,n\\}} \\Phi_j = ASE_{a_{i,t},\\tau(A_{i,t})}^{\\{1,...,n\\}} (Y|\\tau)_M$\nInvariance: Agents who do not marginally contribute to the total agent-specific effect are assigned a zero contribution score. Formally, if for every $S \\subseteq \\{1, ..., n\\}\\\\{j\\}$\n$ASE_{a_{i,t},\\tau(A_{i,t})}^{S\\cup\\{j\\}} (Y|\\tau)_M - ASE_{a_{i,t},\\tau(A_{i,t})}^{S} (Y|\\tau)_M = 0,$ then $\\Phi_j = 0$.\nSymmetry: Agents who contribute equally to the total agent-specific effect are assigned the same contribution score. Formally, if for every $S \\subset \\{1, ..., n\\}\\\\{j, k\\}$\n$ASE_{a_{i,t},\\tau(A_{i,t})}^{S\\cup\\{j\\}} (Y|\\tau)_M - ASE_{a_{i,t},\\tau(A_{i,t})}^{S} (Y|\\tau)_M = ASE_{a_{i,t},\\tau(A_{i,t})}^{S\\cup\\{k\\}} (Y|\\tau)_M - ASE_{a_{i,t},\\tau(A_{i,t})}^{S} (Y|\\tau)_M,$ then $\\Phi_j = \\Phi_k$.\nContribution monotonicity: The contribution score assigned to an agent depends only on its marginal contributions to the total agent-specific effect and monotonically so. Formally, let $M_1$ and $M_2$ be two MMDP-SCMs with n agents, if for every $S \\subseteq \\{1, ..., n\\}\\\\{j\\}$\n$ASE_{a_{i,t},\\tau(A_{i,t})}^{S\\cup\\{j\\}} (Y|\\tau)_{M_1} - ASE_{a_{i,t},\\tau(A_{i,t})}^{S} (Y|\\tau)_{M_1} \\ge ASE_{a_{i,t},\\tau(A_{i,t})}^{S\\cup\\{j\\}} (Y|\\tau)_{M_2} - ASE_{a_{i,t},\\tau(A_{i,t})}^{S} (Y|\\tau)_{M_2},$ then $\\Phi_j^{M_1} > \\Phi_j^{M_2}$."}, {"title": "F ALGORITHM FOR CONDITIONAL VARIANCE", "content": "In this section, we present our approach for approximating the expected conditional variance from Eq. 4. Algorithm 1 estimates $E[Var(\\Delta Y_{I,a_{i,t}} |\\tau, U^{<S_k})|\\tau]_M$. To estimate the conditional variance $E[Var(\\Delta Y_{I,a_{i,t}}|\\tau, U^{<S_k}, U^{S_k})|\\tau]_M$, it suffices to modify Algorithm 1 to sampling conditioning noise variables from $P(u^{<S_k}, u^{S_k}|\\tau)$ and non-conditioning ones from $P(u^{\\ge S_{k+1}}|\\tau)$.\n\\noindent\\fbox{\\begin{minipage}{\\textwidth}\nAlgorithm 1 Estimates $E[Var(\\Delta Y_{I,a_{i,t}}|\\tau,U^{<S_k})|\\tau]_M$\n\\textbf{Input:} MMDP-SCM M, trajectory $\\tau$, action variable $A_{i,t}$, action $a_{i,t}$, response variable $Y$, state variable $S_k$, number of conditioning/non-conditioning posterior samples $H_1/H_2$\n\\begin{algorithmic}[1]\n\\STATE $h_1 \\leftarrow 0$, $h_2 \\leftarrow 0$\n\\STATE $\\mu_1 \\leftarrow 0$, $\\mu_2 \\leftarrow 0$\n\\WHILE{$h_1 < H_1$}\n\\STATE $U_{cond} \\sim P(u^{<S_k}|\\tau)$ \\# Sample conditioning noise variables\n\\STATE $h_1 \\leftarrow h_1 +1$\n\\STATE $c_1 \\leftarrow 0$, $c_2 \\leftarrow 0$\n\\WHILE{$h_2 < H_2$}\n\\STATE $U_{non} \\sim P(u^{\\ge S_k}|\\tau)$ \\# Sample non-conditioning noise variables\n\\STATE $h_2 \\leftarrow h_2+1$\n\\STATE $u = (u_{cond}, U_{non})$\n\\STATE $\\tau_{cf} \\sim P(V|u)\\; M_{do(A_{i,t}:=a_{i,t})}$ \\# Compute counterfactual trajectory\n\\STATE $y_{cf} \\leftarrow \\tau_{cf}(Y)$\n\\STATE I $\\leftarrow \\{A_{i',t'} := \\tau_f(A_{i',t'})\\}_{i'\\in\\{1,...,n\\},t'>t}$\n\\STATE $y \\sim P(Y|u)\\; M_{do(I)}$ \\# Compute response to natural intervention\n\\STATE $c_1 \\leftarrow c_1 + (y - y_{cf})$\n\\STATE $c_2 \\leftarrow c_2 + (y - y_{cf})^2$\n\\ENDWHILE\n\\STATE $\\mu_1 \\leftarrow \\mu_1 + (\\frac{c_1}{H_2})^2$\n\\STATE $\\mu_2 \\leftarrow \\mu_2 + \\frac{c_2}{H_2}$\n\\STATE $h_2\\leftarrow 0$\n\\ENDWHILE\n\\RETURN $\\frac{\\mu_2}{H_1}-\\mu_1$\n\\end{algorithmic}\n\\end{minipage}}"}, {"title": "G PROOFS", "content": "G.1 PROOF OF THEOREM 3.3\nProof. Eq. 3 follows directly from Definition 2.1, Definition 3.1 and Eq. 2:\n$TCFE_{a_{i,t},\\tau(A_{i,t})}(Y|\\tau)_M = E[Y_{a_{i,t}}|\\tau]_M \u2013 \\tau(Y)$\n$= E[Y_{a_{i,t}}|\\tau]_M - \\tau(Y) + E[Y|\\tau; M]_{M_{do(I)}} - E[Y|\\tau; M]_{M_{do(I)}}$\n$= ASE_{a_{i,t},\\tau(A_{i,t})}^{\\{1,\\dots,n\\}} (Y|\\tau)_M \u2013 SSE_{\\tau(A_{i,t}),a_{i,t}}(Y|\\tau)_M,$\nwhere $I = \\{A_{i',t'} := A_{i',t'}[a_{i,t}]\\[/i']_{i'\\in\\{1,...,n\\},t'>t}$"}, {"title": "H DISCUSSION ON COMPUTATIONAL COMPLEXITY", "content": "In this section, we analyze the computational complexity of the ASE-SV (Definition 5.2) and r-SSE- ICC (Definition 4.1) methods, and discuss some potential mitigation strategies for when the number of agents or the length of the time horizon are prohibitively large.\nH.1 COMPUTATIONAL COMPLEXITY OF ASE-SV\nThe number of agent-specific effect evaluations required by the exact ASE-SV calculation grows exponentially with the number of agents n. One potential mitigation strategy for this problem is to adapt to our setting sampling based approaches that efficiently approximate Shapley value without violating efficiency, i.e., attributing the entire effect. Jia et al. (2019) propose such an algorithmic approach, which requires $O(n(log n)^2)$ evaluations for any bounded utility. This means that their algorithm is applicable to ASE-SV in settings where the value of agent-specific effects is bounded, as is the case in both our experiments.\nH.2 COMPUTATIONAL COMPLEXITY OF R-SSE-ICC\nComputing the contribution scores assigned by the r-SSE-ICC method to all state variables requires O(h), where h denotes the time horizon, executions of Algorithm 1. When we deal with long- horizon MMDPs, this linear dependence on the number of time-steps can slow down our method. One intuitive strategy to reduce the number of computations in this case is by grouping together state variables from consecutive time-steps. That way, the r-SSE-ICC method would attribute the effect to sets of consecutive state variables instead of individual ones. If the time horizon (between action and outcome) is partitioned in groups of the same fixed size k, except maybe for the last one, then the modified r-SSE-ICC method would require $O(\\lceil \\frac{h}{k} \\rceil)$ executions of Algorithm 1."}, {"title": "I EXPERIMENTAL SETUP AND IMPLEMENTATION", "content": "In this section", "instructions": "examine box 1, examine box 2, pickup pink, pickup green, pickup yellow, goto pink, goto green and"}]}