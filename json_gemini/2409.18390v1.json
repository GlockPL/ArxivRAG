{"title": "Speech to Reality: On-Demand Production using Natural Language, 3D Generative AI, and Discrete Robotic Assembly*", "authors": ["Alexander Htet Kyaw", "Se Hwan Jeon", "Miana Smith", "Neil Gershenfeld"], "abstract": "We present a system that transforms speech into physical objects by combining 3D generative Artificial Intelligence (AI) with robotic assembly. The system leverages natural language input to make design and manufacturing more accessible, enabling individuals without expertise in 3D modeling or robotic programming to create physical objects. We propose utilizing discrete robotic assembly of lattice-based voxel components to address the challenges of using generative Al outputs in physical production such as design variability, fabrication speed, structural integrity, and material waste. The system interprets speech to generate 3D objects, discretizes them into voxel components, computes an optimized assembly sequence, and generates a robotic toolpath. The results are demonstrated through the assembly of various objects, ranging from chairs to shelves, which are prompted via speech and realized within 5 minutes using a 6-axis robotic arm.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advancements in 3D generative Al are changing the future of design and manufacturing by allowing the rapid creation of 3D digital assets. Tools like Shap-E [1], AssetGen [2], GET3D [3], and LATTE3D [4] can transform text prompts into intricate 3D shapes in mere seconds. With decreasing generation times and computational costs [5], the potential for instant, natural language-driven design and manufacturing is becoming increasingly feasible. The ability to make physical objects through speech input could enable people to create objects on-demand by simply articulating their needs [6]. This would eliminate the need for people to navigate a design software or undertake a complex manufacturing process [7]. The convenience and accessibility of this approach could democratize access to custom-made objects, allowing users to bring their ideas to life with using natural language [8]. However, translating these digital creations from 3D generative AI or text-to-3D models, into physical objects remains an open challenge [9].\nWhile text-to-mesh Al models has made it possible to produce complex designs within seconds [10], materializing these designs in the physical world demands the consideration of fabrication time, geometric complexity, structure and material waste [11]. To address these constraints, this paper presents a new approach that pairs generative Al outputs with discrete robotic assembly. We argue that discrete assembly complements generative AI because it has the potential to manage the variability of text-to-mesh outputs while enabling larger assembly, faster fabrication, structural integrity, and sustainability. In particular, the assembly of lattice-based voxel components can offer advantages in lightweight construction and high stiffness [12]. The modular nature of the assembly process can enable disassembly and reusability, addressing potential concerns about material waste when physically realizing the expansive possibilities of generative AI outputs [13]. Thus, the combination of voxel-based discrete robotic assembly with text-to-mesh models could provide a sustainable way for rapid iteration and creation, enabling on-demand production.\nWithin this context, we present an automated system that transforms speech into physical objects though generative AI and discrete robotic assembly. The system reduces geometric complexity of AI-generated meshes by discretizing them into voxel-based components, improves assembly feasibility by optimizing the assembly sequence, decreases fabrication time by using robotic assembly of prefabricated voxels, and minimizes material usage by disassembling and reusing voxels for new assemblies (Fig. 1). Our method differs from conventional production processes by leveraging generative AI outputs to quickly create large-scale objects. For the first time, we introduce a fully automated system that starts with a text-to-mesh model and ends with a robotically assembled object. This paper details the software framework, hardware components, implementation, and calibration required to create a system for translating natural language into tangible objects."}, {"title": "II. BACKGROUND", "content": "Previous efforts using generative Al to create physical objects have primarily focused on 3D printing. Edwards et al. developed a framework that integrates text and sketch input to enhance the manufacturability of AI-generated designs for 3D printing [14]. McClelland introduced a workflow that uses Al-driven generative design to create parts compatible with the fabrication constraints of 3D printing or CNC machining [15]. Faruqi et al. demonstrated the application of generative AI to stylize existing 3D models based on functionality, which are then 3D printed [16]. These approaches predominantly emphasize the use of Generative AI for 3D printing or CNC machining of small objects or parts. Although generative AI can create 3D models of any scale in seconds, conventional digital fabrication can take hours to make a small part, leading to a disconnect between Al capabilities and physical production.\nSimilarly, Makatura et al. showcased the potential of ChatGPT in design and manufacturing by using it to generate the design for a laser-cut shelf [8]. While generative AI tools such as ChatGPT and Text-to-Mesh models can sometimes create designs that can be digitally fabricated, the process still required manual assembly of physical part. Therefore, this paper offers an alternative approach by introducing a fully automated system that connects natural language, 3D generative Al and robotic assembly, integrating the entire production process. This enables the on-demand creation of large-scale objects like furniture, bridging the gap between rapid AI generation and production at any scale.\nPrior research has focused on automating various aspects of the robotic assembly and manufacturing process. Tian et al. presents a physics-based method for automated assembly sequence planning using graph neural networks [17]. Gandia et al. present an automated path planning workflow that adjusts path planning parameters according to assembly geometry [18]. Macaluso et al. has demonstrated the use of ChatGPT to automate the robotic programming by decomposing complex assembly tasks into simpler subtasks [19]. These studies have demonstrated automation in assembly sequence, path planning, and robotic programming. However, they are mainly tested with human-generated designs or conventional CAD models. Integrating outputs from 3D generative AI, such as Text-to-Mesh models, with robotic automation require a different set of consideration.\nAs demonstrated by Tian et al. [17], Gandia et al. [18], and Macaluso et al. [19] in the previous paragraph, current robotic assembly automation methods are focused on conventional CAD assemblies, usually composed of many parts. For each new assembly, the specific parts have to be designed and manufactured in advance, whether through 3D printing, CNC machining, or other fabrication methods. This process is time-consuming and resource-intensive, as each new CAD assembly requires new components to be produced before the automated robotic assembly can take place. This approach poses a significant challenge for on-demand production with generative AI. While generative AI can generate new geometries within seconds, the need to fabricate individual parts for each design would continue to constrain the production process, even with automated robotic assembly. Therefore, this paper proposes an alternative method that discretizes the output from text-to-3D models into modular lattice-based voxel components. These voxel components can be prefabricated, disassembled, and repurposed for different assemblies, eliminating the need to manufacture new parts for each design.\nVoxel-based systems or cellular assembly offer highly modular frameworks where unit cells, or voxels, are designed for ease of connection and reassembly [13], [20]. This modularity and reusability allow for the rapid creation of versatile 3D geometries using the same component. Smith et al. present construction voxels with geometry designed to facilitate indexing and error correction, incorporating self- aligning features using collaborative mobile robots [21]. Gregg et al. demonstrate reprogrammable metamaterial assemblies made from fiber-reinforced voxels, showcasing different mechanical behaviors and material composition [12]. In these studies, voxel assembly geometries were either manually modeled in CAD or predefined structures. This paper, however, introduces a different approach where the geometry for voxel assembly is determined by the outputs from a text-to-3D generative Al model. By leveraging the advantage of rapid assembly, modularity, and reusability of voxels with generative AI and robotic assembly, this paper presents a novel system for on-demand production of physical objects directly from natural language."}, {"title": "III. SYSTEM FRAMEWORK", "content": "The system first interprets speech to create 3D objects with Generative AI, then discrete the mesh output into lattice- based voxel components, compute an optimized assembly sequence, and generate the corresponding robotic toolpath (Fig. 2). A Python-based application is developed to create a data pipeline between these methods to automate the dataflow."}, {"title": "A. Speech to Text and Intention Recognition", "content": "The system utilizes speech recognition, natural language processing and a large language model to translate spoken commands into actionable prompts to produce 3D objects using generative AI. In the first stage, spoken language is converted into text using the Google Speech Recognition's Application Programming Interface (API) [22]. When the system is activated, the microphone captures audio input, which is processed using the speech recognition library. The second part of the process involves analyzing the transcribed prompt to identify the object the user desires to be assembled. This is accomplished using GPT-4 Turbo, a Large Language Model (LLM) from OpenAI [23]. A tailored prompt guides the LLM in distinguishing between actionable commands involving physical objects and non-physical concepts unsuitable for 3D generative AI. The prompt was structured to guide the language model in interpreting various user inputs effectively. If the prompt identifies a physical object, the system extracts and returns it as a response. If no object is detected, the response should return \"false\" and the system would ask the user to restate their command to request a physical object. The prompt we used was: \"Your task is to analyze the given text and determine whether it refers to a physical object or shape that is not an abstract idea. If it refers to something physical, return the relevant phrase that describes it; otherwise, respond with 'false.'\" To ensure the correct format, we paired this prompt with two examples, which are listed below.\n\u2022 Object Request: \"I need a shelf\" \u2192 Response: \"shelf\"\n\u2022 Non-Object Request: \"Knowledge\" \u2192 Response: \"false\""}, {"title": "B. Text to Mesh via 3D Generative AI", "content": "The output from the LLM is used as a text prompt for the 3D generative AI model to create a mesh. In this study, we utilized Meshy.ai as the generative AI model [24], though any text-to-mesh generative Al model could be substituted. The system communicates with Meshy.ai via API call, submitting the text input through a POST request. This request is encapsulated within a JSON payload that specifies parameters. The generation mode is configured as \"preview,\" and the style parameter is set to \"low-poly\" for a faster output. The system monitors the progress of the mesh generation by continuously tracking the API to check the status. Once the process is completed, the URL for the resulting 3D model is retrieved. However, this initial mesh is not directly suitable for robotic assembly. The generated mesh is saved in .obj format for additional geometric processing required for physical production."}, {"title": "C. AI Generated Mesh to Voxel Component Discretization", "content": "The voxelization process converts a mesh into discrete components suitable for robotic assembly, facilitating the translation of generative AI-designed objects into physical structures. The script processes the text-to-mesh geometry by unifying mesh normal and welding mesh vertices, eliminating any geometric inconsistencies. Once the mesh is refined, its bounding box is calculated along the X, Y, and Z axes. The mesh is then uniformly scaled to fit within a predefined assembly space of 60 cm by 60 cm by 60 cm to match the available assembly area in our physical work cell. An important factor for assembly feasibility is the availability of voxel components. In our setup, we have a maximum of 40 voxels, each measuring 10 cm x 10 cm x 10 cm, available for assembly. If the initial voxelization exceeds this limit, the longest edge of the bounding box is reduced by 10 cm until the resulting voxel count is under 40.\nThe voxelization process divides the scaled bounding box into a grid by generating planes along the x, y, and z axes. These planes intersect the mesh at regular intervals determined by the voxel size. At each intersection, contour curves are generated, which are then used to create horizontal and vertical slices of the mesh. These slices are analyzed to determine where the mesh intersects the grid (Fig. 3a). Voxels are created at these intersection points and are represented as cubic elements centered at the grid points. The resulting voxels, along with their center points are stored as coordinates (Fig. 3). The discretization step is crucial for preparing Al-generated objects for physical production, ensuring compatibility with the robotic arm, work cell, and physical components."}, {"title": "D. Voxel Coordinates to Assembly Sequence and Feasibility", "content": "The voxel coordinates are defined by their x, y, and z values relative to the assembly space. Sorting these coordinates is essential to prevent the robotic arm from colliding with assembled parts and placing unsupported cantilevers. To ensure assembly feasibility, any cantilevering voxels that remains unsupported for more than four voxels are removed from the assembly sequence. Additionally, floating voxels that does not share a face with another voxel are removed.\nThe assembly sequence is primarily determined by the z values, allowing the process to proceed layer by layer from the bottom up. Within each layer, voxels are initially sorted by their x values, followed by their y values. However, this approach does not fully account for cantilevers (Fig. 3c). \u03a4\u03bf properly address cantilevering voxels, an additional sorting algorithm is employed to prioritize voxels based on their proximity to previously assembled voxels. Specifically, the algorithm identifies the voxel with the shortest distance to a voxel in the layer below, ensuring structural connectivity and support (Fig. 3d). This verified and optimized assembly sequence is saved as a sorted list of coordinates to generate the robotic toolpaths."}, {"title": "E. Assembly Sequence to Robotic Toolpath", "content": "The robotic toolpath for voxel assembly is generated using the Python-URX library [25]. Several key parameters, such as the coordinate of the Voxel Source and the Safe Movement Height, are predetermined based on the configuration of the assembly space, robotic arm, and the conveyor belt in the work cell.\nThe Voxel Source defines the x, y, and z value where the voxels are picked. The Safe Movement Height defines the z value where the robot arm can safely move without colliding with the conveyor belt or assembled voxels. The Voxel Coordinates define the final x, y, and z where the voxels should be placed for assembly. The Voxel Coordinates are extracted from the sorted assembly sequence (described in the previous section). Using the coordinate values of the Voxel Source, Safe Movement Height, and the Voxel Coordinates, the toolpath is generated.\nEach Voxel Coordinate in the sorted assembly sequence undergoes two main actions: picking the block and placing the block. The robot movements are executed by translating the robot's global positions. In the grab movement, the robot first moves to the x, y value of the Voxel Source with the z value at the Safe Movement Height. While maintaining the x, y value of the Voxel Source, the z position is lowered to the z value of the Voxel Source. Once the robot end effector is at the x, y, and z value of the Voxel Source, the gripper is activated to grab the voxel. The robot arm then returns to the Safe Movement Height and starts the place sequence. In the place sequence, the robot moves to the x, y value of the Voxel Coordinate with the z value at the Safe Movement Height. While maintaining the x, y value of the Voxel Coordinate, the z position is lowered to the z value of the Voxel Coordinate. Once the robot end effector is at the x, y, and z value of the Voxel Source, the gripper is activated again to release and place the voxel in the assembly. This path planning process is automated, and repeated for each Voxel Coordinate in the assembly sequence until the assembly is complete."}, {"title": "IV. SYSTEM HARDWARE", "content": "The system hardware enables the physical execution of the speech-to-reality framework through the design of voxel geometry, a robotic arm end-effector, and an integrated conveyor belt system."}, {"title": "A. Voxel Geometry", "content": "The system involves the robotic assemble of voxels, or volumetric pixels, which act as modular building blocks. Referencing previous voxel designs [12], [13], [21], each voxel is made up of six 3D-printed faces forming a cuboctahedron geometry, allowing it to be assembled from any direction. The voxel geometry follows a 10 cm by 10 cm by 10 cm dimension, forming a cuboidal shape (Fig. 4a). Each face is embedded with magnets, ensuring secure attachment between adjacent voxels while allowing for reversible connections (Fig. 4). The voxels are constructed with finger joints for internal connections and pin joints for additional stability. The geometry and connections of the voxels can be modified according to the user's needs or the desired performance of the objects."}, {"title": "B. Robotic Arm and Custom End-Effector", "content": "The system employs a 6-axis robotic arm, specifically the Universal Robot UR10 [26]. The voxel gripper end effector is attached to the mounting plate of the robotic arm. Similar to [13], the gripping mechanism is designed to minimize the use of actively controlled moving parts. A single actuator rotates a plus-shaped latch clockwise by 45\u00b0, establishing four contact points with the underside of each edge of the voxel's top square face (Fig. 5). This configuration limits movement along the z-axis and restricts rotation around the x and y axes as the robotic arm performs assembly. Communication between the robot and the gripper is facilitated using the built-in digital I/O pins from the UR10 robotic arm to the ATtiny412 microcontroller.\nTo handle potential errors during the process, the robotic arm's end effector uses indexers that guide precise alignment. These act as passive self-correcting mechanisms to ensure accurate and secure placement of each part. Thus, the end- effector doesn't primarily rely on the torque capacity of the motor and is aided by the geometric and material properties of the latching mechanism. The Hitec HS-5087MH servo motors are responsible for the actuation needed to grab and release the voxels, rotating the latch counterclockwise enables the release mechanism."}, {"title": "C. Conveyor Belt", "content": "A conveyor belt system is designed to fully automate the robotic assembly process. The belt is constructed from vinyl material and is driven by a NEMA 17 stepper motor. The conveyor belt has the capacity to hold a sequence of 25 horizontally connected voxels as feedstock. While the belt area only has space for 5 voxels and the remaining 20 voxels are positioned on a smooth surface adjacent to the belt, the traction provided by the belt is sufficient to pull the remaining connected voxels onto the conveyor.\nThe belt features a stopper at the end to accurately position the voxel at the end with the custom robotic arm end effector. The stopper halts the movement of the voxels while allowing the belt to move underneath the voxels. This enables the motor to run continuously without needing to synchronize with the grip sequence. The conveyor is also equipped with a cap that covers all but the voxel at the end where the end effector engages (Fig. 6). This ensures that the end effector can grab and pick up the exposed voxel at the end, while the remaining connected voxels are blocked by the cap and detaches from the grabbed voxel."}, {"title": "V. SYSTEM IMPLEMENTATION", "content": "To implement the system, two key parameters must be calibrated in advance: the location of the Voxel Source and the speed of the robotic arm. The Voxel Source defines where the robot retrieves unassembled voxels from the conveyor belt. This involves calibrating the robot into position until it precisely aligns with the voxel's pick location. Once the robot is accurately positioned, the movement coordinates are recorded and set as the voxel source in the system. Before actual implementation, a simulation is conducted to ensure the workspace is collision-free and the voxel source is within the robot's reach. The simulation also checks for potential self-collisions or singularities when the robot picks up the voxel and places it in the assembly.\nGiven that the workflow emphasizes the on-demand nature in the production process, ensuring fast and reliable assembly is essential. This requires the calibration of the robotic arm's acceleration and velocity values to ensure efficiency while preventing failure. An AI-generated mesh of a simple stool was used as the calibration object. The calibration process begins at an initial velocity of 1 mm/s, which is gradually increased in 0.5 mm/s increments until a failure is detected. The acceleration was calibrated using two fixed ratios to the maximum velocity, specifically at (1:1) or (1:2) to the maximum velocity. Assembly failure can be attributed one or a combination of these following factors:\n\u2022 Vibration-Induced Failure: Excessive vibration of the table from the robotic arm movement can cause the assembled voxels to vibrate or move.\n\u2022 Impact-Induced Failure: The force of impact from placing voxels can cause adjacent voxels to fall.\n\u2022 Robot Misinterpretation of Collision: The robot may interpret vibrations or unexpected displacements as collisions, causing it to stop as a protective measure."}, {"title": "VI. RESULTS AND DISCUSSION", "content": "The results highlight both the effectiveness and limitations of the proposed speech-to-reality system, which converts spoken commands into physical objects through 3D generative AI and discrete robotic assembly. The system was able to demonstrate a diverse range of object requests, ranging from functional items like stools, chairs, tables, and shelves, to more unconventional ones, such as a dog or the letter \"T\" as shown in (Fig.7). All objects processed through the assembly feasibility algorithm developed specifically for the discrete assembly of generative AI outputs, were successfully constructed. Additionally, every object in Fig. 7 created through the speech-to-reality system, was assembled using the same set of 40 reusable voxel components. The reusability demonstrates the potential to scale production in line with the output capacity of Generative AI, without increasing material waste. The research demonstrates how discrete robotic assembly can address key challenges associated with physically producing generative Al outputs, including geometric variability, assembly feasibility, and resource consumption.\nThe current system leverages LLMs to interpret user requests and distinguish between abstract concepts and physical objects. For example, it accurately processes commands like \"Make me a coffee table\" as \"coffee table\" and \"I want a simple stool\" as \"simple stool.\" It also successfully handles functional specifications such as \"a shelf with two tiers\" or \"a stool with four legs \" (Fig. 7). In addition to physical requests, the system effectively identifies abstract prompts, such as \"Create beauty\" or \"I need something to hold memories,\" and correctly labels them as \"false.\" However, the system struggles when abstract concepts are paired with physical requests. For instance, with the input \"I need a box to hold memories,\" it correctly filters out the abstract portion but still outputs \"box.\" Fine-tuning the structure of the prompt could enhance its ability to handle more nuanced inputs.\nThe text-to-mesh model we used, Meshy.AI, averaged a generation time of 1 minute and 16 seconds across 10 different attempts. While Meshy.AI was used for this study, other text-to-mesh models could be substituted. As these models continue to improve, we anticipate a reduction in generation time, further accelerating the speech-to-reality pipeline."}, {"title": "VII. CONCLUSION", "content": "This research introduces an automated system that converts speech into physical objects by integrating 3D generative Al with robotic assembly. The system demonstrates a seamless pipeline from natural language input to tangible output, bridging the gap between Al-driven design and on-demand production. By discretizing AI- generated models into voxel-based discrete components and optimizing the robotic assembly sequence, the system significantly reduces geometric complexity, fabrication time, and material waste. Future studies could also begin to implement the use of having multiple component types. While the current system uses a single robotic arm, the system can be deployed with mobile voxel assemblers [21] to assemble objects in parallel. The research provides a new approach for translating natural language into tangible objects while offering rapid, sustainable and scalable production processes using generative AI and robotic automation."}]}