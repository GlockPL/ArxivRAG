{"title": "MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic Differential Diagnosis", "authors": ["Daniel Rose", "Chia-Chien Hung", "Marco Lepri", "Israa Alqassem", "Kiril Gashteovski", "Carolin Lawrence"], "abstract": "Differential Diagnosis (DDx) is a fundamental\nyet complex aspect of clinical decision-making,\nin which physicians iteratively refine a ranked\nlist of possible diseases based on symptoms,\nantecedents, and medical knowledge. While\nrecent advances in large language models have\nshown promise in supporting DDx, existing ap-\nproaches face key limitations, including single-\ndataset evaluations, isolated optimization of\ncomponents, unrealistic assumptions about\ncomplete patient profiles, and single-attempt di-\nagnosis. We introduce a Modular Explainable\nDDx Agent (MEDDxAgent) framework de-\nsigned for interactive DDx, where diagnostic\nreasoning evolves through iterative learning,\nrather than assuming a complete patient pro-\nfile is accessible. MEDDxAgent integrates\nthree modular components: (1) an orchestra-\ntor (DDxDriver), (2) a history taking simulator,\nand (3) two specialized agents for knowledge\nretrieval and diagnosis strategy. To ensure ro-\nbust evaluation, we introduce a comprehensive\nDDx benchmark covering respiratory, skin, and\nrare diseases. We analyze single-turn diagnos-\ntic approaches and demonstrate the importance\nof iterative refinement when patient profiles are\nnot available at the outset. Our broad evalua-\ntion demonstrates that MEDDxAgent achieves\nover 10% accuracy improvements in interactive\nDDx across both large and small LLMs, while\noffering critical explainability into its diagnos-\ntic reasoning process.", "sections": [{"title": "1 Introduction", "content": "Differential Diagnosis (DDx) is a crucial step in\nmedical decision-making, where doctors systemati-\ncally narrow down the most likely diagnosis from\na range of possible diseases (Rhoads et al., 2017).\nIn real-world clinical practice, DDx is essential\nbecause it accounts for uncertainty in the diagno-\nsis (Henderson et al., 2012). It's also incredibly\nchallenging given the large number of potential\ndiseases, rapidly evolving medical knowledge, and\nthe fact that symptoms and antecedents can point\nto multiple diseases (Winter et al., 2024).\nExpert clinicians rely on pattern recognition and\npast experience to narrow down potential diseases.\nHowever, the complexity and variability of real-world clinical presentations have prompted recent\nresearch into computational frameworks that use\nlarge language models (LLMs) to improve the DDx\nprocess (Fansi Tchango et al., 2022a; Zhou et al.,\n2024).\nThough LLM-based systems have shown\npromise in improving diagnostic assistance, exist-\ning methods face several limitations: (1) reliance\non single-dataset evaluations, limiting the gener-\nalizability across diverse patient populations and\ndisease categories (Alam et al., 2023); (2) focus\non optimizing a single diagnostic component (e.g.,\ndiagnosis strategy only) (McDuff et al., 2023), with-\nout an integrated approach to enhance multiple"}, {"title": "2 MEDDxAgent Overview", "content": "Our proposed MEDDxAgent framework (see Fig-\nure 1 and a detailed version in Figure 4) com-\nprises a central orchestrator (DDxDriver), a his-\ntory taking simulator, and two specialized diag-\nThe DDx process typically involves three key compo-\nnents: history taking, knowledge retrieval, and diagnosis strat-\negy (Cook and D\u00e9cary, 2020; Kavanagh et al., 2024).\nnostic agents dedicated to knowledge retrieval and\ndiagnosis strategy. Both the simulator and diag-\nnostic agents communicate exclusively with the\nDDxDriver, which monitors, stores, maintains, and\nupdates patient information and ranked differen-\ntial diagnoses. This central role also positions the\nDDxDriver to coordinate an iterative feedback loop,\nwherein observations from each agent are leveraged\nto enhance and refine subsequent agent calls with\nagent instructions (see an example in Figure 2). In\nthe following, we introduce the design of simulator\n(\u00a7 2.1), agents (\u00a7 2.2), orchestrator (DDxDriver)\n(\u00a7 2.3), and iterative learning mechanism (\u00a7 2.4)."}, {"title": "2.1 Simulator", "content": "History taking is a critical first step in differential\ndiagnosis, where clinicians gather essential infor-\nmation by asking patients questions about their\nsymptoms, medical history, and lifestyle factors.\nIn real-world clinical settings, a full patient profile\nis rarely available at the outset (Li et al., 2024b)\ndoctors typically start with only partial information\n(e.g., age, gender, chief complaint). The process of\ninteractive DDx allows clinicians to gather more\npatient information and refine their diagnostic hy-\npotheses before making a follow-up decision.\nTo simulate such an interactive environment, we\nintroduce a history taking simulator. We initialize\nthe simulator with two LLMs (Wu et al., 2023) in\nour experiments. The first LLM simulates the pa-\ntient and receives access to the full patient profile.\nThe second LLM simulates the doctor and receives\nan initial patient profile and optionally a set of\nconversational goals defined by DDxDriver (ac-\ntion). During the interactions, the doctor role asks\nquestions relevant to the diagnosis process, and the\npatient role provides answers based on its patient\nprofile. The interaction continues until either the\nconversational goals are achieved or a predefined\nstopping criterion (e.g., maximum number of ques-\ntions) is reached. Once the conversation concludes,\nthe dialogue history is forwarded to DDxDriver."}, {"title": "2.2 Agents", "content": "Knowledge Retrieval Agent. This agent aids the\ndiagnostic process by retrieving relevant medical\nknowledge from external sources, such as scientific\nliterature, medical databases, and clinical guide-\nlines. This is particularly critical for diagnosing\nrare or complex conditions where external knowl-\nedge (as compared to internal knowledge learned\nby LLMs' training data) is required to enhance"}, {"title": "2.3 Orchestrator", "content": "Inspired by the concept of a unified interface layer\nfrom previous work (Gioacchini et al., 2024), we\nintroduce DDxDriver as the central coordination\nhub in the MEDDxAgent framework (Figure 1).\nDDxDriver enables modular compatibility between\nthe diagnostic agents and benchmark datasets, with\nminimal adaptation efforts. DDxDriver uses the Re-\nAct paradigm (Yao et al., 2023) \u2013 which combines\nstep-by-step reasoning (thought) with decision-making (action) and feedback processing (observa-\ntion). At each step, DDxDriver obtains the infor-\nmation from the environment (input/output) and the\nresults from the previous state of the simulator and\nagents (observation, if it exists), then reasons about\nthe current state of evidence (thought) and gener-\nates agent-specific instructions based on the current\nstate of the patient profile. It dispatches these in-\nstructions to the selected simulator/agent, executes\nthe simulator/agent, and subsequently updates the\npatient profile with newly obtained information (ac-\ntion). Beyond execution management, DDxDriver\nserves four primary functions. First, it manages the\npatient profile, storing and maintaining all relevant\nclinical information, including demographics, med-\nical history, symptoms, and evolving diagnostic\nrankings. Second, it schedules and dispatches diag-\nnostic actions, dynamically determining which sim-\nulator/agent to invoke next based on the evolving\ndiagnostic context. Third, it ensures traceability by\nlogging all interactions, including inputs, outputs,\nand intermediate reasoning steps, thereby provid-\ning transparency in the decision-making process.\nFinally, it enforces stopping criteria by monitoring\ndiagnostic convergence and applying configurable\nthresholds, such as the number of iterations or the\nstabilization of ranked diagnoses."}, {"title": "2.4 Iterative Learning Mechanism", "content": "Diagnoses in the real world are rarely made in a\nsingle step. They are refined through multiple in-\nteractions with patients, clinical data, and external\nknowledge. To mirror this process, the iterative\nlearning mechanism is designed to avoid relying\non any single diagnostic agent or static decision\nprocess. We implement two settings: (i) fixed iter-\nation, and (ii) dynamic iteration. Fixed iteration\ncycles through the history taking simulator, knowl-\nedge retrieval agent, and diagnosis strategy agent\nin order until the predefined stopping criterion is\nmet (e.g., n iterations). In contrast, the dynamic\niteration process lifts constraints on the predeter-\nmined execution order, allowing the DDxDriver to\nadapt dynamically during the differential diagnosis\nprocess. After each observation, the DDxDriver\nreasons about which component \u2013 history taking\nsimulator, knowledge retrieval agent, or diagnosis\nstrategy agent to call next based on up-to-date\nobservations (i.e. updated patient profile, medi-\ncal documents, predicted DDx). For instance, if\nthe current diagnosis indicates a rare condition for\nwhich it needs clarifying details, the system may\ninvoke the knowledge retrieval agent to search for\nspecialized information. This allows for flexible\ndecision-making, opening up the opportunity for\nboth ideal and non-ideal choices. The iterative\nlearning mechanism allows MEDDxAgent to con-\ntinuously refine diagnosis while offering transpar-\nent insights into its reasoning process."}, {"title": "3 Experimental Setup", "content": "3.1 DDx Benchmark\nWe introduce a comprehensive DDx benchmark\nintegrating three datasets \u2013 DDxPlus, iCraft-MD,\nand RareBench, covering respiratory, skin, and\nrare diseases for a robust assessment of diagnos-\ntic performance. This addresses limitations of\nprior work, which often relies on a single dataset\nand single-turn evaluation for differential diagno-\nsis. DDxPlus (Fansi Tchango et al., 2022b) pro-\nvides a large-scale structured dataset with 1.3 mil-\nlion synthetic respiratory patient cases across 49\nrespiratory-related pathologies. iCraft-MD (Li\net al., 2024b) includes 394 skin diseases, adapting\nstatic dermatological clinical vignettes (from orig-\ninal Craft-MD dataset (Johri et al., 2024, 2025))\ninto an interactive setting\u00b3 \u2013 the system is only\nprovided with partial patient information and is\nexpected to proactively ask questions and gather in-\nformation. RareBench (Chen et al., 2024) expands\nDDxPlus with 421 rare diseases. We select three\nsubsets from RareBench \u2013 RAMEDIS (Europe),\nMME (Canada), and PUMCH (China) \u2013 to ensure\ndiversity in regional representation.\nTo enable a consistent evaluation across datasets,\nwe standardize each dataset into a structured for-\nmat: (i) optional initial patient information (e.g.,\nage, sex, chief complaint); (ii) full patient profile"}, {"title": "3.2 Evaluation Metrics", "content": "To evaluate diagnostic performance, we employ\nthree metrics. First, we compute the average rank\nof the correct disease, which represents the model's\nability to position the correct diagnosis closer to\nthe top. If the diagnosis does not appear in the\ntop-10 position, we assign a rank of 11. Second,\nwe use GTPA@k (Ground Truth Pathology Accu-\nracy) (Fansi Tchango et al., 2022b), which mea-\nsures whether the ground truth diagnosis appears\nwithin the top-k predicted diagnoses. Third, we\nintroduce a new metric suitable for the iterative set-\nting: average progress rate (\u2206 Progress). Inspired\nby AgentQuest (Gioacchini et al., 2024), it tracks\nchanges in rank r of the ground truth pathology in\nthe differential diagnosis. For each patient case i,\nwe average the progress in rank (ri,t - ri,t+1) over\nN iterations of differential diagnosis, then aggre-\ngate over M patients. This metric quantifies how\neffectively the system refines and converges on the\ncorrect diagnosis over successive iterations:\n$\\Delta progress = {1 \\over M}\\sum_{i=1}^{M}({1 \\over N-1}\\sum_{t=1}^{N-1}(r_{i,t}-r_{i,t+1}))$\n3.3 Models and Tasks\nWe evaluate on GPT-40 (version: 2024-11-\n20) (Hurst et al., 2024), Llama3.1-70B and\nLlama3.1-8B (Dubey et al., 2024) across all tasks,\nensuring a comparison of LLMs at varying scales.\nOur experiments are conducted in two setups: (1)\noptimizing individual agents; and (2) interactive\ndifferential diagnosis. In the first task, we evaluate\nthe two agents (knowledge retrieval, diagnosis strat-\negy) in a single-turn setting. This allows us to iso-\nlate the effectiveness of the reasoning mechanisms\nwithout the confounding factor of incomplete in-\nformation. In the second task, we assess MEDDx-\nAgent's performance at interactive DDx, compar-\ning it against the single-turn diagnostic agents and\nhistory taking simulator. Interactive differential\ndiagnosis, as suggested by Li et al. (2024b), is a\nchallenging yet realistic scenario, where only initial"}, {"title": "3.4 Hyperparameters and Optimization", "content": "For the knowledge retrieval agent, we limit\nsearches to a maximum of three medical keywords\nper query. Wikipedia is used as an open-access re-\nsource, while PubMed retrieval is restricted to full-\ntext articles from commercially licensed sources,4\nensuring that retrieved information is clinically vali-\ndated and relevant to the diagnostic task. For the di-\nagnosis strategy agent, we take 5 examples for few-\nshot learning. For dynamic few-shot, we use Bio-\nClinicalBERT (BERT) (Alsentzer et al., 2019) and\nBGE-BASE-EN-V1.5 (BAII) (Xiao et al., 2024) em-\nbeddings, based on the structure proposed by Wu\net al. (2024). Specifically, it uses L2 distance on\nnormalized embeddings, a similar setting to cosine\nsimilarity. With the history taking simulator, we\ncreate an iterative environment, which we evaluate\nat 5, 10, and 15 maximum questions. This is based\non prior clinical studies that indicate physicians\ntypically ask fewer than 15 questions per consulta-\ntion (Ely et al., 1999). This ensures that our model\noperates within a realistic range, capturing essen-\ntial patient details without excessive interaction.\nFor MEDDxAgent's iterative learning, we select\nthe optimized history taking simulator and diag-\nnostic agents and experiment on interactive DDx.\nOur setup is inspired by previous work (Johri et al.,\n2025), which demonstrates that updating the pa-\ntient profile with new history-taking dialogue sig-\nnificantly enhances performance. We experiment\nwith 1 to 3 iterations, with 5 questions per iteration.\nThis aligns with the history-taking simulator setting\n(5 questions per iteration, max 15 for 3 iterations).\nAdditionally, we set the DDxDriver's instruction\nfor each agent and simulator to a list of length 10."}, {"title": "4 Evaluation Results", "content": "We experiment on two configurations: (1) optimiz-\ning individual agents (\u00a7 4.1), by determining the\nbest settings for knowledge retrieval and diagno-\nsis strategy agents; and (2) interactive differential\ndiagnosis (\u00a7 2.4), where the optimized agents are"}, {"title": "4.1 Optimizing Individual Agents", "content": "We first explore the optimal single-turn configu-\nration for the knowledge retrieval and diagnosis\nstrategy agents, before integrating them into iter-\native setup. For this, we provide the full patient\nprofile as in previous work (Wu et al., 2024; Chen\net al., 2024), and present the results in Table 1. For\nthe knowledge retrieval agent, PubMed performs\nslightly better overall than Wikipedia, especially\nfor Rarebench, which demands more complex dis-\nease information. For the diagnosis strategy agent,\nthe best setting varies by dataset. Namely, dynamic\nfew-shot with BAII embeddings performs the best\non DDxPlus and RareBench, where relevant patient\nexamples offer reliable contextual cues to likely dis-"}, {"title": "4.2 Interactive Differential Diagnosis", "content": "We now evaluate the more challenging task of in-\nteractive DDx, where we begin with limited pa-"}, {"title": "5 Analysis", "content": "Fixed vs. Dynamic Iterations. A key feature of\nMEDDxAgent is its iterative DDx process, which\noperates in fixed or dynamic iteration. Our exper-\niments (see Table 8 in Appendix) show that fixed\niteration consistently outperforms dynamic itera-\ntion in both accuracy and system efficiency.6 Fixed\niteration ensures a structured sequence where all\nmodules - history taking simulator, knowledge re-\ntrieval agent, and diagnosis strategy agent, are uti-\nlized in each cycle, preventing over-reliance on a\nsingle component. In contrast, dynamic iteration,\nwhich allows DDxDriver to choose the component\nat each step, introduces some suboptimal decision-\nmaking. We observe that Llama3.1 models, for\ninstance, frequently favor the history taking simu-\nlator rather than leveraging the knowledge retrieval\nor diagnosis strategy agents, leading to redundant\nquestioning rather than efficient diagnostic reason-\ning. Despite this, our findings demonstrate the\ngeneral applicability of MEDDxAgent for dynamic\niteration and highlight future work toward optimiz-\ning dynamic iteration for interactive DDx.\nError Analysis. To better understand the strug-\ngles of MEDDxAgent, we conduct error analysis\non cases where it failed to reach the correct di-\nagnosis efficiently. We emphasize that our MED-\nDxAgent's logging of intermediate logic greatly\nenhances our understanding and explanations of\nfailure cases. First, in RareBench, over-reliance\non few-shot examples often misprioritizes frequent"}, {"title": "6 Related Work", "content": "6.1 LLM-based Methods\nResearchers have studied the capabilities of LLMs\nfor automatic diagnosis (Mizuta et al., 2024). One\nline of work have found that the performance of\nLLMs is comparable to the performance of physi-\ncians (Hirosawa et al., 2023; Rutledge, 2024) or\nthat the performance of the physicians themselves\nis improved when they use LLMs (Ten Berg et al.,\n2024). However, researchers have also observed\nthat LLMs struggle to perform this task when ap-\nplied on rarer diseases or on more unusual cases\n(Fabre et al., 2024; Shikino et al., 2024). For this\nreason, we assemble a benchmark that measures\nperformance for different rarity levels.\nMany of the current methods are typically tar-\ngeting either automatic standard diagnosis or auto-\nmatic differential diagnosis in an end-to-end man-\nner. For example, there are methods that use Chain-\nof-Thought strategies (Wu et al., 2023; Savage\net al., 2024; Nachane et al., 2024), reinforcement\nlearning (Fansi Tchango et al., 2022a), fine-tuning\nLLMs (Alam et al., 2023; Reese et al., 2024),\npreranking-reranking methods (Sun et al., 2024)\nor specifically trained neural networks (Liu et al.,\n2020; Hwang et al., 2022). Such LLM-based meth-\nods do not allow for modularity, thus posing dif-\nficulties in integrating specific modules that solve\ncertain sub-problems within the diagnosis process."}, {"title": "6.2 Agent-based Methods", "content": "Recent work shifts from standalone LLMs to multi-\nagent frameworks, enhancing efficiency by en-"}]}