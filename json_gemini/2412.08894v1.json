{"title": "SMMF: Square-Matricized Momentum Factorization for Memory-Efficient Optimization", "authors": ["Kwangryeol Park", "Seulki Lee"], "abstract": "We propose SMMF (Square-Matricized Momentum Factorization), a memory-efficient optimizer that reduces the memory requirement of the widely used adaptive learning rate optimizers, such as Adam, by up to 96%. SMMF enables flexible and efficient factorization of an arbitrary rank (shape) of the first and second momentum tensors during optimization, based on the proposed square-matricization and one-time single matrix factorization. From this, it becomes effectively applicable to any rank (shape) of momentum tensors, i.e., bias, matrix, and any rank-d tensors, prevalent in various deep model architectures, such as CNNs (high rank) and Transformers (low rank), in contrast to existing memory-efficient optimizers that applies only to a particular (rank-2) momentum tensor, e.g., linear layers. We conduct a regret bound analysis of SMMF, which shows that it converges similarly to non-memory-efficient adaptive learning rate optimizers, such as AdamNC, providing a theoretical basis for its competitive optimization capability. In our experiment, SMMF takes up to 96% less memory compared to state-of-the-art memory-efficient optimizers, e.g., Adafactor, CAME, and SM3, while achieving comparable model performance on various CNN and Transformer tasks. The code is available at a GitHub\u00b9.", "sections": [{"title": "1 Introduction", "content": "To identify the optimal weight parameters of deep neural networks, various optimization methods (Abdulkadirov, Lyakhov, and Nagornov 2023; Martens 2016; Amari 2010; Liu and Nocedal 1989) have been studied. One of the most popular approaches is SGD (Stochastic Gradient Descent) (Ruder 2016) which takes the weight update direction towards the current gradient with a learning rate uniformly applied to all weight parameters. To further improve SGD's optimization performance, many adaptive learning rate optimizers, such as Adam (Kingma and Ba 2014) and RMSProp (Hinton, Srivastava, and Swersky 2012), have been proposed to leverage 1) history of the gradients to compute the momentum direction (Ruder 2016) and 2) the squared gradients to compute the adaptive learning rate for each weight parameter. Despite their lack of theoretical convergence guarantee in non-convex settings of many deep learning tasks, those adaptive learning rate optimizers have been empirically found to outperform SGD in practice.\nHowever, since the momentum value of each weight parameter, which linearly increases over the size of a deep learning model, should be maintained in memory during the whole training process, the adaptive learning rate optimizers can easily limit the size of models that can be trained on memory-constrained platforms, e.g., embedded systems. Even when training small models like Transformer-base (Vaswani et al. 2017), 1.4 GiB of memory is required. This means it would be unusable in environments with extremely limited memory devices, such as Raspberry Pi (1 GiB). To tackle the memory challenge of the adaptive learning rate optimization, several memory-efficient optimizers have been proposed. Adafactor (Shazeer and Stern 2018) and CAME (Luo et al. 2023) factorize the 2nd momentum in the form of a matrix into a set of vectors to decrease the memory space required to store momentums, achieving comparable performance to Adam. SM3 (Anil et al. 2019) reduces memory usage by approximating the similar elements of the 2nd momentum into a smaller set of variables. Although they effectively reduce the memory space of adaptive learning rate optimizers by projecting a gradient tensor onto several rank-one vectors, 1) they apply only to a specific rank (shape) and pattern of momentum tensors, 2) their memory space is still huge (1.1 GiB) making them unsuitable for memory constrained devices, and 3) their optimization performance has not been theoretically analyzed and compared to that of Adam family (Kingma and Ba 2014).\nIn this paper, we propose SMMF (Square-Matricized Momentum Factorization), a memory-efficient optimizer amicable to an arbitrary rank (shape) and pattern of both the 1st and 2nd momentum tensors, i.e., a vector, matrix, and rank-d tensor, which reduces the amount of memory required in model optimization by up to 96% compared to existing memory-efficient optimizers, e.g., Adafactor, CAME, and SM3. Unlike such existing memory-efficient optimizers, either confined to a particular 1) momentum rank (shape) (i.e., a rank-2 matrix) and/or 2) momentum pattern (i.e., a set of similar elements in a matrix) (Anil et al. 2019), the proposed SMMF performs competitive optimization without being restricted by the rank (shape) and pattern of momentums allowing the models to be trained on extremely memory constrained embedded systems from ~0.001 to ~1 GiB."}, {"title": "2 Related Work", "content": "Adafactor (Shazeer and Stern 2018) factorizes the 2nd momentum matrix via Non-Negative Matrix Factorization (NNMF) (Finesso and Spreij 2006) that decomposes a non-negative matrix into two vectors by differentiating the I-divergence (Lee and Seung 1999). Theoretically, it reduces the memory complexity of the 2nd momentum in the form of a non-negative matrix, i.e., $V \\in \\mathbb{R}^{n \\times m}$, from O(nm) to O(n + m) with the two factorized vectors. Empirically, it shows comparable optimization to Adam on Transformers.\nCAME (Luo et al. 2023), a variant of Adafactor, is proposed as a memory-efficient optimizer for large batch optimization. To alleviate the unstable behavior of Adafactor, it introduces the factorized confidence term that guides the optimization direction, empirically achieving faster convergence on language models (Raffel et al. 2020; Radford et al. 2019) at the cost of using more memory than Adafactor. Since CAME also requires a momentum to be a non-negative matrix to be factorized with NNMF, it slices a high-rank weight tensor, appearing in CNN models such as MobileNet (Dong et al. 2020), into multiple matrices and factor-ize them separately. Hence, given a rank-d 2nd momentum $V \\in \\mathbb{R}^{n_1 \\times ... \\times n_d}$, the memory complexity of CAME becomes $O((n_{d-1} + n_d) \\prod_{r=1}^{d-2} n_r)$, which is similar to Adafactor.\nSM3 (Anil et al. 2019), unlike Adafactor and its variants such as CAME, applies the min-max scheme to approximate the similar elements of the 2nd momentum to a smaller set of variables. It shows competitive optimization performance to Adam and Adafactor on Transformers that exhibit a grid pattern of similar elements in their weight matrices. Given a rank-d momentum tensor $R^{n_1 \\times ... \\times n_d}$, the memory complexity of SM3 becomes $O(\\sum_{r=1}^{d} n_r)$ if similar elements appear on each axis of the weight tensor, which can be found in some Transformer weight matrices (Anil et al. 2019).\nAlthough those existing memory-efficient optimizers effectively reduce the memory requirement and perform competitive optimization primarily on the Transformer architectures (Vaswani et al. 2017; Devlin et al. 2018; Radford et al. 2019; Raffel et al. 2020) by projecting the gradient onto rank-1 vectors, each optimizer has limitations. First, since Adafactor and CAME rely on matrix factorization (Finesso and Spreij 2006), a momentum tensor should be first sliced into multiple matrices before being factorized, degrading the memory reduction effect given a high-rank momentum tensor. Next, SM3 needs sets of similar elements in a momentum tensor to perform effective optimization, neither easy nor guaranteed to find in the huge weight parameter space of many deep neural networks. However, unlike Adafactor and CAME, the proposed SMMF applies one-time single matrix factorization to any rank (shape) of momentum tensors based on the proposed square-matricization without the memory increase caused by tensor-to-matrices slice. Also, since the proposed SMMF utilizes NNMF, it does not require strong patterns on the weight parameter space, readily applying to an arbitrary pattern of weight tensors, in contrast to SM3 that assumes the existence of specific element patterns on each axis in a weight tensor."}, {"title": "3 SMMF (Square-Matricized Momentum Factorization)", "content": "Algorithm 1 shows the overall procedure of the proposed SMMF (Square-Matricized Momentum Factorization), applied to the weight tensor (momentum) at each layer of the model. In short, given the 1st and 2nd momentum tensors as $M, V \\in \\mathbb{R}^{n_1 \\times ... \\times n_d}$, SMMF reduces the memory complexity required for optimization into $O_M(\\hat{n} + \\hat{m})$ and $O_V(\\hat{n} + \\hat{m})$ for M and V, respectively, with $\\hat{n}, \\hat{m} = arg \\min_{n,m} |n - m|$ such that $nm = \\prod_{r=1}^{d} n_r$ where $n_r, n, m, \\hat{n}$, and $\\hat{m}$ are in N. It first transforms M, $V \\in$"}, {"title": "3.1 Square-Matricization", "content": "In Algorithm 1, SMMF first obtains a rank-d gradient $G_t \\in \\mathbb{R}^{n_1 \\times ... \\times n_d}$ for the weight and bias tensor at each layer of the model and converts it into a matrix closest to a square matrix $G_t \\in \\mathbb{R}^{\\hat{n} \\times \\hat{m}}$ where $\\hat{n} \\approx \\hat{m}$, for factorization, naturally leading to the square-matricization of the 1st and 2nd momentum, M and V. To this end, we propose a square-matricization method that reshapes $G_t \\in \\mathbb{R}^{n_1 \\times ... \\times n_d}$ into a matrix closest to a square matrix $\\hat{G}_t \\in \\mathbb{R}^{\\hat{n} \\times \\hat{m}}$ such that $\\hat{n} \\approx \\hat{m}, \\hat{n} \\hat{m} = \\prod_{r=1}^d n_r$ and $(\\hat{n}, \\hat{m}) = arg \\min_{n,m} (n+m) = arg \\min_{n,m} |n-m|$, where $n, m, \\hat{n}, \\hat{m} \\in \\mathbb{N}$. Following theorems show that the square-matricization of $\\hat{G}_t \\in \\mathbb{R}^{\\hat{n} \\times \\hat{m}}$, i.e., having $\\hat{n} \\approx \\hat{m}$, also minimizes $\\hat{n} + \\hat{m}$.\nTheorem 3.1. Given $n_r \\in \\mathbb{N}, r \\in [1, d]$, and a constant $N = \\prod_{r=1}^{d} n_r$, then $\\prod_{r=1}^{d-2} n_r (n_{d-1} + n_d)$ decreases if both $n_{d-1}$ and $n_d$ increase (Proof provided in Appendix C).\nCorollary 3.1.1. Given $N = \\prod_{r=1}^{d} n_r$, there exist $N = \\hat{n}\\hat{m}$ such that $\\hat{n} + \\hat{m} = \\min_{n \\hat{m} = N} n(n_{d-1} + n_d), (n, \\hat{m}) \\in \\mathbb{N}$.\nTheorem 3.2. Given $n_r, n, m \\in \\mathbb{N}, r \\in [1, d], N = \\prod_{r=1}^{d} n_r = nm$, and $n \\le m$, then $\\hat{n}, \\hat{m} = arg \\min_{n,m} (n + m) = arg \\min_{n,m} |n - m|$ (Proof provided in Appendix D).\nFrom Corollary 3.1.1, square-matricizing $G_t \\in \\mathbb{R}^{n_1 \\times ... \\times n_d}$ into $\\hat{G}_t \\in \\mathbb{R}^{\\hat{n} \\times \\hat{m}}$ reduces the memory complexity since $\\prod_{r=1}^{d-2} n_r (n_{d-1} + n_d) \\le \\prod_{r=1}^{d} n_r$. Also, based on Theorem 3.2, minimizing |n - m| is equivalent to minimizing n + m. From this, we derive the square-matricization algorithm (Algorithm 2) that finds $\\hat{n}$ and $\\hat{m}$, which minimizes n + m by solving $(\\hat{n}, \\hat{m}) = arg \\min_{n,m} |n-m|$. By reshaping a rank-d gradient into a matrix closest to a square matrix through square-matricization, it becomes"}, {"title": "3.2 Decompression and Compression", "content": "Decompression \u2192 Compression. After square matricizing the gradient, SMMF decompresses the 1st and 2nd momentum from two vectors factorized at the previous step t-1 to update the momentums, as in Algorithm 1. Then, it compresses the momentums obtained at step t into vectors and updates the weight W using the decompressed momentums. We call this process the decompression \u2192 compression scheme, in which the gradient $G_t$ at the current step t is reflected to the 1st and 2nd momentum before it is factorized, enabling the precise update of the weight.\nThe significance of information in the current gradient, e.g., tensor patterns, has been emphasized in some previous works (Anil et al. 2019). Since reshaping and factorization of the gradient may ruin some potentially important patterns of the momentums contained in the previous gradient $G_{t'<t}$, reflecting the current gradient $G_t$ (and its pattern) is crucial in model performance. Thus, by performing decompression with $G_t$ prior to updating and compressing (factorizing) $M_t$ and $V_t$, it is expected to improve the optimization performance. On the contrary, existing optimizers, such as Adafactor, first compress $G_t$ and then updates momentums through decompression, which we call the compression \u2192 decompression scheme. In this scheme, some useful information of $G_t$ would be lost by compression (factorization), implying that an optimizer can hardly utilize the intact state of $G_t$, likely to degrade the model performance.\nDecompression. First, in the decompression phase (Algorithm 3), the 1st and 2nd momentum, {$|M_{t-1}, V_{t-1}|$} $\\in \\mathbb{R}^{\\hat{n} \\times \\hat{m}}$, are defactorized from the two vectors for each, i.e., {$r_{M_{t-1}}, c_{M_{t-1}}$} for $M_{t-1}$| and {$r_{V_{t-1}}, c_{V_{t-1}}$} for $V_{t-1}$, which have been factorized from the square-matricized momentums at the previous step t-1, by performing outer product between them. To apply NNMF to the 1st momentum $M_t$, its sign values are stored as a bi-"}, {"title": "3.3 Time (Computation) Complexity of SMMF", "content": "The time complexity of SMMF consists of two parts, i.e., square-matricization and decompression/compression. First, computing $\\hat{n}$ and $\\hat{m}$ for square-matricization (Algorithm 2) is $O(\\sqrt{N})$, where N is the number of elements in the momentum tensor. However, this computational overhead is negligible since $\\hat{n}$ and $\\hat{m}$ are calculated only once before starting model training (optimization). Next, the time complexity of decompression (Algorithm 3) and compression (Algorithm 4) are both O(N), which is asymptotically equivalent to existing memory-efficient optimizers, i.e., Adafactor and CAME. While taking a similar computational complexity to existing memory-efficient optimizers (Shazeer and Stern 2018; Luo et al. 2023), SMMF is able to save up to 96% of memory, as shown in Section 5."}, {"title": "4 Regret Bound Analysis", "content": "We analyze the convergence of SMMF by deriving the upper bound of the regret that indicates an optimizer's convergence (Kingma and Ba 2014). The regret R(T) is defined as the sum of differences between two convex functions $f_t(w_t)$ and $f_t(w^*)$ for all t \u2208 [1, T], where $w^*$ is an optimal point.\n$R(T) = \\sum_{t=1}^{T} (f_t(w_t) - f_t(w^*))$       (1)\nSince SMMF factorizes (compresses) the momentum tensors, unlike Adam, we introduce some compression error terms in our analysis as follows. First, $\\hat{m}_t$ and $\\hat{v}_t$ are the"}]}