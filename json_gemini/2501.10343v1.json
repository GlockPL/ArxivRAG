{"title": "3rd Workshop on Maritime Computer Vision (MaCVi) 2025: Challenge Results", "authors": ["Benjamin Kiefer", "Lojze \u017dust", "Jon Muhovi\u010d", "Matej Kristan", "Janez Per\u0161", "Matija Ter\u0161ek", "Uma Mudenagudi", "Chaitra Desai", "Arnold Wiliem", "Marten Kreis", "Nikhil Akalwadi", "Yitong Quan", "Zhiqiang Zhong", "Zhe Zhang", "Sujie Liu", "Xuran Chen", "Yang Yang", "Matej Fabijani\u0107", "Fausto Ferreira", "Seongju Lee", "Junseok Lee", "Kyoobin Lee", "Shanliang Yao", "Runwei Guan", "Xiaoyu Huang", "Yi Ni", "Himanshu Kumar", "Yuan Feng", "Yi-Ching Cheng", "Tzu-Yu Lin", "Chia-Ming Lee", "Chih-Chung Hsu", "Jannik Sheikh", "Andreas Michel", "Wolfgang Gross", "Martin Weinmann", "Josip \u0160ari\u0107", "Yipeng Lin", "Xiang Yang", "Nan Jiang", "Yutang Lu", "Fei Feng", "Ali Awad", "Evan Lucas", "Ashraf Saleem", "Ching-Heng Cheng", "Yu-Fan Lin", "Tzu-Yu Lin", "Chih-Chung Hsu"], "abstract": "The 3rd Workshop on Maritime Computer Vision (MaCVi) 2025 addresses maritime computer vision for Unmanned Surface Vehicles (USV) and underwater.\nThis report offers a comprehensive overview of the findings from the challenges. We provide both statistical and qualitative analyses, evaluating trends from over 700 submissions. All datasets, evaluation code, and the leaderboard are available to the public at https://macvi.org/workshop/macvi25.", "sections": [{"title": "1. Introduction", "content": "Maritime environments, with their unique challenges such as dynamic lighting, reflections, and cluttered scenes, demand specialized computer vision techniques [24, 31, 33, 54, 65]. Autonomous systems like Unmanned Surface Vehicles (USVs) and Underwater Vehicles (UUVs) rely heavily on robust vision algorithms to navigate, detect, and interpret complex surroundings [5, 34, 84]. Addressing these challenges requires not only cutting-edge algorithms but also standardized benchmarks and a collaborative research ecosystem [4, 6, 18, 38]."}, {"title": "2. Challenge Participation Protocol", "content": "As for the first two challenge iterations, all challenges followed a superset of rules and protocols outlined in this section while challenge specific rules are described in the respective subsections."}, {"title": "2.1. Submission Process", "content": "Participants could get the challenge rules and resources on the workshop webpages. There, we provided links to evaluation code, datasets and starter kits.\nThen, participants submitted their models and predictions through the official evaluation server. General submission requirements included:\n\u2022 Predictions formatted according to standardized task-specific formats.\n\u2022 Compliance with model submission guidelines, including export to ONNX format where applicable.\n\u2022 A limit of 1-3 submission per day per challenge track."}, {"title": "2.2. Evaluation Server and Leaderboard", "content": "An online evaluation server provided automated scoring for all submissions. Test set evaluations were conducted on the server, with results displayed on public leaderboards. The leaderboards were frozen (results of each method visible only to its authors) a week prior to the results announcement to encourage competition and submissions without relying on the results from other teams."}, {"title": "2.3. Timeline", "content": "The timeline for all MaCVi 2025 challenges was:\n\u2022 Submissions open: 13th Sep 2024\n\u2022 Leaderboards frozen: 12th Dec 2024\n\u2022 Submissions closed: 19th Dec 2024"}, {"title": "2.4. Recognition and Rewards", "content": "After additional manual review of the submissions, the top team(s) in each challenge were determined by the challenge-specific metrics and whether the performance was above the provided baselines. They then were invited to submit a short technical report detailing their methods. These reports will are included in the appendix below. Winners were also acknowledged during the workshop and featured on the official website. Additionally, there were prizes for some of the competitions as described on the workshop website."}, {"title": "3. USV-based Perception Challenges", "content": null}, {"title": "3.1. Approximate Supervised Distance Estimation", "content": "Distance estimation using monocular vision is a complex problem that has gained considerable attention in recent years due to its key role in fields such as autonomous driving and robotics [51]. This approach is equally crucial for maritime applications, where it can support essential tasks such as navigation and path planning for unmanned surface vehicles (USVs). State-of-the-art monocular depth estimation frameworks, such as MiDAS [57] and SPI-Depth [39], show significant limitations for this type of application, particularly in terms of real-time performance and accuracy for smaller, distant objects. In addition, many of these approaches generate depth maps with values normalized between 0 and 1, rather than providing the metric distances required for practical use.\nIn the context of this challenge, participants are required to develop novel methods for estimating distances to maritime navigational aids while simultaneously detecting them in images. To facilitate this, we provide a dataset comprising approximately 3,000 labeled training samples, including single-class normalized bounding-box coordinates and the metric distance between the camera and the object. The ground truth distance is calculated using the Haversine formula, which computes the distance between the GNSS coordinates of the ship, where the camera is mounted, and the corresponding navigational buoys as mapped on a nautical chart. Additionally, participants are provided with a modified version of the YOLOv7 object detector [67], similar to an approach developed by Vajgl et al. [64], where the predicted tensor for each anchor consisting of $[C_x, C_y, w, h, P_{obj}, P_{class_1}, \u2026, P_{class_n}]$ is extended by a distance estimate."}, {"title": "3.1.1 Evaluation Protocol", "content": "To evaluate the methods submitted by participants, we utilize metrics that assess both object detection performance and distance estimation accuracy. For object detection performance, we employ well-established metrics, specifically AP50 and AP50:95. The AP50 metric calculates the average precision across varying confidence thresholds with an IoU threshold of 50%. In contrast, AP50:95 averages the precision over a range of IoU thresholds from 50% to 95% in increments of 5%.\nTo evaluate the deviation of distance estimates from ground truth values, we define the absolute error $(E_{abs})$ and relative error $(E_{rel})$ as follows:\n$E_{abs} = \\frac{1}{n} \\sum_{i=1}^n |d_i - \\hat{d_i}|$,\n$E_{rel} = \\frac{1}{n} \\sum_{i=1}^n c_i \\frac{|d_i - \\hat{d_i}|}{d_i}$,\nwhere $n$ is the total number of predictions, $\\hat{d_i}$ denotes the predicted distance, $d_i$ represents the ground truth distance and $c_i$ is the predicted confidence. It is important to note that each distance prediction is weighted by the corresponding normalized confidence for Erel. Furthermore, we include the relative distance error to penalize deviations between the prediction and ground truth more heavily for objects that are in closer proximity."}, {"title": "3.1.2 Submissions, Analysis and Trends", "content": "60 Submission from 6 different teams, including one baseline model from the MaCVi2025 committee [34], were evaluated.  Both top submissions outperform the baseline model in object detection performance and distance estimation metrics. However, the winning model is significantly better with regards to the object detection (AP) metrics, while the second-place submission excels in minimizing the distance estimation errors. In the following section the top two submissions are compared to the provided baseline model. The best-performing model for this task is Yolov7 Depth-Widened, however both approaches are capable of significantly reducing the absolute error compared to the baseline, particularly for objects in closer proximity. This is mainly achieved by limiting the number of large outlier"}, {"title": "3.1.3 Discussion and Challenge Winners", "content": "The winners of the USV Supervised Object Distance Estimation challenge are:\n1st place: Nanjing University of Science and Technology with Data Enhance\n2nd place: FER LABUST with YOLOv7 Depth-Widened\nThe analysis demonstrates that both methods significantly outperform the baseline model in terms of object detection and distance estimation. Although most architectural modifications were primarily aimed at enhancing object detection performance, the results indicate that these adaptations can also contribute to improvements in distance estimation accuracy."}, {"title": "3.2. USV-based Obstacle Segmentation Challenge", "content": "The methods participating in the USV-based Obstacle Segmentation Challenge were required to predict the scene segmentation (into obstacles, water and sky) for a given input image. The submitted methods have been evaluated on the recently released LaRS benchmark [86]. In addition to the publicly available training set, the authors were also allowed to use additional datasets (upon declaration) for training their methods."}, {"title": "3.2.1 Evaluation Protocol", "content": "To evaluate segmentation predictions, we employ the LaRS [86] semantic segmentation evaluation protocol. Segmentation methods provide per-pixel labels of semantic components (water, sky and obstacles). However, traditional approaches for segmentation evaluation (e.g. mIoU) do not consider the aspects of predictions that are relevant for USV navigation. Instead, the LaRS protocol evaluates the predicted segmentations with respect to the downstream tasks of navigation and obstacle avoidance and focuses on the detection of obstacles.\nThe detection of static obstacles (e.g. shoreline) is measured by the water-edge accuracy (\u03bc), which evaluates the segmentation accuracy around the boundary between the water and static obstacles. On the other hand, the detection of dynamic obstacles (e.g. boats, buoys, swimmers) is evaluated by counting true-positive (TP), false-positive (FP) and false-negative (FN) detections, summarized by the F1 score. A ground-truth obstacle is counted as a TP if the intersection with the predicted obstacle segmentation is sufficient, otherwise it is counted as a FN. FPs are counted as the number of segmentation blobs (after connected components) on areas annotated as water in ground truth. For further details, please see [86]."}, {"title": "3.2.2 Submissions, Analysis and Trends", "content": "The USV-based obstacle segmentation challenge received 59 submissions from 16 teams. Only the best performing method from each team is included in the final challenge results. The results of the submitted methods are available on the public leaderboards of the challenge on the MaCVi website [36].  This section will focus on analyzing the three best-performing submissions, which were the only ones that improved the results of the top performing approach from the previous MaCVi challenge.  Short descriptions of the top three methods are available in the Appendix B."}, {"title": "3.2.3 Discussion and Challenge Winners", "content": "The overall winners of the USV-based Obstacle Segmentation challenge are:\n1st place: GIST AI Lab with WaterFormer,\n2nd place: Multi-institution team (Yancheng Institute of Technology, Hong Kong University of Science and Technology, University of Liverpool, Xi'an Jiaotong-Liverpool University) with WSFormer, and\n3rd place: Independent researcher with Advanced K-Net\nAll three methods outperformed the previous challenge's winner on the LaRS benchmark and show notable improvement, especially with regard to very small objects, thin structures, and low-light circumstances. However, a large number of tiny objects and visually ambiguous appearances remain open issues. We expect the performance will continue to improve with new datasets and newer methods."}, {"title": "3.3. USV-based Embedded Obstacle Segmentation", "content": "Modern obstacle detection methods often depend on high-performance, energy-intensive hardware, making them unsuitable for small, energy-constrained USVs [63]. The USV-based Embedded Obstacle Segmentation challenge aims to address this limitation by encouraging development of innovative solutions and optimization of established semantic segmentation architectures which are efficient on embedded hardware. The challenge builds on the success of last year's challenge [35] and is an extension of the USV-based Obstacle Segmentation task outlined in Section 3.2. Submissions are evaluated and benchmarked on a real-world OAK4 device from Luxonis."}, {"title": "3.3.1 Evaluation Protocol", "content": "To ensure compatibility with embedded devices, we impose additional constraints that must be adhered to during the development and submission phases like in [35]. Submissions must follow:\n\u2022 Static Graph \u2013 Neural networks must have a well-defined static graph and fixed shapes, and must be exportable to ONNX format without custom operators.\n\u2022 Operation Support \u2013 ONNX can contain only a specific set of operations constrained by the hardware, details of which we provide before the start of the challenge.\n\u2022 Standardized inputs and outputs \u2013 Models are evaluated directly on the device, so they must accept fixed-size input images (768\u00d7384) normalized using ImageNet [16] mean and standard deviation. Output must be a single-channel 2D segmentation mask containing per-pixel class indices. Only methods with a single image input and a single output are permitted.\n\u2022 Performance Requirement \u2013 Models must achieve a throughput of at least 30 FPS on the embedded device OAK4.\nSubmitted architectures are quantized to INT8 using the validation set of LaRS [86] and leveraging Qualcomm's SNPE toolkit [55]. The models are then compiled into a binary executable for the target hardware. During inference,"}, {"title": "3.3.2 Submissions, Analysis and Trends", "content": "The embedded obstacle segmentation challenge received 26 submissions from 4 different teams. In addition, we consider the winning method from last year's competition. We show the best submission from each team in Table 3. For final rankings, we only consider submissions from this year which are faster than 30 frames per second. We received submission reports for the 2 winning methods.\nBoth RSOS-Net (C.1) and EFFNet (C.2) prioritize lightweight architectures and efficient real-time inference leveraging ResNet [28] backbones. RSOS-Net excels at balancing speed and accuracy, while EFFNet explores ensemble techniques to enhance prediction robustness.\nRSOS-Net (Section C.1) features a ResNet-101 backbone paired with a lightweight decoder employing separable convolutions and an attention-based fusion module. Its Fast Pyramid Pooling Module (FPPM) enhances the receptive field, effectively distinguishing obstacles from surface disturbances. EFFNet (Section C.2) utilizes an ensemble of two models - ResNet-50 and ResNet-101 backbones combined with DeepLabv3+ [11] decoders. Prolonged training improved performance, while additional augmentations or heavier backbones did not. The method emphasizes multi-scale feature extraction and stability after quantization. No additional data is used in both methods and they are trained only on the LaRS [86] dataset.\nBoth winning methods significantly outperformed last year's winner and baseline method eWaSR-RN50  and +7.0 Q for RSOS-Net and EFFNet, respectively). At a small cost in recall (-2.6% and -2.5%), precision was significantly better (+17.5% and 12.6%). Both methods were slower (85.1 FPS and 56.3 FPS for RSOS-Net and EFFNet, respectively) than baseline (103.4 FPS eWaSR-RN50) but still achieved high throughput and are suitable for embedded device deployment. We provide full quantitative results in Table 3."}, {"title": "3.3.3 Discussion and Challenge Winners", "content": "The overall winners of the USV-based Embedded Obstacle Segmentation are:\n1nd place: Dalian Maritime University (DLMU) with RSOS-Net.\n2nd place: Advanced Computer Vision Lab (ACVLab) with EFFNet, and\n3rd place: University of Information Technology, VNU-HCM (EAIC-UIT) with eWaSR-RN101 p2.\nThe 2 best-performing submissions significantly outperformed winning methods from last year, while maintaining above real-time throughput. Submitted methods adapt better to challenging scenarios such as glitter, reflections, and lightning. However, due to fixed input shape requirements, they still perform worse in the presence of many small obstacles, especially compared to regular segmentation track. We leave this open for future competitions."}, {"title": "3.4. USV-based Panoptic Segmentation Challenge", "content": "This year MaCVi introduced a new panoptic sub-challenge based on the recent LaRS dataset [86]. In contrast to our segmentation challenges, the panoptic challenge calls for a more fine-grained parsing of USV scenes, including segmentation and classification of individual obstacle instances. This formulation encapsulates the requirements of scene parsing for USV navigation in a more principled way, paving the road for downstream tasks such as tracking individual obstacles, trajectory prediction and obstacle avoidance.\nThe task of the USV-based Panoptic Segmentation Challenge was to develop a panoptic segmentation method that parses the input USV scene into several background (i.e. stuff) and foreground (i.e. things) classes and outputs a set of instance masks and their class labels. Stuff classes include water, sky and static obstacles, and thing classes include 8 different types of dynamic obstacles \u2013 boat/ship, buoy, row boat, swimmer, animal, paddle board, float and other. In addition to training on the LaRS training set, which uses the aforementioned classes, participants were allowed to use other datasets for (pre-)training if disclosed."}, {"title": "3.4.1 Evaluation protocol", "content": "The panoptic performance of submitted methods is evaluated using standard panoptic performance evaluation measures [37,86]: segmentation quality (SQ), recognition quality (RQ) and the combined panoptic quality (PQ):\n$PQ = \\frac{\\sum_{(p, g)\\in TP} IOU(p, g)}{|TP|}  \\times \\frac{|TP|}{|TP|+|FP|+|FN|}$\nsegmentation quality (SQ) recognition quality (RQ)\nTo determine the ranking of the submitted methods, overall panoptic quality (PQ) was considered. For additional insights, individual metrics are also reported separately for thing and stuff classes indicated by superscripts $(\\cdot)^{Th}$ and $(\\cdot)^{St}$.\nFollowing [86], additional dynamic obstacle detections inside static obstacle regions are not considered as false positives. Finally, every participant was required to submit information about the speed of their method including the hardware used for benchmarking and the measured speed in frames-per-second (FPS)."}, {"title": "3.4.2 Submissions, Analysis and Trends", "content": "The USV-based Panoptic Segmentation Challenge received 21 submissions in total from 7 different teams. We only analyze the best-performing method of each team as per the rules of the challenge. Results of the remaining submitted methods are available on the public leaderboards of the challenge on the MaCVi website [36].  The MaCVi team also contributed a baseline Mask2Former (Swin-B) method [13] for comparison. Additionally, a recent PanSR method [85] was submitted to the challenge by the MaCVi team, but is not considered for the winning positions of the challenge.  Short descriptions of the top three methods and baseline approaches are also available in the Appendix D.\nWe are happy to report that all the teams contributed strong methods and improve over the baseline Mask2Former method by a significant margin. Specifically, the first-placing MaskDINO method [40] achieves a PQ of 53.9, outperforming the baseline by +12.5% PQ. While not considered for winning the challenge, the top-performing PanSR method boosts performance by an additional +3.4% PQ.\nInterestingly, the top three methods take a vastly different approach to the problem, but reach similar performance. The (#3) method utilizes an open-vocabulary CLIP-based method [81], the (#2) method combines a top-performing segmentation network from the Obstacle Segmentation Challenge (Section 3.2) with an instance segmentation network [7] for addressing thing classes, and the (#1) method utilizes a strong detection-optimized panoptic network [40], while also experimenting with an open vocabulary approach. None of analysed methods utilized additional data during training other than standard pre-training datasets (COCO [42]). In the following, we analyze the top-performing methods of each of the teams in more detail.In Figure 11 we compare the performance of methods with respect to scene attributes, including environment type, illumination, amount of reflections and scene conditions."}, {"title": "3.4.3 Discussion and Challenge Winners", "content": "The overall winners of the USV-based Panoptic Segmentation challenge are as follows:\n1st place: Fraunhofer IOSB with MaskDINO,\n2nd place: GIST AI Lab with PanopticWaterFormer,"}, {"title": "4. Marine Vision Restoration Challenge", "content": "Underwater image restoration [17, 19\u201321] is the process of enhancing the visual quality of images captured in aquatic environments by eliminating degradations caused by light absorption and scattering. These degradations reduce contrast and introduce color casts, making object detection and localization of underwater species a challenging task. Restoring lost visibility and improving color balance aid in the detection and localization of underwater species, facilitating tasks such as biodiversity monitoring, marine conservation, and underwater robotics. The Marine Vision Restoration Challenge (MVRC) focuses on developing robust image restoration methods to enhance the detection and localization of underwater species. In the first edition of the MVRC challenge the participants are provided with the RSUIGM dataset [18]. The dataset consists of 30 ground truth scenes and 200 corresponding synthetically generated degraded observations for each scene. The challenge is divided into two steps: in the first step, participants must develop a robust algorithm for restoring the degraded images; in the second step, participants must perform the detection and localization of the underwater species considering the restored images from the first step. Participants are permitted to utilize other publicly available datasets to enhance underwater image restoration."}, {"title": "4.1. Evaluation Protocol", "content": "The evaluation is conducted in two phases. In the first phase, we consider perceptual quality metrics (PQM)\u2013 Underwater Image Quality Metric (UIQM), Underwater Color Image Quality Evaluation (UCIQE), Colorfulness Contrast Fog density index (CCF) to assess the performance of restoration methods. In the second phase, we consider Mean IoU (mIoU), F1 Score, and mean Average Precision (mAP) performance metrics to evaluate the effectiveness of the detection (OD) and localization methods. Further, we compute the overall score Q as shown below:\n$Q = 0.8(\\frac{CCF + UIQM + UCIQE}{3}) + 0.2 (\\frac{F1 score + mAP + mIoU}{3})$\nNote: The CCF metric does not have a defined range of values. Therefore, we normalize CCF values between 0 and 1 for fairness and ease of evaluation. We emphasize more on perceptual enhancement of the images and showcase marine species detection as a downstream application of underwater image restoration, hence the weightage (80%) for perceptual metrics and (20%) for object detection evaluation metrics."}, {"title": "4.2. Submissions, Analysis and Trends", "content": "In the first edition of the challenge, we received over 40 submissions from 8 different teams.  The top 4 teams were invited to submit the description of their methods, which are provided in sections E.1, E.2, E.3, E.4 respectively. The submissions show the authors refer to a wide variety of models and approaches for underwater image restoration and enhancement.  In contrast other teams (#2) BUPT, (#3) MTU, and (#4) ACVLab use end to end approaches for underwater image restoration and enhancement followed by object detection.  Followed by restoration, the authors use pre-trained YOLO-NAS model for detecting marine species in the restored underwater images."}, {"title": "4.3. Discussion and Challenge Winners", "content": "The winners of the challenge are as follows:\n1st place: Nanjing University of Science and Technology\n2nd place: Beijing University of Posts and Telecommunications, Beijing, China with MCPRL"}, {"title": "5. Conclusion", "content": "In this summary paper, we analyzed the challenges of the 3rd Workshop on Maritime Computer Vision. Specifically, MaCVi 2025 hosted four different surface domain maritime challenges, and one underwater image restoration challenge.\nThe USV-based Object Distance Estimation challenge focused on detecting buoys in image space and their metric distances in world space.  In future versions, it might be interested to see the more complicated multi-class or open vocabulary distance estimation setups.\nUSV-based Obstacle Segmentation Challenge addressed semantic scene parsing for USVs with a focus on obstacle detection and saw significant improvements over submissions of the previous iteration of MaCVi.\nUSV-based Embedded Segmentation Challenge evalu-"}, {"title": "Appendix - Submitted Methods", "content": null}, {"title": "A. Supervised Object Distance Estimation", "content": null}, {"title": "A.1.  Data Enhance", "content": "This report presents the solution to the Approximate Supervised Object Distance Estimation challenge at the 3rd Workshop on Maritime Computer Vision. We use the YOLOv7 base model, enhancing it with an additional detection head and the BiFormer attention mechanism to better focus on small objects. Additionally, we increase the sample size by horizontally flipping images and apply a range of data augmentation techniques to improve the model's performance. We also integrate the SimAM attention mechanism to further enhance detection performance. Ultimately, we achieve a score of 0.2719, securing first place.\nIntroduction: We focus on several key areas, including multimodal machine learning, open-environment machine learning, and incremental . This challenge involves developing algorithms for estimating the distance to buoys from monocular images.\nMethod: This section provides an overview of our approach. Our method is built upon the YOLOv7 model and primarily includes data augmentations, an additional detection head, the BiFormer attention and the SimAM attention.\n\u2022 Data Augmentations: We increase the sample size by horizontally flipping the images and enhance the model's performance and robustness using a range of data augmentation techniques, including image rotation, translation, scaling, shearing, perspective transformation, vertical and horizontal flipping, mosaic, mixup, and copy-paste.\n\u2022 Additional Detection Head: YOLOv7 is an anchor-based object detection model, rather than anchor-free. Therefore, we add a small initial anchor box and incorporate an additional detection head to detect small objects in the image.\n\u2022 BiFormer: BiFormer [44] is an innovative attention mechanism that enhances the model's detection performance by simultaneously capturing both local and global features through a dual-layer routing design. During the feature extraction process, BiFormer computes attention for each local region to capture detailed features. This step improves the model's sensitivity to small objects and fine details, thereby enhancing detection accuracy. We integrate BiFormer Attention as a plugin into YOLOv7. Specifically, we add BiFormer Attention at the end of the SPPCSPC module and also at the end of the Conv module.\n\u2022 SimAM: SimAM [74] is a parameter-free attention mechanism inspired by neuroscience that assigns 3D attention weights to feature maps in a flexible manner, without increasing model complexity. This enables more efficient and lightweight processing. By using an energy function to measure the linear separability between neurons, SimAM identifies and prioritizes important neurons, thereby enhancing feature extraction. We integrate SimAM into a modified YOLOv7 model to improve object detection performance, leveraging its"}, {"title": "A.2.  YOLOv7 Depth - Widened", "content": "Our method is based on the YOLOv7 architecture provided by the MaCVi 2025 Distance Estimation Challange organizers, which was already customized to include metric distance estimation for detected objects. Our primary contribution was further enhancing the architecture by increasing its width to improve performance.\nWe increased the model's capacity by modifying the width_multiple parameter, which scaled up the number of feature channels in each layer, leading to a total of approximately 49 million parameters. We utilized the default training configuration provided by the organizers with a few key adjustments. Specifically:\n\u2022 Optimization: The distance loss function was weighted, doubling the loss when the ground-truth object distance was below 441 meters. We have also changed the model fitness calculation to consider only the mAP@50:95 metric as that was the metric scored for the challenge.\n\u2022 Augmentation: We employed HSV (hue-saturation-value) augmentation, left-to-right flipping with a 50% probability, and increased the scale parameter to 2.0, which significantly improved model performance."}, {"title": "A.3. Baseline methods", "content": "We include a slightly adapted method from [34] as baseline method."}, {"title": "B. USV-based Obstacle Segmentation", "content": null}, {"title": "B.1.  WaterFormer", "content": "Method. We introduce WaterFormer, a model for USV-based obstacle segmentation illustrated in Figure 15, which leverages parameter-efficient fine-tuning of vision foundation models using adapters. Specifically, we used Mask2Former [13] with the DINOv2 (Large) [52] backbone. We inserted an adapter with an FC(1024, 128)-GELU-DC-GELU-FC(128, 1024) architecture between the DINOv2 layers, where FC(a, b) is a fully connected"}, {"title": "B.2.  WSFormer", "content": "Method: We designed a novel marine image semantic segmentation method, named WSFormer, inspired by Mask2Former [13]. Mask2Former utilizes masked-attention to focus on local features centered around predicted segments, thereby facilitating faster convergence and enhanced performance. Moreover, it employs multi-scale high resolution to accurately segment small regions.  Additionally, we integrated MultiHeadAttention from the Mask2Former framework, which significantly enriches feature representation and spatial awareness in our segmentation task.\nDatasets: For the pretraining phase, we employed the WaterScenes dataset [79], a comprehensive collection of water-related scenes that spans a diverse array of environments and conditions. This dataset is particularly well-suited for our purposes, owing to its extensive coverage of various water scenarios accompanied by semantic segmentation annotations. Following the pretraining stage, we fine-tuned our model using the LaRS dataset [86], as specified in this challenge.\nTraining: We conducted our training on a NVIDIA RTX 4090 GPU, equipped with 24GB of memory. The training process was configured with a batch size of 2 to ensure efficient computation and memory utilization. For optimization, we employed the AdamW optimizer, with a learning rate of 0.0001, an epsilon value of le-8 to prevent division by zero, a weight decay of 0.05 for model regularization, and betas set to (0.9, 0.999) to control the exponential moving averages of the gradient and its square.  To enhance the robustness and generalization of our model, we employed a suite of data augmentation techniques, including RandomResize, RandomCrop, RandomFlip, and PhotoMetricDistortion. Additionally, we integrated three loss functions-classification loss, mask loss, and dice loss-to comprehensively measure the discrepancy between predicted and true class labels, ensure accurate segmentation mask prediction, and optimize the model's performance for"}, {"title": "B.3.  Advanced K-Net", "content": "All of my submissions are marked with Individual as an author in the leaderboard. I have used the K-Net: Towards Unified Image Segmentation method provided in the mmsegmentation-macvi repo.\nI have experimented with different Swin transformer backbones, particularly Swin tiny and large. Swin Tiny is trained on the ImageNet-1K dataset. Swin large is pre-trained on a large dataset, ImageNet-22K, which can significantly improve performance on smaller datasets. No extra dataset was used to train the model.  For optimization, I replaced Adam with Lion, which improves convergence and resulted in a marginal increase in mIoU. I tried introducing stochasticity to the model with a stochastic head, which did not yield significant results.\nFor the training pipeline, resizing, random crop and filling, normalization, photometric distribution, and padding were applied. For validation and inference, only Resize and normalization were performed."}, {"title": "C. USV-based Embedded Obstacle Segmentation", "content": null}, {"title": "C.1.  RSOS-Net", "content": "Method:\nInspired by the encoder-decoder architecture, a realtime surface obstacle segmentation network (RSOS-Net) is created to enable online surface-obstacle detection for an unmanned waterborne vehicle. The network architecture primarily consists of an encoder section and a decoder section. In the encoder, we utilize ResNet-101 [28] as the backbone network to extract multi-scale features.  It is noteworthy that the integration of local and global contextual information is crucial for distinguishing between water surface disturbances and real obstacles.  Finally, the outputs of the lightweight FPN are fused through the AFFM and passed through an FCN segmentation head to obtain the final results.\nTraining:\nWe implemented RSOS-Net using the MMSegmentation framework with an image input size of 768 \u00d7 384 and a batch size of 16.  In terms of data augmentation, techniques such as random flipping, cropping, brightness adjustment, and saturation modification were incorporated to enhance the diversity of the training data."}, {"title": "C.2.  Seg-aware Ensemble (EFFNet)", "content": "Method:\nWe trained two models and combined them to get the final ensemble model.  We selected the lightweight network architectures and further improved prediction accuracy through the ensemble technique. The first model employs ResNet-50 as the encoder paired with DeepLabv3+ as the decoder", "training.\nTraining": "nThe training configurations for the two models are outlined below:\n\u2022 Device: 2 \u00d7 RTX 2080 Ti / 2 x RTX 3090\n\u2022 Epochs: 150/200\n\u2022 Batch size: 16/32\n\u2022 Learning rate: 0.0001 for both\n\u2022 Loss function: Dice loss + weighted cross-entropy loss for both\n\u2022 Optimizer: AdamW for both\n\u2022 Scheduler: Cosine annealing for both\n\u2022 Augmentations: Horizontal flip", "rotation\nObservations": "n\u2022 We observed that extending the training duration consistently enhanced model performance, whereas additional augmentations and larger backbones did not yield significant improvements. Using ResNet-50 as the"}]}