{"title": "Cross-Task Pretraining for Cross-Organ\nCross-Scanner Adenocarcinoma Segmentation", "authors": ["Adrian Galdran"], "abstract": "This short abstract describes a solution to the COSAS 2024\ncompetition on Cross-Organ and Cross-Scanner Adenocarcinoma Seg-\nmentation from histopathological image patches. The main challenge in\nthe task of segmenting this type of cancer is a noticeable domain shift\nencountered when changing acquisition devices (microscopes) and also\nwhen tissue comes from different organs. The two tasks proposed in\nCOSAS were to train on a dataset of images from three different organs,\nand then predict segmentations on data from unseen organs (dataset T1),\nand to train on a dataset of images acquired on three different scanners\nand then segment images acquired with another unseen microscope. We\nattempted to bridge the domain shift gap by experimenting with three\ndifferent strategies: standard training for each dataset, pretraining on\ndataset T1 and then fine-tuning on dataset T2 (and vice-versa, a strat-\negy we call Cross-Task Pretraining), and training on the combination of\ndataset A and B. Our experiments showed that Cross-Task Pre-training\nis a more promising approach to domain generalization.", "sections": [{"title": "1 Introduction", "content": "Effective segmentation of tumoral areas from histopathology images remains a\ncentral problem in the field of computational pathology [2]. Even when circum-\nventing the challenge of processing gigapixel-size whole-slide images by breaking\nthem down into local patches, it is still extremely hard to train segmentation\nmodels that generalize across an array of diverse scenarios, like different acqui-\nsition devices or tissue organs. Such a demanding problem is known as domain\nshift, and has been the focus of intense research in latest years [1,5].\nIn the above context, the Cross-Organ and Cross-Scanner Adenocarcinoma\nSegmentation (COSAS) competition was held in MICCAI 2024, in order to\nbenchmark domain adaptation capabilities of current segmentation models. The\nchallenge was divided into two tracks:\n1. Task 1: Cross-Organ Generalization The goal is to train a segmentation\nmodel on images corresponding two three different organs affected by adeno-\ncarcinoma (gastric adenocarcinoma, colorectal adenocarcinoma, and pancre-\natic ductal adenocarcinoma). The test set contains images from these three\norgans and another three unseen ones."}, {"title": "2 Methodology", "content": "We succinctly describe next our approach to the COSAS competition, with a\ndetailed numerical comparison of several different domain adaptation strategies."}, {"title": "2.1 Segmentation Model", "content": "The segmentation architecture we selected for this task was a Feature-Pyramid\nNetwork with a Mix-Vision Transformer encoder pretrained on the Imagenet\ndataset [3]. Images were resized to a common resolution of 1024 \u00d7 1024, and\nmodels were all trained using an Adam optimizer with an initial learning rate\nof $l = 1e-4$ that was cosine-annealed to zero cyclically during 30 epochs. The\nbinary cross-entropy loss was minimized, as we found no improvement when\nadding a Dice loss component [4]."}, {"title": "2.2 Data Engineering for improving Domain Generalization", "content": "The above model was trained on different datasets, using three strategies, in an\nattempt to obtain better performance but also improve generalization ability:\n1. Standard Training: We used the training data for each task independently.\nWe conducted cross-validation, resulting in a five-fold ensemble submission.\n2. Cross-Task Training: Weights after doing Standard Training for each task\nwere used as initialization when fine-tuning a model to solve the other task.\n3. Task Union: We simply trained for each task on the combination of both\ndatasets. Note that validation sets were different for each task."}, {"title": "3 Experimental analysis", "content": ""}, {"title": "3.1 Datasets and Performance Evaluation", "content": "The dataset for Task 1 is composed of 290 image patches showing six different\nadenocarcinomas. Patches have a resolution of 1500x1500 pixels, and they all\ncome from WSIs digitised using the same scanner. Three organs are present in\nthe training set, but the preliminary test set has 4 and the final test set six\norgan types, so we can expect some performance gap between cross-validation\nscores, our submissions to the preliminary test set (all three approaches were\nsubmitted) and performance on the final test set (only the best approach was\nsubmitted). Similarly, for Task 2 the dataset has images from six scanners, but\nthe training set only has images from three of them, whereas the preliminary\ntest set has four scanners and the final test set has the six of them. Performance\nin this case was measured by means of the standard Dice Similarity Coefficient."}, {"title": "3.2 Numerical Results", "content": "As can be seen from Table 1, a conventional strategy leads to the lowest cross-\nvalidation results in both tasks, a trend that is reproduced in the preliminary\ntest set. In contrast, both using the union of the two datasets for training,\nand specially pretraining on one task and then fine-tuning on the other one\n(Cross-Task Pretraining improve performance across the board, with the latter\nobtaining sizeable improvements. Note that we made allowed six submissions to\nthe preliminary test set, but only one to the final one, so we selected the best\nperforming approach (Cross-Task Pretraining) for our final submission."}, {"title": "4 Conclusions", "content": "We summarized and experimentally tested several strategies for dealing with\ndomain shift in adenocarcinoma segmentation from histopathological images.\nCross-Task Pretraining, i.e. training on one task and then fine-tuning on the\nother resulted in the highest performance for both tasks."}]}