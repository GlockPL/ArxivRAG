{"title": "AKiRa: Augmentation Kit on Rays for optical video generation", "authors": ["Xi Wang", "Robin Courant", "Marc Christie", "Vicky Kalogeiton"], "abstract": "Recent advances in text-conditioned video diffusion have greatly improved video quality. However, these methods offer limited or sometimes no control to users on camera aspects, including dynamic camera motion, zoom, distorted lens and focus shifts. These motion and optical aspects are crucial for adding controllability and cinematic elements to generation frameworks, ultimately resulting in visual content that draws focus, enhances mood, and guides emotions according to filmmakers' controls. In this paper, we aim to close the gap between controllable video generation and camera optics. To achieve this, we propose AKiRa (Augmentation Kit on Rays), a novel augmentation framework that builds and trains a camera adapter with a complex camera model over an existing video generation backbone. It enables fine-tuned control over camera motion as well as complex optical parameters (focal length, distortion, aperture) to achieve cinematic effects such as zoom, fish-eye effect, and bokeh. Extensive experiments demonstrate AKiRa's effectiveness in combining and composing camera optics while outperforming all state-of-the-art methods. This work sets a new landmark in controlled and optically enhanced video generation, paving the way for future optical video generation methods.", "sections": [{"title": "1. Introduction", "content": "Creating high-quality video sequences has always been a complex interplay between the content itself and the means, the camera, by which it is portrayed. A powerful narrative or stunning visual concept can only reach its full po-"}, {"title": "2. Related work", "content": "Our work contributes to the field of controllable video generation, which can be approached in two ways: in the first, the generation is guided by text, while in the second way, the generation is guided by camera and text. Another related area is virtual cinematography, which does not generate videos but instead focuses on entities within an existing environment, e.g. camera angles, lighting, and composition.Text-to-video generation (T2V). The first video diffusion model is introduced by [25], building on the success of image diffusion [14, 23, 45, 46]. Later, Imagen-Video [24] and Make-A-Video [43] introduced cascaded pixel-based diffusion models for high-definition video generation. To reduce training costs, some works [1, 6, 54] perform diffusion in latent space. Others [5, 7, 18, 51] fine-tune temporal adapters on 2D layers of pre-trained text-to-image models [41] using video datasets [3]. Recently, [34, 35, 70] explored transformer backbones (DiT) for scalability, while RIVER [13] and MovieGen [40] moved to flow matching, achieving state-of-the-art performance. These methods rely solely on text guidance, which is often sufficient for image generation. However, video generation requires additional complexity, incorporating temporal dynamics and camera"}, {"title": "3. Method", "content": "In this section, we present our approach for training an optical video generation model, enabling users to manipulate the camera's motion as well as optics to produce cinematic effects such as zoom, distortion, and bokeh.\nOverview. To train an optical video generation model capable of controlling both camera motion and optics, we require training data with associated camera parameters. Here, optics refer to the camera's optical characteristics, including focal length, lens distortion, aperture and focus point, which together shape how the camera captures field-of-view (zoom), lens characteristics (distortion), and light exposure (in- or out-of-focus). Although some recent datasets pair videos with camera trajectories [12, 71], to our knowledge, there are no datasets that include videos with rich varying optical information. Hence, we propose a set of data augmentations based on a complex camera model that parameterizes optics (Section 3.1). To better disentangle each parameter, we design a highly expressive representation for our model (Section 3.2). Finally, we describe the augmentations to generate videos paired with optical parameters (Section 3.3). As illustrated in Figure 2, with these augmentations, we extend prior works [21, 56], by training a camera adapter that controls camera motion and optics on top of a pre-trained, frozen video generation backbone."}, {"title": "3.1. Camera model", "content": "To train an optical video generation model, we need a camera model to represent the camera parameters. Here we propose a camera model by relying on and extending the pinhole camera model to represent not only camera motion and focal length (pinhole camera model) but also the lens distortion and the aperture and focus point (distorted pinhole camera model with aperture), as described below.\nPinhole camera model. We build our camera model upon the standard pinhole camera model [20], which includes extrinsic and intrinsic camera parameters. The extrinsic"}, {"title": "3.2. Camera model representation", "content": "Ray-based camera model representation. To train our optical video generation model, we require an effective camera representation that connects the optical properties of the camera to the generated visual content. For this, we map the geometric camera model (P, K, D) to screen pixels (u, v) \u2208 R2 (or patches in practice) using a ray representation r, where each pixel is associated with a ray (a line) passing through the camera's centre O \u2208 R3.\nPl\u00fccker map. We adopt the Pl\u00fccker coordinates [39] to represent our camera model, as in [21, 64]. A ray r = (d, m) \u2208 R6 is represented by its direction d and its moment m about any point p on the ray, such that m = p \u00d7 d. The direction d is computed by reprojecting the pixel coordinates (u, v) with camera parameters P and K, and the moment m is calculated by taking the camera centre O as the point p since all rays pass through O:\n Aperture map. The camera's aperture parameter a is not captured by Pl\u00fccker coordinates, as it is not directly ray-related in the Pinhole model, as shown in Figure 3d. To address this, we introduce an aperture map with the same structure and dimensions as the direction and moment maps, assigning an aperture parameter to each pixel in the frame. To achieve this, we first define the coordinates of the focus point (uin, Vin) representing the sharpest point in the frame."}, {"title": "3.3. AKiRa: Augmentation Kit on Rays", "content": "To augment and disentangle optical features in our extended Pl\u00fccker camera model, we propose AKiRa, an Augmentation Kit on Rays. It contains augmentation techniques for both video frames and corresponding optical parameters:"}, {"title": "4. Experiments", "content": "For video quality of generated videos, we report the commonly used Fr\u00e9chet Inception Distance (FID) [22], Fr\u00e9chet Video Distance (FVD) [49]. Given that recent works [17, 59] point out that the FVD tends to be biased toward content while overlooking the temporal aspects, we propose to report the Content-Debiased FVD (CD-FVD) [17], considered more relevant.\nEvaluating camera motion fidelity is challenging for generated content. Some approaches use SLAM and pose estimation methods [37, 42, 67] to evaluate the estimated trajectory from generated video content (MotionCtrl [56], CameraCtrl [21]). However, these often assume (partial) static consistency, which is hard to maintain in the generated content due to flickering or unrealistic motion artefacts but irrelevant to motion quality; (ii) the absence of precise camera model definitions (e.g., focal length, distortion) further complicates trajectory-based assessments [33, 47] given that optic parameters greatly influence motion interpretation; (iii) trajectory-based methods are computationally intensive (can reach ~10 minutes per 16-frame video [67]) and struggle to scale efficiently for large-scale video assessments. See motion evaluation in supplementary materials.\nTherefore, to show the camera motion fidelity towards the control we report two metrics: first, following [21, 56] reporting trajectory errors, we estimate dense camera poses using ParticleSfM [67] but calculate scale-corrected rotational and translational relative pose errors between frames (RPE-R and RPE-t, respectively). The choice of using scale correction and relative pose aims at reducing the unstable estimation of trajectory-based metrics [33, 47].\nAdditionally, to measure motion alignment between reference and generated videos we propose a flow similarity metric (FlowSim). This metric relies less on camera model parameters and focuses primarily on frame-to-frame motion. Moreover, the optical flow-based approach is computationally efficient and scalable with GPU parallelism. For this, we estimate the optical flow of both videos Fr and Fg using RAFT [48], then extract the flow magnitude ||F|| and direction \u03c6. To filter out residual noise, we consider only flow components with a magnitude above a set threshold t, and we compute the cosine similarity of the directional components as follows, with 1 an indicator function:"}, {"title": "5. Conclusion", "content": "In this paper, we introduce the concept of optical video generation, a framework that allows users to control camera motions as well as optical parameters. We trained a dedicated camera adapter using a set of data augmentation techniques -AKiRa that pairs camera/lens parameters with corresponding videos. Results show that our framework generates optically coherent content, outperforming state-of-the-art approaches while offering extra control. AKiRa expands the possibilities of video generation and bridges the gap between synthetic and real-world capabilities."}]}