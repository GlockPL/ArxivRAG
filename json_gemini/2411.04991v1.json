{"title": "Rethinking Bradley-Terry Models in Preference-Based Reward Modeling: Foundations, Theory, and Alternatives", "authors": ["Hao Sun", "Yunyi Shen", "Jean-Francois Ton"], "abstract": "The Bradley-Terry (BT) model is a common and successful practice in reward modeling for Large Language Model (LLM) alignment. However, it remains unclear why this model originally developed for multi-player stochastic game matching can be adopted to convert pairwise response comparisons to reward values and make predictions. Especially given the fact that only a limited number of prompt-response pairs are sparsely compared with others. In this paper, we first revisit the foundations of using BT models in reward modeling, and establish the convergence rate of BT reward models based on deep neural networks using embeddings, providing a theoretical foundation for their use. Despite theoretically sound, we argue that the BT model is not a necessary choice from the perspective of downstream optimization. This is because a reward model only needs to preserve the correct ranking predictions through a monotonic transformation of the true reward. We highlight the critical concept of order consistency in reward modeling and demonstrate that the BT model possesses this property. Consequently, we propose a simple and straightforward upper-bound algorithm, compatible with off-the-shelf binary classifiers, as an alternative order-consistent reward modeling objective. To offer practical insights, we empirically evaluate the performance of these different reward modeling approaches across more than 12,000 experimental setups, using 6 base LLMs, 2 datasets, and diverse annotation designs that vary in quantity, quality, and pairing choices in preference annotations.", "sections": [{"title": "1 Introduction", "content": "The alignment of Large Language Models (LLMs) is crucial for their safe and effective deployment across various applications. Current research on reinforcement learning from human feedback (RLHF) (Christiano et al., 2017; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a) has largely focused on utilizing preference-based annotations provided by humans or general-purpose LLMs (Bai et al., 2022b; Lee et al., 2023; Guo et al., 2024). In general, there are two primary approaches to RLHF, namely the direct policy optimization (Rafailov et al., 2024; Zhao et al., 2023; Azar et al., 2023) that aligns LLMs with supervised learning objectives, and the alternate method that constructs a reward model to guide the"}, {"title": "2 Rethinking the Usage of BT Models in LLM Alignment", "content": ""}, {"title": "2.1 Two Different BT Models \u2014 Parameter Estimation and Prediction", "content": "The original BT model (Bradley and Terry, 1952), aligned with the Luce-Shephard choice rule (Luce, 1959; Shepard, 1957), posits that in a simplified two-option scenario, the probability of selecting option i from a set i, j is proportional to the utility u(\u00b7) assigned to that option. Formally, this can be expressed as a softmax output of the log utilities r(\u00b7) (Bockenholt, 1988)"}, {"title": null, "content": "P(i > j) = \\frac{u(i)}{u(i) + u(j)} = \\frac{exp(r(i))}{exp(r(i)) + exp(r(j))} = softmax(r(i), r(j)).\t\t\t\t\t\t\t\t\t\t\t\t(1)"}, {"title": "LLM Arena with the BT Model", "content": "One of the successful applications of the classical BT model in the LLM is the chatbot arenas (Chiang et al., 2024), where multiple LLMs compete against one another based on human feedback through paired comparisons. Here, each LLM functions as a player, and the human-annotated preferences represent the outcome of these game matches. The goal is to assign a single performance score to each LLM player. In Chiang et al. (2024) 130 models were compared across more than 1.7 million comparisons, with each model participating in over 26,000 matches on average.\nIn such a setting, estimating the values of r(\u00b7) is sufficient to achieve the primary goal of evaluating each chatbot's performance. This aligns closely with the original objective of BT model in ranking sports teams (Bradley and Terry, 1952). Previous work has shown that, with enough pairwise competition, one can estimate these ability scores well (Ford Jr, 1957; Han et al., 2020; Wu et al., 2022) up to a constant additive factor. It is shown that to estimate N scores via random pairwise comparisons, the theoretical lower bound on the number of comparisons is O(Nlog(N))\u00b9 while the best-known methods require O(N log\u00b3(N)) comparisons (Han et al., 2020)."}, {"title": "Reward Modeling with BT Model: Understanding Implicit Assumptions", "content": "In contrast, the application of the BT model to reward modeling is not as straightforward. First, the implications of using the BT model in this context are not well-defined in the"}, {"title": "2.2 Comprehending the BT Model Application in Preference Annotations", "content": "In the literature, several prior works have challenged the practical application of the BT model (Azar et al., 2023; Munos et al., 2023; Tang et al., 2024). While the previous anal-ysis of whether the BT model is a good choice focused on the compatibility between the BT model and data (e.g., transitivity (Gardner, 1970), existance of hard decision bound-ary (Azar et al., 2023; Tang et al., 2024)). In this section, we revisit the basic assumptions of modeling human preference annotations with the BT model, and focus on answering the following question:\nWhat are the underlying assumptions when we assume the BT model can be used to model the preference annotations?\nThe canonical interpretation of how to apply Equation (1) in preference-based learning is that: when randomly sampling a human annotator from the population, the human annotator's choice of the preferred response is proportional to the response's utility value. Formally, we use x, Y1, Y2 to denote the prompt and responses, the above interpretation implies the following assumptions:"}, {"title": "Assumption 1 (Existence of Deterministic Oracle Utility Values)", "content": "The (log-)utility value rx,y of any response y given x exists and is deterministic."}, {"title": "Assumption 2 (Deterministic Comparisons)", "content": "For annotator A, the annotation result is deterministic and depends on the comparison of their biased evaluation of the utility values of both responses y1 and y2, such that"}, {"title": null, "content": "1(y1 > y2|x, A) = 1 (rx,y1 + b(x, y1, A) > rx,y2 + b(x,y2, A))\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(2)"}, {"title": null, "content": "The randomness of annotations originates from the bias b associated with annotators. The following distributional assumption on b leads to BT model in the annotation process:"}, {"title": "Assumption 3 (Logistic Difference Assumption)", "content": "The b(x, y1, A) \u2013 b(x, y2, A) is sam-pled i.i.d. from a standard logistic distribution for all x,y:"}, {"title": null, "content": "P(b(x, y1, \u0410) \u2013 b(x,y2, A) \u2264 t|A) = \\frac{1}{1+ e^{-t}}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(3)"}, {"title": "Remark 1 (Transitive property of difference)", "content": "A reader might be (rightfully) worried if this assumption is consistent with transitive property of difference, e.g., when considering multiple comparisons we have to have b(x, y1, A) \u2013 b(\u0445, \u0443\u0437, A) = b(x, y1, A) \u2013 b(x, y2, A) +\nb(x, y2, A) \u2013 b(x,y3, A) while knowing sum of (independent) logistic distributions is not lo-gistic. One should be aware that the two terms being summed are not independent and the assumption can be achieved by assuming all annotator biases are independently Gumbel distributed with the same scale parameter."}, {"title": null, "content": "With those assumptions, we arrive at the BT-type model"}, {"title": "Proposition 2 (Modeling Annotations under Logistic Difference Assumption)", "content": ""}, {"title": null, "content": "P(y1 > y2|x) = P(rx,y1 - rx,y2 > b(x, y1, A) \u2013 b(x, y2, A)) = \\frac{1}{1+ e^{-(r_{x,y1}-r_{x,y2})}}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(4)"}, {"title": null, "content": "While the above assumption on logistic bias differences lead to the BT-type models, it is also natural to check other assumptions such as the Gaussian difference assumption and its corresponding model:"}, {"title": "Assumption 4 (Gaussian Difference Assumption)", "content": "The b(x, y1, A)-b(x,y2, A) is sam-pled from a standard Gaussian distribution:"}, {"title": null, "content": "b(x, y1, A) \u2013 b(x, y2, A) ~ N(0, 1)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(5)"}, {"title": "Proposition 3 (Modeling Annotations under Gaussian Difference Assumption)", "content": ""}, {"title": null, "content": "P(y1 > y2|x) = P(x,y1 - rx,y2 > b(x, y1, A) \u2013 b(x, y2, A)) = \\Phi(r_{x,y1} - rx,y2),\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(6)"}, {"title": null, "content": "where is the CDF of the Gaussian distribution.\nTo elaborate implications of those different assumptions and provide an alternative per-spective to understand those assumptions, we have the following remark:"}, {"title": "Remark 4 (Assumption on Performance in a Game)", "content": "Assuming the performance of players A and B in a game is a Gaussian distribution centered at \u03bc\u03b1,\u03bc\u0392, and the value of performance determines which player wins in the game, we have"}, {"title": null, "content": "P(A wins B) = P (\u0438\u0430 \u2265 \u0438\u044c) = \\frac{1}{2} + \\frac{1}{2} erf \\left(\\frac{\\mu_{A}-\\mu_{B}}{2\\sigma}\\right), ua ~ \u039d(\u03bc\u03b1, \u03c3\u00b2), ub ~ N(\u00b5\u00df, \u03c3\u00b2).\t\t\t\t\t\t\t\t\t\t(7)"}, {"title": null, "content": "Alternatively, when assuming the performance of players A and B in a game is a Gumble distribution located at \u03bc\u03b1, \u03bc\u03b5 with scale parameter b, and the value of performance deter-mines which player wins in the game, we have"}, {"title": null, "content": "P(A wins B) = P (ua \u2265 \u0438\u044c) = \\frac{1}{2} + \\frac{1}{2}tanh\\left(\\frac{\\mu_{A}-\\mu_{B}}{2b}\\right) = \u03c3\\left(\\frac{\\mu_{A}-\\mu_{B}}{b}\\right), ua ~ G(\u03bc\u0104, b), \u044c ~ G(\u00b5\u00df, b).\t\t\t\t(8)"}, {"title": "2.3 BT Regression: How BT Model Works with Sparse Comparisons", "content": "Additionally, estimating a separate r(\u00b7) for each prompt-response pair is impractical. In typical LLM alignment scenarios, we often have only N/2 comparisons for N pairs, far below the theoretical lower bound for consistent estimation (Han et al., 2020). Furthermore, unlike the arena setting, there is no clear way to predict the score for a new, unseen pair. However, this challenge is not unique to LLM alignment; sports analysts also need to estimate a team's ability before many competitions or predict the ability of a new team. A common approach in such cases is to use features or covariates, such as team composition or funding status, to predict scores. For LLM, one could have sentence embeddings as such covariates.\nThese extensions of the BT model on regression settings were explored shortly after its original introduction: Springall (1973) assumed that r(\u00b7) could be expressed as a linear com-bination of covariates. In this scenario, the problem reduces to a classic logistic regression on the difference between two sets of covariates. This allows us to predict the score for a new team or prompt-response pair based on its covariates before any comparisons are made. More complex nonlinear models such as spline models have also been explored (De Soete and Winsberg, 1993). For a more comprehensive review from a statistical perspective, we refer readers to Cattelan (2012)."}, {"title": null, "content": "In practice, reward modeling in LLM alignment often employs neural networks, with multilayer perceptrons (MLPs) being a common choice to map embeddings to scores. How-ever, there is currently no theoretical justification for why this particular choice of model and loss function is effective for learning reward functions. From a theoretical standpoint, this model variant can be viewed as a nonparametric logistic regression problem (Bocken-holt, 1988; Schmidt-Hieber, 2020) with additional structural assumptions on the network, and our analysis builds on this framework. In the following section, we establish the asymp-totic theory for learning reward models using MLPs and BT loss."}, {"title": "2.4 Asymptotic Theory on MLP-based BT Regression in Reward Modeling", "content": "In preference-based LLM alignment, we work with the dataset under the form of Dpref = {(Xi, Y1,i, Y2,i, hi)}i\u2208[n], where each tuple consists of the prompt xi, the corresponding re-sponses Y1,i and y2,i sampled from the LLM l to be aligned Y1,i, Y2,i ~ l(xi), and the human-annotated preference hi, being 1 if Y1,i is preferred and -1 otherwise."}, {"title": "3 Rethinking Reward Modeling Objectives in LLM Alignment", "content": "Practical implementation of the BT model poses several requirements including the paired data, the specially designed anti-symmetric model structure, and the inherent assumptions of the BT model itself. This leads us to question whether we can have alternative approaches to reward modeling. To address this, it is helpful to pause and reflect on the essential requirements of a reward model in LLM alignment. Our data consists of binary preferences,"}, {"title": "3.1 The Unified Target of Order Consistency", "content": "In basic binary classification, we prioritize accuracy over modeling output probabilities precisely. For example, neural classifiers, despite being overconfident (Guo et al., 2017), are widely used for their accuracy. Similarly, we do not have to require the reward model to predict comparison probabilities accurately as in BT, but rather to provide a reliable signal for ranking LLM outputs at inference.\nSince our goal is response optimization using a reward proxy, it is sufficient to learn the reward function up to a monotonic transformation. While this might alter preference probabilities, it won't affect optimization results. To this end, the learned reward function \u2191 only needs to satisfy the following condition: for any two distinct prompt-response pairs (X1,Y1) and (x2, y2) (note that we do not preclude x1 = x2), we require that (f(x1,Y1) \u2013 r(x2,Y2))(r(x1,Y1) - r(x2, y2)) > 0. In other words, the learned reward function must preserve the ordering as the true reward function.\nThis condition implies the existence of a strictly monotonic increasing function h such that (.) = h(r(\u00b7)). Such an equivalence is sufficient for optimizing the reward in settings, such as sampling-based optimization, and contextual bandits (Agarwal et al., 2014; Latti-more and Szepesv\u00e1ri, 2020). Ideally, if we have access to the ground truth ordering, we can define h = sign(r(x1, y1) - r(x2, y2)). If we can (1) construct a model H : X \u00d7 Y \u00d7 XXY H {+1, -1} that predicts the correct ordering with high accuracy (i.e., \u0124 is order consistent), and (2) map this ordering into a continuous value, then we can meet the requirements for downstream optimization. That is, an ideal goal for the order model is to achieve"}, {"title": null, "content": "\\hat{H}(r(x1, y1) \u2013 r(x2, y2)) > 0\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(15)"}, {"title": null, "content": "However, Equation (15) cannot be directly evaluated because the true reward was never observed and observed ordering from the human annotator is often subject to noise. Drawing on insights from the psychological bottleneck literature (Stewart et al., 2005; Guest et al., 2016), it is reasonable to assume that when the true underlying scores of two responses are similar, it becomes more difficult for annotators to distinguish between them. Formally, we have"}, {"title": "Assumption 5 (Imperfect Preference Annotation in Approximating True Scores)", "content": "Denote the true utility difference \u2206r := |r(x1,y1) \u2014 r(x2, y2)|, and the annotator function h(x1,x2, Y1, Y2) provides feedback that probabilistically aligns with the oracle utility r(x,y). We will assume it is harder for them to assign correctly when the reward difference between two pairs is \u2206r according to:"}, {"title": null, "content": "Ph(x1, x2, Y1, Y2) (r(X1,Y1) - r(x2,Y2))\n> 0 Ar\n= \u03be(\u0394r),\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(16)"}, {"title": "Definition 8 (Order Consistency)", "content": "We consider the loss over an ordering model H"}, {"title": null, "content": "Loc(r) = Ex1,42,41,42,41 [h = \u0124]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(17)"}, {"title": null, "content": "That is, the probability that a reward model ordering agrees with annotation.\nWe show with the following proposition that minimizing this (observable) loss would help us achieve order consistency with true reward function Equation (15) with high probability:"}, {"title": "Proposition 9 (Lower bound on population level order consistency)", "content": "Suppose a learned model \u0124 achieves objective equation 17 up to 1 \u2013 de error for some small 0 < d < 1 and \u0454 < 3/20, i.\u0435.,"}, {"title": null, "content": "Ex1,2,41,42,41 [h = H] \u2265 1 \u2013 \u03b4\u03b5\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(18)"}, {"title": null, "content": "Then, with probability at least 1 \u2013 \u03b4 over Ar, for any given Ar the order consistency of r with respect to the oracle utility is bounded below by:"}, {"title": null, "content": "Ex1,x2,41,42 ~l(x) \n0) Ar \u2265 (1 \u2013 \u20ac) \u00b7 \u03be\u00b2 (Ar) + \u20ac. (1 \u2013 (Ar))2\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(19)"}, {"title": null, "content": "Further if we assume that with probability at least 1 \u2013 \u03ba, that \u03be(\u2206r) > \u221a\u20ac\u00b2 + 1 \u2212 3\u20ac + \u0454, we have"}, {"title": null, "content": "Ex1,x2,41,42~l(x) \n1 (H. [r(x1, y1) - r(x2,y2)] > 0)] \u2265 1 - 46 - \u043a - \u03b4\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(20)"}, {"title": null, "content": "Note that, in analogy to a classic classification problem, Equation (17) correspond to classification accuracy while the BT model attempted to have not only accuracy but also cal-ibrated conditional class probability, a potentially harder problem, and a cross-entropy loss is almost necessary. Indeed in classical classification problems, accuracy is often optimized indirectly using losses like cross-entropy but not restricted to this particular choice. This hints at us applying techniques for improving prediction accuracy to improve reward model-ing even if the techniques cannot provide a calibrated probability. While the BT model uses cross-entropy loss and an antisymmetric structure, in the following, we show an alternative choice that could lead to a simple classification-based algorithm."}, {"title": "3.2 The BT Model as a Choice", "content": "The BT model is designed to enforce order consistency in the following way: it models the probability that h = 1 using \u03c3(fBT(x1, y1) - \u00ceBT(x2,y2)), where o is the sigmoid function. This allows training with a binary cross-entropy loss:"}, {"title": null, "content": "LBT = E [1h=10(\u00ceBT(X1,Y1) \u2013 \u00ceBT(X2,Y2)) + 1h=\u22121(1 \u2212 \u03c3(\u00ceBT(x1, y1) - \u00ceBT(x2,y2)))]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(21)"}, {"title": null, "content": "This structure guarantees that flipping the comparison order will also flip the prediction."}, {"title": "3.3 Relaxing the Anti-Symmetry Constraint: a Classification-based Method", "content": "BT's difference-in-reward structure inherently enforces antisymmetry. To better understand this, consider an order model \u0124 that outputs a preference vector for both prompt-response pairs, i.e., \u0124 := (H1, H2), where H1, \u01242 : X \u00d7 Y \u2192 {1, -1} and ideally align with (h, -h). The BT model imposes a hard constraint such that H\u2081 = -\u01242. With sufficient data, instead of explicitly enforcing this constraint, the structure could be learned implicitly by ensuring order consistency, i.e., H\u2081 \u2248 h and H2 \u2248 -h. Consider a single model e.g., a neural network or tree Helf. Under this construction, the order consistency could be written as Loc := E(h = Hclf(x1, y1) ^ -h = Hclf(x2, y2)), we could train a model targeting on predicting these two parts separately, and if the model predicts well, it should satisfy the antisymmetry constraint. A union bound of order consistency is"}, {"title": null, "content": "Loc < Lclf := E(h = Hclf(x1, y1)) + E(-h = Hclf(x2,Y2))\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(22)"}, {"title": null, "content": "instead of directly enforcing order consistency, we can use the classification accuracy of each prompt-response pair as a surrogate. In practice, this means training a classifier by treating the annotations and prompt-response pairs independently. Then the logit can be used as a proxy for the reward model. For an alternative perspective: instead of learning the joint probability P(i > j) that depends on both players i and j, we focus on learning the marginal probability P(i wins). These two are related via Jensen's inequality, with further details provided in Proposition 24."}, {"title": "4 Rethinking the Preference Annotation Process for Reward Modeling", "content": "In both BT and classification-based reward modeling, there is no theoretical requirement to limit comparisons to the same prompts. For classification models, this is straightfor-ward, as they do not rely on paired data at all. Similarly, in traditional BT applications, random pairwise comparisons among players are common. This further motivates our inves-tigation into how randomized comparisons across different prompts affect reward modeling performance.\nTo further motivate the usage of cross-prompt comparison, we introduce the following notation on annotation quality and analysis as a case study under a Gaussian assumption on score distributions. In equation 16, we consider a special case of & in equation 16 to"}, {"title": null, "content": "be \u03be(\u00b7) = \u03c3(\u03b2.), that the annotators' ability is characterized by B. When \u03b2 = 0, we have random annotations:"}, {"title": null, "content": "IP \\left(\n h(x1, x2, Y1, Y2) (r(x1, y1) - r(x2, y2))\n> 0 Ar\n\\right) = \u03c3(\u03b2\u2206r) \\stackrel{\\beta \\rightarrow 0}{\\rightarrow} 0.5,\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(23)"}, {"title": null, "content": "and when \u2192 \u221e, we have perfect annotations:"}, {"title": null, "content": "IP (h(X1, X2, Y1, Y2)(r(x1, y1) - r(x2, y2)) >\n0/Ar) = \u03c3(\u03b2\u0394r) \u03b2\u221e 1.\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(24)"}, {"title": null, "content": "In a nutshell, the annotator's abilities and the differences among prompt-response pairs together determine how much preference is correctly labelled in the annotation. In the following, we show a special case when two responses of a prompt x are randomly sampled from a single LLM l."}, {"title": "Example 1 (Annotation Quality under Gaussian Score)", "content": "When data for pairwise an-notation is generated through random sampling of two responses Y1, Y2 ~ l(x), we further assume the utility of those two responses are sampled from a Gaussian distribution with variance oz, i.e., y ~ l(x), r(x,y) ~ \u039d(\u03bc\u03b1,\u03c32). Then the annotation quality Qpair(x) on such a prompt can be defined as the averaged annotation order consistency:"}, {"title": null, "content": "Qpair(x) = Ey1,y2|x[Tx] = Ey1,y2|x [\u03c3(\u03b2|r(x, y1) \u2013 r(x, y2)|)]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(25)"}, {"title": null, "content": "where \u03c4\u03b1 = \u03c3(\u03b2|r(x,y1) \u2013 r(x,y2)|) is a random variable (over Y1,Y2) and the probability density function of Tx is"}, {"title": null, "content": "frxx(t) = \\frac{1}{\\sqrt{\\pi}\\beta\\sigma_{x}} exp \\left(-\\frac{\\left(\\log \\frac{t}{1-t}\\right)^{2}}{4\\beta^{2}\\sigma_{x}^{2}}\\right) \\frac{1}{t(1-t)}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(26)"}, {"title": null, "content": "Next, we show that cross-prompt comparison in this example can be an effective practice to increase response utility diversity. And show that cross-prompt annotation can improve annotation quality."}, {"title": "Cross-Prompt Annotation Improves Quality under Gaussian Score", "content": "When con-sidering multiple prompts xi, i = 1, 2, ..., N, we denote the corresponding responses as Yi, and scores r(xi, Yi) ~ N(\u03bc\u03b5, \u03c3?). In the following, we show that cross-prompt annotation can improve annotation quality."}, {"title": "Proposition 10 (Cross-Prompt Comparisons Increase Utility Diversity)", "content": "When data for pairwise annotation is generated through random sampling of two responses Y1, Y2 ~"}, {"title": null, "content": "l(x), and the utility of those two responses are sampled from a Gaussian distribution with variance 6, i.e., y ~ l(x),rx,y ~ N(\u03bc\u03b1, \u03c32), when there are multiple prompts x, we have"}, {"title": null, "content": "ExEy1,y2 x [|rx,y1 - Tx,y2|] \u2264 Ex1,x2Ey1|x1,y2|22 [|Tx1,y1 - r_{x2,y2}|]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(27)"}, {"title": null, "content": "More generally, cross-prompt comparisons improve data quality when the utility dis-tribution of different randomly sampled responses given a single prompt is unimodal and symmetric (e.g., Gaussian)."}, {"title": "Theorem 11 (Cross-Prompt Annotation Improves Annotation Quality)", "content": "When data for pairwise annotation is generated through random sampling of two responses Y1, Y2 ~ l(x), and the utility of those two responses are sampled from a location-scale family with probability density function gx(x) = f((x \u2212 \u03bcx)/\u03c3\u30a7) for f being unimodal and symmetric to 0. For any \u00a7: R+ \u2192 [1/2,1], first order differentiable, monotone increasing and concave, we have"}, {"title": null, "content": "Ex [Qpair(x)"}]}