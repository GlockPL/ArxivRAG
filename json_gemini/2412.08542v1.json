{"title": "MAESTROMOTIF: SKILL DESIGN FROM ARTIFICIAL INTELLIGENCE FEEDBACK", "authors": ["Martin Klissarov", "Mikael Henaff", "Roberta Raileanu", "Shagun Sodhani", "Pascal Vincent", "Amy Zhang", "Pierre-Luc Bacon", "Doina Precup", "Marlos C. Machado*", "Pierluca D'Oro*"], "abstract": "Describing skills in natural language has the potential to provide an accessible\nway to inject human knowledge about decision-making into an AI system. We\npresent MaestroMotif, a method for AI-assisted skill design, which yields high-\nperforming and adaptable agents. MaestroMotif leverages the capabilities of Large\nLanguage Models (LLMs) to effectively create and reuse skills. It first uses an\nLLM's feedback to automatically design rewards corresponding to each skill,\nstarting from their natural language description. Then, it employs an LLM's code\ngeneration abilities, together with reinforcement learning, for training the skills\nand combining them to implement complex behaviors specified in language. We\nevaluate MaestroMotif using a suite of complex tasks in the NetHack Learning\nEnvironment (NLE), demonstrating that it surpasses existing approaches in both\nperformance and usability.", "sections": [{"title": "1 INTRODUCTION", "content": "Bob wants to understand how to become a versatile AI researcher. He asks his friend Alice, a\nrespected AI scientist, for advice. To become a versatile AI researcher, she says, one needs to\npractice the following skills: creating mathematical derivations, writing effective code, running and\nmonitoring experiments, writing scientific papers, and giving talks. Alice believes that, once these\ndifferent skills are mastered, they can be easily combined following the needs of any research project.\nAlice is framing her language description of how to\nbe a versatile researcher as the description of a set of\nskills. This often happens among people, since this\ntype of description is a convenient way to exchange\ninformation on how to become proficient in a given\ndomain. Alice could instead have suggested what piece\nof code or equation to write, or, at an even lower level of\nabstraction, which keys to press; but she prefers not to do\nit, because it would be inconvenient, time-consuming,\nand likely tied to specific circumstances for it to be\nuseful to Bob. Instead, describing important skills is\neasy but effective, transmitting large amounts of high-\nlevel information about a domain without dealing with\nits lowest-level intricacies. Understanding how to do the\nsame with AI systems is still a largely unsolved problem.\nRecent work has shown that systems based on Large\nLanguage Models (LLMs) can combine sets of skills to\nachieve complex goals (Ahn et al., 2022; Wang et al.,\n2024). This leverages the versatility of LLMs to solve\ntasks zero-shot, after the problem has been lifted from"}, {"title": "2 BACKGROUND", "content": "A language-conditioned Markov Decision Process (MDP) (Liu et al., 2022) is a tuple M =\n(S, A, G, r, p, \u03b3, \u03bc), where S is the state space, A is the action space, G is the space of natural\nlanguage task specifications, r : S \u00d7 G \u2192 R is the reward function, p : S \u00d7 A \u2192 A(S) is the\ntransition function, \u03b3 \u2208 (0, 1] is the discount factor, \u03bc \u2208 \u0394(S) is the initial state distribution.\nA skill can be formalized through the concept of option (Sutton et al., 1999; Precup, 2000). A\ndeterministic Markovian option w \u2208 \u03a9 is a triple (\u0399\u03c9, \u03c0\u03c9, \u03b2\u03c9), where Iw : S \u2192 {0,1} is the\ninitiation function, determining whether the option can be initiated or not, \u03c0\u03c9 : S \u2192 \u0394(A) is the\nintra-option policy, and \u03b2\u03c9 : S \u2192 {0, 1} is the termination function, determining whether the option\nshould terminate or not. Under this mathematical framework, the skill design problem is equivalent\nto constructing a set of options \u03a9 that can be used by an agent. The goal of the agent is to provide a\npolicy over options \u03c0 : G \u00d7 S \u2192 \u03a9. Whenever the termination condition of an option is reached, \u03c0\nselects the next option to be executed, conditioned on the current state. The performance of such a\npolicy is defined by its expected return J(\u03c0) = \u0395\u03bc,\u03c0,\u03a9[\u03a3\u03c4=\u03bf\u03b3tr(st)]."}, {"title": "3 METHOD", "content": "MaestroMotif leverages AI-assisted skill design to perform zero-shot control, guided by natural\nlanguage prompts. To the best of our knowledge, it is the first method that, while only using language\nspecifications and unannotated data, is able to solve end-to-end complex tasks specified in language.\nIndeed, RL methods trained from scratch cannot typically handle tasks specified in language (Touati\net al., 2023), while LLM-based methods typically feature labor-intensive methodologies for learning\nlow-level control components (Ahn et al., 2022; Wang et al., 2024). MaestroMotif combines the\ncapability of RL from an LLM's feedback to train skills with an LLM's code generation ability which\nallows it to compose them at will. We first introduce MaestroMotif as a general method, describing\nits use for AI-assisted skill design and zero-shot control, then discussing its implementation."}, {"title": "3.1 AI-ASSISTED SKILL DESIGN WITH MAESTROMOTIF", "content": "MaestroMotif performs AI-assisted skill design in four phases shown in Figure 2. It leverages LLMs\nin two ways: first to generate preferences, then to generate code for initiation/termination functions\nand for a training-time policy over skills. It then uses these components to train skills via RL.\nAutomated Skills Reward Design In the first phase, an agent designer provides a description for\neach skill, based on their domain knowledge. Then, MaestroMotif employs Motif (Klissarov et al.,\n2024) to create reward functions specifying desired behaviors for each skill: it elicits preferences\nof an LLM on pairs of interactions sampled from a dataset D, forming for each skill a dataset of\nskill-related preferences Dw\u2081, and distilling those preferences into a skill-specific reward function r\u03c6\u03af."}, {"title": "3.2 ZERO-SHOT CONTROL WITH MAESTROMOTIF", "content": "After AI-assisted skill design, MaestroMotif has generated a set of skills, available to be combined.\nDuring deployment, a user can specify a task in natural language; MaestroMotif processes this"}, {"title": "3.3 MAESTROMOTIF ON NETHACK", "content": "We benchmark MaestroMotif on the NetHack Learning Environment (NLE) (K\u00fcttler et al., 2020).\nIn addition to being used in previous work on AI feedback, NetHack is a prime domain to study\nhierarchical methods, due to the fact that it is a long-horizon and complex open-ended system,\ncontaining a rich diversity of situations and entities, and requiring a vast array of strategies which\nneed to be combined for success. To instantiate our method, we mostly follow the setup of Motif\n(Klissarov et al., 2024), with some improvements and extensions. We now describe the main choices\nfor instantiating MaestroMotif on NetHack, reporting additional details in Appendix A.\nSkills definition Playing the role of agent designers, we choose and describe the following skills:\nthe Discoverer, the Descender, the Ascender, the Merchant and the Worshipper. The\nDiscoverer is tasked to explore each dungeon level, collect items and survive any encounters.\nThe Descender and Ascender are tasked to explore and specifically find staircases to either go\nup, or down, a dungeon level. The Merchant and the Worshipper are instructed to find specific\nentities in NetHack and interact with them depending on the context. These entities are shopkeepers\nfor the Merchant, such that it attempts to complete transactions, and altars for the Worshipper,\nwhere it may identify whether items are cursed or not. The motivation behind some of these skills\n(for example the Descender and Ascender pair) can be traced back to classic concepts such as\nbottleneck options (Iba, 1989; McGovern & Barto, 2001; Stolle & Precup, 2002).\nDatasets and LLM choice To generate a dataset of preferences Dw; for each one of the skills,\nwe mostly reproduce the protocol of Klissarov et al. (2024), and independently annotate pairs of\nobservations collected by a Motif baseline. Additionally, we use the Dungeons and Data dataset of\nunannotated human gameplays (Hambro et al., 2022b). We use Llama 3.1 70B (Dubey et al., 2024)\nvia vLLM (Kwon et al., 2023) as the LLM annotator, prompting it with the same basic mechanism\nemployed in Klissarov et al. (2024).\nAnnotation process In the instantiation of Motif presented in Klissarov et al. (2024), preferences\nare elicited from an LLM by considering a single piece of information provided by NetHack, the\nmessages. Although this was successful in deriving an intrinsic reward that was generally helpful\nto play NetHack, our initial experiments revealed that this information alone does not provide enough\ncontext to obtain a set of rewards that encode more specific preferences for each skill. For this reason,\nwe additionally include some of the player's statistics (i.e., dungeon level and experience\nlevel), as contained in the observations, when querying the LLM. Moreover, we leverage the idea\nproposed by Piterbarg et al. (2023a) of taking the difference between the current state and a state\npreviously seen in the trajectory, providing the difference between states 100 time steps apart as the\nrepresentation to the LLM. This provides a compressed history (i.e. a non-Markovian representation)\nto LLM and reward functions, while preventing excessively long contexts.\nCoding environment and Policy Over Skills A fundamental component of MaestroMotif is an\nLLM coder that generates Python code (Van Rossum & Drake Jr, 1995). MaestroMotif uses Llama 3.1\n405b to generate code that is executed in the Python interpreter to yield initiation and termination\nfunctions for the skills, the train-time policy over skills, and the policies over skills employed during\ndeployment. In practice, we find it beneficial to rely on an additional in-context code refinement\nprocedure to generate the policies over skills. This procedure uses the LLM to write and run unit tests\nand verify their results to improve the code defining a policy over skill (see Appendix A.2 for more\ndetails). In our implementation, a policy over skills defines a function that returns the index of the skill\nto be selected. For the training policy, the prompt given to the LLM consists of the list of skills and\na high-level description of an exploratory behavior of the type \u201calternate between the Ascender\nand the Descender; if you see a shopkeeper activate the Merchant...\", effectively transforming\nminimal domain knowledge to low-level information about a skill's desired state distributions.\""}, {"title": "4 EXPERIMENTS", "content": "We perform a detailed evaluation\nof the abilities of MaestroMotif\non the NLE and compare its per-\nformance to a variety of base-\nlines. Unlike most existing meth-\nods for the NLE, MaestroMotif\nis a zero-shot method, which pro-\nduces policies entirely through\nskill recomposition, without any\nadditional training. We empha-\nsize this in our evaluation, by\nfirst comparing MaestroMotif to\nother methods for behavior spec-\nification from language on a\nsuite of hard and composite tasks.\nThen, we compare the resulting\nagents with the ones trained for\nscore maximization, and further\nanalyze our method. We report\nall details related to the experi-\nmental setting in Appendix A.5.\nAll results are averaged across\nnine seeds (for MaestroMotif, three repetitions for skill training and three repetitions for software\npolicy generation), with error bars representing the standard error. All MaestroMotif results are\nobtained by recombining the skills without training, and the skills themselves were learned only\nthrough LLM feedback, without access to other types of reward signals."}, {"title": "4.1 PERFORMANCE EVALUATION", "content": "Baselines We measure the performance of MaestroMotif on the evaluation suite described above.\nFor MaestroMotif to generate a policy, it is sufficient for a user to specify a task description in natural\nlanguage. For this reason, we mainly compare MaestroMotif to methods that are instructable via\nlanguage: first, to using Llama as a policy via ReAct (Yao et al., 2022), which is an alternative\nzero-shot method; second, to methods that require task-specific training via RL, with reward functions\ngenerated by using either AI feedback or cosine similarity according to the embedding provided"}, {"title": "4.2 COMPARISON TO SCORE MAXIMIZATION", "content": "The vast majority of previous work on the NetHack Learning Environment has focused on agents\ntrained to maximize the score of the game (Sypetkowski & Sypetkowski, 2021; Piterbarg et al.,\n2023b; Wolczyk et al., 2024). Although the score might seem like a potentially rich evaluation signal,\nit has been observed by previous work that a high-performing agent in terms of its score does not"}, {"title": "4.3 ALGORITHM ANALYSIS", "content": "Having demonstrated the performance and adaptability of MaestroMotif, we now investigate the im-\npact of different choices on its normalized performance across task categories. Additional experiments\ncan be found in Appendix A.8.\nScaling behavior Central to the approach behind MaestroMotif is an LLM producing a policy\nover skills in code, re-composing a set of skills for zero-shot adaptation. It is known that the code\ngeneration abilities of an LLM depend on its scale (Dubey et al., 2024): therefore, one should expect\nthat the quality of the policy over skills generated by the LLM coder will be highly dependent on\nthe scale of the underlying model. We verify this in Figure 7, showing a clear trend of performance\nimprovement for large models. In Appendix A.4, we also investigate the impact of code refinement\non the performance of MaestroMotif.\nHierarchical architecture As illustrated in Figure 10 of\nAppendix A.7, the neural network used to execute the skill poli-\ncies follows almost exactly the same format as the PPO base-\nline (Miffyli, 2022), with the only difference of an additional\nconditioning via a one-hot vector representing the skill currently\nbeing executed. We found that this architectural choice to be\ncrucial for effectively learning skill policies. In Figure 8a, we\ncompare this choice to representing the skills through different\npolicy heads, as is sometimes done in the literature (Harb et al.,\n2017; Khetarpal et al., 2020). This alternative approach leads\nto a collapse in performance. We hypothesize that this effect\ncomes from gradient interference as the different skill policies\nare activated with different frequencies.\nEmergent skill curriculum In Figure 8a, we also verify the importance of learning all the skills\nsimultaneously. We compare this approach to learning each skill in a separate episode. We notice\nthat without the use of the training-time policy over skills, the resulting performance significantly\ndegrades. To better understand the reason behind this, we plot in Figure 8b and Figure 8c, for each\nskill, the corresponding reward during training. Learning each skill in isolation leads to a majority\nof the skills not maximizing their own rewards. On the other hand, learning multiple skills in the\nsame episode leaves space to learn and to leverage simpler skills, opening the possibility of using"}, {"title": "5 RELATED WORK", "content": "LLM-based hierarchical control methods Our method relates to a line of work which also uses\nLLMs to coordinate low-level skills in a hierarchical manner. SayCan and Palm-E (Ahn et al.,\n2022; Driess et al., 2023) also use an LLM to execute unstructured, natural language commands by\nrecomposing low-level skills in a zero-shot manner. A key difference in our work is how the skills\nare obtained: whereas they leverage a combination of large human teleoperation datasets of language-\nconditioned behaviors and hand-coded reward functions, we train skills from intrinsic rewards which\nare automatically synthesized from unstructured observational data and natural language descriptions.\nMaestroMotif is particularly related to those approaches in which a high-level policy is generated\nas a piece of code by an LLM (Liang et al., 2023). Voyager (Wang et al., 2024) also uses an LLM\nto hierarchically create and coordinate skills, but unlike our method, assumes access to control\nprimitives which handle low-level sensorimotor control. LLMs have also been used for planning in\nPDDL domains (Silver et al., 2023), see Appendix A.10 for a detailed discussion.\nHierarchical reinforcement learning There is a rich literature focusing on the discovery of skills\nthrough a variety of approaches, such as empowerment-based methods (Klyubin et al., 2008; Gregor\net al., 2017), spectral methods (Machado et al., 2017; Klissarov & Machado, 2023), and feudal\napproaches (Dayan & Hinton, 1993; Vezhnevets et al., 2017). Most of these methods are based on\nlearning a representation, which is then exploited by an algorithm for skill learning (Machado et al.,\n2023). In MaestroMotif, we instead work in the convenient space of natural language by leveraging\nLLMs, allowing us to build on key characteristics such as compositionality and interpretability. This\nabstract space also allows the possibility to define skills through high-level human intuition, a notion\nfor which it is very hard to define an formal objective. Interestingly, some of the skills we leverage in\nour NetHack implementation are directly connected to early ideas on learning skills, such as those\nbased on notions of bottleneck and in-betweeness (Iba, 1989; McGovern & Barto, 2001; Menache\net al., 2002; \u015eim\u015fek & Barto, 2004). Such intuitive notions had not been scaled yet as they are hard\nto measure in complex environments. This is precisely what the LLM feedback for skill training\nprovides in MaestroMotif: a bridge between abstract concepts and low-level sensorimotor execution.\nHRL approaches with code policies MaestroMotif is particularly related to approaches that\ncombine code to define policies over skills and RL to learn low-level policies, such as concurrent\nhierarchical Q-learning (Marthi et al., 2005), policy sketches (Andreas et al., 2017), and program-\nguided agents (Sun et al., 2020). MaestroMotif employs LLMs as generators of reward functions,\ntermination/initiation functions, and policies over skills, significantly simplifying the interaction\nbetween humans and the AI system which is used in existing hierarchical RL methods."}, {"title": "6 DISCUSSION", "content": "Modern foundation models possess remarkable natural language understanding and information\nprocessing abilities. Thus, even when they are not able to completely carry out a task on their own, they\ncan be effectively integrated into human-AI collaborative systems to bring the smoothness and efficacy\nof the design of agents to new heights. In this paper, we showed that MaestroMotif is an effective\napproach for AI-assisted skill design, allowing us to achieve untapped levels of controllability for\nsequential decision making in the challenging NetHack Environment. MaestroMotif takes advantage\nof easily provided information (i.e., a limited number of prompts) to simultaneously handle the\nhighest-level planning and the lowest-level sensorimotor control problems, linking them together by\nleveraging the best of the LLM and the RL worlds. In MaestroMotif, LLMs serve as pivotal elements,"}, {"title": "A APPENDIX", "content": "A.1 SKILL REWARDS\nWe now list and discuss the prompts used for eliciting preferences from the 70b parameters Llama 3.1\nmodel."}, {"title": "A.2 POLICY OVER SKILLS", "content": "Leveraging the semantic nature of the skill set of MaestroMotif, we use the coding abilities of LLMs\nto craft a reasonable strategy for their execution. For the high level code policy, we use the largest\nopen source model available, the 405b parameters Llama 3.1 model.\nWe use the template of Prompt 4 to obtain snippets of code that constitute the high-level policies\nfor different tasks. In it, we present the LLM with the set of skills, a high level definition of\neach of them and a desired strategy, all in natural language, which the LLM leverages to write its"}, {"title": "A.3 INITIATION AND TERMINATION", "content": "Finally, we leverage the coding abilities of the LLM to also define the termination and initiation\nfunctions of the skills. These quantities, together with the skill policies, define the option tuple\nfrom the options framework (see Section 2). The termination function indicates when a skill should\nfinish its execution and the initiation function when it can be selected by the high level policy. As\nthese functions are significantly simpler than the high level policy, we do not leverage the same\nself-refinement through unit tests. In Prompt 8, we present the prompt used to define the termination\nfunction and in Prompt 9 the one to define the initiation function."}, {"title": "A.4 CODE REFINEMENT", "content": "In Figure 9, we further compare the importance of leveraging code refinement through self-generated\nunit tests. We notice that this leads to improved results when using the 405b LLM, however no\nsignificant difference is observed for the smaller models."}, {"title": "A.5 ENVIRONMENT AND METHOD DETAILS", "content": "We base our implementation on the NetHack Learning Environment (K\u00fcttler et al., 2020) and Chaotic\nDwarven GPT-5 baseline (Miffyli, 2022), which itself was defined on the fast implementation of PPO\n(Schulman et al., 2017) within Sample Factory (Petrenko et al., 2020). As discussed in Klissarov\net al. (2024), although some actions are available to the agent (like the 'eat' action), it is not possible\nfor the agent to actually eat most of the items in the agent's inventory. This limitation is also true for\nother key actions such as the action for drinking, or the 'quaff' action in NetHack terms. To overcome\nthis limitation, we make a simple modification to the environment by letting the agent eat and quaff\nany of its items, at random, by performing a particular command (the action associated with the key\ny). We also include standard actions such as pray, cast and enhance. All agents that we train are\nevaluated using these same conditions, except the behaviour cloning based agents in Figure 5 which\nhave access to an even larger action set.\nFor the skill reward training phase of MaestroMotif, we use the message encoder from the Elliptical\nBonus baseline (Henaff et al., 2022). Similar to Klissarov et al. (2024), we train the intrinsic reward\nr with the following equation,"}, {"title": "A.6 BENCHMARK DESIGN AND MOTIVATION", "content": "We note that out of the original tasks from the NLE paper, the Staircase (and closely related\nPet) tasks have by now been solved (Zhang et al., 2021; Klissarov et al., 2024). The Score task\nis effectively unbounded, but as noted in (Wolczyk et al., 2024), it is possible to achieve very high\nscores by adopting behaviors which correlate poorly with making progress in the game of NetHack\n(for example, by staying at early levels and killing weak monsters). This is also an observation\ncorroborated by our experiments in Section 4.2.\nTo define a set of compelling and useful tasks in the NLE, we take inspiration from the NetHack\ncommunity, in particular, from the illustrated guide to NetHack Moult (2022). This guide describes\nvarious landmarks that every player will likely experience while making progress in the game. Some\nof these landmarks were also suggested in the original NLE release K\u00fcttler et al. (2020). The first\nsuch landmark is the Gnomish Mines which constitutes the first secondary branch originating\nin the main branch, the Dungeons of Doom (see Figure 4). The second landmark is Minetown, a\ndeeper level into the Gnomish Mines in which players might interact with Shopkeepers and gather\nitems. The third landmark is the Delphi, which is a level that appears somewhere between depth\n5 and 9 in the main branch and is the home to the Oracle, a famous character in the game. It is not\nnecessary to interact with the Oracle to solve the game of NetHack, but reaching the Delphi is a\nnecessary step towards it, which is the reason we include it and not the Oracle task.\nAs these tasks are navigation oriented, we additionally include a set of tasks that require the agent to\ninteract with entities found across the dungeons of NetHack. The interactions we select are chosen\nbecause they key to the success to any player playing the NetHack game. For this reason, we focus\non interactions that will give the agent more information about its inventory of items. In NetHack,\nmost items that are collected have only partially observable characteristics. For example, a ring"}, {"title": "A.7 HIERARCHICAL ARCHITECTURE", "content": null}, {"title": "A.8 ADDITIONAL ABLATIONS", "content": "Preference elicitation In Section 3.3, we have presented the ways in which the annotation process\nused in the NetHack implementation of MaestroMotif differs from the one presented in Klissarov\net al. (2024). In Figure 11, we verify how each of these choices affects the final performance of our\nalgorithm. The importance of providing the player statistics within the prompt eliciting preferences\nfrom the LLM is made apparent, as without such information the performance drops to almost 30%\nof its full potential. When the player statistics are provided but no information about how they differ\nfrom recent values (i.e. diffStats), the resulting performance is similarly decreased. This is explained\nby the non-Markovian nature of observations in NetHack: as an example, a status shown as hungry\ncould be the result of being previously satiated or fainting, which present two quite different\nways of behaving and would produce difference preferences. Finally, our preference elicitation phase\nintegrates episodes from the Dungeons and Data dataset (Hambro et al., 2022b), which provides\ngreater coverage of possible interactions and observations of NetHack. We notice that this choice is"}, {"title": "A.9 CONSIDERATIONS FOR THE SKILL SELECTION", "content": "In this work, we have leveraged an LLM to define a training-time high-level policies, termination\nand initiation functions in order to learn the skills. These components defining the skills selection\nstrategy were then fixed during the skill learning process. As we have seen in Section 4.3, this led to\nan emerging curriculum over skills, where easier skills developed first and harder skills developed\nlater on. However, we could see significant improvements in skill learning efficiency if the high-level\npolicies, termination and initiation functions were instead adapted online. This could be done, for\nexample, by deciding what skills to select and how to improve them (Kumar et al., 2024). Ideas from\nactive learning (Daniel et al., 2014; Mendez-Mendez et al., 2023) would be of particular value for\npursuing this research direction. Another consideration with respect to the high-level policy is its\nrobustness. Currently, before the high-level policy is deployed, it is verified through a self-generated\nunit test. This strategy was generally successful to avoid particular failure modes and obtain good\nstrategies. However, it is not a full-proof strategy, and adapting the high-level policy through online\ninteractions could be significantly more robust. One way to approach to adapt the high-level policy\nwould be to provide in-context execution traces from the environment through which the LLM\ncould iterate on a proposed strategy. Another approach would be through RL, for example through\nintra-option value learning (Sutton et al., 1999). We are then faced with the following question: what\nreward would this high level policy optimize? A possible answer would be to apply Motif to define\nsuch reward function on a per-task basis."}, {"title": "A.10 CONNECTIONS TO THE PLANNING LITERATURE", "content": "MaestroMotif learns skills through RL and, when faced with a particular task, re-composes them\nzero-shot through code that defines the execution strategy. To do so, the LLM writing the code needs\nto specify where skills can initiate, where they should terminate and how to select between them.\nMaestroMotif is in fact an instantiation of the options formalism (Sutton et al., 1999; Precup, 2000),\nwhich defined the necessary quantities for learning skills in RL. However, the idea to abstract behavior\nover time in the form of skills has a long history in AI, for example through STRIPS planning (Fikes\net al., 1993), macro-operators Iba (1989), Schemas Drescher (1991) and Planning Domain Definition\nLanguage (PDDL) (McDermott et al., 1998). The structure behind the option triple can also be seen\nin related fields, such as formal systems through the Hoare logic (Hoare, 1969). Silver et al. (2023)\nrecently investigate how LLMs can be used as generalized planners by writing programs in PDDL\ndomains, which is similar to how MaestroMotif write code to sequence skills. Their results show that\nLLMs are particularly strong planners. Another promising direction would be to use LLMs to convert\nnatural language into PDDL, to then leverage classical planning algorithms (Liu et al., 2023). Further\ninvestigating the connections between the options framework and symbolic representations would be"}, {"title": "A.11 ADDITIONAL PROMPTING EXPERIMENTS", "content": "We further verify the hypothesis that the hierarchical structure of the MaestroMotif algorithm is key\nto obtain performance. In Table 3, we present two additional baselines. LLM Policy (equivalent\nprompting) based the LLM Policy baseline but its prompt contains all the information that used\nwithin the different prompts of MaestroMotif. This includes skill descriptions, high-level descriptions\nof the task and also the generated code by the policy-over-skills that is used within MaestroMotif.\nWe also investigate Motif (equivalent prompting), which similarly builds on the Motif baseline\nbut provides all the prior knowledge given to MaestroMotif. Despite giving significantly more\ninformation to both baselines, the performance does not improve. Although additional information\nis provided, the burden on how and when to leverage this information, from context, makes it very\nchallenging."}]}