{"title": "SCALE-FREE GRAPH-LANGUAGE MODELS", "authors": ["Jianglin Lu", "Yixuan Liu", "Yitian Zhang", "Yun Fu"], "abstract": "Graph-language models (GLMs) have demonstrated great potential in graph-based\nsemi-supervised learning. A typical GLM consists of two key stages: graph gen-\neration and text embedding, which are usually implemented by inferring a latent\ngraph and finetuning a language model (LM), respectively. However, the former\noften relies on artificial assumptions about the underlying edge distribution, while\nthe latter requires extensive data annotations. To tackle these challenges, this pa-\nper introduces a novel GLM that integrates graph generation and text embedding\nwithin a unified framework. Specifically, for graph generation, we leverage an in-\nherent characteristic of real edge distribution\u2014the scale-free property\u2014as a struc-\ntural prior. We unexpectedly find that this natural property can be effectively ap-\nproximated by a simple k-nearest neighbor (KNN) graph. For text embedding,\nwe develop a graph-based pseudo-labeler that utilizes scale-free graphs to provide\ncomplementary supervision for improved LM finetuning. Extensive experiments\non representative datasets validate our findings on the scale-free structural approx-\nimation of KNN graphs and demonstrate the effectiveness of integrating graph\ngeneration and text embedding with a real structural prior. Our code is available\nat https://github.com/Jianglin954/SFGL.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, graph-language models (GLMs) have been widely explored in graph-based semi-\nsupervised classification on documents, especially for citation networks (Qin et al., 2023; Yu et al.,\n2025; Lu et al., 2023; He et al., 2024). When designing a GLM for classification, two key challenges\narise: graph generation\u2014how to generate a reasonable graph structure for the given documents, and\ntext embedding-how to encode the textual sequences into meaningful semantic features. To ad-\ndress these problems, various GLMs have been proposed, which can be broadly categorized into\nlatent graph inference (LGI) models and language-assisted graph (LAG) models.\nLGI models focus on graph generation and typically rely on feature engineering approaches, such\nas bag-of-words (Harris, 1954), TF-IDF (Aizawa, 2003), and skip-gram (Mikolov et al., 2013), to\nencode textual sequences into shallow representations. These representations serve as the foundation\nfor optimization objectives that jointly learn the underlying graph structure and node embeddings\nusing graph neural networks (GNNs) (Franceschi et al., 2019; Fatemi et al., 2021; Lu et al., 2023;\nKazi et al., 2023). However, LGI models typically rely on artificial assumptions about the underlying\nedge distribution, which may not accurately reflect the real graph structure.\nOn the other hand, LAG models assume a predefined graph structure and focus on enhancing text\nembedding through powerful language models (LMs), such as DeBERTa (He et al., 2021) and GPT\n(Ouyang et al., 2022). The core idea behind LAG models is to finetune LMs on the target datasets,\nthereby generating more informative text embeddings (Yu et al., 2025; Duan et al., 2023; He et al.,\n2024). However, finetuning a pretrained LM typically requires target datasets with extensive anno-\ntations (Brown et al., 2020), which is often impractical for semi-supervised learning tasks. When\nsufficient annotations are unavailable, LM finetuning may become unreliable.\nIn this paper, we propose a novel scale-free graph-language (SFGL) model that integrates graph\ngeneration and text embedding into a unified framework. The proposed SFGL framework addresses"}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 PROBLEM DEFINITION", "content": "Given a set of documents denoted as \\(D = {X_L, X_U, Y_L }\\), where \\(X_L = {x_i}_{i=1}^m\\) and \\(X_U = {x_i}_{i=1}^n\\) rep-\nresent the text sequences of \\(m\\) labeled and \\(n\\) unlabeled documents, respectively, and \\(Y_L = {y_i}_{i=1}^m\\)\nis the label set for the labeled samples, with \\(m < n\\) indicating a scarcity of labeled nodes compared\nto the abundance of unlabeled ones, we tackle a graph-based semi-supervised classification task that\ninvolves training a model on \\(D\\) to classify unlabeled documents \\(X_U\\). Here, we face two key prob-\nlems: graph generation\u2014how to generate a reasonable graph structure for the given documents, and\ntext embedding-how to encode the textual sequences into meaningful semantic features.\nFor graph generation, the main challenge lies in identifying a reasonable structural prior that charac-\nterizes the underlying distribution of edges. The choice of this prior is critical, as it determines both\nthe complexity of the learning model and the inductive bias introduced. We suggest that an effective\nstructural prior should exhibit two key characteristics:\nreal-it should reflect an inherent attribute of the data; and\nsimple-it should allow for straightforward graph genera-\ntion without requiring additional model training.\nFor text embedding, advanced LMs are preferred over tra-\nditional feature engineering approaches for their superior\nability to capture semantic information from textual se-\nquences of documents. The primary challenge here lies\nin improving LM finetuning in a semi-supervised setting\nwith very limited labeled samples. We suggest addressing\nthis challenge with a graph-based semi-supervised learning\nmodel that propagates supervised information from labeled\nto unlabeled samples through the graph structure. How-\never, this brings us back to the central question: how to\nidentify a reasonable structural prior for graph generation."}, {"title": "2.2 SCALE-FREE NETWORKS", "content": "We investigate a fundamental structural nature of real-world networks: the scale-free property\n(Barab\u00e1si & Albert, 1999; Barab\u00e1si & Bonabeau, 2003). In essence, real citation networks ex-\nhibit scale-free characteristics (Redner, 1998; Hummon & Dereian, 1989; Radicchi et al., 2011).\nTheir degree distribution \\(P(\\theta)\\) follows a power law: \\(P(\\theta) \\propto \\theta^{-\\alpha}\\), where \\(\\theta\\) denotes the degree of a\nnode and \\(\\alpha\\) is a scaling parameter of the distribution (Clauset et al., 2009). For illustration, Fig. 1\ndepicts the degree distribution of a scale-free network with \\(\\alpha = 2.8\\). As observed, the distribution is\nheavy-tailed, with a few highly connected nodes, known as hubs, and many small nodes with only\na few edges (Barab\u00e1si & Albert, 1999; Barab\u00e1si & P\u00f3sfai, 2016). We suggest that leveraging the\nscale-free property for graph generation is a strong choice, as it reflects an inherent characteristic\nof citation networks and aligns with our real principle. The challenge lies in constructing a latent\ngraph that can (approximately) form a scale-free network."}, {"title": "2.3 K-NEAREST NEIGHBOR GRAPHS", "content": "We begin by exploring KNN graphs, as their construction aligns with our simple principle. The\nconstruction process is straightforward: for each node, we identify its \\(k\\)-nearest neighbors using a\nspecified distance metric based on node features and establish an edge between the node and each\nof its \\(k\\) nearest neighbors. The degree distribution of KNN graphs is primarily shaped by three key\nfactors: \u2460 node distribution, which is unknown a priori; \u2461 the choice of similarity metric, which\naffects the probability of node connections; and \u2462 the value of \\(k\\), which directly determines node\ndegrees. Without detailed knowledge on these factors, characterizing the degree distribution of KNN\ngraphs remains challenging. We first explore simple cases and analyze each factor independently.\nIn the absence of prior knowledge about the underlying node distribution, we analyze attribute-\nfree nodes that are uniformly distributed across regular grid structures, including circles, hexagons,\nquadrilaterals, and triangles. As shown in Fig. 2, the subgraph formed by any node and its first-order\nneighbors exhibits characteristics of a random network (Barab\u00e1si & Albert, 1999). Within a specified\nvalue of \\(k\\), the connection probabilities between two nodes within the subgraph remain identical.\nThus, building KNN graphs on such uniform distributions yields consistent node connectivity.\nIn KNN graphs, various distance metrics can be used to measure similarity between nodes, including\nEuclidean, Manhattan, and Cosine distances. Euclidean distance is sensitive to variations in feature\nscale and highly influenced by outliers. Manhattan distance is more robust to outliers but disregards\nthe actual geometric relationships between data points. Cosine distance is particularly suitable for\nhigh-dimensional, sparse data, as it prioritizes vector direction over magnitude.\nThe value of \\(k\\) directly determines the magnitude of node degrees. As \\(k\\) increases, more edges are\nformed, leading to higher node degrees. In the extreme case where \\(k = m + n - 1\\), the KNN graph\nbecomes a complete graph, where every node is connected to all others and has the same degree."}, {"title": "3 SCALE-FREE GRAPH-LANGUAGE MODEL", "content": ""}, {"title": "3.1 SCALE-FREE GRAPH GENERATION", "content": "Hypothesis & Analysis. In this paper, we argue that constructing a KNN graph that approximates a\nscale-free network is practically feasible. To support this, we first present the following hypothesis:"}, {"title": "3.2 IMPROVED TEXT EMBEDDING", "content": "The previous section addresses the challenge in graph generation, as outlined in Sec. 2.1. This\nsection focuses on the challenge in text embedding, i.e., improving the finetuning of LMs.\nPseudo-Labeler. Leveraging the graphs generated in Sec. 3.1, we design a graph-based semi-\nsupervised learning model to produce high-quality pseudo-labels, which enhance LM training.\nGiven the node features \\(F\\), adjacency graph \\(A\\), and available labels \\(Y_L = {y_i}_{i=1}^m\\), we train a GNN\nusing the standard node classification loss:\n\\[L_{GNN} = \\sum_{i=1}^m y_i \\ln z_i,  \\quad z_i = \\text{softmax}(GNN_{\\Phi}(f_i, A)),\\]\nwhere \\(\\Phi\\) represents the GNN parameters. Here, any GNN model, such as graph convolutional\nnetworks (GCN) (Kipf & Welling, 2016), GraphSAGE (Hamilton et al., 2017), or graph attention\nnetworks (GAT) (Velickovic et al., 2017), can be employed (see Sec. 4.3 for experimental compar-isons). Once trained, the pseudo labels \\(\\hat{y}_j\\) for all unlabeled nodes (\\(j \\in {1, . . ., n}\\)) are generated by\n\\(\\hat{y}_j = \\text{argmax}(GNN_{\\Phi}(f_j, A))\\), enabling effective label propagation."}, {"title": "3.3 CLASSIFICATION", "content": "Up to this point, we have addressed the two key problems outlined in Sec. 2.1, obtaining semantic-enriched text embeddings \\(E = [e_1, e_2,\\ldots\\ldots, e_{m+n}]\\) and a meaningful graph \\(A\\). We can now tackle\nthe classification task by training a graph-based semi-supervised learning model on \\(E\\) and \\(A\\) using\nthe available labels \\(Y_L\\). Following (He et al., 2024), we utilize a GNN as the classification model.\nWe also evaluate the performance of the LMs with generated pseudo-labels. Notably, the proposed\nSFGL can use an iterative training mechanism. The improved text embeddings facilitate the gen-\neration of higher-quality pseudo-labels, which, in turn, further enhance the discriminability of the\nembeddings. We explore this iterative training strategy in Sec. 4.3. Due to space limitations, the\ndetailed training stages are summarized in Appendix A.5."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 EXPERIMENTAL SETTINGS", "content": "Datasets. We conduct extensive experiments on four citation networks: Cora (Sen et al., 2008),\nPubmed (Kipf & Welling, 2016), ogbn-arxiv (Hu et al., 2020), and arxiv23 (He et al., 2024)\n(see Appendix A.1 for more details.). Following standard practice, we utilize traditional text en-\ncoders to process textual attributes for graph generation. For Cora, each publication is represented\nusing a bag-of-words model (Harris, 1954) with a vocabulary of 1, 433 unique words. For Pubmed,\ndocuments are encoded as TF/IDF weighted word vectors (Aizawa, 2003) from a dictionary of 500\nunique words. For ogbn-arxiv and arxiv23, textual features are extracted using a skip-gram\nmodel (Mikolov et al., 2013) and a word2vec model (Mikolov et al., 2013), respectively. Additional\ncomparisons with other text encoders are provided in Sec. 4.3. In our experiments, the learning\nmodels have access to node attributes, while edge attributes remain unavailable.\nBaselines. We compare our approach against several state-of-the-art methods. \u2460 GCN (Kipf &\nWelling, 2016): trains a GCN using hand-crafted shallow features \\({f_i}_{i=1}^{m+n}\\) and labels \\({y_i}_{i=1}^m\\).\n\u2461 DEBERTA (He et al., 2021): finetunes a pretrained DeBERTa on raw text attributes \\({x_i}_{i=1}^m\\)\nand labels \\({y_i}_{i=1}^m\\). \u2462 GCN+DEBERTa (Zhao et al., 2022): trains a GCN using text embeddings\n\\({e_i}_{i=1}^{m+n}\\) and labels \\({y_i}_{i=1}^m\\), where \\({e_i}_{i=1}^{m+n}\\) are obtained by finetuning DeBERTa on \\({x_i, y_i}_{i=1}^m\\).\n\u2463DEBERTA+GPT (Ouyang et al., 2022): finetunes a DeBERTa using GPT-generated responses\n\\({r_i}_{i=1}^m\\) of labeled samples and labels \\({y_i}_{i=1}^m\\). \u2464 GCN+DEBERTa+GPT (He et al., 2024): trains a\nGCN using text embeddings \\({e_i}_{i=1}^{m+n}\\) and labels \\({y_i}_{i=1}^m\\), where \\({e_i}_{i=1}^{m+n}\\) are obtained by finetun-\ning a DeBERTa on \\({r_i, y_i}_{i=1}^m\\). \u2465 SFGL (ours): trains a GCN using text embeddings \\({e_i}_{i=1}^{m+n}\\) and\nlabels \\({y_i}_{i=1}^m\\), where \\({e_i}_{i=1}^{m+n}\\) are obtained by finetuning a DeBERTa with pseudo-labels \\({\\hat{y}_j}_{j=1}^n\\). \u2466\nSFGL+GPT (ours): similar to SFGL but uses GPT-generated responses to finetune the DeBERTa."}, {"title": "4.2 COMPARISON ANALYSIS", "content": "Table 1 presents the classification performance of various methods across different datasets and la-\nbeling rates. Our key observations are as follows: \u2460 Across all datasets, classification accuracy\ngenerally improves as the labeling rate increases. This aligns with expectations, as more labeled\ndata provides better supervision, enhancing model performance. \u2461 The baselines DEBERTa and\nDEBERTa+GPT perform poorly at low labeling rates, particularly on ogbn-arxiv and arxiv23,\nwhere finetuning LMs becomes unreliable under extreme label scarcity. This is because insufficient\nlabeled data fails to provide sufficient information for gradient updates, leading to the learning of\noverly narrow patterns, which impairs generalization. However, incorporating additional pseudo-\nlabels enables LMs to generate informative text embeddings, thereby enhancing performance. \u2462\nIncorporating GPT-generated responses significantly enhances both GCN and DeBERTa, particu-\nlarly on the Pubmed dataset. This highlights the potential of LLMs to extract and leverage rich\nsemantic information for text modeling. \u2463 In most cases, the proposed SFGL and SFGL+GPT\nachieve the highest performance, demonstrating their effectiveness in low-supervision scenarios."}, {"title": "4.3 ABLATION STUDY", "content": "Iterative Performance. As mentioned in Sec. 3.3, improving the GNNs allows us to generate more\naccurate pseudo-labels. These enhanced pseudo-labels, in turn, enable us to retrain the LMs, pro-"}, {"title": "5 RELATED WORK", "content": "Latent Graph Inference Models. Traditional LGI methods adopt linear projections to learn node\nrepresentations and optimize various objective functions to learn latent graphs. For example, Zhang\net al. (2010) design an entropy regularization to control the uniformity level of edge weights. Nie\net al. (2016) infer an optimal graph by assigning adaptive neighbors. Lu et al. (2018) impose spec-\ntral sparsity on the graph Laplacian matrix. Lu et al. (2021a) assume that two samples are likely to\nbelong to the same class if one sample is close to the reconstructed representation of the other. Ad-\nvanced LGI models exploit GNNs to learn the latent graphs. For instance, Franceschi et al. (2019)\nmodel a discrete probability distribution for the edges. Fatemi et al. (2021) provide supplementary\nsupervision for latent graphs through a self-supervision task. Lu et al. (2023) propose a model-\nagnostic model that obtains supplementary supervision directly from true labels. However, existing\nmethods typically rely on artificial assumptions about the underlying edge distribution. For example,\nZhang et al. (2010) impose a uniformity assumption on edge weights. Lu et al. (2018) introduce a\nblock diagonal prior to the graph Laplacian matrix. Lu et al. (2021b) construct an adaptive neighbor-\nhood graph by assuming the probability property of edge weights. Fatemi et al. (2021) assume that a\ngraph structure effective for predicting features is also effective for label prediction. These artificial\nassumptions may not accurately reflect real graph structures and require specific optimizations with\nadditional model training across the entire dataset.\nLanguage-Assisted Graph Models. LGI models typically rely on feature engineering approaches,\nsuch as skip-gram and TF-IDF, to encode textual sequences into feature vectors. In contrast, re-\ncent LAG models seek to enhance text embeddings by leveraging various LMs to extract richer\nsemantic features from text sequences (Li et al., 2024). For example, Zhao et al. (2022) design a\nvariational expectation-maximization framework to fuse graph structure and language learning for\nclassification. Duan et al. (2023) first conduct parameter-efficient finetuning on a pretrained LM\nand then generate text embeddings using the finetuned LM. He et al. (2024) leverages an LLM to\ncapture textual information and applies a small LM as the interpreter to transform the responses of\nthe LLM into informative features. Yu et al. (2025) use LLMs to generate nodes and edges for text-\nattributed graphs, which harnesses LLMs for enhancing class-level information. For most existing\nLAG models, finetuning an LM on the target dataset is essential to generate semantically enriched\ntext embeddings. However, it is notorious that finetuning a pretrained LM typically demands a large\namount of annotated data (Brown et al., 2020), which poses a significant challenge for LAG models\nin semi-supervised learning tasks, where available annotated data is often scarce."}, {"title": "6 CONCLUSION", "content": "In this paper, we identify two primary challenges in exiting graph-language models for semi-\nsupervised learning, i.e., artificial structural assumptions in graph generation and unreliable LM\nfinetuning for text embedding. We tackle these challenges by establishing a well-grounded struc-\ntural prior. Specifically, we examine the scale-free property of citation networks and reveal that this\nstructural characteristic can be effectively approximated using a simple KNN graph. Building on\nthis observation, we propose a novel graph-language model that employs a customized KNN algo-\nrithm for scale-free graph generation and utilizes a graph-based pseudo-labeler to provide additional\nsupervision for improved text embedding. Extensive experiments on representative datasets validate\nour findings on the scale-free structural approximation of KNN graphs and demonstrate the effec-\ntiveness of integrating graph generation and text embedding with a real structural prior. We hope\nthis study highlights the synergistic potential of GNNs and LMs, providing valuable insights for\nresearchers in both the GNN and NLP communities."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 DATASET DESCRIPTION", "content": "We conducted extensive experiments on four text-attributed graph benchmarks: Cora (Sen et al.,\n2008), PubMed (Kipf & Welling, 2016), ogbn-arxiv (Hu et al., 2020), and arxiv23 (He et al.,\n2024). The Cora dataset contains 2, 708 papers belonging to seven different classes. The PubMed\ndataset includes 19, 717 publications categorized into three classes. The ogbn-arxiv (Hu et al.,\n2020) dataset includes 169, 343 arXiv papers across 40 subject areas in computer science. Labels\nfor this dataset are manually annotated by the authors and arXiv. The arxiv23 (He et al., 2024)\ndataset includes all computer science papers submitted in 2023 or later on arXiv. Similar to ogbn-\narxiv, these papers are divided into 40 classes."}, {"title": "A.2 MODEL CONFIGURATION", "content": "We follow (He et al., 2024) for model configurations. For GNNs, we use a two-layer GCN (see Sec.\n4.3 for comparisons with other GNN architectures). The hyper-parameters for hidden dimension,\nlearning rate, dropout ratio, and weight decay are set to 128, 0.001, 0.5, and 0.0005, respectively.\nFor LMs, we finetune a pretrained DeBERTa (He et al., 2021) on the target datasets. The batch\nsize, learning rate, and dropout ratio are set to 20, 2 \u00d7 10^{-5}, and 0.3, respectively. We also em-\nploy GPT3.5 (Ouyang et al., 2022) for inference. We empirically set \\(k\\) to 25, 15, 25, 20 for Cora,\nPubmed, ogbn-arxiv, and arxiv23, respectively. We conduct five independent experiments\nwith different random seeds and report the average test accuracy along with the standard deviation.\nFor LLM prompt inputs, we follow established methods (He et al., 2024). Each prompt consists of a\npaper's title and abstract, along with a task-specific question designed to elicit a class prediction and\na corresponding explanation from the LLM. The generated textual responses by the LLMs serve as\ninputs for the LMs, which use them to generate text embeddings for all nodes. These embeddings\nare then further processed by GNNs to enhance model performance."}, {"title": "A.3 PROOF OF PROPOSITION", "content": "Proposition 1. For a scale-free network, assuming that there is at most one node \\(v\\) whose degree\nbelongs to \\([@_{max}, \\infty)\\), we have \\(@_{max} = @_{min}|V|^{\\frac{1}{\\alpha-1}}\\, where \\(@_{max}\\) and \\(@_{min}\\) refer to the maximum\nand minimal degree in the scale-free network.\nProof. We write the degree distribution \\(P(@)\\) of a scale free network as: \\(P(@) = \\xi @^{-\\alpha}\\), where \\(@\\)\nis the degree of a node, \\(\\xi\\) is a constant, and \\(\\alpha\\) is a scaling parameter of the distribution. Since\n\\(\\int @^{-\\alpha} d@ = \\frac{@^{1-\\alpha}}{1-\\alpha} + constant\\) and \\(\\int_{@_{min}}^{\\infty} P(@) d@ = 1\\), we have \\(\\xi = \\frac{1}{\\int_{@_{min}}^{\\infty} @^{-\\alpha} d@} = \\frac{1}{\\frac{@^{1-\\alpha}}{1-\\alpha}|^{\\infty}_{@_{min}}} = \\frac{1-\\alpha}{@_{min}^{1-\\alpha}}\\).\nSince \\(lim_{\\alpha \\rightarrow 0} \\frac{@^{1-\\alpha}}{1-\\alpha} = 0 (\\alpha > 1)\\), we have \\(\\xi = \\frac{1-\\alpha}{0-\\frac{@_{min}^{1-\\alpha}}{1-\\alpha}} = \\frac{\\alpha-1}{@_{min}^{1-\\alpha}}\\). As a result, we obtain \\(\\xi = (\n\\alpha - 1)@_{min}^{\\alpha-1}\\) and \\(P(@) = (\\alpha - 1)@_{min}^{\\alpha-1}@^{-\\alpha}\\). Assume that there is at most one node whose degree\nbelongs to \\([@_{max}, \\infty)\\), where \\(@_{max}\\) is the maximum degree in the network. We have \\(\\int_{@_{min}}^{@_{max}} P(@) d@ =\n\\frac{\\alpha-1}{@_{min}^{1-\\alpha}} \\int_{@_{min}}^{@_{max}} @^{-\\alpha} d@ = 1\\), \\(\\int_{@_{min}}^{@_{max}} @^{-\\alpha} d@ = @_{max} = (\\alpha - 1)@_{min}^{-\\alpha}\\) = \\(\\frac{1}{|V|}\\), where \\(|V|\\) is the number of nodes. After\nsimplification, we obtain \\((\\frac{@_{max}}{@_{min}})^{\\alpha-1} = |V|\\), which leads to \\(@_{max} = @_{min}|V|^{\\frac{1}{\\alpha-1}}\\).\n\nProposition 2. For a KNN graph, we have \\(\\sum_{v\\in V} CID(v) = \\sum_{v\\in V} CD(v) - \\sum_{v\\in V} COD(v) =\n2|E| - k|V|\\), where \\(|E| \\le k|V|\\).\nProof. In a KNN graph, we have \\(CD(v) = CID(v) + COD(v)\\), and \\(\\sum_{v\\in V} CD(v) = 2|E|\\). For\n\\(\\forall v \\in V\\), \\(COD(v) = k\\). Considering that two nodes may regard each other as its nearest neighbors,\nthe total number of edges is at most \\(k|V|\\), i.e., \\(|E| \\le k|V|\\). Based on the above points, we can derive:\n\\(\\sum_{v\\in V} CID(v) = \\sum_{v\\in V} CD(v) - \\sum_{v\\in V} COD(v) = 2|E| - k|V|\\), where \\(|E| \\le k|V|\\)."}, {"title": "A.4 ADDITIONAL EXPERIMENTAL RESULTS", "content": "As mentioned in Sec. 3.2, various GNNs can be used to implement the pseudo-labeler. To assess\ntheir impact, we evaluate model performance using different GNNs, including GCN, SAGE, and GAT.\nTable 5 presents the comparison results across these variants. Our findings indicate that the proposed\nmodels exhibit robustness to the choice of GNN, as they achieve similar performance improvements\nregardless of the specific architecture used.\nIn Sec. 4.1, our experiments employ traditional text encoders for graph construction and\npseudo-labeling. In fact, a variety of text encoders can be utilized for this task. To\nevaluate their impact, we experiment with several text encoders, including a traditional TF-\nIDF encoder (Enc1), a pretrained DeBERTa (Enc2), a pretrained E5-Mistral-7B-Instruct\n(Enc3) (Wang et al., 2024), a pretrained bge-en-icl (Enc4) (Li et al., 2025), and a fine-\ntuned DeBERTa (Enc5). Table 6 presents the comparison results on the PubMed dataset.\nAs observed, pretrained models such as De-BERTa, E5-Mistral-7B-Instruct, and bge-en-\nicl fail to achieve satisfactory results. In contrast, the fine-tuned DeBERTa model\ndemonstrates significant improvement, with\nTF-IDF performance falling between that of\nthe pretrained and finetuned models. How-\never, finetuning DeBERTa requires a suffi-\ncient number of labels, which is why we ini-\ntially use shallow text encoders to facilitate\npseudo-label generation. Notably, the two"}, {"title": "A.5 TRAINING STRATEGIES", "content": "The training algorithms of SFGL, SFGL+GPT, SFGL+GCN and SFGL+GCN are outlined in Algo-rithms 1 and 2. The primary difference between SFGL and SFGL+GPT lies in steps 5 and 7, where\nSFGL+GPT incorporates GPT responses to enhance text embeddings. Similarly, the distinction be-tween SFGL+GCN and SFGL+GCN is that the former employs a LM to generate text embeddings,\nwhile the latter further refines them using an LLM. Specifically, SFGL+GCN replaces the original\nnode textual sequences with GPT-generated responses for finetuning. All these models use a GCN\nas the final classifier."}, {"title": "A.6 LIMITATION", "content": "Due to the inherent differences in the formation mechanisms of KNN graphs and real-world net-works, constructing a fully exact scale-free KNN graph remains challenging. A potential solu-tion is to incorporate the scale-free property as a constraint within an optimization objective and\nthen optimize it to learn an exact latent graph, which presents an intriguing direction for future re-search. Furthermore, this work primarily focuses on citation networks. Extending our findings and\nmethodologies to other types of networks, such as social and biological networks, represents another\npromising avenue for future exploration."}]}