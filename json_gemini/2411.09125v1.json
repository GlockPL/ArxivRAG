{"title": "DROJ: A PROMPT-DRIVEN ATTACK AGAINST LARGE LANGUAGE MODELS", "authors": ["Leyang Hu", "Boran Wang"], "abstract": "Large Language Models (LLMs) have demonstrated exceptional capabilities across various natural language processing tasks. Due to their training on internet-sourced datasets, LLMs can sometimes generate objectionable content, necessitating extensive alignment with human feedback to avoid such outputs. Despite massive alignment efforts, LLMs remain susceptible to adversarial jailbreak attacks, which usually are manipulated prompts designed to circumvent safety mechanisms and elicit harmful responses. Here, we introduce a novel approach, Directed Rrepresentation Optimization Jailbreak (DROJ), which optimizes jailbreak prompts at the embedding level to shift the hidden representations of harmful queries towards directions that are more likely to elicit affirmative responses from the model. Our evaluations on LLaMA-2-7b-chat model show that DROJ achieves a 100% keyword-based Attack Success Rate (ASR), effectively preventing direct refusals. However, the model occasionally produces repetitive and non-informative responses. To mitigate this, we introduce a helpfulness system prompt that enhances the utility of the model's responses. Our code is available at https://github.com/Leon-Leyang/LLM-Safeguard.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) are powerful conversational systems that have demonstrated significant potential across numerous domains, including content generation, data analysis, and the health-care industry. Their remarkable performance is largely due to training on extensive text datasets, which enables them to generate high-quality responses on a wide range of topics. However, because these datasets are often sourced from internet materials that may contain inappropriate content, there is a risk of such content appearing in the model's outputs (Ousidhoum et al., 2021). To mitigate this risk, LLMs typically undergo extensive safety alignment through human feedback during development to prevent harmful or inappropriate responses (Anwar et al., 2024).\nDespite these safety measures, LLMs-such as the widely used ChatGPT-remain vulnerable to adversarial attacks. Upon its release, vulnerabilities were exploited using carefully crafted prompts that elicited responses capable of spreading misinformation, hate speech, and other harmful content, posing significant societal risks (Zvi, 2022). Recent research on enhancing the adversarial robustness of LLMs has largely focused on two approaches: (1) developing defense mechanisms to improve models' ability to detect and decline malicious queries (Xie et al., 2023; Alon & Kamfonas, 2023), and (2) designing new jailbreak attacks, which often involve specially optimized prefixes or suffixes added to malicious queries (Lapid et al., 2023; Wang et al., 2024; Wei et al., 2023).\nOne common safeguarding technique is the use of a safety prompt at the system level, which typically includes explicit instructions to ensure safe outputs and prevent harmful responses, as illustrated in Figure 1. This approach has been successfully implemented in various LLMs, including Mistral (Jiang et al., 2023) and GPT-4 (Achiam et al., 2023). Further research by Zheng et al. (2024) examined the effect of safety prompts on LLMs' representational space, revealing that both harmful"}, {"title": "BACKGROUND AND RELATED WORKS", "content": "LLM Alignment The purpose of safety alignment is to ensure that responses produced by LLMs are aligned with human values and are free from content that is objectionable or potentially harmful. Alignment is typically achieved using instruction tuning (Ouyang et al., 2022) and reinforcement learning with human feedback (RLHF) (Bai et al., 2022). Instruction tuning is a supervised learning method that fine-tunes LLMs using datasets composed of carefully curated prompt-response pairs, often including contextual information related to the prompt. This method enhances the relevance and informativeness of responses across a variety of input queries while safeguarding against the generation of harmful content (Wei et al., 2021). In comparison, RLHF formulates the alignment task as a reinforcement learning problem. It involves training a reward model to convert human preferences regarding model outputs into reward signals, which are then used to guide LLMs in learning policies that favor responses more closely aligned with human preferences, thus refining outputs via fine-tuning (Christiano et al., 2017). Despite these advancements, LLMs aligned using these methods are still susceptible to various jailbreak attacks, which we explore in detail below.\nJailbreak Attacks against LLMs Jailbreak attacks consist of specially crafted input strings designed to circumvent LLM defense mechanisms and elicit harmful responses that well-aligned models should not produce. These attacks can be further categorized into two types based on the at-"}, {"title": "METHOD", "content": ""}, {"title": "ANCHORING PROCESS", "content": "To determine the direction of refusal within the model's hidden space, we employ the anchoring method outlined by Zheng et al. (2024). We begin by inputting N queries, containing both harmful and harmless examples, into the LLM. We then capture the hidden state from the top layer of the LLM after processing the last token of each query, denoted as x \u2208 Rn.\nFollowing this, we anchor a lower m-dimensional space using Principal Component Analysis (PCA), and project the hidden states x of the queries into this low-dimensional space according to the transformation defined by:\np : Rn \u2192 Rm, p(xi) = VT (xi \u2212 a), for i = 1, 2, ..., N\nHere, V \u2208 Rn\u00d7m represents the matrix of the first m principal components, and a \u2208 Rn is the mean vector of the queries' representations.\nWe then fit a logistic regression model to predict each query's empirical refusal rate, denoted by y \u2208 {0,1}, based on its lower-dimensional representation p(x). We use the Binary Cross-Entropy (BCE) loss for this purpose, as formulated in the following equations:"}, {"title": null, "content": "arg min\u03a61N\u2211i=1N[yilogf\u03a6(xi)+(1\u2212yi)log(1\u2212f\u03a6(xi))]"}, {"title": null, "content": "f\u03a6 : Rm \u2192 R, f(xi) = wTp(xi) + b\u00f8, for i = 1, 2, . . ., N"}, {"title": "OPTIMIZATION PROCESS", "content": "After determining the refusal direction, we optimize the jailbreak prompt in the embedding space to minimize the predicted refusal rate for the query prefixed with the jailbreak prompt. This optimization is achieved by minimizing the loss La, defined as follows:"}, {"title": null, "content": "La(\u03b8)=1N\u2211i=1Nlog(\u03c3[f\u0398(x\u03b8,i)\u2212f\u0398(x\u02c6\u03b8,i)])"}, {"title": "REGULARIZATION", "content": "Direct manipulation of the hidden representation can significantly alter the meaning of the query, potentially causing the LLM to generate answers that do not align with the original intent. Such misalignment can impede an effective jailbreak. To address this issue, we adopt the regularization approach used by Zheng et al. (2024) during our optimization process.\nThe loss La, which we aim to minimize, focuses only on the m-dimensional subspace of the hidden representation. To regularize changes in the remaining n \u2212 m dimensions and preserve the meaning of the original query as much as possible, we consider the full change in the hidden dimension ||x\u0398,i \u2212 x\u02c6\u03b8,i||2. This change can be expressed using an orthogonal matrix Q = [V, U], where V \u2208 Rn\u00d7m is the matrix of the first m principal components obtained by PCA, and U \u2208 Rn\u00d7(n\u2212m) is a matrix containing n \u2212 m column vectors that are orthogonal to V, which can be computed using the Gram-Schmidt algorithm.\nBy the norm-preserving property of orthogonal transformations, we have:"}, {"title": null, "content": "||x\u03b8,i\u2212x\u02c6\u03b8,i||2=||QT(x\u03b8,i\u2212x\u02c6\u03b8,i)||2\n=||VT(x\u03b8,i\u2212x\u02c6\u03b8,i)||2+||UT(x\u03b8,i\u2212x\u02c6\u03b8,i)||2\n=||p(x\u03b8,i)\u2212p(x\u02c6\u03b8,i)||2+||UT(x\u03b8,i\u2212x\u02c6\u03b8,i)||2."}, {"title": null, "content": "Lr(\u03b8)=1n\u00d7N\u2211i=1N||UT(x\u03b8,i\u2212x\u02c6\u03b8,i)||2"}, {"title": null, "content": "argmin\u0398L(\u0398)=argmin\u0398[L\u03b1(\u0398)+\u03b2Lr(\u0398)]"}, {"title": "EXPERIMENTS", "content": ""}, {"title": "DATA, MODEL AND BASELINES", "content": "Training Data We utilize the dataset used by Zheng et al. (2024), which consists of 100 harmful and 100 harmless queries. Each query is presented in four variants:\n\u2022 The original query without any safety prompt,\n\u2022 The query prefixed with the official LLaMA-2 safety prompt,\n\u2022 The query prefixed with a shortened version of the LLaMA-2 prompt,\n\u2022 The query prefixed with the official Mistral safety prompt.\nThis setup results in a total of (100 + 100) \u00d7 4 = 800 training samples.\nTesting Data For evaluation, we use the Harmful Behaviors subset of the AdvBench dataset by Zou et al. (2023), containing 520 harmful queries. This subset is a benchmark commonly used to evaluate jailbreak methods on LLMs. Additionally, we use the MaliciousInstruct dataset by Huang et al. (2023), which includes 100 harmful queries.\nTo demonstrate the consistency of the PCA transformation across different datasets, we also visualize 100 held-out harmless queries provided by Zheng et al. (2024). These queries are not used to compute performance metrics but to show that the lower-dimensional representation of the testing set aligns with that of the training set.\nModel Since DROJ is a white-box attack method, open-source models are used for our experiments. Based on the performance of eight open-source models on the harmful queries of the training dataset (Zheng et al., 2024), we select LLaMA-2-7b-chat, which demonstrates the highest refusal rate to harmful queries, suggesting that it is the most challenging model among them to successfully jailbreak.\nBaselines We compare our method against several baseline settings:\n1. No Jailbreak Prompts: Queries are processed without any jailbreak prompts.\n2. GCG Jailbreak Prompts: Queries prefixed with jailbreak prompts trained using the GCG method (Zou et al., 2023).\n3. AutoDAN Jailbreak Prompts: Queries prefixed with jailbreak prompts trained using AutoDAN (Liu et al., 2023).\nFurther, we explore the impact of combining our optimized jailbreak prompt with a helpfulness prompt, detailed in \u00a74.4."}, {"title": "TRAINING RESULT", "content": "Using the above settings, we have n = 4096, N = 800 and set m = 4 following Zheng et al. (2024). We initialize our jailbreak prompt @ randomly in the embedding space with a length of 20. The optimization process runs for 100 epochs with a batch size of 50 and a learning rate of 1 \u00d7 10-3. The regularization term \u03b2 is set to 1 \u00d7 10\u22123. The final values are La = 14.74 and Lr = 1.99.\nTo verify that the optimization behaves as expected, we visualize the hidden representations of queries in the first two principal components. As shown in the left panel of Figure 3, both harmful and harmless queries are moved away from the direction of refusal, confirming the efficacy of the optimization. Moreover, harmful queries are less likely to be refused after the addition of the jailbreak prompt, aligning with our expectations."}, {"title": "TESTING RESULT", "content": "To assess the effectiveness of our optimized jailbreak prompt, we use the keyword-based Attack Success Rate (ASR) as the metric (Zou et al., 2023). An attack is categorized as successful if the LLM's output does not contain certain keywords, such as \"I'm sorry\" or \"I cannot\", typically occurring in refusal responses. We adhere to the same keywords as those used by Liu et al. (2023) to maintain consistency. When obtaining the results for DROJ and the No Jailbreak baseline, we set the generation configuration hyperparameters as follows:\n\u2022 temperature = 1\n\u2022 top_p = 0.9\n\u2022 number of returned sequences = 25\nWe compute the Attack Success Rate (ASR) based on the most harmful response for each query. All results are reported for LLaMA-2-7b-chat without a safety prompt, excluding those obtained with the helpfulness system prompt.\nAdvBench Dataset On the AdvBench dataset, we compare the performance of DROJ against three baselines. For GCG and AutoDAN, we rely on the results reported by Liu et al. (2023), which are trained on AdvBench directly.\nThe results, shown in Table 1, indicate that DROJ significantly outperforms other methods even when not directly trained on AdvBench, achieving a perfect ASR of 1.0000. In contrast, the No Jailbreak setting achieves only 0.0500, highlighting our method's effectiveness.\nMaliciousInstruct Dataset For the MaliciousInstruct dataset, since statistics for GCG and AutoDAN are not available here, we present a partial comparison. As shown in Table 2, DROJ again achieves an ASR of 1.0000, while the No Jailbreak method fails almost universally, with an ASR of only 0.0001.\nVisual Analysis To further validate our findings, we visualize the hidden representations of the queries in the first two principal components, as shown in the right panel of Figure 3. Our visualization confirms that DROJ successfully transfers from the training to the testing dataset, moving both"}, {"title": "EFFECT OF ADDING HELPFULNESS PROMPT", "content": "While DROJ is able to achieve perfect ASR scores, a closer manual inspection of the model's responses reveals a critical limitation as shown in the left panel of Figure 4: the LLM often fails to provide genuinely informative responses, frequently repeating the question without providing a relevant answer."}, {"title": "INTERPRETABILITY ANALYSIS", "content": "To better understand the meaning behind the optimized jailbreak prompt, we analyze its embedding by comparing it with embeddings of actual tokens in LLaMA-2-7b-chat. Specifically, we identify the closest real token to each token in the soft prompt using two metrics: (1) Euclidean distance, and (2) cosine similarity in the embedding space. The results are as follows, which appear to be largely uninterpretable:\n\u2022 Based on Euclidean distance:\n<0x8D> Claudeias Odkazy\u00fcr beiden refactorzia\u0142 Cat properties absent FPdg neckharm likely rendering proxim ogni wand\n\u2022 Based on cosine similarity:\nVien Claude\u00f3r instruction\u00fcr beiden refactorzia\u0142 Cat Sint AlabamaFPdg neckharm likely rendering proxim ogni wand"}, {"title": "DISCUSSION & CONCLUSION", "content": "In this paper, we explore the concept of jailbreaking large language models (LLMs) by optimizing a jailbreak prompt at the embedding level, aiming to reduce the likelihood of these models refusing to respond. Our proposed method, DROJ, effectively prevents direct refusals by LLMs and achieves a keyword-based Attack Success Rate (ASR) of 100%. However, we observe that while the model may not refuse directly, it can still output repetitive and uninformative responses. To address this issue, we introduce a helpfulness system prompt, which improves the utility of the model's responses.\nDespite these advances, our research has several limitations. First, due to time and resource constraints, we have not independently reproduced the GCG and AutoDAN methods. In the future, we plan to replicate these methods to enable a more equitable comparison. Second, the addition of the helpfulness prompt is a post hoc solution. A potential future direction is to initiate our optimization process from a helpfulness prompt to see if this approach can better elicit useful responses from LLMs. Finally, our method is not currently transferable across different models. To mitigate this restriction, we might consider using alternative optimization methods, such as Greedy Coordinate Gradient-based Search (Zou et al., 2023), focusing on the token space rather than the embedding space, to enhance transferability of DROJ to other models."}, {"title": "ETHICS STATEMENT", "content": "The adversarial attack method introduced in this paper, DROJ, is intended for research purposes only. While there is potential for misuse by malicious users to elicit harmful responses from large language models (LLMs), we believe our work will provide valuable insights for enhancing LLM safety. This research follows in the footsteps of prior work on model jailbreaking, aiming to develop better defense mechanisms that make LLMs more reliable, robust, and safe for societal benefit.\nWe would also like to note that since DROJ operates within the white-box attack paradigm, we used only open-source models fine-tuned from unaligned models in our evaluations (e.g., LLaMA-2-7b-chat is fine-tuned from LLaMA-2-7b). Therefore, harmful responses could be generated directly through prompt engineering targeting these unaligned base models, even without the assistance of DROJ."}]}