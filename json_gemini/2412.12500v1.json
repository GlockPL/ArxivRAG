{"title": "Beyond Data Quantity:\nKey Factors Driving Performance in Multilingual Language Models", "authors": ["Sina Bagheri Nezhad", "Ameeta Agrawal", "Rhitabrat Pokharel"], "abstract": "Multilingual language models (MLLMs) are\ncrucial for handling text across various lan-\nguages, yet they often show performance dis-\nparities due to differences in resource avail-\nability and linguistic characteristics. While the\nimpact of pre-train data percentage and model\nsize on performance is well-known, our study\nreveals additional critical factors that signifi-\ncantly influence MLLM effectiveness. Ana-\nlyzing a wide range of features, including ge-\nographical, linguistic, and resource-related as-\npects, we focus on the SIB-200 dataset for clas-\nsification and the Flores-200 dataset for ma-\nchine translation, using regression models and\nSHAP values across 204 languages. Our find-\nings identify token similarity and country sim-\nilarity as pivotal factors, alongside pre-train\ndata and model size, in enhancing model per-\nformance. Token similarity facilitates cross-\nlingual transfer, while country similarity high-\nlights the importance of shared cultural and lin-\nguistic contexts. These insights offer valuable\nguidance for developing more equitable and\neffective multilingual language models, partic-\nlarly for underrepresented languages.", "sections": [{"title": "Introduction", "content": "Multilingual language models have garnered sig-\nnificant attention due to their ability to handle and\ngenerate text across various languages, playing a\ncrucial role in tasks such as machine translation,\ncross-lingual information retrieval, and multilin-\ngual content creation. However, achieving fair and\neffective performance across languages with di-\nverse linguistic characteristics and varying resource\navailability remains a formidable challenge.\nPrior research has identified several features that\ninfluence the performance of multilingual language\nmodels (Zhong et al., 2024; Bagheri Nezhad and\nAgrawal, 2024; Zhu et al., 2024; Chau and Smith,\n2021). Although many factors are widely acknowl-\nedged to impact model performance, potentially\neven in a manner similar to the butterfly effect,\nthese studies have often focused on a limited set of\nfeatures. In contrast, our work aims to conduct a\ncomprehensive analysis to systematically explore\nand quantify the effects of a broader range of fea-\ntures. Specifically, we examine 12 distinct features\nrelated to both the models and the languages they\nare designed to process.\nIn this study, we analyze the performance of\nmultilingual language models (Bloom, XGLM and\nBloomZ in different sizes) in 204 languages, using\nboth classification (SIB-200 dataset (Adelani et al.,\n2024)) and generation (Flores-200 dataset (NLLB\net al., 2022)) tasks. We evaluate these models in\nzero-shot and two-shot learning settings, consid-\nering 14 different model configurations and sizes.\nOur experiments involve over 2.3 million instances,\nproviding a robust basis for our analysis.\u00b9 Figure 1\nshows the overview of the analysis.\nThe primary contributions of this paper are as\nfollows:\n\u2022 Comprehensive Feature Analysis: We in-\nvestigate the impact of 12 distinct features,\nencompassing model-specific attributes (e.g.,\nmodel size, pre-train data percentage) and\nlanguage-specific attributes (e.g., script type,\ngeographical proximity), to understand their\ninfluence on model performance across a di-\nverse set of languages.\n\u2022 Evaluation Across Tasks and Configura-\ntions: Our study spans both classification and\ngeneration tasks, assessed in zero-shot and\ntwo-shot learning settings. We consider mul-\ntiple model architectures and sizes, offering\ninsights into how different configurations af-\nfect multilingual model performance."}, {"title": "Related Work", "content": "The development and evaluation of multilingual\nlanguage models have been widely studied, with\nmodels like mBERT, XLM-R, Bloom, XGLM, and\nLlama 3.1 demonstrating their capability to handle\nmultiple languages with varying resource levels ef-\nfectively (Devlin et al., 2019; Conneau et al., 2020;\nBigScience et al., 2023; Lin et al., 2022; Dubey\net al., 2024). Despite these advancements, achiev-\ning fair performance across diverse languages re-\nmains challenging.\nRecent efforts, such as the Glot500 project and\nthe BigTranslate project, have focused on expand-\ning multilingual corpora and enhancing transla-\ntion capabilities, emphasizing the need for inclu-\nsive benchmarks and tailored training approaches\n(Imani et al., 2023; Yang et al., 2023). Addi-\ntionally, studies have explored key factors driv-\ning multilingual model performance, highlight-"}, {"title": "Methodology", "content": "In this section, we detail the datasets used, the\nmodels evaluated, the features extracted, and the\nevaluation methods employed in our study."}, {"title": "Dataset Description", "content": "We used two datasets in our experiments: SIB-200\nfor classification tasks and Flores-200 for genera-\ntion tasks.\nFlores-200 Dataset Flores-200 is a multi-way\nparallel corpus with sentences translated into over\n200 languages, widely used to benchmark machine\ntranslation and multilingual models. It highlights\nperformance gaps between high- and low-resource\nlanguages, promoting inclusive evaluations (NLLB"}, {"title": "Model Configuration", "content": "We conducted a direct evaluation of three multilin-\ngual models: Bloom, BloomZ, and XGLM, each\ntested across various sizes. Although newer mul-\ntilingual models, such as Llama 3.1 (Dubey et al.,\n2024), are now available, we selected these mod-\nels because they were trained on a wide range of\nlanguages, are represented in different model sizes,\nand have accessible training dataset statistics. This\nmakes them ideal for our comprehensive analysis\nof multilingual language model performance.\nBloom is a large language model developed\nby the BigScience collaboration, trained on the\nROOTS corpus and capable of generating text in 46\nnatural languages and 13 programming languages.\nFor our experiments, we used five sizes of Bloom,\nranging from 560 million to 7.1 billion parameters\n(BigScience et al., 2023).\nBloomZ is a fine-tuned variant of Bloom, opti-\nmized with multitask prompts to improve perfor-\nmance on specific tasks. We evaluated the same\nsizes as Bloom, ensuring consistency in compar-\nisons (Muennighoff et al., 2023).\nXGLM is another multilingual model trained\non 30 natural languages. The four sizes tested\nfor XGLM ranged from 564 million to 7.5 billion\nparameters (NLLB et al., 2022)."}, {"title": "Features", "content": "We extracted a variety of features to analyze their\nimpact on model performance. These features en-\ncompass geographical, linguistic, token similarity,\nand training-related aspects, including a total of 12\nfeatures drawn from both model characteristics and\nlanguage-specific attributes."}, {"title": "Model features", "content": "In our analysis, we considered several key fea-\ntures related to the language models themselves, in-\ncluding model size, the distribution of pre-training\ndata, and Instruction tuning data (specifically for\nBloomZ).\n1. Model size refers to the number of parameters,\nimpacting the model's learning capacity. We\nexamined models of various sizes to see how\ncapacity affects multilingual performance.\n2. Pre-training data represents the language dis-\ntribution in the initial training data, helping\nassess its impact on cross-language general-\nization.\n3. Instruction tuning data involves additional\ndatasets for refining models on instruction-\nbased tasks, particularly in BloomZ."}, {"title": "Language features", "content": "To examine the impact of geography and culture on\nlanguage models, we analyze two distinct features:\ngeographical proximity and country similarity.\n4. Geographical proximity represents the phys-\nical distance between languages, derived from\nlatitude and longitude data from Glottolog\n(Hammarstr\u00f6m et al., 2024). This feature, re-\nduced with Multi-Dimensional Scaling (MDS)\n(Kruskal, 1964), captures linguistic traits in-\nfluenced by regional contact, such as phonetic\nor lexical similarities arising from shared land-\nscapes or historical migrations.\n5. Country similarity, in contrast, captures so-\nciopolitical and cultural overlap by identifying\nthe countries where each language is spoken\n(also sourced from Glottolog (Hammarstr\u00f6m\net al., 2024)). Using a Jaccard similarity\nmatrix, reduced with MDS, this feature em-\nphasizes shared cultural and linguistic traits,\neven among geographically distant languages\nthat coexist within similar cultural or political\nspheres.\nLinguistic features were extracted by consider-\ning both the language family and the script used\nfor each language.\n6. Language family for each language was ob-\ntained from Ethnologue including their ge-\netic classifications (Eberhard et al., 2024).\n7. Script type refers to the specific writing\nsystem used by a language, identified by\nISO 15924 codes (for Standardization, 2022),\nwhich categorize scripts based on their visual\nand structural characteristics. This informa-\ntion was directly available in the datasets we\nused."}, {"title": "Feature Analysis", "content": "To evaluate multilingual language model perfor-\nmance, we conducted a comprehensive analysis\nacross classification and translation tasks, testing\neach of the 14 models in zero-shot and two-shot in-\ncontext learning settings (Brown et al., 2020). This\ndual-task evaluation enabled us to assess model per-\nformance across different languages and learning\nscenarios, providing insights into their effective-\nness in handling multilingual data.\nFor the classification task, we used the SIB-200\ndataset, calculating F1 scores based on model out-\nputs compared to ground truth for each language.\nFor the generation task, we translated from var-\nious languages to English using the Flores-200\ndataset, assessing accuracy with sacreBLEU scores\nagainst reference translations (Post, 2018).\nTo better understand the factors influencing\nmodel performance and to quantify the relation-\nships between input features and performance met-\nrics (F1 and sacreBLEU scores), we applied ten\nregression models: Linear Regression (Galton,\n1886), Random Forest (Breiman, 2001), Decision\nTree (Quinlan, 1986), Support Vector Regression\n(SVR) (Vapnik et al., 1995), Gradient Boosting\n(Friedman, 2001), XGBoost (Chen and Guestrin,\n2016), K-Nearest Neighbors (Fix and Hodges,\n1989), Lasso (Tibshirani, 1996), Ridge (Hoerl and\nKennard, 1970), and Elastic Net (Zou and Hastie,\n2005).\nWe split the data into an 80-20 training-test split\nand assessed each model's performance using R-\nsquared (R2) and Mean Squared Error (MSE), pro-\nviding a robust evaluation of predictive accuracy\nacross different language and model configurations.\nTo further understand the impact of each feature\non model performance, we utilized SHAP (SHap-\nley Additive exPlanations) values, which offer a\nunified measure of feature importance for each pre-\ndiction (Lundberg and Lee, 2017). We focused\non models that demonstrated strongest predictive\nperformance for each task, and analyzed both in-\ndividual and aggregated (abstract) features to gain\ninsights into broader categories like geographical,\nlinguistic, and token similarity. This analysis pro-\nvided a deeper understanding of how these features\ncontribute to overall model performance."}, {"title": "Token similarity", "content": "Both language family and script are categorical\nvariables. To include these categorical variables in\nour regression models, we applied one-hot encod-\ning.\nAlthough script type is an important factor in our\nanalysis, token similarity provides a more granular\nview of linguistic overlap at the lexical level, which\nis crucial for understanding how languages may\ninfluence one another in a multilingual model.\n8. Token similarity, measuring vocabulary over-\nlap between languages, offers insight into lin-\nguistic similarity. We tokenized the SIB-200\ntrain-set using model-specific tokenizers and\ncalculated Jaccard similarity between token\nsets. This similarity matrix was then reduced\nto ten features using MDS.\nAdditionally, we included Socio-Linguistic and\nDigital Support Features, which offer insights into\nthe demographic, vitality, and digital presence of\nlanguages. These ordinal features \u2013 population, lan-\nguage vitality, digital support, and resource level \u2013\nwere numerically encoded to preserve their ordinal\nnature for regression analysis.\n9. Population data, sourced from Ethnologue,\ncategorizes the number of speakers for each\nlanguage into ranges like \u201810K to 1 million',\n'1 million to 1 billion', and '1 billion plus'\n(Eberhard et al., 2024).\n10. Language Vitality is categorized by Ethno-\nlogue into 'Institutional', 'Stable', 'Endan-\ngered', and 'Extinct', reflecting the language's\ncommunity support and risk of endangerment\nor extinction (International, 2019).\n11. Digital Language Support assesses a lan-\nguage's digital presence, including content,\nlocalization tools, and resources. Ethnologue\ncategorizes this support from 'Still' (no dig-\nital presence) to \u2018Thriving' (comprehensive\ndigital ecosystem) (Eberhard, 2019).\n12. Resource Level refers to the availability of\nlinguistic resources like dictionaries and gram-\nmars for each language. Joshi et al. (2020)\nclassify languages into six levels, from those\nwith minimal resources (Class 0) to those with\nextensive support (Class 5), reflecting varying\nlevels of resource availability and digital ad-\nvancement potential."}, {"title": "Regression Model Predictions", "content": "This section explores factors influencing multilin-\ngual model performance by addressing three ques-\ntions. First, we assess which regression models\nbest predict performance, using R-squared (R2)\nand Mean Squared Error (MSE) for F1 and sacre-\nBLEU scores. Next, we identify key features driv-\ning model success. Finally, we examine how fac-\ntors like geographical proximity, socio-linguistic\ncontext, and resource availability affect prediction\naccuracy, providing a comprehensive view of ele-\nments shaping model effectiveness.\nFurthermore, the very low Mean Squared Error\n(MSE) values achieved by the best-performing re-\ngression models indicate that the features analyzed\nin this study are comprehensive and highly pre-\ndictive of the model behavior. This low error rate\nsuggests that there are no significant additional fea-\ntures with a high impact on model performance that\nwere left out of the analysis. The completeness of\nthe set of features implies that we have effectively\ncaptured the key factors driving the performance\nof multilingual language models, thus providing a\nrobust framework for understanding and predicting\ntheir behavior."}, {"title": "Feature Importance Analysis", "content": "To quantify the contribution of each feature to\nthe performance of multilingual language models,\nwe employed SHAP values, a powerful method\nfor explaining individual predictions by measuring\nthe marginal contribution of each feature, making\nit particularly suitable for complex models with\nnon-linear interactions. In our analysis, SHAP\nvalues were used to rank the importance of var-\nious features, providing insights into which factors\nhad the most significant impact on model perfor-\nmance across both classification and translation\ntasks. This method allowed us to understand the\nunderlying drivers of performance disparities in\nmultilingual models.\nIn both classification and generation tasks, as\nillustrated in Figures 2 and 3, key features such\nas Token Similarity, Model Size, Pre-train Data\nPercentage, and Country Similarity consistently\nemerged as significant predictors of model per-\nformance across different settings. Among these,\nModel Size was the most important feature in three\nout of six classification model setups and in three"}, {"title": "Model Features", "content": "The model features such as Pre-train Data Per-\ncentage, Instruction Tuning Data (specific to\nBloomZ), and Model Size-are crucial determi-\nnants of multilingual language model performance.\nPre-train Data Percentage consistently emerged\nas a significant factor across both classification\nand generation tasks, as evidenced by its high\nSHAP values. This suggests that models are better\nequipped to capture linguistic nuances and achieve\nhigher performance when more training data is\navailable. The analysis highlights the importance\nof increasing pre-training data, particularly for un-\nderrepresented languages, to enhance the model's\nability to understand and generate language effec-\ntively.\nModel Size also plays a critical role in deter-\nmining performance. Larger models, with their\nincreased number of parameters, have a greater ca-\npacity to learn complex patterns and relationships\nwithin the data, which is reflected in the consis-\ntently high SHAP values for this feature across var-\nious tasks. While larger models offer the advantage\nof more accurate predictions and higher-quality\noutputs, they also come with trade-offs, including\nhigher computational demands and longer training\ntimes, which need to be considered when scaling\nup model sizes.\nIn contrast, Instruction Tuning Data a feature\nunique to BloomZ-showed very low SHAP val-\nues, indicating its minimal impact on the model's\nperformance. This suggests that the model's effec-\ntiveness is more strongly influenced by the amount\nof pre-training data rather than the fine-tuning pro-\ncess. The analysis implies that while fine-tuning\ncan refine a model's capabilities, the scope and\nquality of pre-training data are far more critical in\ndetermining the overall effectiveness of the model,\nparticularly in multilingual contexts."}, {"title": "Geographical and Country Similarity", "content": "The analysis of geographical proximity and country\nsimilarity revealed varying impacts on the perfor-\nmance of multilingual language models. While\ngeographical proximity had a relatively modest in-\nfluence, their SHAP values indicated that they still\nprovided valuable context by capturing regional\nlinguistic variations that could affect model predic-\ntions. For instance, languages spoken in geographi-\ncally close regions might share linguistic character-\nistics that models can leverage for improved perfor-\nmance, even if these features were less important\ncompared to others like Model Size and Token Sim-\nilarity.\nIn contrast, country similarity had a more pro-\nnounced effect, frequently ranking among the top\nfour features. The overlap of countries where lan-\nguages are spoken often implies shared cultural\nand linguistic traits (Fishman, 1972), which mul-\ntilingual models can utilize to enhance their pre-\ndictions. This suggests that languages with higher\ncountry similarity benefit from shared linguistic\nresources and transfer learning, thereby improving\nmodel performance.\nThe lower significance of geographical proxim-\nity might stem from the fact that geographical prox-\nimity does not always correlate with linguistic sim-\nilarity. However, the stronger impact of country\nsimilarity, which directly relates to shared cultural\nand linguistic traits, underscores the importance of\nsociolinguistic factors in model performance."}, {"title": "Linguistic Features", "content": "The impact of linguistic features, specifically Lan-\nguage Family and Script, on the performance of\nmultilingual language models was analyzed, but\nthe SHAP values indicated that these features had\na relatively minor effect.\nFor Language Family, the SHAP values across\nboth classification and generation tasks were gen-\nerally low, suggesting that this feature did not sig-\nnificantly influence model performance. Although\nlinguistic relatedness can facilitate transfer learn-\ning, the results imply that other features capture\nmore crucial aspects of language modeling. Sim-\nilarly, the Script feature also showed low impor-\ntance according to the SHAP values. However, it\nis worth noting that Script type can indirectly in-\nfluence model performance through its impact on\nToken Similarity."}, {"title": "Token Similarity", "content": "Token similarity emerged as one of the most crucial\nfeatures influencing the performance of multilin-\ngual language models across both classification\nand generation tasks. This feature measures the\noverlap and similarity of tokens between differ-\nent languages, providing a direct insight into how\nwell the model can generalize and transfer learned\nknowledge from one language to another.\nThe consistent importance of token similarity\nacross both tasks highlights its role in facilitating\ntransfer learning and generalization in multilin-\ngual models. Languages with high token similarity\nallow the model to reuse and adapt learned repre-\nsentations effectively, reducing the need for exten-"}, {"title": "Resource-Related Features", "content": "Resource-related features, including Population,\nLanguage Vitality, Digital Language Support, and"}, {"title": "Discussion", "content": "The results of this study provide valuable insights\ninto the factors that drive the performance of multi-\nlingual language models across classification and\ngeneration tasks."}, {"title": "Ensemble Models and Feature Complexity:", "content": "\u2022 Ensemble models (Random Forest, Gradient\nBoosting, XGBoost) outperformed simpler\nlinear models (SVR, Lasso Regression) across\nboth classification and generation tasks.\n\u2022 These models are better at capturing complex,\nnon-linear interactions between features, high-\nlighting the intricate relationships in multilin-\ngual language models."}, {"title": "Critical Role of Model Features:", "content": "\u2022 Pre-train Data Percentage and Model Size\nemerged as the most influential factors in\nmodel performance."}, {"title": "Geographical and Sociolinguistic Context:", "content": "\u2022 While geographical proximity had a modest\nimpact, country similarity was more signifi-\ncant in driving model performance.\n\u2022 Shared cultural and linguistic traits across\ncountries enhance model predictions, empha-\nsizing the importance of considering sociolin-\nguistic factors."}, {"title": "Resource-Related Features:", "content": "\u2022 Features like Population, Language Vital-\nity, Digital Language Support, and Resource\nLevel had limited direct impact on model per-\nformance.\n\u2022 Although, the availability of resources is es-\nsential for providing high-quality training\ndata, they are not primary determinants of\nmodel success."}, {"title": "Conclusion", "content": "This study offers a detailed analysis of the factors\ninfluencing multilingual language model perfor-\nmance across classification and generation tasks.\nOur findings show that performance is shaped by\ncomplex, non-linear interactions among features.\nKey factors include pre-train data percentage and\nmodel size, which significantly affect effective-\nness. Token similarity enhances cross-lingual trans-\nfer learning, while country similarity highlights\nthe role of shared cultural and linguistic contexts.\nResource-related features like population and dig-\nital support showed limited direct impact but re-\nmain useful for understanding data availability and\ntraining strategies. These insights are crucial for\ndeveloping more equitable multilingual models, es-\npecially for underrepresented languages."}, {"title": "Limitation", "content": "This study, while comprehensive, has several limi-\ntations. The analysis is focused on specific models\n(Bloom, BloomZ, and XGLM), which may limit\ngeneralizability to other architectures. Additionally,\nreliance on SHAP values might overlook complex\ninteractions between features. The datasets (SIB-\n200 and Flores-200) cover many languages but may\nnot fully capture dialectal diversity, and computa-\ntional constraints restricted testing to a range of\nmodel sizes. Future work could address these as-\npects by exploring more models, diverse datasets,\nand further feature interactions."}]}