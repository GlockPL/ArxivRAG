{"title": "Partition Tree Weighting for Non-Stationary Stochastic Bandits", "authors": ["Joel Veness", "Marcus Hutter", "Andr\u00e1s Gy\u00f6rgy", "Jordi Grau-Moya"], "abstract": "This paper considers a generalisation of universal source coding for interaction data, namely data streams that have actions interleaved with observations. Our goal will be to construct a coding distribution that is both universal and can be used as a control policy. Allowing for action generation needs careful treatment, as naive approaches which do not distinguish between actions and observations run into the self-delusion problem in universal settings. We showcase our perspective in the context of the challenging non-stationary stochastic Bernoulli bandit problem. Our main contribution is an efficient and high performing algorithm for this problem that generalises the Partition Tree Weighting universal source coding technique for passive prediction to the control setting.", "sections": [{"title": "Introduction", "content": "This paper explores an alternate perspective on how to construct a type of universal agent from compression principles. In contrast to popular decision-making frameworks such as reinforcement learning, which are built upon appealing to decision-theoretic notions such as Maximum Expected Utility, we instead construct an agent by trying to minimise the expected number of bits needed to losslessly describe general agent-environment interactions. The appeal with this approach is that if we can construct a good universal coding scheme for arbitrary agent interactions, one could simply sample from this coding distribution to generate a control policy. However when considering general agents, whose goal is to work well across multiple environments, this question turns out to be surprisingly subtle. Naive approaches which do not discriminate between actions and observations fail, and are subject to the self-delusion problem [Ortega et al., 2021]. In this work, we will adopt a universal source coding perspective to this question, and showcase its efficacy by applying it to the challenging non-stationary stochastic bandit problem.\nIn the passive case, namely, sequential prediction of observations under the logarithmic loss, there is a well developed universal source coding literature for dealing with non-stationary sources under various types of non-stationarity. The most influential idea for modelling piecewise stationary sources is the transition diagram technique of Willems [1996]. This technique performs Bayesian model averaging over all possible partitions of a sequence of data, cleverly exploiting dynamic programming to yield an algorithm which has both quadratic time complexity and provable regret guarantees. Since then, a variety of more efficient algorithms [Willems and Krom, 1997, Shamir and Merhav, 1998, Hazan and Seshadhri, 2009, Gy\u00f6rgy et al., 2012, Veness et al., 2013, Daniely et al., 2015, Cutkosky, 2020, Zhao et al., 2024] have been developed which trade off worst-case regret for computational efficiency, by dealing with only a subset of the possible partitions, while still maintaining regret guarantees for the entire class of possible partitions. Of these techniques, arguably the technique of Veness et al. [2013] has the best complexity/performance trade-off, and this is the technique our work focuses on to extend to the case of control for non-stationary stochastic bandit problems.\nThe non-stationary stochastic bandit problem is currently an active area of study, with many algorithms recently being proposed to address aspects of the problem. A central challenge in this setting is to provide algorithms with good worst-case guarantees without unrealistic knowledge of the change-point structure in advance. There are a number of algorithms which adapt stationary stochastic bandit algorithms to incorporate ideas such as windowing or discounting. Notable examples include Sliding Window/Discounted UCB [Garivier and Moulines, 2008], as well as Thompson Sampling based variants [Russo et al., 2017, Trov\u00f2 et al., 2020]. These algorithms perform well when a good problem-dependent choice of hyperparameters is known in advance. Recently, meta-algorithms built around stochastic restarting of base stationary bandit algorithms [Wei and Luo, 2021] have shown much promise in addressing this problem in essentially a parameter-free way. Our new approach will be similar in spirit, but rather than stochastically restarting a base algorithm, we will adapt the techniques from the universal source coding literature to instead perform a type of Bayesian inference over a large class of possible restart configurations, essentially running many different restarting schedules in parallel efficiently. As we shall see later, this will lead to an algorithm which can be considered to be a generalisation of the usual Thompson Sampling approach [Thompson, 1933, Chapelle and Li, 2011] for stationary stochastic bandit problems. We investigate this technique"}, {"title": "Background", "content": "We first give a brief overview of our notation for history based agents, which closely follows the recent textbook presentation of Hutter et al. [2024].\nInteraction Data. A string $X_1X_2...X_n$ of length $n$ is denoted by $X_{1:n}$. The prefix $X_1X_2...X_j$ of $X_{1:n}, j \\le n$, is denoted by $x_{<j}$ or $X_{<j+1}$. The empty string is denoted by $\\epsilon$. The concatenation of two strings $s$ and $r$ is denoted by $sr$. We will use $e$ to denote the empty string, and use $l(s)$ to denote the length of a string $s$. We let $A, O, R$ denote the finite action, observation and reward spaces respectively. The percept space is defined by $E := O \\times R$. A history is a finite string of interaction data, namely\n$h_{1:t} = a_1o_1r_1a_2o_2r_2 ... a_to_tr_t = a_1e_1a_2e_2... a_te_t$\nfor any $t \\in \\mathbb{N}$ and where each $a_i \\in A, o_i \\in O, r_i \\in R$ and $e_i \\in E$. We will also use the shorthand notation $h_{<t} := h_{1:t-1}$. The set of all possible histories will be denoted by $H$. A policy is a function $\\pi: (A \\times X)^* \\to (A\\to [0, 1])$ that maps a string of interaction experience to a distribution $\\pi(\\cdot|h_{1:t})$ over actions.\nAgent-Environment Interaction Loop. An agent interacts with an environment in cycles $t = 1,2,....$ In each cycle, an agent samples an action $a_t \\sim \\pi(\\cdot|h_{<t})$ from its policy, and communicates it to the environment. The environment then sends a percept $e_t \\sim \\mu(\\cdot | h_{<t}a_t)$ to the agent, and the next cycle $t +1$ begins. A combination of a policy $\\pi$ and an environment $\\nu$ gives rise to an agent-environment measure $\\nu^{\\pi} : H \\to [0, 1]$ whose conditional form is given by\n$\\nu^{\\pi} (h_{t:m} | h_{<t}) := \\prod_{k=t}^{m} \\pi(a_k| h_{<k})\\nu(e_k|h_{<k}a_k)$.\nThis can always be decomposed into an environment measure and policy measure by respectively defining\n$\\nu(e_{t:m} || a_{<t}) := \\prod_{k=t}^{m} \\nu(e_t | h_{<k}a_k)$ and $\\pi(a_{t:m} || e_{<t}) := \\prod_{k=t}^{m} \\pi(a_k | h_{<k})$,\nwhere the $||$ notation is used to make explicit that the right-hand side is given (equiv-alently, one can think of it as an intervention) and not conditioned on in the usual probabilistic sense."}, {"title": "Universal Coding of Interaction Sequences", "content": "For now we consider the case of coding a single interaction sequence.\nUnknown environment, known policy. Let us now consider how to optimally encode an interaction sequence in the case where we knew the actions were generated by some choice of fixed policy $\\pi$, but only knew that the true environment was sampled from a known distribution $w(.)$ over $M$. One can show in $\\xi, \\pi$-expectation that the optimal solution to code the percepts now is to use the predictive Bayes mixture distribution\n$\\xi(e_t/h_{<t}a_t) := \\sum_{\\rho \\in M} w_t^{-1} \\rho(e_t|h_{<t}a_t), \\text{ with } w_t^{-1} := \\frac{w\\rho(e_{<t} || a_{<t})}{\\sum_{\\nu \\in M} w\\nu(e_{<t} || a_{<t})}$ (2)\ndenoting the posterior belief in $\\rho$ after seeing $t - 1$ percepts, with prior $w_0 := w(\\rho)$. Even if the prior is unknown, and we use a prior $w'(\\cdot)$ with the same support as $w(\\cdot)$ in Equation 2, then one can show that the regret\n$R_{\\xi}^{\\mu^{\\pi_\\mu}}(h_{1:t}) = log \\frac{\\mu(e_{1:t} || a_{1:t})}{\\xi(e_{1:t} || a_{1:t})} < log \\frac{\\mu(e_{1:t} || a_{1:t})}{w'(\\mu) \\mu(e_{1:t} || a_{1:t})} = - log w'(\\mu)$\nIn other words, in a regret sense, not much is lost due to not knowing $w(.)$, and for example one can chose a uniform prior over $M$ if $|M|$ is finite to minimize the worst case regret."}, {"title": "Problem Setting", "content": "We first develop some notation for stationary stochastic Bernoulli bandits, which we then will generalise to a non-stationary setting.\nStationary stochastic Bernoulli bandits. A stationary stochastic Bernoulli bandit problem can be described by a tuple $(A, \\Theta, \\mu)$, where $A > 1$ is the number of arms, with an associated parameter vector $\\Theta \\in [0,1]^A$. The $i$th component of $\\Theta$ will be denoted by $\\theta_i$, which will determine the probability of observing a success when arm $i$ is pulled. This gives rise to an environment measure $\\mu$ that is defined by $\\mu(1|h_{<t}a) := \\theta_a$ and $\\mu(0 | h_{<t}a) := 1 - \\theta_a$, with action space $A := \\{1, ..., A\\}$ and percept space $E := \\{0,1\\}$.\nNon-stationary stochastic Bernoulli bandits. We first introduce some notation to describe temporal partitions. A segment is a tuple $(c,d) \\in \\mathbb{N}\\times\\mathbb{N}$ with $c \\le d$. A segment $(c, d)$ is said to overlap with another segment $(c', d')$ if there exists an $i\\in \\mathbb{N}$ such that $c \\le i \\le d$ and $d \\le i \\le d'$. A temporal partition $P$ of a set of time indices $S = \\{1, 2, . . . n\\}$, for some $n \\in \\mathbb{N}$, is a set of non-overlapping segments such that for all $x \\in S$, there exists a unique segment $(c, d) \\in P$ such that $c \\le x \\le d$.\nAn abruptly changing non-stationary stochastic Bernoulli bandit problem (NSSBP) can be described by a tuple $(A, P, \\{\\Theta_{c,d}\\}_{(c,d)\\in P}, \\mu)$, where $A > 1$ is the number of arms, $P$ is a temporal partition describing a change-point regime, and each $\\Theta_{c,d} = (\\theta_{c,d}^1, ..., \\theta_{c,d}^A) \\in [0, 1]^A$ determining the probability of success $\\theta_{c,d}^i$ of pulling arm $i$ at time $c \\le t \\le d$. This gives rise to an environment measure $\\mu$ that is defined by\n$\\mu(e_{1:t} || a_{1:t}) := \\prod_{(c,d)\\in P} \\prod_{t=c}^{d} [e_t \\theta_{a_t}^{c,d} + (1 - e_t) (1 - \\theta_{a_t}^{c,d})]$ (4)\nwith action space $A := \\{1, . . ., A\\}$ and percept space $E := \\{0,1\\}$. Of course another way to look at this problem definition is a sequence of stationary stochastic bandit problems sharing a common action space.\nGoal. Our goal will be to develop an algorithm which works well for a meaningful subset of the class of abruptly changing NSSBPs. A general solution is of course impossible, since the change-point structure might be too rapid to allow for sufficient periods of exploitation, but in cases where the change-point structure is simple enough there is still hope for an effective algorithm, which is the subject of our next section."}, {"title": "The ActivePTW Algorithm", "content": "This section adopts the perspective of Section 3 to develop a universal source coding technique for interaction data generated by an initially unknown environment measure $\\mu$ associated with a choice of abruptly changing NSSBP $(A, P, \\{\\Theta_{c,d}\\}_{(c,d)\\in P}, \\mu)$. We will first construct a universal environment measure for this problem class, and then turn our attention to constructing a universal policy which will work well for a meaningful subset of these problems. Although this section aims to be self-contained, our work relies heavily on the Context/Partition Tree Weighting techniques [Willems et al., 1995, Veness et al., 2013], and we highly encourage the interested reader to first study the original papers for a more complete understanding.\nUniversal Environment Measure for NSSBPs\nWe now construct our universal environment measure by an appropriate combination of the Krichevsky-Trofimov (KT) Estimator and the Partition Tree Weighting technique [Veness et al., 2013], two universal source coding techniques for passive (i.e. no actions or rewards, just observations) streams of data. Our construction starts by defining a universal environment measure for stationary stochastic Bernoulli environments, which we shall call a KT Environment, which we then generalise to the non-stationary case by combining with the Partition Tree Weighting technique.\nKT Environment. We now describe a universal environment model for modelling the percepts received from any stationary stochastic bandit problem. The main idea is to associate a distinct KT Estimator with each possible arm to model its unknown success probability.\nA KT estimator can be obtained from a Bayesian analysis which combines a Binomial$(\\theta)$ likelihood with a Jeffreys prior $w_\\eta(\\theta) := \\pi^{-1}[\\theta(1 - \\theta)]^{-1/2}$. For all $n \\in \\mathbb{N}$, the KT-probability of a binary string $X_{1:n} \\in X$ is defined as\n$\\text{KT}(X_{1:n}) = \\mathbb{P}_{\\text{KT}}(\\#_0, \\#_1) := \\int_0^1 w_\\eta(\\theta) \\text{Binomial}_{\\theta} (x_{1:n}) d\\theta = \\frac{B(\\#_0 + \\frac{1}{2}, \\#_1 + \\frac{1}{2})}{B(\\frac{1}{2}, \\frac{1}{2})}$\nwhere $\\text{Binomial}_{\\theta}(x_{1:n}) := \\theta^{\\#_1}(1 - \\theta)^{\\#_0}$ and $B(\\alpha, \\beta) = \\Gamma(\\alpha)\\Gamma(\\beta)/\\Gamma(\\alpha + \\beta)$ with $\\#_0$ and $\\#_1$ denoting the number of 0s and 1s in $X_{1:n}$ respectively. For the KT-estimator, it is known that its redundancy can be upper bounded by\n$-\\log_2 \\text{KT}(X_{1:n}) + \\log_2 \\text{Binomial}_{\\theta} (x_{1:n}) \\le \\frac{1}{2} \\log_2(n) + 1$ (5)\nfor all $n \\in \\mathbb{N}$, all $x_{1:n} \\in \\{0,1\\}^n$ and any $\\theta \\in [0, 1]$. Given the above, we can now define an environment measure\n$\\text{KTE}(e_{1:t} || a_{1:t}) := \\prod_{a \\in A} \\text{KT}(e_{1:t}^a)$\nwhere $e_{1:t}^a$ is defined as the subsequence (potentially non contiguous) formed of the percepts $e_i$ such that $a_i = a$ for all $1 < i < t$. The KTE probability can be maintained online using O(1) time per step, with a fixed space overhead of O(|A|) by using an |A|-sized array maintaining the sufficient statistics, i.e. the a and b counts, for each KT-estimator. The following result shows that the KTE environment has bounded redundancy with respect to any sequence of arm pulls and any sequence of observations.\nProposition 2 (KTE Redundancy) For any stationary stochastic bandit problem $(A, \\Theta, \\mu)$, for all $n \\in \\mathbb{N}$, for all $e_{1:n} \\in E^n$ and for all $a_{1:n} \\in A^n$ we have\n$-\\log_2 \\text{KTE}(e_{1:t} || a_{1:t}) + \\log_2 \\mu(e_{1:t} || a_{1:t}) \\le \\frac{|A'|}{2} \\log_2 (\\frac{n}{|A'|}) + |A'|$, (6)\nwhere $A' := \\{a \\in A : l(e_{i:t}) > 0\\}$ is the set of all actions taken at least once.\nPTW-KT Environment. Our PTW-KT environment is constructed hierarchically. A tree-based prior is first placed over a class of temporal partitions, and then, given a temporal partition, the parameters governing a segments behavior are modelled using a KTE. For computational reasons, we chose to use the class of binary temporal partitions, which have some useful properties.\nDefinition 3 (Veness et al. [2013]) Given a depth parameter $D \\in \\mathbb{N}$ and a time $t \\in \\mathbb{N}$, the set $C_D(t)$ of all binary temporal partitions from $t$ is recursively defined by\n$C_D(t) := \\{\\{(t,t + 2^D - 1)\\}\\} \\cup \\{S_1 \\cup S_2 : S_1 \\in C_{D-1} (t), S_2 \\in C_{D-1} (t + 2^{D-1})\\}$,\nwith $C_0(t) := \\{\\{(t,t)\\}\\}$. Furthermore, we define $C_D := C_D(1)$.\nFor example, $C_2 = \\{\\{(1, 4)\\}, \\{\\{(1, 2), (3, 4)\\}\\}, \\{\\{(1, 1), (2, 2), (3, 4)\\}\\}, \\{\\{(1, 2), (3, 3), (4, 4)\\}\\}, \\{\\{(1, 1), (2, 2), (3, 3), (4,4)\\}\\} \\}$. Each binary temporal partition can be naturally thought of in terms of representing a tree structure which we will call a partition tree. This class has some useful properties. Given a set of segments $S$, the set of time indices they cover is given by $T(S) := \\bigcup_{(c,d)\\in s} \\{c, c + 1, ...,d\\}$. Any partition $P$ of consecutive time indices upto $2^D$ can be covered by some binary temporal partition $P' \\in C_D$, in the sense that $T(P) = T(P')$ and every segment endpoint in $P$ appears in $P'$. Furthermore, there always exists a covering binary partition which contains no more than $|P|([\\log_2n] + 1)$ segments [Veness et al., 2013, Lemma 2].\nOur PTW-KTE environment measure, for $t < 2^D$, can now be specified by\n$\\text{PTW-KTE}_D(e_{1:t} || a_{1:t}) := \\sum_{P \\in C_D} 2^{-F_D(P)} \\prod_{(c,d) \\in P} \\text{KTE}(e_{c:d} || a_{c:d})$, (7)\nwhere $F_D(P)$ is a proper prior over binary temporal partitions within $C_D$. Intuitively, one can think of $F_D(P)$ as a mapping which returns the description length of the tree structure associated with $P$ under a natural encoding, with smaller tree structures receiving higher weight. This particular weighting over tree structures was first introduced with the Context Tree Weighting method of Willems et al. [1995]. Although the number of binary temporal partitions grows exponentially with $t$, it is possible to maintain both the marginal and predictive probabilities for PTW-KTE$_D$ with a space overhead of O(D|A|) and time overhead of O(D) for times $t < 2^D$.\nThe next theorem upper bounds the redundancy of the environment measure PTW-KTED with respect to an arbitrary NSSBP.\nTheorem 4 Given an abruptly changing NSSBP $(A, P, \\{\\Theta_{c,d}\\}_{(c,d)\\in P}, \\mu)$, for all $t \\in \\mathbb{N}$ such that $t \\le 2^D$, we have that the redundancy of PTW-KTED with respect to the true environment $\\mu$ is upper bounded by\n$-\\log_2 \\frac{\\mu(e_{1:t} || a_{1:t})}{\\text{PTW-KTE}_D (e_{1:t} || a_{1:t})} \\le |P|([\\log_2t]+1) [\\frac{|A|}{2} \\log_2 (\\frac{t}{|A||P|([\\log_2t] + 1)}\\) + A + 2\\]$\nfor all $e_{1:t} \\in E^t$, and for all $a_{1:t} \\in A^t$. Proof. This follows by a straightforward adaption of Theorem 1 in Veness et al. [2013], and combining it with Proposition 2.\nIt is important to note that this bound is only meaningful in a predictive sense, since although it holds with respect to any given sequence of actions, it implies nothing about whether the true environment will be identified or not. This point will become clearer in the next section when we come to specifying the ACTIVEPTW policy, as we shall see the benefits of forcing exploration.\nThe ActivePTW Policy\nFollowing along the lines of Section 3 where we gave a universal source coding interpretation to the Bayesian Control Rule, our ActivePTW algorithm will sample from the policy which minimizes the expected single-step coding cost of an action at time $t$, where the expectation is taken with respect to the PTW-KTE posterior over environments.\nPosterior Probability Density over NSSBP environments. The PTW-KTE prior probability over a NSSBP $(A, P, \\{\\Theta_{c,d}\\}_{(c,d)\\in P}, \\mu)$ is given by\n$w_0(P, \\{\\Theta_{c,d}\\}_{(c,d)\\in P}) = 2^{-F_D(P)} \\prod_{(c,d) \\in P} \\prod_{a \\in A} w_s(\\Theta_{c,d}^a)$\nwith associated likelihood function $\\mu(\\cdot || a_{1:t})$ as defined in Equation 4. The posterior probability of an NSSBP after seeing $t > 0$ observations is\n$w_t(P, \\{\\Theta_{c,d}\\}_{(c,d)\\in P}) \\propto w_0(P, \\{\\Theta_{c,d}\\}_{(c,d)\\in P}) \\mu(e_{1:t} || a_{1:t})$\nwith the marginal probability (i.e. normalising constant) given by PTW-KTE$_D(e_{1:t} || a_{1:t})$. Furthermore, one can define a posterior over partitions by marginalising out the parameters governing the arms' behavior in each segment, giving\n$W_t(P) \\propto 2^{-F_D(P)} \\prod_{(c,d) \\in P} \\text{KTE}(e_{c:d} || a_{c:d})$\nwhere the normalizing constant is given by Equation 7. The posterior probability of a segment $(c, d)$ is thus given by the sum of the posterior probabilities of all partitions containing $(c, d)$, i.e.\n$w_t((c, d)) := \\sum_{P'\\in\\{P\\in C_D: (c,d)\\in P\\}} W_t(P')$. (8)\nAt any given time $t < 2^D$, there are exactly D + 1 different active segments due to the tree structured nature of the partitions, whose lengths are $2^0, 2^1, . . . 2^D$. More formally, the set of active segments at time $t$ is defined by\nACTIVESEGMENTS$_D(t) := \\{(c, d) \\in \\mathbb{N} \\times \\mathbb{N} : c \\le t \\le d,\\exists P \\in C_D \\text{ such that } (c, d) \\in P\\}$\nNote that ACTIVESEGMENTS$_D(t)$ can be computed in O(D) time from the binary representation of $t - 1$; see Appendix B for details.\nWe can now define our key quantity of interest, the posterior probability distribution over the active segments at time $t$, namely\n$q_t((c,d) := \\{\\begin{array}{ll}W_t((c, d)) & \\text{ if } (c, d) \\in \\text{ACTIVESEGMENTS}_D(t); \\\\0 & \\text{ otherwise.} \\end{array}$\nWe will subsequently show that this posterior probability always sums to 1 and can be computed in O(D) time. This will form the basis of an efficient computational realisation of the ActivePTW approach.\nComputing the posterior over the active segments. We first present a helpful identity which characterises the recursive structure of the marginal probability of the observations given a history, which we will exploit to efficiently compute the posterior weight of each active segment.\nLemma 5 For depth $D \\in \\mathbb{N}$, given a history $h_t := a_1e_1 ... a_te_t$ where $t \\le 2^D$, we have\n$\\text{PTW-KTE}_D(e_{1:t} || a_{1:t}) = \\frac{1}{2} \\text{KTE}(e_{1:t} || a_{1:t}) +$\n$\\frac{1}{2} \\text{PTW-KTE}_{D-1}(e_{1:k} || a_{1:k}) \\text{PTW-KTE}_{D-1}(e_{k+1:t} || a_{k+1:t})$\nwhere $k = 2^{D-1}$ and PTW-KTE0(e || a) = KTE(e ||a) by definition. Proof. The proof is a straightforward adaptation of Lemma 1 in Veness et al. [2013].\nFrom here onward, we assume that all relevant marginal probabilities of the form PTW-KTE$_i(\\cdot || \\cdot)$ are already known for 0 \u2264 i \u2264 D, as they can be efficiently computed from Lemma 5 and maintained within an array of size O(D); see Section 3.3 of Veness et al. [2013] for details.\nNow we show how to calculate the posterior weight $q_t ((c, d)), \\forall(c, d) \\in \\text{ACTIVESEGMENTS}_D(t)$ efficiently, and show that for all $t \\in \\mathbb{N}$ we have\n$\\sum_{(c,d)} q_t ((c, d)) = \\sum_{(c,d) \\in \\text{ACTIVESEGMENTS}_D (t)} W_t ((c, d)) = 1$.\nFirst note that the set of active segments can be calculated in O(D) time as per Appendix B. Now at any time $t$ there will be D + 1 active segments of length $2^D, 2^{D-1}, ..., 1$. Recall that the posterior weight for a given segment $(c, d)$ is defined by\n$w_t((x,y)) := \\sum_{P'\\in\\{P\\in C_D: (x,y)\\in P\\}} W_t(P'),$\nin other words, the sum of all the posterior weights of the partitions in CD containing segment $(c, d)$. Also, recall that the posterior weight of any given partition is\n$W_t(P) = \\frac{2^{-F_D(P)} \\prod_{(c,d)\\in P} \\text{KTE}(e_{c:d} || a_{c:d})}{\\text{PTW-KTE}_D (e_{1:t} || a_{1:t})}$\nwith the normalising constant given by\n$\\text{PTW-KTE}_D(e_{1:t} || a_{1:t}) := \\sum_{P\\in C_D} 2^{-F_D(P)} \\prod_{(c,d) \\in P} \\text{KTE}(e_{c:d} || a_{c:d})$.\nNow let $s_i := (c_i, d_i)$ denote the active segment with length $2^i$ at an arbitrary time $t$ (whose index is suppressed as it is non-essential). Note that $\\sum_{P \\in C_D} W_t(P) = 1$. Now for any $P, s_i \\in P$ for exactly one i, hence $C_i := \\{P \\in C_D : S_i \\in P\\}$ for $i \\in \\{0,...,D\\}$ partitions CD, i.e. $\\bigcup_i C_{i} = C_D$ and $C_{i} \\cap C_{j} = \\emptyset$ for $i \\neq j$. This implies\n$\\sum_{(c,d)} W_t ((c,d)) = \\sum_i W_t (s_i) = \\sum_i \\sum_{P'\\in C_i} W_t (P') = \\sum_{P'\\in C_D} W_t (P') = 1$\nhence $q_t()$ is a valid prior over the active segments. Lemma 5 can be used inductively to efficiently compute the posterior weight for each active segment. In particular we have that\n$W_t(s_d) = \\frac{1}{2} \\text{KTE}(e_{c_d:d_d} || a_{c_d:d_d})/ \\text{PTW-KTE}_D(e_{c_d:d_d} || a_{c_d:d_d})$\nand for $s_i$ with $0 < i < d$, one can show\n$W_t(s_i) = [1 - W_t(s_{i+1})] \\times \\frac{1}{2} \\text{KTE}(e_{c_i:d_i} || a_{c_i:d_i})/ \\text{PTW-KTE}_i(e_{c_i:d_i} || a_{c_i:d_i})$,\nwith the base case $w_t(s_0) := [1 - W_t(s_{1})]$.\nReference policies for known environments. To completely describe a universal class of agent-environment measures, we need to first specify a policy for each possible environment. Because the number of possible NSSBPs is large, we do this analytically. We consider the following two policies, which given an arbitrary NSSBP $(A, P, \\{\\Theta_{c,d}\\}_{(c,d)\\in \\rho}, \\mu)$, are defined as follows:\n(Maximum Expected Utility) Given a segment $(c, d) \\in P$, at times $c \\le t \\le d$, the Maximum Expected Utility (MEU) policy is defined as\n$\\pi^{\\mu}(a|h_{<t}) := \\{\\begin{array}{ll}1/|A| & \\text{ if } a \\in A_t; \\\\0 & \\text{ otherwise,} \\end{array}$\nwhere $A_t := arg \\max_{a\\in A}\\{\\Theta_{c,d}^a\\}$. This policy simply plays an arm with the highest single-step $\\mu$-expected return.\n(Maximum Expected Utility with Forced Exploration) The Maximum Expected Utility with Forced Exploration (MEUFE) policy\n$\\pi^{\\mu}(a/h_{<t}) := \\alpha(d - c)/|A| + (1 - \\alpha(d - c)) \\pi^{\\mu}(a|h_{<t})$\nexplores uniformly at random with a segment length $l$-dependent probability $\\alpha(l) := 1-2$, otherwise it behaves like the MEU policy. We shall see later (Sections 6 and 7) that such forced exploration is essential to both the analysis and for certain challenging change-point regimes.\nAn efficient Bayesian Control Rule Policy for NSSBPs. Recall that the Bayesian Control Rule can be interpreted as a policy which if used as a coding distribution, would minimise the single step expected code length of the actions with respect to the agent's posterior distribution over possible environments. This arguably underappreciated approach is already known to yield interesting algorithms in the stationary setting, as one can show that combining a greedy MEU reference policy with a KTE environment gives rise to the classic Thompson Sampling algorithm for stationary stochastic bandit problems [Ortega and Braun, 2012]. The ActivePTW algorithm is derived from the same principles, only that now we consider a richer class of environments. In particular, the BCR-type policy using the MEU reference policies now becomes\n$\\pi^* (a_t | h_{<t}) := \\sum_\\alpha W_t(P, \\{\\Theta_{c,d}\\}_{(c,d)\\in P}) \\pi^{\\mu}(a_t | h_{<t}),$"}, {"title": "Theoretical Analysis.", "content": "Earlier we provided redundancy guarantees for the PTW-KTE environment measure", "p": "alpha/|A|$. Let $B_{t,i} \\sim"}]}