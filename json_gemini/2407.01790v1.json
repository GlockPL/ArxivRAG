{"title": "Label-free Neural Semantic Image Synthesis", "authors": ["Jiayi Wang", "Kevin Alexander Laube", "Yumeng Li", "Jan Hendrik Metzen", "Shin-I Cheng", "Julio Borges", "Anna Khoreva"], "abstract": "Recent work has shown great progress in integrating spatial conditioning to control large, pre-trained text-to-image diffusion models. Despite these advances, existing methods describe the spatial image content using hand-crafted conditioning inputs, which are either semantically ambiguous (e.g., edges) or require expensive manual annotations (e.g., semantic segmentation). To address these limitations, we propose a new label-free way of conditioning diffusion models to enable fine-grained spatial control. We introduce the concept of neural semantic image synthesis, which uses neural layouts extracted from pre-trained foundation models as conditioning. Neural layouts are advantageous as they provide rich descriptions of the desired image, containing both semantics and detailed geometry of the scene. We experimentally show that images synthesized via neural semantic image synthesis achieve similar or superior pixel-level alignment of semantic classes compared to those created using expensive semantic label maps. At the same time, they capture better semantics, instance separation, and object orientation than other label-free conditioning options, such as edges or depth. Moreover, we show that images generated by neural layout conditioning can effectively augment real data for training various perception tasks.", "sections": [{"title": "Introduction", "content": "Controllable image synthesis enables users to specify the desired image content, while relying on a generative model to fill in details that align with the distribution of natural images. One way to express the content is through natural language, as popularized by the recent advances of large-scale text-to-image (T2I) diffusion models [1,3,26,28]. However, it can be extremely time-consuming to comprehensively specify the detailed spatial composition of the desired image using only textual prompts. Precisely describing a complex scene (such as in Fig. 1), including layout, pose, shape, and orientation of objects can be a tedious task. It usually requires trial-and-error for the user to refine the prompt such that the generation matches the desired image.\nThis becomes prohibitive when images need to be generated on a large scale without human-in-the-loop; for instance, when using them for training models in"}, {"title": "Related Work", "content": "Controllable Image Synthesis. In the realm of generative adversarial networks (GANs), previous methods [15,23,32,33,37,38,51] have attempted to utilize various types of conditioning information to specify the spatial and semantic content of the synthesized images. With the increasing popularity of large-scale text-to-image (T2I) diffusion models, e.g., DALL-E 2 [26], eDiff-I [3] and Stable Diffusion (SD) [28], more recent works seek to incorporate additional control mechanism into the T2I models, to better steer the generation process. One popular direction in this effort is to leverage extra image specifications such as label maps, edges, or depth [18,21,41,45,47] to provide a more detailed description of the spatial composition of the desired image, and thus gain finer control over the synthesis process. FreestyleNet [41] finetunes the entire SD model to rectify the cross attention maps based on the semantic labels. T2I-Adapter [21] and ControlNet [45] both introduce an adapter network on top of a frozen SD backbone to accommodate the conditioning information. Uni-ControlNet [47] extends ControlNet by accepting multiple conditioning inputs via two adapters, further enhancing its controllability.\nDifferent from these prior works and complementary to them, we focus on improving the representation of the image descriptor used by these mechanisms. We introduce the novel concept of \"neural semantic image synthesis\", which makes use of the rich spatial and semantic knowledge within large-scale pre-trained foundation models (FMs) as conditioning input for T2I diffusion models. In contrast to existing representations, our proposed neural layouts can capture high-level semantic and geometry information, without human labeling efforts.\nDense Neural Features. Benefiting from advanced pretraining techniques such as self-supervised learning [6,22] and joint vision-language pretraining [25, 28], large-scale pretrained foundation models (FMs) like CLIP [25], DINO [6], DINOv2 [22], and Stable Diffusion (SD) [28] itself can serve as robust feature extractors for diverse tasks. For example, several works [2, 12, 44] have discovered"}, {"title": "Method", "content": "In this section, we introduce the concept\nof neural semantic image synthesis. In-\nstead of using ad-hoc conditioning to de-\nscribe the desired output, neural seman-\ntic image synthesis makes use of neural\nlayouts derived from the dense features\nof pretrained foundation models. This se-\nmantically and spatially rich representa-\ntion extracted from a reference image can\nbetter specify the desired semantic con-\ntent and scene geometry without requir-\ning annotations for finetuning.\nOur model LUMEN (Fig. 2) takes ad-\nvantage of the observation that informa-\ntion is stored in easily separable form in-\nside the dense features of foundation models [2,6,22,34]. It has been shown that\neven a simple linear projection is sufficient to extract diverse contents such as se-\nmantic segmentation, depth estimates, and part correspondences [12, 22, 48, 50].\nAnalogously, we observe that semantic and spatial scene composition useful for\nspecifying a desired image can be separated from the exact object appearance\nusing a simple linear projector. Through this, LUMEN can align the genera-\ntion with the spatial and semantic content of a reference image while providing\nmeaningful variations in appearance through the sampling process.\nWe first introduce how neural layouts are extracted from a reference image\nin Sec. 3.1. We then explain how this can be used for conditional image synthesis\nwith diffusion models in Sec. 3.2. Finally, we will provide the best practices for\nextracting semantically and spatially meaningful features from different founda-\ntion models in Sec. 3.3."}, {"title": "Neural Layout", "content": "Dense Feature Extraction. Modern foundation models make heavy use of self-attention (SA) and cross-attention (CA) modules [35]. Following the convention by Amir et al. [2], we introduce the query, key, value, and token features available at each attention layer that is associated with different patches of an input image Xref. At layer 1, a SA module takes the tokens of the previous layer tl-\u00b9, and linearly projects them into queries qsa, keys k'sa, values USA using matrices $W_q^{SA}$,\n$W_k^{SA}$, $W_v^{SA}$:\n$q_{SA}^l = W_q^{SA} \\cdot t^{l-1}, k_{SA}^l = W_k^{SA} \\cdot t^{l-1}, v_{SA}^l = W_v^{SA} \\cdot t^{l-1}$.\nCA modules instead compute the features based on another conditioning input y:\n$q_{CA}^l = W_q^{CA} \\cdot t^{l-1}, k_{CA}^l = W_k^{CA} \\cdot y, v_{CA}^l = W_v^{CA} \\cdot y$.\nThe resulting features are combined through a series of scaled dot-product attention, normalization, multi-layer perceptron, and residual connections to obtain the token feature t\u00b9 = TransformerBlock(q\u00b9, k\u00b9, v\u00b9, tl\u22121).\nOnce extracted, these features are reshaped into a dense features map f corresponding to the original image patches. It was noted by several works [2, 44] that dense features can have different properties depending on where they are extracted. In Sec. 3.3, we detail the conventions we followed for different foundation models we investigated.\nSemantic Separation. Retaining the entire dense feature map f would reveal too detailed information about the reference image xref. Neural semantic image synthesis would then typically lead to samples that are highly similar to xref, lacking diversity (see Fig. 3). To prevent this, it is preferable to separate semantic and spatial features from those that encode appearance details. Based on existing works [22], we hypothesize that the principal directions of variation in the dense features should at least partially correspond to what humans intuitively understand as spatial and semantic image content. Thus, we implement Principal Component Analysis (PCA) to obtain a linear projector that can remove nuisance variations.\nTo obtain the neural layout ci, used as conditioning in LUMEN, we retain only the information in the top N PCA components, where N is a hyperparameter of our method. In practice, we compute the PCA projection on a random sample of 40,000 feature vectors extracted from images in the training set."}, {"title": "Conditional Image Synthesis", "content": "Neural semantic image synthesis generates the images x based upon text prompts ct and a neural layout ci. Although this task is compatible with a variety of conditional synthesis frameworks, we build LUMEN upon ControlNet [45], due to its popularity and the ease of comparison, as it is compatible with most existing image conditioning. ControlNet makes use of the frozen Stable Diffusion model [28] for text-conditional image synthesis and adds a trainable adapter to incorporate the conditioning."}, {"title": "Foundation Model Backbones", "content": "We considered 4 different foundation models for extracting neural layouts ci. This covers features from state-of-the-art methods trained in a self-supervised fashion (DINO [6] and DINOv2 [22]), vision-language models trained contrastively (CLIP [25]), as well as text-to-image synthesis (Stable Diffusion [28]). Since the output resolution of these features varies, all features are upscaled using exact nearest neighbor interpolation to match the image resolution.\nDINO. We follow established work [2] and use the \"key\" feature from SA layers when processing Xref as these features contain information useful for challenging semantic correspondence problems. We use the last SA layer and exclude the key corresponding to the CLS token to compute the neural layout.\nDINOv2. As a scaled-up version of DINO that is trained on a large amount of curated data, DINOv2 potentially contains more generalizable features. Here, we use the non-CLS tokens of the last transformer layer for neural layout [44].\nCLIP. We use the CLIP vision encoder to encoding Xref and keeping all but the CLS tokens from the last feature layer before the final pooling [50]. These features were shown to be well-suited for segmentation tasks.\nStable Diffusion. We first use BLIP [17] to obtain text prompts ct for the respective image xref, and encode Xref in the latent space of Stable Diffusion 1.5 (SD): zo = E(xref). Next, we apply SD's U-Net to predict the noise term \u03b5\u03b8 (zo, 0, ct). We extract the intermediate activations from layer 2, 5, and 8 of SD's U-Net and upsampled them to match the resolution of layer 8. All activations are then concatenated across the channel dimension. These SD features [44] were shown to have a strong sense of spatial layout, which makes them a promising candidate for neural layouts."}, {"title": "Experiments", "content": "In this section, we first explain our experimental setup and evaluation metrics. Then we show the impact of our design choices for neural layout conditioning in Sec. 4.1. The advantages of neural layouts compared to existing conditioning inputs are demonstrated in Sec. 4.2. Finally in Sec. 4.3, we validate the quality of images synthesized by LUMEN by showing its impact on downstream applications."}, {"title": "Neural Layout Design Space", "content": "We explore the design space of neural layouts on the diverse COCO-Stuff dataset [4] to determine how to best extract descriptive semantic and spatial information from a given reference image.\nBest Features for Neural Layout. For the task of neural semantic image synthesis, we want neural layouts to preserve semantic and geometry content but to discard appearance details. Thus, we want a backbone that extracts features where this is readily separable. To evaluate this, we compare in Tab. la the image quality of DINO, DINOv2, CLIP, and SD features at N = 10 PCA components.\nWe observe that SD features provide the best perceptual image quality and also retain the semantic content best, while DINOv2 is a close second. Although CLIP conditioning can generate more varied images, this diversity is due to the weak semantic and spatial constraints imposed during synthesis. This is likely because CLIP's image-level training objective is less suitable to capture precise pixel-level information without further processing.\nNumber of PCA Components. Since discarding more PCA components removes more information, useful or not, the resulting synthesis task also becomes less constrained (See Fig. 3). This is reflected in Tab. 1b where we compare synthesis results across different numbers of PCA components. We observe a clear"}, {"title": "Comparison to Existing Conditioning", "content": "We compare LUMEN using neural layouts with N = 20 PCA components against different established image conditioning. For this, we consider the common ControlNet condition: Canny edges [5] (Canny), HED edges [40] (HED), MiDaS depth [27] (MiDaS), predicted semantic segmentation (Sem. Seg. pred), and ground truth semantic segmentation (Sem. Seg. GT). For Sem. Seg. (pred) conditioning, we use the same pretrained network as the one used for evaluating the mIoU metric."}, {"title": "Downstream Applications", "content": "In this section, we show the applicability of data from LUMEN to different downstream applications. Since image diversity is crucial for model training, N = 10 PCA components are kept in the neural layout.\nCross Domain Multi-Task Training. Being label-free, we can leverage unsupervised fine-tuning on a large dataset in order to augment a labeled training set with diverse variants. We demonstrate this by using LUMEN finetuned on COCO-Stuff [4] to create synthetic variants of training data from NYUv2 [31]. NYUv2 [31] is a small dataset of indoor scenes commonly used for multi-task learning. LibMTL [20], with its standard setting, was used to train a multi-task network based on ResNet-50 [11] to"}, {"title": "Conclusion", "content": "We introduced the concept of neural semantic image synthesis and established LUMEN as a strong label-free baseline that can simultaneous specify semantic and spatial concepts of the outputs. Neural semantic image synthesis has additional applications that we hope will be explored in future works. For example, targeted removal of irrelevant information could be possible by replacing PCA with a learned projector specialized to a downstream task. Similarly, one could use neural semantic image synthesis to create images in a target domain from images in a source domain if a projection direction can be learned to eliminate the domain differences. This allows the reuse of labels from one image domain to another in order to save costly annotation efforts."}]}