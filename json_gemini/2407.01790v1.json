{"title": "Label-free Neural Semantic Image Synthesis", "authors": ["Jiayi Wang", "Kevin Alexander Laube", "Yumeng Li", "Jan Hendrik Metzen", "Shin-I Cheng", "Julio Borges", "and Anna Khoreva"], "abstract": "Recent work has shown great progress in integrating spatial conditioning to control large, pre-trained text-to-image diffusion models. Despite these advances, existing methods describe the spatial image content using hand-crafted conditioning inputs, which are either semantically ambiguous (e.g., edges) or require expensive manual annotations (e.g., semantic segmentation). To address these limitations, we propose a new label-free way of conditioning diffusion models to enable fine-grained spatial control. We introduce the concept of neural semantic image synthesis, which uses neural layouts extracted from pre-trained foundation models as conditioning. Neural layouts are advantageous as they provide rich descriptions of the desired image, containing both semantics and detailed geometry of the scene. We experimentally show that images synthesized via neural semantic image synthesis achieve similar or superior pixel-level alignment of semantic classes compared to those created using expensive semantic label maps. At the same time, they capture better semantics, instance separation, and object orientation than other label-free conditioning options, such as edges or depth. Moreover, we show that images generated by neural layout conditioning can effectively augment real data for training various perception tasks.", "sections": [{"title": "1 Introduction", "content": "Controllable image synthesis enables users to specify the desired image content, while relying on a generative model to fill in details that align with the distribution of natural images. One way to express the content is through natural language, as popularized by the recent advances of large-scale text-to-image (T2I) diffusion models [1,3,26,28]. However, it can be extremely time-consuming to comprehensively specify the detailed spatial composition of the desired image using only textual prompts. Precisely describing a complex scene (such as in Fig. 1), including layout, pose, shape, and orientation of objects can be a tedious task. It usually requires trial-and-error for the user to refine the prompt such that the generation matches the desired image.\nThis becomes prohibitive when images need to be generated on a large scale without human-in-the-loop; for instance, when using them for training models in"}, {"title": "3 Method", "content": "In this section, we introduce the concept of neural semantic image synthesis. Instead of using ad-hoc conditioning to describe the desired output, neural semantic image synthesis makes use of neural layouts derived from the dense features of pretrained foundation models. This semantically and spatially rich representation extracted from a reference image can better specify the desired semantic content and scene geometry without requiring annotations for finetuning.\nOur model LUMEN (Fig. 2) takes advantage of the observation that information is stored in easily separable form inside the dense features of foundation models [2,6,22,34]. It has been shown that even a simple linear projection is sufficient to extract diverse contents such as semantic segmentation, depth estimates, and part correspondences [12, 22, 48, 50]. Analogously, we observe that semantic and spatial scene composition useful for specifying a desired image can be separated from the exact object appearance using a simple linear projector. Through this, LUMEN can align the generation with the spatial and semantic content of a reference image while providing meaningful variations in appearance through the sampling process.\nWe first introduce how neural layouts are extracted from a reference image in Sec. 3.1. We then explain how this can be used for conditional image synthesis with diffusion models in Sec. 3.2. Finally, we will provide the best practices for extracting semantically and spatially meaningful features from different foundation models in Sec. 3.3."}, {"title": "3.1 Neural Layout", "content": "Dense Feature Extraction. Modern foundation models make heavy use of self-attention (SA) and cross-attention (CA) modules [35]. Following the convention by Amir et al. [2], we introduce the query, key, value, and token features available at each attention layer that is associated with different patches of an input image $X_{ref}$. At layer $l$, a SA module takes the tokens of the previous layer $t^{l-1}$, and linearly projects them into queries $q_{SA}$, keys $k_{SA}$, values $v_{SA}$ using matrices $W_q$, $W_k$, $W_v$:\n$q_{SA} = W_q \\cdot t^{l-1}, k_{SA} = W_k \\cdot t^{l-1}, v_{SA} = W_v \\cdot t^{l-1}$.\nCA modules instead compute the features based on another conditioning input $y$:\n$q_{CA} = W_q \\cdot t^{l-1}, k_{CA} = W_k \\cdot y, v_{CA} = W_v \\cdot y$.\nThe resulting features are combined through a series of scaled dot-product attention, normalization, multi-layer perceptron, and residual connections to obtain the token feature $t^l = TransformerBlock(q^l, k^l, v^l, t^{l-1})$.\nOnce extracted, these features are reshaped into a dense features map $f$ corresponding to the original image patches. It was noted by several works [2, 44] that dense features can have different properties depending on where they are extracted. In Sec. 3.3, we detail the conventions we followed for different foundation models we investigated.\nSemantic Separation. Retaining the entire dense feature map $f$ would reveal too detailed information about the reference image $x_{ref}$. Neural semantic image synthesis would then typically lead to samples that are highly similar to $x_{ref}$, lacking diversity (see Fig. 3). To prevent this, it is preferable to separate semantic and spatial features from those that encode appearance details. Based on existing works [22], we hypothesize that the principal directions of variation in the dense features should at least partially correspond to what humans intuitively understand as spatial and semantic image content. Thus, we implement Principal Component Analysis (PCA) to obtain a linear projector that can remove nuisance variations.\nTo obtain the neural layout $c_i$, used as conditioning in LUMEN, we retain only the information in the top $N$ PCA components, where $N$ is a hyperparameter of our method. In practice, we compute the PCA projection on a random sample of 40,000 feature vectors extracted from images in the training set."}, {"title": "3.2 Conditional Image Synthesis", "content": "Neural semantic image synthesis generates the images $x$ based upon text prompts $c_t$ and a neural layout $c_i$. Although this task is compatible with a variety of conditional synthesis frameworks, we build LUMEN upon ControlNet [45], due to its popularity and the ease of comparison, as it is compatible with most existing image conditioning. ControlNet makes use of the frozen Stable Diffusion model [28] for text-conditional image synthesis and adds a trainable adapter to incorporate the conditioning."}, {"title": "3.3 Foundation Model Backbones", "content": "We considered 4 different foundation models for extracting neural layouts $c_i$. This covers features from state-of-the-art methods trained in a self-supervised fashion (DINO [6] and DINOv2 [22]), vision-language models trained contrastively (CLIP [25]), as well as text-to-image synthesis (Stable Diffusion [28]). Since the output resolution of these features varies, all features are upscaled using exact nearest neighbor interpolation to match the image resolution.\nDINO. We follow established work [2] and use the \"key\" feature from SA layers when processing $X_{ref}$ as these features contain information useful for challenging semantic correspondence problems. We use the last SA layer and exclude the key corresponding to the CLS token to compute the neural layout.\nDINOv2. As a scaled-up version of DINO that is trained on a large amount of curated data, DINOv2 potentially contains more generalizable features. Here, we use the non-CLS tokens of the last transformer layer for neural layout [44].\nCLIP. We use the CLIP vision encoder to encoding $X_{ref}$ and keeping all but the CLS tokens from the last feature layer before the final pooling [50]. These features were shown to be well-suited for segmentation tasks.\nStable Diffusion. We first use BLIP [17] to obtain text prompts $c_t$ for the respective image $x_{ref}$, and encode $x_{ref}$ in the latent space of Stable Diffusion 1.5 (SD): $z_0 = E(x_{ref})$. Next, we apply SD's U-Net to predict the noise term $\\epsilon_\\theta (z_0, 0, c_t)$. We extract the intermediate activations from layer 2, 5, and 8 of SD's U-Net and upsampled them to match the resolution of layer 8. All activations are then concatenated across the channel dimension. These SD features [44] were shown to have a strong sense of spatial layout, which makes them a promising candidate for neural layouts."}, {"title": "4 Experiments", "content": "In this section, we first explain our experimental setup and evaluation metrics. Then we show the impact of our design choices for neural layout conditioning in Sec. 4.1. The advantages of neural layouts compared to existing conditioning inputs are demonstrated in Sec. 4.2. Finally in Sec. 4.3, we validate the quality of images synthesized by LUMEN by showing its impact on downstream applications."}, {"title": "4.1 Neural Layout Design Space", "content": "We explore the design space of neural layouts on the diverse COCO-Stuff dataset [4] to determine how to best extract descriptive semantic and spatial information from a given reference image.\nBest Features for Neural Layout. For the task of neural semantic image synthesis, we want neural layouts to preserve semantic and geometry content but to discard appearance details. Thus, we want a backbone that extracts features where this is readily separable. To evaluate this, we compare in Tab. 1a the image quality of DINO, DINOv2, CLIP, and SD features at N = 10 PCA components.\nWe observe that SD features provide the best perceptual image quality and also retain the semantic content best, while DINOv2 is a close second. Although CLIP conditioning can generate more varied images, this diversity is due to the weak semantic and spatial constraints imposed during synthesis. This is likely because CLIP's image-level training objective is less suitable to capture precise pixel-level information without further processing.\nNumber of PCA Components. Since discarding more PCA components removes more information, useful or not, the resulting synthesis task also becomes less constrained (See Fig. 3). This is reflected in Tab. 1b where we compare synthesis results across different numbers of PCA components. We observe a clear"}, {"title": "4.2 Comparison to Existing Conditioning", "content": "We compare LUMEN using neural layouts with N = 20 PCA components against different established image conditioning. For this, we consider the common ControlNet condition: Canny edges [5] (Canny), HED edges [40] (HED), MiDaS depth [27] (MiDaS), predicted semantic segmentation (Sem. Seg. pred), and ground truth semantic segmentation (Sem. Seg. GT). For Sem. Seg. (pred) conditioning, we use the same pretrained network as the one used for evaluating the mIoU metric."}, {"title": "4.3 Downstream Applications", "content": "In this section, we show the applicability of data from LUMEN to different downstream applications. Since image diversity is crucial for model training, N = 10 PCA components are kept in the neural layout.\nCross Domain Multi-Task Training. Being label-free, we can leverage unsupervised fine-tuning on a large dataset in order to augment a labeled training set with diverse variants. We demonstrate this by using LUMEN finetuned on COCO-Stuff [4] to create synthetic variants of training data from NYUv2 [31]. NYUv2 [31] is a small dataset of indoor scenes commonly used for multi-task learning. LibMTL [20], with its standard setting, was used to train a multi-task network based on ResNet-50 [11] to"}, {"title": "5 Conclusion", "content": "We introduced the concept of neural semantic image synthesis and established LUMEN as a strong label-free baseline that can simultaneous specify semantic and spatial concepts of the outputs. Neural semantic image synthesis has additional applications that we hope will be explored in future works. For example, targeted removal of irrelevant information could be possible by replacing PCA with a learned projector specialized to a downstream task. Similarly, one could use neural semantic image synthesis to create images in a target domain from images in a source domain if a projection direction can be learned to eliminate the domain differences. This allows the reuse of labels from one image domain to another in order to save costly annotation efforts."}]}