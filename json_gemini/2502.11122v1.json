{"title": "Hierarchical Expert Prompt for Large-Language-Model: An Approach Defeat Elite AI in TextStarCraft II for the First Time", "authors": ["Zongyuan Li", "Chang Lu", "Xiaojie Xu", "Runnan Qi", "Yanan Ni", "Lumin Jiang", "Xiangbei Liu", "Xuebo Zhang", "Yongchun Fang", "Kuihua Huang", "Xian Guo"], "abstract": "Since the emergence of the Large Language Model (LLM), LLM has been widely used in fields such as writing, translating, and searching. However, there is still great potential for LLM-based methods in handling complex tasks such as decision-making in the StarCraft II environment. To address problems such as lack of relevant knowledge and poor control over subtasks of varying importance, we propose a Hierarchical Expert Prompt (HEP) for LLM. Our method improves the understanding of game situations through expert-level tactical knowledge, improving the processing quality of tasks of varying importance through a hierarchical framework. Our approach defeated the highest level (Elite) standard built-in agent in TextStarCraft II for the first time and consistently outperformed the baseline method[1] in other difficulties. Our experiments suggest that the proposed method is a practical solution for tackling complex decision-making challenges.", "sections": [{"title": "1 Introduction", "content": "StarCraft II, known for its complexity, is commonly utilized as a testing ground for decision-making algorithms, serving as a representative platform for complex decision-making tasks. In 2017, DeepMind, in collaboration with Blizzard, developed the StarCraft II game into StarCraft II Learning Environment [2] (SC2LE).\nTwo years later, AlphaStar [3], an algorithm based on reinforcement learning (RL), defeated the world champion of StarCraft II at BlizzCon, proving that intelligent algorithms first surpass expert-level decision-making abilities in complex tasks. In 2022, an open-source algorithm DI-star[4] was proposed, reaching the highest segmented competition among human masters. Nevertheless, all these methods use a large amount of trajectory data and undergo long-term training on powerful computing devices. At the same time, there is often a lack of trajectory data to train algorithms in other tasks, which limits the deployment of RL-based algorithms in real-world decision-making problems.\nIn November 2022 and April 2023, OpenAI released ChatGPT[5] and GPT-4[6], which enables computers to understand natural language and answer questions. Under this circumstance, the Institute of Automation, Chinese Academy of Sciences, constructed TextStarCraft II[1], an environment that takes text information as input and output, providing a decision-making environment with enough complexity in LLM research. At the same time, they also proposed a decision framework called Chain of Summary (CoS) as a benchmark for decision LLM in TextStarCraft II. Compared to RL-based methods, this method can directly analyze observed information to make decisions without further training, showcasing potential in LLM-based decision-making.\nHowever, the LLM equipped with CoS defeated only the build-in agents ranging from Very Easy to Harder. Yet it failed to secure a single victory in 12 matches against the bots set to the VeryHard difficulty. The following issues are associated with this method: (1)Due to the direct use of StarCraft II knowledge obtained from pre-train, LLM has a weak understanding of resource management, military development, and technology upgrades. (2) The situation analysis and suggestions are relatively rough and cannot effectively handle fine-grained information, ultimately leading to decisions that are vulnerable to mistakes in some critical problems.\nTo resolve these challenges, we propose the Hierarchical Expert Prompt for LLM, which introduces expert tactic knowledge to LLM and improves the quality of processing key issues such as the construction of Nexus and Mineral-Gas management. Contrasted with Imitation Learning, injecting textual knowledge aligns with the customary approach to knowledge representation. It also circumvents the need for the imitation phase which demands a lot of trajectory data and computational resources. This approach effectively incorporates high-quality external knowledge into the model at a minimal expense.\nIn our experiments, we tested our approach in high-difficulty combat scenarios, analyzed time-varying data curves such as resource and supply, and conducted ablation studies on the Expert Tactic Module and the Hierarchical Decision Module. Results show that LLM with HEP can not only defeat VeryHard opponents with a high winning rate but also, for the first time, defeat Elite AI in this environment. Ablation studies show that the two proposed modules are indispensable in improving the quality of LLM-based decision-making."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 StarCraft-II Decision Environment", "content": "Since the inception of StarCraft II, it has been receiving a great deal of attention. With its characteristics of huge state space and action space, multi-agent, incomplete information, long time series, real-time, and good software support, it is recognized as one of the best environments for studying decision-making algorithms.\nIn 2017, the SC2LE learning environment was proposed, followed by the proposal of the SMAC environment[7] two years later. However, these environments do not provide textual observations and cannot recognize textual instructions, limiting the ability to interact with LLMs.\nTo provide an environment for LLM-based decision-making methods, TextStarCraft II was proposed. This environment separates macro decision-making and micro-operations, liberates LLM from high-speed micro-operations by handing over these operations automatically, and enables LLM to focus on macro decision-making. In interactive interfaces, the environment organizes the observed information into textual summaries and can recognize textual instructions, providing favorable conditions for LLM-based methods."}, {"title": "2.2 Decision Making in Multi-Agent Games", "content": "Multi-agent games include two directions of research. One is the study of a simple game with many agents, such as 27Marine-vs-30Marine task, controlling a group of agents to achieve maximum combat capability in the SC2LE. The second is the study of complex decision tasks with fewer agents, such as playing Go[8], Texas Poker[9], Mujoco Football Game[10], and complete battle in StarCraft II.\nIn the study of the first direction, many multi-agent reinforcement learning algorithms have emerged, such as value decomposition (VD) methods such as VDN[11], Qmix[12], Weighted QMIX[13] and so on, and centralized training decentralized execution (CTDE) methods such as MAPPO[14] and MADDPG[15]. These algorithms focus on solving the credit assignment problem of agents in a team, avoiding lazy agent problem, and aiming at maximizing the collaborative efficiency of the group of agents.\nIn the study of the second direction, algorithms such as AlphaGo[8], AlphaStar, and AlphaHoldem[9] showcase the ability to surpass top human players in complex multi-agent games. As a milestone in StarCraft II decision-making, AlphaStar uses two-stage training to acquire decision-making abilities beyond humans, first training on expert trajectories for about two weeks, then applying to more than one month of reinforcement learning to obtain higher abilities.\nHowever, all these algorithms mentioned above face the same problem: the demand for computing resources and training time. Some also require a large amount of high-quality trajectory data for imitation. These problems hinder further development of these methods and may also lead to the Sim-to-Real problem when migrating to other tasks."}, {"title": "2.3 Large Language Model", "content": "LLM usually refers to a language processing network with billions of parameters. In November 2022, OpenAI released the LLM-based chat application ChatGPT. In April 2023, OpenAI released GPT-4, which further improved the performance of LLM with more parameters and training data. After that, more and more LLM emerge, such as Claude-2[16] and 3[17], LLAMA-2[18] and 3[19].\nLLMs have demonstrated remarkable skills in handling textual data, yet LLM-based decision-making systems have not been well advanced. In March 2024, an LLM enhanced with CoS achieved a milestone by securing a 50% winning rate against Harder opponent in TextStarCraft II for the first time. However, this approach cannot defeat the VeryHard and Elite opponents. On their basis, we constructed Hierarchical Expert Prompt, introducing expert knowledge and enhancing the ability to deal with tasks with different priorities for LLM. For the first time, it defeated the TextStarCraft II agent under Elite difficulty, suggesting that the LLM-based method is a feasible solution to apply to decision-making tasks with greater complexity."}, {"title": "3 Methods", "content": "We propose Hierarchical Expert Prompt for StarCraft II decision-making. This approach comprises two primary components: the Expert Tactic Prompt (ETP) and the Hierarchical Decision Prompt (HDP).\nAs shown in Fig 2, HEP consists of a system prompt, an example input prompt, and an example output prompt. In the part of the system prompt, we link the Role Prompt, ETP, HDP, and Legal Action Library together. LLM then absorbs knowledge from ETP and follows the hierarchical decision logic of HDP. The methodology for constructing the HEP is detailed in Algorithm 1, while the interaction process between LLM and environment is illustrated in Algorithm 2."}, {"title": "3.1 Expert Tactic Prompt", "content": "To implement expert-level tactic knowledge, we propose ETP, construct several classic tactics into an expert knowledge database, and inject them into the LLM as part of the system prompt. The LLM then assesses the situation, selects an appropriate tactic, makes decisions based on the chosen tactic, and gathers information. After all aspects are well developed, the LLM agent will launch a decisive attack to defeat the opponent."}, {"title": "3.2 Hierarchical Decision Prompt", "content": "Hierarchical structure is often employed in decision-making problems. Inspired by hierarchical decision-making algorithms such as Decision Tree and Hierarchical Reinforcement Learning, we propose the Hierarchical Decision Prompt. We separate legal actions into two groups with different priorities and ask the LLM to choose actions following the hierarchical decision logic of HDP. The LLM first conducts the necessary analysis, determines whether there are any priorities, selects a set of actions that follow the hierarchical decision logic, and finally generates textual legal actions at the end of the output prompt."}, {"title": "4 Experiments", "content": "All the experiments run on the TextStarCraft-II environment. To compare with the baseline results[1], we used exactly the same setting as in Ref.1.\nThe content of this chapter is arranged as follows. In section 4.1, we tested HEP-Agent fighting against opponents from Hard (Level-4) to Elite (Level-7), calculated the winning rates, and compared the costs of two methods. In section 4.2, we visualized the changes in population and resources, made a detailed analysis of the battle process. In the final section, we conducted ablation studies on proposed modules. Experiments results show that (1)Our method significantly improves the decision-making ability of LLM at a reasonable cost. (2)Both the proposed modules are essential to our method."}, {"title": "4.1 Evaluation of the Hierarchical Expert Prompt method", "content": "In this section, we compared the winning rates and costs of our method with the baseline method. In our experiments, agents with GPT-3.5 back-end played Protoss in TextStarCraft-II against Zerg of varying difficulties. \nAs shown in Table 1, our method increased the winning rate by 16% and 25% on Harder and VeryHard difficulties and managed to defeat VeryHard opponents with a 75% winning rate. It is worth noting that, our method even obtained victories from Elite opponents, indicating that LLM can defeat the highest level standard build-in AI without further training."}, {"title": "4.2 Detailed Analysis", "content": "To figure out how our method defeated VeryHard AI, we visualized key data such as mineral and gas storage, estimation of total collected resources, worker and army supply, the composition of troops, and technological upgrades during the game. All the data visualized here were collected in games of difficulty level-6,\nEconomy We visualized some economic-related data of both methods during the game, including resources and supplies. As shown in Figure 4 and Figure 5, our method managed to collect 20% more mineral than the baseline in 5 minutes and 40% more gas in 8 minutes. Meanwhile, our method can build more Pylons and train more units at a corresponding speed. All of the results above indicate that our method provides a significant economic improvement over the baseline method, and the improvement increases over time."}, {"title": "4.3 Ablation Study", "content": "To prove the effectiveness of our proposed ETP and HDP, we set up two sets of ablation experiments and visualized some key data. Each group of ablation experiments performed three games against the VeryHard opponent. The results in Figure 7 show that only a complete HEP can develop the economy, build a strong army with many carriers, and eventually win the game, suggesting that both the ETP and the HDP module are necessary parts of our method. Without EDP, LLM didn't know it needed to train carriers in the later stage of the game and could not build up the military strength thus losing the game. Without HDP, LLM has been slower to develop its economy, especially in the gas collection, making it impossible to train carriers and lose the game."}, {"title": "5 Conclusion", "content": "In this paper, we introduce HEP for decision-making LLM, inject textual knowledge into LLM, and improve the agent's ability to deal with tasks of varying importance. In experiments, results show that our method significantly improves the winning rates, and both proposed modules are indispensable. Most importantly, we prove that LLM can defeat Elite AI without any further training, revealing the great potential of LLM in dealing with complex decision-making problems."}]}