{"title": "FullStack Bench: Evaluating LLMs as Full Stack Coders", "authors": ["Siyao Liu", "He Zhu", "Jerry Liu", "Shulin Xin", "Aoyan Li", "Rui Long", "Li Chen", "Jack Yang", "Jinxiang Xia", "Z.Y. Peng", "Shukai Liu", "Zhaoxiang Zhang", "Ge Zhang", "Wenhao Huang", "Kai Shen", "Liang Xiang"], "abstract": "As the capabilities of code large language models (LLMs) continue to expand, their applications across diverse code intelligence domains are rapidly increasing. However, most existing datasets only evaluate limited application domains. To address this gap, we have developed a comprehensive code evaluation dataset FullStack Bench focusing on full-stack programming, which encompasses a wide range of application domains (e.g., basic programming, data analysis, software engineering, mathematics, and machine learning). Besides, to assess multi-lingual programming capabilities, in FullStack Bench, we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages to reflect real-world usage scenarios rather than simple translations. Moreover, we also release an effective code sandbox execution tool (i.e., SandboxFusion) supporting various programming languages and packages to evaluate the performance of our FullStack Bench efficiently. Comprehensive experimental results on our FullStack Bench demonstrate the necessity and effectiveness of our FullStack Bench and SandboxFusion.", "sections": [{"title": "1. Introduction", "content": "The code large language models (LLMs) have achieved significant improvements in code intelligence [Roziere et al., 2023, Zheng et al., 2023, Guo et al., 2024a, Hui et al., 2024, Huang et al., 2024b], which are pre-trained on extensive datasets comprising billions of code-related tokens. Recently, to discover the limitations of existing code LLMs and facilitate further development of code intelligence, many code evaluation benchmark datasets (e.g., HumanEval [Chen et al., 2021a], MBPP [Austin et al., 2021b], DS-1000 [Lai et al., 2022], xCodeEval [Khan et al., 2023]) have been proposed as shown in Figure 1.\nHowever, as shown in Figure 1, we observe that the existing benchmarks cover limited application domain types, which cannot access the code-related abilities of the real-world code development scenarios. Specifically, in Figure 1, we sample 500k questions from the widely-used software development community (i.e., \u201cStackOverflow\u201d) and tag the application domain label for these questions based on GPT-4 [Achiam et al., 2023]4. Then, based on the labels on \u201cStackOverflow\u201d, we summarize 11 main-stream application domains (e.g., Basic Programming, Software Engineering, Data Analysis), which cover about 88.1% problems in \u201cStackOverflow\u201d. Meanwhile, using these domain labels, we also tag four popular code evaluation datasets (i.e., HumanEval, MBPP, DS-1000, xCodeEval), and observe that these benchmarks usually focus on very limited domains. For example, a large portion of DS-1000 (>95%) is related to data analysis and machine learning tasks, and even the so-called multi-task benchmark xCodeEval (with code understanding, generation, translation and retrieval tasks) mainly focuses on advanced programming and mathematics domains.\nTo address the abovementioned limitation, we propose the FullStack Bench, an evaluation set spanning multiple computer science domains and programming languages, which aims to assess large models' capabilities across various real-world code development scenarios. As shown in Figure 1, when compared to existing benchmarks, our FullStack Bench covers more application domains, which demonstrates the diversity and necessity of our FullStack Bench. Besides, based on the analysis of StackOverflow, we observe that our FullStack Bench can simulate StackOverflow well for real-world programming scenes, where ratios of the selected 11 application domains (excluding \u201cOthers\u201d) for our FullStack Bench and StackOverflow are 94.3% and 88.1%, respectively.\nMoreover, automating the evaluation on FullStack Bench is challenging due to the various data formats and dependencies for different application domains and programming languages. Recently, some sandbox execution environments (i.e., DifySandbox [LangGenius, 2024], MultiPL-E [Cassano et al., 2023], MPLSandbox [Dou et al., 2024]) have been proposed. However, there are significant limitations (e.g., supporting limited packages and programming languages) in these sandboxes, which cannot evaluate our FullStack Bench well. For example, the front-end browsers and deep-learning packages (e.g., PyTorch [Paszke et al., 2019], Tensorflow [Abadi et al., 2015]) are not supported in these sandboxes. Besides, our FullStack Bench has 16 programming languages (i.e., Bash, C++, C#, D, Go, HTML, Java, Javascript, PHP, Python, R, Ruby, Rust, Scala, SQL, Typescript), and many sandboxes do not fully support these languages. Therefore, we also introduce a new execution environment (i.e., SandboxFusion) to support the evaluation on our FullStack Bench, and the main features of SandboxFusion are as follows: (1) Supporting various languages: our SandboxFusion supports 23 commonly-used programming languages, which satisfies different real-world usage scenes (e.g., front-end development, backend development,"}, {"title": "2. FullStack Bench", "content": "As illustrated in Table 1, the FullStack Bench consists of 3374 problems, where each problem in FullStack Bench includes question, unit test cases, reference solution, and unit test cases. Besides, we also calculate the token lengths of the question and correct code using the LLaMA3 tokenizer [Team, 2024], where the average question length is 210.2 tokens. To ensure judgment"}, {"title": "2.1. Data Overview", "content": "As illustrated in Table 1, the FullStack Bench consists of 3374 problems, where each problem in FullStack Bench includes question, unit test cases, reference solution, and unit test cases. Besides, we also calculate the token lengths of the question and correct code using the LLaMA3 tokenizer [Team, 2024], where the average question length is 210.2 tokens. To ensure judgment"}, {"title": "2.2. Data Construction and Quality Control", "content": "To curate the multilingual full stack code evaluation benchmark FullStack Bench, we employ a comprehensive and systematic human annotation process for producing code samples of different application domains, where meticulously pre-defined guidelines are provided to guarantee accuracy and consistency.\nSpecifically, as shown in Figure 3, we il-lustrate the overall dataset construction pro-cess. Specifically, we first collect code snip-pets from Github, code-related documents (e.g., blog and Book) and XLCoST [Zhu et al.,2022]. Then, we use LLM and human ver-ification to generate the instruction, unitcases and corresponding reference solution.Besides, we also employ programming ex-perts actively in each field to create domain-specific questions for LLMs. These questionsdo not involve proprietary information, butare designed to assess essential skills in therespective application domains, similar tointerview questions. For example, we en-gaged our internal data engineering team todevelop a series of data analysis questions,including data filtering, data mining, anddata visualization. After obtaining the initialdataset, to improve the annotation quality, the annotators evaluate the annotated code based onthree criteria: problem difficulty, ambiguity and solveability. Furthermore, after completing theirannotations, each annotator exchanges data with another annotator for cross-refining, aiming"}, {"title": "2.3. Bilingual Benchmark Construction", "content": "The collected questions are in Chinese or English. For Chinese or English problems, we translate these problems into English or Chinese, which results in both Chinese and English versions. Finally, in FullStack Bench, the numbers of Chinese and English problems are both 3374/2 = 1687."}, {"title": "2.4. Evaluation Metrics", "content": "Following HumanEval and MBPP, we directly use the Pass@1 as the default evaluation metric for our proposed FullStack Bench."}, {"title": "3. SandboxFusion", "content": "Execution-based datasets are crucial for discriminating code generation tasks [Hendrycks et al., 2021]. Automating the evaluation of these datasets requires extracting complete code from the model's responses, and executing it in a compatible environment. This is a complex task due to the varying data formats and dependencies. To facilitate the evaluation of FullStack Bench, we also propose the SandboxFusion execution environment. SandboxFusion is a unified architecture that is compatible with many datasets as well as Fullstack Bench. This makes the sandbox widely applicable for data processing, model evaluation, reinforcement learning, etc."}, {"title": "4. Experiments", "content": "We select 27 popular (code) language models as full-stack AI coders and test them with FullStack Bench. For open-sourced models, we select AI coders from well-known and uprising code LLM series, including CodeQwen1.5 [Bai et al., 2023], Qwen2.5-Coder [Hui et al., 2024], DeepSeek-Coder [Guo et al., 2024b], Deep-Seek-Coder-v2 [Zhu et al., 2024b], CodeLlama [Roziere et al., 2023], Yi-Coder [Young et al., 2024], StarCoder2 [Lozhkov et al., 2024], and OpenCoder [Huang et al., 2024a]. Further, we involve two open-sourced general LLMs, Qwen2.5 and Llama3.1 [Team, 2024], into the comparison. As the majority of problems in FullStack Bench are complex natural language instructions, we adopt the instruction-tuned version of those AI coders rather than their base models. According to the model size, we categorize the AI coders into five groups: 1B+, 6B+, 13B, 20B+, and 70B+.\nOn the other hand, we also evaluate some prominent close-sourced LLMs including GPT-4, OpenAI-01, Claude, GLM4, DeepSeek-v2.5, Qwen-Max, and the upcoming Doubao-Coder-"}, {"title": "4.1. Experimental Setup", "content": "We select 27 popular (code) language models as full-stack AI coders and test them with FullStack Bench. For open-sourced models, we select AI coders from well-known and uprising code LLM series, including CodeQwen1.5 [Bai et al., 2023], Qwen2.5-Coder [Hui et al., 2024], DeepSeek-Coder [Guo et al., 2024b], Deep-Seek-Coder-v2 [Zhu et al., 2024b], CodeLlama [Roziere et al., 2023], Yi-Coder [Young et al., 2024], StarCoder2 [Lozhkov et al., 2024], and OpenCoder [Huang et al., 2024a]. Further, we involve two open-sourced general LLMs, Qwen2.5 and Llama3.1 [Team, 2024], into the comparison. As the majority of problems in FullStack Bench are complex natural language instructions, we adopt the instruction-tuned version of those AI coders rather than their base models. According to the model size, we categorize the AI coders into five groups: 1B+, 6B+, 13B, 20B+, and 70B+.\nOn the other hand, we also evaluate some prominent close-sourced LLMs including GPT-4, OpenAI-01, Claude, GLM4, DeepSeek-v2.5, Qwen-Max, and the upcoming Doubao-Coder-"}, {"title": "4.2. Results and Analysis", "content": "We conduct a systematic evaluation of those AI coders with FullStack Bench. Results across the 11+ real-world domains are presented in Table 2. Owing to the powerful reasoning capability, OpenAI o1-preview unsurprisingly leads the board. However, the dominant position of closed-sourced models has been challenged, with some closed-sourced models being matched or even surpassed by pioneers in open-sourced ones. DeepSeekCoder-v2-Instruct, a 236B-MoE model, is the best behavior of open-sourced models, which pulls away the runner-ups in AP, OS, and Others. OpenCoder-1.5B-Instruct, Qwen2.5-Coder-7B-Instruct, and Qwen2.5-Coder-14B-Instruct achieve the top spot in their groups and outperform some models in the closest higher level.\nAs illustrated in Figure 5, we visualize the model performance on different domains in FullStack Bench. From Figure 5a, we could find that the performance of AI coders varies significantly in BP, AP, MA, ML, and MM. The largest range occurs in MA, with the best mathematician being OpenAI o1-preview (80.42) while the worst is CodeLlama-34B-Instruct (14.34). Mathematical programming requires models to be proficient in both math and code, and those trained on a code-highly-concentrated corpus would struggle to achieve high scores in MA. Similarly, the variances of ML, SC, and MM, are also remarkable, as each of these problems requires domain knowledge beyond coding.\nIn Figure 5b, we visualize the first place in each model division. The performance trends of Qwen2.5-Coder-7B-Instruct, Qwen2.5-Coder-14B-Instruct, DeepSeek-Coder-v2-Instruct, and Qwen2.5-72B-Instruct are generally consistent. Except for OpenAI o1-preview, the results of all models on SC lie in the trough. Besides, the curve of OpenCoder-1.5B-Instruct goes in a different direction than others, which indicates that the model might be trained on distinctive distributed data."}, {"title": "4.3. Analysis on the performance of different programming languages", "content": "We present results of different programming languages in Table 3 and Figure 6. The majority of the AI coders are able to solve Bash programming well. Whereas the performance of C++, C#, and Ruby varies more, suggesting that they are selectively included in the training corpus by the model designers. Worser, some coders, especially those 1B+ tiny models behave poorly on D, R, and Scala as their pass rates are lower than 10%.\nOur SandboxFusion provides feedback from the compilers. We evaluate the compilation pass rates of model responses in compiled languages including C++, D, Java, Rust, and Scala. As depicted in Figure 7a, there is a positive correlation between the compile pass rate and the"}, {"title": "4.4. Scaling Laws on FullStack Bench", "content": "We categorized the model into 5 series based on the criteria in Table 4 of Appendix and visualize the performance of different model series in Fig-ure 8. As the parameter increases, performancegains are achieved for most model families. Whilethe scaling law seems to fail on Qwen2.5, the per-formance peaks at Qwen2.5-Coder-14B-Instruct,while 32B and 72B show performance drops. Thisphenomenon is also reported in their original pa-per [Hui et al., 2024]."}, {"title": "4.5. Analysis on the performance of different difficulties", "content": "Figure 9 presents model performance on different difficulties. As stated in Section 2.2, we implement a voting method with the involvement of six AI coders to determine the difficulty of each problem. Overall, the 1B+ models and CodeLlama series are less effective on all difficulty levels. While the rest of the models could solve simple problems equally well, and gaps appear in the medium questions. As for hard questions, the closed-source models generally outperform the open-source coders."}, {"title": "4.6. Analysis on the effect of feedback from SandboxFusion", "content": "As shown in Figure 10, to demonstrate the effectiveness of the feedback using SandboxFusion, we compare the \u201cReflection\u201d and \u201cBoN\u201d strategies. For \u201cReflection\", we reproduce the self-refine strategy [Madaan et al., 2024] by refining the answers of N times using the feedback context of SandboxFusion. For \u201cBoN\u201d, we just infer N times to obtain the results. In Figure 10, we observe that the \"Reflection\" is better than \u201cBoN\" a lot, which demonstrates the effectiveness of the feedback context provided by the SandboxFusion.\""}, {"title": "5. Related Works", "content": "Code Large Language Models. Code large language models (LLMs) [Chen et al., 2021b, Zhao et al., 2024, Black et al., 2021, 2022, Le et al., 2022, Chowdhery et al., 2023, Nijkamp et al., 2023, Fried et al., 2023, Xu et al., 2022, Sun et al., 2024, Hui et al., 2024] has shown powerful capabilities in code generation [Li et al., 2022, Allal et al., 2023], code debug, code translation [Zheng et al., 2023, Li et al., 2023], and other coding tasks, which is essential for modern software engineering. For example, there is a wide variety of in-file benchmarks to evaluate different capabilities of code LLMs [Zheng et al., 2023, Austin et al., 2021b, Jain et al., 2024], which focus on a limited range of programming languages (e.g. Python and Java). Further, recent code LLMs such as Code Llama [Roziere et al., 2023], DeepSeek-Coder [Guo et al., 2024a], and Qwen2.5-Coder [Hui et al., 2024] gains remarkable progress in multilingual programming code generation and debugging tasks, such as MultiPL-E [Cassano et al., 2022], McEval [Chai et al., 2024], and MdEval [Liu et al., 2024b].\nCode Benchmark. Program synthesis is an important task for code LLM, which forces the LLM to read the natural language description and then generates the corresponding code snippet meeting the user requirements [Athiwaratkun et al., 2023, Austin et al., 2021a, Gu et al., 2024,"}, {"title": "6. Conclusion", "content": "In this paper, we provide a more holistic evaluation framework FullStack Bench with a corresponding effective execution environment SandboxFusion for code intelligence, which aims to evaluate multilingual programming capabilities in real-world code development scenarios. Specifically, first, our FullStack Bench mainly involves mainstream application domains (e.g., basic programming, software engineering, and machine learning ) from 3374 problems, where each problem has corresponding unit test cases. Second, our SandboxFusion has three distinct features (i.e., Supporting various languages, Easy-to-deploy and Unified multi-dataset execution environment), which can satisfy the requirements of evaluating FullStack Bench. Finally, we hope that FullStack Bench could guide the researchers to better understand the code intelligence abilities of existing LLMs and accelerate the growth of foundation models."}, {"title": "A. Appendix", "content": null}, {"title": "A.1. Visualization on the cases of FullStack Bench", "content": null}, {"title": "A.2. Details of SandboxFusion", "content": null}, {"title": "A.2.1. Dataset Module", "content": "The sandbox aims to provide a unified framework for most execution-based datasets, making it easier to add existing datasets or create new datasets based on them. In particular, SandboxFu-sion implements many open-source code evaluation datasets, including HumanEval [Chen et al., 2021a], MultiPL-E [Cassano et al., 2022], Shadow Humaneval [Wei et al., 2023], CodeContests [Li et al., 2022], MBPP [Austin et al., 2021b], MBXP [Chai et al., 2024], MHPP [Dai et al., 2024], CRUXEval [Gu et al., 2024], NaturalCodeBench [Zhang et al., 2024], PAL-Math [Gao et al., 2023], verilog-eval [Liu et al., 2023b] and miniF2F [Zheng et al., 2022]. Besides, as the judgment logic of most datasets is very similar, to maximize code reuse, we created two representative types of datasets: AutoEvalDataset and CommonOJDataset as follows:\n\u2022 AutoEvalDataset. This type of dataset is primarily prepared for instruction-tuning models and can also be used to test the performance of pre-trained models through few-shot learning. The basic process is to extract the entire code block from the model's output, then concatenate the model code with pre-written test scripts and execute them, and finally check the return value of the executed program to determine whether the problem is passed. Note that we support FullStack Bench in this dataset mode.\n\u2022 CommonOJDataset. Most algorithmic competition problems belong to this category. These problems are language-agnostic and can be tested in any programming language. Most"}, {"title": "A.2.2. Sandbox Execution Module", "content": "SandboxFusion covers a wide range of programming languages that have received attention in the field of code generation and provides a unified interface for executing them. Currently, SandboxFusion supports a total of 23 programming languages, including Python, C++, C#, Go, Java, NodeJS, TypeScript, PHP, Rust, Bash, Lua, R, Perl, D, Ruby, Scala, Julia, Kotlin, Verilog, Lean, Swift, Racket, and CUDA. Together with these language compilers and interpreters, SandboxFusion also integrates many commonly used packages like PyTorch and TensorFlow for machine learning and a browser for front-end related application domains. The main features of the Sandbox Execution Module are as follows:\n\u2022 Our execution interface accepts a code snippet, language, and other input contexts, and returns the program's return code, standard output, and other information. This inter-face hides many language-specific implementation details from researchers and greatly improves the efficiency of data cleaning, compiler feedback, etc.\n\u2022 When dealing with incompatible versions of languages and packages, our principle is to select more advanced and popular versions. For example, while there are still many existing Lean codes written in Lean 3 on Github, we still use Lean 4 as the Lean community has fully transitioned to it. In cases where an older version of some package is indeed necessary, SandboxFusion also provides a function to run code in an isolated execution system to avoid affecting the main environment.\n\u2022 SandboxFusion provides functionality in some task-specific areas. For example, sandbox supports running CUDA and Python code in a GPU environment, making it possible to explore enhancing the performance of CUDA code. Another example is the supported Jupyter mode, which executes multiple code blocks in sequence, providing training signals for online user interaction.\n\u2022 SandboxFusion also has a built-in resource isolation functionality. Given limiting CPU and memory usage, it is possible to test a program's performance in a controlled environment, providing a reliable metric for program optimization. Its file system isolation allows testing scenarios that require file operations without affecting the host system, while network isolation enables multiple programs to bind to the same port simultaneously, preventing conflicts during concurrent testing."}, {"title": "A.3. Comparison with Other Sandboxes", "content": "For comparison, we selected three representative sandboxes (i.e., DifySandbox, MultiPL-E, and MPLSandbox) as follows:\n\u2022 DifySandbox [LangGenius, 2024] represents online code execution sandboxes with pow-erful and fine-grained security controls. In contrast, SandboxFusion, primarily used for internal model code evaluation, has lower security isolation requirements and thus only implements comprehensive isolation through basic Linux kernel capabilities like names-paces and cgroups, without providing fine-grained permission controls. Additionally,"}]}