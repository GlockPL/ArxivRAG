{"title": "Transfer Learning Applied to Computer Vision Problems: Survey on Current Progress, Limitations, and Opportunities", "authors": ["AARYAN PANDA", "DAMODAR PANIGRAHI", "SHASWATA MITRA", "SUDIP MITTAL", "SHAHRAM RAHIMI"], "abstract": "The field of Computer Vision (CV) has faced challenges. Initially, it relied on handcrafted features and rule- based algorithms, resulting in limited accuracy. The introduction of machine learning (ML) has brought progress, particularly Transfer Learning (TL), which addresses various CV problems by reusing pre-trained models. TL requires less data and computing while delivering nearly equal accuracy, making it a prominent technique in the CV landscape. Our research focuses on TL development and how CV applications use it to solve real-world problems. We discuss recent developments, limitations, and opportunities.", "sections": [{"title": "Introduction", "content": "Storage capacity and computation power have increased considerably in recent times, especially with the expansion of the Internet and cloud services. Artificial Intelligence (AI) is one of the prominent beneficiaries of this expansion. However, even with these impressive developments, the AI models struggle with a lack of data and computation power. As a result, a company that harnesses its powers ends up in an advantageous competitive position, as vouched by 77% of businesses in a Verizon study [5]. In addition, the volume of data available to a company has exploded with the increasing use of the Internet. The Internet traffic volumes increased by 30% in 5 years, from 2017 to 2022, as reported by Nokia [33]. Abundant data and computing now empower researchers and companies to try solving complex problems that were not possible earlier. One such category is Computer Vision (CV) problems which deal with image processing [52]. Data volume is critical in Computer Vision problems using Machine Learning (ML), where an ML model learns better with more images. However, obtaining training data can be difficult and expensive for certain CV problem domains. Thus, there is an effort to reuse a trained ML model in one CV domain and apply it to a related CV domain. This effort of re-utilizing a model trained for one use case and applying it to another use case is called Transfer Learning (TL) [56]. Although TL can be applied to many problem domains, such as Natural Language Processing (NLP) using different techniques, such as genetic algorithms, our research paper focuses only on TL applied to CV problems.\nIn the current research paper, we introduce the concepts of TL and CV, review some research papers that study these topics, and provide a gist of our research work in the background, Review"}, {"title": "Background", "content": "In this section, we attempt to provide a preliminary understanding of the topic with relevant background. Following we outline the relationship among Artificial Intelligence (AI), Machine Learning (ML), Neural Networks (NN), Deep NN (DNN), Convolution NN (CNN), Recurrent Neural Networks (RNN), and Transfer Learning (TL). Refer to Fig. 1 for enhanced reader comprehension. To maintain the scope of our research, we will limit the background section to the necessary details.\nArtificial Intelligence (AI) is a field of science that utilizes machines to learn and simulate various aspects of intelligence [8] to solve real-world problems. The branch is divided into rule- based Expert Systems (ES), Fuzzy Systems, and Machine Learning (ML). Expert Systems (ES) emphasizes mimicking the decision-making abilities through creating rule-based algorithms on human reasoning/logic [50]. Contrary to this, fuzzy logic is a mathematical/statistical approach that deals with reasoning based on degrees of truth, rather than the traditional Boolean logic of true or false. It provides a framework for dealing with uncertainty and imprecision and more human-like decision-making in real-world problems depending upon the data. The marriage of the previous two introduces the development of Machine Learning (ML), which focuses on learning from existing data sets and making predictions/decisions through the development of algorithms and statistical models without explicit rule-based programming. There are different ML sub-fields/classifications. One such classification [31] is supervised learning (e.g., Linear Regression, Logistic Regression),"}, {"title": "Existing Research in Transfer Learning", "content": "In this section, we present reviews of selected research work on the genesis of TL and its application in CV problems. The list of papers we studied in different areas are summarized in the Table 1.\n3.1 What is Transfer Learning (TL)? (Pan et al. [45] and Weiss et al. [56])\nHumans, by instinct, correlate knowledge learned from one domain and apply it to similar ones. For example, when one is fa- miliar with a programming language, such as Java, they are already aware of con- cepts such as variables, data types, func- tions, loops, etc. They can learn relatively easily another new language, e.g., Python, by reusing the concepts they have already learned. Thus, they transferred the knowl- edge gathered while programming Java to learn Python. However, a computer does not inherently transfer knowledge acquired in one application to another. In this sec- tion, we study the definition of TL, with its advantages, and types. Weiss et al. [56] used the notations in Table 2 to define TL.\nFirst, we define the domain and task we use to define TL. A domain D consists of two parts: input feature space X and a marginal probability distribution P(X), where X = {X1, X2, ..., Xn} \u2208 X and x\u1d62 is the ith feature vector and n is the number of feature vectors in X, which is a particular learning sample in a set of all samples X. Thus D = {X, P(X)}. A task also consists of two parts: a label space Y and an objective function f(.). Thus, T = {Y, f(.)}. f(x) predicts a label for a new instance x. f(.) learns from existing learning data pair {xi, yi }\u2200i \u2208 {1, 2, 3, . . ., N}, where N is the total number of samples. Thus f(x) = P(y|x).\nSecond, we give an example to comprehend the notations introduced so far. If our learning task is to classify a document in a binary form, then each term is taken as a binary feature. The input space X is the set of all term vectors. x is a particular document or a learning sample. x\u1d62 is the ith term vector of x. When we have, say, 1000 labeled documents, then we have the following data pairs for learning: {xi, yi}\u2200i \u2208 {1, 2, 3, . . ., 1000}, and we have Y as all labels either True or False for all 1000 documents i.e., a vector of length 1000 of 0s or 1s and Y\u1d62 represents the label of document xi at ith position.\nThird, for simplicity, we assume that there is only one source, Ds, and one target domain, DT and define TL as the following: Given Ds, Ts, Dr and T\u2020, TL aims to improve fT(.) in D\u2081 using knowledge in Ds and Ts, where Ds \u2260 D\u012b or Ts \u2260 T\u2081 and DS is related to Dr. We describe related as either explicit or implicit relationship between feature spaces of Ds and Ts.\nFourth, in our documentation classification example, Ds \u2260 D\u012b arises when Ds and D\u012b represent two different languages. TS \u2260 TT occurs when Ds and D\u012b represent two different topics but of the same language. Ts \u2260 T\u012b also implies that Ys \u2260 Yr occurs or P(Ys|Xs) \u2260 P(YT|XT), where Ys\u1d62 \u2208 Ys and YT; \u2208 YT. Ys \u2260 Yr happens, for example, when Ys is binary and Yr is multi (e.g. 10 classes.) P(Ys|Xs) \u2260 P(YT|XT) emerges when the source and target documents have unbalanced features.\nFifth, the TL that we defined is using supervised learning strategy. There are other strategies for TL such as informed supervised learning [13] and semi-supervised learning [17]. Moreover, TL has many categories such as Inductive, Unsupervised, and Transductive using different approaches based on what to transfer, how to transfer, and when to transfer. In addition, TL could also be categorized based on different learning approaches e.g., instance-based, parameter-based, etc. [59]"}, {"title": "TL to find crack in pavements (Khaitan et al. [18])", "content": "The authors [18] conducted their work based on the idea that using deep learning-trained models on large scale image datasets such as ImageNet [32] and transferred their ability to a new classification on a new dataset, from another domain, which is cheaper and more efficient than training the model from scratch [2]. Therefore, they truncated a VGG-16 model that is developed by the Visual Geometry Group (VGG) at the University of Oxford using the ImageNet dataset. However, they only removed the last fully connected classifier layers, as shown in Fig. 6. They used the truncated model with their input images to produce a new binary classifier model that predicts if an image has a crack or not.\nUsing this approach, the authors developed a model to automatically detect cracks in pavements using TL in a six-step method. First, they pre-processed the raw pavement images sampled from a publicly available dataset [11]. They also included images with lane markings, oil spills, etc.,"}, {"title": "TL to improve tree image classification (Caceres et al. [29])", "content": "Caceres et al. [29] stated that they leveraged TL to increase the quality of the trained model with a limited dataset. They detected invasive trees among the classified trees in the forest images taken by drones. However, they only could gather a limited dataset because of the hardship of collecting data due to natural phenomena such as inclement weather (e.g., rain.) Moreover, they concluded that TL was critical in building an ML model that predicts accurately for their domain. In addition, they concluded that using a data set similar to their dataset in the TL process improved the model accuracy.\nThe researchers followed a five-step approach:\n(1) They developed a model reusing the ResNet50 [22] architecture to classify tree species using the Multi-Label Patch algorithm.\n(2) They analyzed the model quality by testing with the limited data set.\n(3) They studied the learning rate impact on the ML model by adopting three approaches, namely,\n(a) without using TL, (b) using TL once with ImageNet [32], and (c) using TL twice: once with Planet Satellite [6] data set and second with the domain dataset.\n(4) They improved the accuracy and the learning time (by reducing the computation) of the ML model, tuning it by the conclusions from the analysis.\n(5) They applied the model to detect invasive tree species in a coastal forest."}, {"title": "TL in medical image segmentation (Karimi et al. [28])", "content": "In medical image segmentation research Karimi et al. [28] indicated that although there are different TL techniques, such as Deep Adaption Networks [34] and few/zero-shot learning [57], the favored technique for vision applications is to pre-train a model on a source domain and fine-tune the model on a target domain. It is the same approach that the other research papers have employed that we have reviewed.\nThe authors made the following observations:\n\u2022 TL reduces the training time on the target task, T of target domain, Dr. Moreover, the accuracy depends on the data, and the accuracy improvement is marginal.\n\u2022 The model parameters used in the TL process remain relatively the same, regardless of using either randomly-initialized or pre-trained values.\n\u2022 The convolution filters remain almost unchanged during the TL training phase.\n\u2022 The model accuracy increases by freezing the network encoder section and training only the decoder section.\n\u2022 TLs produce different FCN representations than the FCNs with random initialization. However, the variability in the FCN set by TL can be as high as the FCN set by random initialization.\n\u2022 Feature reuse can be more significant in the deeper layer than in the earlier encoding layers.\n\u2022 Applying TL to different model architectures yields similar results.\nThe researchers made these observations by applying TL to the medical image segmentation issues. They leveraged 16 datasets containing 11 body parts for the model training. Next, they preprocessed the data by resampling and normalizing the datasets. Finally, they used 70% of the combined dataset for model training and validation and the rest for model training. Their experiment used four networks, HRNet [55], UNet++ [58], Tiramisu [27] (a modified DenseNet [26],) and Autofocus [47], as is, and a modified V-Net [35] version to deduce those observations. The modified version has six layers. It also has three encoding and three decoding layers. They used the 'Adam' optimizer with an initial learning rate of 10\u207b\u2074, which was reduced by 0.9 after every 2000 training iterations if the loss did not decrease. In addition, they initialized the convolution filters with zero-mean and $\\sqrt{2/n}$, where n is the number of connections to the convolutional filter from the"}, {"title": "TL to recognize Psoriasis skin disorder (Hridoy et al. [23])", "content": "Hridoy et al. [23] concluded study to recognize Psoriasis skin disorder and found that TL reduces ML model training time while improving accuracy over a model built from scratch. They reused EfficientNet pre-trained ML models [53] to reclassify a skin order dataset to recognize a Psoriasis type from 12 classes. EfficientNet models are trained on ImageNet [32], which has more than 1.2 million images. Their methodology followed the steps mentioned below:\n(1) They gathered and labeled 6,300 sample data.\n(2) They synthesize the sample data to augment their original image set to 52,500. They used image transformation techniques such as rotation and horizontal and vertical flips.\n(3) They pre-processed sample data and reshaped the sample images into eight different dimen- sions.\n(4) They selected all 8 EfficientNet models. All layers used 'Swish' as the action function except the last fully connected layer, which used 'softmax. The output layer has 1000 inputs and 21 outputs that match the 21 Psoriasis classes. The authors used the 'early stopping' method while training the NN. They also used 'adam' optimizer. They used 42,00 samples to train, 4,200 to validate, and 6,300 to test.\n(5) They trained the NN and gathered the ML performance confusion metrics. They calculated the sensitivity, specificity, accuracy, and precision metrics.\n(6) They concluded that the ML model accuracy was 97.1% which is better than the accuracy of several other ML models, as shown in the Table 3."}, {"title": "TL to recognize human activity (Deep et al. [7])", "content": "The authors in [7] concluded that employing TL to extract deep image features and leverage an existing pre-trained model yielded a model with 96.95% accuracy. Moreover, they mentioned that the accuracy of the model using TL performs 1 - 6% better than the results achieved from other approaches. However, they alerted that their technique might be flawed because they are using pre-trained ImageNet [32] weights containing images of several categories. The trained ML model classifies seven human activities: bend, back, jump, run, side, skip, walk, wave1 and wave2. First, they choose the Weizmann dataset [19] for their ML model, which consists of 9 people performing ten activities. Second, they pre-processed the video and extracted individual frames from the videos. They collected 4,917 frames for all seven activities. They used 70% of the frames to train, 10% to validate, and 10% to test the ML model. Third, they experimented with three CNNs, namely, VGG-16, VGG-19, and Google's InceptionNet-v3. Thus, they leveraged the knowledge from the large-scale ImageNet dataset. They extracted the features from the penultimate CNN layers. Finally, they created a confusion matrix and using it, they calculated the accuracy, precision, recall, and f1-score for the three CNNs, as shown in the Table 4."}, {"title": "TL in Genetic Programming (Thi Thu Huong Dinh et al. [9])", "content": "Thi Thu Huong Dinh et al. [9] applied TL to Genetic Programming (GP) by taking final-generation individuals, or algorithmic solutions, from the source task to the target task as first-generation individuals. They compared the results of the TL-assisted GP and GP without any help from TL, which is called standard GP. They found that it improved training errors, especially with unseen data, compared to standard GP. Furthermore, they claimed that limiting the transferred individuals reduced unnecessary code growth, also called code bloat. Thus, the technique helped them to avoid negative transfer, or when transferring does not aid the GP's performance. They used a parameter-based GP model which shared some source and target parameters and used different configurations of k, where k is the variable of the parametric values, either expressing a percentage of individuals or the number of individuals transferred. The three used TL methods are following:\n(1) FullTree: k% of best individuals from the last generation of the source problem are transferred to the first generation of the target problem.\n(2) SubTree: From k% of best individuals, a random subtree is chosen from each last- generation individual of the source problem, pooled, and transferred to the target problem as first- generation individuals.\n(3) BestGen: k best individuals from each generation of the evolutionary process are taken, pooled, and transferred like the other methods.\nThe experiment was conducted over two families of regression problems: polynomial and trigonometric. The target and source problems are similar, but the target problem is more complex. For Full-Tree and Sub-Tree, the authors used k values of 25, 50, 75, and 100. With BestGen, they had two experiments. The first is the non-restricted experiment, where they transferred k best individuals in this case, k being the number of individuals; k = 1, 2\u2013from each generation of the source problem to the target problem, with no limit on the number of nodes that make up the individuals. The second is the restricted experiment, where they restricted the transferred individuals' sizes to a maximum of 50 nodes but followed the same procedure as the first experiment. Finally, the authors concluded the experiment with three tests: training error, GP's generalization ability, and code bloat.\n(1) Training Error: FullTree performed its best at k=25. However, compared to standard GP, it did not do well with trigonometric problems. SubTree and BestGen both had small training errors with SubTree performing its best at k=25 and 50. BestGen (restricted and non-restricted) performed consistently with all four configurations.\n(2) Generalization Ability: TL-assisted GP was better than standard GP overall. Their testing errors, especially with SubTree and BestGen, had been significantly better when faced with unseen data.\n(3) Code Bloat: FullTree incurred higher code growth than standard GP, showing negative transfer at times. SubTree and the restricted BestGen, however, bloated less than standard GP. Thus, restricting the transferred individuals' sizes helped TL reduce code growth."}, {"title": "Discussion", "content": "Although CV problems have existed for a while, recent data and compute availability have sparked interest in revisiting them. However, obtaining sufficient data to train an ML model from scratch remains a challenge, as an ML model for CV requires many examples for training. Gathering enough data may not be possible in specific domains. For illustration, there might not be enough image scans available for a relatively newly discovered disease in health care. Similarly, there are often financial or time constraints when using nearly infinite computing, especially with cloud computing. Developing a usable ML model with a shorter training time is cost-effective. Researchers and academicians have proposed several techniques to accomplish this by using proper hyperparameters, such as early training stopping when the algorithm learning drops below a certain threshold [4].\nHence, considering limited data availability and efficient compute usage, one needs to train usable ML models. One method to do so is the transfer learning. In this survey paper, we studied several computer vision problems that leverage TL to solve them. Khaitan et al. [18] concluded that training a base model already trained on large-scale image data with a limited pavement crack data proved cheaper. Caceres et al. [29] had to use TL to train a model from a base model as they had limited evasive plant data. Karimi et al. [28] have noticed significantly less training time and increased model accuracy when they used TL to train on limited medical image data. Similar conclusions were drawn by Hridoy et al. [23], where they had only 6,000 samples of psoriasis skin disorder to train their model. Deep et al. [7] also demonstrated that using TL produced a model with up to 6% better performance than other approaches. Hence, the TL method is a proven technique to train a model with limited data for the CV domain. The method also has a benign advantage: cheaper."}, {"title": "Conclusion", "content": "In this paper, we reviewed Transfer Learning, an ML model training optimization technique applied to the Computer Vision problems. TL re-purposes a pre-trained model on a similar problem domain with a different task. TL also requires fewer samples than a model trained from scratch does. It reuses (freezes) most of the layers of the trained NN architecture and only retrains the final fully connected classifiers layers with fewer samples. TL has shown better training speed because of training only a few layers and, at times, better accuracy. For instance, training an ML model with fewer samples achieves a similar performance than is trained on the entire set, as concluded empirically by the authors in the study [24]. Finally, although we have studied only optimizing training using TL in computer vision, TL can be applied to many other domains. A few examples of such domains are autonomous driving, gaming, healthcare, spam filtering, speech, and natural language processing applications."}]}