{"title": "ILAEDA: An Imitation Learning Based Approach for Automatic Exploratory Data Analysis", "authors": ["Abhijit Manatkar", "Hima Patel", "Devarsh Patel", "Naresh Manwani"], "abstract": "Automating end-to-end Exploratory Data Analysis (AutoEDA) is a challenging open problem, often tackled through Reinforcement Learning (RL) by learning to predict a sequence of analysis operations (FILTER, GROUP, etc). Defining rewards for each operation is a challenging task and existing methods rely on various interestingness measures to craft reward functions to capture the importance of each operation. In this work, we argue that not all of the essential features of what makes an operation important can be accurately captured mathematically using rewards. We propose an AutoEDA model trained through imitation learning from expert EDA sessions, bypassing the need for manually defined interestingness measures. Our method, based on generative adversarial imitation learning (GAIL), generalizes well across datasets, even with limited expert data. We also introduce a novel approach for generating synthetic EDA demonstrations for training. Our method outperforms the existing state-of-the-art end-to-end EDA approach on benchmarks by upto 3x, showing strong performance and generalization, while naturally capturing diverse interestingness measures in generated EDA sessions.", "sections": [{"title": "1 INTRODUCTION", "content": "Exploratory Data Analysis (EDA) stands as a pivotal step within any data science pipeline, offering an effective means to comprehend datasets by uncovering patterns, anomalies, and critical insights. Traditionally, this process has heavily relied on human intuition, leading to a trial-and-error exploration guided by the analyst's domain expertise. However, recent advancements have sought to automate this process through AutoEDA methods [3, 18, 23, 27, 30], aiming to provide analysts with initial insights into datasets automatically, which serve as a springboard for further exploration. Some of these efforts have sought to automate the entire EDA process using deep reinforcement learning (RL)-based AutoEDA systems [3] [21]. These systems treat AutoEDA as a sequential decision-making problem, with RL algorithms trained to predict a sequence of EDA operations. The rewards for such RL systems are heuristics/rules. However, such approaches face significant challenges:\n(1) Defining appropriate reward functions that encapsulate all the relevant aspects of a dataset is inherently complex. Often, the reward function used is insufficient to capture all the interesting dimensions of the data.\n(2) For a given dataset, previous expert data analysis sessions provide major hints about the usefulness of certain reward functions. One has to analyze these expert sessions to get those hints.\n(3) Precise dataset-specific reward function definitions are re- quired to learn meaningful policies in RL-based systems like the one presented by [3]. Defining detailed dataset-specific reward functions requires a good deal of data analysis to already have been done, a requirement which contradicts the motivations behind building an AutoEDA system.\nTo address these challenges, we propose ILAEDA, an imitation learning-based framework for AutoEDA. Unlike reward-based methods, ILAEDA learns directly from expert EDA sessions, eliminating the need for handcrafted reward functions and dataset- specific analyses. Our approach learns to imitate a human when shown a few expert sessions. To make sure that we don't overfit the model to the available expert sessions, we introduce a novel approach to automatically generate synthetic EDA demonstrations"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 AutoEDA Methods", "content": "In recent years, several methods have attempted to automate dif- ferent aspects of the EDA process. A set of next-step recommen- dation systems make use of pre-defined notions of interestingness to quantify the output of a specific EDA operation, e.g. for data- visualization [35], for finding interesting data tuple-subsets or data cube subsets [27] [26] [8] [23], OLAP drill-down [13], and data summaries [29]. Log-based EDA recommender systems, such as [1] [9] [36], use a collection of the previous exploratory sessions of the same or different users to generate recommendations for the next exploratory operation. Systems in [18] [19] utilize session logs to retrieve similar prefixes, generate candidate next-actions, and con- vert them into concrete recommendations of EDA operations based on interestingness measures. The approach in [30] recommends next action using a kNN-based classifier on user exploration logs. This classifier predicts the best interestingness measure that could capture user's interest at every step of the EDA session. EDA can also be modeled as a meta-learning problem [4].\nThe approach in [6] generates comparison queries to highlight insights in a dataset. In [16], authors propose a novel formulation of basic data patterns to facilitate extraction of insights from a dataset. [17] present a Large Language Model [33] based system for extracting insights from a dataset and presenting them to the user through a natural language interface.\nRL based AutoEDA systems are described in [3] [22] [15]. ATENA [3] automatically generates EDA sessions for a given tabular dataset. In this approach, EDA is formulated as a sequential decision-making problem. A deep reinforcement learning algorithm is used to learn a policy to generate a sequence of EDA operations which maximize the defined reward function. Authors of [15] describe a system built with ATENA as a base, augmenting it with a novel action space formulation and user-specified constraint compliance rewards."}, {"title": "2.2 Imitation Learning", "content": "In this setup, we do not have access to the reward structure. We want to learn the optimal policy without knowing the rewards. Behavioral cloning [32] [25], Imitation learning [7] [12] [24] [10], Inverse RL [37] are some of the techniques which are used to learn an optimal policy when we do not know the reward structure. All these approaches require expert trajectories to learn an optimal policy.\nGenerative Adversarial Imitation Learning (GAIL) [12] treats imitation learning as a min-max adversarial problem. The method describes two modules a discriminator D and a policy with opposing objectives:\n(1) Given any state s, the objective of the policy is model the distribution \\(\\pi(a|s)\\) to mimic the expert demonstrations as closely as possible.\n(2) Given a pair of state and action taken in the state (s, a), the objective of the discriminator is to differentiate between (s, a) pairs coming from an expert session and the policy. For any (s, a), \\(D(s, a) \\in [0, 1]\\), where D(s, a) = 1 indicates that (s, a) comes from expert demonstrations and D(s, a) = 0 indicates that it is a generated pair.\nThe two modules are optimized in a procedure that alternates be- tween optimizing D and optimizing \\(\\n\\) until D is no longer able to distinguish between state-action pairs coming from expert demon- strations and state-action pairs coming from \\(\\pi\\). GAIL has been shown to effectively imitate expert demonstrations even when the number of demonstrations is very low, which makes it an attractive candidate for the end-to-end AutoEDA problem as the availability of expert demonstrations is generally quite low."}, {"title": "2.3 Interestingness Measures", "content": "Various interestingness measures have been studied in literature. Some of them have been used as rewards in RL based systems and they are also useful for analyzing EDA operations. We present a brief summary of some of the interestingness measures studied in literature, which we use in later sections.\u00b9 (a) A-INT. This interest- ingness score [3] is used as a reward function that rewards display which are compact in case of GROUP operations, and displays with high deviation from the previous display in case of FILTER op- erations. (b) Diversity score. This metric favors a display that highlights parts of the dataset that are different from those seen in any of the previous displays in the session so far [3]. (c) Readabil- ity score. This metric rewards highly compact displays [5] with few rows as they are considered more readable and are given a higher reward. (d) Peculiarity score. This score helps quantify anomalous patterns by favoring display with a high difference from the initial display at the start of the analysis [35]. (e) Coherence score. This score is a highly detailed metric determined by a set of handcrafted rules which assign each display a penalty or reward based on whether the operation performed at the current step is coherent with previous operations [3]."}, {"title": "3 PROBLEM SETUP", "content": "Our proposed approach attempts to tackle the end-to-end AutoEDA problem using imitation learning. In this section, we describe the problem setup and illustrate some of the challenges with reward-based approaches for this problem.\nThe input is a tabular dataset D with a set of attributes (columns) A and the desired output is a sequence of analysis operations which will form an EDA session. This problem can be modeled as a Markov Decision Process (S, A, R, P) where S, A are the state and action space respectively, \\(R: S \\times A \\rightarrow R\\) is the re- ward function, and \\(P: S \\times A \\rightarrow S\\) is a deterministic transition function. Let \\(\\Pi\\) be the space of all stationary stochastic policies which return a distribution over actions in A given a state in S. The goal is to learn an optimal policy \\(\\pi^* \\in \\Pi\\) which maximizes a discounted cumulative sum of rewards for a fixed horizon T, i.e. \\(\\pi^* = \\arg \\max_{\\pi} E_{\\pi}[\\Sigma_{t=0}^T \\gamma^tR(s_t, a_t)]\\), where \\(s_t \\in S\\) is the state at time-step t given by \\(s_t = P(s_{t-1}, a_{t-1})\\), \\(a_t \\sim \\pi(\\cdot|s_t)\\) and \\(\\gamma\\in [0, 1]\\) is the discounting factor."}, {"title": "3.1 State Space", "content": "State space [3] is described as follows. An EDA session begins with an initial display \\(d_0\\) of the dataset D. At each subsequent timestep t, an action \\(a_t \\in A\\), which is an EDA operation, is ap- plied to display \\(d_t\\) to obtain display \\(d_{t+1}\\). For each display \\(d_t\\), corresponding state \\(s_t \\in S\\) is represented as \\(s_t = Encode(d_{t-2}) \\otimes Encode(d_{t-1}) \\otimes Encode(d_t)\\), where \\(\\otimes\\) is the vector concatenation operator and Encode(dt) encodes a display into a vector by ex- tracting the following features. (a) For each attribute A \\(\\in\\) A in the display, we include its entropy, number of distinct values, and the number of null values. (b) For each attribute A \\(\\in\\) A, we include a feature to determine whether that column is grouped, aggregated, or neither. (c) Three global features: number of groups, the mean"}, {"title": "3.2 Action Space", "content": "The action space A, in addition to basic BACK and STOP actions for directing the flow of the analysis, consists of parameterized FILTER and GROUP operations with possible parameters drawn from the columns of the dataset A and their distributions as described below:\n(1) GROUP (grp_col, agg_col, agg_func): This action groups on the grp_col \\(\\in\\) A and aggregates the agg_col \\(\\in\\) A using the agg_func, which can be one of { SUM, COUNT, MEAN, MIN, MAX }.\n(2) FILTER(filter_col, filter_func, filter_term): This action filters the dataset by using the comparison operator filter_func \\(\\in\\) {=, \\(\\neq\\), CONTAINS, STARTS_WITH, ENDS_WITH} and filter_term, which (numerical or textual) appears from within the filter_col \\(\\in\\) A.\n(3) BACK(): Allows to take a step backward and return to the previous display in the ongoing analysis session.\n(4) STOP(): Allows the signaling of the end of the current anal- ysis session."}, {"title": "3.3 Issues With Reward-Based Modeling", "content": "ATENA [3] uses hand-crafted rewards based on interestingness measures. The reward function used is given by:\n\\(R(s_t, a_t) = r_{int} (s_t, a_t) + \\lambda_1 r_{div} (s_t, a_t) + \\lambda_2 r_{coh} (s_t, a_t)\\)\n(1)\nHere, \\(r_{int}, r_{div}\\) are the A-INT and Diversity score respectively (refer Section 2). \\(r_{coh}\\) is the output of a binary classifier trained using rules based on the Coherence score. \\(\\lambda_1, \\lambda_2\\) are constants that are tuned to calibrate the reward. As described before, some of the rules defined for the Coherence reward include:\n(1) Filtering/Grouping on a specific set of filterable or groupable columns is rewarded, and specific non-filterable or non- groupable columns are penalized.\n(2) Using a specific set of operators to filter the info_line col- umn is rewarded."}, {"title": "4 ILAEDA: PROPOSED METHODOLOGY", "content": "The overall architecture of the proposed approach ILAEDA is pro- vided in Figure 1. Our main goal is to learn a policy directly from the expert EDA sessions (from here on, also referred to as expert trajectories) using Imitation Learning, allowing the system to learn to imitate a human expert. We have a set of expert trajectories \\(& = {\\tau_1, \\tau_2, . . . \\tau_n}\\) where each \\(\\tau_i\\) is a sequence of state-action pairs \\(\\{(s_1, a_1), (s_2, a_2)... (s_T, a_T)\\} \\) where each \\(s_i \\in S\\) and \\(a_i \\in A\\). We assume that expert actions come from an underlying expert policy \\(\\pi_E\\), i.e. \\(a_t \\sim \\pi_E(\\cdot|s_t), \\forall (s_t, a_t) \\epsilon \\tau, \\forall \\tau \\epsilon &\\). ILAEDA uses three neural networks: Policy \\(\\pi_\\theta\\), Value V, and Discriminator \\(D_w\\) pa- rameterized by \\(\\theta, \\phi, w\\) respectively. The policy and discriminator networks undergo training at alternate steps. The discriminator is trained to minimize the loss:\n\\(L_D = E_{\\pi_\\theta} [log(D_w(s, a))] + E_{\\pi_E} [log(1 \u2013 D_w(s, a))]\\)\n(2)\nIn the first term, the expectation is over a mini-batch of (s, a) pairs collected by environment interaction using the current policy \\(\\pi_\\theta\\). A set of trajectories \\(T = {\\tau_1. . . \\tau_\\& }\\) is obtained where each \\(\\tau_1 = \\{(s_1, a_1) ... (s_{T}, a_{T})\\}\\) consists of (s, a) pairs such that action \\(a \\sim \\pi_\\theta(s)\\). In the second term, the expectation is over an equal sized mini-batch of (s, a) pairs randomly picked from \\(\\tau \\epsilon &\\). The policy and value networks are trained using Proximal Policy Optimization (PPO) [28] on a mini-batch of 4-tuples \\((r_t, s_t, a_t, s_{t+1})\\) where \\(r_t = -log(1-D_w(s_t, a_t)) + P(S_{1:t}, a_{1:t})\\). The first term is the imitation reward, based on the discriminator output as in GAIL [12]. The second term is our penalty term that we describe below.\nHalf of the mini-batch is composed of expert demonstrations (\\(s_t, a_t, s_{t+1}\\)) obtained from \\(\\tau \\epsilon &\\), each of which is appended with"}, {"title": "4.1 Penalties for Incoherent Action Sequences", "content": "An important feature of ILAEDA is that it augments the reward term with a penalty term for incoherent sequences of actions. Some such incoherent sequence of actions are as follows. (a) Model takes a BACK action at the beginning of the analysis, (b) model consecu- tively repeats the same FILTER/GROUP action, (c) model alternates between a FILTER/GROUP action and BACK action.\nA sequence of operations is typically helpful when it consists of at least 2-3 continuous FILTER/GROUP operations before the steps are retraced using BACK. Taking a single FILTER/GROUP action and immediately retracing with a BACK action, and continuing to do so repeatedly signals a lack of confidence in the chosen path, and hence an uncertainty in the actions taken. To discourage the learning of these behaviours, we use the following reward function,\n\\(r_t = log(1-D_w(s_t, a_t)) + P(S_{1:t}, a_{1:t})\\)\n(3)\nwhere \\(P(S_{1:t}, a_{1:t})\\) is an incoherence penalty defined as follows:\n\n\\begin{equation}\nP(S_{1:t}, a_{1:t}) = \n\\begin{cases}\n-1.0 & \\text{if } a_t = BACK \\text{ and } t = 1 \\\\\n-1.0 & \\text{if } a_t \\neq BACK \\text{ and } a_t = a_{t-1} \\\\\n-1.0 \\times l & \\text{if } a_{t-2} \\neq a_{t-1}, a_{t-3},...,a_{t-2(l+1)} \\text{ and } a_t = a_{t-2(l+1)} \\\\\n0.0 & \\text{{FILTER, GROUP}} \\text{ and } l > 1 \\text{ otherwise} \\\\\n\\end{cases}\n\\end{equation}\n(4)"}, {"title": "4.2 Initialization of Policy with Behavioral Cloning", "content": "We initialize the policy network using a behavioral cloning objec- tive. We observe that such initialization results in stronger final performance of the learnt model. Our reasoning for this finding is that behavioural cloning provides a warm start to the policy net- work compared to random initialization of parameters. As very few expert demonstrations are available for imitation, pre-training with behavioural cloning is valuable in initializing the policy at a point in parameter space which is likely to converge to the optimum faster. The pre-training stage seeks to minimize the following objective:\n\\(\\min_{\\theta} E_{(s,a) \\in &} [-log \\pi_\\theta(a|s) + ||\\theta||^2]\\)\n(5)\nwhere (s, a) pairs come from trajectories \\(\\tau \\epsilon &\\). We conduct an ablation study to examine the effects of this initialization choice."}, {"title": "4.3 Proximal Policy Optimization Updates", "content": "In ILAEDA, we train the policy network using Proximal Policy Optimization (PPO) [28]. Specifically, we use the PPO-clip update given as:\n\\(\\theta_{t+1} = \\arg \\max_{\\theta} E_{T \\sim B} [\\min (\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_t}(a|s)} A^\\theta_t(s, a), g(\\epsilon, A^\\theta_t(s, a)))]\\)\n(6)"}, {"title": "5 EXPERIMENTAL EVALUATION", "content": ""}, {"title": "5.1 Dataset Details", "content": "To evaluate the efficacy of our proposed method, we train and generate EDA sessions on two types of datasets:\n5.1.1 Cyber Security Datasets.\nThis is a collection of four mutually exclusive datasets [31], which we will refer to as the cyber security datasets. Expert EDA sessions have been generated in two ways for each of the four datasets:\n(1) REACT Dataset: 56 cy- ber security analysts were asked to explore the four cy- ber security datasets. Each dataset may reveal a unique security event, and analysts were asked to discover the specifics of the underlying security event for each"}, {"title": "5.1.2 Synthetic Datasets.", "content": "We algorithmically generate datasets with a given schema and inject interesting patterns into them. The injected patterns are in the form of a dependencies between columns which induce a graph structure. We traverse this graph of dependencies to generate expert EDA trajectories. Five synthetic datasets, each with three categorical columns, two text columns and three numeric columns and 1000 rows, and their corresponding expert trajectories are generated for our experiments. The gener- ated trajectories are randomly split into train and eval sets. Further details on the generation process are provided in Appendix C."}, {"title": "5.2 Baselines", "content": "(1) Behavioural Cloning: Behavioural Cloning (BC) is an imi- tation learning technique that uses expert demonstrations to learn a state-to-action mapping in a supervised way. We use it as a baseline to compare against ILAEDA. For the Cy- ber Security datasets, we train a BC model on three out of four datasets and report results on sessions generated on the fourth dataset. We train a BC model on the first five ta- bles for the Synthetic datasets and report results for sessions generated on tables 6 and 7."}, {"title": "5.3 Evaluation Metrics", "content": "We evaluate the performance of ILAEDA and the baselines using the following similarity metrics introduced in [3] for evaluation.\n(1) Precision: Precision measures the accuracy of the generated EDA sessions and is calculated as the number of times a view occurs in the gold standard divided by the total number of views.\n(2) TBLEU score: This measure is adopted from BLEU [20] score.It is stricter than precision as it compares subsequences of size n (1 to 3) and considers the order and prevalence of each view in the gold-standard set. The benchmark measures TBLEU-1, TBLEU-2, and TBLEU-3.\n(3) EDA-Sim: EDA-Sim [19], considers the order of views and enables a fine-grained comparison of EDA views. It considers nearly identical views as hits. The generated session is com- pared to each gold-standard session to get the final EDA-Sim score, using the highest score."}, {"title": "5.4 Implementation Details", "content": "5.4.1 Architecture. ILAEDA's policy and value networks are im- plemented as fully-connected neural networks with three hidden layers. There are 50 neurons in each layer with tanh non-linearities. The final output layer consists of multiple heads for predicting dif- ferent components of the action. The design of this layer is identical to that of [3]. The discriminator network is a fully-connected neural network with two hidden layers of 32 neurons each, with ReLU non-linearities.\n5.4.2 Training. The pre-training phase for ILAEDA is identical to the training for the Behavioural Cloning baseline. It consists of"}, {"title": "6 RESULTS AND ANALYSIS", "content": "Table 2 shows a performance comparison between ILAEDA and the baselines on the evaluation metrics discussed in Section 5.3. We see that our method overall delivers a significantly better perfor- mance than the baselines. The results are all the more significant because, in each setting, ILAEDA is evaluated on datasets that it has not seen during training. In contrast, ATENA is evaluated on the same datasets it trains on. This indicates that ILAEDA generates better EDA sessions and has better generalization capability."}, {"title": "6.1 Ablation studies", "content": "6.1.1 Training without penalty scores: We trained a version of ILAEDA without the coherence penalties defined in equation 3 and kept all other configurations the same to study the impact of our design choice. We see that this model underperforms in comparison to ILAEDA trained with coherence penalties (Table 4). We explain the lower scores by inspecting the generated EDA session (Table 5). The EDA views 2 and 3 are identical because the same GROUP operators have been repeated consecutively. Such actions are penalized in our original model."}, {"title": "6.1.2 Training without BC initialization:", "content": "ILAEDA, which is pre- trained with Behavioural Cloning, performs better than a model without pretraining. This result suggests that pretraining the policy before training on the GAIL-based objective improves the overall model. We conjecture that this phenomenon is observed due to the pre-trained policy getting a warm start allowing it to start in a better position which gives it an advantage during the adversarial training."}, {"title": "6.2 Analysis of ILAEDA's Ability to Capture A Diverse Set of Interestingness Measures", "content": "In Table 6, we show a session generated by our model on Cyber Se- curity dataset 1 and normalized scores obtained on different interest- ingness metrics. We highlight scores greater than 0.7. Throughout this session, generated views obtained high readability, diversity, and peculiarity scores. This indicates that our model can effec- tively filter the dataset and present important tuples to users. Views presented are diverse, showing different insights from the dataset.\nTable 7 shows a session generated using ATENA on the same dataset. We observe that views produced have high A-INT and diversity scores. This is because these scores were used as reward signals to train ATENA. In comparison, each action taken by our model shows different scores being highlighted, which shows that our model can capture a wider set of interestingness measures than ATENA."}, {"title": "6.3 Analysis of ILAEDA's Ability to Capture Specific Interestingness Measures", "content": "To analyze ILAEDA's ability to capture specific underlying interest- ingness measures we conducted an experiment where we trained ILAEDA on a subset of expert sessions filtered to optimize one specific measure on a source datasets. The goal is to examine if the sessions generated by the trained model on other datasets also reflect the domination of the same interestingness metric.\nWe filter the expert sessions from the Synthetic Datasets (refer to section 5.1.2) to obtain subsets where the sessions maximize one particular interestingness metric over others. We consider A- INT, Diversity, and Readability as the metrics for this analysis. To filter the expert sessions, we classify each session into one of three categories. (1) Category 1: Maximizing A-INT more than others. (2) Category 2: Maximizing Diversity more than others. (3) Category 3: Maximizing Readability more than others.\nWe compare the 75th percentile normalized score obtained for each metric over all the steps in that session's trajectory to classify a session. If the 75th percentile normalized score over that trajectory is highest for the A-INT metric, then that session is classified as maximizing A-INT more than others. Similarly, we can classify a session as maximizing Diversity more than others or maximizing Readability more than others.\nWe do this classification for all Synthetic datasets 1-5 sessions and obtain three subsets. We then trained an ILAEDA model on these subsets using the default settings described before. We use the trained models to generate 100 EDA sessions each on Synthetic Dataset 7. For each generated session, we compare the median normalized score obtained for each metric across all the steps in the trajectory of that session. If the median normalized score is"}, {"title": "7 CONCLUSION AND FUTURE WORK", "content": "In this paper, we present ILAEDA, a novel imitation learning based end-to-end AutoEDA system which does not rely on handcrafted interestingness heuristics to learn a policy. Through automated benchmarking and manual analysis of generated EDA sessions, we demonstrate that ILAEDA can learn to perform meaningful EDA and outperform baselines across benchmarks. Further, we demonstrate the generalizability of our system on unseen datasets. We also show ablation studies to validate our design choices. For future work, some of the following investigations may be worth- while: (a) training a system that generalizes to perform AutoEDA across datasets with different schemas, (b) exploring the effect of other penalties and (c) incorporating actions other than FILTER and GROUP into ILAEDA."}, {"title": "A INTERESTINGNESS MEASURES", "content": "(1) A-INT. This interestingness score [3] is used as a reward func- tion. It is defined for FILTER and GROUP operations as follows:\n(a) Interestingness Score for a GROUP operation: This score uses conciseness measures [5] [11] which rewards compact group- by views covering many rows as these views are considered informative and easy to understand. This measure considers the number of groups g, the number of grouped attributes a, and the number of tuples r. The score Int(dt) is calculated as\n\\(Int(d_t) = \\frac{h_1 (g, a)}{h_2(r)}\\)\n(9)\nwhere \\(h_1\\) and \\(h_2\\) are normalized sigmoid function with fixed width and center.\n(b) Interestingness Score for a FILTER operation: To quantify the interestingness of a FILTER operation, exceptionality [27] [34] [35] of filtered rows (generated after applying the op- eration) is compared with those of the unfiltered table. The Kullback-Leibler (KL) divergence [14] on each column is used to measure how much the filtered data view differs from the unfiltered view. This score favors filter operations whose resultant display deviates significantly from the previous dis- play. Let \\(P_A\\) be the distribution of an attribute (column) A at time step t in the analysis. The score Int(dt) is given by:\n\\(Int(d_t) = h(\\max_A D_{KL} (P_{t-1}^A, P_t^A))\\)\n(10)\nwhere h is the sigmoid function.\n(2) Diversity score. This metric favors a display that highlights parts of the dataset that are different from those seen in any of the previous displays in the session so far [3]. It is calculated as the minimum Euclidean distance between the resultant display"}, {"title": "B ALGORITHMIC DETAILS OF ILAEDA", "content": "Algorithm 1 gives a detailed description of the ILAEDA procedure."}, {"title": "C SYNTHETIC DATA GENERATION", "content": "Our synthetic data generation algorithm consists of three steps:\n(1) Pattern Injection and Correlation: In this step, we take in a schema \\(S = \\{C_1, C_2 ... C_k, N_1, N_2... N_l, T_1, T_2... T_m\\}\\) where \\(C_i, N_i, T_i\\) represent Categorical, Numeric and Text columns respec- tively. For each column \\(c \\in S\\), we randomly generate a set of patterns \\(P(c) = \\{p_1, p_2... p_n\\}\\) and a random weighting \\(W(c) = \\{w(p_1), w(p_2)... w(p_n)\\}\\) over these patterns (\\(\\sum w(p_i) = 1\\)). These patterns are representative artifacts of the data appearing in col- umn c, and the weights represent the likelihood that an element appearing in a column follows a pattern:\n\u2022 For categorical columns, each pi is a category that appears in the column.\n\u2022 For a numeric column, each pi is a tuple \\((\\mu_i, \\sigma_i)\\) represent- ing a Gaussian distribution with mean and variance \\((\\mu_i, \\sigma_i)\\) respectively. Every element in the numeric column Nj will be sampled from one of the \\(p_i \\in P(N_j)\\)."}]}