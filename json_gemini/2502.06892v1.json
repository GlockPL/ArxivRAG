{"title": "CERTIFYING LANGUAGE MODEL ROBUSTNESS WITH FUZZED RANDOMIZED SMOOTHING: AN EFFICIENT DEFENSE AGAINST BACKDOOR ATTACKS", "authors": ["Bowei He", "Lihao Yin", "Hui-Ling Zhen", "Jianping Zhang", "Lanqing Hong", "Mingxuan Yuan", "Chen Ma"], "abstract": "The widespread deployment of pre-trained language models (PLMs) has exposed them to textual backdoor attacks, particularly those planted during the pre-training stage. These attacks pose significant risks to high-reliability applications, as they can stealthily affect multiple downstream tasks. While certifying robustness against such threats is crucial, existing defenses struggle with the high-dimensional, interdependent nature of textual data and the lack of access to original poisoned pre-training data. To address these challenges, we introduce Fuzzed Randomized Smoothing (FRS), a novel approach for efficiently certifying language model robustness against backdoor attacks. FRS integrates software robustness certification techniques with biphased model parameter smoothing, employing Monte Carlo tree search for proactive fuzzing to identify vulnerable textual segments within the Damerau-Levenshtein space. This allows for targeted and efficient text randomization, while eliminating the need for access to poisoned training data during model smoothing. Our theoretical analysis demonstrates that FRS achieves a broader certified robustness radius compared to existing methods. Extensive experiments across various datasets, model configurations, and attack strategies validate FRS's superiority in terms of defense efficiency, accuracy, and robustness.", "sections": [{"title": "1 INTRODUCTION", "content": "Pre-trained language models (PLMs) have become the cornerstone of numerous natural language processing tasks, with fine-tuning being the most common approach for adapting these models to customized downstream applications (Kenton & Toutanova, 2019; Liu et al., 2019; Touvron et al., 2023). However, the widespread adoption of PLMs also has new vulnerabilities, especially textual backdoor attacks. These attacks involve injecting malicious knowledge into PLMs, either through poisoned training data or direct modification of model parameters, compromising their reliability and trustworthiness (Cheng et al., 2024; Zhao et al., 2024). Different from the ordinary poisoning data attack, textual backdoor attacks are particularly insidious because they do not significantly impact model performance on benign inputs, making them difficult to detect through standard evaluation methods. The attacked PLMs only exhibit malicious behavior when presented with specific trigger inputs, allowing them to evade human inspection.\nIn the PLM pre-training and fine-tuning phases, there are two backdoor planting paradigms (Guo et al., 2022): 1) embedding backdoors in the pre-trained model by poisoning training before model weights are published for downstream use; 2) embedding backdoors in the downstream model during the fine-tuning phase via poisoning the fine-tuning data. Note that the second paradigm is fundamentally the same as the backdoor attacks to conventional standalone models. Considering the more widespread and potentially more harmful nature of pre-training phase attacks, which can simultaneously affect multiple downstream applications, we focus on the pre-training backdoor attack.\nThough different backdoor attack strategies for PLMs have been investigated, the effective defense schemes against them are less explored. As steganography techniques for ensuring trigger invisibility"}, {"title": "2 RELATED WORK", "content": "Textual Backdoor Attacks and Defenses Different from the previous evasion attacks to the language models, the textual backdoor attacks take effect in both training and inference phases via poisoning training data/model parameters and perturbing inputs with triggers respectively, which make them more covert and difficult to defend against. Some pioneering works (Dai et al., 2019; Chen & Dai, 2021) discussed how to toxify the training corpus to attack the LSTM-based language models. Due to the prevalence of pre-training and fine-tuning paradigm for transform-structure language models, more recent works (Zhao et al., 2023; Chen et al., 2021a;b; Shen et al., 2021; Yang et al., 2021a; Li et al., 2021; Zhang et al., 2021; Guo et al., 2022; Qi et al., 2021b;c) explored how to inject the lethal backdoor attacks to pre-trained models, making them vulnerable in various downstream tasks. Correspondingly, to alleviate the harms brought by such kinds of textual backdoor attacks, some empirical defense methods (Qi et al., 2021b;c;a) have also been proposed. Nevertheless, most of them are based on heuristic rules, lacking the enough theoretical guarantees though achieving acceptable performance in some specific scenarios. Our focus in this paper is to equip language models with certified robustness against textual backdoor attacks, regardless of the attack strategies and forms.\nCertified Robustness of Language Models Though many empirical defense methods (Qi et al., 2021a; Cui et al., 2022; Yan et al., 2023) against various textual attacks have been proposed and widely deployed in industrial applications, the certified defense approaches with theoretical guarantees are still being regarded as the Holy Grail of research in this direction. Among existing attacks to language models, the evasion attacks and backdoor attacks are two kinds of most common and impactful ones. Concretely, interval bound propagation (Jia et al., 2019; Huang et al., 2019; Ye et al., 2020;"}, {"title": "3 PRELIMINARIES", "content": "In this section, we provide the formulation of the textual backdoor attack on PLMs and the corresponding goal of defense. In our scenario, the language model (LM) $f(\u00b7)$ parameterized by $\\Theta$ is first pre-trained with the mixture of clean and poisoned corpus to plant the back patterns by the malicious attackers. The pre-trained model parameter checkpoints are then uploaded to the open-source repositories like Hugging Face\u00b9. The users download the attacked pre-trained model parameters $\\Theta'_p$ and fine-tune them to $\\theta'$ on the local downstream data $D_F$ with $x = [X_1,X_2,..., X_L]$ as the textual input and $y \\in Y$ as the output label. We introduce the normalized Damerau-Levenshtein distance (Damerau, 1964; Levenshtein et al., 1966) $d_{DL}(x, x')$ to measure the edit distance between the original benign input $x$ and the perturbed input $x'$ by the triggers, which allows the operations like token insertion, deletion, substitution, and transposition. Thus, due to the above flexibility, the normalized Damerau-Levenshtein distance can be applied to almost all trigger patterns of existing textual backdoor attack methods, including character-level, word-level, and sentence-level ones.\nThe goal of the defense is to guarantee that the model prediction of $f(x'; \\theta')$ can be consistent with that of $f(x; \\theta_F)$ whose training procedure is not attacked by the poisoned corpus. The LM $f(\u00b7)$ is certified robust against the backdoor attack if it satisfies the following criterion: for any input x,\n$f(x';\\theta'_p) = f(x;\\theta_F), \\forall x' s.t. d_{DL}(x, x') < R_{r,L}.$                                                                                              (1)\n, where $R_{r,L}(0 < R \\leq 1)$ denotes the robustness radius. A certified robust LM is expected to generate the robust prediction, given that at most $R_{r,L}$ tokens in the input x are perturbed."}, {"title": "4 METHODOLOGY", "content": "4.1 RANDOMIZED SMOOTHING DEFENSE\nRandomized smoothing was originally proposed to achieve the certified robustness effect against evasion attacks in computer vision scenario (Cohen et al., 2019). We first extend its basic framework to the textual backdoor attacks which have not been thoroughly explored before. Generally, randomized smoothing introduces a smoothed model $\\hat{f}$ based on the base model $f(\u00b7)$ by exerting the random noise on the fine-tuning data and test samples. In essence, the rationale behind leveraging the randomized smoothing defense lies in the observation that the inclusion of noise mitigates the prevalence of decision boundaries with pronounced curvature, thereby reducing the susceptibility to backdoor attacks. Thus, we denote the noisification operator as $\\oplus$ and define the smoothed model $\\hat{f}$ as:\n$\\hat{f}(x') = \\underset{y \\in Y}{\\text{arg max}} P_{u,\\epsilon}(f(x' \\oplus u; \\Omega(\\theta'_p, D_F \\oplus \\epsilon)) = y),$                                                                                              (2)\nwhere random noise variables $u \\sim P_u$, $\\epsilon \\sim P_{\\epsilon}$ follow the independent random distributions and are added to the perturbed test samples and fine-tuning data, respectively. Here, the $\\Omega$ indicates the fine-tuning procedure which takes the poisoned pre-trained model parameters $\\theta'_p$ and randomized fine-tuning data $D_F \\oplus \\epsilon$ (notes as $\\tilde{D_F}$) as inputs and returns the smoothed fine-tuned parameters $\\theta_F$.\nIn practice, considering the complexity of LM $f(\u00b7)$ itself, Monte Carlo simulation is an effective approach to approximate the above probability $P_{u,\\epsilon}(f(x' \\oplus u; \\Omega(\\theta'_p, D_F \\oplus \\epsilon)) = y)$ in Eq 2."}, {"title": "4.2 BIPHASED MODEL PARAMETER SMOOTHING", "content": "If directly following the approach described in Section 4.1, one might consider fine-tuning K pre-trained language models on K distinct, randomized downstream datasets. However, this approach would impose a considerable computational burden. As a result, it becomes an unrealistic strategy for practical scenarios. Therefore, there exists an urgent need for post-attack defense mechanisms that not only have certified robustness guarantees but also provide efficient execution. Thus, we propose the biphased model parameter smoothing as a targeted solution particularly for large language models, which is performed during both the fine-tuning and inference phases. This biphased approach notably diminishes data storage requirements for different versions of randomized fine-tuning datasets and drastically reduces the computational overhead associated with training. Besides, this strategy advocates for the selective smoothing of parameters in H layers proximal to the output \u2013 those most vulnerable to backdoor attacks, as highlighted in the literature (Kurita et al., 2020). The detailed smoothing procedures in such two phases are as follows:\nFine-tuning Phase: In iteration i (1 \u2264 i \u2264 I) of fine-tuning process, the model parameter smoothing is performed as follows:\n$\\theta^i_F = Clip_\\rho(\\theta^{i-1}_F - \\eta g(\\theta^{i-1}_F; B_i)) + \\epsilon_{top-H},$                                                                                              (4)\nwhere \u03b7 and \u03c1 indicate the learning rate of the fine-tuning process and the norm bound, respectively. g(\u00b7) denotes the gradient function and $B_i$ is the mini-batch in iteration i. Especially, $\\theta^0_F = \\theta'_p$.\nInference Phase: When completing the model fine-tuning, we conduct the parameter smoothing to the finally obtained $\\theta_F$ independently for K times:\n$\\theta_{F,k} = Clip_\\rho(\\theta) + \\epsilon_{k,top-H}, k = 1, 2, ..., K,$                                                                                              (5)\nOnce the K smoothed copies of LMs are generated at the beginning of the inference phase, they are fixed and employed for every test sample during the whole inference phase."}, {"title": "4.3 FUZZED TEXT RANDOMIZATION", "content": "Traditional text randomization in randomized smoothing relies on uniform randomization in the text input, suffering from low efficiency and limited certified robustness radius. Motivation by the fuzzing technique in the software verification research, we design the Monte Carlo tree search (MCTS)-based fuzzed text randomization to first proactively identify the vulnerable areas containing the triggers in the input text. We choose MCTS for its ability to efficiently explore high-dimensional discrete textual spaces and adaptively focus on promising areas. Then, such areas will be imposed more possibilities to conduct the text randomization operations. In such a way, the obtained randomized samples are"}, {"title": "4.3.1 VULNERABLE AREA IDENTIFICATION", "content": "The primary objective of this MCTS-based fuzzing approach is to efficiently identify potential vulnerable areas in the input text that may contain backdoor triggers. Thus, we first define S as a search tree, with nodes n \u2208 S corresponding to segments $Seg(x', i, j)$ of the perturbed text x' from i-th token to j-th token. Each node n is associated with a score V(n), reflecting the potential of the corresponding segment to exhibit trigger impacts. The detailed fuzzing process is as follows:\nInitialization Initialize S with a root node $n_{root}$ representing the original input x'. Set $V (n_{root}) = 0$.\nMCTS Iterations Then, in each iteration of MCTS, the following steps are executed:\n\u2022 Step 1: Selection Traverse from the root to a leaf node $n_i$ using the Upper Confidence Bound (UCB) applied to trees policy:\n$UCB(n_i) = V(n_i) + C \\sqrt{\\frac{ln(N_{parent})}{N_{n_i}}},$                                                                                              (6)\nwhere C is an exploration constant, $N_{n_i}$ signifies the number of visits to node $n_i$, and $N_{parent}$ is the number of visits to $n_i$'s parent node.\n\u2022 Step 2: Expansion Upon reaching a leaf node $n_i$ at the conclusion of the Selection phase, we evaluate whether the text segment corresponding to $n_i$, denoted as $Seg(x', i, j)$, can be further devided. This evaluation is based on the linguistic features of the segment, such as phrase boundaries, clause demarcations, or named entities contained within. If the further subdivision is viable, a child node $n_{new}$ for $n_i$ will be generated, which represents a subdivision of $Seg(x', i, j)$. The generation of $n_{new}$ is thus expressed as:\n$n_{new} = Seg(x', i, k) or Seg(x', k + 1, j),\nk = i + 1, ..., j - 1.$                                                                                              (7)\n\u2022 Step 3: Simulation Select a mutation operation m from a predefined mutation set M, which includes insertion, deletion, substitution, and transposition operations in the Damerau-Levenshtein space. Next, apply m to $n_{new}$, thus generating a new textual variant x of original x'. Then, x is evaluated to ascertain the deviation in LM's response. Here, we utilize an evaluation criterion $E(x, x')$ based on KL divergence (Kullback & Leibler, 1951) to quantify this deviation:\n$E(x,x') = D_{KL}(P_f(y|x)||P_f(y|x')),$                                                                                              (8)\nwhere $P_f(y|x)$ indicates the probability distribution of LM f on different outputs y.\n\u2022 Step 4: Backpropagation Update scores of all nodes n from $n_{new}$ up to $n_{root}$ based on $E(x, x')$, thus refining the selection process in subsequent iterations:\n$V_i(n) = \\frac{N_n-1}{N_n}V_{i-1}(n) + \\frac{E(x, x')}{N_n},$                                                                                              (9)\nUpon reaching a predefined number of iterations or a termination criterion, we identify segments corresponding to nodes with the highest scores V(n) as the most likely vulnerable areas T(x') with the greatest potential to contain backdoor triggers."}, {"title": "4.3.2 TEXT RANDOMIZATION", "content": "Building upon the established MCTS-based fuzzing framework, we proceed to employ text randomization while maintaining the normalized Damerau-Levenshtein distance within a specified threshold. This constraint ensures the semantic and structural integrity of the text by limiting the number and type of textual transformations, thereby preserving the original meaning and syntactic structure.\nRandomization Process: After identifying vulnerable areas T(x') that potentially contain triggers, we apply a targeted randomization strategy which employs differential probabilities for textual segments within and outside T(x'). The process consists of the following key steps:"}, {"title": "5 EXPERIMENTS", "content": "When conducting the experiments, we focus on answering following questions to deeply analyze the advantages of our proposed approach: 1) RQ1: Can our method achieve better backdoor defense performance compared with other empirical defense and randomized smoothing-based certified defense strategies? 2) RQ2: Can our method achieve broader certified robustness radius? 3) RQ3: Can our proposed biphased model parameter smoothing and fuzzed text randomization modules both contribute to the defense performance positively? 4) RQ4: Can our method achieve consistent defense performance over different victim models?\n5.1 EXPERIMENT SETUP\nVictim Language Models: To demonstrate the effectiveness of our approach on PLMs of different configurations, we conduct extensive experiments on a group of diverse PLMs. These include BERT (Kenton & Toutanova, 2019), a pioneering encoder-structured model with hundreds of millions of parameters, RoBERTa (Liu et al., 2019), a more robust PLM of similar size trained on larger dataset with dynamic masking, and the recently developed LLaMA3 (Dubey et al., 2024), a decoder-structured model with billions of parameters which has been pre-trained on colossal corpus.\nAttack Methods: Based on the assumptions regarding different degrees of attacker knowledge about the target downstream task, the existing pre-training phase textual backdoor attack schemes can be classified into three types: full data knowledge, domain shift, and data free. To comprehensively validate the defense effectiveness against different types of attacks, we adopt RIPPLea (Kurita et al., 2020), LWP (Li et al., 2021), and BadPre (Chen et al., 2021a) to instantiate such three kinds of methods, respectively.\nDefense Baselines: We compare our proposed FRS with both empirical defense and certified defense methods. The first category includes methods working on different phases: inference-time defense: RIPPLed (Kurita et al., 2020), ONION (Qi et al., 2021a), RAP (Yang et al., 2021b), Bite (Yan et al., 2023), PSIM (Zhao et al., 2024); training-time defense: BKI (Chen & Dai, 2021), R-Adaptor (Zhu et al., 2022). As for the certified defense, we adopt the recently proposed TextGuard (Pei et al., 2023).\nEvaluation Tasks and Datasets: Following previous literature (Pei et al., 2023), we evaluate the performance of different defense methods on several representative downstream tasks with corresponding datasets: sentiment analysis: SST-2 (Socher et al., 2013); toxicity detection: OffensEval (Zampieri et al., 2019); topic classification: AG'News (Zhang et al., 2015). When conducting domain shift poisoning pre-training, we utilize the IMDB (Maas et al., 2011), Twitter (Founta et al., 2018), and 20"}, {"title": "5.2 EXPERIMENT RESULTS AND ANALYSIS", "content": "5.2.1 DEFENSE PERFORMANCE (RQ1)\nTo demonstrate that our method can achieve better backdoor defense performance compared with baselines, we provide the performance of different empirical defense baselines, certified defense baseline TextGuard, and our FRS under above three backdoor attack approaches in Table 1. We can first observe that the certified method TextGuard and our FRS outperform empirical defense baselines on both PA and ASR metrics on three datasets, which verifies the advantage of this scheme. Second, the poor performance of TextGuard on CA can be noticed, which is due to that it breaks the syntactic and semantic integrity of the original texts during the word hashing assignment. Third, our FRS almost outperforms all empirical defense baselines and certified defense baseline TextGuard on CA, PA, and ASR metrics among three datasets, which demonstrates that it can not only effectively mitigate the negative impact brought by backdoor attacks on perturbed samples, but also minimize the"}, {"title": "5.2.2 CERTIFIED ROBUSTNESS RADIUS (RQ2)", "content": "To validate that our FRS method can indeed bring broader certified robustness radius, we directly calculate the robustness radius value achieved by our method. In detail, for each test sample, we find the maximum percentage of tokens that can be perturbed while the model still maintains correct prediction with high probability (e.g., 95% confidence) as the robustness radius. For ensuring comprehensiveness, we calculate the average and maximum of these robustness radii across all test samples. We compare the results of FRS with the best -performing baseline, TextGuard, which is also the only certified defense baseline. According to the results presented in Table 2, we can obtain the following observations: First, our FRS method consistently outperforms TextGuard across all datasets in both average and maximum robustness radius. Second, FRS achieves notably higher average robustness radius compared to TextGuard. The improvements range from 25.72% (OffensEval) to 34.87% (AG's News), indicating substantially better average-case robustness across various downstream tasks. Third, FRS also extends the maximum achievable robustness radius across all datasets, with improvements ranging from 11.82% (AG's News) to 20.30% (OffensEval). This demonstrates FRS's ability to provide certified robustness against more severe perturbations. All these observations align with our theoretical expectations of a broader certified robustness radius as discussed in Section 4.4. Besides, more results on certified accuracy provided in Appendix C also demonstrate the broadened robustness radius by our FRS. Thus, the Corollary 1 is persuasively validated with empirical results."}, {"title": "5.2.3 ABLATION STUDY (RQ3)", "content": "To validate that our proposed biphased model parameter smoothing (BMPS) module and fuzzed text randomization (FTR) module are both meaningful for the ultimate performance, we remove them from the overall method framework, respectively and conduct the experiments on above three datasets. The corresponding results are provided in Table 3. First, we can find that removing such two modules will indeed result in the performance drop on PA and ASR metrics under different attack approaches among three datasets. This phenomenon can be even more obvious on the AG's News dataset which is more fragile. This effectively demonstrates that the contribution brought by the BMPS and FTR are both positive. Besides, performance on CA metric of -FTR version and original FRS version are similar, which indicates the influence of FTR to model performance on benign samples is weak. Interestingly, removing the BMPS module leads to the slight improvement on CA metric, which can"}, {"title": "5.2.4 CONSISTENCY OVER DIFFERENT VICTIMS (RQ4)", "content": "To further explore whether our FRS's advantage remains against other baselines when the model size increases or structure varies, we extend the experiments in Section 5.2.1. In detail, we compare the empirical defense performance of our FRS with the strongest baseline, TextGuard against the most powerful attack method, RIPPLea under different victim language models with various architectures and sizes. The language model here include BERT-base, BERT-large, RoBERTa-base, ROBERTa-large, and LLaMA3-8B, which cover both encoder-based and decoder-based architectures with model parameter numbers ranging from 110 million to 8 billion.\nThe results provided in Table 4 illustrate the performance of our FRS method compared with TextGuard across various language models of different sizes and architectures. Several key observations can be made: 1) FRS consistently outperforms TextGuard across all model configurations and datasets. This is evident in the higher CA and PA, as well as lower ASR achieved by FRS. 2) As we move from BERT-base to BERT-large, and from ROBERTa-base to RoBERTa-large, both FRS and TextGuard show improved performance. This suggests that larger models generally exhibit better robustness against backdoor attacks, even without specialized defenses. 3) The performance difference between FRS and TextGuard remains substantial for both encoder-based (BERT, ROBERTa) and decoder-based (LLaMA3) architectures, indicating that FRS's effectiveness is not limited to a specific model structure. 4) While FRS maintains its advantage over TextGuard even for the largest model (LLaMA3-8B), the relative improvement is less pronounced compared to smaller models. For instance, on the SST-2 dataset, the ASR reduction from TextGuard to FRS for BERT-base is 7.50 percentage points, while for LLaMA3-8B, it's 3.13 percentage points. This suggests that as language models grow in size and capability, they may become inherently more robust to certain backdoor attacks, potentially reducing the marginal benefit of defenses like FRS."}, {"title": "6 CONCLUSION", "content": "In this paper, we have presented fuzzed randomized smoothing (FRS), a novel defense strategy to enhance the robustness of pre-trained language models against textual backdoor attacks injected during the pre-training phase. Our approach integrates fuzzing techniques with randomized smoothing, introducing fuzzed text randomization to proactively identify and focus on vulnerable areas in the input text. This innovation, combined with our biphased model parameter smoothing, enables FRS to achieve a broader certified robustness radius and superior performance across diverse datasets, victim models, and attack methods. While we observed diminishing returns for very large models, our work significantly advances PLM robustness against backdoors and opens new avenues for research in language model security, particularly for increasingly large and complex models."}, {"title": "A EQUIVALENCE PROOF BETWEEN BIPHASED MODEL PARAMETER SMOOTHING AND STANDARD RANDOMIZED SMOOTHING", "content": "This appendix part establishes the theoretical equivalence between our proposed biphased model parameter smoothing (BMPS) method described in Section 4.2 and the standard randomized smoothing defense framework described in Section 4.1. This proof reinforces the theoretical foundation of our approach while highlighting its computational efficiency and flexibility.\nA.1 REVIEW OF METHODS\nA.1.1 STANDARD RANDOMIZED SMOOTHING DEFENSE\nThe standard approach, as described in Section 4.1, involves fine-tuning the model on K distinct randomized datasets to obtain K voters. Let f (x; 0) denote the model function with parameters 0, and $D_k = D_F \\oplus \\epsilon_k$ represent the k-th randomized dataset, where $\\epsilon_k \\sim N(0, \\sigma^2I)$. The K voters are obtained as:\n$\\theta_{F,k} = \\Omega(\\theta'_\\rho, \\tilde{D_k}), k = 1,..., K$                                                                                              (19)\nwhere \u03a9 represents the fine-tuning process, and $\\theta'_\\rho$ are the poisoned pre-trained model parameters.\nA.1.2 BIPHASED MODEL PARAMETER SMOOTHING\nOur biphased model parameter smoothing method consists of two phases:\nFine-tuning phase: at each iteration i,\n$\\theta^i_F = Clip_\\rho(\\theta^{i-1}_F - \\eta g(\\theta^{i-1}_F; B_i)) + \\epsilon_{top-H}$                                                                                              (20)\nInference phase:\n$\\theta_{F,k} = Clip_\\rho(\\theta) + \\epsilon_{k,top-H}, k = 1,..., K$                                                                                              (21)\nwhere $\\epsilon_{top-H}, \\epsilon_{k,top-H} \\sim N(0, \\sigma^2I)$ for the top H layers."}, {"title": "A.2 EQUIVALENCE PROOF", "content": "A.2.1 APPROXIMATE EQUIVALENCE IN THE FINE-TUNING PHASE\nTo facilitate the equivalence proof, we need to first introduce two assumptions:\nAssumption 2. We assume the learning rate \u03b7 is chosen appropriately such that the Clip operation rarely affects the parameter updates significantly. Under this assumption: at iteration i,\n$E[Clip_\\rho(\\theta^{i-1}_F - \\eta g(\\theta^{i-1}_F; B_i))] \\approx E[\\theta^{i-1}_F - \\eta g(\\theta^{i-1}_F; B_i)]$                                                                                              (22)\nAssumption 3. We assume that the statistical properties of $\\theta^{i-1}_F$ and $\\theta^i_F$ are similar enough that:\n$E[g(\\theta^i_F; B_i)] \\approx E[g(\\theta^i_F; B_i)]$                                                                                              (23)\nThis assumption is based on the following considerations: The standard approach introduces randomness by adding noise to the data. BMPS introduces randomness by adding noise to the parameters. Both methods optimize the same objective function and explore the parameter space in a similar manner over many iterations.\nTheorem 2. Under Assumption 2 and 3, the BMPS fine-tuning phase is approximately equivalent to training on randomized datasets in expectation.\nProof. Let x be an input sample and y its corresponding label. We consider the entire training process over I iterations.\nFor the standard approach, at iteration i, we have:\n$\\theta^i_F = \\theta^{i-1}_F - \\eta g(\\theta^{i-1}_F; B_i \\oplus \\epsilon_i)$                                                                                              (24)\nwhere $\\epsilon_i \\sim N(0, \\sigma^2I)$.\nFor BMPS, at iteration i, we have:\n$\\theta^i_F = Clip_\\rho(\\theta^{i-1}_F - \\eta g(\\theta^{i-1}_F; B_i)) + \\epsilon_{top-H}$                                                                                              (25)\nwhere $\\epsilon_{top-H} \\sim N(0, \\sigma^2I)$ for the top H layers.\nConsider the expectation of the parameter updates in both cases:\nFor the standard approach:\n$E[\\theta^i_F] = E[\\theta^{i-1}_F - \\eta g(\\theta^{i-1}_F; B_i \\oplus \\epsilon_i)]$                                                                                              (26)\n$= \\theta^{i-1}_F - \\eta E[g(\\theta^{i-1}_F; B_i \\oplus \\epsilon_i)]$                                                                                              (27)\nUsing a first-order Taylor expansion around $B_i$:\n$E[g(\\theta^{i-1}_F; B_i \\oplus \\epsilon_i)] \\approx E[g(\\theta^{i-1}_F; B_i) + \\nabla _{B_i}g(\\theta^{i-1}_F; B_i)^T\\epsilon_i]$                                                                                              (28)\n$= g(\\theta^{i-1}_F; B_i) + \\nabla _{B_i}g(\\theta^{i-1}_F; B_i)E[\\epsilon_i]$                                                                                              (29)\n$= g(\\theta^{i-1}_F; B_i)$ (since $E[\\epsilon_i] = 0$)                                                                                              (30)\nFor BMPS:\n$E[\\theta^i_F] = E[Clip_\\rho(\\theta^{i-1}_F - \\eta g(\\theta^{i-1}_F; B_i)) + \\epsilon_{top-H}]$                                                                                              (31)\n$= E[Clip_\\rho(\\theta^{i-1}_F - \\eta g(\\theta^{i-1}_F; B_i))]$ (since $E[\\epsilon_{top-H}] = 0$)                                                                                              (32)\nUnder above Assumptions 2 and 3, we can conclude that the expected parameter updates in both methods are approximately equivalent:\n$E[\\theta^i_F - \\theta^{i-1}_F] \\approx E[\\theta^i_F - \\theta^{i-1}_F]$                                                                                              (33)\nThis approximate equivalence holds for each iteration, and thus can be extended to the entire training process."}, {"title": "A.2.2 APPROXIMATE EQUIVALENCE IN THE INFERENCE PHASE", "content": "Building upon the results from the fine-tuning phase, we now extend our analysis to the inference phase. Recall that in the fine-tuning phase, we established the approximate equivalence between BMPS and the standard randomized smoothing approach in terms of their expected parameter updates:\n$E[\\theta^i_F - \\theta^{i-1}_F] \\approx E[\\theta^i_F - \\theta^{i-1}_F]$                                                                                              (34)\nThis equivalence suggests that the final model parameters obtained from BMPS (\\theta_F) should have similar statistical properties to those obtained from the standard randomized smoothing approach. Furthermore, we demonstrated that adding noise to parameters (in BMPS) and adding noise to data (in the standard approach) produce similar effects during training.\nExtending this reasoning to the inference phase, we introduce an additional assumption that builds directly on these findings:\nAssumption 4. Given the equivalence established in the fine-tuning phase, we assume that the effect of adding noise to the parameters during inference in BMPS is approximately equivalent to the effect of fine-tuning on randomized datasets in the standard framework. Formally, for each k = 1, ..., K:\n$\\theta + \\epsilon_{k,top-H} \\sim \\Omega(\\theta'_\\rho, D_F \\oplus \\epsilon_k)$                                                                                              (35)\nwhere \u03a9 represents the fine-tuning process, $\\theta'_\\rho$ are the poisoned pre-trained model parameters, $D_F$ is the fine-tuning dataset, and $\\epsilon_k$ is the noise added to the dataset in the standard framework.\nThis assumption is a natural extension of our findings from the fine-tuning phase, positing that the equivalence between parameter noisification and data randomization continues to hold during inference.\nWith this foundation, we can now proceed to prove the approximate equivalence of BMPS and the standard randomized smoothing framework in the inference phase.\nTheorem 3. Under Assumption 2, 3, and 4, the BMPS inference phase is approximately equivalent to the standard randomized smoothing framework described in Section 4.1.\nProof. Recall from Section 4.1, the standard randomized smoothing framework involves fine-tuning K models on K distinct randomized datasets to obtain K voters. The smoothed model $\\hat{f}$ is defined as follows:\n$\\hat{f}(x') = \\underset{y \\in Y}{\\text{arg max}} \\sum_{k=1}^{K}\\mathbb{1}(f(x_k;\\theta_{F,k})  = y)$                                                                                              (36)\nwhere $x_k = x' \\oplus u_k, \\theta_{F,k} = \\Omega(\\theta'_\\rho, D_F \\oplus \\epsilon_k)$.\nFor BMPS in the inference phase, we have:\n$\\theta_{F,k} = Clip_\\rho(\\theta) + \\epsilon_{k,top-H}$                                                                                              (37)\nwhere $\\epsilon_{k,top-H} \\sim N(0, \\sigma^2I)$ for the top H layers.\nUnder Assumption 2, we have:\n$\\theta_{F,k} \\approx \\theta + \\epsilon_{k,top-H}$                                                                                              (38)\nThen, applying Assumption 4, we can see that the K voters in BMPS:\nf(x; \\theta_{F,k}) \\approx f(x; \\Omega(\\theta'_\\rho, D_F \\oplus \\epsilon_k))                                                                                              (39)\nare approximately equivalent to the K voters in the standard randomized smoothing framework.\nFurthermore, BMPS also applies randomized input perturbation $x' \\oplus u_k$ during inference, which is identical to the standard framework."}, {"title": "A.2.3 CONCLUSION OF EQUIVALENCE"}]}