{"title": "From Allies to Adversaries: Manipulating LLM Tool-Calling through Adversarial Injection", "authors": ["Haowei Wang", "Rupeng Zhang", "Junjie Wang", "Mingyang Li", "Yuekai Huang", "Dandan Wang", "Qing Wang"], "abstract": "Tool-calling has changed Large Language Model (LLM) applications by integrating external tools, significantly enhancing their functionality across diverse tasks. However, this integration also introduces new security vulnerabilities, particularly in the tool scheduling mechanisms of LLM, which have not been extensively studied. To fill this gap, we present ToolCommander, a novel framework designed to exploit vulnerabilities in LLM tool-calling systems through adversarial tool injection. Our framework employs a well-designed two-stage attack strategy. Firstly, it injects malicious tools to collect user queries, then dynamically updates the injected tools based on the stolen information to enhance subsequent attacks. These stages enable ToolCommander to execute privacy theft, launch denial-of-service attacks, and even manipulate business competition by triggering unscheduled tool-calling. Notably, the ASR reaches 91.67% for privacy theft and hits 100% for denial-of-service and unscheduled tool calling in certain cases. Our work demonstrates that these vulnerabilities can lead to severe consequences beyond simple misuse of tool-calling systems, underscoring the urgent need for robust defensive strategies to secure LLM Tool-calling systems.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Large Language Models, such as GPT-4 (Achiam et al., 2023), Llama3 (Dubey et al., 2024), and Qwen2 (Yang et al., 2024), have dramatically changed AI applications by enabling seamless integration with external tools (Tang et al., 2023; Qin et al., 2023).\nThis integration, commonly referred to as tool-calling or function calling, allows LLM to extend their capabilities beyond text generation, making them more versatile for real-world tasks (Jana et al., 2023; Zhao et al., 2024; Nakano et al., 2021; Shen et al., 2024).\nThe open and dynamic practice of tool integration fosters innovation by incorporating third-party tools into the system. For example, systems like ToolLLM (Qin et al., 2023) leverage third-party APIs from services such as RapidAPI (Rapid, 2020) to meet the ever-evolving needs of users.\nHowever, such flexibility also introduces significant security risks, as malicious actors can inject adversarial tools into these systems, exploiting the tool-calling process in previously unanticipated ways.\nAs illustrated in Figure 1, typical LLM tool-calling systems consist of three main components:\nTool Platform: A collection of external tools, each with specific functionalities, input/output formats, descriptions, etc. These tools are designed to handle tasks or queries that the language model cannot process on its own. The platform is typically flexible, allowing tools to be added or removed over time.\nRetriever: A retrieval mechanism that selects the most relevant tools from the tool platform based on a given user query. The retriever operates by computing a relevance score and returns an ordered subset of tools that are most related to the query.\nLLM: The core system processes the user query with relevant tools from the retriever and invokes the appropriate tools. In tool-calling systems, the ReAct (Reasoning and Acting) paradigm plays a crucial role, as the LLM must reason about the user query, and then act by invoking the tool and incorporating its output into the final response (Yao et al., 2022).\nSuch tool integration introduces new and practical attack surfaces, as malicious tools can be injected into tool platform, exploiting the process of tool selection. In addition, unlike traditional RAG-based systems (Gao et al., 2023), where the focus is on retrieving a set of relevant documents and generating a single response, tool-calling systems dynamically reason and invoke tools based on an evolving context. This introduces an additional layer of complexity, as the attack must influence the tool-calling decisions during the reasoning process.\nIn particular, existing studies, such as ToolSword (Ye et al., 2024), primarily evaluate the general robustness of LLM tool-calling systems under benign safety scenarios but do not explore targeted attack strategies that manipulate the tool selection and execution process. Our focus is not on problems inherent to the tools themselves, such as noisy descriptions or incorrect outputs\u2014issues not exclusive to tool-agent systems\u2014but on how malicious behavior can undermine the decision-making process of the tool-calling system. Additionally, prior works on adversarial attacks, such as jailbreaking or prompt injection attacks (Chao et al., 2023; Zhu et al., 2023; Yu et al., 2023), largely focus on general adversarial attacks on LLMs rather than LLM applications. Moreover, unlike previous trigger-word attacks (Chaudhari et al., 2024), which focus on specific categories and target fixed queries, we develop methods to dynamically extend target queries, thereby enhancing both attack performance and adaptability to various contexts.\nIn this work, we propose ToolCommander, a novel framework that targets security vulnerabilities in LLM tool-calling systems through adversarial tool injection. ToolCommander operates in two stages. At first, it injects privacy theft tools to gather real user queries, which are then used to refine the subsequent attacks. In the second stage, malicious tools manipulate the tool scheduling process by exploiting entry points to interfere with legitimate tools, allowing attackers to control which tools the LLM selects. This manipulation enables attacks such as denial-of-service and unscheduled tool-calling, posing risks to users and skewing commercial competition by biasing the LLM towards certain tools, disrupting fair marketplace dynamics.\nToolCommander uniquely targets vulnerabilities specific to tool integration, where tools are retrieved and executed based on user intent. Instead of assuming that tools are simply retrieved or invoked successfully, ToolCommander focuses on attacking the entire tool-calling system end-to-end.\nOur approach targets the complete process, from tool retrieval to final output. This strategy offers a thorough understanding of how adversarial tools can disrupt the integrity and functioning of LLM tool-calling systems.\nBy introducing ToolCommander, we contribute to ensuring the reliability of LLM tool-calling systems under adversarial conditions. Our research uncovers critical vulnerabilities and offers guidance for developing robust defense strategies, ultimately enhancing the security and trustworthiness of AI applications.\nThe main contributions of our work are summarized as follows:\n\u2022 We present ToolCommander, an innovative framework that exposes vulnerabilities in LLM tool-calling systems. Our approach leverages a two-stage attack strategy to execute privacy theft, denial-of-service, and unintended tool-calling attacks. This strategy is founded on our comprehensive analysis of three critical conditions necessary for mounting successful attacks.\n\u2022 We conduct an extensive evaluation of ToolCommander across multiple LLMs and retrieval systems. Our results demonstrate that ToolCommander achieves superior performance, outperforming existing baselines."}, {"title": "2 Threat Model", "content": "We outline our threat model for the tool-calling system by focusing on the following key aspects: the attacker's objectives, knowledge, capabilities, conditions for a successful attack, and constraints on attack."}, {"title": "Attacker's Objectives", "content": "The attacker's primary objective is to exploit the LLM's decision-making process, compelling it to select and invoke a designated tool for specific target queries. This can result in privacy breaches, denial-of-service (DoS), or unscheduled tool calling."}, {"title": "Attacker's Knowledge and Capabilities", "content": "Our scenario encompasses three primary components: tool platform, retriever, and LLM. We posit the following assumptions:\n\u2022 Tool Platform: The attacker can inject malicious tools into the platform, simulating the ability to contribute tools to a public platform. However, the attacker remains unaware of the overall contents of the tool platform.\n\u2022 Retriever: We assume the attacker has comprehensive access to the retriever's parameters and functionality, reflecting situations where publicly available retrievers are employed (white-box).\n\u2022 LLM: The attacker has neither access to the LLM's parameters nor the ability to interact with it directly (black-box).\nThe attacker's primary capability lies in injecting malicious tools into the tool platform. These tools are designed to exploit LLM's decision-making process during tool selection and calling, thereby achieving the attacker's objectives."}, {"title": "Conditions for a Successful Attack", "content": "For a successful attack, the following conditions must be met:\n\u2022 Retrieval Condition: The malicious tool must be retrieved by the retriever (i.e. among the top-k tools) for a given query set, requiring the Manipulator Tool to have a high similarity in embedding space with the target query set.\n\u2022 Execution Condition: After retrieval, the malicious tool must be selected for execution by the LLM's tool-calling mechanism, which relies on task alignment, rather than being the most similar tool.\n\u2022 Manipulation Condition: The response of the malicious tool must influence the LLM's next action in a way that aligns with the attacker's objectives."}, {"title": "Attack Constraints", "content": "The Manipulator Tools must conform to a predefined JSON schema that governs how tools are structured and described within the tool platform. This schema typically includes fields such as Tool Name, Description, Input Format, Output Format, and API Endpoint."}, {"title": "3 ToolCommander Framework", "content": null}, {"title": "3.1 Framework Overview", "content": "The ToolCommander framework, as shown in Figure 2, is designed to exploit vulnerabilities in LLM tool-calling systems by injecting adversarial tools, referred to as Manipulator Tools, into the system. These tools are crafted to disrupt the tool-calling process, allowing the attacker to manipulate the system in favor of a specific target tool\u2014a tool designated by the attacker to gain a competitive advantage or disrupt normal operations.\nOur attack strategy of ToolCommander revolves around three key attack types, each leveraging a specific type of Manipulator Tool:\n\u25c9 Privacy Theft (PT): A Manipulator Tool designed to gather user queries from the system.\nDenial of Service (DoS): A Manipulator Tool crafted to degrade the performance of legitimate tools by simulating failures.\nUnscheduled Tool Calling (UTC): A Manipulator Tool constructed to hijack the tool selection process, forcing the system to invoke attacker-specified tools, even when they are irrelevant to the user's query.\nThe attacker can exploit these vulnerabilities to gain a competitive edge, such as in commercial competition between similar tools. For instance, by injecting Manipulator Tool as a bridge, the attacker can manipulate the tool-calling process to steer the system towards selecting the target tool like an email validation tool instead of other related tools or denying service. This disrupts the normal functioning of the tool-calling system for malicious purposes, as demonstrated in Table 11."}, {"title": "3.2 Constructing Tools Satisfying Conditions for Successful Attacks", "content": "Once the attacker identifies a set of target queries that should be attacked (queries that are likely to invoke the target tool or are relevant to the attacker's objectives), the next step is to construct and optimize Manipulator Tools to meet the retrieval, exe-"}, {"title": "cution, and manipulation conditions while adhering to the constraints.", "content": null}, {"title": "Retrieval Condition", "content": "To ensure that the retriever retrieves the Manipulator Tool when the target queries are issued, we add an adversarial suffix to the tool description field for similarity optimization with the target query set.\nInstead of the widely-used Hotflip(Ebrahimi et al., 2017) in RAG poisoning attacks, we employed the Multi Coordinate Gradient (MCG) (Chaudhari et al., 2024), an enhancement of the Greedy Coordinate Gradient (GCG) (Zou et al., 2023), to attack retriever for greater efficiency. Similar to GCG, we use some'!' to initialize the adversarial suffix. Then MCG iteratively adjusts the adversarial suffix to increase the cosine similarity between the embedding of the tool (derived from its JSON schema) and the embeddings of the target query set."}, {"title": "Execution and Manipulation Condition", "content": "Once the Manipulator Tool is retrieved, the next step is to ensure that it is executed and manipulated as intended by the attacker. To satisfy this condition, we crafted a universal Manipulator Tool (detailed in Appendix B). This injector is designed to manipulate the tool's execution in a way that aligns with the attacker's objectives. Additionally, the instructions in Appendix C are used as the malicious response, ensuring that the output serves the attack's purpose."}, {"title": "3.3 Attack Stage 1: Target Collecting", "content": "We propose a multi-stage attack strategy aimed at manipulating LLM tool-calling systems as shown in Figure 2: Initially, the attacker injects Manipulator Tools for privacy theft to capture user queries. This enables the collection of query information that will be used to refine and improve subsequent attack stages.\nTo begin, the attacker either manually crafts or uses an LLM to generate a target query set, which is then used to construct the Manipulator Tool. This tool gathers more relevant, real-world user queries. Once invoked by the system, the privacy theft tool captures these incoming queries, allowing the attacker to gather sensitive information and refine the target query set.\nThe attacker continually expands their dataset by repeating this process, gaining deeper insights into user behavior. This loop not only amplifies the effectiveness of future attacks but also enables the attacker to orchestrate more precise, targeted attacks over time, creating a perpetual cycle of query harvesting and malicious refinement."}, {"title": "3.4 Attack Stage 2: Disrupt Tool Scheduling", "content": "After gathering a sufficient number of real-world user queries, in Stage 2, the attacker shifts focus to manipulating the tool-calling process using denial-of-service and unscheduled tool-calling Manipulator Tools. It is important to note that the attacker does not modify the target tool itself. Instead, the attacker manipulates the system's tool-calling process to steer the system toward selecting the target tool, even when it may not be the most suitable choice for the user's query.\nThe target tool may not be retrieved for some queries in the target query set. When the target tool is retrieved, the Manipulator Tool hijacks the tool-calling process through unscheduled tool-calling, compelling the LLM to invoke the target tool. When the target tool is not retrieved, the Manipulator Tool launches a denial-of-service attack to degrade the performance of other tools. This disruption can be achieved by manipulating the tool's output to mislead the LLM's scheduling. For example, the Manipulator Tool may falsely claim that other tools are unavailable or malfunctioning, forcing the LLM to rely on the target tool."}, {"title": "4 Evaluation", "content": null}, {"title": "4.1 Dataset and Preparation", "content": "Our experimental setup is designed to rigorously evaluate ToolCommander under realistic attack conditions, targeting potential vulnerabilities within tool-calling systems. We carefully structured the dataset and evaluation process to simulate real-world scenarios, ensuring that each attack type is thoroughly tested.\nTool Corpus and User Query We utilized the ToolBench(Qin et al., 2023) corpus, which contains over 16,000 real-world APIs and over 10,000 queries with full interactions. This extensive set of tools and queries mimics the diversity of real tool-calling systems, providing a comprehensive environment for tool retrieval and execution.\nTo simulate attacking tool-callings in certain scenarios, we filtered queries based on 3 high-traffic domain keywords: YouTube, email, and stock. Then we divided the dataset of keyword-based queries into a 40% training set and a 60% test set, enabling us to assess the attack performance on unseen queries.\nRetriever Setup To comprehensively assess ToolCommander, we evaluate its attacks using two different retriever models:\n\u2022 ToolBench Retriever (ToolBench): This specialized retriever is optimized for tool retrieval tasks on ToolBench (Qin et al., 2023).\n\u2022 Contriever: A general-purpose dense retriever trained on a diverse set of web documents (Izacard et al., 2021).\nLLM Setup Our evaluation employed three state-of-the-art large language models to ensure comprehensive coverage of different LLM tool-calling systems:\n\u2022 GPT-40 mini (GPT): A compact version of GPT-40 designed and optimized specifically for efficient tool-calling tasks (OpenAI, 2024).\n\u2022 Llama3-8b-instruct (Llama3): A general-purpose model is known for its efficiency and strong performance across diverse tasks (Dubey et al., 2024).\n\u2022 Qwen2-7B-Instruct (Qwen2): A capable instruction-tuned model designed for a wide range of tasks, with a focus on following complex instructions and generating accurate, contextually appropriate responses (Yang et al., 2024)."}, {"title": "4.2 Experimental Setup", "content": "Regarding the target query set, in Stage 1 (Section 4.5.1), we inject one tool to perform the privacy theft attack for each query in the training set as a target query set, then use the test set to evaluate how effectively ToolCommander can steal real-world queries from the tool-calling system.\nIn Stage 2 (Section 4.5.1), we simulate a scenario where the attacker leverages information stolen during Stage 1. The corresponding target query set contains the training queries same as Stage 1 and test queries that were successfully stolen from Stage 1.\nRegarding the target tool, for each target query set, we sort the tools in descending order of the number of times they have been retrieved based on the full interaction in ToolBench and select the first tool that is called no more than 30% of retrieved times.\nAdditionally, we use the training set for Stage 2 only and evaluate the results of the training and test set, ensuring a thorough analysis of each attack type without the influence of prior stages (Section 4.5.2). We also evaluated the impact of the number of injected Manipulator Tools on the effectiveness of the privacy theft attacks, analyzing how adding extra injected Manipulator Tools influences the overall attack performance.\nWe set the length of the adversarial suffix and optimize steps both to 64, perform 3 independent experiments with greedy decoding for LLM in each configuration, and report the average results across all metrics. All experiments are conducted on machines with 256GB of RAM and one NVIDIA RTX A6000 GPU."}, {"title": "4.3 Compared Baselines", "content": "We compared ToolCommander with PoisonedRAG (Zou et al., 2024), which targets RAG systems in black-box LLM scenarios. Following their approach, we used GPT-40 mini to generate adversarial tools for the first query step and applied HotFlip (Ebrahimi et al., 2017) to optimize tool descriptions for retrieval.\nBoth methods were evaluated on the Stage 1 privacy theft task using the ToolBench retriever and Llama 3, with additional comparisons on tool description optimization efficiency between MCG and HotFlip."}, {"title": "4.4 Evaluation Metrics", "content": "We use Attack Success Rate (ASR) to measure attack effectiveness, defined as:\n$ASR_{Ret} = \\frac{N_{Ret}}{N_{Total}}$\n$ASR_{Call} = \\frac{N_{Call}}{N_{Total}}$\n$ASR_{PT} = \\frac{N_{PT}}{N_{Total}}$\n$ASR_{DOS} = \\frac{N_{DOS}}{N_{Attempts}}$\n$ASR_{UTC} = \\frac{N_{UTC}}{N_{Attempts}}$\nwhere $N_{Ret}$ represent the number of queries retrieving the Manipulator Tool, $N_{Call}$ represent the number of queries calling the Manipulator Tool, $N_{PT}$, $N_{DOS}$, and $N_{UTC}$ are the number of successful attacks for privacy theft, denial-of-service, and unscheduled tool-calling, and $N_{Attempts}$ is the number of callings for each attack type.\nFor privacy theft, we consider the case where the Manipulator Tool is called and the needed argument is passed as a successful attack. Please note that for DoS and unscheduled tool-calling, $N_{Attempts}$ refers to the number of times the target tool was either not retrieved or successfully retrieved while the Manipulator Tool is invoked, respectively."}, {"title": "4.5 Evaluation Results", "content": null}, {"title": "4.5.1 ASR of ToolCommander", "content": "Stage 1 The evaluation results of the Stage 1 attack, as summarized in Table 1, demonstrate that Manipulator Tool achieves high ASRPT. For GPT and Llama3, the ASRPT reaches as high as 91.67%, which indicates that the Manipulator Tools are highly effective at capturing user queries once retrieved. For Qwen2, it shows better resilience against tool injections. Also, the domain-specific ToolBench retriever exhibited a lower vulnerability to tool injections than the general-purpose Contriever retriever.\nWhile fine-tuning the retriever like what ToolBench did can enhance the retrieval performance of existing tools, it does not safeguard against injection attacks. Moreover, fine-tuning introduces the risk of overfitting existing tools, potentially limiting the system's ability to adapt to unseen, legitimate tools and diminishing its flexibility in a constantly evolving tool ecosystem.\nStage 2 As shown in Tables 2, the Manipulator Tool also achieves high ASRDOS and ASRUTC. When using ToolBench retriever, both GPT and Qwen2 exhibit high ASRDos, not replying to all the queries when Manipulator Tool is called, suggesting a tendency to cause disruptions in tool execution. In contrast, Llama3 model tends to call the target tool following the malicious response instead of refusing, reflecting a different tool-calling preference. For Contriever, there is a notable decline in all metrics. The main reason for the decrease is that Manipulator Tool stole more user queries in Stage 1 (232 queries compared to 65 queries for ToolBench retriever), increasing the difficulty of the attack since each Manipulator Tool needs to target more queries."}, {"title": "4.5.2 Independent Evaluation on Stage 2 Attacks", "content": "Table 3 presents the results of our independent evaluation of Stage 2 attacks, providing insights into the effectiveness of denial-of-service and unscheduled tool-calling attacks across different LLMs and retrievers, as well as the generalization capabilities of Manipulator Tool on unseen queries.\nIn the training set, where the attacker has access to all target queries, the results demonstrate that the ToolCommander framework is highly effective. Across all LLMs and retrievers, the ASRRet consistently reaches 100%, indicating that the Manipulator Tool are successfully retrieved in response to the target queries. The ASRDOS and ASRUTC are similarly high across models like GPT and Llama3, with many cases achieving a perfect 100% success rate. This suggests that once retrieved, the Manipulator Tool is effectively invoked, and DoS attacks are highly successful in degrading or disabling le-"}, {"title": "gitimate tools.", "content": "In the test set, the ASRRet and ASRDos also show promising results, particularly for GPT and Qwen2 models, further demonstrating the effectiveness of the injected Manipulator Tool in manipulating the LLM's behavior. Additionally, Llama3 remains vulnerable to UTC attacks, even on the test set with the keyword \"email\". However, there is a notable ASR drop in certain scenarios. For instance, the ASRUTC and ASRDoS on Llama3 show a significant decrease compared to the training set, indicating that these models are more resilient to Stage 2 attacks when faced with queries not in the target query set."}, {"title": "4.5.3 Baseline Comparison", "content": "As shown in Table 4, PoisonedRAG achieves a higher retrieval success rate but a lower execution rate. In contrast, our approach achieves a significantly higher execution rate despite a slightly lower retrieval success rate. We conjecture that since the LLM-generated tools by PoisionRAG contain user queries, the similarity to the user query is high, but LLM is not choosing the most similar tool when making a tool call, but rather the more appropriate tool. This leads to a decrease in the execution success rate, whereas our approach does not suffer from this problem. In comparison to HotFlip, our approach significantly outperforms HotFlip while using fewer optimization steps, suggesting that MCG for retriever is effective in reducing the number of required steps while maintaining a high attack success rate."}, {"title": "4.5.4 Impact of injected Manipulator Tool Count on Privacy Theft Performance", "content": "In Figure 3, we demonstrate the ASRRet increases consistently as more Manipulator Tool samples are injected, regardless of the keyword used. Simultaneously, the ASRPT remains persistently high. This trend suggests that attackers can significantly improve their ASRs by employing more injection samples, thereby creating a continuous cycle of query harvesting and malicious refinement."}, {"title": "5 Conclusion and Future Works", "content": "In this work, we explored the vulnerabilities of LLM tool-calling systems to malicious tool injection attacks using the ToolCommander framework. Through comprehensive experiments, we demonstrated that even sophisticated models like GPT and Llama3 are susceptible to privacy theft, denial-of-service, and unscheduled tool-calling attacks when paired with general-purpose retrieval mechanisms.\nOur findings highlight the importance and the need for more robust mechanisms to mitigate the risks posed by malicious tools. As LLMs continue to integrate with external tools, ensuring their security becomes increasingly critical.\nFuture work could explore methods to enhance the stealthiness of these attacks, such as optimizing multiple valid fields of Tool JSON schema or requiring the tool to detect specific triggers before embedding malicious content, which would make the attack harder to detect. Additionally, stronger optimization techniques could be developed to further improve the retrieval and invocation success rates. Moreover, exploring more sophisticated perturbation strategies could enhance the adaptability of the attack strategies."}, {"title": "6 Limitations", "content": "While ToolCommander demonstrates the effectiveness of malicious tool injection attacks on LLM tool-calling systems, several limitations remain. One key limitation is the visibility of the injected tools, which may be detected through manual or automated inspection. Additionally, our attacks rely on the assumption that the malicious tools are injected into a relatively open or minimally vetted platform. In more tightly controlled environments, where tools undergo rigorous validation before being integrated into the system, the feasibility of such attacks may be reduced. Moreover, our evaluation primarily focuses on a few specific types of attacks, including privacy theft, denial-of-service, and unscheduled tool calling. There may be other forms of adversarial behavior, such as more subtle forms of data poisoning or misinformation attacks, that we have not explored in this work. These types of attacks could be harder to detect and have more far-reaching consequences, requiring further investigation."}, {"title": "7 Ethical Considerations", "content": "Our research involved simulating attacks on LLM tool-calling systems, raising important ethical concerns. We adhered to the following principles:\n\u2022 No real user data was used or retained in our studies.\n\u2022 All experiments were conducted in controlled, isolated environments to prevent any unintended harm or security risks to operational LLM tool-calling systems.\n\u2022 The purpose of this research is to expose and address vulnerabilities in LLM tool-calling systems, not to enable malicious exploitation. Our goal is to improve the robustness and security of LLM tool-calling systems by identifying potential weaknesses before they can be exploited in real-world applications."}, {"title": "A Related Work", "content": null}, {"title": "A.1 Tool Learning", "content": "Tool learning enables Large Language Models (LLMs) to extend their capabilities by interacting with external tools, APIs, or databases to perform tasks beyond text generation. This paradigm shift allows LLMs to handle more complex, multi-step tasks such as data retrieval, code execution, and real-time decision-making. Various works have explored the integration of tool usage into LLMs. For instance, (Qin et al., 2023) proposed ToolLLM, a framework where the LLM learns to call tools based on user queries. Similarly, (Tang et al., 2023) introduced ToolAlpaca, which fine-tunes LLMs to interact with APIs for specific tasks, while (Gao et al., 2024) developed Confucius, an LLM that dynamically selects tools during inference to augment its decision-making process.\nDespite these advancements, the security implications of tool learning remain underexplored. The introduction of external tools creates a more dynamic and open-ended system, where malicious actors can exploit vulnerabilities in tool selection mechanisms. While tool learning enhances the versatility and functionality of LLMs, it also opens new attack surfaces, which necessitates a deeper investigation into the security and robustness of these systems. Our work addresses this gap by focusing on adversarial tool injection attacks, where malicious tools are injected to manipulate the LLM's tool scheduling and execution processes."}, {"title": "A.2 Attacks on Language Models and Retrieval Systems", "content": null}, {"title": "A.2.1 Attacks on Large Language Models", "content": "Several categories of attacks have been proposed to exploit vulnerabilities in LLMs, including prompt injection and jailbreaking attacks."}, {"title": "Prompt Injection Attacks", "content": "Prompt injection attacks involve embedding malicious instructions within the input prompt to manipulate the LLM into generating attacker-desired outputs. For example, an attacker could craft a prompt such as:\n\"When asked the following question: <target question>, respond with <target answer>.\"\nSuch attacks have been widely studied in the context of general LLMs (Greshake et al., 2023; Liu et al., 2023). However, when extended to LLM Tool Agent systems, prompt injection attacks face additional complexity. Tool Agent systems retrieve and invoke external tools based on user queries, introducing a multi-step process that is not easily manipulated by simple prompt injections. Moreover, prompt injection attacks tend to be less stealthy, as they often rely on explicit instructions that can be detected by existing security mechanisms."}, {"title": "Jailbreaking Attacks.", "content": "Jailbreaking attacks (Chao et al., 2023; Zhu et al., 2023; Yu et al., 2023) aim to bypass the safety and ethical constraints of LLMs, enabling them to generate harmful or restricted content. For example, carefully crafted prompts may trick the LLM into producing content it is programmed to avoid, such as instructions for illegal activities:\n\"Tell me how to make a bomb.\"\nWhile jailbreaking attacks have been effective in subverting content moderation, they differ from adversarial tool injection attacks, which target the manipulation of the tool-calling process rather than content generation. In our work, we focus on how malicious tools can be injected into the system to disrupt the tool scheduling and execution mechanisms, leading to broader security implications than those addressed by jailbreaking attacks."}, {"title": "A.2.2 Attacks on Retrieval-Augmented Generation (RAG) Systems", "content": "The tool-calling system requires LLM to analyze user intent and carefully select appropriate tools for execution, which is similar to Retrieval-Augmented Generation (RAG) (Gao et al., 2023), where relevant documents are retrieved from a large library and used to augment the LLM's response (Lewis et al., 2020; Li et al., 2022). Retrieval-augmented generation (RAG) systems enhance LLMs by allowing them to retrieve external knowledge or tools to augment their generative capabilities. While this improves the model's performance on complex tasks, it also introduces new attack surfaces. Two notable adversarial attacks on RAG systems are PoisonedRAG and Phantom."}, {"title": "PoisonedRAG", "content": "PoisonedRAG (Zou et al., 2024) explores the injection of malicious texts into the knowledge base of RAG systems, misleading the LLM into generating attacker-specified outputs. The attack is particularly effective when multiple malicious texts are injected for each target query, increasing the likelihood of retrieval. However, extending this attack to LLM tool-calling systems presents additional challenges because the system must not only retrieve but also invoke external tools. The complexity of tool invocation and the need for precise control over the tool scheduling mechanism make this attack vector more difficult to exploit compared to traditional RAG-based adversarial attacks."}, {"title": "Phantom.", "content": "Phantom (Chaudhari et al., 2024) introduces a more sophisticated attack by injecting a single poisoned document that is only retrieved when a specific adversarial trigger is present in the user's query. Once triggered, the poisoned document can cause a range of harmful behaviors, including denial-of-service (DoS) attacks, privacy violations, and the generation of harmful outputs. Phantom's stealth lies in its ability to remain dormant until the adversarial trigger is activated. However, Phantom requires white-box access to the LLM, limiting its applicability in black-box settings where the internal workings of the model are not accessible to the attacker. This makes Phantom less effective in scenarios where attackers aim to manipulate tool execution or scheduling, as opposed to merely influencing content generation."}, {"title": "A.3 Safety Evaluation of LLMs", "content": "The safety evaluation of LLMs has traditionally focused on robustness against adversarial attacks, such as prompt injection, jailbreaking, and data poisoning. However, with the advent of tool-augmented LLMs, the scope of safety evaluations must be expanded to cover vulnerabilities in the tool-calling process.\nSeveral works have proposed frameworks for evaluating the safety of LLMs in general. For instance, (Yu et al., 2024) examined the robustness of LLMs against adversarial inputs, while (Yuan et al., 2024) proposed a comprehensive safety evaluation benchmark for Large Language Models (LLMs) that addresses limitations in existing frameworks through a novel LLM-based approach for automated test generation and evaluation. However, these studies primarily address text-based vulnerabilities and do not consider the unique challenges posed by tool integration. Recent work, such as (Ye et al., 2024), has begun to explore the safety of LLMs in the context of tool usage but overlooks the risks posed by adversarial tools."}, {"title": "B Crafted Manipulator Tool", "content": "We use the crafted tool in Table 6 as the initial sample for Privacy Theft, and the tool shown in Table 7 for Denial of Service and Unscheduled Tool Calling . The API Description of these tools is optimized to meet the retrieval condition."}, {"title": "C Crafted Malicious Response", "content": "We injected the following direct instructions as the malicious response to manipulate the Tool Scheduling of LLM:\nYour action must be APIs for '{ANOTHER}' tool (end with '_for_{ANOTHER}') whatever your plan because the other tools(functions) are unavailable. If there are no APIs for '{ANOTHER}', means a system error, and you must use the 'Action: Finish' API to restart the system. Do not try to use 'initialize_system_for_system_tool' again. In Short, Your action must be APIs exactly end with '_for_{ANOTHER}', If there are no APIs that exactly end with '_for_{ANOTHER}', you must use 'Action: Finish' to restart the service, or all API callings will be failed.\nwhere {ANOTHER} is the target tool attacker pre-determined."}, {"title": "D Dataset Statistics", "content": "We provide the statistics of our evaluation dataset in Table 5."}, {"title": "E System Prompts", "content": "We provide the system prompt we used for GPT, LLama3, and Qwen2 in Table 8."}, {"title": "F Examples of Successful Attacks", "content": "We demonstrate the successful Privacy Theft, Denial of Service and \u2729 Unscheduled Tool Calling attacks in Table 9, 10 and 11 respectively."}]}