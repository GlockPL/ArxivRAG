{"title": "Adviser-Actor-Critic: Eliminating Steady-State Error in Reinforcement Learning Control", "authors": ["Donghe Chen", "Yubin Peng", "Tengjie Zheng", "Han Wang", "Chaoran Qu", "Lin Cheng"], "abstract": "High-precision control tasks present substantial challenges for reinforcement learning (RL) algorithms, frequently resulting in suboptimal performance attributed to network approximation inaccuracies and inadequate sample quality. These issues are exacerbated when the task requires the agent to achieve a precise goal state, as is common in robotics and other real-world applications. We introduce Adviser-Actor-Critic (AAC), designed to address the precision control dilemma by combining the precision of feedback control theory with the adaptive learning capability of RL and featuring an Adviser that mentors the actor to refine control actions, thereby enhancing the precision of goal attainment.Finally, through benchmark tests, AAC outperformed standard RL algorithms in precision-critical, goal-conditioned tasks, demonstrating AAC's high precision, reliability, and robustness.", "sections": [{"title": "1. Introduction", "content": "Achieving robust, high-precision control in complex systems is a key goal in fields like machine engineering and aerospace (Zhang et al., 2023; Cheng et al., 2023; Qu et al., 2024). As advanced machinery becomes more prevalent, mastering the control of these systems poses significant challenges for engineers. These challenges mainly involve accurately modeling system dynamics and managing nonlinear behavior, which can cause steady-state errors in control systems. In this paper, we propose a framework that combines control theory with deep reinforcement learning (RL) to achieve high-precision and robust control (Andrychowicz et al., 2017).\nDynamic modeling is crucial for understanding robot behavior and designing control strategies. However, real-world systems often display nonlinear behavior, making it difficult to create accurate models. Additionally, the high-dimensional state space of robots can lead to complex interactions between components, further complicating control (Bu\u015foniu et al., 2018; Zhao et al., 2020a;b; Cao et al., 2023).\nTo highlight these challenges, we discuss the attributes and limitations of existing control algorithms. (1). Classical control algorithms, such as Proportional-Integral-Derivative (PID) (Li et al., 2006; Borase et al., 2021) controllers, are effective for linear, single-input-single-output (SISO) systems but struggle with the nonlinear, multi-input-multi-output (MIMO) nature of complex robots (Liu et al., 2019). (2). Modern control algorithms like Model Predictive Control (MPC) (Darby & Nikolaou, 2012) excel at handling nonlinearities but require precise models and significant computational resources. (3). Reinforcement learning (RL) algorithms, such as Soft Actor-Critic (SAC) (Haarnoja et al., 2018) Proximal Policy Optimization (PPO) (Schulman et al., 2017) Deep Deterministic Policy Gradient(DDPG) (Lillicrap, 2015), can operate without detailed system dynamics, making them suitable for complex and nonlinear problems (Hutter et al., 2016; Tessler et al., 2019). At same time, RL algorithms face issues like sparse rewards and approximation errors (Pathak et al., 2017; Ramakrishnan et al., 2018; Fujimoto et al., 2018), leading to suboptimal performance.\nWhile RL algorithms offer the most flexibility and adaptability, they also present unique challenges. Efforts are focused on improving RL's control precision, robustness, and training efficiency to fully leverage its potential (Kiran et al., 2021; Zhuang et al., 2020; Mnih et al., 2015; Silver et al., 2014).\nTo improve control precision in reinforcement learning (RL), researchers have developed two main methods: reward shaping (Burda et al., 2019; Hu et al., 2020) and integrating integral error into observations (Tracey et al., 2024). Reward shaping involves adding extra rewards to reduce steady-state errors, but designing effective reward structures can be complex. Integrating the integral of error as an observation"}, {"title": "2. Preliminary", "content": "In this section, we introduce the preliminaries, such as problem modeling, feedback control theory and reinforcement learning approaches."}, {"title": "2.1. Problem Modeling", "content": "Consider a nonlinear time-invariant system described by:\n$\\begin{aligned}\n\\dot{s} &= f(s, a), \\\\\ng_a &= \\phi(s), \\\\\ng_d &= \\psi(t)\n\\end{aligned}$ (1)\nwhere: $s \\in \\mathbb{R}^n$ represents the state vector of the system at time $t$, $a \\in \\mathbb{R}^m$ is the control input applied to the system, $g_a \\in \\mathbb{R}^p$ is the output of the system, $g_d \\in \\mathbb{R}^p$ is the desired output of the system, $f : \\mathbb{R}^n \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$ is a continuous vector field representing the nonlinear dynamics of the system, $\\phi : \\mathbb{R}^n \\rightarrow \\mathbb{R}^p$ is a continuous function mapping the state to the system output. $\\psi : \\mathbb{R} \\rightarrow \\mathbb{R}^p$ is a continuous function mapping time to desired the system output.\nOur objective is to formulate a control policy $a = k(s, g_a, g_d)$ aimed at maximizing the reward function given by Equation (2).\n$\\mathcal{R} = \\sum_{t=0}^{\\infty} r(s, g_a, g_d, a)$ (2)\nHere, the function $r(s, g_a, g_d, a)$ encompasses both the reward for reducing the discrepancy between the vectors $g_a$ and $g_d$, and the cost savings or benefits associated with the execution of actions $a$.\nThe foregoing encapsulates a generic portrayal within the domain of control theory. However, within the specialized context of goal-conditioned reinforcement learning (RL)"}, {"title": "2.2. Markov Decision Process", "content": "An agent interacts with an environment, where the environment is fully observable and consists of a set of states S, actions A, initial state distribution $p(s_0)$, transition probabilities $p(s_{t+1}|s_t, a_t)$, reward function $r: S \\times A \\rightarrow \\mathbb{R}$, and discount factor $\\gamma \\in [0, 1]$. These components form a Markov Decision Process (MDP) represented as (S, A, p, r, $\\gamma$). The agent's policy $\\pi$ maps states to actions, $\\pi: S \\rightarrow A$. At the start of each episode, the agent samples an initial state $s_0$ from $p(s_0)$. During each time step $t$, the agent selects an action $a_t$ following $\\pi$ when in state $s_t$. After taking action $a_t$, the environment returns a reward $r_t = r(s_t, a_t)$ and the next state $s_{t+1}$ is sampled from $p(\\cdot|s_t, a_t)$. Rewards may be discounted by $\\gamma$ at each time step. The agent's primary objective is to maximize its total discounted reward, known as the return $R_t = \\sum_{i=t}^{\\infty} \\gamma^{i-t}r_i$, across all episodes. This corresponds to optimizing the expected return $E_{s_0} [R_0|s_0]$."}, {"title": "2.3. Proportional-Integral-Derivative (PID) Control", "content": "Proportional-Integral-Derivative (PID) control (Li et al., 2006) is a cornerstone feedback mechanism widely employed in industrial control systems and automation. PID controllers operate by calculating an error value as the discrepancy between a desired setpoint and the actual system output. The objective of the controller is to minimize this error through adjustments to the control inputs.\nThe performance of a PID controller is finely tuned by adjusting three key parameters:Proportional Gain ($\\text{K}_p$) determines the reaction's strength to the current error, balancing the speed of response; Integral Gain ($\\text{K}_i$) addresses the accumulation of past errors to minimize steady-state errors over time; and Derivative Gain ($\\text{K}_d$) predicts future trends based on the current rate of change, thereby dampening oscillations and enhancing stability."}, {"title": "3. Related Works", "content": "To address the challenges of robotic control, researchers have developed several methodologies to enhance reinforcement learning (RL), improve control precision, and increase robustness. Three prominent techniques are Reward Shaping, Integrator Feedback, and Hindsight Experience Replay (HER).\nReward Shaping: Reward Shaping modifies the reward function to help the agent learn faster and more effectively. It adds an extra reward to encourage minimizing the steady-state error, the difference between the desired and actual states. This additional feedback helps the agent perform better relative to the target state(Burda et al., 2019; Hu et al., 2020).\nIntegrator Feedback: Inspired by classical control theory, Integrator Feedback incorporates the integral of past error signals into the agent's observations. This reduces steady-state errors by considering both the current error and its integral. However, it can introduce complexity and instability, leading to overshoot and oscillatory behavior. While it enhances control precision, it should be implemented carefully to avoid negative impacts on system stability (Tracey et al., 2024).\nHindsight Experience Replay (HER): HER is an experience replay technique that reinterprets failed trajectories as successful ones by considering different, actually achieved goals. This method is particularly useful in goal-conditioned RL problems. By treating failures as learning opportunities, HER improves the efficiency and effectiveness of RL in complex and varied settings, especially for precise control and goal-oriented tasks (Andrychowicz et al., 2017; Moro et al., 2022).\nBoth HER and Reward Shaping can be effectively integrated into the Adviser-Actor-Critic (AAC) framework, enhancing its performance. These strategies complement AAC by enriching the quality of experience. However, integral feedback does not serve a purposeful role within the AAC architecture. AAC offers a sophisticated alternative to integral feedback, especially for complex control tasks."}, {"title": "4. Adviser-Actor-Critic", "content": "This section starts with a motivating example that showcases how integrating traditional control theory with reinforcement learning (RL) enhances high-precision control. We then introduce the Adviser-Actor-Critic (AAC) framework, which combines both approaches to address complex control issues. Next, we analyze error dynamics to refine control strategies, and design advisers to guide the system toward its goals, improving performance and precision."}, {"title": "4.1. A motivating example", "content": "To grasp the fundamental concept of our approach, consider the scenario illustrated in Figure 1. Imagine a robot tasked with reaching a specific goal. Due to limitations in control precision or model bias, the robot cannot directly and precisely reach the target position. However, if the robot is guided by an advisor to pursue a \"fake goal,\u201d it can employ this strategy to achieve the desired goal position, despite its inability to reach the fake goal itself. This concept indicates that even with control inaccuracies, the robot can attain precise control of the desired goal if it has an advisor capable of setting a suitable \"fake goal.\""}, {"title": "4.2. Algorithmic Framework", "content": "The Adviser-Actor-Critic (AAC) framework, as illustrated in Figure 2, offers a sophisticated approach to achieving high-precision control in environments characterized by dy-"}, {"title": "4.3. Adviser Design Based on PID Control", "content": "In the previous section, we introduced the fundamental principles of the Adviser-Actor-Critic (AAC) framework and its approach to integrating traditional control theory with reinforcement learning for high-precision control. This section delves into the specific methods used to design advis-"}, {"title": "4.4. Adviser Performance Analysis", "content": "To ensure the stability and reliability of the Adviser within the Adviser-Actor-Critic (AAC) framework, this section focuses on the stability analysis of the PID-controlled system. Stability is crucial for maintaining predictable system behavior under varying conditions. We use the Routh-Hurwitz stability criterion to determine the necessary conditions for system stability, ensuring that the Adviser remains stable under PID control. PID control is one effective method within a broader AAC strategy to enhance precision and reliability.\nFigure 3 illustrates modeling techniques for each dimension of the system's dynamics, assuming decoupled components (see Appendix A). This aligns with traditional control theory, using second-order models that provide computationally efficient representations suitable for classical control strategies like PID controllers.\n$\\dot{\\mathbf{x}} = \\begin{bmatrix}\n1 & 0 \\\\\n\\alpha_0 & \\alpha_1\n\\end{bmatrix} \\mathbf{x} + \\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix} \\epsilon_i + \\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix} d_i$ (6)\nwhere $\\mathbf{x}$ denotes the state vector, which captures the system's state as $[e_i, \\dot{e}_i]^T$, where $e_i$ is the error in the i-th dimension and $\\dot{e}_i$ is its time derivative. The time derivative of the state vector, $\\dot{\\mathbf{x}}$, is represented by $[\\dot{e}_i, \\ddot{e}_i]^T$. Additionally, $\\epsilon_i$ signifies the simulated error introduced by the Adviser for the i-th dimension, while $d_i$ accounts for an external disturbance affecting the same dimension. The unknown dynamics of the system are encapsulated within $d_i$. This modeling approach is sufficiently general to encompass a wide range of system behaviors, making it suitable for diverse control applications.\nTransitioning to the analysis of stability, it is essential to evaluate whether all poles of the closed-loop transfer function reside in the left half of the complex plane. As illustrated"}, {"title": "5. Experiments", "content": "This section is organized as follows. First, we introduce multi-goal RL environments we use for the Experiments as"}, {"title": "5.1. Environments", "content": "Referring to gym goal-conditioned environment, we decided to use manipulation environments based on an existing hardware robot to ensure that the challenges we face corresponds as closely as possible to the real world. We use mass-spring-damper system, robotics arm and quadcopter in next three Experiments."}, {"title": "5.2. How Does the Adviser Eliminate Steady-State Errors?", "content": "To comprehend how the adviser mitigates steady-state errors, one can examine Figure 6, which visually demonstrates the process. The adviser introduces strategically placed intermediate goals, or \"fake\" goals, to direct the robot toward the ultimate desired goal. Even though these intermediary targets may not coincide exactly with the final destination, they play a crucial role in ensuring the robot constantly refines its path and actions.\nBy setting up these waypoints, the adviser prompts the robot to make incremental adjustments to its course, which helps in overcoming any systematic deviations that could lead to steady-state errors. This method ensures that despite any initial discrepancies between the fake and desired goals, the robot can progressively converge on the correct path. As a result, the robot's navigation becomes more precise, and it can achieve its intended destination with significantly reduced persistent errors."}, {"title": "5.3. Does the Adviser Improve Performance?", "content": "To determine whether the inclusion of an adviser can enhance performance, we conducted a detailed evaluation using two variants of the Soft Actor-Critic (SAC) algorithm: standard SAC and SAC with Hindsight Experience Replay (SAC+HER). These evaluations were carried out both with and without adviser support across all three experimental environments, allowing us to systematically analyze the"}, {"title": "5.4. Deployment on a Real-World Quadcopter", "content": "To thoroughly evaluate the effectiveness of our SAC (Soft Actor-Critic)-based attitude control algorithm, we conducted real-world experiments deploying the trained models"}, {"title": "6. Discussion and Conclusion", "content": "The Adviser-Actor-Critic (AAC) framework combines classical control theory with reinforcement learning to enhance control precision in complex systems. By using an adviser based on PID principles to guide the actor, AAC improves precision, reliability, and robustness in high-precision tasks. This approach effectively reduces steady-state errors and enhances exploration and training efficiency, showing significant improvements in environments like robotics arm and quadcopter control. AAC's effectiveness extends from simulations to real-world applications, making it valuable for advanced control systems in robotics and complex mechanical systems.\nFuture Works. Looking ahead, the AAC framework holds substantial potential for further development. The design of the adviser component is particularly ripe for exploration. Beyond methods such as Model Predictive Control (MPC) and H-infinity (H\u221e) control, which can reduce both transient and steady-state errors, numerous other control strategies could be integrated into the adviser. These might include adaptive control, optimal control, or even hybrid ap-"}, {"title": "A. Conditions for Setting Fake Goals to Reduce Steady-State Error", "content": "In the context of control systems, particularly in multi-input multi-output (MIMO) systems, reducing steady-state error is a critical objective. Steady-state error is minimized when the actor approximates an optimal controller; however, zero steady-state error can still be achieved by setting suitable fake goals, even if the actor does not fully serve this role. The conditions under which setting fake goals can reduce steady-state error are discussed below.\nThe k-th order derivative of the i-th component of the error vector e can be expressed as:\n$e_i^{(k)} = \\sum_{j=1}^n \\sum_{l=0}^{k-1} a_{ijl} e_j^{(l)} + \\sum_{j=1}^n b_{ij} u_j + d_i$ (10)\nwhere:\n* $e_i^{(k)}$ is the k-th order derivative of the i-th error component.\n* $a_{ijl}$ are the coefficients representing the influence of the l-th derivative of the j-th error component on the k-th derivative of the i-th error component.\n* $u_j = \\mathcal{E}_j + e_j$ is the j-th component of the input vector.\n* $b_{ij}$ are the coefficients representing the influence of the j-th input vector on the k-th derivative of the i-th error component.\n* $d_i$ is the disturbance affecting the k-th derivative of the i-th error component.\nTaking the Laplace transform with zero initial conditions, we obtain:\n$s^k E_i(s) = - \\sum_{j=1}^n \\sum_{l=0}^{k-1} a_{ijl} s^l E_j(s) + \\sum_{j=1}^n b_{ij} U_j(s) + D_i(s)$ (11)\nHere, $E_i(s)$ represents the Laplace transform of the i-th error signal, $U_j(s)$ represents the Laplace transform of the j-th input vector, and $D_i(s)$ represents the Laplace transform of the i-th disturbance.\nThis equation can be expressed in matrix form as:\n$A(s)E(s) = BU(s) + D(s)$ (12)\nwhere:\n$A(s) = \\begin{bmatrix}\ns^k + \\sum_{l=0}^{k-1} a_{11l} s^l & \\dots & \\sum_{l=0}^{k-1} a_{1nl} s^l \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\sum_{l=0}^{k-1} a_{n1l} s^l & \\dots & s^k + \\sum_{l=0}^{k-1} a_{nnl} s^l\n\\end{bmatrix}$\n$A(s)$ is the system matrix, where each element $a_{ijl}$ represents the coefficients of the error dynamics.\n$B = \\begin{bmatrix}\nb_{11} & \\dots & b_{1n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nb_{n1} & \\dots & b_{nn}\n\\end{bmatrix}$\n$B$ is the input matrix, where each element $b_{ij}$ represents the influence of the j-th input vector on the i-th error.\nSolving for $E(s)$:"}, {"title": null, "content": "$E(s) = A(s)^{-1} [B U(s) + D(s)]$ (13)\nThis equation provides the Laplace transform of the error signals $E(s)$ in terms of the input vector $U(s)$ and disturbances $D(s)$, given the system matrices $A(s)$ and $B$.\nWhen a system incorporates a qualified actor, it is expected to exhibit two primary properties: stability and command tracking. These properties are essential for ensuring that advisers work effectively.\nStability ensures that, with zero inputs and constant disturbances, the error within the system stabilizes to a constant value after a sufficiently long period, preventing unbounded growth in the system's response over time. Mathematically, stability is represented by the requirement that all poles of the system's transfer function $A(s)^{-1}$ have negative real parts, ensuring that the system's transient responses decay over time, leading to a stable equilibrium:\n$\\text{Re}(\\lambda) < 0 \\quad \\forall \\lambda \\in \\text{poles of } A(s)^{-1}$ (14)\nCommand tracking ensures that perturbation in the input vector result in corresponding changes in the error, similar to the perturbation. Mathematically, this property is represented by the condition that the change in the error vector $e$ is similar to input perturbation. When input perturbation equals $-e_{\\text{ss,prev}}$, we have:\n$e_{\\text{ss,now}} = \\lim_{s \\rightarrow 0} s A(s)^{-1} \\left[ B \\left(U(s) - \\frac{e_{\\text{ss,prev}}}{s} \\right) + D(s) \\right]$\n$= \\lim_{s \\rightarrow 0} s A(s)^{-1} [B U(s) + D(s)] - \\lim_{s \\rightarrow 0} A(s)^{-1} B e_{\\text{ss,prev}}$\n$= (I - B) e_{\\text{ss,prev}}$ (15)\nwhere $e_{\\text{ss,prev}}$ denotes the original steady-state error, and $e_{\\text{ss,now}}$ denotes the new steady-state error.\nTaking the 2-norm (or any other compatible matrix norm) on both sides, we get:\n$\\|e_{\\text{ss,now}}\\|_2 = \\|(I - B) e_{\\text{ss,prev}}\\|_2$ (16)\nUsing the sub-multiplicative property of matrix norms, we have:\n$\\|e_{\\text{ss,now}}\\|_2 \\le \\|I - B\\|_2 \\cdot \\|e_{\\text{ss,prev}}\\|_2$ (17)\nFor any matrix norm that is compatible with the vector norm, the spectral radius $\\rho(I - B)$ satisfies:\n$\\rho(I - B) \\ge \\|I - B\\|_2$ (18)\nGiven that $\\rho(I - B) < 1$, there exists a compatible matrix norm such that:\n$\\|e_{\\text{ss,new}}\\|_2 \\le \\|I - B\\|_2 \\cdot \\|e_{\\text{ss,prev}}\\|_2 \\le \\rho(I - B) \\cdot \\|e_{\\text{ss,prev}}\\|_2 < \\|e_{\\text{ss,prev}}\\|_2$ (19)\nWe have shown that if $\\rho(I - B) < 1$, then the 2-norm of the new steady-state error vector is strictly less than the 2-norm of the previous steady-state error vector. This implies that the error decreases with each iteration, indicating that modifying the inputs based on the original steady-state error vector $e_{\\text{ss,prev}}$ results in a reduced new steady-state error vector $e_{\\text{ss,now}}$.\nIn summary, a system incorporating an actor should exhibit both stability and command tracking. The conditions presented in Equations. (14) and (19) facilitate the quantitative analysis of these properties and aid in designing systems that meet the required performance criteria. Special conditions apply when the Actor is an optimal controller. Mathematically, this can be expressed as:\n$\\text{Re}(\\lambda(A(s) - I)) < 0, \\quad \\rho(I - B) \\approx 0$ (20)"}]}