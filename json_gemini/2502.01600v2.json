{"title": "Reinforcement Learning for Long-Horizon Interactive LLM Agents", "authors": ["Kevin Chen", "Marco Cusumano-Towner", "Brody Huval", "Aleksei Petrenko", "Jackson Hamburger", "Vladlen Koltun", "Philipp Kr\u00e4henb\u00fchl"], "abstract": "Interactive digital agents (IDAs) leverage APIs of stateful digital environments to perform tasks in response to user requests. While IDAs powered by instruction-tuned large language models (LLMs) can react to feedback from interface invocations in multi-step exchanges, they have not been trained in their respective digital environments. Prior methods accomplish less than half of tasks in sophisticated benchmarks such as AppWorld. We present a reinforcement learning (RL) approach that trains IDAs directly in their target environments. We formalize this training as a partially observable Markov decision process and derive LOOP, a data- and memory-efficient variant of proximal policy optimization. LOOP uses no value network and maintains exactly one copy of the underlying LLM in memory, making its implementation straightforward and as memory-efficient as fine-tuning a single LLM. A 32-billion-parameter agent trained with LOOP in the AppWorld environment outperforms the much larger OpenAI ol agent by 9 percentage points (15% relative). To our knowledge, this is the first reported application of RL to IDAs that interact with a stateful, multi-domain, multi-app environment via direct API calls. Our analysis sheds light on the effectiveness of RL in this area, showing that the agent learns to consult the API documentation, avoid unwarranted assumptions, minimize confabulation, and recover from setbacks.", "sections": [{"title": "1. Introduction", "content": "Consider an interactive digital agent (IDA) faced with the task illustrated in Figure 1. The task spans multiple software apps and requires common-sense knowledge about human life and language. Successful completion requires chains of information-gathering and state-changing actions, and potential replanning in response to information that is obtained along the way. The best open-weights models (Yang et al., 2024) have a success rate below 40% in these kinds of tasks, while top-of-the line reasoning models (OpenAI, 2024) succeed barely more than half the time as measured by the AppWorld benchmark (Trivedi et al., 2024). This is not surprising. Solving a task can take up to 40 interactions between the agent and the Python read-eval-print loop (REPL), using up to 32K tokens. The AppWorld environment state comprises up to 30M text tokens, making thoughtful management of context a necessity.\nIn this paper, we demonstrate that reinforcement learning (RL) is an effective approach for training long-horizon interactive LLM agents. Our approach does not require either expensive-to-gather ground-truth action sequences or large datasets of training scenarios. With a simple task completion reward on only 24 training scenarios, our agent learns behaviors that generalize to diverse held-out tasks resembling the one illustrated in Figure 1.\nWe present a framework for RL with LLM-based IDAs and provide a systematic evaluation of various design choices for policy gradient in this domain. Our best approach, LOOP,"}, {"title": "2. Related Work", "content": "LLM agents. Pretrained, instruction-tuned LLMs have demonstrated an ability to interact with external software environments by invoking structured APIs, both for information retrieval (Nakano et al., 2021; Schick et al., 2023) and for acting in stateful external environments (Yao et al., 2020; 2022). \u03a4o improve performance in this domain, further works introduced structured workflows that combine reasoning, acting, and reflection steps (Yao et al., 2023; Shinn et al., 2024; Kim et al., 2024), and interaction with code interpreters (Wang et al., 2024; Yang et al., 2023). Others apply supervised fine-tuning on datasets of agent trajectories (Chen et al., 2023; Qin et al., 2024; Mitra et al., 2024).\nReinforcement learning for LLMs. RL was first used to train LLMs in the setting of reinforcement learning from human feedback (RLHF) (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022). These works used proximal policy optimization (PPO) (Schulman et al., 2017) to train an LLM policy based on a reward model inferred from human preferences. RLHF with PPO uses up to four separate LLMs during training: a reward model, trained policy, reference policy, and critic. Ahmadian et al. (2024) showed that the much simpler REINFORCE Leave-One-Out (RLOO) algorithm (Kool et al., 2019) performs competitively. RLOO avoids the need for the reference and critic LLMs using on-policy updates and using multiple rollouts from the same query for a sampling-based advantage estimate instead of a learned critic. Our method, LOOP, is a generalization of RLOO that allows for policy updates to drift off-policy using the trust region defined in PPO. This enables reusing rollouts and a looser loop between rollout collection and policy updates.\nPPO and its variants have been used to train LLMs to perform reasoning and static code generation using programmatic reward functions. GRPO (Shao et al., 2024; DeepSeek-AI, 2025) replaces the critic in PPO with baselines computed from a collection of samples from the same query. VinePPO (Kazemnejad et al., 2024) estimates per-step advantages via rollouts branched from each reasoning step of the training rollout. Other RL algorithms for training reasoning LLMs depart from the policy gradient framework, alternating between generating datasets of filtered rollouts and supervised training on these datasets (Singh et al., 2024; Havrilla et al., 2024; Yuan et al., 2023). We compare to these methods and show that a straightforward combination of PPO with a leave-one-out estimate performs significantly better in training IDAs.\nReinforcement learning for LLM agents. RL has been used to train stateful multi-turn agents in text-based games (Narasimhan et al., 2015; Yao et al., 2020; Carta et al., 2023), web shopping and navigation environments (Yao et al., 2022), mobile device control (Bai et al., 2024), and embodied environments (Zhai et al., 2024). Most closely related to our approach are several works that train LLM policies with RL in WebShop: Yao et al. (2022) apply REINFORCE with a learned value baseline, ArCHer (Zhou & Zanette, 2024) uses a hierarchical approach that combines off-policy and on-policy training, and AgentQ (Putta et al., 2024) combines tree search with direct policy optimization (DPO). Our work targets AppWorld, which is substantially more complex than the WebShop environment. While the goal of all WebShop scenarios is to purchase a described item from a simulated site with 8 actions (with at most 1 parameter per turn), AppWorld tasks leverage 9 apps, 457 API endpoints with up to 17 parameters, and require non-trivial logic. LOOP outperforms both REINFORCE-based and"}, {"title": "3. Preliminaries", "content": "Autoregressive language models. An autoregressive language model (LM) \\( p_{\\theta} \\) maps a sequence of tokens \\( x_{1:t} \\) to a probability distribution over the next token \\( p_{\\theta}(x_{t+1} | x_{1:t}) \\). Generation with an autoregressive LM proceeds token-by-token by sampling the next token \\( x_{t+1} \\sim p_{\\theta}(x_{1:t}) \\) until a stop token \\(\\perp\\) is reached. Conditional generation takes a starting sequence \\( c = [c_1...c_m] \\) of m tokens and generates a sequence of new tokens from \\( p_{\\theta} (x_{1:n} | c) = \\prod_{t=1}^{n} p_{\\theta} (x_t | c, x_{1:t-1}) \\). We denote the sampling processes as \\( x \\sim p_{\\theta}(\\cdot) \\) and \\( x \\sim p_{\\theta}(|c) \\). Unless otherwise noted, all samples x end with the stop token \\(\\perp\\).\nLanguage modeling as a Markov decision process. Language generation can be cast as a Markov decision process (MDP). The state \\( [c, x_{1:t}] \\) of the MDP is the context c and the generation thus far \\( x_{1:t} \\). Actions produce the next token \\( x_{t+1} \\sim p_{\\theta}(c, x_{1:t}) \\), transitioning to the new state by appending the token \\( [c, x_{1:t}, x_{t+1}] \\). Language modeling tasks often assume a terminal reward, leading to a trajectory-based return \\( R(c, x) \\). The MDP optimization objective is\n\\[ L_{\\theta}(c) = \\mathbb{E}_{x \\sim p_{\\theta}(|c)} [R(c, x)]. \\quad (1) \\]\nThe terminal reward structure and deterministic state transitions reduce this MDP to a contextual bandit (Ahmadian et al., 2024). In the bandit formulation, an entire generation x is an action, which simplifies the problem significantly.\nREINFORCE (Williams, 1992) provides a sampling-based gradient estimate of the above objective (1):\n\\[ \\nabla_{\\theta} L_{\\theta}(c) = \\mathbb{E}_{x \\sim p_{\\theta}(|c)} [A(c, x) \\nabla_{\\theta} \\log p_{\\theta}(x|c)], \\quad (2) \\]\nwhere \\( A(c, x) \\) is an advantage estimate that lowers the variance of the gradient estimate (Schulman et al., 2016). Leave-one-out (Kool et al., 2019) estimates the advantage using sampling. Specifically, REINFORCE Leave-One-Out (RLOO) generates K independent samples \\( x_1, ..., x_K \\sim p_{\\theta}(|c) \\) and uses all other samples to compute a baseline for the current return. \\( A(c, x_k) = R(c, x_k) - \\frac{1}{K-1} \\sum_{i=1, i \\neq k}^{K} R(c, x_i) \\). An equivalent form of this objective estimates the advantage by subtracting the average return baseline (Kool et al., 2019):\n\\[ A(c, x_k) = \\frac{K}{K-1} R(c, x_k) - \\frac{1}{K-1} \\sum_{i=1}^{K} R(c, x_i) \\equiv \\frac{K}{K-1} \\left( R(c, x_k) - \\frac{1}{K} \\sum_{i=1}^{K} R(c, x_i) \\right) \\quad (3) \\]\nThis results in a simple, unbiased, low-variance advantage estimate, which has been successfully applied to large language models (Ahmadian et al., 2024). However, the gradient estimate needs to be on-policy. (For each gradient step, samples need to be drawn from the current policy.) In practice, on-policy methods can be inefficient because they do"}, {"title": "4. LOOP", "content": "We start by describing a partially observable Markov decision processes (POMDP, Kaelbling et al. (1998)) for interactive digital agents. We then present a variant of Proximal Policy Optimization with a Leave-One-Out advantage estimate. We show that GRPO (Shao et al., 2024; DeepSeek-AI"}, {"title": "5. Evaluation", "content": "We compare our approach with methods outlined below. For each fine-tuned method, we select the highest performing checkpoint according to the validation (dev) set.\nNo fine-tuning (NFT) baselines evaluate on AppWorld zero-shot. We include current open-weight and closed-source SOTA models (Llama 3 70B (Dubey et al., 2024) and GPT-40) from Trivedi et al. (2024), as well as two others: Qwen2.5-32B-Instruct (Yang et al., 2024), which is used as the base LLM in our setup, and OpenAI 01 (OpenAI, 2024).\nGround truth supervised fine-tuning (SFT-GT). We transform the AppWorld solution code into a ReAct-style dataset and apply supervised fine-tuning. Details in Appendix C.1.\nRejection sampling fine-tuning (RFT) (Yuan et al., 2023) collects rollouts generated with the base model and fine-tunes on successful ones (R(so, c, x) = 1). Details in Appendix C.2.\nExpert iteration (EI) (Anthony et al., 2017) runs multiple smaller iterations of RFT. It uses the current best model for rollout collection and fine-tunes on many smaller collections of rollouts (Zelikman et al., 2022; Singh et al., 2024; Havrilla et al., 2024). Details in Appendix C.3.\nDirect Preference Optimization + MCTS (DPO-MCTS) (Putta et al., 2024). We implement a simplified version of Agent Q. It collects preference pairs into a replay buffer using Monte Carlo Tree Search (MCTS). Unlike Agent Q, we rely only on an upper confidence bound (UCB) to expand nodes and do not use an LLM critic heuristic. Details in Appendix C.4.\nDirect multi-turn preference optimization (DMPO) (Shi et al., 2024). We sample a pair of rollouts per task and form a preference pair if the return difference is greater than a threshold. We use the DMPO loss on the winning and losing rollout, and treat the interactions as turns within the loss. We run this on-policy without an offline replay buffer (the reference policy is updated after every iteration).\nPPO with a learned critic (Schulman et al., 2017). We implement a version of PPO with a learned baseline and Generalized Advantage Estimation (Schulman et al., 2016). See Appendix C.5 for details.\nREINFORCE leave-one-out (RLOO) (Ahmadian et al., 2024) is the on-policy equivalent to per-trajectory LOOP. RLOO and LOOP share the same experimental setup and all hyperparameters.\nGroup relative policy optimization (GRPO) (Shao et al., 2024). We implement GRPO strictly on-policy as described in Shao et al. (2024, Sec. 4.2). We evaluate GRPO with and without the KL penalty. Finally, we compare to off-policy PPO with a GRPO advantage estimate (LOOP RwNorm)."}, {"title": "6. Discussion", "content": "We formalized training interactive digital agents (IDAs) as a reinforcement learning (RL) problem. We presented a simple and effective learning algorithm (LOOP) for IDAS. Experimental results indicate that LOOP substantially improves agent performance. In particular, we showed that RL can produce meaningfully better IDAs after training on only a small number of scenarios.\nMuch remains to be done to realize the dream of broadly effective IDAs. Even our best agents succeed on ~7 out of 10 tasks in AppWorld. This may be acceptable for agents with close human supervision, but is below the level of robustness required for broader autonomy. Additionally, while AppWorld is the literature's most advanced multi-turn IDA benchmark (Trivedi et al., 2024), it still lacks some key features of everyday environments: non-determinism, transient failures, unsolvable and ambiguous tasks, adversarial scenarios (e.g. scams), user clarification and confirmation steps, and interactive counterparties such as customer service representatives. Notwithstanding these challenges, our findings, along with other recent work, demonstrate the transformative potential of applying RL to LLMs."}]}