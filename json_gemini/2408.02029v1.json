{"title": "Mining Path Association Rules in Large Property Graphs (with Appendix)", "authors": ["Yuya Sasaki", "Panagiotis Karras"], "abstract": "How can we mine frequent path regularities from a graph with edge labels and vertex attributes? The task of association rule mining successfully discovers regular patterns in item sets and substructures. Still, to our best knowledge, this concept has not yet been extended to path patterns in large property graphs. In this paper, we introduce the problem of path association rule mining (PARM). Applied to any reachability path between two vertices within a large graph, PARM discovers regular ways in which path patterns, identified by vertex attributes and edge labels, co-occur with each other. We develop an efficient and scalable algorithm PIONEER that exploits an anti-monotonicity property to effectively prune the search space. Further, we devise approximation techniques and employ parallelization to achieve scalable path association rule mining. Our experimental study using real-world graph data verifies the significance of path association rules and the efficiency of our solutions.", "sections": [{"title": "1 INTRODUCTION", "content": "Association rule mining is the task of discovering regular correlation patterns among data objects in large data collections [1, 69]. An association rule, represented as X \u2192 Y, where X is an antecedent set, list, or other structure and Y is a corresponding consequent, signifies that a data record containing X is likely to also contain Y. Association rules are useful in applications ranging from web mining [41] to market analysis [39] and bioinformatics [45].\nGraph association rule mining aims to discover regularities among entities on a single large graph [21, 25, 64]. A graph association rule is represented as Gx \u2192 Gy, where Gx and Gy are graph patterns. Since graphs are widely used in many applications, the mining of association rules from graphs promises to discover valuable insights and knowledge. Its applications include:\nSocial analysis: Graph association rule mining can be used to discover regularities in social relationships. For example, as social relationship patterns affect people's health [34] and happiness [32], we may discover a rule like \u201cpeople who identify as happy are likely to connect with others who also identify as happy through multiple intermediaries with high probability.\u201d\nDiscrimination checking: Machine learning models trained on graph data are vulnerable to discriminatory bias [28]. For example, automated systems reviewing applicant resumes incorporated a significant bias in favor of male candidates due to bias inherent in the training data [15]. To build fair machine learning models, we should eschew such data-driven discrimination. Since graph association rules discover regularities, they can reveal discriminatory bias.\nKnowledge extraction: Knowledge bases are often represented as graphs with labeled edges and attributed vertices, known as knowledge graphs [65]. We can mine interesting patterns from such graphs as association rules. For example, an interesting rule may be \"people often have occupations similar to those of some of their ancestors.\"\nMotivation. While graph association rule mining on a single large graph is fundamental for graph analysis, existing methods [21, 25, 64] are inapplicable to the aforementioned applications due to the following shortcomings: (i) regarding the vertices in Gy and Gx, existing methods [21, 25] require the vertex set in the consequent Gy to be a subset of that in the antecedent Gx and mainly focus on missing edges - association rules where Gy includes vertices not in Gx is out of their scope; (ii) they consider specific restricted graph patterns, e.g., a single edge in Gy [25], a subgraph including at least one edge in Gx and Gy [64], or a subgraph without attributes in Gx and a single edge or attribute in Gy [21], and cannot handle edge labels and vertex attributes together; and (iii) their antecedent and consequent do not capture reachability (or transitive closure) patterns, which denote that one vertex is reachable by any number of label-constrained directed edges from another, such as the one regarding the examples on social analysis and knowledge extraction. Therefore, we need a different approach to graph association rule mining that addresses these shortcomings and thereby ensure wide applicability.\nContribution. In this paper, we introduce a novel, simple, and elegant concept, path association rule, which expresses regular co-occurrences of sequences of vertex attribute sets and edge labels, or path patterns, and allow for measures such as absolute/relative support, confidence, and lift. Such rules are in the form px \u2192 py, where px and py are path patterns with a minimum support of common source vertices. To mine path association rules while eschewing the aforementioned shortcomings, we propose a novel, efficient, and scalable algorithm that imposes no restriction on how the vertices in the consequent relate to those in the antecedent, accommodates reachability patterns, and considers both vertex attributes and edge labels.\nEXAMPLE 1. Figure 1 presents an example of path association rule mining on a social network with 12 vertices, 15 edges, 4 types of edge labels {Follows, Belong To, LocatedIn, Likes} and 8 types of vertex attributes {Museum, Uni, City, Male, Female, CS, Chem, Art}. We set 0 = 2; then path pattern p1 = <{CS}, Follows, {Art}> is frequent, as it has source vertices (or matches) 08 and 09; likewise, path pattern p6 = <{Male}, BelongTo, {Uni}> matches 08 and 09. We mine the path association rule r3 = <{CS}, Follows, {Art}> \u21d2 <{Male}, BelongTo, {Uni}>, i.e., frequently a computer scientist who"}, {"title": "2 RELATED WORK", "content": "We review existing works and research topics related to our work.\nFrequent graph mining on a single large graph. Definitions of frequency (i.e., support) in a single graph are different across studies. The support measures applied on transaction data do not preserve anti-monotonicity properties on a single graph. That is because, intuitively, the number of paths in a graph is usually larger than the number of vertices, even though paths are more complex than vertices. Several support measures that enforce anti-monotonicity properties have been proposed, such as maximum independent set based support (MIS) [62] minimum-image-based support (MNI) [10], minimum clique partition (MCP) [11], minimum vertex cover (MVC) [48], and maximum independent edge set support (MIES) [48]. Their common goal is to use anti-monotonic properties in case a vertex is involved in multiple matches. However, these support measures have three drawbacks. First, they do not apply to relative support because it is hard to count the maximum number of graph patterns that may appear in a graph; while the support measure proposed in [25] can be applied to relative support for a single large graph, it is inefficient because it needs isomorphic subgraph matching. Second, their time complexity is very high. For instance, the problems of computing MIS and MNI are NP-hard. Third, they are not intuitive, as it is difficult to understand why some vertices match graph patterns and others do not.\nEach algorithm on frequent subgraph mining in a single graph uses anti-monotonic properties specialized for their support. Existing supports for subgraph mining did not consider how to handle reachability patterns. Support of subgraphs with reachability patterns is untrivial, and thus, it is hard to directly apply algorithms for frequent subgraph mining to our problem.\nTheir basic concepts of frequent subgraph mining algorithms consist of (1) finding small sizes of frequent patterns, (2) combining frequent patterns to generate new candidates of patterns, (3) removing infrequent patterns based on anti-monotonic properties, and (4) repeating until candidates are empty. Commonly, steps (2) and (3) are extended to efficient processing for their patterns. Our baseline in Sec. 4.2 follows this basic method.\nGraph pattern mining. Several algorithms have been developed for graph pattern mining [3, 17, 18, 27, 40, 50, 54, 58], each with different semantics. For example, Prateek et al. [54] introduce a method for finding pairs of subgraphs that appear often in close proximity; Nikolakaki et al. [50] propose a method that finds a set of diverse paths that minimize a cost of overlapping edges and vertices. However, graph pattern mining does not handle reachability path"}, {"title": "3 THE CONCEPT", "content": "We propose the novel concept of path association rule mining (PARM), which effectively discovers regularities among attributes of vertices connected by labeled edges. The distinctive characteristic of PARM compared to existing graph association rule mining techniques is that it captures correlations of distinct path patterns among the same vertices, which are useful in many applications. In addition, PARM discovers rules on general property graphs, which cover many graph types (e.g., labeled graphs).\n3.1 Notations\nWe consider a graph G = (V, E, L, A), where V is a set of vertices, & \u2282 V \u00d7 L \u00d7 V is a set of edges, L is a set of edge labels, and A is a set of attributes. Each edge e \u2208 & is a triple (v, le, v') denoting an edge from vertex v to vertex v' with label le. Attribute a \u2208 A is a categorical value representing a feature of a vertex. Each vertex v \u2208 V has a set of attributes A(v) \u2286 A.\nA path is a sequence of vertices and edges (v0, e0, v1, . . ., en\u22121, vn), where n is its length, v0 its source, and vn its target. A path prefix (suffix) is an arbitrary initial (final) part of a path.\nEXAMPLE 2. In Figure 1, L = {Follows, Belong To, LocatedIn, Likes} and A = {Museum, Uni, City, Male, Female, CS, Chem, Art}. (08, e9, v5, e3, v2) is a path of length 2 path with source v8 and target v2.\n3.2 Path Association Rules\nWe define path association rules after defining path patterns.\nPath pattern. We define simple and reachability path patterns.\n\u2022 A simple path pattern is a sequence of attribute sets and edge labels (A0, l0, A1, ..., ln\u22121, An) where Ai \u2286 A (Ai \u2260 0) and li \u2208 L; n indicates the pattern's length.\n\u2022 A reachability path pattern is a pair of attribute sets with an edge label (A0, l*, A1), where A0, A1 CA (A0, A1 \u2260 0) and l* \u2208 L."}, {"title": "3.3 Measures of association rules", "content": "Path association rules support measures similar to those of association rules [1]. Here, we define support, confidence, and lift for path association rules.\nSupport: The support of a path association rule r indicates how many vertices it matches. We define absolute and relative support.\nSignificantly, most graph association rule mining methods do not offer relative support (see Section 2), as it is hard to compute the number of matched graph patterns. Absolute support is defined as ASupp(px \u21d2 py) = |V(px) \u2229 V (py)|. Since the maximum value of |V(p)| is the number of vertices, relative support is defined as RSupp(px \u21d2 px) = \\frac{\\left|V(p_{x}) \\cap V(p_{x})\\right|}{|V|}\nConfidence: The confidence of a path association rule indicates the probability that a vertex satisfies py if given it satisfies px. We define confidence as Conf(px \u21d2 px) = \\frac{\\left|V(p_{x}) \\cap V(p_{x})\\right|}{|V(p_{x})|}\nLift: Lift, which most graph association rule mining methods do not support, quantifies how much the probability of py is lifted given the antecedent px, compared to its unconditioned counterpart. We define lift as Lift(px \u21d2 px) = \\frac{\\left|V(p_{x}) \\cap V(p_{x})\\right|\\cdot|V|}{|V(p_{x})|\\cdot|V(p_{y})|}\nEXAMPLE 5. In Figure 1, for r1 = p1 \u21d2 p5, we have V(p1) = V(p5) = {v8, v9}. Since |V| = 12 and |V(p1) \u2229 V(p5)| = 2, it is ASupp(r1) = 2, RSupp(r1) = 1/6, Conf (r1) = 1, and Lift(r1) = 6."}, {"title": "3.4 Problem Definition", "content": "We now define the problem that we solve in this paper.\nPROBLEM 1 (PATH ASSOCIATION RULE MINING (PARM)). Given graph G, minimum (absolute or relative) support 0, and maximum path length k, the path association rule mining problem calls to find all rules r, where (1) supp(r) \u2265 0, (2) path lengths are at most k, and (3) px is not dominated by py or vice versa.\nRemark. Path association rule mining generalizes conventional association rule mining for itemsets [1]. We may consider each vertex as a transaction and its attributes as items in the transaction. Ignoring edges, path association rule mining degenerates to conventional itemset-based association rule mining.\nExtensions. We can modify PARM according to the application, for example, we can find rules where py dominates px, a set of attributes in path patterns is empty, and we can specify other thresholds (e.g., high confidence and Jaccard similarity) to restrict the number of outputs. We may also use other quality measures (e.g., [16, 53]) in place of the conventional measures we employ. As we focus on introducing the PARM problem, we relegate such extensions and investigations to future work."}, {"title": "4 MAIN PARM ALGORITHM", "content": "We now present our core algorithm for efficient path association rule mining. To solve the PARM problem, we need to (1) enumerate frequent path patterns as candidates, (2) find vertices matching frequent path patterns, and (3) derive rules. Our PARM algorithm first finds simple frequent path patterns and then generates more complex path pattern candidates therefrom using anti-monotonicity properties. Its efficiency is based on effectively containing the number of candidates.\n4.1 Anti-monotonicity properties\nThe following anti-monotonicity properties facilitate the reduction of candidate path patterns.\nTHEOREM 1. If p c p', then V(p) 2 V(p').\nThe following two lemmata follow from Theorem 2.\nLEMMA 1. If the prefix of a path pattern is infrequent, the whole pattern is infrequent.\nLEMMA 2. If path pattern p is infrequent and p' comprises A \u2287 Ai and l = li for all i, then p' is also infrequent.\nEXAMPLE 6. We illustrate these anti-monotonicity properties using Figure 1. Let the minimum absolute support be 2. Path pattern p = <{CS}, Follows, {Chem}> is infrequent, thus no path pattern that extends p is frequent either. Similarly, path patterns that dominate p, such as <{Male, CS}, Follows, {Female, Chem}>, are infrequent.\n4.2 Baseline algorithm\nOur baseline algorithm discovers, in a sequence, (1) frequent attribute sets, (2) frequent simple path patterns, (3) frequent reachability path patterns, and (4) rules by Lemma 1. We describe each step in the following."}, {"title": "4.3 PIONEER", "content": "To achieve efficiency in PARM, we develop PIONEER, an algorithm that lessens the explored path patterns and rules by bound-based pruning and enhanced candidate generation.\nBound-based pruning. Anti-monotonicity properties eliminate candidates of path patterns whose prefixes are infrequent. Here, we introduce two upper bounds on the number of vertices matching a path pattern: (1) an upper bound on the number of vertices matching (p, l, A), and (2) an upper bound on the number of vertices matching (A, l, p), where p = (A0, l0, A1, ..., An\u22121, ln, An) is an arbitrary path pattern, l is an edge label, and A is an attribute set. We prune results with upper bound below \u03b8.\nTo derive such upper bounds, we use (i) the set of edges with label l that connect to a vertex whose attribute set includes A, E(A, l) = {(v, le, v')|(v, le, v') \u0454 \u0415\u043b \u0410(') \u2287 A \u2227 le = l}; and (ii) the set of vertices whose attribute set covers A and which have an out-going edge with label l, V (A, l) = {v|\u2203(v, le, v\u2032) \u2208 & \u2227 A(v) 2 A le = l}. The following lemmata specify our bounds:\nLEMMA 3. Given length i > 0, edge label l, and attribute set A, the number of vertices that match a path pattern of length i ending with (l, A) is upper-bounded by |E(A, l)| \u00b7 dmi\u22121 where dm is the maximum in-degree in the graph.\nLEMMA 4. Given length i > 0, attribute set A, and edge label l, the number of vertices that match path pattern of the form p = (. . ., li\u2212 2, A, l, Ai, . . .) is upper-bounded by|V(A, l)|\u00b7dmi-1 where dm is the maximum in-degree in the graph.\nWe use these lemmata to prune candidate path patterns. We prune patterns of length i with suffix (l, A) if |E(A, l)|\u00b7 dmi\u22121 < \u03b8. We collect the set of single attributes that pass this pruning when combined with edge label f and path length i as A\u2081\u2081 (l) = {a \u2208 A | |E(a, l)|dmi-1 \u2265 0}. Likewise, we prune patterns of length i with attribute set A whose li\u22121 is l, if |V(A, l)|\u00b7 dm1-1 < 0. We collect the set of edge labels that pass this pruning when combined with single attribute a \u2208 A and path length i as L\u2081\u2081 (a) = {l \u2208 L | |V(a, l)| dm1-1 \u2265 0}. The case of l* is equivalent to LT\u2081 (a).\nThus, bound-based pruning reduces the candidate edge labels and attribute sets for addition to frequent path patterns.\nEnhanced candidate generation. Using the two anti-monotonicity properties and bound-based pruning, we eliminate candidate path patterns that (1) have an infrequent prefix, (2) are dominated by infrequent path patterns, and (3) fall short of bound-based pruning. Pruned patterns are not included in the candidates. Here we introduce optimized candidate generation, which extends path patterns in the following ways.\n\u2022 Vertical: starting with path patterns of length zero (i.e., frequent attributes), we extend them to length k following Lemma 1 and adding suffixes that are not pruned by either Lemma 3 or Lemma 4.\n\u2022 Horizontal: starting with unit path patterns (whose attribute sets include only a single attribute each), we combine them to form frequent path patterns by applying Lemma 2.\nWe revise our algorithm of Section 4.2 to use enhanced candidate generation. We explain the modifications in each step.\n(1) Frequent attribute set discovery. We additionally find the AT and LT sets.\n(2) Frequent simple path pattern discovery. We first enumerate unit path patterns of length 1, vertically extending Po by adding an edge label \u2208 Ly and an attribute \u2208 Ar. After checking the frequency of all unit path patterns of length 1, we horizontally combine pairs of frequent path patterns to obtain new path patterns with more than one attributes. This horizontal extension drastically reduces candidates because, by Lemma 2, if any of two paths is not frequent, the combined path patterns are not frequent either. We repeat vertical and horizontal extensions until we obtain frequent path patterns of length k.\n(3) Frequent reachability path pattern discovery. We restrict the candidates for l* to edge labels in Ly and those for A1 to AT. We find frequent reachability path patterns whose attribute sets include a single attribute and then combine pairs of such path patterns to obtain complex reachability path patterns, following Lemma 2.\n(4) Rule discovery. We generate candidate rules utilizing path patterns found to be frequent in Steps 2 and 3, applying both Lemma 1 and Lemma 2. We first search for frequent rules that combine unit path patterns of length 1 and then extend those patterns vertically and horizontally. For a frequent rule p \u21d2 p', we generate candidates pv \u21d2 p', Ph \u21d2 p',"}, {"title": "4.4 Auxiliary data structure", "content": "To facilitate efficient rule discovery, we maintain a data structure that stores, for each vertex, a list of matched path patterns and the targets of their corresponding paths. We extend paths using this data structure without searching from scratch. In addition, we maintain pairs of dominating and dominated path patterns, so as to generate rule candidate so as to avoid generating rule involving them. In effect, for a frequent rule p \u21d2 p', we obtain pu, Ph, Po and p via this auxiliary data structure."}, {"title": "4.5 Complexity analysis", "content": "We now analyze the time and space complexity of our algorithm.\nWe denote the sets of candidate attribute sets as CA, simple path patterns as CS, reachability path patterns as C, and rules as CR, where j denotes the iteration.\nTime complexity: The frequent attribute discovery incurs the same time complexity of traditional algorithms such as Apriori algorithm [1], i.e., O(|A| + |E||V| + \u03a3\u0391 \u03a3\ucf08 |V||CA|). The time complexity of frequent simple path pattern discovery depends on the number of frequent attributes and edge labels; it is O(|LT||AT|\u00b2 + \u03a32 IV||CSI). The frequent reachability path discovery step incurs a similar complexity as the previous step, while it also performs bread-first search; thus, it needs O(|L7|(|V| + |E|) + |LT||AT|\u00b2 + \u03a3=2 |V||C|). The rule discovery step combines pairs of path patterns and extends the patterns in found rules. It takes a worst-case time of O(|V||P1||P*| + Zj=2 |V||CR|). In total, time complexity is O(|V||E| + |LT|(|V| + |E| + |AT|\u00b2) +SA + \u03a3 VI(ICA]+[C+ |C| + |CR|)), which highly depends on the size of the candidates.\nSpace complexity: The space complexity of the algorithm is O(|V|+ |E|+|A|+|CA|+|CS|+|C*|+|CR|+|V|(\u03a3=0 |Pi|+|P*|+|R|)), where |CA|, |CS||C*|, and |CR| are the maximum sizes of |CA|, |C| |C|, and |CR| over iterations j, respectively. Practically, memory usage does not pose an important problem on commodity hardware."}, {"title": "5 APPROXIMATION TECHNIQUES", "content": "PIONEER reduces the candidates of path patterns for efficient mining. However, it still does not scale to large graphs as it has to exactly compute the frequency of all unpruned path patterns, whether they are frequent or not. We present two approximation methods to reduce the computation costs for checking the frequency.\n5.1 Approximate Candidate Reduction\nThe bound-based pruning in Lemma 3 computes upper bounds using the maximum in-degree, hence may retain candidates that are not likely to be involved in frequent path patterns. Our first approximation strategy aims to eliminate such candidates in AT."}, {"title": "5.2 Stratified Vertex Sampling", "content": "Sampling effectively reduces the computation cost in many data mining tasks [21, 22, 43, 60]. We propose a sampling method that picks a set of vertices to work with, aiming to reduce the computation cost to find matched vertices and theoretically analyze its approximate accuracy.\nWe do not reduce the graph itself, as in [23], as then it would be hard to guarantee accuracy. Instead, as our algorithm focuses on frequent path patterns, rather than subgraphs, we can afford to reduce the number of source vertices by vertex sampling.\nMethod. We need a sampling strategy that preserves accuracy as much as possible. To achieve that, we use stratified sampling according to attributes of vertices [59]. We group vertices into strata according to their attribute sets and remove vertices that have no frequent attributes because they do not contribute to frequent path patterns. From each stratum, we pick vertices with sampling ratio \u03c1. We estimate the frequency of p as follows:\n|V(p)| = \\frac{|V_{s}(p)|}{\\rho} (1)\nVs denotes the set of sampled vertices in strata related to A0 of p. Theoretical analysis. The accuracy of sampling is expressed by variance. The variance of our sampling strategy is:\n\\sigma^{2}=\\frac{\\sum_{v_{i}\\in V_{s}}(x_{i} - \\bar{x})^{2}}{\\rho V_{s} - 1} (2)\nwhere xi = 1 if vi matches p, otherwise 0. \\bar{x} is \\frac{|V_{s}(p)|}{V_{s}}\nThe confidence interval is then the following:\n|V_{s}|(\\bar{x}-z\\cdot\\frac{\\sigma}{\\sqrt{\\rho V_{s}}}) < |V(p)| < |V_{s}|(\\bar{x}+z\\cdot\\frac{\\sigma}{\\sqrt{\\rho V_{s}}}) (3)\nwhere z indicates the z-value for a confidence interval.\nContrary to candidate reduction, vertex sampling may cause false positives, that is, infrequent path patterns could be reported as frequent path patterns. Yet it can reduce the computation costs, even in cases where the candidate reduction does not."}, {"title": "6 PARALLELIZATION", "content": "We accelerate PIONEER by parallelization. Given a number of threads N, we partition the set of vertices into N subsets to balance the computing cost among threads in terms of the frequency of vertex attributes and adjacent edges. We describe how we estimate computing costs in the following.\nCost estimation. The cost to find matched vertices increases with the number of matched paths from each vertex. First, we prune vertices that do not have target attributes or outgoing edges with target edge labels, as these vertices have no matched frequent path patterns, as the following lemma specifies.\nLEMMA 5. Ifv \u2208 V has no target attributes and no outgoing edges with target labels, then v has no matched path patterns.\nAmong non-pruned vertices, the more frequent their attributes and outgoing edges are, the more matched path patterns they may match. We estimate the cost C(v) of a vertex v as:\nC(v) = d_{T}(v) \\cdot \\left|A_{T}(v)\\right| (4)\nwhere dT (v) is the number of out-going edges with target edge labels and AT (v) is the number of target attributes on v. This estimation is O(1) considering the numbers of edges and attributes on v as constants.\nPartitioning. We partition the set of vertices into N subsets according to estimated costs by a greedy algorithm. We sort vertices in ascending order of their costs and repeatedly assign the unassigned vertex of the largest cost to the thread with the smallest sum of assigned vertex costs; we do not assign a vertex to a thread that is already assigned \u2308\\frac{|V|}{N}\u2309 vertices. The time complexity of this algorithm is O(N|V|log |V|)."}, {"title": "7 EXPERIMENTAL STUDY", "content": "We conduct an experimental study on PIONEER to assess (1) its efficiency, (2) its scalability, and (3) the accuracy of our approximations. We also assess (4) the effectiveness of PARM. We implemented all algorithms in C++ and ran experiments on a Linux server with 512GB of main memory and an Intel(R) Xeon(R) CPU E5-2699v3 processor. Some experimental settings and results (e.g., length k and approximation factors) are in the supplementary file.\n7.1 Experimental Setting\nDataset. We use four real-world graphs with edge labels and vertex attributes: knowledge graph Nell [12], coauthorship information network MiCo [6], knowledge graph extracted from Wikipedia DBpedia [6], and social network service Pokec. We also use two types of synthetic graphs, uniform and exponential. They differ on how we generate edges; in uniform, we generate edges between randomly selected vertices following a uniform distribution, while in exponential we follow an exponential distribution. We vary the number of vertices from 1M to 4M and generate a fivefold number of edges (i.e, 5M to 20M). Table 7 presents statistics on the data.\nCompared methods. We assess a baseline and four variants of PIONEER. Baseline is the algorithm of Section 4.2 without any optimization strategy. PIONEER is our algorithm using the strategies of Section 4.3. PIONEER W/ CR using the candidate reduction of Section 5.1, while PIONEER w/ SA uses the stratified sampling of"}, {"title": "E.2 Efficiency in detail", "content": "Run time analysis. Our results indicate that the data size is not a dominant factor in the run time. For instance, DBpedia takes a longer time in Pokec even though Pokec is larger than DBpedia and the minimum support in Pokec is set smaller than that in DBpedia. To further investigate this matter, we plot, in Figure 5, the distribution of run time components on each step. Table 8 shows the numbers of frequent attribute sets, patterns, and rules on each data set. Table 9 shows the numbers of path pattern types in rules. We observe that the run time distribution differs across datasets. On Nell, most time is devoted to finding rules and length-2 simple path patterns. On DBpedia, the run time is predominantly spent in finding length-2 simple patterns. On Pokec, time goes to finding all kinds of path patterns rather than rules. These distributions of run time are generally consistent with the numbers of patterns and rules in Table 8; overall, our results corroborate our time complexity analysis, where the numbers of candidates highly affect run time."}, {"title": "E.3 CSM-A setting", "content": "We juxtapose the run time of our algorithms to that of CSM-A. CSM-A has three parameters minimum support e, distance threshold h, and the size of outputs k. We describe how to set each value in each dataset.\nFirst, we set the same number of rules found by our algorithm to k in Table 8. Second, we set zero to h; zero is the lightest parameter on CSM-A. As h increases, the search space increases. Finally, we set 0 to values that CSM-A found at least k patterns. CSM-A works on labeled graphs rather than attributed graphs, so we use one of the attributes in each vertex as a vertex label. Since CSM-A processes only labeled graphs, we reduce the number of vertex attributes in Nell, DBpedia, and Pokec to 1 for CSM-A, letting our algorithms work on a larger search space than CSM-A. MiCo is a labeled graph, so we let our algorithms and CSM-A operate on the same graph. As CSM-A runs on a single thread, we also run our algorithms single-threaded in this comparison. We summarize the parameters in Table 10."}, {"title": "E.4 Memory usage on synthetic graphs", "content": "Figure 8 shows the memory usage on synthetic graphs. This results show the our approximation can reduce the memory usage; In particular, sampling can reduce the memory usage compared to the candidate reduction in the exponential, though the candidate"}]}