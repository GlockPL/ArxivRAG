{"title": "Mining Path Association Rules in Large Property Graphs (with Appendix)", "authors": ["Yuya Sasaki", "Panagiotis Karras"], "abstract": "How can we mine frequent path regularities from a graph with edge labels and vertex attributes? The task of association rule mining successfully discovers regular patterns in item sets and substructures. Still, to our best knowledge, this concept has not yet been extended to path patterns in large property graphs. In this paper, we introduce the problem of path association rule mining (PARM). Applied to any reachability path between two vertices within a large graph, PARM discovers regular ways in which path patterns, identified by vertex attributes and edge labels, co-occur with each other. We develop an efficient and scalable algorithm PIONEER that exploits an anti-monotonicity property to effectively prune the search space. Further, we devise approximation techniques and employ parallelization to achieve scalable path association rule mining. Our experimental study using real-world graph data verifies the significance of path association rules and the efficiency of our solutions.", "sections": [{"title": "1 INTRODUCTION", "content": "Association rule mining is the task of discovering regular correlation patterns among data objects in large data collections [1, 69]. An association rule, represented as X \u2192 Y, where X is an antecedent set, list, or other structure and Y is a corresponding consequent, signifies that a data record containing X is likely to also contain Y. Association rules are useful in applications ranging from web mining [41] to market analysis [39] and bioinformatics [45].\nGraph association rule mining aims to discover regularities among entities on a single large graph [21, 25, 64]. A graph association rule is represented as Gx \u2192 Gy, where Gx and Gy are graph patterns. Since graphs are widely used in many applications, the mining of association rules from graphs promises to discover valuable insights and knowledge. Its applications include:\nSocial analysis: Graph association rule mining can be used to discover regularities in social relationships. For example, as social relationship patterns affect people's health [34] and happiness [32], we may discover a rule like \u201cpeople who identify as happy are likely to connect with others who also identify as happy through multiple intermediaries with high probability.\u201d\nDiscrimination checking: Machine learning models trained on graph data are vulnerable to discriminatory bias [28]. For example, automated systems reviewing applicant resumes incorporated a significant bias in favor of male candidates due to bias inherent in the training data [15]. To build fair machine learning models, we should eschew such data-driven discrimination.\nKnowledge extraction: Knowledge bases are often represented as graphs with labeled edges and attributed vertices, known as knowledge graphs [65]. We can mine interesting patterns from such graphs as association rules. For example, an interesting rule may be \"people often have occupations similar to those of some of their ancestors.\"\nMotivation. While graph association rule mining on a single large graph is fundamental for graph analysis, existing methods [21, 25, 64] are inapplicable to the aforementioned applications due to the following shortcomings: (i) regarding the vertices in Gy and Gx, existing methods [21, 25] require the vertex set in the consequent Gy to be a subset of that in the antecedent Gx and mainly focus on missing edges - association rules where Gy includes vertices not in Gx is out of their scope; (ii) they consider specific restricted graph patterns, e.g., a single edge in Gy [25], a subgraph including at least one edge in Gx and Gy [64], or a subgraph without attributes in Gx and a single edge or attribute in Gy [21], and cannot handle edge labels and vertex attributes together; and (iii) their antecedent and consequent do not capture reachability (or transitive closure) patterns, which denote that one vertex is reachable by any number of label-constrained directed edges from another, such as the one regarding the examples on social analysis and knowledge extraction. Therefore, we need a different approach to graph association rule mining that addresses these shortcomings and thereby ensure wide applicability.\nContribution. In this paper, we introduce a novel, simple, and elegant concept, path association rule, which expresses regular co-occurrences of sequences of vertex attribute sets and edge labels, or path patterns, and allow for measures such as absolute/relative support, confidence, and lift. Such rules are in the form px \u2192 py, where px and py are path patterns with a minimum support of common source vertices. To mine path association rules while eschewing the aforementioned shortcomings, we propose a novel, efficient, and scalable algorithm that imposes no restriction on how the vertices in the consequent relate to those in the antecedent, accommodates reachability patterns, and considers both vertex attributes and edge labels."}, {"title": "2 RELATED WORK", "content": "We review existing works and research topics related to our work.\nFrequent graph mining on a single large graph. Definitions of frequency (i.e., support) in a single graph are different across studies. The support measures applied on transaction data do not preserve anti-monotonicity properties on a single graph. That is because, intuitively, the number of paths in a graph is usually larger than the number of vertices, even though paths are more complex than vertices. Several support measures that enforce anti-monotonicity properties have been proposed, such as maximum independent set based support (MIS) [62] minimum-image-based support (MNI) [10], minimum clique partition (MCP) [11], minimum vertex cover (MVC) [48], and maximum independent edge set support (MIES) [48]. Their common goal is to use anti-monotonic properties in case a vertex is involved in multiple matches. However, these support measures have three drawbacks. First, they do not apply to relative support because it is hard to count the maximum number of graph patterns that may appear in a graph; while the support measure proposed in [25] can be applied to relative support for a single large graph, it is inefficient because it needs isomorphic subgraph matching. Second, their time complexity is very high. For instance, the problems of computing MIS and MNI are NP-hard. Third, they are not intuitive, as it is difficult to understand why some vertices match graph patterns and others do not.\nEach algorithm on frequent subgraph mining in a single graph uses anti-monotonic properties specialized for their support. Existing supports for subgraph mining did not consider how to handle reachability patterns. Support of subgraphs with reachability patterns is untrivial, and thus, it is hard to directly apply algorithms for frequent subgraph mining to our problem.\nTheir basic concepts of frequent subgraph mining algorithms consist of (1) finding small sizes of frequent patterns, (2) combining frequent patterns to generate new candidates of patterns, (3) removing infrequent patterns based on anti-monotonic properties, and (4) repeating until candidates are empty. Commonly, steps (2) and (3) are extended to efficient processing for their patterns. Our baseline in Sec. 4.2 follows this basic method.\nGraph pattern mining. Several algorithms have been developed for graph pattern mining [3, 17, 18, 27, 40, 50, 54, 58], each with different semantics. For example, Prateek et al. [54] introduce a method for finding pairs of subgraphs that appear often in close proximity; Nikolakaki et al. [50] propose a method that finds a set of diverse paths that minimize a cost of overlapping edges and vertices. However, graph pattern mining does not handle reachability path patterns.\nGraph association rule mining. Graph association rule mining applies to two type of data: a set of (transactional) graphs and a single large graph. Methods for transactional graph data and those for a single large graph are not interchangeable because their anti-monotonicity properties are different. Algorithms for a set of graphs aim to find rules that apply in at least \u03b8 graphs in the collection (e.g., [37, 40, 55, 66]). On the other hand, algorithms for a single large graph aim to find rules that appear in a single graph at least \u03b8 times [21, 23, 25, 35, 49, 64]. \u03a4o our best knowledge, none of these methods focuses on paths or reachability patterns. Table 1 shows the characteristics of such methods, including ours.\nGraph pattern association rules on a large single graph, or GPARs, were introduced in [25]. Their association rules focus on specific patterns where the consequent specifies a single edge and a set of vertices that is a subset of the vertices in antecedent. A rule evaluates whether the antecedent contains the edge specified in the consequent. Besides, the algorithm in [25] aims to find rules with a fixed consequent rather than all valid frequent rules, so it is hard to extend to the latter direction. They use a vertex-centric support measure that counts the number of vertices in a subgraph that match a specified pivot; this measure allows for measuring relative support via extensive subgraph matching.\nCertain works extend or generalize GPAR. Wang et al. [64] find association rules using the MIS support measure [62] and require the antecedent and consequent to be subgraphs with at least one edge each but no common edges. Yet this technique cannot use relative support and cannot find regularities among vertex attributes (e.g., occupation and gender) because it does not allow specifying a single property as consequent. Fan et al. [21] proposed graph association rules, or GARs, that generalize GPARs with vertex attributes; this is the only graph association rule mining method that handles both edge labels and vertex attributes, albeit it allows only a single edge or attribute in the consequent. It also provides machine learning-based sampling to reduce graph size according to a set of required graph patterns in Gy. The difference between this sampling method and ours is that our sampling reduces the candidate source vertices, while the GAR algorithm reduces the graph itself.\nThe GPAR is applicable to find missing edge patterns in quantified graph patterns [23, 26] that include potential and quantified edges and to discover temporal regularities on dynamic graphs [49].\nMining other rules. Several studies extract other rules from graphs, such as graph functional dependencies [24] and Horn rules [13, 14, 31, 46, 47, 52, 57], which are similar to a path association rules. In a Horn rule, a consequent is a single edge whose vertices are included in the antecedent on RDF data. Yet Horn rules do not cover general property graphs.\nSubsequence mining [2, 29, 30, 51, 68] finds regularities of subsequence patterns in sequences. These are a special type of graph association rules, since a sequence can be seen as a path graph. Yet subsequence mining methods cannot apply to complex graphs."}, {"title": "3 THE CONCEPT", "content": "We propose the novel concept of path association rule mining (PARM), which effectively discovers regularities among attributes of vertices connected by labeled edges. The distinctive characteristic of PARM compared to existing graph association rule mining techniques is that it captures correlations of distinct path patterns among the same vertices, which are useful in many applications. In addition, PARM discovers rules on general property graphs, which cover many graph types (e.g., labeled graphs).\n3.1 Notations\nWe consider a graph G = (V, E, L, A), where V is a set of vertices, E \u2282 V \u00d7 L \u00d7 V is a set of edges, L is a set of edge labels, and A is a set of attributes. Each edge e \u2208 E is a triple (v, le, v') denoting an edge from vertex v to vertex v' with label le. Attribute a \u2208 A is a categorical value representing a feature of a vertex. Each vertex v \u2208 V has a set of attributes A(v) \u2286 A.\nA path is a sequence of vertices and edges (v0, e0, v1, . . ., en\u22121, vn), where n is its length, v0 its source, and vn its target. A path prefix (suffix) is an arbitrary initial (final) part of a path.\n3.2 Path Association Rules\nWe define path association rules after defining path patterns. Path pattern. We define simple and reachability path patterns.\n\u2022 A simple path pattern is a sequence of attribute sets and edge labels (A0, l0, A1, ..., ln-1, An) where Ai \u2286 A (Ai \u2260 0) and li \u2208 L; n indicates the pattern's length.\n\u2022 A reachability path pattern is a pair of attribute sets with an edge label (A0, l*, A1), where A0, A1 \u2286 A (A0, A1 \u2260 0) and l* \u2208 L."}, {"title": "3.3 Measures of association rules", "content": "Path association rules support measures similar to those of association rules [1]. Here, we define support, confidence, and lift for path association rules.\nSupport: The support of a path association rule r indicates how many vertices it matches. We define absolute and relative support.\nSignificantly, most graph association rule mining methods do not offer relative support (see Section 2), as it is hard to compute the number of matched graph patterns. Absolute support is defined as ASupp(px \u21d2 py) = |V(px) \u2229 V (py)|. Since the maximum value of |V(p)| is the number of vertices, relative support is defined as RSupp(px \u21d2 px) = |V(px)\u2229V(px)|/|V|.\nConfidence: The confidence of a path association rule indicates the probability that a vertex satisfies py if given it satisfies px. We define confidence as Conf (px \u21d2 px) = |V(px)\u2229V(px)|/|V(px)|.\nLift: Lift, which most graph association rule mining methods do not support, quantifies how much the probability of py is lifted given the antecedent px, compared to its unconditioned counterpart. We define lift as Lift(px \u21d2 px) = |V(px)|\u00b7|V(py)|/|V(px)\u2229V(py)|\u00b7|V|."}, {"title": "3.4 Problem Definition", "content": "We now define the problem that we solve in this paper.\nPROBLEM 1 (PATH ASSOCIATION RULE MINING (PARM)). Given graph G, minimum (absolute or relative) support \u03b8, and maximum path length k, the path association rule mining problem calls to find all rules r, where (1) supp(r) \u2265 \u03b8, (2) path lengths are at most k, and (3) px is not dominated by py or vice versa.\nRemark. Path association rule mining generalizes conventional association rule mining for itemsets [1]. We may consider each vertex as a transaction and its attributes as items in the transaction. Ignoring edges, path association rule mining degenerates to conventional itemset-based association rule mining.\nExtensions. We can modify PARM according to the application, for example, we can find rules where py dominates px, a set of attributes in path patterns is empty, and we can specify other thresholds (e.g., high confidence and Jaccard similarity) to restrict the number of outputs. We may also use other quality measures (e.g., [16, 53]) in place of the conventional measures we employ. As we focus on introducing the PARM problem, we relegate such extensions and investigations to future work."}, {"title": "4 MAIN PARM ALGORITHM", "content": "We now present our core algorithm for efficient path association rule mining. To solve the PARM problem, we need to (1) enumerate frequent path patterns as candidates, (2) find vertices matching frequent path patterns, and (3) derive rules. Our PARM algorithm first finds simple frequent path patterns and then generates more complex path pattern candidates therefrom using anti-monotonicity properties. Its efficiency is based on effectively containing the number of candidates.\n4.1 Anti-monotonicity properties\nThe following anti-monotonicity properties facilitate the reduction of candidate path patterns.\nTHEOREM 1. If p \u2282 p', then V(p) \u2287 V(p').\nThe following two lemmata follow from Theorem 2.\nLEMMA 1. If the prefix of a path pattern is infrequent, the whole pattern is infrequent.\nLEMMA 2. If path pattern p is infrequent and p' comprises A \u2287 Ai and l = li for all i, then p' is also infrequent.\n4.2 Baseline algorithm\nOur baseline algorithm discovers, in a sequence, (1) frequent attribute sets, (2) frequent simple path patterns, (3) frequent reachability path patterns, and (4) rules by Lemma 1. We describe each step in the following."}, {"title": "4.3 PIONEER", "content": "To achieve efficiency in PARM, we develop PIONEER, an algorithm that lessens the explored path patterns and rules by bound-based pruning and enhanced candidate generation.\nBound-based pruning. Anti-monotonicity properties eliminate candidates of path patterns whose prefixes are infrequent. Here, we introduce two upper bounds on the number of vertices matching a path pattern: (1) an upper bound on the number of vertices matching (p, l, A), and (2) an upper bound on the number of vertices matching (A, l, p), where p = (A0, l0, A1, ..., An\u22121, ln, An) is an arbitrary path pattern, l is an edge label, and A is an attribute set. We prune results with upper bound below \u03b8.\nTo derive such upper bounds, we use (i) the set of edges with label l that connect to a vertex whose attribute set includes A, E(A, l) = {(v, le, v')|(v, le, v') \u2208 E \u2227 A(v') \u2287 A \u2227 le = l}; and (ii) the set of vertices whose attribute set covers A and which have an out-going edge with label l, V (A, l) = {v|\u2203(v, le, v') \u2208 E \u2227 A(v) \u2287 A le = l}. The following lemmata specify our bounds:\nLEMMA 3. Given length i > 0, edge label l, and attribute set A, the number of vertices that match a path pattern of length i ending with (l, A) is upper-bounded by |E(A, l)| \u00b7 dmi\u22121 where dm is the maximum in-degree in the graph.\nLEMMA 4. Given length i > 0, attribute set A, and edge label l, the number of vertices that match path pattern of the form p = (. . ., li\u22122, A, l, Ai, . . .) is upper-bounded by|V(A, l)|\u00b7dmi\u22121 where dm is the maximum in-degree in the graph.\nWe use these lemmata to prune candidate path patterns. We prune patterns of length i with suffix (l, A) if |E(A, l)|\u00b7 dmi\u22121 < \u03b8. We collect the set of single attributes that pass this pruning when combined with edge label f and path length i as Al\u2081 (l) = {a \u2208 A | |E(a, l)|dmi\u22121 \u2265 \u03b8}. Likewise, we prune patterns of length i with attribute set A whose li\u22121 is l, if |V(A, l)|\u00b7 dm1-1 < \u03b8. We collect the set of edge labels that pass this pruning when combined with single attribute a \u2208 A and path length i as Ll\u2081 (a) = {l \u2208 L | |V(a, l)| dm1-1 \u2265 \u03b8}. The case of l* is equivalent to LT\u2081 (a).\nThus, bound-based pruning reduces the candidate edge labels and attribute sets for addition to frequent path patterns.\nEnhanced candidate generation. Using the two anti-monotonicity properties and bound-based pruning, we eliminate candidate path patterns that (1) have an infrequent prefix, (2) are dominated by infrequent path patterns, and (3) fall short of bound-based pruning. Pruned patterns are not included in the candidates. Here we introduce optimized candidate generation, which extends path patterns in the following ways.\n\u2022 Vertical: starting with path patterns of length zero (i.e., frequent attributes), we extend them to length k following Lemma 1 and adding suffixes that are not pruned by either Lemma 3 or Lemma 4.\n\u2022 Horizontal: starting with unit path patterns (whose attribute sets include only a single attribute each), we combine them to form frequent path patterns by applying Lemma 2.\nWe revise our algorithm of Section 4.2 to use enhanced candidate generation. We explain the modifications in each step.\n(1) Frequent attribute set discovery. We additionally find the A\u0442 and L\u0442 sets.\n(2) Frequent simple path pattern discovery. We first enumerate unit path patterns of length 1, vertically extending P0 by adding an edge label \u2208 L\u0442 and an attribute \u2208 A\u0442. After checking the frequency of all unit path patterns of length 1, we horizontally combine pairs of frequent path patterns to obtain new path patterns with more than one attributes. This horizontal extension drastically reduces candidates because, by Lemma 2, if any of two paths is not frequent, the combined path patterns are not frequent either. We repeat vertical and horizontal extensions until we obtain frequent path patterns of length k.\n(3) Frequent reachability path pattern discovery. We restrict the candidates for l* to edge labels in L\u0442 and those for A1 to A\u0442. We find frequent reachability path patterns whose attribute sets include a single attribute and then combine pairs of such path patterns to obtain complex reachability path patterns, following Lemma 2.\n(4) Rule discovery. We generate candidate rules utilizing path patterns found to be frequent in Steps 2 and 3, applying both Lemma 1 and Lemma 2. We first search for frequent rules that combine unit path patterns of length 1 and then extend those patterns vertically and horizontally. For a frequent rule p \u21d2 p', we generate candidates pv \u21d2 p', ph \u21d2 p',"}, {"title": "4.4 Auxiliary data structure", "content": "To facilitate efficient rule discovery, we maintain a data structure that stores, for each vertex, a list of matched path patterns and the targets of their corresponding paths. We extend paths using this data structure without searching from scratch. In addition, we maintain pairs of dominating and dominated path patterns, so as to generate rule candidate so as to avoid generating rule involving them. In effect, for a frequent rule p \u21d2 p', we obtain pu, ph, po and p via this auxiliary data structure."}, {"title": "4.5 Complexity analysis", "content": "We now analyze the time and space complexity of our algorithm. We denote the sets of candidate attribute sets as CA, simple path patterns as CS, reachability path patterns as C*, and rules as CR, where j denotes the iteration.\nTime complexity: The frequent attribute discovery incurs the same time complexity of traditional algorithms such as Apriori algorithm [1], i.e., O(|A| + |E||V| + \u03a3A \u03a3 l |V||CA|). The time complexity of frequent simple path pattern discovery depends on the number of frequent attributes and edge labels; it is O(|LT||AT|\u00b2 + \u03a3 2 |V||CS|). The frequent reachability path discovery step incurs a similar complexity as the previous step, while it also performs bread-first search; thus, it needs O(|LT|(|V| + |E|) + |LT||AT|\u00b2 + \u03a3 2 |V||C|). The rule discovery step combines pairs of path patterns and extends the patterns in found rules. It takes a worst-case time of O(|V||P1||P*| + \u03a3j=2 |V||CR|). In total, time complexity is O(|V||E| + |LT|(|V| + |E| + |AT|\u00b2) +\u03a3A + \u03a3 V(ICA]+[C+ |C*| + |CR|)), which highly depends on the size of the candidates.\nSpace complexity: The space complexity of the algorithm is O(|V|+ |E|+|A|+|CA|+|CS|+|C*|+|CR|+|V|(\u03a3=0 |Pi|+|P*|+|R|)), where |CA|, |CS|,|C*|, and |CR| are the maximum sizes of |CA|, |C|, |C*|, and |CR| over iterations j, respectively. Practically, memory usage does not pose an important problem on commodity hardware."}, {"title": "5 APPROXIMATION TECHNIQUES", "content": "PIONEER reduces the candidates of path patterns for efficient mining. However, it still does not scale to large graphs as it has to exactly compute the frequency of all unpruned path patterns, whether they are frequent or not. We present two approximation methods to reduce the computation costs for checking the frequency.\n5.1 Approximate Candidate Reduction\nThe bound-based pruning in Lemma 3 computes upper bounds using the maximum in-degree, hence may retain candidates that are not likely to be involved in frequent path patterns. Our first approximation strategy aims to eliminate such candidates in A\u0442."}, {"title": "5.2 Stratified Vertex Sampling", "content": "Sampling effectively reduces the computation cost in many data mining tasks [21, 22, 43, 60]. We propose a sampling method that picks a set of vertices to work with, aiming to reduce the computation cost to find matched vertices and theoretically analyze its approximate accuracy.\nWe do not reduce the graph itself, as in [23], as then it would be hard to guarantee accuracy. Instead, as our algorithm focuses on frequent path patterns, rather than subgraphs, we can afford to reduce the number of source vertices by vertex sampling.\nMethod. We need a sampling strategy that preserves accuracy as much as possible. To achieve that, we use stratified sampling according to attributes of vertices [59]. We group vertices into strata according to their attribute sets and remove vertices that have no frequent attributes because they do not contribute to frequent path patterns. From each stratum, we pick vertices with sampling ratio \u03c1. We estimate the frequency of p as follows:\n|V(p)| = |Vs(p)|/\u03c1\nVs denotes the set of sampled vertices in strata related to A0 of p. Theoretical analysis. The accuracy of sampling is expressed by variance. The variance of our sampling strategy is:\n\u03c3 2 = \u03a3 v \u2208 Vs (x i\u2212 x) 2 /\u03c1V s \u2212 1 where x\u2081 = 1 if v\u00a1 matches p, otherwise 0. x is |V(p)|/Vs. The confidence interval is then the following:\n|Vs|(x- z. \u221a pV s /\u221a pV s /S ) < |V(p)| < |Vs|(x + z. \u221a pV s /\u221a pV s /S ) where z indicates the z-value for a confidence interval.\nContrary to candidate reduction, vertex sampling may cause false positives, that is, infrequent path patterns could be reported as frequent path patterns. Yet it can reduce the computation costs, even in cases where the candidate reduction does not."}, {"title": "6 PARALLELIZATION", "content": "We accelerate PIONEER by parallelization. Given a number of threads N, we partition the set of vertices into N subsets to balance the computing cost among threads in terms of the frequency of vertex attributes and adjacent edges. We describe how we estimate computing costs in the following.\nCost estimation. The cost to find matched vertices increases with the number of matched paths from each vertex. First, we prune vertices that do not have target attributes or outgoing edges with target edge labels, as these vertices have no matched frequent path patterns, as the following lemma specifies.\nLEMMA 5. If v \u2208 V has no target attributes and no outgoing edges with target labels, then v has no matched path patterns.\nAmong non-pruned vertices, the more frequent their attributes and outgoing edges are, the more matched path patterns they may match. We estimate the cost C(v) of a vertex v as:\nC(v) = d\u03c4 (v) \u00b7 |A\u03c4 (v)|\nwhere d\u03c4 (v) is the number of out-going edges with target edge labels and |A\u03c4 (v)| is the number of target attributes on v. This estimation is O(1) considering the numbers of edges and attributes on v as constants.\nPartitioning. We partition the set of vertices into N subsets according to estimated costs by a greedy algorithm. We sort vertices in ascending order of their costs and repeatedly assign the unassigned vertex of the largest cost to the thread with the smallest sum of assigned vertex costs; we do not assign a vertex to a thread that is already assigned |V|/N vertices. The time complexity of this algorithm is O(N|V|log |V|)."}, {"title": "7 EXPERIMENTAL STUDY", "content": "We conduct an experimental study on PIONEER to assess (1) its efficiency, (2) its scalability, and (3) the accuracy of our approximations. We also assess (4) the effectiveness of PARM. We implemented all algorithms in C++ and ran experiments on a Linux server with 512GB of main memory and an Intel(R) Xeon(R) CPU E5-2699v3 processor. Some experimental settings and results (e.g., length k and approximation factors) are in the supplementary file.\n7.1 Experimental Setting\nDataset. We use four real-world graphs with edge labels and vertex attributes: knowledge graph Nell [12], coauthorship information network MiCo [6], knowledge graph extracted from Wikipedia DBpedia [6], and social network service Pokec. We also use two types of synthetic graphs, uniform and exponential. They differ on how we generate edges; in uniform, we generate edges between randomly selected vertices following a uniform distribution, while in exponential we follow an exponential distribution. We vary the number of vertices from 1M to 4M and generate a fivefold number of edges (i.e, 5M to 20M). Table 7 presents statistics on the data.\nCompared methods. We assess a baseline and four variants of PIONEER. Baseline is the algorithm of Section 4.2 without any optimization strategy. PIONEER is our algorithm using the strategies of Section 4.3. PIONEER w/ CR using the candidate reduction of Section 5.1, while PIONEER w/ SA uses the stratified sampling of Section 5.2, and PIONEER w/ CR+SA employs both. All algorithms are parallelized by the technique of Section 6. Further, we compare the run time of CSM-A [54]\u00b9, a method that approximately finds the top-k frequent correlated subgraph pairs, to PIONEER, even though the output of CSM-A is different from that of PARM. Notably, other extant graph association rule mining methods [21, 23, 25, 64] do not address the PARM problem and do not have open codes.\nParameters. We set minimum support threshold 1000 on Nell, 4000 on MiCo, 120 000 on DBpedia, and 45 000 on Pokec, respectively; both the candidate reduction factor \u03b3 and the sampling rate \u03c1 are set to 0.4; we set the maximum path length k = 2, and use 32 threads. We compute absolute support, relative support, confidence, and lift. We vary these parameters to evaluate their impacts while using the above values as default parameters.\n7.2 Efficiency\nVarying minimum support \u03b8. Figure 2 plots run time vs. minimum support on each data. The minimum support directly affects the number of rules to be discovered, hence computational cost. As the minimum support falls, the number of candidate path patterns, and hence run time, grows. Our algorithms outperform the baseline in terms of efficiency. In Pokec, the baseline did not finish within 24 hours due to the large number of attributes.\nWe observe that the enhanced candidate generation, employed in Ours, is effective in reducing candidate path patterns. Further, our approximation methods reducing the number of candidate path patterns and processed vertices by sampling further reduce the computational cost. Our algorithm employing both of these approximation methods consistently achieves the lowest runtime.\nRegarding the comparison between those two methods, our algorithm with candidate reduction is more effective than that with sampling on Nell, DBpedia, and Pokec, as the number of candidates is often larger than the number of vertices on those data. In MiCo, on the other hand, candidate reduction is not so effective because it does not reduce the candidates with the default \u03b3 = 0.4.\nComparison to CSM-A. We juxtapose the run time of PIONEER to that of CSM-A. Figure 3 presents our results. On Pokec, PIONEER is much faster than CSM-A, indicating that CSM-A is less scalable in graph size. On DBpedia PIONEER is less efficient, as DBpedia has a large average number of attributes per vertex, yielding a larger search space for PIONEER than for CSM-A (i.e., we reduce the number of attributes per vertex to one for CSM-A).\nVarying path length k. As the path length k grows, the candidate path patterns increase, thus computation cost grows. On Nell, the number of rules drastically increases as k grows, hence candidate reduction becomes more effective than sampling when k = 3. With k = 4, due to the growing number of rules, our algorithms did not finish within 24 hours. Arguably, to effectively find path association rules, we need to either increase the minimum support or reduce approximation factors. On DBpedia the number of path patterns does not grow when k > 2, so run time does not increase either. This is because some vertices have no outgoing edges, thus most path patterns have length 2. In Pokec, when the k > 2, the number of reachability patterns increases, thus the run time increases. Overall, our approximation methods work well when k is large, as the run time gap between the exact and approximate algorithms widens.\nVarying approximation factors. The approximation factors \u03b3 for candidate reduction and \u03c1 for sampling indicate the degree of approximation. As both decrease, the extent of approximation increases. Notably, when approximation factors are small, run time is low, with the partial exception of Nell. On Nell, the run time of PIONEER w/ SA is high when \u03c1 = 0.2 due to many false positives that burden the rule discovery step, as Nell is a small graph compared with DBpedia and Pokec. On the other hand, candidate reduction does not increase the number of found frequent path patterns and rules, so the run time of PIONEER w/ CR grows with the approximation factor. In conclusion, sampling effectively reduces run time in large graphs, yet it may not be effective on small graphs.\nE.3 CSM-A setting\nWe juxtapose the run time of our algorithms to that of CSM-A. CSM-A has three parameters minimum support \u03b5, distance threshold h, and the size of outputs k. We describe how to set each value in each dataset.\nFirst, we set the same number of rules found by our algorithm to k in Table 8. Second, we set zero to h; zero is the lightest parameter on CSM-A. As h increases, the search space increases. Finally, we set \u03b8 to values that CSM-A found at least k patterns. CSM-A works on labeled graphs rather than attributed graphs, so we use one of the attributes in each vertex as a vertex label. Since CSM-A processes only labeled graphs, we reduce the number of vertex attributes in Nell, DBpedia, and Pokec to 1 for CSM-A, letting our algorithms work on a larger search space than CSM-A. MiCo is a labeled graph, so we let our algorithms and CSM-A operate on the same graph. As CSM-A runs on a single thread, we also run our algorithms single-threaded in this comparison. We summarize the parameters in Table 10."}, {"title": "7.3 Scalability", "content": "Varying the number of threads. Table 3 shows the run time vs. the number of threads. As expected, the run time of our algorithms decreases almost linearly as the number of threads rises, especially on larger data.\nMemory usage. Table 4 presents the memory usage of our algorithms. DBpedia raises the highest memory requirements, Pokec the lowest, indicating that memory use depends on the number of candidates more than on graph size. Our algorithms reduce memory usage by reducing candidates, while vertex sampling further reduces memory use by reducing path patterns to be stored for each vertex. When the number of frequent path patterns is large, the approximation methods effectively reduce memory use, as on Nell, confirming our space complexity analysis.\nGraph size. Figure 4 depicts run time on uniform and exponential synthetic graphs with 0.01 as the relative minimum support. The results suggest that run time grows linearly. Thus, our algorithms are highly scalable to large graphs. On uniform data, vertex sampling is more effective than candidate reduction, while on exponential data, candidate reduction proves to be more effective, since it works best when the maximum degree deviates far from the average.\n7.4 Accuracy\nWe evaluate our approximation methods in comparison to exact ones in terms of recall and precision. Recall is the fraction of true frequent rules that are found and precision is the fraction of found frequent rules that are true; thus, both recall and precision are the value 1.0 if approximation methods return the same results of the exact algorithm. Both recall and precision are quite high in MiCo"}]}