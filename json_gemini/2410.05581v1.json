{"title": "Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?", "authors": ["F\u0131rat \u00d6ncel", "Matthias Bethge", "Beyza Ermis", "Mirco Ravanelli", "Cem Subakan", "\u00c7a\u011fatay Y\u0131ld\u0131z"], "abstract": "In the last decade, the generalization and adaptation abilities of deep learning models were typically evaluated on fixed training and test distributions. Contrary to traditional deep learning, large language models (LLMs) are (i) even more overparameterized, (ii) trained on unlabeled text corpora curated from the Internet with minimal human intervention, and (iii) trained in an online fashion. These stark contrasts prevent researchers from transferring lessons learned on model generalization and adaptation in deep learning contexts to LLMs.\nTo this end, our short paper introduces empirical observations that aim to shed light on further training of already pretrained language models. Specifically, we demonstrate that training a model on a text domain could degrade its perplexity on the test portion of the same domain. We observe with our subsequent analysis that the performance degradation is positively correlated with the similarity between the additional and the original pretraining dataset of the LLM. Our further token-level perplexity observations reveals that the perplexity degradation is due to a handful of tokens that are not informative about the domain. We hope these findings will guide us in determining when to adapt a model vs when to rely on its foundational capabilities.", "sections": [{"title": "1 Introduction", "content": "Deep learning generalization theory and empirical studies have traditionally assumed a fixed data distribution from which training and test datasets are sampled (Neyshabur et al., 2017). This train-test paradigm was later evolved by domain adaptation and continual learning, where the original training distribution differs from future distributions to be fitted. The advent of foundation models has marked a significant shift, as these general-purpose models are pretrained on enormous datasets, which may not even be published (Kaplan et al., 2020).\nFurthermore, many datasets are known to have data leakage, where train and test points are duplicates (Soldaini et al., 2024). Consequently, in this modern era of machine learning, the clear train-test dichotomy does not apply for LLM training.\nThis short paper stems from our curiosity about whether conventional machine learning principles remain relevant amidst the aforementioned paradigm shift. Specifically, we aim to understand to what extent the deep learning optimization and generalization practices of the last decade can be applied today. Our primary question is the following: Is it still relevant to study additional pretraining of models that have already been trained on possibly unknown text corpora by LLM engineers?\nThe earlier works in the literature pertinent to this question have conflicting findings (Gururangan et al., 2020; Cheng et al., 2023). However, we believe that the empirical findings in the paper help to improve our understanding on this subject by presenting more consistent observations.\nFor our investigation, we adapt LLMs of various sizes and architectures to different domains within the Massively Multi-Domain Dataset (M2D2, (Reid et al., 2022)), a carefully curated collection of over 200 text domains from Wikipedia (Wiki) and Semantic Scholar (S2ORC). We compare the perplexities obtained on the test set of a domain before and after training on the same domain. While it is generally expected that adaptation to a new domain would improve the within-domain test perplexity, our findings suggest this is not always the case.\nInterestingly, we observe that additional pretraining on Wiki domains tends to degrade test perplexity, while pretraining on S2ORC domains always improves it. To quantify this intuitive observation, we measure the distributional similarities between additional training domains and the original pretraining corpora. Our results show that the performance degradation is positively correlated with"}, {"title": "2 Method", "content": "In this section we present training details, source corpora and adaptation domains details, evaluation method and domain similarity measures."}, {"title": "2.1 Models and Training", "content": "We conduct our experiments with decoder-only GPT2 model family (Radford et al., 2019), such as GPT2-small, GPT2-large and GPT2-xlarge, OLMo-1B (Groeneveld et al., 2024) and LLaMA-7B (Touvron et al., 2023) models. We additional pretrain models on M2D2 domains separately (see the next section for details) using the Deepspeed library (Rasley et al., 2020). We use a learning rate of 0.00005 for the GPT2 models, 0.000005 for the LLaMA-7B, and 0.0000085 for the OLMo-1B model. We additional pretrain each model for 1 epoch on a single GPU.\nOur domain similarity analyses require access to the training corpus of the said LLMs, which is why we choose open-data models. To conduct the analyses, we sample 400k texts from GPT2's training corpus, OpenWebText (Gokaslan and Cohen, 2019), 650k texts from OLMo's training corpus, Dolma (Soldaini et al., 2024) and 930k text from LlaMa's training corpus, (Computer, 2023)."}, {"title": "2.2 Tasks", "content": "We conduct experiments on 20 adaptation domains from M2D2 Dataset, adaptation domains are presented in Appendix A.1. Half of the domains belong to the Wiki portion, while the other half belong to the S2ORC portion of the dataset. We choose the adaptation domains based on the similarity measures explained in section 2.4. The selected S2ORC domains include: High Energy Physics, Nuclear Experiment, Condensed Matter, Mathematics, Super Conductivity and Astrophysics while Wiki domains include: Society and Social Sciences, Technology and Applied Sciences, Human Activities, Culture and the Arts, History and Events, Philosophy and Thinking, Natural and Physical Sciences and General Reference."}, {"title": "2.3 Evaluation", "content": "We evaluate model's perplexities on generation task with the additional pretrained model on that specific domain for a single epoch."}, {"title": "2.4 Domain Similarity Measures", "content": "We use two similarity measures to compare the similarity between original corpora and adaptation domains. For each source corpus and target domain, we randomly sample 5% of the texts for large domains or up to 50 000 texts for small domains (when feasbible). We then extract d-dimensional $l_2$ normalized embeddings using Sentence Transformers (SBERT) (Reimers and Gurevych, 2019). We define the corpus embeddings with M samples as $C = [\\theta(Ct_1), ..., \\theta(Ct_m)]$, and domain embeddings with N samples as $D = [\\theta(Dt_1), ..., \\theta(Dt_n)]$ where $\\theta$ is the feature extractor SBERT model. We calculate the following similarity measures as follows:\nMaximum Mean Discrepancy (MMD). We use the closed-form expression from (Gretton et al., 2012) to calculate MMD, with linear kernel, between the source corpus C and target domain D: $MMD(C,D) = ||\\mu_C - \\mu_D ||^2$, where $\\mu_C$ and $\\mu_D$ are d-dimensional sample means.\nFr\u00e9chet Distance (FD). We use the closed-form expression from (Dowson and Landau, 1982) to calculate FD between the source corpus and target domain. FD between corpus C and target domain D: $FD(C,D) = ||\\mu_C - \\mu_D||^2 + tr(\\Sigma_C + \\Sigma_D - 2\\sqrt{\\Sigma_C\\Sigma_D})$ where $\\mu_C$ and $\\mu_D$ are d-dimensional sample means, $\\Sigma_C$ and $\\Sigma_D$ are (d, d)-dimensional sample covariances."}, {"title": "3 Results", "content": "For different sizes of GPT2 as well as OLMO-1B and LLaMA-7B, we first compute the zero-shot perplexity (Figure 1) on all domains. After the additional pretraining the models on each domain individually, we also compute the test perplexity on the corresponding test sets and refer it as the adaptation perplexity (Figure 1). Because the models are tested on the same domain as they are trained on, one would naturally expect the perplexity to"}, {"title": "4 Discussion", "content": "This short paper aims to improve our understanding of additional pretraining of models already pretrained on diverse text corpora. Interestingly, we observed that within-domain perplexity does not always increase. Below we summarize our main findings and takeaways:\nSimilarity between original corpus and adaptation domain affects the performance. When the original corpus and the adaptation domain are more similar, test perplexity in this adaptation domain after additional pretraining tends to increase. This phenomenon is not observed while adapting a less similar domain. Therefore, we recommend practitioners analyze the domains in their original pretraining corpora and then decide (not) to adapt.\nAdaptation influences smaller models more Regardless of the training domain, the perplexity of GPT2-small models seems to change the most through adaptation. This finding suggests that adapting larger models may not be necessary.\nGoing beyond perplexity? Our token-level observations reveal that most of the perplexity degradation arises from specific special tokens. This indicates that perplexity alone may not fully capture the impact of adaptation. For future work, we plan to extend our analysis to include domain-specific tokens to better quantify the gains and degradations resulting from adaptation, providing recipes for when to stop adaptation or continue adapting."}, {"title": "5 Limitations", "content": "Our analyses require access to the training corpus of a pretrained LLM, thus not applicable to all models. One way to overcome this issue could be gathering a large representative corpus across the Internet and conducting analyses using this corpus. Further, our analysis quantifies the gains and degradations only via perplexity while computing downstream performance would be equally interesting."}]}