{"title": "ARCHILLES' HEEL IN SEMI-OPEN LLMS: HIDING BOTTOM AGAINST RECOVERY ATTACKS", "authors": ["Hanbo Huang", "Yihan Li", "Bowen Jiang", "Lin Liu", "Ruoyu Sun", "Zhuotao Liu", "Shiyu Liang"], "abstract": "Closed-source large language models deliver strong performance but have limited downstream customizability. Semi-open models, combining both closed-source and public layers, were introduced to improve customizability. However, parameters in the closed-source layers are found vulnerable to recovery attacks. In this paper, we explore the design of semi-open models with fewer closed-source layers, aiming to increase customizability while ensuring resilience to recovery attacks. We analyze the contribution of closed-source layer to the overall resilience and theoretically prove that in a deep transformer-based model, there exists a transition layer such that even small recovery errors in layers before this layer can lead to recovery failure. Building on this, we propose SCARA\u00b9, a novel approach that keeps only a few bottom layers as closed-source. SCARA employs a fine-tuning-free metric to estimate the maximum number of layers that can be publicly accessible for customization. We apply it to five models (1.3B to 70B parameters) to construct semi-open models, validating their customizability on six downstream tasks and assessing their resilience against various recovery attacks on sixteen benchmarks. We compare SCARA to baselines and observe that it generally improves downstream customization performance and offers similar resilience with over 10 times fewer closed-source parameters. We empirically investigate the existence of transition layers, analyze the effectiveness of our scheme and finally discuss its limitations.", "sections": [{"title": "1 INTRODUCTION", "content": "Vendors of Large Language Models (LLMs) have recently launched several closed-source models with impressive capabilities, serving diverse user needs across different scenarios. Access to closed-source models is typically through black-box APIs, such as GPT-40 and Claude 3.5. These APIs hide model weights and internal structures, restricting downstream customizability. To address this, semi-open LLMs have emerged, offering more flexibility while still hiding certain infrastructure details. As shown in Figure 1, these semi-open models keep some modules closed-source but allow access to others. This enables users to interact with closed-source modules via embedding APIs and fine-tune the open modules for downstream tasks.\nOpen-sourcing more parameters and structure details apparently enhances downstream customizability. However, Zanella-Beguelin et al. (2021) showed that semi-open LLMs with only a few closed-source parameters are vulnerable to model recovery attacks. Recovery attackers query the closed-source module and then train a new module that imitates its functionality. This can lead to the full replication and theft of closed-source modules (Solaiman, 2023). Although the community"}, {"title": "2 RELATED WORKS", "content": "Model Customization. Vendors have introduced three main strategies for model customization, each with distinct trade-offs. First, fine-tuning APIs allow users to customize fully closed-source models (e.g., La Plateforme, Azure AI Services), while restricting access. Second, embedding models provide richer customization by allowing users to select and modify subsequent structures. However, the absence of joint pre-training can degrade performance and expose the closed-sourced models to recovery attacks. Third, open-source models enable full customization flexibility through free fine-tuning, yet pose challenges to model control and usage supervision."}, {"title": "3 PRELIMINARIES", "content": ""}, {"title": "3.1 SECURITY THREAT: SEMI-OPEN MODEL RECOVERY", "content": "Semi-open LLMs. Let $X \\in \\mathbb{R}^{n\\times d}$ denote the input data matrix, where each row corresponds to a d-dimensional feature vector representing a single token. Let $f : \\mathbb{R}^{n\\times d} \\rightarrow \\mathcal{Y}$ denote a victim model, capable of processing the feature matrix X and producing an element in the set $\\mathcal{Y}$ as output. Modern LLMs typically adopt a multi-layer architecture to capture complex patterns in the input data. Specifically, f is a composition of multiple decoder layers, i.e., $f(X; \\theta) = \\varphi_L \\circ ... \\circ \\varphi_1(X)$.\nAll decoder layers $\\varphi_1,..., \\varphi_L$ share the same architecture but each layer is equipped with distinct parameters. The parameters of all layers are denoted by the vector \u03b8. We consider a semi-open setting, in line with Zanella-Beguelin et al. (2021) and Xu et al. (2021), where certain layers of the LLM are closed-sourced while others remain public. Let the closed-sourced set $I \\subseteq \\{1, ..., L\\} \\triangleq [L]$ denote the index set of hidden layers, while its complement $I^c$ contains the public layer indices.\nSemi-open Model Recovery Attack. The semi-open model recovery attack is one of the high-fidelity attacks (Carlini et al., 2024) that aims to recover the general functionality of the victim model. As illustrated in Figure 2, the adversary is able to interact with the victim LLM and possesses access to the architecture and parameters of its public layers. The adversary aims to emulate the behavior of the victim LLM by learning the architecture and parameters associated with the closed-source layers. This task is accomplished through a structured procedure comprising three main steps: (1) Attack Dataset Construction: The adversary begins by querying the victim model, thereby gathering an attack dataset $\\mathcal{D}$ containing samples representing the capabilities of the victim LLM. (2) Parameter Initialization: Next, the adversary randomly initializes the parameters associated with the closed-sourced layers, setting the stage for subsequent model fine-tuning. (3) Model Fine-Tuning: Leveraging the dataset $\\mathcal{D}$ obtained in the first step, the adversary fine-tunes the parameters of the entire model, iteratively adjusting the parameters to better align with the observed behavior of the victim LLM. Let $\\theta_{FT}(I, \\mathcal{D})$ denote the recovered parameters under the attack dataset $\\mathcal{D}$ and closed-sourced set I. Prior work has shown that this approach enables the adversary to replicate the general functionality of the semi-open LLMs, despite limited access."}, {"title": "3.2 PROBLEM FORMULATION", "content": "In this paper, we consider the performance of a large language model within a defined distribution, denoted as $P_{\\mathcal{X} \\times \\mathcal{Y}}$, representing the relationship between the input matrix X and corresponding label Y. We assume that the victim LLM $f(X; \\theta)$ performs well within this distribution. Additionally, we presume the attack set $\\mathcal{D}$ consists of independent and identically distributed (i.i.d.) samples drawn from $P_{\\mathcal{X} \\times \\mathcal{Y}}$. To assess the alignment between the outputs of LLM and the ground-truth labels, we"}, {"title": "4 METHODOLOGY", "content": "In this section, we investigate how each layer affects customizability and resilience against recovery attacks. We begin with an experiment involving two semi-open Llama2-70B models, each with either the first two (Semi-Open-1) or the last two (Semi-Open-2) decoder layers closed-sourced. We compare their customization performance and recovered performance under the recovery attack. Figure 3 (a) and (b) show that although two semi-open models perform similarly on six downstream tasks, closed-sourcing the first two layers offers significantly greater resilience than the last two. Moreover, we compare the Semi-Open-1 model to the fully-closed model and observe that this semi-open model can achieve better customizability and comparable resilience at the same time. Therefore, we conjecture that, with a sufficient number of closed-sourced layers before a certain transition layer, a semi-open model can simultaneously achieve great customizability on downstream tasks and strong resilience against recovery attacks. In this section, we first present a theoretical result showing the existence of transition layers and then introduce our selective closed-sourcing approach."}, {"title": "4.1 RESILIENCE TRANSITION LAYER IN INFINITELY DEEP TRANSFORMERS", "content": "Model Overview. Let us revisit our large language model composed of L layers, denoted as $f(X; \\theta) = \\varphi_L \\circ ... \\circ \\varphi_1(X)$. Recall that each row of the feature matrix $X \\in \\mathbb{R}^{n\\times d}$ represents a d-dimensional vector for an input token. We treat each layer i as a transformer layer, where each layer processes an n \u00d7 d dimensional matrix as input and outputs another n \u00d7 d matrix. Thus, the model f outputs a matrix of n rows and d columns, indicating that the large language model outputs a feature vector for each token. Moreover, we assume that each layer contains a normalized residual self-attention function, defined as\n$Y_i (X; K_i, Q_i) = X + \\text{softmax}\\left(\\frac{XQ_i(XK_i)^T}{\\sqrt{d_Q}||X||^2}\\right)X$,\nwhere $Q_i \\in \\mathbb{R}^{d \\times d_Q}$ and $K_i \\in \\mathbb{R}^{d \\times d_Q}$ are projection parameter matrices for the Q and K matrices in the transformer, respectively. Additionally, $\\sqrt{d_Q}$ and the matrix norm $||X||$ denote normalization factors provided by the normalization layer. We consider the strategy of concealing the L-th layer"}, {"title": "4.2 SCARA: SELECTIVE CLOSED-SOURCING APPROACH AGAINST RECOVERY ATTACK", "content": "We propose a method to approximately find the smallest bottom layer index set I that satisfies $R(I) \\leq (1 + \\epsilon)R([L])$. A simple approach is to start with $I_1 = \\{1, ..., l\\}$ for each l beginning from 1, then evaluate the recovery ratio R(I\u2081) after the attack, and identify the smallest l that meets the inequality. This extensive fine-tuning process is time-consuming, prompting the critical question: Can we create a fine-tuning-free metric that predicts LLM performance under semi-open model recovery attacks? Hence, our goal is to establish a metric directly correlated with the recovery ratio.\nIn the recovery ratio R(I), each I has the same denominator, so our focus is on a metric related to the numerator, specifically $E[s(f(X; \\theta_{FT}(I, \\mathcal{D})), Y)]$, which measures the average performance score of the recovered model. This average performance score generally inversely correlates with the average testing loss $E[l(f(X; \\theta_{FT}(I, \\mathcal{D})), Y)]$. Therefore, our goal becomes finding the l such that\n$E[l(f(X; \\theta_{FT}(\\{1, ..., l\\}, \\mathcal{D})), Y)] \\geq (1 - \\epsilon)E[l(f(X; \\theta_{FT}([L], \\mathcal{D})), Y)]$."}, {"title": "5 EXPERIMENTS", "content": ""}, {"title": "5.1 EXPERIMENTAL SETTINGS", "content": "Models. We consider five open-source, decoder-only structured LLMs with various architectures. Specifically, we select Llama2-70B-chat, Llama2-7B-chat, Mistral-7B- v0.1, Phi-2, and Phi-1.5. We designate these pre-trained models as the base models for customization and victims in semi-open model recovery attacks. Details of each model is provided in Appendix B.1.\nAttack Methods. We recover models produced by different closed-source approaches using three attack methods: FT-all, FT-closed, and SEM (Stealing Embedding Model). FT-all finetunes the entire model, whereas FT-closed fine-tunes the closed-sourced set only. We also consider SEM, a typical attack that trains a substitute embedding module with data from the closed-sourced set. Following (He et al., 2021), a diverse attack set is required for full recovery. Therefore, we merge data evenly for two general datasets, MMLU benchmark and Alpaca 52k, resulting in a 51k combined set. Details are in Appendix B.2\nBaselines. We compare SCARA with the other two baselines: SAP-DP and the fully-closed approach. The SAP framework keeps the first six decoder layers open and the rest closed-source. SAP-DP extends SAP by adding Laplace noise to the model outputs, a common strategy for model protection. The fully-closed approach represents the extreme, where all layers are closed-sourced. Further details can be found in Appendix B.3\nImplementation Details of SCARA. We apply the SCARA algorithm to identify the smallest closed-sourced set I such that $R(I) \\leq (1 + \\epsilon)R([L])$. To calculate the recovery difficulty (RD), we use a 1.5k evaluation set sourced from the same datasets as the attack set. We explore e values from 0.05 to 1 and discover that setting \u025b to 0.05 yields optimal performance on the evaluation set. We randomly initialize the hidden layers and average the RD across three seeds to select the smallest layer set. For more details on the dataset and the SCARA sensitivity on &, see Appendix B.4 and Section 5.3.\nEvaluation Benchmarks We assess customizability on six downstream tasks: Code , Math , Medical , Finance , Law , and Alignment . To fully evaluate recovered functionalities, we focus on six capabilities domains following Llama2 report . Specifically, we assess the recovered model across sixteen benchmarks grouped into (1) Commonsense Reasoning (Rsn.); (2) Reading Comprehension (Read.); (3) World Knowledge (Knl.); (4) Code; (5) Math; and (6) General Ability (Gen.). More details can be found in Appendix B.5."}, {"title": "5.2 MAIN RESULTS", "content": "In this subsection, we compare SCARA with three baselines, demonstrating its superior customizability on downstream domains while preserving similar resilience against model recovery attacks.\nCustomizability: SCARA vs. Baselines. We compare the customization performance of SCARA with closed-source baselines. Results are shown in Figure 4 and detailed in Appendix B.6.\nOn 70B and 7B models, SCARA consistently surpasses SAP-DP and fully-closed approaches across six domains and aligns closely with the performance of the fully-open approach, where all parameters are accessible. For instance, in the Law domain, SCARA improves scores by 10% over SAP-DP and fully-closed approaches on Llama2-70B, with this improvement rising to 35% on 7B models. Similar patterns of enhanced customizability are also evident in Phi-2 model, though the improvement on the Law domain narrows to only 1%. Furthermore, SCARA maintains performance comparable to the"}, {"title": "5.3 ANALYSIS ON SCARA", "content": "In this subsection, we assess the effectiveness of SCARA on four LLMs, showing the resilience transition in LLMs and the strong correlation between recovery difficulty and ARR.\nTransition Layer. We investigate the existence of transition layers in resilience against model recovery and customizability in the math domain. Results are in Figure 6(a) and detailed in Appendix C.7\nFor resilience, we hide same-sized layer sets with different start points, and attack them using FT-all. Specifically, the sets consist of one layer for Llama2-7B and two layers for Phi-2. We observe clear resilience transitions in both models. For example, in Llama2-7B, the transition occurs at the eighth layer set. For sets preceding the eighth, \u2206ARR remains close to zero, indicating that concealing any set up to the eighth layer set yields resilience comparable to the fully-closed approach. However, concealing sets beyond the eighth leads to decreased resilience, as indicated by increasing \u2206ARR values. In the smaller Phi-2 model, the transition happens earlier at the first layer set, with hiding later sets resulting in reduced resilience. These patterns suggest resilience transitions occur deeper in larger models. Similar trends of transitions in Mistral-7B and Phi-1.5 can be found in Appendix B.8.\nFor customizability, we conceal varying numbers of layers from the start and fine-tune the open set. We observe the customizability transition in the 7B model. Specifically in Llama2-7B, concealing only the first layer set improves performance by 43%, but this gain drops to 7% when concealing up to the third layer set and eventually fluctuates around 0%. However, we barely observe transitions in the smaller model, with improvements limited to 3%. This suggests that fewer closed-sourced layers enhance customizability, and larger models possess greater customizability than smaller ones.\nResilience vs. Closed-source Size. We explore how closed-sourced set size impacts the model resilience in Llama2-7B. We start from the smallest self-attention module (k_projection) in a decoder layer and extend to the entire model. Results are shown in Figure 6 and detailed in Appendix C.8.\nIn the general domain (Figure 6(b)), we observe that resilience emerges when a sufficient number of parameters are closed-sourced. Specifically in Llama2-7B, we observe that when the proportion of closed-sourced increases to 3%, resilience emerges. Furthermore, we observe similar patterns in specific domains, yet resilience transitions vary across capabilities as shown in Figure 6(c). For instance, technical skills such as Math show earlier transitions, with resilience emerging at 1% parameters closed-sourced, whereas domains such as Commonsense Reasoning require hiding 3%. In summary, closed-sourcing a small portion of parameters can provide sufficient resilience against model recovery, meanwhile, technical capabilities tend to be more challenging to recover than other domains. Other models are analyzed in Appendix B.9."}, {"title": "5.4 DISCUSSIONS", "content": "Effectiveness of RD on large models. We assess the efficacy of the recovery difficulty (RD) in estimating the performance of the recovered model. Specifically, we calculate the Pearson and Spearman correlation coefficients between RD and ARR across different capability groups. As shown"}, {"title": "6 CONCLUSION", "content": "In this paper, we explored finding minimal closed-sourced sets to enhance LLM customizability while preserving their resilience against semi-open model recovery attacks. We theoretically prove that minor errors in bottom decoder layers prior to a transition layer greatly reduce recovery attack success. We introduced SCARA, which selectively closed-sources a small set of layers, achieving superior customizability and comparable resilience to SAP-DP and fully-closed. We empirically investigated the existence of customization and resilience transitions, showed the impact of closed-source size on model resilience, analyzed the effectiveness of our approach, and finally discussed its limitations."}, {"title": "A PROOF OF THEOREM 1", "content": "In this section, we prove Theorem 1. We first revisit the our model, present several important lemmas and finally present the proof."}, {"title": "A.1 MODEL OVERVIEW", "content": "The recovered model $f(X; \\theta)$ is structured as a sequence of L transformer layers,\n$f(X) = \\varphi_L \\circ \\varphi_{L-1} \\circ ... \\circ \\varphi_{\\alpha_L+1} \\circ \\hat{\\varphi}_{\\alpha_L} \\circ \\varphi_{{\\alpha_L}-1} \\circ ... \\circ \\varphi_1(X)$,\nwhere $X \\in \\mathbb{R}^{n\\times d}$ represents the input, interpreted as an assembly of n tokens, each possessing d hidden dimensions. Each transformer layer, indexed by $1 < i < L$, is represented by $\\varphi_i$, which maps $\\mathbb{R}^{n\\times d}$ to $\\mathbb{R}^{n\\times d}$ and can be defined as follows,\n$\\varphi_i (X; K_i, Q_i) = I_n + \\text{softmax}\\left(\\frac{XQ_i(XK_i)^T}{\\sqrt{d_Q}||X||^2}\\right)X$,\nwhere $Q_i \\in \\mathbb{R}^{d \\times d_Q}$, $K_i \\in \\mathbb{R}^{d \\times d_Q}$ represent projection parameter matrices. Here, the $\\alpha_L$-th layer is the recovered layer and the others are the public layers. For simplicity, we use the function $\\varphi_{\\alpha_L}$ to denote mapping of the recovered layer, i.e., $\\varphi_{\\alpha_L}(X) = \\varphi_{\\alpha_L}(X; \\tilde{K}_{{\\alpha_L}}, \\tilde{Q}_{{\\alpha_L}})$."}, {"title": "A.2 BOUNDS ON DIFFERENT ORTHOGONAL COMPONENTS", "content": "Lemma 1. For any $1 < l < L$, $1 < p < d$, any $X \\in \\mathbb{R}^{n\\times d}$, we have\n$\\max_{\\mathbf{v}: ||\\mathbf{v}||_2=1, \\mathbf{v}\\perp \\mathbf{I}_n} |\\mathbf{v}^T\\varphi_l (X; K_l, Q_l) [p]| \\leq (1 + \\beta_D) \\max_{\\mathbf{v}: ||\\mathbf{v}||_2=1, \\mathbf{v}\\perp \\mathbf{I}_n} |\\mathbf{v}^TX[p]|$,\nwhere $\\mathbf{I}_n$ is a column vector with dimensions n \u00d7 1 and each element is 1, X[p] is the p-th column of the input X, $\\varphi_l (X; K_l, Q_l) [p]$ is the p-th column of the l-th self-attention output, the coefficient $B_D$ satisfies $0 < B_D < 1$ and it is related to the upper bound of the L2-norm of matrices $K_l$, $Q_l$.\nProof. Let $u = \\{\\mathbf{u}_{l,1} = \\frac{\\mathbf{I}_n}{n}, \\mathbf{u}_{l,2},..., \\mathbf{u}_{l,n}\\}$ denote the eigenvectors of $\\text{softmax}\\left(\\frac{XQ_l(XK_l)^T}{\\sqrt{d_Q}||X||^2}\\right)$. Assume $\\sigma_{l,1}, \\sigma_{l,2}, ..., \\sigma_{l,n}$ denote the eigenvalues of $\\text{softmax}\\left(\\frac{XQ_l(XK_l)^T}{\\sqrt{d_Q}||X||^2}\\right)$ and $-1 < \\sigma_{l,n} < B_D$ for any l, n. Thus we have\n$\\mathbf{v}^T \\varphi_l (X; K_l, Q_l) [p] = \\mathbf{v}^T \\left[ \\mathbf{I}_n + \\text{softmax}\\left(\\frac{XQ_l(XK_l)^T}{\\sqrt{d_Q}||X||^2}\\right) \\right] X[p]$\n$= \\mathbf{v}^T \\left[ \\mathbf{I}_n + \\text{softmax}\\left(\\frac{XQ_l(XK_l)^T}{\\sqrt{d_Q}||X||^2}\\right) \\right] \\sum_{k=1}^n \\alpha_{pk} \\mathbf{u}_{l,k}$\n$= \\mathbf{v}^T \\sum_{k=1}^n \\alpha_{pk} (1 + \\sigma_{l,k}) \\mathbf{u}_{l,k}$\n$\\leq \\max_{\\mathbf{v}:|| \\mathbf{v} ||_2=1,\\mathbf{v}\\perp \\mathbf{I}_n}  \\sum_{k=2}^n \\alpha_{pk} (1 + \\sigma_{l,k})\\mathbf{u}_{l,k}$\n$= \\sum_{k=2}^n \\alpha_{pk} (1 + \\sigma_{l,k})\\mathbf{u}_{l,k}$\n$= \\sqrt{\\sum_{k=2}^n \\alpha_{pk} (1 + \\sigma_{l,k})^2}$\n$\\leq (1 + B_D) \\max_{\\mathbf{v}:|| \\mathbf{v} ||_2=1,\\mathbf{v}\\perp \\mathbf{I}_n} |\\mathbf{v}^TX[p]|$,"}, {"title": "A.3 PROOF OF THEOREM 1", "content": "We first prove the following result. For simplicity of notations, we use $f(X) [p]$ to denote the p-th (1 \u2264 p < d) column of the the recovered model f(X), where the parameters in the $\\alpha_L$-th layer is replaced with the matrices $K_{{\\alpha_L}}$ and $Q_{{\\alpha_L}}$. We use the function $\\phi_l (X) = \\varphi_{\\alpha_L}(X; \\tilde{K}_{{\\alpha_L}}, \\tilde{Q}_{{\\alpha_L}})$ to"}]}