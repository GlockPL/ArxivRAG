{"title": "SIGNAL COLLAPSE IN ONE-SHOT PRUNING: WHEN SPARSE\nMODELS FAIL TO DISTINGUISH NEURAL REPRESENTATIONS", "authors": ["Dhananjay Saikumar", "Blesson Varghese"], "abstract": "Neural network pruning is essential for reducing model complexity to enable deployment on\nresource-constrained hardware. While performance loss of pruned networks is often attributed to the\nremoval of critical parameters, we identify signal collapse\u2014a reduction in activation variance across\nlayers as the root cause. Existing one-shot pruning methods focus on weight selection strategies\nand rely on computationally expensive second-order approximations. In contrast, we demonstrate\nthat mitigating signal collapse, rather than optimizing weight selection, is key to improving accuracy\nof pruned networks. We propose REFLOW that addresses signal collapse without updating train-\nable weights, revealing high-quality sparse sub-networks within the original parameter space. RE-\nFLOW enables magnitude pruning to achieve state-of-the-art performance, restoring ResNeXt-101\naccuracy from under 4.1% to 78.9% on ImageNet with only 20% of the weights retained, surpassing\nstate-of-the-art approaches.", "sections": [{"title": "Introduction", "content": "Neural networks are widely used across applications like natural language processing [32] and computer vision [18,\n14]. However, their increasing size, often reaching billions of parameters [36, 30], presents significant computational\nand memory challenges, making deployment in resource-constrained environments impractical [26]. Network pruning\nhas emerged as a key technique to reduce model complexity and enable faster inference [34, 16, 21, 33, 31].\nThe increase in neural network sizes [36] and the widespread availability of pre-trained models have shifted the fo-\ncus of pruning strategies. Gradual pruning, which iteratively removes parameters with retraining [38, 19, 8, 4], is\ncomputationally expensive for large models, often requiring days or weeks of fine-tuning [1]. This becomes infea-\nsible for modern networks, particularly in settings with limited training data, such as privacy-sensitive or low-data\nscenarios [28]. As a result, one-shot pruning, which compresses pre-trained models in a single step [28, 37, 1, 29, 6],\nhas emerged as a scalable and efficient alternative, directly addressing the runtime and computational cost of gradual\npruning for large-scale models.\nPruning methods fall into two categories: magnitude pruning (MP) and impact-based pruning (IP). MP [11, 24, 10,\n9] removes small-magnitude weights, but may not be effective as magnitude alone may not reliably capture parameter\nimportance [1]. IP methods, such as Optimal Brain Damage [20], Optimal Brain Surgeon [13], and their modern ex-\ntensions, use loss-aware weight selection via second-order approximations to identify and remove low-impact weights,\nfollowed by weight updates to compensate for loss. Despite outperforming MP, these methods are computationally\nexpensive as they rely on second-order approximations to select which weights to prune.\nThis raises an important question: What drives the higher pruning quality produced by IP methods compared to MP?\nIt is natural to attribute the success of IP methods to loss-aware weight selection, which identifies and preserves\ncritical parameters for the network's performance [20]. To test this assumption, we systematically decouple the two"}, {"title": "Background & Related Work", "content": "This section provides the mathematical formulation of pruning and reviews existing work on pruning techniques."}, {"title": "Problem Setup", "content": "Consider a pre-trained deep neural network (DNN) f(\u03b8; x) parameterized by \u03b8 \u2208 Rd and input x. Pruning produces\na sparse sub-network f(\u03b8 \u2299 m; x), where m \u2208 {0, 1}d is a binary mask, and \u2299 denotes element-wise multiplication.\nSparsity \u03ba \u2208 [0, 1] is the proportion of parameters set to zero. Pruning assigns scores z \u2208 Rd to parameters importance,\nusing methods ranging from simple weight magnitude to loss-aware based pruning scores."}, {"title": "Related Work", "content": "Magnitude-Based Pruning (MP) is a simple and widely used pruning strategy [10, 5, 24, 22, 31, 27, 9, 11, 23, 3].\nMP ranks weights based on their absolute values:\n\\(z_i = |\u03b8_i|.\\)\nIt prunes parameters with the smallest magnitudes, which is computationally efficient. However, MP does not account\nfor the impact of pruning on the loss function, which can result in suboptimal pruning decisions.\nImpact-Based Pruning (IP) explicitly considers the loss function to guide pruning decisions [20, 12, 28]. The impact\nof pruning is quantified as a second-order Taylor expansion of the loss function L centered at the pre-trained weights\n\u03b8:\n\\(L(\u03b8 + \u03b4\u03b8) \u2212 L(\u03b8) = \u03b4\u03b8\u1d40\u2207L(\u03b8) + \\frac{1}{2}\u03b4\u03b8\u1d40H\u03b4\u03b8 + O(||\u03b4\u03b8||\u00b3),\\)\nwhere H = \u2207\u00b2L(\u03b8) is the Hessian.\nAssuming \u03b8 represents a local minimum of the loss (as is often the case for pre-trained networks), the gradient term\n\u2207L(\u03b8) = 0. For small perturbations \u03b4\u03b8, the higher-order terms become negligible, leading to the local quadratic\napproximation:\n\\(L(\u03b8 + \u03b4\u03b8) \u2212 L(\u03b8) \u2248 \\frac{1}{2}\u03b4\u03b8\u1d40H\u03b4\u03b8.\\)\nBelow we review key IP methods that build on this quadratic approximation.\nOptimal Brain Damage (OBD) improves on MP by explicitly estimating the increase in loss due to pruning [20].\nAssuming the Hessian H is diagonal, the pruning score for a weight \u03b8\u2081 is:\n\\(z_i = \\frac{\u03b8_i\u00b2}{2H_{ii}}.\\)\nOBD ranks weights based on their impact on loss, using a diagonal Hessian approximation, but ignores parameter\ninteractions.\nOptimal Brain Surgeon (OBS) generalizes OBD by considering the full Hessian to capture cross-parameter interac-\ntions [13]:\n\\(z_i = \\frac{\u03b4\u03b8_i^*}{2[H^{-1}]_{ii}}, \\delta \u03b8_i^* = -\\frac{\u03b8_i[H^{-1}]_{ei}}{[H^{-1}]_{ii}}.\\)\nHere, zi represents the pruning score, and \u03b4\u03b8* defines the Hessian-based weight updated applied to the unpruned\nweights. OBS is computationally expensive for modern networks due to the cost of inverting the Hessian H; nonethe-\nless, it outperforms MP and OBD.\nModern Hessian-Based Methods: To reduce the computational cost of OBS, WoodFisher [28] introduces block-\ndiagonal approximations of the Hessian via the empirical Fisher information matrix derived from a subset of training\ndata:\n\\(H \u2248 \\frac{1}{n}\\sum_{i=1}^n\u2207l_i(\u03b8)\u2207l_i(\u03b8)^T,\\)\nwhere li(\u03b8) is the loss for a single data point. This approximation reduces computational overhead but still focuses on\npruning individual weights, without explicitly accounting for interactions between multiple weights.\nPruning Multiple Weights: Combinatorial Brain Surgeon (CBS) [37] considers the joint effect of pruning multiple\nweights simultaneously, outperforming WoodFisher. However, its reliance on a dense Hessian H \u2208 RP\u00d7P makes it\ncomputationally intensive, taking hours to prune MobileNet and is not scalable for large networks, such as ResNet-50.\nCHITA [1] uses memory-efficient quadratic approximations for faster pruning than CBS but still relies on Hessian-\nbased updates, modifying unpruned weights rather than identifying existing sparse sub-networks in the original param-\neter space."}, {"title": "Reassessing Impact-based Pruning", "content": ""}, {"title": "Revisiting Weight Selection", "content": "As discussed in Section 2, MP selects weights based on their absolute magnitudes, while IP's weight selection lever-\nages second-order approximations of the loss (see Equation 5), followed by Hessian-based weight updates. To evalu-\nate the role of weight selection in pruning, we compare MP with variants of IP methods, such as WF-S, CBS-S, and\nCHITA-S (referred to as IP-selection), which only prune weights (no weight updates). For additional context, we\ninclude random pruning and MP as naive baselines.\nFigure 2 shows that IP-selection (WF-S, CBS-S, CHITA-S) offers only marginal improvements (up to 2%) over MP,\nwhile random pruning severely reduces accuracy. This indicates that both MP and IP-selection identify meaningful\nparameters, unlike random pruning. However, the negligible difference between MP and IP-selection underscores the\nlimited role of weight selection in pruning performance."}, {"title": "Role of Hessian-Based Weight Updates", "content": "While weight selection has negligible impact on accuracy, Hessian-based updates are critical for recovering accuracy\nby adjusting the remaining weights to compensate for accuracy loss due to pruning.\nIP methods combine weight selection with Hessian-based updates (WF-U, CBS-U, CHITA-U). To evaluate the role of\nupdates, we apply Hessian-based updates to MP and the resultant is denoted as MP-U. MP-U tests whether the benefits\nof Hessian-based updates generalize to MP's simpler selection strategy. As shown in Figure 3, MP-U achieves accu-\nracy gains comparable to WF-U, CBS-U, and CHITA-U. This demonstrates that Hessian-based updates, not weight\nselection, is the primary driver of accuracy recovery. Combining Hessian-based updates with MP achieves perfor-\nmance on par with state-of-the-art pruning methods, eliminating the need for computationally expensive IP-selection\nstrategies."}, {"title": "Understanding Signal Collapse and Restoring Performance Loss with REFLOW", "content": "We examine the performance loss of pruned networks by introducing signal collapse - a phenomenon we observe for\nthe first time, where one-shot pruning progressively reduces activation variance across layers, ultimately impairing\nthe network's ability to distinguish between inputs. In this section, we formally define signal collapse, explain its\nmechanisms, and demonstrate its impact on network performance. Finally, we introduce REFLOW, a method to\nmitigate signal collapse and restore the performance of one-shot pruned networks."}, {"title": "Notation and Setup", "content": "Consider a pre-trained neural network f(\u03b8), parameterized by \u03b8 \u2208 Rd. For a given layer l \u2208 {1, . . ., L}, let the input\nto layer l be denoted as Hl\u22121. The pre-BatchNorm (pre-BN) activation at layer l is defined as:\n\\(X_l = f(H_{l-1}; \u03b8_l),\\)\nwhere \u03b8l represents the parameters of layer l.\nBatch Normalization (BN) normalizes the pre-BN activation Xl across the batch as follows:\n\\(Z_l^{(n)} = \\frac{X_l^{(n)} - \u00b5_l}{\\sqrt{Var^{(Orig)}(X_l) + \u03b5}} \u00b7 \u03b3_l + \u03b2_l,\\)\nwhere n is the index of the batch dimension, \u03bcl and Var(Orig)(Xl) are the running mean and variance of the BN layer,\n\u03b3l and \u03b2l are fixed affine parameters, and \u03f5 > 0 is a small constant for numerical stability.\nDefining Signal Collapse. Signal collapse occurs in a pruned network if the variance of the activations reduces\nsignificantly in deeper layers compared to the original, unpruned network. Formally, let Var(Pruned) and Var(Orig)\ndenote the variances of activations at layer l in the pruned and original networks, respectively. Signal collapse occurs\nif:\n\\(lim_{\u03ba\u2192L} \\frac{Var_l^{(Pruned)}}{Var_l^{(Orig)}} \u2192 0,\\)\nwhere L is the total number of layers.\nWhen the variance ratio approaches zero in deeper layers, the activations become nearly constant, resulting in a loss\nof distinction between inputs, causing uniform predictions."}, {"title": "Why Pruning Causes Signal Collapse", "content": "Signal collapse arises due to two reasons explored below:\nA) Activation variance reduces due to weight pruning\nPruning zeroes out weights based on a selection criterion, typically removing those with lower scores under the as-\nsumption that they contribute less to the network's performance. Formally, for layer l, pruning modifies the weights\n\u03b8l to \u03b8\u2032l, where weights are set to zero if their score zi falls below a threshold \u03c4:\n\\(W_{l,i}' = \\begin{cases} W_{l,i}, & \\text{if } z_i > \u03c4 \\\\ 0, & \\text{otherwise} \\end{cases},\\)\nwhere zi represents the pruning score (e.g., magnitude, impact (loss) based heuristic, as discussed in Section 2) for\nweight Wl,i, and \u03c4 is the pruning threshold determined by the desired sparsity level \u03ba.\nTo calculate the variance of the pre-BN activation after pruning, consider the activation in its pruned state:\n\\(X_l' = \\sum_{i \u2208 S} W_{l,i}' H_{l-1,i},\\)"}, {"title": "Cumulative Reduction in Activation Variance and Signal Collapse Across Layers", "content": "Signal collapse arises because of the reduction in activation variance which cumulatively compounds across deeper\nlayers in the network. In this subsection, we analyze how this progressive decline emerges when considering post-BN\noutputs.\nScaling Factor \u03b7l. For each layer l, define the variance scaling factor \u03b7l as:\n\\(\u03b7_l = \\frac{Var_l^{(Pruned)}(Z_l)}{Var_l^{(Orig)}(Z_l)} < 1,\\)\nwhere \u03ba \u2208 [0, 1] is the fraction of weights pruned. As \u03ba \u2192 1, more weights are removed, which generally makes \u03b7l\nsmaller via Equations 12 and 18."}, {"title": "REFLOW: Restoring Signal Propagation to Mitigate Collapse", "content": "Signal collapse due to pruning stems from diminished activation variance. Hessian-based IP methods only partially\nmitigate signal collapse by updating the unpruned weights. These observations point to a more direct solution: if the\ncore issue is the compounding mismatch between the pruned and original activation variances \\frac{Var^{(Pruned)}}{Var^{(Orig)}} < 1, resulting\nin signal collapse (Equation 9), then the running BN statistics can be calibrated to induce activation variance to mitigate\nsignal collapse, without updating any unpruned (trainable) weights."}, {"title": "Experimental Results", "content": "We apply REFLOW to magnitude pruning (MP) and evaluate it across small, medium, and large architectures. The re-\nsults highlight REFLOW's consistently recovers performance in pruned networks, achieving state-of-the-art accuracy\nwithout requiring computationally expensive Hessian-based updates. By mitigating signal collapse, REFLOW enables\nthe discovery of high-quality sparse subnetworks within the original parameter space."}, {"title": "Performance on Small Architectures", "content": "We begin by evaluating REFLOW on small architectures, namely ResNet-20 [14] pre-trained on CIFAR-10 [17] and\nMobileNet [15] pre-trained on ImageNet [2], with less than 5 million parameters and comparing them to state-of-the-\nart one-shot pruning approaches, namely WF [28], CBS [37], and CHITA [1]."}, {"title": "Scaling REFLOW to Medium-sized Architectures", "content": "We next evaluate REFLOW on medium-sized architectures, namely ResNet-50 (ImageNet) with less than 25 million\nparameters. For this size, we compare REFLOW to CHITA and M-FAC [7], as WF and CBS are computationally\nprohibitive."}, {"title": "Scaling REFLOW to Large Architectures", "content": "Finally, we consider large architectures, namely ResNet-101, ResNet-152, RegNetX-32GF (with nearly 107 million\nparameters), and ResNeXt-101 (64x4d) that has over 45 million parameters. These models pose significant challenges\nfor pruning, particularly for CHITA as it relies on memory- and computation-intensive second-order approximations."}, {"title": "Convergence with REFLOW", "content": "Building on the results in Table 1, we evaluate the impact of REFLOW across pruning methods with varying com-\nplexities: MP, CHITA-S (selection-only), and CHITA (selection with Hessian-based updates). CHITA updates the\nunpruned weights using second-order information, while CHITA-S applies the same selection criteria without weight\nupdates. This distinction isolates the role of weight updates and quantifies whether REFLOW can compensate for their\nabsence."}, {"title": "Conclusion", "content": "This work identifies signal collapse as a critical bottleneck in one-shot neural network pruning. Performance loss in\npruned networks is due to signal collapse in addition to the removal of critical parameters. We propose REFLOW\n(Restoring Flow of Low-variance signals), a simple yet effective method that mitigates signal collapse without com-\nputationally expensive weight updates. By focusing on signal preservation, REFLOW highlights the importance of\nmitigating signal collapse in sparse networks and enables magnitude pruning to match or surpass state-of-the-art one-\nshot pruning methods such as CHITA, CBS, and WF.\nREFLOW consistently achieves state-of-the-art accuracy across diverse architectures, restoring ResNeXt-101 from un-\nder 4.1% to 78.9% top-1 accuracy at 80% sparsity on ImageNet. Its lightweight design makes it a practical solution for\nboth research and deployment, delivering high-quality sparse models without the overhead of traditional approaches.\nThese findings challenge the traditional emphasis on weight selection strategies and underscore the critical role of\nsignal propagation for achieving high-quality sparse networks in the context of one-shot pruning."}, {"title": "Experimental Setup", "content": "This section provides a detailed overview of the experimental setup used in our study, including the pruning techniques,\ndatasets, sparsity ranges, and computational environment.\nWe employed a range of established one-shot pruning techniques, which perform pruning in a single step, followed\nby Hessian-based updates of the remaining weights and reduce the impact on loss after pruning. Specifically, we con-\nsidered WoodFisher [28], CBS [37], CHITA [1], and Matrix-Free Approximate Curvature (M-FAC) [7]. Performance\nmetrics for these methods were sourced from existing literature [37, 1], with results averaged over three independent\nruns.\nApplication of REFLOW: In this work, REFLOW is applied to networks pruned using magnitude pruning. After pruning,\nBatch Normalization (BN) running statistics are recalibrated using a forward pass over a limited number of training\nsamples.\nHyperparameters: For REFLOW, we used 50 training batches to recalibrate the running BN statistics, with a batch\nsize of 128 across all experiments.\nPre-Trained Networks and Datasets: To ensure comparability with prior studies [37, 1], we adopted datasets and\nmodel architectures from the same studies. The analysis included three pre-trained networks: ResNet-20 [14] trained\non the CIFAR-10 dataset [17], and MobileNet [15] and ResNet-50 [14] trained on the ImageNet dataset [2].\nWe extended the analysis to include larger architectures that prior leading one-shot pruning methods [28, 37] did not\nexplore and are unable to scale to efficiently. Specifically, we evaluated REFLOW on ResNet-101 [14], ResNet-152 [14],\nRegNetX [25], and ResNeXt-101 [35], all trained on the ImageNet dataset.\nSparsity Range: We evaluated REFLOW across the following sparsity ranges, consistent with prior works [37, 1]:\n\u2022 ResNet-20 on CIFAR-10: Sparsity range of 0.4 to 0.9.\n\u2022 MobileNet on ImageNet: Sparsity range of 0.4 to 0.8.\n\u2022 ResNet-50 on ImageNet: Sparsity range of 0.4 to 0.9.\nHardware: All experiments were conducted on a computational setup comprising an NVIDIA RTX A6000 GPU with\n48GB memory, 10,752 CUDA cores, and 336 Tensor cores capable of 309 TFLOPS peak performance, coupled with\nan AMD EPYC 7713P 64-Core CPU.\nSoftware: The computational environment operated on Ubuntu 20.04.6 LTS (Focal Fossa), utilizing Python version\n3.8.10 and PyTorch version 2.1.0."}, {"title": "Ablation Studies", "content": "In this section, we evaluate the performance of REFLOW through ablation studies. We analyze the impact of the number\nof training batches (N), layer-wise BN recalibration, and batch size on accuracy recovery in pruned networks."}, {"title": "Effect of the Number of Training Batches on Performance", "content": "We analyze the impact of varying the number of training batches (N) on the performance of REFLOW, focusing on\ntest accuracy. REFLOW is applied to sparse networks after magnitude pruning, recalibrating Batch Normalization (BN)\nstatistics through a forward pass over N training batches.\nFigure 11 shows the relationship between N and test accuracy for MobileNet at 80% sparsity. Accuracy improves\nsignificantly for small values of N, saturating around N = 50. Using N = 50 training batches with a batch size of\n128 corresponds to only 6,400 images\u2014less than 0.5% of the 1.28 million training samples in ImageNet.\nIn contrast, leading impact-based pruning methods such as WoodFisher [28] and CBS [37] require 960,000 training\nsamples for gradient computation, while CHITA [1] requires 16,000 samples. REFLOW achieves comparable perfor-\nmance using just 6,400 samples without any gradient computation, relying solely on forward passes to update BN\nstatistics. This minimal data requirement enables REFLOW to operate in scenarios where access to the full training\ndataset is limited, such as privacy-preserving applications or resource-constrained environments, where re-training is\ninfeasible."}, {"title": "Impact of Layer-wise Recovery on Performance", "content": "To gain deeper insights into the recovery of test accuracy in sparse networks, we analyzed the contribution of individual\nBatch Normalization (BN) layers by recalibrating them sequentially. Specifically, the recalibration was performed one\nlayer at a time, measuring the cumulative improvement in test accuracy after recalibrating each BN layer. This process\nwas conducted in two directions: from the first BN layer to the last (forward direction) and from the last BN layer to\nthe first (backward direction).\nFigure 12 presents the cumulative effect of BN recalibration on test accuracy for MobileNet at 80% sparsity after one-\nshot pruning. In the forward direction, recalibrating early BN layers contributes minimally to accuracy recovery, with\nnotable improvements only emerging as deeper layers are recalibrated. This pattern suggests that the shallower layers\nare less sensitive to changes in their BN statistics, whereas deeper layers play a more critical role in preserving network\nperformance. Conversely, in the backward direction, recalibrating late BN layers produces substantial accuracy gains\nearly on, with diminishing returns as earlier layers are recalibrated. These observations indicate that later layers are\ndisproportionately impacted by pruning-induced changes, reflecting their higher sensitivity.\nThis behavior aligns with the phenomenon of signal collapse, where the variance of activations diminishes significantly\nin deeper layers of the pruned network. As described in Equation 9, the variance ratio between pruned and original\nactivations approaches zero in the final layers, leading to near-constant activations. This results in indistinguishable\nrepresentations, which propagate to the output, causing uniform or incorrect predictions. The pronounced recovery\nobserved when recalibrating the last layers supports this theoretical insight: correcting the BN statistics in these layers\nmitigates signal collapse, restoring the discriminative power of the network's activations."}, {"title": "Effect of Batch Size on Performance", "content": "Here, we investigate the influence of varying batch sizes on the test accuracy of REFLOW for different target sparsity\nlevels (K) as shown in Figure 13.\nAt lower sparsity levels (k = 0.4 and k = 0.5), using smaller batch sizes for REFLOW results in a drop in accuracy below\nthe baseline performance of Magnitude Pruning (MP). This indicates that insufficient recalibration data can negatively\nimpact performance in less sparse networks. However, increasing the batch size leads to a noticeable improvement\nin accuracy, with REFLOW surpassing MP at moderate and large batch sizes. These results demonstrate that networks\nwith lower sparsity still benefit from recalibration when sufficient batch statistics are available."}, {"title": "Analyzing Pruning Similarity Using Hamming Distance", "content": "To further understand the limited role of weight selection, we analyze the Normalized Hamming Distance between\npruning masks produced by MP, CHITA, and random pruning. CHITA is used as the representative state-of-the-art\n(SOTA) IP method.\nThe Hamming Distance between two masks m(A) and m(B) is defined as:\n\\(H(m^{(A)}, m^{(B)}) = \\sum_{i=1}^d I(m_i^{(A)} \u2260 m_i^{(B)}),\\)\nwhere II() is the indicator function, d is the total number of parameters, and mi = 1 indicates that parameter i is\nretained. The Normalized Hamming Distance, which measures the fraction of differing pruning decisions between\ntwo masks, is defined as:\n\\(H_{norm}(m^{(A)}, m^{(B)}) = \\frac{H(m^{(A)}, m^{(B)})}{d}\\)\nwhere H(m(A), m(B)) is the Hamming Distance, and d is the total number of parameters.\nFigure 14 shows that the Normalized Hamming Distance between MP and CHITA is negligible, indicating close\nsimilarity in their pruning decisions compared to the significant variation with random pruning. For ResNet-20 on\nCIFAR-10, it is 0.0018%. For MobileNet on ImageNet, it is 0.0095%. These results show that magnitude-based and\nIP-selection methods make nearly identical pruning decisions, supporting the conclusion that the choice of weight\nselection (MP or IP-selection) has minimal influence on pruning performance."}]}