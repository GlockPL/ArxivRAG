{"title": "Artificial intelligence to improve clinical coding practice in\nScandinavia: a crossover randomized controlled trial", "authors": ["Taridzo Chomutare", "Therese Olsen Svenning", "Miguel \u00c1ngel Tejedor Hern\u00e1ndez", "Phuong Dinh Ngo", "Andrius Budrionis", "Kaisa Markljung", "Lill Irene Hind", "Torbj\u00f8rn\nTorsvik", "Karl \u00d8yvind Mikalsen", "Aleksandar Babic", "Hercules Dalianis"], "abstract": "Trial design Crossover randomized controlled trial. Methods An AI tool, Easy-ICD, was developed\nto assist clinical coders and was tested for improving both accuracy and time in a user study in Norway\nand Sweden. Participants were randomly assigned to two groups, and crossed over between coding com-\nplex (longer) texts versus simple (shorter) texts, while using our tool versus not using our tool. Results\nBased on Mann-Whitney U test, the median coding time difference for complex clinical text sequences\nwas 123 seconds (P<.001, 95% CI: 81 to 164), representing a 46% reduction in median coding time when\nour tool is used. There was no significant time difference for simpler text sequences. For coding accuracy,\nthe improvement we noted for both complex and simple texts was not significant. Conclusions This\nstudy demonstrates the potential of AI to transform common tasks in clinical workflows, with ostensible\npositive impacts on work efficiencies for complex clinical coding tasks. Further studies within hospital\nworkflows are required before these presumed impacts can be more clearly understood.\n\nKeywords: Large language models, AI, ICD-10, ICD-11, clinical coding, Easy-ICD, CAC-Computer\nAssisted Coding", "sections": [{"title": "1 Introduction", "content": "International Statistical Classification of Diseases and Related Health Problems codes, tenth revision (ICD-\n10) [1] play an important role in healthcare. All hospitals in Scandinavia record their activity by summarizing\npatient encounters into ICD-10 codes. Clinical coding directly affects how health institutions function on a\ndaily basis because they are partially reimbursed based on the codes they report. The same codes are used\nto measure both volume and quality of care, thereby providing an important foundation of knowledge for\ndecision makers at all levels in the healthcare service.\nClinical coding is a highly complex and challenging task that requires a deep understanding of both the\nmedical terminology and intricate clinical documentation. Coders must accurately translate detailed patient\nrecords into standardized codes, navigating the inherently complex medical language, which make this task\nprone to errors and inconsistencies. Continuously evolving, and progressively complex coding standards in"}, {"title": "1.1 Background", "content": "International Statistical Classification of Diseases and Related Health Problems codes, tenth revision (ICD-\n10) [1] play an important role in healthcare. All hospitals in Scandinavia record their activity by summarizing\npatient encounters into ICD-10 codes. Clinical coding directly affects how health institutions function on a\ndaily basis because they are partially reimbursed based on the codes they report. The same codes are used\nto measure both volume and quality of care, thereby providing an important foundation of knowledge for\ndecision makers at all levels in the healthcare service.\nClinical coding is a highly complex and challenging task that requires a deep understanding of both the\nmedical terminology and intricate clinical documentation. Coders must accurately translate detailed patient\nrecords into standardized codes, navigating the inherently complex medical language, which make this task\nprone to errors and inconsistencies. Continuously evolving, and progressively complex coding standards in"}, {"title": "1.2 Related work", "content": "Although research on CAC tools has a long history [15, 16], much of the significant progress is only very recent,\nand can be attributed to the recent advances in deep learning methods [17, 18]. These new methods are\nbetter adept at dealing with the challenges of the large label space (tens of thousands of codes), unbalanced\ndatasets and long text sequences, even though they also introduce new challenges such as explainability\n[19]. While deep learning is a key component in recent studies, ensembles combining multiple methods are\nquite common [20-22]. A review by Yan et al. [23] provides an overview of how methodologies evolved from\npurely rule-based methods to neural network-based methods. It further demonstrates how renewed interest\nin the problem spans different training datasets in not only major languages such as English and Chinese,\nbut also French, Italian, Hungarian, German and Spanish [20] [24]. Increased research activity suggests that\nCAC systems have the potential to streamline the coding process and reduce the burden on clinical coders,\nespecially when processing high volumes of patient records.\nHowever, the evidence regarding the overall utility of CAC systems is not conclusive. Some studies have\nreported improved accuracy when a CAC system is used but without any time savings, and others have\nalso reported time savings when a CAC system is used but without improving accuracy. For instance, Zhou\net al. [25] used regular expressions to implement a CAC system to enable automatic ICD-10 coding. The\ntool was developed and used during 16 months 2017-2018. 160,000 codes were automatically assigned and\ncompared with manual coding. The finding was that the automatic system was 100 times faster in ICD-10\ncoding compared to manual coding, while still maintaining good coding quality. The F\u2081-score of the system\nwas relatively low, around 0.6086.\nIn contrast, another study by Chen et al. [18] constructed an automatic coding tool using the BERT\n(Bidirectional Encoder Representations from Transformers) algorithm trained on patient records from one\nTaiwanese hospital. The discharge summaries contained 14,602 labels in total. The ICD-10 auto-coding tool\npredicted ICD-10 codes with the best F\u2081-score of between 0.715 and 0.618. The tool was used in a user\nstudy where the tool did not really decrease coding time but the coding quality increased significantly from\na median F\u2081-score of between 0.832 to 0.922.\nWhile heterogeneity in the different studies partially explains the inconclusive evidence, there are some"}, {"title": "1.3 Objective", "content": "This study seeks to provide evidence on the potential of AI to enhance clinical coding practices, thereby\nimproving operational efficiency and data quality in healthcare. We evaluate the effectiveness of the tool (see\nFig 2 for a screen dump of this tool), under two conditions: (i) when the clinical notes are complex versus\n(ii) when they are simple. We denote complexity by the number of tokens or words in each clinical note,\nrather than by complexity of the medical case. Word count is a simplified, yet also more objective measure\nthat is effective, since longer texts generally require more time to read than shorter texts. We aim to answer\nthe following research question: Does the Easy-ICD tool increase the speed and quality of ICD-10 coding\ncompared to traditional manual coding for both complex and simple Swedish discharge summaries?."}, {"title": "2 Methods", "content": "We followed the CONSORT-AI template [26] for reporting our study, which is a CONSORT extension for\nstudies reporting AI-related interventions."}, {"title": "2.1 Trial design", "content": "The study is designed as a 2x2 crossover randomised controlled trial, using the test of equivalence as our\nframework, and an allocation ratio of 1:1.\nThe study design is illustrated in Fig 1, where participants alternate between using Manual Coding and\nthe Easy-ICD tool to code complex and simple notes, with random assignment to different sequences to\ncontrol for period and order effects. We denote complexity based on the number of words in the clinical\nnote and not complexity of the medical case, where Period 1 contains 65% of the total words and Period\n2 contains only 35%. This design helps assess the impact of the Easy-ICD tool on coding performance, in\nterms of accuracy and speed, by controlling for the complexity of the coding task and balancing the order\nof interventions."}, {"title": "2.2 Participants", "content": "Participants were required to have prior experience in medical coding or relevant healthcare documentation\nto ensure familiarity with the coding tasks. The participants were nurses, coding experts, and physicians"}, {"title": "2.2.1 Eligibility criteria for participants", "content": "Participants were required to have prior experience in medical coding or relevant healthcare documentation\nto ensure familiarity with the coding tasks. The participants were nurses, coding experts, and physicians"}, {"title": "2.2.2 Settings and location where the data was located", "content": "The study was organised online, participants were recruited in Norway and Sweden. Each participant received\na web link to the study and was allowed to complete it using a device of his/her choice whenever it was\nconvenient."}, {"title": "2.3 Interventions", "content": "The coding tool, Easy-ICD, was designed as a web application using a typical Model View Controller (MVC)\nsoftware design pattern [27]. The model has two components; a BERT-based clinical LLM as described in\n[28], and a fuzzy logic component described in [21]. The view is composed of custom HTML5 templates,\nwhile the controller that connects the user interface is based on Flask, a Python framework, see figure 2, for\na screen dump of the DEMO.\n\nData used for the training of the LLM model behind Easy-ICD-tool is the Stockholm EPR Gastro ICD\nPseudo Corpus II\u00b9 fine tuned on the SweDeClin-BERT2 Swedish clinical language model [28]. A subset\nof 20 clinical notes from discharge summaries were re-coded by a senior Swedish coder for the purposes of\nquality-assuring the notes used in this study, that is, creating the gold standard.\nTo use our solution, the participants needed a good understanding of the coding process. This usually\nmeans they have had some training in coding. For a given chunk of text, our solution suggests 5-7 codes, from\nwhich the participants have to select one or more codes. Participants may also choose not to select any code\nif they feel the suggestions do not contain any appropriate code. The number of codes the solution suggests\n(5-7 codes) was based on our discussion with clinical coders during the requirements gathering phases of the\nproject. The solution's output was expected to help the coder identify correct codes more efficiently."}, {"title": "2.4 Outcomes", "content": "There were two primary outcome measures in this study; the time used in assigning codes, and the accuracy\nof the coding. Time was logged as the participants navigated through the study by clicking the \"Next\"\nbutton. In terms of accuracy, we measured how the participants performed, compared to the gold standard.\nOne important change to the trial outcome was that we originally planned to measure usability of our\nsolution as a secondary measure. We then considered that by the time participants reached the usability\nsurvey, they would likely be exhausted, which might have had an impact on the result. Since we had\nsome usability feedback during the co-design process with clinical coders, we decided to conduct a separate\nusability study at a later date. However, we used star-rating of usefulness as an indirect qualitative gauge\nof user satisfaction with the system."}, {"title": "2.5 Sample size", "content": "To calculate the sample size for the two-sided tests used in our study, particularly for the Mann-Whitney U\ntest (a non-parametric test), we needed to consider several factors, such as the desired power of the study\n(80%), medium effect size (0.5), significance level (0.05). The significance level ($\\alpha$) is the probability of a\nType I error; rejecting the null hypothesis when it is true, while the power (1 - $\\beta$) is the probability of\ncorrectly rejecting the null hypothesis when it is false (avoiding a Type II error). For the effect size, we used\nCohen's d for Mann-Whitney U test, which quantifies the difference between groups. Given that smaller\neffect sizes require larger samples, we used medium effect size.\nFor a rough sample size estimate using Cohen's effect size (d), the formula for the sample size per group\n(n) in a non-parametric context can be approximated using the following formula:\n$n = \\frac{Z_{\\alpha/2} + Z_{\\beta}}{d^2}$\nWhere:\n\u2022 $Z_{\\alpha/2}$ is the Z-score for the desired significance level (for $\\alpha$ = 0.05, Z = 1.96),\n\u2022 $Z_{\\beta}$ is the Z-score for the desired power (for 80%, Z = 0.84),\n\u2022 d is the estimated effect size (Cohen's d).\nGiven that we assume a medium effect size d = 0.5, power of 80%, and $\\alpha$ of 0.05, approximately 32\nobservations per group would be needed in each of Period 1 and Period 2. Recruitment strategies included\ncontacting hospitals and health authorities, announcing the study at coding seminars and conferences, and\nutilizing popular science media, social media, and other electronic platforms for advertising and outreach."}, {"title": "2.6 Randomization", "content": "The randomization sequence was generated using a alternate switching between 0 and 1 to ensure random-\nness and minimize allocation bias, and participants were assigned to Group 1 (sequence AB) and Group 2\n(sequence BA) in a 1:1 ratio. The randomization process was implemented as follows: after obtaining in-\nformed consent, the participant was randomly assigned the next number in the 0-1 toggle sequence to reveal\nthe participant's group assignment. The group assignment was then recorded in the study log. Participants\nwere not informed of their group allocation, but they were made aware that they would be allocated to either\ngroup. Due to the nature of the intervention, there was no allocation concealment."}, {"title": "2.7 Blinding", "content": "Blinding was not utilized in this study because the intervention involved a clearly distinguishable user\ninterface, making it impossible to mask from the participants and investigators. The nature of our solution\nrequired participant awareness for proper adherence. Additionally, the primary outcomes were objective\nmeasurements not subject to observer bias, further reducing the need for blinding. Thus, while blinding is a\nvaluable technique to minimize bias, its application was neither feasible nor necessary for the integrity and\nvalidity of our particular study."}, {"title": "2.8 Similarity of interventions", "content": "Participants interacted with two different user interfaces that were designed to be visually and functionally\nsimilar to ensure a consistent user experience across both intervention and control groups. Both interfaces\nfeatured the same layout, design elements, and core functionalities to minimize any potential confounding\nvariables related to usability or user satisfaction.\nThe primary distinction between the two interfaces was the inclusion of an artificial intelligence (AI)\nfeature in the intervention interface. This AI feature provided real-time suggestions and recommendations\nto the users based on the clinical text they are reviewing. Conversely, the control interface did not include this\nAI suggestion feature; users reviewed the clinical texts and assigned codes without receiving any automated\nsuggestions.\nMaintaining similar user interfaces ensured that any differences observed between the two groups could\nbe attributed to the presence or absence of the AI feature, rather than other extraneous factors related to\ninterface design or functionality."}, {"title": "2.9 Statistical methods", "content": "In this study, we opted not to perform commonly used statistical analyses in crossover studies, which typically\naccount for the dependency between periods. The primary reason for this decision is because we needed\nto consider the impact of clinical note complexity, and we assigned complex notes to Period 1 and simple\nnotes to Period 2. Therefore, we treated the two periods as independent since the levels of difficulty of the\nclinical notes varied significantly between the two periods. It would have been inappropriate to assume that\nperformance in one period could be directly compared to performance in the other, as this could introduce\nbias related to task difficulty rather than the intervention itself.\nTreating the periods independently enabled us to evaluate the effect of each intervention (Manual coding\nvs. Easy-ICD) under different task conditions (complex vs. simple notes) without conflating these factors.\nIf we assume independence between periods, we avoid the complexities introduced by possible carry over\neffects and ensure a clearer, unbiased assessment of each intervention within each period.\nWe used the Mann-Whitney U test to analyze coding time for each period separately. The Mann-Whitney\nU test was appropriate because our data had a non-normal distribution. Also, the test is robust to outliers,\nwhich allowed us to compare the performance between the two groups without assuming normality. This\napproach provided a more straightforward interpretation of the results, given the distinct nature of the\nperiods. Statistical analyses were performed using statistical software R and StatsDirect (version 4.0.4). A\nconfidence interval (CI) if 95% and p-value of less than 0.05 was considered statistically significant."}, {"title": "2.10 Data collection and management", "content": "All participant interactions with the user interfaces were logged to ensure comprehensive data collection.\nEvery action performed by the participants, such as clicks and selections where logged. To track the duration\nof tasks and response times, the exact times at which each action was performed was also logged. This\nalso includes all AI-generated suggestions and whether they were accepted, modified, or ignored by the\nparticipants. This logging was conducted in real-time and stored securely on the server, ensuring that no\ndata was lost and that it was immediately available for analysis.\nData management was handled with strict adherence to privacy and security protocols to protect partic-\nipant confidentiality and data integrity. All communication had end-to-end encryption using transport layer\nsecurity (TLS) [29], a commonly used standard for securing internet communications using web browsers.\nTo keep the data anonymous, personal identifiers were not collected. No Internet protocol (IP) addresses\nwere collected and cookies were only used to track progress in the experiment. This ensured that individual\nparticipants could not be directly identified from the dataset.\nEven though the raw data is anonymous, access to the data was restricted to authorized personnel only.\nA secure login system was used to manage access rights. Regular data integrity checks were performed to\nensure that the data was complete, consistent, and free from corruption. Regular backups of the data were\ntaken to prevent data loss. These backups were stored in separate, secure locations and could be restored in\ncase of data corruption or accidental deletion."}, {"title": "3 Results", "content": null}, {"title": "3.1 Participant flow", "content": "This study utilized a 1:1 crossover design involving 17 participants from an initial invitation pool of over a\n100 coders. Each participant interacted with both the intervention and control interfaces in two separate"}, {"title": "3.2 Losses and exclusions", "content": "During the data collection, one participant's data was deemed invalid due to incomplete data logging, and\nthe other participant only generated a random sequence but went no further."}, {"title": "3.3 Recruitment", "content": "Participants of the user study where recruited through acquaintances from our professional network in\nSweden and the Diagnosis-related Group (DRG) Forum, a network of professionals dedicated to enhancing\nthe practice of DRG classification in Norway. The participants where given a link to the user study along\nwith the instructions. They where also given a digital instruction of the study in a video tutorial right after\nthe consent form.\nRecruitment commenced November 2023 and continued on a rolling basis until May 2024. We had to stop\nthe recruitment because the project funds had run out and it was difficult recruiting more participants. This\nis because our target audience are practising professional coders, and our speculation was that uncertainty\nregarding privacy may have resulted in the relatively low response rate. We also have some anecdotal\nevidence of coders expressing concerns that new AI systems may take over their jobs, and this may have\ncaused some anxiety among potential participants."}, {"title": "3.4 Baseline data", "content": "Table 1 shows the baseline demographic and characteristics of the participants. There were seven Norwegian\nand eight Swedish coders. The majority (n=11) had more than 5 years coding experience, while only one\nhad less than one year's experience."}, {"title": "3.5 Numbers analysed", "content": "The total number of participants were 15, with Group 1 consisted of 9 participants and Group 2 consisted\nof 6 participants (see Fig.3)."}, {"title": "3.6 Outcomes and estimation", "content": null}, {"title": "3.6.1 Median coding time", "content": "In terms of the descriptive statistics, the control had a mean coding time of 242.6 sec. (192.3 SD) while\nthe intervention had 147 sec. (94.5 SD). Due to the presence of outliers and the non-parametric nature\nof the data distribution, we used the Mann-Whitney U test to analyze the data. The Mann-Whitney U\ntest is a non-parametric test that does not require the assumption of normality and is less sensitive to\noutliers and imbalanced sample sizes. Data distribution and outliers are illustrated in the box plot in\nFig 4. The test is specifically designed to assesses whether the median difference between the observations\nis significantly different from zero. This allowed us to effectively compare the performance metrics between\nthe two interventions for each participant."}, {"title": "3.6.2 Clinical coding accuracy", "content": "Table 3 shows the weighted average metrics for coding performance, where we see a general increase in\naccuracy when our tool was used compared to when it was not. However, testing these outcomes for\nsignificance based on two independent proportions, we discovered that the improvement in performance was\nnot statistically significant, as shown in Table 4."}, {"title": "3.6.3 User satisfaction", "content": "In terms of the user satisfaction with the provided suggestions, participants were slightly more satisfied with\nthe suggestion during the complex notes compared to suggestions for simpler notes, as illustrated in star\nrating in Fig. 5. However, it should be noted that it is only about a third (n=55/150, 36.7%) of the clinical\nnotes whose suggestions were rated."}, {"title": "3.7 Harms", "content": "No harms were recorded."}, {"title": "4 Discussion", "content": "Results emerging from our study show that the Easy-ICD tool was most useful for complex or longer clinical\nnotes, compared to simple or short clinical notes. These results are not surprising, since we initially hypoth-\nesised that such an AI assistive tool could be most useful for complex clinical notes, and that the varying\nreports in the literature could possibly be partially explained by this factor.\nOur study confirms that tools such as our Easy-ICD have the potential to contribute to decision making\nand to reducing the excessive documentation burden on healthcare staff. Through automating the discharge\nsummary coding process, such tools are expected to improve the speed and quality of the coding, thus\nfacilitating more efficient healthcare delivery. Use of AI can result in more accurate ICD-10 codes, reducing\nthe probability of errors and missing codes in clinical documentation. This automation not only saves\ntime, but it also ensures consistency in coding practices; leading to improved patient care and streamlined\nadministrative processes."}, {"title": "4.1 Limitations", "content": "One limitation of the trial could be selection bias, as participants probably had much higher confidence\nand affinity to testing new technology, and they were recruited from specific pool of healthcare settings in\nNorway and Sweden. There is also a risk of order effect or learning effect bias in a crossover design where\nthe sequence in which participants receive the intervention may influence their response or performance. For\nexample, participants may become more proficient and improve their coding skills due to repeated exposure\nto the task or the introduction of the intervention. This could lead to better outcomes in the later period of\nthe trial.\nStudy findings may have been influenced by factors such as coder experience; a sample size meant we\ncould not analyse the influence of experience on the results, and this could be an opportunity for further\nstudy.\nAnother limitation is that the Easy-ICD tool only predict chapter XI codes, the so called K-codes. This\nis a very limited scope of the overall ICD-10 coding system. Perhaps including other chapters would have\npresented more realistic, and possibly also more challenging, scenarios for coders. We leave this question as\na possible object of future inquiry."}, {"title": "4.2 Generalizability", "content": "The successful application of Easy-ICD to improve the efficiency of clinical coding in the study suggests that\nsimilar AI tools could be beneficial in healthcare systems in general. The findings of this study highlight\nthe potential of AI interventions to improve clinical coding practices, regardless of geographical location or\nhealthcare setting.\nAdditionally, the positive outcomes observed in this trial indicate that the benefits of AI in clinical coding\nare transferable and applicable to various healthcare settings. By demonstrating the effectiveness of Easy-\nICD in optimizing coding processes and improving data quality, this study provides valuable information on\nthe generalizability of AI interventions in healthcare.\nThis may also present opportunities for porting our tools to support ICD-11. Even though world gov-\nernments are gearing towards implementing this new version, healthcare staff are less accepting of this new\nversion. Assistive tools, such as the one presented in this study, may be the key to adoption and final\ntransition to ICD-11.\nHowever, we do acknowledge that the limitations we discussed have to be addressed, and more studies\nwith bigger sample sizes are required before we can fully understand generalizability in this context."}, {"title": "4.3 Interpretation", "content": "The reduction in median coding time was only statistically significant for complex notes, suggesting that\nsuch assistive tools are comparatively more appropriate for complex tasks. We also noted that accuracy\nimproved in both complex and simple notes, albeit not significantly, when our tool was used. This suggests\nthat even though participants were quick to pick out codes for simple texts, those that used our tool had\nbetter accuracy even for simple texts, and this is something that needs further investigation with a larger\nsample size.\nHowever, it should also be noted that two possible explanations for the non-significant results on accuracy\nare that the code space was comparatively smaller (K-codes) and that the participants were highly motivated\nindividuals with a high level of skill, and some of them were in leadership positions responsible for coding."}, {"title": "5 Conclusion", "content": "Our results highlight the potential of assistive tools, especially for more complex clinical coding tasks. This\nhas important practical implications for the use of AI in clinical coding as these findings demonstrate that\nassistive technology can be effective productivity tools that reduce the excessive burden of administrative\ndocumentation. This is particularly relevant in healthcare where manpower is limited and accurate task\ncompletion is critical. Overall, the study demonstrates the value of AI in augmenting human performance,\nproviding a compelling case for the broader adoption of AI-assisted interfaces to enhance productivity and\naccuracy in clinical coding. Globally, such AI tools have the potential to ease the adoption of more complex\nand detailed classification systems like ICD-11."}, {"title": "6 Other information", "content": null}, {"title": "6.1 Registration", "content": "https://clinicaltrials.gov/study/NCT0628686"}, {"title": "6.2 Protocol", "content": "JMIR Protocols [30]"}, {"title": "6.3 Funding", "content": "This research is funded by the ClinCode project through the Research Council of Norway, grant no. 318098.,\nawarded to HD. The funder had no role in study design, data collection and analysis, decision to publish, or\npreparation of the manuscript."}, {"title": "6.4 Data availability", "content": "The datasets generated during the current study are available in the GitHub repository at https://github.com/icd-\ncoding/clincode_demo. The 20 clinical texts used in this study are not publicly available, but may be made\navailable to qualified researchers on reasonable request from the corresponding author."}, {"title": "6.5 Code availability", "content": "A demonstration of Easy-ICD is located at https://easy-icd.ehealthresearch.no."}]}