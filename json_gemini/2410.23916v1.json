{"title": "Transformer-based Model Predictive Control: Trajectory Optimization via Sequence Modeling", "authors": ["Davide Celestini", "Daniele Gammelli", "Tommaso Guffanti", "Simone D'Amico", "Elisa Capello", "Marco Pavone"], "abstract": "Model predictive control (MPC) has established itself as the primary methodology for constrained control, enabling general-purpose robot autonomy in diverse real-world scenarios. However, for most problems of interest, MPC relies on the recursive solution of highly non-convex trajectory optimization problems, leading to high computational complexity and strong dependency on initialization. In this work, we present a unified framework to combine the main strengths of optimization-based and learning-based methods for MPC. Our approach entails embedding high-capacity, transformer-based neural network models within the optimization process for trajectory generation, whereby the transformer provides a near-optimal initial guess, or target plan, to a non-convex optimization problem. Our experiments, performed in simulation and the real world onboard a free flyer platform, demonstrate the capabilities of our framework to improve MPC convergence and runtime. Compared to purely optimization-based approaches, results show that our approach can improve trajectory generation performance by up to 75%, reduce the number of solver iterations by up to 45%, and improve overall MPC runtime by 7x without loss in performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Trajectory generation is crucial to achieving reliable robot autonomy, endowing autonomous systems with the capability to compute a state and control trajectory that simultaneously satisfies constraints and optimizes mission objectives. As a result, trajectory generation problems have been formulated in many practical areas, including space and aerial vehicles [2], [3], robot motion planning [4], chemical processes [5], and more. Crucially, the ability to solve the trajectory generation problem in real time is pivotal to safely operate within real-world scenarios, allowing the autonomous system to rapidly recompute an optimal plan based on the most recent information.\nMotivated by its widespread applications, a collection of highly effective solution strategies exist for the trajectory generation problem. For example, numerical optimization provides a systematic mathematical framework to specify mission objectives as costs or rewards and enforce state and control specifications via constraints [6]. However, for most problems of interest, the trajectory optimization problem is almost always nonconvex, leading to high computational complexity, strong dependency on initialization, and a lack of guarantees of either obtaining a solution or certifying that a solution does not exist [7]. The above limitations are further exacerbated in the case of model predictive control (MPC) formulations, where the trajectory generation problem needs to be solved repeatedly and in real-time as the mission evolves, enforcing strong requirements on computation time. Moreover, MPC formulations typically require significant manual trial-and-error in defining ad-hoc terminal constraints and cost terms to, e.g., achieve recursive feasibility or exhibit short-horizon behavior that aligns with the full trajectory generation problem.\nBeyond methods based on numerical optimization, recent advances in machine learning (ML) have motivated the application of learning-based methods to the trajectory generation problem [8]. ML approaches are typically highly computationally efficient and can be optimized for (potentially nonconvex) performance metrics from high-dimensional data (e.g., images). However, learning-based methods are often sensitive to distribution shifts in unpredictable ways, whereas optimization-based approaches are more readily characterized both in terms of robustness and out-of-distribution behavior. Additionally, ML methods often perform worse on these problems due to their high-dimensional action space, which increases variance in, e.g., policy-gradient algorithms [9], [10]. As a result, real-world deployment of learning-based methods has so far been limited within safety-critical applications.\nIn this work, we propose a framework to exploit the specific strengths of optimization-based and learning-based methods for trajectory generation, specifically tailored for MPC formulations (Fig. 1). By extending the framework introduced in [1], we propose a pre-train-plus-fine-tuning strategy to train a transformer to generate near-optimal state and control sequences and show how this allows to (i) warm-start the optimization with a near-optimal initial guess, leading to improved performance and faster convergence, and (ii) provide long-horizon guidance to short-horizon problems in MPC formulations, avoiding the need for expensive tuning of cost terms or constraints within the optimization process. Crucially, we show how the proposed fine-tuning scheme results in substantially improved robustness to distribution shifts caused by closed-loop execution and how the"}, {"title": "II. RELATED WORK", "content": "Our work is closely related to previous approaches that combine learning and optimization for control [11]\u2013[13], exploit high-capacity neural networks for control [14]\u2013[16], and methods for warm-starting nonlinear optimization problems [1], [17]\u2013[19], providing a way to train general-purpose trajectory generation models that guide the solution of inner optimization problems.\nNumerous strategies have been developed for learning to control via formulations that leverage optimization-based planning as an inner component. For instance, prior work focuses on developing hierarchical formulations, where a high-level (learning-based) module generates a waypoint-like representation for a low-level planner, e.g., based on sampling-based search [11], trajectory optimization [13], or model-based planning [12]. Within this context, the learning-based component is typically trained through either (online) reinforcement learning or imitation of oracle guidance algorithms. An alternate strategy consists of directly optimizing through an inner controller. A large body of work has focused on exploiting exact solutions to the gradient of (convex) optimization problems at fixed points [20], [21], allowing the inner optimization to be used as a generic component in a differentiable computation graph (e.g., a neural network). On the contrary, our approach targets non-convex problems and leverages the output of a learning-based module as input to the unmodified trajectory optimization problems. This simplifies non-convex trajectory planning, avoiding the potential misalignment caused by intermediate problem representations that do not necessarily correlate with the downstream task (e.g., fixed-point iteration in value function learning).\nOur method is closely related to recent work that leverages high-capacity generative models for control. For example, prior work has shown how transformers [14], [15] and diffusion models [16] trained via supervised learning on pre-collected trajectory data are amenable to both model-free feedback control [14] or (discrete) model-based planning [15]. These methods have two main drawbacks: (i) they typically ignore non-trivial state-dependent constraints, a setting that is both extremely common in practice and challenging for purely learning-based methods and (ii) they do not exploit all the information available to system designers, which typically includes approximate knowledge of the system dynamics. As in [1], our method alleviates both of these shortcomings by (i) leveraging online trajectory optimization to enforce non-trivial constraint satisfaction, and (ii) taking a model-based view to transformer-based trajectory generation and leveraging readily available approximations of the system dynamics to improve autoregressive generation.\nLastly, prior work has explored the idea of using ML to warm-start the solution of optimization problems. While the concept of warm-starting optimization solvers is appealing in principle, current approaches are typically (i) limited by the choice of representation for the output of the ML model (e.g., a fixed degree polynomial) [17], [18], (ii) restricted to the open-loop planning setting [1], whereby the impact of distribution shifts due to closed-loop execution [22] (e.g., model mismatch, stochasticity, etc.) is often overlooked, and (iii) confined to linear con-"}, {"title": "III. PROBLEM STATEMENT", "content": "Let us consider the time-discrete optimal control problem (OCP):\nminimize $\\mathcal{I} = \\sum_{i=1}^{N} J (x(t_i), u(t_i))$  (1a)\nx(ti), u(ti)\nsubject to $x(t_{i+1}) = f (x(t_i), u(t_i))$    (1b)\n$x(t_i) \\in \\mathcal{X}_{t_i}, u(t_i) \\in \\mathcal{U}_{t_i}$   $\\forall i \\in [1, N],$   (1c)\n$\\forall i \\in [1, N],$ where $x(t_i) \\in \\mathbb{R}^{n_x}$ and $u(t_i) \\in \\mathbb{R}^{n_u}$ are respectively the $n_x$-dimensional state and $n_u$-dimensional control vectors, $J : \\mathbb{R}^{n_x+n_u} \\rightarrow \\mathbb{R}$ defines the running cost, $f : \\mathbb{R}^{n_x+n_u} \\rightarrow \\mathbb{R}^{n_x}$ represents the system dynamics, $\\mathcal{X}_{t_i}$ and $\\mathcal{U}_{t_i}$ are generic state and control constraint sets, and $N \\in \\mathbb{N}$ defines the number of discrete time instants $t_i$ over the full OCP horizon T.\nIn particular, we consider a receding-horizon reformulation of Problem (1), whereby we recursively solve an OCP characterized by a moving (and typically shorter) horizon $H = [h, h + H] \\subseteq [1, N]$, with h being the moving initial timestep and $H \\in (0, N]$ the length of the horizon. To ensure desirable long-horizon behavior from the solution of a series of short-horizon problems, MPC formulations typically require the definition of a terminal cost $J_T$ or constraint set $\\mathcal{X}_{h+H}$. Specifically, we will consider cost functions of the form:\n$\\mathcal{J} = J_T (x(t_{h+H})) + \\sum_{i=h}^{h+H-1} J (x(t_i), u(t_i))$,  (2)\nwhere the running cost $J$ is evaluated exclusively over H and $J_T: \\mathbb{R}^{n_x} \\rightarrow \\mathbb{R}$ is the terminal cost function.\nIn this work, we explore approaches to address Problem (2) by incorporating high-capacity neural network models within the optimization process for trajectory generation, explicitly tailored for closed-loop performance."}, {"title": "IV. TRAJECTORY OPTIMIZATION VIA SEQUENCE MODELING", "content": "We consider a strategy for trajectory generation whereby state and control sequences $X = (x_1, ..., x_N)$, $U = (u_1, ..., u_N)$ are obtained through the composition of two components,\n$(X, \\hat{U}) \\sim p_{\\theta} (X, U| \\sigma_0)$  (3)\n$(X, U) = Opt (x(t_1), X, \\hat{U}),$  (4)\nwhere $p_{\\theta} (.)$ denotes the conditional probability distribution over trajectories (given an initial condition $\\sigma_0$) learned by a transformer model with parameters $\\theta$, $Opt$ denotes the trajectory optimization problem, and $X, \\hat{U}$ denote the predicted (state and control) trajectories that are taken as an input to the optimization problem. The initial condition $\\sigma_0$ is used to inform trajectory generation in the form of, e.g., an initial state $x(t_1)$, a target state $x(t_T)$ to be reached by the end of the trajectory, or other performance-related parameters. In this section, we will first dive into the details of the trajectory representation for sequence modeling. We will then discuss both open-loop pre-training and MPC fine-tuning formulations of our approach, together with specific inference algorithms for open-loop planning and MPC.\nAt the core of our approach is the treatment of trajectory data as a sequence to be modeled by a transformer model [1]. Specifically, given a pre-collected dataset of trajectories of the form $\\mathcal{T}_{raw} = (X_1, U_1, r_1, ..., X_N, U_N, r_N)$, where $x_i$ and $u_i$ denote the state and control at time $t_i$, and $r_i = -J(x(t_i), u(t_i))$ is the instantaneous reward, or negative cost, we define the following trajectory representation:\n$\\mathcal{T} = (\\mathcal{T}_1, R_1, C_1, X_1, U_1, ..., \\mathcal{T}_N, R_N, C_N, X_N, U_N)$,  (5)\nwhere $\\mathcal{T}_i \\in \\mathbb{R}^{n_x}$, $R_i \\in \\mathbb{R}$ and $C_i \\in \\mathbb{N}^+$ represent a set of performance parameters that allow for effective trajectory generation. In particular, we define $\\mathcal{T}_i$ as the target state, i.e., the state we wish to reach by the end of the trajectory, that could be either constant during the entire trajectory or time-dependent. We further define $R_i$ and $C_i$ as the reward-to-go and constraint-violation-budget evaluated at $t_i$, respectively. Formally, as in [1], we define $R_i$ and $C_i$ to express future optimality and feasibility of the trajectory as:\n$R(t_i) = \\sum_{j=i}^{N} r_j, C(t_i) = \\sum_{j=i}^{N} C_j, C_j = \\begin{cases} 1, & \\text{if } \\exists x_j, u_j \\notin \\mathcal{X}_j, \\mathcal{U}_j \\\\ 0, & \\text{otherwise}. \\end{cases}$  (6)\nThis definition of the performance parameters is appealing for two reasons: (i) during training, the performance parameters are easily derivable from raw trajectory data by applying (6) for $R_i$ and $C_i$, and by setting $\\mathcal{T}_i = x_T$, (ii) at inference, according to (3), it allows to condition the generation of predicted state and control trajectories $X$ and $\\hat{U}$ through user-defined initial conditions $\\sigma_0$. Namely, given user-defined $\\sigma_0 = (\\mathcal{T}_1, R_1, C_1, x_1)$, a successfully trained transformer should be able to generate a trajectory $(X, U)$ starting from $x(t_1)$ that achieves a reward of $R_1$, a constraint violation of $C_1$, and terminates in state $\\mathcal{T}_1$.\nIn what follows, we introduce the pre-training strategy used to obtain a transformer model capable of generating near-optimal trajectories $X = (x_1,...,x_N)$, $\\hat{U} = (\\hat{u}_1, ..., \\hat{u}_N)$ in an open-loop setting.\nDataset generation. The first step entails generating a dataset for effective transformer training. To do so, we generate $N_D$ trajectories by repeatedly solving Problem (1) with randomized initial conditions and target states, and then re-arranging the raw trajectories according to (5). We construct the datasets to include both solutions to Problem (1) as well as to its relaxations. We observe that diversity in the available trajectories is crucial to enable the transformer to learn the effect of the performance metrics; for example, solutions to relaxations of Problem (1) will typically yield trajectories with low cost and non-zero constraint violation (i.e., high R and C > 0), while direct solutions to Problem (1) will be characterized by higher cost and zero constraint violation (i.e., lower R and C = 0), allowing the transformer"}, {"title": "V. EXPERIMENTS", "content": "In this section, we demonstrate the performance of our framework on three trajectory optimization problems: two in simulation (i.e., spacecraft rendezvous and quadrotor control scenarios) and one real-world robotic platform (i.e., a free flyer testbed). We consider three instantiations of Problem (1) based on the following general formulation:\nminimize $\\sum_{i=1}^{N} ||u_i||$  (9a)\nXi, Ui\nsubject to $x_{i+1} = f (x_i, u_i)$  $\\forall i \\in [1, N],$ (9b)\n$\\Gamma (x_i) \\geq 0$  $\\forall i \\in [1, N],$ (9c)\n$x_1 = x_{start}, x_{N+1}= x_{goal}.$   (9d)\nwhere the objective function in (9a) expresses the minimization of the control effort (with p, q denoting application-dependent parameters), (9b) represents the (potentially nonlinear) approximate system dynamics, (9c) defines the obstacle avoidance constraints through a nonlinear distance function$\\Gamma : \\mathbb{R}^{m \\times n_x} \\rightarrow \\mathbb{R}^m$ with respect to $m \\in [1, M]$ non-convex keep-out-zones, and (9d) represents the initial and terminal state constraints.\nExperimental design. While the specific formulations of the trajectory optimization problem will necessarily depend on the individual application, we design our experimental setup to follow some common desiderata.\nFirst, we care to isolate the benefits of (i) the initial guess (in both open-loop and MPC settings) and (ii) learning the terminal cost for MPC; thus, we keep both the formulation and the solution algorithm for the OCP fixed and only evaluate the effect of different initializations. In particular, we resort to sequential convex programming (SCP) methods for the solution of the trajectory optimization problem and compare different approaches that provide an initial guess for the sequential optimizer (or, for MPC, also a specification of the terminal cost). Second, in the open-loop setting, we always compare our approach, identified as Transformers for Trajectory Optimization (TTO), with the following classes of methods: (i) a quantifiable cost Lower Bound (LB), characterized by the solution of a relaxation (REL) to the full-horizon Problem (9) which ignores obstacle avoidance constraints from (9c) and thus represents a (potentially infeasible) cost lower bound to the full problem and (ii) a state-of-the-art SCP approach where the initial guess to Problem (9) is provided by the solution to the REL relaxation described in (i) [17], [24]. Third, in the model predictive control setting, we always consider terminal cost terms of the form $J_T(x_{h+H}, \\bar{x}) = ||x_{h+H} - \\bar{x}||_P$, where $||.||_P$ denotes the weighted norm with respect to a diagonal matrix P and compare different choices for $\\bar{x}$ and P. Lastly, as introduced in Sec. IV-B, to obtain diverse trajectories for offline training we solve $N_d = 400,000$ instances of the trajectory optimization problem, whereby $N_d$ are solutions to the full non-convex Problem (9), and $N_d$ to its REL relaxation.\nIn our experiments, we care about three main performance metrics: cost, speed of optimization (expressed in terms of the number of SCP iterations), and overall runtime. As in [1], throughout our experiments, we assume knowledge of the fixed operational environment and perfect state estimation at all times. We further assume that the target state is reachable within N discrete time instants.\nOur model is a transformer architecture designed to account for continuous inputs and outputs.\nGiven an input sequence and a pre-defined maximum context length K, our model takes as input the last 5K sequence elements, one for each modality: target state, reward-to-go, constraint-violation-budget, state, and control. The sequence elements are projected through a modality-specific linear transformation, obtaining a sequence of 5K embeddings. As in [15], we further encode an embedding for each timestep in the sequence and add it to each element embedding. The resulting sequence of embeddings is processed by a causal GPT model consisting of six layers and six self-attention heads. Lastly, the GPT architecture autoregressively generates a sequence of latent embeddings which are projected through modality-specific decoders to the predicted states and controls.\nIn this section, we describe our experimental scenarios, the definition of cost, state, and control variables, and the specific implementation of constraints in (9b-9d).\nWe consider the task of performing an autonomous rendezvous, proximity operation, and docking (RPOD) transfer, in which a servicer spacecraft equipped with impulsive thrusters approaches and docks with a target spacecraft. The relative motion of the servicer with respect to the target is described using the quasi-nonsingular Relative Orbital Elements (ROE) formulation, i.e. $x := \\delta \\rho \\in \\mathbb{R}^6$, [1], [25]. Cartesian coordinates in the Radial Tangential Normal (RTN) frame can be obtained through a time-varying linear mapping [1] and employed for imposing geometric constraints.\nProblem (9) is detailed as follows: (i) in (9a), we select p = 2 and q = 1, to account for impulsive thrusters aligned with the desired control input; (ii) in (9d), $x_{start}$ is defined by a random passively-safe relative orbit with respect to the target, while $x_{goal}$ is selected among two possible docking ports located on the T axis, as shown in Fig. 2a; (iii) in (9b), $\\delta \\rho_{i+1} = \\Phi_i (\\Delta t) (\\delta \\rho_i + B_i u_i)$ is used to enforce dynamics constraints, with $\\Phi_i(\\Delta t) \\in \\mathbb{R}^{6 \\times 6}$ and $B_i \\in \\mathbb{R}^{6 \\times 3}$ being the linear time-varying state transition and control input matrices [1],$\\Delta t$ the time discretization and $u_i = \\Delta v_i \\in \\mathbb{R}^3$ the impulsive delta-velocity applied by the servicer; (iv) in (9c), we consider an elliptical keep-out-zone (KOZ) centered at the target, limiting the time index i to [1, $N_{wp}$]. We further consider two additional domain-specific constraints: (v) a zero relative velocity pre-docking waypoint to be reached at i = $N_{wp}$; (vi) a second-order cone constraint enforcing the service spacecraft to approach the docking port inside a cone with aperture angle $\\theta_{cone}$ for $N_{wp} \\leq$"}, {"title": "VI. CONCLUSIONS", "content": "Despite its potential, the application of learning-based methods for the purpose of trajectory optimization has so far been limited by the lack of safety guarantees and characterization of out-of-distribution behavior. At the same time, methods relying on numerical optimization typically lead to high computational complexity and strong dependency on initialization. In this work, we address these shortcomings by proposing a framework to leverage the flexibility of high-capacity neural network models while providing the safety guarantees and computational efficiency needed for real-world operations. We do so by defining a structure where a transformer model\u2014trained to generate near-optimal trajectories\u2014is used to provide an initial guess and a predicted target state as long-horizon guidance within MPC schemes. This decomposition, coupled with a pre-training-plus-fine-tuning learning strategy, results in substantially improved performance, runtime, and convergence rates for both open-loop planning and model predictive control formulations.\nIn future works, we plan to extend our formulation to handle"}]}