{"title": "Beyond Label Attention: Transparency in Language Models for Automated Medical Coding via Dictionary Learning", "authors": ["John Wu", "David Wu", "Jimeng Sun"], "abstract": "Medical coding, the translation of unstructured clinical text into standardized medical codes, is a crucial but time-consuming healthcare practice. Though large language models (LLM) could automate the coding process and improve the efficiency of such tasks, interpretability remains paramount for maintaining patient trust. Current efforts in interpretability of medical coding applications rely heavily on label attention mechanisms, which often leads to the highlighting of extraneous tokens irrelevant to the ICD code. To facilitate accurate interpretability in medical language models, this paper leverages dictionary learning that can efficiently extract sparsely activated representations from dense language model embeddings in superposition. Compared with common label attention mechanisms, our model goes beyond token-level representations by building an interpretable dictionary which enhances the mechanistic-based explanations for each ICD code prediction, even when the highlighted tokens are medically irrelevant. We show that dictionary features can steer model behavior, elucidate the hidden meanings of upwards of 90% of medically irrelevant tokens, and are human interpretable.", "sections": [{"title": "1 Introduction", "content": "Transparency is a vital factor in healthcare to gain patients' trust, especially when AI models make critical decisions in clinical practice (Rao et al., 2022). One of the essential applications of AI models is to assign International Classification of Diseases (ICD) codes automatically based on the clinical text (we name this task as medical coding). These ICD codes categorize patient diagnoses, conditions, and treatments for billing, reporting, and treatment purposes (Hirsch et al., 2016; Johnson et al., 2021). However, assigning ICD codes is complex and requires expertise and time (O'Malley et al., 2005). Recent advancements in medical pre-trained language models (PLMs) have made it possible to treat medical coding as a high-dimensional multilabel classification challenge (Edin et al., 2023; Huang et al., 2022). These AI models led to significant success in efficient ICD coding (Kaur et al., 2021). However, their transparency remains of great concern (Hakkoum et al., 2022). Therefore, developing automated interpretability methods is crucial to upholding transparency in medical coding processes.\nSignificant progress has occurred in the field of black-box interpretability, particularly concerning feature attribution, with the emergence of perturbation-based methods such as SHAP (Lundberg and Lee, 2017) and its approximate counterpart LIME (Ribeiro et al., 2016; Moraffah et al., 2020). These techniques, rooted in information and game theory, are recognized for assessing feature relevance in detail by intelligently perturbing and ablating input features (Lundberg and Lee, 2017; Ribeiro et al., 2016). While approximation methods have greatly improved the speed of calculating Shapley values, exact computations remain expensive (Lundberg et al., 2020; Chen et al., 2022; Shrikumar et al., 2019; Mosca et al., 2022). The huge computational cost makes their application impractical towards automated medical ICD coding since clinical notes usually contain thousands of high dimensional token embeddings in a vast multilabel prediction space (Johnson et al., 2023).\nAs such, we seek a human-interpretable approach that scales efficiently with large datasets, highlights essential features, and offers more comprehensive explanations of PLM predictions. Recent advancements in mechanistic interpretability methods (Cunningham et al., 2023; R\u00e4uker et al., 2023) have demonstrated the potential to surpass the computational challenges posed by traditional black-box approaches by elucidating the roles of specific neuron subsets within a network. This level of mechanistic understanding is precious in the medical field, where explaining the significance"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Automated Interpretability in ICD Coding", "content": "Alternative automated ICD coding methods, like phrase matching (Cao et al., 2020) and relevant phrase extraction using manually curated knowledge bases (Duque et al., 2021), offer inherent interpretability but fall short in expressive power compared to neural network-based approaches. This discrepancy highlights a persistent tradeoff between interpretability and performance in ICD coding tasks.\nFurthermore, the prevailing interpretability method for deep neural models in ICD coding tasks rely on the attention mechanism (Yan et al., 2022). Specifically, the LAAT mechanism projects token embeddings into a label-specific attention space, where each token receives a score indicating its relevance to each ICD prediction. Such attention-based associations between tokens and classes have been employed in various architectures, including convolutional models like CAML (Mullenbach et al., 2018), recurrent neural networks (Vu et al., 2020), and large language models (Huang et al., 2022; Yang et al., 2023). While computationally efficient, it overlooks the potentially richer information hidden within the embedding space, hindering our understanding of automated ICD predictions. Our work builds on top of such past works, and directly interprets the medical PLM embeddings."}, {"title": "2.2 Dictionary Learning", "content": "Dictionary learning aims to find a sparse representation of input data in the form of linear combina-"}, {"title": "3 Methodology", "content": "As depicted in Figure 2, our focus within dictionary learning involves explicitly building dictionaries where relevant tokens and ICD codes are mapped to dictionary features. We examine two sparse autoencoder approaches aimed at creating interpretable representations from dense language model embeddings: one via L\u2081 minimization (Cunningham et al., 2023; Bricken et al., 2023) and another via SPINE's loss function (Subramanian et al., 2017). While our discussion primarily centers on the L\u2081 minimization technique for its widespread application and illustrative clarity regarding dictionary learning's objectives in section 3.1, further details on SPINE are provided in the Appendix A.6.\nThen, using our trained sparse autoencoder, we perform ablation studies to understand the downstream effects of dictionary features in section 4.1 and map the relevant ICD codes to each dictionary feature as discussed in section 3.2. Finally, we leverage sparse encoding and its ablation techniques in constructing our final dictionary, mapping both relevant tokens and ICD codes to each dictionary feature in section 3.3, which is used in our new proposed method AutoCodeDL in Figure 3."}, {"title": "3.1 Sparse Autoencoders", "content": "Following (Bricken et al., 2023)'s approach, let x \u2208 Rd be the token embedding we wish to interpret, d the dimension size of the token embedding, and m the dimension size of the latent sparse dictionary feature activations f \u2208 Rm generated by the sparse autoencoder. Our L\u2081 sparse autoencoder is shown below in equations 1 through 4 where We \u2208 Rm\u00d7d is the encoder weight matrix, be \u2208 Rm is the encoder bias term, ba \u2208 Rd is the decoder bias term, and Wa \u2208 Rd\u00d7m represents the sparse dictionary embeddings:\n$$ \\tilde{x} = x$$\n$$ f = ReLU(W_e x + b_e) $$\n$$ \\hat{x} = W_a f + b_a$$\n$$ \\mathcal{L} = \\frac{1}{|X|} \\sum_{x \\in X} ||x - \\hat{x}||_2 + \\lambda_1 ||f||_1$$\n\nThe L\u2081 norm $$|| f ||_1$$ in the loss function (see eq. 4) enforces the sparse representation of f in training. As a result, only certain elements within f activate for certain types of token embeddings x,"}, {"title": "3.2 Mapping Dictionary Features to ICD Codes", "content": "To build a medically relevant dictionary, ICD codes should map to their respective meaningful dictionary features. Following the methodology in (Bricken et al., 2023), we ablate features in clinical notes by targeting any activated dictionary feature $$f_i > 0$$ in token embedding $$x \\in R^d$$. For each feature $$f_i$$ with corresponding feature embedding $$h_i \\in R^d$$, we define the ablated token embedding as $$ \\tilde{x}$$.\n$$ \\tilde{x} = x - f_i h_i$$\n\nFor any token in a clinical note, we perform token embedding ablations, recalculate the ablated model's softmax probabilities for all C classes or ICD codes, and compute the probability differences $$ \\delta_i$$.\n$$ \\delta_i = p(x) \u2013 p(\\tilde{x}),  \\delta_i \\in R^C$$\n\nFinally, for any given class $$c \\in \\{1, 2, ..., C\\}$$, we sort and record the top $$ \\delta_{i,c}$$'s when ablating each dictionary feature $$f_i$$ and its embedding $$h_i$$, identifying its most relevant ICD codes. Such ablations are later used for evaluating the model explainability of dictionary features and building more human-interpretable medical dictionaries with the sparse autoencoder."}, {"title": "3.3 Building Medically Relevant Dictionaries to Augment ICD Explanations", "content": "In essence, the sparse autoencoder contains a dictionary within its latent space. While efficient in time and space complexity, its direct interpretation requires the construction of a more human-interpretable and literal dictionary containing the most relevant tokens and ICD codes for each dictionary feature $$f_i$$. Building a dictionary can be summed up into two sorting steps.\nSampling and Encoding. We sample a certain number of clinical notes from our test set. For every token in each clinical note, we encode their PLM embeddings with our sparse autoencoder and retrieve its sparse feature activations $$f_i$$. Then, for each dictionary feature i, we sort by each token's re-"}, {"title": "4 Interpretability Evaluations", "content": "While there does not exist a unified singular definition of neural model interpretability, there is a general consensus that model interpretability methods should be both model explainable and human understandable (Zhang et al., 2021). We define explainability as how much do the dictionary features learned predict the pre-trained language model's ICD predictions. We define human understandability within the lens of monosemanticity: a dictionary feature is only human interpretable when the tokens that highly activate said dictionary feature are related and its underlying concept can be easily identified.\nSparse Autoencoders. We explore two major dictionary learning (DL) sparse autoencoder approaches, SPINE by (Subramanian et al., 2017) and L\u2081 described in section 3.\nBaselines. Following (Cunningham et al., 2023), we explore four unsupervised baseline encoders that we compare to a true dictionary encoding: Independent Component Analysis (ICA), capable of decomposing word embeddings into semantically meaningful independent components (ICs) (Musil and Mare\u010dek, 2022); Principal Component Analysis (PCA), which has been used to analyze the structure of word embeddings (Musil, 2019); an Identity ReLU encoder, which effectively treats"}, {"title": "4.1 Model Explainability", "content": "Faithfulness. Inspired by explainability evaluation metrics within the vision domain (Chattopadhay et al., 2018; Samek et al., 2015), we assess our interpretable dictionary features by removing them (ablation) and measuring their impact on predicted ICD codes. This approach helps us quantify how well our explanations align with the model's predictions. For example, ablating the \"depression\" feature should primarily impact related ICD codes like \"depressive disorders\" while leaving unrelated ones like \"postoperative wound infection\" relatively unaffected.\nTo address this, we consider both the decrease in the most likely code's softmax probability after ablation and the sum of absolute changes in softmax probabilities for all other codes. Computing a ratio of these measures (detailed in Table 1) allows us to accurately gauge an interpretability method's explanatory power. This ablation metric is analogous to Comprehensiveness (Comp) by (Chan et al., 2022; DeYoung et al., 2020), but instead of erasing entire tokens, we ablate specific components of the embedding (see eq. 7).\nSetup. We compare our interpretability framework, AutoCodeDL, against three sets of baselines.\nThe first set involves feature ablations using baseline encoders and full token ablations (labeled \"token\" in Table 1) with LAAT to pre-highlight relevant tokens. The second set excludes LAAT and simply explains only with DL feature ablations across all tokens. The third set excludes LAAT and only utilizes the baseline encoder ablations. For additional details on the ablations for each baseline encoder, please refer to Appendix A.7.\nResults. From Table 1, ablations of dictionary features of highlighted tokens with our proposed AutoCodeDL method show minimal impact on other ICD code predictions while retaining large drops in softmax probabilities. As a result, our"}, {"title": "4.2 Human Understandability of Dictionaries", "content": "Evaluating the human understandability of interpretability methods lacks a clear definition, often"}, {"title": "5 Conclusion", "content": "This study introduces a novel method that combines dictionary learning with label attention mechanisms to improve the interpretability of medical coding language models. By uncovering interpretable dictionary features from dense language embeddings, the proposed approach offers a dictionary-based rationale for ICD code predictions, addressing the growing demand for transparency in automated healthcare decisions. This work lays the groundwork for future explorations in dictionary learning to enhance healthcare interpretability."}, {"title": "Limitations", "content": "We note that there are several key limitations of the dictionary learning applied here. First, as noted in the training details within the appendix, the sparse autoencoders are unable to perfectly reconstruct PLM embeddings, and thus cannot fully capture a model's downstream performance (Cunningham et al., 2023; Bricken et al., 2023; Templeton et al., 2024). Furthermore, we note as similarly explored by (Cunningham et al., 2023; Bricken et al., 2023; Templeton et al., 2024) that there exist dead or missing features, features that do not activate regardless of token embedding or concepts that exist in the model but are not captured within the dictionary, indicating not all features are useful or interpretable.\nIn terms of the coherence of highly activating features, we highlight that the sparse dictionaries learned are not as coherent as supervised mappings"}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Sparse Autoencoder Training Details", "content": "We train our sparse autoencoders on the PLM activations generated by a 110M medical RoBERTa encoder PLM on the cleaned MIMIC-III train dataset of 38,427 clinical notes and evaluated on their test set of 8,750 clinical notes for a total of 52,712 clinical notes, as detailed by (Edin et al., 2023). We follow the advice of (Bricken et al., 2023) and (Subramanian et al., 2017) in training the L\u2081 and SPINE autoencoders respectively. Our hyperparameters are shown in Table 6. We also use AdamW as our optimizer. For a fair comparison, we reuse the dictionary feature size m as it has been noted by (Bricken et al., 2023) that larger dictionary feature sizes can potentially increase the resolution of concepts of dictionary features. For instance, dictionary feature may be further decomposed into features of of more specific meanings given different token sequence contexts as further discussed by (Bricken et al., 2023). We use PyTorch as our deep learning framework of choice."}, {"title": "A.2 Scalability Discussion", "content": "Training these sparse autoencoders takes approximately 6 minutes per epoch on A6000 GPUs. However, the current training process samples new token embeddings for every batch of clinical notes, which is suboptimal. A quick adaptation of existing dataloaders to improve token diversity during training could be beneficial. We find that precomputing and caching PLM embeddings can significantly reduce training time to approximately 15 minutes for 10 epochs, albeit requiring substantial memory (at least 128 GB RAM for caching millions of embeddings). In contrast, decomposing PLM embeddings using our method is extremely fast, on par with LAAT (approximately 0.04 seconds per clinical note). While there is an upfront cost (a couple of hours for sorting and sampling millions of tokens for each dictionary feature), once a dictionary is constructed, interpreting the embedding space of any clinical note is very fast and efficient, which is suitable for this high dimensional multilabel task."}, {"title": "A.3 Build Dictionary", "content": "For further clarity, we write up our dictionary construction algorithm here in algorithm 1. In principle, one is just sorting based on encoded activations and ablation softmax drops of different ICD codes for each dictionary feature."}, {"title": "A.4 Additional Baseline Details", "content": "There were four main baseline methods that were compared against our exploration of two sparse autoencoders, specifically an ICA encoder, PCA encoder, an identity encoder, and a random encoder. All of their implementations were taken from (Cunningham et al., 2023). The ICA encoder was trained using the FastICA decomposition method from scikit-learn to estimate the activation's respective independent components that act"}, {"title": "A.5 Label Attention Details", "content": "We recognize that we don't explicitly describe LAAT (Vu et al., 2020; Huang et al., 2022) in detail in the main manuscript. For interested readers, we depict the label attention mechanism in Figure 5.\nEssentially, LAAT computes a cross attention score for each token and ICD code, creating a label attention matrix where each row is an ICD and every column is the token's attention score with respect to that ICD code."}, {"title": "A.6 Additional SPINE Details", "content": "While the L\u2081 minimization is most commonly used to train sparse autoencoders due to its simplicity of training (Zhang et al., 2015b), we also revisit an alternative sparse formulation SPINE proposed by (Subramanian et al., 2017), specifically designed to decompose neural word embeddings. (Subramanian et al., 2017) showed that they could improve interpretability in GloVe (Pennington et al., 2014) and other forms of neural token embeddings (Almeida and Xex\u00e9o, 2023). Their formulation replaces the L\u2081 regularization loss in favor of using a"}, {"title": "A.7 Baseline Ablation Experiment Details", "content": "For every baseline, we iterate through each activated dictionary feature for the tokens, measuring the softmax drop for the selected ICD code"}, {"title": "A.8 Sufficiency Discussion", "content": "If comprehensiveness measures the performance drop when ablating key features, sufficiency measures the performance retention when keeping only the key features (Chan et al., 2022). In particular, rather than performing an ablation, we simply set all of the token embeddings to their respective dictionary feature embeddings multiplied by their activations. Clinical coding is high-dimensional, making it computationally expensive to compute different quantiles. Therefore, we only compute metrics for the highly relevant 95% quantile tokens when using LAAT, or for all tokens when not using LAAT.\n$$ \\hat{x} = f_i h_i$$\n\nHowever, when using our specific feature encoders, this metric yields mixed results. While retaining only the relevant token embeddings defined by LAAT preserves the model's class probabilities, we find that a randomly generated embedding from our random encoder remarkably performs similarly or better than the LAAT explanation. This observation raises further questions about the potential inner workings and importance of the embedding space in explaining model behavior. Especially, as the steering experiment is effectively related to the sufficiency metric, but with difference being"}, {"title": "A.9 Hidden Meaning Stop Words Experiment", "content": "We explain more details of our stop words experiments here. To begin, we sample all (not just the attention highlighted) tokens from 1,600 clinical notes sampled from the test set and then map highly activating tokens to each dictionary feature as well as relevant ICD code predictions through dictionary feature ablations outlined in section 3.2 and 3.3. Then, we collect (12,891) highly relevant stop words (using NLTK) identified by the label attention mechanism, documenting their corresponding labels and PLM embeddings as defined by the label attention matrix. After shuffling each label-embedding pairing, we employ our dictionary to query the stop word's relevant classes via a trained sparse autoencoder, assessing if the original label ranks among the top 10 classes of each of its most"}, {"title": "A.10 Additional Siamese BERT Cosine Similarity Experiment Details", "content": "We use \"all-mpnet-base-v2\" from the sentence-transformers package as our Siamese encoder. We show our steps for computing the cosine similarity scores in our evaluation of coherence scores below in algorithm 2. Furthermore, since sparse encoding is a more efficient operation than searching and ablating the most relevant dictionary feature for an ICD prediction, we sample all tokens from every clinical note in the test set, and discern all of the top tokens for each dictionary feature with our sparse autoencoder."}, {"title": "A.11 Human Evaluations", "content": "We conducted human evaluations with a medical scientist trainee and a licensed physician, specifically the distinctiveness experiment inspired by Subramanian et al. (2017). Below, we showcase examples of incoherent dictionary features (low cosine similarity) and highly coherent features (high cosine similarity). We also provide examples from the human evaluations, including cases where annotators failed to identify the randomly chosen context and cases where it was easy for them to distinguish the random token."}, {"title": "A.12 Initial Sparse Autoencoder Experiments", "content": "We trained new sparse autoencoders for this revision, as we felt the previous ones were ill-trained due to not filtering out pads during the training process, leading to many irrelevant medical features. Surprisingly, despite finding that only approximately 500 features accounted for many of the ablation downstream results, their feature ablations still highly affected downstream performance, as shown in Table 9. Note that these experiments measured the top ground truth medical code rather than the top predicted medical code, hence the discrepancy in the probability drops. However, the order of these results remains the same. We also report the previous results, such as coherence and similarity, in Section A.12.2. We were surprised to find the relative order of baselines in terms of explainability performance remained the same in this revision."}, {"title": "A.12.1 Other Initial Ablation Experiments", "content": "We perform additional validation experiments to showcase that sparse autoencoder (dictionary learning) do outperform their none-sparse baselines in terms of model explainability. For reference, we have investigated the downstream effects of dictionary learning through a total of four ablation experiments (Figure 15). Our analysis centered on the ICD code most likely to be predicted for each note, determined by softmax probabilities, and employed two token ablation benchmarks: (A) complete ablation of all highlighted tokens and (C) random ablation of half the top highlighted tokens.\nIn parallel, dictionary features underwent ablation in two forms: (B) solely ablation of the paramount dictionary feature for all highlighted tokens or (D) ablating half of the tokens and ablating the most significant dictionary feature of the remaining highlighted tokens. For reference, tokens surpassing the 95th percentile in attention scores for their respective ICD code in the label attention matrix were considered \"highlighted\". We note that we have only shown experimental results for ablation types (A) and (B) in section 4.1.\nTo investigate if such ablations are perfectly additive, we perform further experiments (C) and (D) where we only ablate half of the relevant tokens and observe the overall changes from ablating the dictionary features of the unablated other half of tokens in Table 11.\nWe observe that while ablating such dictionary"}]}