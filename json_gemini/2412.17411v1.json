{"title": "Pretraining with random noise for uncertainty calibration", "authors": ["Jeonghwan Cheon", "Se-Bum Paik"], "abstract": "Uncertainty calibration, the process of aligning confidence with accuracy, is a hallmark of human intelligence. However, most machine learning models struggle to achieve this alignment, particularly when the training dataset is small relative to the network's capacity. Here, we demonstrate that uncertainty calibration can be effectively achieved through a pretraining method inspired by developmental neuroscience. Specifically, training with random noise before data training allows neural networks to calibrate their uncertainty, ensuring that confidence levels are aligned with actual accuracy. We show that randomly initialized, untrained networks tend to exhibit erroneously high confidence, but pretraining with random noise effectively calibrates these networks, bringing their confidence down to chance levels across input spaces. As a result, networks pretrained with random noise exhibit optimal calibration, with confidence closely aligned with accuracy throughout subsequent data training. These pre-calibrated networks also perform better at identifying \"unknown data\" by exhibiting lower confidence for out-of-distribution samples. Our findings provide a fundamental solution for uncertainty calibration in both in-distribution and out-of-distribution contexts.", "sections": [{"title": "One sentence summary", "content": "Pretraining with random noise enables networks to align confidence with accuracy, effectively calibrating uncertainty for both in-distribution and out-of-distribution data."}, {"title": "Research Highlights", "content": "\u2022 Deep neural networks often struggle to properly calibrate both accuracy and confidence.\n\u2022 Pretraining with random noise aligns confidence levels with actual accuracy.\n\u2022 Random noise pretraining facilitates meta-learning by aligning confidence with the chance level.\n\u2022 Pre-calibrated networks assess both known and unknown datasets through confidence estimation."}, {"title": "Introduction", "content": "Recent advances in deep neural networks have significantly enhanced their ability to learn with accuracy 1. For instance, deep learning models achieved human-level accuracy in classifying natural images over a decade ago 2-5. Building on this remarkable capability, deep neural networks are now being applied to a wide range of real-world problems that involve complex decision-making, such as autonomous driving systems 6,7, medical diagnosis 8,9, and financial engineering 10,11. However, in these applications, simply inferring the correct answer is insufficient; an additional dimension of information must be considered: confidence or uncertainty (Fig. 1). It is crucial to predict answers with calibrated probabilities that accurately reflect the likelihood of correctness.\nDespite their remarkable capacity for accurate learning, modern deep neural networks are often poorly calibrated in terms of confidence estimation 12\u201314. Specifically, the rapid increase in model capacity enables accurate classification but negatively impacts confidence calibration 12. Modern neural networks frequently exhibit overconfidence, even with out-of-distribution samples despite lacking knowledge about them 15,16. Miscalibrated confidence poses significant problems in real-world applications, where false predictions can be critical and costly 17-22. Hallucinations in large language models 23-26, where the model confidently produces false and unsubstantiated outputs 27-30, are a well-known issue arising from the miscalibration of neural networks 31-33.\nSeveral studies have proposed solutions to calibrate confidence for predictions from classifier models. These research efforts suggest post-processing techniques to compute calibrated probabilities from the na\u00efve output of the trained network, such as binning the output probability 12,34-36 or adjusting the level of network output 12,36,37. Recent studies also utilize Bayesian neural networks to achieve state-of-the-art performance in uncertainty calibration 38-40, but they require computationally intensive repetitions to calculate uncertainty in each trial. Many state-of-the-art algorithms employ auxiliary networks or score functions to distinguish overconfident predictions for unseen out-of-distribution data from those for in-distribution data 15,16,41\u201344. Overall, previous research typically handles calibration issues separately for in-distribution data and unseen out-of-distribution data. Moreover, these methods often require additional, computationally expensive steps to obtain calibrated confidence or auxiliary scores.\nIn contrast to the scenario in machine learning, where calibrated probabilities must be obtained through additional processes, human intelligence naturally exhibits meta-cognition, which estimates uncertainty and is aware of what is \u201cknown\u201d and \u201cunknown\u201d 45\u201348. This raises the question: what is the underlying mechanism of uncertainty calibration in the biological brain, and how can we enable artificial neural networks to learn human-like uncertainty calibration without relying on post-processing or auxiliary computation? To address this question, we draw inspiration from our previous work 49, which proposed pretraining with random noise as a method that mirrors the biological prenatal learning stage. Specifically, we focused on the fact that random noise pretraining stabilizes the input space to align with the output chance level uniformly before learning from data. We hypothesized that these initial calibrations could facilitate the learning of calibrated uncertainty during subsequent data training.\nIn this study, we demonstrate that pre-calibrating uncertainty through training with random noise enables neural networks to learn probabilities that accurately reflect the likelihood of correctness. Our findings show that randomly initialized neural networks tend to exhibit high confidence even when they lack sufficient knowledge, but training with random noise effectively reduces loss by calibrating this confidence to chance levels uniformly across input spaces. As a result, pre-calibrated networks learn calibrated probabilities with accuracy and confidence aligned throughout subsequent data training. Moreover, pre-calibrated networks exhibit lower confidence when encountering unknown data, aiding in the detection of out-of-distribution samples. Taken together, our results suggest pretraining with random noise as a simple yet powerful strategy to address uncertainty calibration issues."}, {"title": "Results", "content": "Failure of confidence calibration in deep neural networks\nTo investigate the issue of confidence calibration in deep neural network models, we used a feedforward neural network designed for pattern classification of three-channel natural images (32\u00d732\u00d73) into ten categories. The network consists of multiple hidden layers with ReLU nonlinearities and a final classification layer that employs the SoftMax function. For a given image sample, the model outputs probabilities for each object category (Fig. 2a), and the predicted label corresponds to the category with the maximum probability. The model also provides a measure of prediction confidence, which is derived from the probability assigned to the predicted category. The predictive uncertainty, can be quantified as the difference between the confidence value and one. Ideally, a neural network is expected to output calibrated confidence, where the confidence level accurately reflects the actual likelihood of the prediction being correct. In contrast, a poorly calibrated network may output confidence levels that do not correspond to the actual likelihood of correctness, leading to potential misinterpretations of the model's predictions.\nWe trained the feedforward neural network using a subset of the CIFAR-10 dataset 50, which consists of natural images labeled with ten classes representing animals and objects. After training, we evaluated the model's predictions and confidence using test images that were not part of the training set. Specifically, we investigated whether the confidence level of the trained network accurately reflects the likelihood of correctness. By measuring binned confidence values and the corresponding expected accuracy within each binned trial prediction, we constructed a reliability diagram 37,51 to visualize the model's calibration (Fig. 2b). In this diagram, ideal calibration appears as an identity function, where the estimated confidence corresponds to the actual probability of correctness. In our results, we observed a significant gap between ideal calibration and the model's outputs. In general, the model's accuracy was lower than its confidence level, demonstrating the model's tendency for overconfidence.\nWe investigated the pattern of miscalibration by training neural networks of varying complexity with different training data sizes. To quantitatively measure the degree of miscalibration, we calculated the expected calibration error (ECE) 52, which averages the difference between accuracy and confidence in each bin, weighted by the number of samples. As a result, we found that both model complexity and training data size systematically affect confidence calibration (Fig. 2c). Specifically, we observed that the degree of miscalibration increases as the training data size decreases (Fig. 2d) and as model complexity increases (Fig. 2e). Thus, miscalibration arises when the training data is insufficient relative to the network's complexity. This result explains why state-of-the-art models with high complexity struggle with the mismatch between confidence and accuracy. Recent network architectures often increase in structural depth to enhance learning capacity, but the available data size does not increase correspondingly. In real-world applications, acquiring and training on massive datasets is expensive and requires substantial computational resources. As a result, the miscalibration problem becomes inevitable in deep learning models."}, {"title": "Pretraining with random noise for confidence calibration", "content": "Recently, we introduced a novel strategy for pretraining networks inspired by the development of the biological brain 49. In this work, we demonstrated that pretraining networks with random noise enables robust learning without weight transport. Here, we revisit our random noise pretraining method and show that this process facilitates the preconditioning of networks, calibrating confidence to the chance level. We pretrained neural networks with noise inputs randomly sampled from a Gaussian distribution, and with random labels sampled from a uniform distribution (Fig. 3a, Supplementary Fig. 1). Notably, we observed that the loss gradually decreased when the network was trained with random noise, even though the accuracy remained at the chance level (Fig. 3b, Supplementary Fig. 2). Furthermore, when the network was trained with real data following the noise pretraining (Fig. 3b, Supplementary Fig. 3), we observed a more robust decrease in test loss during subsequent training, which was significantly lower than in networks trained without random noise pretraining (Fig. 3b). We also confirmed that the network pretrained with random noise achieved higher accuracy (Fig. 3c). These results demonstrate that pretraining with random noise enables faster and more effective loss reduction in subsequent training.\nNext, we examined whether random noise pretraining can improve the calibration of probabilities to align confidence with accuracy levels. First, we measured the confidence distribution of the network's output on the test dataset and compared it with the actual accuracy (Fig. 3d). We found that random noise pretraining significantly reduced the difference between confidence and accuracy in subsequent data training (Fig. 3e, w/o vs. w/ random pretraining, $N_{net} = 10$, Wilcoxon rank-sum test, $P < 10^{-3}$; w/o vs. zero, Wilcoxon signed-rank test, NS, $P < 10^{-3}$; w/ vs. zero, Wilcoxon signed-rank test, NS, $P = 0.131$). We also investigated the difference between the two networks using a reliability diagram (Fig. 3f). We observed that the randomly pretrained network's outputs were closer to ideal calibration than those of networks trained with data alone. Additionally, we confirmed that the pretrained networks exhibited significantly lower calibration error compared to networks solely trained with data (Fig. 3g, w/o vs. w/ random pretraining, $n_{net} = 10$, Wilcoxon rank-sum test, $P < 10^{-3}$). \nWe then extended this analysis to neural networks trained with varying amounts of data and found that the effect of random noise pretraining in reducing calibration error was significant, regardless of the training data size (Fig. 3h, w/o vs. w/ random pretraining, $n_{net} = 10$, Wilcoxon rank-sum test, $P < 10^{-3}$). We also tested neural networks of different depths and confirmed the robust effect of noise pretraining (Fig. 3h, w/o vs. w/ random pretraining,"}, {"title": "Pre-calibration of network uncertainty over input space via random noise", "content": "We further investigated how pretraining with random noise influences the properties of neural networks prior to their exposure to real data. To explore this, we introduced a simplified toy-model network with a two-dimensional input space (Fig. 4a), enabling straightforward visualization of the input space with only two features (Fig. 4b). Under these conditions, we observed that the initial state of a conventionally processed network \u2014 randomly initialized 53 and untrained \u2014 exhibited a highly biased and arbitrary distribution of confidence across the input space (Fig. 4b, Left). Additionally, we confirmed that this initial miscalibration was consistent across multiple trials with randomly initialized networks (Supplementary Fig. 5a).\nIn contrast, networks pretrained with random noise exhibited a more homogeneous distribution of confidence across the input space, effectively calibrated to the chance level (Fig. 4b, Right; Supplementary Fig. 5b). This result demonstrates that pretraining with random noise mitigates overconfidence by calibrating the confidence levels across the entire input space, bringing them closer to the chance level (Fig. 4c, w/ vs. w/o random pretraining, $n_{trial} = 1000$, Wilcoxon rank-sum test, $P < 10^{-3}$; Supplementary Fig. 5c). We also observed that untrained networks could exhibit initial biases toward specific output classes due to fluctuations in confidence. However, random noise pretraining significantly reduced this initial class bias, resulting in a more uniform confidence distribution across the classes (Fig. 4d, w/ vs. w/o random pretraining, $n_{net} = 10$, Wilcoxon rank-sum test, $P < 10^{-3}$). \nNext, we extended this analysis to models with higher-dimensional input spaces (Fig. 4e). Specifically, the model used input features with dimensions of 32\u00d732\u00d73 and output Softmax probabilities for ten classes, allowing it to train on CIFAR-10 data. We first measured the probability outputs for random noise inputs in both untrained and randomly pretrained networks (Fig. 4f). Consistent with the toy model results, we observed overconfidence and class bias in untrained networks (Fig. 4f, Left, $n_{trial} = 100$, one-way ANOVA, $P < 10^{-3}$), whereas such biases were barely detectable in randomly pretrained networks (Fig. 4f, Right, $n_{trial} = 100$, one-way ANOVA, NS, $P = 0.429$). We confirmed that the differences between the two networks were consistent with the toy model findings, both in the distribution of confidence (Fig. 4g, w/ vs. w/o random pretraining, $n_{trial}=100$, Wilcoxon rank-sum test, $P < 10^{-3}$) and in class bias (Fig. 4h, w/ vs. w/o random pretraining, $n_{net} = 10$, Wilcoxon rank-sum test, $P < 10^{-3}$). These results suggest that random noise pretraining enables the network to maintain lower, more uniformly calibrated confidence levels, even when handling more complex, higher-dimensional input data.\nWe investigated whether the pre-calibration effect observed with random noise training extends to unseen, more realistic data. Specifically, we examined how the network's confidence on CIFAR-10 and SVHN 54 datasets changes during random noise pretraining (Fig. 4i, Left) by measuring the network's predictions for real data after each epoch during the pretraining phase. Initially, we observed that the confidence for unseen CIFAR-10 data was well-calibrated and approached the chance level, while accuracy remained at the chance level throughout random noise training (Fig. 4i, Right). We confirmed that this trend was consistent across different unseen datasets, including CIFAR-10 (Fig. 4j, CIFAR-10; w/ vs. w/o random pretraining, $n_{trial} = 10000$, Wilcoxon rank-sum test, $P < 10^{-3}$) and SVHN (Fig. 4j, SVHN; w/ vs. w/o random pretraining, $n_{trial} = 10000$, Wilcoxon rank-sum test, $P < 10^{-3}$). While accuracy remained at the chance level, the network maximized uncertainty for unseen data, aligning confidence with accuracy at the chance level. As a result, calibration error \u2014 the discrepancy between confidence and accuracy \u2014 was significantly reduced for unseen data (Fig. 4j, w/ vs. w/o random pretraining, $n_{net} = 10$, Wilcoxon rank-sum test, CIFAR-10, $P < 10^{-3}$; SVHN, $P < 10^{-3}$). These findings suggest that networks initialized conventionally tend to exhibit overconfidence and class bias even before training on data. In contrast, random noise pretraining effectively eliminates this initial bias and calibrates confidence to the chance level, maximizing uncertainty for unseen datasets. This result indicates that random noise pretraining can serve as an novel initialization strategy, offering an alternative to conventional random initialization 53."}, {"title": "Aligning confidence levels with actual accuracy during random noise pretraining", "content": "To further validate the effect of pre-calibration, we examined the learning dynamics of networks during subsequent training. To assess the impact of initial loss reduction where the loss levels of networks were already lowered through random noise pretraining before learning from data we investigated the training trajectory in a two-dimensional plane of accuracy and loss (Fig. 5a). Notably, the trajectory differed significantly between networks with and without random noise pretraining. Specifically, when comparing the points on both curves with the same accuracy value, we found that the randomly trained network had significantly lower loss values than the network trained solely on data. By selecting arbitrary points of the same accuracy (Fig. 5a, (1) and (2)), we examined the reliability diagram and calibration error for the two networks. We found that networks with random pretraining achieve better confidence calibration than those without pretraining, even with the same accuracy (Fig. 5b, w/o vs. w/ random pretraining, $N_{net} = 10$, Wilcoxon rank-sum test, $P < 10^{-3}$). Similarly, by selecting arbitrary points of the same total training epochs (Fig. 5a, (3), (4)), we observed that pretrained networks achieve better confidence calibration (Fig. 5c, w/o vs. w/ random pretraining, $n_{net} = 10$, Wilcoxon rank-sum test, $P < 10^{-3}$). These results demonstrate that random noise pretraining modulates the dynamics of subsequent learning, particularly by reducing the loss level for the same accuracy."}, {"title": "Detection of out-of-distribution samples using calibrated network confidence", "content": "Lastly, we investigated the effect of random noise pretraining on confidence calibration for out-of-distribution (OOD) data. OOD samples refer to unseen data that the model has not been trained on, in contrast to in-distribution (ID) samples, which the model has been trained on (Fig. 6a). After training our model on CIFAR-10 (ID), we measured the network's response to unseen SVHN data (OOD) (Fig. 6b). Notably, networks without random noise pretraining exhibited high confidence, even though they had no knowledge of OOD data (Fig. 6c, Left). In contrast, randomly pretrained networks showed significantly lower confidence for OOD samples, aligning closer to the chance level, which represents the ideal confidence for \"unknown\" data. We confirmed that randomly pretrained networks exhibited lower average confidence, closer to the chance level, for OOD samples (Fig. 6c, Right; w/ vs. w/o random pretraining, $n_{Sample} = 10000$, Wilcoxon rank-sum test, $P < 10^{-3}$). \nWe further investigated how calibrated confidence, for both in-distribution (ID) and out-of-distribution (OOD) samples, enhances the OOD detection task. To do this, we implemented a simple framework that used only raw confidence values to distinguish between ID and OOD samples (Fig. 6d). We analyzed the confidence histograms of ID and OOD samples and their cumulative distribution functions (Fig. 6e). In networks without pretraining, we observed significant overlap in the confidence histograms between ID and OOD samples, making it challenging to differentiate between the two groups (Fig. 6e, Left). In contrast, networks with random noise pretraining showed less overlap, making the task easier (Fig. 6e, Right). We then measured the classification accuracy at various thresholds and plotted the results in the receiver operating characteristic (ROC) curve (Fig. 6f, Left). By calculating the area under the ROC curve (AUROC), we quantified the performance of the networks (Fig. 6f, Right). The results indicate that random noise pretraining significantly improves OOD detection performance (Fig. 6f, Right; w/ vs. w/o random pretraining, $n_{net} = 10$, Wilcoxon rank-sum test, $P < 10^{-3}$). These findings suggest that calibrated confidence helps maintain low confidence for unseen OOD data even after training with ID data. This implies that properly calibrated confidence, for both ID and OOD data, enables robust discrimination between \"known\" and \"unknown\" data without the need for post-processing or auxiliary computations."}, {"title": "Discussion", "content": "We demonstrated that pretraining with random noise enables neural networks to learn calibrated probabilities, aligning their confidence levels with expected accuracy. Specifically, we found that random noise training maximizes the initial uncertainty of the network's predictions, calibrating its confidence to the chance level. We validated that this pre-calibration allows networks to learn with reduced calibration errors. Furthermore, we showed that the network can effectively detect previously unseen data, i.e., out-of-distribution samples. These results suggest that properly calibrating the initial conditions through random noise training can significantly influence learning dynamics and enhance performance.\nA longstanding question in machine learning concerns the proper initialization of neural networks for efficient learning. If the initialization is not appropriate, the network may converge to a local minimum or even diverge 55,56. Previous studies have shown that altering the initialization can help the network converge with higher accuracy 53,57. Several optimized initialization techniques have been proposed based on the network's activation function, which were considered optimal starting points for achieving higher accuracy. However, our results suggest that these previously proposed initialization methods are not ideal for learning in terms of calibration, as they tend to be initially overconfident and biased toward specific output neurons. We have confirmed that this initial miscalibration makes it challenging to align confidence with accuracy during subsequent training.\nPrevious research has primarily addressed overconfidence issues through post-calibration, where \"calibrated confidence\" is recalculated separately from the raw network output 12,34\u201337. In addition to post-calibration, recent studies have explored the potential of Bayesian neural networks to learn calibrated probabilities 38\u201340. However, these methods require significantly more computational resources in addition to the original networks. Moreover, detecting out-of-distribution (OOD) samples necessitates auxiliary computations, such as training additional networks or calculating extra scores to identify unseen samples 15,16,41\u201344. Traditionally, the calibration of in-distribution (ID) and OOD samples has been treated as separate problems, often requiring additional, complex computations beyond the core learning process. In contrast, our proposed method offers a unified solution for both ID and OOD data, without the need for extra resources or post-processing."}, {"title": "Methods", "content": "Neural network model\nWe employed a multi-layer feedforward model as a simplified representative of conventional neural networks to investigate the miscalibration problem and the effect of random noise pretraining. The model consists of multiple non-linear layers, each performing a series of operations: multiplying weights, adding biases, and passing through a ReLU activation function. To mitigate overfitting, batch normalization was applied to every layer. The hidden layer size was fixed at 256 neurons, with the layer depth varied across different experiments. We observed that our results were fairly consistent regardless of hyperparameter settings, such as the hidden layer size, under typical conditions. The network weights were randomly initialized following a Gaussian distribution, with a mean of 0 and a standard deviation determined to control the gain across layers, as per the standard initialization method 53. The biases were initialized to zero.\nPretraining with random noise\nThe random noise input was sampled from a Gaussian distribution with a mean of O and a standard deviation of 1. We set the noise size to 32\u00d732\u00d73, matching the size of the input data and the input dimension of the neural networks. The corresponding labels for the random noise were also randomly sampled from a uniform distribution and one-hot encoded. Both the random noise inputs and their labels were re-sampled at each iteration and were not paired with one another. During training, the random noise inputs were fed into the neural network, and the model output was obtained. The error was calculated as the distance between the model output and the random labels, and the network was trained to minimize this error, similar to conventional training procedures.\nSubsequental training with real data\nAfter pretraining the neural network with random noise, we proceeded to train it with real data. We used a subset of the CIFAR-10 dataset, which consists of 32\u00d732\u00d73 natural images representing 10 categories: airplanes, automobiles, birds, cats, deer, dogs, frogs, horses, ships, and trucks. In the primary analysis, we used a training subset of 1000 samples to assess the robustness of calibration following random noise pretraining under challenging conditions. To investigate the effects of training data size and model capacity, we employed feedforward networks with depths of 2, 3, 4, 5, and 6 and trained them on CIFAR-10 subsets of sizes 500, 1000, 2000, 4000, 8000, 16000, and 32000.\nFor both the random noise pretraining and subsequent training with real data, we followed commonly used neural network training procedures. We applied standard backpropagation with the Adam optimizer, using learning rates between 0.0001 and 0.00001 and betas of (0.99, 0.999). To prevent overfitting, we used weight decay with a decay constant of 0.001. During the analysis, we compared the networks pretrained with random noise to those trained solely on data, without any random noise pretraining. All conditions, except for the random noise pretraining, were carefully controlled to isolate the effect of the random noise pretraining."}, {"title": "Evaluating calibration of neural networks", "content": "To evaluate the calibration of the model, we used a reliability diagram, which visualizes the relationship between the model's confidence and its actual accuracy. In this approach, we bin the model's confidence and compute the expected accuracy within each bin. The reliability diagram then plots the expected accuracy as a function of the model's confidence.\n$acc(B_m) = \\frac{1}{B_m} \\sum_{i=1}^{B_m} I(pred = label_i)$ (1)\n$conf(B_m) = \\frac{1}{B_m} \\sum_{i=1}^{B_m} conf_i$ (2)\n, where M is the total number of bins, $B_m$ is the number of samples in the m-th bin, N is the total number of samples, acc($B_m$) is the accuracy of the m-th bin, and conf($B_m$) is the average confidence in the m-th bin. In our simulations, we used 10 bins. To quantitatively measure the calibration of the model, we used the Expected Calibration Error (ECE), which calculates the average difference between the model's confidence and its actual accuracy within the binned confidence. ECE is calculated as:\n$ECE = \\sum_{m=1}^{M} \\frac{B_m}{N} |acc(B_m) - conf(B_m)|$ (3)\nIn an ideally calibrated model, the expected accuracy perfectly matches its confidence. Practically, a model is considered well-calibrated if its ECE is close to zero."}, {"title": "Visualization of confidence in toy-model and high-dimensional networks", "content": "As a proof-of-concept, we first employed a simple toy-model network with a two-dimensional (2D) input space. The input space is defined as the span of two features, each varying between -1 and 1. This allows us to visualize the input space in the 2D plane, where the x-axis represents the first feature and the y-axis represents the second feature. The neural network model consists of two layers, with a hidden layer of size 10. It outputs a two-dimensional vector for binary classification. The model is initialized using He initialization.\nWe trained this toy-model network with 2-dimensional random noise. Subsequently, we measured the model's confidence for various inputs and plotted the resulting confidence on the input space, comparing both untrained and random-pretrained networks. This enables the visualization of the model's confidence map across the input space. In addition, we analyzed the class bias of the model. The model outputs both the predicted class and its associated confidence. We calculated the ratio of predictions for each class and computed the standard deviation of these ratios:\n$Class bias = std([N_0, N_1, ..., N_M]/\\sum_{i=1}^{M} N_i)$ (4)\nwhere $N_i$ represents the count of outputs predicted to class i, and M is the total number of output classes. A high class bias indicates that the model is biased toward specific readout neurons, while a class bias close to zero suggests that the model is unbiased, with predictions distributed more uniformly across the readout neurons.\nTo extend this analysis, we performed similar experiments with high-dimensional feedforward neural networks. We sampled high-dimensional random noise and measured the softmax probability output of the network. We visualized the model's softmax outputs for both untrained networks and those pretrained with random noise. During the random noise pretraining of the high-dimensional networks, we also measured the confidence and accuracy for real data, such as CIFAR-10 and SVHN. Additionally, we calculated the expected calibration error (ECE) for CIFAR-10 and SVHN during random noise pretraining."}, {"title": "Out-of-distribution detection task", "content": "To design the out-of-distribution (OOD) detection task, we used CIFAR-10 as the in-distribution (ID) dataset for training. For the OOD samples, we selected SVHN, which also has a 32\u00d732\u00d73 input size. After training the model on CIFAR-10, we measured the confidence of the network for both ID and OOD samples. Based on the measured confidence, we distinguished between ID and OOD samples by setting an arbitrary threshold. If the confidence exceeded the threshold, the sample was predicted as an ID sample; if the confidence was lower than the threshold, it was predicted as an OOD sample. Since the performance of OOD detection depends on the chosen threshold, we varied the threshold continuously from 0 to 1 and measured the corresponding false positive rate (FPR) and true positive rate (TPR). We then plotted the detection performance as a receiver operating characteristic (ROC) curve, with the true positive rate plotted against the false positive rate. To quantitatively assess the detection performance, we measured the area under the ROC curve (AUROC), which serves as a metric for the performance of OOD detection."}, {"title": "Statistical analysis", "content": "All statistical variables, including sample sizes, exact P values, and statistical methods, are provided in the corresponding text or figure legends."}, {"title": "Data availability", "content": "The datasets used in this study are publicly available: https://www.cs.toronto.edu/~kriz/cifar.html (CIFAR-10 and CIFAR-100), http://ufldl.stanford.edu/housenumbers (SVHN)."}, {"title": "Code availability", "content": "The analysis and simulations were performed using Python 3.11 (Python software foundation) with PyTorch 2.1 and NumPy 1.26.0. Statistical tests were conducted using SciPy 1.11.4 was used to perform the statistical test and analysis. The code used in this study is available from the corresponding author upon request."}, {"title": "Acknowledgements", "content": "This work was supported by the National Research Foundation of Korea (NRF) grants (NRF2022R1A2C3008991 to S.P.) and by the Singularity Professor Research Project of KAIST (to S.P.)."}, {"title": "Author contributions", "content": "J.C. conceived the project. J.C., and S.P. designed the model. J.C. performed the simulations. J.C. and S.P. analyzed the data. J.C. and S.P. wrote the manuscript and designed the figures."}, {"title": "Declaration of completing interests", "content": "The authors declare that they have no competing interests."}]}