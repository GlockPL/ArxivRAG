{"title": "Learning Elementary Cellular Automata with Transformers", "authors": ["Mikhail Burtsev"], "abstract": "Large Language Models demonstrate remarkable mathematical capabilities but at the same time struggle with abstract reasoning and planning. In this study, we explore whether Transformers can learn to abstract and generalize the rules governing Elementary Cellular Automata. By training Transformers on state sequences generated with random initial conditions and local rules, we show that they can generalize across different Boolean functions of fixed arity, effectively abstracting the underlying rules. While the models achieve high accuracy in next-state prediction, their performance declines sharply in multi-step planning tasks without intermediate context. Our analysis reveals that including future states or rule prediction in the training loss enhances the models' ability to form internal representations of the rules, leading to improved performance in longer planning horizons and autoregressive generation. Furthermore, we confirm that increasing the model's depth plays a crucial role in extended sequential computations required for complex reasoning tasks. This highlights the potential to improve LLM with inclusion of longer horizons in loss function, as well as incorporating recurrence and adaptive computation time for dynamic control of model depth.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have become valuable tools in mathematics, demonstrating impressive capabilities in problem-solving and reasoning tasks. Notably, OpenAI's o1 model achieved a ranking among the top 500 students in the US in a qualifier for the USA Math Olympiad (AIME) [1]. Despite these successes, LLMs still face challenges in reasoning [2\u20136] and planning [7], particularly when required to infer and apply underlying rules from data.\nThese observations raise a question: Do the limitations of LLMs in reasoning stem from the nature of their training data, the procedures employed during training, or inherent architectural constraints?\nTransformers [8], which form the backbone of many modern LLMs, are universal approximators [9-\n12] and theoretically capable of simulating Turing machines using intermediate computational steps, making them Turing complete and, by extension, capable of formal reasoning [13-16]. Earlier studies found that transformers can be trained to perform symbolic integration and solving differential equations [17], as well as symbolic regression [18, 19].\nIn our study, we continue the line of research exploring the trainability of Transformers for mathematical reasoning tasks by focusing on Elementary Cellular Automata (ECAs). The simplicity and clarity of this \"toy\" problem make it an ideal testbed for assessing the ability of Transformers to abstract and apply logical rules. By demonstrating that Transformers can learn and generalize Boolean functions of fixed arity inherent in ECAs, we aim to evaluate their ability to infer, generalize, and apply logical rules solely from observed data, without relying on memorization."}, {"title": "Methods", "content": "An Elementary Cellular Automaton (ECA) is a one-dimensional, dynamical system in which space and time are discrete. Let $r \\in \\mathbb{N} : r \\geq 1$ be the neighbourhood radius in space represented by a regular lattice of $W\\in \\mathbb{N} : W > 2r + 1$ identical, locally-interconnected cells with a binary state space, $S = \\{0,1\\}$. The ECA's global state, $x \\in S^W$, is a lattice configuration specified by the values of all the states of all cells in the lattice at a given time. This state evolves deterministically in synchronous, discrete time steps according to a global map $g_p : S^W \\rightarrow S^W$ defined by a local rule $p : S^{2r+1} \\rightarrow S$, so $[g_p(x)]_w = p(x_{w-r}, ..., x_w, ..., x_{w+r})$. The sequence of states an ECA passes through during its space-time evolution, $O_T(x) = [x, g_p(x), g_p(g_p(x)),..., (g_p^t(x)),...,g_p^{T-1}(x)]$, defines its trajectory or orbit from an initial condition (configuration) $x$ for $T \\in \\mathbb{N} : T > 1$.\nLet consider four modifications of learning tasks designed to evaluate different aspects of predictive modeling and rule inference in ECAs.\nOrbit-State (O-S), given an orbit $O_T(x) = [x^{(0)}, x^{(1)}, . . ., x^{(T-1)}]$ where $x^{(0)} \\in S^W$, the objective is to predict the state $x^{(T)}$ at time $T$.\nOrbit-Orbit (O-O), given an orbit $O_k(x)$ for some $k < T$ predict the subsequent states up to time $T$, generating $O_T^k(x) = [x^{(k)},...,x^{(T)}]$.\nOrbit-State and Rule (O-SR), given an orbit $O_T(x)$ the objective is to predict the state $x^{(T)}$ at time $T$ and the local rule $p$.\nRule and Orbit-State (RO-S), given an orbit $O_T(x)$ and the local rule $p$ the objective is to predict the state $x^{(T)}$ at time $T$.\nOur base model is Transformer encoder with full self-attention. It has 4 layers and 8 heads with $d_{model} = 512$. The input vocabulary of the model consists of tokens: [0], [1], [SEP], and [M]. The states and the local rule $p$ are encoded as binary strings. The model receives the orbit as a sequence of bits representing consecutive states separated by the [SEP] tokens. For the prediction of future states or the inference of the local rule, the end section of the input sequence is filled with mask tokens [M] corresponding to the positions of the unknown elements.\nWe generated a dataset with the CellPyLib [20] \u00b9 for fixed lattice size $W = 20$ and neighborhood radius $r = 2$. This configuration results in a total of $2^{2^{2r+1}} \\approx 4.3 \\times 10^9$ possible boolean functions defining local rules. For each sample in the dataset, both the initial state and the local rule $p$ were generated randomly. We then computed the orbit for $T = 20$ time steps using these parameters. The training dataset consists of $9.5 \\times 10^5$ and the test of $10^5$ samples. Importantly, the local rules included in the test set are exclusive and not present in the training set. This separation ensures that the model's"}, {"title": "Results and Discussion", "content": "We first assess whether the samples in our dataset provide sufficient information for the model to learn the underlying dynamics successfully. The minimal number of time steps $T_{min}$ needed to recover the local rule $p$ from the observed orbit can be estimated analogous to the coupon collector's problem $T_{min} = 2^{2^{2r+1}}(ln 2^{2^{2r+1}} + \\gamma)/W \\approx 6.47$, where $\\gamma$ is the Euler-Mascheroni constant ($\\gamma \\approx 0.5772$). Training of the model for the next state prediction (O-S task) confirms this theoretical estimation by showing that the accuracy plateaus after 8 time steps. Therefore, we choose to use input orbits of 10 steps $O^{10}$ as our main setting.\nSuccessful learning of the O-S task demonstrate that the Transformer model is capable of generalizing not only over initial conditions for a particular function commonly the focus in studies of transformer trainability in CA domain [21\u201335] - but also across different Boolean functions of fixed arity (5 in our case). This indicates that the model has learned to abstract a class of rules.\nNext, we investigated the Transformer's ability to plan ahead by predicting future states beyond the immediate next state. Specifically, we trained the model to predict the state at time $x^{(T+k)}$ for look-ahead steps $k \\in \\{1,2,3\\}$. As presented in Figure 2B, this task proved to be significantly more challenging. While the average accuracy for next-state prediction (O-S task) was 0.96, it dropped to 0.80 for $k= 1$ and fell below 0.75 for $k = 2$ and $k = 3$.\nTo determine whether this decline was due to the Transformer's architecture or the training objective, we explored whether accuracy could be improved by training the model to predict intermediate steps. This approach is analogous to the \"chain-of-thought\" method [36] used in large language models for"}, {"title": "Conclusions", "content": "In this study, we have demonstrated that Transformer models possess the ability to learn and generalize the underlying dynamics of Elementary Cellular Automata. By designing specific tasks and training regimes, we showed that Transformers can abstract the governing rules and predict future states with notable accuracy. However, their performance diminishes when required to plan multiple steps ahead without intermediate context, highlighting limitations in storing and propagating state information over longer sequences. Our analysis reveals that including future states or rule prediction in the training loss improves the performance of next-state prediction and, as a result, enhances the overall performance in autoregressive generation. This finding might guide training strategies for LLMs to improve their perplexity and reasoning capabilities. We also confirmed that the model's depth plays an important role in extended sequential computations required for complex reasoning; thus, recurrence and adaptive computation time are promising directions for dynamic control of the model depth. These insights contribute to a deeper understanding of how neural networks can abstract rules and point toward future research directions in improving their planning and generalization skills."}]}