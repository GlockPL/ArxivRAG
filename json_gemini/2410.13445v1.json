{"title": "Parameter-efficient Adaptation of Multilingual Multimodal Models for Low-resource ASR", "authors": ["Abhishek Gupta", "Amruta Parulekar", "Sameep Chattopadhyay", "Preethi Jyothi"], "abstract": "Automatic speech recognition (ASR) for low- resource languages remains a challenge due to the scarcity of labeled training data. Parameter- efficient fine-tuning and text-only adaptation are two popular methods that have been used to address such low-resource settings. In this work, we investigate how these techniques can be effectively combined using a multi- lingual multimodal model like SeamlessM4T. Multimodal models are able to leverage un- labeled text via text-only adaptation with fur- ther parameter-efficient ASR fine-tuning, thus boosting ASR performance. We also show cross-lingual transfer from a high-resource lan- guage, achieving up to a relative 17% WER reduction over a baseline in a zero-shot setting without any labeled speech.", "sections": [{"title": "1 Introduction", "content": "Across the languages of the world, the automa- tion of various speech and text tasks has led to the creation of massive multilingual datasets such as Multilingual LibriSpeech (Pratap et al., 2020), that contain speech, text, and other metadata for a number of different languages. This large-scale col- lection has catalyzed the emergence of large multi- lingual automatic speech recognition (ASR) mod- els (Yadav and Sitaram, 2022), which utilize the structural similarities between different languages to learn language-invariant features and boost accu- racy. Subsequently, multimodal multilingual mod- els, such as M3P (Ni et al., 2021), that bridge the gap between speech and text using joint representa- tion spaces, have also emerged. These models are trained using large amounts of multilingual speech and text data.\nHowever, less-spoken languages, especially those from developing countries, do not have such large data corpora available (Magueresse et al., 2020), thus hurting model performance for extremely low-resource languages (Chang et al., 2023). Thus, creating targeted models for severely low-resource languages has become crucial. One efficient way to do this is by adapting existing mod- els to the target language using limited amounts of labeled data. Such adaptation has to be done carefully so as to not overfit to the target language characteristics.\nParameter-efficient fine-tuning (PEFT) (Han et al., 2024) techniques have gained wide a\u0441\u0441\u0435\u0440- tance where only relevant parts of a model are identified and fine-tuned for a specific downstream task. Text-only adaptation is another sub-area that is gaining popularity for low-resource ASR (Bataev et al., 2023; Vuong et al., 2023). Multimodal mod- els have training pathways for both speech and text data, offering a good framework to combine both approaches. Multilingual models, on the other hand, allow for cross-lingual transfer (Khare et al., 2021), i.e., using a higher resource language to im- prove performance on a lower resource language.\nIn this work, we have leveraged the multimodal nature of Meta's SeamlessM4T (Communication et al., 2023) to explore the benefits of speech-based adapter fine-tuning and text-only adaptation. These techniques have been used both in isolation and in combination to identify the best strategy to improve low-resource ASR for a number of Indic languages. We have also exploited the multilingual nature of the model to use higher-resource languages to im- prove low-resource ASR. Thus, our main contri- butions include: (a) identifying how to combine speech-based parameter-efficient fine-tuning and text-only adaptation to boost low-resource ASR, (b) identifying a cross-lingual transfer technique that can give more than 17% relative reduction in WER for a low-resource language without using any speech of that language, (c) the use of small amounts of available data to boost the performance of SeamlessM4T (Communication et al., 2023) on six Indic languages, Bengali, Gujarati, Kannada, Maithili, Malayalam and Odia."}, {"title": "2 Related Work", "content": "One of the key challenges in current ASR research is enabling systems to handle multilingual inputs (Yadav and Sitaram, 2022; Kannan et al., 2019) while minimizing resource requirements in terms of training, inference, and storage costs. Currently, the most popular paradigm using multilingual mod- els are to initially pre-train the models in a self- supervised manner on a large multilingual dataset (Babu et al., 2021) before being fine-tuned on a set of target languages (Toshniwal et al., 2018; Bai et al., 2022). A general way of performing such model fine-tuning is by updating all the weights or some specific model components while training. These kinds of methods are parameter inefficient and often cause catastrophic forgetting (Kessler et al., 2021), for all non-target languages. Also, training and storage costs for such methods in- crease linearly with both the model size and the number of languages.\nTo mitigate these limitations, recent literature on NLP has introduced several parameter-efficient fine-tuning methods (Xu et al., 2023; Tomanek et al., 2021; Hu et al., 2021), often involving train- able modules called adapters (Houlsby et al., 2019), whose weights are updated while freezing the orig- inal backbone. Significant efforts are being made to develop better adapter architectures and efficient training methods (Yu et al., 2023) to utilize con- trastive learning (Zhang and R\u00e9, 2024) and meta- learning (Hou et al., 2021). These modules can also be used to adapt multilingual ASR models for a low-resource setting, with Simadapter (Hou et al., 2022) being one of the first models to utilize adapters to leverage cross-lingual features.\nIn the context of speech recognition, a low- resource setting could refer to any scenario with insufficient training data. This includes challenges such as recognizing atypical speech (Tomanek et al., 2021) or processing less commonly spoken languages. A recent work (Mainzinger and Levow, 2024) demonstrated the benefits of using adapters for very low-resource languages with less than five hours of training data. For the low-resource situ- ation, task- or language-specific adapter modules showcase superior performance (Hu et al., 2024) compared to fine-tuning the model components, but even such approaches are constrained by inherent limitations of the base model.\nOver the past few years, considerable effort has gone into developing multilingual ASR founda- tional models with more generalizable features. These models offer a stronger starting point for low-resource adaptations and enable the use of cross-lingual transfer learning. The exponential growth in computing power has led to the creation of increasingly large language models, which are now used for a wide range of tasks, including as backbones for multimodal ASR models (Ruben- stein et al., 2023; Zhang et al., 2023; Chang et al., 2023). For such models, the foundational backbone is expanded using audio tokens generated using techniques like wav2vec (Schneider et al., 2019) and Hubert (Hsu et al., 2021) in order to learn a joint representation in a multimodal space; the to- ken vocabulary is expanded to encompass both text and audio. Note that models with joint multimodal representations are not only useful for ASR but can also be integrated with a vocoder for TTS or conversational chatbots (Zhang et al., 2023).\nMultimodal models can be trained with joint text-"}, {"title": "3 Methodology", "content": "audio tasks through self-supervision with masked language modeling and denoising objectives; fur- ther fine-tuning is often done with ASR and speech- to-text or speech-to-speech translation tasks. One of the most recent examples of such a multilingual multimodal model has been SeamlessM4T (Communication et al., 2023) by Meta AI, which is built upon the NLLB (Team et al., 2022a) backbone and can process speech and text inputs from nearly 100 languages. An implicit advantage of using such multimodal models for low-resource ASR is the ability to benefit from text-only learning for shared parameters. In most cases, there is significantly more text data available than speech data. Thus, the capability to leverage text-only adaptation for ASR models can be highly advantageous in these scenarios.\nWhile there is a lot of prior work in the domain of text-only adaptation for ASR (Vuong et al., 2023; Bataev et al., 2023; Chen et al., 2023; Mittal et al., 2023), and there has been some work on a com- parative analysis of various fine-tuning strategies for low-resource ASR (Liu et al., 2024), to the best of our knowledge, our work is the first to explore them for multilingual multimodal models.\nIn this work, we leverage a combination of parameter-efficient adaptation, unlabeled textual data, and minimal amounts of transcribed speech to improve ASR performance in low-resource lan- guages using multilingual multimodal models. Figure 1 demonstrates the overall workflow of our proposed pipeline."}, {"title": "3.1 Multimodal base model: SeamlessM4T", "content": "We use SeamlessM4T (Communication et al., 2023) as our base model for all our experiments. SeamlessM4T, i.e., Massively Multilingual & Mul- timodal Machine Translation, is a versatile end-to- end model that provides support for multiple tasks, including speech-to-speech translation, speech-to- text translation, text-to-speech translation, text-to- text translation, and automatic speech recognition for up to 100 languages. The model has been trained using over a million hours of unlabeled speech in a self-supervised manner, along with more than 400K hours of human and machine- labeled audio. It supports 96 different languages for input speech and text, as well as output text, and can generate speech in 35 languages.\nThe SeamlessM4T model architecture is inspired by UnitY (Inaguma et al., 2023), a two-pass mod- eling framework that, unlike cascaded models, can be jointly optimized. The text encoder and de- coder models of SeamlessM4T are initialized by the NLLB model (Team et al., 2022b), a text-to- text translation model. To process speech inputs, the model employs the Wav2Vec-BERT 2.0 speech encoder, which is an enhancement over the origi- nal model proposed by Chung et al. (2021) with additional codebooks. The model also includes a modality adapter (Zhao et al., 2022), referred to as the length adapter, to align the speech modality with text, projecting it to a unified representation space. Lastly, the model uses a text-to-unit (T2U) component for speech generation that produces discrete speech units from the text output. These units are then transformed into audio waveforms us- ing a multilingual HiFi-GAN unit vocoder (Kong et al., 2020). There are multiple variants of the SeamlessM4T model; we have used SeamlessM4T- medium with a total of 1.2 Billion parameters.\nAlthough the entire model comprises multiple components, our analysis focuses primarily on ap- plying SeamlessM4T for multilingual ASR. The ASR pipeline of SeamlessM4T consists of the speech encoder (311M parameters), the length adapter (46M parameters), and the text decoder (201M parameters). Next, we will elaborate on parameter-efficient fine-tuning of SeamlessM4T (Section 3.2) and how we can use text-only adapta- tion within such a multimodal model (Section 3.3)."}, {"title": "3.2 Parameter-efficient Fine-tuning", "content": "The ASR components of SeamlessM4T amount to more than 500M parameters. Full fine-tuning of these components using limited amounts of labeled data for low-resource languages may re- sult in overfitting and degradation of ASR perfor- mance. To alleviate these challenges, parameter- efficient fine-tuning paradigms like the adapter framework (Houlsby et al., 2019) are very popular, especially for natural language processing tasks. Adapters have also found success in low-resource ASR tasks such as accent adaptation (Tomanek et al., 2021) and cross-lingual adaptation (Hou et al., 2022). Next, we will elaborate on the struc- ture of an existing length adapter within Seam- lessM4T and the new adapters we introduce in the encoder and decoder layers."}, {"title": "3.2.1 The Length Adapter", "content": "The length adapter in SeamlessM4T aims to bridge the gap between speech and text representations. It is inspired by the M-adapter architecture (Zhang et al., 2023) and uses a Transformer-based module to adapt speech representations to text. By com- pressing the speech sequence, the length adapter generates features tailored for multilingual speech- to-text tasks by modeling both global and local dependencies within the speech.\nThe main part of the original M-adapter archi- tecture, illustrated in Figure 2, is the Multi-head Pooled Self-Attention (MPSA) mechanism. In the original MPSA, convolutional layers pool the in- put X and are further projected to the inputs of the multi-head attention module using linear trans- formation matrices. An additional pooling is ap- plied in parallel to X and then added to the output of the attention module before being processed through a feedforward network. These processes together generate a lower dimensional representa- tion of X, denoted by X as the current layer output, addressing any length mismatches between embed- dings from different modalities. Unlike the original M-adapter architecture with independent pooling modules for the multi-head attention inputs, the length adapter utilizes a shared pooling module, generating a single X for each X to improve effi- ciency. More formally, given an input sequence X \u2208 \\mathbb{R}^{L \\times D}, where L is the sequence length and D is the embedding dimension, the MPSA mecha- nism starts by applying shared pooling to the input X to obtain X \u2208 \\mathbb{R}^{L' \\times D}. This pooling operation is performed using a 1D convolutional layer with ker- nel size k, stride s, and padding p. Subsequently, X is linearly projected into the query, key, and value matrices, denoted as Q, K, and V, respectively.\n \\begin{aligned}\nX &= \\text{SharedPooling}(X) \\\\\nQ &= XW^Q, \\quad \\text{where} \\ Q \\in \\mathbb{R}^{L'\\times D},  \\\\ \nK &= XW^K, \\quad \\text{where} \\ K \\in \\mathbb{R}^{L'\\times D}, \\\\ \nV &= XW^V, \\quad \\text{where} \\ V \\in \\mathbb{R}^{L' \\times D} \\\\\n\\end{aligned} \nwhere the new sequence length L' is given by:\n\\[ L'=\\lfloor \\frac{L + 2p - k}{s} \\rfloor + 1. \\]"}, {"title": "3.2.2 Encoder and Decoder Adapters", "content": "We hypothesize that the length adapter module could potentially learn prosodic characteristics of languages, such as phoneme durations, by map- ping speech embeddings which include both segmental and suprasegmental information to text embeddings that contain only content infor- mation. Learning certain prosodic characteristics like durations can be particularly beneficial for ex-tremely low-resource languages that lack sufficient data for learning fine-grained contextual and syn- tactical information.\nIn addition to the pre-existing length adapter (Figure 2) in the SeamlessM4T architecture, we in-serted additional trainable adapter layers within the encoder and decoder modules to adapt this mul-tilingual model for low-resource languages. The adapter modules, following the architecture pro-posed in (Houlsby et al., 2019), initially project the original D\u2081-dimensional features into an inter-mediate space of dimension D2. A non-linearity, specifically GeLU (Hendrycks and Gimpel, 2023) in our implementation, is then applied, after which the features are projected back to the original D1 dimensions. To adjust the number of parameters for these adapters, we can change the intermediate dimension D2. By decreasing the value of D2, the number of trainable parameters in the adapters is reduced accordingly.\nIn our current experimental setup, we have in- serted adapters after every Conformer layer in the encoders and after every Transformer layer in the text decoder. By setting the intermediate dimen- sion D2 to one-fourth of D\u2081 for all adapters, we introduce 6 million new trainable parameters each in the encoder and decoder modules."}, {"title": "3.3 Text-only Adaptation", "content": "The text decoder in the SeamlessM4T model is shared between the ASR pipeline and the text-to- text translation pipeline, allowing it to be trained for both tasks. This shared component in mul- timodal models possesses the ability to transfer knowledge from one task to another, thereby simul- taneously enhancing the performance of multiple tasks. We hypothesize that we can improve the ASR performance for a target language by fine- tuning the text decoder adapters via text-to-text translation into that language. This allows us to per- form a purely text-only fine-tuning of ASR models and is especially beneficial for languages where speech data is scarce. With the latest advancements in NLP, the quality of machine-translation models has greatly improved, allowing these models to be utilized to augment the existing parallel text using machine-translated text for these languages.\nIn our text-only fine-tuning experiments, we fine- tuned the decoder adapters on an English-to-target language translation task to help them learn the relevant syntactical features for the target language."}, {"title": "4 Experimental Setup", "content": null}, {"title": "4.1 Dataset", "content": "The IndicVoices dataset (Javed et al., 2024) was utilized for all our experiments. This dataset is a multilingual, multi-speaker collection of natural and spontaneous speech in 22 Indian languages. It comprises 9% read speech, 74% extempore speech, and 17% conversational speech. Among these lan- guages, Maithili is classified as a zero-shot lan- guage for SeamlessM4T, while Bengali is the sole high-resource Indic language. The remaining lan- guages are categorized as low-resource languages for the model (Communication et al., 2023). One of the main reasons for using this dataset is that it is among the most comprehensive open-source, multilingual speech datasets for Indic languages covering many low-resource languages and one of the few published after the release of SeamlessM4T, ensuring there is no data leakage between the eval- uation sets and the SeamlessM4T training data."}, {"title": "4.1.1 Transcribed Speech Data", "content": "The speech data and the corresponding transcripts from the Indic Voices dataset were used for the ASR fine-tuning experiments. The dataset, primarily consisting of extempore speech recorded under nat- ural conditions, is characterized by a significant amount of noise and includes occasional disfluen- cies. For each language, 5 hours of speech were selected for the training set, sourced from an av- erage of 336 speakers, to simulate an extremely low-resource setting. On average, each of the test and validation sets had 1 hour of speech by 68 and 206 speakers respectively. The out-of-vocabulary (OOV) rate of the test set was calculated to de- termine the amount of test-train domain overlap in the data. The OOV rates for Gujarati, Bengali, Kannada, Maithili, Malayalam, and Odia test sets were 39%, 35%, 58%, 41%, 53%, and 37%, re- spectively, averaging to an OOV of 43.87% on the test sets, further demonstrating the challenging na- ture of the task."}, {"title": "4.1.2 Text-only Data", "content": "The IndicTrans2 (Gala et al., 2023) model was used to translate all the transcriptions present in the IndicVoices dataset to obtain parallel English-X text. Another set of parallel text data was created by using only the transcriptions of the 5-hour speech data in the training set for every language. For Ben- gali, Gujarati, Kannada, Maithili, Malayalam, and"}, {"title": "4.1.3 Implementation Details", "content": "Odia, the number of tokens in the 5-hour text sets were 40k, 43k, 30k, 42k, 34k, and 34k, respectively, while those in the large text set were 785k, 118k, 297k, 834k, 398k and 503k respectively. Thus, on average, each of the larger text data sets contained 489000 tokens for every language, while each of the smaller sets contained only 37261 tokens.\nThe SeamlessM4T model comprises a speech en- coder with 12 Conformer blocks and a text de- coder with 12 Transformer blocks, with a model dimension D\u2081 = 1024. Two D2 configurations were tested: D2 = 256 (about 500K parameters per adapter layer, totaling 6M parameters) and D2 = 2048 (matching adapter parameters with the length adapter, totaling 50M parameters). Text- only adaptation needed roughly 200 epochs of fine- tuning, while ASR fine-tuning required up to 40 epochs. All experiments were performed with a learning rate of 5 \u00d7 10-6 and a batch size of 16."}, {"title": "5 Experiments and Results", "content": null}, {"title": "5.1 System A: Pure ASR Fine-tuning", "content": "We use the name System A to refer to the stan- dard speech-to-text fine-tuning of SeamlessM4T using labeled speech and the ASR objective. The results of this experimental setup are summarized in Table 1. From the results, it is evident that fine- tuning the length adapter requires fewer parameters while providing similar benefits to text decoder fine- tuning across both metrics. Additionally, the ASR fine-tuning of the speech encoder proves to be sig- nificantly beneficial, although it involves training a substantially larger number of parameters.\nIn order to reduce the computational and storage requirements, the fine-tuning was substituted with language-specific adaptations, wherein adapters were introduced in the encoder and decoder, and these were fine-tuned in various combinations us- ing transcribed speech data while freezing the base model. Table 3 depicts the results for the adapta- tions on System A. The results demonstrate that larger encoder adapters with 50M parameters are the most beneficial in enhancing the ASR perfor- mance, achieving WER and CER close to full fine-tuning of the model and the adapters while reduc- ing trainable parameters by 90%. Additionally, Table 3 indicates that for the same number of train- able parameters, speech-based training of encoder adapters performs much better than that of decoder adapters. The performance of the length adapter fine-tuning surpasses that of the decoder adapters but falls short compared to the encoder adapters."}, {"title": "5.2 System T-A: Using Text-only Adaptation", "content": "The parallel English-target language text data gen- erated by translating the transcripts of IndicVoices data was used to fine-tune the decoder adapters on an English-to-target language MT objective. Table 2 shows the ASR word error rates (WERs) with the complete transcription data and a smaller 5-hour text data subset (described in Section 4.1) to check the comparative benefits of text-only adaptation, without any ASR fine-tuning. For most languages, using the larger text corpus led to better perfor- mance. However, the smaller parallel dataset, with significantly fewer tokens, demonstrated compa- rable performance to that of the complete corpus. This suggests that text-only adaptation can be ef- fective for multilingual multimodal models, even with very limited amounts of data."}, {"title": "5.3 Cross-lingual Transfer", "content": "We hypothesize that the length adapter could cap- ture content-agnostic prosodic characteristics of a language without overfitting on its syntax. Conse- quently, fine-tuning this adapter using data from a closely related high-resource language might en- hance the model's predictions for a low-resource target language. The target languages chosen for this experiment were Maithili and Odia, catego- rized as zero-shot and low-resource languages for SeamlessM4T, respectively. Bengali, a language belonging to the same Eastern Indo-Aryan lan- guage family (Eberhard et al., 2020) as Maithili and Odia, was selected as the high-resource pivot. To further justify our choice of the pivot, we examined the genetic distance between the pivot and target languages using lang2vec (Malaviya et al., 2017). Genetic distance (Bjerva et al., 2019) refers to the measure of divergence between languages based on their evolutionary relationship. The results showed that Bengali was quantifiably close to both target languages. The labeled Bengali speech was used to fine-tune the length adapter and encoder adapters individually and in combination. Separately, Kan- nada speech was used for length adapter fine-tuning to check if any benefits are obtained with an un- related language. We also combined this with the text-only adaptation of target language text data to check if both approaches complement each other. Table 4 summarizes the performance of the cross- lingual systems with both the target low-resource languages. Length adapter fine-tuning outperforms encoder adaptation for cross-lingual transfer."}, {"title": "6 Discussion", "content": "We observe that for decoder adapters, it is more beneficial to use text-only adaptation compared to ASR-based training; the latter's benefit is mainly derived via the encoder layers. This emphasizes the role played by text data in improving the decoder's ability to enhance the internal language model of the ASR system. We also observed that 5-hour text data adaptation, having on average 92% fewer tokens than the full text, performed comparably to full-text data adaptation. This indicates that even limited amounts of text data can significantly boost ASR.\nFor a given target language with labeled speech, we found that fine-tuning the encoder adapters was the most accurate and parameter-efficient strategy. However, for cross-lingual zero-shot settings with no labeled data in a target language, we found it beneficial to fine-tune the length adapter with data in a related language rather than fine-tuning en- coder adapters; the latter led to overfitting to the related language rather than enabling transfer to the target language. Text-based adaptation led to further improvements in the cross-lingual setting, indicating that even without speech data, ASR for low-resource languages can be improved by fine- tuning the length adapter. Lastly, a curious observa- tion was that higher cross-lingual transfer was seen for genetically closer language pairs, with Odia- Bengali outperforming Maithili-Bengali in terms of relative WER reduction."}, {"title": "7 Conclusion", "content": "In this work, we explored the combination of parameter-efficient ASR fine-tuning and text-only adaptation techniques to enhance ASR for low- resource Indic languages using a multi-lingual multi-modal base model (SeamlessM4T). We find that a limited amount of text data was sufficient for adaptation, text-based adaptation was superior to ASR fine-tuning of decoder adapters, and encoder adapters were most effective in limited speech set- tings. In cross-lingual settings, however, the length adapter (and not the encoder adapter) was most successful, and text adaptation was additionally beneficial. Future work will focus on developing a better understanding of the interplay between dif- ferent adapters within multimodal models."}]}