{"title": "A SCALABLE COMMUNICATION PROTOCOL FOR NETWORKS OF LARGE LANGUAGE MODELS", "authors": ["Samuele Marro", "Emanuele La Malfa", "Jesse Wright", "Guohao Li", "Nigel Shadbolt", "Michael Wooldridge", "Philip Torr"], "abstract": "Communication is a prerequisite for collaboration. When scaling networks of AI- powered agents, communication must be versatile, efficient, and portable. These requisites, which we refer to as the Agent Communication Trilemma, are hard to achieve in large networks of agents. We introduce Agora, a meta protocol that leverages existing communication standards to make LLM-powered agents solve complex problems efficiently. In Agora, agents typically use standardised routines for frequent communications, natural language for rare communications, and LLM-written routines for everything in between. Agora sidesteps the Agent Communication Trilemma and robustly handles changes in interfaces and mem- bers, allowing unprecedented scalability with full decentralisation and minimal involvement of human beings. On large Agora networks, we observe the emer- gence of self-organising, fully automated protocols that achieve complex goals without human intervention.", "sections": [{"title": "1 INTRODUCTION", "content": "Human language evolved primarily for communication purposes (Fedorenko et al., 2024). Despite its inherent ambiguity, natural language provides great versatility and allows humans and machines to collaborate and achieve complex goals that they otherwise could not (Russell & Norvig, 2016).\nDecades of literature in computer science explored how to foster collaboration between agents mod- elled as programs (Wooldridge & Jennings, 1995; Gilbert, 2019). Several research papers design networks of agents to solve complex problems by leveraging each model's specialisation, the so- called rule-based agents paradigm (Wooldridge, 2009). Despite its influence, such a paradigm faces two major limitations: agents hardly adapt to environmental changes and require structured data that limits their versatility (Gilbert & Terna, 2000).\nWith the advent of Large Language Models (LLM) (Vaswani et al., 2017; Brown et al., 2020), there has been a resurgent interest in networks of collaborative agents. LLMs can solve a variety of problems (Achiam et al., 2023; Dubey et al., 2024a) expressed in natural language as they excel at following instructions (Schulman et al., 2017; Rafailov et al., 2024). LLMs also showed remarkable improvements at handling structured data such as graphs and formatted languages (Kassner et al., 2020; Collins et al., 2022; Jin et al., 2023; Lin et al., 2024).\nIn terms of performance (e.g., accuracy on classification), the literature suggests that specialised LLMs outperform general purpose models (Hu et al., 2021; Zhang et al., 2024), as well as mitigating the difficulties of handling gargantuan models and the drawbacks of data and model centralisation (Song et al., 2023).\nThus, we hypothesise that:"}, {"title": "Hypothesis", "content": "A network of heterogeneous LLMs can automate various complex tasks with nearly no hu- man supervision via specialised and efficient protocols.\nHowever, networks of LLM-powered agents face three key challenges that make communication at scale significantly more difficult:\n\u2022 LLMs are heterogeneous: different LLMs have different architectures, makers, capabilities and usage policies.\n\u2022 LLMs are (mostly) general-purpose tools: enumerating and standardising each task they can perform is infeasible.\n\u2022 LLMs are expensive: the computational footprint and inference time of \"small\" LLMs dwarfs that of comparable, specialised APIs.\nScalable communication between heterogeneous LLMs must be versatile, i.e., capable of handling a variety of use cases, efficient, i.e., requiring the least computational effort, and portable, i.e., supporting the protocol should require the least human effort possible. The above-mentioned issues constitute the Agent Communication Trilemma, which we expand in Section 3.\nIn light of this, the aim of this paper is the following:"}, {"title": "Key Contribution", "content": "We design and implement a communication protocol between heterogeneous LLM-powered agents and assess its feasibility and scalability for solving high-order tasks.\nWe sidestep the Trilemma with Agora, a meta protocol that relies on the dual use of structured data for frequent communications and natural language for infrequent ones. With Agora, we instantiate large networks of LLM-powered agents that solve complex tasks autonomously by leveraging effi- cient communications schemas. In such networks, we observe agents develop an emergent fully automated protocol to solve a complex task starting from an instruction expressed in natural language. We believe that this observation can serve as a basis to renew interest in emergent proto- cols/languages in large networks of LLMs (Lazaridou et al., 2018; Chaabouni et al., 2019; Lazaridou & Baroni, 2020; Chaabouni et al., 2022).\nThe paper is structured as follows. We first outline the key challenges that constitute the Agent Communication Trilemma (Section 3); we then detail how Agora addresses the Trilemma and serves as a communication protocol for networks of LLMs (Section 4). Finally, in Section 5, we provide two fully functional demos2: the former, with two agents, to clarify Agora's operating principles; the latter, with 100, to prove Agora's scalability and show the emergence of self-organising behaviours."}, {"title": "2 RELATED WORK", "content": "Multi-agent LLMs and communication. At the time of writing, Multi-Agent-Systems of Large Language Models (MAS-LLM) have become an active area of research (Guo et al., 2024) after the upsurge of LLMs as general purpose problem solvers (Brown et al., 2020; Achiam et al., 2023; Dubey et al., 2024b). Many fields have adapted techniques from the MAS-LLM paradigm to solve problems single models fail at, including reasoning and math (Li et al., 2024), Theory of Mind (Cross et al., 2024; Li et al., 2023b), planning (Singh et al., 2024), alignment to human values (Pang et al., 2024), and simulation of games, economics, and political scenarios (Bakhtin et al., 2022; Hua et al., 2023; Wu et al., 2024a). The common intuition of these works is that by breaking a task into sub-components (Hong et al., 2023) and allocating a large number of specialised models (Li et al.,"}, {"title": "3 THE AGENT COMMUNICATION TRILEMMA", "content": "An agent is a computer system that, in an envi- ronment, is capable of autonomous actions (the so- called 'agency' (Horty, 2001)) to meet its design objec- tive (Wooldridge & Jennings, 1995; Wooldridge, 2009, p. 15). Just as humans must negotiate and cooperate to achieve shared goals, so too must agents within multi- agent systems (Wooldridge, 2009, p. 24-25). However, when designing communication protocols for heteroge- neous networks (i.e., networks where agents have differ- ent architectures, capabilities and design constraints), we run into difficulties when attempting to optimise for three properties at the same time:\n\u2022 Versatility: communication between agents should support a wide variety of messages, both in terms of content and format;\n\u2022 Efficiency: the computational cost of running an agent and networking cost of communi- cation should be minimal;\n\u2022 Portability: supporting the communication protocol should require the least implementa- tion effort by the largest number of agents involved.\nWe name the trade-off between such properties the Agent Communication Trilemma, which is illustrated in Figure 1. In the next sections, we will discuss how an LLM-powered communication protocol can trade off versatility, efficiency, and portability."}, {"title": "3.1 VERSATILE VS. PORTABLE COMMUNICATION", "content": "In networks of agents, versatility and portability are at tension for two fundamental reasons (Oliv\u00e9, 2007). A prerequisite for two agents who communicate is (1) a shared conceptual understanding of the topic on which they communicate. For instance, two agents can communicate about the weather if they both 'know' what it means to be sunny, rainy and overcast. For example, they should share a similar notion of describing and measuring temperature (e.g., in degrees Celsius). In addition, (2) agents must encode and decode messages in a way that is intelligible for both. Continuing the weather example, if two agents exchange data using JSON objects, both the sender and the receiver must know the syntax (e.g., the keys of a JSON object, such as temperature) and the"}, {"title": "3.2 EFFICIENT VS. VERSATILE AND PORTABLE COMMUNICATION", "content": "As previously mentioned, rule-based agents excel at the tasks they are designed to solve but hardly adapt to new environments. Decades of research in reinforcement learning (Sutton, 2018) and then in deep reinforcement learning (Arulkumaran et al., 2017; Henderson et al., 2018), introduced a paradigm where agents learn to optimise their reward as proxy of the task we want them to solve. Agentic-LLMs, i.e., multi-agent systems powered by language models, is a recent paradigm for machine-to-machine communication that relies mostly on their proficiency at handling natural lan- guage and following instructions (Li et al., 2023a).\nNatural language is highly expressive, making it a suitable choice for versatile communication (Rus- sell & Norvig, 2016). Additionally, LLMs trained on massive corpora seem to develop an implicit understanding of various concepts that abstracts and makes communication independent from their internal architecture. Moreover, LLMs can integrate external tools, write code and invoke APIs with relatively little or no training (Schick et al., 2024), since the only requirement is a natural-language description of the tool and its parameters.\nConversely, natural language as a communication medium has two major drawbacks. While en- gineering and hardware improvements (Dubey et al., 2024b) mitigate costs over time, the compu- tational requirements of invoking an LLM dwarf those of comparable APIs, representing a major bottleneck for scaling networks of LLMs. On the other hand, using closed-source pay-per-usage LLMs hosted by third parties is expensive and raises concerns in terms of replicability of the re- sults (La Malfa et al., 2023). Additionally, natural language is inherently ambiguous: while LLMs have a certain degree of \"common sense\" to fulfil requests, non-determinism and natural language specifics leave space for errors that routines minimise (for instance, if someone asks for the temper- ature in Fahrenheit and the agent has a tool that returns the temperature in Celsius, the model must know that Celsius and Fahrenheit are both units of measure for temperature). These factors make LLMs and natural language more prone to errors than other alternatives like handwritten APIs.\nIn conclusion, RESTful APIs (efficient), RDF tuples (portable) and natural language (versatile) are all trade-offs in the Trilemma. While some approaches are more useful in practice than others, the fact that no communication format achieves all three properties simultaneously suggests that we need a hybrid communication protocol that leverages all of them. The next section outlines our solution."}, {"title": "4 AGORA: A COMMUNICATION PROTOCOL LAYER FOR LLMS", "content": "The key to solving the Communication Trilemma involves accepting that no single protocol can achieve optimal efficiency, portability and versatility at the same time. In this section we introduce"}, {"title": "4.1 COMMUNICATION IN (AN) AGORA", "content": "Agora introduces a machine-readable way to transfer and refer to protocols, namely the protocol documents (PDs). A PD is a plain-text description of a communication protocol.3 PDs are self- contained, implementation-agnostic, and contain everything an agent needs to support a protocol: this means that most descriptions of existing protocols, such as RFCs, are also suitable PDs. How- ever, instead of relying on a central body to assign identifiers, a PD is uniquely identified by its hash (for multiplexing).\nIn Agora, the most frequent communications have dedicated efficient routines, and the least frequent ones use inefficient but flexible LLMs and natural language. In particular:\n\u2022 When possible, frequent communications are handled through traditional protocols, for which there are standard, human-written implementations (e.g., OBP);"}, {"title": "4.2 AN EXAMPLE OF COMMUNICATION OVER AGORA", "content": "We now describe how two agents, Alice and Bob, can efficiently communicate over Agora using a PD routine, as illustrated in Figure 3. Alice initially sends a query with the hash of its corresponding PD. Bob uses the hash to determine if he has a corresponding routine. If so, he calls it and handles the communication without invoking the LLM. Otherwise, Bob handles the response with the LLM itself.\nIf Bob uses an LLM to reply several times to queries that follow a given protocol over time, to the point where using an LLM every time becomes expensive, he can use the LLM to write a routine that handles future communications.\nIf the routine fails or the communication is a one-off instance that does not require a protocol, Alice and Bob use natural language, which is again handled by the LLM. Natural language is also available to bootstrap communication between nodes that have never interacted before, as well as to negotiate new protocols. That said, the lower cost of routines and the lack of ambiguity are strong incentives for agents to prefer structured data.\nNote that PDs can be shared with other nodes in the network, which means that two agents that have never interacted before can use protocols developed by other agents."}, {"title": "4.3 AGORA AS A LAYER ZERO PROTOCOL", "content": "Figure 2 illustrates that Agora is implementation and technology agnostic. The implementation of the agents themselves (e.g., LLMs), the database used to store data (e.g., VectorDB, SQL, Mon- goDB, etc.), the language in which implementations are written (Python, Java, etc.) and the nature of tools are all abstracted.\nAt the same time, PDs can refer to other protocol documents, and since routines can call other routines, agents can build upon previous negotiations to solve more complex tasks.\nFinally, the versatility and portability of Agora make it straightforward to handle the addition or removal of a node, a change in the capabilities of a node, or a change in the goals of the network, as illustrated in the demo, Section 5.3.\nAll these factors contribute to making Agora a natural Layer Zero protocol, i.e. a foundation layer, for higher-order communication and collaboration between LLMs. We hope our protocol can fuel theoretical and applied research on complex protocols, negotiation schemes, and consensus algo- rithms in large networks of LLMs."}, {"title": "5 AGORA IN PRACTICE", "content": "We implement and showcase two scenarios where Agora can be applied. The former, with two agents whose objective is to exchange some data; the latter, with 100, to test Agora scalability and the capacity of LLM-powered agents to autonomously coordinate in complex scenarios. For space reasons, the scenarios are further expanded in Appendices C and D; here, we instead focus on their functionalities and the key observations we drew in terms of efficiency/versatility/portability, reduction of costs, scalability and emergent behaviours of fully automated networks of LLMs."}, {"title": "5.1 IMPLEMENTATION DETAILS", "content": "The design of Agora for our working demos follows three key principles:\n\u2022 Minimality. Agora enforces the basic standards that allow for efficient negotiation and use of protocols, leaving everything else to PDs or other higher-order standards;\n\u2022 Decentralisation. Agora does not rely on central authorities, with any collection of nodes being able to use Agora independently;\n\u2022 Full backward compatibility. Agora supports existing communication protocols and schemas such as OpenAPI and JSON-Schema.\nFrom a practical point of view, Agora uses HTTPS as base communication layer and JSON as format to exchange metadata. When sending a message in a given protocol, an agent sends a JSON document with three keys: the protocol hash, the body of the request formatted according to the protocol, and a non-empty list of sources from which the protocol can be downloaded. The receiver downloads the PD from its preferred source and, upon checking that the hash matches, stores it for future uses. This hash-based identification system ensures that any node can reference any PD without relying on a central authority to assign identifiers. Where PDs are stored is entirely up to the agents; aside from regular cloud storage, hash-based indexing makes decentralised storage options (such as IPFS Benet (2014)) viable. Additionally, since essentially all protocols can be stored as PDs, Agora has full backwards compatibility with existing protocols (although human programmers are encouraged to provide existing, standardised implementations instead of having the LLM re- implement them from scratch).\nTo simplify negotiation, an agent can expose an endpoint with a list of supported protocols: a poten- tial sender can thus compare the list with its own to automatically determine if there is a common protocol. The sender can also use a potentially unsupported protocol, although the receiver can choose to reject it by returning a predefined error message."}, {"title": "5.2 DEMO: RETRIEVING WEATHER DATA", "content": "Consider two agents, Alice and Bob. Alice is a Llama-3-405B (Dubey et al., 2024b) powered agent managing the bookings of a guided tour service in London.5 Bob is a GPT-40 (Achiam et al., 2023) agent for weather service that provides weather forecasts for a given date and location. As part of the user interaction loop, Alice notifies the user if heavy raining is expected on a booked date.\nTo check the weather, she initially uses her LLM to send a natural language query to Bob (phase A1):\nWhat is the weather forecast for London, UK on 2024-09-27?\nBob uses his Toolformer LLM (Schick et al., 2024) to query his database (phase B1) and returns a natural language reply (phase B2):\nThe weather forecast for London, UK, on 2024-09-27 is as follows:\n\"Rainy, 11 degrees Celsius, with a precipitation of 12 mm.\"\nOver time, the cost of invoking an LLM for phases A1 and B2 dominate all the other costs; Al- ice and Bob thus decide to develop a protocol. Alice checks if Bob already supports a suitable protocol but finds none. Therefore, she decides to negotiate a protocol with Bob. After a few rounds of negotiation, Alice and Bob agree on the following protocol: Alice sends a JSON docu- ment with two fields, location and date, and Bob replies with a JSON document containing three fields, namely temperature (in degrees Celsius), precipitation (in millimetres), and weatherCondition (one of \"sunny\", \"cloudy\u201d, \u201crainy\u201d and \u201csnowy\u201d). From there on, Alice specifies the protocol hash when performing a query. An example of exchanged message (excluding Agora's metadata) is:\n{\"location\": \"London, UK\", \"date\":\"2024-09-27\"}\nBoth Alice and Bob independently decide to write a routine to handle their side of the communica- tion. From now on, Alice and Bob do not need to use the LLM to transmit traffic data: a routine now automates phases A1, B1 and B2 and leverages the costs of invoking the respective LLMs.\nA cost analysis. In our demo, negotiating the protocol and implementing the routines cost 0.043 USD in API calls, compared to an average cost of 0.020 USD for a natural-language exchange. This means that, as long as Alice and Bob use the agreed-upon protocol more than twice, Agora reduces the overall cost. Please refer to Appendix C for a transcription of the negotiation process and the final protocol.\nAs a final note, we stress that the entire communication happened without human intervention. Additionally, should Bob become unavailable, Alice can simply reuse the PD with a new node that may use a different LLM/database/technology stack."}, {"title": "5.3 DEMO: A NETWORK OF 100 AGENTS", "content": "We now show the scaling capabilities and emergent behaviours of Agora by considering a net- work of 100 LLM-powered agents. In particular, we scale the number of agents, which, as posited in Chaabouni et al. (2022), is a requisite for the emergence of complex behaviours in multi-agent networks.\nWe design a network of 85 assistant agents interacting with 15 server agents, all powered by LLMs. The server agents offer various services, such as booking hotel rooms, calling taxis, ordering food, etc. An example of a sub-network for food delivery is sketched in Figure 4, left. Their specialisation is handled via prompting, as in Deshpande et al. (2023); Joshi et al. (2023); Li et al. (2023a). As part of their workflow, server agents must interact with several tools and databases; additionally, some servers need to interact with other servers to complete assistants' requests (e.g., taxi services use the traffic data agent to adjust estimated fares for a run). We bootstrap the network by leveraging the underlying communication layer (as described in Section 4 and Figure 2) and inform the nodes of which URLs correspond to which node, as well as manually creating the connection links between agents (e.g. the Taxi Service server knows that the server on port 5007 is a traffic server, but it does not know how to communicate with it and what information it requires);\nTo showcase the portability of Agora throughout the network, we use different database technologies (SQL and MongoDB) and different LLMs, both open- and closed-source (GPT-40, Llama-3-405B, and Gemini 1.5 Pro (Reid et al., 2024)). We then generate 1000 random queries, which range from simple ones, such as requesting today's weather, to more complex ones, like booking rooms in ski resorts, buying tickets for movies, ordering one of each dish from a menu, and so on. For each query, assistants receive a JSON document (which represents the task data) and are tasked with fulfilling the request and returning a parsed response that follows a given schema. Queries are distributed among assistants following a Pareto distribution, to simulate some assistants sending significantly more requests than others. Each node can also read and share PDs to one of three protocol databases. Overall, these design decisions result in a very heterogeneous network, testing the limits of Agora. Refer to Appendix D for further implementation details.\nEmergent protocols in large networks. Once the connections are established and the networks can send and receive messages, we observe several noteworthy behaviours. As PDs are progressively shared between agents (see Figure 5b), we observe the emergence of a decentralised consensus on the appropriate protocols for a given task. An example of this behaviour involves ordering food from restaurants: an agent queries another to request food to be delivered to a certain address. The restaurant agent requests a delivery driver from a food delivery service, who, in turn, checks with the traffic data agent to see if the traffic is smooth enough to fulfil the delivery. None of the agents know each other's roles and the protocols involved beyond their immediate communication. Still, the interaction of the various agents creates an automated workflow that takes care of everything."}, {"title": "6 CONCLUSIONS", "content": "In this paper, we introduced Agora, a meta protocol that sidesteps the Agent Communication Trilemma by using a mix of natural language and structured protocols. We showed that Agora agents can negotiate, implement and use protocols, creating self-organising networks that solve complex tasks. Additionally, we demonstrated the scalability of Agora by testing a 100-agent demo and achieving a five-fold reduction in costs compared to natural language-only communication. Our results showcase the power of negotiation as a basis for efficient, scalable, and decentralised agent networks. As LLMs continue to improve and as interactions between them increase, LLM-powered agent networks have the potential to surpass the scale limitations of single LLMs. Developing frameworks and protocols that enable decentralised, flexible and efficient communication, either through Agora or other technologies, can lay the foundations for a future where complex activities are partially, if not fully, automated by LLMs."}, {"title": "B.1 TRANSACTIONS", "content": "An Agora transaction operates as follows. Suppose that an agent, Alice, is trying to communicate with another agent Bob:\n\u2022 Alice sends to Bob over HTTPS a JSON document containing three fields:\n protocolHash: The hash of the protocol document. If natural language is used, then the value of protocolHash is null;\n protocolSources: A list of URIs where the protocol document can be found. Must be empty if protocolHash is null and non-empty otherwise;\n body: A string containing the body of the request as specified by the given protocol.\n\u2022 If Bob does not have the protocol document, he fetches it (either from the sources provided by Alice or from another repository);\n\u2022 If Bob is unable to use the protocol, he returns a JSON document with one field, namely status, which is equal to \"rejected\";\n\u2022 Otherwise, Bob computes the response using the LLM, routines, or a combination of both;\n\u2022 Bob sends as response a JSON document with the following fields:\n status: a string indicating the status of the response (can be \"success\u201d or \u201cfailure\");\n body: the response returned by the agent.\n\u2022 Note that \"status\":\"failure\" must be used only for errors that are not covered by the protocol document (e.g., the agent failing to instantiate the LLM); when the protocol prescribes how to handle an error, the agent should return \"status\":\"success\" and the correct error message as body."}, {"title": "B.2 PROTOCOL DOCUMENTS", "content": "A protocol document is identified by its SHA1 hash. Protocol documents can refer to other protocol documents by adding a preamble containing the protocol hash and at least one source for the protocol document. Further discussions concerning the optimal way to reference other protocol documents is left to future work."}, {"title": "B.3 PREFERRED PROTOCOLS", "content": "In order to signal to other agents which protocols are supported by default, an agent can expose an endpoint /.wellknown. This endpoint returns a JSON object where each key is a supported protocol hash and each value is a non-empty list of protocol sources."}, {"title": "C EXAMPLE OF NEGOTIATION PROCESS BETWEEN TWO AGENTS", "content": "We report the negotiation process between Alice and Bob for a protocol to transmit weather forecast. Note that while the LLMs used Markdown-style formatting, for the sake of readability we replaced Markdown syntax with the corresponding typeset style."}, {"title": "D.1 IMPLEMENTATION NOTES", "content": "We implemented our demo in Python, using Flask servers as basis for our agents. Each agent is either a user or a server:\n\u2022 Users receive a random task, some randomly generated data and a description of the task data (including its schema). Their objective is to execute the requested action and return a reply according to a certain schema. This allows us to generate a large number of queries without needing to handcraft them. Note that all tasks are single-round, i.e. they can be fulfilled in one round of communication;\n\u2022 Servers receive queries from other users and reply to them using a combination of three types of tools:\n Database tools, which involve connecting to a personal SQL or MongoDB database (assigned at random). Depending on the server, some databases are initialised with dummy data;\n Mock tools, which are simplifications of actual tools (e.g., for taxi service agents, the assignTaxi tool is a mock tool that, instead of actually sending a taxi to a location, mimics the request flow);\n External tools, which are tools that enable the agent to start a Agora communica- tion with a predefined server, although no information about the respective agents' schema is provided. In other words, the skiLodge agent can open a channel with the weatherService agent\nMoreover, we added three protocol databases, which are simple Flask servers that host protocol documents. The first protocol database is a peer with the second one, the latter of which is also a peer with the third protocol database (but the first protocol database is not a peer of the third one). Every 10 executed queries, one protocol databases shares its protocol documents with its peers. This simulates the propagation of protocol documents between different databases.\nPicking a Protocol Users track the number of communications with a given server about a certain type of task until it hits one of two thresholds: one for using a protocol instead of natural language and one for negotiating a protocol ex novo.\nWhen the first threshold is hit, the user invokes the LLM to check if either the server or the reference protocol database (which is randomly assigned to the user at the start of the demo) already have a suitable protocol. If there are none, the user continues using natural language until the second threshold is hit: in that case, the user begins a negotiation with the server and submits the final protocol to the reference protocol database.\nSimilarly, each server has a counter that tracks the number of natural language communications with any user since the last negotiation. Once the counter hits a threshold, the server requests a negotiation with the user, regardless of how many of the tracked queries were sent by the current user. After negotiation, the counter is reset.\nIn our demo, we set the thresholds for the user to respectively 3 and 5 communications, and the threshold for the server to 10.\nAPIs For GPT-40 and Gemini 1.5 Pro, we used respectively the OpenAI and Google API. For Llama 3 405b, we used the SambaNova API. Prices per million tokens are reported in Table 1.\nBootstrapping Quality-of-Life Extensions For the sake of bootstrapping the network, while im- plementing the demo we added two features to our nodes:\n\u2022 Providing each node with a simple protocol for multi-round communication in natural lan- guage;\n\u2022 Allowing the protocol document to include machine-readable metadata, such as the name or a short description of the protocol. This helps an agent to determine quickly which protocols, among a list of potential protocols, can be suitable for a certain task."}, {"title": "D.2 EXPERIMENTAL SETUP", "content": "Preliminary Tests We first ran a series of qualitative tests to determine which among the consid- ered LLMs (OpenAI GPT 40, Llama 3 405b, Gemini 1.5 Pro) were the most suitable for negotiation and programming. We found that while all three LLMs were capable of negotiating and implement- ing protocols, GPT 4o was the most robust, followed by the Llama 3 405b and finally Gemini 1.5 Pro. Surprisingly, the main factor behind the brittleness of Gemini 1.5 Pro was not the model's inherent performance, but rather the lack of robustness of the API itself: even with tailored retry systems, the API sometimes failed to respond in a nondeterministic manner (i.e. the same query would at times succeed and at times fail). We believe that our experience was due to temporary server issues, rather than fundamental problems with the model.\nLLM Distribution In light of our preliminary results, we manually assigned a model to each server node, following a power law consistent with our findings (9 nodes with GPT-40, 4 nodes with Llama 3 405b, 2 nodes with Gemini 1.5 Pro). User agents were instead randomly assigned one of the three LLMs with uniform distribution. Overall, the breakdown of nodes by model is:\n\u2022 GPT-40: 38 nodes (9 server nodes, 29 user nodes)\n\u2022 Llama 3 405b: 32 nodes (4 server nodes, 28 user nodes)\n\u2022 Gemini 1.5 Pro: 30 nodes (2 server nodes, 28 user nodes)\nOut of 1000 queries, 8 (representing thus 0.8% of the total query volume) failed due to Google's Gemini API not responding. This phenomenon was unrelated to the use of Agora, with 500 Internal Server errors appearing both in the Agora demo and the natural language counterfactual with roughly the same frequency.\nTask Distribution To simulate the heterogeneity in communication frequency (i.e. how some nodes tend to be more active than others), we assigned to each user a \"query budget\" (which rep- resents how many queries are sent by a given user) following a Pareto distribution with shape pa- rameter equal to 0.5, adapted so that each user has at least 1 query. The query budget is then split between three randomly chosen types of queries using a Pareto law with a shape parameter of 1 and a minimum of 1 query per type (unless the budget is less than 3 queries). See Figure 6 for a visualisation of the distribution."}, {"title": "D.3 ADDITIONAL OBSERVATIONS", "content": "Cost Breakdown The breakdown of cost by activity is as follows:\n\u2022 Natural language communication: 54%;\n\u2022 Negotiation: 6%;\n\u2022 Checking the suitability of existing protocols 22%;\n\u2022 Implementing the protocols: 17%;\nNote that negotiation, despite being the most expensive activity (since it involves several rounds of communication), actually represented the smallest contribution to the total cost, with cheaper but"}]}