{"title": "Temporal reasoning for timeline summarisation in social media", "authors": ["Jiayu Song", "Mahmud Akhter", "Dana Atzil-Slonim", "Maria Liakata"], "abstract": "This paper explores whether enhancing temporal reasoning capabilities in Large Language Models (LLMs) can improve the quality of timeline summarization, the task of summarising long texts containing sequences of events, particularly social media threads. We introduce NarrativeReason, a novel dataset focused on temporal relationships among sequential events within narratives, distinguishing it from existing temporal reasoning datasets that primarily address pair-wise event relationships. Our approach then combines temporal reasoning with timeline summarization through a knowledge distillation framework, where we first fine-tune a teacher model on temporal reasoning tasks and then distill this knowledge into a student model while simultaneously training it for the task of timeline summarization. Experimental results demonstrate that our model achieves superior performance on mental health-related timeline summarization tasks, which involve long social media threads with repetitions of events and a mix of emotions, highlighting the importance of leveraging temporal reasoning to improve timeline summarisation.", "sections": [{"title": "1 Introduction", "content": "Timeline summarization organizes and presents a sequence of events in a coherent and concise manner (Steen and Markert, 2019; Li et al., 2021; Hu et al., 2024). It involves extracting event-related timelines and then summarising them (Hu et al., 2024; Rajaby Faghihi et al., 2022). Researchers generally create event graphs (Li et al., 2021) or cluster event related timelines (Hu et al., 2024) to identify relevant events. Recent work (Song et al., 2024) has introduced the challenging task of social media timeline summarisation, especially in the context of capturing fluctuations in individuals' state of mind as reflected in posts shared online over time. In these posts, numerous events may occur without explicit timestamps, requiring contextual inference to determine their chronological sequence. Moreover, mental health-related events are not easy to identify: they can be connected to an individual's emotions, interpersonal interactions, and the entire timeline is necessary to provide enough context(Song et al., 2024). It is particularly challenging to identify events pertaining to psychological states and to extract these from posts. When generating mental health related summaries from longitudinal posts, models need to understand related events and maintain temporal consistency to make inferences. This raises the question of whether temporal reasoning can be leveraged to enhance the quality of complex timeline summaries.\nTemporal reasoning involves understanding and processing temporal information in text to deduce time-based relationships between events (Wenzel and Jatowt, 2023a). (Zhou et al., 2019) categorises temporal commonsense reasoning with respect to five aspects (duration, temporal ordering, typical time, frequency and stationarity). Subsequently, (Tan et al., 2023a; Jain et al., 2023) explore the temporal reasoning capabilities of Large Language Models (LLMs) with respect to the temporal commonsense aspects. LLMs with a strong understanding of temporal context can perform better on downstream tasks, including storytelling, natural language inference, timeline comprehension and tracking user status (Jain et al., 2023). Thus temporal commonsense reasoning is beneficial for timeline summarisation, as it helps maintain temporal consistency and the correct event order (Wenzel and Jatowt, 2023b; Vashishtha et al., 2020). Recent work (Chan et al., 2024; Feng et al., 2023; Chen et al., 2024; Zhang et al., 2024) has primarily focused on improving the temporal reasoning capabilities of LLMs, without exploring how enhancing these abilities impacts downstream tasks, such as timeline summarisation.\nHere we propose combining temporal reason-"}, {"title": "2 Related Work", "content": "Temporal reasoning for LLMs Temporal reasoning in Natural Language Processing (NLP) is the ability to understand and process information related to time within natural language text. It includes reasoning about the chronology and duration of events, and understanding and capturing different temporal relations (Vashishtha et al., 2020). Despite the impressive performace of Large Language Models (LLMs), like GPT-4, across a wide range of tasks (e.g. translation, generation), they have been shown to perform suboptimally in temporal reasoning (Wang and Zhao, 2024; Chu et al., 2024; Qiu et al., 2023). However the ability to perform temporal reasoning is crucial for understanding narratives (Nakhimovsky, 1987; Jung et al., 2011a; Cheng et al., 2013), answering questions (Bruce, 1972; Khashabi, 2019; Ning et al., 2020), and summarising events (Jung et al., 2011b; Vashishtha et al., 2020). Consequently, efforts are being made to enhance the temporal reasoning capabilities of LLMs.\n(Xing and Tsang, 2023) (Huang et al., 2024). To increase understanding of temporal expressions, Tan et al. introduced the TEMPREASON dataset which addresses three types of relations (time-time, time-event, event-event). TEMPREASON was used to fine-tune a LLM to improve the model's temporal reasoning, and they analysed the performance of different LLMs on this dataset showing it is challenging for LLMs to capture the temporal relationships between different events. Xiong et al. use the aligned timeline to help the LLM to understand temporal reasoning by translating the context into a temporal graph,identifying valid time expressions and generating related temporal knowledge. The temporal relationship between events is inferred based on specific times (e.g., the year the events occurred). However in a narrative, events often occur without a clear indication of time.\nTemporal reasoning for summarization Jung et al. developed a natural language understanding (NLU) system with a temporal reasoning component to create comprehensive timelines, applied tomedical records, presenting medical history in a more intuitive way. They found that temporal reasoning in NLU is tightly integrated into the NLP system's deep semantic analysis. Temporal reasoning can help the LM analyze temporal relationships between different events, which is very beneficial for event or news summarisation (Vashishtha et al., 2020). However, few studies explore how improvements in temporal reasoning in LLMs directly benefit downstream tasks such as text summarisation."}, {"title": "3 Methodology", "content": "Task Given an individual's timeline (a series of posts between two dates (Tsakalidis et al., 2022)), the goal is to generate an abstractive summary that reflects changes in the individual over time (Song et al., 2024).\n3.1 Proposed architecture\nTo generate timeline summaries on social media we consider two sub-processes (see Fig. 1):"}, {"title": "3.2 Teacher Model", "content": "Here the goal is to improve an LLM's temporal reasoning. There is evidence showing that fine-tuning on datasets such as TEMPLAMA may enable an LLM to memorise the most frequent answer rather develop temporal reasoning (Tan et al., 2023b). In other words, the model does not truly understand temporal relationships, such as \"before\" and \"after\". Most temporal reasoning datasets involve pairs of events rather than multiple events. However, processing a sequence of events requires more intricate reasoning, including recognising patterns, dependencies, and causal chains among multiple events. This is useful for more sophisticated tasks such as narrative comprehension and timeline summarisation, where understanding the full sequence of events is crucial.\nTo prevent the LLM from learning shortcuts and memorising the most frequent answer, we created a temporal reasoning dataset NarrativeReason, which contains relationships between a series of events based on a given story.\nEvent extraction To create NarrativeReason we restructured the NarrativeTime dataset (Rogers et al., 2024), which re-annotated TimeBankDense (Cassidy et al., 2014) with a timeline-based annotation framework, NarrativeTime. They annotated all possible temporal links (TLINKS) and the temporal relationships of all events occurring within astory. They thus provide a complete temporal relation for the sequence of events in the text, rather than just the temporal relation between event pairs. That means they provide a clear timeline for these events. However, they only used one word (e.g. \"fallen\") to represent an event, which does not fully capture the complete picture of the event (e.g. \"the Indonesian stock market has fallen by twelve percent\"). In general, an event is considered an action involving its corresponding participants, and is marked by annotating a representative expression of the event. In the context of temporal reasoning, events are usually the head of the verb phrase. (Ning et al., 2018; Pustejovsky et al., 2003).\nTherefore, we filtered the dataset TimeBankNT, keeping only annotated verbs to represent events. In order To represent a complete event, we use these verbs as triggers to extract relational triplets e.g.  to represent the event fallen. Then, we use these triplets to construct the temporal relationship of a series event. (e.g. Event"}, {"title": "Dataset construction", "content": "In a story, we consider the temporal relations for all events, and construct event-event relation question/answer pairs, which addrees the chronological relation between events, such as 'before', 'after', 'during', and 'simultaneous' (Tan et al., 2023a). Specifically, we obtain the temporal relations of all events and then use questioanswering prompts to reconstruct the dataset.\nQuestion: your task is to identify the temporal relation between EVENT A and EVENT B: based on the Story: STORY. Answer: EVENT A temporal relation (BEFORE/ AFTER/INCLUDES/ IS_INCLUDED/ SIMULTANEOUS) EVENT B. Although a single question-answer pair is used to determine the temporal relationship between a pair of events, for a complete story, we construct multiple question-answer pairs to cover the temporal relationsamong all events. This ensures that the model can clearly outline the temporal relations between all events in the story."}, {"title": "Fine-tuning task", "content": "We apply supervised fine-tuning (SFT) on a large LLM (teacher model) utilising Low-Rank Adaptation (LoRA) (Hu et al., 2022). The input and output of the model are the temporal questions and corresponding answers. Our experiments reveal that fine-tuning on this dataset brings improvements to other temporal reasoning tasks (Appendix A.2) ."}, {"title": "3.3 Student Model", "content": "After we fine-tune a teacher model on the reconstructed dataset, we transfer the temporal reasoning knowledge to a student model, while, also fine-tuning the student on a news timeline summarisation dataset (Chen et al., 2023b). Thus we aim for the student to learn temporal reasoning and use this ability to generate timeline summaries. We fine-tune Phi-3-mini-4k-instruct as a student model. We use three knowledge distillation (KD) objectives to transfer knowledge from the teacher to the student: Neuron Selectivity Transfer (NST) (Huang and Wang, 2017), transfers heatmap like spatial activation patterns of teacher neurons to student neurons; Contrastive Representation Distillation (CRD) (Tian et al., 2019), maximises the mutual information between the teacher and student representations with contrastive learning; Probabilistic Knowledge Transfer (PRT) (Passalis and Tefas, 2018), matches the probability distribution of the data in the feature space."}, {"title": "PRT", "content": "The core idea is to match the probability distribution of the data in the feature space between teacher and student models. However, learning a significantly smaller model that accurately recreates the whole geometry of a complex teacher model is often impossible. (Passalis and Tefas, 2018) uses the conditional probability distribution to describe the samples. Here, $Y_t = {y_{t_1}, y_{t_2},\u2026, y_{t_l}} \\in \\mathbb{R}^{\\text{vocab}_t}$ denote the output logits of the teacher model, and $Y_s = {y_{s_1}, y_{s_2}, ..., y_{s_l}} \\in \\mathbb{R}^{\\text{vocab}_s}$ denote the output logits of the student model, where $y_{t/s}$ is vector and $l$ is the length of sentences, $\\text{vocab}_t$ and $\\text{vocab}_t$ are the vocabulary sizes of teacher and student models respectively. We can define the conditional probability distribution for the teacher model as Eq. 1, and student model as Eq. 2.\n$P_{il|j} = \\frac{K(y_{t_i}, y_{t_j}; 2\\sigma_t^2)}{\\sum_{k=1, k\\neq j}^{L_{il}} K (y_{t_k}) y_{t_j}; 2\\sigma_t^2)}$ (1)\n$Q_{il|j} = \\frac{K(x_{s_i}, x_{s_j}; 2\\sigma_s^2)}{\\sum_{k=1, k\\neq j}^{L_{il}} K (x_{s_k}, x_{s_j}; 2\\sigma_s^2)}$ (2)\nIn these two equations (Eq. 1 and Eq. 2), we use the cosine similarity as a kernel metric allows for more robust affinity estimations.\n$K_{\\text{consine}}(y_{t_i},y_{t_j}) = \\frac{1}{2} (\\frac{y_{t_i} \\cdot y_{t_j}}{||y_{t_i}|| ||y_{t_j}||} + 1) \\in [0, 1].$"}, {"title": "NST", "content": "matches the distributions of neuron selectivity patterns between teacher and student networks. We transfer the last hidden layer $T = t(x)$ of the teacher model to the last hidden layer $S = s(x)$ of the student model given input text x. Specifically, we transfer neuron selectivity knowledge from ${t(x)}_{i=1}^{N}$ to ${s(x)}_{j=1}^{M}$, where N and M are the hidden state dimensions. Then we follow the idea in (Huang and Wang, 2017) using Maximum Mean Discrepancy (MMD) to calculate the distance between these two distributions. The basic idea of MMD is that if the moments of two"}, {"title": "CRD", "content": "maximises the lower-bound to the mutual information between the teacher and student representations. Here, we follow (Tang et al., 2021) to sample one positive pair from the joint distribution $p(S,T) = q(S,T|\\text{positive})$ for $N$ negative pairs sampled from the product of marginal $p(S)p(T) = q(S, T|\\text{negtive})$, where $N$ is the batch size. We can maximize the lower bound of mutual information by minimizing the following loss function:\n$\\mathcal{L}_{CRD}(x) = -E_{q(s,t|\\text{positive})} [\\log h(s, t)]$\n$-N \\cdot E_{q(s,t/\\text{negtive})} [\\log(1 - h(s,t))]$ (3)\nIn Eq 3, function h should satisfy $h : {s,t} \\rightarrow [0, 1]$,\n$h(s, t) = \\frac{\\exp(s^T t)}{\\exp(s^T t) + M}$,\nwhere M is the cardinality of the dataset, and we need to normalize s and t by L-2 norm before the inner product."}, {"title": "3.4 Mental Health Timeline Summary", "content": "We apply the student model to other domains, specifically to generate mental health-related summaries for timelines \u00a74.1 from social media. For the mental health summary, we use the format proposed by Song et al. (2024), which includes three key clinical concepts (diagnosis, inter- and intra- personal relations, moments of change). We follow their method to prompt the student model to generate a summary for each timeline."}, {"title": "4 Experiments", "content": "4.1 Datasets\nWe conduct experiments on three different datasets. We fine-tune the teacher model on the 'NarrativeReason' dataset. When distilling the temporal reason knowledge to the student model, we use a news timeline summarisation dataset(Chen et al., 2023a). Finally, we apply the model to a different domain, specifically generating mental health-related timeline summaries from social media.\nNarrative Reason We extracted 668 events from 30 articles, containing a total of 19,614 temporal relations between events Rogers et al. (2024), lead-"}, {"title": "4.3 Evaluation", "content": "We use the timeline summaries from (Song et al., 2024)(\u00a74.1) forautomated evaluation. We employFactual Consistency (FC), to measure whether timeline summaries are consistent with the original timelines, and Evidence Appropriateness (EA), to measure the consistency of human written summaries with their corresponding timeline summaries (Song et al., 2024).\nFor human evaluation, we worked with two clinical psychology graduate students fluent in English to evaluate 30 summaries generated from 30 timelines (TalkLfe). We follow the metrics used in (Song et al., 2024) to evaluate the summaries from the perspectives of Factual Consistency and Usefulness (general/diagnosis/inter-&Interpersonal/MOC). A factually consistent summary should accurately represent the content of the timeline and excludes any information not present in the timeline. We also evaluate the usefulness of a mental health summary on the basis of four aspects: General (contains the most clinically important information from the timeline and aids the clinician in understanding a patient's condition); Diagnosis (provides useful information about an individual's diagnosis); inter-&Interpersonal (provides helpful information about an individuals' main needs and patterns of self and other relationships); \u041c\u041e\u0421 (provides useful information about an individual's changes over time in terms of emotion/cognition and behaviour)."}, {"title": "5 Results and Discussion", "content": "5.1 Automatic evaluation\nWe conducted experiments with different combinations of KD methods. Table 1 shows experiment results. Since we did not change the output dimensions when fine-tuning LLaMA, the CRD method was not used during the KD process. Among the individual methods, PKT performed the best; however, combining PKT with NST achieved the best overall results.\nTable 2 shows the results of applying alternative fine-tuning strategies on LLMs. The results for $Phi_{\\text{temp}}$ and $Phi_{\\text{timeline}}$ on FC indicate that fine-tuning on a single dataset does not improve model performance; instead, it exacerbates hallucination"}, {"title": "5.2 Human evaluation", "content": "Based on the results of the automatic evaluation, we selected the best-performing LLaMA-Phi and Phi-Phi models. Additionally, we included the non-fine-tuned versions of LLaMA and Phi. This can help us understand in which specific aspects the model has improved with the inclusion of temporal reasoning information. The fine-tuned model shows the greatest improvement in terms of factual consistency and usefulness (general). This aligns with our findings when analyzing the summaries, where the fine-tuned model significantly reduces hallucination, as shown in Table 3. In addition, we found that the fine-tuned model did not show significant improvement in terms of Moments of Change (MOC). This could be because, in the timelines, users do not experience many emotional switches, resulting in the model-generated summaries being relatively similar."}, {"title": "5.3 Why knowledge distillation works", "content": "In this section, we analyze why the $Phi_{\\text{timeline}}$ model performed better from a representation learn-"}, {"title": "6 Conclusions", "content": "We created a dataset named NarrativeReason, which is utilised to enhance the temporal reasoning abilities of LLMs. This dataset enables models to uncover temporal relationships between events within timelines. We fine-tune a larger LLM with NarrativeReason, then distill its enhanced temporal reasoning capability to a smaller LLM while leveraging the distilled knowledge to enhanceperformance in timeline summarisation tasks. We apply the model to a different domain from the one it was trained on, namely generating mental health related timeline summaries. Our results demonstrate that our approach using knowledge distillation produces more accurate summaries while significantly reducing hallucinations. We further analyzed the internal representations of the model and found that knowledge distillation leads to better feature representations within the model that are more aligned with the task of timeline summarisation."}, {"title": "Limitations", "content": "In our work we aim to leverage temporal reasoning to enhance the performance of LLMs in timeline summarisation tasks. We apply this to social media timelines from the mental health domain to enable LLMs to recognize and analyze events chronologically, in order to capture the dynamics of user behavior and evolving mental states more effec-"}, {"title": "Ethics Statement", "content": "Ethics institutional review board (IRB) approval was obtained from the corresponding ethics board of the lead University prior to engaging in this research study. Our work involves ethical considerations around the analysis of user generated content shared on a peer support network (TalkLife). A license was obtained to work with the user data from TalkLife and a project proposal was submitted to them in order to embark on the project.\nThe final summaries in all cases are obtained by feeding the timeline summaries into an LLM. Given that LLMs are susceptible to factual inaccuracies, often referred to as 'hallucinations,' and tend to exhibit biases, the clinical summaries they generate may contain errors that could have serious consequences in the realm of mental health decision-making. These inaccuracies can encompass anything from flawed interpretations of the timeline data to incorrect diagnoses and even recommendations for potentially harmful treatments. Mental health professionals must exercise caution when relying on such generated clinical summaries. These summaries should not serve as substitutes for therapists in making clinical judgments. Instead, well-trained therapists must skillfully incorporate these summaries into their clinical thought"}, {"title": "A Appendix", "content": "A.1 Implementation Details\nWe use Meta-Llama-3-8B as teacher model and Phi-3-mini-4k-instruct as student model. We use Low-Rank Adaptation (LoRA) (Hu et al., 2022) for both of these two models while fine-tuning. We use an AdamW (Kingma and Ba, 2015) optimizer with learning rate 5e-5. While fine-tuning, we set the batch-size as 1 for each task, but set gradient accumulation steps as 16.\nA.2 Experiment on teacher model\nAfter fine-tuning the teacher model, we used the TEMPREASON dataset (Tan et al., 2023b) to evaluate its performance pre- and post-fine-tuning. Specifically, we focused on the L3 part of the"}]}