{"title": "Deep Learning for Speaker Identification: Architectural Insights from AB-1 Corpus Analysis and Performance Evaluation", "authors": ["Matthias Bartolo"], "abstract": "In the fields of security systems, forensic investigations, and personalized services, the importance of speech as a fundamental human input outweighs text-based interactions. This research delves deeply into the complex field of Speaker Identification (SID), examining its essential components and emphasising Mel Spectrogram and Mel Frequency Cepstral Coefficients (MFCC) for feature extraction. Moreover, this study evaluates six slightly distinct model architectures using extensive analysis to evaluate their performance, with hyperparameter tuning applied to the best-performing model. This work performs a linguistic analysis to verify accent and gender accuracy, in addition to bias evaluation within the AB-1 Corpus dataset.", "sections": [{"title": "1. Introduction", "content": "Speaker identification (SID) is the task of determining a speaker's identity from a specific audio sample chosen from a pool of known speakers. With applications in forensics, security, and customization [1], SID may be expressed as a pattern recognition problem. The SID pipeline, according to [2], is dependent on two critical components: feature extraction and feature classification. These factors work together to classify an input speech segment as belonging to one of N known enrolled speakers.\nIn feature extraction, certain characteristics required for an individual's identification are taken from the voice data. During the feature classification process, features extracted from an unidentified individual are compared to those of various speakers in order to detect and match the unique qualities that will eventually lead to the identification of the specific speaker [1]. A perfect speaker identification system should have minimal intra- and inter-speaker variance, readily quantifiable attributes, be less susceptible to noise, respond correctly to imitation, and not rely on other qualities [2]."}, {"title": "2. Feature Extraction", "content": "Feature extraction is a critical step in speech analysis, stemming from the fact that it allows raw audio data to be transformed into useful features. To this extent, Mel Spectrogram and Mel Frequency Cepstral Coefficients (MFCC) are two widely used techniques in this field of study [1]. By translating frequencies into the Mel scale, the Mel Spectrogram visualises the frequency content of an audio source across time, emphasising human auditory perception. Meanwhile, MFCCs extract compact representations by capturing the spectral features of the audio signal [2]. In this study, both of the aforementioned feature extractors were tested as inputs to the developed system architectures, with the goal of producing a thorough analysis to evaluate their performance and appropriateness for the current context."}, {"title": "3. Model Architecture", "content": "This study investigated a total of six slightly distinct model architectures. The first model architecture incorporates elements from both architectures listed in [3] and [4]. The model incorporates early feature extraction convolution layers modified from the structure suggested in [3], followed by an altered single lightweight LSTM layer suggested by [4]. The addition of batch normalization and dropout layers was intended to regularize the flattened LSTM output, hence improving the model's robustness and avoiding overfitting. Finally, classification was performed by activating softmax in the final fully connected dense layer. This function generated probability scores for each of the 285 speakers in the AB-1 corpus dataset, indicating the likelihood that the input data corresponds to one of the aforementioned speakers.\nThe succeeding model architectures in this research are all derived from the above architecture. Furthermore, the second architecture focused on raising the convolutional depth in the first feature extractors (CNNs) to improve feature extraction capabilities. Meanwhile, in the third model architecture, the LSTM was modified by increasing the number of LSTM units and incorporating an extra layer to increase sequence comprehension. The fourth architecture included an extra dense layer following the CNN-LSTM configuration to further enhance the output sequence. In contrast to the second model architecture, the fifth architecture was intended to reduce model complexity by recommending fewer convolution filters. Finally, the sixth architecture used batch normalisation across all CNN blocks in the model with the aim of reducing the effects of overfitting."}, {"title": "4. Evaluation", "content": "The aforementioned model architectures were trained with the TensorFlow Keras framework, which was chosen for its ease of use, efficacy in creating neural network models, and facilitation of efficient training methods. During training, the Adam optimizer, which is noted for its versatility and extensive use, was used [5]. Additionally, an early stopping callback with a patience score of 5 was also included to prevent overfitting.\nThe findings shown in Table 2 reveal that models 1 and 5 outperformed all others in terms of test accuracy, precision, recall, and f-score. Overall, all the presented architectures worked relatively well, with the exception of model 2 using the MFCC feature extractor, which consistently yielded comparable poor results over several runs. Furthermore, the results also illustrate that models using the Mel Spectrogram feature extractor outperformed those using the MFCC feature extractor. Additionally, the models consistently produced test accuracy, precision, recall, and f-score metrics in the 0.8 to 0.97 range, indicating a noteworthy degree of performance. Furthermore, the test loss observed among the models ranged between 0.14 and 0.65, indicating that the models were not overfitting.\nFollowing the discovery of model 1's superior efficiency, a thorough hyperparameter tuning procedure comprising fifteen trials was carried out to fine-tune its parameters. From the test, the best hyperparameters found were a learning rate of 0.001, a dropout rate of 0.4, and the activation functions tanh applied specifically to the second layer in the model architecture presented in Table 1, whilst the remaining layers utilised relu.\nThe findings shown in Table 2 demonstrate a significant performance improvement attained by the best model refined with these optimised parameters over the original model 1.\nThe visible improvements in the curves, which reflect the training and validation loss as well as accuracy for the best model, illustrate the efficiency of the adjusted parameters in optimising the model's performance. Furthermore, the curves exhibit a consistent and continuous evolution, demonstrating the model's incremental improvement during the training period, which is characterised by a decreasing loss and an increasing accuracy."}, {"title": "5. Analysis", "content": "A linguistic analysis was performed on the top-performing model to assess its gender and accent correctness while assuring the model's neutrality and lack of bias towards certain accents or genders. Figure 4 demonstrated a slight variance in gender accuracy. Surprisingly, the model performed somewhat better in predicting female speakers than male speakers, by a margin of 0.02. Furthermore, these findings imply that the model's predictions exhibit gender equality, indicating that there is equal representation within the AB-1 corpus dataset. This equality in predicted accuracy indicates that there is no bias towards any one gender, emphasizing a fair and impartial training dataset.\nAs observed in Figure 5, the accent accuracy scores were quite similar across accents. However, when compared to the gender accuracy distribution, a more obvious disparity between the best and lowest accuracy, approximately 0.05, emerged. From Figure 5 it can be deduced that the three easiest accents to predict (in order) were the following:\n1. Standard Southern English\n2. Scottish Highlands\n3. East Anglia\nIn contrast, the three hardest accents to predict (in order) were the following:\n1. Newcastle\n2. Northern Wales\n3. Cornwall"}, {"title": "6. Conclusion", "content": "This paper investigates the efficacy of Mel Spectrogram and MFCC as feature extraction approaches for speaker identification (SID) while proposing robust model architectures designed particularly for SID. The study's best model accuracy, precision, recall, and F-score of 0.97 demonstrate the usefulness of these techniques. Gender accuracy analysis revealed minimal variation, with slightly greater accuracy in identifying female speakers, confirming dataset balance and the lack of gender bias in predictions. Nonetheless, accent accuracy resulted in a more noticeable discrepancy, with Standard Southern English being the most predicted and Newcastle being the least predictable. The results of this study highlight the significance of future analysis and model development, notably in the case of accent-related challenges in SID."}]}