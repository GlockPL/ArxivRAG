{"title": "RMLR: Extending Multinomial Logistic Regression into General Geometries", "authors": ["Ziheng Chen", "Yue Song", "Rui Wang", "Xiao-Jun Wu", "Nicu Sebe"], "abstract": "Riemannian neural networks, which extend deep learning techniques to Riemannian spaces, have gained significant attention in machine learning. To better classify the manifold-valued features, researchers have started extending Euclidean multinomial logistic regression (MLR) into Riemannian manifolds. However, existing approaches suffer from limited applicability due to their strong reliance on specific geometric properties. This paper proposes a framework for designing Riemannian MLR over general geometries, referred to as RMLR. Our framework only requires minimal geometric properties, thus exhibiting broad applicability and enabling its use with a wide range of geometries. Specifically, we showcase our framework on the Symmetric Positive Definite (SPD) manifold and special orthogonal group SO(n), i.e., the set of rotation matrices in Rn. On the SPD manifold, we develop five families of SPD MLRs under five types of power-deformed metrics. On SO(n), we propose Lie MLR based on the popular bi-invariant metric. Extensive experiments on different Riemannian backbone networks validate the effectiveness of our framework.", "sections": [{"title": "1 Introduction", "content": "In recent years, significant advancements have been achieved in Deep Neural Networks (DNNs), enabling them to effectively analyze complex patterns from various types of data, including images, videos, and speech [28, 37, 26, 65]. However, most existing models have primarily assumed the underlying data with a Euclidean structure. Recently, a growing body of research has emerged, recognizing that the latent spaces of many applications exhibit non-Euclidean geometries, such as Riemannian geometries [9]. Various frequently-encountered manifolds in machine learning have posed interesting challenges and opportunities, including special orthogonal groups SO(n) [66, 30], symmetric positive definite (SPD) [29, 10, 41, 72, 18], Gaussian [14, 46], Grassmannian [31, 71] spherical [55], and hyperbolic manifolds [22]. These manifolds share an important Riemannian property their Riemannian operators, including geodesics, exponential & logarithmic maps, and parallel transportation, often possess closed-form expressions. Leveraging these Riemannian operators, researchers have successfully generalized different types of DNNs into manifolds, dubbed Riemannian neural networks.\nAlthough Riemannian networks demonstrated success in many applications, most approaches still rely on Euclidean spaces for classification, such as tangent spaces [29, 30, 10, 46, 68, 70, 47, 48, 36, 69, 15], ambient Euclidean spaces [67, 56, 57], or coordinate systems [12]. However, these strategies distort the intrinsic geometry of the manifold, undermining the effectiveness of Riemannian networks. Researchers have recently started directly developing Riemannian Multinomial Logistic Regression (RMLR) on manifolds. Inspired by the idea of hyperplane margin [38], Ganea et al. [22] developed a hyperbolic MLR in the Poincar\u00e9 ball for Hyperbolic Neural Networks (HNNs). Motivated by HNNs, Nguyen and Yang [49] developed three kinds of gyro SPD MLRs based on three distinct gyro"}, {"title": "2 Preliminaries", "content": "This section provides a brief review of the basic geometries of SPD manifolds and special orthogonal groups. Detailed review and notations are left in Apps. B and B.1.\nSPD manifolds: The set of n \u00d7 n symmetric positive definite (SPD) matrices is an open submanifold of the Euclidean space Sn of symmetric matrices, referred to as the SPD manifold S+ [3]. There are five kinds of popular Riemannian metrics on S: Affine-Invariant Metric (AIM) [51], Log- Euclidean Metric (LEM) [3], Power-Euclidean Metrics (PEM) [21], Log-Cholesky Metric (LCM) [40], and Bures-Wasserstein Metric (BWM) [5]. Note that, when power equals 1, the PEM is reduced to the Euclidean Metric (EM). Thanwerdas and Pennec [62] generalized AIM, LEM, and EM into two-parameters families of O(n)-invariant metrics, i.e., (\u03b1, \u03b2)-AIM, (\u03b1, \u03b2)-LEM, and (\u03b1, \u03b2)-EM, with min(a, a + n\u1e9e) > 0. We denote the metric tensor of (\u03b1, \u03b2)-AIM, (\u03b1, \u03b2)-LEM, (\u03b1, \u03b2)-\u0395\u039c, LCM, and BWM as g(\u03b1,\u03b2)-AIM, g(\u03b1,\u03b2)-LEM, g(\u03b1,\u03b2)-EM, gLCM, and gBWM, respectively.\nRotation matrices: The special orthogonal group SO(n) is the set of n \u00d7 n orthogonal matrices with unit determinant, the elements of which are also referred to as rotation matrices. As shown in [24], SO(n) forms a Lie group. We adopt the widely used bi-invariant Riemannian metric [8]."}, {"title": "3 Riemannian multinomial logistic regression", "content": "Inspired by [38], Ganea et al. [22], Nguyen and Yang [49], Chen et al. [16], Nguyen et al. [50] extended the Euclidean MLR into hyperbolic, SPD, and SPSD manifolds. However, these classifiers rely on specific Riemannian properties, such as the generalized law of sines, gyro structures, and flat metrics, which limits their generality. In this section, we first revisit several existing MLRs and then propose our Riemannian classifiers with minimal geometric requirements.\n3.1 Revisiting existing multinomial logistic regressions\nGiven C classes, the Euclidean MLR computes the multinomial probability of each class:\n$\\forall k \\in \\{1,..., C'\\}, p(y = k|x) \\propto exp \\langle a_k, x \\rangle - b_k,$\nwhere bk \u2208 R, and x, ak \u2208 Rn\\{0}. As shown in [22], the Euclidean MLR can be reformulated by the margin distance to the hyperplane:\n$p(y = k | x) \\propto exp \\left(sign(\\langle a_k, x - p_k \\rangle) \\| a_k \\| d(x, H_{a_k,p_k})\\right),$\n$H_{a_k,p_k} = \\{x \\in R^n : \\langle a_k, x - p_k \\rangle = 0\\},$\nwhere $\\langle a_k, p_k \\rangle = b_k$, and $H_{a_k,p_k}$ is a hyperplane.\nEqs. (2) and (3) can be naturally extended into manifolds M by Riemannian operators:\n$p(y = k | S) \\propto exp \\left(sign(\\langle \\bar{A}_k, Log_{p_1} (S) \\rangle_{p_2}) \\| \\bar{A}_k \\|_{p_1} d(S, H_{\\bar{A}_k,P_k})\\right),$\n$H_{\\bar{A}_k,P_k} = \\{S \\in M: g_{p_1} (Log_{p_1} S, \\bar{A}_k) = 0\\},$\nwhere Pk \u2208 M, \u0100k \u2208 TP\u2081M\\{0}, gp is the Riemannian metric at Pk, and Logp\u2081 is the Riemannian logarithm at Pk. The margin distance is defined as an infimum:\n$d(S, H_{\\bar{A}_k,P_k}) = \\inf_{Q \\in H_{\\bar{A}_k,P_k}} d(S, Q).$\nThe MLRs proposed in [38, 22, 49, 16] can be viewed as different implementations of Eq. (4)- Eq. (6). To calculate the MLR in Eq. (4), one has to compute the associated Riemannian metrics, logarithmic maps, and margin distance. The associated Riemannian metrics and logarithmic maps often have closed-form expressions on the frequently-encounter manifolds in machine learning. However, the computation of the margin distance can be challenging. On the Poincar\u00e9 ball of hyperbolic manifolds, the generalized law of sines simplifies the calculation of Eq. (6) [22]. However, the generalized law of sines is not universally guaranteed on other manifolds. Additionally, Chen et al. [16] developed a closed-form solution of margin distance on the SPD manifold under any metric pulled back from Euclidean spaces. For curved manifolds, solving Eq. (6) would become a non-convex optimization problem. To address this challenge, Nguyen and Yang [49] defined gyro structures on the SPD manifold and proposed a pseudo-gyrodistance to calculate the margin distance. Similarly, Nguyen et al. [50] proposed a pseudo-gyrodistance on the SPSD manifold based on the gyro product space. However, gyro structures do not necessarily exist in general geometries. In summary, the aforementioned methods often rely on specific properties of their associated Riemannian metrics, which usually do not generalize to general geometries.\n3.2 Riemannian multinomial logistic regression\nRecalling Eqs. (4) and (5), the least requirement of extending Euclidean MLR into manifolds is the well-definedness of Logp (S) for each k. In this subsection, we will develop Riemannian MLR, which depends solely on the Riemannian logarithm, without additional requirements, such as gyro structures and generalized law of sines. In the following, we always assume the well-definedness of the Riemannian logarithm. We start by reformulating the Euclidean margin distance to the hyperplane from a trigonometry perspective and then present our Riemannian MLR.\nAs we discussed before, obtaining the margin distance of Eq. (6) could be challenging. Inspired by [49], we resort to the perspective of trigonometry to reinterpret Euclidean margin distance. In Euclidean space, the margin distance is equivalent to\n$d(x, H_{a,p})) = sin(\\angle xpy^*)d(x,p)$, with $y^* = arg \\underset{y\\in H_{a,p}\\{p\\}}{max} (cos \\angle xpy).$\nWe extend Eq. (7) to manifolds by the Riemannian trigonometry and geodesic distance, the counterparts of Euclidean trigonometry and distance."}, {"title": "4 SPD multinomial logistic regressions", "content": "This section showcases our RMLR framework on the SPD manifold. We first systematically discuss the power-deformed geometries of SPD manifolds. Based on these metrics, we will develop five families of deformed SPD MLRs.\n4.1 Deformed geometries of SPD manifolds\n4.2 Five families of SPD multinomial logistic regressions\nThis subsection presents five families of specific SPD MLRs by our general framework in Thm. 3.3 and metrics discussed in Sec. 4.1. We focus on generating Ak by parallel transportation from the identity matrix, except for 20-BWM. Since the parallel transportation under 20-BWM would undermine numerical stability (please refer to App. F.2.1 for more details), we resort to a newly"}, {"title": "5 Lie multinomial logistic regression", "content": "This section introduces our Lie MLR on SO(n) based on the general RMLR framework in Thm. 3.3. The Riemannian metric on SO(n) is assumed to be the invariant metric in Tab. 13.\nThe two ways to generate \u0100k in RMLR, i.e.,Eqs. (12) and (13), are equivalent on SO(n).\nLemma 5.1. [\u2193]\n$FQ\\rightarrowP = LPQ^{-1}_{*,Q}, \\forallP, Q \\in SO(n).$\nSimilar with SPD MLRs, we set Q = I. The Lie MLR on SO(n) is presented in the following.\nTheorem 5.2. [\u2193] The Lie MLR on SO(n) is given as\n$p(y = k | R \\in SO(n)) \\propto \\langle log(P_kS), A_k \\rangle,$\nwhere Pk \u2208 SO(n) and Ak \u2208 50(n).\nWe refer to the Riemannian hyperplanes (Eq. (5)) on SO(n) as Lie hyperplanes. As SO(3) is homeomorphic to 3-dimensional real projective space RIP\u00b3 [25], Fig. 3 illustrates Lie hyperplanes in the closed ball in R\u00b3 of radius \u03c0."}, {"title": "6 Experiments", "content": "We first validate our SPD MLRs on four SPD neural networks: SPDNet [29] and TSMNet [36] for Riemannian feedforward networks, RResNet [35] for Riemannian residual networks, and SPDGCN [75] for Riemannian graph neural networks. Then, we proceed with experiments of our Lie MLR under the classic LieNet architecture [30]. The classifier in all the above networks is the LogEig MLR (matrix logarithm + FC + softmax), a Euclidean MLR on the tangent space at the identity matrix. We substitute the original non-intrinsic LogEig MLR in each baseline model with our RMLRs. Notably, the gyro SPD MLRs [49] are special cases of our SPD MLRs under the standard AIM, LEM, and LCM ((\u03b8, \u03b1, \u03b2) = (1,1,0)), while flat SPD MLRs [16] are incorporated by our SPD MLRs under (\u03b1, \u03b2)-LEM and 0-LCM. More implementation details are presented in App. G.\n6.1 Experiments on the proposed SPD MLRS\nIn the following, we abbreviate SPD MLR-metric as metric. For instance, (\u03b8, \u03b1, \u03b2)-AIM denotes the baseline endowed with the SPD MLR induced by (\u03b8, \u03b1, \u03b2)-AIM and (1,1,0) as the value of (\u03b8, \u03b1, \u03b2).\n6.1.1 Experiments on the Riemannian feedforward network\nWe evaluate our SPD MLRs for Riemannian feedforward networks under the SPDNet and TSMNet backbones. Following [29, 10], on SPDNet, we use the Radar dataset [10] for radar recognition and the HDM05 dataset [43] for human action recognition. TSMNet [36] is one of the state- of-the-art methods for the EEG classification task. Following [36], we use the Hinss2021 [27] dataset. For each family of SPD MLRs, we report the SPD MLR induced from the standard metric (0 = 1, \u03b1 = 1, \u03b2 = 0), and the one induced from the deformed metric with best (\u03b8, \u03b1, \u03b2). Besides, if the standard SPD MLR is already saturated, we only report the results of the standard one. Under each metric, We highlight the results in bold of our SPD MLR under the best hyperparameters. We visualize the results in App. G.1.6.\n6.1.2 Experiments on the Riemannian residual network\n6.1.3 Experiments on the Riemannian graph network\n6.1.4 Ablations of SPD MLRS on direct classification\n6.2 Experiments on the proposed Lie MLR"}, {"title": "7 Conclusions", "content": "This paper presents a novel and versatile framework for designing RMLR for general geometries, with a specific focus on SPD manifolds and SO(n). On the SPD manifold, we systematically explore five families of Riemannian metrics and utilize them to construct five families of deformed SPD MLRs. On SO(n), we develop the Lie MLR for classifying rotation matrices. Extensive experiments demonstrate the superiority of our intrinsic classifiers. We expect that our work could present a promising direction for designing intrinsic classifiers on diverse geometries."}, {"title": "A Limitations and future avenues", "content": "Limitation: Recalling our RMLR in Eq. (11), our RMLR might be over-parameterized. In our RMLR, each class would require a Riemannian parameter Pk and Euclidean parameter Ak. Consequently, as the number of classes grows, the classification layer would become burdened with excessive parameters. We will address this problem in future work.\nFuture work: We highlight the advantage of our approach compared to existing methods that our framework only requires the Riemannian logarithm, which is commonly satisfied by various manifolds encountered in machine learning. Therefore, as a future avenue, our framework offers various possibilities for designing intrinsic classifiers for neural networks on other manifolds."}, {"title": "B Preliminaries", "content": "B.1 Notations\nWe briefly summarize the notations in Tab. 11 for better clarity.\nB.2 Brief review of Riemannian geometry\nIntuitively, manifolds are locally Euclidean spaces. Differentials are the generalization of derivatives in classic calculus. For more details on smooth manifolds, please refer to [63, 39]. Riemannian manifolds are the manifolds endowed with Riemannian metrics, which can be intuitively viewed as point-wise inner products.\nDefinition B.1 (Riemannian Manifolds). A Riemannian metric on M is a smooth symmetric covariant 2-tensor field on M, which is positive definite at every point. A Riemannian manifold is a pair {M, g}, where M is a smooth manifold and g is a Riemannian metric.\nW.l.o.g., we abbreviate {M, g} as M. The Riemannian metric g induces various Riemannian operators, including the geodesic, exponential, and logarithmic maps, and parallel transportation. These"}, {"title": "C RMLR as a natural extension of the Euclidean MLR", "content": "Proposition C.1. When M = R\u2122 is the standard Euclidean space, the RMLR defined in Thm. 3.3 becomes the Euclidean MLR in Eq. (1).\nProof. On the standard Euclidean space Rn, Logy x = x y, \u2200x, y \u2208 Rn. Besides, the differential maps of left translation and parallel transportation are the identity maps. Therefore, given x, pk \u2208 Rn and ak \u2208 Rn/{0} = ToRn/{0}, we have\n$p(y = k | x \\in R^n) \\propto exp(\\langle Log_{p_k} x, a_k \\rangle_{p_k}),$\n$\\propto exp(\\langle x - p_k, a_k \\rangle),$\n$\\propto exp(\\langle x, a_k \\rangle - b_k),$\nwhere bk = \u27e8x, Pk\u27e9."}, {"title": "D Gyro SPSD MLR as special cases of our RMLR", "content": "Gyro SPSD MLR [50] is derived by the product of the Grassmannian and SPD gyro spaces. This section will show that the gyro SPSD MLR is the special case of our RMLR on the product geometry of the SPSD manifold. We first review some necessary results about gyro SPSD MLR and then show the equivalence.\nFollowing the notations in [50], we denote the Grassmannian with canonical metric under the projector and ONB perspective as Gr(p, n) and Gr(p, n), respectively. The space of n \u00d7 n SPSD matrices with a fixed rank p, denoted as St, forms an SPSD manifold [7]. As shown in [7, 50], the SPSD manifold is a product space, i.e.,St,p = Gr(p, n) \u00d7 S+. In other words, every P \u2208 Srt,p can be decomposed as P = UpSpUp with Up \u2208 Gr(p, n) and Sp \u2208 S +. We further denote SP S as the SPD manifold with metric g, where g could be AIM, LEM, and LCM. As shown in [50], the gyro space in Stp can be defined by the product of gyro spaces of Gr(p, n) and S. By this product structure, Nguyen et al. [50] proposed the SPSD Pseudo-gyrodistance to a hyperplane.\n++\nDefinition D.1. (SPSD Hypergyroplanes [50]) Let P, W \u2208 Gr(p, n) \u00d7 S. Then hypergyroplanes in structure space Gr(p, n) \u00d7 S are defined as\nW,P\n$H_{d} = \\{Q \\in Gr(p, n) \\times S_{++}^p : \\langle \\oplus_{psd,g} P, \\oplus_{psd,g} Q, W \\rangle_{psd,g} = 0\\} .$\nwhere \u2295psd,g and \u27e8,\u27e9psd,9 are gyro addition and gyro inner product, which are defined in [50]."}, {"title": "E Theories on the deformed metrics", "content": "E.1 Limiting cases of the deformed metrics\nThanwerdas and Pennec [58] generalized (\u03b1, \u03b2)-AIM into three-parameters families of metrics by power deformation, i.e., (\u03b8, \u03b1, \u03b2)-AIM. The family of (\u03b8, \u03b1, \u03b2)-AIM comprises (\u03b1, \u03b2)-AIM for 0 = 1 and approaches (\u03b1, \u03b2)-LEM with 0 \u2192 0 [58].\nChen et al. [17] extended LCM and (\u03b1, \u03b2)-LEM into power-deformed metrics, denoted as (\u03b8, \u03b1, \u03b2)-LEM and 0-LCM. The authors show that (\u03b8, \u03b1, \u03b2)-LEM is equal to (\u03b1, \u03b2)-LEM, and 0-LCM interpolates between \u011f-LEM (0 \u2192 0) and LCM (0 = 1), with \u011f-LEM defined as\n$\\langle V_1, V_2 \\rangle_p = \\langle V_1, V_2 \\rangle - \\frac{1}{n} \\langle D(V_1), D(V_2) \\rangle, \\forall V_i \\in T_pS_{++}^n,$\nE.2 Proof of the properties of the deformed metrics (Tab. 2)\nIn this subsection, we prove the properties presented in Tab. 2. We first present a useful lemma and then present our detailed proof. This lemma will be useful in the proof of our SPD MLRs as well."}, {"title": "F Computational details on the SPD MLR under power-deformed BWM", "content": "F.1 Matrix square roots in the SPD MLR under power-deformed BWM\nIn the case of MLRs induced by 20-BWM, computing square roots like (BA) and (AB) with B, A \u2208 S+ poses a challenge. Eigendecomposition cannot be directly applied since BA and AB are no longer symmetric, let alone positive definitity. Instead, we use the following formulas to compute these square roots [42]:\n$(BA)^{\\frac{1}{2}} = B^{\\frac{1}{2}} (B^{\\frac{1}{2}}AB^{\\frac{1}{2}})^{\\frac{1}{2}} B^{-\\frac{1}{2}}$ and $(AB)^{\\frac{1}{2}} = [(BA)^{\\frac{1}{2}}]^T,$\nwhere the involved square roots can be computed using eigendecomposition or singular value decomposition (SVD).\nF.2 Numerical stability of the SPD MLR under power-deformed BWM\nLet us first explain why we abandon parallel transportation on the SPD MLR derived from 20-BWM. Then, we propose our numerically stable methods for computing the SPD MLR based on 20-BWM."}, {"title": "G Implementation details and additional experiments", "content": "This section offers additional details on the experiments of SPD and Lie MLRs.\nG.1 Additional details and experiments on the SPD MLRS\nG.1.1 Basic layers in SPDNet and TSMNet\nSPDNet [29] is the most classic SPD neural network. SPDNet mimics the conventional densely connected feedforward network, consisting of three basic building blocks:\nBiMap layer: Sk = WkSk\u22121WkT, with Wk semi-orthogonal,\nReEig layer: Sk = Uk\u22121 max(\u2211k\u22121, \u20acIn)Uk\u22121T, with Sk\u22121 = Uk\u22121\u2211k-1Uk\u22121T,\nLogEig layer: Sk = log(Sk\u22121),\nwhere max() is element-wise maximization. BiMap and ReEig mimic transformation and non- linear activation, while LogEig maps SPD matrices into the tangent space at the identity matrix for classification.\nG.1.2 Datasets and preprocessing\nG.1.3 Implementation details\nG.2 Additional details and experiments on the Lie MLR\nG.2.1 Basic layers in LieNet"}, {"title": "H Proofs", "content": "H.1 Proof of Thm. 3.2\nProof of Thm. 3.2. Let us first solve Y* in Eq. (8), which is the solution to the following constrained optimization problem:\n$\\underset{Y}{max} \\left( \\frac{\\langle Log_p Y, Log_p S \\rangle_P}{\\| Log_p Y \\|_P, \\| Log_p S\\|_P} \\right) s.t. \\langle Log_p S, \\bar{A} \\rangle_P = 0$\\nH.2 Proof of Thm. 3.3\nProof for Thm. 3.3. Putting the margin distance (Eq. (10)) into Eq. (4), we have the following:\n$p(y = k | S) \\propto exp \\left(sign((\\bar{A}_k, Log_{p_1} (S))_{P_k}) \\| \\bar{A}_k \\|_{P_k}d(S, H_{\\bar{A}_k,P_k})\\right),$\n$= exp \\left(sign((\\bar{A}_k, Log_{p_k} (S))_{P_k}) \\| \\bar{A}_k \\|_{P_k} \\frac{|(Log_{P_k} (S), \\bar{A}_k)_{P_k}|}{\\| \\bar{A}_k \\|_{P_k}} \\right),$\n$= exp \\left((\\langle Log_{P_k} S, \\bar{A}_k \\rangle_{P_k}) \\right).$\nH.3 Proof of Prop. 4.1\nH.4 Proof of Thm. 4.2"}]}