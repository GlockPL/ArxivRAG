{"title": "Antelope: Potent and Concealed Jailbreak Attack Strategy", "authors": ["Xin Zhao", "Xiaojun Chen", "Haoyu Gao"], "abstract": "Due to the remarkable generative potential of diffusion-based models, numerous researches have investigated jailbreak attacks targeting these frameworks. A particularly concerning threat within image models is the generation of Not-Safe-for-Work (NSFW) content. Despite the implementation of security filters, numerous efforts continue to explore ways to circumvent these safeguards. Current attack methodologies primarily encompass adversarial prompt engineering or concept obfuscation, yet they frequently suffer from slow search efficiency, conspicuous attack characteristics and poor alignment with targets. To overcome these challenges, we propose Antelope, a more robust and covert jailbreak attack strategy designed to expose security vulnerabilities inherent in generative models. Specifically, Antelope leverages the confusion of sensitive concepts with similar ones, facilitates searches in the semantically adjacent space of these related concepts and aligns them with the target imagery, thereby generating sensitive images that are consistent with the target and capable of evading detection. Besides, we successfully exploit the transferability of model-based attacks to penetrate online black-box services. Experimental evaluations demonstrate that Antelope outperforms existing baselines across multiple defensive mechanisms, underscoring its efficacy and versatility. Disclaimer: This paper contains unsafe imagery that might be offensive to some readers.", "sections": [{"title": "1. Introduction", "content": "Recent advancements have highlighted the revolutionary capabilities of generative models, particularly those utilizing transformer [2, 6, 37] and diffusion [11, 35] architectures. The convergence of these technologies has produced increasingly powerful models for image [21, 28, 29, 31] and video [7, 24] generation. However, the vulnerabilities inherent in these models give rise to emerging safety concerns [4, 16, 26, 30, 41, 44]. Chief among these is the issue of misalignment, which facilitates the generation of harmful or inappropriate content, such as Not-Safe-for-Work (NSFW) imagery that includes nudity, violence, gore, and other potential sensitive materials [26, 40].\nTo mitigate the issue of inappropriate generation, developers of Text-to-Image (T2I) models have implemented external defense measures like text filters [20, 28, 29] and image filters [31], as shown in Fig. 1. Additionally, significant efforts [8, 15, 34] have been made to enhance the internal safety and robustness of T2I models through retraining or fine-tuning. Under these defense mechanisms, the generation of explicit NSFW content from inappropriate prompts is effectively blocked, while normal prompts continue to produce appropriate and non-sensitive imagery.\nDespite these safeguards, the inherent ambiguity in the text space and the misalignment between text and image space sustainably create fertile ground for jailbreak attacks, which seek to circumvent the system's safety and ethical guardrails. For instance, SneakyPrompt [41] demonstrates that perturbing similar words (e.g., \"nice\" vs. \"nlce\u201d) or using synonyms to paraphrase input while preserving the original semantics can alter the prediction results of T2I models. As illustrated in Fig. 1, the overarching goal of jailbreaking T2I models is to craft adversarial prompts that, while classified as benign, generate harmful imagery capable of evading multiple defense mechanisms. To achieve this, JPA [17] identifies adversarial prompts within the sensitive regions of text space by appending learnable tokens to each input, while SneakyPrompt [41] employs reinforcement learning to uncover such adversarial prompts. MMA [39] introduces a greedy search method based on gradi-"}, {"title": "2. Related Work", "content": "Defensive methods against NSFW generation. Current defense strategies for Text-to-Image (T2I) models can be broadly divided into external and internal defenses. External defenses typically involve post-hoc content moderation, employing prompt checkers to identify and filter malicious prompts or image checkers to censor NSFW elements in synthesized images. For instance, Rando et al. describe how the Stable Diffusion safety filter blocks images that closely resemble any of 17 pre-defined \u201csensitive concepts\" within CLIP model's embedding space. Similarly, platforms such as Dalle 3 [1], Leonardo.Ai [14], and Midjourney [20] implement prompt checkers that detect and reject malicious prompts upon submission. Internal defenses, on the other hand, focus on model-level modifications to eliminate unsafe content. ConceptPrune [3] demonstrates that neurons in latent diffusion models (LDMs) [31] often specialize in specific concepts like nudity and that pruning these neurons can permanently eliminate undesired concepts from image generation. Approaches like ESD [8] and SLD [34] employ model fine-tuning to directly reduce NSFW outputs, enhancing the intrinsic safety of T2I models. To counter jailbreak attempts via text prompts, SafeGen [15] modifies self-attention layers within the model, effectively filtering out unsafe visual representations regardless of the textual input. In this paper, we intend to explore potential strategies that can effectively bypass these defense mechanisms.\nAdversarial attacks on T2I models. SurrogatePrompt [41] and DACA [5] harness the power of large language models (LLMs) [2, 23] to substitute explicit words or disassemble unethical prompts into benign descriptions of individual elements, successfully bypassing safety filters of T2I models like Midjourney [20] and Dalle 2 [29]. Rather than relying on auxiliary models or tools, other works [36, 43, 45] focus on internal mechanisms such as concept retrieval [36] or concept removal [43, 45] to achieve attacks. However, Ring-A-Bell [36] lacks precise control over synthesis specifics, and UnlearnDiff [43] offers limited effectiveness against more comprehensive defense strategies. Notably, QF-Attack [45] empirically shows that a subtle five-character perturbation can induce significant content shifts in images synthesized by Stable Diffusion [31], though it risks misalignment due to simple character substitution. Furthermore, SneakyPrompt [41] leverages reinforcement learning to substitute explicit target words in the original prompts, while MMP-Attack [38] effectively replaces primary objects in images by appending optimized suffixes. Additionally, both MMP-Attack [38] and RT-attack [9] specifically align adversarial prompts with reference images, which effectively increase similarity scores and enhance alignment with target images. The primary distinction of PRISM [10] and MMA-Diffusion [39] from previous methods lies in their approach of updating the en-"}, {"title": "3. Methodology", "content": "3.1. Preliminary\nText-to-Image (T2I) models, initially demonstrated by Mansimov et al. [19], generate synthetic images from natural language descriptions known as prompts. These models typically consist of a language model to process the input prompt, such as BERT [6] or CLIP's text encoder [27], and an image generation module like VQGAN [42] or diffusion model [11] for synthesizing images. In case of Stable Diffusion [31], a pre-trained CLIP encoder $T: \\mathcal{X} \\rightarrow \\mathcal{E}$ is utilized to tokenize and project a text $x \\in \\mathcal{X}$ into its corresponding embedding representation $e \\in \\mathcal{E}$. The text embedding guides the image generation process which is facilitated by a latent diffusion model. This model compresses the image space into a lower-dimensional latent space, and utilize a U-Net [32] architecture to sample images. The architecture serve as a Markovian hierarchical denoising autoencoder to generate images by sampling from random latent Gaussian noise and iteratively denoising the sample. Once the denoising process is complete, the latent representation is decoded back into image space by an image decoder $D: \\mathcal{E} \\rightarrow \\mathcal{Y}$."}, {"title": "3.2. Threat Model", "content": "In this study, we conduct a comprehensive evaluation of the impact of Antelope on robust T2I models across two practical attack scenarios.\nWhite-Box Setting: Adversaries exploit open-source T2I models like SDv14 [31] for image generation, with full access to the model's architecture, checkpoints, and integrated safety mechanisms. However, attackers do not alter the model's architecture or parameters; rather, they focus on utilizing the outputs produced by the model's components (i.e., text encoder and image encoder) to perform in-depth exploration and analysis that inform their attack strategies.\nBlack-Box Setting: Attackers generate images using online T2I services like Midjourney [20] and Leoanrdo.AI [14]. Without direct access to proprietary model parameters or visibility into the integrated safety mechanisms, they merely rely on transfer attacks. By interacting with these services, adversaries adapt their jailbreaking methods to effectively bypass internal safety measures."}, {"title": "3.3. System Design", "content": "Given a T2I model $G$, we define the following functions: the text encoder $T: \\mathcal{X} \\rightarrow \\mathcal{E}$, which tokenizes and projects text"}, {"title": "4. Experiment", "content": "4.1. Experimental Setting\nSetup. We implement Antelope using Python 3.8.10 and PyTorch 1.10.2 on a Ubuntu 20.04 server, conducting all experiments on a single A100 GPU. We set \u03b3 = 0.2, N = 5, a learning rate of 0.001, and conduct 2000 iterations.\nDatasets. We evaluate the performance of Antelope using the Inappropriate Image Prompt (I2P) dataset [13] which is disproportionately likely to produce inappropriate images in generative Text-to-Image (T2I) tasks. Although the prompts in this dataset avoid explicit sensitive words, they can still prompt T2I models lacking safety checkers to generate images with explicit NSFW content. However, the dataset becomes ineffective when safety checkers exist. In our experiments, we select 333 prompts with a harm rating exceeding 90% for nudity, labeled NSFW-333, and 59 prompts with a similar harm rating for violence, labeled NSFW-59.\nDetector. To classify whether images contain nudity, we employ the NudeNet detector [22] which flags an image as nudity if any of the following labels are detected: GENITALIA_EXPOSED, BREAST_EXPOSED, BUTTOCKS_EXPOSED and ANUS_EXPOSED. For identifying images with harmful content, such as depictions of blood or violence, we utilize the Q16 classifier [33].\nMetrics. (1) Attack Success Rate (ASR): ASR quantifies the attack's effectiveness, calculated as the ratio of adversarial prompts that bypass the NSFW detector to the total number of adversarial prompts. A higher ASR indicates a more effective attack. For ASR computation, we instruct the T2I models to generate five images per prompt. If any of these"}, {"title": "4.2. Experimental Results", "content": "Evaluation on offline baselines across defensive methods. Table 1 and Table 2 present the ASR and FID scores of different attack methods against various defensive baselines for the \"nudity\" and \"violence\" target attributes. For each defense method, the best-performing results in each column are highlighted in bold, while the second-best results are underlined. We have several key observations. Firstly, Antelope consistently achieves the highest ASR and FID performance in most cases, demonstrating its effectiveness and superiority in bypassing defenses while maintaining image quality. Secondly, MMP-Attack and MMA-Diffusion show comparatively higher attack success rates, while SneakyPrompt and QF-Attack have lower ASR.\nOnline services. To evaluate the robustness and transferability of our method on black-box interfaces, we test whether adversarial prompts bypass the NSFW filters and generate inappropriate images on two popular online platforms: Midjourney [20] and Leoanrdo. AI [14]."}, {"title": "4.3. Ablation Study", "content": "We conduct a series of experiments to identify the threshold \u03c4, loss weight y, and the number of searched tokens N for achieving the best performance with Antelope.\nTo determine the best \u03b3 value, we disable the threshold judgment module for \u03c4 and fix N = 5. We then select 10 representative prompts for each target attribute, nudity and violence, increasing \u03b3 incrementally from 0.0 to 1.0 with an interval of 0.2. For each \u03b3 setting, we generate adversarial prompts and produce 5 images per prompt to measure the ASR. We observe that \u03b3 = 0.2 yields the highest ASR. Similarly, for identifying the optimal N, we fix \u03b3 = 0.2 and increase N from 1 to 8, finding that N = 4 or 5 achieves the best results. The corresponding outcomes are detailed in Tab. 4 and Tab. 5. Additionally, we calculate the minimum average loss function values for each variant and visualize them in Fig. 7. The optimal points for \u03b3 = 0.2 and N = 4 or 5 are marked with stars. At these optimal hyperparameter points, the loss values approach 0.7, leading us to set \u03c4 = 0.7 in our experiments."}, {"title": "5. Ethics Statement", "content": "This research may produce some socially harmful content, but our aim is to reveal security vulnerabilities in the T2I diffusion models and further strengthen these systems, rather than allowing abuse. We urge developers to responsibly use our findings to improve the security of T2I models. We advocate for raising ethical awareness in AI research, especially in generative models, and jointly build an innovative, intelligent, practical, safe, and ethical AI system."}, {"title": "6. Conclusion", "content": "In this paper, we introduce a potent and concealed attack strategy, Antelope, which effectively bypasses diverse safety checkers in Text-to-Image (T2I) models to generate Not-Safe-for-Work (NSFW) imagery. Through the incorporation of semantic alignment and early stopping mechanisms, Antelope addresses challenges of low search effi-"}, {"title": "loss function", "content": "$\\mathcal{E}_{c||s}$, and the image loss function as the cosine similarity\nbetween Ei and $\\mathcal{E}_{c||s}$, which can be clarified as Eq. (2) and\nEq. (3):\n$L_{txt} = 1 - cos(E_{c\\s}, E_t)$\n(2)\n$L_{img} = 1 \u2212 cos(E_{c\\\\s}, E_i)$\n(3)\nThen our learning objective is to optimize Eq. (4) where\n\u03b3 is a weighting factor to balance the loss terms between the\nimage and text modalities.\nmins: L = \u03b3Ltxt + (1 \u2212 \u03b3)Limg\n(4)"}, {"title": "Adjust text embedding", "content": "By subtracting the negative embedding En and adding the positive embedding Ep, we obtain\nthe adjusted text embedding Et for text alignment, which\ncan be formulated as:\nEt = Ec - En + Ep\n(1)"}]}