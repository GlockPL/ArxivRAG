{"title": "Membership Inference Attacks Against Vision-Language Models", "authors": ["Yuke Hu", "Zheng Li", "Zhihao Liu", "Yang Zhang", "Zhan Qin", "Kui Ren", "Chun Chen"], "abstract": "Vision-Language Models (VLMs), built on pre-trained vision encoders and large language models (LLMs), have shown exceptional multi-modal understanding and dialog capabili- ties, positioning them as catalysts for the next technological revolution. However, while most VLM research focuses on en- hancing multi-modal interaction, the risks of data misuse and leakage have been largely unexplored. This prompts the need for a comprehensive investigation of such risks in VLMs.\nIn this paper, we conduct the first analysis of misuse and leakage detection in VLMs through the lens of membership in- ference attack (MIA). In specific, we focus on the instruction tuning data of VLMs, which is more likely to contain sensi- tive or unauthorized information. To address the limitation of existing MIA methods, we introduce a novel approach that infers membership based on a set of samples and their sensi- tivity to temperature, a unique parameter in VLMs. Based on this, we propose four membership inference methods, each tai- lored to different levels of background knowledge, ultimately arriving at the most challenging scenario. Our comprehensive evaluations show that these methods can accurately determine membership status, e.g., achieving an AUC greater than 0.8 targeting a small set consisting of only 5 samples on LLaVA.", "sections": [{"title": "1 Introduction", "content": "Recently, vision-language models [2, 11, 14, 50, 80, 87] repre- sent a significant step towards more comprehensive AI sys- tems capable of understanding and interacting with the world in a more human-like manner as in Figure 1, where both visual and textual information are crucial. Unlike large lan- guage models (LLMs) that focus on the text modality only, VLMs are designed to process and reason about multi-modal data, enabling them to perform complex tasks such as visual question answering [24] and creative content creation [87].\nDespite their novel capability, VLMs typically rely on large- scale datasets, which often include copyrighted or sensitive data. A notable copyright infringement case [27] involved Twitter demanding that Clearview AI stop scraping images from its platform for model training. Besides, medical VLMs [25,43,81] may be trained on datasets containing medical im- ages and corresponding diagnostics, raising privacy concerns. Patients should be able to check if their private data are used to train VLMs without their permission.\nA commonly used technique for these issues is membership inference attacks (MIAs) [13, 15, 34, 37, 44, 46, 54, 67, 70, 84], where adversaries attempt to determine whether specific data samples are included in the model's training set. This can expose unauthorized data usage or lead to privacy risks [19, 57]. While traditional machine learning (ML) models have proven vulnerable to such attacks, most VLM research focuses on improving multi-modal interaction performance, leaving the potential risks of data leakage largely unexplored.\nIn this study, we consider membership inference attack in which a model user attempts to infer the instruction tun- ing data of VLMs. Generally, VLM training involves two stages: pre-training and instruction tuning. Pre-training typ- ically uses public image caption datasets [9, 49, 62, 68] for initial feature alignment. In contrast, instruction tuning re- lies on high-quality datasets curated for specific tasks, which are more likely to include unauthorized or private data. This significant potential for containing sensitive or unauthorized data makes the instruction tuning data particularly valuable for studying their vulnerability to MIAs.\nUnfortunately, existing MIAs that work well on traditional ML models are largely ineffective against LLMs. Systematic evaluations [17] show that MIAs against LLMs often perform no better than random guessing. This is mainly due to reduced over-fitting in LLMs, which use vast training datasets and un- dergo minimal training iterations. Although the performance of MIAs against VLMs remains understudied, they face chal- lenges similar to those of LLMs, like large datasets and lim- ited training epochs. For instance, LLaVA [50] are trained on massive datasets (e.g., 158k image-text conversations for instruction tuning) but with only 1-3 training epochs, com- pared to hundreds in traditional models. Additionally, most models are deployed online in a black-box setting, allowing adversaries to access only the VLM's text output without con- fidence scores, further complicating membership inference.\nTo address these challenges, we propose a novel member- ship signal that still leverages over-fitting but from two new perspectives. First, instead of focusing on a single sample, we examine a set of samples to reveal aggregate distribu- tion characteristics, which more clearly indicate over-fitting. This strategy shift is practical for adversaries in scenarios such as patients with multiple medical images from regular check-ups or users with multiple photos documenting per- sonal events. Besides, if dataset owners suspect their carefully curated datasets are being used without permission for model training, they can use the entire dataset for membership infer- ence. Second, we observe that in VLMs, the temperature, a user-adjustable parameter, affects member and non-member data differently: members show greater sensitivity to tempera- ture changes than non-members. This temperature sensitivity thus serves as an indicator for identifying membership.\nBased on the above, we propose four different types of adversaries with different background knowledge. As shown in Table 1, we gradually relax the assumptions until we arrive at the worst-case adversary.\nShadow Model Inference. We follow the previous MIAs [6, 46,67,70] and assume that the adversary has access to an aux- iliary dataset (called shadow dataset) and use it to construct a local shadow model that mimics the target model's behavior.\nThe adversary utilizes member sets and non-member sets from their shadow dataset to query the shadow model at a specific temperature. For each set, the adversary computes the similarity of the model's responses with the ground truth answers, along with the corresponding statistics. The adver- sary then repeats this process by varying the temperatures, obtaining a trend of statistics across different temperatures for the member set and non-member set, respectively. Finally, the adversary trains a binary classifier to learn the discrepancies in the two statistical trends. Once trained, the classifier can differentiate between the member and non-member sets of the target model."}, {"title": "2 Preliminaries", "content": "Inspired by the fact that the fundamental capabilities of VLMs lie in image understanding and language generation, tasks typically handled by existing pre-trained visual encoders and large language models, VLMs such as MiniGPT-4 [87] and LLaVA [50] have opted to build upon these pre-trained back- bones and achieved capabilities comparable to commercial proprietary models like GPT-4 [2] at a very low training cost. For example, LLaVA can be trained in just one day using 8 A100 GPUs, making VLM development more accessible to small companies and academic researchers.\nModel Structure. As shown in Figure 2, an image $x_v$ is pro- cessed by a vision encoder to produce a sequence of image tokens, which are $e_v = {e_v^i }_1^n \\in \\mathbb{R}^d$. A projector $f_\\Theta$ is intro- duced to transform $e_v$ to $t_v = f_\\Theta(e_v) \\in \\mathbb{R}^{d_1}$, thereby mapping the vision tokens into the LLM embedding space. Simultane- ously, a textual prompt $x_q$ is input into the tokenizer to obtain $t_q \\in \\mathbb{R}^{d_1}$. Subsequently, the combined token sequence $[t_v, t_q]$ is fed into the LLM, denoted as $f_\\varphi$, which ultimately generates the language response $y_a = f_\\varphi(t_v, t_q)$.\nTraining. Trainable parameters differ across different VLMs, as some VLMs [14,80,87] freeze the vision encoder and LLM, training only the projector with less than 1 million parame- ters, denoted as $\\Theta = {\\theta}$, while others [11,50] train both the projector and LLM, represented as $\\Theta = {\\omega, \\varphi}$. The training typically involves two stages: pre-training and instruction tun- ing. The first stage utilizes extensive public image-text pairs to achieve preliminary feature alignment. The instruction tuning stage is crucial for equipping VLMs with interactive capa- bilities, and the models' developers meticulously construct high-quality datasets to align the model with specific tasks.\nThe training data are represented as $D = {(x_v, x_q, y_a), ..., x_v, x_q, y_a }$, where $x_v$ denotes the input image, $x_q$ is the question prompt, and $y_a$ is the answer. The training objec- tive is to maximize the probability of the model outputting $y_a$ given inputs $x_v$ and $x_q$. The loss for a single data sample is:"}, {"title": "2.1 Vision-Language Models", "content": "Inspired by the fact that the fundamental capabilities of VLMs lie in image understanding and language generation, tasks typically handled by existing pre-trained visual encoders and large language models, VLMs such as MiniGPT-4 [87] and LLaVA [50] have opted to build upon these pre-trained back- bones and achieved capabilities comparable to commercial proprietary models like GPT-4 [2] at a very low training cost. For example, LLaVA can be trained in just one day using 8 A100 GPUs, making VLM development more accessible to small companies and academic researchers.\nModel Structure. As shown in Figure 2, an image $x_v$ is pro- cessed by a vision encoder to produce a sequence of image tokens, which are $e_v = {e_v^i }_1^n \\in \\mathbb{R}^d$. A projector $f_\\Theta$ is intro- duced to transform $e_v$ to $t_v = f_\\Theta(e_v) \\in \\mathbb{R}^{d_1}$, thereby mapping the vision tokens into the LLM embedding space. Simultane- ously, a textual prompt $x_q$ is input into the tokenizer to obtain $t_q \\in \\mathbb{R}^{d_1}$. Subsequently, the combined token sequence $[t_v, t_q]$ is fed into the LLM, denoted as $f_\\varphi$, which ultimately generates the language response $y_a = f_\\varphi(t_v, t_q)$.\nTraining. Trainable parameters differ across different VLMs, as some VLMs [14,80,87] freeze the vision encoder and LLM, training only the projector with less than 1 million parame- ters, denoted as $\\Theta = {\\theta}$, while others [11,50] train both the projector and LLM, represented as $\\Theta = {\\omega, \\varphi}$. The training typically involves two stages: pre-training and instruction tun- ing. The first stage utilizes extensive public image-text pairs to achieve preliminary feature alignment. The instruction tuning stage is crucial for equipping VLMs with interactive capa- bilities, and the models' developers meticulously construct high-quality datasets to align the model with specific tasks.\nThe training data are represented as $D = {(x_v, x_q, y_a), ..., x_v, x_q, y_a }$, where $x_v$ denotes the input image, $x_q$ is the question prompt, and $y_a$ is the answer. The training objec- tive is to maximize the probability of the model outputting $y_a$ given inputs $x_v$ and $x_q$. The loss for a single data sample is:\n$\\displaystyle -\\log P_\\Theta(y_a \\vert x_v, x_q) = - \\sum_{i=1}^{n_a} \\log P_\\Theta(t_a^i \\vert t_a^{1:i-1}, x_q, x_v),\\qquad (1)$"}, {"title": "2.2 Membership Inference", "content": "Membership inference in the ML field is when an adversary aims to determine whether a particular data sample is used for the training of an ML model. The objective is typically focused on a single data sample [6,46,67,70]. Formally, in the context of VLM, given a target data sample $x = (x_v, x_q, y_a)$, a trained VLM $f_\\Theta$, and the external knowledge of an adversary, denoted by $\u03a9$, a sample membership inference $A_{sample}$ can be defined as:\n$A_{sample}: (x, f_\\Theta, \u03a9) \\rightarrow {0, 1},\\qquad (3)$\nwhere 0 indicates that x is not a member of the training dataset of $f_\\Theta$, and 1 indicates that x is a member.\nInference Target in VLMs. After examining the training process of VLMs, we choose to conduct membership in- ference on the training data of the instruction tuning stage."}, {"title": "3 Initial Exploration and Key Insights", "content": "In this section, we present our initial attempts at conducting membership inference on VLMs and how these observations inspire our later algorithm designs."}, {"title": "3.1 Training Data Memorization", "content": "Successful membership inference relies on the model's over- fitting of training data. Therefore, the initial step involves verifying whether the model has \u201cmemorized\u201d the training data. According to Equation 1, the goal during the instruction tuning stage is to maximize the probability that the model's output $r_a$ matches the ground truth answer $y_a$. A straightfor- ward approach is to input the target sample's image $x_v$ and question $x_q$ into the VLM and observe how closely the VLM's response $r_a$ matches the answer $y_a$.\nSince directly obtaining sample loss or confidence scores is not feasible in black-box scenario, we instead measure the similarity between $r_a$ and the $y_a$. We employ OpenAI's embedding model [61] to transform the texts into embed- ding vectors and compute their cosine similarity. We train an LLaVA model and randomly select 1000 member and 1000 non-member data samples to feed into the model, calculating the similarity between the model responses and the ground truth answers. The distribution of these similarities, as shown in Figure 3, reveals slight differences between member and non-member data. We calculated the AUC score, which was 0.5673, only slightly higher than the random guess baseline of 0.5. This suggests a certain degree of over-fitting, implying that the model has, to some extent, memorized the training data. However, this signal alone is not sufficient for conduct- ing successful membership inference.\nSet-Level Membership Inference. As aforementioned, exist- ing MIA methods struggle against LLMs [17], and we see a similar trend with VLMs, as shown by the low AUC score of 0.5673. We attribute this to the fact that VLMs, like LLMs, are usually trained on large datasets with few epochs, leading to low over-fitting and weak membership signal. To address this, we shift our focus from inferring the membership of in- dividual samples to that of a set of samples, which we believe can better capture the membership signal by aggregating the signals form individual samples, thereby forming a stronger, more reliable and identifiable signal. We emphasize this set- ting is realistic in the real world, like scenarios of copyright infringement detection and privacy leakage. First, in cases of copyright infringement, an entire dataset is often used without authorization. Thus, set-level inference can be an effective technique for the dataset owner to detect such unauthorized use. Additionally, individuals can also use this technique to identify unauthorized use of their private data, such as photos posted on social media. Second, set-level inference also poses significant privacy risks. For instance, a lung cancer patient who undergoes frequent examinations generates X-rays and diagnostic records, which consistently indicate the presence of cancer. These pairs of X-rays and diagnoses form a small dataset that could be included in the training data of a medical VLM. Note that small sets, such as those containing 5-10 items are sufficient for effective set-level inference in some cases (see Figure 7). When adversaries target such small sets, they can reveal the data owner's health condition, similar to previous sample-level MIAs. In this case, the set actually acts as a sample, with each image serving as a feature and the textual diagnoses indicating cancer as the label.\nFormally, given a target set $X = {X_1, X_2, ..., X_x}$, a set mem- bership inference $A_{set}$ can be defined as:\n$A_{set}: (\u03a7, f_\\Theta, \u03a9) \\rightarrow {0, 1},\\qquad (4)$\nwhere $A_{set}$ serves as a binary classifier."}, {"title": "3.2 Temperature", "content": "We continue to explore ways to enhance the distinction be- tween members and non-members. As shown in Figure 3, the model tends to produce responses closer to the ground truth answer for members, assigning higher scores to tokens that match the answer. Equation 2 further shows that the selec- tion of the next token is influenced by the temperature $T$. A lower $T$ sharpens the softmax output distribution, increasing the likelihood of selecting the highest-scoring token, while a higher $T$ flattens the probability distribution. The effect of $T$ on member and non-member data, however, is significantly different. As shown in the first column of Figure 4, the model, trained on member data, assigns a significantly higher score to the ground truth token during next-token prediction. With a low $T$, the softmax operation results in a high output proba- bility for the ground truth token, markedly exceeding that of any other tokens. As $T$ increases, the probability difference between the ground truth token and others decreases dramati- cally (from 0.93 to 0.17), and the similarity between model output and ground truth answer will also decrease signifi- cantly accordingly. Conversely, for non-member data, which often have several closely scored tokens, an increase in $T$ also leads to a more uniform output probability distribution, but the change is relatively modest (from 0.29 to 0.13). Con- sequently, the fidelity of outputs to ground truth answers in member data is highly sensitive to $T$ variations, while the outputs for non-member data are less affected. Since most commercial LLMs\u00b2 and VLMs\u00b3 allow users to adjust the tem- perature, adversaries can use this feature to better distinguish between member and non-member data.\nWe study how similarity scores vary with temperature by inputting both members and non-members into the LLaVA model at different temperature settings. We randomly select 1000 samples from both members and non-members and ex- amine how the similarity scores vary across different tempera- tures, as depicted in Figure 5. While similarity scores for both"}, {"title": "4 Shadow Model Inference", "content": "In this section, we present the first type of set membership inference against VLMs, i.e., shadow model inference, as shown in the top row of Figure 6. We start by introducing our key intuition. Then we describe the attack methodology. Finally, the evaluation results are presented."}, {"title": "4.1 Intuition", "content": "As observed in Section 3.2, when a set of data samples is input into a target VLM with varying temperatures $T \\in {T_i}_1^l$, the distribution of similarity scores changes as the temperature increases. The trend in similarity score distributions differs between member and non-member sets.\nTo leverage this observation, we propose training a binary classifier to learn the patterns in the distribution trends of member and non-member sets. Following previous MIAs [6, 67,70], we assume that the adversary has access to a shadow dataset $D_s$ drawn from the same distribution as the target model's dataset $D_t$. The adversary is then able to train a local shadow model $f_\\Theta^s$ on $D_s$. Subsequently, the adversary generates training data for the binary classifier by querying the shadow model with the shadow dataset. The trained binary classifier can then be employed to conduct inference on a target set with the target model."}, {"title": "5 Reference Inference", "content": "The shadow model reference requires that the adversary pos- sesses both a shadow dataset and adequate computational resources to train the shadow model. However, this assump- tion may not always hold in practical membership inference scenarios. Consequently, we seek to relax the assumption and propose the method of reference inference."}, {"title": "5.1 Intuition", "content": "In the previous inference, the necessity for a shadow dataset stemmed from the need to characterize the distribution pat- tern of member and non-member sets. By comparing the distribution of the target set with those of the member and non-member sets, the classifier can determine its membership status. We wonder if it is possible to assess, without relying on a trained classifier, whether the similarity scores of the target set originate from the same distribution as those of the member/non-member sets. Statistical hypothesis testing offers a solution: z-test [42] can be utilized to assess whether the distributional difference between two data statistics is sig- nificant, thereby aiding in the determination of whether they originate from the same distribution.\nIf the adversary possesses a reference set whose member- ship status in the target model's training dataset is already known, z-test can be employed to calculate the probability that the similarity scores of the reference set and the target set come from the same distribution 4, as shown in the middle line of Figure 6. This probability can indicate whether the ref- erence set and the target set belong to the same membership status. In practical inference scenarios, acquiring reference data is generally not challenging. For example, data posted after the publication of the model can be considered as non- member data."}, {"title": "5.2 Methodology", "content": "In this section, we discuss the scenario where a known non- member set is used as a reference. Please see Appendix A for discussions and evaluations on scenarios involving a mem- ber set as reference. For the non-member reference set $X_r$ of size $g_r$, then each data sample $x \\in X_r$ is used to query the target model, and the similarity score $s_r$ between the model's response $r_r$ and the ground truth answer $y_a$ is computed, re- sulting in an array of similarity scores $s_r = [s_1, s_2, ..., s_{g_r}]$. The target set $X_t$ is processed in a similar manner, yielding an array of similarity scores, $s_t = [s_1, s_2, ..., s_{g_t}]$. Note that each data sample queries the target model only once with a single temperature, as it's already sufficiently discriminative with the existence of the reference set. The next step involves conducting a z-test on these two arrays:\n$\\displaystyle p = 1 - \\Phi(\\frac{\\overline{s_t} - \\overline{s_r}}{\\sqrt{\\frac{\\sigma_t}{g_t} + \\frac{\\sigma_r}{g_r}}}), \\qquad (5)$\nwhere $\u03a6$ denotes the cumulative distribution function of the standard normal distribution, $s_t$ and $s_r$ are the means, $\u03c3_t$ and $\u03c3_r$ are the standard deviations.\nThe outcome of the z-test is a p-value. In hypothesis test- ing, it is conventionally accepted that a p < 0.05 indicates a significant difference between the distributions of two data arrays, suggesting that the target set is a member in our sce- nario. Our evaluations indicate that a p < 0.05 threshold can sometimes be too stringent, leading to a high rate of false negatives. The adversary may choose a suitable threshold depending on their requirements, similar to many machine learning applications [4, 20, 39]. In our experimental evalua- tion, we primarily use the area under the ROC curve (AUC), which is a threshold-independent metric."}, {"title": "5.3 Evaluation Setting", "content": "We utilize the target MiniGPT-4 model from Section 4.3. We randomly sample 1,000 sets of size $g_t$ from $D_m$ to serve as target member sets. The dataset $D_n$ is equally split into $D_n^1$ and $D_n^2$. 1000 sets of size $g_t$ are sampled from $D_n^1$ as target non-member sets, and 1000 sets of size $g_r$ are sampled from $D_n^2$ as reference non-member sets. Thus, all samples in a set share the same membership status, and the membership status of the individual samples determine that of the set. Each set serves then serves as a data point in evaluation. The scenario in which one set contains samples with different membership status is discussed in Section 8.2. Each reference non-member set is then fed into Algorithm 2 with a target member set or a target non-member set. The resulting p-values are used to compute the AUC scores. For LLaVA, as there is no need to allocate data for a shadow dataset, the data is re-partitioned in an 8:2 ratio into $D_m$ and $D_n$, and $D_m$ is used to retrain a target model. Other settings are similar to those used for MiniGPT-4."}, {"title": "6 Target-Only Inference", "content": "Although reference inference relaxes the capability require- ments for the adversary compared to shadow model inference, the necessity of a reference set may still pose a barrier in certain scenarios. Therefore, we consider a more challenging scenario where the adversary has only a set of samples that need to be inferred."}, {"title": "6.1 Intuition", "content": "The need for a shadow dataset or a reference set stems from the requirement to establish a reference coordinate system. This system allows us to calculate the distance between the tar- get set and the member/non-member sets to determine mem- bership status. Figure 5 inspires a solution for conducting inference without an external reference coordinate system: member data is more sensitive to the change in temperature, meaning that querying the target model at two different tem- peratures results in two different similarity scores, and the difference between the two scores is generally larger for mem- bers than for non-members. The results of queries at different temperatures establish an internal reference coordinate sys- tem, which facilitates distinguishing between members and non-members, as shown in the third row of Figure 6."}, {"title": "6.2 Methodology", "content": "For each data sample $x$ in the target set $X_t$ of granularity $g$, the target model is queried with a high temperature $T_h$ and a low temperature $T_l$ for responses $r_h$ and $r_l$, respectively. The similarity scores $s_h$ and $s_l$ between the responses and the ground truth answer $y_a$ are calculated, resulting in two arrays of similarity scores $s_h = [s_h^1, s_h^2, ..., s_h^{g}]$ and $s_l = [s_l^1, s_l^2, ..., s_l^{g}]$ corresponding to the pair of temperatures. Z-test is then con- ducted on these two arrays using Equation 5 to compute their p-value. Given that the data at different temperatures inher- ently come from different distributions, regardless of whether they are from the member set or non-member set, the p-values are typically less than 0.05. Therefore, rather than using 0.05 as a threshold, we utilize the AUC score, which is independent of any threshold."}, {"title": "6.3 Evaluation Setting", "content": "This section employs the same target models as described in Section 5.3. We sample 1,000 sets of granularity $g$ from member data and non-member data, respectively. Each set is then fed into Algorithm 3 for target-only inference."}, {"title": "6.4 Experimental Results", "content": "Figure 13 illustrates the influence of granularity, similarity calculation methods, and type of LLM on AUC, with the experimental setup specifying $T_l$ = 0.1 and $T_h$ = 1.6. The trends of the impact of granularity and type of LLM on infer- ence performance are consistent with previous experiments; however, in this experiment, the embedding-based similar- ity calculation outperforms the rouge method. Examination of the actual responses revealed that the quality of VLM re- sponses deteriorates at higher $T$ values, displaying significant semantic differences compared to responses at lower $T$, where the embedding-based similarity calculation captures these se- mantic discrepancies more effectively.\nWe show the effects of varying $T_l$ and $T_h$ on LLaVA with Vicuna-13B in Figure 14. Generally, a larger difference be- tween $T_l$ and $T_h$ correlates with higher AUC scores, although larger is not always better. Particularly at very high $T_h$ values, such as 1.8, the quality of model responses is poor, offering limited utility. Please refer to Appendix B for the results on MiniGPT-4 in Figure 28."}, {"title": "7 Image-only Inference", "content": "In some practical scenarios, the adversary may possess only images without the corresponding text. Therefore, we con- sider the most challenging scenario in which the adversary has only a set of images that need to be inferred."}, {"title": "7.1 Intuition", "content": "Due to inherent randomness in the model generation process, querying the same image multiple times may yield slightly different responses. For member sets, since the model has been trained on these sets, the descriptions are more consistent and closely aligned with the ground truth, resulting in greater similarity among them. In contrast, for non-member sets, to which the model has not been exposed, the output probability distribution is more uniform, leading to more diverse and less consistent responses. Thus, when only images are available, an attacker can repeatedly query the model with the same image, assess the similarity among the responses, and use this information to infer membership status, as illustrated in the bottom line of Figure 6."}, {"title": "7.2 Methodology", "content": "For each image $x_i$ within the target set, the target model inde- pendently describes the image $k$ times, yielding $k$ responses $[r_1, ..., r_k]$. The similarity among these $k$ responses is calcu- lated, resulting in $\\binom{k}{2}$ similarity scores $[s_1, ..., s_{k(k-1)/2}]$, and their mean, $s_{avg}$, is calculated. The adversary determines the membership status based on the mean $s_{avg}$ of all images in the target set, where a higher $s_{avg}$ indicates membership."}, {"title": "7.3 Evaluation Setting", "content": "This section employs the same target models as described in Section 5.3. We sample 1,000 sets of granularity $g$ from member data and non-member data, respectively. The images in each set are then fed into Algorithm 4 for image-only inference."}, {"title": "7.4 Experimental Results", "content": "Figure 15 illustrates the impact of granularity, similarity cal- culation methods, and the type of LLM on the AUC, with the experimental setup specifying $T$ = 0.5 and the number of queries $k$ = 10. The impact of these variants remains consis- tent with previous experiments.\nFigure 16 demonstrates the effects of temperature on in- ference success rates. The highest success rate occurs at $T$ = 0.5. When the temperature is too low, the randomness during model generation is insufficient, leading to very sta- ble descriptions of non-member images by the model, which is inadequate for distinguishing between member and non- member images. Conversely, when the temperature is too high, the excessive randomness results in significant varia- tions between descriptions of member images by the model. Therefore, selecting an appropriate temperature is crucial.\nThe impact of the number of queries is depicted in Fig- ure 17. There is a noticeable improvement when the number of queries increases from 5 to 10; however, further increases in the number of queries only marginally enhance the inference success rate."}, {"title": "8 Additional Experiments", "content": "This section studies the impact of different settings on the ef- fectiveness of inference methods, including query type, hetero- geneous target set, mismatched set size and response length. We also discuss and compare the shadow model inference method with existing attacks that share similar threat models. The experiments in this section are conducted on LLaVA with Vicuna-13B model. More experimental results can be found in Appendix B."}, {"title": "8.1 Query Type", "content": "As discussed in Section 4.3, the instruction tuning dataset for LLaVA is categorized into three types: multi-turn conver- sations, detailed image descriptions, and complex reasoning. We test the performance of three inference methods across these query types, and the results are presented in Table 3. Note that the image-only inference is not included as it can only be performed on the type of detailed image description. Generally, queries of the complex reasoning type achieved the highest success rate in inference, followed by multi-turn conversations, while those of the detailed image description type were less effective. We believe that this is due to the answers in detailed image descriptions being more definitive, allowing the VLM to produce responses very close to the ground truth answer, regardless of whether it has learned the data. In contrast, answers in complex reasoning are more di- vergent, resulting in larger discrepancies between members and non-members. Similarly, multi-turn conversations, which often involve reasoning questions, also exhibit higher infer- ence success rates."}, {"title": "8.2 Heterogeneous Target Set", "content": "It is typically assumed that the data samples within a target set share the same membership status. This is because model developers generally aim to comprehensively collect all ac- cessible data, and data in one target set often possess the same accessibility. For instance, a set of medical images from a single patient is usually either entirely collected or entirely omitted.\nIn cases where this uniformity does not hold, we explore the scenario that each target set includes a certain proportion of data with opposing membership statuses. The results, dis- played in Figure 18, show that for all types of inference, the success rate significantly decreases as the heterogeneity ratio increases. However, when the ratio is low (less than 0.2), the inference success rate remains relatively high (higher than 0.8), showing robustness to some extent."}, {"title": "8.3 Mismatched Set Size", "content": "In shadow model inference and reference inference", "granularity": "one for the target set and the other for the shadow set or the reference set. We investigate whether a mismatch in the size of these sets affects the success rate of inference and illustrate the experimental results in Fig- ure 19. For shadow model inference, the size of the shadow granularity appears to have minimal impact on the success rate of inference. However, the size of the target granularity is positively correlated with the success rate of inference. In the case of reference inference, the success rate is positively cor- related with the size of both granularities. Yet, the variation in"}]}