{"title": "TS-TCD: Triplet-Level Cross-Modal Distillation for Time-Series Forecasting Using Large Language Models", "authors": ["PengfeiWang", "Huanran Zheng", "Silong Dai", "WenJing Yue", "Wei Zhu", "XiaolingWang"], "abstract": "In recent years, large language models (LLMs) have shown great potential in time-series analysis by capturing complex dependencies and improving predictive performance. However, existing approaches often struggle with modality alignment, leading to suboptimal results. To address these challenges, we present a novel framework, TS-TCD, which introduces a comprehensive three-tiered cross-modal knowledge distillation mechanism. Unlike prior work that focuses on isolated alignment techniques, our framework systematically integrates: 1) Dynamic Adaptive Gating for Input Encoding and Alignment, ensuring coherent alignment between time-series tokens and QR-decomposed textual embeddings; 2) Layer-Wise Contrastive Learning, aligning intermediate representations across modalities to reduce feature-level discrepancies; and 3) Optimal Transport-Driven Output Alignment, which ensures consistent output predictions through fine-grained cross-modal alignment. Extensive experiments on benchmark time-series datasets demonstrate that TS-TCD achieves state-of-the-art results, outperforming traditional methods in both accuracy and robustness.", "sections": [{"title": "I. INTRODUCTION AND RELATED WORK", "content": "Time-series forecasting plays a pivotal role in various domains such as finance [1], healthcare [2], environmental monitoring [3], and industrial processes [4]. Capturing complex temporal dependencies and dynamics is challenging, especially when handling high-dimensional, multimodal data. Traditional time-series models, such as CNN-based methods [5]\u2013[7], transformer-based methods [8]\u2013[13], and MLP-based methods [14], [15], often encounter challenges in effectively addressing these complexities due to their limited capacity to model long-term dependencies and the lack of support for multimodal inputs. The recent success of large language models (LLMs) in capturing intricate dependencies in text has motivated their application in time-series forecasting, potentially overcoming some limitations of traditional methods [16].\n\nThe introduction of LLMs into time-series forecasting has led to two primary strategies in the literature: sequential approaches and parallel approaches. In the sequential approach, time-series data is tokenized and transformed into a format that LLMs can process, such as discrete tokens. Works such as [17]\u2013[22] employ this strategy, leveraging the strong sequence modeling capabilities of LLMs to extract complex temporal patterns. However, the challenge of modality mismatch the inherent difference between time-series data and natural language-poses a significant barrier, often leading to performance degradation and catastrophic forgetting during fine-tuning [21], [23]. These works highlight the need for improved modality alignment, especially when adapting pre-trained LLMs to non-linguistic data like time series. In contrast, parallel approaches simultaneously encode both time-series data and textual modalities, enabling a more semantically rich representation. Prior works [24]\u2013[29] have explored multimodal learning frameworks where paired data across modalities allows for mutual knowledge transfer. While this method can enhance representation learning, it faces the limitation of requiring paired textual data, which is scarce in time-series applications. Time-series data is often collected directly by sensors or smart devices, with little or no accompanying textual information. Although some methods [30], [31] for textualizing time-series data have been proposed, they lack diversity and flexibility. Previous efforts to overcome this limitation have been constrained by their reliance on synthetic or manually generated text, which often lacks temporal coherence with the underlying time-series data.\n\nIn this paper, we address these challenges by introducing a novel triplet-level cross-modal knowledge distillation framework that merges the adaptability of sequential methods with the multi-level distillation capabilities of parallel approaches. Unlike existing methods, our framework does not rely on real paired textual data. Instead, we propose a dynamic adaptive gating mechanism to generate virtual text tokens by combining time-series data with QR-decomposed word embeddings from the LLM's vocabulary. This novel mechanism ensures temporal coherence and alignment between modalities, overcoming the limitations of prior parallel approaches. Furturemore, we adopt layer-wise contrastive learning, and optimal transport loss to ensure consistent knowledge transfer and alignment at all stages. While contrastive learning has been used in cross-modal settings to maintain feature consistency [33], [34], its application to align intermediate layers of LLMs and time-series models remains unexplored. Moreover, the combination of contrastive loss with optimal transport for output alignment is a novel contribution in cross-modal distillation, particularly in the context of time-series forecasting. The main contributions of this paper are as follows:\n\u2022 We propose a novel cross-modal knowledge distillation framework that integrates dynamic adaptive gating to align and generate virtual text.\n\u2022 We introduce the combination of layer-wise contrastive learning and optimal transport loss to align intermediate representations and output distributions, a novel approach in the context of time-series forecasting.\n\u2022 We demonstrate the effectiveness of the proposed framework through extensive experiments, achieving near state-of-the-art performance on both long-term and short-term forecasting tasks across benchmark time-series datasets."}, {"title": "II. PRELIMINARIES", "content": "In this study, we employ a dual-tower architecture to facilitate the transfer of knowledge from the textual domain ($D_{text}$) to the time-series domain ($D_{time}$) via a transformation function T:\n\n$T: D_{text} \\rightarrow D_{time}$\n\nWe define the domains as follows:\n\n$D = \\{p(X), p(X, y), p(y)\\}$\n\nIntroducing the intermediate layer representation h, In the model, we have $p(X,y) = \\int p(y | h) p(h | X) p(X) dh$ and $p(y) = \\int \\int p(y | h) p(h | X) p(X) dh dX$."}, {"title": "III. METHOD OVERVIEW", "content": "As shown in fig. 2, we propose a dual-tower cross-modal knowledge distillation framework for time series forecasting using a language model. Both the time series student and language teacher branches share the same architecture and pre-trained weights. First, the time-series is encoded (section III-A1). Then, virtual text tokens are generated by combining the encoded time-series and the QR-decomposed language model vocabulary section III-A2 via dynamic adaptive gating (section III-A3). layer-wise contrastive learning (section III-B) and optimal transport loss (section III-C) ensure alignment between the time series and virtual text representations, with the overall loss defined in section III-D."}, {"title": "A. Dynamic Adaptive Gating for Input Encoding and Alignment", "content": "In the input phase, we encode time-series data into tokens, apply QR decomposition to the word embeddings, and generate virtual text tokens using a dynamic adaptive gating mechanism.\n\n1) Time-Series Encoding:\nGiven a multivariate time series $I \\in R^{L \\times P}$, where L is the sequence length and P is the number of variables, an embedding layer maps each variable across timestamps into a shared latent space [9]. Multi-Head Self-Attention (MHSA) is then applied to obtain the time series tokens $X_{time} = MHSA(Embedding(I)) \\in R^{P \\times M}$, with M as the feature dimension of the pre-trained large language models (LLMs). The resulting token set is $X_{time} = \\{x^{time}_i | i = 1,2, . . ., P\\}$, where $x^{time}$ are ordered tokens, and i denotes their position in the sequence.\n2) QR-decomposed Word Embedding:\nIn pre-trained LLMs, word embeddings serve as pivotal anchors, structuring and expanding the input distribution within the feature space. To leverage this structured distribution, the vocabulary of the language model $D = \\{e^{text}_1, v^{text}_2, ..., v^{text}_n\\}$ is derived by performing QR decomposition on the word embedding dictionary $D \\in R^{|A| \\times M}$ to reduce its dimensionality. Specifically, the QR decomposition factorizes the matrix D into an orthogonal matrix Q and an upper triangular matrix R. By selecting the first d vectors from Q (corresponding to the rank of R), we construct a reduced principal embedding matrix $D = Q[:,:d] \\in R^{d \\times M}$, where d < |A|. The orthogonality of Q ensures that the selected vectors are linearly independent, unlike those obtained through PCA [31] or representative selection [22], allowing them to effectively span a subspace of the language model's feature space.\n3) Virtual Text Generation via Dynamic Adaptive Gating:\nThe text representation $X_{text} \\in R^{P \\times M}$ is generated from $X_{time}$ by applying self-attention within the time series token and prompt-enhanced cross-attention with D. The attention mechanism is described by:\n\n$X_{att} = Softmax(\\frac{Q K^T}{\\sqrt{C}}) V,$\n\n$Q = X_{time}W_q$,\n\n$K_{self}, V_{self} = X_{time} W^{self}_k, X_{time} W^{self}_v (self-attention)$,\n\n$K_{cross}, V_{cross} = [P_k; DW^{cross}_k], [P_v;DW^{cross}_v](Cross-Attention)$,\n\n(1)\n\nThe primary difference between self-attention and cross-attention lies in the source of the keys (K) and values (V), resulting in $X_{self}$ and $X_{cross}$ being obtained. $P_k \\in R^{L_p \\times d_k} and P_v \\in R^{L_p \\times d_k}$ are the learnable prompt matrices with $L_p$ being the prompt length and $d_k$ being the dimensionality of each attention head. The prompts guide the cross-attention process, aligning time series data with text representations by focusing on task-relevant features [35], and compensating for the possible absence of key-value pairs, ensuring effective handling of unseen time-series tokens. The final text representation $X_{text}$ is obtained by adaptively combining the outputs from self-attention and cross-attention:\n\n$g = Sigmoid (W_{gate} [X_{cross}; X_{self}]) $\n\n$X_{text} = X_{cross} \\otimes g + X_{self} \\otimes (1 \u2212 g),$\n\n(2)\n\nHere $W_{gate} \\in R^{2d_k \\times d_k} and g \\in R^{d_k \\times d_k}$. This dual attention effectively integrates the time series and language information, enabling the generation of virtual text data based on time-series features, thereby facilitating cross-modal learning and downstream applications."}, {"title": "B. Layer-Wise Contrastive Learning for Intermediate Network Alignment", "content": "To align the outputs of each intermediate layer in the temporal student branch with those in the textual teacher branch, we use a layer-wise contrastive learning method. This approach enhances the alignment of semantically congruent information between modalities by optimizing relative similarity in the feature space, rather than minimizing absolute differences [31]. Specifically, we apply the InfoNCE loss at each layer:\n\n$L_{InfoNCE} = - \\frac{1}{N} \\sum_{i=1}^{N} log\\frac{exp (sim (F_{time}, F_{text}) /\\tau)}{\\sum_{j=1}^{K} exp (sim (sim (F_{time}, F_{text})/\\tau)'}$        (3)\n\nwhere sim(,) denotes the similarity function, and $ \\tau$ is the temperature parameter. The terms $F_{time}()$ and $F_{text}()$ represent the aggregated intermediate features from the temporal and textual modalities, respectively, where average pooling has been applied to combine all tokens within each layer. The overall feature loss $L_{feature}$ integrates these losses across layers:\n\n$L_{feature} = \\sum_{m=1}^{L} \\gamma^{(L-m)}L_{InfoNCE},$\n\n(4)\n\nwhere a decay factor $ \\gamma^{(L-m)}$ is applied to give more weight to losses from layers closer to the input, reflecting their importance in the alignment process. This ensures that corresponding time-series and language token representations are closely aligned, while non-corresponding tokens are pushed apart."}, {"title": "C. Optimal Transport-Driven Alignment in the Output Layer", "content": "To achieve coherent and unified predictions from both the time-series and language modalities, we adpot optimal transport (OT) [36] to ensure distribution consistency in the output layer. Unlike the traditional $L_2$ loss, which captures pointwise differences, OT aligns these distributions holistically, making it more effective at capturing semantic relationships between the two modalities. The loss function is defined as:\n\n$L_{ot} = \\sum_{i,j} P_{ij} W_{ij} + \\mu H(P_{ij})$\n\n(5)\n\nWhere $P_{ij}$ represents the optimal transport plan, and $W_{ij} = W(Y_{time}, Y_{text})$ is the cost matrix based on the pairwise distances between $Y_{time}$ and $Y_{text}$. The entropy term $H(P_{ij})$ regularizes the transport plan, with $ \\mu$ controlling the trade-off between minimizing transport cost and encouraging smoother solutions. The Sinkhorn algorithm [37] is employed to solve this problem. By using this $L_{ot}$ loss, we ensure that the different modalities converge, maintaining a consistent semantic space for high-level tasks in multimodal learning."}, {"title": "D. Total Loss Function for Cross-Modal Distillation", "content": "To ensure effective training and alignment of the model across modalities, we employ a combined loss function that integrates various components. Specifically, the total loss for the textural student branch, following [31], [38], is a weighted sum of the supervised loss of task $L_{task}$, the feature alignment loss $L_{feature}$, and the output consistency loss $L_{ot}$. The total loss $L_{total}$ is given by:\n\n$L_{total} = L_{task} + \\alpha L_{feature} + \\beta L_{ot}$\n\n(6)\n\nwhere $ \\alpha$ and $ \\beta$ are hyperparameters that balance the contributions."}, {"title": "IV. EXPERIMENTS", "content": "We evaluated our framework on standard time-series forecasting benchmarks, comparing it with state-of-the-art models for both long-term table I and short-term forecasting section IV, in line with competitive research [7], [20], [31]. Ablation studies (table III) were conducted to assess the impact of individual components. All experiments were conducted under uniform settings across models, adhering to established protocols [7], with evaluation pipelines available online\u00b9.\n\nA. Implementation Details.\nFollowing [23], we used GPT-2 [41] with its first six Transformer layers as the backbone. The Adam optimizer [42] was applied with a learning rate of 5 \u00d7 10-4. Loss hyperparameters were set to \u03b1 = 0.1, and \u03b2 = 0.01. Task loss is smooth L1 for long-term forecasting, and smape for shot-term forecasting. The Optimal Transport parameter is set to \u03bc = 0.1, and the number of iterations is set to 100. InfoNCE is implemented based on the Lightly toolbox\u00b2. The decay factor is set to \u03b3 = 0.8 to balance the lower and higher layers.\n\nB. Experiment Results.\nOverall Performance. 1) Long-term Forecasting: TS-TCD outperforms baseline models like TimeLLM [20] and CALF [31], as shown in table I. On the ETTm2 dataset, it achieves an MAE of 0.301, a 6.2% improvement over CALF (0.321). For Electricity, TS-TCD achieves an MSE of 0.164, 6.3% lower than CALF's 0.175. It also demonstrates superior performance on Weather and Electricity datasets, effectively capturing long-term dependencies. 2) Short-term Forecasting: In short-term tasks, TS-TCD leads with the lowest SMAPE (11.651), MASE (1.563), and OWA (0.837) across average intervals section IV, improving on CALF by 3.5%. Overall, LLM-based methods [20], [23], [31] outperform traditional MLP-based [14], [15], CNN [5]\u2013[7], and Transformer models [8]\u2013[13], due to their enhanced ability to capture contextual information and generalize across diverse tasks. TS-TCD demonstrates higher accuracy and resilience across both long- and short-term forecasting, making it a versatile solution for time-series data.\nAblation Studies. We performed ablation studies to assess the impact of each component: Dynamic Adaptive Gating (DAG), feature loss (Lfeature), and optimal transport loss (Lot). Table III shows that removing DAG increases SMAPE by 3.4% (+0.394), proving its importance. The full model achieves the best SMAPE (11.651), confirming the necessity of all components. Alignment modules roughly have the greatest impact at the input stage and the least at the output stage.\nVisualization. fig. 3 shows long-term forecasting results for the Electricity dataset. On the left, the prediction is for 96 time steps, while on the right, it extends to 720 time steps. In both cases, our model consistently performs well, closely aligning its predictions with the ground truth."}, {"title": "V. CONCLUSION", "content": "In this paper, we proposed TS-TCD, a novel framework designed to enhance time-series forecasting by integrating LLMs through a triplet-level cross-modal distillation process. Unlike existing methods that focus on individual alignment components, TS-TCD provides a holistic approach by aligning input, feature, and output layers across modalities. Our dynamic adaptive gating mechanism addresses input modality mismatch by generating virtual text tokens that are coherently aligned with time-series data. Additionally, layer-wise contrastive learning ensures intermediate feature consistency, while optimal transport-driven output alignment reduces discrepancies at the task level. Experimental results highlight the framework's superior performance across multiple benchmarks, confirming the value of integrating these mechanisms."}]}