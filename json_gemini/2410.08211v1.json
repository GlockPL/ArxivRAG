{"title": "LATTECLIP: Unsupervised CLIP Fine-Tuning via LMM-Synthetic Texts", "authors": ["Anh-Quan Cao", "Maximilian Jaritz", "Matthieu Guillaumin", "Raoul de Charette", "Loris Bazzani"], "abstract": "Large-scale vision-language pre-trained (VLP) models (e.g., CLIP [46]) are renowned for their versatility, as they can be applied to diverse applications in a zero-shot setup. However, when these models are used in specific domains, their performance often falls short due to domain gaps or the under-representation of these domains in the training data. While fine-tuning VLP models on custom datasets with human-annotated labels can address this issue, annotating even a small-scale dataset (e.g., 100k samples) can be an expensive endeavor, often requiring expert annotators if the task is complex. To address these challenges, we propose LATTECLIP, an unsupervised method for fine-tuning CLIP models on classification with known class names in custom domains, without relying on human annotations. Our method leverages Large Multimodal Models (LMMs) to generate expressive textual descriptions for both individual images and groups of images. These provide additional contextual information to guide the fine-tuning process in the custom domains. Since LMM-generated descriptions are prone to hallucination or missing details, we introduce a novel strategy to distill only the useful information and stabilise the training. Specifically, we learn rich per-class prototype representations from noisy generated texts and dual pseudo-labels. Our experiments on 10 domain-specific datasets show that LATTECLIP outperforms pre-trained zero-shot methods by an average improvement of +4.74 points in top-1 accuracy and other state-of-the-art unsupervised methods by +3.45 points.", "sections": [{"title": "1. Introduction", "content": "Large-scale vision-language pre-training [46] has emerged recently and demonstrated impressive generalization performance on various downstream tasks [7,8,23,30, 44], especially in zero-shot classification [46, 47]. This success is attributed to its robust visio-linguistic representation, learned from a vast amount of large-scale web-scraped datasets [48]. However, these models often face challenges in specialized domains due to domain discrepancies and insufficient representation in the training data. Prior studies have demonstrated improvements on custom datasets through supervised fine-tuning [15, 60] or few-shot learning [50,68]. Nevertheless, acquiring human-annotated labels is costly, even for relatively small datasets (e.g., 100k samples), and often requires expert annotators for complex tasks. To address this, we propose LATTECLIP, which fine-tunes CLIP for classification on unlabeled training data to maximize performance on a test set from the same domain. Here, a domain refers to a set of shared characteristics within a dataset (e.g., cars, flowers, textures). Like in Unsupervised Domain Adaptation (UDA) [12, 19, 58], we consider the list of class names to be known a priori. An overview of LATTECLIP is shown in Fig. 1.\nRecent progress of Large Language Models (LLMs) [3,\n24,41,55,56] and Large Multimodal Models (LMMs) [4,34] have led to a fundamental shift in training and fine-tuning methodologies. The research community is transitioning from a class-focused paradigm towards a more descriptive approach, where data is annotated with detailed textual de-"}, {"title": "2. Related works", "content": "Adapting CLIP for Classification.\nCLIP-based meth-ods [46, 62, 63, 65] exhibit competitive zero-shot classification performance. For further improvement on downstream classification datasets, CLIP can be adapted to close the gap between pre-trained representations and specific domains.\nIn few-shot learning, one has access to a small number\nof labels, typically between 1-16 samples per class, and\nmany works have adapted it to CLIP [9, 13, 14, 38,45,50,64,\n66, 68, 69]. Prototypical learning [51] is a seminal work in\nfew-shot learning and builds an average embedding (proto-types) for each class. During inference, one then matches the\ntest sample to the nearest prototype. This concept recently\nre-emerged for adapting CLIP, by building a cache model\nholding the knowledge from the few-shot training set [66,69].\nDifferent from that, we continuously update the prototypes\nwith momentum during the training process with multimodal\nfeatures from unsupervised texts and images. Other works\nleverage prompt learning [68] or efficient fine-tuning [9, 64].\nSupervised fine-tuning methods require a significant\namount of labeled examples for training [15, 26, 46, 59, 60].\nLinear probing [46, 60] is a simple technique that trains a\nclassifier on top of frozen image features, but can lead to\nworse results due to overfitting. This problem has been tack-led by using two-step training schedules of linear probing\nand full fine-tuning [26], masked image modeling [59] and\nby fine-tuning with contrastive loss by aligning the image\nwith a template text including the class label (FLYP [15]).\nIn contrast to FLYP, we add LMM-generated descriptions to\nthe contrastive loss, and stabilize unsupervised training by\nlearning prototypes with momentum.\nDifferent from few-shot and fine-tuning, we focus on the\nchallenging scenario of unsupervised fine-tuning, where no\nlabels are available, because they are too costly to annotate.\nUnsupervised Model Adaptation Our Unsupervised fine-tuning task is related to Unsupervised Domain Adaptation, where one typically reduces the discrepancy between the source and target data. However, lately, the task of source-free domain adaptation (SFDA) has emerged, where target adaptation is performed without access to the source data, see survey paper [31]. Many methods exploit that the source model can partially generalize to the target domain, and fine-tune with pseudo-labels [33], adversarial learning [32], historical contrastive learning [21] or mixup [27]. While [21] perform momentum contrastive learning [17] on different"}, {"title": "3. Method", "content": "Fine-tuning CLIP with combination of predefined tem-plates, such as \"a photo of a [class].\", was shown to yield effective results when using ground-truth class labels [15, 26, 60]. However, in the absence of ground-truth class labels, fine-tuning CLIP models with pseudo-labels [29], using FLYP [15], leads to limited improvements\u00b9 This can be caused by two factors. First, the text employed as supervision, resulting from the combination of the template and pseudo-label, lacks expressivity and discriminativity. This is typically the case for classes that are not visually descriptive, such as types of land use (e.g., annual crop, industrial, etc.) or names of textures (e.g., paisley, sprinkled, etc.). Second, pseudo-labels are inherently noisy, which negatively affects the downstream classification performance due to domain shifts relative to the original training data.\nOur method, LATTECLIP, addresses these limitations by proposing an expressive unsupervised text genera-tion (Sec. 3.1) and a prototype-based learning mechanism (Sec. 3.2) to mitigate noisy pseudo labels. To improve expressivity beyond pseudo-labeling, we build upon a recent LMM [34], generating descriptions at multiple levels of contextual granularity, describing the individual image, group of similar images, and entire class. Individual image descriptions offer detailed though possibly extraneous information, which is addressed by group descriptions that capture shared characteristics of similar images, albeit with some noise. This noise is mitigated by class descriptions, which provide stable representations to address inconsistencies. Equipped with such textual description, we additionally introduce a prototype-based learning framework that learns a set of class prototypes from the generated text features. These prototypes are updated in a momentum setting to produce a smooth optimization over the whole training set, reducing the effect of noise from outlier samples and incorrect synthesized texts."}, {"title": "3.1. Expressive Text Generation with LMMS", "content": "Without access to ground-truth labels for training CLIP models, we must rely on noisy pseudo-labels. Furthermore, class names alone often lack visual descriptiveness. Consequently, using only cross-entropy loss or solely relying on class names leads to suboptimal performance in our setting. To address this challenge, we introduce a novel approach that leverages generated text to provide additional contextual information. In addition to the more standard class-description mentioned above, we propose two additional ways using a recent LMM [34] to generate textual descriptions of images: image-description and group-description, as depicted in Fig. 2. These generated texts hold complementary information with increasing semantic abstraction, from class\u00b2, to single image, to group of images, all of which help the model to learn more precise classification boundaries. The image-description texts provide detailed descriptions of individual images, capturing their unique characteristics and subtle features. The group-description texts offer a comprehensive description representing the entire class, covering shared features and common attributes.\nImportantly, we found that the above mentioned descriptions are complementary to the use of template text with pseudo-label class, which we refer as class-description. In fact, we later show that preserving this class-description in the training process is crucial as our generated texts can be noisy due to missing details or hallucination. The combination of class-/image-/group-description provides a stable and reliable representation corresponding to the classes.\nMore formally, for each image x we generate three texts, illustrated in Fig. 2, and defined as follows:\nClass-description (Tclass) provides a consistent class representation using template \"a photo of a [class].\" where [class] is substituted with the image pseudo-label c obtained from a CLIP zero-shot.\nImage-description (Timage) captures unique features of image x. We generate Timage by prompting LLAVA [34] with: \"Describe the [domain] in the photo concisely, using less than 20 words.\" where [domain] is replaced with the dataset domain (e.g., flower, product, pet, car, etc.). We show image-description examples in Fig. 5.\nGroup-description (Tgroup) captures shared visual characteristics between similar images, to combat known limitation of LMM which may miss or hallucinate visual characteristics [36]. To generate Tgroup from image x, we randomly sample multiple images with the same pseudo-label as x. These are collaged into a single image xgroup fed to LLAVA which is prompted with: \"Describe the common visual attributes of the [domain] in all the photos concisely, in fewer than 20 words.\". Examples of such group-descriptions are illustrated in Fig. 5.\nTo generate these descriptions, we use LLAVA 1.6 [34] with a 4-bit quantized Mistral 7B model. This model requires approximately 5GB of GPU memory and takes around 1.2 seconds to generate a single description per image on a Tesla V100 GPU. This makes description generation relatively cost-effective, as we can run five instances of this model in parallel on a Tesla V100 32GB GPU, taking approximately 3.4 hours to generate descriptions for 50k images."}, {"title": "3.2. Prototype-based CLIP fine-tuning", "content": "Adopting directly the generated texts from Sec. 3.1 is ineffective, because the text encoder overfits to the distribution of generated texts, which are noisy by construction due to hallucinations of the LMM and missing details. We confirm this experimentally in Tab. 2, rows 4, 5, 6. Therefore, we propose a prototype learning approach that is capable of determining the important synthetic texts and learning better class representations from them. Our approach mixes three key ingredients as shown in Fig. 3: (1) a simple strategy to preserve robustness by leveraging pseudo-labels from both frozen and fine-tuning CLIP models; (2) a feature mixer that dynamically balances the importance of each text Tclass, Timage and Tgroup; (3) a module that updates the prototypes during training, stabilizing the learning process.\nDual Pseudo-labels. As in WISE-FT [60], we observe that training only with pseudo-labels from the fine-tuning model improves accuracy but at the cost of overfitting to the training distribution. Hence, to preserve robustness, for each image we employ two pseudo-labels {Czs, Cft} originating from both the zero-shot model (Czs) and the fine-tuning model (Cft). We later show that this simple strategy offers greater generalization and accuracy.\nPrototype Learning. From the generated texts and pseudo-labels, we aim to learn a set of prototypes corresponding to all classes, denoted as {pc}_C=1. These prototypes are designed to capture class-specific details of the synthesized texts and pseudo-labels within the CLIP embedding space. First, the prototypes are initialized with features derived from the Tclass, generated based on its associated class name. Then, for an image x, we use our dual pseudo-labels from zero-shot and fine-tuning {Czs, Cft} to generate two class-description texts {Tclass, Tclass} and select the corresponding prototypes Pzs and Pft. Our feature mixer strategy, detailed below, then combines the two class-descriptions with the image-description and the group-description, therefore obtaining two prototype-text embeddings tzs and tft, see Fig. 3. We then apply a momentum update to the corresponding prototypes. Finally, we apply two contrastive losses [15] between the image embedding f(x) and each of the prototype-text embeddings tzs and tft.\nDynamic Feature Mixer. To compensate for noisy text descriptions, we propose a mechanism that dynamically re-weights the three descriptions as a function of the cosine similarity between each description embedding and corresponding prototype, see Fig. 4. Intuitively, our goal is to assign higher weights to descriptions uniquely describing a class and lower weights to generic descriptions. In the general case, for a text T we first compute the cosine similarities between its CLIP embedding g(T) and each of the prototypes, and obtain its weight w from the difference between the two closest similarities. This writes:\n$$w=top1(\\frac{g(T) \\cdot Pc}{||g(T) ||||pc||})_C - top2(\\frac{g(T) \\cdot Pc}{||g(T)||||pc||})_C$$\nwhere top1(\u00b7) and top2(\u00b7) return the largest and second largest values of the input set, respectively. A high weight indicates a large gap between top1(\u00b7) and top2(\u00b7) similarities, ensuring the text feature is uniquely similar to a single prototype while dissimilar from the rest, as top2(\u00b7) value serving as an upper bound for the similarity of the remaining prototypes. Alternatively, we could use the mean or median, but this might result in a text being very similar to a few prototypes while remaining dissimilar to others. Subsequently, given the set of texts {Timage, Tgroup, Tclass} and the weights {wimage, wgroup, wclass} computed using Eq. (1). The resulting prototype embedding t is defined as\n$$t = (1 - a)\\frac{\\Sigma_{i \\in I} w_i^2 g(T_i)}{\\Sigma_{i \\in I} w_i} + a p_c$$\nwhere I={image, group, class} and a is the prototype weight. We empirically set a to 0.99 in all experiments to stabilize training, as the prototypes are more reliable than the synthetic text embeddings. Thus, this act a strong regularization mechanism against the noise induced by the synthetic texts. Yet, t remains tailored for each image as Timage and Tgroup differ. With two pseudo-labels per image, this results in two prototype-text embeddings {tzs, tft}.\nTraining. Given an image x, we train both the image encoder f(\u00b7) and text encoder g(\u00b7) using contrastive loss to align the image embedding f(x) with both the prototype-text embeddings tzs and tft, resulting in two losses Lzs = Lcon(x, tzs) and Lft = Lcon(x, tft) respectively with Lcon(\u00b7, \u00b7) defined as:\n$$Lcon(x,t) = \\frac{1}{N} \\Sigma_{i=1}^{N} log(\\frac{exp(f(x)\\cdot t_i/\\tau)}{\\Sigma_{j=1}^{N} exp(f(x)\\cdot t_j/\\tau)}) + \\frac{1}{N} \\Sigma_{i=1}^{N} log(\\frac{exp(t_i\\cdot f(x)/\\tau)}{\\Sigma_{j=1}^{N} exp(t_j\\cdot f(x)/\\tau)})$$\nwhere N is the batch size and \u03c4 is the temperature parameter as in [46]. The first term of Eq. (3) normalizes over text embeddings to match the correct text to an image, while the second normalizes over image embeddings to match the correct image to a text. The final loss is Lzs + Lft.\nMomentum update prototypes. For a pseudo-label c, we derive the corresponding prototype-text embedding t for each image. During training, the average prototype-text embedding tbatch is computed over the images in the batch. Using the pseudo-label c, we update the respective prototype pc with a momentum \u03bc, obtaining the updated embedding pc = (1 - \u03bc)tbatch + \u03bcpc, which is then stored back in the prototype bank as the prototype for class c. Momentum update works effectively when \u03bc\u2208 {0.99, 0.999, 0.9999} [17]. As we fine-tune on smaller dataset with fewer iterations, we set \u03bc to 0.99 for faster updates of the prototypes. Intuitively, the prototype can be viewed as the running average"}, {"title": "4. Experiments", "content": "We evaluate LATTECLIP on the task of fine-tuning on 10 specialized classification datasets, without using any ground truth labels. We use the training set for unsupervised training and use the test set to compute the top-1 accuracy.\nDatasets. We employ a mixture of datasets covering various specialized domains, including satellite imagery, food dishes, airplane models, and others: EuroSAT [18], SUN397 [61], Food101 [5], DTD [6], FGVC [37], Oxford Pets [43], Cars [25], UCF101 [52], Caltech101 [11], Flower102 [40]. These datasets feature specific classes, such as the car model, making the unsupervised fine-tuning setup challenging. We follow the standard train/val/test splits in [68]. We train LATTECLIP using the combined train and val sets and report its performance on the test set.\nBaselines. We compare our method to four unsupervised baselines and one fully supervised baseline, which serves as an oracle. First, we perform zero-shot classification with a pre-trained CLIP model. As in CLIP [46], we compute text embeddings for all classes with template \"a photo of a [class].\". For classification, we compute the cosine similarity between each image and all class text embeddings. Our second baseline, ReCLIP [20], also performs fine-tuning without labels but utilizes improved pseudo labels and self-training. However, ReCLIP primarily focuses on experiments conducted in a transductive manner, which involves training and evaluating on the test split of each dataset. To ensure a fair comparison, we retrained ReCLIP using the same CLIP-based model and identical dataset splits as our method. Third, we combined FLYP [15] with pseudo-labeling [29] for unsupervised fine-tuning, as the original method relies on supervised fine-tuning. Note that we use FLYP without weight ensembling to maintain a fair comparison with ReCLIP, which also does not employ weight ensembling. Finally, we add \"LLAVA zero-shot\" baseline which prompts LLAVA to classify the image from a given list of classes, using the following prompt \"Select the most appropriate category for the image from the following options: [options]. Write only the category name.\", where options is replaced with the list of class names. For the supervised baseline, we train FLYP using ground-truth labels, serving as an oracle. The evaluation is performed in a zero-shot fashion, like zero-shot CLIP. Since our method uses prototypes, no class template embeddings have to be computed, and we directly use the prototype vectors. For all baselines and ours, we use OpenCLIP [22], the open-source implementation of CLIP [46], with a ViT/B-32 architecture, pre-trained on the LAION-2B dataset. Performance is reported based on the last epoch since we have no supervision signal. Additional implementation details are in Appendix B."}, {"title": "4.1. Results", "content": "The main results with top-1 accuracy on the 10 datasets are shown in Tab. 1. Across all datasets, LATTECLIP improves the average top-1 accuracy of CLIP by 4.74 points. Furthermore, it outperforms all unsupervised baselines, including the recently published ReCLIP [20] and our proposed baseline that integrates FLYP [15] with pseudo-labeling [29], by 3.45 and 2.22 points, respectively. Interestingly, FLYP + pseudo-label outperforms ReCLIP, likely due to the robustness and effectiveness of fine-tuning both image and text encoders with contrastive loss, instead of just the image encoder with cross-entropy loss, as demonstrated in FLYP [15]. Notably, LLAVA zero-shot has low overall performance, which could be attributed to LLAVA being trained in generative autogressive manner, thus not optimal for discriminative tasks. Lastly, the oracle is shown on the first line by training FLYP with ground-truth labels. The 9.53-point average performance gap between the fully supervised oracle and unsupervised LATTECLIP highlights room for improvement. Still, LATTECLIP performs competitively, narrowing the gap across multiple datasets, particularly on Oxford Pets, Cars, and Caltech101, to less than 3%."}, {"title": "4.2. Ablations", "content": "Different types of synthetic descriptions. Tab. 2 illustrates the impact of different generated texts on overall performance. We observe that all texts are essential for achieving the best performance. Specifically, excluding the image-description reduces the average performance across all datasets by 0.6 (row 1 vs. row 7). The impact of removing the group-description is even more significant with a 1.49 points reduction (row 1 vs. row 2). Additionally, omitting both the image-description and group-description results in an even larger loss of 1.56 points (comparing row 1 to row 3). Rows 4, 5, and 6 show that relying solely on synthetic texts causes a drop in performance due to the noise and inaccuracies introduced by the generated descriptions.\nDynamic Feature Mixer. We ablate our Dynamic Feature Mixer in Tab. 3 (row \"w/o Dynamic Feature Mixer\") by setting all the text weights to 1.0, so that all texts contribute equally. The average performance drops by 2 points, with significant decreases on multiple datasets, such as -14.23 on EuroSAT, -1.41 on DTD, and -2.03 on Cars. This demonstrates that our Dynamic Feature Mixer module effectively assigns relevant weights to the meaningful descriptions.\nDual Pseudo-Labels. Best performance is achieved using both zero-shot and fine-tuning pseudo-labels {Czs, Cft}. This is assessed in Tab. 3 by removing the corresponding losses. Removing the zero-shot pseudo-label (row \"w/o Lzs\") leads"}, {"title": "5. Conclusion", "content": "LATTECLIP is a novel method for unsupervised CLIP fine-tuning on specialized datasets where human annotations are costly or require expert knowledge. Leveraging LMMs, LATTECLIP generates rich and expressive synthetic textual descriptions at various levels of contextual granularity, including image-description, group-description, and class-description. To effectively learn from these potentially noisy descriptions, we propose a prototype learning framework with three key elements: (1) dual pseudo-labels from frozen and fine-tuning CLIP models; (2) a Dynamic Feature Mixer for optimal text feature weighting; and (3) momentum update to enhance training stability. LATTECLIP surpasses comparable baselines on average across all datasets."}, {"title": "A. Limitations", "content": "Despite promising results, LATTECLIP considers a limited number of description types. Expanding description generation to include more contextual levels, such as scenes, objects, and attributes, would provide richer contextual information. Additionally, our performance is constrained by the underlying LMM model, and improvements could be made with better models in the future. Lastly, it is unclear why the method improves on some datasets but not others. Understanding this discrepancy could lead to better methods."}, {"title": "B. Implementation details", "content": "We implement LATTECLIP based on the standard fine-tuning pipeline of OpenCLIP [22] using the VIT-B/32 model. The hyperparameters used are the default ones provided in OpenCLIP [22], except for batch size and learning rate.We use a batch size of 512 and a learning rate of le-7 for the datasets Caltech101 [11], DTD [6], Eurosat [18], FGVC [37], Oxford Pets [43], Cars [25], Flower102 [40], and UCF101 [52]. For the datasets Food101 [5] and SUN397 [61], we use a learning rate of le-6. LATTECLIP is trained for min{2000 iterations, 50 epochs}.\nFor FLYP [15], we reimplement it based on its official implementation\u00b3 and OpenCLIP [22], as its idea is intuitive and simple: fine-tuning using contrastive loss with class templates instead of cross-entropy loss. We use the same OpenCLIP-based model and training hyperparameters as LATTECLIP. The pseudo-labels are recalculated after every weight update, following [29].\nFor ReCLIP [20], we use the official implementation4, but substitute OpenCLIP as the base CLIP model to ensure a fair comparison across all methods. While ReCLIP is designed for transductive learning (train/test on test set), as shown in the paper and by its official implementation, we adapt it to our experimental setup. Specifically, we retrain and evaluate ReCLIP using identical dataset splits as LATTECLIP."}, {"title": "C. Additional ablations", "content": "Incorrect images in generating Tgroup. Tab. 4 presents the results across all datasets when varying the number of correct images, which are selected using ground-truth labels, in groups of 4 images used for generating group-descriptions. Using more correct images generally leads to improvements in most datasets. However, the average performance gap remains small, demonstrating the robustness of our method"}, {"title": "D. Additional results", "content": "Examples of LMM-synthetic texts and pseudo-labels.Fig. 7 illustrates examples of image-description Timage and group-description Tgroup generated from individual images x and image groups xgroup, respectively. The figure also presents ground-truth labels (GT) along with pseudo-labels derived from the frozen CLIP model (Czs) and the fine-tuning model (Cft). Note that the class-description is generated by substituting the pseudo-label c\u2208 {Czs, Cft} into a pre-defined template: \"a photo of a [c].\". Combining both types of pseudo-labels increases the chance of capturing"}]}