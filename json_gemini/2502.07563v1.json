{"title": "LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid", "authors": ["Weigao Sun", "Disen Lan", "Yiran Zhong", "Xiaoye Qu", "Yu Cheng"], "abstract": "Linear sequence modeling approaches, such as linear attention, provide advantages like linear-time training and constant-memory inference over sequence lengths. However, existing sequence parallelism (SP) methods are either not optimized for the right-product-first feature of linear attention or use a ring-style communication strategy, which results in lower computation parallelism, limits their scalability for longer sequences in distributed systems. In this paper, we introduce LASP-2, a new SP method to enhance both communication and computation parallelism when training linear attention transformer models with very-long input sequences. Compared to previous work LASP, LASP-2 rethinks the minimal communication requirement for SP on linear attention layers, reorganizes the whole communication-computation workflow of LASP. In this way, only one single AllGather collective communication is needed on intermediate memory states, whose sizes are independent of the sequence length, leading to significant improvements of both communication and computation parallelism, as well as their overlap. Additionally, we extend LASP-2 to LASP-2H by applying similar communication redesign to standard attention modules, offering an efficient SP solution for hybrid models that blend linear and standard attention layers. Our evaluation on a Linear-Llama3 model, a variant of Llama3 with linear attention replacing standard attention, demonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2 achieves training speed improvements of 15.2% over LASP and 36.6% over Ring Attention, with a sequence length of 2048K across 64 GPUs.", "sections": [{"title": "1. Introduction", "content": "Transformer, originally introduced by Vaswani et al. (Vaswani et al., 2017), has become the backbone of modern models across a wide range of domains, including language, vision, audio, video, graphs, and even time-series data (Achiam et al., 2023; Team, 2023; Qu et al., 2024). Although the Transformer dates back to 2017, its adaptability and robustness have made it indispensable for a variety of tasks. Central to its success is the self-attention mechanism, which is highly effective for sequence modeling, but has quadratic complexity (w.r.t. sequence length), leading to significant computational costs during training. However, the ability to handle long-context sequences is crucial for large model applications, not only for language tasks but also for multi-modal tasks, where sequences naturally tend to be long (Xue et al., 2024). FlashAttention series (Dao et al., 2022; Dao, 2023; Shah et al., 2024) have provided substantial advancements in scaling attention to handle longer sequences by optimizing the CUDA-level computations for better hardware utilization. However, the theoretical complexity of FlashAttention remains quadratic. Moreover, the need to maintain the KV cache presents further difficulties in managing memory as the sequence length extends (Qin et al., 2024c). As a result, long-sequence processing in Transformer models continues to be a complex and resource-intensive problem.\nRecently, numerous variations of attention have been proposed, primarily aimed at addressing its quadratic computational and memory complexity, as well as the growing size of the KV cache (Peng et al., 2023; 2024). One promising approach line is linear attention (Katharopoulos et al., 2020), which replaces the exponential kernel in softmax attention with a simpler dot product between key and query vectors. This shift allows linear attention to be structured as a linear recurrent neural network (RNN) with matrix-valued hidden states, thereby eliminating the need for a KV cache. In consequence, it supports constant-memory inference and reduces training complexity from quadratic to linear (Yang et al., 2023). A parallel line of research focuses on State Space Models (SSMs), such as Mamba (Gu & Dao, 2023) and Mamba 2 (Dao & Gu, 2024), which draw upon concepts from control theory. Both linear attention and SSMs share a common recurrent formulation, expressed as Ms = Ms-1 + Ms, where M, represents the incremental memory state of the s-th token (Yang et al., 2024). However, despite these advantages, they tend to perform poorly on recall-intensive tasks, such as in-context learning (e.g., five-shot MMLU (Hendrycks et al., 2020), Phone-book lookup (Jelassi et al., 2024), Needle In A Haystack (Briakou et al., 2023)) and long-context reasoning. Empirical research (Lieber et al., 2024; Ren et al., 2024; Waleffe et al., 2024; Li et al., 2025) has shown that models relying solely on linear sequence modeling struggle to excel in these domains. However, a hybrid architecture combining linear sequence modeling layers with standard transformer layers has been demonstrated to significantly enhance model performance on tasks that are recall-intensive.\nSequence Parallelism (SP) techniques (Korthikanti et al., 2022; Jacobs et al., 2023; Liu et al., 2023) are commonly employed to partition long sequences into smaller sub-sequences, allowing them to be processed across multiple GPUs in parallel. Despite the advantages offered by SP for handling large sequences, current SP methods do not fully exploit the right-product-first feature of linear attention, which can lead to inefficiencies in parallelism and communication. LASP (Sun et al., 2024a) (referred to as LASP-1) introduced a SP approach specifically tailored for linear attention, that uses a point-to-point (P2P) communication strategy. In this method, intermediate states are transferred across GPUs in a ring-style pattern within the distributed world. However, although such P2P ring-style communication offers certain benefits, part of its computation has to be executed sequentially, which leads low computation parallelism. In addition, too many small P2P operators make the overlapping of communication and computation difficult.\nIn this paper, we introduce LASP-2 by rethinking the minimal communication requirement involved in SP of linear attention. Specifically, we innovatively reorganize the whole computation and communication workflow with an optimized execution mechanism. In this way, only a single all-gather collective communication is needed in the forward or backward of each iteration. These bring both communication and computation efficiency improvements: 1) the size of intermediate memory state tensor the all-gather operator works on is independent of the sequence length, making the communication burden insignificant in the context of long sequences. The communication parallelism and accessibility to overlap with computation are notably improved. 2) the refactored workflow improves both communication and computation parallelism over multiple devices. Additionally, we separately present LASP-2 with and without masking for autoregressive and bidirectional tasks, respectively, as the presence of a mask matrix significantly impacts the design criterion of LASP-2. To extend LASP-2 to hybrid models with both linear and standard attention layers, we introduce LASP-2H. This extension employs the same all-gather-based communication for standard attention layers,"}, {"title": "2. Preliminary", "content": "Notation In this paper, we ensure the consistent use of notations to enhance clarity. Table 1 provides a complete list of all the symbols utilized throughout, including indices, constants, vectors, and matrices. Vectors and matrices are represented in boldface. For simplicity, we have omitted the dimensions related to batch size and number of heads in tensor shapes.\nLinear Attention The term \"attention\" generally refers to a computation that assigns scores to pairs of positions within a sequence, enabling each element to \"attend\" to others. The most widely used and significant variant of this mechanism is softmax self-attention, which is central to standard transformer models (Vaswani et al., 2017). During training, with an assumption of a single attention head for simplicity, softmax self-attention computes as follows:\nQ, K, V = XWQ,XWK,XWv,\nO = Softmax(QKT)V.\n(1)\nThe mechanism of pairwise comparisons (induced by materializing QKT) leads to the characteristic quadratic training cost of softmax self-attention. Recently, Linear Attention (Katharopoulos et al., 2020; Shen et al., 2024; Qin et al., 2024a) has gained attention as a potential alternative to softmax self-attention, with two key distinctions. First, it removes the Softmax(\u00b7) operation, incorporating it into a kernel feature map. Second, it leverages the associativity of matrix multiplication to reformulate (QKT)V = Q(KTV)."}, {"title": "3. Method", "content": "3.1. LASP-2 without Masking\nSP methods work by dividing long input sequences into several smaller chunks, which are then distributed across multiple computational devices. Each device independently processes the queries, keys, and values for its assigned chunk in parallel. To complete the attention computation for the entire sequence, necessary communication steps are performed to either gather the results from all devices or exchange information between them. LASP (Sun et al., 2024a) was introduced as a sequence parallelism technique designed specifically for the linear attention module.\nLet us consider a distributed computing setup where there are W devices, and the input sequence is divided into T chunks, referred to as the sequence parallel size. In the usual case, T is evenly divisible by W, and we often assume W = T. It means each chunk is assigned to a single device, ensuring that every chunk is processed in parallel across the distributed system. This scenario exemplifies pure sequence parallelism. Additionally, in Sec.A.4.1, we will explore cases where W \u2260 T, representing a hybrid approach that combines sequence parallelism with data parallelism.\nIn LASP-2, the input sequence X is divided into T smaller chunks, represented as [Xt]1, and each chunk is distributed across the devices in the distributed system. For each chunk Xt, its corresponding query, key, value, and the linear attention memory state can be computed in parallel across all chunks. This parallel computation is carried out as follows:\nQt, Kt, Vt = XtWQ,XtWK, XtWV,\nMt = KVt.\n(5)\nBy performing this concurrent computation for each chunk, LASP-2 efficiently handles long input sequences in a distributed setting. The query Qt, key Kt, value Vt, and the memory state Mt are calculated individually for every chunk of the sequence, ensuring that no single device is overburdened with processing the entire sequence at once. This distributed approach facilitates better memory management and computational efficiency, especially when dealing with extremely long sequences. Thus, LASP-2 leverages the power of sequence partitioning to optimize the calculation of linear attention in a distributed framework.\nNotably, in LASP-2, only a single all-gather collective communication operation is required during the forward pass."}, {"title": "3.2. LASP-2 with Masking", "content": "In autoregressive tasks, the mask matrix \u03a8 \u2208 {\u2212\u221e,1}N\u00d7N is typically a lower triangular matrix, where \u03a8ij = 1 for i \u2265 j and \u03a8ij = -\u221e when i < j. This structure enforces a causal constraint during computation. Specifically, when calculating O = Softmax(QK\u03a8)V, it becomes impossible to leverage the associative property of matrix multiplication to reduce the computational complexity from quadratic to linear in a parallel form.\nTo address this challenge in linear attention with a causal mask, we adopt the approach of computation decomposition, as proposed in earlier work (Yang et al., 2023; Sun et al., 2024a). Figure 1 provides an illustration that highlights the difference between intra-chunk and inter-chunk computations in linear attention. Inter-chunk calculations, which have no dependencies on other chunks across devices, can be treated as if they have no causal mask. As a result, these computations can be parallelized across all devices in the distributed setup. In contrast, intra-chunk calculations account for the influence of previous chunks (1 to (t-1)) on the t-th chunk. These intra-chunk operations are affected by the mask matrix, and therefore, require specialized handling to respect the causal constraints.\nFor linear attention computation on intra-chunks, given the query, key, and value matrices Qt, Kt, and Vt corresponding to the chunk Xt, the output is computed as\nOt,intra = [(QtK) \u03a8]Vt,\n(7)\nThis formulation adheres to the standard left-product matrix multiplication. Although the computation can be executed in parallel across devices, it retains the quadratic complexity commonly associated with traditional attention mechanisms during training. This limitation arises from the element-wise masking operation (\u03a8), which enforces causal constraints within the chunk, preventing the use of optimizations that would reduce the computational cost to linear.\nFor linear attention computation across inter-chunks, we follow a similar approach as the procedure outlined for LASP-2 without masking. First, the memory states for each chunk are computed concurrently across different devices as Mt = Kt Vt. These memory states, corresponding to each chunk, are initially distributed across separate devices. To synchronize the results, an AllGather collective communication operation is performed. This step ensures that all devices hold the memory states for all chunks, enabling further parallel processing. Once the memory states have been gathered, we proceed with a concurrent PrefixSum operation across all devices. This operation accumulates the memory states from the 1st chunk up to the (t \u2212 1)-th chunk, effectively building the necessary intermediate states. This can be expressed as:\n[Mt]] = AllGather([M+][),\nM1:t\u22121 = PrefixSum([M+]1\u00af\u00b9).\n(8)\nThe Prefix Sum operation can be optimized by implementing it recursively, utilizing cached memory states stored on the HBM. Specifically, the accumulation of memory states is computed as:\nM1:t-1 = M1:t-2 + Mt-1.\n(9)\nBy caching M1:t-1, the backward pass computation is facilitated since this cached value is a necessary activation for gradient calculations. This approach not only speeds up the backward pass but also reduces the computational load, as the cached memory state eliminates the need for repeated re-computation.\nFollowing the calculation of the memory states, the outputs corresponding to the inter-chunks and the final output for the t-th token can be derived with ease. The overall output for the t-th token is obtained by summing both the intra-chunk and inter-chunk outputs.\nOt,inter = QtM1:t\u22121, Ot = Ot,intra + Ot,inter.\n(10)\nWe provide the complete algorithm for LASP-2 with masking in Algorithm 2, and its backward pass in Algorithm 4 in Appendix A.1. Note that, in Algorithm 2, the communication operation in line 7 (in magenta), along with the computation of Ot,intra in line 8 (in cyan), can be overlapped by executing them on separate threads. This concurrent execution helps improve overall efficiency, as it allows for the overlap of communication and computation."}, {"title": "3.3. LASP-1 vs LASP-2", "content": "LASP-2, as well as its previous version LASP-1, both aim on efficient SP on linear attention. Although, in theory, LASP-1 and LASP-2 share similarity on communicating the KV activation (d \u00d7 d), whose size is independent of the sequence or chunk length. They have fundamental distinctions where the key differences lie in their communication manners and the computational order reorganization, as elaborated as below:\nLASP-1 utilizes a ring-style P2P communication, which needs to launch many send & receive operators between devices, to sequentially transfer the KV activation one-by-one among the devices. This makes the communication process relatively slow and hard to adequately overlap with intra-chunk computations.\nWhile LASP-2 uses a single AllGather collective communication operator to exchange KV activation concurrently among all decices. This offers practical advantages: (1) Only one well-optimized collective communication operator needs to be launched, and the exchange of KV activation on all devices can be finished concurrently all at once; (2) the collective communication can be more easily overlapped with computations. Like in LASP-2 with masking, the AllGather communication is able to overlap with the intra-chunk output computations. And, in addition, LASP-2 reorganizes the whole computation order to make the AllGather based communication strategy feasible and efficiency."}, {"title": "3.4. Theoretical Cost Analysis", "content": "For better understanding the superiorities of LASP-2, we provide a theoretical cost analysis of both LASP-1 and LASP-2. We consider the pure SP scenario, i.e., the distributed world size is W, and an input sequence with a"}, {"title": "3.5. Hybrid Model Sequence Parallelism", "content": "The hybrid model, which combines linear transformer layers with standard transformer layers that utilize softmax self-attention, has been demonstrated to effectively enhance long-context capabilities, particularly in tasks such as re-"}, {"title": "Impact Statement", "content": "This work represents a notable advancement in artificial intelligence and machine learning, particularly in improving the efficiency and scalability of linear attention-based models. LASP-2 enables the processing of much longer sequences compared to existing methods while significantly accelerating computation, making it highly beneficial for tasks like natural language understanding, genomic sequence analysis, and time-series forecasting. However, the enhanced capabilities and efficiency introduced by LASP-2 also raise ethical and societal considerations, such as the potential for misuse in generating persuasive but misleading content or in surveillance applications. Nevertheless, the contributions of LASP-2 to reducing computational overhead and energy consumption in training large models may also bring positive environmental impacts."}]}