{"title": "INTEGRATING REINFORCEMENT LEARNING AND AI AGENTS\nFOR ADAPTIVE ROBOTIC INTERACTION AND ASSISTANCE IN\nDEMENTIA CARE", "authors": ["Fengpei Yuan", "Nehal Hasnaeen", "Ran Zhang", "Bryce Bible", "Joseph Riley Taylor", "Hairong Qi", "Xiaopeng Zhao", "Fenghui Yao"], "abstract": "This study explores a novel approach to advancing dementia care by integrating socially assistive\nrobotics, reinforcement learning (RL), large language models (LLMs), and clinical domain exper-\ntise within a simulated environment. This integration addresses the critical challenge of limited\nexperimental data in socially assistive robotics for dementia care, providing a dynamic simulation\nenvironment that realistically models interactions between persons living with dementia (PLWDs)\nand robotic caregivers. The proposed framework introduces a probabilistic model to represent the\ncognitive and emotional states of PLWDs, combined with an LLM-based behavior simulation to\nemulate their responses. We further develop and train an adaptive RL system enabling humanoid\nrobots, such as Pepper, to deliver context-aware and personalized interactions and assistance based on\nPLWDs' cognitive and emotional states. The framework also generalizes to computer-based agents,\nhighlighting its versatility. Results demonstrate that the RL system, enhanced by LLMs, effectively\ninterprets and responds to the complex needs of PLWDs, providing tailored caregiving strategies.\nThis research contributes to human-computer and human-robot interaction by offering a customizable\nAI-driven caregiving platform, advancing understanding of dementia-related challenges, and fostering\ncollaborative innovation in assistive technologies. The proposed approach has the potential to enhance\nthe independence and quality of life for PLWDs while alleviating caregiver burden, underscoring the\ntransformative role of interaction-focused AI systems in dementia care.", "sections": [{"title": "1 Introduction", "content": "Alzheimer's disease (AD) and related dementias (ADRD) account for 60\u201380% of dementia cases, affecting approxi-\nmately 6.9 million Americans aged 65 and older, with projections rising to 13.9 million by 2060 [1]. Persons living with\nAD/ADRD (PLWDs) often face challenges such as short-term memory loss, attention deficits, and negative emotions"}, {"title": "2 Agent PLWD", "content": "Inspired by the potential of robot caregiver for individuals with dementia at their home and the lack of real-world dataset\nin this application, we build a simulated environment including two AI agents, PLWD and Robot Caregiver. Fig. 1\nillustrates the system architecture of the two simulated AI agents and their dynamic interactions. In the simulation, the\nagent PLWD performs daily routine tasks and the assistive agent Robot Caregiver observes PLWD's status and provides\nassistance accordingly, to help PLWD complete the task. The agent PLWD consists of two modules, an abstract-level\nstatistical model for PLWD's cognitive and emotional states and an LLM-based PLWD behavior simulation model\nwhich models the realistic behavior, both verbal and nonverbal, during performing the task. The agent Robot Caregiver\nincludes three modules, robotic perception, decision-making, and action execution. The modules of perception and\naction execution are leveraging the powerful LLM model. More details on the development of agent Robot Caregiver\nare given in the next section."}, {"title": "2.1 Probabilistic Model of PLWD States", "content": "We have developed a statistical model to simulate the probability of performance breakdown in PLWDs during\nperforming ADLs, with and without external assistance. The performance breakdown is particularly manifested as\nPLWD's cognitive and affective statuses, including forgetfulness, confusion, anger, and disengagement, which are"}, {"title": "2.1.1 Without external assistance", "content": "Based on the assumptions and qualitative observations of PLWD's cognitive and affective behaviors from previous\nstudies [21, 24], we simulate the dynamics of PLWD's four statuses using the probabilities in Table 3. While\nthese probabilities are simplifications of the complexity inherent in dementia, they provide a useful framework for\nunderstanding the general trends in PLWD's cognitive and emotional state transition, considering factors such as\ncognition, mood, task nature, and coping mechanisms.\nFor example, Forgetfulness is influenced by Anger and Disengagement, showing the impact of cognitive-emotional\nstressors on memory. Recognizing that Anger can severely disrupt memory and cognitive functions, it contributes"}, {"title": "2.1.2 With external assistance", "content": "The transition probabilities of the four cognitive and affective statuses will be influenced by the presence of external\nassistance in Table 2. More specifically, in the presence of verbal supportive assistance, if PLWD current status Anger\n(Disengagement) is Yes, the following status of Anger (Disengagement) being Yes is 5%; Otherwise, it will be 0. For\nthe other two statuses, Forgetfulness and Confusion, their dynamics will follow the rules in Table 3. In the presence\nof verbal non-directive assistance or verbal directive assistance, these two types of assistance will not influence the\ndynamics of emotional states, Anger and Disengagement, whose dynamics will follow the rules in Table 3. Regarding\nthe statuses Forgetfulness and Confusion, if their current status is No, the probability of following status being Yes will\nbe 0% and 0%, correspondingly. Comparatively, if their current status is Yes, the probability of following status being\nYes will be 40% and 60%, correspondingly, given verbal non-directive assistance, and 5% and 5%, correspondingly,\ngiven verbal directive assistance."}, {"title": "2.1.3 Subtask skipping", "content": "Acknowledging the complexity of PLWDs' cognitive and affective responses, skipping a task might alleviate some\nnegative emotions but does not guarantee an immediate cognitive and/or emotional reset. Therefore, the transition\nprobability of subtask skipped states follows the following rules: For Forgetfulness (Confusion), the probability of\npersistence after skipping a subtask is set at 0.5 if the PLWD is already forgetful and confused, and 0.2 if forgetful\n(confused) but not confused (forgetful), reflecting the impact of emotional stress on memory retention. For the statuses\nAnger and Disengagement, a probability of 0.5 has been assigned for their persistence after subtask skipping, recognizing\nthe potential for these emotions to either continue or abate."}, {"title": "2.2 LLM-based PLWD Behavior Simulation", "content": "Based on the aforementioned statistical model, we utilize a LLM, specifically GPT-40, and design a structured prompt\nto mimic the realistic, detailed verbal and nonverbal behaviors of PLWD during ADLs. As shown in Fig 5, the prompt\nconsists of seven carefully designed components: (1) the role definition, which establishes the core characteristics\nof an older adult with moderate dementia, providing a consistent baseline for generating behavioral responses; (2)\nthe task context, which situates interactions within realistic daily scenarios involving a caregiver; (3) the interaction\nhistory between the PLWD and caregiver, which ensures contextually appropriate and consistent responses over time;\n(4) the latest caregiver interaction/assistance, which provides immediate situational context and allows the behavior\nsimulation module to generate appropriate, relevant reactions to caregiver interventions; (5) the state guidelines, which\ndefine and provide examples of PLWD's four key cognitive and emotional states (forgetfulness, confusion, anger,\nand disengagement), ensuring consistent and accurate manifestation of behaviors; (6) a binary vector representing\nPLWD's cognitive and emotional states, enabling dynamic behavior modification based on the PLWD's current state.\nIncorporating this state information enhances our control over the LLM's outputs and reduces the likelihood of\nhallucinations [25]; and (7) the output format specification, which clearly outlines the goals, expectations, and response\nformat [25], constraining the responses to realistic verbal and nonverbal behaviors of PLWD. This structured approach\nensures that the simulated behaviors are both theoretically grounded and practically applicable in ADL scenarios."}, {"title": "3 RL-based Agent Robot Caregiver", "content": "The robot's perception is enhanced through an LLM-based evaluation, translating PLWD's behaviors into actionable\ninsights. This interpretative capability tailors the robot's responses to PLWD's specific needs. The action selection\nstrategy involves choosing between RL decision-making or random action selection. The selected actions are translated\ninto robotic assistance through the LLM-based action execution, guiding the robot's behavior."}, {"title": "3.1 LLM-based Robot Perception", "content": "With an observation of a detailed description of PLWD's behavior, another GPT-40 model is defined to identify PLWD's\ncognitive and emotional states, i.e., Forgetfulness, Confusion, Anger, and Disengagement. The input of the perception\nmodule is the text description of PLWD's behaviors, generated by the LLM-based PLWD behavior simulation module.\nTo implement this function, the purpose of perception is included in the prompt and additionally a definition of PLWD's\nfour cognitive and emotional status, as shown in Fig 6. The output of the perception model $s_p$ is a binary vector\n[Forgetfulness, Confusion, Anger, Disengagement], where 1 and 0 indicate the presence and absence of the state,\ncorrespondingly. Noticeably, the perceived state, $s_p$, may be different from the PLWD state s."}, {"title": "3.2 RL-based Robot Action Selection Module", "content": "We have built a module in which different action selection strategies can be explored. In this work, RL is used to allow\nthe agent Robot Caregiver to provide the optimal assistive action for PLWD to complete the daily routine task. In each\nepisode, the RL agent needs to assist PLWD to complete a scenario, consisting of a sequence of tasks and subtasks, as\ndefined in Table 1."}, {"title": "3.2.1 State Space S", "content": "A state is defined based on PLWD's four cognitive and affective statuses, that is, a state s includes four components,\nForgetfulness (denoted as $s_{for}$), Confusion (denoted as $s_{con}$), Anger (denoted as $s_{ang}$), and Disengagement (denoted as\n$s_{dis}$). Values of these four components can be Yes (1) or No (0). Therefore, the dimension of the state space is 16. For\neach episode, the state will be reset to the starting state, $s_0$ ([0, 0, 0, 0]), where PLWD is not in a state of forgetfulness,\nconfusion, anger, or disengagement, and end in the terminal state. The terminal state is reached when the PLWD\nsuccessfully finishes all the tasks and subtasks of the scenario, possibly with skipping subtasks."}, {"title": "3.2.2 Action Space A", "content": "The action space includes 'No Assistance' (ao), 'Verbal Supportive Assistance' (a1), 'Verbal Non-directive Assistance'\n(a2), and 'Verbal Directive Assistance' (a3), as given in Table 2. In addition, considering the possibility that many\nfailures might negatively influence PLWD's status, for each subtask, if the PLWD has tried MaxTrial times with\nactions taken by the RL agent but still cannot finish the subtask successfully, the RL agent will take the action 'Skip\ncurrent task' to move to the next subtask. Here we set MaxTrial as 5."}, {"title": "3.2.3 Immediate Reward", "content": "The immediate reward function is designed such that the robotic assistant can help PLWDs following two principles: 1)\nkeeping PLWDs in a positive affective mood; and 2) helping PLWD out of performance breakdown (e.g., forgetful,\nconfused, and/or angry) and thus complete the task by providing minimum but effective assistance. The strategy of\nproviding minimal assistance aims to maximize engagement in PLWDs, augment their autonomy, and potentially slow\ndown disease progression. As a result, the immediate reward function takes into consideration of PLWD's cognitive and\naffective status, total timesteps, number of trials, subtask completion, and task completion. Inspired by this, we have\nspecified the immediate reward function $r_t$ as Equation 1.\n\n$r_t=W_{forget} \\times S_{for} \\\\\n+ W_{confuse} X S_{con} \\\\\n+ W_{anger} X S_{ang} \\\\\n+ W_{disengaged} \\times S_{dis} \\\\\n+ W_{increasedTrial} \\times \\triangle N_{trial} \\\\\n+ (W_{subtaskComplet} + W_{assist}) \\times Flag_{subtaskComplet} \\\\\n+ W_{subtaskSkip} \\times Flag_{subtaskSkip} \\\\\n+ W_{increasedTimestep} \\times \\triangle N_{timestep} \\\\\n+ W_{taskComplet} \\times Flag_{taskComplet}$                                        (1)"}, {"title": "3.2.4 Learning Algorithm Design and Training", "content": "Q-learning is used by the RL agent to learn the assistive strategy. The learning rate and discount factor are set to be\n0.05 and 0.95, respectively. The RL agent is trained for 6000 epochs, each with 30 episodes. To explore the potential\ninfluence of exploration and exploitation, two types of epsilon-greedy approaches are used: one with a constant epsilon\n(\u03b5 = 0.1) and the other with an exponentially decaying epsilon \u025b(t). The exponential decaying expression is shown in\nEquation 2, where \u20acmin = 0.03, \u20acmax = 1, and t is the current epoch. The decay rate A is adjusted such that \u025b = 0.8 at\nt = 300 epochs, starting from Emax = 1.\n\n$\u20ac(t) = \u20ac_{min} + (\u20ac_{max} - \u20ac_{min}) \\cdot e^{-\\lambda t}$                                                        (2)"}, {"title": "3.2.5 Evaluation Metrics", "content": "To evaluate the performance, we extract the temporal optimal policy \u03c0' suggested at the 10th episode of each epoch,\napply it to run 40 experiments, and calculate the average return, compared to the performance of random action selection\nstrategy. Additionally, we record all the optimal policies suggested in the last 100 episodes and use the top five policies\nmost frequently suggested to run 10000 experiments and choose the policy which enables the maximum return as the\nfinal policy, denoted as \u03c0."}, {"title": "3.3 LLM-based Robot Action Execution Module", "content": "An LLM model is employed in the Robot Caregiver agent to enable the robot to provide natural, detailed, and actionable\nassistance to PLWD based on the abstract-level actions suggested by the action selection module. For this purpose, we\nuse GPT-40 as the LLM model. The prompt design incorporates several key components, each serving a distinct role:\n(1) the robotic role and purpose establish the robot's caregiver identity; (2) the task context situates the assistance within\na specific ADL task (e.g., selecting three items on a shopping list) that the PLWD needs to perform; (3) the interaction\nhistory between the PLWD and the caregiver ensures a consistent task procedure flow and enables contextually relevant\nresponses based on prior exchanges; (4) PLWD's current behavior captures the immediate situation, allowing the robot\nto adapt its actions accordingly; (5) the abstract-level assistive action a generated by the robot action selection module\nguides the robot's executive actions; (6) a structured output format ensures the robot's verbal responses remain focused\nand effective; (7) assistance guidelines provide detailed or brief definitions of assistance types (Table 2) and examples\nthat clarify the assistance strategies; and (8) the inclusion or exclusion of PLWD's current state s.\nWe explore four variations of this prompt by modifying the last two elements: the inclusion of detailed versus brief\nassistance guidelines and the presence or absence of PLWD's current state s. Figs. 7 and 8 present the four variations\nof the prompt. Including detailed definitions and examples of assistance types might enable the LLM to generate more\nnuanced and precise assistance. However, this could lead to redundancy or distraction in longer prompts [26]. Similarly,\nincluding PLWD's current state might enhance the LLM's ability to generate state-aware, personalized interactions and\nassistance. These variations provide insights into designing an effective assistive agent, Caregiver, using RL and LLM."}, {"title": "4 Results and Discussion", "content": null}, {"title": "4.1 RL-based Action Selection Strategy", "content": "The learning processes of the three action selection strategies-constant-epsilon greedy RL, decaying-epsilon greedy\nRL, and random action selection strategy\u2014are detailed in Fig. 9. After convergence, both the constant-epsilon greedy\nRL and decaying-epsilon greedy RL achieve an average return of approximately 140, indicating similar performance\nlevels. In contrast, the agent using the random action selection strategy achieves an average return of around 70,\ndemonstrating the superiority of our RL policies over the random strategy."}, {"title": "4.2 Agent PLWD-Robot Interaction", "content": "After implementing the two agents, PLWD and the RL-based assistive agent, Robot Caregiver, experiments were\nconducted to observe their performance in simulating PLWD behaviors during ADLs, PLWD-caregiver interaction,\nand the caregiver's ability to assist PLWD. The PLWD state and behaviors, the abstract-level actions and actionable\nassistance provided by the Robot Caregiver, and other relevant information were saved in a text file. Fig. 11 presents an\nexample of the saved results, which capture the interaction between the PLWD and the robot at the beginning of the\nexperiment. Information in black font in the figure corresponds to data saved in the file.\nAgent PLWD. The LLM-based PLWD behavior simulation module successfully generates realistic, detailed verbal\nand nonverbal behaviors for PLWDs during ADLs, tailored to their diverse states. For instance, as shown in Fig. 11,\nin Timestep 1, with the PLWD state [Forgetfulness=0, Confusion=0, Anger=0, Disengagement=0], the PLWD agent\nexhibited natural and cooperative behavior. This included nonverbal responses such as gently nodding to the caregiver\nand step-by-step actions to check soups from a list. The detailed output encompassed both nonverbal descriptions (e.g.,\n\"picks up the cans one by one, looking at each label carefully...\") and verbal utterances (e.g., \"Campbell's Chicken\nNoodle soup... no, that's not it\").\nIn contrast, for the PLWD state [Forgetfulness=0, Confusion=0, Anger=0, Disengagement=1] in Timestep 2 (Fig. 12),\nthe PLWD agent exhibited disengaged behaviors. These included avoiding eye contact, looking down at the table,\nand wandering attention. The verbal responses mirrored this state, featuring hesitant and incomplete statements such\nas \"I... I think... yeah, I found them all. I got... the... the ones from the list.\" These examples demonstrate how the\nmodule effectively captures a range of emotional and behavioral states, generating contextually appropriate and realistic\ninteractions for each scenario.\nAgent Robot Caregiver. For the Robot Caregiver, no significant differences were observed in the generated interactions\nacross the four tested prompts (described in Figs. 7-8). Observations by the authors (with backgrounds in dementia care)\nindicate that the interactions between the two agents appeared reasonable and aligned with expected caregiver-patient\ndynamics in such scenarios. However, we acknowledge the limitation of relying on internal observations without\nformal evaluation. Future work will address this limitation in two ways: (1) dementia care experts will evaluate the\nsimulated interactions to assess whether they represent real-world PLWD-caregiver interactions, and (2) individuals\nwith Alzheimer's will directly interact with the Robot Caregiver to evaluate the appropriateness and effectiveness of its\nassistance."}, {"title": "4.3 Limitations and Future Work", "content": "This study presents a novel approach to advancing dementia care by integrating assistive agents, Generative AI, decision-\nmaking AI, and clinical domain expertise within a simulated environment using probabilistic modeling, reinforcement\nlearning, and LLMs. While the findings demonstrate the feasibility of this approach and its potential to enhance\ndementia caregiving capabilities, particularly in light of the limited availability of real-world data on PLWD-caregiver\ninteractions during ADLs, several limitations warrant attention. These, along with proposed directions for future\nresearch, are discussed below:\nProbabilistic PLWD model complexity. The probabilistic model used to simulate the cognitive and affective states\nof PLWDs captures transitions between emotional and cognitive states such as forgetfulness, confusion, anger, and\ndisengagement. However, its stochastic nature impacts the optimal policy recommendations made by RL-based\ndecision-making algorithms, particularly in complex scenarios involving diverse ADLs and complex PLWD status. For\nexample, discrepancies in the optimal policy for the PLWD state [NFor, NCon, NAng, NDis] recommended by the\nRL-based action selection module, as shown in Fig. 10, highlight this issue. Future work will focus on refining the\nprobabilistic model to more accurately represent real-world PLWDs' behaviors and dynamics, improving synthetic data\ngeneration, and enhancing long-term training of intelligent assistive agents in dementia care [30]. Achieving this goal\nwill require interdisciplinary collaboration among human-robot/computer interaction experts, AI researchers, clinical\ndementia specialists, and ethicists to ensure the model aligns with real-world experiences [28, 29, 31].\nLLM-Based PLWD behavior simulation. LLMs were employed to emulate the natural verbal and nonverbal behaviors\nof PLWDs, addressing the scarcity of real-world data. However, LLMs' susceptibility to stereotyping, hallucinations,\nand prompt sensitivity raises concerns about the authenticity and reliability of simulated behaviors [32, 33]. Future\nresearch will involve collaborating with dementia care experts to formally evaluate the alignment of simulated behaviors\nwith real-world PLWD behaviors. Additionally, strategies to mitigate LLM hallucinations and biases, such as fine-\ntuning models with domain-specific data and refining prompt engineering techniques, will be explored. Incorporating\nreal-world PLWD behavioral data during ADLs will also be critical for enhancing the robustness and reliability of\nLLM-generated simulations [33, 31].\nReal-world application of the Robot Caregiver and its Evaluation. The evaluation of the Robot Caregiver relied\nprimarily on RL performance metrics and author observations. While these results provide valuable insights, they lack\nvalidation from external stakeholders and primary end-users in dementia care. Improvements in RL returns do not\nnecessarily translate to enhanced caregiving outcomes for PLWDs. Additionally, while the framework shows promise\nin a simulated environment, real-world deployment introduces challenges such as individual variability among PLWDs\nand dynamic caregiving contexts. Future work will prioritize: (1) collaboration with dementia care professionals and\ncaregivers (e.g., unpaid family caregivers of PLWDs) to evaluate the appropriateness and effectiveness of simulated\ninteractions and assistance in real-world scenarios; (2) benchmarking the Robot Caregiver agent's strategies against\ncaregiving approaches informed by professional dementia care or human caregivers; and (3) conducting pilot studies\nwith PLWDs to assess the usability, acceptability, and effectiveness of the Robot Caregiver in assisting them during\nADLs, as well as its impact on their cognitive, emotional, and social well-being.\nContext-awareness and task understanding. The Robot Caregiver exhibited limitations in understanding context and\ntask-specific procedures, as evidenced in scenarios where it misinterpreted PLWD behaviors. For instance, it perceived\nuncertainty (e.g., \u201cWhat do I need to do next?\u201d at Timestep 0 in Fig. 11) as forgetfulness. Enhancing the agent's\ncontext-awareness and task comprehension will require integrating more advanced models for task understanding and\nsituational reasoning, potentially incorporating multimodal inputs and enriched datasets [34].\nBy addressing these limitations, this research aims to enhance the realism, reliability, and practical applicability of\nAI-driven assistive agents in dementia care. These advancements will contribute to the broader fields of socially\nassistive robotics, human-computer/robot interaction, and AI agents for healthcare, fostering transformative solutions\nfor dementia care. The proposed future directions underscore the necessity of continued interdisciplinary collaboration\nto realize the transformative potential of these technologies in improving the lives of PLWDs and alleviating caregiver\nburdens."}, {"title": "5 Conclusion", "content": "This study demonstrates the potential of advanced AI and robotics in enhancing dementia care, particularly for\nindividuals with Alzheimer's Disease and related dementias. By integrating LLMs with an RL framework, we present a\nnovel approach to simulating the challenging behaviors of PLWDs during activities of daily living and the dynamic\ninteractions between PLWDs and robot caregivers. This framework enables the robot to engage with and assist PLWDs"}]}