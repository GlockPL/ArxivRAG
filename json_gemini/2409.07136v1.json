{"title": "Leveraging Unstructured Text Data for Federated Instruction Tuning of Large Language Models", "authors": ["Rui Ye", "Rui Ge", "Fengting Yuchi", "Jingyi Chai", "Yanfeng Wang", "Siheng Chen"], "abstract": "Federated instruction tuning enables multiple clients to collaboratively fine-tune a\nshared large language model (LLM) that can follow humans' instructions without\ndirectly sharing raw data. However, existing literature impractically requires\nthat all the clients readily hold instruction-tuning data (i.e., structured instruction-\nresponse pairs), which necessitates massive human annotations since clients' data is\nusually unstructured text instead. Addressing this, we propose a novel and flexible\nframework FedIT-U2S, which can automatically transform unstructured corpus\ninto structured data for federated instruction tuning. FedIT-U2S consists two key\nsteps: (1) few-shot instruction-tuning data generation, where each unstructured data\npiece together with several examples is combined to prompt an LLM in generating\nan instruction-response pair. To further enhance the flexibility, a retrieval-based\nexample selection technique is proposed, where the examples are automatically\nselected based on the relatedness between the client's data piece and example pool,\nbypassing the need of determining examples in advance. (2) A typical federated\ninstruction tuning process based on the generated data. Overall, FedIT-U2S can\nbe applied to diverse scenarios as long as the client holds valuable text corpus,\nbroadening the application scope of federated instruction tuning. We conduct a\nseries of experiments on three domains (medicine, knowledge, and math), showing\nthat our proposed FedIT-U2S can consistently and significantly brings improvement\nover the base LLM.", "sections": [{"title": "Introduction", "content": "Instruction tuning has become one of the most imperative components in training contemporary\ninstruction-followed large language models (LLMs) [1, 2, 3, 4], where typically, the training samples\nare collected from diverse sources by a central party [5, 6, 7]. However, these data could contain sen-\nsitive (e.g., private or proprietary) information that cannot be directly shared, making such centralized\nlearning paradigm inapplicable especially for domains such as medicine [8] and finance [9].\nAddressing this, federated learning [10, 11] has emerged as a well-suited technique to achieve\ninstruction tuning of LLMs without direct data sharing. In federated instruction tuning (FedIT), each\nparty (i.e., client) keeps its private data locally and shares the instruction-tuned LLM with the central\nserver, while the server aggregates LLMs from multiple parties and distributes the aggregated LLM\nback to participating parties. Such paradigm has attracted massive attention and interests from both\nacademia [12, 13, 14] and industry [15, 16, 17].\nDespite extensive efforts dedicated to FedIT, existing methods impractically rely on the assumption\nthat each party possesses structured instruction-tuning data (i.e., instruction-response pairs), which\nsignificantly constrains the real-world applicability of FedIT. In practice, while clients may possess"}, {"title": "", "content": "valuable data locally, this data often exists in an unstructured format (just strings of text) rather than\nnaturally aligns with the structured format required for IT [18]. Consequently, current FedIT systems\nface challenges in scalability, as they necessitate manual annotation of data by each client.\nTo fill this gap, we propose a novel and flexible framework FedIT-U2S, which can automatically\ntransform unstructured corpus into structured instruction-tuning data for FedIT, bypassing the massive\nhuman efforts required for data annotation. Specifically, FedIT-U2S consists of two key steps: few-\nshot instruction-tuning data generation and FedIT on the generated data. (1) The server first distributes\nan open-sourced general LLM and a few examples (could be as few as only one) to participating\nclients. During data generation, each client queries the LLM to generate multiple instruction pairs,\nwhere each pair is generated by feeding the LLM with a prompt that is composed of few examples\nas the context and a sampled piece of its unstructured data. To further enhance the generality\nand scalability of FedIT-U2S, we propose a retrieval-based example selection approach, where for\neach sampled piece of unstructured data, similarity scores are computed by comparing it with all\nthe examples sent from the server, after which the top-k examples are selected as the few-shot\nexamples in the context for data generation. (2) Subsequently, typical federated instruction tuning\nis launched based on the general LLM and the generated datasets in the previous step. Considering\ncommunication and computation efficiency, LoRA [19] is applied and therefore only a small set of\nparameters are learned and communicated. Overall, our FedIT-U2S framework makes FedIT system\nas practical as Google's GBoard application (next word prediction) [20], where the supervision data\ndirectly comes from user's data without any manual effort.\nTo verify the effectiveness of our proposed framework, we conduct a series of experiments covering\nthree domains (i.e., medicine, knowledge, and math). We show that across these domains, our FedIT-\nU2S consistently improves the performance of the general LLM on the corresponding downstream\ntask. Besides, we show the effectiveness of several designs, including retrieval-based example\nselection and filtering during data generation, providing potential directions for further improving the\nthe performance of FedIT-U2S.\nOur contributions are as follows:\n1. We propose the first end-to-end framework (FedIT-U2S) for directly leveraging unstructured data\nfor federated instruction tuning of large language models.\n2. We propose a retrieval-based example selection technique and a few-shot data generation mecha-\nnism, which automatically selects examples for higher relatedness and generates structured data in\nan expected manner.\n3. We verify the effectiveness of FedIT-U2S through a series of experiments on multiple domains."}, {"title": "2 Related Work", "content": "Federated Learning of Large Language Models. Federated learning is a privacy-preserving\nmachine learning paradigm that enables multiple clients to collaboratively train machine learning\nmodels without sharing their raw data [10, 11]. With the rise of large language models (LLMs),\nresearchers have recently begun to consider federated training of LLMs to safeguard client data\nprivacy or to address the scarcity of publicly available data [21, 12], which has attracted massive\nattention and interests from both academia [12, 13, 14] and industry [15, 16, 17].\nSpecifically, OpenFedLLM [12] offers an integrated framework and provides a comprehensive\nempirical study to show the potential of federated instruction tuning of LLMs (FedIT). Similarly,\nFederatedScope-LLM [17] and FedML-LLM [15] provide frameworks that implement FedIT; while\nFedLLM-Bench [13] offers real-world datasets and benchmarks. Besides frameworks and bench-\nmarks [22], a series of methods are proposed to target various perspectives including safety align-\nment [23], privacy [24], heterogeneous computation [25].\nHowever, existing literature assumes that client data is structured in the form of instruction-response\npairs, overlooking the reality that client data often exists in an unstructured format. In such cases,\nclients are required to annotate data before participating in FedIT, which is labor-intensive and limits\nits broader adoption. In this paper, we address this issue for the first time by proposing FedIT-U2S,\na method that automates the transformation of unstructured client data into structured data prior to\nFedIT. This reduces the need for manual annotation and broadens the applicability of FedIT."}, {"title": "3 Methodology", "content": "In this section, we first introduce the overall framework of our proposed FedIT-U2S (Figure 1), which\nconsists of two key steps: few-shot instruction-tuning data generation (which transforms unstructured\ndata into structured instruction-tuning data pairs) and federated instruction tuning on the generated\ndata. Then, we detail our design of retrieval-based example selection for few-shot data generation."}, {"title": "3.1 Pipeline of FedIT-U2S", "content": "At the beginning of FedIT-U2S, the server first distributes an open-sourced general LLM (denoted\nby 0*) and a set of examples (unstructured and structured text pairs, denoted by O) to participating\nclients.\nStep 1: few-shot instruction-tuning data generation. Suppose there are M clients in the system and\neach client m holds an unstructured dataset $D_{um} = \\{d_i\\}_{i=1}^{N_m}$, where $d_i$ is a data piece and $N_m$ denotes\nthe number of data pieces. Since such unstructured data cannot be directly used for instruction tuning,\nit conventionally requires each client's efforts to manually create instruction-response pairs for tuning,\nwhich is costly and faces the challenges of scaling up. To address this, we design to automatically\ntransform the unstructured data into a structured instruction-response format via a few-shot data\ngeneration process, which leverages LLM's in-context learning capability [32].\nSpecifically, upon receiving example set $O = \\{(d_i, x_i, Y_i)\\}_{i=1}^{|O|}$, where O is the example number, $d_i$\nis an unstructured data document, $x_i$ and $y_i$ is the document-grounded instruction and response re-\nspectively, each client selects several (denoted by k) examples as few-shot examples prompt the LLM\n0*. Denote the instruction for generation as I and the selected examples as $S = \\{(d_i, \\hat{x}_i, \\hat{y}_i)\\}_{i=1}^k$,\ngiven a user's data piece d, the prompt P is constructed as: $P = Concat(I,S, d)$, where Concat"}, {"title": "", "content": "denotes the concatenation operation (see full prompt in A). Note that these examples can be either\nrandomly selected for diversity or selected according to relatedness between user data and examples\nfor better diversity-relatedness trade-off, which will be detailed in Section 3.2. Based on the prompt,\nthe LLM 0* will generate an instruction-response pair: $(x, y) = f(P; 0^*)$. Therefore, by iterating\non client's unstructured dataset $D_{um} = \\{d_i\\}_{i=1}^{N_m}$, we obtain a structured dataset for instruction tuning:\n$D_m = \\{x_i, Y_i\\}_{i=1}^{N_m}$.\nSince the responses of LLMs are in an open-ended form and there are randomness during generation,\nsome generated data might fall short in terms of data quality. Therefore, additional data filtering is\nnecessary for enhancing the data quality. Here, we consider two filtering mechanisms: rule-based\nfiltering to remove data with undesired format and reward-based filtering to ensure the quality of\nselected data. Specifically, we first filter out data that does not follow the format of instruction-\nresponse pair. Secondly, we use an publicly available reward model to score the generated data\nsamples and select the top two-thirds samples. This enables us to select data that is more aligned\nwith human preference since reward model is trained to model human preference.\nStep 2: federated instruction tuning on the generated data. With the generated data, a typical\nprocess of federated instruction tuning is started. Considering computation and communication\nefficiency, we apply LoRA [19] as the parameter-efficient fine-tuning technique. Suppose there are T\nrounds of federated learning rounds in total. At each round t, the server sends the model parameters\n$\\theta^t$ to each available client. Then, each client m initializes its local trainable parameters with $\\theta^t_0$,\nkeeps the base model parameters $\\theta^*$ fixed, and starts supervised fine-tuning on its generated dataset\n$D_m = \\{x_i, Y_i\\}$, where the model learns to predict the response $\\hat{y}$ given the instruction $x_i$. By\nfine-tuning for several steps, each client m obtains a fine-tuned model parameters $\\theta_m^t$ and sends it\nto the server. Finally, the server aggregates model parameters of clients to obtain the global model\nparameters for the next round: $\\theta^{t+1} = \\sum_m p_m\\theta_m^t$, where $p_m = \\frac{N_m}{\\sum_i N_i}$, is the relative dataset size of\nclient m."}, {"title": "3.2 Retrieval-based Example Selection for Few-Shot Generation", "content": "The chosen examples (i.e., the context) in the prompt could significantly affect the behaviour of\nLLMs [33, 34], resulting in different quality of the genrated data. Therefore, to generate high-\nquality structured data, selecting appropriate few-shot examples is essential. Generally, examples\nthat closely match the target text in terms of content and structure tend to produce more effective\nresults. However, in practical applications, manually identifying suitable examples can be a time-\nconsuming process, making it inflexible in adapting to diverse scenarios. To mitigate this challenge,\nwe propose a retrieval-based example selection method for few-shot generation which automatically\nselects few-shot examples from a mixed example pool according to similarity between user data and\nexamples.\nGiven the set of examples sent from the server $O = \\{(d_i, x_i, Y_i)\\}_{i=1}^{|O|}$, each client aims to select k\nexamples for each of its sampled unstructured data piece. Specifically, for each data piece d, we\ncompute the similarity $Sim(d, d_i)$ for each $d_i$ in the example pool O using BERT Score as the\nmetric, which gives a similarity score that reflects the relatedness between the target data piece\nand the example's content. Subsequently, we rank the similarity scores and select top-k examples\n$S = \\{(d_i, x_i, Y_i)\\}_{i=1}^k$, which are mostly likely to guide the LLM to generate high-quality and\nhighly-related data. The other procedures remain unchanged as in Section 3.1."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Experimental Details", "content": "Training Dataset. We consider three datasets for our experiments [27], which cover domains\nincluding medicine, knowledge, and math. Specifically, PubMedQA [35] is a medical dataset for\nbiomedical research question answering with corresponding abstracts as the context. HotpotQA [36]\nis a dataset of Wikipedia-based questions with supporting facts as the context. AQUA_RAT [37]\nis a math dataset for algebraic word problems answering. The problems, together with solutions,\nform the context. We select 10,000 samples from each dataset for the experiments [27], with each\nsample comprising a piece of original unstructured text, along with a human annotated instruction"}, {"title": "4.2 Experimental Results", "content": "Comparisons with baselines. In Table 1, we compare models trained via our methods on generated\ndata with base model and model trained via FedAvg [10] on human-annotated data (as a reference).\nExperiments are conducted on three datasets and evaluated by two metrics. From the table, we see\nthat (1) our methods consistently and significantly improves the performance of the base model across\ndatasets and evaluation metrics, indicating the effectiveness of our proposed methods. Specifically,\nin HotpotQA, our method can achieve 0.1873 higher BERT Score (0.2439 v.s. 0.0566). (2) Our\nmethods hugely fill the gap between base model and that tuned via FedAvg on human data, further\nverifying FedIT-U2S's effectiveness. However, there is still a room for improvement, calling for\nmore future works to further enhance the performance. With the increasing generation capability of\nLLMs [29, 28], we even believe that there is potential for surpassing this baseline (FedAvg on human\ndata). (3) Although the data filtered using the reward model is smaller in quantity, it brings a more\nsignificant improvement to the model's performance, indicating the importance of data quality in this\nscenario.\nAnalysis of example selection for few-shot generation. The effectiveness of few-shot generation\nmay heavily rely on the chosen examples in the context. Therefore, here, we deeply analyze the\nexample selection by conducting a series of experiments on HotpotQA dataset since we observe a"}, {"title": "5 Conclusions", "content": "This paper proposes FedIT-U2S, which directly leverages clients' unstructured text data to achieve\nfederated instruction tuning of large language models. FedIT-U2S consists of two key steps: few-shot\ninstruction-tuning data generation and federated instruction tuning on the generated data. During data\ngeneration, for each unstructured data piece, a client firstly selects related examples via a retrieval-\nbased example selection mechanism and then uses these examples for guiding the LLM to generate\ninstruction-response pair based on the data piece. A typical process of federated instruction tuning is\nthen conducted based on the generated data. Experiments on three domains (medicine, knowledge,\nand math) verify the effectiveness of our proposed FedIT-U2S. Our method for the first time enables\nclients with unstructured data to be involved in the process of federated instruction tuning, which\noccupy a large proportion in practice and are underutilized previously. We believe that this work can\ncontribute to broadening the application scope of federated instruction tuning."}]}