{"title": "BigMac: A Communication-Efficient Mixture-of-Experts Model Structure for Fast Training and Inference", "authors": ["Zewen Jin", "Shengnan Wang", "Jiaan Zhu", "Hongrui Zhan", "Youhui Bai", "Lin Zhang", "Zhenyu Ming", "Cheng Li"], "abstract": "The Mixture-of-Experts (MoE) structure scales the Transformer-based large language models (LLMs) and improves their performance with only the sub-linear increase in computation resources. Recently, a fine-grained DeepSeekMoE structure is proposed, which can further improve the computing efficiency of MoE without performance degradation. However, the All-to-All communication introduced by MoE has become a bottleneck, especially for the fine-grained structure, which typically involves and activates more experts, hence contributing to heavier communication overhead.\nIn this paper, we propose a novel MoE structure named BigMac, which is also fine-grained but with high communication efficiency. The innovation of BigMac is mainly due to that we abandon the communicate-descend-ascend-communicate (CDAC) manner used by fine-grained MoE, which leads to the All-to-All communication always taking place at the highest dimension. Instead, BigMac designs an efficient descend-communicate-communicate-ascend (DCCA) manner. Specifically, we add a descending and ascending projection at the entrance and exit of the expert, respectively, which enables the communication to perform at a very low dimension. Furthermore, to adapt to DCCA, we re-design the structure of small experts, ensuring that the expert in BigMac has enough complexity to address tokens. Experimental results show that BigMac achieves comparable or even better model quality than fine-grained MoEs with the same number of experts and a similar number of total parameters. Equally importantly, BigMac reduces the end-to-end latency by up to 3.09\u00d7 for training and increases the throughput by up to 3.11x for inference on state-of-the-art AI computing frameworks including Megatron, Tutel, and DeepSpeed-Inference.", "sections": [{"title": "Introduction", "content": "Increasing the size of Transformer-based large language models (LLMs) can continuously improve downstream application performance. Such a phenomenon, known as the scaling law, has been demonstrated by the auto-regressive dense models such as GPT-series (OpenAI et al. 2024) and Llama-series (Dubey et al. 2024). However, this comes at the price of higher computing complexity and more resource consumption. Fortunately, the Mixture-of-Experts (MoE) technique, capable of expanding the model size tens or even hundreds of times without significantly increasing the computation, is widely used in various emerging huge models, such as GShard (Lepikhin et al. 2020), GLaM (Du et al. 2022), Switch Transformer (Fedus, Zoph, and Shazeer 2022), and Mixtral (Jiang et al. 2024), each of which consists of hundreds of billion parameters or even beyond.\nRecently, DeepSeekMoE (Dai et al. 2024), a fine-grained and more parameter-efficient MoE structure, has been proposed. Compared to conventional MoE models, for the same model size, DeepSeekMoE has significantly more experts per MoE layer and fewer parameters per expert. It was demonstrated that such a new structure can achieve comparable or even better results than conventional MoE models with much less time complexity and hence it is adopted by many later released models, such as Qwen2 (Yang et al. 2024) and DeepSeek-v2 (DeepSeek-AI et al. 2024).\nHowever, the MoE faces a serious All-to-All communication bottleneck during both training and inference. This is mainly because of the underlying expert parallelism (EP) strategy, which assigns experts to several hardware accelerators to avoid out-of-memory errors or improve efficiency (Fedus, Zoph, and Shazeer 2022). As a result, for each MoE layer, two All-to-All communication steps are introduced for dispatching tokens to their best-fit experts, which may be stored in the other accelerators and then gathering the results back before proceeding to the next layer."}, {"title": "Related Work and Motivation", "content": "Fine-grained MoE. Starting from GShard (Lepikhin et al. 2020), the Mixture-of-Experts (MoE) technology has been applied to the Transformer architecture, allowing for a significant increase in the number of parameters with only a sub-linear increase in computational resources. As illustrated in Figure 2a, the dense Feed-Forward Network (FFN) modules in Transformer are replaced with MoE sub-layers, each consisting of multiple parallel experts. The number of experts activated by each token is defined as top_k. However, it is challenging for these conventional MoE models to exploit expert specialization, since they are based on the top-1 or top-2 routing strategies with the coarse-grained expert activation. To address this issue, a new fine-grained MoE architecture is proposed in DeepSeekMoE (Dai et al. 2024). To improve expert specialization, DeepSeekMoE maintains the same number of parameters as the conventional MoE models, while splitting experts into finer granularity and choosing a higher top_k for token distribution. Such an architecture with a large number of smaller experts has been adopted by DeepSeek-V2 (DeepSeek-AI et al. 2024) and Qwen2-57B-A14B (Yang et al. 2024), demonstrating better model quality and computation efficiency than the conventional ones with a small number of large experts.\nNevertheless, this fine-grained MoE architecture faces a severe communication problem for its training and inference tasks, due to the following reasons. First of all, to improve computation efficiency and cope with the single hardware accelerator's memory limit, the common practice is to leverage Expert Parallelism (EP) for assigning experts to different accelerators (Fedus, Zoph, and Shazeer 2022). Second,"}, {"title": "BigMac: Communication-Efficient MoE Structure", "content": "In this paper, we propose BigMac, a novel MoE structure that eliminates the well-known All-to-All communication bottleneck. Note that BigMac builds atop the success of fine-grained MoE models such as DeepSeekMoE and Qwen, where it also assigns a large number of small experts for each MoE layer, as shown in Figure 2c. However, beyond this similarity, BigMac has the following two main differences in structure that reflect its design rationales, compared to fine-grained ones.\n\u2022 Low-dimensional communication: we scale down the input/output tokens of experts to decrease the hidden dimension of the tokens to transfer, which greatly reduces the All-to-All communication overhead.\n\u2022 Performance assurance: to adapt to the decreased dimension of input/output tokens, we have to re-design the structure of each expert, using reversed projections to avoid the expert parameter count decreasing synchronously with the dimension and to align with the fine-grained MoE in terms of the total parameter count, to avoid diminishing the model quality.\nBelow, we will detail the BigMac's design with necessary notations, summarized in Table 2."}, {"title": "DCCA: Low-dimensional Communication Strategy", "content": "BigMac's efficient communication strategy is motivated by the estimation of the All-to-All communication overhead in each MoE layer of the fine-grained MoE models. This overhead can be described as\n\\(C = 2 \\times top\\_k \\times \\frac{e p-1}{e p} \\times b s h\\),\nwhich is proportional to the standard hidden dimension h. For the fine-grained MoE model, as shown in Figure 2b, the model follows a communicate-descend-ascend-communicate (CDAC) manner, namely, the dimension of the tokens will be scaled down by a descending projection after the first All-to-All communication, and further be scaled up before the second All-to-All communication. Therefore, actually the fine-grained MoE model always transmits the token at the highest dimension, contributing to the serious overhead analyzed previously. Inspired by this fact, we ask a key question: is it possible for models like fine-grained MoEs communicate at low-dimensional level while maintaining the overall performance without degradation?\nTo this end, as shown in Figure 2c, at each MoE layer, BigMac moves the descending and ascending projections outside of every small expert and places the descending projection before the first All-to-All operation for remarkably scaling down tokens sent to their best-fit experts. This change allows the communication to happen at the lowest dimension. Following this, we place the ascending projection after the second All-to-All operation to scale up the tokens to their standard sizes. In contrast to the above CDAC manner used in fine-grained MoE models, BigMac follows a descend-communicate-communicate-ascend (DCCA) manner. Within the DCCA execution, the whole process of the MoE module is described by the following equation:\nx' = xW\u2193; y' = \u2211pi(x)Ei(x'); y = y' W\u2191.\ni\u2208T\nHere, x and y represent the output and input of two consecutive attention layers, W\u2193 and W\u2191 are the descending and ascending projection matrices, T refers to the set of top_k experts for token distribution, Ei refers to the expert computation in BigMac, and pi refers to the gate-value of activating the ith expert. Note that we can choose to use either x or x' as the input of the gating function for token routing. Here, we choose x, the vector before downscaling for routing, since the routing function is computationally efficient and a high-dimensional input vector generally leads to more accurate routing. In conclusion, DCCA reduces C in Equation 1 into a much smaller C' by changing h to rh, where r is the downscaling factor. Later, we will explain the value assignment to r and overall communication savings."}, {"title": "BigMac Expert Design", "content": "Based on the DCCA strategy, following the expert structure of the fine-grained MoE models is impractical. Otherwise, the expert will have much fewer parameters and consequently hurt model quality. Recall that expert computation can be described as \\(E(x) = \u03c3(xW_{h\\times h\\_f})W_{h\\_f\\times h}\\), where \u03c3is an activation function, h is the dimension of the input/output tokens and h_f refers to the intermediate dimension. Compared to CDAC, DCCA significantly reduces the input/output dimension h, resulting in a smaller E(x) with the same intermediate dimension h_f.\nAs a result, to align BigMac's model size to that of fine-grained MoE models, we should increase the dimension h_f. The specific structure of the expert designed for adapting the DCCA strategy is shown in Figure 2c. From the appearance, it is closer to the conventional MoE structure in Figure 2a, and it can be seen as swaping the two projection matrices of fine-grained MoE experts in Figure 2b. In this way, the expert in the fine-grained MoE in Equation 3 can be replaced with the one in BigMac as shown in Equation 4:\nE\u2193(x) = \u03c3(xWi,\u2193)Wi,\u2191,\nE\u2191(x) = \u03c3(xWi,\u2191) Wi,\u2193\nIt can be verified that the BigMac expert involves the same size and same computational complexity compared with the expert in fine-grained MoE."}, {"title": "Advantages Beyond Efficient Communication", "content": "Except communication efficiency, BigMac further possesses many beneficial characteristics.\nEnabling Dropless Token Routing. In both training and inference phases of MoE, the token routing imbalance problem occurs frequently, and it results in a severe straggler problem. To reduce the overhead brought by the imbalanced routing, most of the existing MoE models will set a threshold for the expert capacity (Fedus, Zoph, and Shazeer 2022), determined by the expert capacity factor f, which is often set in a range from 1 to 1.25. Each expert will drop the tokens exceeding the expert capacity. It was demonstrated in (Sanseviero et al. 2023) that the quality of the MoE model can be continuously improved by increasing the capacity factor, which implies that token dropping is harmful for model's generation. To ensure the performance, the recently proposed DeepSeekMoE and Mixtral remove the expert capacity limit, at the cost of high communication overhead (Dai et al. 2024; Jiang et al. 2024; Xue et al. 2024). Fortunately, the communication overhead has been greatly mitigated in BigMac. The increased token transmission brought by removing expert capacity limit will not significantly affect the overall training or inference efficiency.\nEnabling Flexible Selection of top_k. The number of activated experts, top_k, is another key factor affecting model quality and overall latency. To some extent, a larger top_k contributes to better model performance (Dai et al. 2024). However, a larger top_k corresponds to a heavier communication overhead, leading to lower efficiency for training and inference. Taking this into account, the existing MoE models generally select a relatively small top_k. Considering the high efficiency of BigMac in both computation and communication, BigMac is able to withstand a much larger top_k to enhance the performance. Hence, BigMac provides a more flexible choice for practitioners."}, {"title": "Pre-Training and Downstream Evaluation", "content": "Pre-Training Tasks\nTo show the acceleration of training convergence with constant model quality, we first pre-train three MoE models with different MoE structures, namely GPT-Vanilla, GPT-Fine-Grained, and GPT-BigMac, all of which use GPT3-XL as the base model. Vanilla represents the conventional MoE with large experts, Fine-Grained refers to the MoE model with small experts, while BigMac is our design. For a fair comparison, we keep the same parameter size of MoE layers across the three models. We use the Wikipedia dataset (Wikimedia 2024) containing 3.6 B tokens to train these models on Megatron (NVIDIA 2019), one of the state-of-the-art LLM training frameworks.\nFigure 1 shows the curve of validation perplexity of pre-training, indicating that GPT-BigMac converges much faster than others and achieves the lowest validation perplexity within the same time. For example, to achieve the same validation perplexity of 13.69, GPT-Fine-Grained requires 38.9 hours while GPT-BigMac only needs 22.8 hours, which is 1.71x faster. In addition, among the three model structures, GPT-Vanilla fails to converge to the same validation perplexity under the time budget, indicating that with the same parameter size, the MoE structure with small experts outperforms the conventional MoE. Further, with the evaluation on WikiText2 (Merity et al. 2016), GPT-BigMac achieves the perplexity score of 17.4, while GPT-Vanilla and GPT-Fine-Grained get 27.4 and 17.9, respectively. The hyper-parameters for pre-training are shown in Table 5 and the degree of Tensor Parallelism, Expert Parallelism, and Data Parallelism is set as 4, 4, and 2, respectively.\nDownstream Tasks\nTo demonstrate how BigMac impacts the model quality on downstream tasks, we utilized a larger dataset named Open-WebText2 dataset (EleutherAI 2020) with 14.8 B tokens. First, we compare the performance after training for the same duration (8 days) based on the hyper-parameters in Table 5. We evaluate the fine-grained and BigMac variants, which are based on GPT3-XL, on eight popular zero-shot tasks, including four long-term dependence prediction tasks (LAMBADA (Paperno et al. 2016), PTB (Marcus, Santorini, and Marcinkiewicz 1993), WikiText103 and WikiText2 (Merity et al. 2016)) and four question answering"}, {"title": "Training and Inference Speedups", "content": "In the last section, we have shown that compared with the traditional MoE structure, MoE structures with small experts are more powerful. In this section, we further compare the communication efficiency of the fine-grained MoE structure and BigMac in more depth."}, {"title": "Experimental Setup", "content": "We intensively profile the time ratios of training and inference for GPT-Fine-Grained and GPT-BigMac, based on the state-of-the-art frameworks Megatron (Shoeybi et al. 2020), Tutel (Hwang et al. 2023), and DeepSpeed-Inference (Microsoft 2024). Megatron supports various parallelism strategies including data parallelism (DP), tensor parallelism (TP), and expert parallelism (EP). Tutel is a specialized framework to optimize the All-to-All communication for MoE models. DeepSpeed-Inference supports techniques specialized for LLM inference including KV cache management to efficiently serve the models. All the experiments are conducted on a cluster of 4 machines connected with 100 Gbps InfiniBand. Each machine has the same configuration and is equipped with eight GPUs. Each GPU is connected with PCIe 4.0 x 16 and has 48 GB HBM, delivering up to 149.7 TFLOPS (FP16) with 96 cores. For all the experiments, the input sequence length is 2,048 and the global batch size is 64. We mainly compare the two structures in terms of training step latency, the corresponding All-to-All latency, and the inference throughput.\nComparing Training Latency via Megatron\nWe first compare the training step time of fine-grained and BigMac models under the Megatron framework. Here, we adopt four base models including GPT3-Medium, GPT3-XL, GPT3-2.7B, and GPT3-6.7B.\nFigure 3 shows that GPT-BigMac achieves the speedups of 1.53-2.41x and 2.45-3.07\u00d7 than GPT-Fine-Grained for Top4 and Top8 routing settings, respectively. Note that larger top_k generally indicates the heavier communication, hence GPT-BigMac enjoys greater advantages in the Top8 setting. For the MoE models with small experts, larger top_k implies better performance to some extent. Due to"}, {"title": "Inference Throughput Comparison with Megatron", "content": "For inference, we measure the throughput of the forward pass under the Megatron framework. We keep the number of the tokens per batch to be 128k, but with varying prompt lengths, ranging from 128 to 1,024. We use 16 and 32 GPUs for evaluation and we set the expert parallelism degree ep to 16 and 32, respectively. Here we do not adopt tensor parallelism since it is less efficient.\nFigure 5 shows that GPT-BigMac consistently outperforms GPT-Fine-Grained and achieves 1.72-2.45\u00d7 speedups across all the settings. First, BigMac can obtain higher speedups with larger top_k,. Second, the amplitude of speedup decreases slightly as the prompt length increases. Note that the larger prompt length brings heavier computation overhead in the attention layer, and then the proportion of All-to-All communication decreases correspondingly, especially for BigMac, which explains its slight decline in the inference throughput."}, {"title": "Comparison on All-to-All Optimized System", "content": "Finally, we investigate if BigMac's model structure can bring benefits further on systems which have already optimized the All-to-All bottleneck of MoE from systems perspectives. For training, we evaluate on Tutel and for in-"}, {"title": "Conclusion", "content": "We proposed a novel MoE structure named BigMac which uses a descend-communicate-communicate-ascend (DCCA) strategy to reduce the communication overhead by performing All-to-All operations at the lowest dimension. Results demonstrate that BigMac achieves comparable or superior model quality to the existing MoE structures, with significant speedups in training and inference across different platforms, making it a strong contender among MoE-based large language models."}, {"title": "Acknowledgements", "content": "We thank the anonymous reviewers for their insightful comments. This work is supported by the Strategic Priority Re-"}]}