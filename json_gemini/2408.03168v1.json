{"title": "Training on the Fly: On-device Self-supervised Learning aboard Nano-drones within 20mW", "authors": ["Elia Cereda", "Alessandro Giusti", "Daniele Palossi"], "abstract": "Abstract-Miniaturized cyber-physical systems (CPSes) pow-\nered by tiny machine learning (TinyML), such as nano-drones,\nare becoming an increasingly attractive technology. Their small\nform factor (i.e.,, ~10 cm diameter) ensures vast applicability,\nranging from the exploration of narrow disaster scenarios to safe\nhuman-robot interaction. Simple electronics make these CPSes\ninexpensive, but strongly limit the computational, memory, and\nsensing resources available on board. In real-world applica-\ntions, these limitations are further exacerbated by domain shift.\nThis fundamental machine learning problem implies that model\nperception performance drops when moving from the training\ndomain to a different deployment one. To cope with and mitigate\nthis general problem, we present a novel on-device fine-tuning\napproach that relies only on the limited ultra-low power resources\navailable aboard nano-drones. Then, to overcome the lack of\nground-truth training labels aboard our CPS, we also employ a\nself-supervised method based on ego-motion consistency. Albeit\nour work builds on top of a specific real-world vision-based\nhuman pose estimation task, it is widely applicable for many\nembedded TinyML use cases. Our 512-image on-device training\nprocedure is fully deployed aboard an ultra-low power GWT\nGAP9 System-on-Chip and requires only 1 MB of memory while\nconsuming as low as 19mW or running in just 510ms (at\n38 mW). Finally, we demonstrate the benefits of our on-device\nlearning approach by field-testing our closed-loop CPS, showing\na reduction in horizontal position error of up to 26% vs. a\nnon-fine-tuned state-of-the-art baseline. In the most challenging\nnever-seen-before environment, our on-device learning procedure\nmakes the difference between succeeding or failing the mission.", "sections": [{"title": "I. INTRODUCTION", "content": "INIATURIZED unmanned aerial vehicles (UAVs) as\nsmall as the palm of one hand, also known as nano-\nUAVs, are appealing cyber-physical systems (CPS), which,\nleveraging tiny machine learning (TinyML) algorithms de-\nployed aboard, have reached an unprecedented level of auton-\nomy. Thanks to their small form factor, i.e., 10 cm in diameter\nand sub-50g in weight, nano-UAVs can fly in narrow and\nconstrained spaces [1] or safely in the human proximity [2]"}, {"title": "II. RELATED WORK", "content": "Domain shift [5] is a significant challenge for machine\nlearning approaches in every field: models that perform well\non their training domain often underperform when deployed\nbecause real-world conditions differ or even change over\ntime. Robotics further exacerbates the issue due to a scarcity\nof training data, which compounds on hardware-constrained\nplatforms such as nano-UAVs, with the tiny DNN architectures\naffordable aboard. Past efforts address this issue by better-\ntaking advantage of the limited real-world training data [14],\ngenerating vast training datasets in simulation [15], and taking\nadvantage of additional sensors that are more robust to sim-\nto-real domain shift (e.g., depth sensors) [16].\nOn-device learning is an emerging field that proposes a\nradically different approach [17], [18], [19]: abandon the tra-\nditional train-once, deploy-everywhere paradigm by enabling\ndevices to adapt a model to their domain directly in the\nfield. However,\napplying on-device learning to nano-UAVs is still hurdled by\nthe limitations of the onboard sensors and computationally-\nconstrained processors. Compared to larger-scale drones [20],\nnano-UAVs can only afford MCU devices with 1/1000th the\nmemory and computational power of their bigger counterparts.\nEmbodiments of the Parallel Ultra Low Power (PULP) plat-\nform [21], in particular, have enabled a number of break-\nthrough applications on nano-UAVs [3], [4]. Regarding on-\ndevice learning on PULP devices, PULP-TrainLib [8] was re-\ncently proposed to accelerate neural network back-propagation\nthrough parallelization, software optimizations, and reduced\n16-bit floating-point precision.\nA number of approaches have been also demonstrated on\nother embedded platforms. For example, gradient rescaling,\nsparse updates, and compile-time graph optimization allow\nPockEngine [11] to achieve 8-bit approximated backward\npasses on STM32 MCUs. RLtools [9] introduce a highly"}, {"title": "III. BACKGROUND", "content": "We demonstrate our approach aboard an autonomous nano-\nUAV that performs the human pose estimation task [2]. A\nperception CNN takes gray-scale 160 \u00d7 96 px camera frames\nand estimates the subject's 4DOF pose [x, y, z, \u03c6], relative to\nthe drone's horizontal frame as shown in Figure 1. Therefore,\nthe model output poses do not depend on the pitch and\nroll orientations of the drone and human subjects. The poses\nare processed with a Kalman filter and fed to a closed-loop\nvelocity controller that autonomously flies the drone, keeping\na fixed distance A in front of the human subject and centering\nthem in the image."}, {"title": "A. Regression task", "content": "We demonstrate our approach aboard an autonomous nano-\nUAV that performs the human pose estimation task [2]. A\nperception CNN takes gray-scale 160 \u00d7 96 px camera frames\nand estimates the subject's 4DOF pose [x, y, z, \u03c6], relative to\nthe drone's horizontal frame as shown in Figure 1. Therefore,\nthe model output poses do not depend on the pitch and\nroll orientations of the drone and human subjects. The poses\nare processed with a Kalman filter and fed to a closed-loop\nvelocity controller that autonomously flies the drone, keeping\na fixed distance A in front of the human subject and centering\nthem in the image."}, {"title": "B. Robotic platform", "content": "Our target robot is the Bitcraze Crazyflie 2.1, a commercial\noff-the-shelf (COTS) nano-UAV, extended by the pluggable\nAI-deck and Flow-deck companion boards."}, {"title": "IV. METHOD", "content": "For our fine-tuning procedure, we implement the full back-\npropagation algorithm on an ultra-low power embedded de-\nvice. As in [8], we consider a feed-forward neural network\nof N layers. Each layer computes a non-linear function f\nparameterized by a weight tensor Wi:\nY\u2081 = fw(X\u2081)\ni \u2208 0, ..., \u039d-1\nwhere X and Y are input and output tensors. The network is a\ncomposition of the layers, where each layer's input Xi = Yi-1\nis the previous layer's output, up to the model's input X0.\nWe train the network on a loss function L defined on the\nmodel outputs. An optimization procedure, e.g., stochastic\ngradient descent (SGD), iteratively updates the weights Wi\nto minimize L according to the update rule:\nWi \u2190 Wi + \u03b7dWi\nbased on the gradients of the loss function w.r.t. each weight,\ndWi = 8L/8Wi, and a learning rate \u03b7.\nBack-propagation enables efficient computation of the gra-\ndients through the gradient chain rule. It performs a forward\npass (FW), which computes the model's output YN-1 and the\nloss function, followed by a backward pass (BW). For each\nlayer starting from the last, the BW pass computes the input\ngradient dXi, used to propagate the errors to the previous\nlayers (BW-IG), and the weight gradient dWi, used to update\nthe layer's own parameters (BW-WG)."}, {"title": "A. On-device fine-tuning", "content": "For our fine-tuning procedure, we implement the full back-\npropagation algorithm on an ultra-low power embedded de-\nvice. As in [8], we consider a feed-forward neural network\nof N layers. Each layer computes a non-linear function f\nparameterized by a weight tensor Wi:\nY\u2081 = fw(X\u2081)\ni \u2208 0, ..., \u039d-1\nwhere X and Y are input and output tensors. The network is a\ncomposition of the layers, where each layer's input Xi = Yi-1\nis the previous layer's output, up to the model's input X0.\nWe train the network on a loss function L defined on the\nmodel outputs. An optimization procedure, e.g., stochastic\ngradient descent (SGD), iteratively updates the weights Wi\nto minimize L according to the update rule:\nWi \u2190 Wi + \u03b7dWi\nbased on the gradients of the loss function w.r.t. each weight,\ndWi = 8L/8Wi, and a learning rate \u03b7.\nBack-propagation enables efficient computation of the gra-\ndients through the gradient chain rule. It performs a forward\npass (FW), which computes the model's output YN-1 and the\nloss function, followed by a backward pass (BW). For each\nlayer starting from the last, the BW pass computes the input\ngradient dXi, used to propagate the errors to the previous\nlayers (BW-IG), and the weight gradient dWi, used to update\nthe layer's own parameters (BW-WG)."}, {"title": "B. Self-supervised loss", "content": "We perform self-supervised learning with the state-\nconsistency loss introduced by Nava et al. [27]. We consider\na dataset of fine-tuning samples acquired at timesteps i \u2208T,\nwhere each sample comprises an image from the drone's\ncamera and the drone's and subject's poses. We define T\nin SE(3) as the relative pose of reference frame B w.r.t. A.\nThe fine-tuning process minimizes the loss function\nL = Ltask + AscLsc,\ncomposed of a task loss term Ltask and a state-consistency\nloss term Lsc. In accordance with [27], we set Asc = 1.\nFigure 5 depicts the two loss terms and their effects on model\npredictions. The task loss is defined on individual timesteps\ni, taken from the (possibly empty) subset T \u2286 T for which\ntarget relative poses Th are known:\nDi\n1\nH\nLtask = \u03a3\u0394(\u03a4\u039f, \u03a4)\n\u03a4\u2081\u0395\u03a4\nDi\nH\nwhere To represents the model's estimate of the subject w.r.t.\nthe drone relative pose at time i and \u0394(T1, T2) is a distance\nfunction between relative poses, which we define as the L1\nloss between 4DOF pose vectors (x, y, z, \u00a2)1.\nDi\nWhen Th are ground-truth relative poses (e.g., acquired by\na motion capture system), this loss is equivalent to ordinary\nsupervised learning. Noisy estimates can also be used as TD\nOur experiments explore the case when the relative pose is\nknown at a time i, and the subject subsequently remains still,\nwhich allows us to define the relative pose at a later time\nj as T = TDT. The relative pose TDDT indicates the\n(possibly noisy) odometry estimate of the drone's pose at time\nj w.r.t time i.\nThe state-consistency loss is defined instead on pairs of\ntimesteps i and j = i + dt at a fixed time delta (a hyperpa-\nrameter) sampled from the subset Tsc \u2282 T:\nLsc =\n1\n|Tsc |\\\n\u03a3\u0394(\u03a4\u039f\u03a4\u039f,\u03a4)\ni ETsc\nwhere TH is the subject's relative pose at time j w.r.t time i.\nCompared to [27], we reformulate the state-consistency loss\nto model drone and subject movements separately. Noisy\nestimates for the former are generally available through drone\nodometry, while the latter are not known by the drone. In the\nexperiments, we evaluate the regression performance impact\nof these sources of uncertainty individually as depicted in\nFigure 5-BCD. Subject movements TH will be replaced by\nthe identity matrix I, i.e., we assume the subject stands still."}, {"title": "C. Embedded implementation", "content": "We implement the proposed fine-tuning process in optimized C\ncode for the PULP architecture. Figure 6 shows the algorithm\npipeline, depicting dependencies and concurrent operations.\nWe compute the forward, loss function, and backward passes\nusing hand-written 32-bit float kernels that operate entirely\nin L1 memory. The 512-sample fine-tuning set, composed of\ninput images and the corresponding 4-element label outputs,\ntotals 7.8 MB stored as 8-bit quantized integers in the off-chip\nL3 memory. The fc method, which only ever needs to perform\nthe forward pass before the fully-connected layer once (see\nSection IV-A), stores just the fully-connected layer's 1920-\nelement input features and label outputs (~1 MB).\nDuring fine-tuning, the dataset is processed in 16 batches of\n32 samples each. Each batch is transferred from L3 to L2\nmemory, while samples are transferred to L1 memory one\nby one. Both transfers exploit the SoCs' DMA peripherals.\nWeight gradients are accumulated in L1 memory and applied\nto the weights once per batch. The updated weights are written\nout to L2 at the end of each epoch.\nOptimizations: In our kernel implementations, we apply\nfour software optimizations to exploit the target platform's"}, {"title": "V. RESULTS", "content": "We start our experiments by analyzing the proposed fine-\ntuning methods offline in the PyTorch framework on a set\nof 4.7k real-world images from [14]. This dataset comprises\n18 in-flight sequences from three subjects in challenging\nsituations, e.g., different subject appearances, dynamic mo-\ntions of both subjects and the drone. It is collected with the\nsame robotic platform introduced in Section III (including the\nHimax camera) and is used both for fine-tuning the pre-trained\nmodels and testing them. This dataset stresses the domain shift\nproblem, as it represents a completely novel domain for the\nbaseline models, which are trained either entirely in simulation\nor in a different real-world environment.\nFor each subject, we select a random temporally-contiguous\n128 s segment of the dataset (512 samples @ 4Hz) as the\nfine-tuning set, while the rest is used as the test set. To\nprovide unbiased measures of regression performance, we\ndiscard 100 contiguous samples (25 s) between fine-tuning and\ntest segments, and we apply cross-validation, repeating 3 runs\nfor each subject with different random fine-tuning segment (9\ntotal experiment runs). At most, 75% of each subject's samples\nare used as the fine-tuning set.\nBaseline performance: A lower bound on the test re-\ngression performance achieved by our models is obtained by\nnot doing any fine-tuning. An upper bound corresponds to\nfine-tuning in the best-case scenario: we fine-tune all model\nparameters, i.e. method all, entirely on the task loss (i.e.,"}, {"title": "A. Fine-tuning strategy", "content": "We start our experiments by analyzing the proposed fine-\ntuning methods offline in the PyTorch framework on a set\nof 4.7k real-world images from [14]. This dataset comprises\n18 in-flight sequences from three subjects in challenging\nsituations, e.g., different subject appearances, dynamic mo-\ntions of both subjects and the drone. It is collected with the\nsame robotic platform introduced in Section III (including the\nHimax camera) and is used both for fine-tuning the pre-trained\nmodels and testing them. This dataset stresses the domain shift\nproblem, as it represents a completely novel domain for the\nbaseline models, which are trained either entirely in simulation\nor in a different real-world environment.\nFor each subject, we select a random temporally-contiguous\n128 s segment of the dataset (512 samples @ 4Hz) as the\nfine-tuning set, while the rest is used as the test set. To\nprovide unbiased measures of regression performance, we\ndiscard 100 contiguous samples (25 s) between fine-tuning and\ntest segments, and we apply cross-validation, repeating 3 runs\nfor each subject with different random fine-tuning segment (9\ntotal experiment runs). At most, 75% of each subject's samples\nare used as the fine-tuning set.\nBaseline performance: A lower bound on the test re-\ngression performance achieved by our models is obtained by\nnot doing any fine-tuning. An upper bound corresponds to\nfine-tuning in the best-case scenario: we fine-tune all model\nparameters, i.e. method all, entirely on the task loss (i.e.,"}, {"title": "B. In-field experiments", "content": "In this experimental section, we assess the end-to-end per-\nformance of our system when deployed in the real world, i.e.,"}, {"title": "C. On-device fine-tuning", "content": "As a final experiment, we deploy the proposed method on\nthe GAP8 and GAP9 SoCs and profile its workload. Due to\nits computational and memory advantages, combined with the\nhighest in-field self-supervised performance, we focus on fc\nfor on-device deployment.\nFigure 10 shows the detailed power traces for one fine-\ntuning epoch at different operating points."}, {"title": "VI. CONCLUSION", "content": "We present on-device learning aboard nano-drones to miti-\ngate the general TinyML problem of domain shift. Our fine-\ntuning approach requires only 19mW, 1MB of memory, and\nruns in just 510 ms (5 epochs) on the best-in-class GWT GAP9\nSoC. We employ self-supervised learning to cope with the\nlack of ground-truth labels aboard our UAV. In-field results\nshow an improvement in control performance up to 26% vs. a\nnon-fine-tuned SotA baseline, making the difference between\nmission failure and success in a never-seen-before challenging\nenvironment."}]}