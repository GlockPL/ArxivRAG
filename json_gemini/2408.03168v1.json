{"title": "Training on the Fly: On-device Self-supervised Learning aboard Nano-drones within 20mW", "authors": ["Elia Cereda", "Alessandro Giusti", "Daniele Palossi"], "abstract": "Miniaturized cyber-physical systems (CPSes) powered by tiny machine learning (TinyML), such as nano-drones, are becoming an increasingly attractive technology. Their small form factor (i.e.,, ~10 cm diameter) ensures vast applicability, ranging from the exploration of narrow disaster scenarios to safe human-robot interaction. Simple electronics make these CPSes inexpensive, but strongly limit the computational, memory, and sensing resources available on board. In real-world applications, these limitations are further exacerbated by domain shift. This fundamental machine learning problem implies that model perception performance drops when moving from the training domain to a different deployment one. To cope with and mitigate this general problem, we present a novel on-device fine-tuning approach that relies only on the limited ultra-low power resources available aboard nano-drones. Then, to overcome the lack of ground-truth training labels aboard our CPS, we also employ a self-supervised method based on ego-motion consistency. Albeit our work builds on top of a specific real-world vision-based human pose estimation task, it is widely applicable for many embedded TinyML use cases. Our 512-image on-device training procedure is fully deployed aboard an ultra-low power GWT GAP9 System-on-Chip and requires only 1 MB of memory while consuming as low as 19mW or running in just 510ms (at 38 mW). Finally, we demonstrate the benefits of our on-device learning approach by field-testing our closed-loop CPS, showing a reduction in horizontal position error of up to 26% vs. a non-fine-tuned state-of-the-art baseline. In the most challenging never-seen-before environment, our on-device learning procedure makes the difference between succeeding or failing the mission.", "sections": [{"title": "I. INTRODUCTION", "content": "INIATURIZED unmanned aerial vehicles (UAVs) as small as the palm of one hand, also known as nano-UAVs, are appealing cyber-physical systems (CPS), which, leveraging tiny machine learning (TinyML) algorithms deployed aboard, have reached an unprecedented level of autonomy. Thanks to their small form factor, i.e., 10 cm in diameter and sub-50g in weight, nano-UAVs can fly in narrow and constrained spaces [1] or safely in the human proximity [2], embodying the ultimate dynamic IoT smart sensors, capable of analyzing their surrounding and flying where their presence is most needed. To cope with extremely limited onboard sensory, memory, and computational resources, i.e.,, low-resolution cameras, a few MB off-chip memories, and sub-100mW power envelope for the computational units, recent works make extensive use of optimized TinyML workloads, such as convolutional neural networks (CNNs) [2], [3], [4]. Apart from the well-known challenges in deploying complex deep learning (DL) models on ultra-low-power microcontroller units (MCUs), TinyML algorithms are vulnerable to the fundamental problem of domain shift [5]. Domain shift occurs when a DL model, such as a classifier or regressor, trained on data acquired in a given context, i.e., domain, is deployed in a different one. Then, the predictive performance of the system (i.e., the accuracy or estimation error) often decreases because the training data is not representative of the deployment domain. This problem is particularly present in real-world applications, such as the robotic task we address in this work. This paper explores on-device learning as a viable and effective solution to the domain shift problem. We consider the case in which a CNN, previously trained on a given task using large datasets, is fine-tuned on-device, using a small amount of data collected after deployment. Fine-tuning consists of updating the parameters of the pre-trained models by executing a limited number of additional training steps. With on-device self-supervised learning, data can be collected by the robot precisely in the domain in which the model will operate, counteracting the domain shift. Despite on-device learning being conceptually simple and attractive, pulling it off in a real-world application is extremely challenging. Fine-tuning typically requires significant memory and computational resources, including hardware support for floating-point arithmetic (not always available on MCUs) and data availability with the corresponding ground-truth labels. In this work, we study this problem in the context of a real-world robotic application, covering the full pipeline of design, deployment, and testing of a vertically integrated on-device learning system and evaluating the practical implications of our design choices. In particular, we strive to answer three key research questions: i) What is the best fine-tuning strategy to exploit the limited computing power? iii) Can the limited onboard memory fit enough data for fine-tuning to be effective? ii) How to deal with the lack of ground-truth data in the field? While analysed in a specific context, these questions yield valuable insights for real-world applications of on-device learning also on other tasks. We start from the State-of-the-Art (SotA) PULP-Frontnet CNN [2] for the human pose estimation task, whose outputs control the nano-UAV and allow to keep it in front of the user at a predefined distance. We tackle the first computational challenges by adopting an ultra-low-power GWT GAP8 System-on-Chip (SoC) and its next generation, the GAP9. Then, we investigate four fine-tuning strategies, spanning from the most memory-hungry fine-tuning of all layers of the CNN down to minimal fine-tuning of only the final fully connected layer. Further, we propose a self-supervised state-consistency loss term [13] to handle the data availability problem, relieving our fine-tuning process from ground-truth labels. Our main contributions are the following:\n\u2022 an in-depth analysis of four fine-tuning strategies, which explores the trade-off between computational and memory requirements and their prediction performance in a real-world scenario. Our results show a regression performance improvement up to 30% of our self-supervised method against a non-fine-tuned baseline, which grows to 56% when using ground-truth labels.\n\u2022 On-device implementation and profiling of the resulting best fine-tuning approach, requiring only 6.6s@102mW and 511ms@38mW for fine-tuning on 512 images (5 epochs) on the GAP8 and GAP9 SoC, respectively.\n\u2022 An in-field evaluation in a very challenging deployment field where the SotA PULP-Frontnet baseline fails in following the user, while all self-supervised fine-tuned models can complete between 92 and 100% of the expected path (over 3 runs for each model).\n\u2022 A quantitative evaluation of the nano-UAV position error resulting from an autonomous closed-loop controller that uses the fine-tuned model for perception. Compared to a pre-trained perception model, it reduces by 26% the horizontal position error.\nOur work marks the first real-world demonstration of on-device learning aboard a nano-UAV, addressing the critical domain shift problem and paving the way for more general advancements in the scientific community."}, {"title": "II. RELATED WORK", "content": "Domain shift [5] is a significant challenge for machine learning approaches in every field: models that perform well on their training domain often underperform when deployed because real-world conditions differ or even change over time. Robotics further exacerbates the issue due to a scarcity of training data, which compounds on hardware-constrained platforms such as nano-UAVs, with the tiny DNN architectures affordable aboard. Past efforts address this issue by better-taking advantage of the limited real-world training data [14], generating vast training datasets in simulation [15], and taking advantage of additional sensors that are more robust to sim-to-real domain shift (e.g., depth sensors) [16].\nOn-device learning is an emerging field that proposes a radically different approach [17], [18], [19]: abandon the traditional train-once, deploy-everywhere paradigm by enabling devices to adapt a model to their domain directly in the field. However, applying on-device learning to nano-UAVs is still hurdled by the limitations of the onboard sensors and computationally-constrained processors. Compared to larger-scale drones [20], nano-UAVs can only afford MCU devices with 1/1000th the memory and computational power of their bigger counterparts. Embodiments of the Parallel Ultra Low Power (PULP) platform [21], in particular, have enabled a number of breakthrough applications on nano-UAVs [3], [4]. Regarding on-device learning on PULP devices, PULP-TrainLib [8] was recently proposed to accelerate neural network back-propagation through parallelization, software optimizations, and reduced 16-bit floating-point precision.\nA number of approaches have been also demonstrated on other embedded platforms. For example, gradient rescaling, sparse updates, and compile-time graph optimization allow PockEngine [11] to achieve 8-bit approximated backward passes on STM32 MCUs. RLtools [9] introduce a highly portable on-device reinforcement learning implementation and test it, among others, on an i.MX RT1060 MCU. Despite being tailored for robotics applications, RLtools focus on fully connected models and control tasks, while our focus is on more computationally intensive perception tasks with CNN-based methods.\nVarious approaches propose to reduce the re-training workload through spare updates, i.e., by fine-tuning only a subset of the model parameters [6], [7], [10], [12]. Mudrakarta et al. [6] limits updates to just the batch normalization layers, which TinyTL [7] further reduces to only the bias parameters. The latter translates to significant memory savings (up to 6.5x) by not storing activation maps during the forward pass since they are needed only for back-propagation to weight parameters, not biases. TinyTrain [10] introduces a dynamic strategy based on task-adaptive sparse updates, which achieves a 5.0% accuracy gain on image classification benchmarks and reduces the backward-pass memory and computation by up to 2.3x and 7.7\u00d7, respectively. LifeLearner [12] applies lossy compression to the stored activation maps to reduce their size in memory by at least 11.4\u00d7. Crucially, these methods are tested only on image classification tasks and assume that a training dataset with ground-truth labels is provided externally. Further, not all methods are deployed and evaluated on the target embedded devices. On the contrary, our approach extends these techniques [6], [7] to a regression task, and deploys them on-device and in the real world, where ground-truth labels are not readily available. Addressing these fundamental problems has a far-reaching impact on many real-life perception problems.\nExploiting self-collected data and deriving supervisory information exclusively from onboard sensors (as opposed to external infrastructure, manual annotation, or user intervention) is the core idea of self-supervised learning. Three categories of approaches have been recently applied in autonomous robotics. Most approaches derive noisy approximations to the ground-truth labels using task-specific methods, such as exploring an environment with a drone until it crashes [22] or while continuously measuring its distance from the surrounding environment [23]. Others learn a secondary task, for which ground-truth labels are known to the robot, as a pretext for learning the task of interest, for which no or little data is available. Namely, predicting a quadrotor's sound from camera images is a strong proxy for estimating its location [24]. Additionally, a masked autoencoder for image reconstruction can be adapted to perform a range of robot manipulation tasks [25]. Finally, audio, optical flow, and depth estimation are useful pretexts for visual odometry [26]. The third group imposes geometric consistency constraints to improve model predictions: an object pose estimation model must be consistent over time with the robot's ego motion [27], while visual odometry between pairs of camera images must satisfy the transitive property [28]. The photometric reprojection error is another common constraint used to train self-supervised monocular depth [29], optical flow [30], and visual odometry [31], [32] models.\nOur work proposes a self-supervised fine-tuning process based on an ego-motion consistency loss and approximated labels derived through task-specific collaboration with the user. However, in contrast to the above approaches, which assume abundant self-supervised data, we are tightly constrained by the amount of fine-tuning data we can store on our embedded system's 8MB DRAM memory. Additionally, previous consistency approaches assume self-supervised labels that are noisy but fully measurable (e.g., learning object pose estimation in the presence of odometry error while the object remains still [27]). By comparison, we also deal with inherently unknown data, such as the movements of human subjects."}, {"title": "III. BACKGROUND", "content": "We demonstrate our approach aboard an autonomous nano-UAV that performs the human pose estimation task [2]. A perception CNN takes gray-scale 160 \u00d7 96 px camera frames and estimates the subject's 4DOF pose [x, y, z, \u03c6], relative to the drone's horizontal frame. Therefore, the model output poses do not depend on the pitch and roll orientations of the drone and human subjects. The poses are processed with a Kalman filter and fed to a closed-loop velocity controller that autonomously flies the drone, keeping a fixed distance \u2206 in front of the human subject and centering them in the image.\nOur target robot is the Bitcraze Crazyflie 2.1, a commercial off-the-shelf (COTS) nano-UAV, extended by the pluggable AI-deck and Flow-deck companion boards. The Crazyflie relies on an STM32 single-core microcontroller unit (MCU) for low-level flight control and state estimation. It can reach up to 7 min flight time on a single 380 mAh battery. The AI-deck extends onboard sensing and processing capabilities with a GreenWaves Technologies (GWT) GAP8 SoC and a Himax HM01B0 gray-scale QVGA camera, while the Flow-deck provides a time-of-flight laser-based altitude sensor and an optical flow sensor to improve the drone's state estimation for autonomous flight.\nThe GAP8 SoC is an embodiment of the parallel ultra-low power platform (PULP) [21]. It features two power domains: a computationally-capable 8-core cluster (CL) and a single-core fabric controller (FC), in charge of data orchestration for the CL's execution. All cores are based on the RISC-V instruction set architecture; the FC can reach up to 250 MHz, while the CL peaks at 175 MHz. The on-chip memories are organized in a fast 64kB scratchpad L1 and a slower 512kB L2 memory. Additionally, the AI-deck features also 8 MB off-chip DRAM memory and 64 MB Flash. Finally, the GAP8 does not provide any hardware support for floating-point calculations and requires either costly soft-float emulation (10\u00d7 overhead) or fixed-point arithmetic through quantization.\nThe next-generation SoC GAP9 marks significant improvements compared to GAP8. Most significantly, GAP9's CL includes four shared floating point units (FPUs), which execute floating-point instructions in a single clock cycle. In the context of on-device training, floating-point hardware support is extremely valuable for back-propagation, for which the basic primitives are implemented in the PULP-TrainLib [8] software library. For convolutional layers, PULP-TrainLib running on the GAP9 achieves a peak performance efficiency of 5.3 and 4.6 multiply-accumulate operations (MAC) per clock cycle for, respectively, the forward and backward passes."}, {"title": "IV. METHOD", "content": "For our fine-tuning procedure, we implement the full back-propagation algorithm on an ultra-low power embedded device. As in [8], we consider a feed-forward neural network of N layers. Each layer computes a non-linear function f parameterized by a weight tensor W\u1d62:\nY\u1d62 = f_{W\u1d62}(X\u1d62) \n i \u2208 0, ..., N-1  (1)\nwhere X\u1d62 and Y\u1d62 are input and output tensors. The network is a composition of the layers, where each layer's input X\u1d62 = Y\u1d62\u208b\u2081 is the previous layer's output, up to the model's input X\u2080.\nWe train the network on a loss function L defined on the model outputs. An optimization procedure, e.g., stochastic gradient descent (SGD), iteratively updates the weights W\u1d62 to minimize L according to the update rule:\nW\u1d62 \u2190 W\u1d62 + \u03b7dW\u1d62  (2)\nbased on the gradients of the loss function w.r.t. each weight, dW\u1d62 = \u2202L/\u2202W\u1d62, and a learning rate \u03b7.\nBack-propagation enables efficient computation of the gradients through the gradient chain rule. It performs a forward pass (FW), which computes the model's output Y_{N-1} and the loss function, followed by a backward pass (BW). For each layer starting from the last, the BW pass computes the input gradient dX\u1d62, used to propagate the errors to the previous layers (BW-IG), and the weight gradient dW\u1d62, used to update the layer's own parameters (BW-WG)."}, {"title": "Crucially, BW-IG and BW-WG make neural network training ~3\u00d7 as computationally expensive as inference (which executes only the FW phase). In addition, computing dWi requires storing intermediate outputs Xi from the FW to the BW pass, quickly making training unfeasible on memory-constrained embedded devices. Finally, weights and weight gradients typically have order-of-magnitude differences in scale, which complicates resorting to reduced precision arithmetic (16-bit float or quantized int). Thus, we focus our implementation to 32-bit float arithmetic.", "content": "Fine-tuning strategies: Our hardware limitations motivate us to investigate strategies to reduce the cost of the fine-tuning workload. We select four fine-tuning strategies from the family of sparse updates, adhering to two design constraints: i) each strategy updates a fixed subset of the model parameters, ii) each subset contains a uniform type of parameters (e.g., batch normalization, fully connected, or biases). Under these constraints, the strategies follow from optimizing different regression performance/memory/computation trade-offs due to the characteristics of each layer's forward and backward phases. In Table III, we analyze the workload of one optimization step (FW + BW) with our architecture on a single frame, depending on which subset of model parameters is updated.\n(a) Entire model (all): our baseline setting is to update the weight and biases of every layer in the model, the most expensive method at 53 MMAC per frame. In addition, it requires storing all intermediate activations and weight gradients, which in 32-bit floats amount to 2MB for a single frame, 25% of the AI-deck's 8 MB DRAM, which is also needed to store our fine-tuning dataset.\n(b) Only batch-norm layers (bn): batch normalization layers contain a mere 0.33% parameters of the entire model while significantly impacting model performance [6]. This method requires 70% less memory and 26% less computation due to the smaller weight gradients.\n(c) Only biases (bias): as BW-WG of the biases is simply the BW-IG of the following layer, full intermediate activations don't need to be stored [7]. The 1-bit signs of the intermediate activations are still needed for BW-IG of ReLU non-linearities, 0.8 kB per frame. Total memory decreases to just 17.7 kB, while computation remains comparable to bn.\n(d) Only the fully-connected layer (fc): by far, the greatest reduction comes from fine-tuning only the final layer of the network, although this also has the least expected improvement among the four methods [7]. The forward pass of all layers up to the final fully connected one can be pre-computed just once at the beginning of fine-tuning, a 99.9% reduction in computation. In our implementation, this pre-computed forward pass is further quantized to 8-bit integers and runs in real-time at 48 Hz during fine-tuning set acquisition. The fine-tuning dataset then only stores the resulting feature vectors (1920 elements) instead of full input images (160 \u00d7 96 px), reducing memory storage by 98.1%.\nHyper-parameters: compared to the initial training, we substitute Adam with stochastic gradient descent (SGD) due to 3\u00d7 lower computation cost. With hyper-parameter search, we set the learning rate to 10\u207b\u00b2 to retain comparable regression performances to Adam. Additionally, we select a fine-tuning set size of 512 samples, such that the entire dataset fits the AI-deck's 8 MB DRAM memory even for methods that require storing the full input images (7.8 MB), and set five epochs as fixed fine-tuning length. We always select the final model at the end of fine-tuning, thus avoiding storing multiple model checkpoints.\nData augmentation: we employ standard photometric data augmentations, including exposure and contrast adjustment, Gaussian noise, box blurring, and vignetting. These transformations operate mostly pixel-wise, benefiting from parallelization and high spatial locality, allowing efficient implementations on embedded devices such as GAP SoCs. Furthermore, we randomly flip images and ground-truth labels horizontally to guarantee the fine-tuning set follows a symmetric distribution along the y axis and yaw orientation. Similarly, we randomly apply time reversal when fine-tuning with state consistency to impose that relative poses T^{H}_{ij} and T^{D}_{ij} also respect a zero-mean symmetric distribution (i.e., centered on the identity)."}, {"title": "B. Self-supervised loss", "content": "We perform self-supervised learning with the state-consistency loss introduced by Nava et al. [27]. We consider a dataset of fine-tuning samples acquired at timesteps i \u2208T, where each sample comprises an image from the drone's camera and the drone's and subject's poses. We define T^{B}_{A} in SE(3) as the relative pose of reference frame B w.r.t. A. The fine-tuning process minimizes the loss function\nL = L_{task} + \u03b1_{sc}L_{sc}, (6)\ncomposed of a task loss term L_{task} and a state-consistency loss term L_{sc}. In accordance with [27], we set \u03b1_{sc} = 1. Figure 5 depicts the two loss terms and their effects on model predictions. The task loss is defined on individual timesteps i, taken from the (possibly empty) subset T_{t} \u2286 T for which target relative poses T^{H}_{Di} are known:\nL_{task} = \\frac{1}{|T_t|} \\sum_{i \\in T_t} \\Delta(T^H_{Di}, \\hat{T}^H_{Di}), (7)\nwhere \\hat{T}^H_{Di} represents the model's estimate of the subject w.r.t. the drone relative pose at time i and \u2206(T1, T2) is a distance function between relative poses, which we define as the L1 loss between 4DOF pose vectors (x, y, z, \u03c6).\nWhen T^{H}_{Di} are ground-truth relative poses (e.g., acquired by a motion capture system), this loss is equivalent to ordinary supervised learning. Noisy estimates can also be used as T^{H}_{Di}.\nOur experiments explore the case when the relative pose is known at a time i, and the subject subsequently remains still, which allows us to define the relative pose at a later time j as \\hat{T}^{H}_{Dj} = T^{D}_{Di}T^{H}_{Di}T^{D}_{Dj}. The relative pose T^{D}_{Di}T^{D}_{Dj} indicates the (possibly noisy) odometry estimate of the drone's pose at time j w.r.t time i.\nThe state-consistency loss is defined instead on pairs of timesteps i and j = i + dt at a fixed time delta (a hyperparameter) sampled from the subset T_{sc} \u2282 T:\nL_{sc} = \\frac{1}{|T_{sc}|} \\sum_{i \\in T_{sc}} \\Delta(\\hat{T}^{H}_{ij}, T^{D}_{ij}), (8)\nwhere \\hat{T}^{H}_{ij} is the subject's relative pose at time j w.r.t time i. Compared to [27], we reformulate the state-consistency loss to model drone and subject movements separately. Noisy estimates for the former are generally available through drone odometry, while the latter are not known by the drone. In the experiments, we evaluate the regression performance impact of these sources of uncertainty individually as depicted in Figure 5-BCD. Subject movements T^{H}_{ij} will be replaced by the identity matrix I, i.e., we assume the subject stands still."}, {"title": "Embedded implementation", "content": "We implement the proposed fine-tuning process in optimized C code for the PULP architecture.  We compute the forward, loss function, and backward passes using hand-written 32-bit float kernels that operate entirely in L1 memory. The 512-sample fine-tuning set, composed of input images and the corresponding 4-element label outputs, totals 7.8 MB stored as 8-bit quantized integers in the off-chip L3 memory. The fc method, which only ever needs to perform the forward pass before the fully-connected layer once (see Section IV-A), stores just the fully-connected layer's 1920-element input features and label outputs (~1 MB).\nDuring fine-tuning, the dataset is processed in 16 batches of 32 samples each. Each batch is transferred from L3 to L2 memory, while samples are transferred to L1 memory one by one. Both transfers exploit the SoCs' DMA peripherals. Weight gradients are accumulated in L1 memory and applied to the weights once per batch. The updated weights are written out to L2 at the end of each epoch.\nOptimizations: In our kernel implementations, we apply four software optimizations to exploit the target platform's hardware architecture efficiently.\ni) Double buffering: L3-L2 memory transfers are constrained by the limited off-chip memory bandwidth (~90MB/s). As shown in Figure 6, we introduce double buffering to schedule DMA transfers concurrently with forward/backward computation, thereby fully hiding transfer latency (except for the first batch in each epoch).\nii) Loop unrolling: on GAP chips (strictly in-order architectures), memory instructions suffer from load stalls due to access latency. To hide these latencies, we manually unroll 4 iterations of our kernels along the output tensor dimension.\niii) L1 memory: We take advantage of the cluster L1 memory area (64kB on GAP8, 128 kB on GAP9, single-cycle access time) for every buffer (i.e., input, outputs, labels, weights, and weight gradients) in the inner loops of our kernels. The cluster DMA handles L2-L1 transfers sample by sample.\niv) Parallelization: We take advantage of the GAP chips' multi-core cluster by executing the forward/backward computation on 8 cores, with a blocking strategy that distributes work among cores along the input tensor dimension. Section V-C measures the latency improvements of each optimization individually."}, {"title": "V. RESULTS", "content": "We start our experiments by analyzing the proposed fine-tuning methods offline in the PyTorch framework on a set of 4.7k real-world images from [14]. This dataset comprises 18 in-flight sequences from three subjects in challenging situations, e.g., different subject appearances, dynamic motions of both subjects and the drone. It is collected with the same robotic platform introduced in Section III (including the Himax camera) and is used both for fine-tuning the pre-trained models and testing them. This dataset stresses the domain shift problem, as it represents a completely novel domain for the baseline models, which are trained either entirely in simulation or in a different real-world environment.\nFor each subject, we select a random temporally-contiguous 128 s segment of the dataset (512 samples @ 4Hz) as the fine-tuning set, while the rest is used as the test set. To provide unbiased measures of regression performance, we discard 100 contiguous samples (25 s) between fine-tuning and test segments, and we apply cross-validation, repeating 3 runs for each subject with different random fine-tuning segment (9 total experiment runs). At most, 75% of each subject's samples are used as the fine-tuning set.\nBaseline performance: A lower bound on the test regression performance achieved by our models is obtained by not doing any fine-tuning. An upper bound corresponds to fine-tuning in the best-case scenario: we fine-tune all model parameters, i.e. method all, entirely on the task loss (i.e., T = T_{t} and T_{sc} = \u00d8) and assuming perfect knowledge of the drone and subject poses.\nTable IV reports the resulting mean absolute errors (MAE) and R\u00b2 scores\u00b2. Model (A), trained on real-world data and not fine-tuned, represents the performance of the original PULP-Frontnet [2] on our test set. Model (B) is trained on simulated data, is not fine-tuned, and constitutes our lower bound.\nFor the fine-tuned models, we report several averages: (D) reports the average performance of a model when tested on the same subject and the same environment that it was fine-tuned on; (C) is the average performance of a model, tested on a different subject (but the same environment) that it was fine-tuned on. Table IV shows that, despite the higher initial performance, the models trained with real-world data achieve a poorer fine-tuned performance than those trained on simulation data. This confirms that training on a larger and more varied simulation train set translates to a model that better adapts to unseen subjects and environments when fine-tuned. Therefore, in the following experiments, we focus on scenario (D): models trained on simulated datasets and tested on the same subject and environment they were fine-tuned on.\nSelf-supervised learning: in Figure 8, we report our fine-tuning results obtained when ground-truth labels are unavailable. We consider three setups for the drone pose - perfect absolute pose, perfect odometry (dm), and uncertain odometry (do) - and three for the pose of the human subject - perfect absolute pose, perfect odometry (sm), and unknown (si).\nWhen perfect absolute poses are known for both drone and subject, we have the ground-truth information to fine-tune using Lt (i.e., regular supervised learning). The supervised case where these are known for all samples, considered in all previous experiments, is named t(a) and reaches MAE 0.27.\nTo reduce our reliance on privileged information, we assume only odometry is known, i.e. relative poses between two instants in time. When odometry is perfect for both drone and subject on all samples sc(a, dm, sm), we can fine-tune using L_{sc}, which achieves 93% of the ideal improvement. Uncertain drone odometry sc(a, do, sm) also has a limited impact on performance and achieves 85% of the ideal. Performance improves at higher state-consistency time deltas dt (horizontal axis), as imposing state-consistency between samples farther in time carries a larger information content.\nOn the other hand, an unknown subject odometry sc(a, do, si) drastically reduces performance. In this case, in order to compute L_{sc}, we assume that \\hat{T}^H_{ij} = I, i.e., that the subject is always still. Although the time reversal augmentation ensures this holds on average, i.e., the fine-tuning set has E[T^H_{ij}] = I by design, the model degenerates to a dummy predictor that always outputs a constant value.\nTo address the above issue, we design a cooperative scenario in which the subject moves to a known pose w.r.t. the drone (e.g., 1 m in front of the drone, directly facing the camera and centered in the field of view). While the subject stands still, the drone randomly moves around to acquire a small number of samples. The procedure is repeated from multiple start locations in the environment to acquire highly diverse fine-tuning data. We test this scenario, called t(r32) + sc(a, do, si), by selecting a random 32-sample subset of frames on which we optimize L_{t}. It relies on realistic in-field infrastructure-free data acquisition and achieves a significant improvement, up to 39%. As such, we select it as the SSL loss function in the following experiment, in the best-performing dt = 2s configuration.\nFine-tuning methods: in Table V, we explore the effectiveness of methods that reduce the fine-tuning workload by limiting the subset of model parameters to update. Full fine-tuning, named all, sets the lower bound at an MAE of 0.27 (-56% compared to the non-finetuned baseline). Optimizing only the batch-norm layers, bn, is second best, followed closely by fine-tuning the biases, bias. Fine-tuning the final fully connected layer, fc, performs the worst, but notably still shows an MAE improvement of -26%."}, {"title": "B. In-field experiments", "content": "In this experimental section", "models": "four fine-tuning methods", "2": "the human subject walks a predetermined path"}, {"2": "immediately fails the experiment: it moves away from the subject instead of approaching, thus completing 0% of the path. The SotA simulator baseline [16"}]}