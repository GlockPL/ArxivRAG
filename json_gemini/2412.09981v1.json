{"title": "SUMI-IFL: An Information-Theoretic Framework for Image Forgery Localization with Sufficiency and Minimality Constraints", "authors": ["Ziqi Sheng", "Wei Lu", "Xiangyang Luo", "Jiantao Zhou", "Xiaochun Cao"], "abstract": "Image forgery localization (IFL) is a crucial technique for preventing tampered image misuse and protecting social safety. However, due to the rapid development of image tampering technologies, extracting more comprehensive and accurate forgery clues remains an urgent challenge. To address these challenges, we introduce a novel information-theoretic IFL framework named SUMI-IFL that imposes sufficiency-view and minimality-view constraints on forgery feature representation. First, grounded in the theoretical analysis of mutual information, the sufficiency-view constraint is enforced on the feature extraction network to ensure that the latent forgery feature contains comprehensive forgery clues. Considering that forgery clues obtained from a single aspect alone may be incomplete, we construct the latent forgery feature by integrating several individual forgery features from multiple perspectives. Second, based on the information bottleneck, the minimality-view constraint is imposed on the feature reasoning network to achieve an accurate and concise forgery feature representation that counters the interference of task-unrelated features. Extensive experiments show the superior performance of SUMI-IFL to existing state-of-the-art methods, not only on in-dataset comparisons but also on cross-dataset comparisons.", "sections": [{"title": "Introduction", "content": "Driven by the extensive accessibility of large-scale digital image datasets and the advancement of AIGC technologies, generating vast quantities of forgery images that surpass human detection has become remarkably effortless. However, the malicious utilization of these forged images can lead to severe consequences, such as identity theft, privacy violations, large-scale economic fraud, and the proliferation of misinformation. Given the severe consequences of image forgeries, there has been an increasing focus on developing advanced image forgery localization (IFL) technologies.\nIFL is a technique that performs true/false judgment on suspicious images and further predicts tampered regions at a pixel level. Research on image forgery localization (IFL) has been proliferating and can be broadly categorized into two main groups: those focusing on extracting more comprehensive forgery clues, and those focusing on obtaining more accurate forgery clues. One line of work explores comprehensive forgery clues by designing multi-stream structures or utilizing multiple dimensions of auxiliary information (Sun et al. 2023; Liu et al. 2024). For instance, (Zhang, Li, and Chang 2024) designed a two-stream architecture incorporating RGB and frequency features to detect tampered images. MVSS-Net (Dong et al. 2022) proposed an edge-sensitive branch and noise-sensitive branch to mine the forgery edge with the aid of edge information and noise information. After fully acquiring the forgery features, some task-unrelated noises will inevitably be introduced. For example, the post-processing operation traces, the JPEG artifacts (Kwon et al. 2022a) contained in JPEG images, and reconstruction artifacts in stereo images (Luo et al. 2022). This task-irrelevant information can significantly affect the localization performance of the IFL task. Therefore, another line of work is dedicated to obtaining more accurate forgery features (Zhang et al. 2024), so as to resist the interference of task-unrelated information. For example, (Zhuo et al. 2022) utilized a self-attention mechanism including the spatial attention branch and channel attention branch to better localize forgery regions. (Li et al. 2023a) devised a region message passing controller to weaken the message passing between the forged and authentic regions, thus obtaining a refined forgery feature. Although these methods have largely advanced the IFL field, the urgent challenge of extracting forgery clues comprehensively and accurately still exists due to the rapid advancement of image forgery techniques.\nTo meet this challenge, we propose an information-theoretic IFL framework named SUMI-IFL employs sufficiency-view and minimality-view constraints to obtain more comprehensive and accurate forgery features. The sufficiency-view constraint is applied to the feature extraction network to ensure that the latent forgery feature contains comprehensive forgery clues by maximizing the mutual information between the latent feature and the ground-truth label. Besides, to further explore comprehensive forgery clues, we construct the latent forgery feature from several individual forgery features from multiple perspectives."}, {"title": "Related work", "content": "Nowadays, many researchers are making an effort to design sophisticated and complex models to achieve sound performance in image forgery localization tasks (Sheng, Yin, and Lu 2025; Wang et al. 2024). Span (Hu et al. 2020) designed a pyramid structure of local self-attention blocks to model spatial correlation in suspicious images. Object-Former (Wang et al. 2022) utilized an object encoder and a patch encoder to mine both the RGB features and frequency features to identify the tampering artifacts. MMFusion (Triaridis and Mezaris 2024) combined RGB images with an auxiliary forensic modality to perform image manipulation localization. MVSS-Net (Dong et al. 2022) proposed an edge-supervised branch to learn the forgery edge and a noise-sensitive branch to capture abnormal noise. In the meantime, some methods work on reasoning and fine-tuning the learned tampering features to enhance the performance. After obtaining a latent feature from a baseline detector, IF-OSN (Wu et al. 2022) further modeled the noise involved by the online social network for robust image forgery detection. CAT-Net (Kwon et al. 2022b) learned multi-scale forgery features from both the RGB stream and the DCT stream, and then all these learned features are subsequently fed into the fusion stage for a final prediction. HiFi-IFDL (Guo et al. 2023) devised a hierarchical fine-grained network to learn feature maps of different resolutions for a comprehensive representation of image forgery detection. All these works contribute a lot to the image forgery detection field. Nevertheless, challenges remain as these learned forgery features are still somewhat incomplete and redundant due to the lack of concrete theoretical guarantees. In this paper, we propose the innovative framework SUMI-IFL to explore the representation of forgery features guided by rigorous theoretical proofs."}, {"title": "Information bottleneck", "content": "The concept of information bottlenecks (Tishby, Pereira, and Bialek 2000) is currently used in deep learning both theoretically and practically and provides a solid foundation to constrain the feature representation in a variety of research domains. (Li et al. 2023b) found the minimal sufficient statistics of the whole slide image and fine-tuned the backbone into a task-specific representation. IBD (Kuang et al. 2024) devised two distillation strategies that align with the two optimization processes of the information bottleneck to improve the robustness of deep neural networks. SCMVC (Cui et al. 2024) solved the issue of feature redundancy across multiple views for multi-view clustering from an information-theoretic standpoint. (Ba et al. 2024) proposed a Deepfake detection scheme to extract task-relevant local features and learn a global feature by eliminating superfluous information. Inspired by these brilliant methods, we introduce the Information Bottlenecks theory into the field of forgery image localization, constrain the representation of forgery features, and thus effectively improve localization performance."}, {"title": "Method", "content": "As shown in Fig. 2, we denote \\(h_\\theta = r(e)\\) as a deep neural network with parameter \\(\\theta\\), in a standard IFL task. Here, \\(e : \\mathbb{R}^{d_x} \\rightarrow \\mathbb{R}^{d_f}\\) maps the inputs image \\(X\\) to the latent forgery feature \\(F\\), and \\(r : \\mathbb{R}^{d_f} \\rightarrow \\mathbb{R}^{d_z}\\) further maps the latent feature \\(F\\) to the final predict feature \\(Z\\), so that \\(e\\) is a feature extraction network, \\(F = e(X)\\) and \\(r\\) is a feature reasoning network, \\(r(F) = r(e(X)) = Z\\). Furthermore, a set of individual features from different backbones is denoted as \\(F = e(X) = \\{f_1, f_2,..., f_n\\}\\), \\(n\\) represent the number of backbones in the feature extraction network. Each feature has its own feature map size and channel dimension, denoted as \\(f_i \\in \\mathbb{R}^{C\\times H \\times W}\\), where \\(C\\), \\(H\\) and \\(W\\) represent the channel numbers, feature height, and width, respectively and \\(i = 1...n\\).\nThe sufficient-view constraint is applied to the feature extraction network \\(e\\) to ensure the comprehensiveness of feature representation. Specifically, we ensure the comprehensiveness of the latent forgery feature \\(F\\) by maximizing the mutual information between \\(F\\) and the ground-truth label. Besides, we uncover the independent forgery feature \\(f_i\\) from different perspectives to ensure that any forgery trace hidden in the tampered image is not missed.\nMeanwhile, the minimality-view constraint is applied to the feature reasoning network \\(r\\) to guarantee that the concise forgery feature \\(Z\\) discards task-unrelated information while retaining task-related information. We obtain a formal representation of this constraint by deriving it from the theory of information bottleneck."}, {"title": "Sufficiency-view constraint", "content": "The sufficiency-view constraint \\(\\mathcal{L}_{su}\\) is constructed by maximizing the mutual information between \\(F\\) and the ground-truth label. In this section, we provide the key derivation of \\(\\mathcal{L}_{su}\\) and the detailed structure of the feature extraction network.\nGiven a corrupted image \\(X\\), we have carefully designed several feature extraction networks to extract individual forgery features \\(\\{f_i\\}_{i=1}^n\\) from different perspectives. Subsequently, we employ a learnable feature fusion layer \\(\\mathcal{B}\\) to blend and reason over multiple-view forgery features to obtain the latent forgery feature \\(F\\), i.e. \\(F = \\mathcal{B}(\\{f_i\\}_{i=1}^n)\\). Our sufficiency-view constraint objective attempts to ensure the important properties within the set \\(F = \\mathcal{B}(\\{f_i\\}_{i=1}^n)\\), i.e., comprehensiveness. Comprehensiveness mandates the inclusion of the maximal task-related information within \\(F\\).\nFor the comprehensive objective, we specify the relationship between the localization label \\(M\\) and the latent forgery feature \\(F\\) as follows:\n\\(I(M; F) = I(M; f_1,..., f_n)\\) \\((1)\\)\nwhere \\(I(*)\\) is the mutual information. \\(I(M; F)\\) represents the amount of predictive information (i.e. current task-related information) contained in \\(F\\). The comprehensiveness objective of information in \\(F\\) is given by:\n\\(\\max[I(M; F)]\\) \\((2)\\)\nThen we apply the mutual information chain rule to derive eq. (2):\n\\(\\max[I(M; F)] = \\max[I(M; f_1, ..., f_n)]\\)\n\\(=\\max(\\sum_{i=1}^n I(f_i; M | f_1, ..., f_{i-1}))\\)\n\\(\\leq \\max[\\sum_{i=1}^n I(f_i; M | \\mathcal{F} \\backslash f_i)]\\) \\((3)\\)\nwhere \\(\\mathcal{F} \\backslash f_i = \\mathcal{B}(f_1,..., f_{i-1}, f_{i+1},..., f_n)\\), \\(\\mathcal{B}\\) is a learnable feature fusion layer. For mutual information, expanding the known conditions causes the mutual information to increase or remain constant, so the inequality in Eq. (3) stands.\nNevertheless, directly estimating Eq. (3) is generally infeasible. (Poole et al. 2019) have highlighted significant challenges in mutual information estimation, chiefly attributed to the curse of dimensionality, where the number of samples required for an accurate estimate grows exponentially with the embedding dimension. To address this issue, we employ variational inference to optimize Eq. (3), bypassing the need for explicit mutual information estimation. We have the following derivation (detailed proof is in supplementary files):\n\\(\\sum_{i=1}^n I(M; f_i | \\mathcal{F} \\backslash f_i) \\geq \\sum_{i=1}^n D_{KL}[P_{\\mathcal{F}}||P_{\\mathcal{F} \\backslash f_i}]\\) \\((4)\\)\nwhere \\(P_{\\mathcal{F}} = p(y|\\mathcal{F}), P_{\\mathcal{F} \\backslash f_i} = p(y|\\mathcal{F}\\backslash f_i)\\) represent the predicted distributions. \\(D_{KL}\\) denotes the Kullback-Leibler(KL) divergence.\nGiven the above analytical derivations, we can thus denote the sufficiency-view constraint as:\n\\(\\mathcal{L}_{su} = \\min[exp(-D_{KL}[P_{\\mathcal{F}}||P_{\\mathcal{F} \\backslash f_i}])]\\) \\((5)\\)\nHere, since the KL-divergence is not bounded above, i.e. \\(D_{KL} \\in [0,\\infty)\\), we take the exponential of its negative value to transform the objective from maximization to minimization. The transformed objective is bounded within (0, 1] which is numerically advantageous. Next, the structure of the feature extraction network is elaborated to illustrate how forgery traces can be adequately extracted from different perspectives.\nThe structure of the feature extraction network comprises three backbones which are based on the U-Net (Ronneberger, Fischer, and Brox 2015). All of the backbones have 5 layers of U-Net architecture with 3 blocks at each scale. The three attention backbones are constructed by substituting the Conv layer in U-Net with three novel attention blocks respectively. These attention blocks are shown in Fig. 3. The channel attention block (CAB) (Fig. 3(a)) uses global average pooling to squeeze the input feature from C dimension to 1 dimension, then generates a channel attention map to guide the model focusing on the luminance information of tampered images. Then the spatial attention block (SAB) (Fig. 3(b)) operates the input feature by both the global average pooling and max average pooling to squeeze dimension to 2. Because of the pooling, the output features have non-local (global) information. In other words, the SAB mainly responds to changes in global information, i.e., structure, and color information. As for the pixel attention block (PAB) (fig. 3(c)), it directly generates an attention map without any pooling or sampling operations, which means PAB is focused on local information of tampered images. Overall, the feature extraction network extracts forgery features, \\(f_1\\), \\(f_2\\), and \\(f_3\\) from different individual aspects. The sufficiency-view constraint \\(\\mathcal{L}_{su}\\) captures forgery clues from multiple and diverse perspectives, thereby reducing the risk of missing or misjudging cases and improving localization performance.\nInstead of adopting concatenation or addition operations, we propose a learnable feature fusion layer \\(\\mathcal{B}\\) to integrate these individual features. \\(\\mathcal{B}\\) utilizes a learnable parameter \\(\\gamma\\) to optimize the feature fusion operation through back-propagation. As a result, the fused feature \\(F\\) from \\(\\mathcal{B}\\) can be represented as:\n\\(F = \\mathcal{B}(f_1, f_2, f_3) = \\frac{(1 - \\gamma_{\\phi})}{2}f_1 + \\gamma_{\\phi}f_2 + \\frac{(1 - \\gamma_{\\phi})}{2}f_3\\). \\((6)\\)"}, {"title": "Minimality-view constraint", "content": "The minimality-view constraint \\(\\mathcal{L}_{MI}\\) is derived from the theory of information bottleneck to ensure that the concise forgery feature effectively discards task-unrelated information while retaining task-related information. In this section, we provide the key derivation of minimality-view constraint \\(\\mathcal{L}_{MI}\\) and the detailed structure of the reasoning network.\nAfter the feature extraction network, the latent forgery feature \\(F\\) already contains sufficient forgery clues but also inevitably contains task-unrelated information. Thus, we pass the latent forgery feature \\(F\\) through the reasoning network to eliminate superfluous information and obtain a concise forgery representation \\(Z\\) with the guidance of the minimality-view constraint. The concept of information bottlenecks (Tishby, Pereira, and Bialek 2000) is attributed to distilling superfluous noises while retaining only useful information. The information bottleneck (IB) objective can be formulated as follows:\n\\(\\max[I(Z; M) - \\beta I(F; Z)]\\), \\((7)\\)\nwhere \\(I\\) denotes mutual information and \\(\\beta\\) controls the trade-off between the two terms. However, IB may not fully leverage the available label information, which can be crucial for improving inference performance. A study in (Fischer 2020) proposes a conditional entropy bottleneck (CEB), which enhances IB by introducing label priors in variational inference. CEB can be formulated as follows:\n\\(\\max[I(Z; M) - \\beta I(F; Z|M)]\\). \\((8)\\)\nA major challenge in making the CEB practical is to estimate the mutual information accurately. We adopt the practice of variation information bottle (Alemi et al. 2016), utilizing variational inference to construct the lower bound to estimate the mutual information. Then Eq. (8) can be rewritten as :\n\\(I(Z; M) - \\beta I(F; Z|M)\\)\n\\(\\geq \\mathbb{E}_{p(f,m)p(z/m)}[\\log q(m/z) - \\beta \\mathbb{E}_{p(z|f)}[\\log \\frac{p(z|f)}{q(z/m)}]]\\), \\((9)\\)\nwhere \\(p(z|f)\\) is feature distribution, \\(q(m|z)\\) and \\(q(z\\vert m)\\) are a variational approximation to the true distribution \\(p(m/z)\\), \\(p(z|m)\\), respectively. The detailed proof is in supplementary files. Then, the first term in Eq. (9) can be derived as:\n\\(\\mathbb{E}_{p(f,m)p(z|f)} [\\log q(m|z)]\\)\n\\(=\\mathbb{E}_{p(f)}\\mathbb{E}_{q(z|f)} [\\int p(m/z) \\log q(m/z) dm ]\\)\n\\(=\\mathbb{E}_{p(f)} [-L_{CE}(q(z|f),m)]\\). \\((10)\\)\nTherefore, we obtain the localization loss \\(L_{loc}\\),\n\\(L_{loc} = L_{CE}(q(z|f),m),\\) \\((11)\\)\nwhere \\(L_{CE}\\) is the cross-entropy loss.\nThe second term in Eq. (9) can be derived as:\n\\(\\mathbb{E}_{p(f,m)p(z|f)} [\\log \\frac{p(z|f)}{q(z/m)}]\\) \\((12)\\)\n\\(=\\mathbb{E}_{p(f)}\\mathbb{E}_{p(z|f)} [KL(p(z|f)||q(z|m))],\\)\nFinally, we arrive at the minimality-view constraint \\(\\mathcal{L}_{MI}\\):\n\\(\\mathcal{L}_{MI} = \\mathbb{E}_{p(f)}\\mathbb{E}_{p(z|f)} [KL(p(z|f)||q(z|m))].\\) \\((13)\\)\nIn order to model the distribution \\(q(z|m)\\) in the Eq. (13) of the model, we propose a mask-guided encoder-decoder structure in the feature reasoning network. As shown in Fig. 4, we first add noise to the ground truth mask. This step is because predicting the auxiliary mask from the discrete GT mask may be too simple for the encoder-decoder structure and not beneficial for training. Hinder by the box denoising training of DN-DETR (Li et al. 2022), we add the point noises to the GT mask to obtain robust models. We randomly select the points within the mask and invert the original value to represent the distinct region. In addition, we use a hyper-parameter \\(\\gamma\\) to denote the noise percentage of area, so the number of noise points is \\(\\gamma\\times HW\\). Given the noise mask, we further project it to the forgery feature space to obtain the distribution \\(q(z|m)\\) through a convolution encoder network. Therefore, the KL distance between the \\(p(z|f)\\) and \\(q(z|m)\\) in Eq. (13) can be easily measured by mapping the GT mask to the forgery feature space. Since \\(p(z|f)\\) can get the predicted mask by the predictor, \\(q(z\\vert m)\\) can also obtain the auxiliary mask \\(\\tilde{m}\\) by fed into the predictor. We can derive the auxiliary mask loss:\n\\(L_{aux} = L_{CE}(\\tilde{m}, m),\\) \\((14)\\)\nwhere \\(L_{CE}\\) is the cross-entropy loss."}, {"title": "Overall objective", "content": "The total loss function \\(L\\) include four parts: the localization loss \\(L_{loc}\\), the sufficiency-view constraint \\(L_{su}\\), the minimality-view constraint \\(L_{MI}\\), and the auxiliary mask loss \\(L_{aux}\\):\n\\(L = L_{loc} + \\lambda_1 \\times L_{SU} + \\lambda_2 \\times L_{MI} + \\lambda_3 \\times L_{aux},\\) \\((15)\\)\nwhere \\(\\lambda_1 = 0.1\\), \\(\\lambda_2 = 1\\), and \\(\\lambda_3 = 0.1\\)."}, {"title": "Experiments", "content": "Table 1 presents the training and test datasets used in our method. We first pre-train our model on the training portions of four public datasets: DEFACTO-12 (Mahfoudi et al. 2019)(real/tampered), SSRGFD (Yin et al. 2023)(real/tampered), CASIAv2 (Dong, Wang, and Tan 2013) (real/tampered), and Spliced COCO (Kwon et al. 2022b) created by CAT-Net (Kwon et al. 2022b) based on the COCO 2017 dataset (Lin et al. 2014). Then we test our model on the testing portions of the above datasets, except Spliced COCO. To further evaluate the generalization capability of SUMI-IFL, we also compare the localization performance on two other datasets: CIMD (Zhang, Li, and Chang 2024) (real/tampered) and NIST16 (Guan et al. 2019) (real/tampered). All forgery images are cropped into 256 \u00d7 256 patches. To evaluate the localization performance of the proposed SUMI-IFL, following the previous method (Rao et al. 2022), we adopt the F1 score and Area Under Curve (AUC) as the evaluation metric.\nThe proposed SUMI-IFL is implemented with PyTorch and all experiments are performed on the NVIDIA GTX GeForce A100 GPU platform. The whole model is trained with batch size 12 for 100 epochs with AdamW optimizer, and an initial learning rate of 5e-4 set by cosine annealing scheduler, weight decay as 0.005.\nWe compare SUMI-IFL with other state-of-the-art methods under three settings: 1) in-dataset comparisons: training on the compound forgery dataset and evaluating on the comprehensive test datasets. 2) cross-dataset comparisons: directly applying the pre-trained model on an unseen dataset to assess generalization. 3) robustness evaluation: applying JPEG compression and Gaussian blur to the test dataset to evaluate robustness. We evaluate the performance with the seven state-of-the-art methods: MMFusion (Triaridis and Mezaris 2024), EITL-Net (Guo, Zhu, and Cao 2024), HiFi-IFDL (Guo et al. 2023), WSCL (Zhai et al. 2023), IF-OSN (Wu et al. 2022), MVSS-Net (Dong et al. 2022), PSCC-Net (Liu et al. 2022)."}, {"title": "Ablation study", "content": "In this section, we study the effect of removing the sufficiency-view constraint \\(\\mathcal{L}_{su}\\), the minimality-view constraint \\(\\mathcal{L}_{MI}\\), and the auxiliary mask loss \\(L_{aux}\\). We train models on the compound datasets mentioned before and test them on the test portions of the DEFACTO-12 and SSRGFD datasets. The absence of either loss leads to a significant drop in model performance. Quantitatively, \\(\\mathcal{L}_{su}\\) and \\(\\mathcal{L}_{MI}\\) have a dominant contribution to our method, resulting in an F1 increase of 9.8% and 5.1% on DEFACTO-12 and SSRGFD, respectively. Without \\(L_{aux}\\), the F1 score drops 2.5% and 9.8% on DEFACTO and SSRGFD, respectively. This empirical evidence suggests that the incorporation of the proposed losses results in extracting more comprehensive and less task-unrelated forgery features, facilitating the subsequent localization performance."}, {"title": "Visualization results", "content": "As shown in Fig. 6, we provide predicted forgery masks of various methods. It can be observed that some methods incorrectly identify certain image objects as tampered regions, such as in the third row of the first column, where MMFusion mistakenly identifies the lower-right area of the image as tampered. The comparison of visualization results demonstrates that SUMI-IFL can not only locate the tampered regions more accurately but also produce clearer regions. This is attributed to the sufficiency-view constraint \\(\\mathcal{L}_{su}\\) and the minimality-view constraint \\(\\mathcal{L}_{MI}\\), which enable the model to obtain comprehensive task-related feature representations while effectively resisting the interference of task-unrelated features."}, {"title": "Conclusion", "content": "In this paper, we proposed a novel information-theoretic IFL framework, SUMI-IFL, that leverages sufficiency-view constraints and minimality-view constraints to constrain the representation of forgery features. In one respect, the sufficiency-view constraint is applied to the feature extraction network, guaranteeing the latent forgery features capture comprehensive task-related information. The feature extraction network consists of three attention backbones to uncover forgery clues from different perspectives. In another aspect, the minimality-view constraint is employed in the feature reasoning network, assuring the concise forgery feature to eliminate superfluous information thus helping the model to resist the interference of the redundancy feature. We provided a detailed derivation of these two constraints based on the theories of mutual information maximization and information-theoretic bottlenecks, respectively. The superior performance of SUMI-IFL is demonstrated by extensive experimental results obtained across several benchmark tests, demonstrating that the two critical constraints contribute to a more comprehensive and accurate feature representation."}, {"title": "Appendix", "content": "In this section, we will provide the formulation derivation of the sufficiency-view constraint \\(\\mathcal{L}_{su}\\) and the minimality-view constraint \\(\\mathcal{L}_{MI}\\) in the main text.\nThe sufficiency-view constraint aims to ensure the essential properties comprehensiveness in the individual-view forgery feature extracted from the feature extraction network. Fig. 7 illustrates the sufficiency-view constraint when n = 2. Comprehensiveness needs the forgery feature F to contain a sufficient amount of label-related information. To achieve this goal, we maximize the mutual information between M and F, formulated as \\(\\max I(M, F)\\). According to the derivation of the main text, the final sufficiency-view constrain is defined as:\n\\(\\max \\sum_{i=1}^n I(f_i; M|\\mathcal{F} \\backslash f_i)\\). \\((16)\\)\nFollowing the definition of conditional mutual information, given three random variables X, Y, and Z, the conditional mutual information can be expressed as follows:\n\\(I(X; Y | Z) = \\sum_{x,y,z}p(X, Y, Z) \\log\\frac{p(Z)p(X, Y, Z)}{p(X, Z)p(Y, Z)}\\) \\((17)\\)\nTherefore, the sufficiency-view constrain objective can be expressed as:\n\\(\\sum_{i=1}^n I(f_i; M | \\mathcal{F} \\backslash f_i)\\)\n\\(=\\sum_{i=1}^n \\sum_{M,\\mathcal{F_i}}p(M, \\mathcal{F_i}) \\log\\frac{p(\\mathcal{F}\\backslash f_i)p(M, \\mathcal{F_i})}{p(\\mathcal{F})p(M, \\mathcal{F} \\backslash f_i)}\\)\n\\(=\\sum_{i=1}^n \\sum_{M,\\mathcal{F_i}}p(M, \\mathcal{F_i}) \\log\\frac{p(\\mathcal{F} \\backslash f_i)}{p(\\mathcal{F})} \\\\+ \\sum_{M,\\mathcal{F_i}}p(M, \\mathcal{F_i}) \\log\\frac{p(M, \\mathcal{F_i})}{p(M, \\mathcal{F} \\backslash f_i)} \\)\nBased on Bayers' theorem, i.e., \\(p(X, Y) = p(X)p(Y|X)\\), the term Q1 can be expanded as follows:\n\\(Q_1 = \\sum_{M,\\mathcal{F_i}} p(M, \\mathcal{F_i}) \\log\\frac{p(\\mathcal{F} \\backslash f_i)}{p(\\mathcal{F})}\\)\n\\(=\\sum_{M,\\mathcal{F_i}} p(M, \\mathcal{F_i}) \\log\\frac{p(\\mathcal{F} \\backslash f_i)}{p(f_i | \\mathcal{F} \\backslash f_i)p(\\mathcal{F} \\backslash f_i)}\\)\n\\(=\\sum_{M,\\mathcal{F_i}} p(M, \\mathcal{F_i}) \\log\\frac{1}{p(f_i | \\mathcal{F} \\backslash f_i)}\\)\n\\(=\\sum_{M,\\mathcal{F_i}} p(M, \\mathcal{F_i}) \\log 1 - \\sum_{M,\\mathcal{F_i}} p(M, \\mathcal{F_i}) \\log p(f_i | \\mathcal{F} \\backslash f_i)\\)\n\\(=- \\sum_{M} p(M | \\mathcal{F_i}) p(\\mathcal{F_i}) \\log p(f_i | \\mathcal{F} \\backslash f_i)\\)\nAccording to the definition of conditional entropy, i.e.,H(X | Y) = - \u03a3x,yp(X, Y) log p(X, Y), Eq. (19) can be derived as :\n\\(Q_1 = \\sum_{M} p(M | \\mathcal{F_i})H(f_i | \\mathcal{F} \\backslash f_i) \\)\nReferring to the definition of KL-divergence, i.e., DKL = \u03a3x, P(X)log), then the term Q2 can be defined as:\n\\(Q_2 = D_{KL}[P(M, \\mathcal{F_i} || p(M, \\mathcal{F} \\backslash f_i)]\\)\nThus, the comprehensive objective can be derived as:\n\\(\\sum_{i=1}^n I(f_i; M | \\mathcal{F} \\backslash f_i)\\)\n\\(=\\sum_{M} p(M | \\mathcal{F_i})H(f_i | \\mathcal{F} \\backslash f_i) + D_{KL}[P(M, \\mathcal{F_i} || p(M, \\mathcal{F} \\backslash f_i)]\\)\nDue to the non-negativity of information entropy and probability, it follows that Q1 \u2265 0. As a result, we deduce the lower bound:\n\\(\\sum_{i=1}^n I(f_i; M | \\mathcal{F} \\backslash f_i) \u2265 D_{KL}[p(M, \\mathcal{F_i} || p(M, \\mathcal{F} \\backslash f_i)]\\)\nWith this equality, the Eq. (26) can be represented to a lower bound as:\n\\(Q_3\\)\n\\(= \\int p(m, z) \\log\\frac{p(m, z)}{p(m)p(z)} dm dz\\)\n\\(=\\int p(m, z) \\log\\frac{p(m/z)}{p(m)} dm dz\\)\n\\(=\\int p(m, z) \\log q(m|z) dm dz - \\int \\frac{p(m)}{\\log q(m)} dm \\)\n\\(=\\int p(m, z) \\log q(m|z) dm dz - \\int \\frac{H(m)}{\\log q(m)} \\)\n\\(>\\int p(m, z) \\log q(m|z) dm dz \\)\n\\(=\\mathbb{E}_{p(x)p(z|x)} [\\logq(m|z) dm]\\)\n\\(=\\mathbb{E}_{p(x)q(z|x)} [\\int p(m/z) \\log q(m/z) dm]\\)\n\\(=\\mathbb{E}_{p(x)} [-L_{CE}(q(z|x),m)], \\)\nwhere the Shannon entropy of target labels H(m) is a positive constant. Then the Q3 is defined as the localization loss Lloc According to the definition of variational mutual information, the Q4 can be defined as:\n\\(Q_4 = H(ZM) \u2013 H(Z|F)\\)\n\\(=\\mathbb{E}_{p(f)p(z|f)} \\log q(z/m) - log \\frac{p(z|f)}{q(z/m)}\\)\n\\(=\\mathbb{E}_{p(f)p(z|f)} [\\int p(z|f) log q(z/m) \u2013 KL(p(z|f)||q(z/m))]\\)\n\\(<\\mathbb{E}_{p(f)p(z|f)} [log \\frac{q(z/m)}{p(zf)}]\\)\nThen we map the GT mask M to the forgery feature space to model the distribution q(z|m). Thus, we arrive at the minimality-view constraint LMI:\n\\(\\mathcal{L}_{MI} = \\mathbb{E}_{p(f)p(z|f)} [KL(p(z|f)||q(z|m))].\\)"}]}