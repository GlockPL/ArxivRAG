{"title": "DIGESTION ALGORITHM IN HIERARCHICAL SYMBOLIC FORESTS: A FAST TEXT NORMALIZATION ALGORITHM AND SEMANTIC PARSING FRAMEWORK FOR SPECIFIC SCENARIOS AND LIGHTWEIGHT DEPLOYMENT", "authors": ["Kevin You"], "abstract": "Text Normalization and Semantic Parsing have numerous applications in natural language processing, such as natural language programming, paraphrasing, data augmentation, constructing expert systems, text matching, and more. Despite the prominent achievements of deep learning in Large Language Models (LLMs), the interpretability of neural network architectures is still poor, which affects their credibility and hence limits the deployments of risk-sensitive scenarios. In certain scenario-specific domains with scarce data, rapidly obtaining a large number of supervised learning labels is challenging, and the workload of manually labeling data would be enormous. Catastrophic forgetting in neural networks further leads to low data utilization rates. In situations where swift responses are vital, the density of the model makes local deployment difficult and the response time long, which is not conducive to local applications of these fields. Inspired by the multiplication rule, a principle of combinatorial mathematics, and human thinking patterns, a multilayer framework along with its algorithm, the Digestion Algorithm in Hierarchical Symbolic Forests (DAHSF), is proposed to address these above issues, combining text normalization and semantic parsing workflows. The Chinese Scripting Language \"Fire Bunny Intelligent Development Platform V2.0\" is an important test and application of the technology discussed in this paper. DAHSF can run locally in scenario-specific domains on little datasets, with model size and memory usage optimized by at least two orders of magnitude, thus improving the execution speed, and possessing a promising optimization outlook.", "sections": [{"title": "1 Introduction", "content": "The development of deep learning[1] has become increasingly prominent with the advancement of computing power and the accumulation of data. While deep learning has achieved remarkable success in natural language processing (NLP), particularly with large language models (LLMs) attempting inference, there are still unacceptable drawbacks in some aspects. According to a paper by Apple Company[2], LLMs do not truly reason; they just \"memorize\" answers and then perform pattern matching. A study published in Scientific Reports[3], which tested with the prompt like \"Steve hugged Molly and Donna was kissed by Molly. In this context, was Molly kissed?\" found that the accuracy of large models is random, with significant fluctuations in answers and a lack of strong, consistent responses. This uncertainty and instability have impeded the practical application of deep learning in risk-sensitive areas such as medicine and precision instrument manipulation. Despite the current practicality of AI models, they have not yet reached human-like language capabilities. While these models may outperform humans quantitatively in tests, their answers sometimes exhibit clear non-human errors in language comprehension qualitatively. These indications may suggest a lack of combination operator information for effective grammatical and semantic regulations. Credibility has always been a major issue in deep learning due to its black-box nature[4]; catastrophic forgetting is also one of the main culprits for low data utilization. Furthermore, deep learning models are dense and require the use of all parts of the models for text normalization every time input data are fed, making it difficult to solve problems satisfying rigid time restrictions, leading to concerns in local deployment and delayed responses. Additionally, quickly obtaining a large amount of data for certain Scenario-Specific domains can be challenging, leading to the demand for manual data labeling. Nevertheless, deep learning is a big data method, making manual data labeling a tremendously laborious chore. In lieu of tediously labeling a large amount of data manually, it is more effective to directly tackle these problems by setting rules, which increases universality and achieves more with less effort."}, {"title": "2 Research Subject", "content": "The research focuses on text normalization and semantic parsing, which have numerous uses in natural language processing. One of the most notable applications is their ability to bridge between advanced programming language and natural language[5] by matching synonymous sentences. Apart from this, there are various other wide-ranging employments. For example, by combining different language metrics (emotions, language styles, etc.), text normal-ization enables computers to produce statements in given styles or to modify wording. It can also be used for data augmentation to generate training sets, even when data is limited, by extending a single sentence to all similar sentences, thus enhancing data diversity. Text normalization is also a technology that boosts machine thinking[6] by assessing the extent, to which external conditions match existing knowledge or not, and deducing conclusions from existing knowledge bases with the assistance of reasoning engines like Production Systems[7]. Furthermore, this technique may aid in text matching[8], for instance in search engines and recommendation algorithms.\nThis article aims to construct a lightweight deployable text normalization and semantic parsing algorithm called the \"Digestion Algorithm in Hierarchical Symbolic Forests (DAHSF)\", a fast algorithm for connecting free sequences to highly standardized sequences. The algorithm takes a finite-length sequence as input (e.g., \"\u9f20\u6807\u5de6\u952e\u53cc\u51fb (19,16) \u968f\u540e\u7b49\u5f857\u79d2,\u590d\u5236\u5f53\u524d\u9009\u4e2d\u5185\u5bb9,\u7f29\u5c0f\u9875\u9762,\u5173\u95ed\u5f53\u524d\u9875\u9762,\u5e76\u6309\u4e0b\u56de\u8f66\u952e\") and outputs a finite-length regularized sequence (e.g., \"\u53cc\u51fb(19,16),\u7b497\u79d2,\u62f7,\u7f29\u5c0f,\u5173\u9875,\u56de\u8f66\"), which is an equivalent expression of the input but in a standardized and relatively deterministic form. The precursor project has simplified its usage as much as possible, making it intuitive and clear. Therefore, this article can present an algorithm for text normalization (i.e., a synonym sentence transformation algorithm under the same semantics) and semantic parsing (including tokenization) algorithms."}, {"title": "3 Problem Formulation", "content": "To prevent ambiguity, let us redefine some important concepts in this article and make necessary assumptions.\nThere are multiple dimensions to analyze the semantics of texts, such as actions, emotions, and the formality of expression. We will use the term \"semantic dimensions\" to represent them. For example, when connecting the advanced Chinese scripting language in the predecessor project \"Fire Bunny Intelligent Development Platform V1.0\" to natural language, controlling the semantic dimension \"action information\" is crucial for synonymous transformations. Focused on the speaker's requests and commands, if two text segments convey the same action information, regardless of other irrelevant details like the speaker's tones and emotions, they are said to be \"action-synonymous\". Likewise, we can define emotion-synonymous and more. Breaking down text semantics into multiple dimensions facilitates language transformations in specific styles.\nA sentence may be transformed into multiple predetermined sequence segments. If a sentence can only be transformed into one specific predetermined segment, it is defined as a \"simple sentence\". For example, in our precursor project, \"\u6700\u5927\u5316\" can only be transformed into \"\u6700\u5927\u5316\". Hence, \"\u6700\u5927\u5316\" is a simple sentence. Therefore, in terms of semantic equivalence, all compound sentences are equivalent to a graph with corresponding simple sentences as nodes and logical structures as edges. Synonymous simple sentences can be transformed into each other through 1) synonym replacement and 2) grammatical structure transformations maintaining conceptual identity and sentence structural parallelism.\nWe further abstract the elements of sequences. From the following context, \"Word\" and \"Sentence\" are defined as follows."}, {"title": "Definition 3.1: Word", "content": "A word is a sequence segment with a specific meaning that originates from a division of a sequence according to certain rules.\nAn n-length sequence $e_1e_2 \\cdots e_n$ is divided into $(e_1, \\cdots, e_{i_1}), \\cdots, (e_k,\\cdots, e_{i_k})$ according to predefined principles, where $i_k = n$. Then $(e_j,\\cdots, e_{i_z}), 1 \\leq j \\leq k$ is a word.\nTake a sequence of tuples as an example:\n(1, 2, 3) (4, 5, 6) (2, 3, 4) (5, 6, 7)\nIf we join the tuples together to form words according to the rule of increasing numbers between consecutive tuple intervals (in this case: 3<4 and 4<5), then both \"(1, 2, 3)(4, 5, 6)\" and \"(2,3,4)(5,6,7)\" are words of the above sequence."}, {"title": "Definition 3.2: Sentence", "content": "A sentence is a specific word sequence that adheres to certain rules.\nIf $S = (e_{j_1}, ..., e_{i_{j_1}}), ..., (e_{j_m} , ..., e_{i_{j_m} })$ obeying preset principles, then S is a sentence.\nFor instance, if we define that a sentence is exactly composed of 4 words, then\n(1, 2, 3) (4, 5, 6)\n(2, 3, 4)(5, 6, 7)\n(2, 3, 4) (5, 6, 7)\n(1,2,3) (4, 5, 6)\nis a sentence. We denote it as So. The sentences at the original level can be reconsidered as words when we elevate our horizons to a higher level. Upon ascending a level, the sentences form a sequence, such as the repetition of So five times:\n$C: S_0 S_0 S_0 S_0 S_0$\nNow, we can consider the original sentence of the original level So as a word at a new higher level.\nWhen discussing words and sentences, it is advisable to indicate the level they belong to, unless the context makes it explicitly clear."}, {"title": "Definition 3.3: Sentence Framework", "content": "A sentence framework is a fixed pattern with predetermined vacancies that, after filling these vacancies with specified types of words, a sentence is composed."}, {"title": "Definition 3.4: Data Word", "content": "A data word is a word whose patterns and contents may be diverse but can be classified by identifiers, though it can be hard to predict directly without given contexts."}, {"title": "Definition 3.5: Keyword", "content": "Keywords are the words but not vacancies that compose sentence frameworks.\nGiven the abundance of data words, it is often necessary to categorize them. To classify them, we shall label them and indicate their types (they are the subclasses of data words).\nIn Python codes, for instance, the following commands can be found:\nimport numpy as np\nFrom this, we can extract the sentence framework:\nimport [module name] as [alias]\nIn this text, \"import\" and \"as\" are keywords, while \"numpy\" and \"np\" represent the module name and the alias of the module, respectively. Both \"numpy\" and \"np\" are data words. Similarly, we can have:\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nBoth of their sentence frameworks are:"}, {"title": "4 Digestion Algorithm in Hierarchical Symbolic Forests", "content": "This article aims to perform text normalization, which outputs an equivalent form of the input, and semantic parsing. Under the transformation of the Digestion Algorithm in Hierarchical Symbolic Forests, each word of this output sequence either represents a keyword, a data word, or an unknown word beyond knowledge.\nSince our task is to convert versatile sequences into standardized ones and, inspired by the multiplication rule in combinatorial mathematics, we think that it requires a hierarchical architecture to solve this problem. Diverse sentence forms can be generated step by step through the combination of different levels. Similar to a neural network, our architecture can be represented as a graph with nodes and edges. However, the lack of interpretability in neural networks stems from the inexplicability of their nodes and edges.\nDrawing inspiration from human thinking patterns[9][10] and artificial neural networks[11], in each layer of our architecture, we assign meanings to all edges and nodes that are relevant to the input data of that layer. By linking an edge with a mapping and binding every object (or word) to each node, we aim to eliminate interpretability issues fundamentally.\nSynonym matching is a process of partitioning into equivalence classes at the word level, where multiple different words often belong to the same equivalence class. Motivated by this fact, this article introduces the architecture of Hierarchical Symbolic Forests (HSF). The process of \"digestion\" is the process in which free texts are firstly \"swallowed\" and then gradually transformed into increasingly digestible forms within different \"organs\". Through Hierarchical Symbolic Forests, we achieve synonym matching, and through the Digestion Algorithm (DA), we accomplish semantic parsing.\nTo match synonyms, it is often necessary to know the type of a word - either it is a keyword or a data word. Given a data word, which subclass it belongs to? For this purpose, we have innovated a tokenization method based on a lexicon. By combining Hierarchical Symbolic Forests with the Digestion Algorithm, we parse semantics layer by layer, capturing the structural information of language from lower to higher levels, and progressively transforming free text into more standardized sequences. This approach allows us to peel back the layers of hierarchical information in reality.\nHierarchical Symbolic Forests are layers of forests, as shown in Figure 2, where each node is endowed with a meaning, hence the name \"symbolic\". For instance, if the input sequence for the first layer is \"a1, c5\", then the first layer would output sequence \"ao, co\", where \"ao\" is equivalent to \"a1\" and \"co\" is equivalent to \"c5\", achieving the first tokenization step as well. Subsequently, the types of words are identified and information is injected into the output of the first layer: this is the process of digestion. The Digestion Algorithm represents the cumulative effect of this \"digestion\" at each layer. Words form sentences, and when viewed from a higher level, sentences are words of a new level. Therefore, this architecture can be chained multiple times, each handling sequences at different levels. This is the essence of Hierarchical Symbolic Forests."}, {"title": "5 Evaluations and Further Analyses", "content": "By establishing \"Fire Bunny Intelligent Development Platform V2.0\", we have deployed the DAHSF to a local laptop. To obtain a test set, we first simulate the correct answers and then use ChatGLM-4 to simulate user inputs in batches.\nDealing with long corpus, our local lightweight architecture outperforms both cloud and locally deployed large language models. Specifically:\n\u2022 Text normalization and semantic parsing are rapid to the point of being nearly imperceptible. Whereas, the runtime of locally deployed large language models is a disaster. Even in the cloud, large language models have a noticeable time lag.\n\u2022 There is almost no limit on the input length, which distinguishes our framework significantly from large language models."}, {"title": "5 Evaluations and Further Analyses", "content": "By establishing \"Fire Bunny Intelligent Development Platform V2.0\", we have deployed the DAHSF to a local laptop. To obtain a test set, we first simulate the correct answers and then use ChatGLM-4 to simulate user inputs in batches.\nDealing with long corpus, our local lightweight architecture outperforms both cloud and locally deployed large language models. Specifically:\n\u2022 Text normalization and semantic parsing are rapid to the point of being nearly imperceptible. Whereas, the runtime of locally deployed large language models is a disaster. Even in the cloud, large language models have a noticeable time lag.\n\u2022 There is almost no limit on the input length, which distinguishes our framework significantly from large language models."}, {"title": "6 Prospects", "content": "Based on the application purposes, we will focus on two main directions: natural language programming[13] and the application of production systems with a self-learning mechanism.\nWhen NLP empowers advanced programming languages like in the \"Fire Bunny Intelligent Development Platform V1.0\", programming paradigms can transcend from high-level language programming to the realm of natural language programming, using a single line of Chinese command can control mouse and keyboard operations, open specific applications with a simple command, or manipulate software interfaces and systems with ease. Such exploration is conducive to discovering more concise and powerful human-computer interaction methods, providing more references and support for building other human-computer interfaces and other artificial intelligence technologies. The natural language programming paradigm is poised to challenge existing programming patterns. On the one hand, with text normalization and semantic parsing pipelines, automating office tasks will become seamless - write down the workflows and execute them directly, significantly improving work efficiency. Natural language programming facilitates automated office tasks, enabling humans to control computers deeply, laying a crucial foundation for deep manipulation of the external world, and freeing humans from repetitive, tedious, and potentially life-threatening tasks. On the other hand, breaking the monopoly of English computer languages will lower the barriers to programming. For people unfamiliar with English, the ability to control computers deeply will be a piece of cake, and sharpening computer skills will no longer depend on proficiency in English. Particularly for younger students, natural language programming is an effective way to quickly cultivate programming, interdisciplinary, and AI thinking, with significant educational value. In the future, as AI impacts the world more directly (there would be more avenues of influence with the assistance of multimodal models[14][15]), compared to traditional chatbots that must influence human thinking before impacting the world, AI will serve as a new productive force to elevate living standards and stimulate a new round of economic growth.\nThe second purpose of the design in this article is to understand human thinking and decision-making patterns, to comprehend why computers think the way they do, to enhance the interpretability of computer thinking, to allow humans to understand computer decisions when it comes to teaching students computers, to identify areas where computers may make thinking mistakes, etc. By correcting computer-made errors, the credibility of computer decisions will be increased, achieving mutual enhancement and efficient collaboration between humans and computers. Knowledge-based decision-making systems hold the potential to revolutionize the learning paradigm of deep learning and thoroughly address the challenges of interpretability, the problem of catastrophic forgetting, and long local inference time issues in deep learning. In traditional neural networks, there is a lack of clear binary opposition between connections, which, to some extent, may contribute to the looseness of neural networks' inference. The interpretability issue of neural networks stems from the interpretability issues of individual neurons and edges (connections between neurons). If each neuron corresponds to a specific condition (encoding neurons as switches that determine whether a condition is met), such as \"[One object] is a particle\", and connects the neurons through logical rules (e.g., \"and\", \"or\", \"not\"), the black box problem of neural networks may be fundamentally addressed. In conjunction with inference engines and large models, lighting up a \"knowledge\" (i.e., satisfying condition of certain theorems in the theorem library) requires mapping natural language to formatted text \"conditions\". This process relies on text normalization, a technology that maps unstructured external observations to our knowledge bases. Knowledge-based decision-making systems (such as expert systems) essentially consist of two: condition matching and inference engines. The architecture outlined in this article helps to handle the condition-matching problem. Additionally, the lexicon can be the foundation for making decisions and act as a \"memory repository\". Thus, for machines, effective \"learning\" should involve adding new content to the lexicon and correcting potential errors."}, {"title": "7 Future Works", "content": "Every framework has imperfections: without exception, there is room to optimize this architecture as well. This framework currently lacks a \"learning mechanism\" to improve its performance, with the model's performance being entirely dependent on its structural design. So far, the knowledge base has not been able to update automatically, thus the model's performance. Therefore, in the upcoming \"Fire Bunny Intelligent Development Platform V3.0\", we will explore automated learning algorithms. For instance, we may utilize generative AI to create test cases and consider testing \"hypothesis\" to automatically edit the knowledge base, thereby designing algorithms to train models of this type. Furthermore, the errors made by the model in test cases exhibit significant commonalities. In the future, we will automate the process of model error analysis, knowledge base updates, and building logic frameworks to lay the foundation for the model's self-learning and self-optimization, while maintaining a high level of interpretability.\nCurrently, we have not considered situations where users input incorrectly. We aim to solve this issue in the third edition and design a more detailed classification and parsing framework. In the \"Fire Bunny Intelligent Development Platform V3.0\", we will establish a mechanism for autonomous learning. To enable themselves to truly \"extract knowledge\", machines should \"practice\" in various real-world scenarios, use generative techniques to generate \"assumptions\", test their validity, retain those that perform well in the dataset as knowledge, and question existing hypotheses that may be incorrect. The theorem base works similarly. So does the lexicon. Only through this approach can we create an AI brain that \"learns on its own\", capable of sharing knowledge with us."}]}