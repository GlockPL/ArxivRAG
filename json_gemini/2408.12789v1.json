{"title": "Context-Aware Temporal Embedding of Objects in Video Data", "authors": ["Ahnaf Farhan", "M. Shahriar Hossain"], "abstract": "In video analysis, understanding the temporal context is crucial for recognizing object interactions, event patterns, and contextual changes over time. The proposed model leverages adjacency and semantic similarities between objects from neighboring video frames to construct context-aware temporal object embeddings. Unlike traditional methods that rely solely on visual appearance, our temporal embedding model considers the contextual relationships between objects, creating a meaningful embedding space where temporally connected object's vectors are positioned in proximity. Empirical studies demonstrate that our context-aware temporal embeddings can be used in conjunction with conventional visual embeddings to enhance the effectiveness of downstream applications. Moreover, the embeddings can be used to narrate a video using a Large Language Model (LLM). This paper describes the intricate details of the proposed objective function to generate context-aware temporal object embeddings for video data and showcases the potential applications of the generated embeddings in video analysis and object classification tasks.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of technology and the widespread adoption of web and mobile applications have resulted in an exponential increase in data generation, particularly\nin the form of images and videos. This data, predominantly unstructured and unla-beled, presents significant challenges for extracting meaningful insights. The surge\nin unstructured video data necessitates innovative approaches for extracting useful\nfeatures and trends for video analysis.\nIn the field of image and video data analysis, significant progress has been made\nin computer vision, particularly in areas such as classification [1-3], recognition [4-\n6], object detection [4, 7], object tracking [8, 9], and segmentation [10, 11]. These\ndevelopments primarily depend on visual features and employ various Convolutional\nNeural Network (CNN) models [12-15]. While visual features are instrumental in\nmany computer vision tasks, they are often insufficient for fully understanding scenes\nand events. The addition of contextual features can greatly improve the detection\nof patterns and insights in visual data. Contextual features involve examining the\ninteraction between objects, providing essential insights for a deeper comprehension\nof the scene and a more accurate interpretation of the actions and events occurring\nwithin the scene.\nIn the domain of feature extraction from unstructured and unlabeled data,\nembedding models have become increasingly popular recently for their potential to\nextract and leverage both visual and contextual features effectively. Embeddings are\ncompressed vector-space representations of objects. Contextual embedding brings con-\ntextually similar or connected objects near to each other in a vector space. Contextual\nembedding has been popular with text [16-18] and imagery data [12, 19, 20]. The\nconcept of contextual object embedding in video data goes beyond traditional object\nembedding techniques in imagery data by incorporating the sequence information\n(temporal) of the frames. The frame-sequence information helps in encoding frame-\nlevel contexts, using the fact that visual objects that appear in nearby frames are more\ncontextually connected than objects that appear only in nonconsecutive frames.\nMost computer vision applications [12, 21] rely on visual similarity to generate fea-\nture vectors for visual objects. Visual similarity focuses on bringing objects that look\nsimilar in the vicinity. In contrast, contextual similarity brings visual objects - that\nappear in the same frame or nearby frames of a video - in proximity. As an example,\nFig. 1 shows that objects visually similar to a \"computer monitor\" are other computer\nmonitors or televisions. In contrast, visual objects contextually similar to a \"com-puter monitor\" are a \"keyboard\", a \"mouse\", and a \"stapler\". Contextual similarity\nfocuses on the spatial placement of visual objects in video frames. Additionally, in\nour work, the frame sequence plays an important role in constructing the context too.\nFor example, if the visual object \"stapler\" is seen within a few frames after the visual\nobject \"computer monitor\" is detected, then the visual object \"stapler\" will have some\ndegree of contextual similarity with the \"computer monitor\". Despite the significant\npotential, the learning of contextual features remains largely unexplored in the com-\nputer vision area. Some recent studies [22, 23] have begun to integrate visual features\nfrom neighboring frames to capture the context. However, these approaches primarily\nfocused on visual similarities, which do not correspond to contextual relevance.\nFurthermore, the context of objects within a video is dynamic and changes over\ntime. Accurately modeling this temporal shift in object context is crucial for under-\nstanding how associations between objects evolve. For example, as shown in Figure 2,\nthe transition of items associated with a computer monitor, from staplers to barcode\nscanners, indicates a shift from an office setting to a retail environment. Visual features\ncan not capture these kinds of contextual transformations alone. Thus, it is important\nto develop temporal object embeddings that can precisely depict the changing context\nof objects in a video to capture these evolving associations effectively.\nAdditionally, a significant research gap exists in the area of temporal embedding\nfor video data. Several recent research studies [2, 24] in this field concentrate on\nincorporating temporal context by predicting frame timestamps using frame features\nfor specific applications such as segmentation and action recognition. However, these\nmodels do not generate separate embedding for each timestamp. Rather, they utilize\ntemporal context to generate global embeddings of frames or other entities. This leaves\na void in developing a generic, dynamic model capable of tracking the evolving context\nof objects in videos.\nTemporal contextual embeddings of visual objects have the potential to aid in\nseveral computer vision applications, such as extracting insight from video and extract-\ning narrative of video. Furthermore, the rich contextual information contained in the"}, {"title": "2 Related Works", "content": "Temporal modeling of visual data, particularly in the context of video analysis, has\ngarnered substantial attention within the computer vision community. Over the years,\na diverse array of techniques and methodologies has emerged, aiming to capture the\ntemporal dynamics of objects and scenes. In this section, we provide an overview of\nthe key approaches and advancements of embedding models in text, image, and video\ndatasets that have paved the way for our proposed temporal visual object embedding\nmodel."}, {"title": "2.1 Text Embeddings", "content": "The use of context, which is implicitly derived from the data, has been proven to\nbe a powerful source of information for learning representations in natural language\nprocessing (NLP) [16, 17]. Contextual embedding maps each word of a large text\ncorpus to a low dimensional feature vector. The embedding model self-supervises by\nobserving each word and the words surrounding it in the original data to preserve the\ncontextual relationships between words in the generated vector space. A wide variety\nof applications in NLP, such as semantic word similarity [16], automatic summariza-\ntion [27], named entity recognition [28], sentiment analysis [29], and identifying future\ntrends [30] benefit from contextual embeddings. In this paper, we present a model that\ncreates contextual embeddings for visual objects from video data instead of text."}, {"title": "2.2 Image Embeddings", "content": "Embedding models are widely used in the imagery domain, encompassing tasks such\nas retrieval [31, 32], few-shot learning [33, 34], self-supervised learning [35, 36]. The\nmost popular application of embedding in image collections is the classification of\nimages [37]. A common approach involves training a CNN with labeled images to\ngenerate representations of image classes, such as dog, cat, chair, and table [12, 21].\nFrome et al. [20] developed a method to embed both images and corresponding labels\ninto a joint space. Frome's deep visual-semantic embedding model (DeViSE), was\ntrained to identify visual objects using both labeled image data and the semantic\ninformation from unannotated text. All these methods train a CNN with the images,\nassociated labels, and/or unannotated text around the images to generate the image\nclass embedding. While many applications focus on the visual features of the image and\nsemantics of labels, a wide variety of other applications require contextual embedding,\nwhich is overlooked in these methods. Although our approach is designed for video\ndata, our proposed model is also suitable for imagery data. In imagery data, our model\nconsiders the context of objects within each image. With video data, objects in the\nsurrounding frames are also included in the context.\nThe work of L\u00fcddecke et al. [19] included image-level context for each visual object,\nwhich closely relates to our work. However, L\u00fcddecke's approach is not suitable for an\nextension to video data where the context of an object may span multiple surrounding\nframes. In our work, the context of a reference object is a continuous function of\nthe positions of the other objects in the current frame (where the reference object is\nsituated) and the surrounding frames. Later in the experimental results section, we\ndemonstrate the strength of our proposed definition of context in creating embedding\nfor objects in video data and image data."}, {"title": "2.3 Video Embeddings", "content": "Various categories of embedding models have been explored for video data, includ-ing embeddings for entire video clips [38-41], video frames [22, 42, 43], and visual\nobjects [44]. Despite the differences in the entities for which these embeddings are\nlearned, the underlying goal remains consistent: to develop representations that effec-\ntively group similar entities while distinguishing dissimilar ones. In [41] features of"}, {"title": "2.3.1 Contextual Embeddings", "content": "Our proposed model is rooted in the concept of contextual embeddings. This approach\nresonates with contextualized word embeddings in natural language processing, where\nwords are embedded based on their contextual usage in sentences. Similar ideas have\nbeen explored in video analysis. Bertasius and Torresani [46] proposed a model that\nuses a pretrained language model to learn contextualized object embeddings by asso-\nciating video frames from instructional videos with text narrations. It employs a\ndetection model to predict object instances and corresponding embeddings and a con-\ntrastive loss function to align these embeddings with contextualized word embeddings.\nThis approach is limited only to the video with text multimodal data.\n[22] employed a skip-gram model on video frames to learn contextual embedding\nof frames. [23] propose a method similar to skip-gram, where their loss function\npromotes high cosine similarity between embeddings of adjacent frames in a video\nwhile ensuring low similarity with negative frames from different videos. In addition\nto the objective of bringing adjacent frames closer, [47] also aims to cluster visually\nsimilar frames by incorporating a graph structure regularization based on a similarity\nmatrix encompassing all video frames. All these approaches are based on the concept\nthat context frames around the target frame represent the context of the target frame\nbetter than other frames. However, they focus on a limited range of adjacent frames,\noverlooking the potential context from distant frames.\nOur work differs by introducing a context discrepancy score between objects in\ntarget and surrounding frames, assigning lower scores to nearby objects and higher\nto distant ones. This approach enables the capture of long-range contextual rela-\ntionships. Initially, our research concentrated on the static contextual embedding of\nobjects within video data, with some findings already published [48]. In this paper,\nwe extended our research towards the temporal embedding of objects in video data,\nexploring how these objects interact and how the context of objects changes over time."}, {"title": "2.3.2 Temporal Embeddings", "content": "In recent years, there has been a shift toward embedding-based approaches to cap-ture temporal dynamics. Embeddings offer a compact and expressive representation\nof objects that can encompass both visual appearance and temporal context. To cap-\nture this temporal context, some techniques employ frame features to estimate the\nframe's relative time. Kukleva et al. [2] utilize frame features to predict the time of\nframes, aiding in learning temporal embeddings. Vidal-Mata et al. [24] adopt a tempo-ral self-attention mechanism, combining a visual embedding from a predictive U-Net\narchitecture with a temporal embedding that predicts the timestamp of given frames.\nSome other transformer-based models focus on image patches or object bounding boxes\ninstead of whole frames. For instance, Wang et al. [49] introduce a transformer-based\nframework featuring dual encoders for image and video streams, employing decoders to\ngenerate spatio-temporal representations by predicting tokens for masked image and\nvideo patches. Zhang et al. [50] present a transformer-based method, where an encoder\ngenerates visual representations, while a trajectory encoder processes object bounding\nboxes. An Object Learner module merges these streams using cross-attention Trans-\nformers to generate spatio-temporal embedding of the object bounding box. Though\nthese approaches are powerful, they face challenges due to their high computational\ndemands and limited capacity in contextual data aggregation. Furthermore, these\nmodels are not focused on tracking contextual changes over time, as they do not cre-\nate embeddings for the same instance at different timestamps. Our model constructs\ntemporal contextual object embeddings that explicitly capture not only the context\nof objects but also the change of context over time."}, {"title": "2.3.3 Temporal Object Embedding", "content": "In video analysis, it is crucial to understand how object representations evolve over\ntime in a video. This requires establishing a dynamic embedding space that can gener-\nate embeddings for objects at individual time slices. Despite its importance, research\nin this domain is sparse. Some studies have concentrated on trajectory-based methods,\nutilizing object motion paths to decipher temporal embeddings, specifically for object\ntracking applications. Yan et al. [51] developed a transformer-based model with an\nencoder that handles spatio-temporal feature dependencies and a decoder dedicated to\npredicting spatial positions of target objects. Their approach is enriched by a dynami-\ncally updated template from intermediate frames, enhancing the temporal information\nwith changes in target appearance. On a similar note, Wan et al. [52] pioneered the\nuse of temporal priors embedding, utilizing long-term dynamics of tracked targets over\nvideo clips. This method employs logical reasoning to assess the activation status of a\ntarget, maintaining accurate tracking even under occlusion and identifying when tar-\ngets enter or leave the scene. While these methods significantly advanced in tracking\nobject representation over time, they fall short of capturing the context of objects.\nIn summary, the landscape of temporal modeling in video analysis has evolved sig-\nnificantly, with a growing recognition of the importance of contextual information and\ntemporal relationships. Our work builds upon these foundations by introducing a novel\napproach that leverages both adjacency and contextual similarities between objects to"}, {"title": "3 Problem Description", "content": "Let $F = \\{f_1, f_2,....f_{|F|}\\}\\$ be a video consisting of $|F|$ frames and $O = \\{o_1, o_2,..., o_{|O|}\\}\\$ be the set of $|O|$ visual objects extracted from the video $F$. As an example of visual objects $o_i$ can be any item such as a cup, a table, a refrigerator, or any artifact detected by preprocessing tools, such as YOLO9000 [25] and YOLOv4 [26], or any human-annotated visual objects. The set of visual objects in frame $f$ is $O_f \\subset O$. In this paper, the phrase visual object and the word object are used interchangeably. The objective of this paper is two-fold, as outlined below.\n1.  Static Embedding Generation for Visual Objects: Our initial approach is to develop a static embedding model. This model constructs an embedding set $E_{static} = \\{e_1,e_2,..., e_{|O|}\\}\\$ for each visual object in $O$ within video $F$. The output, $E_{static}$, is a $|O|\\times |e|$ matrix where $|e|$ is a user-defined integer parameter denoting the length of each embedding vector.\n2.  Temporal Embedding Generation for Visual Objects: Considering our objective of temporal embedding, we focus on video recordings captured over extended periods. Such recordings, like security camera videos marked with date and time, or dashcam clips with timestamps, are ubiquitous. We define $T = \\{t_1, t_2,....t_{|T|}\\}\\$ as the set encompassing $|T|$ timestamps. Each timestamp $t_j$ has $n_f = [|F|/|T|]$ frames. Here, we assume the timestamps are ordered chronologically. We note that the set of visual objects $O$ contains all visual objects. It is possible for certain $o_i \\in O$ to be absent in some timestamps $t_j \\in T$.\nGiven such a time-annotated video dataset, our goal is to formulate a temporal embedding model denoted as $E_{temporal}$. This model is represented as $E_{temporal} = \\{\\[e_1, e_2, ..., e_{|O|}\\]\\_1, \\[e_1, e_2, ..., e_{|O|}\\]\\_2, ..., \\[e_1, e_2, ..., e_i, ..., e_{|O|}\\]\\_j,..., \\[e_1, e_2, ..., e_{|O|}\\]_{|T|}\\}$ for all visual objects in $O$ and across all timestamps $T$ derived from video $F$. The output temporal embedding, $E_{temporal}$, is a $|O| \\times |T| \\times |e|$ matrix where $e$ is a user-settable integer parameter denoting the length of each embedding vector."}, {"title": "4 Methodology", "content": "Fig. 3 summarizes the pipeline of the static and temporal visual object embedding\nframeworks. Table 1 lists symbols used in this paper. The static embedding framework\n(Figure 3(a)) takes a video as input, extracts the frames, detects visual objects and\nthe location of the objects in frames, computes the contextual similarity between pairs\nof objects, and then trains a neural network to generate static embeddings of size\n|O| x |e|.\nTo construct a static embedding space, we design an objective function that con-\nsiders the spatial distance between each pair of objects in a reference frame and\nsurrounding frames, including a frame-level diffusion. The objective function minimizes"}, {"title": "4.1 Frame Extraction", "content": "Frames are initially extracted from the video at a predetermined frame-per-minute\nrate. For the static embedding approach, these frames are labeled solely based on their\nframe numbers, resulting in the sequence $F = \\{f_1, f_2,..., f_{|F|}\\}$, where $|F|$ is the total\nnumber of frames extracted from a video."}, {"title": "4.2 Detection and Selection of Context Objects", "content": "We use YOLO9000 [25] and YOLOv4 [26] to detect and label all the visual objects\nin every frame of a video, or we manually label the objects in reasonably-sized video\nfor our experiments. In addition, we keep track of (i) the location coordinate of the"}, {"title": "4.3 Training Data Preparation: Contextual Discrepancy Score between a Reference Object and a Context Object", "content": "We generate embeddings using a neural network setting, where in the input, we pro-\nvide a pair of objects - the reference object and a context object - and in the output,\nwe provide the contextual discrepancy between the two objects. The contextual dis-\ncrepancy between two objects reflects the contextual difference between the objects.\nThe higher the contextual discrepancy score, the more different the contexts of the"}, {"title": "4.3.1 Diffusion-based contextual discrepancy score for static embedding", "content": "We first assess the spatial distance between objects within frames to compute the\ncontextual discrepancy score. The spatial distance between any reference object $o_r$ and\na context object $o_c$ is calculated using the Euclidean distance", "distances": "i) Distance threshold ii)\nMin-Max Scaling and iii) Gaussian Decay. Using the distance threshold approach", "equation": "n$N\\_m(o\\_r, o\\_c) = \\frac{d(o\\_r, o\\_c)}{\\text{max}(d)}$\nwhere max(d) is the longest possible distance of two objects in a frame which the\nmaximum value for all pairs in $d(o_r, o_c)$.\nFor gaussian distribution we convert the distance $d(o_r, o_c)$ to a nonlinear score\nusing a Gaussian decay function.\n$G(o\\_r, o\\_c) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(d(o\\_r, o\\_c))^2}{2\\sigma^2}}$\nwhere $\\sigma$ is a user-settable parameter.\nTo bring the Gaussian distance into the range of 0 to 1, we first determine its maxi-\n$1\n$"}]}