{"title": "Context-Aware Temporal Embedding of Objects in Video Data", "authors": ["Ahnaf Farhan", "M. Shahriar Hossain"], "abstract": "In video analysis, understanding the temporal context is crucial for recognizing object interactions, event patterns, and contextual changes over time. The proposed model leverages adjacency and semantic similarities between objects from neighboring video frames to construct context-aware temporal object embeddings. Unlike traditional methods that rely solely on visual appearance, our temporal embedding model considers the contextual relationships between objects, creating a meaningful embedding space where temporally connected object's vectors are positioned in proximity. Empirical studies demonstrate that our context-aware temporal embeddings can be used in conjunction with conventional visual embeddings to enhance the effectiveness of downstream applications. Moreover, the embeddings can be used to narrate a video using a Large Language Model (LLM). This paper describes the intricate details of the proposed objective function to generate context-aware temporal object embeddings for video data and showcases the potential applications of the generated embeddings in video analysis and object classification tasks.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of technology and the widespread adoption of web and mobile applications have resulted in an exponential increase in data generation, particularly\n1"}, {"title": "2 Related Works", "content": "Temporal modeling of visual data, particularly in the context of video analysis, has garnered substantial attention within the computer vision community. Over the years, a diverse array of techniques and methodologies has emerged, aiming to capture the temporal dynamics of objects and scenes. In this section, we provide an overview of the key approaches and advancements of embedding models in text, image, and video datasets that have paved the way for our proposed temporal visual object embedding model.\n4"}, {"title": "2.1 Text Embeddings", "content": "The use of context, which is implicitly derived from the data, has been proven to be a powerful source of information for learning representations in natural language processing (NLP). Contextual embedding maps each word of a large text corpus to a low dimensional feature vector. The embedding model self-supervises by observing each word and the words surrounding it in the original data to preserve the contextual relationships between words in the generated vector space. A wide variety of applications in NLP, such as semantic word similarity, automatic summarization, named entity recognition, sentiment analysis, and identifying future trends benefit from contextual embeddings. In this paper, we present a model that creates contextual embeddings for visual objects from video data instead of text.\n5"}, {"title": "2.2 Image Embeddings", "content": "Embedding models are widely used in the imagery domain, encompassing tasks such as retrieval, few-shot learning, self-supervised learning. The most popular application of embedding in image collections is the classification of images. A common approach involves training a CNN with labeled images to generate representations of image classes, such as dog, cat, chair, and table. Frome et al. developed a method to embed both images and corresponding labels into a joint space. Frome's deep visual-semantic embedding model (DeViSE), was trained to identify visual objects using both labeled image data and the semantic information from unannotated text. All these methods train a CNN with the images, associated labels, and/or unannotated text around the images to generate the image class embedding. While many applications focus on the visual features of the image and semantics of labels, a wide variety of other applications require contextual embedding, which is overlooked in these methods. Although our approach is designed for video data, our proposed model is also suitable for imagery data. In imagery data, our model considers the context of objects within each image. With video data, objects in the surrounding frames are also included in the context.\nThe work of L\u00fcddecke et al. included image-level context for each visual object, which closely relates to our work. However, L\u00fcddecke's approach is not suitable for an extension to video data where the context of an object may span multiple surrounding frames. In our work, the context of a reference object is a continuous function of the positions of the other objects in the current frame (where the reference object is situated) and the surrounding frames. Later in the experimental results section, we demonstrate the strength of our proposed definition of context in creating embedding for objects in video data and image data.\n5"}, {"title": "2.3 Video Embeddings", "content": "Various categories of embedding models have been explored for video data, including embeddings for entire video clips, video frames, and visual objects. Despite the differences in the entities for which these embeddings are learned, the underlying goal remains consistent: to develop representations that effectively group similar entities while distinguishing dissimilar ones. In features of\n5"}, {"title": "2.3.1 Contextual Embeddings", "content": "Our proposed model is rooted in the concept of contextual embeddings. This approach resonates with contextualized word embeddings in natural language processing, where words are embedded based on their contextual usage in sentences. Similar ideas have been explored in video analysis. Bertasius and Torresani proposed a model that uses a pretrained language model to learn contextualized object embeddings by associating video frames from instructional videos with text narrations. It employs a detection model to predict object instances and corresponding embeddings and a contrastive loss function to align these embeddings with contextualized word embeddings. This approach is limited only to the video with text multimodal data.\nemployed a skip-gram model on video frames to learn contextual embedding of frames. [23] propose a method similar to skip-gram, where their loss function promotes high cosine similarity between embeddings of adjacent frames in a video while ensuring low similarity with negative frames from different videos. In addition to the objective of bringing adjacent frames closer, also aims to cluster visually similar frames by incorporating a graph structure regularization based on a similarity matrix encompassing all video frames. All these approaches are based on the concept that context frames around the target frame represent the context of the target frame better than other frames. However, they focus on a limited range of adjacent frames, overlooking the potential context from distant frames.\nOur work differs by introducing a context discrepancy score between objects in target and surrounding frames, assigning lower scores to nearby objects and higher to distant ones. This approach enables the capture of long-range contextual rela- tionships. Initially, our research concentrated on the static contextual embedding of objects within video data, with some findings already published [48]. In this paper, we extended our research towards the temporal embedding of objects in video data, exploring how these objects interact and how the context of objects changes over time.\n6"}, {"title": "2.3.2 Temporal Embeddings", "content": "In recent years, there has been a shift toward embedding-based approaches to cap- ture temporal dynamics. Embeddings offer a compact and expressive representation of objects that can encompass both visual appearance and temporal context. To cap- ture this temporal context, some techniques employ frame features to estimate the frame's relative time. Kukleva et al. utilize frame features to predict the time of frames, aiding in learning temporal embeddings. Vidal-Mata et al. adopt a tempo- ral self-attention mechanism, combining a visual embedding from a predictive U-Net architecture with a temporal embedding that predicts the timestamp of given frames. Some other transformer-based models focus on image patches or object bounding boxes instead of whole frames. For instance, Wang et al. introduce a transformer-based framework featuring dual encoders for image and video streams, employing decoders to generate spatio-temporal representations by predicting tokens for masked image and video patches. Zhang et al. present a transformer-based method, where an encoder generates visual representations, while a trajectory encoder processes object bounding boxes. An Object Learner module merges these streams using cross-attention Trans- formers to generate spatio-temporal embedding of the object bounding box. Though these approaches are powerful, they face challenges due to their high computational demands and limited capacity in contextual data aggregation. Furthermore, these models are not focused on tracking contextual changes over time, as they do not cre- ate embeddings for the same instance at different timestamps. Our model constructs temporal contextual object embeddings that explicitly capture not only the context of objects but also the change of context over time.\n7"}, {"title": "2.3.3 Temporal Object Embedding", "content": "In video analysis, it is crucial to understand how object representations evolve over time in a video. This requires establishing a dynamic embedding space that can gener- ate embeddings for objects at individual time slices. Despite its importance, research in this domain is sparse. Some studies have concentrated on trajectory-based methods, utilizing object motion paths to decipher temporal embeddings, specifically for object tracking applications. Yan et al. developed a transformer-based model with an encoder that handles spatio-temporal feature dependencies and a decoder dedicated to predicting spatial positions of target objects. Their approach is enriched by a dynami- cally updated template from intermediate frames, enhancing the temporal information with changes in target appearance. On a similar note, Wan et al. pioneered the use of temporal priors embedding, utilizing long-term dynamics of tracked targets over video clips. This method employs logical reasoning to assess the activation status of a target, maintaining accurate tracking even under occlusion and identifying when tar- gets enter or leave the scene. While these methods significantly advanced in tracking object representation over time, they fall short of capturing the context of objects.\nIn summary, the landscape of temporal modeling in video analysis has evolved sig- nificantly, with a growing recognition of the importance of contextual information and temporal relationships. Our work builds upon these foundations by introducing a novel approach that leverages both adjacency and contextual similarities between objects to\n7"}, {"title": "3 Problem Description", "content": "Let F = {f1, f2,....f|F|} be a video consisting of |F| frames and O = {01, 02,..., 0|0|} be the set of O| visual objects extracted from the video F. As an example of visual objects or can be any item such as a cup, a table, a refrigerator, or any artifact detected by preprocessing tools, such as YOLO9000 and YOLOv4 , or any human-annotated visual objects. The set of visual objects in frame f is Of C O. In this paper, the phrase visual object and the word object are used interchangeably.\nThe objective of this paper is two-fold, as outlined below.\n1. Static Embedding Generation for Visual Objects: Our initial approach is to develop a static embedding model. This model constructs an embedding set Estatic = {1,2,..., e|0|} for each visual object in O within video F. The output, Estatic, is a |O|\u00d7 |e| matrix where |e| is a user-defined integer parameter denoting the length of each embedding vector.\n2. Temporal Embedding Generation for Visual Objects: Considering our objective of temporal embedding, we focus on video recordings captured over extended periods. Such recordings, like security camera videos marked with date and time, or dashcam clips with timestamps, are ubiquitous. We define T =\n{t1, t2,....t|r|} as the set encompassing |T| timestamps. Each timestamp tj has nf = [|F|/|T|] frames. Here, we assume the timestamps are ordered chronolog- ically. We note that the set of visual objects O contains all visual objects. It is possible for certain or \u2208 O to be absent in some timestamps tj \u2208 T.\nGiven such a time-annotated video dataset, our goal is to formulate a temporal embedding model denoted as Etemporal. This model is represented as Etemporal =\n{[e1, 2, ..., e|0|]1, [e1, \u20ac2, ..., e|0|]2, ..., [e1, C2, ..., ei, ..., e|0|]j,..., [e1, 2, ..., e\\o\\]\\T\\} for all visual objects in O and across all timestamps T derived from video F. The output temporal embedding, Etemporal, is a |O|\u00d7 |T|\u00d7 |e| matrix where e is a user-settable integer parameter denoting the length of each embedding vector.\n8"}, {"title": "4 Methodology", "content": "Fig. 3 summarizes the pipeline of the static and temporal visual object embedding frameworks. Table 1 lists symbols used in this paper. The static embedding framework (Figure 3(a)) takes a video as input, extracts the frames, detects visual objects and the location of the objects in frames, computes the contextual similarity between pairs of objects, and then trains a neural network to generate static embeddings of size O x el.\nTo construct a static embedding space, we design an objective function that con- siders the spatial distance between each pair of objects in a reference frame and surrounding frames, including a frame-level diffusion. The objective function minimizes\n8"}, {"title": "4.1 Frame Extraction", "content": "Frames are initially extracted from the video at a predetermined frame-per-minute rate. For the static embedding approach, these frames are labeled solely based on their frame numbers, resulting in the sequence F = {f1, f2,..., f|F|}, where |F| is the total number of frames extracted from a video.\n9"}, {"title": "4.2 Detection and Selection of Context Objects", "content": "We use YOLO9000 and YOLOv4 to detect and label all the visual objects in every frame of a video, or we manually label the objects in reasonably-sized video for our experiments. In addition, we keep track of (i) the location coordinate of the\n10"}, {"title": "4.3 Training Data Preparation: Contextual Discrepancy Score between a Reference Object and a Context Object", "content": "We generate embeddings using a neural network setting, where in the input, we pro- vide a pair of objects - the reference object and a context object - and in the output, we provide the contextual discrepancy between the two objects. The contextual dis- crepancy between two objects reflects the contextual difference between the objects. The higher the contextual discrepancy score, the more different the contexts of the\n12"}, {"title": "4.3.1 Diffusion-based contextual discrepancy score for static embedding", "content": "We first assess the spatial distance between objects within frames to compute the contextual discrepancy score. The spatial distance between any reference object or and a context object oc is calculated using the Euclidean distance, d(or, 0c) = ||Or, 0c||. This measurement stems from the central coordinates of each object in their corresponding frames.\nd(or, Oc) = ||Or, Oc||\n13"}, {"title": "4.3.2 Diffusion-based contextual discrepancy score for temporal embedding", "content": "We applied the preliminary contextual discrepancy score in the previous section when selecting objects from the current and surrounding frames. For temporal embeddings, the context objects are also picked from neighboring timestamps, and we modify the preliminary score (or, oc) with a weight factor. The weight for each timestamp is derived using a Gaussian filter, as shown in (Eq. 5).\n\u03b3(\u03c4, \u03c3) = U\u2211i=1  \u221a(1/(2\u03c0\u03c32))*e^((-((i-t)^2)/(2\u03c32))\nThe Gaussian filter ensures that the weight of each timestamp is smoothly dis- tributed between different timestamps. o is a user-settable parameter representing the standard deviation of the Gaussian distribution.\nWe invert the Gaussian distribution value to ensure that object pairs from closer timestamps are given lesser weights than those from farther. After applying the Gaus- sian filter, the distance between a reference object or at time tr and a context object oj at time te is\n\u03b4(or, Oc, tr) = d(or, oc) \u00b7 (1 \u2013 y(tr, tc, 0))\nwhere,\n\u03b3(tr, tc, 0) = (1/\u221a2\u03c0\u03c32)  e^((-((tc-tr)^2)/(2\u03c32))\n\u03b3(tr, tc, \u03c3) retrieves the weight factor from Eq. 5 for reference timestamp try and context timestamp te for a given \u03c3.\n15"}, {"title": "4.4 Initial objective function", "content": "We store all the reference-context object pairs and the corresponding discrepancy score in a list Ds for static embeddings and Dt for temporal embeddings. For static embeddings, each row of Ds = U=1[Or, Oc, di(Or, Oc)] consists of a reference object, a context object, and the context discrepancy score. For temporal embedding, each row of Dt = U1 [Or, Oc, tr, di(Or, Oc, tr)] has an additional column tr that represents the timestamp of the reference-context object pair.\nOur primary objective is to derive a low-dimensional object embedding model E wherein the cosine distance between object vectors aligns closely with the discrepancy score. Equation 7 formulates the objective for static embedding as V1(static). Here, we optimize the vectors in Es to reduce the difference between the cosine distance of each vector of reference-context object pair and the corresponding discrepancy score within Ds:\nV1(static) (Es) = \u2211Ni=1 (dist(er, ec) \u2013 \u03b4i (or, Oc))^2\n15"}, {"title": "4.5 Incorporating temporal diffusion within the objective function", "content": "One of the goals of obtaining temporal embeddings of objects in videos is to learn how the contexts of objects change over time. An analysis involving contexts can lead to the ability to explain a situation in an ongoing scenario in a video. To capture the contexts of an object from all timestamps, we need to generate embeddings that evolve smoothly over time. To introduce this concept in our objective function, we model the effect of every reference-context weight in all timestamps to some degree.\nWe use a Gaussian filter (Eq. 5) to diffuse the contribution of each vector smoothly before and after the timestamp of the reference object. The filter considers its highest peak at the timestamp tr of the reference object or with a decay before and after tr. \u03c3is a user-settable parameter representing the standard deviation of the Gaussian distribution.\nEquation 9 presents the updated objective function, 02, which includes the tempo- ral weight of all the timestamps for or. It is important to emphasize that this adjusted objective function is exclusive to the temporal embedding model, given that static embeddings do not factor in time.\nLoss1 = (E11. Vn2 \u2013 Vn1. Vn2)\nEquation 9 does not consider the frequency of an object's appearance in a timestamp (a timestamp may contain multiple frames), resulting in no variation between frequent and non-frequent objects. The following subsection incorporates the frequency.\n16"}, {"title": "4.6 Influencing context via frequency of objects", "content": "A crucial aspect in determining contextual temporal similarity among objects is the analysis of their frequency of occurrence. When a reference object and its surround- ing entities consistently emerge within a given timestamp, their relational likelihood\n16"}, {"title": "4.7 Incorporating negative context", "content": "In order to increase the separation between embedding vectors of non-context objects in the embedding space, we introduced negative relationships between object pairs that lack proximity-based connections. These negative relationships involve objects that are not found in the reference frame (for static embedding on image data), surrounding frames (for static embedding on video data), or the reference timestamp (for temporal embedding on video data). Negative objects are randomly selected from the pool of objects appearing outside the context window. This approach draws inspiration from negative sampling techniques used in word2vec . However, we employ the cumulative frequency of objects at each timestamp to make weighted random selections from the objects located outside the context window. This enables us to choose the negative context for a reference object from those that occur more frequently outside the reference object's context window. The probability of selecting the ith object from a list is given by:\n|T|\nf(0i) = \u2211t=1 f (oi, t)\nP(i) =  f(oi) - f(oi,tr)/(\u2211(f(oj) \u2013 f(oj, tr)))\n20"}, {"title": "4.8 Embedding generation using a neural network model", "content": "We implemented a neural network-based model using Tensorflow to generate our tem- poral object embeddings. An overall view of the architecture of our neural network\n20"}, {"title": "4.9 Integration of Visual Features and Temporal Contextual Embedding", "content": "So far, in our objective functions, we have not considered the visual features of the objects. Visual features, along with contextual temporal embeddings, have immense potential in downstream applications (later demonstrated in the experimental results section). The current section describes the fusion of visual features and the temporal contextual embeddings produced by our neural network of Figure 6.\nWe used two methods to obtain the visual features of detected objects in video frames.\n21"}, {"title": "5 Datasets", "content": "Benchmark datasets for object embedding in video contexts are relatively undevel- oped, primarily because this is an emerging area of research. Commonly available benchmark datasets with object annotations are primarily intended for object recogni- tion or detection. In our research of generating static or temporal embeddings of video data, the neural network is trained to learn embedding vectors for detected objects in video data. To the best of our knowledge, no dataset has been specifically crafted to evaluate the learning mechanism of object embeddings. Hence, for this study, we cre- ated our own annotated video datasets. We utilized an existing object detection model to detect objects and label them. In addition, for one video, we manually annotated objects frame by frame.\nAs summarized in Table 2, our research utilized four distinct types of video data: (1) synthetic, (2) videos captured using our camera, (3) videos downloaded from YouTube, and (4) renowned annotated datasets such as COCO and LabelMe .\n23"}, {"title": "6 Experiments", "content": "In the problem description (Section 3), we outlined a two-step process for gener- ating temporal contextual object embeddings. Initially, we create contextual object embeddings, which capture the context of each detected object in a video. In this paper, we refer to this model as the static contextual object embedding model, as this model represents each object with a single vector. Following this, we incorporate a time dimension to develop temporal contextual object embeddings. This temporal model produces an embedding vector for each object at every timestamp. The tem- poral model is designed not only to capture the context of objects but also to capture changes in objects' context over time. In this experimental analysis section, we evalu- ate the effectiveness of both static and temporal models with steps involved in them. The experimental results are divided into two Subsections (Subsection 6.1 and 6.2), with one focusing on evaluating the static contextual embedding model and the other on assessing the temporal contextual embedding model.\n24"}, {"title": "6.1 Evaluation of the Static Embedding Model", "content": "This subsection seeks to answer the following questions relevant to static contextual embeddings.\n1. How well do the static embeddings capture the context when video frames are independent images? (Subsection 6.1.1)\n2. How well do static embeddings capture the context of objects when a video is considered a sequence of frames? (Subsection 6.1.2)\n3. How well do vectors from static embedding cluster visual objects in images compared to a state-of-the-art model? (Subsection 6.1.3)\n4. Case study: How well do the generated vectors of visual objects in video data agree with a language model? (Section 6.1.4)\n5. Case study: How can static embeddings be used to analyze the context of visual objects in a video? (Subsection 6.1.5)\n6. Case study: How can the mathematical space generated by static embeddings be used to analyze a video? (Subsection 6.1.6)\n24"}, {"title": "6.1.1 Embedding with reference-context pair in the reference frame", "content": "This experiment evaluates the effectiveness of our static embedding model in clustering objects that are concurrently present in the same video frame in close proximity. In this approach, each video frame is considered an independent image, with object contexts being derived exclusively from that specific frame, excluding context objects from surrounding frames (Context 1 in Section 4.2). To conduct this study, we generated a synthetic video containing 10,000 frames, each containing a grid of 5 \u00d7 5 = 25 squares (Fig. 8). Among these square grids, 12 grids contain images of letters or digits from three distinct sets \u2013 {A to Z}, {a to z}, and {0 to 9}. Notably, objects from the same set consistently appear close to one another in groups of four side-by-side grids in all the frames. We employ three of our reference-context scoring approaches (Equations 1, 2, and 4) to train the static embedding model 1. Each training iteration produces a set of embedding vectors for all objects. Given that each frame contains characters from three distinct sets, we utilize k-means clustering with k=3 to assess the efficacy of our embeddings in separating these three clusters.\nAcross all reference-context scoring methods, the clustering outcomes, utilizing various hyperparameters of the algorithms, demonstrate positive average silhouette coefficients (ASC). Positive ASC indicates good structure in the clusters in the embedding space. Fig. 9 demonstrates the average silhouette coefficients. Notice that the\n25"}, {"title": "6.1.2 Impact of surrounding frames for context selection", "content": "This experiment assesses the capacity of our static embedding model to cluster objects that co-occur across consecutive video frames. In this approach, the model consid- ers object contexts from both the reference and surrounding frames (Context 2 in Section 4.2). To study the impact of incorporating surrounding frames in context con- struction, we created a synthetic video that incorporated letter and digit images from the Chars74K dataset [55]. Each frame in the video had space to contain four letters or digits. A frame may contain no more than four consecutive letters from exactly one of the following sets: {0 to 9}, {A to Z}, and {a to z}. We created the consecutive frames in such a way that, four consecutive objects are picked sequentially from the three sets. That is, the content of the frames are: {0, 1, 2, 3}, {4, 5, 6, 7}, {8, 9}, {A, B, C, D}, {E, F, G, H}, so and so forth. Several frames of the video are shown in Fig. 12.\nIf we include surrounding frames of each reference frame in the context of a visual object, the embedding space should be able to capture the numbers and the letters in a sequence. That is, objects in frame {8, 9}, should have A, B, C, and D as context because {A, B, C, D} appears as the next frame of {8, 9} (Fig. 12).\nWe repeated the sequence of frames for a rich training video, which resulted in a total of 10,000 frames. That is, after the frame {y, z} of the sequence, {0, 1, 2, 3,} reappeared in the video. Therefore, the static embedding model should keep the vectors of y and z close to the vectors of 0, 1, 2, and 3 in the embedding space.\n28"}, {"title": "6.1.3 Embedding with reference-context pair in static image dataset", "content": "Due to the scarcity of labeled video data to test our model, we include COCO image dataset and compared our results with an image embedding method proposed by L\u00fcddecke et al . We used single-frame context window (Context 1 in Section 4.2), and negative sampling (as described in Section 4.7), to make our model suitable for image data.\nFor the evaluation, we used two mechanisms as proposed by L\u00fcddecke et al . The evaluation is driven by two metrics: (1) clustering consistency, and (2) system- to-human correlation. Both the techniques are defined below.\nClustering consistency: Clustering consistency compares clusters of objects obtained from an embedding with clusters defined by super-categories of objects in the labeled data. In the COCO dataset, 80 object classes are grouped into eleven super-categories, such as animal, vehicle, and kitchen . In cluster consistency, we measure the proportion of K-nearest neighbors of the embedding vector in the same super-category as the object's super-category.\nSystem-to-human correlation: This evaluation computes as score using two rankings - (a) based on vectors and (b) based on human-annotated ranks - by esti- mating the Spearman rank correlation. We used Scene250 annotation as the benchmark human-annotation of ranking.\nWe compared results from our static embedding model with the context-based models of Luddecke et al : co-count-noself context (CCn), co-count-self con- text (CCs), co-occurrence-noself context (COn), co-occurrence-self context (COs), and word2vec(Skip-gram neg25 COn). For a fair comparison, we only included contex- tual aspects of the methods presented in Luddecke et al in this experiment. We included three of our contextual discrepancy scores (Distance threshold, Min-Max scaling, and Gaussian decay).\nFig. 14 shows that our models driven by static embedding outperform almost all of the contextual models proposed by Luddecke et al [19]. In each plot of Fig. 14, the three right-most bars represent static embedding models. The cluster consistency and system human correlation (similarity) scores of static embedding models are compet- itive to other models. In the case of relatedness, static embedding models results in much higher scores. Our static embedding framework uses visual resemblance only for detection of objects, not for context generation. It is natural that contextually con- nected visual objects - such as a monitor and a keyboard - might not have any visual resemblance at all. The static embedding exhibits higher relatedness scores because it prioritizes contextual similarity of objects over the visual resemblance.\nThrough this comparison of cluster consistency, similarity, and relatedness, we observe that, even though our static embedding model is designed for video data, it\n30"}, {"title": "6.1.4 Sense-making using with a language model: Case study 1", "content": "In subsection 6.1.3, we compared our static embedding model in terms of objects similarity and relatedness to an image embedding model where the ground truth of similarity was taken from the COCO dataset and ground truth of object relatedness was taken from human-annotated ranks. In this subsection, we attempt to make sense\n31"}, {"title": "6.1.5 Sense-making using contextual vectors: Case study 2", "content": "In this study, we explore the utility of static embeddings in analyzing the context of visual objects within video content. For this study, we recorded a video beginning in a research lab, progressing through a corridor, visiting a restroom, using an elevator, passing by a Starbucks, and finally, moving the camera to the outside of the building. Fig. 15 shows some of the frames of the video.\nIn this experiment, we select vectors of context objects in the embedding space given a reference object. We select the context vectors based on several nearest neigh- bors (using cosine similarity between reference-context pairs) of the reference object. We used two approaches for the context selection, one selects contexts from the same frame and the other includes surrounding frames. The reference-context strength scor- ing in both approaches used the Gaussian decay-based formula of Eq. 4 for the static embedding model. As an example of reference and context, in this subsection, we used a \"marker\", which has a \"dry_eraser\" and a \"whiteboard\" as the context present in the same frame, as shown in Fig. 16.\nTable 5 demonstrates that both the \"dry_eraser\" and the \"whiteboard\" were detected as two neighbor objects of \"marker\" using both context selection approaches. The single-frame-only approach failed to detect any other correct object that was detected in the surrounding frames. It captured \"fire_alarm\", \"push_button\", and \"mug\" as neighbor objects that were, in reality, random because the embedding vectors from the third nearest neighbors were drastically different from the first two neighbors in the embedding space. The approach that included both the reference frame and the consecutive frames was able to capture several other objects that were in the lab around the marker such as \"table\", \"shelf\", and \"telephone\". The case study reveals that the embedding of the objects using contexts from surrounding frames encodes a story of how the events relevant to the objects proceeded. We analyzed the nearest\n33"}, {"title": "6.1.6 Sense-making using an embedding space: Case study 3", "content": "In this section, we focus on the entire holistic view of the embedding space created by the static embedding model. Here, we compute the reference-context discrep- ancy scores by employing the Gaussian decay normalization function, denoted by Equation 4. We compared the context selection using the reference-frame-only approach with the approach that includes objects from the surrounding frames for context selection.\nWe used the same video of the previous subsection (Figure 15) and constructed T-SNE plots of all the vectors using two approaches context selection using the reference frame only and context selection using the reference and surrounding frames. Fig. 17a shows that the objects from similar scenes are clustered in groups. We drew arrows manually to indicate the movements of the camera from one set of similar scenes to another set. Since the context selection using the reference frame-only approach\n34"}, {"title": "6.2 Temporal Embedding Model", "content": "The following sub-sections discuss the experimental evaluation of temporal embed- dings, where our primary aim is to address the subsequent questions.\n1. How do different objective functions perform in predicting the neighboring objects of a target object within frames? 6.2.1\n2. Can our model effectively detect contextual relationships that extend across long distances? 6.2.1\n3. Does our model successfully track the evolution of objects' context in video? 6.2.2\n4. How can we utilize temporal contextual object embedding to construct a narrative for a video? 6.2.3\n5. Does our model enhance performance in applications such as classification when integrated with visual features? 6.2.4\n37"}, {"title": "6.2.1 Selection of optimal objective functions:", "content": "We incorporated various features to formulate the objective functions of our temporal embedding model, as detailed in the methodology section (Section 4). The functions include diverse adaptations of spatial distance between objects and the diffusion of object frequency. In this subsection, we evaluate the effectiveness of the objective functions through both quantitative and qualitative analysis.\nQuantitative Evaluation: In this quantitative evaluation, we assess whether our temporal object embeddings can accurately identify the actual nearest neighbors of objects. We have employed the hit@k metric to evaluate the effectiveness of each objective function in identifying potential neighbors of objects within a timestamp.\nThis metric calculates the average intersection of k nearest neighbors between the actual neighbors and neighbors identified by the temporal embeddings of randomly chosen objects in random timestamps. We determined these actual neighbors of an object in a timestamp based on the average Euclidean distance of the surrounding objects within the frames in that timestamp. This group of neighbors is denoted as the base neighbors.\nWe randomly choose video-frames for training and testing sets. We generate tem- poral embedding vectors using different objective functions trained on the object pairs in the training set frames. Subsequently, these generated temporal embedding vec- tors are utilized to identify the nearest neighbors of each object in the timestamps of\n37"}, {"title": "6.2.2 Change in object context over time", "content": "In this experiment, we analyzed how our temporal embeddings capture the change in object context over time. An object can be seen or detected in different contexts\n41"}, {"title": "6.2.3 Narrating the Formed Context using ChatGPT", "content": "In this section, we explored the potential of temporal embeddings to encapsulate the narrative within a video. For this study, we calculated the cosine similarity between the embedding vectors of all object pairs at each timestamp. Subsequently, these similarity scores were sorted, and the pairs with the highest similarity for each timestamp were\n43"}, {"title": "6.2.4 Contextual classification", "content": "The objective of this experiment is to explore how well temporal contextual embed- ding vectors can perform in a contextual classification problem for objects. We seek to answer the question, \""}]}