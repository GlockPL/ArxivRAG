{"title": "Large Language Models for Human-like Autonomous Driving: A Survey", "authors": ["Yun Li", "Kai Katsumata", "Ehsan Javanmardi", "Manabu Tsukada"], "abstract": "Large Language Models (LLMs), AI models trained on massive text corpora with remarkable language understanding and generation capabilities, are transforming the field of Autonomous Driving (AD). As AD systems evolve from rule-based and optimization-based methods to learning-based techniques like deep reinforcement learning, they are now poised to embrace a third and more advanced category: knowledge-based AD empowered by LLMs. This shift promises to bring AD closer to human-like AD. However, integrating LLMs into AD systems poses challenges in real-time inference, safety assurance, and deployment costs. This survey provides a comprehensive and critical review of recent progress in leveraging LLMs for AD, focusing on their applications in modular AD pipelines and end-to-end AD systems. We highlight key advancements, identify pressing challenges, and propose promising research directions to bridge the gap between LLMs and AD, thereby facilitating the development of more human-like AD systems. The survey first introduces LLMs' key features and common training schemes, then delves into their applications in modular AD pipelines and end-to-end AD, respectively, followed by discussions on open challenges and future directions. Through this in-depth analysis, we aim to provide insights and inspiration for researchers and practitioners working at the intersection of AI and autonomous vehicles, ultimately contributing to safer, smarter, and more human-centric AD technologies.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous Driving (AD) has emerged as a transformative technology with the potential to revolutionize intelligent transportation systems, improve road safety, and enhance mobility. At the core of AD lies the decision-making process, which involves analyzing data, understanding the environment, and making informed decisions about navigation and safety. As depicted in Fig. 1, the development of AD systems can be categorized into three categories based on the techniques employed.\nThe first category of AD primarily relies on deterministic methods, such as rule-based and optimization-based approaches [1]. While these methods provide reliability and interpretability, they often lack the ability to generalize to complex and novel scenarios [2]\u2013[4]. The second category of AD, driven by the rapid advancements in deep learning [5], leverages techniques such as deep reinforcement learning (DRL) [6] in AD systems. Although these learning-based approaches have demonstrated success in handling complex scenarios, they still face challenges in the long-tail scenarios, and as they learn the patterns in the limited training set, they lack generalization in environment out of the training set [4], [7], [8].\nOne of the key challenges in AD is addressing the long-tail problem, which refers to the ability to handle rare but complex scenarios in real-world traffic [9]. Despite efforts to model AD environments using simulators [4], [10], [11], the complexity of real-world scenarios, encompassing factors such as traffic regulations, weather conditions, pedestrian behaviors, and diverse emergencies, poses significant difficulties for existing approaches. Furthermore, the lack of interpretability in AD systems hinders human trust and poses obstacles to their widespread adoption [12], [13]. Fig. 2 intuitively shows the limitations of traditional AD methods, further highlighting the necessity of introducing LLMs.\nRecently, the advancement of Large Language Models (LLMs) [14], [15] has ushered in a revolutionary change in AD. Making human-like decisions is crucial for autonomous vehicles as they need to operate among humans in a safe and socially-compliant manner. With their robust language understanding and reasoning capabilities, these models have introduced a new era or third category for autonomous vehicles. On one hand, LLMs can serve as knowledge bases to provide rich commonsense knowledge for AD, compensating for the shortcomings of pure data-driven methods. On the other hand, LLMs are adept at processing natural language instructions, enabling vehicles to understand human intentions and exhibit more human-like driving behaviors. Moreover, LLMs have demonstrated excellent few-shot learning and transfer learning abilities [16], [17], which are expected to solve the problems of few corner cases and many long-tail issues in AD. LLMs, represented by GPT-4, have made initial breakthroughs in key tasks such as road risk perception, decision planning, and human-machine interaction, greatly improving the performance of AD. Although there are still many challenges in introducing powerful language models into the complex physical world, LLMs are expected to become a key enabling technology for future AD. AD research sometimes treats LLMs as \u201cblack boxes\", neglecting their scientific foundations and general applicability, while AI-oriented research may rely on AD datasets without considering the importance of simulators or real-world testing. This paper aims to bridge these gaps by providing a comprehensive overview of the application of LLMs.\nThe main contributions of this paper include: 1) systematically reviewing the latest progress of LLMs in the field of AD, and proposing a taxonomy analysis framework covering perception, decision-making, planning, and control; 2) focusing on the two paradigms of modularized decision-\""}, {"title": "II. LARGE LANGUAGE MODELS AND MULTIMODAL LARGE LANGUAGE MODELS", "content": "LLMs and MLLMs have revolutionized the field of natural language processing and beyond, enabling machines to understand, generate, and reason with natural language at an unprecedented level. LLMs are trained on vast amounts of text data, allowing them to capture intricate linguistic patterns, contextual dependencies, and semantic relationships. The input to an LLM is typically represented as a set of tokens \\(X = {x_{i,j} | i \\in 1,..., N, j \\in 1,...,T_i}\\), where \\(i\\) denotes the i-th sentence, \\(j\\) denotes the j-th token within the sentence. The output of the LLM, denoted as \\(y_{i,j}\\), is usually a probability distribution over the next token. Thus, an LLM can be represented as a function \\(f\\) that maps the input to the output:\n\\[y_{i,j} = f(x_{i,j}).\\]\nBuilding upon the foundation of LLMs, MLLMs have been proposed to handle multimodal inputs, such as images, videos, text, audio, point clouds, and even depth or thermal information [18]. Mathematically, an MLLM can be formulated as a function \\(f'\\) that maps a set of multimodal inputs \\((x^{(1)}_{i,j}, x^{(2)}_{i,j},..., x^{(k)}_{i,j})\\) to an output \\(y_{i,j}\\):\n\\[y_{i,j} = f'(x^{(1)}_{i,j}, x^{(2)}_{i,j},..., x^{(k)}_{i,j}),\\]\nwhere \\(x^{(k)} \\in X^{(k)}\\) represents the input from modality k (e.g., image I, text T, point cloud P) for the (i, j)-th example, and \\(y_{i,j}\\) is the model's output (e.g., a control command for AVs) for the same example.\nThe key components of an MLLM include modality encoders \\(E^{(k)}\\), a multimodal fusion module F, a language model \\(f\\), and an output projector P. Formally, the MLLM's output can be expressed as:\n\\[V_{i,j} = P {f [F (E^{(1)}(x^{(1)}_{i,j}), E^{(2)} (x^{(2)}_{i,j}),..., E^{(n)} (x^{(n)}_{i,j})))]}.\\]\nSeveral influential MLLMs, such as BLIP-2 [19], LLaVA [20], and Flamingo [21], have laid the foundation for the development of this field. These models have demonstrated impressive performance in tasks like Visual Question Answering (VQA), image captioning, and multimodal reasoning. By integrating visual, textual, and potentially other sensory inputs, MLLMs can provide richer contextual information for decision-making processes in complex driving scenarios."}, {"title": "A. Pre-training and Fine-tuning of LLMs and MLLMs", "content": "1) Pre-training: Pre-training plays a pivotal role in the development of LLMs and MLLMs by enabling them to acquire a broad and nuanced understanding of natural language and multimodal data. The mathematical objective of pre-training is described as follows [22]:\n\\[\\Phi^* = arg \\max_{\\Phi} \\sum_{i=1}^{N_u} \\sum_{j=1}^{T_i} log P (Y_{i,j} | Y_{i,1:j-1}; \\Phi),\\]\nwhere \\(\\Phi^*\\) denotes the optimal parameters after pre-training, \\(N_u\\) represents the number of training sequences, and \\(T_i\\) is the length of the i-th target sequence. The indices \\(i\\) and \\(j\\) represent the position of the current token in the training data, with \\(i\\) iterating over the training sequences and \\(j\\) iterating over the tokens within each sequence. The context window size c determines the number of previous tokens considered as context when predicting the next token, influencing the model's ability to capture long-range dependencies.\n2) Fine-tuning: Fine-tuning is the process of further training a pre-trained LLM or MLLM on specific tasks or domains to enhance its performance. The main fine-tuning techniques include Parameter-Efficient Fine-Tuning (PEFT), prompt tuning, instruction tuning, and Reinforcement Learning from Human Feedback (RLHF).\nPEFT methods, such as Low-Rank Adaptation (LoRA) [23], aim to adapt pre-trained models to specific tasks while minimizing the number of additional parameters and computational resources required. LoRA freezes the pre-trained model weights and injects trainable rank decomposition matrices \\(A_i \\in R^{d\\times r}\\) and \\(B_i \\in R^{r\\times d}\\) into each layer k of the Transformer architecture, where \\(r < d\\). The adapted weights \\(W \\in R^{d\\times d}\\) are computed as:\n\\[W = W_k + A_kB_k,\\]\nwhere \\(W_k \\in R^{d\\times d}\\) are the frozen pre-trained weights."}, {"title": "B. In-context Learning and Theoretical Analysis", "content": "Prompt tuning employs learnable continuous vectors called \"soft prompts\" [24] to increase the performance of LLMs in downstream tasks. The objective of prompt tuning is to minimize the following loss function:\n\\[V^*_{1:t} = arg \\min_{V_{1:t}} [-\\sum_{i=1}^{N_u} \\sum_{j=1}^{T_i} log P(y_{i,j} | V_{1:t}, x_{i,j}) + \\lambda ||V_{1:t}||_1 ]\\]\nwhere \\(P(y_{i,j} | V_{1:t}, x_{i,j})\\) is the probability of generating output \\(y_{i,j}\\) given input \\(x_{i,j}\\) and soft prompt \\(v_{1:t}\\), \\(N^u\\) refers to the number of fine-tuning sequences, and \\(\\lambda ||V_{1:t}||_1\\) is an L1 regularization term to encourage sparsity in the learned prompts.\nInstruction tuning aligns LLMs with human intents by fine-tuning them on a collection of datasets phrased as instructions [16]. The objective function of instruction tuning can be expressed as:\n\\[\\Phi^* = arg \\max_{\\Phi} \\sum_{i=1}^{N^u} \\sum_{j=1}^{T_i} log P(y_{i,j} | x_{i,j}, z_{i,j}; \\Phi),\\]\nwhere \\(P(y_{i,j} | x_{i,j}, z_{i,j}; \\Phi)\\) represents the probability of generating the correct output \\(y_{i,j}\\) given the input \\(x_{i,j}\\) and instruction \\(z_{i,j}\\), parameterized by \\(\\Phi\\). RLHF uses human feedback as a reward signal to fine-tune LLMs via RL, generating safer and more aligned outputs [25]. The Boltzmann distribution plays a fundamental role in modeling human choices in the RLHF framework [26]:\n\\[P(l | M(q)) = \\frac{exp(\\beta \\cdot U(l))}{\\sum_{l'\\epsilon M(q)} exp(\\beta \\cdot U(l'))},\\]\nwhere \\(\\beta \\in [0,\\infty)\\) is a rationality coefficient reflecting the labeler's precision, and \\(M(q_{i,j})\\) denotes the set of available options for query \\(q_{i,j}\\).\nFine-tuning techniques like PEFT and RLHF are particularly beneficial for AD applications. PEFT methods allow for efficient adaptation of large models to specific driving tasks, such as pedestrian behavior prediction or traffic sign recognition, without the need for complete model retraining. RLHF, on the other hand, can be used to align AD systems with human driving preferences and safety standards, potentially leading to more natural and acceptable driving behaviors.\nIn-context learning is a surprising phenomenon that has emerged in LLMs, where the model can be specialized to perform a specific task simply by providing a prompt consisting of a few input-output examples, without any gradient updates to the model parameters [27]. Wies et al. [27] proposed a Probably Approximately Correct (PAC)-learning framework to formalize in-context learning, showing that with high probability, the in-context learner can PAC-learn the downstream task D, i.e.,\n\\[L_{In-context, D} - Bayes Error Rate < \\epsilon,\\]\nwhere the Bayes Error Rate represents the theoretically lowest possible error rate for any classifier on a given task. Xie et al. [28] proposed another theoretical framework where the in-context learning capabilities of LLMs are conceptualized through implicit Bayesian inference. They established several foundational results delineating in-context learning behavior, including asymptotic optimality, monotonic error reduction with example length, and the ability to handle varying-length test examples.\nIn-context learning enables AD systems to adapt to new scenarios without extensive retraining, crucial for handling diverse and dynamic driving environments. For instance, an LLM-enhanced AD system could quickly adjust its behavior based on a few examples of new traffic patterns or road conditions."}, {"title": "III. MODULAR DECISION MAKING", "content": "In this section, we delve into the formulation of AD decision-making using LLMs and explore various approaches to integrate LLMs with traditional AD techniques. We aim to provide a comprehensive overview of state-of-the-art methods and highlight the potential benefits of leveraging LLMs in enhancing vehicle performance, safety, and human-like decision-making capabilities. AD decision-making can be formulated as a function F that maps input data x to output actions a, considering the influence of LLMs denoted as f:\n\\[a = F(x, f),\\]\nwhere x = {x1, x2,..., xn} represents the input data from various sources, such as images, sensor data, traffic regulations, and human queries, and f represents the LLM, which processes the input data and generates output actions a = {a1, a2,..., am}, including vehicle control, decision-making, and reasoning explanations. The LLM f can be further decomposed into a pre-trained model fpre and a fine-tuned model ffine, adapted to the specific AD task:\n\\[f = f_{fine} (f_{pre}, D),\\]\nwhere D represents the domain-specific data used for fine-tuning the pre-trained LLM. Moreover, the decision-making process can be enhanced by incorporating prompt tuning techniques, denoted as P, which guide the LLM to generate more relevant and accurate outputs:\n\\[a = F(x, f, P).\\]\nConsidering a traffic scenario with N intelligent agents (vehicles) over a time horizon T, the decision-making process can be formulated as:\n\\[a = F(x, f, P), \\forall i \\in 1, 2, ..., N, \\forall t \\in 1, 2, ..., T,\\]\nwhere \\(a_t^i\\) represents the action of agent i at time t, and \\(x_t^i\\) represents the corresponding input data.\nOne promising approach for integrating LLMs with AD is to employ these models for precise vehicle control and decision-making in dynamic environments. Azarafza et al. [29] investigated the potential of LLMs in analyzing a combination"}, {"title": "IV. END-TO-END DRIVING", "content": "The integration of LLMs into autonomous driving has opened up new possibilities for end-to-end decision-making and control. The end-to-end autonomous driving process with LLMs can be formalized as follows: \\(D_t = F(X_t, f, M, P, \\Theta)\\), \\(\\forall t \\in 1,2,...,T\\) where \\(D_t\\) represents the driving decision at time step t, \\(X_t\\) is the input data (sensor data, navigation information, etc.), M is the memory component, P is the prompt engineering, and \\(\\Theta\\) are the learnable parameters. The function F encompasses the entire end-to-end pipeline, from perception to decision-making and control. To further elaborate on the LLM component L, it can be expressed as: \\(f = f_{pre} \\copyright f_{fine} \\copyright f_{prompt}\\) where \\(f_{pre}\\) is the pre-trained LLM, \\(f_{fine}\\) is the fine-tuned model adapted to the autonomous driving domain, and \\(f_{prompt}\\) represents the prompt-tuning techniques employed to guide the LLM's behavior.\nResearchers have explored various approaches to integrate LLMs into autonomous driving systems, focusing on different aspects such as closed-loop evaluation, knowledge-driven decision-making, and interpretability.\nIn the domain of closed-loop evaluation, Fu et al. [41] introduced LimSim++, a platform for evaluating Multimodal Large Language Model ((M)LLM)-driven autonomous driving. LimSim++ takes scenario descriptions XD, task descriptions XT, navigation information XN\u2081, visual content xv, road network XR, and vehicle information xv, as inputs, and generates"}, {"title": "V. CONCLUSIONS AND FUTURE DIRECTIONS", "content": "This survey comprehensively reviewed the recent progress of integrating LLMs into the knowledge-based AD systems. We first traced the evolution of AD systems, from rule-based and optimization-based approaches to learning-based techniques. We then introduced the key features and training schemes of LLMs that empower them to serve as knowledge bases and reasoning engines for AD. By categorizing existing works into modular AD pipelines and end-to-end AD systems, we analyzed in detail how LLMs can enhance scene understanding, action planning, and human-machine interaction, filling the gap between data-driven AI and human-like AD. Compared with traditional AD methods, LLM-based approaches exhibit superior adaptability to complex scenarios and stronger generalization to novel environments, showing the potential to surpass human drivers in terms of safety and efficiency.\nHowever, the application of LLMs in AD is not without challenges. Firstly, the inference speed and computational cost of LLMs need to be significantly optimized to meet the real-time requirements of AD systems [54]. Secondly, the safety, robustness and interpretability of LLM-based decisions must be rigorously validated before deployment in safety-critical AD tasks. Moreover, as LLMs are trained on large-scale online corpora, they may inherit social biases and generate outputs misaligned with human values, raising ethical concerns to be addressed [55].\nTo fully harness the potential of LLMs for human-centric AD, we propose the following research directions:\n\u2022 Develop lightweight and efficient LLMs tailored for AD applications with limited computational budgets, e.g., distilling knowledge from large LLMs to smaller models.\n\u2022 Design multi-modal training frameworks that can fuse textual, visual and geographical information to ground LLMs' reasoning in physical AD contexts.\n\u2022 Introduce safety constraints and ethical guidelines into LLM's training objectives and inference processes to improve the reliability and value alignment of their outputs.\n\u2022 Leverage LLMs to generate human-understandable explanations of AD behaviors to enhance transparency and user trust.\n\u2022 Conduct more closed-loop tests of LLM-based AD in realistic simulators and real-world trials to validate their performance in diverse driving scenarios.\nIn conclusion, the integration of LLMs into AD systems marks an exciting frontier in AI and transportation research. By synergizing the reasoning power of pre-trained models with the learning power of AD agents, this new paradigm holds the promise to achieve human-like or even superhuman vehicle autonomy. We hope this survey can provide a comprehensive reference and inspire more research efforts to address the challenges and realize the full potential of LLMs for human-centric AD."}]}