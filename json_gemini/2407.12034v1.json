{"title": "Understanding Transformers via N-gram Statistics", "authors": ["Timothy Nguyen"], "abstract": "Transformer based large-language models (LLMs) display extreme proficiency with\nlanguage yet a precise understanding of how they work remains elusive. One way\nof demystifying transformer predictions would be to describe how they depend on\ntheir context in terms of simple template functions. This paper takes a first step in\nthis direction by considering families of functions (i.e. rules) formed out of simple\nN-gram based statistics of the training data. By studying how well these rulesets\napproximate transformer predictions, we obtain a variety of novel discoveries: a\nsimple method to detect overfitting during training without using a holdout set, a\nquantitative measure of how transformers progress from learning simple to more\ncomplex statistical rules over the course of training, a model-variance criterion\ngoverning when transformer predictions tend to be described by N-gram rules, and\ninsights into how well transformers can be approximated by N-gram rulesets in\nthe limit where these rulesets become increasingly complex. In this latter direction,\nwe find that for 78% of LLM next-token distributions on TinyStories, their top-1\npredictions agree with those provided by our N-gram rulesets.", "sections": [{"title": "1 Introduction", "content": "This paper is an attempt to answer the following\nQuestion: How does a transformer-based large language model (LLM) make use of its context when\npredicting the next token?\nOur approach proceeds via studying the statistical properties of training data. This is perhaps the\nmost natural place to start even though it is not exhaustive (e.g. it does not include in-context\nlearning (Brown et al., 2020)). The reasons to understand LLM behavior in terms of the statistics\nof their training data are plenty. First, the functional form of how LLMs use their training data is\nnot well-understood (though there has been progress on understanding memorization (Nasr et al.,\n2023; Carlini et al., 2023)). Second, the over-reliance of LLMs on training data statistics leads\nto brittleness (e.g. the \"reversal curse\" (Berglund et al., 2024)) and the perpetuation of dataset\nbiases (Gallegos et al., 2024). Understanding the nature of this statistical dependence can lead to\nimproved and more informed dataset curation and training methods. Finally, in various scenarios, the\nperformance of LLMs on downstream tasks are found to be correlated with frequency of relevant\ntraining data (Razeghi et al., 2022; Elazar et al., 2023; Kandpal et al., 2023; Kang and Choi, 2023).\nA better understanding of this phenomenon would allow better steering of models towards desired\nperformance levels.\nWe can think of the complexity of an LLM next token prediction (regarded as a probability distribution\nover tokens) along two axes: form and selection. Form refers to the functional form of the prediction\nas a function of the context, e.g. whether the prediction is some explicit function of associated\ntraining data statistics (see Figure 1). Selection refers to which functional form, chosen from a set\nof functional templates, suitably describes the transformer prediction (supposing the choice set is\nsufficiently rich). As a first nontrivial step, one might hope that an approximate model for an LLM"}, {"title": "2 Related Work", "content": "Rule extraction methods for neural networks have been studied in quite different settings, e.g.\n(Jacobsson, 2005; Mcmillan et al., 1991). Some recent works have performed N-gram analyses for\nlarge-language models in the setting of in-context learning (Aky\u00fcrek et al., 2024) and associative\nrecall (Arora et al., 2023). The \u201cinfini-gram\" model (Liu et al., 2024) compares LLM predictions with\nthe single N-gram rule given by retrieving the largest possible matching context from the training\ndata. Our work uses shorter but more sophisticated N-gram rules. In (Voita et al., 2023), an approach\nto understanding how LLMs process N-grams is carried out at the level of individual neurons. This\ncomplements our dataset-based work, which treat models as a black box. In (Edelman et al., 2024),\nthe evolution of the type of N-gram statistics that transformers learn during training is analyzed in\nthe setting of synthetic Markov chain data, in contrast to our natural language setting. Other works\nstudying the learning trajectory of language models include (Chen et al., 2024; Choshen et al., 2022).\nThere is a large literature on building more sophisticated N-gram models, e.g. (Kneser and Ney,\n1995; Goodman, 2001). Such models could have been incorporated into our set of rules, but for\nsimplicity we choose not to include them."}, {"title": "3 Experimental Setup", "content": "We train standard decoder-only transformer models on the TinyStories (Eldan and Li, 2023) dataset\n(480M tokens) consisting of children's stories synthetically generated from GPT-4 using templated\nprompts. The value of this dataset lies in its linguistic simplicity, whereby it is possible to model\nlanguage well on the dataset using very small models. Unless stated otherwise, our experiments\nuse a 160M parameter model trained for 4 epochs, which achieves a loss of around 1.11 nats on the\nvalidation set. We train for 4 epochs since we use learning rate warmup and cosine learning rate\ndecay and we want to ensure all datapoints receive updates with a high learning rate (this way all\nN-gram statistics have a fair chance of being learned during training). For overfitting experiments\nin Section 6.2, we train a 1.4B model for 10 epochs. In the Appendix, we include some additional\ncorresponding experiments on Wikipedia (from MassiveText (Rae et al., 2022)) with a single epoch\nof training in order to validate that our results are of a general nature and extend to more complex\ndatasets. For a fixed dataset, the only source of randomness among different runs are different dataset\nshuffles. Full experimental details are described in the Appendix."}, {"title": "4 N-Gram Rules", "content": "The attention layer within a transformer is in essence a soft context-selection mechanism. The\nN-gram rules we consider will be loosely modeled on this mechanism. Namely, given a context\nwe will form a derived context in which each token will either be kept, discarded, or marginalized,\nwhich is meant to mimic positive attention, no attention, and semantic invariance\u00b3, respectively. More\nformally, we proceed as follows:\nGiven a regular expression\u2074 a, all contexts from the training data can be retrieved which match the\nregular expression. This allows us to define a corresponding rule that defines for us a distribution\nover tokens t:\n\nRa(t) = \\frac{\\#{at}}{\\#{a*}}\n                                                                                                                                                                        (1)\n\nFor instance, the next token distribution for the context \"... the tired dog\" may be insensitive to replacing\n\"tired\" with \"brown\" or \"furry\". Statistics which thus marginalize over all extant substitutions for \"tired\" yield a\ncrude but generally applicable way of capturing semantic invariance. One can imagine an attention mechanism\nfor which there is a many-to-one mapping of keys to a particular value that might implement semantic invariance.\nOur regular expressions operate on tokens not string characters, since our contexts are formed out of\nsequences of tokens."}, {"title": "5 Approximating Transformer Predictions with Rules", "content": "Let p(t C) denote the next-token distribution of an LLM conditioned on the context C and for\nnotational similarly, write pr(t|C') for r(t|C'), where r is one of the rules defined in Section 4. We\nwish to measure how similar these distributions are (higher similarity corresponds to a better rule\ndescription). To that end, we use the variational distance to measure the difference of two distributions\n(we discuss our choice in the Appendix):\n\nd(p,q) = \\frac{1}{2} \\sum_{i} |p_i - q_i|\n                                                                                                                                                                         (10)\n\nSince variational distance may be lacking in concrete interpretability, we will sometimes use top1-\naccuracy to measure similarity, defined by\n\ntop1-acc(p, q) = \\frac{|argmax(p) \\cap argmax(q)|}{|argmax(p) \\cup argmax(q)|}\n                                                                                                                                                                         (11)\n\n(in general, the argmax of a probability distribution is a set due to potential ties among maximal\nprobabilities). When the argmaxes in (11) are singletons, top1-accuracy just measures agreement\nbetween greedy predictions.\nGiven a context C, we want to understand how d(p(t|C), pr(t|C)) varies with different rules r and\nin particular if it can be made small. To that end, we introduce some terminology:\nWe are interested in determining the optimal rule pr(t|C) (as defined in Table 1) and if it has small\noptimal rule distance then we regard the rule as being a good description of the corresponding\ntransformer prediction(s) pi(t|C). As a first step, note there is a distinguished rule\n\nPfull(t C) = \\frac{\\#{Ct}}{\\#{C*}}\n                                                                                                                                                                        (12)\n\nwhose rule context is the full unmodified context C.10 This is because (roughly) the language-\nmodeling objective aims to make p(t|C) similar to Pfull(t|C).11 All other rules in our rulesets are\n\"subleading\" in that they drop or marginalize over tokens in the context C. Our goal is to quantify\nwhich rules, either (12) or subleading ones, are optimal rules and what their optimal rule distances\nare.\nOur main finding is an approximation-variance association: contexts with low model-variance tend\nto have low optimal rule distance. The surprising aspect of this association is the sufficiency of\nlow model-variance (necessity is a given).12 We present the case of 7-gram contexts in Figure 2\nto corroborate this association, with additional examples relegated to the Appendix. We sample"}, {"title": "6 Learning Dynamics", "content": "We can track how well LLM predictions are described by our N-gram rules over the course of training\nby tracking optimal rule distance as a function of train step. Here optimal rule distance is defined as\nin Table 1 with R any of the rulesets (7)-(9), and we will measure how optimal rule distances vary\nwith maximum context length M (the resulting analyses are similar for \u201call\", \u201csubgram\", and \"suffix\"\nrules so we show our analysis for \"all\").\nFigure 3 summarizes our results. Early in training, LLM predictions acquire structure and thus\nbecome approximable by rule predictors. However, with further training, LLM predictions eventually\ndiverge from simpler rules (small context length) while continuing to increase in similarity with\nmore complex rules (larger context length). Moreover, the rightmost plot of Figure 3 shows that\ntop1-acc(p(t|C'), pr(t|C)) increases over the course of training for optimal r \u2208 Rall (for M > 1)\nshowing that the rule selection improves with time. Altogether, this shows that LLMs undergo a\ncurriculum style learning, in which their predictions gradually move away from simpler rules to more\ncomplex and effective rules."}, {"title": "6.2 Early Stopping Criterion", "content": "Our investigations of approximating LLMs with rules given by limited contexts naturally lead us to\nconsider LLMs with limited context. The latter have predictive distributions given by\n\nPn(t|C) = p(t|C_{-n}\u2026C_{-1})\n                                                                                                                                                                            (13)\n\nwhere n is the maximum context length. In Figure 4, we plot the loss of an LLM trained to overfit\n(train loss decreases while validation loss increases) along with its limited context versions for\n1 \u2264 n \u2264 7. For the limited context models with n > 1, we see that on both the train and validation\nset, the two respective loss curves track each other closely and both eventually go up. This suggests\nthe following picture: an overfitting LLM is spending capacity to minimize train loss by memorizing\nthe full context at the expense of using capacity to learn statistics of subcontext, i.e. the reduced\ncontext in (13). This manifests itself both during training (where subcontext arises from a subset of a\nlarger memorized context) and during validation (where subcontext arises from the partial overlap\nbetween novel context and the train set).\nOur discovery suggests a simple and computationally inexpensive early stopping criterion: during\ntraining, evaluate the transformer on train data consisting of short contexts and when this quantity\nbegins increasing, stop training. Significantly, this method involves no holdout set and is a training\ndataset intrinsic criterion."}, {"title": "7 Rule Peformance", "content": "Finally, addressing our main question from the introduction, we track how well our rulesets describe\nLLM predictions (in the sense of Section 5) as a whole at inference time. Here, the utility of our\nN-gram rules defined in Section 4 becomes apparent, since on a holdout set, there will be novel\ncontexts and being able to drop or marginalize context tokens aid in being able to retrieve or aggregate\ncorresponding training dataset statistics. In Table 2, we show the average top1-accuracy between\nthe optimal rule from our various rulesets and LLM predictions on 100 random stories from the\nvalidation set. Here, we include as baseline backoff, the single rule given by the predictive model\nwhich performs \u201cstupid backoff\" (Brants et al., 2007) using M tokens of context.13\nWe see significant gains in accuracy at large M when adding additional types of rules (for M = 7 we\ngain 6% each time in going from \"suffix\" to \"subgram\" to \"all\"). In the end, we are able to obtain 78%\ntop1-accuracy between the per-prediction optimal rule and the LLM predictions, averaged over all\ntokens. This is perhaps a remarkably high figure, considering that the top1 accuracy of the model with\nrespect to the ground truth on the validation set is 69.6%. At minimum, we have provided a precise\nquantification of structure in LLM next-token predictions: they are often matched (as measured by\ntop token prediction) by some simple N-gram rule derived from the training data. See Section D.1\nfor some supplementary analysis.\nTo ground our rule optimization procedure, we provide Figure 5 which shows side-by-side how LLM\npredictions compare with ground truth and optimal rule predictions in an example heldout story.\nFor instance, for the target token \"climb\" in \"... Roxy loved to climb\", both the LLM and\noptimal rule Ra predict \u201cplay\u201d, where a = \u201c. * loved to\". For target token \"climb\" in \"...\nShe climbed\", the LLM predicts \"would\" whereas the ground truth and Ra predict \u201cclimb\u201d,\nwhere a = \u201cloved to climb * She\". In general, optimal rules provide the closest statistical\nmatch from the training data to the given LLM predictive distributions (from amongst our rulesets),\nand their top1-predictions can agree or disagree agree (as indicated by target token color). Additional\nexamples, including those from Wikipedia, are shown in Section D. For interpretability purposes, we\nre-emphasize that our optimal rules currently only provide descriptions, not explanations. We leave\nthe possibility of the latter for future work."}, {"title": "8 Conclusions and Limitations", "content": "Our work provides quantitative measures of how well the predictions of transformer-based LLMs are\ndescribed (i.e. approximated) by simple N-gram rules. Such rules were motivated by the simplest\ntoken-level operations applied to the context (keep, ignore, or marginalize). The results we obtained\nin Section 7 imply that, at least on simple datasets like TinyStories and Wikipedia, LLM predictions\ncontain much quantifiable structure insofar that they often can be described in terms of our simple"}, {"title": "A Choice of Distance Measure", "content": "We choose variational distance since it is a symmetric and bounded distance function (unlike the KL\ndivergence). Symmetry means we do not have to make a choice between computing the distance\nbetween model predictions and rule predictions or vice versa. Boundedness ensures that when we\nmeasure average distance across tokens, large outliers do not dominate the average. In fact, for\nthe KL divergence, since KL(p||q) is infinite when p > 0 wherever q = 0, were we to use KL\ndivergence, we would have to set p equal to rule predictions and q equal to model predictions (since\nrule predictions are typically sparse). To avoid such constraints and potential pathologies, we choose\nthe variational distance as our metric. We find that other measures like Jensen-Shannon distance give\nsimilar results. It is worth noting that while the L\u221e-metric often gives similar results, it has a failure\nmode when comparing two very high entropy distributions. If p and q are two distributions such that\npi and qi are all small, then their L\u221e distance will be small even though their variational distance\ncan be large."}, {"title": "B Additional Experimental Details", "content": "Our transformer architecture and training procedure is based on that of Chinchilla (Hoffmann et al.,\n2024). The architecture hyperparameters are as follows:\nWe use a linear learning rate warmup of 1000 steps up to a maximum value of 2 \u00d7 10-4 and then use\na cosine learning rate decay. We use weighted Adam optimizer (Loshchilov and Hutter, 2017) with\nweight decay 10-4. Our models are trained using TPU accelerators. The 160M models use 16 TPU\naccelerators while the 1.4B models use 64 TPU accelerators (to exploit data parallelism) per run. We\nuse a batch size of 128 sequences with each sequence consisting of 2048 tokens.\nOur training datasets (TinyStories and MassiveText Wikipedia) are prepared as follows. After tok-\nenizing the individual documents (stories for TinyStories and articles for Wikipedia), we concatenate\nthem all into one long sequence, with each document separated by the (BOS) token14. The full\nsequence is then subdivided into contiguous sequences of length 2048 (with padding as needed) and\nthen shuffled to form a static dataset of shuffled sequences. We refer to the previous procedure as\n\"chunking\". Crucially, observe that chunking results in most sequences not starting with the (BOS)\ntoken (hence a model will be trained to predict the next token conditioned on incomplete contexts, as\ndesired).\nFor TinyStories experiments, we train 160M models for 4 epochs except for the overfitting experiments\nwhere we train 1.4B models for 10 epochs. We use the train and validation splits provided by\nHuggingFace15. For Wikipedia experiments, we train a 1.4B model for a single epoch. We have train\nand validation splits based on using choosing random sets of disjoint documents. Our Wikipedia train\nset has 4.4B tokens. In places where we perform several training runs (Section 5), the only source of\nvariance (randomness) among the runs are different dataset shuffles.\nOur tokenizer16 uses byte-pair encoding trained on MassiveText with a vocabulary size of 32,678."}, {"title": "B.1 N-gram statistics", "content": "The computation of N-gram statistics of the training data is formed after chunking (as described\nabove), so that they correspond to the N-gram statistics seen by models during training. In particular,\ntokens which are contiguous in a story but separated by the chunking will not contribute to the"}, {"title": "C Additional Approximation-Variance Association Analysis", "content": "We provide additional commentary and experimental settings for our analysis in Section 5."}, {"title": "C.1 Full-context vs Subcontext", "content": "As noted in footnote 11, there is usually a mismatch between the contexts that N-gram rules and\nLLMs receive during training: the latter can receive very long contexts (up to one less than the\nnumber of tokens in a document) while the former typically receives very short contexts (in our case,\nup to 7 tokens). Concretely, while a bigram model is trained on consecutive pairs of tokens (c, t), an\nLLM is rarely trained so as to optimize p(tc). Indeed, given a training sequence x, only the target for\nthe first token of x has context consisting of a single token; the other targets will have more tokens\nof context accordingly. Thus, it is unclear how well LLM predictions p(t\u2758c) should match bigram\nrule predictions as e varies over the vocabulary set, since LLMs almost always receive c within a\nmuch larger context. More generally, it is unclear how well p(t|C') matches Pfull(t|C). Nevertheless,\nbecause in practice LLMs learn how to use context effectively, LLMs manage to learn p(t|C) despite\nbeing optimized for p(t|C) with C a context containing C as a suffix.\nAs a measure of how much training context \u201cdilutes\" the LLM ability to learn the bigram distribution,\nin Figure 6 we plot the distance between LLM predictions and the bigram rule for two LLMs:\none trained in the usual fashion with full context and one trained with only one token of context\n(concretely, a token can only attend to itself in attention layers). In both cases, we have the same"}, {"title": "C.2 TinyStories Unigram Context", "content": "We repeat Figure 2 for the simplest case of unigram context. In this case, there is only one rule (the\nbigram rule) and so there are only three plots to consider. It turns out also the correlation between\noptimal distance and count is slightly stronger than with model variance. However, given the unigram\ncontext case is extreme (in the sense that there is only a single token of context), we treat this case as\nan edge case."}, {"title": "C.3 Tinystories Bigram Context", "content": "Next, consider the case when there are two tokens of context. To get a more fine-grained analysis, we\nconsider the case of full-context bigrams, i.e. those starting with the (BOS) token. This is because\nsuch bigrams do not appear within a larger context and so a transformer's corresponding predictions\nare more fair to compare with those of N-gram models (both are trained using equal contexts).\nConveniently, there are only 691 full-context bigrams in this case and so we do not have to randomly\nsample a subset.\nWe will consider the ruleset Rall for which there are three relevant N-gram rules of interest: one\nwhich uses the entire bigram of context (a trigram model), one which uses only the last token (a\nbigram model), and one which uses only the first token (the next token distribution of (BOS)).17 We\nwill refer to these as the trigram, bigram, and (BOS) rule respectively."}, {"title": "C.4 Wikipedia 6-gram contexts", "content": "We plot the analog of Figure 2 but for contexts consisting of 6-grams from Wikipedia. We also\nsubsample as before, from logarithmically spaced buckets, for a total of around 6.8K total contexts.\nWe get nearly identical behavior as with TinyStories. Our approximation-variance association is thus\nnot specific to small datasets like TinyStories."}, {"title": "D Rule Performance: Additional Analysis and Examples", "content": "We supplement Table 2 with Table 4 to show how optimal rule distances decrease with increasing\nrule strength. This is to preclude a trivial situation in which by having sufficiently many rules (say a\none-hot distribution for every vocabulary token), one can have a ruleset that for any model prediction\nalways returns an optimal rule with 100% top-1 accuracy! Such coarse rules will not in general yield\nsmall optimal distances however18 and our variational distances decreasing in Table 4 shows that our\nrulesets are truly better approximating the predictions with increasing strength."}, {"title": "D.2 Rule Approximation: Another Interpretation", "content": "Our rule approximation is formulated as a retrodictive procedure in which after an LLM prediction is\nmade, one uses the optimization procedure defined in Section 5 to select an optimal rule for describing\nthe prediction. This retrodictive viewpoint however can be replaced with an alternative interpretation:\nRegard the LLM output next-token distribution as a value-vector and each rule prediction as a\nkey-vector in probability space. We use nearest-neighbors with respect to variational distance to\nselect the closest key to the given LLM value-vector. This key then becomes the resulting predicted\nprobability distribution of this joint system of an LLM plus N-gram rules. The joint system is then a\npredictive model that forces LLM predictions through a \u201cbottleneck layer\" of a small set of N-gram\nrules. Our results from Section 7 can be interpreted as saying that such an N-gram bottleneck layer\nachieves 78% fidelity with respect to the original LLM predictions on TinyStories as measured by\ntop-1 accuracy."}, {"title": "D.3 TinyStories", "content": "Here we supplement our example in Figure 5 by showing how the smaller rulesets Rsubgram and Rsuffix\ncompare in Figures 10 and 11. As expected, the top1 accuracy between transformer predictions and\noptimal rule predictions decrease with smaller rulesets."}, {"title": "D.4 Wikipedia", "content": "We present analogous results in Section 7 for a 1.4B model trained on Wikipedia. In Table 5 we\npresent the analogue of Table 2 (except we go up to maximum context length M = 6). To investigate\nsensitiy to the choice of distance measure on probability distributions, for this section, optimal rules\nare chosen using the L\u221e metric\n\ndo (p,q)\n= max |Pi - qi|\n                                                                                                                                                                               (14)\n\ninstead of the variational distance.\nThe top1-accuracy when using optimal rules from Rall is 64.5%. As with TinyStories, we see\nsignificant gains in accuracy when we increase rule strength. Achieving the number 64.5% (versus\nthe corresponding 74.0% number for TinyStories from Table 1), perhaps a surprisingly a high score,\nis the result of two competing factors: on the one-hand, Wikipedia has a much greater diversity of\nN-gram statistics (which makes prediction harder), while on the other hand, the training data has\nmore N-grams for use by the rules. Note that our reference LLM (a 1.4B model) achieves 50.1%\ntop-1 accuracy on the 100 validation stories and a train loss of around 2.1 nats at the end of training."}, {"title": "E Broader Impacts", "content": "Large language-models are having significant impacts on society, due to their use as question-answer\ntools and natural language generators. A better understanding of such language models will only\nserve to improve their capabilities. Our work here presents steps towards a fundamental understanding"}]}