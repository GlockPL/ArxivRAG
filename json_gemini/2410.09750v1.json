{"title": "Surgical-LLaVA: Toward Surgical Scenario Understanding via Large Language and Vision Models", "authors": ["Juseong Jin", "Chang Wook Jeong"], "abstract": "Conversation agents powered by large language models are revolutionizing the way we interact with visual data. Recently, large vision-language models (LVLMs) have been extensively studied for both images and videos. However, these studies typically focus on common scenarios. In this work, we introduce an LVLM specifically designed for surgical scenarios. We integrate visual representations of surgical images and videos into the language feature space. Consequently, we establish a LVLM model, Surgical-LLaVA, fine-tuned on instruction following data of surgical scenarios. Our experiments demonstrate that Surgical-LLaVA exhibits impressive multi-modal chat abilities in surgical contexts, occasionally displaying multi-modal behaviors on unseen instructions. We conduct a quantitative evaluation of visual question-answering datasets for surgical scenarios. The results show superior performance compared to previous works, indicating the potential of our model to tackle more complex surgery scenarios.", "sections": [{"title": "Introduction", "content": "The rapid advancements in AI have increasingly focused on developing versatile assistants that can effectively understand and interact with the world through multiple sensory modalities, such as vision [13] and language [5]. This multi-modal approach harnesses the unique strengths of each channel, enhancing the AI's ability to perform a wide range of real-world tasks more accurately and efficiently [4, 12]. Despite significant progress with large language models (LLMs) like GPT-3 [19], GPT-4 [1], and open-source alternatives such as LLaMA [31] and Vicuna [8], these models typically handle language tasks in isolation, limiting their potential in applications that require a comprehensive understanding of multimodal data. Recent efforts have attempted to bridge this gap by integrating visual comprehension within a single model, aiming to create a unified representation that captures both visual and linguistic information. For example, models such as LLaVA [18] and Video-LLaMA ([37]) utilize shared visual encoders to process images and videos.\nIn the surgical applications, the ability to understand and process both images and videos is of paramount importance [27, 14]. Surgical procedures generate a wealth of visual data, including static images and dynamic videos. While general-domain vision-language models have been successful, they are less effective in surgical contexts because surgical visual-text pairs differ significantly from typical web content. This discrepancy can cause general-domain visual assistants to act like laypersons, either avoiding surgical questions or providing incorrect or completely fabricated responses. Despite significant advances in surgery visual question answering (VQA), prior methods often treat the problem as a classification task (e.g., choosing among specific answers from the training set) [11, 33]. As a result, conversational generative AI for surgical applications is often restricted to specific tasks.\nIn this paper, we present Surgical-LLaVA, a first attempt to extent multimodal instruction-tuning to the surgical domain for multimodal conversational assistant. Inspired by recent work in instruction-"}, {"title": "Related Work", "content": "Large Language Models The emergence of LLMs such as GPT, LLaMA and OPT [38] has led to a paradigm shift in the field of natural language processing. These models excel in language generation and in-context learning, and demonstrate the ability to understand complex tasks. The high adaptability and generalisability of LLMs has led researchers to fine-tune these models for optimal performance.\nOne of the key strategies in such research is instruction tuning. This approach focuses on improving the model's alignment with user intent and optimising the quality of its output. For example, InstructGPT [22] and ChatGPT use this technique to improve their ability to interact with a variety of dialogues and answer complex questions. This effective approach has recently been applied to open source models such as Alpaca [23] and Vicuna, resulting in performance improvements.\nLeveraging LLMs for Multimodal Understanding The recent advancements in multimodal under- standing have been primarily driven by the integration of image-based vision models with LLMs. Pioneering contributions, such as Flamingo [2] and BLIP-2 [15], have demonstrated the power of leveraging web-scale image-text data and cross-modal alignment techniques to exhibit impressive capabilities in conversational and few-shot learning settings. Equally, noteworthy is the emergence of Large Language and Vision Assistant (LLaVA) [18], a model derived from the LLaMa architecture, which capitalizes on GPT-4's language proficiency to generate multimodal instruction-following data. Through instruction tuning on the derived data, LLaVA has showcased promising multimodal chat capabilities, hinting at the scalability potential of such an approach. Furthermore, the InstructBLIP [9] model has demonstrated strong image-based dialogue capabilities through vision-language instruction tuning and innovative instruction-aware visual feature extraction. Inspired by these success, several medical vision-language model have been studied [30, 36, 34]. LLaVA-Med [18] fine-tuned from biomedical data to instruction-following data and achieved superior performance on a variety of prompts.\nSurgical Scenario Visual Question Answering Early surgery video datasets primarily consisted of images and their corresponding annotations, focusing on tasks such as instrument detection, segmentation, and procedural step recognition. The Cholec80 dataset [32] and the EndoVis18 dataset [3] were pioneering efforts in this domain, providing annotated laparoscopic videos and surgical scenes for instrument recognition and segmentation, respectively. However, the creation and annotation processes for these datasets were labor-intensive and time-consuming, limiting their scalability and diversity. To address these limitations, some researches shifted their focus towards leveraging the abundance of visual-text resources available in the medical domain. [29] and [28] pioneered the integration of visual and textual information by constructing datasets tailored for visual- question answering tasks in surgical settings. We aim to capture the rich multimodal information present during surgical procedures, enabling the development of models capable of simultaneously understanding and reasoning about complex visual and textual cues, thereby opening new avenues for research and allowing the exploration of novel tasks and applications that leverage the synergy between visual and textual information. Surgical-LLaVA aimed to develop an effective vision- language assistant for various complex prompts by generating multimodal instruction-following data for surgical scenarios by utilizing the language capabilities of LLMs such as GPT."}, {"title": "Surgical Visual Instruction Data Generation", "content": "This section describes a data-driven approach for multimodal instruction following data collection using LLMs within a novel framework specifically tailored to the surgical scenarios. Inspired by the recent success of visual language models in text annotation tasks, our approach is based on widely available image pair data, We adopted the LLaVA approach [23] for data generation and incorporated annotation information as input to facilitate the generation of instructional data tailored to the surgical scenario. Specifically, our framework is the basis for generating a variety of contextualized instructions using expert-annotated surgical image data.\nRecognizing the lack of comprehensive information in the original annotations, we attempted to leverage LLM's medical and background knowledge, such as GPT-3.5. We leveraged the original annotations to create instruction-following annotations with various prompts and instructions, as shown in Figure 1. By leveraging LLM's powerful language understanding and generation capabilities, it plays a key role in expanding the original annotations and incorporating relevant medical knowledge, procedural details, and contextual cues to create comprehensive and informative guideline-following annotations. To achieve this, we create a test set based on the ActivityNet-200 dataset [6] form, which contains videos accompanied by detailed descriptive captions and human-annotated question-answer pairs. Moreover, we construct an evaluation pipeline utilizing the GPT-3.5 model. The prompt used to generate the instruction data is provided in Appendix A.1. This approach not only allows us to generate high-quality, multimodal instruction data specific to surgical scenarios but also effectively utilizes existing annotation resources."}, {"title": "Surgical-LLaVA", "content": "Surgical-LLaVA is a vision-language model that enhances surgical scenario analysis and conversation capabilities by aligning visual representations with a LLM. To achieve this, we leverage existing approaches used in the development of vision-language models for visual tasks. Given the scarcity of visual-caption pairs and the significant resources required for training from scratch, our strategy involves adapting pretrained image-based VL models for visual applications, as seen in previous works [26, 21]. We specifically build upon the LLaVA, a large multimodal model that combines the visual encoder of CLIP [25] with the Vicuna language decoder [8], and is fine-tuned end-to-end on generated instructional vision-language data. We further fine-tune LLaVA with our visual-instruction data to tailor it for conversation tasks."}, {"title": "Architecture", "content": "The primary goal is to effectively apply the capabilities of the pre-trained LLM and visual model to surgical scenarios. The architecture is illustrated in Figure 2. We adopted LLaVA as the base- line, which vicuna as the LLM model and the pre-trained CLIP visual encoder ViT-L/14 as the visual model. Our visual encoder, originally designed for image processing, is extended to handle video inputs. Given a video sample $V_i \\in \\mathbb{R}^{T\\times H\\times W\\times C}$ with $T$ frames, the encoder generates both temporal and spatial features. To derive video-level features, we perform average pooling on the frame-level embeddings along the temporal dimension, resulting in video-level temporal represen- tations $t_i \\in \\mathbb{R}^{N\\times D}$. Similarly, average pooling along the spatial dimension produces video-level spatial representations $z_i \\in \\mathbb{R}^{T\\times D}$. By concatenating the temporal and spatial features, we obtain comprehensive video-level features."}, {"title": "Visual Understanding Training", "content": "The overall training process for Surgical-LLaVA follows a similar approach to LLM models like GPT. The model takes as input a text seqeunce $X_q$ and visual data $X_v$ (image or videos). These inputs are encoded into a token representation according to Eq 1. The training objective is to maximize the likelihood probability in Eq 2.\n$Z_T = f_w(X_T), Z_v = f_p(f_v(X_v))$ (1)\n$P(X_A | X_v, X_T) = \\prod_{i=1}^{L} P_{\\theta} (X_A^i | Z_v, Z_T, X_A^{[1:i-1]})$ (2)\nwhere $L$ represents the length of the generated sequence, and $\\theta$ denotes the trainable model parameters. This phase focuses on enabling the model to interpret visual representation from an extensive dataset comprising image/video-text pairs. Each visual sample corresponds to a single round of original caption data $(X_q, X_a)$, where $X_T = X_q$.\nJoint Contrastive Learning In our approach, we employ a dynamic joint training that includes both image and video samples within each batch. We employ a transformer model for our language"}, {"title": "Visual Instruction Tuning", "content": "During instruction tuning, the model is trained to generate responses based on both visual inputs and text-based instructions. The input sequence for each conversation consists of text queries $X_q$ and corresponding visual data $X_v$, structured as follows:\n$X_T = \\begin{cases} [X_q], & \\text{if } r = 1 \\\\ \\text{Concat}(X_v^{r-1}, X_q^{r-1}, X_a^{r-1}, X_q^r), & \\text{if } r > 1 \\end{cases}$ (4)\nwhere $r$ represents the round number. For the first round, only the text query $X_q$ is considered. For subsequent rounds, the model concatenates the previous query-answer pairs with the current query to form the input. This allows the model to maintain conversation history across multiple rounds. The training objective is to maximize Eq 2, the same as in the previous step."}, {"title": "Experiments", "content": "Implementation Details We use LLaVA as our baseline model. We finetune the model for 3 epochs using a learning rate of 1e-5 and overall batch size of 16. The training of our 7B model took around 16 hours on 4 \u00d7 RTX3090 24GB GPUs. During inference, for memory efficiency, we load the model in FP16 mode. The data in each batch is random combination of images and videos.\nData Description We utilized three datasets as visual datasets for our surgical scenario.\n\u2022 Cholec80-VQA [32] contains Q&A pairs for 80 video sequences, including 97,251 Q&A pairs on surgical phases and instrument presence of the Cholec80 dataset. The videos are configured at 25 frames per second (fps), while the annotations are provided at 1 fps. To align with the annotation frame rate, we extracted frames from the videos at 1 fps. We split the dataset into train/test sets (test set video sequences: 71-80).\n\u2022 EndoVis-18-VQA [3] contains Q&A pairs for 18 robotic Nephrectomy procedure video sequences, with 11,783 Q&A pairs based on 2,007 surgical scenes from the MICCAI Endoscopic Vision Challenge 2018 dataset. For this dataset, we utilized 2,600 images and leveraged multiple annotations per single image. We followed the original train/test split of the EndoVis-18-VQA dataset.\n\u2022 PSI-AVA-VQA [33] consists of 10291 Q&A pairs with 35 answer classes (locations, surgical phases, and surgical steps) of holistic surgical scenario. They are constructed based on the surgical phase, step and location annotation provided in the PSI-AVA dataset. We followed the original train/test split of the PSI-AVA dataset."}, {"title": "Surgical Video Understanding", "content": "To evaluate the performance of Surgical-LLaVA on surgical scenario conversation, we present a benchmark designed to assess the text generation capabilities of visual models. The evaluation pipeline for video understanding follows Video-ChatGPT [20]. This pipeline evaluates the model's performance and assigns relative scores to the generated responses on a scale of 1-5, in the following three dimensions:\n(i) Conversation: We assesses the accuracy and relevance of the model's responses during the visual dialogue, ensuring it accurately reflects the video content without any misinterpretations or false information.\n(ii) Detail description: We evaluate the thoroughness of the model's responses, checking for completeness by ensuring all major points from the video are covered, and for specificity by including precise details rather than generic statements.\n(iii) Complex reasoning: We assess the model's ability to engage in complex reasoning, ensuring its responses demonstrate an understanding of the video's context and logical connections between the content points.\nAmong the models evaluated, Surgical-LLaVA stands out with the highest scores across all three dimensions compared to other LVMLs that fine-tuned surgical instruction tuning data, as shown in Table 1. The Surgical-LLaVA model not only demonstrates superior conversational abilities and detailed descriptions but also excels in complex reasoning, particularly in understanding and articulating intricate surgical scenarios. This capacity to grasp and reason through complex medical content is crucial, showcasing its potential for applications in surgical environments where accurate and nuanced interpretation of video content is paramount. Figure 3 illustrates examples of surgical visual conversations using different representative chatbots on images. Surgical-LLaVA responds to questions accurately, leveraging medical knowledge, whereas Video-LLaVA [17] responds more like a layperson, often producing commonsense-based hallucinations. We used GPT-3.5 to compare Surgical-LLaVA to other LVLMs and to annotate the data. Additionally, when calculating the relative scores in Table 2, we consistently compared the answers of the candidate models to those of GPT-3.5.\nWhile the ranking of the resulting values is consistent, the values themselves may be biased in favor of GPT's answers. Given this self-enhancement bias, we expect Surgical-LLaVA's performance in practice to be more similar to GPT-3.5's than the current results suggest."}, {"title": "Evaluation on Visual Question-Answering Benchmarks", "content": "In this evaluation, we assess the performance of various models on VQA tasks, particularly focusing on the Cholec80-VQA, EndoVis18-VQA, and PSI-AVA-VQA datasets. Table 2 provides a com- parative analysis of different models based on their performance metrics. The results in Table 2 demonstrate that Surgical-LLaVA significantly outperforms existing models, achieving the highest accuracy rates across all three datasets. This ability to maintain superior performance across diverse datasets underscores Surgical-LLaVA's versatility and reliability in processing various types of visual and contextual information in surgical videos. The model's consistent excellence across multiple benchmarks represents a significant advancement in the field of surgical video data interpretation and interaction."}, {"title": "Qualitative Evaluation", "content": "To comprehensively assess the capabilities of our proposed Surgical-LLaVA model, we conducted an extensive qualitative evaluation spanning a diverse array of open-ended video question-answering tasks.\nConversation We confirmed whether the model accurately reflects the content of the surgical videos without introducing any hallucinations or misinterpretations. This involves verifying that the generated text stays true to the visual information presented and is contextually appropriate as illustrated in top of Figure 4.\nDetail Description We evaluated the model's capacity to generate detailed and descriptions of the surgical scenes. Surgical-LLaVAs describe the tools, steps, and even a description of the surrounding tissues in a surgery as illustrated in middle of Figure 4.\nComplex Reasoning These tasks focused on the model's capability to perform complex reasoning based on the visual information and contextual knowledge. Surgical-LLaVA identified the current phase from the visual data and effectively suggest things to watch out for at that stage, as exemplified in bottom of Figure 4.\nThroughout the evaluation, our Surgical-LLaVA model demonstrated remarkable proficiency in comprehending the visual content of the surgical videos and generating accurate, informative, and contextually relevant responses across the various tasks. The model effectively leveraged the visual information present in the videos to provide precise answers, detailed descriptions, and reasoned insights, showcasing its capability in understanding and reasoning about complex surgical procedures."}, {"title": "Ablation Study", "content": "We conducted an ablation study on joint contrastive learning. As shown in Table 3, we compared the performance of Surgical-LLaVA* without image training. The model trained with both images and"}, {"title": "Limitations", "content": "The success of Surgical-LLaVA underscores the potential of combining large language models with specialized visual encoders for domain-specific applications. However, current public surgical datasets have limitations in providing limited information such as phase, tool and small amount of"}, {"title": "Conclusion", "content": "In this work, we introduced Surgical-LLaVA, a multimodal model designed for engaging in meaning- ful conversations and reasoning about surgical scenarios. By integrating the language understanding capabilities of LLMs with pretrained visual encoders tailored for spatiotemporal representations of surgical procedures, Surgical-LLaVA exhibits impressive multi-modal chat abilities in surgical contexts. A contribution of our work is the introduction of a novel dataset consisting of high-quality surgical visual instruction pairs, generated through a scalable and diverse annotation framework specifically designed for the medical domain. Through quantitative and qualitative evaluations, we demonstrated Surgical-LLaVA's superior performance compared to existing state-of-the-art models in various tasks, including visual question-answering, video reasoning about surgical scenarios."}, {"title": "Appendix", "content": "A.1 A.1. Instruction Tuning Data Generation Code\nWe generated instruction tuning data for conversation, detailed description, and complex reasoning with the following prompt code \"gpt info\" part with the original caption."}]}