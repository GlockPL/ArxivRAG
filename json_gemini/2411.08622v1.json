{"title": "Precision-Focused Reinforcement Learning Model for Robotic Object Pushing", "authors": ["Lara Bergmann", "David Leins", "Robert Haschke", "Klaus Neumann"], "abstract": "Non-prehensile manipulation, such as pushing objects to a desired target position, is an important skill for robots to assist humans in everyday situations. However, the task is challenging due to the large variety of objects with different and sometimes unknown physical properties, such as shape, size, mass, and friction. This can lead to the object overshooting its target position, requiring fast corrective movements of the robot around the object, especially in cases where objects need to be precisely pushed. In this paper, we improve the state-of-the-art by introducing a new memory-based vision-proprioception RL model to push objects more precisely to target positions using fewer corrective movements.", "sections": [{"title": "I. INTRODUCTION", "content": "Humans intuitively interact with objects in everyday situations, often without explicitly planning or thinking about how objects will behave. Non-prehensile object manipulation is an important skill for robots that are designed to assist humans. This work focuses on object pushing, a sub class of robotic manipulation that is crucial e.g. for service robots working in a kitchen, but it can also be beneficial for industrial applications. Consider a robot that has to grasp a cup that is placed on a shelf behind other objects. A simple strategy to reach the desired cup is to push the other items aside. Pushing can also be considered as an alternative to pick-and-place, e.g. if objects are too heavy or too large to be grasped. In addition, fragile objects that are likely to be damaged when grasped can be pushed. However, object pushing is demanding for robots due to the large variety of objects that all behave differently depending on their shape, size, mass, and friction. The task becomes even more challenging, considering that not all physical properties, such as mass or friction, are directly observable. These unknown properties can lead to the object overshooting its target position, requiring fast corrective movements of the robot around the object that are particularly difficult to model explicitly, as they require decisions about when a correction is necessary, how to adapt the pushing direction, and how to plan the corresponding movement. Corrections are even more challenging if the approach should generalize to objects with varying physical parameters. The difficulty of modeling such corrective movements is also evident in other recent work. Cong et al. [1] report that their model for object pushing based on a recurrent neural network (RNN) and model predictive control (MPC) cannot properly switch pushing sides, i.e. the model is not able to perform corrective movements. Additionally, the authors also train a RL agent as a model-free baseline. In contrast to the data-driven model, the RL agent learns to properly perform corrective movements. These results show that it is reasonable to further investigate RL in the context of object pushing. Cong et al. [2] therefore proposed a vision-proprioception model for planar object pushing, which is a state-of-the-art RL model that uses latent representations to encode the characteristics of an object. This vision-proprioception model is the starting point of this work. We investigate the following problem: a robot has to precisely push objects with varying physical parameters from starting positions to goal positions, both of which are randomly chosen. The main contribution of this work is the adaption of the vision-proprioception model to push objects more precisely to target positions by using fewer corrective movements around an object. Originally, the task was considered successful if the distance between the center of the object and the goal is smaller than 5 cm [2], which is a threshold that is commonly used in the literature [3]-[5]. However, this is a comparatively large tolerance, particularly if pushing is considered as an alternative for pick-and-place, which is often used to precisely reposition objects. Therefore, we decrease the threshold to 1 cm. The vision-proprioception model is adapted by providing the agent with the complete episode history of observations, using a gated recurrent unit (GRU) [6] as a feature extractor and improving the sampling of object parameters during training."}, {"title": "II. RELATED WORK", "content": "RL becomes increasingly important in the field of real-world robotic manipulation [7], [8]. One important sub class of manipulation is pushing, which has been heavily studied in the past. Pushing is often considered in combination with grasping, to solve a more complex task. More precisely, pushing and grasping can be combined to play the game Jenga [9] or in the case of object packing [10]. In other works, pushing is used to singulate objects in cluttered environments [11]\u2013[13], e.g. to free space around the object before grasping it [14]\u2013[17]. Not all approaches comprehensively model pushing actions. Pushing is often an additive skill to rearrange objects in the environment (see [15]). In all these works, pushing is usually not the main focus. Therefore, the objects mainly differ in their shape and size, but properties such as friction and mass are often not explicitly varied, as most of the work focuses on grasping. However, to precisely push objects to target positions, the sliding friction force is particularly important. Another part of the literature considers object pushing as a benchmark scenario for RL algorithms that focus on multi-goal RL [3], [4], [18], imitation learning [19] or joining multiple input modalities such as vision and touch [20]. In general, these benchmark scenarios are kept simple and object properties, such as shape, size, or friction are usually not changed. Therefore, training an agent that applies to real-world situations requires more sophisticated environments. In addition, there is a lot of work that only focuses on pushing. The approaches range from purely analytical methods to data-driven and deep learning approaches [21]. Some more recent methods focus on RL [2], [5], [22], [23]. However, precise pushing is not investigated, as the main focus is usually chosen differently, e.g. on pushing objects through cluttered scenes [22] or keeping objects upright while pushing [5]. Moreover, RNNS have already been used in the context of pushing [1], [24], [25], but none of these approaches investigate whether using a RNN in combination with RL leads to more precise control of the objects, especially in the case of small sliding friction forces."}, {"title": "III. SETUP", "content": "This work is based on the vision-proprioception model [2]. In contrast to the original paper, we use a Franka Emika Panda robot with 7 degrees of freedom and a push rod as the end effector (EE). A camera placed below a glass table provides RGB images of the object and the target. To safely train and test the agent without the risk of damaging hardware, a custom simulation environment is built based on Gymnasium [26] (formerly OpenAI Gym [27]) and the physics engine MuJoCo [28]. An overview of our setup in simulation is shown in Fig. 2 (left). Our code is available at: https://github.com/ubi-coro/precise_pushing"}, {"title": "IV. REINFORCEMENT LEARNING MODEL", "content": "We consider a finite-horizon Markov decision process (MDP), that is defined by a tuple $M = (S, A, p, r)$, where $S$ is the continuous state space, $A$ is the continuous action space, and $p:S\\times A \\times S \\rightarrow [0,\\infty)$ characterizes the unknown environment dynamics. The task considered in this work belongs to the field of goal-conditioned RL, as objects should be pushed to various goal positions. Formally, the MDP is augmented by a continuous goal space $G \\subset S$. At the beginning of an episode, a new goal $g \\in G$ is sampled from a probability distribution over all possible goals with probability density $p_g(\\cdot)$. The goal does not change within one episode. At each time step $t = 1,..., T_{max}$, an observation does not only contain information about the current state of the environment $s_t$ but also about the desired goal $g$, i.e. the agent observes a concatenation of $s_t$ and $g$, see [29]. In addition, the environment emits an immediate reward $r_{t+1} := r(s_t, a_t, s_{t+1},g)$ according to the reward function $r:S\\times A\\times S\\times G \\rightarrow \\mathbb{R}$. The objective is to find an optimal, parameterized policy $\\pi_{\\theta,\\zeta}$ that maximizes the expected goal-conditioned return:\n$\\pi_{\\theta,\\zeta}^* := \\underset{\\pi}{\\text{argmax}} \\mathbb{E}_{\\pi, p_g} \\bigg[ \\sum_{k=0}^{T_{max}-1} r_{k+1} \\bigg]$\n(1)\nThe subscripts $\\pi$ and $p_g$ denote that the agent follows the policy $\\pi$ and that $g$ is sampled from the probability distribution with probability density $p_g(\\cdot)$ over all possible goals, respectively."}, {"title": "A. Observation Space", "content": "An observation contains information about the EE position, the object, and the goal. From the RBG images generated by the camera, 64x64 binary images are obtained through color filtering. At the beginning of an episode, the goal mask is generated by placing the (blue) object at the target position. An autoencoder [30] is trained prior to the RL agent to reproduce binary images of objects with randomly selected shape, position, and orientation. The latent representations of the object and goal, produced by the encoder, denoted by $z^o, z^g \\in \\mathbb{R}^6$, are used as encodings of object and goal state. We use a 6-dimensional latent space, as Cong et al. [2] report that best results are achieved with this latent space dimension. Therefore, as shown in Fig. 2, the current state of the environment $s \\in \\mathbb{R}^{14}$ is obtained by concatenating the Cartesian planar position of the EE w.r.t. the base frame, denoted by $p^e := [x_e, y_e]^T \\in \\mathbb{R}^2$, and latent representations $z^o$ and $z^g$."}, {"title": "B. Feature Extraction", "content": "So far, the observation is similar to the original vision-proprioception model. It is important to note that the agent has no information about the sliding friction force between the object and the table, since the mass and friction coefficients of the object are not visible on the binary images and, therefore, not present in the latent representation $z^o$. However, to precisely reposition an object by pushing, information about the sliding friction force is of great importance. This information can be obtained by observing the distance travelled by the object when a force is applied: the smaller the sliding friction force, the larger the distance travelled. Using the original vision-proprioception model, the agent cannot derive this information, since it does not have access to information from previous observations, i.e. mathematically, the Markov property does not hold. In contrast, we use a GRU-layer [6] as a feature extractor for actor and critic networks, as shown in Fig. 2, to provide the agent with the possibility to extract relevant information about the object's behavior from the complete episode history. We chose a recurrent layer to ensure that the agent is able to handle input sequences of variable length. This is important since the agent can only derive information about the sliding friction force after an initial contact with the object and it should be able to memorize the derived information, as contact with the object is lost in the case of corrective movements. However, the number of time steps needed to approach the initial contact position or to perform a corrective movement can vary greatly depending on the situation. Therefore, variable-length inputs are crucial. Using the GRU feature extractor, the agent observes the complete history of observations of one episode up to the current time step $T < T_{max}$. Since the hidden state of a RNN encodes all relevant information from the past and the present, the hidden state of the current time step $T$ is used as the feature vector that is passed to a multi-layer perceptron (MLP). As the recurrent layer is trained together with the actor and critic MLP, the agent learns to decide which information has to be encoded in the hidden state. However, actor and critic networks use separate feature extractors and do not share weights. The initial hidden state of the GRU-layer is set to zero."}, {"title": "C. Action Space", "content": "An action $a := [a_x, a_y, a_s]^T$ is continuous and, in contrast to the original vision-proprioception model, three-dimensional, where $a_x, a_y \\in [-1,1]$ (unit: m) represent the desired EE position offset in the x and y directions of the base frame. $a_s \\in [10,600]$ denotes the number of MuJoCo simulation steps, i.e. the number of robot control cycles, for which to apply the same position offsets $a_x$ and $a_y$. To clarify the meaning of this parameter, we first note that we distinguish between MuJoCo simulation steps and Gymnasium environment steps. At the beginning of a Gymnasium environment step, the agent receives a new observation $s_t$, selects an action $a_t$, and passes a new desired target position of the EE to the controller which remains unchanged throughout the entire period of a single Gymnasium environment step. The time steps $t = 1,..., T_{max}$ previously mentioned in section IV correspond to these Gymnasium environment steps. The time that passes during one Gymnasium environment step is determined by two parameters. The first parameter is the number $a_s$ of MuJoCo simulation steps that are executed per Gymnasium environment step. The second parameter is the duration of a single MuJoCo simulation step. The latter corresponds to the control cycle of the robot controller and was fixed to 1 ms. Thus, the number of MuJoCo simulation steps $a_s$ determines the frequency of environment feedback that the agent receives as well as the time in which the robot controller can converge to the desired goal position. We include this number of simulation steps in the action space to avoid the need for manual tuning, as this parameter is usually crucial for training a RL agent due to the delayed reward problem."}, {"title": "D. Reward Specification", "content": "We use the following immediate reward function based on the ground truth object and goal position that was shown to yield the best results in [2]:\n$r(p^o, p^g) := \\begin{cases} -1, & \\text{if } ||p^o - p^g||_2 \\geq 0.01\\text{m} \\\\ 0, & \\text{otherwise} \\end{cases}$ (2)\nwhere $p^o := [x_o, y_o]^T \\in \\mathbb{R}^2$ and $p^g := [x_g, y_g]^T \\in \\mathbb{R}^2$ denote the planar position of the center of object and goal w.r.t. the base frame, respectively. It is important to note that this ground truth information is not part of the observation. The latter only includes the 6-dimensional latent feature vectors extracted by the vision encoder."}, {"title": "E. Episode Termination", "content": "As indicated by the reward function, an episode is successful if the Euclidean distance between the center of the object and the center of the target is smaller than 1cm in the final time step of an episode. Every episode takes 50 time steps, i.e. $T_{max} = 50$, regardless of whether the goal is reached after fewer time steps. This is important since the object has to remain close to the desired goal position until the end of an episode."}, {"title": "V. REINFORCEMENT LEARNING ALGORITHMS", "content": "The agent is trained using the Stable-Baselines3 [31] implementation of Hindsight Experience Replay (HER) [18] and soft actor-critic (SAC) [32]. HER increases the sample efficiency by storing additional transitions to the replay buffer. These transitions are generated by changing, i.e. relabeling, the desired goal and recomputing the reward assuming the new goal. To this end, we had to extend the HER implementation to also allow relabeling of non-observation data like the ground truth goal position. Additionally, it is important to note that the GRU feature extractor needs to recalculate all hidden states from $t = 1...T$ based on the new goal encoding. This also required adjustments to the HER implementation of Stable-Baselines3."}, {"title": "VI. VELOCITY CONTROLLER", "content": "Fig. 2 shows that the pusher should be perpendicular to the surface of the table to successfully push an object. This constraint is not included in the RL model since it complicates the learning problem, as the agent has to learn both the constraint and the pushing task. Instead, the constraint is solved by a custom velocity controller which moves the EE to a desired (x,y,z) position and keeps the push rod perpendicular to the table."}, {"title": "VII. DOMAIN RANDOMIZATION", "content": "We use domain randomization to ensure that the policy can be transferred to a real setup. To this end, the object parameters are sampled at the beginning of each episode from the ranges shown in Table I. All parameters, except the mass and sliding friction coefficient, are sampled uniformly at random. For the mass and the sliding friction, we use a special sampling distribution, introduced in the next section, to focus learning on the difficult scenarios of small friction forces. Additionally, the start and goal positions of an object are randomly chosen at the beginning of each episode. Even if the object's orientation is not considered in the reward function, the agent should be able to push objects with varying orientations. Since the object and goal are placed on a table, only the initial angle about the z-axis of the local frame has to be sampled. This angle is drawn uniformly at random from the range $[-\u03c0, \u03c0]$."}, {"title": "VIII. DISTRIBUTION OF THE SLIDING FRICTION FORCE", "content": "As already explained in section IV-B, the sliding friction force is of great importance when pushing objects. In our setup, the table is not tilted and since MuJoCo uses the maximum friction coefficient to compute the friction forces between any two objects, the sliding friction force $F_k$ between the table and the object is determined by the mass of the object $m_o$ (unit: kg) and the object's sliding friction coefficient $\u03bc_\u03ba$:\n$F_k = \u03bc_\u03ba\u00b7m_o\u00b7 9.81 \\text{ m/s}^2$ (3)\nObjects with a small mass and a small sliding friction coefficient, i.e. a small sliding friction force, are the most difficult to reposition precisely. Therefore, we improve the sampling of mass and sliding friction coefficient to ensure that small sliding friction forces are sampled more often during training. More precisely, since mass and sliding friction coefficient equally contribute to the sliding friction force, it is sufficient to sample only the mass value. To this end, minimum and maximum values $F_{k}^{\\text{min}}$ and $F_{k}^{\\text{max}}$ are calculated from the ranges of mass, $[m^{\\text{min}}, m^{\\text{max}}]$, and sliding friction coefficient $[\u03bc^{\\text{min}}, \u03bc^{\\text{max}}]$:\n$F_{k}^{\\text{min}} = m^{\\text{min}}\u00b7 \u03bc_\u03ba^{\\text{min}} \\text{ and } F_{k}^{\\text{max}} = m^{\\text{max}}\u00b7 \u03bc_\u03ba^{\\text{max}}$ (4)\nUsing a fixed friction coefficient $\u03bc_\u03ba = 0.4$, the mass has to be sampled from the modified range $\\Big[ \\frac{F_{k}^{\\text{min}}}{\u03bc_\u03ba}, \\frac{F_{k}^{\\text{max}}}{\u03bc_\u03ba}\\Big]$ to obtain values within the range $[F_{k}^{\\text{min}}, F_{k}^{\\text{max}}]$. The mass is sampled according to the following distribution:\n$m_o = \\bigg(\\frac{F_{k}^{\\text{max}}}{\u03bc_\u03ba}-\\frac{F_{k}^{\\text{min}}}{\u03bc_\u03ba}\\bigg) \\big((1-x)(1-y)+xy\\big) + \\frac{F_{k}^{\\text{min}}}{\u03bc_\u03ba}$, (5)\nwhere $x$ is sampled from an exponential distribution with scale parameter $\u03b2 = \\frac{1}{5}$ and clipped to the interval [0,1]. $y$ is obtained by sampling from a Bernoulli distribution with parameter $p = 0.5$. The use of the exponential distribution ensures that the largest values are obtained at the boundaries of the interval, i.e. small or large forces are more likely to be sampled. However, without further modifications, the density of an exponential distribution only decreases which makes it unlikely to sample large forces. Therefore, sampling from the Bernoulli distribution is required to mirror the samples from the exponential distribution by a probability of 50%."}, {"title": "IX. EVALUATION", "content": "We compare our model (eGRU) with the following base-lines to show that both main improvements, adding the GRU-layer and improving the sampling of the sliding friction force, are valuable and important for pushing objects precisely:\n\u2022 VPM: vision-proprioception model from [2], but using a smaller position threshold and an action space extended by $a_s$\n\u2022 Stacked: model shown in Fig. 2, but using the concatenation of the last 5 observations as a feature vector instead of the hidden state of the GRU-layer\n\u2022 uGRU: model shown in Fig. 2\n\u2022 eGRU (ours): model shown in Fig. 2, but trained using the improved sampling (see section VIII)\nAll agents are trained using a position threshold of 1cm and the expanded action space (see section IV-C). Actor and critic networks of the VPM and Stacked agent are MLPs with the same architecture as the MLP used for the two GRU agents (see Fig. 2). The VPM, Stacked, and uGRU agents are trained without the improved sampling of the sliding friction force, i.e. all object parameters are sampled uniformly at random from the parameter ranges shown in Table I. All agents are evaluated over 100 test episodes. We evaluate all models considering the most difficult case, i.e. objects with a small sliding friction force, by comparing the success rates, the mean episode rewards, and the mean number of corrections within one episode. Two types of corrective movements are distinguished, shown in Fig. 4. If the distance between the object and target is smaller than the position threshold in time step t and larger than the position threshold in the next time step t + 1, a corrective movement is necessary to push the object back to the target area. Therefore, these corrections are referred to as overshoot corrections. If the distance between the object and goal increases within one time step and decreases later within the episode, this correction is considered as the second type of corrective movement, i.e. a distance correction. In all experiments, the standard error of the mean (SEM) is used to show the variability of the estimated mean. The evaluation results depending on the shape of the object are shown in Fig. 5. The eGRU agent (ours) achieves the highest success rates, while the VPM model obtains one of the worst success rates. More specifically, the success rates of the eGRU agent are about 15% higher than those of the VPM agent, except for cuboids with a square base where all agents achieve approximately equal success rates. For all object shape configurations, the eGRU agent needs the least number of overshoot and distance corrections, often about half the number of corrective movements compared to the VPM agent. In addition, both GRU agents, especially the eGRU agent, also obtain larger mean episode rewards indicating that fewer episode time steps are required to successfully push the object to the goal position. This result can be explained by a smaller number of corrective movements usually needed by the GRU agents. The results also show that cuboids with a square base are easiest to control, as all agents achieve the best success rates while receiving the highest episode rewards, but require the fewest corrective movements compared to cuboids with a rectangular base and cylinders. In contrast, cuboids with rectangular bases are the most difficult to control. This is an expected result since the movement of these objects depends on the location of the contact point and whether the contact occurs on the smaller or the longer side of the rectangular base. These results show that our eGRU agent clearly outperforms all other baseline agents, especially the original vision-proprioception model."}, {"title": "B. Sim2Real", "content": "Additionally, we evaluate whether our model can be transferred to the real-world setup that is shown in Fig. 6. To this end, we chose four different objects, two of each shape (cylinder and cuboid), as shown in Fig. 6. Masses and sizes of these objects are summarized in Table II. The objects are made of polystyrene to make them as light as possible and thus create a challenging task for the agent. All models from the previous section are evaluated over eight episodes, two episodes with each object. The episodes mainly differ in the start and goal positions of the objects. As in the simulation, each episode takes 50 time steps to ensure that the results are comparable. We evaluate the success rate, the mean number of overshoot, and distance corrections. The mean episode reward is not considered, as the ground truth positions are not available in the real setup, such that the reward cannot be computed. For the same reason we judged the success of an episode based on visual inspection only. The evaluation results are shown in Fig. 7. Our eGRU agent clearly achieves the best success rate at around 90%, requiring the least number of both types of corrective movements. The other agents only obtain success rates between 25% and 50%, with the VPM agent requiring the highest number of corrective movements. When evaluating the agents with our real-world setup, we observed that the Stacked agent is often able to push the object to the target position at the beginning of an episode. Thus, the episode would be successful if the agent no longer interacts with the object in the remaining time steps. However, the agent then \"corrects\" the position of the object, which is not required. This explains the small success rate of the Stacked agent. These results show that our eGRU agent can be transferred to a real-world setup and that the simulation results can be clearly confirmed. A video of our eGRU agent is available at: https://youtu.be/ 7vD2qtGXxSw. Interestingly, the agent attempts to achieve not only the goal position of the object but also the goal orientation although this task was not explicitly encoded in the reward function."}, {"title": "X. DISCUSSION & FUTURE WORK", "content": "In this paper, we improved the vision-proprioception model, a state-of-the-art RL model, to push objects more precisely to target positions requiring fewer corrective movements. Our main contributions are an improved sampling of the sliding friction force during training and the addition of a GRU-layer to provide the agent with a memory. When applied to a real-world setup, our improved agent achieves a success rate that is about 50% higher and requires approximately 1/3 fewer corrective movements compared to the original vision-proprioception agent. We show that our model can push objects of two different shapes. However, these shapes are simple and our results clearly show that cuboids with a rectangular base are the most difficult to control. In the future, we plan to investigate imitation learning in the context of object pushing. It will be interesting to evaluate whether such an agent can push objects with more complex shapes. Since the agent would learn from real-world demonstrations, difficult scenarios such as cuboids with a rectangular base might be easier to learn. In addition, the vision-proprioception model assumes that the environment is fully observable, as the camera is placed below a glass table to avoid occlusions. However, a fully observable environment is a rather unrealistic assumption. Therefore, future work should focus on partially observable environments in which the objects might be occluded. Moreover, the policy in this paper provides position offsets, which do not ensure that the obtained motion is smooth. Thus, learning $C^2$-continuous control policies should be investigated in future work. Another limitation of this work is the lack of safety guarantees. Tasks that require object pushing are often performed close to people or even in collaboration with humans. Future work should therefore focus on manipulating objects while taking safety constraints into account."}]}