{"title": "Causal Responsibility Attribution for Human-AI Collaboration", "authors": ["Yahang Qi", "Bernhard Sch\u00f6lkopf", "Zhijing Jin"], "abstract": "As Artificial Intelligence (AI) systems increasingly influence decision-making across various fields, the need to attribute responsibility for undesirable outcomes has become essential, though complicated by the complex interplay between humans and AI. Existing attribution methods based on actual causality and Shapley values tend to disproportionately blame agents who contribute more to an outcome and rely on real-world measures of blameworthiness that may misalign with responsible AI standards. This paper presents a causal framework using Structural Causal Models (SCMs) to systematically attribute responsibility in human-AI systems, measuring overall blameworthiness while employing counterfactual reasoning to account for agents' expected epistemic levels. Two case studies illustrate the framework's adaptability in diverse human-AI collaboration scenarios.", "sections": [{"title": "1. Introduction", "content": "As Artificial Intelligence (AI) systems increasingly influence decision-making in critical sectors such as healthcare (Budd et al., 2021), finance (Cohen et al., 2023), and autonomous driving (Badue et al., 2021), the need to clearly define and attribute responsibility when outcomes are undesirable or failures occur becomes crucial (Santoni de Sio and Van den Hoven, 2018). The integration of AI complicates traditional accountability mechanisms due to the shared decision-making responsibilities between humans and algorithms. Human-AI systems pose unique challenges for responsibility attribution, given their interactive, complex, and high-stakes nature (Amershi et al., 2019). On one hand, the ability of humans to override or modify AI-driven decisions introduces an element of unpredictability in outcomes. On the other hand, AI decisions often rely on huge datasets or complex models that lack full transparency, making it difficult for human counterparts to fully understand or anticipate AI behaviour.\nTo address these challenges, various methods have been proposed for responsibility attribution in multi-agent settings. Chockler and Halpern (2004) and Halpern and Kleiman-Weiner (2018) introduced frameworks to quantify blameworthiness based on causal relationships. This definition was further extended to multi-agent settings by attributing blameworthiness through the Shapley value, as demonstrated by Friedenberg and Halpern (2019). Additionally, responsibility attribution has been studied in decentralized, partially observable Markov decision processes using the concept of actual causality (Triantafyllou et al., 2022). These approaches rely on actual causality (Halpern, 2016) to measure the degree of blameworthiness among agents."}, {"title": "2. Related Work", "content": "2.1. Responsibility Attribution\nThe need for judging moral responsibility arises both in ethics and in law. However, these notions are notoriously difficult to define carefully. One famous example is the trolley-problem (Thomson, 1984), which is a moral dilemma in which one must decide whether to pull a lever to divert a runaway trolley onto a track where it will kill one person, rather than allowing it to continue on its current track, where it will kill five. To formally define moral responsibility, there is general agreement that a definition of moral responsibility will require integrating causality (Chockler and Halpern, 2004), intention (Cushman, 2015), knowledge (Malle et al., 2014). To formally define these, structural causal models provide a powerful tool (Halpern and Pearl, 2005). Using this framework, Halpern and Kleiman-Weiner (2018) defined the degree of blameworthiness in terms of actual causality. Furthermore, the degree of blameworthiness can be discounted by the cost of the action. This notation of blameworthiness is further generalised into multi-agent setting by ascribing blameworthiness to the group of agents relative to an epistemic state of potential outcomes (Friedenberg and Halpern, 2019). The degree of blameworthiness for individuals can be attributed using the Shapley value, a notion from cooperative game theory (Shapley, 1953). However, when using Shapley value for responsibility attribution, agents which contributes more to the task may end up being blamed more, which may not align with intuitive notions of responsibility in collaborative settings (Kumar et al., 2020).\nIn the regime of AI, Franklin et al. (2022) outlined a causal framework of responsibility attribution which integrates nine factors: causality, role, knowledge, objective foreseeability, capability, intent, desire, autonomy, and character. The causal relationship between these nice factors and the responsibility is extensively discussed. However, no explicit mathematical formulation is given. Regarding multi-agent setting, Triantafyllou et al. (2022) proposed a way to attribute the responsibility in decentralized partially observable Markov decision processes. However, these frameworks do not account for agents' expected epistemic levels, which is not helpful to build reliable and trustworthy AI systems (Bostrom and Yudkowsky, 2018)."}, {"title": "2.2. Human-AI Collaboration", "content": "The integration of AI in critical fields like healthcare (Budd et al., 2021), finance (Cohen et al., 2023), and autonomous driving (Badue et al., 2021) has driven the development of structured human-AI collaboration frameworks (Wang et al., 2020). These frameworks generally fit into three modes, which define the balance of human and AI roles: AI-centric, Human-centric, and Symbiotic (Fragiadakis et al., 2024).\nIn the AI-centric mode, AI systems take the lead, performing tasks with minimal human input. This mode focuses on maximizing computational efficiency, where AI operates autonomously in contexts like agentic systems (Shavit et al., 2023) or complex data predictions, such as protein structures (Jumper et al., 2021). Human-centric frameworks, often called human-in-the-loop, maintain human oversight as central, employing AI as an auxiliary tool to manage data-heavy or repetitive tasks. This mode supports fields where human judgment is crucial, like diagnostics in healthcare (Chaddad et al., 2023) and assisted driving (Badue et al., 2021). The Symbiotic mode promotes a balanced partnership, with human and AI systems sharing decision-making and complementing each other's strengths. This setup enables mutual feedback and close collaboration, ideal for tasks requiring both AI's computational power and human intuition, such as creative co-production (Rezwana and Maher, 2023).\nAs these systems become more prevalent, they underscore the need for responsibility attribution frameworks that address the complexity of human-AI interactions and evaluate responsibility against the epistemic standards expected from trustworthy AI systems.epistemic standards necessary for reliable and trustworthy AI systems.."}, {"title": "3. Preliminaries", "content": "This section introduces the fundamental concepts and mathematical tools utilised in our study, specifically focusing on Structural Causal Models (SCMs), interventional counterfactual, and backtracking counterfactual."}, {"title": "3.1. Structural Causal Models (SCMs)", "content": "SCMs (Pearl, 2009) offer a structured framework for modelling and analysing causal relationships between variables. We introduce SCMs below, using notation adapted from Bongers et al. (2021) and Peters et al. (2017).\nDefinition 1 (Structural Causal Model (Bongers et al., 2021)) A structural causal model (SCM) is a tuple $M = (I,J,X,E, f,P(E))$, where\n\u2022 I and J are disjoint finite index set of endogenous and exogenous variables, respectively.\n\u2022 The domains $X = \\prod_{i \\in I} X_i$ and $E = \\prod_{j \\in J} E_j$ are products of standard Borel space.\n\u2022 The exogenous distribution $P(E) = \\prod_{j \\in J}P(E_j)$ is the product of probability distributions.\n\u2022 The causal mechanism $f : X \\times E \\rightarrow X$ is a measurable function.\nThe solutions of an SCM in terms of random variables are defined up to almost sure equality."}, {"title": "3.2. Counterfactual", "content": "Counterfactual is used to reason about what would happen to a system if we were to intervene and change some of its variables (Peters et al., 2017). In the context of an SCM, an intervention that sets a variable X to x is denoted as $do(X = x)$. The effect of this intervention on another variable Y is computed by modifying the structural equations in the SCM and propagating the effects.\nDefinition 3 (Counterfactuals) Consider an SCM $M = (I, J,X,E, f,P(E))$ and its solution X. Given some observations $x \\in X$, we define a counterfactual SCM $M' = (I,J,X,E, f,P(E|X = x))$ by replacing the distribution of noise variables with the distribution conditioned on observation $X = x$, denoted as $P(E|X = x)$."}, {"title": "4. A Causal Framework of Responsibility Attribution", "content": "This section presents the formal definitions and mathematical formulations for undesirable outcomes and the degree of blameworthiness of an action relative to an alternative action, both of which are fundamental to our causal responsibility attribution framework. We begin by defining these key terms, followed by a formal definition of the overall degree of blameworthiness in a human-AI decision-making system. Finally, we focus on specific outcomes and demonstrate how to attribute blameworthiness to different parties based on their epistemic levels, aligned with responsible AI standards and represented by a probability measure over potential outcomes.."}, {"title": "4.1. Degree of Blameworthiness", "content": "Consider a system whose causal mechanism is defined by the SCM $M = (I, J, X, E, f, P(E))$. In this system, an agent takes action and may affect the probability of occurrence of some unwanted outcome 4, which is defined as\nDefinition 4 (Outcome in an SCM) Consider an SCM $M = (I, J, X,E, f, P(E))$ with a solution X. An outcome $ is defined as a measurable function $ \\phi : X \\rightarrow {0,1}$, where $ \\phi = 1$ if and only if X takes a specified value or falls within a designated subset of X. That is, the outcome $ \\phi = 1"}, {"title": "4.2. Human-in-the-Loop Decision-Making Systems", "content": "The human-in-the-loop decision-making system, illustrated by Figure 2, can be understood through a causal perspective, where both an AI model and human intervention shape the final decision. The process starts with an input variable, X, representing the data fed into the system. This data influences the AI model's output, M, which includes a confidence measure. When the model is confident, its decision flows directly to the final decision output, Y, bypassing human review. However, if the model is uncertain, it passes the decision to a human reviewer, represented by H, who considers both the initial data X and the model's output M before making a judgment. The final decision, Y, is thus causally determined by either the AI model alone (if confident) or by the combined influence of the model and human input (if the model is unsure).\nIn this framework, two main causal pathways affect the final decision: a direct path where the model's confident output directly influences Y, and a human-intervention path where the human review shapes Y when the model is uncertain. In a decision system $M = (I,J,X,E, f, P(E))$. The decision-making policy f can be modified by actions, which include deploying human-in-the-loop policy, denoted as a, and deploying human-only policy, denoted as a'. Deploying human-in-the-loop policy can increase the efficiency of the decision-making system, at the price of reducing performance. The undesired outcome in such system can be making a wrong decision, denoted as 4. Using Definition 6, the degree of blameworthiness can be measure by\n$DB(\\alpha, \\alpha') = \\gamma(\\alpha, \\alpha')\\delta(\\alpha, \\alpha')$, (4.4)\nwhere $\\delta(\\alpha, \\alpha') = max \\{0, P(\\phi = 1|M^a) \u2013 P(\\phi = 1|M^{a'})\\}$. Here, $\\delta(a, a')$ measures the extent of deploying HITL policy $f^a$ will increase the probability of making wrong decisions, compared with using human-only policy $f^{a'}$."}, {"title": "4.3. Attribution of Responsibility for a Specific Undesired Outcome", "content": "Definition 4.4 provides a measure of the overall degree of blameworthiness for an undesired outcome. However, we also aim to attribute responsibility for specific undesired outcomes. For a given outcome 40, responsibility is attributed by assessing whether it could have been avoided if an agent had taken an alternative action. In a human-AI collaboration setting, this analysis is not based on real-world probability measures but on the epistemic level the AI agent is expected to maintain to align with responsible AI standards, represented through an adjusted probability measure. In human-only decision systems, responsibility attribution is straightforward as only one agent is involved. Thus, our focus here is on analysing decision-making systems under the HITL policy, where responsibility attribution becomes more complex."}, {"title": "Definition 7 (Inevitable and avoidable outcomes)", "content": "Consider a decision-making system represented by the SCM $M = (I, J, X, E, f, P(E))$, where the decision-making policy is modified by action a under the HITL policy and by action a' under the human-only policy. For a specific undesirable outcome $ \\phi_0 = 1$, it can be categorized as follows:\n\u2022 Inevitable outcomes: The event $ \\phi_0 = 1$ also occurs in the human-only decision-making system $M^{a'}$.\n\u2022 Avoidable outcomes: The event $ \\phi_0 = 1$ would not occur in the human-only decision-making system $M^{a'}$.\nFurthermore, in the HITL decision-making system, the inevitable outcomes can be categorized as\n\u2022 Flagged outcomes: event $ \\psi_o = 1$, and\n\u2022 Unflagged outcomes: event $ \\psi_o = 0$,\nwhere 0 is 1 when the AI has requested intervention from human and 0 if not.\nIn this definition, inevitable outcomes are those beyond the decision-making capabilities of both human and AI agents, while avoidable outcomes are undesired outcomes that could have been prevented had the AI requested human intervention. Inevitable outcomes can be further categorized as flagged or unflagged, a distinction that may initially seem redundant, as counterfactual reasoning implies the outcome remains unchanged regardless of whether the AI flagged the case. However, this classification is crucial for accurately identifying responsible parties and attributing accountability, aligning with responsible AI standards."}, {"title": "Definition 8 (Responsibility attribution for different types of outcomes)", "content": "Consider a decision-making system represented by SCM $M = (I, J, X,E, f, P(E))$, its decision-making policy is modified by action a under the HITL policy, and is modified by action a' under the human-only policy. For a specific undesired outcome $ \\phi_0 = 1$, responsibility is attributed as follows:\n\u2022 Inevitable outcomes:\nFlagged: Responsibility lies with the human who made the decision.\nUnflagged: Responsibility is attributed to both the AI making the decision and the party responsible for designing the flagging mechanism.\n\u2022 Avoidable outcomes: Responsibility is assigned to both the AI that made the decision and the party who designed the flagging mechanism.\nFor avoidable outcomes, responsibility is shared between the AI that made the decision and the party who designed the flagging mechanism, as these outcomes could have been prevented if the AI had flagged the case or if the flagging mechanism had been more effective. For inevitable outcomes, the human ultimately bears responsibility, as the system's limitations are constrained by human capabilities. However, it may seem counterintuitive for the AI to also bear responsibility for unflagged inevitable outcomes, since the act of flagging by the AI has no causal effect on the final outcome. This responsibility is not measured by the real-world probability measure P(E), but rather by a probability measure aligned with responsible AI standards, which assumes that the AI should always consider the potential benefit of requesting human intervention to reduce the likelihood of undesirable outcomes."}, {"title": "5. Case Study 1: Essay Grading with Large Language Models", "content": "5.1. Background\nThe recent advancements in Large Language Models (LLMs) have enabled their deployment in a variety of educational applications, including automated essay grading (Mizumoto and Eguchi, 2023). LLMs trained on extensive text corpora demonstrate a remarkable ability to evaluate written content, often producing assessments comparable to those of human graders. However, challenges arise when these models encounter atypical or nuanced writing styles, cultural references, or specific linguistic constructs outside of their training distribution, which may lead to inaccurate grading or biased evaluations (Tao et al., 2024).\nIntegrating LLMs into essay grading systems offers significant potential for efficiency, especially in handling large volumes of student essays. Nevertheless, such integration risks introducing variability in grading quality, particularly in cases where LLMs lack sufficient understanding of complex or context-dependent language use. In this case study, we demonstrate how to assess the degree of blameworthiness using the framework proposed in Section 4.\n5.2. Implementation\nIn this case study, we evaluate a human-AI collaboration system for essay grading, in which human and AI agents work together to score student submissions. We utilize the ASAP-AES dataset (Hamner et al., 2012), specifically focusing on Set-1, which includes persuasive, narrative, or expository essays written by 8th-grade students on the societal impacts of computer use. In response to a prompt, students write a persuasive letter to a local newspaper, arguing whether computers benefit or harm society, with the goal of convincing readers. Essays are graded on a rubric from 1 to 6, based on criteria such as content development, organization, detail, fluency, and audience awareness. Each essay receives two independent scores, and when these scores are close (adjacent), they are summed to determine the final score. For significantly different scores (non-adjacent), an expert adjudicator provides a resolved score.\nTo test the human-AI collaboration framework, we replaced one human assessor with GPT-4 (Achiam et al., 2023). The model was provided with the grading rubric and prompted to assign a score based on the essay content. The final grade was calculated by summing GPT-4's score with that of the human assessor, as illustrated in Figure 3."}, {"title": "5.3. Results", "content": "Using the framework from Section 4, we assess the degree of blameworthiness in the human-AI collaborative grading system by comparing the AI-enhanced scores with ground truth scores through the quadratic weighted kappa (QWK) metric (Cohen, 1968). QWK is a widely used measure of agreement between raters in educational assessment, particularly suited for ordinal scoring tasks like essay grading. This metric penalizes larger discrepancies between scores more heavily, providing a nuanced view of rating consistency. QWK values range from -1 (indicating complete disagreement) to 1 (perfect agreement), with 0 implying no agreement beyond chance. The human-AI grading system achieved a QWK of 0.478, yielding an overall degree of blameworthiness of 1 0.478 = 0.522.\nThis analysis highlights that while integrating LLMs in human-AI grading systems can improve efficiency, it also risks introducing grading inconsistencies. By systematically attributing responsibility, our framework proves valuable in assessing these systems' reliability and accountability."}, {"title": "6. Case Study 2: Human-in-the-Loop Pneumonia Detection Using Chest X-ray Images", "content": "6.1. Background\nRecent advances in AI have significantly enhanced automated medical diagnosis systems, achieving performance levels increasingly comparable to human clinicians (Zhou et al., 2023; Komorowski et al., 2018). However, these models struggle with out-of-distribution cases, where limitations in generalization can lead to diagnostic errors, especially on samples outside the model's training data. Integrating AI into medical diagnostics offers efficiency gains through rapid processing of large datasets but may introduce new risks, potentially impacting overall system performance. To address these risks, quantifying responsibility for errors between human and AI agents becomes crucial. In this case study, we illustrate how to assess blameworthiness for undesirable outcomes. Using a chest X-ray dataset of 5,863 images labelled as Pneumonia or Normal (Kermany et al., 2018), we deploy a human-in-the-loop diagnostic system to detect pneumonia based on chest X-rays\n6.2. Implementation\nIn this case study, we examine a human-in-the-loop (HITL) diagnostic system for detecting pneumonia from chest X-ray images. For reproducibility and to simulate different diagnostic capabilities, we use two distinct AI models to represent the roles within the system: a stronger model, ResNet-18 (He et al., 2016), mimicking the diagnostic capabilities of a human doctor, and a weaker model, a standard Convolutional Neural Network (CNN; Fukushima, 1980), representing the AI component of the system.\nAs shown in Figure 4, the HITL system operates as follows: each chest X-ray image is first processed by the weaker AI model, i.e., CNN in this case, which assesses its confidence in making a diagnosis, which is measured by the probability p of detecting pneumonia and pre-defined threshold l and u, where 0 < 1 < u < 1. If the model is confident in its prediction (p > u or p < l), it proceeds to make a decision autonomously. However, if the model detects high uncertainty (l < p < u), it flags the case and requests human intervention, simulating a collaborative decision-making process. This setup allows us to investigate cases where the AI's autonomy may lead to errors, as well as scenarios where human oversight is involved in the decision."}, {"title": "6.3. Results", "content": "Applying the framework to the HITL pneumonia diagnostic system, we measure the overall degree of blameworthiness and count for each category of undesired outcomes. The performance of the diagnosis system is measure using F1-score, which is a metric in classification tasks to evaluate a model's accuracy, especially in cases where the data is imbalanced. It is defined as the harmonic mean of precision and recall:\nF1-score = 2*Precision*Recall/(Precision + Recall), (6.1)\nwhere\nPrecision = TP/(TP + FP), Recall = TP/(TP + FN) (6.2)\nFollowing the deployment of a human-in-the-loop (HITL) policy, the diagnostic system's F1-score decreased from 0.896 to 0.831, with this drop indicating the degree of blameworthiness associated with implementing the HITL system.\nFor inevitable outcomes, errors persist despite human oversight, revealing cases where both human and AI agents struggle with challenging, out-of-distribution data. Specifically, 2 inevitable errors were flagged by the AI for human intervention, while 83 inevitable errors were not flagged, suggesting overconfidence in the AI's predictions. A well-calibrated AI model and an improved flagging mechanism could reduce responsibility for inevitable cases that are not flagged.\nAmong the 73 avoidable errors, responsibility falls primarily on the AI and those responsible for designing the flagging mechanism, as these errors could have been prevented if the AI had flagged them for human intervention. Here, the human is not blameworthy, since these errors were due to the AI's failure to request intervention.\nOverall, this analysis reveals that, while the HITL approach can improve efficiency, it introduces risks when the AI is overconfident in its predictions and the flagging mechanism lacks precision. By precisely defining responsibility for specific outcomes, this study demonstrates the utility of our framework in evaluating accountability for both AI and human agents within HITL diagnostic systems."}, {"title": "7. Conclusion", "content": "This paper presented a structured causal responsibility attribution framework designed for human-AI collaboration decision-making systems, where AI models and human agents collaborate to make critical decisions. As AI integration continues to expand in sectors such as healthcare and education, the need for precise responsibility attribution becomes increasingly important, particularly for undesired outcomes that can impact trust, accountability, and ethical AI deployment.\nThe formalization of outcomes and blameworthiness in our framework provides a rigorous approach to assess the impact of an agent's actions on undesired outcomes, enabling a systematic evaluation of responsibility. By defining an outcome as a measurable function within an SCM, we quantify blameworthiness by comparing the probabilities of outcomes under different actions. We also introduced a discounted blameworthiness measure, which adjusts for the efficiency improvements that an action might bring, reflecting a balanced view of accountability where both performance and cost are considered.\nOur framework distinguishes between inevitable and avoidable outcomes, with avoidable outcomes further divided into flagged and unflagged categories. This classification enables precise responsibility attribution: inevitable outcomes are attributed to limitations inherent to both AI and human agents, while avoidable outcomes specify responsibility based on whether human intervention was flagged or bypassed. Specifically, responsibility for avoidable errors may lie with the AI, the human agent, or the designers of the flagging mechanism, depending on whether intervention was requested. This approach addresses key issues in existing methods, such as Shapley values, which tend to disproportionately blame agents contributing more to an outcome and rely on real-world measures of blameworthiness that may not align with responsible AI standards.\nUsing this approach, we demonstrated the framework's applicability through two case studies: ASAP-AES essay grading and pneumonia detection from chest X-ray images. These examples illustrate how our framework systematically assesses causal responsibility for each agent within HITL systems, offering insights to inform future improvements in AI design and human-AI collaboration strategies. However, this study does not address measuring blameworthiness for sequential decisions that align with responsible AI standards, which remains a direction for future research.\nOur causal responsibility attribution framework represents a step forward in accountability for human-AI collaboration systems, promoting transparent and ethical AI deployment in high-stakes settings. By integrating causal reasoning with aligned with responsible AI standards, this work paves the way for responsible human-AI collaboration, laying a foundation for trust in increasingly autonomous systems."}]}