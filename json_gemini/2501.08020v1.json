{"title": "Cooperative Patrol Routing:\nOptimizing Urban Crime Surveillance through Multi-Agent\nReinforcement Learning", "authors": ["Juan Palma-Borda", "Eduardo Guzm\u00e1n", "Mar\u00eda-Victoria Belmonte"], "abstract": "Patrolling can be defined as the act of visiting locations of interest at regular intervals for either\nsurveillance, control, protection, or monitoring purposes. The effective design of patrol strate-\ngies is a difficult and complex problem, especially in medium and large areas. The objective is to\nplan, in a coordinated manner, the optimal routes for a set of patrols in a given area, in order to\nachieve maximum coverage of the area, while also trying to minimize the number of patrols. In\nthis paper, we propose a multi-agent reinforcement learning (MARL) model, based on a decen-\ntralized partially observable Markov decision process, to plan unpredictable patrol routes within\nan urban environment represented as an undirected graph. The model attempts to maximize a\ntarget function that characterizes the environment within a given time frame. Our model has\nbeen tested to optimize police patrol routes in three medium-sized districts of the city of Malaga.\nThe aim was to maximize surveillance coverage of the most crime-prone areas, based on actual\ncrime data in the city. To address this problem, several MARL algorithms have been studied,\nand among these the Value Decomposition Proximal Policy Optimization (VDPPO) algorithm\nexhibited the best performance. We also introduce a novel metric, the coverage index, for the\nevaluation of the coverage performance of the routes generated by our model. This metric is in-\nspired by the predictive accuracy index (PAI), which is commonly used in criminology to detect\nhotspots. Using this metric, we have evaluated the model under various scenarios in which the\nnumber of agents (or patrols), their starting positions, and the level of information they can ob-\nserve in the environment have been modified. Results show that the coordinated routes generated\nby our model achieve a coverage of more than 90% of the 3% of graph nodes with the highest\ncrime incidence, and 65% for 20% of these nodes; 3% and 20% represent the coverage standards\nfor police resource allocation. The source code of our implementation is available in this public\nrepository (https://github.com/iacomlab/marl-patrol-routing). The data cannot be provided for\nconfidentiality reasons, as they contain sensitive information.", "sections": [{"title": "1. Introduction", "content": "Patrolling can be defined as the act of visiting locations of interest at regular intervals for\nsurveillance, control, protection, or monitoring purposes (Machado et al., 2002b; Guo et al.,"}, {"title": "2.1. Multi-Agent Reinforcement Learning", "content": "MARL stands as a relevant framework for acquiring the agent policies required to accomplish\nspecific tasks across diverse domains, encompassing disaster response (Parker et al., 2016), au-\ntonomous vehicles (Shalev-Shwartz et al., 2016), or multiplayer games (Wijaya and Maulidevi,\n2019; Samvelyan et al., 2019). To complete this task, several algorithms can be found in the\nliterature, principally grouped into 4 families: Proximal Policy Optimization (PPO) (Schulman\net al., 2017), Deep Q-Network (DQN) (Mnih et al., 2015) or Deep Deterministic Policy Gradient\n(DDPG) (Lillicrap et al., 2015), Advantage Actor Critic (A2C) (Mnih et al., 2016), and Trust\nRegion Policy Optimization (TRPO) (Schulman et al., 2015a).\nThe selection of the most suitable algorithm for a certain problem to be solved is generally con-\nditioned by the limitations of each model. Some algorithms may not be applicable to continuous\nor discrete action spaces or may not be designed for competitive or for cooperative problems.\nFor cooperative problems, the approaches and constraints of MARL algorithms are even more\nimportant, primarily due to the necessity of coordinating various agents to achieve a certain task\nor several tasks, and many of them are only achievable when agents perform various tasks in\na coordinated manner. This coordination of agents is not trivial, especially in problems with\nhomogeneous agents, where there are no associated roles, which would allow decomposing the\nproblem into subtasks that agents in each role could learn. Finally, some recent approaches have\nintroduced MARL algorithms for solving problems with incomplete information, as a way to\ntrain agents in particular environments such as natural disasters. These approaches perform well\ncompared to usual solving techniques, especially if they are complemented with expert knowl-\nedge of the subject (Lee and Lee, 2021)."}, {"title": "2.2. Patrol Route Design", "content": "The route design problem focuses on planning the path of one or more patrols, depending\non the selected area of action and the available resources. This task aims to achieve maximum\narea coverage at minimal cost in a coordinated manner between assigned patrols, thus giving\ncoordination an essential role. In addition, in some cases it may be necessary to introduce a\ncertain degree of randomness Yin et al. (2012).\nOne of the main areas of application of the patrol route design problem is robotics, more\nspecifically multi-robot patrolling, which is perhaps the most explored field in terms of the study\nof routes within a certain area. In the literature, various techniques for planning these robots\nusing centralized or distributed coordination can be found (Huang et al., 2019). Centralized co-\nordination includes those methods managed by a central coordinator (Machado et al., 2002a;\nAlmeida et al., 2003; Othmani-Guibourg et al., 2017) or cyclic strategies (Chevaleyre, 2004)\nwhere robots are arranged on a closed path. For instance, Pasqualetti et al. (2012a) used this\ntechnique to generate a route to visit all important locations. Partition-based strategy (Cheva-\nleyre, 2004) is another centralized coordination strategy, where the area is divided into disjoint\nregions, similarly to the environment clusterization but in smaller regions. Using this strategy,\neach agent covers one region of the divided area, having as many regions as agents. Portugal and\nRocha (2010) establishes a multilevel subgraph patrolling (MSP) algorithm by modifying the\npartitioning phase, thus achieving a reduction in redundant work compared to cyclic approach.\nOn another note, Sea et al. (2018) uses k-means clustering to divide the regions and find the\nshortest path using a simulated annealing algorithm. Lastly, Stranders et al. (2013) presents\nseveral environments based on the elaboration of patrol routes on graphs using both multi- and"}, {"title": "3. Methodology", "content": "The patrolling problem in an urban environment can be classified as a decentralized partially\nobservable Markov decision process (dec-POMDP), also generalizable to multi-agent markov\ndecision processes (MMDPs), depending on whether the agents are allowed to know the entire\nstate of the environment in their observations. In our model, the problem is formulated as a dec-\nPOMDP with a tuple < I, S, A, \u0393, R, \u03a9, \u039f, \u03b3 > (Oliehoek et al., 2016), where I = {a\u2081, a\u2082, ...,a\u2099}\nis the society of N patrolling agents; S is the representation of the environment through which\nagents are able to move, represented as a set of states; A is a function modeling the set of ac-\ntions an agent can perform; \u0393 is the set of conditional transitions between states, defined as\n\u0393(S\u209c\u208a\u2081|S\u209c, a\u2081); R is the reward function, defined as R(s\u209c, a\u2081); \u03a9 is the set of observations; O is the\nprobability of these observations; and \u03b3 is the discount factor."}, {"title": "3.2. The Environment Representation", "content": "The monitoring area of our proposal could be an urban environment that is initially trans-\nformed into a grid and subsequently converted into an undirected graph. The urban space is thus\nparceled into nodes, connected among them in terms of their walkability. This approach is one\nof the most common options in the field of spatial representation of urban environments (Devia\nand Weber, 2013; Birks et al., 2008). Consequently, each node will represent a cell of the grid,\nwith edges connecting neighboring cells that share a common road. The use of a grid as the basis\nof our graph also implies that the distance between the nodes in the real environment is the same,\nsimplifying any consideration of choosing one path over another.\nFormally, the environment graph can be represented as a tuple S =< V, E, C,\u03c1, \u03c3 >, where\nV = {v\u2081, v\u2082, ..., vM} is the set of the M nodes of the graph. E: V\u00d7 V \u2192 {0, 1} is the mobility\nfunction, which is also commutative, i.e., E(v\u1d62, v\u2c7c) = E(v\u2c7c, v\u1d62). E(v\u1d62, v\u2c7c) = 1 if an agent can transit\nfrom the node v\u1d62 to the node v\u2c7c and vice versa; otherwise, E(v\u1d62, v\u2c7c) = 0. C is the subset of nodes,\nC\u2286 V, that have to be monitored. \u03c1\u1d62 : I \u2192 V is the location function relating each agent with"}, {"title": "3.3. Actions", "content": "The actions in the model consist mainly of the movements of the agents. An agent can only\nmove to any of the nodes directly connected to the one on which it is. In addition, it can remain\non the same node. Let \u03b4: V \u00d7 V \u2192 \u2115\u207a be a distance function between nodes that can be denoted\nas:\n```latex\n\\delta(v_i, v_j) = \\begin{cases}\n0 & : v_i = v_j \\\\\n1 & : E(v_i, v_j) = 1 \\\\\n1 + \\min_{v_k \\in V, v_k \\neq v_i} \\delta(v_k, v_j) & : E(v_i, v_j) = 0 \\land \\exists v_k \\in V,\nE(v_i, v_k) = 1 \\forall i \\neq k\n\\end{cases}\n```\n\u03b4 measures the distance between two nodes of the graph. If the nodes are adjacent, its value is\n1; otherwise, this function will calculate the shortest distance between these nodes. The action\nfunction of the model, A: I \u00d7 V \u2192 V, can also be denoted, as can be seen in Equation 2.\n```latex\nA(a_i, v_j) = v_k, \\delta(v_j, v_k) \\leq 1\n```\nThe action function represents the movements that an agent can make. Thus, the agent can remain\nat its current node or move only to neighboring nodes, i.e., those with which it is connected via\na path."}, {"title": "3.4. Observations", "content": "Our model is designed for environments where agents may have partial information about the\nenvironment, i.e., we focus on problems with partially observable environments. Observations\ndetermine the information available to an agent when making a decision. The existence of a\nsingle type of agent in our model entails that all of them receive the same type of information\nto make a decision. The reason to withhold environmental information from patrols is largely in\nline with the argument put forward by Santana et al. (2004). Firstly, it simplifies the problem, ac-\ncelerating the training process and facilitating the convergence of the model. Secondly, it sets up\nthe environment more realistically as agents may not have complete information about its state.\nThird, it allows subsequent modifications of the training environment without compromising the\nsize of the observation space. For these reasons, we introduce the concept of agents' line of sight\nas a way to determine what information of the environment state is known by the patrols. As will"}, {"title": "3.5. Rewards", "content": "This problem falls within the scope of cooperative problem solving as the reward is not the\nsame for all agents when performing a particular action. Moreover, the main difficulty of this\nproblem lies in introducing a reward that allows the agent to adequately learn the task we want\nthe model to perform. Unlike other reinforcement learning problems, where the agent clearly\nknows when it wins (it receives a reward) or loses (it does not receive a reward or receives a\npenalty), here there is no clear indicator that we can easily use to train the task of maximizing\narea coverage weighted by the target value of each area, which has been a challenge in the design\nof the reward function.\nFor this reason, we explored different approaches to the reward function in terms of its effect\non agent training. First, we considered the sum of the objective values of all nodes monitored\nby the agents at each point in time, without taking into account the number of visits to a node.\nThis resulted in all agents moving to a node with a high objective value and remaining there,\nsince it was the node that gave them the highest reward. The goal of the problem, if we were to\ntranslate it to the real world, would be to coordinate the agents to search for the highest target\nvalue nodes in the environment rather than the optimal routes. Then, we modified that reward\nvalue by dividing it by the number of visits. This resulted in all agents moving to an area (a\ngroup of neighbor nodes) with a high target value and staying there. The two problems with\nthis behavior are, firstly, that they may not mind passing many nodes outside the area to reach\ntheir target faster and, secondly, that they may not cover the most isolated hotspots in the model,\nresulting in certain areas being overwatched. Finally, the (cooperative) reward function we have\nused in our model is denoted in Equation 4. The function setting the final reward for taking an\naction at a particular moment will be the sum of the rewards of all agents at that moment, plus\nthe added reward for the individual agent. This is aimed at minimizing the occurrence of lazy\nagents (those who wait for others to perform beneficial actions) among the group of agents.\n```latex\nR_t(a_i) = R'(a_i) + \\sum_{1 < j < N} R'(a_j)\n```"}, {"title": "3.6. Model Parameters", "content": "The specific parameters used in the training of the model can be seen in Table 2. Those not\nlabeled as input parameters are calibrated by maximizing the reward function and minimizing\nthe loss function. The first part of the table lists the general input parameters of the model, more\nspecifically, the number of agents, the line of sight of each agent, and the strategy used to place\neach agent in the first node of the model that is being trained.\nThe second section of the table shows all those parameters related to the reward function\nthat have been explained in the previous section. Finally, note that the last column of the table\ncontains the values used during the evaluation of the model, as will be explained in Section 5."}, {"title": "4. Urban Crime Surveillance Optimization", "content": "The allocation of resources by public administrations is a matter of ongoing concern due to\nthe lack of resources, and their misuse may directly impact the lives of citizens. In the case of\nthe police, there is not always an adequate number of officers or resources available to carry out\neffective surveillance. Thus, it is necessary to prioritize surveillance in different areas of the city\nover others. In addition, it is also important to note in this domain that patrol routes are slightly\ndifferent each day (Yin et al., 2012; Sherman et al., 2014). This is intended to prevent offenders\nfrom identifying surveillance patterns that they would learn to avoid in the real world."}, {"title": "4.1. The Urban Environment", "content": "In order to create the environment, several input data have been used. Firstly, we collect infor-\nmation about the roads in the city of M\u00e1laga. This information is extracted and combined from\ntwo datasets: the road map of Andalusia, Spain, obtained from OpenStreetMap (OpenStreetMap\ncontributors, 2017) and the list of the information on the roads of M\u00e1laga on the open data portal\nof M\u00e1laga City Council (M\u00e1laga city council, 2024)."}, {"title": "4.2. Assumptions", "content": "When representing real-world scenarios using reinforcement learning models, it is essential\nto make clear the constraints and assumptions about the real problem to be considered. The\nmore complex the problem to be reproduced, the more difficult it will be to define an effective\nreward function that facilitates an adequate model learning process. Conversely, if the problem\nis oversimplified, there is a risk that the results of model training may not be applicable (or may\nbe meaningless) in real situations. For this reason, the following assumptions have been made to\nmodel this police patrolling scenario:\n\u2022 Agents are not constrained to pass through any particular cell during the surveillance, mean-\ning there is no requirement to monitor any specific cell within the area on the map."}, {"title": "5. Evaluation", "content": "As can be seen in the last column of Table 2, the number of patrols used in the evaluation was\nset at 2, 5, and 10, all these values within the resources available to the police. The line of sight\nrange values of 1, 3, and 6 were selected to represent varying levels of information available to the\nmodel. Finally, zone 10 differs from the others in terms of crime dispersion and size, requiring\nspecific adjustments to train the model effectively. To address these training challenges, the\ncoverage factor for this zone was increased from -25 to -10, and the exploration rewards were\ndoubled to encourage agents to explore the environment thoroughly. These adjustments were\nnecessary because, without increasing the exploration value and reducing penalties, the model\nhad difficulty to train.\nThe reinforcement learning algorithm used in the evaluation was VDPPO. It is noteworthy that\nboth IPPO and MAPPO were also able to complete the training process and achieve the desired\noutcomes. However, IPPO required three times longer to train than VDPPO, with an average\ntraining time of between eight and twelve hours per model. Additionally, MAPPO consistently\nperformed slightly worse than the IPPO and VDPPO algorithms. Regarding the specific param-\neters of the VDPPO algorithm (Table 4), a hyperparameter search was conducted to identify the\nmost suitable values within typical ranges. Further information on the parameters, including\ntheir meanings and explanations, can be found in Schulman et al. (2015b, 2017); Albrecht et al.\n(2024).\nTo evaluate the agents' learning performance, the value of the reward function was initially\nstudied, as well as the effect of the line of sight parameter and the number of patrols deployed\non the simulations. As expected, the greater the amount of information about the state of the\nenvironment that agents receive, the better the results. Also, we wanted to study the difference in\nperformance depending on the initial node of patrol deployment, i.e., whether patrols are initially\ndeployed in high-crime nodes or, on the contrary, in random positions.\nBesides the information provided by the rewards and the loss function of the MARL problem,\nit is important to check the behavior of the agents to ensure that they have learned the expected\nbehavior and that there is no flaw in the problem setup that causes them to behave in an unex-\npected way in order to maximize the reward they receive. The only two ways to achieve this are"}, {"title": "5.1. Comparative Study", "content": "As mentioned above, our model seeks an optimal selection of coordinated paths by maximiz-\ning a target function within a limited time and without having a fixed destination. This means\nthat we cannot compare our results with the state-of-the-art baseline algorithm, CBLS (Portugal\net al., 2013; Portugal and Rocha, 2016; Guo et al., 2023), because it works based on the concept\nof idleness and assumes that all nodes have to be visited. In our problem, it would be impos-\nsible to visit all nodes with our limited resources. For this reason, we have developed a greedy\nalgorithm that we believe is comparable to our solution except in terms of communication. This\ngreedy algorithm allows us to determine the minimum threshold of the target function that can\nbe achieved. In this manner, each agent will move to the neighboring cell with the highest score\nwithin its reach. All agents will make their decisions simultaneously, similar to the MARL mod-\nels.\nFor this greedy algorithm, there is no point in discussing entropy or unpredictability in the case\nof a fixed initial position of the patrols, such as deployment at the optimal location, because this\nmodel is deterministic and, therefore, will always produce the same route with the same starting\npoint. In the case of an initial random position, it will generate something similar; however, if\nthese positions are very close to each other, they may influence the generated routes by moving\ntoward the same cells simultaneously or by one patrol reaching a position before another that\nwas also heading there. This is further exacerbated if the initial position places the patrols in a\nlocation that is not particularly relevant on the map."}, {"title": "5.2. Results", "content": "The model training performance is illustrated in Figures 6, 7, and 8, where it can be observed\nthat the model converges in all instances. Regarding the areas, in both zones 3 and 9, it can be ob-\nserved that the configurations with 10 patrols are the least effective in terms of reward value. This\noccurs because the patrols are penalized for not doing enough individually when there are fewer\ncells to cover, as the most relevant cells are being covered by other patrols. In configurations\nwith fewer patrols, each patrol covers more critical areas throughout the simulation, resulting in\nfewer penalties. For zone 10, the parameters of the reward function were changed (the coverage\nfactor was increased from -25 to -10 and both exploration rewards were doubled) to improve\nthe model exploration capabilities and to allow agents to identify the most relevant areas on the\nmap. By increasing this value, the results with 10 patrols surpass those with 2 and 5 patrols.\nThe performance of our model in terms of the proposed metrics can be seen in Tables 5, 6, and\n7, where the maximum values of each column are marked in bold, and the following \u03d5 values\nhave been used: 3%, 5%, 10%, and 20%. The set of nodes in the 3% of coverage index is the\none with the highest incidence of crime; for this reason, the smaller the value of \u03d5, the more\ncrucial achieving greater coverage becomes. Note also that as the number of nodes expected to\nbe covered increases, the coverage decreases because of the lack of resources to cover them all,\ngiven the constraints of a connected path in the graph for each agent.\nFinally, it must be pointed out that all simulations were carried out on a machine with 128 GB\nof RAM, 20 cores of CPU, and an RTX 4090 graphic card. Training time varies depending on the\nalgorithm used and the specific parameter configuration for both termination and weight update\nof the network. In the final configurations, training times ranged between one and eight hours, for\na termination condition of 100,000,000 steps. The training time varied depending on the number\nof patrols, the line of sight, and the size of the area, but the training time remains between three\nand six hours. Once trained and loaded, the model is able to generate a coordinated route in less\nthan five minutes or provide the recommended direction in a given situation in real time."}, {"title": "6. Discussion", "content": "In the context of patrol routing problems, we have not found metrics that calculate whether\nthe hotspots of the surveillance area have been covered or not. The most common metrics in\nthis field to evaluate the performance are the average duration a node remains unvisited (idle"}, {"title": "7. Conclusions and Future Works", "content": "This paper proposes a MARL-based model for designing patrol routing strategies, based on a\ndecentralized partially observable Markov decision process and in terms of a target function. The\nmodel designs unpredictable and optimized routes for patrols in urban environments represented\nas undirected graphs, providing area coverage cooperatively, without minimal overlaps, and with\npartial or full observability. It generates routes suitable for medium-sized urban areas, feasible\nfor foot patrols with a time limited working day. The model has been tested in three districts\nof the city of M\u00e1laga with the aim of optimizing crime surveillance. se, spatial graphs with\nnodes representing areas of 50 x 50 m were auxomatically generated from the street map, and\nwe counted with rreliedaonthe crimes comed in the city, which were used as a target function of\nthe model.\nWFurthermore,e have also introduced a novel metric, the coverage index, to evaluate the cov-\nerage performance of the routes. This index is inspired by the PAI, a well-known metric in\ncriminology that focuses on detection of crime hotspots. Since our model does not preset routes\nbut learns coordinated paths that cover the areas with the highest crime rates, the most commonly\nused performance criterion, idleness, was not appropriate for our approach. Therefore, we have\nperformed the evaluation of the model in terms of the proposed metric. We have analyzed the\nimpact of the varying information levels observed by agents, the number of agents, and their\nstarting positions. Our findings show that there is no difference in performance between using\nfive and ten patrols. This proves that such a high number of patrols is unnecessary for effective\narea coverage. Furthermore, results show that starting with a moderate line of sight yields results\ncomparable to those with a larger range. This implies that agents do not need extensive infor-\nmation about the system to operate effectively. Random deployment has proven ineffective at\ngenerating significantly more route variety compared to deployment in optimal areas of the map,\nyielding poorer results on the target metric. Results suggest that the most effective approach is\nto deploy patrols initially at hotspots within the monitored area. A comparative study with a"}]}