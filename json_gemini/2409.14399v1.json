{"title": "Beyond Persuasion: Towards Conversational Recommender System with Credible Explanations", "authors": ["Peixin Qin", "Chen Huang", "Yang Deng", "Wenqiang Lei", "Tat-Seng Chua"], "abstract": "With the aid of large language models, current conversational recommender system (CRS) has gaining strong abilities to persuade users to accept recommended items. While these CRSs are highly persuasive, they can mislead users by incorporating incredible information in their explanations, ultimately damaging the long-term trust between users and the CRS. To address this, we propose a simple yet effective method, called PC-CRS, to enhance the credibility of CRS's explanations during persuasion. It guides the explanation generation through our proposed credibility-aware persuasive strategies and then gradually refines explanations via post-hoc self-reflection. Experimental results demonstrate the efficacy of PC-CRS in promoting persuasive and credible explanations. Further analysis reveals the reason behind current methods producing incredible explanations and the potential of credible explanations to improve recommendation accuracy.", "sections": [{"title": "1 Introduction", "content": "Conversational Recommender Systems (CRSs) aims to engage in a natural language conversation with users, provide recommendations, and ultimately achieve a high level of user acceptance (Jannach et al., 2021; Gao et al., 2021). To achieve this, providing proper recommendation explanations along with accurate recommendations is paramount, because users are usually not familiar with the recommendations (Chen et al., 2020; Guo et al., 2023). Such explanations should be carefully crafted, incorporating persuasive elements that can influence user behavior and decision-making, thus increasing the likelihood of user acceptance of the recommendations (Alslaity and Tran, 2019; Yu et al., 2011). Recently, the integration of Large Language Models (LLMs) has dramatically enhanced the persuasive power of current CRSs. LLMs possess the remarkable ability to generate highly convincing content that can rival, and even surpass, human-crafted persuasion (Hackenburg et al., 2023; Carrasco-Farre, 2024), significantly augmenting CRSs in delivering persuasive explanations, which improve user understanding and ultimately result in higher acceptance rates (Huang et al., 2024).\nWhile LLM-based CRS is highly persuasive, a concerning trend has been observed: these CRSs can mislead users by incorporating deceptive elements into their explanations. For example, as illustrated in Figure 1, a CRS mistakenly recommends the film \"Mission: Impossible\" as a comedy movie to unfamiliar users, ultimately resulting in a negative user experience after viewing the film. This practice contradicts the formal definition of persuasion, which emphasizes influencing people's behaviors or attitudes without using coercion or deception (Reardon, 1991; Oinas-Kukkonen and Harjumaa, 2008). Such incredible explanations can create erroneous perceptions about recommended items (Adomavicius et al., 2013), ultimately damaging the long-term trust between users and the CRS (Koranteng et al., 2023; Deng et al., 2022b; Angell and Smithson, 1990). While the need to enhance credibility during persuasion in CRSs is acknowledged (Huang et al., 2024), effective solutions remain elusive.\nTo this end, we introduce a simple yet effective method for Persuasive and Credible CRS, called PC-CRS. It proactively emphasizes both persuasiveness and credibility during the generation of explanations, and then gradually refines them via post-hoc self-reflection. This is achieved by a two-stage process: Strategy-guided Explanation Generation and Iterative Explanation Refinement. Specifically, in the first stage, PC-CRS utilizes the novel Credibility-aware Persuasive Strategies to guide the generation of candidate explanations. Such strategies are informed by social science research on persuasion (Fogg, 2002; Cialdini and Goldstein, 2004) and further tailored with credible information to ensure both persuasive and credible explanations in our scenario. In the second stage, PC-CRS utilizes a Self-Reflective Refiner to identify and correct potential misinformation in the candidate explanations. It is due to generative models have the inherent tendency to prioritize contextual coherence at the expense of faithful adherence to source information (Miao et al., 2021; Chen et al., 2022, 2023). As such, PC-CRS prevents potential deception in candidates, thus enhancing credibility. Its training-free nature also makes it a highly efficient and adaptable solution.\nWe conduct extensive experiments to demonstrate the effectiveness of PC-CRS. Our experiments leverage the widely-used simulator-based evaluation framework\u00b9 (Wang et al., 2023a; Huang et al., 2024) and employ two CRS benchmarks: Redial (Li et al., 2018) and OpendialKG (Moon et al., 2019). Experimental results show that PC-CRS, on average, achieves an improvement of 8.17% on credibility score (i.e., consistency with factual information) and 5.07% on persuasiveness score (i.e., raising user's watching intention towards recommended items), compared to the best baseline. Further analysis reveals the reason why LLM-based CRS generates incredible explanations is that they cater to user's history utterances rather than describing items faithfully. In addition, the in-depth analysis also suggests that our credible explanations promote recommendation accuracy. This is potentially due to that credible explanations avoid the"}, {"title": "2 Related Work", "content": "Our research focuses on the explanations of CRSs, particularly highlighting their persuasiveness and credibility. Hence, we provide an overview of CRS and persuasive and credible recommender systems and then discuss our differences.\nCRS. CRS enables users to engage in free-form natural language conversations with the system to achieve their recommendation-related goals (Li et al., 2018; Deng et al., 2021, 2022a, 2023; Li et al., 2024). To generate human-like responses, early studies leverage pre-trained language models (PLMs) as their backbones (Wang et al., 2022a,b,c), enabling them to proactively interact and engage with users through verbal explanations (Chen et al., 2020; Guo et al., 2023; Zhang et al., 2024). In the era of LLMS, CRSs have shifted from providing simple information to actively persuading users during explanations (Wang et al., 2023a), ultimately increasing user acceptance (Yu et al., 2011; Alslaity and Tran, 2019). While LLMs enable CRSs to generate highly persuasive explanations, a recent study revealed a concerning trend: they may incorporate misinformation to achieve persuasiveness (Huang et al., 2024), jeopardizing the long-term relationship of trust between users and the CRS. To address this challenge, we propose a method to enhance the credibility of CRS explanations during persuasion.\nPersuasive and Credible Recommender Systems. Early research on identifying how people persuade others with credible explanations in recommendations draws heavily on insights from social science and human-computer interaction (Fogg"}, {"title": "3 PC-CRS", "content": "Overview. Our PC-CRS, as illustrated in Figure 2, involves a two-stage process, i.e., Strategy-guided Explanation Generation and Iterative Explanation Refinement. Given the conversation history and item information, the former stage selects an appropriate strategy from Credibility-aware Persuasive Strategies and then generates a candidate explanation accordingly. Then, taking the previous candidate as input, the latter stage critiques and refines it to eliminate misinformation and yield the final explanation. PC-CRS leverages LLMs with detailed Chain-of-Thought instructions (Kojima et al., 2022) to make full use of the generative capabilities of them in the above two stages."}, {"title": "3.1 Strategy-guided Explanation Generation", "content": "As previous CRSs often lack of explicit focus on persuasiveness and credibility, this stage aims to proactively emphasize the two factors when offering explanations in PC-CRS. To achieve this, we take inspirations from social science research (Cialdini and Goldstein, 2004; Fogg, 2002; Zeng et al., 2024) and tailor them to develop our Credibility-aware Persuasive Strategies, guiding the explanation generation process of PC-CRS."}, {"title": "3.1.1 Credibility-aware Persuasive Strategies", "content": "Drawing upon the well-established Elaboration Likelihood Model of persuasion (Cacioppo et al., 1986), we propose six credibility-aware persuasive strategies specifically tailored for credible CRS that encourage the use of factual information during persuasion. These strategies are categorized into three groups. In particular, the first three strategies aim to persuade individuals with carefully constructed content, while the next two aim to influence users through peripheral cues (e.g., the source's credibility), and the last one combines elements of both. We further specify the credible information used in these strategies and construct prompts for the LLM to effectively use these strategies. Strategy examples and detailed prompts are shown in Appendix A and Appendix E.2, respectively.\n\u2022 Logical Appeal refers to faithfully presenting the logic and reasoning process of the system to influence people (Cronkhite, 1964), e.g, describing how a movie's genre is consistent with user's preference. By this means, users can see \"why\" a particular recommendation is suggested, leading them to trust and accept the recommendations more readily.\n\u2022 Emotion Appeal refers to eliciting specific emotions and sharing credible and impactful stories to foster trust and deep connection with users (Petty et al., 2003), e.g., sharing a movie's plot to elicit user's emotion. Validating users' feelings through the system's explanations can build cred-"}, {"title": "3.1.2 Explanation Generation", "content": "As the conversation proceeds, we select suitable strategy to guide the explanation generation at each turn, which helps adapt to the dynamics of dialogue contexts (Wang et al., 2019). As shown in Figure 2, PC-CRS prompts the LLM with detailed instructions to select a strategy and generate an explanation candidate accordingly.\nStrategy Selection. Given a recommended item, PC-CRS retrieves its detailed information from a credible source (e.g., a knowledge base). Then, taking the conversation history H and retrieved item information I as inputs, a strategy selector, powered by the LLM, chooses an appropriate strategy s from Credibility-aware Persuasive Strategies S:\n$s = \\text{StrategySelector}(H, I, S)$.\nExplanation Candidate Generation. Given the selected strategy s, the conversation history H, and item information I, we prompt the LLM to produce recommendation explanation candidates:\n$c = \\text{ExplanationGenerator}(H, I, s)$.\nAs such, PC-CRS customizes explanation candidates to match the user's preferences and context, making the interaction more relevant and engaging. Besides, the Credibility-aware Persuasive Strategies also explicitly guide the explanations to be both persuasive and credible."}, {"title": "3.2 Iterative Explanation Refinement", "content": "As generative models tend to prioritize contextual coherence at the expense of faithfully adhering to the source information (Miao et al., 2021; Chen et al., 2022), there still might be illusory details incorporated in the candidate explanations. To this end, PC-CRS aims to analyze the factual basis and plausibility of each claim, ultimately ensuring that only credible and well-supported explanations are presented to the user. To achieve this, this stage, inspired by the self-reflection mechanism (Ji et al., 2023; Madaan et al., 2024), leverages a self-reflective refiner to criticize and refine incredible claims within the candidates iteratively.\nCritique. Each explanation candidate is treated as an initial proposal. In the k-th iteration, a critic examines if the candidate explanation $c_k$ contains any misinformation based on item information I:\n$cqp_k = \\text{Critic}(c_k, I)$.\nThe critic utilizes a self-reflective approach, starting by summarizing the claims within the explanation candidate. This summary is then compared against the relevant item information. Operating independently of any conversational context, the critic generates a critique ($cqp_k$) which evaluates the explanation's credibility. This critique identifies whether further refinement is necessary and, if so, suggests specific improvements.\nRefinement. If the critic deems refinement necessary, the refiner plays a vital role in generating a revised explanation. This refinement process leverages both the original explanation ($c_k$) and the critic's feedback ($cqp_k$) to produce the new one:\n$c_{k+1} = \\text{Refiner}(H, I, s, c_k, cqp_k)$."}, {"title": "4 Experiments", "content": "In this section, we investigate the superiority of PC-CRS on persuasiveness and credibility (Section 4.2). Subsequently, we provide detailed analyses on characteristics of PC-CRS to gain understanding why it alleviates the incredibility and improves the recommendation accuracy (Section 4.3). Then, ablation studies and human evaluation indicate the necessity of two stages in PC-CRS and the reliability of our evaluation, respectively (Section 4.4)."}, {"title": "4.1 Experimental Setup", "content": "User Simulator & Datasets. Utilizing a user simulator to evaluate CRS is a common practice, as interacting with real humans can be quite expensive (Lei et al., 2020; Wang et al., 2023a,b; Fang et al., 2024). In accordance with prior research, we follow the simulator in Wang et al. (2023a); Huang et al. (2024) tailored for two CRS benchmarks, namely Redial (Li et al., 2018) and OpendialKG (Moon et al., 2019). Specifically, the simulator is initialized with different user preferences and personas. To mimic real-world scenarios, it has only access to a combination of preferred attributes without certain target items. During the conversation, the CRS and simulator converse with each other in free-form natural language. The conversation ends either when the maximum number of turns is reached or the simulator accepts the recommendation provided by the CRS. See more details on the simulator and datasets in Appendix B.\nBaselines. We compare PC-CRS with SOTA PLM-based methods, i.e., BARCOR (Wang et al., 2022b) and UniCRS (Wang et al., 2022c). We also compare PC-CRS with recent LLM-based CRSs, including InterCRS (Wang et al., 2023a), ChatCRS (Li et al., 2024) and MACRS (Fang et al., 2024).\nEvaluation Metrics. Following Ye et al. (2023); Liu et al. (2023b), we utilize the GPT-4-based evaluator, equipped with fine-grained scoring rubrics, to achieve a cost-effective evaluation (we also involve human evaluation in Section 4.4). Concretely, we introduce three metrics to quantitatively measure the performance of CRS explanations:\n\u2022 Persuasiveness. Inspired by human studies on persuasion (Lu et al., 2023), Persuasiveness score focuses on to what extent an explanation can change the watching intention of a user towards the recommended item. This is achieved by instructing the evaluator to score its watching intention, ranging from 1 to 5. Specifically, the evaluator rates its initial intention $i_{pre}$ based solely on the item's title. Then it is required to rate the intention $i_{post}$ after reading the CRS explanation. Finally, the evaluator rates the 'true' intention $\u0130_{true}$ after seeing the full information about the item. And the Persuasiveness is calculated as follows. A higher Persuasiveness score means a stronger ability in arousing user's watching intention towards recommended items.\n$\\text{Persuasiveness} = 1 - \\frac{i_{true} - i_{post}}{I_{true} - I_{pre}}$\n\u2022 Credibility. We resort to metrics used in text summarization to access utterance-level credibility, checking if each explanation (summary) is consistent with the facts (source texts). Following Gao et al. (2023); Luo et al. (2023), we employ GPT-4 and prompt it to score the Credibility ranging from 1 to 5 with a detailed criteria2.\n\u2022 Convincing Acceptance. This metric aims to assess dialogue-level credibility. It measures how often the CRS successfully convinces the simulator to accept a recommendation while maintaining a high credibility. A higher Convincing Acceptance indicates a lower likelihood of users being misled by deceptive explanations.\nIn addition to evaluating the quality of CRS explanations, we also employ metrics to evaluate the recommendation accuracy of CRSs. Following Wang et al. (2023a) and Zhang et al. (2023), we use Success Rate (SR) and Recall@k (R@k), where k = 1, 5, 10.\nImplementation Details. All baselines are implemented by checkpoints and prompts from the"}, {"title": "4.2 Main Results", "content": "We start by examining whether PC-CRS achieves the goal of enhancing credibility during persuasion. Table 1 shows the performance of PC-CRS and other baselines. We also conduct experiments on PC-CRS using Llama3-8B-instruct as the backbone to demonstrate that our PC-CRS can generalize to various LLM options (see Appendix C.2 for details). Here, our findings are as follows.\nLLM-based CRSs are highly persuasive. As shown in Table 1, LLM-based systems achieve a Persuasiveness score that is 3.3 times higher than their PLM-based counterparts on average. This superior performance is attributed to the LLMs' inherent strength in comprehending user needs and effectively modeling context, leading to more convincing and impactful recommendations.\nPC-CRS achieves both persuasive and credible explanations. According to Table 1, we observed that PLM-based CRSs struggle to generate credible or persuasive explanations, resulting in no recommendations being accepted. This limitation stems from their relatively weak generation capabilities, leading to absurd outputs like \"Black Panther (2018) is about a woman who is a human\"."}, {"title": "4.3 In-depth Analysis", "content": "This section delves into the characteristics of credible explanations, with a special focus on the reason behind the current method's incredibility and the role of credibility in influencing the recommendation accuracy and the persuasiveness5.\nWhy does LLM-based CRS lies? \u2013 It caters to user's utterances rather than describing items faithfully. To gain a deeper understanding of the reasons for incredibility, we focus on InterCRS, a low-credibility and high-persuasiveness LLM-based baseline, as an example. Specifically, we"}, {"title": "4.4 Ablation Study & Human Evaluation", "content": "This section aims to sort out the performance variation of PC-CRS regarding the two stages and conduct human studies to assess our evaluation reliability. Details can be found in Appendix C.1 and Appendix D.\nTwo stages in PC-CRS unify as a team to ensure our effectiveness. Our ablation study (Figure 5) reveals that Strategy-guided Explanation Generation is crucial for PC-CRS's success, as the performance on all metrics significantly drops without this stage. It highlights the importance of our Credibility-aware Persuasive Strategies, which explicitly emphasize both persuasiveness and credibility in explanations. While Iterative Explanation Refinement implicitly optimizes PC-CRS's generation space, it primarily focuses on maintaining credibility and further improving recommendation accuracy. This demonstrates the essential nature of both processes in PC-CRS's design, working in tandem to produce persuasive and credible explanations.\nOur proposed strategies have varying effects on different users. We dive into the proposed six credibility-aware persuasive strategies and analyze their effectiveness using the Redial dataset. Specifically, we calculate the top-3 strategies with highest recommendation success rates of users with distinct personas (described in Appendix B). The results in Table 4 indicate that these strategies varies differently on different users. Notably, these strategies for the 12 personas encompass all six strategies, underscoring that each strategy is both effective and essential for PC-CRS.\nOur evaluation framework demonstrates strong reliability, with a high degree of consistency to human evaluation. Given our use of GPT-4 as an automatic evaluator and ChatGPT as a user simulator, we assess their reliability using human judgments (details in Appendix D). The results demonstrate the reliability of GPT-4 as an evaluator, with Spearman correlations of 0.59 for Watching"}, {"title": "5 Conclusion", "content": "The pursuit of trustworthy AI necessitates a profound understanding of credibility. This paper delves into the crucial role of bolstering the credibility of the CRS during its persuasions, recognizing that such credibility is essential for cultivating long-term trust between users and the CRS. We introduce a simple yet effective method for enhancing CRS with the awareness of being both persuasive and credible. Our experimental findings demonstrate the efficacy of this method in promoting persuasive and credible explanations, while also shedding light on the inherent tendency of current LLM-based CRS to prioritize persuasion over honesty. Additionally, our research highlights the delicate balance required when aiming for both persuasiveness and credibility without resorting to deception - a balance demanding sophisticated linguistic capabilities within LLMs. Our work lays the groundwork for further exploration of this vital relationship, and we encourage future research to delve deeper into this critical area."}, {"title": "Limitation", "content": "Current LLM-based CRS methods mainly utilize ChatGPT as their backbones (Wang et al., 2023a; Fang et al., 2024). Due to the constraints of budget and computational resources, we do not extend this setting to other LLMs (e.g., GPT-4) in our experiments. This limited model selection could lead to model bias in the research field of LLM-based CRS. For example, different models may vary in the performance of persuasiveness and credibility as they utilize different alignment mechanisms. We encourage future work to explore the impact of CRSs with diverse LLM backbones.\nAnother limitation of our work is that PC-CRS's strategy for generating explanations tends to be uniform, lacking individualization. To assess this, we engaged the PC-CRS in free-form conversations with 12 user simulators initialized with distinct user profiles, hoping to observe varying strategies tailored to each user's profile. However, our results revealed a consistent pattern: PC-CRS mainly relied on Logical Appeal, Emotion Appeal, and Framing, regardless of the user's characteristics. This finding aligns with recent observations that LLMs exhibit a one-size-fits-all approach in conversational settings (Chen et al., 2024). Besides, PC-CRS only selects one strategy at each turn. While various strategy combinations can be used in multi-turn interactions, this may fail to capture users' interests efficiently. Future research endeavors should prioritize enhancing the flexibility of strategy selection within PC-CRS, enabling it to adapt its approach based on individual user characteristics."}]}