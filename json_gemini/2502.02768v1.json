{"title": "PLANNING WITH AFFORDANCES: INTEGRATING LEARNED AFFORDANCE MODELS AND SYMBOLIC PLANNING", "authors": ["Rajesh Mangannavar"], "abstract": "Intelligent agents working in real-world environments must be able to learn about the environment and its capabilities which enable them to take actions to change to the state of the world to complete a complex multi-step task in a photorealistic environment. Learning about the environment is especially important to perform various multiple-step tasks without having to redefine an agent's action set for different tasks or environment settings. In our work, we augment an existing task and motion planning framework with learned affordance models of objects in the world to enable planning and executing multi-step tasks using learned models. Each task can be seen as changing the current state of the world to a given goal state. The affordance models provide us with what actions are possible and how to perform those actions in any given state. A symbolic planning algorithm uses this information and the starting and goal state to create a feasible plan to reach the desired goal state to complete a given task. We demonstrate our approach in a virtual 3D photorealistic environment, AI2-Thor, and evaluate it on real-world tasks. Our results show that our agent quickly learns how to interact with the environment and is well prepared to perform tasks such as \"Moving an object out of the way to reach the desired location.\"", "sections": [{"title": "1 INTRODUCTION", "content": "In real-world environments, the ability to come up with multi-step plans for a particular task is an important skill for an intelligent agent. Moreover, it is equally important that the agent be able to interact with the environment to execute this plan. For example, for efficient navigation of an environment, the agent must generate multi-step plans, including navigating through different rooms while clearing out any objects blocking the path. However, the agent must also be able to interact with the environment correctly for actions such as opening the door or picking up an object that is blocking the way. Here the agent needs to make both discrete decisions (picking up an object) as well as continuous decisions (the choice of its exact position/orientation to be able to pick up the object).\nThe aforementioned problem falls under a class of problems called the task and motion planning (TAMP) problems. This class of problems generally deals with environments that contain several objects, and the agent is expected to navigate in the environment, interact and change the state of objects to complete a given task.\nThe more specific problem we would like our agent to solve is, to be able to be dropped into a real-world environment or a 3D virtual environment and given a task to perform, learn about the objects in the world, produce and execute plans which achieve complex multi-step tasks by making the required discrete and continuous decisions.\nTo solve the defined TAMP problem, we need a system that is capable of learning affordances in the world in a way that enables composing these affordance models by a higher-level symbolic planning algorithm to solve a given task. The complexity of the problem is increased further when we want to solve the tasks in a hybrid discrete and continuous world. Hence, we need a framework that enables"}, {"title": "2 RELATED WORK", "content": "Learning based approaches to perform tasks: Existing works on embodied agents with pure learning-based approaches try to train reinforcement learning agents to do tasks through trial and error in the environment. They receive a large reward upon completing the tasks. Some of the popular reinforcement learning-based approaches use curiosity (Pathak et al. (2017)) and coverage (Chen et al. (2019)) to overcome the issue of sparse rewards. They are generally trained for very specific tasks such as exploring the world they are in or simple tasks like navigation to a point in the environment. Hence, they end up learning policies for exploration Chen et al. (2019) or policies for specific tasks (Gupta et al. (2017)). In our work, the learning is not done for any task but over properties of objects, and the tasks are completed by planning a sequence of actions based on the learned affordances. These affordances are not specific to any of the tasks.\nAffordance learning for tasks: Some of the recent work in affordance learning in conjunction with reinforcement learning has been used to learn to perform tasks in 3D environments. In these approaches, the affordances models learn to predict possible actions in the given state of the world from the egocentric view the agent has. Now, an RL agent is then trained to perform tasks in the world by using this vision-based affordance model to narrow the set of possible actions in any given situation, which helps in learning to perform tasks faster. (Nagarajan & Grauman (2020) Lohmann et al. (2020)). In our work, we learn to be capable of performing any task, even tasks not seen before but which are possible to execute through a composition of existing skills. The other approach to using affordances to perform tasks is to learn affordances not only to predict what actions are possible at a given moment but also what future actions are feasible if a certain action is taken (Xu et al. (2020)). The main difference with our work is that they too limit their affordance to what actions are feasible whereas our affordances provide us the information about how to perform an action along with what actions are feasible.\nPlanning-based approaches to perform tasks: PDDLStream, as described in the previous section, is a framework for solving TAMP problems that converts the discrete and continuous planning problem into a discrete planning problem using streams. Streams are conditional samplers and also have a declarative component that certifies the facts generated by the samplers about the properties the generated values satisfy in the given environment. These generated values are then used as facts in a discrete planning problem. Learning-based approaches which extend the PDDLStream framework (Wang et al. (2021)) learn the samplers to generate values for certain continuous variables. However, their models depend on the fact that the object orientation does not change. Our approach is an extension of PDDLStream work where we learn the conditional samplers as opposed to human-designed samplers, and the learned models are invariant to the object orientation/pose in the world."}, {"title": "3 PROBLEM FORMULATION", "content": "Our objective is to solve the TAMP problem using planning algorithms. That is, given a starting state and a goal state of the world, along with the information about the world, generate a sequence of actions in the world that enables us to go from the initial state to the goal state.\nWe define our problem in the PDDLStream framework. This formulation helps us have a compact representation of the state of the world, actions, and the required goal. It also helps us use existing PDDL problem solvers to solve our TAMP problem in the hybrid continuous and discrete world."}, {"title": "3.1 PRELIMINARIES", "content": "First, we give a brief summary of PDDL and PDDLStream and then properties about the world we are operating in. Furthermore, we also give a formal description of our problem. We then give a brief overview of how a PDDLStream problem is solved."}, {"title": "3.1.1 PLANNING DOMAIN DEFINITION LANGUAGE (PDDL)", "content": "PDDL has three main components - Predicates, states, and actions. (i) A predicate p is a Boolean function. A fact p(x) is a predicate p applied on an object x that evaluates to true. For example,"}, {"title": "3.1.2 PDDLSTREAM", "content": "PDDLStream is a framework that extends PDDL by incorporating sampling procedures using streams. Streams are conditional generators with procedural and declarative components. The sampling procedures are used to sample values for the continuous variables in the world. The declarative component defines the properties the inputs, and the outputs of the streams satisfy. The overall method PDDLStream uses to solve a given PDDL problem with continuous variables is as follows. Let $Search (Init, Actions, goal)$ be an algorithm which can solve discrete PDDL problems.The hybrid discrete and continuous problem must be reduced to a discrete PDDL problem to utilize this $Search$ algorithm. This is done by sampling values for the continuous variables. These sampled variables are then added to the $Init$ state as facts based on the conditions they satisfy. The $Search$ algorithm then tries to find a sequence of actions using all the facts it has about the world to reach the foal state. If a solution is found, it returns the plan as the final solution. If not, streams are used to generate more facts about the world and this process is repeated until either a solution is found or a stopping condition such as timeout is hit."}, {"title": "3.1.3 STREAMS", "content": "A stream is a conditional generator with a procedural component and a declarative specification of all the facts its inputs and outputs must satisfy. In this section, we go over each component of streams, and we also go more into detail as to how streams are used to generate values and how the generated values can be used as facts in the world.\nA generator g is a sequence of object tuples y. On call to the function next(g), the next value in the sequence of tuples is returned if it exists. A conditional generator takes an input x and produces an object tuple y' related to the input x if it exists.\nThe declarative specification of the stream is represented through the domain and certified facts. Each stream has a domain, which is the set of predicates the inputs must satisfy. Each stream also has a set of certified facts, that is, all the outputs produced by the stream satisfy the predicates in the set of the certified facts. These certified facts are that are sent back to the planner to consider as facts in the world while solving the planning problem.\nThe procedural component of streams is what defines how to actually generate the values which satisfy the required constraints. This is generally a programmatic implementation of a function that takes the domain facts as input, also produces outputs based on the constraints in the certified facts they must satisfy. The implementation of the stream is considered a black box for the planner as"}, {"title": "3.2 PROBLEM DEFINITION", "content": "We now define our TAMP problem in the PDDLStream formulation. We first define what the initial state and goal state is in our environment, then the available actions, and then what streams are required for our environment."}, {"title": "3.2.1 INIT AND GOAL STATE", "content": "As seen in fig 1, the agent is in the AI2Thor environment, a virtual 3D environment Kolve et al. (2017). The environment contains a set of objects in a room. The input to the agent is the bounding box information of all the objects in the environment. Using this information, we create our \u00ce = init state of the world, that is, create predicates for objects and their orientation/poses in the room. This init state contains information about all the objects of the world and the agent's own information. The goal G is described as a set of literals, similar to the initial state. For example, if the goal is to pick up a cube, the goal state would be PickedUp(cube).\nWe assume that we are working in a fully observable setting where the information of all the objects is available to us in the very beginning. We also assume we have a perfect perception system, and hence we have the object orientation and bounding box information."}, {"title": "3.2.2 ACTIONS", "content": "AI2Thor has a host of actions that enable a large number of interactions with the objects in the environment as well as navigation. In our work, we consider the following subset of actions : \u00c2 Move Agent, OpenObject, PickupObject, CloseObj. A solution to the PDDLStream problem is a sequence of these actions and values of the parameters that satisfy the preconditions (the values for the parameters are returned as part of actions to execute). An example of a full description of an action is as follows :\n(:action PickupOobject\n:parameters (?o ?p ?q)\n:precondition (and\n(PickablePos ?o ?q ?p)\n(AtPose ?0 ?p)\n(AtPosition ?q))"}, {"title": "3.2.3 STREAMS", "content": "Streams are required to generate values for the continuous variables position, object pose. The number of streams we need is a lot higher as we need to generate these continuous-valued variables with different certifications - we need to generate agent positions for opening and closing an object, and so on. Let the set of all the streams required be \u015c. An example of a stream is the sample pickable - point stream.\n(:stream sample-pickable-position\n:inputs (?0 ?p)\n: domain (and (Pose ?o?p) (Pickable ?o))\n:outputs (?pos)\n:certified (PickablePos ?o ?q ?pos))\nThis stream takes in an object and its pose information as input. It checks if the object is a pickable object and that specified pose is the pose of the object. It then generates a position ?pos, which it certifies as being a PickablePos (A position from which the object is pickable). This generated value and the certified fact are then used to satisfy the preconditions of the action PickupOobject mentioned above."}, {"title": "3.2.4 FORMAL DEFINITION OF OUR TAMP PROBLEM IN PDDLSTREAM", "content": "We have defined the initial state, actions, and streams for our problem formulation. Hence, the PDDLStream formulation of our problem definition is as follows: $T = (\u00ce, \u00c2, \u015c, \u011c)$ and the solution is a set of actions that when applied sequentially, change the state of the world from initial state \u00ce to goal state \u011c. The environment dynamics captured through the actions are described in the domain.pddl file in the PDDL syntax. The specific instance of the problem we are solving, a particular arrangement of objects and goals, is described in the instance.pddl file in PDDL syntax. The declarative part of the streams is also defined in the PDDL syntax in a stream.pddl file."}, {"title": "3.3 SOLVING A PDDLSTREAM PROBLEM", "content": "We have defined the TAMP problem as an instance of a problem in the PDDLStream framework. In this section, we cover how to solve problems defined in the PDDLStream framework.\nThe existing PDDLStream framework takes the four tuple as input and outputs a sequence of actions. There are multiple algorithms that can be used to solve the PDDLStream problem. However, we present only the simplified version of the incremental algorithm from Garrett et al. (2020) (Other algorithms and details of this algorithm are omitted as our proposed approach works with any algo-rithm and does not depend on any other part of the algorithm)."}, {"title": "4 PDDLSTREAM WITH AFFORDANCES", "content": "This section will cover our proposed addition to the existing PDDLStream framework and how to incorporate this new method to improve the existing framework."}, {"title": "4.1 AFFORDANCES: MOTIVATION AND DEFINITION", "content": "We now know that streams form an integral part of solving a PDDLStream problem. However, these streams have a few drawbacks. Each of these streams, in the original PDDLStream framework, are hand-designed. The drawbacks of defining streams this way are multi-fold. First, since we are designing the samplers based on our knowledge of the world, there might be information about the world we are missing and leading it to to generate values that might not be useful. Second, since streams can be used in more complex situations such as generating droppable configurations for objects, it might not be possible to hand-design these samplers (droppable position is more complex than pickable position because the drop action in AI2Thor is less reliable with where it drops the objects).\nHence, we propose learning these samplers as generative functions. The main intuition behind our idea is that each of the continuous variables we are trying to sample values comes from a particular distribution that depends on the object itself and the action for which the value is being sampled.\nFor example, in the Sample \u2013 Pickable \u2013 Position(cube, pose) stream instance, we try to sample for the continuous variable position from where the object cube can be picked up. We can see that it also depends on which object we are sampling this position for. Hence, we conclude that a. distribution exists for the continuous variable position over each action on a particular object. We now try to learn a generative function for this distribution that can generate a value from this distribution whenever necessary. If we can learn to generate these values using this generative function, we can guarantee that the generated values have the required properties. In this case it would be that the generated position is indeed a position from which the cube can be picked up. We learn a separate probability density function for each object and every actionable property possible in the environment. This is required as each object has its own distribution of values for which an action will be successful.\nWe use the Kernel density estimation (KDE) method to learn the required distribution. Learning to sample solves both the aforementioned problems about streams. Learning ensures that we do not miss any information which can be missed while hand designing, and also, when some functions are too complicated to hand design, the function can be learned efficiently."}, {"title": "4.2 LEARNING THE AFFORDANCES", "content": "Our approach is to first try to perform actions with an object in an empty environment. We let the agent interact with the object by attempting to perform all the actions in the environment. The data is collected the data for the runs where the action was successful. We then use this data and learn the distributions of the continuous variables required for a particular object for different actions. The system learns separate density function for each of the action-object pair. We repeat this pro-cess for each object. The learned generative density functions are what we call affordances. These affordances tell us what actions are possible with what objects (if none of the actions were success-ful while training, we assume that particular action is impossible for the object of interest in that run). The affordances also tell how to perform the action by generating values for the continuous variables that satisfy the action's preconditions. Hence, these learned affordances become our condi-tional samplers which guarantee that when they generate a value, it satisfies the required declarative constraints defined as part of the stream definition.\nThis learning can also be done online - that is, when the agent is put into a new environment, it can go about trying to interact with the objects in the environment. As it succeeds with interactions, it can learn from that data after a certain number of successful interactions of each type with every object."}, {"title": "5 EXPERIMENTS AND RESULTS", "content": "We use our approach to solve three different tasks with increasing order of complexity. We conduct all the experiments in the AI2Thor simulator. It is a 3D virtual environment with a physics engine that helps in simulating real-world tasks. We can generate environments with different configura-tions with any number of objects and have a large number of interactions with these objects in the environment. We compare the two ways of solving the problem - one with hand-designed streams and one with learned streams to generate values for continuous variables. We compare these ap-proaches by comparing the success rate of the approaches while performing these tasks. We also see the effect of the number of objects in the room on the same parameters for both approaches. The first step in our process of generating plans for solving tasks is to learn about the objects in the training phase. We use these learned models while solving the tasks and compare the results to the system performs with hand-designed models.\nThe complexity of a task is defined as the number of required actions to go from the start state to the goal state."}, {"title": "5.1 TASKS", "content": ""}, {"title": "5.1.1 TASK 1: MOVE OBJECT FROM 1 LOCATION TO ANOTHER", "content": "The first task for the agent is to pick up an object and place it in a different location. In this task, the streams of importance are the streams that generate the positions from where an object is pickable and also from what position an agent should be dropped from to place it in the required destination location. The complexity of the task is four, as the expected solution has four actions. An example of this task can be seen in Figure 1, where the agent is expected to pick the blue object and move it to a different location."}, {"title": "5.1.2 TASK 2: PICKUP OBJECT WHILE BEING COMPLETELY BLOCKED INITIALLY", "content": "In this task, the agent is surrounded by large unmovable objects and one small movable object. The small object can be moved out of its position to reach the required goal location. The complexity of the task is six, as the expected solution has six actions. An example of this task can be seen in Figure 2, where the agent is expected to pick the red cube to be able to move to a region currently blocked by sofas and the red cube."}, {"title": "5.1.3 TASK 3: PLACE AN OBJECT IN THE WORLD AT A CERTAIN HEIGHT AND CONSTRUCT A SUPPORT STRUCTURE TO HOLD IT", "content": "In this task, the goal is specified as a mid-air pose for the object. The agent is expected to create a support structure by moving other objects underneath the mid-air goal pose of the first object. The complexity of the task is eight."}, {"title": "5.2 EXPERIMENTS", "content": "For our experiments, we create environments with random object positions and poses. For all the tasks, we test them in 2 separate environment configurations. First, only the objects which are relevant to the task are in the room. For this, we generate scenes with only the objects necessary for completing the task. Next, we add 20 objects to the scene which are not useful to the task but exist in the room. We generate scenes for this set of experiments by randomly placing 20 objects in the room. This is to test how the number of objects in the room affects the task at hand. We run the experiment 10 times for both settings."}, {"title": "5.3 RESULTS", "content": "We can see from Table 1 that using the learned affordance models helps us achieve a higher success rate in task completion. The majority of the failures for both approaches came while trying to drop an object. While our approach can achieve a fair amount of success in all the tasks, the vanilla PDDLStream approach with hand-designed models fails as the object is being dropped incorrectly in a majority of the tasks.\nThe complexity of the task itself does affect the overall performance - we see a performance decline in both approaches as the tasks get more complex. As the length of the task increases, the chances of failure in any one of the steps increases, thereby increasing the overall chances of failure. The performance on task 3 is particularly poor as dropping objects on top of each other as it is a hard task, and very accurate values must be sampled for this to be successful.\nAdding extra objects in the room does affect the success rate but not too much. They come into play when they are very close to the goal location or objects of interest. If they are too close to the object"}, {"title": "6 CONCLUSIONS", "content": "In this work, we describe a new way of incorporating learned models of the world into a planning framework for completing complex multi-step tasks in a hybrid discrete and continuous environ-ment. We do this by extending the PDDLStream framework to include learned affordance models of the environment. The affordance models are learned for each object and action in the environment. They are used to sample values for continuous variables in the world. This helps reduce the hybrid continuous and discrete world into a discrete planning problem that we solve using a symbolic plan-ner. Through our experiments in the AI2Thor simulation environment, we show that this extension leads to the agent being able to solve tasks more consistently in complex environments. Having the capability to plan over learned affordances helps in achieving complex tasks very easily the through composition of these learned models. This framework of learning and planning can be used in a hybrid environment to learn and execute tasks faster than existing approaches for the same."}, {"title": "7 FUTURE WORK", "content": "There are few potential ways to extend the work described in our paper.\nFirst, we can extend the problem statement to cover a more generic view of the world and solve tasks in more complex environments. In our current work, planning happens in a fully observable world. It is possible that the world is partially observable, which it is in many cases (such as an object being stored inside a container and the agent only knows it's in one of the many containers but is not sure which one) and hence planning over belief states might be necessary in these cases. Our framework allows us to easily extend our work to tackle this broader class of problems.\nThe other improvement to our described work is the choice of the generative function itself. In the current approach, we learn a separate model for each object-action pair. We can try to learn a more generic model over actions or over a set of objects. This will reduce the amount of learning required and can also lead to better generalization to objects not seen before.\nLastly, improvements can be made to the PDDLStream algorithms to increase efficiency further. We can see that stream instance generation can be improved to generate a smaller number of stream instances. Currently, the algorithm exhaustively generates stream instances for all the facts that satisfy the domain of the defined streams. This leads to a large number of stream instances being generated which are not useful to the task and increase the planning time as there is a larger set of facts that need to be searched to find a plan. This efficiency can further be improved by looking at the stream instances which have been generated. There are some stream instances that are more useful to the planner as compared to a few others. Designing a way to rank the generated stream instances using heuristics will help use the more useful ones and discard the less useful ones."}]}