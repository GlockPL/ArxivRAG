{"title": "Tabular Data Augmentation for Machine Learning: Progress and Prospects of Embracing Generative AI", "authors": ["LINGXI CUI", "HUAN LI", "KE CHEN", "LIDAN SHOU", "GANG CHEN"], "abstract": "Machine learning (ML) on tabular data is ubiquitous, yet obtaining abundant high-quality tabular data for model training remains a significant obstacle. Numerous works have focused on tabular data augmentation (TDA) to enhance the original table with additional data, thereby improving downstream ML tasks. Recently, there has been a growing interest in leveraging the capabilities of generative AI for TDA. Therefore, we believe it is time to provide a comprehensive review of the progress and future prospects of TDA, with a particular emphasis on the trending generative AI. Specifically, we present an architectural view of the TDA pipeline, comprising three main procedures: pre-augmentation, augmentation, and post-augmentation. Pre-augmentation encompasses preparation tasks that facilitate subsequent TDA, including error handling, table annotation, table simplification, table representation, table indexing, table navigation, schema matching, and entity matching. Augmentation systematically analyzes current TDA methods, categorized into retrieval-based methods, which retrieve external data, and generation-based methods, which generate synthetic data. We further subdivide these methods based on the granularity of the augmentation process at the row, column, cell, and table levels. Post-augmentation focuses on the datasets, evaluation and optimization aspects of TDA. We also summarize current trends and future directions for TDA, highlighting promising opportunities in the era of generative AI. In addition, the accompanying papers and related resources are continuously updated and maintained in the GitHub repository at https://github.com/SuDIS-ZJU/awesome-tabular-data-augmentation to reflect ongoing advancements in the field.", "sections": [{"title": "1 INTRODUCTION", "content": "Tabular data, such as relational tables, Web tables and CSV files, is among the most primitive and essential forms of data [11] in machine learning (ML), characterized by excellent structural properties, readability, and interpretability. A testament to its significance, more than 65% of datasets available on the Google Dataset Search platform are tabular files [6]. This prevalence underscores its critical role across a myriad of fields, such as finance [81], healthcare [41], education [68]. The growing availability of repositories containing structured or semi-structured data offers new opportunities for tabular data research and applications built upon it, particularly in the fields of ML and artificial intelligence (AI).\nHowever, acquiring substantial amounts of high-quality tabular data for ML model training remains a persistent challenge [17, 64]. This is especially demanding because each individual table is modest in size and self-contained, making the overall data collection process resource-intensive and time-consuming. According to the oft-cited [79] statistics, data scientists spend over 80% of their time on ML data preparation tasks, including data discovery and augmentation. The complexity and uneven quality of massive tabular datasets from various domains further complicate the acquisition of high-quality tabular data [15, 33]. Furthermore, in the era of large language models (LLMs), tabular data is one of the preferred data formats that LLMs consume, and existing high-quality tabular datasets may soon be exhausted [117]. Additionally, in the industrial sector where tabular data is most commonly used, the availability of data is often limited due to privacy concerns [57]. All of these factors have led to significant efforts being devoted to developing techniques that support tabular data augmentation (TDA). Through our extensive investigation, we have collected a"}, {"title": "2 PRELIMINARIES", "content": "In this section, we will start by introducing the notation related to TDA and outlining the level-based taxonomy that defines the various levels of TDA methods (i.e., row, column, cell, and table) in Section 2.1. Subsequently, we will present the TDA pipeline and offer a taxonomy of methods from a task-oriented perspective in Section 2.2. In the following sections, tasks will serve as the primary basis for categorization, with levels providing a more granular categorization criterion."}, {"title": "2.1 Notation in TDA and Level-based Taxonomy", "content": "First, we provide a formalization of tables, a prevalent data structure essential for the organization and presentation of data as follows.\nDefinition 1 (Table). A table T is an arrangement that organizes data into rows and columns, forming a grid of cells for systematic information representation. Each cell, denoted as T[i, j], is at the intersection of row i and column j, serving as the basic unit for data storage. The rows (T[i, :]) run horizontally and group data entries, while columns (T[:, j]) extend vertically, with each focusing on a specific data attribute. Additionally, metadata, such as table captions, provides contextual textual information around the table.\nAn intuitive example of a table and its primary components \u2014 columns, rows, and cells \u2014 is depicted in Fig. 1. Given a table T, we use T.R and T.A to denote the set of its rows and columns (attributes), respectively. Notably, our analysis is restricted to tables that solely manage numerical and textual data structured in rows and columns. This explicitly excludes tables that incorporate nested tables, lists, forms, images, or any other non-textual and non-numerical values within their cells. We now provide the formal definition of tabular data augmentation as follows.\nDefinition 2 (Tabular Data Augmentation, TDA). Given an original table T\u00ba and a specific ML model f() parameterized by \u0398, the task of Tabular Data Augmentation aims to expand TO"}, {"title": "Tabular Data Augmentation for Machine Learning: Progress and Prospects of Embracing Generative Al", "content": "into an augmented table TA that includes additional data values in its rows and/or columns. The goal is for the ML model f(\u0398), trained with TA, to achieve superior model performance compared to the version trained with TO. Formally,\nTA \u2190 TDA(TO, level, [T], [G]),\ns.t. E(f(\u0398TA)) < E(f(\u0398TO)),\nwhere level = {row,column,cell, table} refers to the granularity at which the TDA operates; T, an optional input for retrieval-based TDA, represents the pool of tables (simply called a table pool) for information enrichment use\u2074; G, an optional input for generation-based TDA, implies the use of a specific generative method. and E(f(\u0398TA)) and E(f(\u0398TO)) refer to the empirical errors of the ML models trained on the datasets TA and TO, respectively.\nExample 1. Fig. 2 depicts a typical TDA scenario: A data scientist aims to build a house price prediction model, but the available training data (the original table To containing locations and prices) is limited in features and records, and contains numerous missing or incorrect metadata and cell values (shown in gray). A model trained on this low-quality data is likely to produce subpar results. To address this issue, the data scientist needs to augment the original training set with more comprehensive data. This can be done a) by retrieving additional data from table pools for retrieval-based TDA, or b) by generating new data using existing generative methods for generation-based TDA. The augmented data can include additional attributes, records, and/or cell values, reflected as enriched features and samples in the training set. The primary goal of this TDA process is to improve the overall quality and performance of the downstream house price prediction model.\nReferring to Fig. 2, these TDA sub-tasks are instantiated based on the level that the TDA procedure is acting on. We formally define these tasks one by one as follows.\nDefinition 3 (Row-level TDA). One significant challenge in ML is the scarcity and often uneven distribution of samples, such as in long-tail data. To address this, row-level TDA involves adding additional rows to a given table TO . This procedure aims to obtain more samples for training, potentially increasing the variety of sample categories and altering the sample distribution to some extent. For row-level TDA, only additional rows are considered, and the relationship between TA and TO satisfies:\n(\u0422\u041e.A = TA.A) \u2227 (TO.R \u2282 TA.R).\nExample 2. Continuing from the scenario in Example 1, the data scientist aims to expand the size and diversity of samples in the training set. Fig. 2 (a) demonstrates a generation-based TDA that only takes the original table T\u00ba as input without external data. Generative methods typically learn the structure and the pattern of the original table TO and then generate new synthetic record (shown in purple), resulting in the augmented table TA. These missing values can be addressed during post-processing steps, such as filtering and imputation.\nDefinition 4 (Column-level TDA). Adequate features are crucial for training high-performing ML models, but good features alone are not always enough. Therefore, column-level TDA involves extending the original table with additional columns, enriching the feature set. Retrieval-based TDA at the column level often involves joining related tables. This procedure may necessitate feature engineering"}, {"title": "Tabular Data Augmentation for Machine Learning: Progress and Prospects of Embracing Generative Al", "content": "to remove less informative records and features, which can result in the augmented table having fewer rows than the original (|TO .R| > |TA.R|). Generation-based column-level TDA, on the other hand, retains the number of rows (|T\u00ba .R| = |TA.R|). All in all, it satisfies:\n(\u0422\u041e.\u0410 \u0421\u0422\u0410.\u0410) \u2227 (|T\u00ba.R| \u2265 |T^.R|).\nExample 3. Continuing from Example 1, the data scientist decides to expand the number of features in TO. Still, we take the retrieval-based TDA as an example, as shown in Fig. 2 (b). This procedure involves computing column similarity. For example, if the column \u201cLocation\u201d (the 2nd column in TO) and the column \u201cCity\u201d (the 1st column in table T) have largely similar values, TO can be augmented with the column \u201cDIS (distance)\u201d (the 2nd column from TC), resulting in the augmented table TA.\nDefinition 5 (Cell-level TDA). Empty table cells are common, and generating data representations with null values can lead to suboptimal results in downstream tasks. Cell-level TDA involves filling these empty table cells to improve the quality of training data for ML tasks. For cell-level TDA, the relationship between TA and TO satisfies:\n(\u0422\u041e.\u0410 = \u0422\u0410.\u0410) ^ (|T\u00b0 .R| = |TA.R|) ^ (\u2200i, j : TA[i, j] \u2260 null).\nExample 4. Continuing from Example 1, the data scientist decides to fill the empty cells using a generation-based TDA, as shown in Fig. 2 (c). Generative methods leverage the context (e.g., statistical distribution) from the original table To to fill in missing metadata and data values (shown in blue), resulting in the augmented table TA.\nDefinition 6 (Table-level TDA). Table-level TDA involves enriching the original table with both additional rows and columns. This aims to acquire more features and samples for ML purposes. For table-level TDA, the relationship between TA and TO satisfies:\n(\u0422\u041e.\u0410 \u0421\u0422\u0410.\u0410) ^ (|T\u00b0 .R| < |TA.R|)."}, {"title": "2.2 Pipeline of TDA and Task-based Taxonomy", "content": "In this section, we provide an overview of the key topics covered in the survey, structured around the TDA pipeline from a task-oriented perspective, as shown in Fig. 3. We categorize tasks by levels for finer classification. The pipeline highlights critical stages and procedures from the original training dataset TO to the augmented training dataset TA. We first overview the entire TDA pipeline, followed by a brief introduction to each pivotal procedure within it.\nThe TDA pipeline has two main stages:\n\u2022 Data-centric stage includes pre-augmentation and augmentation procedures to transform the original data into augmented data. Pre-augmentation involves preparation to enhance aug-mentation, while augmentation details existing TDA methods, divided into retrieval-based and generation-based methods.\n\u2022 Model-centric stage focuses on post-augmentation, primarily involving ML model training and evaluation to assess and optimize the augmented dataset. If the augmented dataset is not satisfactory, it is necessary to return to the data-centric stage for further augmentation.\n(1) Pre-Augmentation. In the TDA pipeline, pre-augmentation encompasses preparation tasks to facilitate effective augmentation. For tabular data, issues like missing or incorrect cell values and unreliable metadata are common due to inappropriate data sharing and incompatible naming conventions [33]. For the table pool, with tables reaching millions or more, data preparation before TDA is crucial. Pre-augmentation aims to a) improve the quality of both the original table and the tables from the pool (in the case of retrieval-based methods) and b) better organize the table pool for improved acceleration and scalability. This can involve a range of tasks, applicable to both single-table and multi-table settings, as listed in Table 3. The table's right part presents their support for various different TDA tasks. The following observations can be drawn:\n(1) The tasks in single-table setting are applicable to all target TDA tasks, as all target TDA tasks require preprocessing of the original table.\n(2) The multi-table setting tasks are specifically for handling tables from the pool and may not be suitable for generation-based TDA tasks.\n(3) Entity matching, a pre-augmentation task that focuses on the relationships between rows, may not be much beneficial for the schema augmentation (SAR), a TDA task at the column level.\nSection 3 will provide a detailed introduction to relevant techniques for pre-augmentation tasks.\n(2) Augmentation. Augmentation is the core procedure of the TDA pipeline, enhancing the original table with more data to improve downstream ML tasks. The techniques for TDA can be broadly divided into retrieval-based and generation-based methods. Retrieval-based methods are considered a data-driven TDA task based on the original table TO (called query table), with table pools as additional input. The key to this type of methods lies in properly modeling the similarity between the query table and the tables from the pool. Generation-based methods can effectively leverage pre-existing knowledge acquired from pre-training to augment the input tables, without requiring additional data sources. Both methods can be further subdivided by the level of the augmentation, as listed in Table 4. Section 4 will cover the typical techniques for these TDA tasks."}, {"title": "3 TECHNIQUES IN PRE-AUGMENTATION", "content": "In this section, we review the techniques used in the pre-augmentation procedure, as introduced in Section 2.2. As shown in Table 5, we have selected a collection of representative TDA works and summarized the pre-augmentation tasks they involve. Most of the selected works are oriented to TDA and have been published in well-known conferences or journals with high citation counts, reflecting their significance within the field. We have also included several target tasks other than TDA (see the rightmost part of Table 5), namely table search [15] and semantics detection [105], as these often serve as intermediate steps in TDA.\nOur task-oriented approach, illustrated in Table 3, examines four pre-augmentation tasks for the single-table setting (Sections 3.1 to 3.4) and four for the multi-table setting (Sections 3.5 to 3.8). Pre-augmentation is essential for most TDA works, and the pre-augmentation tasks in Table 3 are not mutually exclusive. A TDA work may involve one or more of these eight tasks. For example, Infogather [100] (No.6 work in Table 5) employs multiple pre-augmentation tasks (table representation, table annotations, etc.) to complete its entire TDA process."}, {"title": "3.1 Error Handling", "content": "Error handling in pre-augmentation refers to the preprocessing of the dirty data in tables. Real-world tabular data often contain errors, such as mistakenly substituted proximal characters and unnecessary repetition of tokens in cell values. Generally, there are three types of errors that considered in pre-augmentation, missing values [14, 114], misspellings [30, 42], and numerical outliers [22]. When generating table representations based on token embedding with such errors,\nRemarks. Real-world tables often contain various types of errors, making it essential for ta-ble augmentation models to be robust in combating such issues. Explicit methods require an established procedure of error detection and correction, adding additional steps and potentially leading to error propagation. On the other hand, implicit methods, while more streamlined, lack the interpretability of explicit methods. Moreover, both methods typically ignore addressing errors in numerical columns, such as out-of-bound values, which are often harder to detect and correct than textual errors. It is worth mentioning a recent trend of exploring the use of pre-trained language models (PLMs) for table representation. These approaches [34, 42] show certain resistance to spelling errors and offer a promising direction for handling noisy data."}, {"title": "3.2 Table Annotation", "content": "Table annotation involves inferring metadata information about a table, such as column types and the relationship between columns [87]. This task helps recover the semantic information within a table and is particularly useful for table augmentation by assessing the similarity between tables. Typically, metadata for tables is unreliable or incomplete due to inappropriate data sharing methods [33]. Even when metadata is available, tables from a wide range of sources can have incompatible metadata with different naming conventions and terminologies. Consequently, table annotation is crucial for retrieving syntactically and semantically relevant tables to augment the original table. These approaches are divided into the three following categories.\nRemarks. Real-world tables often have incomplete and incorrect metadata, necessitating table annotation to recover table semantics. Ontology-based table annotation can be limited by the ontology's coverage, particularly for domain-specific data, while supervised-learning table anno-tation depend heavily on large-scale, high-quality labeled data. Ontology-based methods typically offer high efficiency, whereas learning methods (including those based on supervised learning and those based on PLMs) achieve higher accuracy. One potential direction for improvement involves integrating these methods and exploring the efficient use of PLMs for table annotation. Additionally, integrating LLMs and Retrieval-Augmented Generation (RAG) [112] with ontologies"}, {"title": "3.3 Table Simplification", "content": "Table simplification involves streamlining a table down to its essential elements, which can be addressed from both the content and semantic perspectives. From the table content perspective, this procedure, known as table sampling, involves selecting portions of the table to retain as much information as possible. This is particularly useful for fitting data into limited token lengths for language models. From the table semantics perspective, the procedure, referred to as table summarization, entails identifying the main topic or theme of the table to better understand its meaning. Accurate summarization helps in comparing tables for similarity and ensures that any added rows and columns remain consistent with the table's original theme. These two different perspectives are introduced as follows.\nRemarks. Table simplification aims to extract the core information of a table either in content or semantic level. At the content level, table sampling selectively samples table content for constraints like limited token length for LLMs. Table sampling can also improve efficiency and scalability when facing large-scale tables. At present, most table sampling methods are statistical-based which are fast and resource-efficient, yet they fall short in terms of precision. Sampling based solely on the value distribution (such as sampling the most frequent values), can lead to an incomplete representation of the original table. Therefore, there is a need to investigate more efficient and accurate sampling algorithms, potentially leveraging learning-based approaches. At the semantic level, table summarization serves both to extract the key semantic essence of the table and verify the coherence of any table augmentations. At present, as a preprocessing step, there are relatively few techniques that directly extract the primary topics or themes of tables. This is largely due to the challenge of achieving lightweight, efficient table summaries. Emerging LLMs may offer a promising solution in this regard."}, {"title": "3.4 Table Representation", "content": "Table representation involves converting the table elements such as rows, columns, and cells into a latent vector space. This transformation prepares the table for robust use in subsequent TDA model. The past decade has witnessed the effectiveness of employing deep neural networks for table representation learning [9, 28\u201330, 34, 42, 50, 71, 75, 87, 105], thereby enhancing TDA. The basic idea of table representation is to create vector representations of tables that preserve their syntax and semantics. These vectors can then be compared using methods like cosine similarity to assess their relatedness. The effectiveness of these table representations depends on the information source they contain, which can be summarized as table content, table context, and metadata. We briefly introduce each type of approach as follows.\nRemarks. Table representation forms the basis of tabular data processing and is not limited solely to TDA tasks. Consequently, numerous studies have concentrated on this domain. Early approaches that relied on table content were found inadequate. Current research typically lever-ages both the content and contextual information of tables to create richer table representations, yielding improved outcomes. The use of LLMs has further improved the extraction of semantic information from tables. However, a notable challenge remains in accurately capturing the structural information of tables, such as row/column permutations invariance."}, {"title": "3.5 Table Indexing", "content": "Table indexing involves assigning a unique identifier (index value) to table elements, allowing for quick and efficient lookup and retrieval based on their index values. Many retrieval-based TDA methods use indexes to enhance efficiency and scalability, particularly when dealing with large-scale table pools with millions of tables. Researchers have utilized various types of indexes, such as the inverted index [3, 29, 32, 49, 100, 109, 118], Locality Sensitive Hashing (LSH) index [9, 15, 34, 75, 119], and graph index such as Hierarchical Navigable Small World (HNSW) [30, 34, 69].\nRemarks. Table indexing substantially improves the efficiency and scalability of table retrieval for TDA. While inverted indexes provide high accuracy, they can encounter memory management issues and prohibitively long read times in large-scale table pools. To address these limitations, recent works [9, 34, 75] have explored techniques like LSH and HNSW for table indexing, which can significantly improve efficiency and maintain accuracy. Furthermore, some study [34] has adopted a hybrid approach, combining both LSH and HNSW indexing techniques to utilize the strengths of each method. However, most existing approaches are designed for static table pools. In today's information age, table pools are continuously evolving. Adapting table indexing methods to handle dynamic, ever-changing data remains a major challenge that has yet to be fully addressed."}, {"title": "3.6 Table Navigation", "content": "Table navigation involves establishing a navigational framework over a table pool. Essentially, it refers to organizing the table pool in a way that highlights connections between similar tables, such as through edges in a graph or by clustering them together. With table navigation, the subsequent TDA can retrieve relevant data for augmentation more easily and efficiently. Existing works typically employ cluster structures [16, 50], hierarchical structures [77], or linkage graphs [15, 73, 74] to manage tables in table pools.\nRemarks. Table navigation restructures the table pool, allowing retrieval-based TDA to locate and access similar tables more effectively. This concept is fairly new and continues to evolve. Clusters offer a relatively simple method for organizing table pools, indicating whether tables have similarity or belong to the same class; however, they lack the capability to convey more complex information, such as hierarchical relationships. For both cluster and hierarchical structure, they may not fully capture the relationships within table structures, such as the \u201centity-property\u201d relationship between columns (e.g., entity \u201cperson\u201d has the property \u201cgender\u201d). Both hierarchical structure and linkage graph face efficiency issues. Developing scalable and robust methods for navigating such vast table repositories remains a significant challenge in this field."}, {"title": "3.7 Schema Matching", "content": "Schema matching involves evaluating the relatedness between two table columns. In this context, the set of column headers is typically referred to as the table schema [111]. Schema matching methods are frequently adopted in retrieval-based TDA for identifying and fetching those related columns and tables to expand the features in ML models. Due to the varied data types within tables, schema matching methods are categorized into textual matching, numerical matching, and metadata matching.\nRemarks. Schema matching is a crucial step in retrieval-based TDA, used to determine the similarity between table attributes (columns) and infer the overall similarity between tables. This field has been extensively studied, and different types of schema matching methods are often adopted simultaneously, yielding promising results. However, there remain a wide range of research opportunities, particularly in the domain of numerical schema matching, such as handling different numerical display formats. Even current powerful LLMs cannot handle numbers well. Moreover, the vast majority of existing work in this area has focused solely on the similarity between single columns. The similarity between combined or composite columns is rarely addressed."}, {"title": "3.8 Entity Matching", "content": "Entity matching involves identifying connections between entities in different tables, facilitating the localization of relevant entities and tables for retrieval-based TDA. These methods are particularly relevant in the context of horizontal tabular tables, where entities are typically represented as rows and their attributes are in columns. Based on the data source to which the entities are matched, the methods are categorized into KB-referenced entity matching and DB-referenced entity matching.\nRemarks. Entity matching uncovers relationships between entities across different tables, facilitating the retrieval of relevant entities and tables. For KB-referenced matching, knowledge bases may have limited coverage when applied to real-world table pools. To address this, several works [108\u2013110] have integrated KB- and DB-referenced approaches to broaden the range of information sources. However, a common limitation in both KB- and DB-referenced entity matching is the assumption made in previous works that the leftmost column in a table contains the entity ID or name, which is not always the case. Indeed, entity matching has seen decreased use in the past few years compared to schema matching. An emerging field might be the combination of LLMs and RAG to replace traditional KB-based methods for entity matching. LLMs can better capture entity semantics, reducing the reliance on the leftmost column. Meanwhile, RAG techniques can effectively leverage and update the KB, counteracting the issue of limited scope."}, {"title": "3.9 Scenarios for Pre-augmentation Techniques", "content": "This section empirically summarizes the specific pre-augmentation tasks used in different TDA scenarios, as depicted in Fig. 7. The four single-table-setting pre-augmentation tasks apply to both retrieval and generation-based TDA, while the multi-table-setting tasks are only suitable for retrieval-based methods, which require an external table pool to handle multi-table relationships.\nRetrieval-based TDA operates on a table pool often possessing large-scale data, thus leading to issues such as inconsistent formatting and dynamic data. Common pre-augmentation methods in these scenarios include: (1) table simplification for reducing table information; (2) table indexing and table navigation for fast retrieval; (3) schema matching and entity matching for addressing inconsistent formatting; and (4) table indexing and table navigation for managing dynamic and large-scale table pools.\nGeneration-based TDA using a single original table may suffer from issues in data-scarce sce-narios. In such scenarios, pre-augmentation techniques like table annotation (providing additional information or labels) and table summarization (extracting key information) are necessary. Furthermore, in privacy-preserving scenarios, generating synthetic data often benefits from table sampling, which involves using only partial data. Note that both table summarization and table sampling are part of table simplification, as discussed in Section 3.3.\nMeanwhile, both retrieval- and generation-based TDA approaches face some common challenges, such as low-quality and imbalanced tables. In low-quality scenarios (e.g., tables with null or erro-neous values), common preaugmentation techniques include error handling and table annotation (to annotate column when column names are missing). In imbalanced scenarios, table sampling within"}, {"title": "4 TECHNIQUES IN TABULAR DATA AUGMENTATION", "content": "This section will delve into the current state-of-the-art techniques for tabular data augmentation (TDA). As outlined in Table 4, we first classify TDA tasks into two primary categories: retrieval-based TDA (see Section 4.1) and generation-based TDA (see Section 4.2). Within these two categories, the approaches can be further divided into different levels: adding rows [78, 109] or columns [30, 62], augmenting individual cells [110], and extending the original table with both rows and columns [50]. Thus, for each category, we will discuss the corresponding table augmentation work at the row, column, cell, or table level. Fig. 8 provides a concise overview of the TDA works discussed in this section, along with their detailed taxonomy."}, {"title": "4.1 Retrieval-based TDA", "content": "By retrieval-based, we refer to the process of enhancing the original table (query table TO) with realistic data sourced from table pools T = {T;}. One important difference between augmenting tabular data and augmenting other data modalities lies in the availability of existing data resources (such as databases and data warehouses), which provide opportunities for discovering fresh, realistic data. In contrast, other data modalities (such as images) primarily rely on transforming the original data to generate new data that has not been seen before [17]. Retrieval-based TDA tasks are further divided into Entity Augmentation (EAR) at the row level, Scheme Augmentation (sa\u20a8) at the column level, Cell Completion (cc) at the cell level, and Table Integration (TIR) at the table level. They will be introduced as follows."}, {"title": "4.1.1 Entity Augmentation (EAR)", "content": "Previous works [100, 111] define retrieval-based TDA at the row-level as entity augmentation, as rows in tabular data generally correspond to specific entities. Entity augmentation extends a given table with more rows or row elements retrieved from table pools. Directly retrieving table rows without considering the context of the entire table is not feasible. Therefore, existing solutions typically first search for tables that are unionable with the input table and then select entities from these unionable tables to augment the original table. As we shall see, table search [15, 27, 89] is inherently involved in this process, serving as an intermediate step that feeds into tabular data augmentation [111].\nRemarks. Entity augmentation can expand and diversify the samples for ML, making it a long-standing research focus. Statistical entity augmentation methods are typically based on fixed distribution assumptions and hard to refine. Meanwhile, it may encounter issues of computational efficiency and scalability when dealing with large-scale datasets. KB-based entity augmentation, on the other hand, suffers from limited KB coverage, often leading to low recall in practical scenarios. However, with the development of LLMs, the incorporation of RAG may be a future direction. RAG combines the dynamically retrieved information with the ability of generative models, reducing information loss and improving information relevance, thereby increasing recall rates. Graph-based entity augmentation, which converts tables to graphs, can better grasp the structure relationship of tables, but it also requires an additional encoding mechanism for"}, {"title": "4.1.2 Schema Augmentation (sA\u00ae)", "content": "We refer to retrieval-based TDA at the column level as schema augmentation, the process of extending the schema of the original table with additional columns from table pools. This process is somewhat similar to the join operation commonly executed in databases. To be specific, given a table pool T containing m tables {T}, the task of schema augmentation SAR is to search T and find the columns T\u2081[:, j] joinable to the query table TO. As a result, the result table TR = SAR (TO, column, T) can boost the downstream ML task. To determine the joinability between two columns, the column values, column semantics, and table structure are the most considered three perspectives. The corresponding subcategories are introduced below.\nRemarks. Schema augmentation extends the original table with more features, thereby improving the downstream ML tasks. This area has evolved from traditional value-based joins, which depend on exact value matches, to more sophisticated semantic- and structure-based joins. Semantic-based joins have seen a trend of leveraging generative AI (e.g., PLMs) as encoders to capture the inherent semantics and contextual relationships within and across table columns. However, due to token length limitations, these methods are often restricted to handling table columns with relatively small cardinalities. When dealing with columns that have high cardinalities, additional sampling or chunking steps are required, which can lead to potential information loss and reduced join performance. In structure-based joins, most methods utilize graphs to capture the table structure. It has been observed that more complex graph structures can better represent the nuanced aspects of the table schema and relationships. However, this increased complexity often requires more resources. Therefore, it is important to explore a balanced approach that finds a careful equilibrium between the level of structural detail captured and the computational resources needed."}, {"title": "4.1.3 Cell Completion (cc\u00b2)", "content": "As with previous work [110], we refer to retrieval-based TDA at the cell level as cell completion. This task involves filling in empty cells within the input table by leveraging information extracted from table pools. Cell completion can be divided into several subtasks based on cell types: populating attribute names, adding additional entity IDs/names, and filling values for table cells [110]. The approaches per category are introduced as follows.\nRemarks. Missing values are common and typically have a negative impact in practice, making cell completion crucial. This field has been actively studied for decades, and multiple works [40, 109] consider addressing various cell types simultaneously. For attribute name and entity ID/name completion, an internal problem is that additional cell completion is required after these two operations to obtain a complete table, which may lead to problem propagation. Another issue is that retrieval-based methods may sometimes be outperformed by simple statistical techniques like averaging, especially in domain-specific tasks. Consequently, given the successful application of RAG in the NLP field, there is an opportunity to combine retrieval-based and generation-based methods [90, 95], as generative approaches can better capture the inherent structure and patterns within tables. Future advancements could involve integrating generative AI models, such as language models, into this hybrid approach."}, {"title": "4.1.4 Table Integration (\u03c4\u03b9\u00b2)", "content": "We define retrieval-based TDA at the table level as table integration, the procedure of extending the original table with both rows and columns from related tables retrieved from the table pool via table search algorithms. There are two main methods for imple-menting table integration: one is compositional table integration, which combines retrieval-based TDA results at various levels (rows, columns, and/or cells), while the other is direct table integration, which directly enriches the original table with the content from the retrieved related tables.\nRemarks. Table integration is a relatively new concept compared to other levels of retrieval-based TDA tasks, thus it still has a vast range of unexplored opportunities for further research and development. For compositional approaches that concatenate combining different levels of TDA, there is a risk of error propagation. The compounding of potential errors or biases from the individual TDA tasks can diminish the overall effectiveness and reliability of the integrated table. For direct approaches, there are relatively few works so far, possibly due to the lack of integration benchmarks. Most existing works rely on strong assumptions and are only applicable to small table pools. Researchers and practitioners may need to develop more robust and scalable integration techniques, as well as establish comprehensive benchmarking platforms."}, {"title": "4.2 Generation-based TDA", "content": "Generation-based TDA refers to the augmentation of tabular data through the generation of synthetic data. Unlike retrieval-based methods, generation-based methods do not require external data sources and are often built upon generative models. Generation-based TDA tasks can be further categorized into the following sub-tasks: Record Generation (RG\u00ba) at the row level, Feature Construction (Fc\u00ba) at the column level, Cell Imputation (cr\u00ba) at the cell level, and Table Synthesis (Ts) at the table level. These generation-based TDA tasks will be introduced and discussed in more detail in the following sections."}, {}]}