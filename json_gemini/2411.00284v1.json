{"title": "SimpleFSDP: Simpler Fully Sharded Data Parallel with torch.compile", "authors": ["Ruisi Zhang", "Tianyu Liu", "Will Feng", "Andrew Gu", "Sanket Purandare", "Wanchao Liang", "Francisco Massa"], "abstract": "Distributed training of large models consumes enormous computation resources and requires substantial engineering efforts to compose various training techniques. This paper presents SimpleFSDP, a PyTorch-native compiler-based Fully Sharded Data Parallel (FSDP) framework, which has a simple implementation for maintenance and composability, allows full computation-communication graph tracing, and brings performance enhancement via compiler backend optimizations.\nSimpleFSDP's novelty lies in its unique torch.compile-friendly implementation of collective communications using existing PyTorch primitives, namely parametrizations, selective activation checkpointing, and DTensor. It also features the first-of-its-kind intermediate representation (IR) nodes bucketing and reordering in the TorchInductor backend for effective computation-communication overlapping. As a result, users can employ the aforementioned optimizations to automatically or manually wrap model components for minimal communication exposure. Extensive evaluations of SimpleFSDP on Llama 3 models (including the ultra-large 405B) using Torch Titan demonstrate up to 28.54% memory reduction and 68.67% throughput improvement compared to the most widely adopted FSDP2 eager framework, when composed with other distributed training techniques.", "sections": [{"title": "1 Introduction", "content": "Distributed training the ever-growing large models necessitates huge computation resources Rae et al. (2021);\nZhang et al. (2022); Chowdhery et al. (2023); Dubey et al. (2024) and engineering efforts Shoeybi et al.\n(2019); Rasley et al. (2020); Liang et al. (2024), both of which pose significant challenges as the model size\nscales. For example, training the Llama 3.1 405B Dubey et al. (2024) model takes 30.84 million H100 GPU\nhours, and PaLM-540B Chowdhery et al. (2023) model takes 9.4 million TPUv4 hours. During training,\nvarious parallelisms Huang et al. (2019); Shoeybi et al. (2019); Zhao et al. (2023), memory optimizations Chen\net al. (2016); Korthikanti et al. (2023), and communication optimizations Micikevicius et al. (2017); Zhao\net al. (2023); Choudhury et al. (2024) are employed to improve computation throughputs and minimize\ncommunication exposure.\nFully Sharded Data Parallel (FSDP) Zhao et al. (2023), motivated by the DeepSpeed ZeRO Rajbhandari\net al. (2020), is one of the most fundamental techniques for distributed large model training. It significantly\nsaves memory by sharding model parameters, gradients, and optimizer states across multiple devices and\nonly gathers them when needed. As such, it is widely adopted to train large generative models Le Scao et al.\n(2023); Dubey et al. (2024) and has been deployed in open-source libraries, like NeMo Kuchaiev et al. (2019),\nDeepSpeed Rasley et al. (2020), and TorchTitan Liang et al. (2024).\nFSDP is primarily developed in the PyTorch eager (i.e., non-compile) mode, where model operators are\nexecuted immediately after definition. It preserves debuggability and enables certain mechanisms like pre-\nfetching via backward hooks PyTorch Community (2023d), which are hard to trace in the compile mode Ansel\net al. (2024). However, the eager mode impairs the training performance, as the model cannot be compiled as\na whole graph, thereby losing opportunities for hardware-specific computation optimizations and efficient"}, {"title": "memory management.", "content": "Prior works bringing machine learning compilers into distributed training mainly go from two directions: (1)\nJAX-based Bradbury et al. (2018); Xu et al. (2021) that uses XLA Sabne (2020) as the compile backend and\nshard tensors via user annotations; (2) PyTorch-based Liang et al. (2024), which leverages torch.compile Ansel\net al. (2024) to trace per-device compute submodules and insert inter-module communications. JAX adopts\nfunctional programming and imposes certain constraints to ensure compatibility with the XLA backend. This\ngreatly hinders the programmability in distributed training, which stacks many emerging techniques and\nrequires agile development.\nPyTorch-based approach Liang et al. (2024), on the other hand, only compiles the model's computation modules,\nas the FSDP eager-mode implementations like prefetching are hard to be traced by torch.compile. Hence, it\nloses the opportunity to compile a full model graph for communication/computation co-optimization and\nintroduces additional codebase complexity by requiring the manual insertion of inter-module communications.\nThis paper presents SimpleFSDP, a PyTorch-native compiler-based FSDP framework. It features (1) Simplicity:\nusers do not need to alter the eager-mode distributed training codebase while experiencing the performance\nenhancement from full-model compilation; (2) Composability: SimpleFSDP can be seamlessly integrated\nwith emerging distributed training techniques with minimal engineering effort; (3) Performance: training\nthroughputs and memory gains from full-graph tracing and compiler optimizations; and (4) Debuggability:\nSimpleFSDP exhibits usability in PyTorch eager mode, where users have the flexibility to debug and agile\ndevelop the codebase.\nSimpleFSDP achieves the FSDP semantics by utilizing a few existing PyTorch primitives. First, representing\nthe sharded per-parameter as DTensors PyTorch Community (2023b), SimpleFSDP achieves the \"all-gather\nbefore usage\" behavior by applying collective communications (via the DTensor redistribute API) as tensor\nparametrization. Note that the backward gradient reduce-scatter is automatically achieved as parametrization\nand DTensor redistribute are differentiable. Second, given that in parametrization PyTorch Community\n(2023f), parameter all-gathers are treated as activation computations, SimpleFSDP achieves the additional\nmemory optimization of \"release after forward usage, all-gather again before backward usage\" by wrapping\nthe parametrization module using activation checkpointing PyTorch Community (2023a). Since parametriza-\ntion, selective activation checkpointing, and DTensor APIs are all natively supported by torch.compile,\nSimpleFSDP obtains a full graph of communication and computation operations.\nSimpleFSDP introduces two optimization components in torch.compile's backend TorchInductor, namely\nbucketing and reordering, to enhance the per-parameter sharding performance. The bucketing merges the\ncommunication operations\u00b9 in TorchInductor to reduce the frequency of issuing base communication. The\nreordering pre-fetches the parameters used for computation in later stages to overlap with the current stage's\ncomputation for minimized communication exposure. Building on top of the optimizations, SimpleFSDP\nprovides two interfaces to users to wrap the model, enabling both customization and automation. The first\nmanual-wrapping enables users to customize the communications to bucket among modules and reorders the\nbucketed communication operations to reduce exposure. The auto-wrapping employs a greedy algorithm to\nbucket the communication operations as long as they can be overlapped by the computation operations and\ndo not exceed memory limits.\nSimpleFSDP's PyTorch-native implementation enables it to be seamlessly composed with other distributed\ntraining techniques. We demonstrate its composability with Tensor Parallel and Pipeline Parallel, meta\ninitialization, mixed precision training, and activation checkpointing with only a few lines of code. Such\ncomposability is achieved while tracing the model's full computation-communication graph and tested on\nscales up to the ultra-large 405 billion parameter Llama 3.1 model Dubey et al. (2024).\nIn summary, our contributions are as follows:\n\u2022 We introduce SimpleFSDP, a PyTorch-native compiler-based FSDP framework featuring simplicity,\ncomposability, performance enhancement, and debuggability.\n\u2022 We devise SimpleFSDP highlighting (1) a unique collective communication implementation of FSDP via\nPyTorch primitives (parametrizations, selective activation checkpointing, and DTensor API), enabling"}, {"title": "2 Background and Challenges", "content": "This section first introduces techniques and related work for accelerating large models' distributed training.\nWe then present several challenges that state-of-the-art frameworks face when supporting these techniques."}, {"title": "2.1 Distributed Training Large Models", "content": "Training large models in a distributed manner reduces the memory requirements per device and accelerates\nthe computation throughputs.\nFully Sharded Data Parallel (FSDP) Zhao et al. (2023) is one of the most fundamental forms of data parallelism\nin distributed training. It shards model parameters, gradients, and optimizer states across multiple devices.\nDuring training, it gathers the needed parameters for computation and discards them immediately to save\nmemory. A typical FSDP training consists of\n\u2022 Model Initialization & Parameter Sharding: The model is wrapped into FSDP units and partitioned per the\nnumber of devices for parameter sharding. Each device only holds one of the partitions.\n\u2022 Forward Pass: Each FSDP unit gathers the parameters from other devices and performs the computation.\nThe parameters are discarded immediately after the computation to save memory.\n\u2022 Backward Pass: Similar to the forward pass, each FSDP unit re-gathers the parameters and computes the\ngradient. The gradients are averaged and sharded across devices.\nTensor Parallel Shoeybi et al. (2019); Narayanan et al. (2021) partitions and shards tensors of an individual\nlayer across multiple devices. Each device computes the sharded part of the layer simultaneously and\nconcatenates them together for the outputs. Pipeline Parallel Huang et al. (2019); Narayanan et al. (2019); Li\net al. (2021) partitions a model that cannot fit in a single device's memory into multiple stages. Each device\nconcurrently processes a stage over multiple batches of data.\nMeta initialization Zhao et al. (2023) initializes the model parameters on a meta device PyTorch Community\n(2023e) (an abstract device that denotes a tensor and records only metadata) rather than the actual CPU/GPU\ndevice. It reduces the time and memory required for initialization. Mixed precision training Micikevicius\net al. (2017) reduces memory usage by training the model using 16-bit floating-point numbers. In the\ngradient updates, parameters are cast back to 32-bit floating-point numbers for training stability. Activation\ncheckpointing Chen et al. (2016); Korthikanti et al. (2023) reduces the memory consumption by selectively\nstoring activations at certain layers in the forward pass and recomputing the rest during the backward pass.\nIt significantly reduces the peak memory and allows model training on memory-constraint devices."}, {"title": "2.2 Related Work", "content": "Machine learning compilers Chen et al. (2018); Sabne (2020); Ansel et al. (2024) accelerates model training by\noptimizing the computation graph execution on different target hardware devices and by performing careful\nmemory management. In torch.compile Ansel et al. (2024), the frontend TorchDynamo captures the FX\ngraph from the user code by just-in-time (JIT) compiling Python bytecode; the default backend TorchInductor\ntakes the FX graph operations as input and lowers the graph to a set of intermediate representation (IR)"}, {"title": "2.3 Challenges", "content": "Applying machine learning compilers to distributed training exhibits a few challenges, as outlined below.\nComplexity Distributed training combines various parallelisms and memory-saving techniques to fit larger\nmodels and increase throughputs, making the codebase inherently complicated, especially when the techniques\nstrives to improve eager-mode performance. This makes the integration and tracing of machine learning\ncompilers difficult.\nDebuggability While machine learning compilers offer performance enhancements, the debuggability from the\neager mode remains crucial. It allows users to experiment with different building blocks for agile development.\nHowever, these debugging practices may violate the compilation rules, making the code untraceable Bradbury\net al. (2018); Sabne (2020). As a result, a framework that preserves both debuggability under eager mode and\nperformance enhancement from compile mode is essential.\nComposability Existing parallelism implementations (e.g. DDP and FSDP in PyTorch) use backward hooks\nto perform efficient collective communications, making it difficult for torch.compile to trace. Although\nattempts have been made to enable full-graph compilation in those scenarios, integrating them with emerging\ndistributed training techniques are still challenging."}, {"title": "3 SimpleFSDP Design", "content": "Identifying the challenges, we introduce SimpleFSDP, which maintains distributed training's simplicity and\ndebuggability. Then, we incorporate several compiler-only optimization components to enhance SimpleFSDP's\nperformance. We demonstrate SimpleFSDP's composability with other distributed training techniques in\nSection 4."}, {"title": "3.1 Overview", "content": "In this section, we introduce how SimpleFSDP realizes the FSDP semantics using existing PyTorch primitives,\nnamely parametrization PyTorch Community (2023f) and selective activation checkpointing PyTorch Commu-\nnity (2023a), together with the DTensor abstractions PyTorch Community (2023b). All these techniques are\nnow natively supported by PyTorch torch.compile.\nIn FSDP (or ZeRO-3), optimizer states, gradients, and model parameters are all sharded. The parameters are\nall-gathered in the forward pass and used for both forward and backward computation, whereas the computed\ngradients are reduce-scattered after backward computation for optimizer updates. SimpleFSDP shards the\nparameters as DTensors during model initialization and utilize PyTorch primitive parametrization and DTensor\nAPI redistribute to implement the all-gather in the forward pass, as in Figure 1's ReplicateComputation.\nSince DTensor's redistribute and parametrization are differentiable, the gradient reduce-scatter in the\nbackward pass is automatically handled."}, {"title": "3.2 Optimizations", "content": "The graph traced from SimpleFSDP is lowered to a set of IR nodes in TorchInductor. This alone does not yield\noptimized training performance, as the communication and computation operations from ReplicateComputation\nare shared per-parameter in sequential order, and all of the communication operations are exposed. As\ndepicted in Figure 2, we introduce two optimizations in TorchInductor to enhance the distributed training\nperformance: (1) Bucketing to group and merge communication IR nodes to reduce the frequency of issuing\nbase communication; (2) Reodering to prefetch the communication IR nodes for overlapping with current\ncomputation."}, {"title": "3.2.1 Bucketing", "content": "The communication cost between two devices comprises a base latency for establishing the communication and\na transmit latency proportional to the transmitted word size NVIDIA (2024). By bucketing the communication\nIR nodes, SimpleFSDP issues the base communication once for all of the bucketed nodes and thereby reduces\nthe overall communication time.\nAs in Figure 2, the individual all-gather/reduce-scatter IR node reads the data and issues the communication.\nTo bucket the all-gather IR nodes AG1 and AG2, SimpleFSDP allocate a bigger buffer that flattens and\nconcatenates the tensor from each individual all-gather. The new all-gather AG12 and all-gather-wait Wa12\nare created to gather the bigger buffers from other devices and copy out the gathered data based on their\noriginal tensor size.\nTo bucket reduce-scatter, SimpleFSDP splits the obtained gradient into chunks based on world size and\nconcatenates the gradients from the individual reduce-scatter RS1 and RS2's data to create a bigger buffer.\nA new reduce-scatter RS12 and reduce-scatter-wait Wr12 are created to average the buffer data gathered\nfrom other devices. The gradients are read out from RS12 to update the local model weight."}, {"title": "3.2.2 Reordering", "content": "The collective communication all-gather and reduce-scatter are asynchronous, allowing it to occur concurrently\nwith the computation on different CUDA streams. In Figure 2, in the forward pass, each computation\nhas an all-gather and an all-gather-wait to gather the data; in the backward pass, each computation has\nadditional reduce-scatter and reduce-scatter-wait to update the gradient. Reordering the IR nodes ensures\nthe communications can overlap by the computations and thereby reduces the communication exposure.\nAs in Figure 2, we use the manual wrapping as an example; the reordering process is as follows: (1) In the\nforward pass, the AG34 is reordered in front of Wa12. It allows AG34 to overlap with compute C1; (2) In\nthe backward pass, the AG34 is placed after Wa12, enabling AG34 to overlap with C1. The Wr12 is placed"}, {"title": "3.3 Model Wrapping", "content": "Building on top of the optimizations in Section 3.2, SimpleFSDP provides two wrapping interfaces, namely,\nmanual-wrapping and auto-wrapping, to bucket communication IR nodes together and reorder them for\noverlapping with computation operations.\nThe manual-wrapping buckets communication IR nodes based on pre-defined module lists. It provides the\nsame functionality as those in FSDP2 PyTorch Community (2023c), where users can customize module\nwrapping after model definition. The auto-wrapping provides a more fine-grained and automatic bucketing\ninterface, where no input from users is required. As the model is shared per parameter, SimpleFSDP employs a\ngreedy algorithm to atomically bucket communication IR nodes from each parameter for minimized exposure."}, {"title": "3.3.1 Manual-wrapping", "content": "In TorchInductor, each IR node contains metadata that traces its original module name. It enables SimpleFSDP\nto construct a mapping between module names and their corresponding IR nodes. Thus, users can customize the\nwrapping rules by providing a list of module names. SimpleFSDP then wraps the communication/computation\nnodes between these modules. As in Figure 2, the communication IR nodes from module 1 and module 2 are\nbucketed separately and reordered to overlap the bucketed communication."}, {"title": "3.3.2 Auto-wrapping", "content": "Profiling The profiling algorithm estimates the IR nodes' communication and computation time in TorchInduc-\ntor. For the computation node, SimpleFSDP converts the FakeTensor (containing tensor metadata without\nactual data) into real PyTorch Tensors. It executes the computation node's Python kernel with these real\nTensors as input and records the CUDA event time $T_c$ and the peak memory $M_c$. For the communication\nnode, we formulate the estimated communication time as $T_m = \\alpha + \\beta n$ where $n$ denotes the transmitted word\nsize and $\\alpha, \\beta$ are the transmit parameters NVIDIA (2024).\nWrapping The wrapping algorithm automatically buckets the communication IR nodes to minimize communi-\ncation exposure while keeping the memory within the limit."}, {"title": "3.4 User interface", "content": "SimpleFSDP provides simple plug-in-play interface. After defining the parallelism configs, users employ the\nsimple_fsdp API to wrap the model with SimpleFSDP, as introduced in Section 3.1. Then, the torch.compile\nAPI compiles the model, tracing both communication and computation operations.\nThe model wrapping, including reordering and bucketing, is handled by the TorchInductor backend. No\nadditional modifications are required for the existing distributed training codebase. It greatly reduced the\nburden of maintaining distributed training code while providing performance gains.\nfullgraph=True generates a full model graph. If the model has untraceable content, e.g., data-dependent\ncontrol flow, setting fullgraph=False splits the graph into several subgraphs for SimpleFSDP to optimize."}, {"title": "4 Composability", "content": "SimpleFSDP is natively implemented with DTensor, parametrization and selective activation checkpointing,\nmaking it easy to be integrated with techniques in Section 2.1 to train large models with few lines of code.\nMeta initialization During model weight initialization on the meta device, SimpleFSDP disables the all-gather\nparametrization to reduce the time and memory required to load the model.\nMixed precision training The param_dtype and reduce_dtype are parsed into the DTensor redistribution. During\ntraining, the model parameters are cast to param_dtype, while the gradients are cast to reduce_dtype. Mixed"}, {"title": "5 Experiments", "content": "Infrastructure We build SimpleFSDP in PyTorch TorchInductor with ~ 2K LoC. The benchmarking is\nperformed on Torch Titan Liang et al. (2024). Our evaluation environment includes a CPU/GPU cluster with\n16 nodes. Each node has 8 NVIDIA H100 GPUs, and the intra-node is connected via NVLink Wei et al.\n(2023).\nTarget Models and Metrics We evaluate SimpleFSDP on Llama 3.1 series Dubey et al. (2024) models of various\nsizes. The details are in Table. 2."}, {"title": "5.1 SimpleFSDP Performance", "content": "Figure 3a shows the memory and throughput for Llama 3.1 8B trained with FSDP on 32, 64, and 128 H100\nGPUs. Compared to FSDP2-eager, SimpleFSDP on average saves 27.72% peak memory and improves the\nthroughput by 7.49%. The performance gains are primarily from the memory optimizations, IR node fusions,\netc, in torch.compile that make the training more efficient. Compared to FSDP-compile, SimpleFSDP still\nmaintains 8.65% peak memory reduction by tracing the full-graph, where torch.compile obtains a global\nview and allocates the memory better. The transformer-based LLaMA models are computation-intensive,\nmeaning the communication is fully overlapped when only FSDP is applied. As such, both SimpleFSDP and\nFSDP2-compile hit the throughput upper bound for training the Llama 3.1 8B model, resulting in on-par\nthroughput performance.\nWhere memory savings come from We identify the following three main reasons for SimpleFSDP's memory\nsavings. (1) SimpleFSDP works at a tensor-level, allowing a finer granular memory management (e.g. all-\ngathered parameters can be released sooner), compared with FSDP2's module-level behavior. This gives\nmemory advantage to SimpleFSDP on the Llama model. (2) torch.compile makes different decisions on\nwhat activations to save for FSDP2 block-level compilation and SimpleFSDP whole-model compilation. The\nFSDP2-compile will save additional transposed tensors from scaled dot product attention (SDPA) outputs,\nwhereas SimpleFSDP only saves SDPA outputs. (3) SimpleFSDP manages bucketing on the same CUDA\nstream as the rest computes. FSDP2 uses multiple streams, which has a throughput benefit but can cause\nmemory allocation fragmentation."}, {"title": "5.2 SimpleFSDP Composability and Scalability", "content": "In this section, we present the performance after composing SimpleFSDP with Tensor Parallel and Pipeline\nParallel. All of the models employ full activation checkpointing (AC) and mixed precision training. In the\ncompile mode, we apply Asynchronous Tensor Parallel Wang et al. (2022)."}, {"title": "2D Composability", "content": "The Llama 3.1 8B and 70B models are trained with FSDP and Tensor Parallel Shoeybi et al.\n(2019). The Tensor Parallel degree is set to 8, and the batch size is set to 16 and 8, respectively. Figure 3c\nand 3e show the performance when training Llama 3.1 8B and 70B models on 32, 64, and 128 GPUs.\nSimpleFSDP can be integrated with Tensor Parallel without degrading the performance. As seen, when training\nthe 8B model, compared to eager mode, SimpleFSDP averagely saves 2.67% peak memory and improves\nthe throughputs by 29.35%. Apart from the IR node fusion, SimpleFSDP benefits from the Asynchronous\nTensor Parallel Wang et al. (2022) that overlaps the submatrix multiplication with communication operations.\nCompared to FSDP2-compile, by tracing the full-graph, SimpleFSDP further demonstrates 2.09% throughput\nimprovement.\nAs the model size becomes larger, the full graph traced by SimpleFSDP provides more memory optimization\nopportunities in torch.compile and yields better performance. As in the 70B model, SimpleFSDP improves\nFSDP2-eager's throughputs by 28.26% and reduces the training peak memory by 11.61%. While both\nSimpleFSDP and FSDP-compile hit the throughput upper bound, SimpleFSDP further reduces 11.40% peak\nmemory from the full-graph tracing."}, {"title": "3D Composability", "content": "The Llama 3.1 70B models are trained with FSDP, Tensor Parallel Shoeybi et al. (2019),\nand Pipeline Parallel Huang et al. (2019). The Tensor Parallel degree is set to 8, the Pipeline Parallel degree\nis set to 8, and the batch size is set to 16.\nSimpleFSDP can be integrated with Pipeline Parallel without performance degradations. As seen in Figure 3b,\nSimpleFSDP improves the throughput by 20.31% compared to FSDP2-eager while incurring less than 1GiB\npeak memory overhead. The models are partitioned into smaller submodules, with each device receiving a\nsubgraph of the full model. SimpleFSDP optimizes a smaller graph compared to 2D settings, thus achieving\ncomparable performance with FSDP2-compile. Notably, SimpleFSDP traces a communication-computation\npartitioned subgraph on each device, making it possible to overlap the bubbles from 3D training and bring\nopportunities for future optimizations."}, {"title": "Scalability", "content": "We show the Llama 3.1 405B model 2D parallelism training performance in Figure 3d and 3D\nparallelism performance in Figure 3f. The Tensor Parallel degree is set to 8, the Pipeline Parallel degree is set\nto 16, and the batch size is set to 2 in 2D parallelism and 16 in 3D parallelism.\nSimpleFSDP is scalable and maintains the performance enhancement when training ultra-large models. As\nseen, compared to FSDP2-eager, SimpleFSDP improves the throughput by 68.67% and 5.66% on 2D and\n3D parallel, respectively. Besides, SimpleFSDP reduces the memory by 16.26% and 2.64% on 2D and 3D\nparallelism. Compared to FSDP2-compile, by tracing the full model graph, SimpleFSDP improves the\nthroughput by 6.06% on 2D parallel and reduces the memory by 8.37% and 4.63% on 2D and 3D parallel,\nrespectively.\nThe throughput gains and memory savings become more significant when training large models at scale: (1)\nLarge model training requires millions of GPU hours Dubey et al. (2024); Chowdhery et al. (2023), thereby\neven small per-iteration throughput gains substantially reduce overall training time.; (2) The memory savings\nallow for larger batch sizes training per iteration, which in turn increases the throughput."}, {"title": "5.3 Auto-Wrapping Performance", "content": "The auto-wrapping performance is in Figure 4. The communication operations are fully-overlapped when\ntraining Llama models with only FSDP. Hence, We show the performance when training Llama 3.1 8B and\n70B on 2D parallelism, where Asynchronous TP communication is exposed. Other settings follow Section 5.2.\nSimpleFSDP-Auto reduces the exposed communication during large model training and does not require manual\nwrapping plans defined by the users. As seen, in 8B model, SimpleFSDP-Auto achieves ~ 7.34% throughput\nimprovement over SimpleFSDP-Manual while maintaining comparable memory consumption. It means\nmore communications are overlapped by SimpleFSDP-Auto, providing both automation and performance\nenhancement to users.\nHowever, we also provide one case where SimpleFSDP-Auto provides 0.8% throughput improvement when\ntraining Llama 3.1 70B models on 64GPUs but incurs 10.61GiB memory overhead. It is primarily because"}, {"title": "SimpleFSDP", "content": "-Auto prioritizes minimizing the exposed communication, and the memory threshold we set is\nlarger than the peak memory in SimpleFSDP-Manual. As a result, SimpleFSDP-Auto gives a suboptimal\nsolution and scarifies the memory for throughput improvement. The major focus of SimpleFSDP is providing\nan elegant way of tracing a full graph with both communication and computation operations for downstream\napplications. We leave exploring algorithms to generate more optimal overlapping plans as future work."}, {"title": "5.4 Analysis and Ablation Study", "content": "This subsection presents analysis of how different optimization components impact SimpleFSDP's performance.\nBy default, we train the Llama 3.1 8B model on 8 H100 with only FSDP and set the batch size to 1. Additional\nablation studies are in the appendix.\nDebuggability Apart from the compile mode performance gains, SimpleFSDP exhibits usability in the\nPyTorch eager mode. It offers users the flexibility to print variables and experiment with various building\nblocks for debugging and agile development. As is shown in Table 3, SimpleFSDP achieves comparable\nmemory consumption and throughput to FSDP2, which is primarily developed for the eager mode\u00b2. Notably,\nSimpleFSDP offers eager-mode debuggability with greater simplicity, composability, and performance gains in\ncompile mode.\nTraining convergence SimpleFSDP and its optimization components will not alter the training convergence.\nFigure 5 compares the loss plots of FSDP2 and SimpleFSDP when training the Llama 3.1 8B model for\n1,000 epochs on 8 H100 GPUs. As seen, the similar loss convergence for both methods demonstrates that\nSimpleFSDP maintains model convergence and training stability.\nCompilation time The time takes to compile the Llama 3.1 8B model is in Table 4. We split the total\ncompilation time to reorder and bucket IR nodes in SimpleFSDP, and the rest time to compile the model in\nTorchInductor. As seen, compiling SimpleFSDP incurs negligible overhead compared to the overall training\ntime, making it efficient.\nThe effectiveness of reorder and bucket Table 5 shows the impact of reordering and bucketing on single and\nmulti-node training. In single-node, reordering enables the computation and communication to happen\nconcurrently. It increases the training throughput and memory usage. Bucketing further increases memory by"}, {"title": "6 Discussion", "content": "In this section, we discuss SimpleFSDP in different use cases and potential future work.\nGraph breaks in model tracing While SimpleFSDP obtains a full graph with both communication and computation\noperations, it does not require users to write code adhering to strict compilation constraints (e.g., avoiding\ndata-dependent control flow or variable printing). Built upon torch.compile, when encountering non-traceable"}, {"title": "7 Conclusion", "content": "We present SimpleFSDP, a PyTorch-native compiler-based FSDP framework. It features simplicity for\ndistributed training codebase maintenance, composability with other efficient training techniques, performance\nenhancement from full graph tracing, as well as debuggability and programmability from the PyTorch\neager mode. Building on top of the unique parametrizations implementation of all-gather to checkpoint\nparameters, SimpleFSDP buckets and reorders the IR nodes for minimized communication exposure and\nprovides customized and automated model wrapping interfaces to users. Extensive evaluations demonstrate\nSimpleFSDP's efficacy in throughput gains, memory saving, and scalability toward tracing ultra-large models."}]}