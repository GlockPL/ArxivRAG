{"title": "Rethinking the Intermediate Features in Adversarial Attacks: Misleading Robotic Models via Adversarial Distillation", "authors": ["Ke Zhao", "Huayang Huang", "Miao Li", "Yu Wu"], "abstract": "Language-conditioned robotic learning has significantly enhanced robot adaptability by enabling a single model to execute diverse tasks in response to verbal commands. Despite these advancements, security vulnerabilities within this domain remain largely unexplored. This paper addresses this gap by proposing a novel adversarial prompt attack tailored to language-conditioned robotic models. Our approach involves crafting a universal adversarial prefix that induces the model to perform unintended actions when added to any original prompt. We demonstrate that existing adversarial techniques exhibit limited effectiveness when directly transferred to the robotic domain due to the inherent robustness of discretized robotic action spaces. To overcome this challenge, we propose to optimize adversarial prefixes based on continuous action representations, circumventing the discretization process. Additionally, we identify the beneficial impact of intermediate features on adversarial attacks and leverage the negative gradient of intermediate self-attention features to further enhance attack efficacy. Extensive experiments on VIMA models across 13 robot manipulation tasks validate the superiority of our method over existing approaches and demonstrate its transferability across different model variants.", "sections": [{"title": "I. INTRODUCTION", "content": "IMITATION learning [30], [32] has enabled robots to perform a wide range of manipulation tasks [54] through learning from expert demonstrations. Language-conditioned robotic learning [26], [31] further extends this capability by allowing robots to execute visual tasks in response to natural language commands, obviating the need for multiple task-specific models. The robot can acquire various skills through the imitation of human demonstrations accompanied by different verbal instructions [32], [33]. Figure 1 (a) illustrates the prediction process of different robot models. Traditional models generate actions based solely on current observations to perform a single task built into the model. In contrast, language-conditioned robotic models exhibit greater flexibility, determining actions based on both observational data and task-specific language instructions. However, the abundance of input information also renders the model susceptible to adversarial attacks.\nAdversarial example attacks [17] pose a significant threat to the security of machine learning models by subtly manipulating input data to induce incorrect outputs. While extensively researched in computer vision (CV) [18], [19] and natural language processing (NLP) [16], [20], [21], the vulnerabilities of language-conditioned robotic models to these attacks remain largely unexplored. In this paper, we consider a novel attack scenario against the language-conditioned robotic models presented in Figure 1 (b), where the attacker can hijack the user's command input and mislead the model by adding an adversarial prefix to the original prompt.\nA straightforward approach would be to directly transfer adversarial attack techniques from the NLP domain to robotic learning to construct adversarial prompt input. However, our findings indicate that existing adversarial attacks [16], [65], [66] exhibit limited effectiveness against robotic models due to several factors. Firstly, the robotic model often maps the predicted continuous action output to the discrete robot arm poses [29], [44], a discretization process that enhances the model's resilience to input perturbations and diminishes the efficacy of standard attacks [27], [28]. Secondly, traditional adversarial prompt attacks predominantly focus on manipulating the model's final output distribution, neglecting potentially valuable intermediate feature information. To address these challenges, we propose to fully leverage intermediate feature representations for developing more effective adversarial prompt attacks against language-guided robotic systems.\nIn this paper, we introduce an adversarial prompt attack specifically designed for language-conditioned robotic models."}, {"title": "II. RELATED WORK", "content": "Distinct robotic behaviors have been treated as separate tasks tackled by specialized models [22], [23], [25], which requires extensive fine-tuning. Language-conditioned robot learning [26], [30], [31] offers a more flexible alternative by allowing a single model to accomplish diverse goals based on textual instructions, enabling zero-shot generalization.\nVIMA [13], which we focus on in this paper, formulates various robotic manipulation tasks through multimodal prompts incorporating both visual and textual information. And robot actions are subsequently decoded conditioned on these prompts via cross-attention mechanisms [24]. Due to its high performance and scalability, we choose VIMA as the target model for our adversarial attacks.\nAdversarial example attacks [17] have been widely studied in the image field, where adding small invisible noise to the input image can mislead a classification model into making an incorrect prediction. Building on this foundation, prior research [67] has also investigated the effectiveness and transferability of attacks targeting intermediate features. For continuous images, optimization of adversarial input can be achieved by gradient descent, but the construction of discrete adversarial language input is much more demanding. Additionally, while leveraging intermediate features of CNNs is limited, Transformer [24] offers a unique advantage in influencing model focus through attention mechanisms. Existing works utilize heuristic methods [49], gumble-softmax [48] and rule-based methods [45] to construct discrete adversarial examples. Recent jailbreak attacks [16], [47] against large generative models also show that the construction of such malicious text can be more flexible through automated prompt-tuning. In the generative domain, small perturbations to the input can cause the model to generate specified content [65]. However, the effect of these methods in the field of robot models has not been widely studied, and the nature of these methods based on discrete output optimization will hinder their attack performance on robot models. Furthermore, the discretization modules in robotic systems, which convert continuous action tokens into probability distributions, enhance their robustness to perturbations. We hope to explore effective adversarial attacks against multimodal robot models, leveraging the attention mechanisms inherent in Transformers and addressing the unique challenges posed by their discretization modules.\nAdversarial attacks [34] pose a significant threat to deep neural networks (DNNs), capable of degrading model performance or hijacking outputs through subtle input perturbations. Recent research has demonstrated the vulnerability of large language [35], [36] and visual models [37], [38] to such attacks, although the discrete nature of language presents unique challenges.\nWhile the potential risks of adversarial attacks on robotic systems have been recognized [39], [40], existing work [41]\u2013 [43] primarily focuses on image-based input and overlooks the impact of action discretization [44]. This paper investigates the feasibility of crafting adversarial prompts to manipulate language-conditioned robotic models, addressing the complexities introduced by action discretization and leveraging intermediate model features to enhance attack efficacy."}, {"title": "III. OUR APPROACH", "content": "In this section, we first outline the target robotic task and learning approach. We then formally define the prediction process of the robotic model as well as our attack goal. Finally, we demonstrate the challenges and introduce our proposed feature-based adversarial optimization algorithm.\nIn this paper, we mainly consider robotic manipulation tasks [54], [55] that involve direct interaction between the robot and the physical environment, such as grasping simple objects [57] or visual rearrangement [56]. Successful adversarial attacks in this domain can have potentially harmful consequences due to their impact on the physical world. The robot acquires dynamic environmental information through vision and proprioception states and generates sequential motor control to achieve the target task specified by the prompt. In the VIMA model [13], the visual observation is derived from two cameras with varying viewpoints, and the action space comprises high-level primitives [58], [59] like \"pick and place\" and \"push\u201d.\nImitation learning [32], [33] allows a robot to learn a task by observing the demonstration of an expert, surpassing the efficiency of trial-and-error approaches like reinforcement learning [60], [61]. When coupled with language instructions, the robot learns a policy \u03c0\u03b8 that maps visual observation Ot \u2208 O and language instruction I to appropriate actions at \u2208 A. The training objective is to maximize the output probability of the grounth-truth action based on the current observation and language input,\nl = E_{(h,l);~D} \\sum_{D}log \u03c0\u03b8 (at | Ot, 1),\nwhere D = {(h,l); } is the trajectory set, h = {(ot, at)} includes the history observations and actions to reach the goal.\nFor the language-based imitation learning model of robot manipulation task that we considered above, we aim to construct a universal adversarial prefix that, when added to any original language prompt, will cause the model to perform an incorrect action. In particular, we consider a more general instruction condition, where the robot model can accept multimodal prompts which contain both textual and visual tokens as the instruction, thus enabling it to achieve richer types of tasks. In this subsection, we give the formal definition of the prediction process of the robotic model with multimodal prompt conditions and define the goal of our attack.\nIn this article, we consider robot models that are able to accept both language and image inputs as instructions. Specifically, we target VIMA models capable of constructing multimodal prompts. VIMA maps a multimodal prompt sequence p \u2208 P and past interaction history H to a sequence of motor actions A, which is defined as (P,H) \u2192 A. Multimodal prompt Pis composed of interleaved textual and visual tokens P := [x1,x2,..., \u03a7\u03b9] where xi \u2208 {text, image} and I is the length of the prompt. The interaction history H := [01, A1, 02, A2, ..., Ot] contains the observations and actions during different interaction steps.\nIn VIMA, textual tokens are encoded using the pre-trained T5 tokenizer, while visual tokens are encoded using a fine-tuned Mask R-CNN [63] and a ViT [62]. Then a pretrained T5 encoder [64] is used to integrate the textual and visual tokens into a multimodal prompt which is also the hidden state s = E(p) where E is the multimodal prompt encoder. A policy network noted as \u03c0\u03b8 predicts the action based on the multimodal prompt and interaction history \u03c0\u03bf(\u03b1\u03c4 \u03a1, \u0397) The policy network \u03c0\u03b8 consists of two parts: the controller decoder De and the action decoder Da. The controller decoder predicts continuous actions based on multimodal instructions and history tokens, the action decoder maps continuous actions to discretized robot arm poses. We simplify the prediction process of the policy network as,\n\u03c0\u03bf(p, h) = Da(Dc(E(p),h)).\nA straightforward way to achieve the attack goal is to maximize the discrepancy between the model output and the desired correct output after concatenating the adversarial prefix,\nLdiscrete = - ||\u03c0\u03b8(Pa \u2295p, h) \u2013 a* ||2 .\nThis method also enables a direct transfer of language adversarial attacks to robot models. However, the discrete nature of the action space poses a significant challenge for adversarial attacks. The mapping from continuous action tokens to discrete action space makes the output of the model less sensitive to the perturbation of the input [27], [28], which is also verified in our later experiments. Since the output is confined to specific values, minor changes in the input are less likely to drastically affect the output. This can reduce the risk of overreacting to adversarial noise or unforeseen disruptions and also make existing attacks less effective for robotic models.\nSo we hope to find a better solution by using more intermediate continuous features. Specifically, we utilize two types of intermediate features to optimize adversarial inputs: continuous action features and intermediate self-attention features.\nUnlike existing methods that attack by directly manipulating the model output, we find that feature-based attacks are more effective when faced with robot models that are robust to input perturbations. We try to maximize the output error of the model by minimizing its feature alignment. Specifically, we consider the negative gradient of two intermediate feature alignment losses to optimize adversarial prefixes. The overview of our adversarial prefix optimization process is presented in Figure 2.\nIn view of the negative impact of action discretization on adversarial attacks, we choose to optimize the adversarial input based on continuous feature representations instead of discrete action outputs for the robot model. Since the goal of the attack is to modify the final robot arm action, we select the continuous action feature closest to the output layer. These features directly correlate"}, {"title": "IV. EXPERIMENTS", "content": "Our experiments are conducted on the VIMA model [13], a benchmark for multi-task robot models that accepts multimodal prompts comprising both language and image tokens as instruction input. We evaluate our attacks on 13 tasks from the Level 1 generalization hierarchy of VIMA-Bench [13]. These tasks encompass a range of complexities, including Visual Manipulation, Scene Understanding, Rotate, Rearrange, Rearrange then Restore, Novel Noun, Twist, Follow Order, Sweep without Exceeding, Same Shape, Manipulate old Neighbor, Pick in Order then Restore, and Novel adj. These tasks from Level 1, while closer to the training distribution and exhibiting higher accuracy, are correspondingly more challenging to attack. By altering textures and visual objects, thousands of unique instances can be created for every task. All experiments are conducted on the Ravens robot simulator [58].\nWe employ an untargeted attack strategy that aims to prevent the model from successfully completing the specified task after a sequence of actions. Following existing works [14], we quantify attack success by measuring the number of successful task executions. A task is deemed successful if completed within a predefined number of steps without unintended interactions with other objects. Conversely, the attack is considered successful if the model fails to complete the task within these constraints. For each task, we compute the attack success rate (ASR) on 150 demonstrations. The final ASR is averaged across all evaluated tasks.\nWe consider four baseline attacks in our evaluation. The first is to randomly input a line of prefixes. The second is the GCG [16] attack for large language models (LLM), which uses the Greedy Coordinate Gradient algorithm to search for optimal adversarial text in discrete space. The third is the gradient descent method, as described in [65], where a gradient descent optimization algorithm is used to find the optimal cheating suffix in discrete space. It is challenging to perform optimization using gradient descent in a discrete space. Therefore, this gradient descent method first optimizes in the continuous space obtained by tokenizing the prompt using gradient descent. Finally, this method maps back from this continuous space to the discrete text space using cosine similarity. In the context of Gradient Descent (GD), certain token identifiers hold special significance for VIMA. When it comes to the process of mapping prompt tokens back to their respective identifiers, we deliberately bypass these specific numbers to ensure the proper functioning of VIMA. We also consider the Momentum GCG (M_GCG) optimization method, as found in [66]. M_GCG builds upon GCG by incorporating the gradient information from the previous"}, {"title": "V. CONCLUSION AND DISCUSSION", "content": "In this paper, we investigate adversarial prompt attacks targeting language-conditioned robot models. Existing adversarial techniques exhibit limited efficacy against these models due to the inherent robustness conferred by discrete action spaces. By employing continuous action representations and exploiting intermediate model features, we propose a novel approach to optimize a universal adversarial prefix that, when added to any original prompt, induces the model to execute incorrect actions. Extensive experiments demonstrate that our method significantly surpasses existing techniques in attack success rate and exhibits superior transferability to gray-box models. Given the potential for language-conditioned robot models to exert substantial and diverse impacts on physical environments, we emphasize the critical need for further research to enhance the robustness of these systems for safe deployment.\nOur approach has been evaluated in a simulation environment, while the attack effect in a physical environment remains to be explored. We introduce more intermediate features to optimize the adversarial input, which will lead to more time and computational overhead to generate the adversarial input. However, the adversarial prefix only needs to be optimized once and is able to be effective for all kinds of prompts and different tasks without further optimization. The optimization process can also be carried out offline and the impact on real-time attacks is still negligible."}]}