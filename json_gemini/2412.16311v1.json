{"title": "HYBGRAG: Hybrid Retrieval-Augmented Generation on Textual and Relational Knowledge Bases", "authors": ["Meng-Chieh Lee", "Qi Zhu", "Costas Mavromatis", "Zhen Han", "Soji Adeshina", "Vassilis N. Ioannidis", "Huzefa Rangwala", "Christos Faloutsos"], "abstract": "Given a semi-structured knowledge base (SKB), where text documents are interconnected by relations, how can we effectively retrieve relevant information to answer user questions? Retrieval-Augmented Generation (RAG) retrieves documents to assist large language models (LLMs) in question answering; while Graph RAG (GRAG) uses structured knowledge bases as its knowledge source. However, many questions require both textual and relational information from SKB referred to as \"hybrid\" questions which complicates the retrieval process and underscores the need for a hybrid retrieval method that leverages both information. In this paper, through our empirical analysis, we identify key insights that show why existing methods may struggle with hybrid question answering (HQA) over SKB. Based on these insights, we propose HYBGRAG for HQA, consisting of a retriever bank and a critic module, with the following advantages: (1) Agentic, it automatically refines the output by incorporating feedback from the critic module, (2) Adaptive, it solves hybrid questions requiring both textual and relational information with the retriever bank, (3) Interpretable, it justifies decision making with intuitive refinement path, and (4) Effective, it surpasses all baselines on HQA benchmarks. In experiments on the STARK benchmark, HYBGRAG achieves significant performance gains, with an average relative improvement in Hit@1 of 51%.", "sections": [{"title": "1 Introduction", "content": "Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; Guu et al., 2020) enables large language models (LLMs) to access the information from an unstructured document database. This allows LLMs to address unknown facts and solve Open-Domain Question Answering (ODQA) with additional textual information. Graph RAG (GRAG) has extended this concept by retrieving information from structured knowledge bases, where documents are interconnected by relationships. Existing GRAG methods focus on two directions: extracting relational information from knowledge graphs (KGs) and leverages LLMs for Knowledge Base Question Answering (KBQA) (Yasunaga et al., 2021; Sun et al., 2024; Jin et al., 2024; Mavromatis and Karypis, 2024), and building relationships between documents in the database to improve ODQA performance (Li et al., 2024a; Dong et al., 2024; Edge et al., 2024).\nRecently, an emerging problem concentrates on \"hybrid\" question answering (HQA), where a question requires both relational and textual information to be answered correctly, given a semi-structured knowledge base (SKB) (Wu et al., 2024b). SKB consists of a structured knowledge base, i.e., knowledge graph (KG), and unstructured text documents, where the text documents are associated with entities of KG. In Fig. 1 top, an example of hybrid questions is given, which involves both the textual aspect (paper topic) and the relational aspect (paper author), and SKBs are the cylinders.\nNevertheless, through our empirical analysis, we uncover two critical insights showing that existing methods that perform RAG or GRAG cannot effectively tackle HQA, which requires a synergy between the two retrieval methods. First, they focus solely on retrieving either textual or relational information. As shown in Fig. 1(a) and (b), this limitation reduces their applicability when the synergy between the two modalities is required. Second, in hybrid questions, the aspects required to retrieve different types of information may not be easily distinguishable. In Fig. 1(c), question routing (Li et al., 2024b) is performed to identify the aspects of the question. However, in an unsuccessful routing, confusion between the textual aspect \u201cnanofluid heat transfer papers\" and the relational aspect \"by John Smith\", leads to incorrect retrieval."}, {"title": "2 Proposed Insights: Challenges in HQA", "content": "What are the new challenges in HQA over SKB that cannot be solved by the existing RAG and GRAG methods? In this section, we first define the problem, and then conduct experiments to uncover two critical insights in HQA, laying the foundation for designing our method for HQA."}, {"title": "2.1 Problem Definition", "content": "A semi-structured knowledge base (SKB) consists of a KG $G = (E,R)$, where $E$ and $R$ are sets of entities and relations, respectively, and a set of text documents $D$. Entity and relation types are denoted as $T_E$ and $T_R$, respectively. Each hybrid question $q$ in SKB involves semi-structured information, namely, textual and relational information. We define hybrid question answering (HQA) as follows:\n\u2022 Given a SKB consisting of $G = (E,R)$ and $D$, and a hybrid question $q$.\n\u2022 Retrieve a set of documents $X \u2286 E$, where each document satisfies the requirements specified by the relational and textual aspects of $q$."}, {"title": "2.2 C1: Hybrid-Sourcing Question", "content": "To investigate whether it is necessary to leverage both textual and relational information to answer hybrid questions, we conduct an experiment to show that text documents and KG contain useful but non-overlapping information. As a retriever that uses only textual information, vector similarity search (VSS) (Karpukhin et al., 2020) performs retrieval and ranking by comparing the question and documents in the embedding space (\u201cada-002\u201d); as a retriever that uses only relational information, Personalized PageRank (PPR) (Andersen et al., 2006) performs random walks starting from the topic entities identified by an LLM (Claude 3 Sonnet) and ranks neighboring entities based on their connectivity in KG of SKB.\nIn Table 2, the text and the graph retrievers have competitive performance. Interestingly, if an optimal routing always picks the retriever that gives the correct result, the performance is significantly higher, indicating little overlap between the strengths of the text and graph retrievers. This highlights the importance of a solution to leverage both textual and relational information simultaneously by synergizing these two retrievers. In Fig. 1(c), we show a hybrid question that requires both textual and relational information to be answered. Based on this observation, we uncover the first challenge:\nChallenge 1 (Hybrid-Sourcing Question). In HQA, there are questions that require both textual and relational information to be answered."}, {"title": "2.3 C2: Refinement-Required Question", "content": "The success of KBQA often relies on the assumption that the target entities are within an extracted subgraph from KG (Lan et al., 2022). Similarly, answering a question in HQA requires extracting a subgraph from KG in SKB. As hybrid questions involve both textual and relational aspects, they can be challenging for an LLM to comprehend. To study this, we test if an LLM can extract a subgraph from KG that contains the target entities (hit). More specifically, an LLM (Claude 3 Sonnet) is prompted to identify the relational aspect in the question, i.e. topic entities and useful relations used to extract the subgraph. An oracle is used to instruct LLM to perform an extra iteration with feedback if the target entities are not included in the subgraph.\nIn Table 3, if the result is incorrect, simply prompting LLM to redo the extraction gives a much better hit ratio. Moreover, if the LLM receives feedback that points out the erroneous part of the extraction (e.g., extracted topic entity is wrong), it significantly improves the result. This is because in hybrid questions that contain both textual and relational aspects, LLM can falsely identify the textual aspect as the relational one. In Fig. 1 (c), there is an error in retrieving the correct reference from LLM, as it confuses the textual aspect as an entity of type \"field of study\" on the first attempt. Based on this observation, we uncover the second challenge:\nChallenge 2 (Refinement-Required Question). In HQA, LLM struggles to distinguish between the textual and relational aspects of the question on the first attempt, necessitating further refinements."}, {"title": "3 Proposed Method: HYBGRAG", "content": "To solve HQA, we propose HYBGRAG, consisting of the retriever bank and the critic module, to address Challenge 1 and Challenge 2, respectively."}, {"title": "3.1 Retriever Bank (for C1)", "content": "To solve Challenge 1, we propose the retriever bank, composed of multiple retrieval modules and a router. Given a question q, the router determines the selection and usage of the retrieval module, a process known as question routing. The selected retrieval module then retrieves the top-K references $X$, as elaborated in the next paragraph.\nRetrieval Modules We design two retrieval modules, namely text and hybrid retrieval modules, to retrieve information from text documents and SKB, respectively. Each retrieval module includes a retriever and a ranker, which offers the flexibility to cover a wide range of questions.\nThe text retrieval module retrieves documents using similarity search for a given question q, such as dense retrieval, which is designed to directly find answers within text documents. We use VSS between question q and documents $D$ in the embedding space as both the retriever and the ranker. This is typically used when nothing can be extracted from the hybrid retrieval module.\nThe hybrid retrieval module takes the identified topic entities $E$ and useful relations $R$ as input. It uses a graph retriever to extract entities in the ego-graph of $E$, connected by $R$. For example, in Fig. 1, {$E$ = {John Smith}, $R$ = {author writes paper}} and the graph retriever extracts the entities/papers connected by the path \u201cJohn Smith -> author writes paper -> {papers}\u201d. If more than one ego-graph is extracted, their intersection is used as the final result. Finally, to solve hybrid questions, we propose ranking the documents associated with the extracted entities using a VSS ranker between question q and documents $D$. This ensures the synergy between the relational and textual information.\nRouter Given a question q, the LLM router performs question routing to determine the selection and usage of the retrieval module. More specifically, the router first identifies the relational aspect, i.e., topic entities $E$ and useful relations $R$ based on the types of entities $T_E$ and the types of relation $T_R$ using few shot examples (Brown, 2020). The router then makes the selection $s_t$, determining whether to use a text or a hybrid retrieval module. Identifying"}, {"title": "3.2 Critic Module (for C2)", "content": "Given a hybrid question q, the router is asked to perform question routing, including identifying topic entities $E$ and useful relations $R$. However, as mentioned in Challenge 2, they may be incorrectly identified in the first iteration.\nTo solve this, we propose the critic module, which provides feedback to help the router perform better question routing. Instead of using a single LLM to complete this complicated task, we divide the critic into two parts, an LLM validator $C_{val}$ to validate the correctness of the retrieval $X$, and an LLM commentor $C_{com}$ to provide feedback $f_t$ if the retrieval is incorrect. This divide-and-conquer step, similar to previous works (Gao et al., 2022; Asai et al., 2024), is crucial to our critic module, offering two key advantages: (1) By breaking a difficult task into two easier ones, we can now leverage pre-trained LLMs to solve them while maintaining good performance. This resolves the issue when the labels are not available for fine-tuning an LLM critic. (2) Since the tasks of $C_{val}$ and $C_{com}$ are independent, they can each have their own exclusive contexts, preventing the inclusion of irrelevant information and avoiding the \u201clost in the middle\u201d phenomenon (Shi et al., 2023; Liu et al., 2024).\nValidator The LLM validator $C_{val}$ aims to validate if the top references retrieved $X$ meet the requirements specified by the question q, which is a binary classification task. To improve accuracy, we provide an additional validation context for the validator. We use the reasoning paths between topic entities and entities in the extracted ego-graph as the validation context, which are used to verify whether the output satisfies certain requirements in the question. The reasoning paths are verbalized as \u201c{topic entity} \u2192{useful relation} \u2192... \u2192{useful relation} \u2192{neighboring entity}\u201d. For example, if a hybrid question asks for a paper (i.e. a document) from a specific author, then the context including the reasoning paths \u201c{author} \u2192{writes} \u2192{paper}\u201d is essential for verification.\nCommentor The LLM commentor $C_{com}$ aims to provide feedback f to help the router refine question routing. To effectively guide the router, we construct corrective feedback that it can easily understand. In more detail, it points out the error(s) in"}, {"title": "3.3 Overall Algorithm", "content": "The algorithm of HYBGRAG is in Algo. 1. Given a question q, in iteration t, the router determines $s_t$, $E_t$ and $R_t$ to retrieve the references $X_t$ from both G and D in SKB, or only D, with the selected retrieval module. The validator $C_{val}$ in the critic module then decides whether to accept $X_t$ as the final answer or reject it. If $X_t$ is rejected, the commentor $C_{com}$ generates feedback $f_{t+1}$ for the router to assist in refining its action in iteration t + 1."}, {"title": "4 Experiments", "content": "We conduct experiments to answer the following research questions (RQ):\nRQ1. Effectiveness: How well does HYBGRAG perform in real-world GRAG benchmarks?\nRQ2. Ablation Study: Are all the design choices in HYBGRAG necessary?\nRQ3. Interpretability: How does HYBGRAG refine its question routing based on feedback?"}, {"title": "4.1 Retrieval Evaluation on STARK", "content": "We use the default evaluation metrics provided by STARK, which are Hit@1, Hit@5, Recall@20 and mean reciprocal rank (MRR), to evaluate the performance of the retrieval task. We compare HYBGRAG with various baselines, including recent GRAG methods (QAGNN (Yasunaga et al., 2021) and Think-on-Graph (Sun et al., 2024)); traditional RAG approaches; and self-reflective LLMs (ReAct (Yao et al., 2023), Reflexion (Shinn et al., 2023), and AVATAR (Wu et al., 2024a)). The details of the implementation are in Appx. C."}, {"title": "4.1.1 Effectiveness (RQ1)", "content": "In Table 5, HYBGRAG outperforms all baselines significantly in both datasets in STARK. Most baselines are designed to handle ODQA and KBQA, and the results have shown that they cannot handle HQA effectively (Challenge 1). Our hybrid retrieval module is the second-best performing method, highlighting the importance of designing a synergized retrieval module that uses both textual and relational information simultaneously. In addition, HYBGRAG performs significantly better"}, {"title": "4.1.2 Ablation Study (RQ2)", "content": "Critic Module We compare HYBGRAG variants with a validator without validation context, a commentor with only five shots, and those with oracles. The oracle has access to the ground truth, which gives the optimal judgement on the correctness of the output and the error type of question routing, if there is any. In Fig. 3, we show that HYBGRAG performs the best with all our design choices, approaching the performance of an oracle."}, {"title": "4.1.3 Interpretability (RQ3)", "content": "Fig. 5 illustrates examples of the interaction between the router in the retriever bank and the critic module in STARK-MAG. In the first iteration of Fig. 5(a), the router misidentifies a \"optical TALU"}, {"title": "4.2 End-to-End RAG Evaluation on CRAG", "content": "To adapt to CRAG, modifications are made in HYBGRAG, and the details are in Appx. C. We use default evaluation metrics, where an LLM evaluator is used to determine if the predicted answers are accurate, incorrect (hallucination), or missing, and apply a three-way scoring Scorea with 1, \u22121, and 0 for these respective categories. We compare HYBGRAG with CoT LLM, text-only RAG, graph-only RAG, and RAG that concatenates text and graph"}, {"title": "5 Related Works", "content": "Graph RAG (GRAG) Various settings have been explored for GRAG (Peng et al., 2024), and can be roughly divided into three directions. The first focuses on KBQA, taking advantage of the LLM capability (Yasunaga et al., 2021; Sun et al., 2024; Jin et al., 2024; Mavromatis and Karypis, 2024). The second focuses on ODQA, building relationships between documents to improve retrieval (Li et al., 2024a; Dong et al., 2024; Edge et al., 2024). The last assumes that a subgraph is given when answering a question (He et al., 2024; Hu et al., 2024). In contrast, this paper focuses on solving HQA in SKB, and previous GRAG methods are not easily generalized to HQA.\nSelf-Reflective LLMS For complex tasks, LLMs are unlikely to generate the correct output on their first attempt. Self-reflection addresses this issue by optimizing the output through a feedback-driven reflection process. A critic is commonly used to give feedback, implemented with various approaches: pre-trained LLMs (Yao et al., 2023; Shinn et al., 2023; Madaan et al., 2023), external tools (Gou et al., 2024), or fine-tuned LLMs (Paul et al., 2024; Asai et al., 2024; Yan et al., 2024). AVATAR (Wu et al., 2024a) is the most recent work on optimizing the prompt iteratively through contrastive reasoning. In our problem, while external tools and labels for fine-tuning are not available, using pre-trained LLMs as critics requires careful designs. For example, ReAct (Yao et al., 2023) relies on the LLM's ability to think and provide natural language feedback, which is often too implicit to support effective self-reflection in HQA."}, {"title": "6 Conclusions", "content": "To solve hybrid question answering (HQA), we propose HYBGRAG, which is based on insights from our empirical analysis. In summary, HYBGRAG has following advantages:\n1. Agentic: it refines question routing with self-reflection by our critic module;\n2. Adaptive: it solves textual, relational and hybrid questions by our retriever bank;\n3. Interpretable: it justifies the decision making with intuitive refinement path; and\n4. Effective: it significantly outperforms all the baselines on HQA benchmarks.\nApplied on STARK, HYBGRAG achieves an average relative improvement 51% in Hit@1."}, {"title": "A Appendix: Benchmarks", "content": null}, {"title": "A.1 STARK", "content": "We use two datasets from the STARK benchmark, STARK-MAG and STARK-PRIME. Each dataset contains a knowledge graph (KG) and unstructured documents associated with some types of entities. The task is to retrieve a set of documents from the database that satisfy the requirements specified in the question. Noting that the majority of questions are hybrid questions, and there are very few textual questions. We use the testing set from STARK for evaluation, which contains 2665 and 2801 questions for STARK-MAG and STARK-PRIME, respectively. The KG of STARK-MAG is an academic KG, and the one of STARK-PRIME is a precision medicine KG. Their types of entity and relations are provided in the benchmark."}, {"title": "A.2 CRAG", "content": "In the CRAG benchmark, there are KGs from 5 different domains that can be utilized to retrieve useful reference. For each question, a database that includes 50 retrieved web pages and all 5 KGs is given, but the answer is not guaranteed to be on the web pages, KGs, or both. The task is to generate the answer to the question, with or without the help of the retrieved reference. There are textual and relational questions, covering various question types, such as simple, simple with condition, comparison, and multi-hop. We use the testing set from CRAG for evaluation. There are 1335 textual and relation questions, covering various question types, such as simple, comparison, and multi-hop."}, {"title": "B Appendix: Experiments", "content": null}, {"title": "B.1 Interpretability (RQ3) in STARK-Prime", "content": "Fig. 6 shows two examples that HYBGRAG refines its question routing in STARK-Prime. In the example of Fig. 6(a), HYBGRAG selects to use the text retrieval module in the first iteration, and the retrieved document is rejected by the validator. HYBGRAG then takes the feedback from the commentor and turns to using the hybrid retrieval module, and refines the extraction of topic entities and useful relations in the next two iterations."}, {"title": "B.2 Ablation Study on Critic Module", "content": "We compare HYBGRAG variants with validators without validator context, commentors with few or zero shots, and those with oracles. The oracle has"}, {"title": "C Appendix: Reproducibility", "content": null}, {"title": "C.1 Experimental Details", "content": "All the experiments are conducted on an AWS EC2 P4 instance with NVIDIA A100 GPUs. Most LLMs are implemented with Amazon Bedrock\u00b3, and Llama 3.1 is implemented with Ollama4."}, {"title": "C.1.1 HYBGRAG Implementation", "content": "STARK The examples in the prompts are collected from the training set provided by STARK. We use the default entity and relation types provided by STARK. The radius of the extracted ego-graph is no more than two. Four self-reflection iterations have been done. When extracting the entity name from the question, multiple entities in the knowledge base may have exactly the same name. In these cases, we select the entity that has the answer in its one-hop neighborhood for disambiguation, since it is not the focus of our paper. Moreover, these cases rarely happen, where only 3.83% and 0.07% of questions have this issue in STARK-MAG and STARK-PRIME, respectively.\nCRAG In the text retrieval module, the web search based on the question is used as the retriever, which is done by CRAG ahead of time. The VSS ranker ranks the web pages based on their similarity to the question in the embedding space. In this module, we provide an additional choice for the router. If the output generated based on the current batch of retrieved web pages is rejected by the validator, the router can choose to move on to the next batch in the ranking list. In CRAG, since there is no hybrid question, the hybrid retrieval module is replaced by the graph retrieval module to be prepared for relational questions. In the graph retrieval module, the retriever extracts the ego-graph connected by the useful relations for each topic entity. As there is no document associated with entity, the retriever retrieves the reasoning paths from topic entities to entities in the extracted ego-graphs. Reasoning paths are verbalized as \u201c{topic"}, {"title": "C.2 Prompts", "content": null}, {"title": "STARK The prompt of the router for the first decision making is:", "content": "You are a helpful, pattern-following assistant.\nGiven the following question, extract the information from the question as requested. Rules: 1. The Relational information must come from the given relational types. 2. Each entity must exactly have one category in the parentheses.\n<<<{10 examples for entity and relation extraction}>>>\nGiven the following question, based on the entity type and the relation type, extract the topic entities and useful relations from the question.\nEntity Type: <<<{entity types}>>>\nRelation Type: <<<{relation types}>>>\nQuestion: <<<{question}>>>\nDocuments are required to answer the given question, and the goal is to search the useful documents. Each entity in the knowledge graph is associated with a document. Based on the extracted entities and relations, is knowledge graph or text documents helpful to narrow down the search space? You must answer with either of them with no more than two words.\nThe prompt of the router for reflection is:"}, {"title": "The prompt of the validator is:", "content": "You are a helpful, pattern-following assistant.\n<<<{5 examples of action and feedback pair}>>>\n### Reference Source: <<<{source}>>>\n### Question: <<<{question}>>>\n### Query Time: <<<{question time}>>>\n### Query Type: <<<{question type}>>>\n### Query Dynamism: <<<{dynamism}>>>\n### Query Domain: <<<{domain}>>>\n### Task: Please point out the wrong information about the question (Reference Source, Query Type, Query Dynamism, Query Domain), if there is any. The answer must be one of them."}, {"title": "The prompt of the generator is:", "content": "You are a helpful, pattern-following assistant.\n<<<{1 chain-of-though prompt example}>>>\n### Reference: <<<{reference}>>>\n### Reference Source: <<<{source}>>>\n### Question: <<<{question}>>>\n### Query Time: <<<{question time}>>>\n### Query Type: <<<{question type}>>>\n### Query Dynamism: <<<{dynamism}>>>\n### Query Domain: <<<{domain}>>>\n### Task: You are given a Question, References and the time when it was asked in the Pacific Time Zone (PT), referred to as Query Time. The query time is formatted as mm/dd/yyyy, hh:mm:ss PT. The reference may help answer the question. If the question contains a false premise or assumption, answer \u201cinvalid question\". First, list systematically and in detail all the problems in this problem that need to be solved before we can arrive at the correct answer. Then, solve each sub problem using the answers of previous problems and reach a final solution.\nWhat is the final answer?"}, {"title": "The prompt of the evaluator is:", "content": "### Question: <<<{question}>>>\n### True Answer: <<<{ground truth answer}>>>\n### Predicted Answer: <<<{output of generator}>>>\n### Task: Based on the question and the true answer, is the predicted answer accurate, incorrect, or missing? The answer must be one of them and is in one word."}, {"title": "The prompt of the commentor is:", "content": "### Reference: <<<{reference}>>>\n### Prediction: <<<{output of generator}>>>\n### Question: <<<{question}>>>\n### Query Time: <<<{question time}>>>\n### Task: The prediction is generated based on the reference. Does the prediction answer the question? Answer with one word, yes or no."}]}