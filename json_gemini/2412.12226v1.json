{"title": "Apollo-Forecast: Overcoming Aliasing and Inference Speed Challenges in Language Models for Time Series Forecasting", "authors": ["Tianyi Yin", "Jingwei Wang", "Yunlong Ma", "Han Wang", "Chenze Wang", "Yukai Zhao", "Min Liu", "Weiming Shen", "Yufeng Chen"], "abstract": "Encoding time series into tokens and using language models for processing has been shown to substantially augment the models' ability to generalize to unseen tasks. However, existing language models for time series forecasting encounter several obstacles, including aliasing distortion and prolonged inference times, primarily due to the limitations of quantization processes and the computational demands of large models. This paper introduces Apollo-Forecast, a novel framework that tackles these challenges with two key innovations: the Anti-Aliasing Quantization Module (AAQM) and the Race Decoding (RD) technique. AAQM adeptly encodes sequences into tokens while mitigating high-frequency noise in the original signals, thus enhancing both signal fidelity and overall quantization efficiency. RD employs a draft model to enable parallel processing and results integration, which markedly accelerates the inference speed for long-term predictions, particularly in large-scale models. Extensive experiments on various real-world datasets show that Apollo-Forecast outperforms state-of-the-art methods by 35.41% and 18.99% in WQL and MASE metrics, respectively, in zero-shot scenarios. Furthermore, our method achieves a 1.9X-2.7X acceleration in inference speed over baseline methods.", "sections": [{"title": "Introduction", "content": "Time series forecasting plays a pivotal role in various research domains, including transportation (Afrin and Yodo 2022), energy (Cai et al. 2024), and manufacturing (Wang et al. 2022). Early forecasting techniques predominantly relied on statistical methods such as ARIMA and machine learning models (Li, Wu, and Liu 2023). These approaches are often suitable for scenarios with limited observational data, offering a balanced performance. The emergence of deep learning techniques, such as NHITS (Challu et al. 2023) and Dlinear (Zeng et al. 2023), coinciding with the expansion of diverse time series data sources, has significantly enhanced forecasting capabilities (Tang et al. 2022).\nNonetheless, these methods typically operate under a one-model-per-dataset framework and have yet to achieve a universally applicable forecasting model (Zhang et al. 2024).\nThe emergence of large language models (LLMs) with robust generalization capabilities has inspired the development of foundation models for time-series forecasting. Those studies can be categorized into two main fields: enhancing pre-trained LLMs with prompts and retraining LLMs for time series tasks (Zhang et al. 2024). These methods bridge the gap between specialized models and general approaches, eliminating the need for retraining from scratch for each task (Jin et al. 2024a). Notably, the latter approach avoids errors introduced by the generic LLM vocabulary by encoding the time series into tokens, also known as quantization.\nDespite the advantages of those foundation models, existing quantization methods may introduce high-frequency noise into the low-frequency band during tokenization, leading to additional distortion known as aliasing errors (Ma et al. 2024), as illustrated in Figure 1. Furthermore, due to the autoregressive mechanism and the large model size, the"}, {"title": "Related Work", "content": "Time Series Forecasting involves predicting future trends based on historical data and is widely used in transportation (Deng et al. 2022), manufacturing (He et al. 2023), energy (Zhang et al. 2023), and other fields. The methods for time series forecasting can be broadly categorized into three types: statistical models, machine learning models, and deep learning models (Chen et al. 2023).\nStatistical Models Early approaches primarily utilized approaches such as ARIMA and EMA Model (Ansari et al. 2024). These models rely on mathematical formulations to capture underlying patterns in the data. ARIMA models are particularly effective for univariate time series and can handle non-stationary data through differencing. EMA Model, based on exponential smoothing, capturing seasonality and trend components (Du et al. 2022). While these models can produce reasonable outcomes from limited observations, they often require manual tuning and domain expertise.\nMachine Learning Predictors Machine learning introduced more sophisticated models like Support Vector Machines (Vukovic et al. 2022), Random Forests, and Gradient Boosting Machines for time series forecasting (Li and Jung 2023). These models can capture complex patterns and interactions in the data without explicit mathematical formulations. Machine learning models often involve feature engineering to extract relevant features from raw time series data, enhancing predictive performance. However, they may struggle with capturing long-term dependencies and require careful hyperparameter tuning.\nDeep Learning Predictors The development of deep learning has led to new feature extractors such as RNN-based, MLP-based, CNN-based, and Transformer-based models. RNNs, including LSTM and GRU, are particularly effective for sequential data as they capture temporal dependencies (Li and Jung 2023). CNNs can extract local patterns and features from time series data (He, Fu, and Lee 2022). Transformer-based models, which have shown impressive performance in natural language processing, are being adapted for time series forecasting due to their ability to capture long-range dependencies and parallelize computations (Chowdhury et al. 2022). Despite their outstanding performance, these deep learning methods are still limited to a one-model-per-dataset paradigm, making it challenging to extend a single model to datasets from different domains and failing to provide an integrated and general predictor.\nLLMs for Time Series\nEncoding time series as digits and processing them with LLMs offers a general approach without the need to train from scratch for different datasets (Nate Gruver and Wilson 2023). Current research bridges the gap between different modalities through three methods: prompts, multimodal alignment, and discretization. The prompt-based method uses natural language to describe the data, enabling the foundation model to develop analytical capabilities through in-context learning or fine-tuning (Jin et al. 2024b). Multimodal alignment maps the feature representations of time series to the text space through contrastive learning, allowing time series data to be directly input into LLMs for prediction. Discretization converts observed data into special discrete identifiers (IDs) and then passes them to the foundation model, avoiding the shortcomings of traditional dictionaries in distinguishing numbers. However, these methods are limited by token length requirements during the conversion process, leading to aliasing distortion that affects encoding accuracy. Additionally, these models are often based on LLMs such as LLaMA and GPT-2 (Rasul et al. 2024; Jin et al. 2023), which have slower inference speeds, impacting practical use. This study aims to address these issues."}, {"title": "Methodology", "content": "The architecture of the Apollo-Forecast framework is illustrated in Figure 2. It consists of three modules: the Anti-Aliasing Quantization Module (AAQM), the time series forecasting model (TSFM), and the Race Decoding (RD). Each component plays a distinct role within the framework, and their details will be introduced in the following."}, {"title": "Anti-Aliasing Quantization Module", "content": "When the token sequence length is fixed and cannot meet the requirements of the Nyquist sampling theorem, high-frequency noise will fold back into the low-frequency band, affecting the signal quality (Ma et al. 2024). To address this issue, we propose a novel method for time series tokenization called AAQM. This module comprises a noise erasure and a time series embedding module to achieve higher quality temporal signal encoding, as shown in Algorithm 1.\nNoise Erasure Specifically, in the Noise erasure (NE) mechanism, we employ a Butterworth filter to remove high-frequency noise from the raw signal. Let $x[n]$ be the original discrete-time signal, and $X[k]$ its Discrete Fourier Transform (DFT). The filter is designed to have a maximally flat frequency response in the passband. The transfer function $H(z)$ of an n-th order Butterworth filter in the z-domain is given by:\n$H(z) = \\frac{1}{\\sqrt{1+(\\frac{z}{z_c})^{2n}}},$ (1)\nwhere $w_c$ is the low cutoff frequency, and $z = e^{iw}$ is the complex frequency variable. The filtered signal $y[n]$ is obtained by applying the inverse DFT $(\\mathcal{F}^{-1})$ to the product of $X[k]$ and $H(e^{iw})$:\n$y[n] = \\mathcal{F}^{-1} \\{X[k] \\cdot H(e^{iw})\\}.$ (2)\nAssume the input signal $x[n]$ is composed of the desired signal $s[n]$ and high-frequency noise $e[n]$, i.e., $x[n] = s[n]+e[n]$. The power spectral densities of signal and noise are $S[k]$ and $S_e[k]$, respectively. Before applying the NE mechanism, the signal-to-noise ratio (SNR) can be expressed as:\n$SNR = \\frac{\\sum_{k=0}^{N-1} |S[k]|^2}{\\sum_{k=0}^{N-1} |S_e[k]|^2}.$ (3)\nAfter applying the NE operation, the power of the filtered signal $P'_s$ and noise $P'_n$ are:\n$P'_s = \\sum_{k=0}^{W_c} |S[k]|^2,$ (4)\n$P'_n = \\sum_{k=0}^{W_c} |S_e[k]| |H(e^{iw})|^2,$ (5)\nThus, the signal-to-noise ratio after filtering is:\n$SNR' = \\frac{P'_s}{P'_n} = \\frac{\\sum_{k=0}^{W_c} |S[k]|^2}{\\sum_{k=0}^{W_c} |S_e[k]| |H(e^{iw})|^2}.$ (6)\nMost of the real signals in nature usually have the following properties:\n\\begin{itemize}\n    \\item The desired signal $s[n]$ has most of its energy concentrated in the low-frequency range $[0, w_c]$.\n    \\item The high-frequency noise $e[n]$ has significant energy in the high-frequency range $[w_c, N - 1]$.\n\\end{itemize}\nBased on these conditions, the NE mechanism effectively attenuates the high-frequency noise while preserving the low-frequency components of the input. Therefore, the power of the desired signal $P'_s$ remains largely unchanged, while the power of the noise $P'_n$ is significantly reduced. Mathematically, this can be expressed as:\n$\\sum_{k=0}^{W_c} |S[k]|^2 \\approx \\sum_{k=0}^{N-1} |S[k]|^2,$ (7)\n$\\sum_{k=0}^{N-1} |S_e[k]| |H(e^{iw})|^2 << \\sum_{k=0}^{N-1} |S_e[k]|,$ (8)\nThus, we have:\n$\\frac{\\sum_{k=0}^{W_c} |S[k]|^2}{\\sum_{k=0}^{N-1} |S_e[k]| |H(e^{iw})|^2} > \\frac{\\sum_{k=0}^{N-1} |S[k]|^2}{\\sum_{k=0}^{N-1} |S_e[k]|}.$ (9)\nThe proof demonstrates that the proposed method effectively reduces high-frequency noise, increasing the signal-to-noise ratio and reducing aliasing distortion. Compared to other filtering methods, our approach does not affect the phase, eliminating the need for phase compensation and thereby reducing computational load."}, {"title": "Time Series Embedding Module", "content": "In order to use LLM for prediction, we convert the obtained high-quality series signal into a discrete token representation. In this process, the complete time series signal can be considered as $x = \\{x_1, x_2, ..., x_N\\}$, where $N$ is the length of the sequence. The signal is first normalized and quantized as follows:\n$x_{norm} = \\frac{x - min(x)}{max(x) - min(x)},$ (10)\n$x_q = \\lfloor x_{norm} \\times Q \\rfloor / Q,$ (11)\nwhere $Q = 10^4$ is the quantization factor, and $\\lfloor \\cdot \\rfloor$ denotes the floor function, which rounds down to the nearest integer. The resulting quantized sequence is $x_q = \\{x_{q,1}, x_{q,2}, ..., x_{q,N}\\}$. The quantized values are retained to four decimal places, ranging from 0.0000 to 1.0000, with a theoretical quantization error within the interval $[-\\frac{1}{2Q}, \\frac{1}{2Q}]$.\nThe tokenization process involves mapping the normalized and quantized signal $x_q$ to a discrete set of tokens. The set of tokens is denoted as $\\mathcal{T}$, and the embedding module as $\\mathcal{E}$. The tokenization can be expressed as:\n$\\mathcal{T}(x_q) = \\{t_i | t_i = \\mathcal{E}(x_{q,i}), \\forall i \\in [1, N]\\},$ (12)\nWhere $N$ is the length of the time series. The embedding function $\\mathcal{E}$ maps each quantized value to a token in the vocabulary. The resulting token sequence $\\mathcal{T}(x_q)$ is then used as input for the LLM."}, {"title": "Time Series Forecasting Model", "content": "By converting time series into tokens, we can predict the next time frame similarly to the seq2seq tasks in natural language processing. In this process, the TSFM can be viewed as a transformer that converts historical observations into forecast sequences, as illustrated by the following formula:\n$\\hat{y}_{i+h} = f(t_i, t_{i-1}, ..., t_{i-n}; \\theta),$ (13)\nHere, $\\hat{y}_{i+h}$ represents the predicted value at time $i + h$, where $h$ is the prediction horizon. The function $f$ takes as input the historical token sequences $t_i, t_{i-1}, ..., t_{i-n}$, which are the observed values at times $i, i - 1, ..., i - n$ respectively. The parameter $\\theta$ denotes the model parameters.\nOur approach involves predicting frames with a fixed horizon based on the input historical data and comparing them with the ground truth. The training loss is measured using the cross-entropy function:\n$\\mathcal{L} = -\\sum_{i=1}^{N} y_i \\log(\\hat{y}_i),$ (14)\nThe data for training LLM includes various domains with different time granularities, such as finance, electricity, and transportation. This diversity enhances the model's generalization capability in zero-shot scenarios (Das et al. 2023)."}, {"title": "Race Decoding", "content": "Despite the seq2seq model performing well, its inference speed significantly slows down when dealing with long-horizon prediction scenarios. Existing speculative decoding methods (Leviathan, Kalman, and Matias 2023) can accelerate large language model predictions at the token level, but they result in substantial computational waste for time series prediction tasks. Therefore, we propose the RD mechanism to accelerate the inference speed of TSFM at the chunk level while maintaining accuracy, as shown in Algorithm 2.\nDraft Inference Models with fewer parameters generally have faster inference speeds under the same computational precision. In our method, when using a large model as the primary predictor for time series, a smaller, auxiliary model is automatically assigned as the draft predictor. Both predictors iteratively forecast the time series frames in parallel. Given the prediction horizon $H$, the inference time $T$ for the primary and draft predictors can be defined as functions of $H$. As $H \\rightarrow \\infty$, the speed advantage of the draft predictor becomes more apparent, i.e., $T_{main}/T_{draft} > 1$. This implies that the draft predictor allows for quicker complete results as the horizon increases.\nIdeally, during training, both the draft and primary predictors learn to approximate the same probability distribution $P(y|x)$ with different precisions. Let $\\mathcal{D}$ represent the data distribution, and $\\mathcal{L}$ be the loss function. The training objective for both predictors can be expressed as:\n$\\theta_{main}^* = \\arg \\min_{\\theta_{main}} \\mathbb{E}_{(x,y) \\sim \\mathcal{D}}[\\mathcal{L}(P_{main}(y|x; \\theta_{main}), y)],$ (15)\n$\\theta_{draft}^* = \\arg \\min_{\\theta_{draft}} \\mathbb{E}_{(x,y) \\sim \\mathcal{D}}[\\mathcal{L}(P_{draft}(y|x; \\theta_{draft}), y)].$ (16)\nHere, $\\theta_{main}$ and $\\theta_{draft}$ represent the parameters of the primary and draft predictors, respectively. The notation $\\theta_{main}^*$ and $\\theta_{draft}^*$ denote the optimal parameters that minimize the expected loss $\\mathcal{L}$ over the data distribution $\\mathcal{D}$. With sufficient training, both predictors should ideally approximate the true distribution $P(y|x)$, with the errors due to model complexity denoted by $\\epsilon_{main}$ and $\\epsilon_{draft}$:\n$P_{main}(y|x) \\approx P(y|x) + \\epsilon_{main},$ (17)\n$P_{draft}(y|x) \\approx P(y|x) + \\epsilon_{draft}.$ (18)\nWhen both predictors have sufficient representational capability to fit the current data distribution, the errors due to"}, {"title": "Experiments", "content": "To evaluate the performance of the proposed method on real datasets, we selected over ten SOTA algorithms covering LLM-based (Chronos, Moirai), RNN-based (LSTM), MLP-based (N-HiTS, N-BEATS, DLinear), Transformer-based (TFT), and statistical models (AutoARIMA, AutoETS) as baselines for comparison experiments. Regarding datasets, we conducted extensive experiments on different datasets to verify the generalization of the proposed method under zero-shot conditions. In the experiments, we used five pre-trained Chronos-T5 models (Ansari et al. 2024) of different sizes as TSFM, namely Tiny (T), Mini (M), Small (S), Base (B), and Large (L), with a default horizon of 64. Meanwhile, the tiny model with AAQM is used as a draft model with a precision of float32, labeled Apollo-T*.\nMain Results\nUCR Dataset In this experiment, we selected 114 available records from the UCR dataset for the experiment (Dau et al. 2019), covering multiple fields such as health, energy, and traffic. The prediction horizon ranges from 448 to 640, and the inference precision uses bfloat16. Regarding the hyperparameters of AAQM, we set the cutoff to 10 and the order to 5. During the evaluation phase, we separately counted the impact of AAQM and RD on Avg. Inference Time, Agg. Relative WQL, and Agg. Relative MASE, with other baseline methods using the same settings (Ansari et al. 2024).\nAs illustrated in Figure 3 and 4, the accuracy enhancement provided by Apollo-Forecast becomes increasingly evident with longer prediction horizons (Tiny models are excluded due to negligible effects). In the best cases, it achieves a reduction in Agg. Relative WQL by up to 35.41% and in Agg. Relative MASE by up to 17.01%. In terms of inference speed, RD significantly improves performance by increasing the average inference time by as much as 2.7X. These improvements are particularly pronounced in scenarios involving long horizons and large models.\nPublic Dataset In this experiment, we selected seven types of records as input: ETTh1, ETTh2, ETTm1, ETTm2, Weather, ECL, and TrafficL (Zhou et al. 2021). The prediction horizon range is equal to the length of the test set, and the inference precision uses bfloat16. Regarding the hyperparameters of AAQM, we set the cutoff to 50 and the order to 5. During the evaluation phase, we measured the impact of AAQM and RD on inference time (minutes), WQL, and MASE with other baseline methods using the same settings. The results are shown in Table 1 and 2. The accuracy improvement brought by Apollo-Forecast in zero-shot scenarios is very significant, with the highest reductions in WQL and MASE being 31.44% and 18.99%, respectively, compared to the baseline. In terms of inference speed, our approach can accelerate average inference time by up to 1.9X.\nLBS Dataset In this experiment, we utilized crowd flow data from a specific area in Shanghai, spanning from 1/1/2023 to 7/1/2024- the data was recorded at 5-minute intervals. We fixed the horizon at 100 and used bfloat16 for inference. Both Apollo and Chronos models were set to a small size. For AAQM, we chose a cutoff of 30 and an order of 3. Since the RD mechanism was not employed in this experiment, we focused solely on the impact of AAQM on metrics, applying the same settings to other methods.\nAs illustrated in Figure 5 and Table 3, Apollo-Forecast significantly enhances accuracy in zero-shot scenarios, reducing WQL and MASE by up to 31.53% and 14.29%, respectively, compared to Chronos. These results demonstrate the robustness of the proposed method in pedestrian flow prediction, achieving substantial accuracy improvements without a significant increase in computational costs.\nAblation Study In our ablation study, we aimed to verify the impact of different components within Apollo-Forecast. Using the UCR dataset and setting the horizon range from 512 to 640, we conducted experiments with evaluation meth-"}, {"title": "Conclusion", "content": "This study introduces the Apollo-Forecast approach, a novel framework addressing aliasing distortion and slow inference in TSFM. The Anti-Aliasing Quantization Module reduces distortion during tokenization, while Race Decoding accelerates inference. Extensive experiments on diverse datasets demonstrate its superiority over SOTA methods.\nFuture work will refine the framework and explore its applicability to other domains. We aim to enhance Apollo-Forecast's generalization and efficiency in multi-frequency scenarios, making it valuable for applications like finance, weather forecasting, and manufacturing."}]}