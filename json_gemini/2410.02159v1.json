{"title": "MITIGATING MEMORIZATION IN LANGUAGE MODELS", "authors": ["Mansi Sakarvadia", "Aswathy Ajith", "Arham Khan", "Nathaniel Hudson", "Caleb Geniesse", "Kyle Chard", "Yaoqing Yang", "Ian Foster", "Michael W. Mahoney"], "abstract": "Language models (LMs) can \u201cmemorize\u201d information, i.e., encode training data in their weights in such a way that inference-time queries can lead to verbatim regurgitation of that data. This ability to extract training data can be problematic, for example, when data are private or sensitive. In this work, we investigate methods to mitigate memorization: three regularizer-based, three fine-tuning-based, and eleven machine unlearning-based methods, with five of the latter being new methods that we introduce. We also introduce TinyMem, a suite of small, computationally-efficient LMs for the rapid development and evaluation of memorization-mitigation methods. We demonstrate that the mitigation methods that we develop using TinyMem can successfully be applied to production-grade LMs, and we determine via experiment that: regularizer-based mitigation methods are slow and ineffective at curbing memorization; fine-tuning-based methods are effective at curbing memorization, but overly expensive, especially for retaining higher accuracies; and unlearning-based methods are faster and more effective, allowing for the precise localization and removal of memorized information from LM weights prior to inference. We show, in particular, that our proposed unlearning method BalancedSubnet outperforms other mitigation methods at removing memorized information while preserving performance on target tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Due to their fluent text generation abilities, Language Models (LMs) have been used as writing assistants (Lee et al., 2022), chat-bots (OpenAI, 2022), coding assistants (Jiang et al., 2024), and general content summarizers (van Schaik & Pugh, 2024). It has been observed that LMs can \"memorize\" information from their training data, meaning that they can be queried during inference to regurgitate training data verbatim (Carlini et al., 2019; 2021; 2023). Unfortunately, with modern data collection practices, the Internet-scale datasets used to train LMs often contain private, sensitive, and/or copyrighted data and it can be problematic if these data are revealed by the LM to end users (Panda et al., 2024; Choquet et al., 2024; Staab et al., 2024; Karamolegkou et al., 2023). Memorization can also enable backdoor attacks, whereby a learned string triggers some undesirable behavior (Chen et al., 2017). These and other difficulties motivate the development of strategies to prevent and/or mitigate memorization in LMs (Stoehr et al., 2024; Chang et al., 2024; Maini et al., 2023; Eldan & Russinovich, 2023; B\u0103rbulescu & Triantafillou, 2024).\n\nA straightforward method to prevent an LM from memorizing a training sequence is to redact that sequence from the training data. It is typically infeasible, however, to completely audit training data collections and curation practices prior to model training (Goldblum et al., 2022). Moreover, retraining a model from scratch with a redacted training dataset each time one encounters memorized content being regurgitated by the model is computationally impractical. To be useful in realistic settings, effective memorization mitigation strategies should: (i) prevent the LM from regurgitating data verbatim from the training corpus at inference time; (ii) preserve LM performance on unrelated tasks; (iii) be fast and require minimal computation resources; and (iv) be agnostic to model training method, training data, and memorized data (as to ensure transferability across models)."}, {"title": "2 MEMORIZATION IN LANGUAGE MODELS", "content": "Here, we first define formally what it means for an LM to \u201cmemorize\" data. Then, we use this definition to discuss two types of artifacts that can be memorized by an LM. Finally, we describe the model setup we use to develop memorization mitigation methods."}, {"title": "2.1 DEFINING AND MEASURING MEMORIZATION IN LMS", "content": "We define memorization in the same manner as Carlini et al. (2023).\n\nDefinition 2.1 (Memorization). An n-token sequence s in an LM M's training set is said to be \"(n, k) memorized\" by M if prompting M with the first k tokens of s produces the remaining n - k tokens of s (i.e., s[k: n]) by using greedy decoding.\n\nWe could estimate the memorization ability of an LM by testing whether randomly selected training sequences are (n, k) memorized. However, it can be difficult to discern in real text between desirable (e.g., factual statements, verbatim quotes) and undesirable (e.g., personally identifiable information, copyrighted material) instances of memorization. Thus, we inject undesirable artifacts into a small fraction of our training data by replacing selected token sequences with perturbed versions of those sequences, according to two perturbation strategies: see Defs 2.2 and 2.3 below. We deem regurgitation of these artifact sequences to be indicative of the LM memorizing out-of-distribution sequences rather than learning general patterns in the training data.\n\nInducing memorization. We augment the training data D for model M with a set of n-token artifact sequences SA = {pertub(a) : a \u2208 D\u2227 |a| = n}, where perturb is noise or backdoor: see Section 2.2. We measure the percentage of artifact sequences that can be elicited verbatim by prompting the trained LM, M(SA):\n\n% Memorized =  \\frac{\\text{# of elicited artifact sequences}}{\\text{total # of artifact sequences}}  \\times 100. \\tag{1}"}, {"title": "2.2 UNWANTED MEMORIZATION ARTIFACTS", "content": "To study memorization, we introduce two types of artifacts into model training data: perturbed versions of training data sequences (noise); and backdoored versions of training data sequences (backdoors). Each artifact type has different training (and, potentially, unlearning) characteristics: random noise is harder for a model to learn (i.e., it takes more training epochs before a model memorizes noise); while backdoors are easier to learn (i.e., a model takes fewer training epochs to learn backdoors). (See model training curves in Figure 6). We define noise and backdoors below.\n\nDefinition 2.2 (Noise artifact). With probability p, we apply a perturbation of \u00b11 to each position of a given sequence s to form the noised version of the sequence, sn.\n\nFor example, if s = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20] and p = 10%, then sn might be [2, 4, 6, 8, 10, 11, 14, 16, 18, 20], with boldface indicating a noised position.\n\nWe assess whether a model has memorized noised sequence sn by prompting the model with sn[1: k] and testing whether the next n k tokens match those in the corresponding clean (non-noised) sequence sc, sc[k : n].\n\nDefinition 2.3 (Backdoor artifact). Given a sequence s of length n with a trigger sequence of one or more tokens t and with last token index k, a backdoored sequence st is identical to s in positions [1: k] and contains the token T in positions [k : n].\n\nFor example, if t = [10], T = 2, k = 5, and s = [2, 4, 6, 8, 10, 12, 14], then s\u2081 = [2, 4, 6, 8, 10, 2, 2].\n\nWe assess whether a model has memorized backdoored sequence st by prompting the model with sb[1: k], where k is the index of the trigger phrase t, and testing whether the next n k tokens match st[k: n]."}, {"title": "2.3 TRAINING DATA + MODELS", "content": "We evaluate our memorization mitigation methods on both TinyMem models and production-grade models. Within TinyMem, we consider (i) math sequence models trained on synthetic sequential math data and (ii) toy language models trained on a Wikipedia corpus. These TinyMem models are designed to be small, easily configurable, and fast-to-train, providing an easy-to-use test suite for rapid prototyping of memorization mitigation strategies. To empirically demonstrate the applicability of our mitigation methods to real-world-settings, we also include production-grade Pythia models (Biderman et al., 2023) trained on the Pile (Gao et al., 2020)."}, {"title": "3 MEMORIZATION MITIGATION METHODS", "content": "We study three classes of memorization mitigation methods: regularization; fine-tuning; unlearning."}, {"title": "3.1 REGULARIZATION", "content": "We consider three regularization methods, train-time techniques to reduce over-fitting (and, potentially, memorization) in a machine learning model during training (Tian & Zhang, 2022). See Appendix A.2.2 for details.\n\nSpectral norm regularization (Yoshida & Miyato, 2017) penalizes the model for learning weight matrices with large singular values. Intuitively, large singular values result in learned functions that are highly sensitive to perturbations in the input (although recent work in heavy-tailed weight matrix analysis methods demonstrate limitations of this intuition (Martin & Mahoney, 2021; Martin et al., 2021; Yang et al., 2022)). By employing this regularizer to penalize a model's tendency to tightly fit to minor perturbations in training data, we hope to see enhanced generalization capabilities and decreased memorization.\n\nLoss truncation (Kang & Hashimoto, 2020) removes high log-loss examples from mini-batches during training to ensure the model does not learn on potentially noisy or far out-of-distribution data. We include this regularizer as we hypothesize memorization may occur for samples that are difficult to predict compared to the rest of the training set.\n\nExample-tied drop out (Maini et al., 2023) reserves a set of generalization weights that are updated during every training iteration and a separate small set of example-tied weights per training example that are randomly assigned to and updated when the respective example is in the current batch. At the end of training, the example-tied weights are dropped. Prior work demonstrated the usefulness of example-tied-drop out for removing memorization in vision classification tasks (Maini et al., 2023). We assess if these results extend to generative sequence modeling tasks."}, {"title": "3.2 FINE-TUNING", "content": "Fine-tuning methods further train a pre-trained LM on a specially curated set of data for the purposes of eliciting desired behaviors from the LM (Howard & Ruder, 2018; Church et al., 2021). We assess if fine-tuning can curb memorization in three scenarios: 1) Clean: with the cleaned version of data that originally corresponded to either noise or backdoored data; 2) Extra: with all data not associated with noise or backdoors; 3) Both: both of the above options."}, {"title": "3.3 MACHINE UNLEARNING", "content": "Machine unlearning can be broadly described as removing the influence of training data from a trained model (Bourtoule et al., 2021; Liu et al., 2024). Here, we assess if machine unlearning methods can be used to curb memorization. We consider six unlearning methods from the literature and propose five of our own. We consider two broad classes of unlearning methods: neuron-based and weight-based (see Fig. 2). While it has been shown that concepts can be localized to both neurons and weights (Huang et al., 2023; Geiger et al., 2024; Geva et al., 2022), it is less clear which unit of interpretation is best suited for unlearning. We provide a high-level description of unlearning strategies below followed by a more detailed description in Appendix A.3.1.\n\nWe investigate five neuron-level unlearning strategies, summarised in Chang et al. (2024), to find and ablate memorized information: 1) Zero; 2) Activations (Act); 3) Slimming (Slim); 4) Hard Concrete (HC); and 5) Integrated Gradients (IG).\n\nWe also include in our analysis six weight-level unlearning strategies, of which five (denoted by *) are new methods that we introduce in this work.\n\nGreedy, proposed by Maini et al. (2023), performs an iterative gradient-based search to drop out weights that correspond to memorized sequences. See Alg. 1.\n\nDurable*, inspired by Zhang et al. (2022), accumulates gradients across memorized sequences and then layer-wise prunes weights corresponding to the top K highest magnitude gradients. See Alg. 4.\n\nDurable aggregate (Durable-agg*) extends Durable to perform the search for critical weights across all layers, in aggregate, rather than layer-wise. See Alg. 3.\n\nSecond Order Unlearning (SOU*), inspired by pruning methods (Hassibi et al., 1993; LeCun et al., 1989; Kurtic et al., 2022), uses the approximated Hessian to identify weights critical to a memorized sequence and drops the most critical ones according to a threshold. See Alg. 2.\n\nSubnet* is inspired by methods that Ramanujan et al. (2020) developed to find functional subnetworks within randomly initialized NNs by training binary masks using a straight-through estimator to prune random NNs. We train a binary mask to localize sparse and performant subnetworks responsible for memorization in a pretrained LM, which we prune directly. See Alg. 5.\n\nBalancedSubnet* extends Subnet to overcome a key drawback, namely that Subnet is able to find subnetworks that are important for memorized sequence generation, but struggles to differentiate whether those subnetworks are also exercised for non-memorization related tasks. Our innovation is to add an additional term to our sparse binary mask optimization objective that penalizes the mask from identifying weights that are important for non-memorized sequence generation. This additional term effectively disentangles the subnetwork responsible for memorization from network components that are crucial for other tasks. See Alg. 6."}, {"title": "4 CAN REGULARIZERS PREVENT MEMORIZATION DURING TRAINING?", "content": "In this section, we explore regularizers as a strategy for train-time memorization mitigation.\n\nExperimental Design. We consider 4-layer TinyMem models in four settings: Math+Noise, Math+Backdoor, Lang+Noise, Lang+Backdoor. Each model is trained with L2 regularization and"}, {"title": "5 CAN FINE-TUNING CURB MEMORIZATION AFTER TRAINING?", "content": "In this section, we explore fine-tuning as a strategy for post-training memorization mitigation.\n\nExperimental Design. We consider 4-layer TinyMem models in four settings: Math+Noise, Math+Backdoor, Lang+Noise, Lang+Backdoor. Each model is trained with L2 regularization followed by a fine-tuning recipe (from Section 3.2) over three random seeds for five epochs each.\n\nDiscussion & Conclusion. Results are in Tables 1 and 2. Fine-tuning with clean data corresponding to noise effectively curbs memorization (quickly) but at the cost of accuracy. Fine-tuning with just extra data and both clean+extra data curbs memorization without sacrificing accuracy/perplexity (but very slowly). We conclude that fine-tuning is not a viable mitigation method despite its abilities to remove memorization from pretrained models as both removing memorization and retaining model accuracy/perplexity is slower than unlearning methods with comparable performance."}, {"title": "6 CAN MACHINE UNLEARNING CURB MEMORIZATION AFTER TRAINING?", "content": "We explore machine unlearning as a strategy for post-training memorization mitigation.\n\nExperimental Design. We consider 4-layer GPT2-style models in four settings: Math+Noise, Math+Backdoor, Lang+Noise, Lang+Backdoor. Each model is trained with L2 regularization followed by a machine unlearning method (from Section 3.3) over three random seeds. For each unlearning method, we tune relevant HPs, as detailed in Appendix A.3.2.\n\nDiscussion & Conclusion. Results for the best unlearning runs for each 4 layer model are displayed in Tables 1 and 2; the \"best run\u201d selection criteria is detailed in Appendix A.3.2. The results indicate that machine unlearning strategies are effective at removing memorization and preserving model accuracy/perplexity. Additionally, unlearning methods are considerably faster than fine-tuning-based methods across all unlearning strategies. BalancedSubnet outperforms all other unlearning methods in terms of being able to both mitigate memorization and preserve model performance for both noise and backdoors. We conclude that post-training unlearning methods are good candidates for removing memorization from LMs. Thus to ensure robustness we also evaluate their performance as we vary: (i) model size, (ii) training time, and (iii) training data size."}, {"title": "6.1 MODEL SIZE, MODEL TRAINING TIME, DATASET SIZE", "content": "Experimental Design. For each TinyMem model setting (Math+Noise, Math+Backdoor, Language+Noise, Language+Backdoor), we unlearn memorized information for four model sizes: layer \u2208 {2, 4, 8, 16} and at four points during training (epochs) T, as follows, Math+Noise: {500, 1500,"}, {"title": "7 MITIGATING MEMORIZATION IN PRODUCTION-GRADE MODELS", "content": "We demonstrate that memorization mitigation methods developed on TinyMem models can be successfully applied to large production-grade models.\n\nExperimental Design. We extend machine unlearning methods, as these have the best performance on the toy models. We exclude IG, SOU, and Zero, as they are too time- and/or memory-intensive compared to our other unlearning strategies; for Greedy, we perform only one HP run, with ratio = 1e \u2013 5 as this method's linear scaling with LM parameter count is too time consuming relative to the other methods in the billion parameter regime. We deploy our unlearning methods to mitigate memorized sequences in pretrained Pythia 2.8/6.9B models (Biderman et al., 2023); see Appendix A.4.1 for how memorized sequences were extracted. As with the toy models, we want to evaluate unlearning methods across training, so we unlearn at time steps 36000, 72000, 108000, and 143000, testing each method at each time step. For each method, we tune relevant HPs: see Appendix A.4.2.\n\nDiscussion & Conclusion. Table 3 contains results from the \"best\" runs for each unlearning method at the last time point in training (according to our HP search criteria: see Appendix A.4.2). All unlearning methods unlearn memorization, with some methods preserving more of the model's baseline perplexity. We find that BalancedSubnet preserves model perplexities close to their original values while still removing a substantial portion of memorization, and that it does so quickly."}, {"title": "8 RELATED WORK", "content": "Memorization in LMs leaves private, copyrighted, or sensitive training data vulnerable to extraction (Carlini et al., 2019; Patil et al., 2023; Shoaib, 2023; Schwarzschild et al., 2024). Methods have been developed to extract data from trained LMs (Carlini et al., 2021; Nasr et al., 2023). In light of this, it is increasingly important to understand why/how memorization occurs and how to mitigate it; especially amidst recent legislation like GDPR (Voigt & Von dem Bussche, 2017) that aims to enshrine a data owner's \"right to be forgotten.\" Properties such as data duplication, model size, and input context length contribute to eliciting memorized content in LMs (Carlini et al., 2023; Kandpal et al., 2023). Moreover, memorized data has been shown to be localizable within trained neural network weights (Chang et al., 2024; Maini et al., 2023; Stoehr et al., 2024; Baldock et al., 2021; Stephenson et al., 2021; Kassem et al., 2023). Using knowledge of how, why, and where memorization occurs in LMs, many have begun to investigate memorization mitigation methods.\n\nMemorization mitigation methods fall into three broad classes: 1) Prior to training: Data curation, such as by de-duplication (Lee et al., 2021; Biderman et al., 2023; Silcock et al., 2022; Kandpal et al., 2022); 2) During training: Regularizers (Hans et al., 2024; Cheng et al., 2023; Maini et al., 2023); 3) Post-training: Fine-tuning (Howard & Ruder, 2018; Church et al., 2021) and Machine Un-learning methods (Maini et al., 2023; Chang et al., 2024; Eldan & Russinovich, 2023; B\u0103rbulescu & Triantafillou, 2024). Existing methods are quite limited. For example, while unlearning techniques can be effective for preventing LMs from revealing undesirable information during inference, it has been shown that these LMs can still be prompted to produce impermissible content (Shumailov et al., 2024). Further, previous work on memorization mitigation strategies did not systematically compare methods with respect to performance and scalability in different model settings. It is also unclear how these existing methods vary with properties of the training data (e.g., easy-to-learn vs. hard-to-learn data)."}, {"title": "9 CONCLUSIONS", "content": "As memorization of training data becomes increasingly pervasive in modern LMs, it is important to study the causes of, and/or remedies for, this behavior. To this end, we have developed and released the TinyMem memorization test suite of small, fast-to-train models that mimic the known properties of larger LMs that memorize training data. We have also provided the first comprehensive analysis of the three main classes of memorization mitigation strategies (regularizers, fine-tuning, and unlearning-based methods), with five of the latter strategies being new.\n\nWe stress tested each of 17 strategies across a range of model training recipes (e.g., varying model size, training dataset, training lengths) from three perspectives: (i) memorization mitigation effectiveness; (ii) model accuracy preservation; and (iii) method efficiency (speed). We found that machine unlearning strategies vastly outperform regularization and fine-tuning, and that, of the unlearning strategies, our new BalancedSubnet strategy performs the best. We also demonstrated, by applying unlearning methods to Pythia 2.8 and 6.9B models, that methods developed on TinyMem can be effectively applied out-of-the-box to mitigate memorization in production-grade LMs."}, {"title": "A APPENDIX", "content": "A.1 TinyMem: A TINY MODEL SUITE TO STUDY MEMORIZATION\n\nWe detail our TinyMem model data and training criteria in Sections A.1.1 and A.1.2. Results for memorization over the course of training of all TinyMem models are displayed in Fig. 6"}, {"title": "A.1.1 THE TinyMem MATH MODELS", "content": "Vocabulary: Each model has a 14 token vocabulary V \u2192 {\"0\": 0, \u201c1\u201d: 1, \"2\": 2, \"3\": 3, \"4\" :\n4, \"5\": 5, \"6\": 6, \"7\": 7, \"8\": 8, \u201c9\u201d: 9, \u201c ^ \u201d : 10, \u201c$\u201d : 11, \":\" : 12, \u201c", "13}.\n\nLayers": "We train models with varying numbers of layers \u2208 {2, 4, 8, 16}.\n\nTraining Data: For each model, we train two versions: one with additive data, and one with multiplicative data; both types of data are defined in Section 2.3.\n\nFor the additive data, we train each model jointly on five different addition tasks where we vary the additive bias parameter b \u2208 {2, 3, 4, 5, 7} per task. We consider a learnable additive \u201ctask\u201d to be the data set corresponding to a specific b; for example, the \u201ctask\u201d for b = 2 is \u201cadding by 2.\"\n\nFor the multiplicative data, we train each model jointly on five different multiplication tasks where we vary the multiplicative coefficient w \u2208 {2, 3, 4, 5, 7}, bias parameter b = 0, and modulus parameter d = 20134 per task. We consider a learnable multiplicative \u201ctask\" to be the data set corresponding to a specific w; for example, the \u201ctask\u201d for w = 2 is \u201cmultiplying by 2 mod 20134.\"\n\nDataset Size: For both additive and multiplicative data, to better understand the effect of dataset size on memorization, we train three versions of each math model with increasing \u201cdataset size.\u201d Each of the three training datasets includes 19000 samples for the 7-task (the primary task) and 2000, 9000, and 19000 samples (for the small, medium, and large datasets, respectively) for each of the 2-, 3-, 4-, 5-tasks (the auxiliary tasks), for a total of 27000, 55000, and 95000 samples. We also create a test dataset comprising 5000 clean (i.e., non-perturbed) samples: 1000 each for the 2-, 3-, 4-, 5-, and 7-tasks.\"\n\nArtifacts: For each model, we train two versions: one with noise as defined in Def 2.2, and one with backdoors as defined in Def 2.3.\n\nNoise: When injecting a noise artifact into a dataset, we select 1000 random samples from our 7-task dataset and noise them using our noising scheme (see Def 2.2). We test for memorization of noised sequences by prompting a trained model with the first 50 tokens of the corresponding clean version of the noised sequence, and checking if the subsequently generated 50 tokens exactly match the noised completion present in the training set.\n\nBackdoor: In the case of including a backdoor artifact, for a given model, we choose a random trigger phrase of \"100 + random seed.\" We then corrupt all sequences in that model's training data in which the trigger phrase occurs in the first 50 tokens of that sequence. We use our backdooring scheme (see Def 2.3) to corrupt those sequences. Of these backdoored sequences, we hold out 10% of the backdoored data as a testing set to estimate the extent of memorization with backdoors. We prompt the trained model with the first P tokens, where P is the number of tokens prior to and including the trigger phase, and then check if the following 50 tokens match the degenerate backdoor behavior that we train the model with."}, {"title": "A.1.2 THE TinyMem LANGUAGE MODELS", "content": "Vocabulary: Each model has a 50257-token GPT2 vocabulary (i.e., sub-word-based tokenization scheme).\n\nLayers: We train models with number of layers \u2208 {2, 4, 8, 16}.\n\nTraining Data: Models are trained on a Wikipedia corpus (Merity et al., 2017).\n\nArtifacts & Duplication: For each model, we train two versions: one with noise as defined in Def 2.2, and one with backdoors as defined in Def 2.3."}, {"title": "A.1.3 LM MEMORIZATION PROPERTIES", "content": "We describe factors that affect memorization in LMs.\n\nTraining Dataset Size. More training data leads to less memorization. From Fig. 6a, we see that as we increase dataset size from left to right (i.e., the augmentation factor), the overall memorization decreases. This is also supported by the findings of Yu et al. (2022) and Schmidt et al. (2018).\n\nTrain Data Duplication. More duplicated data is memorized to a greater extent than less duplicated data. From Fig. 6d, we see that data duplicated 102 times was memorized less than data duplicated 103 times. This finding follows results from Carlini et al. (2023).\n\nModel Size. Bigger models (i.e., models with more layers) memorize more. We see in Fig. 6 that deeper models typically result in higher rates of memorization than shallower models. This finding follows results from Carlini et al. (2023).\n\nTraining Data Artifacts. Noise artifacts (see Figs. 6a, 6d) are more difficult for LMs to memorize than backdoor artifacts (see Figs. 6b, 6c). Both math and language LMs memorize 100% of backdoors within the first five epochs of training. In contrast, memorization grows more gradually over training in the noise settings.\n\nContext Length. In TinyMem, we constrain the model context length to 150 tokens. Our choice of context length is considerably shorter than many production-grade models (i.e., GPT2-small was trained with a 1024 token context window (Radford et al., 2019)). While a shorter context length enables rapid model inference and training, as the attention operation in transformer-based LMs has a time and memory complexity that scales quadratically with context length (Tay et al., 2021; Dao et al., 2022)), it limits our ability to test whether context length is a key factor in our model's ability to regurgitate memorized information as shown in Carlini et al. (2023). For now, we choose to evaluate (n = 100, k = 50) memorization (as per Def 2.1), and we do not study the effect of context length on memorization. We justify the choice to only consider memorization at a fixed prompt length of k = 50 in our analysis as many prior works have also considered a single fixed prompt length when studying and designing memorization mitigation strategies (Chang et al., 2024; Biderman et al., 2023; Stoehr et al., 2024). The effect of context length on memorization and unlearning strategies can easily be explored in future work, as the highly configurable TinyMem model training framework will allow users to train models with longer context lengths if needed."}, {"title": "A.2 REGULARIZERS", "content": "A.2.1 REGULARIZERS HYPER-PARAMETER SEARCH + SELECTION\n\nHyper-parameter search: For spectral norm regularization, we varied the hyperparameter lam \u2208 {0.001, 0.01, 0.1}; lam is the regularization coefficient for the regularization term in the loss function. For loss truncation, we varied the hyperparameter dropc \u2208 {0.01, 0.05, 0.1}; dropc is the fraction of the data with the highest log-loss to drop from any given batch during training. For example-tied dropout, we varied the hyperparameter pmem \u2208 {0.01, 0.05, 0.1}; pmem is the fraction of neurons to drop (i.e., the example-tied neurons) after training.\n\nSelection Criteria: For language models, we selected the model corresponding to the training setting that resulted in the lowest average (test perplexity + percent memorized) across all three seeds:\n\nLMbest \u2190 min (avg (perplexity + %memorized)seeds) \\tag{2}\n\nLMs\n\nFor math models, we scored the best run by seeing which training setting resulted in the highest average (test accuracy + percent memorized) across all seeds:\n\nLMbest max (avg (accuracy + %memorized)seeds) \\tag{3}"}, {"title": "A.2.2 REGULARIZER DEFINITIONS", "content": "The spectral norm regularization method is described in detail in Yoshida & Miyato (2017); we closely follow their implementation, which can be found at https://github.com/pfnet-research/sngan_projection.\n\nThe loss truncation method is described in detail in Kang & Hashimoto (2020); we closely follow their implementation, which can be found at https://github.com/ddkang/loss_dropper.\n\nThe example-tied dropout method is described in detail in Maini et al. (2023); we closely follow their implementation, which can be found at https://github.com/pratyushmaini/localizing-memorization."}, {"title": "A.2.3 REGULARIZER TRAINING GRAPHS", "content": "We include visualization of how memorization varied over the course of training in our four-layer models from TinyMem with the use of regularizers in Fig. 7. We exclude results from the example-tied dropout strategy for language LMs as the test perplexity consistently exceeded 500 for the entire duration of training."}, {"title": "A.3 MACHINE UNLEARNING", "content": "A.3.1 MACHINE UNLEARNING METHOD DEFINITIONS\n\nThe neuron-level unlearning methods we study are described in detail in Chang et al. (2024); we closely follow their implementation which can be found at https://github.com/terarachang/MemPi.\n\nWe detail the weight-level unlearning methods in Algorithms: 1,2,3,4,5,6. In these algorithms, we vary the following input parameters:\n\n1. LM: original language model\n\n2. K: number of weights to drop (ratio * num_model_parameters)\n\n3. num_epochs: number of iterations to perform the procedure\n\n4. memorized_sequences: set of sequences memorized by the LM\n\n5. random_sequences: set of random sequences\n\n6. loss_weight: weighting coefficient for BalancedSubnet optimization objective"}]}