{"title": "Identifying the Best Arm in the Presence of Global Environment Shifts", "authors": ["Phurinut Srisawad", "Juergen Branke", "Long Tran-Thanh"], "abstract": "This paper formulates a new Best-Arm Identification problem in the non-stationary stochastic bandits setting, where the means of all arms are shifted in the same way due to a global influence of the environment. The aim is to identify the unique best arm across environmental change given a fixed total budget. While this setting can be regarded as a special case of Adversarial Bandits or Corrupted Bandits, we demonstrate that existing solutions tailored to those settings do not fully utilise the nature of this global influence, and thus, do not work well in practice (despite their theoretical guarantees). To overcome this issue, in this paper we develop a novel selection policy that is consistent and robust in dealing with global environmental shifts. We then propose an allocation policy, LinLUCB, which exploits information about global shifts across all arms in each environment. Empirical tests depict a significant improvement in our policies against other existing methods.", "sections": [{"title": "Introduction", "content": "A Multi-Armed Bandit (MAB) is an abstract concept of a decision problem, where a decision maker has a choice between different actions (arms), and selecting an action yields a stochastic reward. Best-arm identification (BAI) [31], a sub-problem in MAB, aims at identifying the best among all designs/arms without caring about accumulating regret during the exploration. The standard assumption for BAI is that each arm has an underlying reward distribution that is stationary. However, in practice, the reward distributions may change over time. One possible objective in such a non-stationary setting is to track the best arm or minimise the cumulative regret over time, adapting to the environmental changes, and this has been explored extensively in the literature [20, 3, 13, 11, 21].\nIn this paper, we consider a different problem of identifying the design that works best in expectation, across environments. We furthermore assume that environmental changes affect the underlying reward distributions of all arms in the same additive way, i.e., the mean of the reward distribution of arm i in environment j can be described as \\(p_{ij} = \\mu_i + s_j\\). We call this problem Multi-Armed Bandits in the Presence of Global Environment Shifts.\nThis is motivated by the fact that environmental changes often influence the reward of different actions in the same way. For example, when aiming to identify the best pricing strategy for a taxi application, customer's willingness to pay may differ from day to day, based on weather or specific events such as concerts or football matches, which may similarly influence the achievable profit for all considered pricing strategies. Or consider advertising on social media, where the click-through rate of different adverts may increase and decrease synchronously over time depending on external effects such as Christmas approaching, the product being discussed in a talk show, or a celebrity wearing the product. This is confirmed by recently published examples of daily empirical means from marketing experiments with uniformly collected data [18]. The trend of empirical means of all arms is positively correlated, and their relative gaps are quite well-behaved.\nIdentifying the best arm under such settings is challenging, because an arm evaluated more often in more favourable environments (positive offset \\(s_j\\)) may appear better than an arm that was evaluated more often in less favourable environments, even though the latter is better according to the underlying (environment-independent) expected reward \\(\\mu_i\\). Note that this setting can be considered as a special case of Corrupted Bandits [39] and Adversarial Bandits [1] where the adversary can only corrupt rewards of all arms with the same constant \\(s_j\\), and the agent can only observe when the adversary attacks the bandits, and otherwise just receives the corrupted feedback. As such, in theory, existing robust BAI algorithms designed for adversarial environments can be applied to our setting. However, as we will show later in this paper, those algorithms can be less efficient compared to a round-robin exploration since they do not exploit information about global attacks and the notice of corruption. As such, we pose the question whether one can design efficient algorithms that work well in under such global environment shifts and perform better than the trivial round-robin policy.\nAgainst this background, this paper proposes a novel method that takes advantage of this special setting by estimating the global shift from rewards across different arms and uses it to design a suitable statistic for an algorithm design.\nOur contribution and organisation: As far as we are aware, this is the first paper to consider MAB in the presence of global environment shifts. In Section 2, we provide a formal definition of the considered problem, then discuss related work. To address the MAB problem with global environment shifts, we transform it into a regression problem in Section 3 and explain why its solution is a good choice for the best-arm predictor. In Section 4, we propose the LinLUCB allocation policy which applies the confidence bound based on a regression estimator. Numerical experiments in Section 5 are conducted to understand the effectiveness of the proposed shift estimator in different allocation policies and to examine how our proposed LinLUCB algorithm performs in various problem settings. Finally, a summary and ideas for future work will be provided in Section 6\nNotation: Vectors are denoted by lowercase boldface letters and matrices by uppercase boldface letters. In general, we use a superscript of t or k to refer to its value at time step t or its kth value, respectively."}, {"title": "Problem Formulation & Related Work", "content": "In this section, we formally define the new K-armed stochastic bandit problem in the presence of global environment shifts. We then review literature related to our setting and show how global environmental shifts negatively affect existing algorithms for finding the best arm."}, {"title": "BAI with Global Environment Shifts", "content": "Given a finite discrete set of arms \\([K]\\), the reward \\(r_{ij}\\) from arm i under the jth environment is an i.i.d random variable, consisting of three components:\n\\[r_{ij} = \\mu_i + s_j + \\epsilon\\]\nwhere \\(\\mu_i \\in \\mathbb{R}\\) is the true quality of arm i, \\(s_j\\) represents a global shift on the reward of all arms that depends on the environment j, and noise is normally distributed, \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\).\nWe assume that an agent can only observe two things:\n1. the reward \\(r_{ij}\\) if arm i is chosen during environment j, and\n2. the time of an environmental change.\nNote that no information about the environment is available, in particular we cannot directly observe its shift \\(s_j\\), nor do we have features that describe the environment and that could be used, e.g., in contextual MAB. In addition, we suppose that shifts and noise are independent and do not make any specific assumptions about the structure of environment shifts.\nThe best arm is defined as the arm with the largest expected reward, \\(i^* = \\arg \\max_i \\mu_i\\), which is independent of the environment. Every environment j is assumed to remain valid from time \\(t = c_{p_{j-1}}\\) to \\(t = c_{p_j}\\), i.e., the environment is piecewise stationary. The duration during which an environment is valid may be stochastic, and we do not need to assume an underlying distribution for the length of environments. Note, however, that since there is no prior knowledge about \\(s_j\\) for each environment, a single observed reward under an environment cannot provide any statistical information about the arm. Such extremely short environment durations would thus have to be ignored in practice. Instead, we here assume that the length of environment \\(\\Delta c_{p_j} := c_{p_j} - c_{p_{j-1}} \\ge 2\\) for all j.\nIn each time step t, we can first observe whether the environment has changed, and then allocate one sample to one of the arms. Our aim is to design an allocation policy that decides which arm to sample next, given all the historical information, and a selection policy that will recommend the best arm at the end of sampling, after having exhausted the available budget of T samples, i.e., we consider a fixed budget setting. A policy is defined as a mapping from sequences of action-reward information, including the environment ordinal, \\(\\mathcal{I}_t := (j^1, i^1, r_{i^1 j^1}, ..., j^{t-1}, i^{t-1}, r_{i^{t-1} j^{t-1}}, j^t)\\), to a set of arms \\([K]\\)."}, {"title": "Related Work", "content": "Our problem formulation shares similarities with (but is different from) many papers on the problem of identifying the best arm in a non-stationary setting. Furthermore, in Section 2.3, we will explain that existing algorithms for stationary settings can identify the best arm in the long run with a high probability under some shift conditions. Therefore, we also review some literature on stationary settings.\nBAI in a stationary setting: There is a rich literature on BAI algorithms which assume rewards are i.i.d, drawn from a stationary distribution and mostly bounded. In fixed-confidence settings, most algorithms are either elimination-based or confidence-bound-based [24], such as Exponential Gap Elimination [29], LUCB [28], and Lil'UCB [26]. A simple and efficient algorithm for the fixed-budget setting is Sequential Halving (SH), which divides the total budget into multiple phases and halves the number of candidate arms after a sampling phase ends [29]. There are also some variations of the upper confidence bound (UCB) algorithm applied to this task [12, 7].\nRanking&Selection (R&S): R&S [23] is a problem class in the stochastic simulation literature, and it is closely related to BAI in MAB, although usually Gaussian reward distributions are assumed [6]. In a fixed-budget setting, most algorithms are derived either from an equivalent problem of the PICS minimisation with a budget constraint or dynamic programming [35], such as the OCBA procedure [14], 0-1 procedure [15] and Knowledge-Gradient policy [19]. These adaptive policies allow batch sampling in multiple phases and work well in practice.\nAdversarial Bandits & Corrupted Bandits: In general, adversarial settings assume that a sequence of rewards \\({r_t\\}_{t=1}^T\\) for each arm is determined by an adversary [9], which results in a reward that is not a random variable. There are some variants of adversarial bandits which merge a stochastic structure into the problem formulation. For instance, corrupted bandits assume reward distributions can be attacked by an adversary which strives to trick an agent by injecting contaminated information [27, 33]. One can treat our setting as a special case of corrupted bandits where an adversary can instead choose a sequence of global shifts to fool a learner in advance. Few papers consider BAI tasks in this formulation. For a fixed-budget setting, [39] assume an agent can only observe corrupted rewards \\(r = \\mu_i + s + \\epsilon\\) where an adversary has a bounded total corruption budget, \\(\\sum_{i: i \\text{ maxi}\\in[K]} |s_i| \\leq S\\) for some constant S and corrupted rewards are bounded. They propose the PSS algorithm, which is an extension of the SH algorithm [29] with uniform randomisation. Besides, BAI in a corrupted model is studied in a more general way for fixed-confidence settings without any strict assumptions of true reward distribution and contaminated distribution by [5]. In adversarial bandits, the unique best arm over the total time horizon T is possibly undefined without rigorous assumptions. [1] assumes the unique best arm with respect to the highest cumulative rewards exists and studies BAI for the Best-of-Both-world problem. They propose an algorithm P1, in which the probability of sampling each arm p is generated from a ranking of the inverse-propensity-score (IPS) estimator, and the final recommendation is an arm with the highest IPS estimator. Note that the IPS estimator \\(\\hat{p} := (1/T) \\sum_{t=1}^T 1/\\pi_t 1[i_t = i]\\) is an unbiased estimator of the average reward up to time T. Another way to define the best arm is by assuming the convergence of the reward sequence or \\(\\lim_{t \\rightarrow \\infty} r_t\\) exists [25, 34] and the universal best arm is defined by the highest limit. To the best of our knowledge, no BAI study in adversarial bandits considers the global structure of change, and in Section 2.3, we will empirically show that without exploiting such a structure, these algorithms do not work well in our setting.\nPiecewise-stationary Bandits: This type of bandit problem is quite relevant to our setting since it allows mean \\(\\mu_i\\) of the reward distribution to remain stationary within a certain time horizon \\(\\Delta c_{p_j}\\) for \\(j \\in [J]\\) where J is the number of environmental changes up to time T. Similar to adversarial settings, the task of minimising regret is more natural to study. When environments do not change too frequently, and the change is abrupt, there are three general approaches to tackle this setting [20, 3, 13, 11, 21]:\n1. Reset strategy if drift is detected\n2. Discounted factors to reduce the importance of rewards received long ago\n3. Sliding window to only evaluate rewards from a desirable time window.\nSome works also introduced an evolutionary algorithm and an adaptive allocation strategy to track the best arm under abrupt changes [30]. We are aware of only one study of BAI for piecewise stationary bandits [4]. Their setting is a generalisation of adversarial settings where an adversary chooses a sequence of reward distributions instead of a sequence of rewards. Some distributions possibly have zero variances. The best arm is defined by \\(i^{pws} = \\arg \\max_i \\sum_{t=1}^T \\mu_i^t\\). They propose the SER3 algorithm that combines a successive elimination mechanism with randomised round-robin sampling, utilising a criterion derived from Hoeffding's inequality to eliminate potentially inferior arms until only one best-predicted arm remains. In our paper, drift detection is not required since we assume the agent knows when the change occurs. Besides, our study is a fixed-budget setting, different to the fixed-confidence setting of [4]. But most importantly, we assume global shifts that affect all arms in the same way, whereas this is not the case in the other publications.\nLinear Bandits: In Section 3, our reward model will be vectorised as a linear function of the index of the arm and of the environment, which is closely related to the linear relationship of feature and reward of linear bandits. For BAI in linear bandits setting, each arm i is represented by a known feature vector \\(x_i \\in \\mathcal{X} \\subset \\mathbb{R}^d, |\\mathcal{X}| = K\\). At time t, a noisy reward \\(r_t\\) is assumed to be a linear function of an unknown model parameter \\(\\theta \\in \\mathbb{R}^d\\); \\(r_t = x_t^\\top \\theta_t + \\epsilon\\). In fixed-budget settings, most of the works assume the unknown parameter is fixed, \\(\\theta = \\theta^*\\) for all t; therefore, the best arm is defined by the highest expected reward mean, \\(i = \\arg \\max_i x_i^\\top \\theta^*\\). [10] develops the GSE algorithm for which the total budget is evenly split into multiple phases, and a specified number of arms is eliminated after each phase ends. The GSE algorithm applies an adaptive sampling in each phase and uses the least square estimator of \\(\\theta^*\\) to rank the arms for elimination of the worst. [38] proposes the OD-LinBAI algorithm which combines the ideas of the SH algorithm and G-optimal design [31]. [2] propose a variant of the SH algorithm equipped with the least square estimator which is robust to moderate levels of misspecification from the linear bandits model. A recent paper [36] generalises the assumption of a static model parameter to a non-stationary setting. The goal is to find the optimal arm \\(i^*\\) over the average model parameter \\(\\bar{\\theta} = \\frac{1}{T} \\sum_{t=1}^T \\theta_t\\) at the specified time horizon T; \\(i^* = \\arg \\max_i x_i^\\top \\bar{\\theta}\\). The authors propose the G-BAI algorithm, which samples the next allocation based on G-optimal design and estimates \\(\\bar{\\theta}\\) from an inverse-propensity score estimator. From the BAI in linear bandits literature, a major difference to linear bandits from our study is that the dimensionality d is fixed, whereas in our setting, the number of dimensions (environments encountered) keeps growing. In order to apply linear bandit algorithms in our setting, since there is no feature about the environment apart from a growing index of environment, a tabular approach and an approach of averaging the model parameter will not be very effective."}, {"title": "Effect of Environment Change on Existing Policies", "content": "In our setting, the global shift can affect policies in two major ways:\n1. the behaviour of the adaptive allocation policy, and\n2. the selection of the best-predicted arm.\nWe consider a sample mean of reward, which is one of the most commonly used statistics in BAI algorithms such as SH, UCB, and LUCB, including the criteria of the selection policy of round-robin sampling. Denote \\(\\bar{r}_i := \\frac{\\sum_{j=1}^J \\sum_{k=1}^{n_{ij}} r_{ij}}{ \\sum_{j=1}^J n_{ij}}\\) as the sample mean of arm i where J is the latest environment during sampling, \\(r_{ij}\\) is the kth reward or arm i in environment j, and \\(n_{ij}\\) is the number of samples on arm i under the jth environment. Under our setting the difference of sample means between arm \\(i_1\\) and \\(i_2\\), \\(\\bar{r}_{i_1} - \\bar{r}_{i_2}\\) contains the term of \\(\\sum_{j=1}^J s_j (n_{i_1j}/\\sum_{j=1}^J n_{i_1j} - n_{i_2j}/\\sum_{j=1}^J n_{i_2j})\\). From such a calculation, the influence of the environment can lead to biased sample means and biased differences if the numbers of samples of each arm under each environment are different. For example, in the case of only one environment change happening or J = 2, suppose an inferior arm \\(i_1\\) such that \\(\\mu_{i_1} < \\mu_{i_2} < 0\\) has more samples than a superior arm \\(i_2\\) in the second environment \\(n_{i_1 2} \\ge n_{i_2 2}\\) meanwhile for the first environment they have an equal number of samples \\(n_{i_1 1} = n_{i_2 1}\\). If \\(s_2\\) is sufficiently larger than \\(s_1\\) then decision-makers may select an inferior arm due to \\(\\bar{r}_{i_1} - \\bar{r}_{i_2} > 0\\).\nSuch a calculation is a main issue for the sample-mean-based final selection if an adaptive allocation policy is used. This phenomenon can also occur in elimination-based algorithms, even when uniform sampling is used, since the change cannot be controlled. We may deduce that the sample mean is not a suitable statistic for both allocation policy and selection policy if no knowledge about the shift is provided. However, if the shift satisfies the conditions in Corollary 2 of [17], such as shift is a uniform random variable, existing BAI algorithms that sample all arms sufficiently under different environments will be able to identify the best arm with a high probability. The main reason is the shift term in the sample-mean calculation \\(\\sum_{j=1}^J s_j (n_{i_1j}/\\sum_{j=1}^J n_{i_1j} - n_{i_2j}/\\sum_{j=1}^J n_{i_2j}) \\rightarrow O\\) as \\(J \\rightarrow \\infty\\) for all \\(i \\in [K], j \\in [J]\\).\nAnother approach is to use the IPS estimator, which is an unbiased estimator for randomisation-based algorithms in adversarial settings. However, with the same reason as sample mean calculation, insufficient sampling for some arms in some environments can still cause a bias for ranking the IPS estimator since a probability-weighted reward in a favourable environment can be excessive when it is compared to the one in a less favourable environment. Lastly, implementing robust BAI algorithms in contaminated bandits could alleviate the estimator problem, but without exploiting the global shift structure, that algorithm still needs high budgets to identify the best arm."}, {"title": "Linear Regression for The Selection Policy", "content": "As explained in Section 2.3, even if \\(s_j\\) is bounded, a sample mean of rewards may not be an appropriate statistic for predicting the best arm since different arms may have been evaluated under different environment shifts. In the following, we derive a point estimate by formulating a regression problem."}, {"title": "Ordinary Least Square (OLS) Estimator", "content": "Since we are only interested in identifying the best arm, without loss of generality, we assume that \\(s_1 = 0\\). Considering the stated problem as a regression model, a reward matrix, given a total number of evaluations N across J environments, can be rewritten in two ways as follows\n\\[\\mathbf{r} = A\\mathbf{\\mu} + B\\mathbf{s} + \\mathbf{\\epsilon} \\]\n\\[\\mathbf{r} = X\\mathbf{\\theta} + \\mathbf{\\epsilon} \\]\nwhere\n*  \\(\\mathbf{r} := [r_1 \\enspace r_2 \\enspace ... \\enspace r_N]^\\prime\\) is a column vector containing all rewards obtained from N evaluations\n*  \\(\\mathbf{\\epsilon} := [\\epsilon_1 \\enspace \\epsilon_2 \\enspace ... \\enspace \\epsilon_N]^\\prime\\) is a corresponding noise vector where \\(\\epsilon \\sim \\mathcal{MN}(0, \\sigma^2 \\mathbf{I}_N)\\).\n*  A is a coefficient matrix in which each row \\(a_t\\) is a vector of the standard basis of \\(\\mathbb{R}^K\\) referring to the chosen arm \\(i_t\\). i.e., \\(a_t = e_K(i_t)\\)\n*  Similarly, B is a coefficient matrix referring to the environment ordinal \\(j_t\\), i.e., each row \\(b_t = e_{J-1}(j_t - 1)\\) for \\(j \\ge 2\\).\n*  \\(\\mathbf{\\mu} := [\\mu_1 \\enspace ... \\enspace \\mu_K]^\\prime\\) is a K-dimensional column vector containing the actual means of all arms.\n*  \\(\\mathbf{s} := [s_2 \\enspace ... \\enspace s_J]^\\prime\\) is a (J-1)-dimensional column vector containing actual shifts relative to the first environment.\n*  \\(X = [A \\enspace B]\\) is a coefficient block matrix. Similarly, the (K+J-1)-dimensional joint parameter vector \\(\\mathbf{\\theta} = [\\mathbf{\\mu}^\\prime \\enspace \\mathbf{s}^\\prime]^\\prime\\).\nNote that the dimensions of J-1 and K + J-1 are due to the zero-valued shift \\(s_1\\) assumption; therefore, such a shift will not be estimated. Model (1) is a hybrid linear model similar to the model in [32]. One difference is the dimension of our parameters s, which grows by 1 when transitioning to a new environment, but the values of parameters in previous environments are unchanged.\nTo find the solution to the regression problem, the second model (2) is easier to solve. Based on a least squares method, we can derive a unique solution;\n\\[\\mathbf{\\hat{\\theta}} = (X^\\prime X)^{-1} X^\\prime \\mathbf{r}.\\]"}, {"title": "Requirements for Regression", "content": "Merely merging an OLS estimator with an allocation policy may lead to an ill-posed problem due to a singularity of matrix \\(X^\\top X\\). In linear bandits, the singularity problem is alleviated by e.g. adding a regularisation term in the regression loss function [32, 22] or applying a dimensionality reduction technique [38, 10]. However, in our setting, these approaches are not helpful if all arms are not observed in the same environment or the loss function can be partitioned and optimised separately since mean estimators are not comparable. Therefore, in general, any allocation policy that applies regression and is not aware of environmental change cannot be directly implemented. In addition, evaluating only one arm in one environment can lead to unchanged mean estimators since an estimated shift in such an environment can be varied arbitrarily. To ensure the existence and uniqueness of the regression solution, there are a few requirements for allocation policies.\nInitialisation for Regression\nDisconnected evaluations across different environments can cause an ill-posed optimisation problem. For instance, given a 5-arms setting, if a policy evaluates arms {1,2} under the first environment, arms {1,2,3} under the second environment and arms {4,5} under the third environment, then parameters of the loss function will be separated into two partitions for arms 1, 2, 3 and arms 4, 5. The information share of regression parameters, in fact, can be represented by a graph where the vertices are arms, and the undirected edge between two vertices exists if the corresponding arms have been sampled under the same environments. From the mentioned example, we can represent it with two sub-graphs where one is 1 - 2 - 3 - 1, and another is 4 - 5 as in the top row of Figure 3. So the estimators of arm 1 and arm 4 are not comparable. The regression approach requires a connected graph connecting all arms to fully share information - if the graph is disconnected, it is impossible to rank solutions from different arms relative to each other. The initialisation phase is crucial for every allocation policy to generate at least a tree structure.\nEvaluating two first distinct arms when the environment changes\nEven if an environment length is relatively short, some allocation policies may evaluate only one arm under the same environment. This would not provide any valuable information since the estimated shift under such an environment can be any arbitrary value subject to the value of the estimated means. In other words, there are no updates in estimator values if only one arm is evaluated in one environment. In order to avoid such an issue, evaluating at least two distinct arms once the environment changes is imperative."}, {"title": "LinLUCB Allocation Policy", "content": "Given a normal distribution of OLS estimator for the actual means \\(\\hat{\\mu}\\) at time t, the upper confidence bound of the actual mean of arm i can be defined as\n\\[UCB_i^t = \\hat{\\mu}_i + \\gamma_t \\mathbf{a}_t^\\prime \\sigma^2[A^\\prime(I - 2H_B + H_B H_A H_B)A]^{-1} \\mathbf{a}_t\\]\nwhere \\(\\gamma_t\\) is an exploration rate at a time step t. We propose a new variant of the LUCB algorithm modified from [28] for our linear model in Algorithm 2. The LUCB algorithm was originally designed for a PAC subset selection in a fixed-confidence setting where sampling two arms every time step is allowed. However, we found its potential to be implemented in a fixed-budget setting, especially in our setting. The LUCB algorithm ensures that at least two arms are evaluated in every environment, allows for an adaptive budget T, and is optimal in a two-armed setting with the worst environment length of 2. The algorithm starts by executing Algorithm 1 for the initialisation phase and then alternating samples, the greedy arm and the most potentially best arm from the rest, while guaranteeing that the two first samples in the new environment are distinct. At time t, the greedy arm is indexed based on the highest mean estimator \\(l_t := \\arg \\max_{i \\in [K]} \\hat{\\mu}_i\\), in which ties are broken arbitrarily, then the rest of arms are filtered to find the highest UCB arm \\(u_t := \\arg \\max_{i \\in [K] \\setminus \\{l_t\\}} UCB_i^t\\). In this part, since our setting does not allow the sampling of two arms in one time step, we mimic the batch sampling by sequentially selecting \\(l_t\\) and \\(u_t\\) instead of using the interleaving strategy. If there is an environment change and the choice of second sampling in such a new environment is the same as the choice of first sampling, we can swap the sampling order of \\(l_t\\) and \\(u_t\\) to ensure two first choices of sampling are different. Motivated by the UCB1-normal algorithm from [8], we use \\(\\gamma_t = \\sqrt{16\\sigma^2 ln(t)/ 2\\sum_{j=1}^J \\sum_{k=1}^{n_{ij}}}\\) as an exploration rate. Finally, the selection policy chooses the highest OLS mean estimator as the best-predicted arm."}, {"title": "Empirical Evaluation", "content": "In order to understand how environmental change influences different policies on various configurations, we conduct numerical experiments for the proposed algorithm and modified versions of some existing policies. We chose the examined problem settings from [16] since it was a seminal paper developing a policy for PICS minimisation for Gaussian rewards. Two configurations are monotone decreasing means (MDM) configuration and slippage configuration (SC), with a modification by adding random shifts \\(s_j \\sim \\mathcal{U}(0, 20)\\). For the MDM configuration, rewards for alternatives \\(i = 1, ..., K\\) are\n\\[r_{ij} \\sim \\mathcal{N}(\\delta (i - 1) + s_j, \\sigma^2),\\]\nwhile for the SC configuration, rewards are\n\\[r_{ij} \\sim \\mathcal{N}(s_j, \\sigma^2) \\text{ for } 1 \\le i < K, \\enspace r_{Kj} \\sim \\mathcal{N}(\\delta + s_j, \\sigma^2).\\]\nWe use PICS as the performance measure estimated by the fraction of replications selecting the true best alternative correctly. For a fair comparison, all procedures in all time steps share the same set of potential observations by controlling random seeds. The PICS convergence plots below are generated using \\(10^5\\) replications. We set the value of parameters in the problem as \\(\\delta = 0.5\\) and \\(\\sigma = 1\\)."}, {"title": "Comparison against standard policies", "content": "From the proof of Theorem 1, the uncertainty of the estimated shift plays a vital role in the convergence of the mean OLS estimator. Since the environment length has a significant influence on the shift estimation, we test the performance of our proposed LinUCB policy against other existing policies in the following environmental change scenarios with 5 arms, additional results on different scenarios can be found in the Supplementary materials C.\n* General scenario, where \\(\\Delta c_{p_j} \\sim \\widetilde{\\mathcal{U}}(2, 10K)\\): The duration of the stationary phase of the environment may vary from very short to relatively long, leading to different challenges in estimating.\n* Cannot-sample-all-arms scenario, where \\(\\Delta c_{p_j} \\sim \\widetilde{\\mathcal{U}}(2, K - 1)\\): The environment is very short and policies cannot explore all arms in one environment\nThe following list describes the tested policies:\n* Round-robin: round-robin sampling with \\(\\bar{r}_i\\) as a selection policy\n* 0-11: procedure proposed in [16] and \\(\\bar{r}_i\\) as a selection policy\n* SER3: the elimination-based algorithm from [4] for fixed-confidence piecewise-stationary bandits where prior knowledge about the optimal gap \\(\\mu_1 - \\max_{i \\neq 1} \\mu_i\\) is provided\n* LinLUCB : Our proposed method (Section 4)\nFollowing [16], 0-11 and LinLUCB first perform an initialisation phase with \\(n_0 = 6\\) samples with a round-bin sampling and Algorithm 1, respectively. As shown in Figure 4, LinLUCB significantly outperforms other policies in all configurations. With short environment durations (Cannot-sample-all-arms), shift estimation has more uncertainty, and consequently we observe a slower decaying PICS compared to the General setting for both MDM and SC configurations. For the 0-11 policy, the General setting seems actually more difficult because an imbalance of samples per environment can strongly bias the sampling strategy and the selection policy. Sampling from several environments can reduce the dominating effect of a few environmental shifts, resulting in better PICS in quickly changing environments (compare Figure 4b with 4a and Figure 4d with 4c including the initial worsening in all cases). But even in the Cannot-sample-all-arms scenario, 0-11 performs worse than Round-Robin."}, {"title": "Reduce-to-MAB strategy", "content": "In order to gauge the benefit of shift estimation, we test an alternative approach by applying any existing policy designed for a stationary environment, and once a change occurs, we simply subtract the OLS shift estimators from the respective rewards (\\(r_{ij}^{new} = r_{ij} - s_j\\)) to approximately reduce the problem to a standard MAB problem without shifts (Reduce-to-MAB strategy). One can suppose that all modified rewards are Gaussian with the expectation of \\(\\mathbb{E}[r_{ij} - s_j"}]}