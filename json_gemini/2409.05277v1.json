{"title": "Disentangled Representations for Short-Term and Long-Term Person Re-Identification", "authors": ["Chanho Eom", "Wonkyung Lee", "Geon Lee", "Bumsub Ham"], "abstract": "We address the problem of person re-identification (reID), that is, retrieving person images from a large dataset, given a query image of the person of interest. A key challenge is to learn person representations robust to intra-class variations, as different persons could have the same attribute, and persons' appearances look different, e.g., with viewpoint changes. Recent reID methods focus on learning person features discriminative only for a particular factor of variations (e.g., human pose), which also requires corresponding supervisory signals (e.g., pose annotations). To tackle this problem, we propose to factorize person images into identity-related and -unrelated features. Identity-related features contain information useful for specifying a particular person (e.g., clothing), while identity-unrelated ones hold other factors (e.g., human pose). To this end, we propose a new generative adversarial network, dubbed identity shuffle GAN (IS-GAN). It disentangles identity-related and -unrelated features from person images through an identity-shuffling technique that exploits identification labels alone without any auxiliary supervisory signals. We restrict the distribution of identity-unrelated features, or encourage the identity-related and -unrelated features to be uncorrelated, facilitating the disentanglement process. Experimental results validate the effectiveness of IS-GAN, showing state-of-the-art performance on standard reID benchmarks, including Market-1501, CUHK03 and DukeMTMC-reID. We further demonstrate the advantages of disentangling person representations on a long-term relD task, setting a new state of the art on a Celeb-reID dataset. Our code and models are available online: https://cvlab-yonsei.github.io/ projects/ISGAN/.", "sections": [{"title": "INTRODUCTION", "content": "PERSON re-identification (reID) aims at retrieving person images of the same identity as a a query from a large dataset, which is particularly important for finding/tracking missing persons or criminals in a surveillance system. This can be thought of as a fine-grained retrieval task in that 1) all images in the dataset belong to the same object class (i.e., person) with large intra-class variations (e.g., pose and scale changes), and 2) they are typically captured with different illumination conditions and background clutter across multiple cameras possibly with different characteristics and viewpoints. To tackle these problems, reID methods have focused on learning metric space [1], [2], [3], [4], [5], [6] and discriminative person representations [7], [8], [9], [10], [11], [12], [13], [14], [15], robust to intra-class variations and distracting scene details.\nConvolutional neural networks (CNNs) have allowed significant advances in person reID in the past few years. ReID methods using CNNs add few more layers for aggregating"}, {"title": "RELATED WORK", "content": "In this section, we briefly introduce representative works related to ours, including short-term and long-term reID methods and disentangled feature representations."}, {"title": "Short-Term relD", "content": "ReID methods typically provide person representations robust to a particular factor, such as human pose, occlusion, and background clutter. Part-based methods represent a person image as a combination of body parts either explicitly or implicitly [9], [10], [11], [12], [16], [17], [32], [33], [34]. Explicit part-based methods use off-the-shelf human parsing/pose estimation models, and extract body parts (e.g., head and torso) with corresponding features [9], [10], [33], [34]. This makes it possible to obtain pose-invariant representations, but the human parsing/pose estimation models often give incorrect pose maps, especially for occluded parts. Instead of using human pose explicitly, a person image is sliced into different horizontal parts of"}, {"title": "Long-Term relD", "content": "Most reID methods [11], [12], [27], [31], [45] mainly focus on a short-term reID task, where a person of interest reappears under another camera in a short period of time (e.g., less than 30 minutes). They assume that the person of interest in a query and gallery images wear the same outfit. As this assumption may not hold after a long time span (e.g., one day after), directly employing typical reID methods [11], [12], [15], [32], [46], [47], e.g., exploiting clothes as discriminative features, for long-term reID is limited. ReIDCaps [48] proposes to use a capsule network [49] to model visual changes of each identity explicitly. While this approach shows competitive performance on long-tern reID, the number of appearance variations affordable to handle is limited by the dimension of capsules. Identity-shuffling in our approach enables eliminating features related to person outfits/belongings from identity-related ones, suggesting that it can be applied to the long-term reID task directly without any bells and whistles. It also does not restrict the number of appearance variations to handle.\nNew datasets for long-term reID have recently been introduced, such as Celeb-reID [48], COCAS [50], Div-Market [51], which consist of multiple images of the same identity but with different clothes and attributes. COCAS [50] and Div-Market use GANs to synthesize realistic images, since it is hard to track persons for a long time and to annotate identification labels manually. The generative models, however, cause inevitable artifacts. Celeb-reID [48] consists of street snap-shots of celebrities on the internet. It provides lots of images for persons wearing different clothes in various scenes, posing a challenge of a practical long-term reID scenario."}, {"title": "Disentangled Representations", "content": "Disentangling the factors of variations in CNN features has been widely leveraged for image synthesis [24], [52], image-to-image translation [25], [53], or feature distillation [23], [40], [54]. D\u00b2AE [52] and IP-GAN [24] disentangle identity and attribute features from face images for face identification and image synthesis, respectively. Similarly, MUNIT [53] and DRIT [25] decompose images into domain-specific and -invariant features for generating diverse outputs. The work of [26] leverages disentangled feature representations for domain-specific image deblurring. In [23], a conditional generative model is exploited to extract class-related and -independent features for image retrieval. Unlike these methods, DR-GAN [54] and FD-GAN [40] use a side information (i.e., pose labels) to learn identity-related and pose-unrelated features for face recognition and person reID, respectively."}, {"title": "APPROACH", "content": "In this section, we describe an overview of our framework, and explain our baseline model. We then present a detailed description of IS-GAN disentangling identity-related/-unrelated features for person reID."}, {"title": "Overview", "content": "We denote by I and y \u2208 {1,2,..., C'} a person image and an identification label, respectively, where C is the number of identities in a dataset. We denote by Ia and Ip anchor and positive images, respectively, that share the same identification label. At training time, we input pairs of Ia and Ip with corresponding identification labels, and train our model to learn identity-related/-unrelated features, denoted by \u03c6R(I) and \u03c6\u03c5(I), respectively. At test time, we compute"}, {"title": "Baseline Model", "content": "To the baseline, we use the identity-related encoder ER only, trained with an identity-related loss. For the encoder ER, we exploit a network architecture, similar to [12]. It has three branches on top of a backbone network, where each branch has the same structure but different parameters. We call them part-1, part-2, and part-3 branches, that slice a feature map from the backbone network equally into one, two, and three horizontal regions, respectively. The part-1 branch provides a global feature of the entire person image. Other branches give both global and local features describing body parts, where the local features are extracted from corresponding horizontal regions. For example, the part-3 branch outputs three local features and a single global one. Accordingly, we extract K features from the encoder ER in total, where K 8 in our case. Without loss of generality, we can use additional branches to consider different horizontal regions of multiple scales."}, {"title": "Identity-Related Loss", "content": "We denote by Ik and \u03c6k (k = 1, . . ., K) horizontal regions of multiple scales and corresponding embedding functions that extract identity-related features, respectively. Following other reID methods [11], [12], [15], [22], we formulate the reID problem as a multi-class classification task, and train the encoder ER with a cross-entropy loss. Concretely, a loss function LR to learn the embedding function \u03c6k is defined as follows:"}, {"title": "IS-GAN", "content": "The identity-related feature \u03c6R(I) from the encoder ER contains information useful for person reID, such as clothing, length of clothing/hair, and gender. However, the feature \u03c6R(I) learned using the classification loss in Eq. (1) only may encode other attributes that are not related to or even distract specifying persons (e.g., human pose, background clutter, scale). This indicates that using the encoder ER alone is not enough to handle these factors of variations. To address this problem, we use an additional encoder EU to extract the identity-unrelated feature \u03c6U(I), and train the encoders, ER and EU, such that they give disentangled feature representations for identifying persons. The key idea behind the feature disentanglement is to distill identity-unrelated information from the identity-related feature, and vice versa. To this end, we propose to leverage image synthesis using an identity shuffling technique. Applying this to the whole body and its parts regularizes the disentangled features. Two discriminators, DC and DD, allow to generate realistic person images of particular identities, further regularizing the disentanglement process."}, {"title": "Identity-Shuffling Loss", "content": "We assume that the disentangled person representations satisfy the following conditions: 1) An original image should be reconstructed from its identity-related and -unrelated features; 2) The shared information between different images of the same identity corresponds to the identity-related feature. To implement this, the generator G is required to reconstruct an anchor image Ia from \u03c6R (Ia) \u2295 \u03c6U(Ia) and \u03c6R(Ip) \u2295 \u03c6U (Ia), while synthesizing a positive image Ip from \u03c6R (Ip) \u2295 \u03c6U (Ip) and \u03c6R(Ia) + \u03c6U(Ip) (Figs. 2b and 2c). We define an identity-shuffling loss as follows:"}, {"title": "Part-Level Shuffling Loss", "content": "We also apply the identity shuffling technique to part-level features (Fig. 3). We randomly choose local features from \u03c6R(Ia), and swap them with corresponding ones from \u03c6R(Ip) at the same locations, and vice versa (Fig. 3a). This assumes that horizontal regions in person images contain discriminative body parts sufficient for distinguishing its identity. Similar to Eq. (3), we compute the discrepancies between the original image and its reconstruction using the identity-related features shuffled in a part-level and the identity-unrelated ones (Fig. 3b). Concretely, we define a part-level shuffling loss as follows:"}, {"title": "Identity-Unrelated Loss", "content": "We disentangle identity-related/-unrelated features without corresponding supervisory signals. Although we train the encoders separately to extract these features, the generator G may largely rely on the identity-unrelated features to synthesize new person images in Eqs. (3) and (4), while ignoring the identity-related ones, which distracts the feature disentanglement process. To circumvent this issue, we use two regularization techniques. In the following, using a slight abuse of notation, we will represent \u03c6R and \u03c6u random variables of identity-related and -unrelated features, respectively, for notational conciseness.\nFirst, we encourage the distribution of identity-unrelated features to be close to the normal distribution N(0, I), and formulate this using a KL divergence loss as follows:\nwhere DKL(P||q) = \u222b p(z)log pq((zz)) dz. The KL divergence loss regularizes the identity-unrelated features by limiting the distribution range, such that they do not contain much identity-related information [24], [25], [26]. This enforces the generator G to use the identity-related features to synthesize new person images, facilitating the disentanglement process.\nSecond, we instead minimize Pearson correlation coefficients in order to encourage identity-related/-unrelated features to be uncorrelated as follows:"}, {"title": "Domain and Class Losses", "content": "To train the generator G in Eqs. (3) and (4), we use two discriminators DD and DC. The domain discriminator DD [20] helps the generator G to synthesize more realistic person images, and the class discriminator Dc [59] encourages the synthesized images to have the same identification labels of"}, {"title": "Training Loss", "content": "The overall objective is a weighted sum of all loss functions defined as"}, {"title": "Implementation Details", "content": "We exploit a ResNet-50 [18] trained for ImageNet classification [19]. Specifically, we use the network cropped at"}, {"title": "Network Architecture", "content": "1 as our backbone to extract CNN features. On top of that, we add two heads for identity-related/-unrelated encoders, respectively. Each encoder has part-1, part-2, and part-3 branches that consist of two convolutional, global max pooling, and bottleneck layers but with different numbers of channels and network parameters. The part-1, part-2, and part-3 branches in the encoders give feature maps of size 1 \u00d7 1 \u00d7 p, 1 \u00d7 1 \u00d7 3p, and 1 \u00d7 1 \u00d7 4p, respectively. See Section 3.2 for details. We set the size of p (i.e., the number of channels) to 256 for both identity-related and -unrelated encoders. We concatenate all features from three branches for each encoder, and obtain identity-related/-unrelated features. The generator consists of a series of six transposed convolutional layers with batch normalization [60], Leaky ReLU [61] and Dropout [62]. It inputs an addition of identity-related and -unrelated features, a noise vector, and a one-hot encoding of identification labels whose dimensions are 2048, 128, and C, respectively. The domain and class discriminators share five blocks consisting of a convolutional layer of stride 2 with instance normalization [63] and Leaky ReLU [61], but have different heads. For the domain discriminator, we add two more blocks, resulting in a features map of size 12 \u00d7 4. We then use this as an input to PatchGAN [64]. For the class discriminator, we add one more block followed by a fully connected layer.\nNote that the IS-GANKL predicts the distribution of identity-unrelated features (i.e., mean and variance), and samples the features using a reparameterization trick [65], in contrast to IS-GANDC that directly outputs the features. This requires IS-GANKL to have additional fully connected layers for estimating the mean and variance of identity-unrelated features, suggesting that it uses extra 83M parameters compared to IS-GANDC. Note also that IS-GANKL shares the same network architecture with IS-GAN [27], the earlier version of our model, except two minor differences: For IS-GANKL, 1) the dimension of identity-unrelated features is set to 2,048 instead of 512, and 2) a generator inputs the addition of identity-related and -unrelated features rather than the concatenation of them. This enables generating images with identity-related or -unrelated features (e.g., Figs. 9 and 12), which is important to infer what information each feature encodes."}, {"title": "Dataset and Evaluation Metric", "content": "We evaluate our models for both short-term and long-term reID tasks. We use Market-1501 [28], CUHK03 [29], and DukeMTMC-reID [30] for the short-term reID, and Celeb-reID [48] for the long-term one. The Market-1501 dataset [28] contains 1,501 pedestrian images captured from six viewpoints. Following the standard split [28], we use 12,936 images of 751 identities for training and 19,732 images of 750 identities for testing. The CUHK03 dataset [29] contains 14,096 images of 1,467 identities captured by two cameras. For the training/testing splits, we follow the experimental protocol in [66]. The DukeMTMC-reID dataset [30], a subset of the DukeMTMC [67], provides 36,411 images of 1,812 identities captured by eight cameras, including 408 identities (distractor IDs) that appear in only one camera. We use the training/test splits provided by [30] corresponding 16,522 images of 702 identities for training and 2,228 query"}, {"title": "Training", "content": "To train encoders and a generator, we use the Adam [68] optimizer with \u03b2\u2081 = 0.9 and \u03b22 = 0.999. For discriminators, we use the stochastic gradient descent with a momentum of 0.9. Similar to the training strategy in [40], we train our models in three stages: In the first stage, we train the identity-related encoder ER using the loss function LR, which corresponds to the baseline model, for 300 epochs over the training data. A learning rate is set to 2e-4. In the second stage, we fix the baseline, and train the identity-unrelated encoder EU, the generator G, and the discriminators DD and DC with the corresponding losses LU, LS, LPS, LD, and LC. This process iterates for 200 epochs with a learning rate of 2e-4. Finally, we train the whole network end-to-end with the learning rate of 2e-5 for 200 epochs. For Celeb-reID [48], motivated by [48], we reduce training epochs for the first and third stages to 50 to prevent an overfitting problem. We plot in Fig. 5 training losses and accuracies of IS-GANDC for each training stage on Market-1501 [28]. We can observe that the losses slowly decrease over epochs, and rank-1 and mAP reach the highest value when the training phase is finished. Note that rank-1 and mAP remain unchanged during the second stage, since we freeze the parameters of the identity-related encoder. We resize all images into 384 \u00d7 128, and augment them with random horizontal flipping, cropping, and erasing [69]. For mini-batch, we randomly select 4 different identities, and sample a set of 4 images for each identity. We use label smoothing, cosine annealing, and a warm-up strategy, following [70], [71], [72]. Motivated by the feature fusion strategy in [48], we do not shuffle local features from lower body parts (i.e., the last horizontal slice of part-2 and -3 branches) for Celeb-reID. Note that recognizing identity-related/-unrelated attributes from lower body parts is extremely challenging even for human, especially when clothing and attributes are changed. The training phase takes 8 hours in total with an RTX2080Ti GPU, and requires 10 GB memory."}, {"title": "Hyperparameter", "content": "For a KL divergence loss, we empirically find that initial training with a large value of \u03bbu is unstable. We thus set du to 1e-3 in the second stage, and increase it to 1e-2 in the third stage to regularize the feature disentanglement. For a decorrelation loss, we set du to 1. Following [25], [40], we fix \u03bbs and \u03bb\u0189 to 10 and 1, respectively. To set other parameters, we randomly split IDs in the training dataset of Market-1501 [28] into 651/ 100, and use corresponding images as training/validation samples. We then sample 160 query images from the validation split, setting the remaining images to a gallery set. We use a grid search to set the parameters (\u03bbR = 20, \u03bbpS = 10, \u03bbc = 2) with \u03bbR\u2208 {5,10,20}, \u03bbpS \u2208 {5,10,20}, and \u03bbc\u2208 {1,2} on the validation split. We fix all parameters, and train our models with the same parameters on Market-1501 [28], CUHK03 [29], DukeMTMC-reID [30], and Celeb-reID [48]. We show in Fig. 6 rank-1 accuracies(%) of IS-GANKL and IS-GANDC on the validation split of Market-1501 [28] while varying hyperparameters. The default hyperparameters are chosen with the aforementioned setting as follows: \u03bbR = 20, \u03bbs = 10, \u03bbps = 10, \u03bbp = 1, \u03bbc = 2, \u03bbu = 1 (IS-GANDC),"}, {"title": "Results", "content": "We show quantitative and qualitative results on short-term and long-term reID benchmarks. We use a single query, and do not use any post-processing techniques (e.g., a re-ranking method [66])."}, {"title": "Quantitative Comparison", "content": "Short-Term reID. We show in Table 1 rank-1 accuracy and mAP for Market-1501 [28], CUHK03 [29] and DukeMTMC-reID [30], and compare our models with the state of the art, including FD-GAN [40], DG-Net [31], MGN [12], P\u00b2-Net [33], and SCAL [37]. We can see that IS-GANDC sets a new state of the art on Market-1501 and CUHK03, achieving 96.1% rank-1 accuracy and 89.4% mAP on Market-1501, and 80.0%/77.4% rank-1 accuracy and 77.3%/73.9% mAP with labeled/detected images on CUHK03. IS-GANKL shows state-of-the-art performance on DukeMTMC-reID, providing 91.4% rank-1 accuracy and 80.9% mAP. Although IS-GANDC uses fewer parameters than IS-GANKL, it shows meaningful performance gains compared to IS-GANKL in terms of rank-1/mAP, i.e., 0.6/0.6, 2.5/3.0, and 2.9/2.1 on Market-1501, CUHK03-detected, and CUHK03-labeled, and shows comparable performance on DukeMTMC-reID.\nFD-GAN [40] is similar to ours in that both use a GAN-based distillation technique for person reID. It extracts identity-related and pose-unrelated features using extra pose labels. Distilling other factors except for human pose is, however, infeasible. Our models on the other hand disentangle identity-related/-unrelated features through identity shuffling, factorizing identity-related factors and others irrelevant to person reID, such as pose, scale, background clutter, without corresponding supervisory signals. Accordingly,"}, {"title": "Qualitative Comparison", "content": "Short-Term reID. We show in Fig. 7a person retrieval results of FD-GAN [40], DG-Net [31], and ours on DukeMTMC-reID [30]. We can see that FD-GAN and DG-Net mainly focus on color information. For example, they retrieve many person images of different identities but with the outfit of similar color to the query person, such as a black jacket, beige trousers, and gray bag. Note that all retrieved images by DG-Net have a similar pose to the query, indicating it fails to get rid of pose-related information from person representations. In contrast, our models retrieve person images of the same identity as the query correctly, and they are robust to large pose variations, occlusion, background clutter, and scale changes.\nLong-Term reID. In Fig. 7b, we show reID results of PCB [11], MGN [12], and ours on Celeb-reID [48]. All models retrieve person images with the same clothing as the query in high rank (i.e., rank 1-3) successfully. PCB and MGN, however, fail to re-identify the query person for the case of outfit changes, retrieving person images with different identities but having a similar pose. We can see that our models better handle the long-term reID problem. For example, they retrieve correct images, w.r.t the query person, even with different clothes and belongings."}, {"title": "Ablation Study", "content": "We show in Table 2 an ablation analysis on different losses in our models. We report rank-1 accuracy and mAP on Market-1501 [28], CUHK03 [29], DukeMTMC-reID [30], and Celeb-reID [48]. We can see that 1) disentangling identity-related/-unrelated features using an identity shuffling technique gives better results on all datasets, but the performance gain for Celeb-reID [48], which contains person images of large appearance variations, is more significant, 2) applying the identity shuffling technique in a part-level further boosts the reID performance, and 3) domain and class discriminators are complementary, and combining all losses gives the best results. Note that IS-GANDC, which encourages disentangled features to be uncorrelated, performs better than IS-GANKL on Market-1501 [28] and CUHK03 [29], which contains person images of comparatively less distracting scene details. On the contrary, IS-GANKL gives better results on challenging datasets, such as DukeMTMC-reID [30] and Celeb-reID [48], where factorizing images into uncorrelated features is ambiguous due to e.g., severe occlusion and/or frequent clothing variations."}, {"title": "Losses", "content": "Decorrelation Loss. We show the effect of a decorrelation loss for IS-GANDC in Table 3. We compare IS-GANDC with two variants: One is trained without the decorrelation loss (IS-GANDC(w/oLu)), and the other is trained with the decorrelation loss but using batch-wise means and standard deviations (IS-GANDC(w/batch \u2013 wiseLu)). All other settings including the network architecture are the same as IS-GANDC. From the first and third rows, we can see that the decorrelation loss is effective to remove identity-unrelated information from the identity-related features, boosting the retrieval performance for both short-term and long-term reID tasks. We can also see from the second and third rows that the loss rather degrades the reID performance significantly with batch-wise means and standard deviations. This indicates that exploiting moving means and standard deviations is critical to decorrelate identity-related/-unrelated features, where the distributions of the features keep varying over every iteration at training time.\nWe show in Fig. 9 images generated using IS-GANDC and the variants. As expected from the results in Table 3, IS-GANDC(w/oLu) and IS-GANDC show similar results. IS-GANDC(w/oLu), on the other hand, does not regularize features to be uncorrelated. As a result, its identity-related/-unrelated features often share common information, similar to IS-GANKL, such as the background in the blue boxes of Fig. 9. We can see that IS-GANDC(w/batch \u2013 wiseLu) reconstructs input images even with identity-unrelated features only, indicating that computing the decorrelation loss in a batch-wise manner prevents regularizing a disentanglement process. We can also observe that IS-GANDC(w/batch \u2013 wiseLu) generates similar images, even using identity-related features from different persons (see the yellow boxes of Fig. 9). This suggests that identity-related features for IS-GANDC(w/batch \u2013 wiseLu) are not discriminative for person reID, degrading the retrieval performance severely.\nWe also provide the result when regularizing IS-GANKL using a decorrelation loss, together with a KL divergence term. Note that we could not regularize IS-GANDC with a KL divergence loss, since applying the KL divergence loss to identity-unrelated features requires a specialized network"}, {"title": "Discussion", "content": "Identity Shuffling. Fig. 8 shows the t-SNE plots for identity-related and -unrelated features of IS-GANKL on the test split of Market-1501 [28], where we randomly sample 26 identities. IS-GANDC shows similar results, so we omit them. We can clearly see that the identity-related features for person images with the same identities are closely embedded, forming compact clusters. On the contrary, the identity-unrelated features are scattered, while the corresponding images with similar poses and background clutter (e.g., concrete pavers) are embedded into close regions.\nWe visualize in Fig. 9 reconstructed images by IS-GANKL (Fig. 9a) and IS-GANDC (Fig. 9b), when a generator G inputs the addition of identity-related/-unrelated features \u03c6R(I) \u2295 \u03c6U(I), identity-related features \u03c6R(I), and identity-unrelated features \u03c6U(I), respectively. We can observe that when our models input identity-related features alone, they generate pose-normalized images that have the same clothing color as the original ones, indicating pose information is successfully removed from the identity-related features. In addition, the green boxes in Fig. 9 show that identity-related features are independent of scale changes. On the other hand, when our models input identity-unrelated features alone, they reconstruct decolorized images with the same pose, scale, and background as the original ones, suggesting that these factors are encoded in identity-unrelated features. Note that identity-related/-unrelated features for IS-GANKL sometimes share common information, since they would not necessarily be uncorrelated. (See the background in the red boxes of Fig. 9 for the example.)\nPart-Level Shuffling. Fig. 10 visualizes the ability of IS-GANKL to disentangle identity-related/-unrelated features in a part-level. We omit results of IS-GANDC, as they show similar results. We shuffle the identity-related/-unrelated features for upper/lower parts between person images of different identities. When identity-related features are shuffled"}, {"title": "Visual Analysis on Long-Term relD", "content": "Identity Shuffling. We visualize in Fig. 11t-SNE embeddings of identity-related (Fig. 11a) and -unrelated (Fig. 11b) features of IS-GANKL trained on Celeb-reID. We randomly sample person images of 46 identities. We omit results of IS-GANDC, as they show similar results. We can see that although a certain person (Clark Gregg, a hollywood actor) changes his outfits, identity-related features extracted from his images are still closely embedded. For identity-unrelated features, on the other hand, person images of celebrities, who wear similar color clothing (e.g., a black jacket with black pants) but have different identities, are embedded into near regions. This demonstrates that identity-unrelated features encode clothing colors for the long-term reID task, which is contrary to the short-term reID task (Fig. 8). We visualize in Fig. 11c t-SNE embeddings of identity-related and -unrelated features, where the color of the points indicates gender. Interestingly, the embeddings of identity-related features are almost linearly separated w.r.t gender, while identity-unrelated ones are not. This further demonstrates that identity-related features encode gender information.\nFig. 12 shows reconstructed images by IS-GANKL (Fig. 12a) and IS-GANDC (Fig. 12b), where the generator inputs the addition of identity-related/-unrelated features, identity-related features, and identity-unrelated features, respectively. We can observe that similar images with a normalized pose are reconstructed regardless of person outfits, when generators take identity-related features only, as in the green boxes. The output images generated using identity-unrelated features, on the other hand, show the same background and clothing color as those using the addition of identity-related/-unrelated features. Note that person images of the same identity do not share most information other than, e.g., gender or face color, in a long-term reID task. The reconstructed images from identity-unrelated features are thus fairly similar to those from the addition of identity-related/-unrelated features.\nPart-Level Shuffling. We show in Fig. 13 the generated images when a part-level shuffling technique is applied to upper body parts. Since we do not shuffle local features from lower body parts to train our model on a long-term reID dataset, we omit the corresponding results. Please refer to Paragraph 4.1.3 for details. We can see that shuffling identity-related features from the upper body parts of two images changes face color between persons. On the other hand, when we shuffle identity-unrelated features, face color remains unchanged, while background of two images is swapped. This suggests that 1) identity-related features encode face color information which is one of the crucial factors for discriminating persons when they change their outfits, and 2) identity-unrelated ones hold background information."}, {"title": "Disentangled Features", "content": "Fig. 14a show examples for the identity-shuffled generation. To synthesize new images, we use identity-related features extracted from source images and identity-unrelated features extracted from target ones. We can observe that synthesized images share the same pose and background with target images, while clothing color changes according to the source images, suggesting that identity-unrelated and -related features encode pose/background and clothing color information, respectively.\nTo further demonstrate the effectiveness of identity-related features, we perform an experiment with attribute labels [84], which are useful for describing person identities, for Market-1501 [28], including gender, long/short hair,"}, {"title": "Comparison With DG-Net [31]", "content": "DG-Net [31", "43": "it transforms the structure feature using the appearance feature by simply scaling and shifting the normalized structure one to synthesize new images. Although AdaIN has shown the effectiveness to alter the color"}]}