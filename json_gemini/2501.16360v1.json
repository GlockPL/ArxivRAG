{"title": "Momentum Contrastive Learning with Enhanced\nNegative Sampling and Hard Negative Filtering", "authors": ["Duy Hoang", "Huy Ngo", "Khoi Pham", "Tri Nguyen", "Gia Bao", "Huy Phan"], "abstract": "Contrastive learning has become pivotal in unsupervised representation learning, with frameworks like Momentum Contrast\n(MoCo) effectively utilizing large negative sample sets to extract discriminative features. However, traditional approaches often\noverlook the full potential of key embeddings and are susceptible to performance degradation from noisy negative samples\nin the memory bank. This study addresses these challenges by proposing an enhanced contrastive learning framework that\nincorporates two key innovations. First, we introduce a dual-view loss function, which ensures balanced optimization of both\nquery and key embeddings, improving representation quality. Second, we develop a selective negative sampling strategy that\nemphasizes the most challenging negatives based on cosine similarity, mitigating the impact of noise and enhancing feature\ndiscrimination. Extensive experiments demonstrate that our framework achieves superior performance on downstream tasks,\ndelivering robust and well-structured representations. These results highlight the potential of optimized contrastive mechanisms\nto advance unsupervised learning and extend its applicability across domains such as computer vision and natural language\nprocessing.", "sections": [{"title": "I. INTRODUCTION", "content": "Unsupervised representation learning has become a cornerstone of modern machine learning, particularly in domains where\nlabeled data is scarce or expensive to obtain. At the forefront of this paradigm, contrastive learning has emerged as an effective\nframework for learning feature representations by distinguishing positive samples from a diverse set of negative samples.\nThis approach, exemplified by methods such as SimCLR [1] and Momentum Contrast (MoCo) [5], has shown remarkable\nsuccess in various domains, including image recognition and natural language processing. MoCo, in particular, introduces a\ndynamic memory bank mechanism that stores a large pool of precomputed negative samples, significantly improving scalability\nand efficiency compared to batch-based methods like SimCLR. By leveraging a momentum encoder to maintain consistency\nin feature embeddings, MoCo effectively balances computational efficiency with robust representation learning. Despite its\nsuccess, several challenges remain, including the presence of noisy or mislabeled negative samples in the memory bank and\nthe underutilization of key-view embeddings during optimization [6]. These limitations can hinder the ability of MoCo to learn\nrobust representations, particularly in datasets with high intra-class variability or class imbalance.\nkjThis paper addresses these challenges by introducing enhancements to the MoCo framework aimed at improving the\nrobustness and discriminative power of contrastive learning. Specifically, we extend the standard InfoNCE loss to balance\ncontributions from both the query and key views, ensuring that both embeddings are effectively optimized [6]. Additionally,\nwe propose a hard negative sampling strategy that filters out noisy or mislabeled negatives by prioritizing samples farthest\nfrom the query embedding in cosine similarity [3]. By combining these innovations, our method seeks to answer the following\nquestion: How can the MoCo framework be enhanced to address the limitations of noisy negative samples and underutilized key-\nview embeddings, thereby improving the robustness of contrastive learning? By mitigating the influence of noisy negatives and\nbalancing the optimization of query and key embeddings, the proposed approach enables more robust feature representations,\nparticularly in noisy or complex datasets.\nOur contributions can be summarized as follows:\n\u2022\nWe extend the InfoNCE loss to a dual-view formulation that leverages both query and key embeddings for contrastive\nlearning.\n\u2022\nWe propose a cosine similarity-based hard negative sampling strategy to mitigate the impact of mislabeled or redundant\nnegatives in the memory bank.\n\u2022 Through comprehensive experiments, we demonstrate that our method achieves superior performance compared to existing\napproaches, particularly in scenarios with noisy or challenging datasets."}, {"title": "II. METHOD", "content": "In MoCo, the memory bank plays a critical role in selecting effective negative samples. Unlike standard methods that\ncompute negative samples dynamically for each batch, the memory bank maintains precomputed feature representations\nfrom previous iterations. This enables the inclusion of a much larger pool of negative samples without incurring significant\ncomputational overhead, thereby improving the model's discriminative ability [5]. k The memory bank enables the model\nto focus on hard negatives\u2014samples that are difficult to distinguish from the positive sample. These challenging samples\nare essential for optimizing contrastive learning objectives, as they push the model to learn robust feature representations by\nemphasizing difficult distinctions [1].\n1) Momentum Encoder: MoCo employs a momentum encoder mechanism to stabilize the feature representations stored\nin the memory bank. The architecture consists of two encoders: the Query Encoder, which processes the current batch of\ndata to generate embeddings for query samples, and the Key Encoder, which produces embeddings for key samples used as\npositives or negatives. This dual-encoder design ensures consistency and stability in the feature representation by leveraging\nthe momentum mechanism for the Key Encoder which is updated using a momentum-based approach [5]:\n$\\theta_{\\kappa} \\leftarrow m\\theta_{\\kappa} + (1 \u2013 m)0_{9},$ (1)\nIn this equation, $\\theta_k$ represents the parameters of the Key Encoder, $0_q$ represents the parameters of the Query Encoder, and\nm is the momentum coefficient, typically set close to 1 (e.g., 0.99) to ensure slow u\nThis update mechanism ensures that the Key Encoder evolves gradually, resulting in more stable and consistent embeddings\nfor negative samples stored in the memory bank.\n2) Updating the Memory Bank: The memory bank is updated dynamically as new batches of data are processed. Each\nbatch's feature embeddings are added to the memory bank, replacing older embeddings in a circular queue structure. This\nensures that the memory bank reflects the most recent feature space while maintaining diversity across samples.\nThe adaptive nature of the memory bank allows it to account for changes in the data distribution as training progresses.\nThis adaptability is especially critical for contrastive learning tasks, where the quality of negative samples plays a significant\nrole in determining model performance [1]."}, {"title": "1) Enhancing Negative Sampling for the Key View:", "content": "The original MoCo framework optimizes the model using the InfoNCE\nloss, introduced by van den Oord et al. [6]. The loss function is defined as:\n$L_{q} = - log \\frac{exp(qk+/T)}{\\sum_{i=0}^{K} exp(qki/T)} $ (2)\nIn this context, let q represent the query feature vector, k+ denote the positive sample feature vector, and ki represent the\nnegative sample feature vectors. The parameter 7 is the temperature parameter, which controls the sharpness of the softmax\ndistribution.\nThis loss function maximizes the similarity between q and k+ while minimizing the similarity between q and ki. However,\nthis method only uses negative samples for the query view, potentially limiting the model's performance.\nTo address this, we propose an enhanced loss function that balances contributions from both the query and key views:\n$L_{q} = -(1-m) log \\frac{exp(qk+/T)}{\\sum_{i=0}^{K} exp(qki/T)} - m log \\frac{exp(kq+/T)}{\\sum_{i=0}^{K} exp(kki/T)} $ (3)\nWhere in (3), m is a hyperparameter that adjusts the importance of the two loss components. Empirical studies suggest that\nmm performs best within the range [0.1, 0.01]. Additionally, k represents the feature vector of the key view, while q+ denotes\nthe positive sample feature vector for the key view.\nThis extension encourages both q and k to focus on maximizing similarity with their respective positive samples while\ndiscriminating effectively against negatives.\n2) Hard negative Sampling Filtering for Key View: As shown in [8], the encoder performs better when sampled with \"true\nnegatives.\" However, in unsupervised learning, selecting \"true negatives\" is not feasible due to the absence of ground-truth\nlabels. To address this challenge, Robinson et al. [8] proposes an alternative: the selection of \"useful negatives\" through hard\nnegative sampling. This approach involves choosing the negative samples that are closest to the anchor at each iteration, making\nthem the most difficult or \"hard\" examples.\nBuilding upon this idea, we move beyond the loss function described previously (see above) and propose a refined version.\nThe motivation behind this is to improve the quality of negative samples by focusing on those that provide the most informative\ncontrast to the positive examples. Instead of using the entire memory bank, which may contain irrelevant or mislabeled negatives,\nwe perform hard negative filtering.\nIn this approach, we select a subset of features that are most distant from the query q, based on cosine similarity. By doing\nso, we ensure that the selected negative samples are more representative and less likely to overlap with the positive class,\nleading to a more effective learning process. The revised loss function is as follows:\n$L_{q} = -(1 \u2013 m) log \\frac{exp(qk+/\u0442)}{\\sum_{i=0}^{K} exp(qki/T)} - m log \\frac{exp(kq+/T)}{\\sum_{i=0}^{F_N} exp(kki/T)}$\nIn (4), FN represents a set of features that are farthest from q in the memory bank, determined using cosine similarity.\nCosine similarity is defined as:\n$\\operatorname{cosine}\\_similarity = \\cos(\\theta) = \\frac{k_iq}{||k_i||||q||} $ (5)\nwhere ki and q are feature vectors of a memory sample and the query, respectively.\nBy focusing on these filtered negative samples, the proposed method minimizes sensitivity to mislabeled negatives and\nimproves the overall performance of contrastive learning [5, 3, 6]."}, {"title": "III. EXPERIMENT", "content": "In this section, we outline the experimental setup to evaluate the proposed model. First, we detail the architecture, training\nconfigurations, and preprocessing techniques. Then, we compare our approach with state-of-the-art self-supervised models,\nevaluating their performance across two benchmark datasets, including CIFAR-10 and CIFAR-100, Additionally, we assess\nthe transfer learning capabilities of our model on a wide range of datasets, comparing its performance against other transfer\nlearning methods to highlight its adaptability and effectiveness in diverse scenarios."}, {"title": "A. Dataset", "content": "We evaluate our model on a diverse set of datasets that span various domains, including object classification, fine-grained\nrecognition, and scene understanding. The datasets are as follows:\n\u2022\nCIFAR-10: This dataset contains 60,000 images categorized into 10 classes (e.g., airplane, automobile, bird, cat, etc.),\nwith 6,000 images per class. The images are of size 32 \u00d7 32 pixels and are often used as a standard benchmark for\ngeneral-purpose image classification.\n\u2022\nCIFAR-100: Similar to CIFAR-10, CIFAR-100 contains 60,000 images, but with 100 classes, each having 600 images.\nThe images are also of size 32 \u00d7 32 pixels, and the dataset is designed to be more challenging due to the larger number\nof fine-grained categories.\nThese datasets provide a comprehensive set of challenges, from simple object classification to more complex fine-grained\nrecognition and scene understanding tasks. They are widely used to benchmark the performance of machine learning models\nin various domains."}, {"title": "B. Implementation Details", "content": "1) Backbone: We use ResNet-18 as the backbone for all experiments. This architecture is widely adopted in self-supervised\nlearning due to its strong feature extraction capabilities and adaptability across various tasks. Following previous works such\nas [5], ensuring compatibility with standard self-supervised learning pipelines.\n2) Augmentation: To improve the robustness and generalization of the learned representations, we adopt the augmentation\npipeline from MoCo v2 [3], which applies a set of strong augmentations designed for contrastive learning. These augmentations\nare as follows:\nTo enhance the model's robustness, several data augmentation techniques are applied to the images. Random Cropping and\nResizing is used to randomly crop each image to a smaller region and then resize it back to the target resolution. This forces\nthe model to learn features from various parts of the image, promoting spatial invariance. Additionally, Color Jittering is\napplied, adjusting the brightness, contrast, saturation (within a range of \u00b10.4), and hue (\u00b10.1), simulating diverse lighting and\nenvironmental conditions. Gaussian Blur is applied to 50% of the images, replicating out-of-focus or noisy scenarios, which\nhelps the model remain resilient to distortions. Around 20% of the images are randomly converted to grayscale, removing\ncolor cues and encouraging the model to focus more on textures and shapes. Multi-View Augmentation is also implemented,\nwhere each image undergoes two independent augmentations, generating two distinct views treated as positive pairs in the\ncontrastive loss. This strategy strengthens the model's ability to learn invariance across heavily transformed inputs. Finally,\nRandom Horizontal Flipping is applied with a 50% probability, adding mirrored versions of objects to the dataset, further\nincreasing variability.\nAfter augmentation, all images are normalized using their dataset-specific statistics. These augmentations form the \"stronger\naugmentation\" pipeline of MoCo v2, proven to enhance contrastive learning tasks significantly."}, {"title": "C. KNN Evaluations", "content": "We evaluate the quality of the embeddings produced by our self-supervised model using a K-Nearest Neighbors (KNN)\nclassifier. This evaluation is conducted on the test sets of the datasets discussed above, providing a non-parametric measure of\nthe discriminative power of the learned representations in an unsupervised setting.\nThe evaluation employs the following metrics:\n\u2022 Top-1 accuracy: Measures the percentage of predictions where the correct class is ranked first among the nearest.\nThe number of neighbors, K, is set to 200, while the softmax temperature (T) is configured to 0.1. These settings are\nderived from empirical observations and align with prior work on instance-level discrimination, ensuring a balance between\ncomputational efficiency and evaluation accuracy (knn).\nThis evaluation provides an intuitive understanding of the discriminative power of the embeddings by emphasizing their\nability to group visually and semantically similar data points. By employing consistent metrics and hyperparameters, this\napproach enables robust and interpretable comparisons across datasets."}, {"title": "D. Finetune Evaluations", "content": "The fine-tuning configuration is established through empirical validation. We initialize the model with pretrained weights\nfrom ImageNet to leverage transferable features and accelerate convergence. The SGD optimizer is utilized with a learning rate\nof 0.01, momentum of 0.9, and a weight decay of 5 \u00d7 10-4, ensuring a balance between performance and stability. Training is\nconducted over 200 epochs with a batch size of 256, selected to efficiently manage memory usage. The learning rate remains\nfixed throughout the training process. All experiments are performed using the PyTorch framework [7], and the results are\ncompared against state-of-the-art self-supervised learning methods for a thorough evaluation."}, {"title": "E. Result", "content": "Our experiments show that, beyond the influence of random seed settings, symmetry adjustments significantly improve model\naccuracy, consistent with findings by Chen and He [2]. By incorporating symmetry tuning, the model achieved an accuracy\nincrease significantly over the baseline, with reduced variance across multiple trials, highlighting its effectiveness in enhancing\nboth performance and stability as the figure 2"}, {"title": "1) Comparison with State-of-the-art:", "content": "Based on the findings in He et al. [5], the memory bank is designed to store representations from previous mini-batches,\nmaking it independent of the current batch size. However, increasing the batch size in practice may necessitate a proportional\nadjustment to the memory bank size to ensure an adequate number of negative samples for effective contrastive learning.\nBuilding on this insight, we systematically modified and fine-tuned the memory bank parameters in our experiments to optimize\nits configuration and achieve the best performance outcomes for our specific setup."}, {"title": "IV. CONCLUSION", "content": "In conclusion, our paper improves upon the method of using negative sampling for view keys by leveraging the base model\nof MoCo. We apply negative sampling to enhance accuracy while reducing the overall computational cost of the model. The\nresults demonstrate that our approach outperforms most state-of-the-art models even with lower configuration settings. The\nkey advantage of our method lies in its solid theoretical foundation, built upon previous research. Moreover, the model's low\nconfiguration makes it deployable on less powerful GPUs, and the implementation is simplified, making it more accessible for\nreal-world applications."}]}