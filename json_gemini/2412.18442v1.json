{"title": "SoK: On the Offensive Potential of AI", "authors": ["Saskia Laura Schr\u00f6er", "Giovanni Apruzzese", "Soheil Human", "Pavel Laskov", "Hyrum S. Anderson", "Edward W. N. Bernroider", "Aurore Fass", "Ben Nassi", "Vera Rimmer", "Fabio Roli", "Samer Salam", "Ashley Shen", "Ali Sunyaev", "Tim Wadwha-Brown", "Isabel Wagner", "Gang Wang"], "abstract": "Our society increasingly benefits from Artificial Intelligence (AI). Unfortunately, more and more evidence shows that AI is also used for offensive purposes. Prior works have revealed various examples of use cases in which the deployment of AI can lead to violation of security and privacy objectives. No extant work, however, has been able to draw a holistic picture of the offensive potential of AI. In this SoK paper we seek to lay the ground for a systematic analysis of the heterogeneous capabilities of offensive AI. In particular we (i) account for AI risks to both humans and systems while (ii) consolidating and distilling knowledge from academic literature, expert opinions, industrial venues, as well as laymen\u2014all of which being valuable sources of information on offensive AI.\nTo enable alignment of such diverse sources of knowledge, we devise a common set of criteria reflecting essential technological factors related to offensive AI. With the help of such criteria, we systematically analyze: 95 research papers; 38 InfoSec briefings (from, e.g., BlackHat); the responses of a user study (N=549) entailing individuals with diverse backgrounds and expertise; and the opinion of 12 experts. Our contributions not only reveal concerning ways (some of which overlooked by prior work) in which AI can be offensively used today, but also represent a foothold to address this threat in the years to come.", "sections": [{"title": "I. INTRODUCTION", "content": "Artificial Intelligence (AI) is an exemplary use-case of a disruptive technology [1, 2]. AI has revolutionized the IT ecosystem worldwide, providing cost-effective solutions for new and existing tasks-potentially exceeding the proficiency of humans [3-5]. Unfortunately, the disruptive nature of AI also has gradually materialized in a more literal sense\u2014as a means to realize, facilitate and enhance cyberattacks. Such an observation underscores that the potential of AI must be proactively scrutinized from a cybersecurity perspective.\nThe domains of AI and cybersecurity are, in fact, strongly intertwined. Abundant works highlight the potential of \"AI for cybersecurity\" [6], e.g., showing that AI can improve cybersecurity routines [7, 8]; or that AI can perform tasks otherwise unfeasible for security operators [9]. At the same time, a large body of literature focuses on \"security of AI\" [10, 11], e.g., elucidating that AI methods can be broken with tiny perturbations [12]; or that some confidential information pertaining to Al solutions (i.e., training data, or the AI model itself) can be leaked [13] or stolen [14]. There is another use case, however, that links AI and cybersecurity, but has not received the same degree of attention so far: \"offensive AI.\"\nSome prior works have considered scenarios wherein AI is used as an offensive tool. For instance, using Large Language Models (LLM) to write phishing emails [15] is cheap and effective [16], and evidence shows that this is already happening [17, 18]. However, no prior work has systematically analyzed the topic of offensive AI, examining a broad range of attack targets and accounting for diverse sources of knowledge. Indeed, prior systematizations,e.g., [19, 20], mostly accounted for the viewpoint of academic literature,which is a profound but not the only source of information. Briefings of industrial conferences, opinions of experts, and even laymen provide complementary perspectives on the ins-and-outs of offensive AI. Furthermore, the offensive potential of AI poses a threat not only to IT systems in a narrow sense, as primarily considered in prior work, but also to any stakeholder relying on them, e.g., humans, or even the society as a whole. Hence, to tackle today's unforseen risks of offensive AI, a broader scope must be considered for systematization of knowledge, as suggested schematically in Fig. 1. New knowledge sources and versatile use-cases should be taken into account for a comprehensive analysis of this inescapable threat."}, {"title": "II. RESEARCH METHODS AND CHECKLIST", "content": "We introduce the research methods applied in our SoK: the systematization of scientific literature (\u00a7II-A) and of InfoSec briefings (\u00a7II-B), the user study with non-experts (\u00a7II-C), and the elicitation and systematization of expert knowledge (\u00a7II-D). We also present our OAI Assessment Checklist, which provides the means for alignment and systematization of diverse classes of prior work considered in our SoK (\u00a7II-E). Some details of our methods are in the Appendix, including a timeline (in Fig. 17) encompassing all our research activities.\nA. Systematic Literature Review (Methodology)\nPrior surveys on OAI [20, 29, 30] are grounded in academic literature. Hence, to ensure continuity, we consider research papers as our first source of knowledge. We perform a systematic literature review, following established guidelines [31], illustrated in Fig. 2. We describe the pivotal points below.\nSearch Queries. We began our literature review by asking ourselves \"how has prior work envisioned offensive AI?\" To systematically encompass a broad spectrum of prior art, we search for related papers indexed by four popular databases (until Nov. 2023): IEEE Xplore, Google Scholar, ACM DL, and arXiv (similarly to Ladisa et al. [32]). We carried out our search by formulating queries corresponding to two macro- search queries. Specifically, the first, straightforward, macro search-query entails \"offensive AI.\" However, we are confident that there are other ways in which prior work has conceived AI-based applications that can fall within our definition of OAI. Hence, as an exemplary use-case to extend our search, we consider another macro-query revolving around \u201cAI in offensive security\": indeed, the idea of using AI in penetration testing (e.g., [33, 34]) can also be leveraged by real attackers"}, {"title": "III. OVERVIEW OF ACADEMIC LITERATURE ON OAI", "content": "We present the first part of our primary contribution (C1) by focusing the attention on academic publications. Our literature search yielded 95 papers on OAI (see \u00a7II-A).4 In what follows, we first discuss the 79 technical papers (\u00a7III-A) and then the 16 non-technical papers (\u00a7III-B), which we analyze under the lens of our checklist (refer to \u00a7II-E).\nA. Technical Papers\nWe present in Table I (in the Appendix) the systematization of the 79 technical papers, in accordance with our checklist (\u00a7III-A1 to \u00a7III-A3). Furthemore, in \u00a7III-A4 we analyze the technical requirements pertaining to this class of works.\n1) OAI Use Case: We begin by considering the OAI use case envisioned by each work, aligning it to MITRE ATT&CK.\nWe mapped 48 papers (61%) to the use-cases covered by MITRE [63]. Among these, 2 papers were mapped to the ICS matrix (one paper focusing on Evasion and another on Process Control), whereas 1 paper was mapped to the Mobile matrix (addressing Credential Access). The remaining 45 papers aligned with the Enterprise matrix. In general, among these 48 papers, most works focus on Initial Access (22%). Other common goals of OAI are Defense Evasion (9%), Credential Access (9%), and Discovery (6%). Only 4% of the papers focus on exploiting OAI for Reconnaissance: this is likely due to the fact that this step can be carried out via various well- known means (e.g., port scanning, or OSINT) which do not require OAI and which are not easily detected [141].\nWe could not find any paper that specifically proposed OAI techniques for Impact or Lateral Movement (some, however, do use autonomous attack agents to carry out also these operations; e.g., [37]).\nThe remaining (39%) papers envisioned use-cases not covered by MITRE ATT&CK. These papers mainly address attacks on society, privacy, or focus on autonomous attack agents (which involves automating various MITRE tactics, as done in [35]). Attacks on society cover, e.g., polarizing summaries [115] or crowdturfing attacks in online review systems [131], while privacy attacks include attribute inference attacks [89], or profile matching across multiple social networks [109, 130]. We report in Fig. 4 the groups (and corresponding relationships) of the OAI use cases not covered by MITRE.\nWith respect to the purpose, out of 79 papers, 43 (54%) propose novel attacks (\u2020); 30 (38%) focus on offensive security (0), and 6 (8%) use AI as a hacking assistant (*). Some works focusing on novel attacks (\u2020) consider potential defenses against the proposed attacks: e.g., [75] considers Web Application Firewalls to protect the targeted application. Only half"}, {"title": "IV. OAI IN INFOSEC BRIEFINGS", "content": "We identified 38 non-academic works (also known as \"briefings\") related to OAI at BlackHat and DefCon. We first analyse these 38 briefings (\u00a7IV-A), and then compare them with the 95 papers from academic literature (\u00a7IV-B).\nA. Analysis\nTo finalize our primary contribution (C1), we assess our 38 briefings through our checklist (\u00a7II-E), and show the results in Table III (in the Appendix), which also reports briefings related to a scientific paper (which occurs for 9 out of 38 briefings\u2014 two of which [120, 127] are also included in Table II). The first briefing on OAI we found (discussing how to \"hack human desire\") dates back to 2008 [50]; many briefings on OAI appeared in 2023\u2014likely due to the rollout of ChatGPT.\n1) OAI Use Case: Most briefings (25, 69%) can be mapped to MITRE, for which \u201cInitial Access\u201d (11, such as using LLM to write phishing emails [164]) and \"Reconnaissance\u201d (8, e.g., via side-channel [167, 180, 187]) are the most prominent use cases. The remaining 13 briefings are not covered by MITRE. These briefings focus mostly on attacks against society (9, such as using AI for virtual kidnapping [168]) and privacy (3, such as deanonymising developers based on their code [184]). Overall, only one briefing [166] used AI as a hacking as- sistant (*). In contrast, 6 briefings envisioned an offensive"}, {"title": "V. USER SURVEY: WHAT DO LAYMEN THINK OF OAI?", "content": "We now present the results of our user survey with non- experts, revealing what laymen think about the offensive potential of AI. We first describe the demographics of our participants (\u00a7V-A), and then present the results (\u00a7V-B)\nA. Demographics\nWe received 570 responses but removed 21 because they were from underage participants or clearly not informative. Hence, our results reflect the opinion of 549 individuals.\nOverall, 63% of our respondents identify themselves as \"male,\" 36% as \"female\" and less than 1% as \"other.\" Our respondents are from 42 countries, mostly from Europe (70%), North America (21%), and Asia (7%). Over 90% are between 18-54 years old, and 2% are over 65. With respect to educational qualifications, 80% of participants hold at least a Bachelor's degree. Employment status among the respondents varies, with 83% employed (full or part-time), 15% students, and 2% retired or unemployed. 75% of the participants are engaged in IT-related work. In terms of knowledge of cybersecurity (or AI), 23% (17%) consider themselves as beginners, 43% (53%) as intermediate, and 34% (30%) as advanced or experts. Additional demographic details are provided in Appendix B.\nCompared to the OECD population, our sample has fewer women and individuals over 55. However, we appreciate that our sample consists of highly educated individuals, of which only a minority consider themselves as experts in Al or security. Therefore, even though we cannot claim represen- tativeness of the world's population, our sample is likely to provide valuable insights for the goal of our SoK. To our knowledge, this is the first survey of this kind on OAI, hence its findings are useful (also) for future studies.\nB. Results\nWe systematically analyze our results quantitatively (\u00a7V-B2) and qualitatively (\u00a7V-B1) before drawing our conclusions.\n1) Quantitative Analysis: We report the results of our binary questions in Figure 5. Let us analyze it at a high-level.\n\u201cHave you thought about OAI?\" The majority (525, 96%) have already considered that AI could be used for ma- licious purposes. However, the remaining 24 (4%) have never considered AI's offensive potential. Among these, 14 people work in an IT-related field, and 13 of these have at least intermediate knowledge in cybersecurity or AI.", "concerned).": "o you think that AI will harm you?\" A slight ma- jority (52%) believe that AI may personally harm them. Intriguingly, nearly 40% of the 442 participants who are"}, {"title": "VI. EXPERT OPINION: WHAT IS THE FUTURE OF OAI?", "content": "In this section, we report the systematization of the expert opinions (refer to \u00a7II-D). We begin by presenting their re- sponses to the survey (\u00a7VI-A); then, we summarize their input after they reviewed this paper (\u00a7VI-B); finally, we compare their opinions before and after having read our paper (\u00a7VI-C).\nA. Expert Survey (opinions before reviewing this paper)\nRecall that we first inquired the 12 experts to participate in a (slightly modified) version of our survey. Hence, their responses reveal their unbiased opinion on OAI. In terms of demographics, 5 (42%) identify as an expert in AI, 4 (33%) as advanced, and 3 (25%) as intermediate; whereas 9 (75%) deem to be an expert in cybersecurity, and 3 (25%) identify as having advanced knowledge in cybersecurity. This is in line with our expected target of \u201cexperts\u201d in these fields.\n1) Quantitative Analysis: We first report the results to the binary questions (as we did in \u00a7V-B1). All 12 experts have already considered that AI could be used for malicious pur- poses, and also all experts are concerned about the offensive potential of AI. For the last question, 9 (75%) experts think that AI could harm them personally, while 3 (25%) think otherwise.\n2) Qualitative Analysis: Next, we qualitatively analyze the answers to the open questions\u2014starting from the one that was also included in the survey with the general population.", "cyberattacks": 4, "speed, automation and ease of use\\\" (4). Some experts are con- cerned about \"privacy attacks": 2, "deepfakes\\\" (1), and the \"spread of misinformation\\\" (1).\n\"Can you think of (or do you know of) some ways in which Al can be used offensively?\\\" [expert-only] We mapped the answers to the OAI use cases of our checklist (\u00a7II-E). Most experts (6) highlighted multiple use cases. For MITRE ATT&CK: 7 experts mentioned \u201cinitial access,": "resource\"\n    },\n    {\n      \"title\": \"VII. DISCUSSION\",\n      \"content\": \"We now reflect on the findings (\u00a7VII-A) and limita- tions (\u00a7VII-B) of our paper and compare it with related meta-research (\u00a7VII-C). Our intention is to demonstrate the importance of analysing various sources of knowledge.\nA. Findings\nWe summarize our findings by making explicit reference to the three-fold contributions of our paper (C1, C2, C3).\nOur SoK provides a snapshot of the OAI landscape (C1). Such a snapshot, however, has been made possible only thanks to the collective \u201ccontributions", "limited practicality\\\" exhibited even by technical papers (which mostly attack \u201ctoy": "ystems) may suggest that OAI does not represent a tangible threat- but, perhaps unfortunately, the analysis of the InfoSec briefings (\u00a7IV) revealed that OAI can be practically exploited in the real world. In contrast, no InfoSec briefing considered OAI in warfare, but we found many papers covering such use cases- which also seem to worry non-experts (\u00a7V), despite not having been mentioned by any of our experts (\u00a7VI). Finally, despite our extensive analyses, we acknowledge that our review of prior work may have missed some OAI use cases: one such example are website fingerprinting [210] attacks, which we overlooked in Table I, but which were mentioned by one expert in their statements. Altogether, these findings show that there is a need of a perpetual and collective effort to monitor the threat of OAI, since we expect more OAI use cases (existing or new) will be identified in the future (see, e.g., [211]).\nOur SoK provides a foundation for a long-term classification of OAI works (C2). It is obvious that the field of potential offensive use-cases for AI is vast. Our simple checklist (\u00a7II-E) encapsulates clear criteria that can be used to systematically"}, {"title": "VIII. CONCLUSIONS", "content": "We consider this paper as a first step in laying down a scientific groundwork for investigating various facets of offensive AI. In short, we found that the offensive capabilities of AI are very heterogeneous and can adversely affect systems, humans and the society as a whole. Due to this heterogeneity, offensive AI use cases cannot be classified into a single framework, such as MITRE ATT&CK, but require a broader systematization which we provide with the help of our OAI assessment checklist (\u00a7II-E) and in our online tool [21].\nWe hope that the insights obtained in this SoK paper enable security and privacy researchers to better appreciate the soci- etal impact of problems related to offensive AI. Our findings also underscore the necessity of interdisciplinary collaboration with the areas of cognitive science, psychology, economics, political science, law, ethics, and perhaps many other, to fully comprehend and mitigate the offensive potential of AI."}]}