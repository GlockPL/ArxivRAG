{"title": "The Silent Majority: Demystifying Memorization Effect\nin the Presence of Spurious Correlations", "authors": ["Chenyu You", "Haocheng Dai", "Yifei Min", "Jasjeet S. Sekhon", "Sarang Joshi", "James S. Duncan"], "abstract": "Machine learning models often rely on simple spurious features \u2013 patterns in training data that\ncorrelate with targets but are not causally related to them, like image backgrounds in foreground\nclassification. This reliance typically leads to imbalanced test performance across minority and\nmajority groups. In this work, we take a closer look at the fundamental cause of such imbalanced\nperformance through the lens of memorization, which refers to the ability to predict accurately\non atypical examples (minority groups) in the training set but failing in achieving the same\naccuracy in the testing set. This paper systematically shows the ubiquitous existence of spurious\nfeatures in a small set of neurons within the network, providing the first-ever evidence that\nmemorization may contribute to imbalanced group performance. Through three experimental\nsources of converging empirical evidence, we find the property of a small subset of neurons or\nchannels in memorizing minority group information. Inspired by these findings, we articulate the\nhypothesis: the imbalanced group performance is a byproduct of \"noisy\" spurious memorization\nconfined to a small set of neurons. To further substantiate this hypothesis, we show that eliminating\nthese unnecessary spurious memorization patterns via a novel framework during training can\nsignificantly affect the model performance on minority groups. Our experimental results across\nvarious architectures and benchmarks offer new insights on how neural networks encode core and\nspurious knowledge, laying the groundwork for future research in demystifying robustness to\nspurious correlation.", "sections": [{"title": "1 Introduction", "content": "Machine learning models often achieve high overall performance yet struggle in minority groups\ndue to spurious correlations \u2013 patterns that align with the class label in training data but have\nno causal relationship with the target Sagawa et al. [2020a], Geirhos et al. [2020]. For example,\nconsidering the task of distinguishing cows from camels in natural images, it is common to find\n95% of cow images with grass backgrounds and 95% of camel images on sand. Models trained using\nstandard Empirical Risk Minimization (ERM) often focus on minimizing average training error by\ndepending on the spurious background attributes (\u201cgrass\u201d or \u201csand\u201d) instead of the core features\n(\"cow\" or \"camel\u201d). In such settings, models may yield good average accuracy but lead to high error\nrates minority groups (\u201ccows on sand\u201d or \u201ccamel on grass", "issue": "even well-trained models can develop systematic biases from\nthese spurious attributes in their data, thus leading to alarmingly consistent performance drop for\nminority groups where the spurious correlation does not hold. Indeed, in Figure 1, we show the\ntraining and test accuracy on majority and minority groups of the Waterbirds benchmark for two\npopular models: ResNet-50 [He et al., 2016] and ViT-small [Dosovitskiy et al., 2021]. We observe\nthat majority groups have a smaller gap between the training and testing accuracy, as compared to\nminority groups which have a more significant gap. Thus, understanding the underlying causes of\nsuch imbalanced performance between majority and minority groups is crucial for their reliable\nand safe deployment in various real-world scenarios Blodgett et al. [2016], Buolamwini and Gebru\n[2018], Hashimoto et al. [2018].\nThe minority groups are atypical examples to\nneural networks (NNs), as these small subsets of\nexamples bear a resemblance to majority groups\ndue to the same spurious attribute but have dis-\ntinct labels. Recent efforts have shown that NNS\noften 'memorize' atypical examples, primarily\nin the final few layers of the model Baldock et al.\n[2021], Stephenson et al. [2021], and possibly\neven in specific locations of the model Maini\net al. [2023]. Memorization, in this context, is\ndefined as the neural network's ability to accu-\nrately predict outcomes for atypical examples\n(e.g., mislabeled examples) in the training set\nthrough ERM training. This is in striking anal-\nogy to the spurious correlation issue, because 1)\nthe minority examples are atypical examples by\ndefinition, and 2) the minority examples are often more accurately predicted during training but\npoorly predicted during testing (as demonstrated in Figure 1). Therefore, a natural open question\narises: Does memorization play a role in spurious correlations?\nIn this work, we present the first study to systematically understand the interplay of memorization\nand spurious correlations in deep overparametrized networks. We undertake our exploration\nthrough the following avenues: 1) What makes the comprehensive condition for the existence or\nnon-existence of spurious correlations within NNs? 2) How do NNs handle atypical examples, often\nseen in minority groups, as opposed to typical examples from majority groups? and 3) Can NNs\ndifferentiate between these atypical and typical examples in their learning dynamics?\nTo achieve these goals, we show the existence of a phenomenon named spurious memorization. We\ndefine 'spurious memorization' as the ability of NNs to accurately predict outcomes for atypical\n(i.e., minority) examples during training by deliberately memorizing them in certain part of the"}, {"title": "3 Results", "content": "model. Indeed, we first identify that a small set of neurons is critical for memorizing minority\nexamples. These critical neurons significantly affect the model performance on minority examples\nduring the training, but only have minimal influence on majority examples. Furthermore, we show\nthat these critical neurons only account for a very small portion of the model parameters. Such a\nmemorization by a small portion of neurons causes the model performance on minority examples\nto be non-robust, which leads to the poor testing accuracy on minority examples despite the high\ntraining accuracy. Overall, our study offers a potential explanation for the differing performance\npatterns of NNs when handling majority and minority examples.\nOur systematic study is performed in two stages.\nIn Stage I, to verify the existence of critical neu-\nrons, we identify two experimental sources to\ntrace spurious memorization at the neuron and\nlayer level. These two sources are unstructured\ntracing (assessing the role of neurons within the\nentire model for spurious memorization using\nheuristics including weight magnitude and gra-\ndient) and structured tracing (assessing the role\nof neurons within each individual layer with\nsimilar heuristics). Specifically, by evaluating\nthe impact of spurious memorization via un-\nstructured and structured tracing at the magni-\ntude and gradient level (Section 3.1), we observe\na substantial decrease in minority group accu-\nracy, contrasting with a minimal effect on the\nmajority group accuracy. This suggests that at\nunstructured and structured level, the learning\nof minority group opposes the learning of majority group, and indicates that 1) critical neurons\nfor spurious memorization indeed exist within NNs; 2) both gradient and magnitude criteria are\neffective tools for identifying these critical neurons; and 3) NNs tend to memorize typical examples\nfrom majority groups on a global scale, whereas the memorization of minority examples is localized\nto a miniature set of nodes (i.e. critical neurons). Overall, we provide converging empirical evidence\nto confirm the existence of critical neurons for spurious memorization.\nIn Stage II, inspired by the observations above, we develop a framework to investigate and un-\nderstand the essential role of critical neurons in spurious memorization that would incur the\nimbalanced group performance of NNs. Specifically, we construct an auxiliary model which is an\nadaptively pruned version of the target model, and then contrast the features of this auxiliary model\nwith those of the target model. Our motivation comes from recent empirical finding Hooker et al.\n[2019] that pruning can improve a network's robustness to accurately predict rare and atypical\nexamples (minority groups in our case). This allows the target model to identify and adapt to\nvarious spurious memorization at different stages of training, thereby progressively learning more\nbalanced representations across different groups. Through extensive experiments with our training\nalgorithm across a diverse range of architecture, model sizes, and benchmarks, we confirm that the\ncritical neurons have emergent spurious memorization properties, thereby more friendly to pruning.\nMore importantly, we show that majority examples, being memorized by the entire network, often\nyield robust test performance, whereas minority examples, memorized by a limited set of critical\nneurons, show poor test performance due to the miniature subset of neurons. This provides a\nconvincing explanation for the imbalanced group performance observed in the presence of spurious\ncorrelations.\nConcretely, we summarize our contributions as follows: (1) To the best of our knowledge, we present"}, {"title": "3.1 Identifying the Existence of Critical Neurons", "content": "In this section, we validate the existence of critical neurons in the presence of spurious correlations.\nWe comprehensively examine the underlying behavior of 'critical neurons' on the Waterbirds dataset\nwith the ResNet-50 backbone. Within this section, the term 'neurons' specifically refers to channels\nin a convolutional kernel. It is worth noting that the Waterbirds dataset comprises two majority\ngroups and two minority groups. For clarity in our discussions and figures, we use the following\nnotations, aligned with the dataset's default setting: The majority groups are Go (Landbird on Land)\nand G3 (Waterbird on Water), while the minority groups are G\u2081 (Landbird on Water), G2 (Waterbird\non Land).\nNotations. In the following discussion, we consider the model as $f (\\theta,\\cdot)$, with e representing the\ncollection of all neurons. Individual neurons are denoted as $z_i$, for $i \\in [M] := {1,\u2026, M}$, and e can\nbe expressed as $\\theta = {z_1, z_2,\u2026, z_M}$. For the training data, we use Do, D1, D2, D3 to represent the\ndatasets, where $D_j$ comprises examples from group Gj, for each $j \\in {0, 1, 2, 3}$, respectively. Finally,\nlet LCE signify the cross-entropy loss. We emphasize that all the group accuracy evaluated before\nand after pruning in this section is evaluated on the training set, which strictly complies with the\ndefinition of memorization from Section 1."}, {"title": "3.1.1 Unstructured Tracing", "content": "To begin with, we adopt unstructured tracing to assess the effect of neurons on spurious memoriza-\ntion across the entire model, using weight magnitude and gradient as criteria.\nFor the gradient-based criterion, we begin with a model trained by ERM. We then select the neurons\nwith the largest gradient, measured in the $l_2$ norm, across the entire model. Zeroing out these\nneurons, we can then observe the resultant impact on group accuracy. To be specific, we compute\nthe loss gradient for each of the 4 Waterbirds groups. The loss gradient $v(\\cdot)$ on group j w.r.t. neuron\ni is defined as\n$v(i, j) = \\frac{\\partial L_{CE}(f(\\theta,D_j))}{\\partial z_i}, i \\in {1,..., M};j \\in {0, \u2026\u2026\u2026, 3}."}, {"title": "Observation and Analysis", "content": "In our study, we plot the change in accuracy, $\u2206acc(j)$, for each group\nj as shown in Figure 2. For every group, we consider three scenarios: pruning the top-1, top-2,\nand top-3 neurons, which corresponds to the 3 bars for each group in Figure 2 Note that we limit\nour reporting to the results involving up to 3 critical neurons based on experimental findings\nindicating that pruning the top-3 neurons is adequate. This decision is supported by the number\nof pruned neurons, detailed in Supplementary materials Table 10 (in Supplementary materials).\nIt can be clearly observed that the accuracy of minority groups exhibits significant shifts, while\nthe accuracy of majority groups shows only minimal impact. Specifically, for the majority groups\nGo and G3, the maximum of the group accuracy shifts stands at 2.15% when we zero out the top 3\nneurons with the largest gradient. While for minority group G\u2081 and G2, the maximum of the group\naccuracy shifts stands at 11.96% when we zero out the top 2 neurons with the largest gradient.\nThis is a sharp contrast between the groups, where accuracy shifts significantly, underscoring the\ncritical role of selected neurons in memorizing minority examples at both gradient and magnitude"}, {"title": "Both the gradient-based and magnitude-based criteria work", "content": "Interestingly, we observe that both\nthe gradient-based and magnitude-based criteria can yield similar effects. We show in the following\nthat it is attributed to an overlap in the distribution of critical neurons identified by each criterion.\nTo delve deeper, in Figure 3, we analyze the relative magnitude ranking among all neurons for the\nneurons with the largest gradient, and the relative gradient ranking for neurons with the largest\nmagnitude. In the left of Figure 3, we show the magnitude ranking for the neurons with top 0.01%\nlargest gradient, and Figure 3 right subfigure demonstrates the gradient ranking for the top 0.01%\nlargest magnitude neurons. In both histograms, there is a noticeable clustering in the rightmost\ntwo bins (ranging from 95% to 100%). This suggests that the neurons with the highest magnitudes\ntend to exhibit large gradients, and the neuron with the largest gradient often coincides with a high\nweight magnitude. This finding provides tantalizing evidence of the similar distribution of critical\nneurons under both criteria and explains the matching phenomenon observed between the two\ncriteria."}, {"title": "Random Noise and Random Initialization", "content": "Our experiments thus far offer preliminary evidence\nfor the existence of critical neurons. To gain a more comprehensive understanding, we explore\nalternatives to pruning, especially studying the effects of random initialization and random noise.\nThese two experiments are motivated by our desire to investigate the effects of perturbation from\ntwo perspectives: perturbation on the original neuron weights and perturbation on the pruned\nneurons. By examining these perturbations, we draw more credible supporting evidence on the\nexistence of critical neurons by evaluating the sensitivity of group accuracy to specific neurons more\ncomprehensively."}, {"title": "\u25b7 How to implement random initialization?", "content": "Instead of performing pruning on the selected\nneurons, we opt to initialize them randomly\nusing a zero-mean Gaussian random variable.\nThat is, we replace the neuron weight $z_i$ with\n$Z_i = \\epsilon_i$ where $\\epsilon_i \\sim N(0,\\sigma^2)$. The accuracy\nchange is formulated as:\n$\u2206acc(j) = |acc(D_j, f (\\Theta, \u00b7)) \u2013 acc(D_j, f (\\bar{\\Theta}, \u00b7))|$,\nwhere $\\bar{\\Theta} = {z_i}_{i\\notin I_j} \\cup {Z_i}_{i\\in I_j}$. The result is in Fig-\nure 4."}, {"title": "\u25b7 How to implement random noise?", "content": "We add an\nextra noise term, which is a zero-mean Gaussian\nrandom variable, to the selected neurons, i.e.,\n$Z_i = z_i + \\epsilon_i$, where $\\epsilon_i \\sim N(0,\\sigma^2)$. The result is\nshown in Figure 5."}, {"title": "Random Initialization and Random Noise", "content": "For all random initialization (Figure 10 in Supplemen-\ntary materials) and noise-adding (Figure 11 in Supplementary materials) experiments in Section 3.1,\nwe choose the random variable with multiple standard deviations to validate the existence of critical\nneuron. For each subfigure in Figure 10 and Figure 11 (in Supplementary materials), the results is\naveraged over 10 random seeds.\nOverall, regardless of the scale variation in the accuracy shifts, our experiments using pruning,\nrandom initialization, and random noise consistently demonstrate that the accuracy of minority\ngroups is significantly sensitive to the alteration of a handful of selected neurons. This finding\nconfirms the existence of a small subset of critical neurons for memorizing minority examples during\ntraining. Moreover, it validates that both gradient-based and magnitude-based criteria are effective\nin identifying these critical neurons."}, {"title": "3.1.2 Structured Tracing", "content": "In unstructured tracing (Section 3.1.1), we select neurons from the entire model without considering\nany sub-structures (i.e., layers, blocks) of networks. To gain a deeper understanding of how these\nsub-structures influence memorization, we use structured tracing for probing and comprehending\nthe role of sub-structures in the networks.\nSpecifically, we begin by fixing a particular layer, and then selecting neurons within the layer to\nassess the importance of these neurons in memorizing examples from groups. We still employ either\ngradient-based or magnitude-based criterion for neuron selection, but the scope of this specific\nexperiment is confined to the individual layer. This process is identically repeated for each layer in\nthe entire model."}, {"title": "Observation and Analysis", "content": "In Figure 7 (in Sup-\nplementary materials), we employ a heatmap to\nvisualize how accuracy changes across different\ngroups when we selectively zero-out a subset of\nneurons within a specific layer. What becomes\nevident is that deactivating the same number\nof neurons with the highest gradients or magni-\ntudes within a layer consistently leads to a more\nsignificant shift in accuracy for the minority\ngroups compared to the majority groups. This\ndifference is clearly discernible in the brighter\ncolor associated with the minority groups G\u2081\nand G2 in the middle two rows. Furthermore,\nwe notice that these within-layer critical neu-\nrons appear to be distributed across multiple\nlayers in the early stages of the model, rather\nthan being confined to the final few layers. This\nfinding aligns with the literature which indicates that the memorization of atypical examples can be\ndistributed and localized throughout the neural networks [Maini et al., 2023]."}, {"title": "3.2 Spurious Memorization by Critical Neurons", "content": "In Section 3.1, our experiments have established the presence of a small set of critical neurons\nresponsible for memorizing minority examples during training. This underscores the role of\nspurious memorization as a significant factor in imbalanced group performance. In this section,\nwe take a further step in demystifying the cause of imbalanced group performance under spurious\ncorrelation, particularly focusing on the discrepancy in the test accuracy between majority and\nminority groups.\nTo further validate the hypothesis that spurious memorization is a key factor in the imbalanced\ngroup performance, we investigate whether countering spurious memorization during training\ncould lead to improved test accuracy on minority groups. Our findings affirmatively answer this\nquestion. By specifically targeting and removing spurious memorization via a specialized fine-\ntuning framework, we observe a consistent improvement in the test accuracy for minority groups.\nWe report extensive experimental results across different model architectures, including ResNet-50\nand ViT-Small, and on benchmark datasets including Waterbirds [Sagawa et al., 2020a, Wah et al.,\n2011] and CelebA [Liu et al., 2015], providing comprehensive analysis on the effects of spurious\nmemorization on imbalanced group performance."}, {"title": "3.2.1 Interference with Spurious Memorization", "content": "Our Framework. Figure 8 (in Supplementary materials) summarizes our fine-tuning framework\nfor analyzing spurious memorization. By default, our framework is built upon simCLR [Chen et al.,\n2020], adhering to its key components such as data augmentations and the non-linear projection\nhead. The primary distinction between ours and simCLR is centered around two models: a target\nmodel and an auxiliary model. The auxiliary model is essentially a pruned version of the target\nmodel, where certain critical neurons are masked while the remaining neurons retain the same\nweights as the target model. This allows the framework to feed two augmented images into separate\nmodels, yielding two distinct feature representations for contrasting with each other.\nMore specifically, we begin with the target model, represented as $f (\\theta,\\cdot)$, where e denotes the model\nweights. These weights are initialized by pretraining the model using ERM. The next stage involves\nfine-tuning the target model. To this end, we construct a pruned model, $f (m\\odot \\theta,\\cdot)$, with m being a\nmasking vector. The mask is created based on criteria derived from either gradient or magnitude, as\ninspired in Section 3.1. In our experiments, we zero-out the top 0.01% of neurons based on their\n$l_2$-norm of their gradient or magnitude, where 0.01% serves as a hyperparameter."}, {"title": "\u25b7 How is the gradient calculated?", "content": "It is worth noting that for gradient calculation, we do not rely on\ngroup labels as in Section 3.1, but instead use the model's predictions as pseudo labels for sample\nselection. During each epoch, we calculate the cross-entropy loss for each sample, select the top 256\nsamples with the highest loss, and randomly sample 128 out of them to form the batch for gradient\ncomputation."}, {"title": "3.2.2 Removing Spurious Memorization Improves Group Robustness", "content": "In this study, our primary objective is to investi-\ngate whether mitigating spurious memorization\ncan lead to an enhancement in the test accuracy\nof minority groups. The findings are illustrated\nin Figure 6, where we compare the Worst Group\nAccuracy (WGA) between standard ERM train-\ning and our proposed framework. The WGA\nfor ERM training is evaluated by the testing set\nwith the best-performing checkpoint (on the val-\nidation set) among the first 100 epochs. Notably,\nwe observe a significant increase in WGA under\nall scenarios. Specifically, with the ResNet-50\nbackbone, we observe a significant 16.87% and\n10.77% improvement in WGA for the Water-\nbirds and CelebA datasets, respectively. Simi-\nlarly, on ViT-Small model, we observe 23.83%\nand 9.45% improvements in WGA.\nIt is important to highlight that our auxiliary model is essentially a pruned version of the target\nmodel, with only 0.01% of the neurons being masked. Despite this seemingly small modification,\nthe consistent performance boost in WGA across different architectures and datasets is strikingly\nremarkable. This improvement suggests that by strategically disrupting the spurious memorization"}, {"title": "3.2.3 Ablation on Hyper-parameters", "content": "In this subsection, we perform extensive ablation studies to offer a more comprehensive perspective\nof the framework. All the ablation experiments are conducted using the Waterbirds dataset with the\nResNet-50 model."}, {"title": "Ablation on Loss Functions", "content": "In Section 3.2.1 we use MSE as one of the loss terms in the model\nfine-tuning (see Eqs. (3.2) and (3.3)). Here we compare MSE with Cross Entropy (CE) loss. Using\nCE, the final loss becomes $L_{total}(\\theta,x,y) = L_{NT}(\\theta,x) + \\lambda L_{CE}(\\theta,x,y)$, as compared to Eq. (3.3). The\nresult is shown in Table 1. We observe that MSE is more effective in terms of WGA gain than CE\nunder the same pruning percentage. Still, both choices manifest significant WGA gain against ERM,\ncorroborating our hypothesis that spurious memorization in the critical neurons might play a critical\nrole in imbalanced group performance."}, {"title": "Ablation on Kick-in Epoch", "content": "In our training framework introduced in Section 3.2.1, we first pretrain\nthe target using ERM for 40 epochs and then switch to a fine-tuning stage with loss function Eq.\n(3.3). In other words, our framework kicks in at epoch 40. Here we test different choices of the\nkick-in epoch. The result is shown in Table 2. Overall, we see that epoch 30 is not effective, while\nthe choice of 40, 50, and 60 all return meaningful returns. This reason is that the ERM training of\nthe target model has not converged yet at epoch 30."}, {"title": "Ablation on Number of Fine-tuning Epochs", "content": "We then compare the number of epochs for the\nfine-tuning stage. The result is shown in Table 3. Observe that, there is no difference between the"}, {"title": "Ablation on Data Source for Gradient Calculation", "content": "We compare different source of gradient in\nthe calculation of gradient-based criterion. As introduced in Section 3.2.1, the neuron gradient\nis computed on a selected subset of training data. By default, this subset is chosen as the worst\npredicted examples by the target model in terms of the CE loss. From Table 4, we observe that,\ncalculating the gradient on the subset of worst predicted examples from the minority groups does\nnot show any benefit. Considering the fact that using minority groups as the gradient source requires\naccess to the group label which is sometimes unavailable, we suggest using the full training set as\nthe gradient source."}, {"title": "Ablation on Loss Term Ratios", "content": "We compare different choice of the loss term ratio A in Eq. (3.3).\nThe result is shown in Table 5."}, {"title": "3.2.4 Visualization Results", "content": "To interpret the outcome of the trained neural networks by ERM and our fine-tuning strategy, we\nvisualize the GradCAM on ResNet-50 trained by solely ERM and our fine-tuning strategy. The\ntarget layer is set to layer4.2.conv3.weight, and the target dimension in output feature is set to\ndimension 0. Figure 9 (in Supplementary) clearly shows that by our fine-tuning strategy, the neural\nnetwork shifts its focus from the spurious element (i.e., background) to the main object (i.e., bird)."}, {"title": "4 Discussion", "content": "In this paper, we conduct the systematic investigation aimed at uncovering the root structural cause\nof imbalanced group performance in the presence of spurious correlations. This phenomenon is\ncharacterized by both majority and minority groups achieving high training accuracy, yet minority\ngroups experiencing reduced testing accuracy. Our comprehensive study verifies the presence\nof spurious memorization, a mechanism involving critical neurons significantly influencing the\naccuracy of minority examples while having minimal impact on majority examples. Building upon\nthese key findings, we demonstrate that by intervening with these critical neurons, we can effectively\nmitigate the influence of spurious memorization and enhance the performance on the worst group.\nOur findings shed light on the reasons behind NNs demonstrating robust performance with majority\ngroups but struggling with minority groups, and establish spurious memorization as a pivotal\nfactor contributing to imbalanced group performance. We hope that our discoveries offer valuable\ninsights for practitioners and serve as a foundation for further exploration into the intricacies of\nmemorization in the presence of spurious correlations.\nMitigating spurious correlations in machine learning and statistical models is a key step towards\ncrafting more reliable and trustworthy medical AI. Our research uncovers that by eliminating spuri-"}, {"title": "5 Methods", "content": "ous memorization, we can pinpoint critical neurons, whose modification significantly influences\nthe model's performance, particularly in recognizing minority groups. Concerning privacy risks,\nthese are relatively low in our approach, as the analysis requires existing access to the dataset and\nthe capability to train models. Looking forward, our future research will aim to address challenges\nwithin the broader scope of spurious correlations, extending beyond vision applications to include\nlanguage datasets, among others. This expansion will help in developing AI solutions that are more\nversatile and universally applicable."}, {"title": "5.1 Experimental Setup", "content": "Datasets and Models. In our study, we conduct experiments on two popular benchmark datasets\nfor spurious correlation: Waterbirds [Sagawa et al., 2020a, Wah et al., 2011], and CelebA [Liu\net al., 2015]. We comprehensively evaluate the extent to which spurious memorization exists in the\nlarge pre-trained models (ResNet-50 [He et al., 2016] and ViT-Small [Dosovitskiy et al., 2021]) on\nImageNet [Deng et al., 2009]. Note that we report the average performance of 10 independent runs\nwith different random seeds for experiments including unstructured tracing and structured tracing.\nIn the experiments detailed in Section 3.2, we strictly adopt the standard dataset splits for both\nWaterbirds and CelebA, following the setting in [Idrissi et al., 2022]. Our adoption of ResNet or\nViT models pre-trained on ImageNet is consistent with the main literature [Kirichenko et al., 2023,\nQiu et al., 2023, Yang et al., 2023]. Furthermore, the high baseline accuracy achieved by pre-trained\nmodels is critical for studying memorization, which is a focal point of our study."}, {"title": "Identification of Critical Neurons", "content": "For identifying critical neurons, we utilize two key metrics:\ngradient-based and magnitude-based criteria. Here, gradient refers to the gradients calculated\nduring backpropagation with respect to a specific data batch. Magnitude, on the other hand, is\ndetermined by the norm of neuron weights. Details of data batch selection are given in Section 3.1.1\nand Section 3.2."}, {"title": "Neurons and Layers", "content": "For our study on convolutional neural networks (e.g., using ResNet-50 as\nthe backbone), we consider channels as the basic units in order to preserve the channel structure,\nas suggested in prior work [Maini et al., 2023]. On the other hand, for our study involving Vision\nTransformer (e.g., using ViT-Small as the backbone), we consider individual neurons as the basic\nunits. Therefore, for ease of reference in our study, we use the term 'neuron' to collectively refer to\nboth channels in ResNet-50 and neurons in ViT-Small."}, {"title": "Experimental Setup", "content": "In all our experiments, we keep the experimental setup consistent. We use\na single NVIDIA Titan RTX GPU. We conduct our experiments using PyTorch 1.13.1+cu117 and\nPython 3.10.4, to ensure reproducibility."}, {"title": "Data Preprocessing", "content": "Our dataset preprocessing remains consistent across all datasets and ex-\nperiments. For details, please refer to Table 8 (in Supplementary materials). These steps ensure\nthat the resulting image size is 224 x 224 pixels, suitable for both ResNet-50 and ViT-S/16@224px.\nFollowing these augmentation steps, we normalize the image by subtracting the average pixel values\n(mean=[0.485, 0.456, 0.406]) and dividing by the standard deviation (std=[0.229, 0.224, 0.225]).\nThis normalization procedure aligns with the approach used in CLIP Radford et al. [2021]. No\nfurther data augmentation is applied after these steps."}, {"title": "Implementation Details", "content": "In Section 3.1, we implemented the ERM with specific configurations."}]}