{"title": "TIMESUITE: IMPROVING MLLMS FOR LONG VIDEO UNDERSTANDING VIA GROUNDED TUNING", "authors": ["Xiangyu Zeng", "Kunchang Li", "Chenting Wang", "Xinhao Li", "Tianxiang Jiang", "Ziang Yan", "Songze Li", "Yansong Shi", "Zhengrong Yue", "Yi Wang", "Yali Wang", "Yu Qiao", "Limin Wang"], "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated impressive performance in short video understanding. However, understanding long-form videos still remains challenging for MLLMs. This paper proposes TimeSuite, a collection of new designs to adapt the existing short-form video MLLMs for long video understanding, including a simple yet efficient framework to process long video sequence, a high-quality video dataset for grounded tuning of MLLMs, and a carefully-designed instruction tuning task to explicitly incorporate the grounding supervision in the traditional QA format. Specifically, based on VideoChat, we propose our long-video MLLM, coined as VideoChat-T, by implementing a token shuffling to compress long video tokens and introducing Temporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of visual representation. Meanwhile, we introduce the TimePro, a comprehensive grounding-centric instruction tuning dataset composed of 9 tasks and 349k high-quality grounded annotations. Notably, we design a new instruction tuning task type, called Temporal Grounded Caption, to perform detailed video descriptions with the corresponding timestamps prediction. This explicit temporal location prediction will guide MLLM to correctly attend on the visual content when generating description, and thus reduce the hallucination risk caused by the LLMs. Experimental results demonstrate that our TimeSuite provides a successful solution to enhance the long video understanding capability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the benchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T exhibits robust zero-shot temporal grounding capabilities, significantly outperforming the existing state-of-the-art MLLMs. After fine-tuning, it performs on par with the traditional supervised expert models.", "sections": [{"title": "INTRODUCTION", "content": "Multimodal Large Language Models (MLLMs) have demonstrated impressive video understanding performance by following the general human instructions to interpret the visual content (Li et al., 2023b; Zhang et al., 2023; Lin et al., 2023a; Jin et al., 2024; Wang et al., 2024d). However, these MLLMs still struggle in long video understanding, as a long video sequence may contain various dynamic actions and complex temporal relationships, making it difficult for MLLMs to effectively locate the key segments related to questions. When humans watch long videos, their attention is consciously focused on prominent segments, which may occur within a few seconds. NEXT-GQA (Xiao et al., 2024) has also verified the relevance of temporal grounding for accurately answering video QA tasks. Therefore, a natural question arises: Can we enhance long video understanding by using temporal grounding as a auxiliary task?\nPreviously, some works have made progress in temporal grounding task by using general MLLMs. They often enhance the temporal grounding capability of video MLLMs by designing specialized modules and perform specific supervised fine-tuning (Ren et al., 2024; Huang et al., 2024a;b). However, these overly specialized designs significantly impair the general QA capabilities of video MLLMs, resulting in great performance drop on the video QA task (as illustrated by TimeChat in Figure 1). Meanwhile, current research on long video understanding primarily focuses on architecture design, such as long-context LLMs (Liu et al., 2024a) and token compression (Song et al., 2024a). They can only capture holistic semantics in videos without the ability of localizing fine-grained information, leading to poor performance in temporal grounding tasks (as illustrated by MovieChat in Figure 1). So far, it is still challenging to build a video MLLM that is good at both tasks of temporal grounding and long video QA. We argue long video understanding could be assisted by explicitly performing temporal grounding, as grounding supervision enables MLLM to establish the detailed correspondance between the visual segments and fine-grained semantics. This fine-grained alignment would guide the MLLM to attend correctly video segments when generating answers and thus relieve the hallucination risk caused by the LLM.\nBased on the above analysis, in this paper, we propose TimeSuite, a collection of new designs to improve the long video understanding capability of the existing short-form MLLMs, with a focus on incorporating grounding supervision in instruction tuning process. First, to address the high computational cost caused by the excessive number of visual tokens in long videos, we propose a simple Token Shuffle scheme to compress visual tokens, allowing the LLM to process more frame inputs. We also propose TAPE to generate adaptive position encodings, enhancing the temporal awareness of visual representations. The proposed structure does not introduce overly complex proprietary designs, which could be efficiently initialized with the parameters of short video MLLMs, without damaging the original performance of pre-trained MLLM. Second, to naturally incorporate the grounding ability into our MLLMs and yet still to preserve its original general QA capability, we design a new instruction tuning task, called Temporal Grounded Caption. This new task requires generating detailed segment-level description with corresponding timestamp prediction. Tuning on this new task will not only endow the MLLM with the extra grounding ability but also enhance its original long video QA performance, thanks to the requirement of building correspondence between grounded segments and detailed captions. Finally, we collect a comprehensive grounding-centric instruction tuning dataset for post-training our designed MLLMs, which is composed of 349K high-quality annotations covering 9 tasks. Based on this new dataset, we are able to perform grounded tuning with detailed captions on our proposed MLLMs (coined as VideoChat-T).\nWe verify the effectiveness of TimeSuite design through extensive experiments on the tasks of long video understanding and temporal grounding. VideoChat-T demonstrates a significant improvement in accuracy over baseline for long video understanding, with a 5.6% increase on Egoschema (Mangalam et al., 2023) and a 6.8% increase on VideoMME (Fu et al., 2024). Additionally, VideoChat-T exhibits robust zero-shot temporal localization capabilities on Charades-STA (Gao et al., 2017) and QVHighlights (Lei et al., 2021a). Our VideoChat-T outperforms the state-of-the-art temporal grounding MLLM of TimeChat from 50% to 100% for different metrics. After fine-tuning on the training set of temporal grounding benchmarks, the performance of VideoChat-T is on par with the state-of-the-art supervised expert models. The experiments demonstrate that our VideoChat-T is the first end-to-end MLLM that is able to perform well on both temporal grounding and general video QA. In particular, we show that grounded tuning with explicit location prediction can facilitate the long video understanding and relieve the hallucination risk."}, {"title": "RELATED WORK", "content": "2.1 VIDEO MLLMs\nWith the advancement of LLMs (Chiang et al., 2023; Touvron et al., 2023; Jiang et al., 2023), video MLLMs have emerged by utilizing projection bridges to link vision encoders with LLMs (Li et al., 2024a; Zhang et al., 2023; Wang et al., 2024d). Limited by the training context length, thought these methods perform well with a small number of frame inputs, they meet significant challenges when processing long videos. The longer video length usually implies longer temporal relationships and more redundancies, resulting in the difficulty of extracting key clues (Zhou et al., 2024). Recently, several methods for long video handling have been proposed, such as exploiting long context LLM (Liu et al., 2024a; Zhang et al., 2024b; Xue et al., 2024; Wang et al., 2024c) and token compression (Li et al., 2023d; Song et al., 2024a; Zhang et al., 2024a) for enabling more visual inputs and agents for task decomposition or retrieval (Fan et al., 2024; Wang et al., 2024b;f). MovieChat (Song et al., 2024a) supports more frames by applying short-term and long-term memory to merge similar visual tokens. Yet, studies in learning objectives for long videos are less explored, making it difficult to alleviate the frequent hallucination of LLMs in long context reasoning. Our proposed TimeSuite leverages temporally-centric tasks to unlock the temporal perception potential of MLLMs, anchoring responses to the most relevant video segments.\n2.2 TEMPORAL GROUNDING\nTemporal grounding is a fundamental capability in video understanding, associating video semantics to clips with corresponding timestamps. Typical expert models have been developed to it (Lei et al., 2021b; Moon et al., 2023a;b; Lin et al., 2023b; Zeng et al., 2024) by formulating it into a timestamp regression from visual inputs and user queries. Most existing video MLLMs fail to address it compared with expert models, while some remedy its temporal grounding by specifically designed architectures and data (Huang et al., 2024a; Wang et al., 2024e; Li et al., 2024b; Huang et al., 2024b; Qu et al., 2024). Timechat (Ren et al., 2024) binds visual features of images with timestamps and uses a sliding window to handle variable token length. From the perspective of training data, an instruction-tuning dataset TimeIT is constructed. Despite impressive improvements in temporal performance, these MLLMs still lag behind expert models and compromise general video dialogue capabilities. In this paper, we explore how to enhance the temporal grounding of MLLMs while preserving their original capabilities."}, {"title": "METHOD", "content": "In this section, we detail the proposed TimeSuite, a new collection of designs for improving short video MLLMs. Specifically, our TimeSuite includes a long video modeling framework, a high-quality video dataset for grounded tuning, and a carefully-designed instruction tuning task. With this new TimeSuite design, we are able to adapt the short-form video MLLM, obtaining significant performance improvements on two types of long video understanding tasks: traditional long video QA and temporal video grounding.\n3.1 VIDEOCHAT-T\nWe first describe the architecture of our proposed long video modeling framework. Specifically, built upon VideoChat2 (Li et al., 2024a), we devise long-video version of VideoChat-T. Our VideoChat-T is composed of a video backbone for extracting visual representations, a visual-language connector to compress visual tokens and bridge the visual and languages modalities, a LLM to follow human instructions to interpret the video content.\nThe architecture of VideoChat-T is illustrated in Figure 2. Its workflow has three stages. In the first stage, long videos are evenly segmented into clips and the clips are embedded by the Video Encoder and Q-Former (Li et al., 2023a). Then, for compressing visual token number and highlighting crucial ones, token shuffling is employed to merge adjacent tokens, and TAPE is used to add temporal adaptive positional encodings. Finally, the long video tokens are concatenated with the user query, serving as the input of LLM, thereby generating appropriate responses.\n3.1.1 BACKBONE DESIGN\nVideo clip encoding. For the given long video, we perform uniform sampling (Wang et al., 2019) to obtain $K \\times T$ frames. We divide these frames into K video segments in chronological order, and sample T frames from each segment. Next, we use the video encoder and its visual-linguistic connector (Q-Former here) to encode each segment into N tokens. After the aforementioned processing, the entire video is encoded into a sequence of visual tokens, denoted by $V_q \\in \\mathbb{R}^{L \\times C_q}$, where $C_q$ is the dimension of output token by the Q-Former and $L = K \\times N$ is the total number of tokens for the entire video.\nLarge Language Model. According to previous research, images and visual cues are projected into the same feature space of the LLM. The LLM acts as an interaction interface in the MLLMs, being used to process multimodal inputs, parse user instructions, and generate appropriate responses. To afford the processing of long video sequence, we need to design an efficient compression module between the visual encoder and LLMs.\n3.1.2 VL-CONNECTOR: TOKEN SHUFFLE\nThe increased number of sampled frames in long videos leads to a larger number of encoded visual tokens, causing a significant rise in the computational complexity and memory consumption of LLMs. Therefore, it is crucial to keep the number of visual tokens within an acceptable range. Some works have proposed various token compression schemes, such as clustering (Jin et al., 2024) and pooling (Huang et al., 2024b). However, clustering methods often struggle to maintain the temporal consistency, and pooling methods usually result in a certain loss of overall performance.\nTo address this, we propose a simple token shuffling compression scheme that ensures the temporal consistency of video tokens before and after compression while avoiding excessive performance loss. Previous methods often used a projector to achieve dimensional conversion. However, projecting visual encoding vectors from low to high dimensions does not increase information density. Therefore, we propose to rearrange multiple visual tokens along the channel dimension. Specifically, for the long video $V_l = [v_1, v_2,...,v_L] \\in \\mathbb{R}^{L \\times C_q}$, we concatenate m adjacent tokens along the channel dimension to obtain the reshaped visual feature $V_m = [v_m^1, v_m^2, ..., v_m^{\\frac{L}{m}}] \\in \\mathbb{R}^{\\frac{L}{m} \\times mC_q}$, where each merged token $v_m^i$ is represented as:\n$v_m^i = Concat(v_{(i-1)*m+1}, v_{(i-1)*m+2}, ..., v_{i*m}) \\forall i = 1,2,...,\\frac{L}{m}$.\nNext, a linear projection layer is applied to the merged visual feature $V_m$, generating the visual token sequences $V_l \\in \\mathbb{R}^{\\frac{L}{m} \\times C_l}$ as input into the LLM, where $C_l$ represents the token channel dimension of the LLM. This scheme effectively reuses the projector of base model by replicating the original linear layer parameters m times along the channel dimension, achieving an initialization equivalent to mean pooling with a window length of m. This design avoids introducing additional randomly initialized parameters that might disturb the original model, thus preserving the its original capabilities. Additionally, compared to directly using pooling, this method offers higher flexibility for fine-tuning to achieve better results (see ablation study, Table 4).\n3.1.3 TEMPORAL ADAPTIVE POSITION ENCODING\nTo bind temporal positional information to visual tokens, we propose an adapter called Temporal Adaptive Position Encoding (TAPE). Inspired by CPVT (Chu et al., 2021), our TAPE uses zero padding at both ends of the convolution as anchors, and gradually transmits relative positional encoding information. Without the need to add any special time tokens, TAPE can automatically perceive the relative temporal positions of the token sequence and generate temporal embeddings.\nSpecifically, the long video token sequence $V_q$ is first compressed in the channel dimension by a linear layer and further compressed in sequence length by a pooling layer. Next, we use a U-Net-like structure composed of one-dimensional depthwise separable convolutions to progressively down-sample the sequence, obtaining three one-dimensional temporal feature sequences with different resolutions. Subsequently, a convolution with a sufficiently long window is applied to the shortest temporal feature sequence, using zero padding at both ends as anchors to encode the relative temporal position of each token in the sequence (Chu et al., 2021). Then, we progressively upsample and restore the temporal feature sequences from short to long, using residual connections to retain temporal features at different scales. Finally, the temporal feature sequences are restored to the same length as $V_i$ and aligned in the channel dimension by a linear layer, thereby obtaining the temporal features $V_t$ output by the TAPE. For detailed implementation of TAPE, please refer to Appendix A.\nOur proposed TAPE offers a plug-and-play module, which could be easily integrated into the network structure via residual connections, adding temporal position information to video tokens without disrupting the distribution of other trainable parameters. With appropriate training strategies, TAPE effectively preserves the model's generalization capabilities and enhances its temporal sensitivity (see ablation study, Table 3), which is important for temporal grounding task.\n3.2 \u03a4\u0399\u039c\u0395PRO: TEMPORAL GROUNDED INSTRUCTION DATA\nTraditional temporal grounding datasets only contain monotonous ground truth, i.e., the start and end times of the target period. This data format performs well in training the classic expert models, but is difficult to unleash the potential of LLMs. Although several temporal grounding-centric datasets have been released for MLLM fine-tuning (Ren et al., 2024; Huang et al., 2024b), they still have deficiencies in data quantity, data quality, and task diversity. Thus, it is necessary to build a more comprehensive temporal dataset designed for the tuning of MLLMs.\nBased on the criteria of diversity, length, and difficulty, we collect and clean several existing high-quality grounding-centric datasets (Ren et al., 2024; Huang et al., 2024a;b), and create two new datasets, resulting in the TimePro. Compared to previous temporal grounding-centric datasets, TimePro offers a larger volume of data, a broader distribution, and a higher task diversity, facilitating the learning of more generalizable temporal representations for MLLMs.\nAs shown in Figure 3(a), TimePro contains 9 task types from 15 datasets that are highly relevant to temporal grounding, containing approximately 349K high-quality temporal grounding annotations. The 9 tasks are specified as follows. Temporal Video Grounding involves identifying the start and end times of video content based on a natural language query (Anne Hendricks et al., 2017; Oncescu et al., 2021; Zala et al., 2023). Dense Video Captioning requires detecting events within a video and providing corresponding timestamps and descriptions (Krishna et al., 2017; Huang et al., 2020; Zhou et al., 2018). Video Summarization focuses on determining key frames or clips in the form of timestamps rather than semantic summaries (Song et al., 2015; Gygli et al., 2014). Step Localization aims to segment and describe important steps in a long video (Tang et al., 2019; Zala et al., 2023). Transcribed Speech Generation predicts speech content and its timestamps from visual signals (Zellers et al., 2022). Reasoning Temporal Localization combines timestamps with explanatory answers (Huang et al., 2024b). Multi-format Temporal Grounding includes single-turn and multi-turn dialogues with diverse question types (Huang et al., 2024a). Highlight\n3.3 TEMPORAL GROUNDED CAPTION TASK\nSome studies have shown that MLLMs often exhibit severe hallucinations when dealing with fine-grained perception tasks (Ji et al., 2023; Huang et al., 2023; Golkar et al., 2023). Since our VideoChat-T directly regresses the timestamps corresponding to the text queries using MLLMs, it is more susceptible to hallucinations compared to methods that use external expert models as decoders (Wu et al., 2024). By forcing the video MLLMs to predict the event occurrence time and simultaneously describe the visual content evidence, we attempt to anchor these queries to the relevant time segments within the video, rather than generating hallucinations originating from LLM itself. Based on this analysis, we design the Temporal Grounded Caption task.\nThe top of Figure 3(b) illustrates the definition of Temporal Grounded Caption. We use a brief scene title of the video segment as the query, requiring the model to simultaneously respond with the precise start and end times of the video segment and provide a detailed description of that segment. While the content in the scene title may leak into the detailed caption response, most of the missing detailed information must be correctly described by attending the corresponding segment. Moreover, temporal grounding and detailed captioning can serve as regularization task for each other, preventing caption model from hallucinations from unrelated visual or linguistic contexts and helping grounding model to regress the timestamp more accurately.\nThe process for collecting our Temporal Grounded Caption data is described at the bottom of Figure 3(b). In the first stage, we use a detailed caption dataset with timestamps as our data source. We remove data with target grounding time intervals that are too short or too long and ensure that the scenes in the video are as diverse as possible. In the second stage, we use a LLM to summarize scene titles. To prevent excessive semantics of video segments from being leaked from the query to the MLLM, we try to retain the minimal subset of key features that are sufficient to distinguish the video segments. In the third stage, to avoid overly similar or identical content appearing at different temporal intervals in the video, we perform similarity filtering on the data annoations. Based on the scene titles and video features, we calculate the similarity between different segments of the same video and remove data with excessively high similarity. In the fourth stage, we randomly sample the generated data and manually assess its quality. Based on human feedback, we refine the threshold parameters for data filtering used in the first three stages to yield the final Temporal Grounded Caption dataset. This new dataset plays an important role in our grounded tuning."}, {"title": "EXPERIMENTS", "content": "4.1 IMPLEMENTATION DETAILS\nBuilt upon VideoChat2, we use UMT-L (Li et al., 2023c) and Mistral-7B (Jiang et al., 2023) as the video encoder and LLM, respectively. Except for the TAPE, all components are initialized from the pre-trained model of VideoChat2-Mistral. For the TAPE, we use random initialization, set the initial values of the final linear layer to zero, and freeze it during the first epoch of training. We set the frame count T for each clip to 8, so the number of clips K for a long video is equal to the total frame count divided by T. We fine-tune the model for 3 epochs using the TimePro with 349K instances and a general QA task dataset with 82K instances. To ensure the stability of model training, we use 192-frame input for the first epoch. In the second and third epochs, we unfreeze the TAPE and adjust the model input to 128 frames. All experiments are conducted on 16 A100 GPUs.\n4.2 PERFORMANCE ON TEMPORAL GROUNDING\nWe evaluate our method using two commonly used temporal localization tasks, i.e., Temporal Grounding and Highlight Detection. The performance comparison between VideoChat-T and other models is shown in Table 1. Our method's zero-shot performance surpasses all previous LLM-based methods and after fine-tuning, VideoChat-T even exceeds some classic expert models on the temporal grounding task.\nTemporal Grounding. This task aims to identify the start and end timestamps of the video content described by the query sentence, using Charades-STA as the evaluation benchmark. VideoChat-T achieves an accuracy of 48.7 in the R@1 (IOU=0.5) metric, significantly surpassing the previous state-of-the-art MLLM method, namely TimeChat, by 16.5 points. Additionally, it outperforms the fine-tuned version of TimeChat on the training set of the evaluation benchmark by 2.0%. Furthermore, the performance of VideoChat-T fine-tuned on the evaluation benchmark training set reaches 67.1 R@1 at IoU=0.5, surpassing most state-of-the-art classic supervised expert models.\nHighlight Detection. We use QVHighlights as the evaluation benchmark. For a given query, this task requires outputting all timestamps of highlight moments and their corresponding saliency scores. Since there could be many sparse highlight moments in a video, this task requires finer-grained video understanding at the frame level. VideoChat-T achieves mAP of 26.5, significantly surpassing the previous MLLM method of TimeChat by 13.0 points, and also outperforms its fine-tuned version by 4.8 points. We observe that after fine-tuning on the corresponding training set, VideoChat-T shows almost no performance improvement. This may be due to the bottleneck in language representation of LLMs. The Highlight Detection task requires outputting a (timestamp, saliency score) pair for each highlight moment, and a video may contain dozens of discrete highlight moments, making it challenging for the model to correctly respond with dozens to hundreds of numbers in a language format. The precise numerical salience score output is very difficult for LLMs, and VideoChat-T can only respond well to queries with fewer highlight moments. Due to the"}, {"title": "PERFORMANCE ON GENERAL VIDEO QA", "content": "In addition to test the grounding ability of our VideoChat-T, we also want to verify its general video question answering performance. According to mainstream evaluation standards, we use both long video and short video QA to assess the general video understanding capability of VideoChat-T. Table 2 shows the performance of VideoChat-T on the video QA evaluation benchmarks.\nLong Video QA. We use Egoschema (Mangalam et al., 2023) and VideoMME (Fu et al., 2024) to evaluate the long video capabilities of VideoChat-T. In conjunction with our proposed architectural improvements, we incremental fine-tune VideoChat2 using only 432K data points. VideoChat-T demonstrates outstanding performance on the Egoschema, achieving an accuracy of 68.4% on the test subset and 60.0% on the entire test set. Compared to VideoChat2, VideoChat-T obtains improvements of 4.8% and 5.6% on the subset and the full test set, respectively. Additionally, for the VideoMME benchmark, VideoChat-T achieves an accuracy of 46.3% by solely analyzing the visual content without using subtitles, representing a 6.8% improvement over VideoChat2. On the long video data division of VideoMME, VideoChat-T achieves an accuracy of 41.9%, which is an 8.7% improvement compared to VideoChat2. The upgraded VideoChat-T demonstrated significant performance improvements on long video QA benchmarks. This indicates the potential of leveraging grounding-centric video tasks to enhance the temporal awareness of MLLMs, thereby further improving long video understanding capabilities.\nShort Video QA. We use MVBench (Li et al., 2024a) to evaluate the general short video understanding capabilities of VideoChat-T. VideoChat-T achieves an overall average accuracy of 59.9% on MVBench, which is a 0.5% decrease compared to VideoChat2. It is important to note that achieving minimal performance loss is a challenging task. According to previous experiences in the field of incremental learning (Van de Ven et al., 2022), models inevitably forget old knowledge while learning new knowledge, necessitating specific research to deal with catastrophic forgetting. VideoChat2 is fine-tuned with 2M data, whereas VideoChat-T is fine-tuned with only 432K data, where 349K annotations are temporal grounding centric, resulting in only a 0.5% accuracy loss. Previous temporal MLLMs like TimeChat (Ren et al., 2024), although achieving strong temporal localization capabilities, yield much weaker general video QA capability, with an accuracy of only 38.5% on MVBench. This demonstrates that the design of our TimeSuite enhances new capabilities for the model while still preserving the original general video understanding capabilities."}, {"title": "QUALITATIVE ANALYSIS", "content": "Figure 4 presents a qualitative comparison between our model and other methods. In the example on the left, VideoChat-T is capable of answering more complex long video reasoning questions. Our model accurately identifies the temporal location of the \"light a cigarette\" event and determines the correct key clue \"the person in a white coat\" based on the video content. This leads to the inference that \"playing the piano very fast and pressing the keys very hard\" are the true reasons. The example"}, {"title": "ABLATION STUDY", "content": "Role of TAPE. To verify the performance improvement brought by TAPE, ablation experiments were conducted. Table 3 lists the performance results of the conducted adapter-related ablation experiments. It can be observed that when the TAPE is removed, the model's performance on long video understanding and temporal grounding benchmarks decreases. TAPE can adaptively embed positional encodings into video tokens, and the absence of TAPE leads to a certain loss in temporal awareness capability. When we unfroze the TAPE in the first epoch, the performance improved on the temporal grounding task but declined on the long video QA task. This is because the TAPE is highly suited for tasks with strong temporal dependencies. If unfrozen too early, the model may become biased towards fitting temporal grounding tasks. Freezing the TAPE during the first epoch allows the model to first optimize and learn a relatively generalized feature representation, thereby balancing the performance across different tasks.\nEffectiveness of Token Shuffle. To verify the effectiveness of token shuffle, we conducted ablation experiments. Table 4 presents the results of these ablation experiments. We compared token shuffle with conventional methods such as pooling and clustering, and also observed the results after removing efficient initialization. When we replaced token shuffle with pooling or clustering methods, the model's performance declined. This is because the efficient initialization of the linear layer in token shuffle makes the initial values of the module equivalent to average pooling, which gradually optimizes better solutions during training. Therefore, our method is inherently superior to pooling. On the other hand, clustering often fails to maintain the spatial/temporal consistency of the video, leading to temporal confusion. When we removed the efficient initialization of the linear layer, the negative impact of random initialization severely damaged the model's original performance.\nEffect of TimePro. We conducted ablation studies to evaluate the effectiveness of the TimePro data components. As shown in Table 5, by gradually adding subsets of TimePro, we observed the model's performance changes across various temporal grounding-centric instruction-tuning data. As we progressively added subsets of TimePro, not only did the model's performance on temporal grounding"}, {"title": "CONCLUSION", "content": "In this paper, we have introduced TimeSuite, a collection of new designs from perspectives of efficient architecture, high-quality data, and new instruction tuning task, to achieve long video understanding by fine-tuning short video MLLMs with temporal grounding-centric data. We address the computational challenges of processing long videos by introducing token shuffle to compress visual tokens. We also propose the TAPE for adaptive position encoding, enhancing the temporal awareness of visual representation. Additionally, our designed Temporal Grounded Caption training task ensure MLLMs to build correspondence between grounded segments and detailed caption, while the TimePro dataset provide comprehensive instruction tuning data for learning more effective temporal perception capability. Experimental results demonstrate that VideoChat-T significantly improves long video understanding, with notable performance gains on Egoschema and VideoMME. Furthermore, VideoChat-T exhibits strong zero-shot temporal grounding capabilities, significantly outperforming the previous MLLMs on temporal grounding. Overall, our TimeSuite provides effective designs for short MLLMs to enhance their performance on temporal grounding and long video QA. We hope our TimeSuite could yield some insights on designing long video MLLMs."}, {"title": "IMPLEMENTATION OF TAPE", "content": "Algorithm 1 details the implementation process of TAPE in code form. Specifically, the long video token sequence input_tokens is first compressed in the channel dimension by a linear layer to obtain time_ad, and the sequence length is compressed through a pooling layer. Next, we use a U-Net-like structure composed of one-dimensional depthwise separable convolutions to progressively down-sample the sequence, obtaining three one-dimensional temporal feature sequences with different time resolutions, namely time_ad1, time_ad2, and time_ad3. Subsequently, a convolution with a sufficiently long window is applied to the shortest temporal feature sequence time_ad3, using zero padding at both ends as anchors to encode the relative temporal position of each token in the sequence. Then, we progressively upsample the temporal feature sequences from short to long, using residual connections to preserve temporal features at different scales. Finally, the temporal feature sequence time_ad_out is restored to the same length as the video features after token shuffling and aligned in the channel dimension through a linear layer."}, {"title": "INSTRUCTION-TUNING DATA", "content": "We fine-tuned VideoChat-T using 432K data, which includes 349K instances from TimePro and 82K instances from normal data. All videos were sampled from existing open-source video datasets, with specific information about the relevant data provided in Table 6.\nWe evaluate the quality of the data from three perspectives: diversity, length, and difficulty. We strive to include different datasets for various tasks, and the distribution of videos in the datasets is as broad as possible. The length of the videos should be controlled within an appropriate range, as excessively long or short videos may pose challenges for training. Each query should clearly describe the video"}, {"title": "FULL PERFORMANCES", "content": "The performance of VideoChat-T on MVBench is shown in Table 8. Compared to VideoChat2, VideoChat-T only experienced a 0.5% accuracy loss. This indicates that our method effectively preserves the capabilities of the base model, preventing catastrophic forgetting caused by incremental fine-tuning. For the Action Localization (AL) task, which requires the model to determine the coarse-grained temporal position of events, the test accuracy improved from 44.0% to 56.5%. This indirectly confirms that our method significantly enhances the model's temporal awareness capabilities.\nThe overall performance of our model on VideoMME is presented in Table 9. VideoChat-T achieved significant improvements on both evaluation benchmarks of VideoMME, which include watching videos only and videos with subtitles. The improvements are particularly notable in the long video subset."}, {"title": "EXTRA ABLATION", "content": "Table 10: The performance changes of the model after removing STAR. Although the video sources of STAR may have some domain correlation with those of Charades-STA and MVBench, the performance of our model is minimally affected by STAR.\nWe found that the video sources in the STAR dataset might have some domain correlation with the video sources in MVBench and Charades-STA. Therefore, we removed STAR from the training set while keeping other training settings consistent with the original. The performance on benchmarks where the video sources might have domain correlation is shown in Table 10. The model's accuracy on Charades-STA (R@1 IOU=0.5) decreased by 1.2%, and the average accuracy on MVBench decreased by 0.5%. This indicates that the domain correlation of video sources did not significantly impact performance for our model. Notably, after removing STAR, our normal data volume was reduced to approximately 36K. This implies that, with sufficiently parameter-efficient initialization and appropriate training strategies, using only a small amount of high-quality normal data is sufficient to retain the model's original capabilities."}, {"title": "CASE STUDY", "content": "To further qualitatively analyze our model, we supplemented it with three types of examples. These examples are aimed at long video QA, short video QA, and captioning tasks, all of which include temporal grounding.\nMore qualitative comparisons about long video QA are shown in Figure 6. VideoChat-T effectively handles various questions across different domains. By better perceiving the temporal relationships of different events occurring in long videos, it can more accurately and deeply understand the detailed content of the entire video.\nMore qualitative comparisons about short video QA are shown in Figure 7. VideoChat-T effectively retains the original capabilities of the base model. Through parameter-efficient initialization methods and appropriate training strategies, we minimize the damage to the base model's capabilities caused by new architectures and data.\nMore qualitative comparisons about captioning are shown in Figure 8. Although VideoChat2 describes more local details in some scenarios compared to VideoChat-T, VideoChat-T focuses more on a series of temporal events, which aligns better with how humans typically describe videos."}, {"title": ""}]}