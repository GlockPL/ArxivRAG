{"title": "Some Insights of Construction of Feature Graph to Learn Pairwise Feature Interactions with Graph Neural Networks", "authors": ["Phaphontee Yamchote", "Saw Nay Htet Win", "Chainarong Amornbunchornvej", "Thanapon Noraset"], "abstract": "Feature interaction is crucial in predictive machine learning models, as it captures the relationships between features that influence model performance. In this work, we focus on pairwise interactions and investigate their importance in constructing feature graphs for Graph Neural Networks (GNNs). Rather than proposing new methods, we leverage existing GNN models and tools to explore the relationship between feature graph structures and their effectiveness in modeling interactions. Through experiments on synthesized datasets, we uncover that edges between interacting features are important for enabling GNNs to model feature interactions effectively. We also observe that including non-interaction edges can act as noise, degrading model performance. Furthermore, we provide theoretical support for sparse feature graph selection using the Minimum Description Length (MDL) principle. We prove that feature graphs retaining only necessary interaction edges yield a more efficient and interpretable representation than complete graphs, aligning with Occam's Razor. Our findings offer both theoretical insights and practical guidelines for designing feature graphs that improve the performance and interpretability of GNN models.", "sections": [{"title": "I. INTRODUCTION", "content": "Feature Interaction, one of the key challenges in predictive modeling, is the behavior of datasets whose two or more independent variables have a common influence on their dependent variable. Mathematically, it refers to operations between features in a conjunctive manner, such as multiplication, rather than treating features in isolation or through simple linear combinations. For example, if we have a dataset generated by the equation $y = x_1x_2 + x_3$, then we say that $x_1$ and $x_2$ interact with each other in the model of $y$.\nModeling interactions can significantly improve the accuracy of predictions and reduce the complexity of models by augmenting datasets with interaction features for the models to learn from. In the best-case scenario, even linear models can be used effectively. For example, it is easier to approximate the previous function by including interaction terms $x_1x_2$ and $x_3$ in the dataset, rather than using more complex models with only the features $x_1$, $x_2$ and $x_3$ with more complex models. Although this approach is not straightforward, it improves the interpretability of downstream tasks.\nApplications of feature interaction modeling span a wide range of domains, particularly in recommendation systems, where it plays a critical role in improving predictions. The click-through rate (CTR) problem in recommendation systems is an example of an application highly based on feature interactions. For example, the age of the user and the type of product may interact in different ways on various platforms. Younger users may prefer to buy fashion products on one platform, while older users might lean towards formal products on all platforms.\nAlthough many models, including traditional machine learning methods like decision trees, are capable of modeling feature interactions through their hierarchical structures. For example, we may consider that, in the context of meal preferences, a decision tree could represent the interaction between a type of wine and a kind of food (steak or fish) by placing the wine-related node as a descendant of the food-related node. Although this structure provides a clear representation of feature interactions, these models face significant limitations when dealing with complex or high-dimensional interactions. As the complexity of the interactions increases, the trees often become too large, leading to reduced interpretability.\nHence, we focus on graph-based methods which are more expressive in interpretability with edges between nodes. Specifically, graph-based methods represent data using graphs and model interactions through Graph Neural Networks (GNNs). Many graph-based methods have been developed to model feature interactions. Feature graphs, where nodes represent features, play an important role in GNN-based models for modeling interactions. However, a major challenge arises because most methods rely on a complete graph, where every possible interaction, i.e., every pair of features, is included. In addition, no insight and analysis about structures of feature graphs concerning features interactions are studied to make us understand the effect of graph structure to model it. One of the key questions that arises is the following: What characteristics"}, {"title": "II. PRELIMINARY", "content": "Feature Interactions: Feature interaction refers to the operation between two or more features concerning the output variable. Linear combinations or additive models represent zero-order interactions, in other words, no interaction [1]. For example, in a ground truth model, $y = 5x_1 + x_2x_3 - 7sin(x_4 - 3x_5x_6)$, we observe interactions between $x_2$ and $x_3$, as well as between $x_4$, $x_5$ and $x_6$ with respect to $y$.\nFeature Graphs and Graph Neural Networks: For graphs, we denote a tuple $G = (V, E, A, X)$ of nodes $V$, a set of edges $E$, an adjacency matrix $A$ and an embedding matrix $X$. For any node $v \\in V$, we denote by $N(v)$ the set of nodes adjacent to $v$, called the neighborhood of the node $v$. If only the structure of a graph is considered, we may use only $G(V, E)$.\nSince we represent the interaction between features by graph edges, we model instances with the graphs whose nodes correspond to features of a dataset. According to Fi-GNN [1], this graph is called a feature graph."}, {"title": "III. RELATED WORK", "content": "A. Feature Interaction Modeling\nFeature interactions in terms of nonadditive terms play a crucial role in model-based predictive modeling. Many works attempted to develop methods to indicate the interaction. Beginning with the statistical-based measure called Friedman's H statistic [6] involving the theory of partial dependency decomposition. However, it is computationally expensive and is dependent on predictive models.\nFrom the perspective of machine learning, many models were proposed to model feature interactions, especially variants of factorization models inspired by FM [7]. FM relies on the inner product between the embedding of the feature-value motivated by the factorization of matrices. Several variants of FM have been proposed to fill the gaps. FFM [8] takes into account the embedding of feature field (i.e. columns in a table of data). AFM [9] considers the weight of the interaction by adding attention coefficients to its model.\nThe success of deep nets in various domains motivates researchers to use them in modeling interactions. For example, Factorization Machine supported Neural Network (FNN) [10] proposed a deep net architecture that can automatically learn effective patterns from categorical feature interactions in CTR tasks. In addition, DeepFM [11] proposed a kind of similar idea but included a special deep net layer that performs the FM task.\nThe attention mechanisms [12] are the method designed to model the importance between different components. In AFM, the attention mechanism was applied because the authors believe that all interactions should not be treated equally likely with respect to prediction values. Attention allows the model to weight feature interactions differently based on their importance. After introducing the attention mechanism to interaction tasks, many works use attention to model the interaction of features based on the factorization machine framework [13]\u2013[17].\nOne of the crucial challenges of interactions in traditional machine learning models is the explainability of which features have interaction, although some of them outperform in prediction performance. So we need an explicit representation that can correspond to pairwise interaction, where a graph is a representation that can fill this gap. In addition, GNN is a tool that can be used to learn information from graphs.\nB. Graph Neural Networks for Feature Interactions\nGNN are becoming more interesting for use in feature interaction problems. Since feature interactions can be seen as relationships between features, most of the works rely on feature graphs for each individual instance to represent the interactions via aggregation of representation from neighbor nodes.\nMotivated by the click-through rate (CTR) prediction, which is believed to influence the rate of clicking on a product, such as age and gender, many works attempted to model this prediction by using GNN applied feature graphs. To the best of our knowledge, Fi-GNN [1] is the first work to use GNN to model the interaction of features in the feature graph for the prediction of CTR. It applied the field-aware embedding layer to compute the latent representation of nodes of features before feeding the feature graph with the computed representations into a stack of message passing layers.\nAfter the introduction of Fi-GNN, many subsequent works have built on this idea. Cross-GCN [18] used a simple graph convolutional network with cross-feature transformation. GraphFM [19] seamlessly combined the idea of FM and GNN using the neural matrix factorization [20] based function to estimate the edge weight. It also adopts the attentional aggregation strategy to compute feature representation. Table2Graph [21] applied attention mechanisms to compute the probability adjacency matrix instead of edge weight. It also used the reinforcement learning policy to capture key feature interactions by sampling the edges within feature graphs.\nAll mentioned works chronically improve the methods of feature interaction learning. However, no work provides information about the effect of the construction of feature graphs on the learning capacity. Most of the works utilize attention mechanisms or edge weights to let models learn graph structures via these parameters."}, {"title": "IV. PROBLEM FORMULATION: PAIRWISE INTERACTION FEATURE GRAPH PROBLEM", "content": "An important open research question in graph representa- tion for data without explicit connections is that What is a suitable graph structure? While current research highlighted the potential of feature graphs, a feature graph that is too dense, for example, a complete graph, may lead to learning issues, as shown and discussed in previous work [1], [22], [23] and in Section VII-E. Thus, finding an optimal feature graph structure for any particular prediction task is the common"}, {"title": "V. THEORETICAL RESULTS", "content": "There are two questions that we should address. The first is \u201cIs the space of the functions and the space of feature graphs a correspondence?\u201d It is significant because we aim to guarantee that we can use some graph to represent any of the given functions and vice versa. The second question is that of \u201cWhy do we need sparse graphs rather than complete graphs?\u201d Here, we consider the second problem to be the problem of model selection according to the concept of MDL. Proofs of results in this section are in the Appendix.\nA. Correspondence Problem\nIt is worth considering the correspondence between expressions and graph structures. Even though we primarily focus on pairwise interactions, in this correspondence, we go towards the arbitrary number of variables interacting, not only pairwise. For convenience of stating, we discuss only the multiplicative interactions, which is a special case of the non-additive interaction. Throughout this, let us call the kind of expression disjoint multilinear interaction which is defined as follows:\nDefinition 1 (Disjoint Multilinear Interaction Expression). The predictive model $y(x)$ is said to be a disjoint multilinear interaction expression (DMIE) if it can be expressed as\n$y(x) = \\sum_{i=1}^{n} \\prod_{j=1}^{m_i} x_{ij}$\nwhere $x_{ij}$ and $x_{k_j}$ are distinct variables when $i \\neq k$ or $j \\neq l$. We denote by $DM(n)$ the set of all DMIE expressions of $n$ variables.\nMultiple graphs can infer the same expression considering the perspective of connected components. For example, the expression $x_0x_1x_2 + x_3x_4 + x_5 + x_6$ corresponds to the graphs $E_1 = \\{\\{0, 1\\}, \\{0, 2\\}, \\{1, 2\\}, \\{3, 4\\}\\}$ and also $E_2 = \\{\\{0, 1\\}, \\{1, 2\\}, \\{3, 4\\}\\}$. Consequently, we use equivalence classes of graphs (based on connectedness) instead of individual graphs. To this end, we define an equivalence class of feature graphs.\nDefinition 2. We define a binary relation $\\sim$ in $G(n)$ as follows: for any graph $G_1, G_2 \\in G(n)$, $G_1 \\sim G_2$ if for any $i, j \\in N_n$, they are reachable in $G_1$ if and only if they are reachable in $G_2$.\nObviously, $\\sim$ is an equivalence relation to $G(n)$. We then denote the set of equivalence classes from $(G(n), \\sim)$ by $\\mathcal{G}(n)$:\n$\\mathcal{G}(n) := \\{\\[G]\\_\\sim : G \\in G(n)\\}$\nAs a result, we can prove the correspondence, as presented in Theorem 3.\nTheorem 3. $DM(n)$ one-to-one corresponds to $\\mathcal{G}(n)$.\nB. MDL for graph representation of pairwise interaction\nThis problem can be considered as a model selection problem for selection of representations. We adopt the idea of Minimum Description Length (MDL) to shape the problem in the manner of selection of graph representation. Generally speaking, a selected feature graph should not be so dense that it captures an uninformative characteristic. On the other hand, it should not be such a light graph that the informative edges to learn interactions are omitted. In this work, we focus on the pairwise interaction case, which is formalized below.\nGiven a graph $G = (V, E)$ whose nodes are of degree at most 1, a function containing pairwise interaction induced from $G$ is defined as\n$f_G(x) = \\sum\\_{deg(i)=0} c\\_i x\\_i + \\sum\\_{\\{i,j\\} \\in E} c\\_{ij} x\\_i x\\_j$\nNote that we represent pairwise interaction by bilinear interaction for convenience in describing by mathematical notation. Therefore, the problem of selecting the graph for this function $f_G$ is framed as Problem 1.\nProblem 1 MDL Pairwise Interaction Graph Problem******\nInput: A dataset $S = \\{\\(x\\_i, Y\\_i\\)\\}\\_{i=1}^n$, and a pre-configured GNN model $F : G \\times R^d \\rightarrow R$\nOutput: The edge set $E^*$ for an input feature graph $G^*(V, E^*) \\in G$ so that\n$G^* = arg\\min\\_{G \\in G} L(f\\_G) + L(S|f\\_G)$\nIn this problem, we denote $L(S, f\\_G) = L(S|f\\_G) + L(f\\_G)$, where\n$L(f\\_G) = L\\_R(|E(G)|) + \\sum\\_{\\text{deg}(i)=0} L\\_R(c\\_i) + \\sum\\_{\\{i,j\\} \\in E(G)} L\\_R(c\\_{ij})$\n$L(S\\vert f\\_G) = \\sum\\_{i=1}^{n} L\\_R(x\\_i) + \\sum\\_{i=1}^n L\\_R(Y\\_i - f\\_G(x\\_i))$\nWe denote by $L\\_R: R \\rightarrow N$ a function of the number of bits $L\\_R(x)$ we need to encode a real number $x$. In the next part,"}, {"title": "VI. METHODOLOGY", "content": "Our experiment provides empirical evidence regarding the relationship between feature graph structures and pairwise interactions in datasets. We train and evaluate several feature graph structures for each dataset and report the results. However, due to the combinatorial number of possible feature graph structures and the stochastic nature of the learning algorithm, we adopt a strategy of generating random edge sets to explore various graph configurations, as illustrated in Figure 2.\nA. Datasets\nReal-world datasets rarely provide information about inter- actions between features, which disables our analysis. To this end, we use synthesized datasets to control interaction characteristics in datasets such as the number of interaction pairs of features, strength of interactions, or kinds of interactions. We assume that there is only one mode of interaction per feature graph. If real-world datasets had many interaction modes, we could apply weighted ensemble feature graphs proposed Li et al. [17]\nConstruction of the simulated dataset (the first arrow in Figure 2) can be formally described as follows. Let $f: R^d \\rightarrow R$ be a hidden ground truth function containing pairwise interaction terms. A synthetic dataset of n instances with respect to f if $\\{(X\\_i, f(X\\_i) +0.1\\varepsilon\\_i)\\}\\_{i=1}^n$ where $X\\_i \\sim N(0,1)$ and $\\varepsilon\\_i \\sim N(0, \\text{var}(f(X)))$. We varied the number of features and interaction pairs to aid our analyses. Note that for each function f, we generated one set of data and used it for all variations of feature graphs.\nB. Feature graphs\nFor a given function $y = f(x\\_1, x\\_2,...,x\\_d)$ of d features, a set of feature graphs is constructed by randomly assigning edges between feature nodes (see Figure 2 after the second arrow). In other words, given a dataset of d features and n samples $\\{(Z\\_i, Y\\_i)\\}\\_{i=1}^n$ and an edge set E for a graph of d nodes, the feature graph dataset of edge set E is a graph dataset $\\{(G(V, E, \\text{concat}(x, X)), y\\_i)\\}\\_{i=1}^n$, where X is a learnable embedding for a feature graph for nodes. The initial node embedding matrix is denoted by\n$H^{(0)} = \\text{concat}(x, X).$"}, {"title": "C. GNN Architectures", "content": "Our GNN model utilizes transformer convolution layers (TransformerConv [24]) for the message passing function, incorporating attention mechanisms to capture the strength of pairwise interactions, as depicted in Figure 3. Then, a mean pooling layer averages the embedding of all nodes for the prediction layer.\nAs Fi-GNN does, this work adopts the idea from Fi-GNN, but simpler architecture. We use TransformerConv [24] which is a message-passing layer based on the attention mechanism whose feature interaction learning capability is shown by many works. For the computation, we start from the initial node embedding $H^{(0)}$ defined in Equation 9. Then a feature graph with the node embedding matrix is fed to TransformerConv with BatchNorm:\n$Z^{(1)} = \\text{TransformerConv}(H^{(1-1)}, E),$\n$H^{(1)} = \\text{ReLU}(\\text{BatchNorm}(Z^{(1)})),$\nwhere $H^{(1)}$ is an intermediate node embedding matrix at the layer l. After applying all message passing layers, we calculate the average pooling of node embeddings from the last GNN layer, and we use this vector as a graph embedding which is fed into the projection layer to get prediction:\n$\\overline{p} = \\frac{1}{|V|} \\sum\\_{i} H^{(L)} \\[i\\],\\$\n$\\hat{y} = w \\overline{p} + b,$\nwhere $H^{(L)}$ is a node embedding matrix from the last GNN layer and $H^{(L)} \\[i]$ is the row i of the matrix which corresponds to the node embedding of the node i and b is a biased term. Equation 11 is the calculation of mean pooling layer and Equation 12 is the final projection to calculate the prediction value.\nEquation 10 is a bit different from the original work [24] which uses LayerNorm after aggregation of message like as the original transformer in NLP. In the first phase of experiment, we found that the GNN model with LayerNorm cannot capture interaction behavior while BatchNorm can.\nIn training the GNN model, we adopt an usual supervised learning approach, where the loss function measures the dif-"}, {"title": "VII. EXPERIMENTS AND RESULTS", "content": "Results shown in this section are from the dataset generated by the equation $f(x) = \\sum_{i=1}^{p} x\\_{2i-1}x\\_{2i} + \\sum_{i=1}^{q} x\\_{2p+i}+\\epsilon$. We present the experiment results in the following insights:\n1) The efficient structures of feature graphs (Section VII-B)\n2) The dependency between edges and interaction pairs (Section VII-C)\n3) The relationship between multi-hops and number of message passing layers (Section VII-D)\n4) Scalability of efficient feature graphs (Section VII-E)\nSince Section VII-B - VII-D consider the variants of graph structures w.r.t. the given dataset, we show only results from an equation containing 2 interaction terms and 2 unary terms, i.e. $p = 2, q = 2$. In Section VII-E, p is varied while q = 2.\nA. Experimental Setting\nSynthetic dataset generating: Each synthetic dataset con- tains 10,000 samples generated by distribution described in Methodology section (Section VI). In every randomization, we use seed number n for feature in and seed number 0 for noise term. We split it into first 7,000 rows for training and next 3,000 rows for testing sets.\nRandom sampling of feature graph structures: Ideally, we want results of all possible feature graph structures. However, there are up to $O(2^{n^2})$ feature graphs for a dataset of n features. This leads us to the combinatorial issue, so we set up an experiment by sampling in random stratified by (1) the number of edges, (2) the number of interaction edges, (3) the number of non-interaction edges, and (4) the number of hops between interacting nodes. Since we also want to study about subgraphs with respect to existence of interaction edges (for example, edges {0,1} and {2,3} for the dataset generated by $y = x\\_0x\\_1 + x\\_2x\\_3 + x\\_4 + x\\_5$), we want these collections (1) graphs having both edges, (2) graphs having \\{0,1\\} but not \\{2,3\\} (3) vice versa and (4) graphs having no these edges. To have full collections to compare, once any feature graph is sampled, all other three graphs are also sampled.\nB. Interaction Edges vs. Non-interaction Edges\nWe are first interested in the existence of interaction edges and that of non-interaction edges. Intuitively, the existence of interaction edges should be significant for learning the interaction between features by attention-based GNNs since they allow paasing of the message between their associated nodes to model interactions.\nFigure 4 illustrates the average MAE in predicting each number of edges without interaction along the x axis. Each line corresponds to the number of interaction edges. We see that among the graphs that have the same number of non- interaction, feature graphs containing all interaction edges give significantly lower errors than the graphs missing some interaction edges give for all of the number of non-interaction edges. Especially, we also see that even though graphs with all interaction edges contain any number of other noise edges, they still give good performance compared to the entire line of the graphs missing some interaction edges. From this result, we may infer that the interaction edges are the most important in input feature graphs to learn a dataset containing pairwise interaction. Although we do not know how to construct feature graphs used by GNN models for the task that we believe that there must be interaction terms, at least the interaction edges (if we know) should be kept.\nThis result also tells us that when the feature graph can maintain all interaction edges, the increasing number of non- interaction edges leads to poorer performance. It seems that the non-interaction edges make a model capture non-underlying information in a dataset and interfere with the learning of pairwise interactions of interaction edges.\nHowever, if we consider the results of the 0-interaction- edge and 1-interaction-edge graphs, we notice a different trend from the 2-interaction-edge graphs. It starts with the worst result when there are no edges in a graph. In this case, the"}, {"title": "VIII. CASE STUDY: CLICK-THROUGH-RATE PREDICTION", "content": "We observed that it is ideal to include all known interac- tion edges in feature graphs. Non-interaction edges can be considered noise, capturing irrelevant information. When all interaction edges cannot be preserved, adjacency constraints can be relaxed to focus on reachability or connectedness, aided by more message passing layers. In this section, we move from simulated data to the real world problems. One of the real world problems that interactions between features are often concerned is the click-through rate (CTR) problem as a recommendation system in various application.\nIn this experiment, we utilized the Criteo dataset [25] to evaluate the performance of different graph structures using the AUC score as the primary metric.\nFiGNN, a widely recognized graph model for CTR predic- tion, achieves an AUC score of 0.8062. Initially, we generated random sparse graphs with varying percentages of connections and evaluated their performance using a simple GNN model. The optimal AUC score for randomly constructed graphs was approximately 0.8072, achieved with 24% graph connections. Subsequently, we experimented with a fully connected graph, utilizing 100% of the graph connections. The AUC score for the fully connected graph was 0.8073. Additionally, we manually constructed a sparse graph inspired by the GDCN heatmap [26]. This manually constructed graph achieved an AUC score of 0.8076 with only 13% of the connections."}, {"title": "IX. CONCLUSION AND FUTURE WORK", "content": "In this study, we investigated how the structure of feature graphs relates to their capacity to represent pairwise feature interactions within GNNs. We offered both theoretical and empirical insights on the development of sparse feature graphs. Our findings reveal that interaction edges are crucial, whereas non-interaction edges can introduce noise, which hinders the learning process of GNN models. From a theoretical stand- point, employing the Minimum Description Length (MDL) principle, we showed that feature graphs that retain only essential interaction edges provide a more efficient and inter- pretable representation compared to complete graphs. Empir- ical tests on synthetic datasets underscored the significance of interaction edges for improved prediction performance, while non-interaction edges were found to introduce noise that reduces model accuracy. Additionally, we discovered that the connection between interacting nodes, even without direct interaction edges, can enhance learning via multi-hop message passing in GNNs.\nOur findings emphasize the significance of feature graph sparsity for both computational efficiency and prediction accu- racy, particularly in large-scale datasets where complete graphs become useless. Furthermore, we validated the idea of our insights through a case study on click-through rate prediction, further highlighting the practical value of constructing well-designed feature graphs for real-world tasks.\nFor future work, our aim is to extend this study by:"}]}