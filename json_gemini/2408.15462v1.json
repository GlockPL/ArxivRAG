{"title": "CTRQNets & LQNets: Continuous Time Recurrent and Liquid Quantum Neural Networks", "authors": ["Alejandro Mayorga", "Alexander Yuan"], "abstract": "Neural networks have continued to gain prevalence in the modern era for their ability to model complex data through pattern recognition and behavior remodeling. However, the static construction of traditional neural networks inhibits dynamic intelligence. This makes them inflexible to temporal changes in data and unfit to capture complex dependen-cies. With the advent of quantum technology, there has been significant progress in creating quantum algorithms. In recent years, researchers have developed quantum neural networks that leverage the capabilities of qubits to outperform classical networks. However, their current formulation exhibits a static construction limiting the system's dynamic intelligence. To address these weaknesses, we develop a Liquid Quantum Neural Network (LQNet) and a Continuous Time Recurrent Quantum Neural Network (CTRQNet). Both models demonstrate a significant improvement in accuracy compared to existing quantum neural networks (QNNs), achieving accuracy increases as high as 40% on CIFAR 10 through binary clas-sification. We propose LQNets and CTRQNets might shine a light on quantum machine learning's black box.\nKeywords: Liquid Networks, Quantum Computing, QNNs, CTRNets, LQNet, CTRQNet", "sections": [{"title": "1 Introduction", "content": "Machine learning is the development of intelligence through mathematical tools. This field has garnered increasing media attention over the past two decades due to the scalability of machine learning methods [1]. While large language models such as ChatGPT [2-4] are the most publicized applications of machine learning, they do not encompass the entire field. Moreover, machine learning itself is merely a sub-field of the more well-known field of arti-ficial intelligence, which is more broadly concerned with developing \u201cintelligence\u201d through either biological processes (synthetic biological intelligence) [5\u20138] or mathematical and com-puter processes (machine learning). While both methods show promise, the rich history of mathematics provides a deeper understanding of how intelligence may be simulated. Specifically, neural networks [9, 10] are notable for their ability to learn abstract patterns even in large datasets, a property that does not extend to other methods, such as support vector machines (SVMs) [11].\nDeep learning is the development of algorithms that are closely related to neural networks. Examples of this are seen in many real-world applications [See [2,3,12,13]]. These models only depend on one set of parameters at any given time, which is key to their scal-ability. Additionally, the sheer size of the parameters that characterize these models makes understanding what these networks truly \u201cknow\u201d an enigmatic task. Thus, controlling and assessing their capabilities becomes difficult [14,15].\nIn 1993, a continuous-time recurrent neural network (CTRNN) developed by [16] proved that \"that any finite time trajectory of a given n-dimensional dynamical system can be approximately realized by the internal state of the output units of a continuous time recurrent neural network with n output units, some hidden units, and an appropriate initial condition.\u201d CTRNNs have been applied to evolutionary robotics, where they have been used to address vision, cooperation, and minimal cognitive behavior [17].\nHowever, the recent development of \u201cliquid models\" [18] has made this task much more feasible. The backbone behind the construction of liquid models lies in the fact that building a model with dynamic intelligence and task awareness will vastly simplify controlling and assessing the model's knowledge. For example, a car was successfully operated using only 19 neurons of a liquid neural network, whereas traditional LSTMs would require millions of neurons to perform this same task [19]. Liquid models have a much closer representation of what is understood to be intelligence, as these models are dynamic and task-aware even past the training process."}, {"title": "1.2 Our new ideas", "content": "Liquid networks [18] and continuous time recurrent neural networks (CTRNets) [16] have shown great promise in displaying dynamic intelligence, a feature that classical networks lack. While these models have shown great promise in current literature, no work has been done to investigate a continuous hidden state in quantum neural networks (QNNs). The current development of QNNs has shown to be superior, theoretically, to classical networks. However, these models still admit a rigid hidden state leading to static intelligence."}, {"title": "2 Preliminaries", "content": "To address these issues, we fuse these continuous networks and quantum neural networks, introducing Liquid Quantum Networks (LQNets) and Continuous-Time Recurrent Quantum Neural Networks (CTRQNets). Firstly, we define a quantum residual block from [20] to construct a residual block to mitigate the vanishing gradient. Inspired by [13], we derive a new model for quantum neural ordinary differential equations, which we then use to formulate an LQNet and CTRQNet. These models surpass quantum neural networks by capturing temporal dependencies and exhibiting dynamic intelligence through an ever-changing set of differential equations governing there hidden state. On CIFAR-10, our models achieved a 40% increase in accuracy compared to QNNs."}, {"title": "2.1 Multi-layer Neural Networks", "content": "We define a neural network as follows:\nDefinition 2.1. If $F(x)$ be a composition from the set ${f_n}$ where ${f_n}$ denotes a finite set of differentiable functions on R, then F is a neural network.\nHowever, this definition only considers static neural networks, not cases where hidden states are solutions to differential equations. Mathematically defining networks with these implicit hidden states is challenging due to the various methods of forward and backward propagation used in different architectures [13,18,21,22]. For example, [18] uses a backprop-agation through a time training algorithm, whereas [13] uses an adjoint method with the ladder being the continuous counterpart of the former. These formulations are mathemati-cally different and, therefore, cannot be encompassed under the same definition. We omit a definition for these types of networks but define dynamical, continuous, liquid hidden states to be hidden states where the corresponding transformation is the solution to a differential equation.\nThe current literature has significant room to explore continuous dynamics in quantum neu-ral networks as their static construction resembles that of their classical counterparts. This static construction is sub-optimal for real-world applications where data may be irregularly spaced, leading to quantum networks' inability to learn from this type of data in their current formulation. By replacing the static hidden states in quantum networks with a dy-namic liquid structure, these newly formulated quantum models, which we call CTRQNets and LQNets, are able to adapt to various types of data that quantum neural networks strug-gle to adapt to. These models exhibit dynamical intelligence and are task-aware even after the training process is complete, so the knowledge of these models can be easily accessed."}, {"title": "2.2 Residual Learning", "content": "A common issue seen in deep neural networks is the degradation of accuracy as the number of layers increases. This issue is not due to overfitting, as deeper networks report lower training accuracy than shallower ones on certain tasks [23]. To address this, [23] proposed residual networks where layers learn the residual between the optimal hidden state"}, {"title": "2.3 Neural Ordinary Differential Equations (NODES)", "content": ""}, {"title": "2.3.1 Neural Ordinary Differential Equations", "content": "Consider a residual block of the form\n$h_{t+1} = h_t + f(\\theta_t, h_t, t)$\nwhere $h_t$ denotes the t-th residual block within the layer and $\\theta_t$ denotes the model parameters corresponding to the t-th residual block within the network. Residual networks are discrete function approximators as they lack a continuous structure. Discrete neural networks i.e. residual networks have 2 main disadvantages:\n1. They lack the adaptability to process non-rigid data (time-series data with non-constant time spacing).\n2. They lack dynamic intelligence and cannot self-adjust to unfamiliar patterns due to the fact that the hyperparameters are fixed."}, {"title": "2.3.2 Numerical Methods", "content": "In 2018, the authors of [13] formulated a solution to this problem, which showed great promise for future work. Euler's method for solving ordinary differential equations (ODES) states that given\n$\\frac{dy}{dt} = f(t, y)$\nsubject to the initial condition $y(t_0) = y_0$ an approximation of the solution is given by\n$y_{n+1} = y_n + hf(t_n, y_n)$\nwhere h is the step size and $(t_n, y_n)$ lies on the approximation curve. If we consider h = 1, the above equation resembles that of a residual block. In theory, by adding more layers, we allow for a continuous neural network where there exist arbitrary layers, i.e., the network is defined at arbitrary layers. Such a network resembles a continuous function f such that\n$\\frac{dz(t)}{dt} = f(z(t), t, \\theta)$  (2.1)\nIf z(0) denotes the input layer of the network and z(T) denotes the output layer of the network, we have\n$\\int_0^T f(z(t), t, \\theta)dt + z(0) = z(T)$.\nIn order to compute the forward propagation of the network, we solve Equation 2.1. The hidden state is not predefined but rather dependent on the specific sample. Thus, the network is task-aware, inhibited by the static construction of residual networks.\nThere exist a variety of methods to perform gradient calculation, but the authors of [13] used a method formulated in [21], which makes the calculation of these values explicit and preserves the continuous structure of Neural ODEs. While the adjoint rapidly performs gradient calculation, many intermediate optimization steps are taken due to the flexibility of ODE Solvers [21,22], slowing the training process. In most cases [18,19,24], backpropagation through time is used, which is faster and more accurate in practice.\nWhen minimizing loss function $L$ at $z(T)$, $L$ depends not only on $z(T)$ but on all $z(t)$ for $t \\in [0,T]$. Neural ODEs have continuous depth, making direct backpropagation through these layers impossible. Instead, we consider partitioning the interval $[0,T]$ and updating the function throughout these intervals. Adaptive ODE solvers, such as Runge-Kutta and Bogacki-Shampine methods [22,25], adjust intervals based on the smoothness of the solution. Larger steps are taken in smooth regions where the accuracy in the region is not critical towards the final solution, while smaller steps are used in volatile regions where accuracy is critical [22, 25, 26].\nHowever, these ODE solvers cannot calculate the gradients directly, and using an ODE Solver to calculate the gradient would be very inefficient as it would have to solve the implicit differential equation:\n$L(z(t_1)) = L(z(t_0) + \\int_{t_0}^{t_1} f(\\theta, z(t), t)dt))$"}, {"title": "2.4 Time Continuous Recurrent Neural Networks", "content": "Continuous Time Recurrent Neural Networks (CTRNNs) had its first formulation [16] demonstrating the ability to approximate any continuous curve to any degree of accuracy. Like Recurrent Neural Networks, the hidden states of CTRNNs depend on time. A CTRNN is defined by the following differential equation:\n$\\frac{dx}{dt} = -\\frac{1}{\\tau} x(t) + W \\sigma(x(t))$   (2.2)\nTo perform the forward pass of the network, the differential equation above is solved in the interval $t = t_0$ to $t = t_1$, subject to the initial condition of our input. In general, letting $I(t)$ denote the input to the CTRNN at time t, we solve Eq. (2.1) in the interval $[t_0, t_1]$ subject to the initial condition $x(t_0) = I(t_0)$ for which $\\sigma$ denotes a nonlinear activation function that is continuous and bounded (e.g., sigmoid). CTRNNs are more expressive than traditional networks due to the system of differential equations underlying their hidden state. These differential equations can be optimized through various numerical differential equation solvers, allowing CTRNNs to trade speed for accuracy in regions where it is necessary.\nThe following theorems were proven in [16], creating the catalyst for more advanced research of differential equations and structure machine learning model structures. Theorems 2.2 and 2.3 below establish that the internal state of the output units of a CTRNN can approximate a finite time trajectory of a given dynamical system to an arbitrary precision:\nTheorem 2.2. Let $\\sigma$ be a strictly increasing $C^1$-sigmoid function such that $\\sigma(\\mathbb{R}) = (0, 1)$, and let $f: I = [0, T] \\rightarrow (0, 1)^n$ be a continuous curve, where $0 < T < \\infty$. Then, for an arbitrary $\\epsilon > 0$, there exists an integer N and a recurrent neural network with n output units and N hidden units such that\n$\\max_{t \\in I} ||f(t) - y(t)|| < \\epsilon$,\nwhere $y(t) = (\\sigma(y_1(t)), ..., \\sigma(y_n(t)))$ is the output of the recurrent network with the sigmoid output function $\\sigma$.\nTheorem 2.3. Let $f: I = [0, T] \\rightarrow \\mathbb{R}^n$ be a continuous curve, where $0 < T < \\infty$. Then, for an arbitrary $\\epsilon > 0$, there exist an integer N and a recurrent network with n output units and N hidden units such that\n$\\max_{t \\in I} ||f(t) - u(t)|| < \\epsilon$,\nwhere $u(t) = (\\sigma(u_1(t)), ..., \\sigma(u_n(t)))$ is the internal state of the output units of the network.\nBackpropagation through automatic differentiation is memory intensive due to the need for values across every point in time to be recalculated at each update of the hidden state. Therefore, gradients are instead computed numerically at the final time step, then, through another ODE solver such as the Adjoint method [21], the gradients at all points in time are calculated. In cases such as with Neural ODEs and LNNs [13,18], it is possible to derive"}, {"title": "2.5 Liquid Neural Networks", "content": "Liquid neural networks [18] were formulated through a neuron-spiking biological process. This process was integrated into the development of CTRNNs, allowing them to be more dynamic and providing benefits similar to NODEs [13]. Formally, the hidden state of these liquid networks is formulated as follows:\n$\\frac{dx(t)}{dt} = -[\\frac{1}{\\tau} + f(x(t), I(t), t, \\theta)] x(t) + f(x(t), I(t), t, \\theta)A$,    (2.3)\nwhere $A$ is a parameter and $\\tau$ denotes a time constant that aids the network in maintaining a numerically stable state.\nDespite the formulation's resemblance to NODEs and CTRNNs, the adjoint method is not used for gradient calculation here since the differential equation that governs the hidden state is a set of stiff differential equations leading to an exponential increase of discretization steps when numerically simulating the equation using a Runge-Kutta-based integrator [26]. Hence, a new type of numerical integrator is necessary, for the adjoint method commonly implements a Runge-Kutta-based Dortmund Prince integrator [13, 18]. [18] introduces the Fused solver, a combination of the implicit and explicit Euler Solver. This fused solver discretizes the interval [0,T] as\n$x(t + \\Delta t) = \\frac{x(t) + \\Delta t f(x(t), I(t), t, \\theta) A}{1 + \\Delta t (\\frac{1}{\\tau} + f(x(t), I(t), t, \\theta))}$    (2.4)\nBackpropagation through time can be applied as the solver is unrolled throughout time, streamlining the gradient calculation. These liquid networks offer enhanced expressiveness compared to Neural ODEs and CTRNNs due to the increased complexity of the hidden state. The theorems below demonstrate that the proposed LTC in [18] satisfies a universal approximation property (Theorem 2.4) and that the time constant and the state of the neurons remain within a finite range (Theorem 2.5):\nTheorem 2.4. Let $x \\in \\mathbb{R}^n$, $S \\subset \\mathbb{R}^n$, and $\\dot{x} = F(x)$ be an autonomous ODE with $F: S \\rightarrow \\mathbb{R}^n$ a $C^1$-mapping on S. Let D denote a compact subset of S and assume that the simulation of the system is bounded in the interval $I = [0, T]$. Then, for a positive $\\epsilon$, there exists an LTC network with N hidden units, n output units, and an output internal state u(t), described by Eq. 1, such that for any rollout ${x(t) | t \\in I}$ of the system with proper network initialization of initial value $x(0) \\in D$,\n$\\max_{t \\in I} ||x(t) - u(t)|| < \\epsilon$.\nTheorem 2.5. Let $x_i$ denote the state of neuron i within an LTC, identified by Eq. 1, and let neuron i receive M incoming connections. Then, the hidden state of any neuron i, on a finite interval $I_{nt} \\in [0, T]$, is bounded as follows:\n$min(0, A_{min}) \\leq x_i(t) \\leq max(0, A_{max})$"}, {"title": "2.6 Quantum Computing", "content": "Quantum computing leverages the principles of quantum mechanics in order to perform operations beyond the scope of classical computers. The necessity for quantum computers is more practically illustrated by Moore's Law, which states that the number of transistors in an integrated circuit will double approximately every other year, continuously reducing the size of transistors in circulation. When dealing with objects as small as our current transistors, quantum tunneling phenomena inhibit their functionality. Physical theory in quantum mechanics allows us to model these microscopically small objects and develop more efficient and robust computers. The probabilistic nature of quantum mechanical theory offers additional benefits when applied to building computers that classical mechanics theory does not. At the backbone of all of quantum mechanical theory is the Schrodinger equation [27], which states that for a given quantum mechanical system, its wave equation follows the partial differential equation below:\n$i\\hbar \\frac{\\partial}{\\partial t} \\Psi(x, t) = -\\frac{\\hbar^2}{2m} \\nabla^2 \\Psi(x, t) + V(x, t) \\Psi(x, t)$ (2.5)\nwhere\n\u2022 i is the imaginary constant\n\u2022 \u0127 is Planck's constant\n\u2022 \u03a8(x, t) denotes the wave function of the system\n\u2022 \u22072 denotes the Laplacian of the system\n\u2022 V(x, t) is the potential energy of the system\nIn most applications of quantum computing, the quantum mechanical system is realized by tracking the evolution of atoms over time. The superposition of these systems is critical for quantum computing as operations can be done to arbitrary states of the system simultane-ously without physically applying the transformation to all possible states of the system. The quantum analog of the classical bit is the qubit. Physically, it is realized through a quantum mechanical system in which certain operations may be applied to the systems, i.e., a rotation along an axis, without knowing the state of the system. While the theory of quantum com-puting is based on quantum mechanics, the details generally rely on the analysis of the qubit.\nWhile a qubit can be in a superposition of states, upon measurement, it must collapse to a basis state, which we label |0\u27e9 or |1\u27e9. It is impossible to calculate deterministically which state a qubit will collapse into. We can, however, determine the probabilities for each possibility through the wave function. Quantum machine learning aims to alter these probabilities by altering the state of the function. Through a series of operations in which the parameters are controlled, it is possible to create a state where a qubit will necessarily collapse into a desired state upon measurement.\nMathematically, a qubit is an element of a Hilbert space. In order to build a composite"}, {"title": "3 Theoretical Framework", "content": "[20] proposed a new framework for a quantum residual block in which they define a quantum residual operator applied to the entangled state $|\\psi_0\\rangle \\otimes |0\\rangle$. We utilize this approach to derive a quantum liquid network. A traditional residual block encoded into a quantum neural network is of the form:\n$|\\Psi_{t+1}\\rangle = |\\Psi_t\\rangle + U|\\Psi_t\\rangle$ .  (3.1)\nThis approach does not retain the benefits of quantum computing. Instead, the authors of [20] present a quantum residual block that leverages the probabilistic nature of quantum computing to further enhance the capability of a residual block. Let $\\mathcal{H}_1 = \\mathbb{C}^{2^n}$ and $\\mathcal{H}_0 = span {|0\\rangle}$ denote Hilbert spaces. Consider an initial quantum state $|\\psi_0\\rangle \\in \\mathcal{H}_1$ and an auxiliary qubit $|0\\rangle \\in \\mathcal{H}_0$ as illustrated in Figure 3. The implementation of the quantum residual operator is realized in the subspace of the ancilla qubit. Once the residual operator is applied to the initial state, one can then obtain the results within the subspace $\\mathcal{V} = {|x\\rangle\\langle x| \\big| |x\\rangle \\in \\mathcal{H}_0}$ [20].\nInspired by [20], we develop an LQNet and a CTRQNet. First, we apply either a Hadamard gate to $|0\\rangle$ or a corresponding transformation to the state $|\\psi_0\\rangle$ depending on whether we are in the encoding stage or parameterized stage. We then apply a CNOT gate to the qubits using $|0\\rangle$ as the control qubit and $|\\phi_0\\rangle$ as the target qubit. From there, we apply another Hadamard to the quantum state whose output is measured in the auxiliary qubit subspace $\\mathcal{H}_0$. The result of this operator is then added to the quantum state lying in the subspace of the first qubit. A final operation is performed on the state lying in $\\mathcal{H}_0$ to re-alize the residual connection. Rewriting in a more succinct notation, we propose a Quantum Liquid Neural Network as follows :\nConsider the entangled state $|\\psi_0\\rangle \\otimes |0\\rangle$. Then the output of the residual connection in the encoding stage(Eq. 3.1) can be rewritten in the following form:\n$F(|\\psi_t\\rangle) = tr_{\\mathcal{H}_0} (\\psi_t|F^{\\dagger} (|\\phi_0\\rangle \\otimes |0\\rangle)F|\\psi_t)$, (3.2)\nwhere $F: \\mathcal{H}_1 \\rightarrow \\mathcal{V}$ denotes the action of the quantum residual block acting on classical data $|x\\rangle \\in \\mathcal{H}_0$ mapping to the subspace $\\mathcal{V} = {|x\\rangle\\langle x| \\big| |x\\rangle \\in \\mathcal{H}_0}$. We can think of the mapping F as a sequence or composition of unitary operators\n$F \\triangleq (H \\otimes U) \\circ (CNOT) \\circ (H \\otimes I)$, (3.3)\nwhere for the initial state $|\\psi_0\\rangle$, $U \\triangleq |\\phi_0\\rangle \\langle \\phi_0|$, CNOT denotes the controlled-not gate, H denotes the Hadamard gate, and I denotes the identity matrix. Note that F is unitary by construction. Similarly, we define $F: \\mathcal{H}_1 \\rightarrow \\mathcal{V}$ by\n$F \\triangleq (H \\otimes W(\\theta)) \\circ (CNOT) \\circ (H \\otimes I)$,  (3.4)\nwhere, as in [20], we replace the encoding operator with the parameterized gate $W(\\theta)$ while the remaining gates remain unchanged. Let\n$\\mathcal{F}(|\\psi_t\\rangle, \\theta) = tr_{\\mathcal{H}_0} (\\psi_t|F^{\\dagger}(|\\phi_0\\rangle \\otimes |0\\rangle)F|\\psi_t)$, (3.5)"}, {"title": "4 Results", "content": "where $|\\psi_t\\rangle$ denotes the evolution of initial quantum state $|\\phi_0\\rangle$ on layer t. $\\mathcal{F}(|\\psi_t\\rangle, \\theta)$ is equivalent to $W(\\theta) |\\phi_0\\rangle$, in $W(\\theta)|\\phi_0\\rangle \\equiv W(\\theta)$. With our definition of the residual block $\\mathcal{F}(\\psi_t, \\theta)$, we have that in the limit, a model equation for quantum NODEs as follows\n$\\frac{d\\varphi(t)}{dt} = \\mathcal{F}(|\\psi_t\\rangle, \\theta)$, (3.6)\nwhere $\\varphi(t)$ denotes a continuous function in time, t. The solution of Eq.(3.6), when evaluated at t = T, forms the output of the quantum NODE block. Substitution of this into the equation for liquid networks, we have that\n$\\frac{d\\varphi(t)}{dt} = -[\\frac{1}{\\tau} + \\mathcal{F}(|\\psi_t\\rangle, \\theta)] \\varphi(t) + \\mathcal{F}(|\\psi_t\\rangle, \\theta)$ (3.7)\nThese new equations form our newly created model, hereafter referred to as LQNets. Utilizing the Fused Solver developed by (Hasani et al., 2020) [18], we simulate the differential equation to obtain\n$\\varphi(t + \\Delta t) = \\frac{\\varphi(t) + \\Delta t \\mathcal{F}(|\\psi_t\\rangle, \\theta)}{1 + \\Delta t (\\tau^{-1} + \\mathcal{F}(|\\psi_t\\rangle, \\theta))}$ (3.8)\nFollowing a similar argument, we develop a quantum time-continuous recurrent neural networks (CTRQNet). Recall that a CTRNN is defined by the equation\n$\\frac{dx(t)}{dt} = \\frac{1}{\\tau} x(t) + W \\sigma(x(t))$,\nin which $\\sigma: \\mathbb{R} \\rightarrow (0, 1) \\in \\mathbb{C}^1$ is a sigmoid function in accordance with Theorems 2.2 and 2.3, and W denotes a weight matrix. With Eq. (3.6) defined, we can amend the standard CTRNN equation to read\n$\\frac{d\\varphi(t)}{dt} = -\\tau^{-1} \\varphi(t) + \\mathcal{F}(|\\psi_t\\rangle, \\theta)$, (3.9)\nwhere the quantity $\\mathcal{F}(|\\psi_t\\rangle, \\theta)$ replaces $W(\\sigma(x(t)))$ and is equivalent to the internal state of the output units of the network (cf. Theorem 2.3). This new equation (Eq. 3.9) forms the proposed model for CTRQNets.\nIn this section we implement the 2 newly created models, the LQNet and CTRQNet. In 4.1 we discuss the methodology used throughout the experiments, in 4.2 we discuss the results of our experiments on each of the datasets listed."}, {"title": "4.1 Methodology", "content": "We perform tests of our networks using the following benchmark datasets:\n\u2022 MNIST Binary Classification"}, {"title": "4.2 Baseline Testing", "content": ""}, {"title": "4.2.1 MNIST", "content": "The MNIST dataset consists of grayscale images of handwritten digits from 0 to 9. Our models use 0s and 1s as binary classification. The training set consists of 150 images, the validation set 50 images, and the testing set 2115 images. We present our loss curves below.\nIn Figure 4, both the LQNet and CTRQNet are able to converge to and maintain perfor-mance around an optimal state. Both models' losses approach 0, and the lost curves exhibit no volatility. While the QNN also converges to an optimal state, its minimum loss is roughly 0.3, while the LQNet and CTRQNet achieve a loss of near 0.001. Additionally, the LQNet and CTRQNet reached optimal states in 10 steps, whereas QNNs were not able to reach an optimal state within 50 steps.\nIn Figure 5, the LQNet and CTRQNet achieve maximal performance in terms of accu-racy on the validation set after 10 steps. Both models can maintain optimal performance consistently after first achieving it, as was observed in the loss curves for both models. The QNN takes longer than these models to achieve an optimal state, requiring around 20 steps, but still maintains the consistency seen in the other two networks."}, {"title": "4.2.2 FMNIST", "content": "We analyze our models on the Fashion MNIST dataset through binary classification, using 150 training images, 50 validation images, and 2000 testing images. Every 15 steps in the optimization process, we report the model's performance on the validation dataset, as shown below.\nIn Figure 6, the CTRQNet converges to an optimal state within 195 steps, making it the fastest of the three models to reach an optimal state. On the other hand, the LQNet reaches an optimal state after 300 steps; it may benefit from more training time as it has not fully"}, {"title": "4.2.3 Wisconsin Breast Cancer", "content": "We note that the CTRQNet and LQNet converge in 200-250 steps, while QNNs require 405-420 steps. Both models generalize to the entire dataset effectively, given that a training set of 150 images was sufficient to achieve optimal performance on a testing set of 2000 images. While the LQNet converges rapidly, it displays decreasing but significant volatility over the training process. We attribute this to the overfitting problem that arises from using the classical set of hyperparameters on these quantum models. The CTRQNet is observed to be most effective due to its accuracy and speed of convergence, with the LQNet performing in a similar manner.\nThe Wisconsin Breast Cancer dataset consists of 405 training samples, 50 validation samples, and 114 testing samples.\nIn Figure 8, all three models show a decreasing loss curve. However, the loss curves of the LQNet and CTRQNet indicate that they converge to an optimal solution much more rapidly than the QNN. The QNN reaches a minimum loss of 0.2, whereas the LQNet and CTRQNet models achieve a loss of 0.03. The QNN also had more training steps than the other two models, having 140 steps compared to the LQNet and CTRQNet 55 steps, respectively."}, {"title": "4.2.4 CIFAR 10", "content": "In Figure 9, all three models reached 95% accuracy. However, the LQNet and CTRQNet took a significantly shorter time to converge, taking only 30 and 10 steps respectively. On the other hand the QNN only converges after 80 steps.\nThe CIFAR 10 dataset consists of 200 training images, 100 validation images and 2000 testing images. We use the first two classes of the original dataset: Airplanes and Automo-biles. For this dataset, we do not use preprocessing from pre-trained models."}, {"title": "4.2.5 CIFAR 10 Feature Extraction", "content": "In Figure 10, the LQNet and CTRQNet demonstrate noticeable volatility in their loss curves; neither model can maintain optimal performance for long periods. The LQNet suf-fers a massive spike in its loss after two steps, and although it returns to its previous state and optimizes properly shortly after, it experiences another spike in loss after 23 steps. The CTRQNet optimizes initially but displays a massive upward spike after 13 steps before re-turning to an optimal state immediately afterward. We attribute the high volatility in both models to the difficulty of the task at hand. Meanwhile, the QNN cannot learn relationships underlying the data, which is reflected in its loss curve. After 65 steps, the QNN sees no no-ticeable difference compared to the initial loss. This task is too difficult for the QNN to learn anything, while the LQNet and CTRQNet are both able to learn meaningful relationships.\nIn Figure 11, both the LQNet and CTRQNet show more volatility compared on this dataset in comparison to other baseline tests shown above. While both models are able to converge to an optimal state with satisfactory performance, they are unable to stay in the state consistently. We attribute this to overtraining as we use a classical set of hyperparameters to train these models. Both models achieve a high of 75-80% accuracy on the validation set. The QNN is unable to learn anything meaningful and maintains a constant accuracy of 47%. Our new models are able to learn tasks that prove to be too difficult for QNNs.\nTable 4 shows that the CTRQNet has the best performance across all metrics on the test dataset, followed closely by the LQNet, achieving 75.8% and 70.35% accuracy, respectively. In comparison, the QNN is unable to learn patterns underlying the data, reflected in the QNN's accuracy of 50% due to the QNN outputting the same class across all test samples. We attribute this to the QNN having a rigid hidden state, leading it to learn only the latent representation of the data rather than the patterns underlying it.\nThe CTRQNet and LQNet can learn the CIFAR 10 dataset and converge rapidly to a satisfactory optimal state. Due to the large samples in the CIFAR 10 dataset, all three models learn a latent space representation of the data rather than the direct patterns underlying the data. While the CTRQNet and LQNet achieve satisfactory results when tested, these results can be further improved by letting the models learn the patterns behind the data rather than a latent representation of it. Additionally, we scale the number of parameters in these models since our models, the LQNet and CTRQNet, only utilize 224,000 parameters. In contrast, convolutional neural networks, such as Resnet 151 [23"}]}