{"title": "CTRQNets & LQNets: Continuous Time Recurrent and Liquid Quantum Neural Networks", "authors": ["Alejandro Mayorga", "Alexander Yuan"], "abstract": "Neural networks have continued to gain prevalence in the modern era for their ability\nto model complex data through pattern recognition and behavior remodeling. However,\nthe static construction of traditional neural networks inhibits dynamic intelligence. This\nmakes them inflexible to temporal changes in data and unfit to capture complex dependen-\ncies. With the advent of quantum technology, there has been significant progress in creating\nquantum algorithms. In recent years, researchers have developed quantum neural networks\nthat leverage the capabilities of qubits to outperform classical networks. However, their\ncurrent formulation exhibits a static construction limiting the system's dynamic intelligence.\nTo address these weaknesses, we develop a Liquid Quantum Neural Network (LQNet) and\na Continuous Time Recurrent Quantum Neural Network (CTRQNet). Both models demon-\nstrate a significant improvement in accuracy compared to existing quantum neural networks\n(QNNs), achieving accuracy increases as high as 40% on CIFAR 10 through binary clas-\nsification. We propose LQNets and CTRQNets might shine a light on quantum machine\nlearning's black box.\nKeywords: Liquid Networks, Quantum Computing, QNNs, CTRNets, LQNet, CTRQNet", "sections": [{"title": "Highlights", "content": "In this paper, we create new classes of models, namely the LQNet and CTRQNet, that\ntake advantage of the benefits of quantum computing to analyze data more effectively. These\nnew models are more dynamic and can apply patterns learned from data to broader datasets.\nAs a result, these models can find meaningful patterns in datasets and learn complex rela-\ntionships between data more effectively. Compared to a QNN, LQNets and CTRQNets are\nable to learn faster, reaching an optimal state in less than half of the iterations typical of\nQNNs. For some datasets, LQNets and CTRQNets can find patterns that QNNs cannot,\nallowing for more general usage of AI."}, {"title": "1 Introduction", "content": "Machine learning is the development of intelligence through mathematical tools. This\nfield has garnered increasing media attention over the past two decades due to the scalability\nof machine learning methods [1]. While large language models such as ChatGPT [2-4] are\nthe most publicized applications of machine learning, they do not encompass the entire field.\nMoreover, machine learning itself is merely a sub-field of the more well-known field of arti-\nficial intelligence, which is more broadly concerned with developing \u201cintelligence\u201d through\neither biological processes (synthetic biological intelligence) [5\u20138] or mathematical and com-\nputer processes (machine learning). While both methods show promise, the rich history of\nmathematics provides a deeper understanding of how intelligence may be simulated. Specif-\nically, neural networks [9, 10] are notable for their ability to learn abstract patterns even in\nlarge datasets, a property that does not extend to other methods, such as support vector\nmachines (SVMs) [11].\nDeep learning is the development of algorithms that are closely related to neural net-\nworks. Examples of this are seen in many real-world applications [See [2,3,12,13]]. These\nmodels only depend on one set of parameters at any given time, which is key to their scal-\nability. Additionally, the sheer size of the parameters that characterize these models makes\nunderstanding what these networks truly \u201cknow\u201d an enigmatic task. Thus, controlling and\nassessing their capabilities becomes difficult [14,15].\nIn 1993, a continuous-time recurrent neural network (CTRNN) developed by [16] proved\nthat \"that any finite time trajectory of a given n-dimensional dynamical system can be\napproximately realized by the internal state of the output units of a continuous time recurrent\nneural network with n output units, some hidden units, and an appropriate initial condition.\u201d\nCTRNNs have been applied to evolutionary robotics, where they have been used to address\nvision, cooperation, and minimal cognitive behavior [17].\nHowever, the recent development of \u201cliquid models"}, {"title": "1.2 Our new ideas", "content": "Liquid networks [18] and continuous time recurrent neural networks (CTRNets) [16] have\nshown great promise in displaying dynamic intelligence, a feature that classical networks\nlack. While these models have shown great promise in current literature, no work has been\ndone to investigate a continuous hidden state in quantum neural networks (QNNs). The\ncurrent development of QNNs has shown to be superior, theoretically, to classical networks.\nHowever, these models still admit a rigid hidden state leading to static intelligence."}, {"title": "2 Preliminaries", "content": "To address these issues, we fuse these continuous networks and quantum neural networks,\nintroducing Liquid Quantum Networks (LQNets) and Continuous-Time Recurrent Quantum\nNeural Networks (CTRQNets). Firstly, we define a quantum residual block from [20] to\nconstruct a residual block to mitigate the vanishing gradient. Inspired by [13], we derive a\nnew model for quantum neural ordinary differential equations, which we then use to formulate\nan LQNet and CTRQNet. These models surpass quantum neural networks by capturing\ntemporal dependencies and exhibiting dynamic intelligence through an ever-changing set of\ndifferential equations governing there hidden state. On CIFAR-10, our models achieved a\n40% increase in accuracy compared to QNNs."}, {"title": "2.1 Multi-layer Neural Networks", "content": "We define a neural network as follows:\nDefinition 2.1. If F(x) be a composition from the set {fn} where {fn} denotes a finite set\nof differentiable functions on R, then F is a neural network.\nHowever, this definition only considers static neural networks, not cases where hidden\nstates are solutions to differential equations. Mathematically defining networks with these\nimplicit hidden states is challenging due to the various methods of forward and backward\npropagation used in different architectures [13,18,21,22]. For example, [18] uses a backprop-\nagation through a time training algorithm, whereas [13] uses an adjoint method with the\nladder being the continuous counterpart of the former. These formulations are mathemati-\ncally different and, therefore, cannot be encompassed under the same definition. We omit a\ndefinition for these types of networks but define dynamical, continuous, liquid hidden states\nto be hidden states where the corresponding transformation is the solution to a differential\nequation.\nThe current literature has significant room to explore continuous dynamics in quantum neu-\nral networks as their static construction resembles that of their classical counterparts. This\nstatic construction is sub-optimal for real-world applications where data may be irregularly\nspaced, leading to quantum networks' inability to learn from this type of data in their\ncurrent formulation. By replacing the static hidden states in quantum networks with a dy-\nnamic liquid structure, these newly formulated quantum models, which we call CTRQNets\nand LQNets, are able to adapt to various types of data that quantum neural networks strug-\ngle to adapt to. These models exhibit dynamical intelligence and are task-aware even after\nthe training process is complete, so the knowledge of these models can be easily accessed."}, {"title": "2.2 Residual Learning", "content": "A common issue seen in deep neural networks is the degradation of accuracy as the\nnumber of layers increases. This issue is not due to overfitting, as deeper networks report\nlower training accuracy than shallower ones on certain tasks [23]. To address this, [23]\nproposed residual networks where layers learn the residual between the optimal hidden state"}, {"title": "2.3 Neural Ordinary Differential Equations (NODES)", "content": ""}, {"title": "2.3.1 Neural Ordinary Differential Equations", "content": "Consider a residual block of the form\n$$h_{t+1} = h_t + f(\\theta_t, h_t, t)$$\nwhere $$h_t$$ denotes the t-th residual block within the layer and $$\theta_t$$ denotes the model parameters\ncorresponding to the t-th residual block within the network. Residual networks are discrete\nfunction approximators as they lack a continuous structure. Discrete neural networks i.e.\nresidual networks have 2 main disadvantages:\n1. They lack the adaptability to process non-rigid data (time-series data with non-constant\ntime spacing).\n2. They lack dynamic intelligence and cannot self-adjust to unfamiliar patterns due to\nthe fact that the hyperparameters are fixed."}, {"title": "2.3.2 Numerical Methods", "content": "There exist a variety of methods to perform gradient calculation, but the authors of [13]\nused a method formulated in [21], which makes the calculation of these values explicit and\npreserves the continuous structure of Neural ODEs. While the adjoint rapidly performs\ngradient calculation, many intermediate optimization steps are taken due to the flexibility of\nODE Solvers [21,22], slowing the training process. In most cases [18,19,24], backpropagation\nthrough time is used, which is faster and more accurate in practice.\nWhen minimizing loss function $$\\mathcal{L}$$at z(T), $$\\mathcal{L}$$ depends not only on z(T) but on all z(t)\nfor t \u2208 [0,T]. Neural ODEs have continuous depth, making direct backpropagation through\nthese layers impossible. Instead, we consider partitioning the interval [0,T] and updating\nthe function throughout these intervals. Adaptive ODE solvers, such as Runge-Kutta and\nBogacki-Shampine methods [22,25], adjust intervals based on the smoothness of the solution.\nLarger steps are taken in smooth regions where the accuracy in the region is not critical\ntowards the final solution, while smaller steps are used in volatile regions where accuracy is\ncritical [22, 25, 26].\nHowever, these ODE solvers cannot calculate the gradients directly, and using an ODE\nSolver to calculate the gradient would be very inefficient as it would have to solve the implicit\ndifferential equation:\n$$\\mathcal{L}(z(t_1)) = \\mathcal{L}(z(t_0) + \\int_{t_0}^{t_1} f(\\theta, z(t), t)dt))$$"}, {"title": "2.4 Time Continuous Recurrent Neural Networks", "content": "Continuous Time Recurrent Neural Networks (CTRNNs) had its first formulation [16]\ndemonstrating the ability to approximate any continuous curve to any degree of accuracy.\nLike Recurrent Neural Networks, the hidden states of CTRNNs depend on time. A CTRNN\nis defined by the following differential equation:\n$$\\frac{dx}{dt} = -\\frac{1}{\\tau}x(t)+ W\\sigma(x(t))$$\nTo perform the forward pass of the network, the differential equation above is solved\nin the interval t = to to t = t\u2081, subject to the initial condition of our input. In general,\nletting I(t) denote the input to the CTRNN at time t, we solve Eq. (2.1) in the interval\n[to, t1] subject to the initial condition x(to) = I(to) for which \u03c3 denotes a nonlinear activation\nfunction that is continuous and bounded (e.g., sigmoid). CTRNNs are more expressive than\ntraditional networks due to the system of differential equations underlying their hidden state.\nThese differential equations can be optimized through various numerical differential equation\nsolvers, allowing CTRNNs to trade speed for accuracy in regions where it is necessary.\nThe following theorems were proven in [16], creating the catalyst for more advanced\nresearch of differential equations and structure machine learning model structures. Theorems\n2.2 and 2.3 below establish that the internal state of the output units of a CTRNN can\napproximate a finite time trajectory of a given dynamical system to an arbitrary precision:\nTheorem 2.2. Let \u03c3 be a strictly increasing C\u00b9-sigmoid function such that \u03c3(R) = (0,1),\nand let f : I = [0,T] \u2192 (0,1)\u207f be a continuous curve, where 0 < T < \u221e. Then, for an\narbitrary \u03f5 > 0, there exists an integer N and a recurrent neural network with n output units\nand N hidden units such that\n$$\\max_{t\\in I} || f(t) - y(t)|| < \\epsilon,$$\nwhere y(t) = (\u03c3(y\u2081(t)), ..., \u03c3(yn(t))) is the output of the recurrent network with the sigmoid\noutput function \u03c3.\nTheorem 2.3. Let f : I = [0,T] \u2192 Rn be a continuous curve, where 0 < T < \u221e. Then,\nfor an arbitrary \u03f5 > 0, there exist an integer N and a recurrent network with n output units\nand N hidden units such that\n$$\\max_{t\\in I} || f(t) - u(t)|| < \\epsilon,$$\nwhere u(t) = (\u03c3(u\u2081(t)), ..., \u03c3(un(t))) is the internal state of the output units of the network.\nBackpropagation through automatic differentiation is memory intensive due to the need\nfor values across every point in time to be recalculated at each update of the hidden state.\nTherefore, gradients are instead computed numerically at the final time step, then, through\nanother ODE solver such as the Adjoint method [21], the gradients at all points in time are\ncalculated. In cases such as with Neural ODEs and LNNs [13,18], it is possible to derive"}, {"title": "2.5 Liquid Neural Networks", "content": "Liquid neural networks [18] were formulated through a neuron-spiking biological process.\nThis process was integrated into the development of CTRNNs, allowing them to be more\ndynamic and providing benefits similar to NODEs [13]. Formally, the hidden state of these\nliquid networks is formulated as follows:\n$$\\frac{dx(t)}{dt} =  [-\\frac{1}{\\tau} + f(x(t), I(t), t, \\theta)] x(t) + f(x(t), I(t), t, \\theta)A,$$\nwhere A is a parameter and \u03c4 denotes a time constant that aids the network in maintaining\na numerically stable state.\nDespite the formulation's resemblance to NODEs and CTRNNs, the adjoint method is\nnot used for gradient calculation here since the differential equation that governs the hidden\nstate is a set of stiff differential equations leading to an exponential increase of discretization\nsteps when numerically simulating the equation using a Runge-Kutta-based integrator [26].\nHence, a new type of numerical integrator is necessary, for the adjoint method commonly\nimplements a Runge-Kutta-based Dortmund Prince integrator [13, 18]. [18] introduces the\nFused solver, a combination of the implicit and explicit Euler Solver. This fused solver\ndiscretizes the interval [0,T] as\n$$x(t + \\Delta t) = \\frac{x(t) + \\Delta tf(x(t), I(t), t, \\theta)A}{1 + \\Delta t ( + f(x(t), I(t), t, \\theta))}$$\nBackpropagation through time can be applied as the solver is unrolled throughout time,\nstreamlining the gradient calculation. These liquid networks offer enhanced expressiveness\ncompared to Neural ODEs and CTRNNs due to the increased complexity of the hidden\nstate. The theorems below demonstrate that the proposed LTC in [18] satisfies a universal\napproximation property (Theorem 2.4) and that the time constant and the state of the\nneurons remain within a finite range (Theorem 2.5):\nTheorem 2.4. Let x \u2208 R\u207f, S C R\u207f, and x = F(x) be an autonomous ODE with F : S \u2192 Rn\na C\u00b9-mapping on S. Let D denote a compact subset of S and assume that the simulation of\nthe system is bounded in the interval I = [0,T]. Then, for a positive \u03f5, there exists an LTC\nnetwork with N hidden units, n output units, and an output internal state u(t), described by\nEq. 1, such that for any rollout {x(t) | t \u2208 I} of the system with proper network initialization\nof initial value x(0) \u2208 D,\n$$\\max_{t\\in I}|x(t) - u(t)| < \\epsilon.$$\nTheorem 2.5. Let xi denote the state of neuron i within an LTC, identified by Eq. 1, and\nlet neuron i receive M incoming connections. Then, the hidden state of any neuron i, on a\nfinite interval Int \u2208 [0,T], is bounded as follows:\n$$\\min(0, A_{min}) \\leq x_i(t) \\leq \\max(0, A_{max})$$"}, {"title": "2.6 Quantum Computing", "content": "Quantum computing leverages the principles of quantum mechanics in order to perform\noperations beyond the scope of classical computers. The necessity for quantum computers\nis more practically illustrated by Moore's Law, which states that the number of transistors\nin an integrated circuit will double approximately every other year, continuously reducing\nthe size of transistors in circulation. When dealing with objects as small as our current\ntransistors, quantum tunneling phenomena inhibit their functionality. Physical theory in\nquantum mechanics allows us to model these microscopically small objects and develop more\nefficient and robust computers. The probabilistic nature of quantum mechanical theory offers\nadditional benefits when applied to building computers that classical mechanics theory does\nnot. At the backbone of all of quantum mechanical theory is the Schrodinger equation [27],\nwhich states that for a given quantum mechanical system, its wave equation follows the\npartial differential equation below:\n$$i\\hbar\\frac{\\partial}{\\partial t} \\Psi(x,t) = -\\frac{\\hbar^2}{2m} \\nabla^2 \\Psi(x,t) + V(x, t)\\Psi(x,t)$$\nwhere\n\u2022 i is the imaginary constant\n\u2022 \u0127 is Planck's constant\n\u2022 \u03a8(x, t) denotes the wave function of the system\n\u2022 \u22072 denotes the Laplacian of the system\n\u2022 V(x, t) is the potential energy of the system\nIn most applications of quantum computing, the quantum mechanical system is realized by\ntracking the evolution of atoms over time. The superposition of these systems is critical for\nquantum computing as operations can be done to arbitrary states of the system simultane-\nously without physically applying the transformation to all possible states of the system. The\nquantum analog of the classical bit is the qubit. Physically, it is realized through a quantum\nmechanical system in which certain operations may be applied to the systems, i.e., a rotation\nalong an axis, without knowing the state of the system. While the theory of quantum com-\nputing is based on quantum mechanics, the details generally rely on the analysis of the qubit.\nWhile a qubit can be in a superposition of states, upon measurement, it must collapse\nto a basis state, which we label |0) or 1). It is impossible to calculate deterministically\nwhich state a qubit will collapse into. We can, however, determine the probabilities for\neach possibility through the wave function. Quantum machine learning aims to alter these\nprobabilities by altering the state of the function. Through a series of operations in which\nthe parameters are controlled, it is possible to create a state where a qubit will necessarily\ncollapse into a desired state upon measurement.\nMathematically, a qubit is an element of a Hilbert space. In order to build a composite"}, {"title": "3 Theoretical Framework", "content": "[20] proposed a new framework for a quantum residual block in which they define a\nquantum residual operator applied to the entangled state $$|\\psi_0\\rangle |0\\rangle$$. We utilize this approach\nto derive a quantum liquid network. A traditional residual block encoded into a quantum\nneural network is of the form:\n$$|\\Psi_{t+1}\\rangle = |\\Psi_t\\rangle + U|\\Psi_t\\rangle.$$\nThis approach does not retain the benefits of quantum computing. Instead, the authors\nof [20] present a quantum residual block that leverages the probabilistic nature of quantum\ncomputing to further enhance the capability of a residual block. Let $$\\mathcal{H}_1 = \\mathbb{C}^{2^n}$$ and $$\\mathcal{H}_0 =$$\nspan {$$|0\\rangle$$} denote Hilbert spaces. Consider an initial quantum state $$|\\psi_0\\rangle \\in \\mathcal{H}_1$$ and an\nauxiliary qubit $$|0\\rangle \\in \\mathcal{H}_0$$ as illustrated in Figure 3. The implementation of the quantum\nresidual operator is realized in the subspace of the ancilla qubit. Once the residual operator\nis applied to the initial state, one can then obtain the results within the subspace V =\n{$$|x\\rangle\\langle x| \\quad | \\quad |x\\rangle \\in \\mathcal{H}_0$$} [20].\nInspired by [20], we develop an LQNet and a CTRQNet. First, we apply either a\nHadamard gate to $$|0\\rangle$$ or a corresponding transformation to the state $$|\\psi_0\\rangle$$ depending on\nwhether we are in the encoding stage or parameterized stage. We then apply a CNOT gate\nto the qubits using $$|0\\rangle$$ as the control qubit and $$|\\phi_0\\rangle$$ as the target qubit. From there, we\napply another Hadamard to the quantum state whose output is measured in the auxiliary\nqubit subspace $$\\mathcal{H}_0$$. The result of this operator is then added to the quantum state lying in\nthe subspace of the first qubit. A final operation is performed on the state lying in $$\\{H}_0$$ to re-\nalize the residual connection. Rewriting in a more succinct notation, we propose a Quantum\nLiquid Neural Network as follows :\nConsider the entangled state $$|\\psi_0\\rangle |0\\rangle$$. Then the output of the residual connection in\nthe encoding stage(Eq. 3.1) can be rewritten in the following form:\n$$F(|\\psi_t\\rangle) \\triangleq tr_{\\mathcal{H}_0} (\\psi_t|F^{\\dagger} (|\\phi_0\\rangle \\otimes |0\\rangle)F|\\psi_t),$$\nwhere $$F: \\mathcal{H}_1 \\rightarrow V$$ denotes the action of the quantum residual block acting on classical data\n$$|x\\rangle \\in \\mathcal{H}_0$$ mapping to the subspace $$V = {|x\\rangle\\langle x| \\quad | \\quad |x\\rangle \\in \\mathcal{H}_0}$$. We can think of the mapping F\nas a sequence or composition of unitary operators\n$$F\\triangleq (H \\otimes U) \\circ (CNOT) \\circ (H \\otimes I),$$\nwhere for the initial state $$|\\varphi_0\\rangle$$, U $$|\\varphi_0\\rangle \\triangleq  |\\varphi_0\\rangle\\langle \\varphi_0$$, CNOT denotes the controlled-not gate, H\ndenotes the Hadamard gate, and I denotes the identity matrix. Note that F is unitary by\nconstruction. Similarly, we define $$F : \\mathcal{H}_1 \\rightarrow V$$ by\n$$F\\triangleq (H \\otimes W(\\theta)) \\circ (CNOT) \\circ (H \\otimes I),$$\nwhere, as in [20], we replace the encoding operator with the parameterized gate W(\u03b8) while\nthe remaining gates remain unchanged. Let\n$$F(|\\psi_t\\rangle, \\theta) = tr_{\\mathcal{H}_0} (\\psi_t|F^{\\dagger}(|\\varphi_0\\rangle \\otimes |0\\rangle)F|\\psi_t),$$"}, {"title": "4 Results", "content": "where $$|\\psi_t\\rangle$$ denotes the evolution of initial quantum state $$|\\phi_0\\rangle$$ on layer t. $$F(|\\Psi_t\\rangle,\\theta)$$ is\nequivalent to $$W(\\theta) |\\varphi_0\\rangle$$, in $$W(\\theta)|\\varphi_0\\rangle \\equiv W(\\theta)$$. With our definition of the residual block\n$$\\mathcal{F}(\\psi_t, \\theta)$$, we have that in the limit, a model equation for quantum NODEs as follows\n$$\\frac{d\\varphi(t)}{dt} = F(|\\psi_t\\rangle,\\theta),$$\nwhere \u03c6(t) denotes a continuous function in time, t. The solution of Eq.(3.6), when evaluated\nat t = T, forms the output of the quantum NODE block. Substitution of this into the\nequation for liquid networks, we have that\n$$\\frac{d\\varphi(t)}{dt} =  [- \\tau^{-1} + \\frac{ \\partial F(|\\psi_t\\rangle,\\theta)}{ \\partial \\varphi(t)}] \\varphi(t) + F(|\\psi_t\\rangle,\\theta).$$\nThese new equations form our newly created model, hereafter referred to as LQNets.\nUtilizing the Fused Solver developed by (Hasani et al., 2020) [18], we simulate the differential\nequation to obtain\n$$\\varphi(t + \\Delta t) = \\frac{\\varphi(t) + \\Delta tF(|\\Psi_t\\rangle, \\theta)}{1+ \\Delta t(\\tau^{-1} + F(|\\Psi_t\\rangle, \\theta))}$$\nFollowing a similar argument, we develop a quantum time-continuous recurrent neural\nnetworks (CTRQNet). Recall that a CTRNN is defined by the equation\n$$\\frac{dx(t)}{dt} = \\frac{1}{\\tau}x(t) + W\\sigma(x(t)),$$\nin which \u03c3: R \u2192 (0,1) \u2208 C\u00b9 is a sigmoid function in accordance with Theorems 2.2 and\n2.3, and W denotes a weight matrix. With Eq. (3.6) defined, we can amend the standard\nCTRNN equation to read\n$$\\frac{d\\varphi(t)}{dt} = -\\tau^{-1}\\varphi(t) + F(|\\psi_t\\rangle,\\theta),$$\nwhere the quantity $$F(|\\psi_t\\rangle, \\theta)$$ replaces $$W(\\sigma(x(t)))$$, and is equivalent to the internal state of\nthe output units of the network (cf. Theorem 2.3). This new equation (Eq. 3.9) forms the\nproposed model for CTRQNets.\nIn this section we implement the 2 newly created models, the LQNet and CTRQNet.\nIn 4.1 we discuss the methodology used throughout the experiments, in 4.2 we discuss the\nresults of our experiments on each of the datasets listed."}, {"title": "4.1 Methodology", "content": "We perform tests of our networks using the following benchmark datasets:\n\u2022 MNIST Binary Classification"}, {"title": "4.2 Baseline Testing", "content": ""}, {"title": "4.2.1 MNIST", "content": "The MNIST dataset consists of grayscale images of handwritten digits from 0 to 9. Our\nmodels use Os and 1s as binary classification. The training set consists of 150 images, the\nvalidation set 50 images, and the testing set 2115 images. We present our loss curves below.\nIn Figure 4, both the LQNet and CTRQNet are able to converge to and maintain perfor-\nmance around an optimal state. Both models' losses approach 0, and the lost curves exhibit\nno volatility. While the QNN also converges to an optimal state, its minimum loss is roughly\n0.3, while the LQNet and CTRQNet achieve a loss of near 0.001. Additionally, the LQNet\nand CTRQNet reached optimal states in 10 steps, whereas QNNs were not able to reach an\noptimal state within 50 steps.\nIn Figure 5, the LQNet and CTRQNet achieve maximal performance in terms of accu-\nracy on the validation set after 10 steps. Both models can maintain optimal performance\nconsistently after first achieving it, as was observed in the loss curves for both models. The\nQNN takes longer than these models to achieve an optimal state, requiring around 20 steps,\nbut still maintains the consistency seen in the other two networks."}, {"title": "4.2.2 FMNIST", "content": "We analyze our models on the Fashion MNIST dataset through binary classification,\nusing 150 training images, 50 validation images, and 2000 testing images. Every 15 steps in\nthe optimization process, we report the model's performance on the validation dataset, as\nshown below.\nIn Figure 6, the CTRQNet converges to an optimal state within 195 steps, making it the\nfastest of the three models to reach an optimal state. On the other hand, the LQNet reaches\nan optimal state after 300 steps; it may benefit from more training time as it has not fully"}, {"title": "4.2.3 Wisconsin Breast Cancer", "content": "converged after 300 steps. The QNN converges to an optimal state in a linear fashion and\nconverges much slower than the LQNet and CTRQNet models.\nThe Wisconsin Breast Cancer dataset consists of 405 training samples, 50 validation\nsamples, and 114 testing samples.\nIn Figure 8, all three models show a decreasing loss curve. However, the loss curves of the\nLQNet and CTRQNet indicate that they converge to an optimal solution much more rapidly\nthan the QNN. The QNN reaches a minimum loss of 0.2, whereas the LQNet and CTRQNet\nmodels achieve a loss of 0.03. The QNN also had more training steps than the other two\nmodels, having 140 steps compared to the LQNet and CTRQNet 55 steps, respectively."}, {"title": "4.2.4 CIFAR 10", "content": "According to Figure 9, all three models reached 95% accuracy. However, the LQNet\nand CTRQNet took a significantly shorter time to converge, taking only 30 and 10 steps\nrespectively. On the other hand the QNN only converges after 80 steps.\nThe CIFAR 10 dataset consists of 200 training images, 100 validation images and 2000\ntesting images. We use the first two classes of the original dataset: Airplanes and Automo-\nbiles. For this dataset, we do not use preprocessing from pre-trained models."}, {"title": "4.2.5 CIFAR 10 Feature Extraction", "content": "In Figure 10, the LQNet and CTRQNet demonstrate noticeable volatility in their loss\ncurves; neither model can maintain optimal performance for long periods. The LQNet suf-\nfers a massive spike in its loss after two steps, and although it returns to its previous state\nand optimizes properly shortly after, it experiences another spike in loss after 23 steps. The\nCTRQNet optimizes initially but displays a massive upward spike after 13 steps before re-\nturning to an optimal state immediately afterward. We attribute the high volatility in both\nmodels to the difficulty of the task at hand. Meanwhile, the QNN cannot learn relationships\nunderlying the data, which is reflected in its loss curve. After 65 steps, the QNN sees no no-\nticeable difference compared to the initial loss. This task is too difficult for the QNN to learn\nanything, while the LQNet and CTRQNet are both able to learn meaningful relationships.\nIn Figure 11, both the LQNet and CTRQNet show more volatility compared on this\ndataset in comparison to other baseline tests shown above. While both models are able\nto converge to an optimal state with satisfactory performance, they are unable to stay\nin the state consistently. We attribute this to overtraining as we use a classical set of\nhyperparameters to train these models. Both models achieve a high of 75-80% accuracy\non the validation set. The QNN is unable to learn anything meaningful and maintains a\nconstant accuracy of 47%. Our new models are able to learn tasks that prove to be too\ndifficult for QNNs.\nLooking at Figure 13, the QNN is unable to learn meaningful information underlying the\ndata within a reasonable training time, unlike the CTRQNet and LQNet, which are both\nable to converge to an optimal state quickly. Both models converge at around 15 steps,\nmaintain their performance after reaching this peak, and have lots of movement in terms of\naccuracy but maintain optimal performance, ensuring that the state of these models does\nnot stay constant. This is a benefit as common issues that arise in classical and quantum\nmachine learning, such as the vanishing gradient, will not affect our models. Below, we\npresent the ROC curves and confusion matrices.\nAfter the CIFAR 10 dataset was preprocessed, the CTRQNet and LQNet showed a\nconsiderable improvement in accuracy, going from a 73.1% average to a 90% average. On\nthe other hand, the QNN showed no such improvements, maintaining the same 50% accuracy\nas when they used the original CIFAR 10 dataset. While having a downward slope, its loss\ngraph remains volatile, showing that it has trouble learning the CIFAR 10 dataset even after\npreprocessing.\nDue to the computational complexity required to model the CIFAR 10 dataset accurately,\nwe use a pre-trained model of residual networks to preprocess the images. The preprocessing\nmodel in question is Resnet 151. With the addition of this model, we can help the models\nlearn deeper patterns due to Resnet taking care of computational complexity and leaving\nthe CTRQNet and LQNet to learn the patterns underlying the data rather than learning a\nlatent space representation of the data.\nBased on Figure 12, both the CTRQNet and LQNet display a similar training curve\nwith relatively low volatility. Additionally, both models converge rapidly to an optimal state\nand maintain their performance after reaching this state. However, they only maintain this\nstate for 30 steps after reaching the optimal state before experiencing a slow increase in loss.\nThis is attributed to the usage of a classical set of hyperparameters on a quantum model.\nThe QNN has a prolonged learning process, showing no real progress in its ability to learn\nthe task. Our models are able to perform well on difficult datasets, which prove to be too\ndifficult for QNNs to perform on"}]}