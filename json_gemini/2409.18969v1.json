{"title": "Integrating SPARQL and LLMs for Question Answering\nover Scholarly Data Sources*", "authors": ["Fomubad Borista Fondi", "Azanzi Jiomekong"], "abstract": "The Scholarly Hybrid Question Answering over Linked Data (QALD) Challenge at International Semantic Web\nConference (ISWC) 2024 focuses on Question Answering (QA) over diverse scholarly sources: DBLP, SemOpenAlex,\nand Wikipedia-based texts. This paper describes a methodology that combines SPARQL queries, divide and\nconquer algorithms, and BERT-based-case-SQuad2 predictions. It starts with SPARQL queries to gather data, then\napplies divide and conquer to manage various question types and sources, and uses BERT to handle personal\nauthor questions. The approach, evaluated with Exact Match and F-score metrics, shows promise for improving\nQA accuracy and efficiency in scholarly contexts.\nKeywords: Scholarly Question Answering, Large Language Models, Divide and conquer.", "sections": [{"title": "1. Introduction", "content": "The Scholarly Hybrid Question Answering over Linked Data (QALD) aims to answer questions in\nscholarly publications provided in natural language [6]. A challenge on Question Answering over\nLinked Data (QALD) which is hosted at the International Semantic Web Conference(ISWC) 2024 [5] since\n2023. The 2024 edition is devoted to the development of question answering (QA) systems capable of\nintegrating and querying information from three distinct but interconnected sources: DBLP Knowledge\nGraph\u00b9, SemOpenAlex Knowledge Graph\u00b2, and Wikipedia-based texts\u00b3.\n1. DBLP Knowledge Graph is a comprehensive dataset documenting research publications, authors,\nand affiliations.\n2. SemOpenAlex Knowledge Graph is an extensive KG containing detailed information about\nauthors, institutions, and publications.\n3. Wikipedia-Based Scholarly Text is composed of textual data derived from Wikipedia, offering\nsupplementary information on scholarly topics.\nThe primary objective of this paper is to describe the methodology employed in addressing the\nScholarly Hybrid QALD Challenge. This includes detailing the integration of SPARQL queries across\ndifferent KGs [2], the application of divide-and-conquer algorithms [9], and the utilization of BERT [10]\nto improve response accuracy. To assess this methodology, the dataset provided by the organisers was\nused. This dataset was composed of training set and test set. The training set was composed of 5000\nquestions along with their answers, while the test set was composed of 702 questions. The approach\nproposed in this paper shows promising results for this challenge.\nThe rest of the paper is organised as follows: Section 2 is the detailed methodology used, the Section\n3 presents the results and the Section 4 conclude this work."}, {"title": "2. Methodology", "content": "To address the Scholarly Hybrid Question Answering over Linked Data (QALD) Challenge, we adopted\na multi-step approach combining natural language processing techniques for data processing, SPARQL\nqueries, divide and conquer algorithms, and LLM-based predictions. This methodology is designed\nto efficiently handle the complexity of integrating information from multiple sources and producing\naccurate answers for a given set of questions. Fig. 1 provides an overview of the methodology pipeline\nused in this work. This figure illustrates the main steps involved in processing the data, executing\nqueries, applying LLM-based predictions, generating answers, and refining them."}, {"title": "2.1. Data Processing and Query Execution", "content": "The process began with executing a general script containing SPARQL queries against SemOpenAlex\nfor both authors and institutions. This step involved querying the semoa_authors.trig file and\nthe institution-semopenalex.trig dataset from October 2023 locally. The query execution took\napproximately 62-65 hours to complete due to the size and complexity of the datasets. In some cases, it\nfilled up the RAM and returned \"Killed\".\nFig. 2 provides an overview of the data processing outputs from the the various KGs given. The data\nprocessing involved cleaning the dataset to remove noise such as un-added parts, mis-matched names,\nNames which are not well spelled, and alot more and irrelevant information such as None-useful links,\nThe questions were subjected to a thorough, individual examination, and keywords from the questions"}, {"title": "2.2. Divide and Conquer Approach", "content": "To manage the diverse nature of the questions and data, we implemented a divide and conquer strategy:"}, {"title": "2.3. Data Retrieval and Aggregation", "content": "We employed a script to generate a CSV file containing all potential responses for each question by\nquerying the endpoints provided on the challenge's website. This CSV file included detailed author\ninformation such as names, publication counts, and institutional affiliations.\n1. CSV to JSON Conversion: The CSV file was converted to JSON format. Duplicate entries,\nresulting from multiple author names, were removed to ensure a clean dataset.\n2. Merging Results: The JSON file was then used to cross-reference and extract answers for each\nspecific question. The answers were aggregated and merged to create a comprehensive set of\nresponses.\n3. Final Refinement: The merged results were refined by integrating them with the initial general\npredictions and LLM-generated responses to ensure accuracy and completeness. This final step\nresulted in the creation of the answers2.txt file submitted for evaluation."}, {"title": "2.4. Large Language Model-Based Predictions", "content": "The LLM used in this challenge was BERT-base-cased-squad2, a pretrained non-finetuned model\ndownload from Hugging Face\u2074, which was used for Question Answering tasks.\nAfter executing SPARQL queries, we used the BERT-based model bert-base-cased-squad2 to\npredict responses to personal questions about authors. The context for these predictions was generated\nfrom the results of the SPARQL queries. This step was crucial for answering questions that required\ndetailed and context-specific information. The overall LLM prediction steps are:\n1. Context Generation: The context for each question was constructed from the data retrieved\nthrough SPARQL queries.\n2. LLM Inference: Using the bert-base-cased-squad2 model, we generated predictions based\non the context. This model was trained on the SQuAD2 dataset to handle the intricacies of\nquestion answering with contextual information. This model was from Hugging Face5.\n3. Integration: The LLM-generated responses were integrated with the initial query-based results\nbefore the final refinement stage to enhance the accuracy and completeness of the answers."}, {"title": "2.5. Evaluation and Finalization", "content": "The evaluation of our approach was carried out by submitting the results obtained after the application\non the test set to the codalab 6 provided by the organisers. The results was assessed based on Exact\nMatch and F-score metrics."}, {"title": "2.6. Experimentation Environment", "content": "The experimentation was conducted using an HP EliteBook 745 G5 laptop equipped with an AMD\nRyzen\u2122\u2122 5 PRO 2500U w/ Radeon\u2122\u2122 Vega Mobile Gfx \u00d7 8 CPU, 24 GB of RAM, and a 512.0 GB SSD disk.\nThe operating system used was Ubuntu 24.04.4 LTS."}, {"title": "3. Results and Discussion", "content": "Fig. 3 presents the results obtained after applying the methodology presented in Section 2. It shows\nthat the best results is obtained when the SPARQL queries are combined with the LLM for predicting\nresponses.\nDuring this work, we found that:\n\u2022\n\u2022 To manage complex queries on authors, institutions, affiliations and publications on the Semope-\nnalex, we integrated SPARQL queries and LLMs prediction.\n\u2022 the BERT-base-cased-squad2 model combined with DPR algorithm significantly improved the ac-\ncuracy of entity and relation extraction on the DBLP KG. It should be noted that these information\nare needed to provide the context for the prediction by the LLMs.\n\u2022 To handle the complete dataset, the DPR algorithm was employed, so as to be able to get through\nall the broken sets of the dataset."}, {"title": "4. Conclusion", "content": "In this paper, we presented a novel approach forHybrid Question answering over Linked Data. This\napproach was assessed on the training and test datasets of the Scholarly Hybrid Question Answering\nover Linked Data (QALD) Challenge 2024. We found that the integration of SPARQL queries with LLM-\nbased predictions offers a robust solution for Question Answering over diverse scholarly data sources.\nOur approach demonstrated significant improvements in handling complex queries and providing\naccurate responses. Despite the results obtained, there were several challenges, particularly in handling\nthe large and complex nature of the SemOpenAlex and DBLP datasets. Future work will focus on\nimproving the model's ability to generalize across different types of scholarly data and incorporating\nmore sophisticated rule-based systems on the one hand. On the other hand, we will focus on refining\nthe methodology and exploring additional enhancements to further improve the system's performance."}]}