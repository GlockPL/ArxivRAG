{"title": "Multi-Agent Reinforcement Learning for Autonomous Driving: A Survey", "authors": ["Ruiqi Zhang", "Jing Hou", "Florian Walter", "Shangding Gu", "Jiayi Guan", "Florian R\u00f6hrbein", "Yali Du", "Panpan Cai", "Guang Chen", "Alois Knoll"], "abstract": "Reinforcement Learning (RL) is a potent tool for sequential decision-making and has achieved performance surpassing human capabilities across many challenging real-world tasks. As the extension of RL in the multi-agent system domain, multi-agent RL (MARL) not only need to learn the control policy but also requires consideration regarding interactions with all other agents in the environment, mutual influences among different system components, and the distribution of computational resources. This augments the complexity of algorithmic design and poses higher requirements on computational resources. Simultaneously, simulators are crucial to obtain realistic data, which is the fundamentals of RL. In this paper, we first propose a series of metrics of simulators and summarize the features of existing benchmarks. Second, to ease comprehension, we recall the foundational knowledge and then synthesize the recently advanced studies of MARL-related autonomous driving and intelligent transportation systems. Specifically, we examine their environmental modeling, state representation, perception units, and algorithm design. Conclusively, we discuss open challenges as well as prospects and opportunities. We hope this paper can help the researchers integrate MARL technologies and trigger more insightful ideas toward the intelligent and autonomous driving.", "sections": [{"title": "I. INTRODUCTION", "content": "LARGE-SCALE autonomous driving systems have attracted tons of attention and millions of funding from industry, academia, and government in recent years [1], [2]. The motivation behind developing such a system is to replace human drivers with automated controllers. It can significantly reduce the time consumption and workload of driving, enhance the efficiency and safety of transportation systems, and promote economic development. Generally, to detect the vehicle states and generate reliable control policies, automated vehicles (AVs) should be equipped with massive electric units, like visual sensors including radars, light detection and ranging (LiDAR), RGB-Depth (RGB-D) cameras, event cameras, inertial measurement units (IMU), global positioning system (GPS) and so on [3]\u2013[5]. A salient challenge in this topic is to build a robust and efficient algorithm that is capable of processing massive information and translating this data into real-time operations. Early works divide this big issue into perception, planning, and control problems and solve them independently, known as modular autonomous driving.\nOn the other hand, as a powerful toolkit for sequential decision-making, reinforcement learning (RL) can optimize agent behavior models with the reward signal. As its evolution, deep RL combines the advantages of RL and deep neural networks, which enable to abstract complex observations and learn efficient feature representations [6]. In the past representative research, it has exhibited performances in domains such as board games [7], [8], video games [9], [10] and robotic control [11]\u2013[13], where it rivaled or even surpassed human performances. For autonomous driving, RL brings end-to-end control into reality, which transitions directly from what the vehicle senses to what the vehicle should do, like human drivers. While RL has obtained many remarkable achievements on AVs, most of the related work has approached the issue from the perspective of individual vehicles, which leads to self-centric and possibly aggressive driving strategies, which may cause safety accidents and reduce the efficiency of transportation systems.\nFor real-world traffic systems, we typically define them as multi-agent systems (MAS) and aim to optimize the efficiency of the entire system rather than merely maximizing individual interests. In MAS, all agents make decisions and interact within a shared environment. This means that the states of each agent depend not only on its actions but also on the"}, {"title": "II. AUTONOMOUS DRIVING BENCHMARKS", "content": "RL is always data-hungry. Generally, it requires continuous interaction with the environment to obtain behavior trajectories, which facilitates more accurate value estimations from deep neural networks [35], [36]. However, due to the economic damage caused by uncertain exploration processes, we typically would not deploy our RL policies on real robots directly. Consequently, within the RL paradigm, data from real driving and high-fidelity simulators are ubiquitously adopted in the development of RL-based autonomous driving. In this section, we will introduce various data sources for large-scale MARL in autonomous driving and traffic systems."}, {"title": "A. What is important for a good benchmark?", "content": "A benchmark involves the simulation of physical models, optical rendering, environment and interaction mechanisms, algorithms, and other complex tasks. For MARL-based autonomous driving, we identify the following crucial criteria. We list them out here and will analyse existing data resourses by these metrics.\n1) Realism and Fidelity: High realism and fidelity ensure that the simulator can accurately replicate real-world driving conditions like weathers, lights, environmental dynamics, etc., and mitigate their distributional bias before real-world deployment. Deep learning models especially deep RL demand accurate data. In this case, the realism simulator ensures that its data is representative of real-world scenarios.\n2) Scalability: Scalability ensures that simulators can handle the dynamic environments with numerous entities and variables, so that we can mimick the complexity of real-world scenarios. Scalable MARL algorithms can efficiently learn from the interaction among time-variant numbers of agents like the real traffic system.\n3) Diversity: Diverse scenarios ensure that vehicles can be tested in a wide range of situations, including different traffic conditions, weather, and road structures. For deployment in the real world, it also contributes to the development of more robust autonomous driving systems and makes AVs more reliable in unpredictable real-world conditions. Simultaneously, diversity also implies that the agents could be heterogeneous, which brings more complex interactions and matches to the realities of actual traffic systems.\n4) Efficiency: Time and computational resources are both significant concerns for MARL and autonomous driving [37], [38]. Lightweight simulators reduce computational consumption, and experiments work on cheaper and smaller hardware. Meanwhile, highly-parallelized simulator allows multiple environments to run concurrently and promotes the training process of MARL algorithms. Note that there is always a hard trade-off between fidelity and efficiency [39]. High fidelity requires complex computations to replicate real-world scenarios, especially for 3D visual information.\n5) Transferability: Transferability requires the simulator to support various sensors technically and to replicate their characteristics. For autonomous driving tasks, there are significant differences in the data formats and frame rates of LiDAR, IMU, and cameras. It is essential for the simulator to maintain consistency in the sensor parameters of the intelligent agents with those of commercially available devices. Moreover, transferability is also presented in the simulator's compatibility with the vehicle's device in terms of programming language, communication protocols, and computing platforms.\n6) Features, Maintenance and Supports: Reproducing the effectiveness of algorithm is always time-consuming. Therefore, it would be beneficial for developers to provide fundamental and verified baselines for testing, also with user-friendly application programming interfaces (APIs), annotations and tutorial documentation. These provisions would establish a fair and open comparison standard and improve the efficiency of subsequent developments. Furthermore, lasting maintenance is necessary. Hardware and software frequently discard old features and develop new ones, which can make data and code outdated. Therefore, continuous maintenance of datasets and code bases is an important task."}, {"title": "B. Advanced Simulators", "content": "The selection of a simulator for MARL-based autonomous driving is a critical step. A good selection would save resources and enable the generation of vast amounts of diverse and high-quality training data, which is essential for effectively training MARL algorithms. Additionally, the simulator allows for parallelized testing and training and significantly accelerates the development process by reducing the time required to experiment with various scenarios and conditions. The advantages on extensive data generation and enhanced time efficiency make simulators indispensable for advancing autonomous driving technologies through MARL.\n1) The Open Racing Car Simulator: TORCS [40] was first released in 2000. As a highly modular simulator for multi-agent racing, each race car offers low-level APIs to access partial vehicle states and provides visual information from multiple perspectives. After decades of evolution, it has accurate and editable vehicle dynamics, including the rotational inertia of different components, mechanical structures, tire dynamics, and a simplified aerodynamic model. The simulator supports discrete-time simulations with high frequency up to 500Hz, which allows for the development of complex, high-speed, and aggressive driving controllers on it [41]. Many representative competitions and RL-based research conduct"}, {"title": "C. Datasets", "content": "After decades of development, to effectively solve real-world driving problems, developers have collected tons of on-road data and established rich repositories with different sensors in various scenarios, such as KITTI [96], nuScenes [97], and Waymo Dataset [98]. However, for decision-making problems, these datasets do not record real-time actions like accelerating, braking, steering, or actuator outputs, so they are typically used only for visual tasks or multi-modal sensor fusion. For example, although the IMU and GPS history is given out in BDD-100K [99], the information is on a different domain from human driver actions, and further policy adaption and transferring would be difficult. Moreover, most datasets address the autonomy of a single vehicle rather than collaboration between multiple AVs. Collecting and aligning multi-vehicle data simultaneously is often expensive and difficult, so nowadays MARL paradigms are mostly verified in simulators. However, real vehicle decision data is still invaluable especially for imitation learning [67] and offline RL [68]. Compared to data obtained in simulators, although theoretically we cannot interact for infinite time like simulation, the data distribution is closer to real driving.\nTo provide diverse interactive data for sequential decision-making, the INTERACTION dataset [100] investigates driving habits across different cultural backgrounds and collects bird's-eye-view data via hovering drones with fine-grained annotations. Based on the nations and road structures, it compiles 11 sub-datasets including highways, roundabouts, intersections, merges, and unstructured roads. Additionally, the dataset includes rare collision data and aggressive driving behaviors. Afterwards, the latest research proposes a new benchmark AD4RL [101] based on Next-Generation Simulation (NGSim) US-101 dataset [102]. This benchmark provides 19 datasets from real-world human drivers after fine correction, value normalization, and alignment with partially observable Markov decision process. In other words, it can directly work as policy trajectories in the RL scheme. Additionally, it offers 7 popular offline RL algorithms applied in 3 realistic driving scenarios. A unified decision-making process model is also attached for verification across various scenarios on Flow simulator [52], which serves as a reference framework for algorithm design. Recently, the development and supplement of datasets for the offline MARL approach in large-scale autonomous driving and traffic systems remain a work in progress. We consider this an up-and-coming area and will discuss it later (see Section VI-B)."}, {"title": "D. Competitions", "content": "We notice that many recent competitions have been hosted during the international conference sessions, and here we address and appreciate the contribution of their organizers. These competitions provide a platform for researchers to showcase and compare their algorithms, drive the emergence of new techniques, and promote the application of MARL in real-world scenarios.\nThe earliest multiple vehicles involved in competition can be traced back to the DARPA's Urban Challenge [103]. Although this challenge involved controlling only one vehicle, it required real-time controller adjustment based on a dynamic environment. In the past five years, an increasing number of companies and institutions have recognized MARL's potential and organized academic competitions based on relevant simulators and datasets. At DAI 2020, a multi-vehicle control competition was organized using the SMARTS [55] simulator. Participants were required to develop a parameter-sharing multi-agent model to control a group of agents to accomplish short missions in ramp, double merge, T-junction, crossroads, and roundabout. Instead of testing in the simulation, the DuckieTown [104] AI Driving Olympics (AI-DO) competition on NeurIPS 2021 required participants to deploy real vehicles on scaled-down tracks. The vehicles had to navigate complex urban roads, avoiding pedestrians and other vehicles. The AI-Do competition marks a milestone in the practical deployment of embodied intelligence in complex MAS. At NeurIPS 2022, Huawei's Noah's Ark Lab again organized competitions based on the SMARTS simulator, featuring online and offline reinforcement learning tracks. OpenDriveLab consecutively hosted multi-track autonomous driving competitions on CVPR 2023 and 2024. In the latest Autonomous Grand Challenge, the organizers provided the offline end-to-end autonomous driving scale competition and the online CARLA competition. For the former one, participants were required to develop motion planning algorithms for complex scenarios using offline data from Motional nuPlan [105] dataset. The second track required the design of flexible policy learning methods in the CARLA [70] simulator. These competitions establish standardized benchmarks for evaluating different algorithms and offer a unified framework to compare performance and identify the most effective solutions."}, {"title": "III. REINFORCEMENT LEARNING PRELIMINARIES", "content": "In this section, we will introduce the fundamental definitions and presentations of RL and MARL. We want to make sure the readers can comprehend the rest of this paper under a unified mathematical language."}, {"title": "A. Deep Reinforcement Learning", "content": "Reinforcement Learning is a classical approach to sequential decision-making problems. We typically use the Markov decision process (MDP) to describe the decision-making procedure in RL paradigm, where the state distribution at the next time-step is only determined by the action and state of the current time-step and irrelevant to its history. Specifically, MDP can be denoted as a tuple (S, A, R, T, \u03b3), where S and A present the state space, action space and the stochastic observation space. T : S \u00d7 A \u00d7 S \u2192 [0, 1] is the transition probability under a given state-action pair and includes the uncertainty of the system. R : S \u00d7 A \u00d7 S \u2192 r indicates the reward function which issues immediate reward value r at each time-step and \u03b3 presents the discount factor in the optimization objective. For a behavior trajectory, Equation (1) accumulates the discounted reward value ri at each time-step and our objective is to find an optimal policy to maximize the total return Gt.\n\nGt = rt+1 + \u03b3rt+2 +\u2026 = \\sum_{k=0}^{\\infty} \u03b3^{k}rt+k+1\\tag{1}"}, {"title": "B. Multi-Agent Deep Reinforcement Learning", "content": "According to the assumption and agent settings, we can introduce diverse probabilistic models to represent the tasks. As an extension of MDP, Markov Game (MG) presents the interaction process of a multi-agent system. MG is denoted a tuple (N, S, {Ai}, {Ri}, P, \u03b3), where N = {1,2,\u2026, N} denotes the set of interacting agents and S is the global state from all agents with i \u2208 N. Similarly, {Ai} and {Ri} are the set of individual action and reward. Note that Ai is the action space of the i-th agent so the joint action space is A := A1\u00d7 A2x...\u00d7 AN. Like single-agent RL, P and \u03b3 indicate the transition probability and discount factor. At time step t, each agent executes action aiti \u223c \u03c0iti according to the system state stiti, and then the system transits to the next state st+1iti+1 and obtain the reward riti+1 := Ri(st+1it+1|staiti,a1t,\u2026\u2026\u2026, aNiti).\nHence, we can"}, {"title": "C. Learning Schemes", "content": "With the increasing number of agents, the complexities of state and action spaces would grow up exponentially, which causes the policy learning of MAS present a computational challenge. Generally, the entire process can be divided into two stages: Training and Testing. Training indicates the process where agents acquire data from interaction to obtain experience and update policies. After that, we evaluate the policy performance in the environment without any policy optimization, which is referred to as testing. Based on the classic categorization [113], the training process can be broadly classified into two paradigms: centralized and decentralized."}, {"title": "D. Issues of MARL", "content": "1) Non-Stationarity: Non-stationarity is a significant issue for decentralized MAS, where agents interact within a shared environment and update their policies synchronously. Consequently, each vehicle doesn't know the full environment dynamics, and the next state doesn't depend solely on its action and current state so it would break the Markov assumption [15]. In the CTDE paradigm [24], the centralized critic has access to all agents' observations and actions. Since only the actor computes the policy and the critic component can be removed during testing, agents in the CTDE scheme have fully decentralized execution. For independent policy learners, a naive approach is to let agents either ignore the presence of others or proceed under the assumption that the behaviors of others are static [121]. In this context, agents are independent learners, which enable the conventional single-agent RL algorithms for policy learning and have been proven to achieve excellent results on various benchmarks [122]. However, in complex and stochastic environments, independent policy learning may result in sub-optimal performance or tend to exhibit a propensity for over-fitting to the policies of other agents, leading to a lack of generalizability in testing. To improve independent learning performance, researchers propose adopting different learning rates with shared rewards to achieve the optimal joint policy [123]. Another method involves the refinement of experience replay. Due to the non-stationarity of the environment, experience replay may store more irrelevant experiences to decentralized learning with the increasing time steps. Importance sampling corrections for stable experience replay is also a solution, which adjusts the weights between the prior and the new experience under different environment dynamics. This approach has been proven to enhance the performance of independent learners in complex gaming and robotic environments [124]\u2013[126].\n2) Partial Observability: In partially observable environments, agents do not have access to the full state of the environment. Instead, each agent receives only a local observation that provides incomplete or noisy information about the true state. To make effective decisions, agents must estimate or infer the underlying state of the environment from their partial observations. This often requires maintaining a a probability distribution over possible states, which adds computational complexity and uncertainty to the decision-making process. At the same time, in a partially observable setting, different agents might have different pieces of information about the environment. Coordinating actions effectively requires agents to share information or make decisions based on limited and potentially inconsistent knowledge."}, {"title": "IV. STATE-OF-THE-ART METHODOLOGIES", "content": "This section will introduce recently the most advanced MARL methodologies for motion planning and control of multi-vehicle systems. We cannot encompass all the related studies, but select representative techniques in this survey are sourced from reports published in the most influential conferences and journals. Furthermore, we encourage the researchers to report more relevant works to our website."}, {"title": "A. Centralized Multi-Agent RL", "content": "In the CTDE scheme, each vehicle has an independent policy network, and a core computer is set to merge and process the information from all vehicles. We first get the merged observation from all vehicles, evaluate the system state by a pre-defined global reward function, and then train the independent policies after credit assignment. PRIMAL [154] is a milestone work in centralized training for pathfinding. It assigns each agent an independent and fine-designed parameter-sharing actor-critic network and trains them with A3C [155] algorithm. In this work, researchers illustrate that independent policies lead to selfish behaviors, and a hand-crafted reward function with a safety penalty is a good solution. Additionally, there is a switch to allow agents to learn from interaction or expert demonstrations. The combination of reinforcement learning and imitation learning contributes to fast learning and alleviates the negative impact of selfish behaviors on the overall system. In this paper, a discrete grid world is defined, and the local state of each agent is set as the information of a 10\u00d710 block with the unit vector directed toward the goal. To verify the feasibility in the real world, the authors also implement PRIMAL on AVs in a factory mockup.\nIn MADDPG [24], the authors propose the first generalizable CTDE algorithm based on deep deterministic policy gradient (DDPG) [156] with a toy multiple-particles environments. It provides an essential platform with easy vehicle dynamics to learn the continuous driving policies with continuous observation and action spaces under design-free scenarios and attracts many remarkable followers [21], [157]. Meanwhile, the combination of value function decomposition methods and CTDE scheme has achieved better scalability w.r.t. the number of agents and mitigates the impact of non-stationary on policy training, thereby improving performance in large-scale multi-agent systems [116], [158]. These methods have been verified in complex scenarios like unsignalized intersections in Highway-Env [84], [159]. Also, expert demonstration contributes to reducing the risk of converging to sub-optimal policies [159]. To verify the feasibility of deploying the CTDE approach in mapless navigation tasks, Global Dueling Q-learning (GDQ) [160] sets up an independent DDQN [161] for each turtlebot3 in the MPE [24] to train policies and estimate values. Additionally, they introduced a global value network that combines the outputs of the value networks of every agent to estimate the joint state value. This method has been proven to be more effective than normal value decomposition methods. Meanwhile, researchers also attempt to extend fundamental algorithms in single-agent RL such as PPO [65] or SAC [66] to multi-agent tasks and provide many significant baselines like MAAC [162] and MAPPO [163]. In particular, MAPPO has been verified comprehensively on massive benchmarks and has systematic guidance of hyperparameter selection and training. To overcome the sim-to-real gap and deploy MAPPO on real robots, developers train a policy in the Duckietown-Gym simulator for following waypoints on the ground. The MAPPO policy network adopts recurrent neural network [164] to recall the knowledge of the prior state and output the high-level target linear velocity and angular rate for each vehicle. Like most indoor navigation tasks, the optical track system captures the position and attitude of vehicles. With the linearized inverse kinetics, the executive low-level command of the vehicle can be obtained"}, {"title": "B. Independent Policy Optimization", "content": "Regarding practical deployment challenges such as communication, bandwidth, and system complexity, the fully decentralized system reduces communication overhead and bandwidth requirements by allowing agents to operate independently without constant coordination. Additionally, it is easier to deploy in environments with limited or unreliable communication infrastructure, lowers decision-making latency, and simplifies local computation for each agent. These factors make decentralized MARL a more practical and adaptable approach for real-world multi-agent applications. In recent years, Independent Policy Optimization (IPO) [165] has obtained increasing attention, and massive related approaches have been proposed. Concurrently, the complexity of the scenarios addressed in these studies and the scale of the agents involved have also been increasing synchronously, which reflects that decentralized learning matches the demands of large-scale autonomous driving in the real world more.\nTo solve the scalability issue in centralized schemes, MAPPER [166] employs a decentralized actor-critic based on the A2C [155] algorithm. Firstly, the local observations of the occupancy map are represented as a 3-channel image containing static scenes, dynamic obstacles, and planned trajectory information from A* planner [167]. These 3-channel observations are abstracted into a latent vector via a CNN, along with waypoint information abstracted by an MLP, input into shared fully connected layers. Later, two independent MLPs output action probabilities and value estimates, respectively. Besides, MAPPER employs an extra evolutionary algorithm to eliminate bad policies during the optimization process. Compared with PRIMAL [154], MAPPER can learn faster and handle dynamic obstacles more effectively in large-scale scenarios. Another work scalability is G2RL [168], a grid map navigation method that can be used for any arbitrary number of agents. Similarly, it leverages A* to provide each agent with a global guiding path. Meanwhile, the local occupancy map is input into a local DDQN [161] planner to capture local observation and generate a corrective command to avoid dynamic obstacles. Since there is no need for communication between agents, this method does not require consideration of communication delays and can be extended to any scale.\nAs the successor to PRIMAL, PRIMAL2 [169] retains the same hierarchical structure, i.e., an A* planner generating global paths and agent training guided by A3C and imitation learning. The key difference lies in PRIMAL2's fully decentralized training approach, which enhances its flexibility in handling structured and high-density complex scenarios. Like MAPPER, it adopts an 11\u00d711 observation range and splits observations into multi-channel image inputs. The first 4 channels include static obstacles, the agent's own goal point, other agents' positions, and other agents' goal points. Channels 5-8 provide the local path from A* and the positions of other agents at three future timesteps within the observation range."}, {"title": "C. Learning with Social Preference", "content": "Although independent policy learning is feasible in many tasks, it would lead to each agent being self-centric [20] when the interests of multiple agents conflict, the pure egoistic independent policy learning may fail. Therefore, an important issue is balancing the agents' egoism and altruism. To find a mathematical presentation of social preference, in the early work, researchers first propose to use a trigonometric function like Eq. (12) to balance the individual and global reward, which inspires afterward studies [20], [62].\nAfterward, as the representative work of driving with social preference, Coordinate Policy Optimization [20] (CoPO) draws inspiration from the self-driven particle systems in nature like fish and bird flocks and implements a decentralized MARL method with heterogeneous vehicle settings in MetaDrive [63]. It proposes hierarchical coordination for policy optimization to trade off the egoism and altruism of the policy. It utilizes the IPO for each vehicle within the traffic system to avoid the credit assignment problem. More specifically, by introducing a local coordination factor (LCF) in the training process, the agent seeks the optimal policy to maximize the averaged reward from all adjacent agents in its observable range. As Fig. 5, for the i-th agent ai in the system with time-invariant observed adjacent agents N(i,t) in the range dn, the agent should balance its ego reward ri and the average reward ri(t) as shown in Eq. (12).\n\nr(t) = \\frac{1}{|N(i,t)|}\\sum_{j \\in N(i,t)} r_j(t) \\tag{11}\n\nr_f(t) = cos(\\phi)r_i(t) + sin(\\phi)r(t), \\phi \\in [-90^{\\circ},90^{\\circ}] \\tag{12}\n\nLCF \u03c6 indicates the specific altruism of the policy under a certain scenario, so the major difficulty of local coordination comes from selecting optimal LCF to maximize the global reward. Hence, in the global coordination, the author presents LCF as a Gaussian distribution \u03c6 \u223c N(\u03c6\u00b5, \u03c6\u03c3) and the global objective is to find out the optimal policy with maximum accumulative rewards JG(\u03b81, \u03b82,...) = E\u03c4 \\sum_{i \\in N} \\sum_{t=0}^T r_i(t). To enable LCF to be learnable, the authors transfer the global objective into individual objective following with an easy factorization technique [116] and then derive the gradient from Eq. (13) to (15), where \u03b8\u00b0 and \u03b8' denote the old new updated policy network of agent i.\n\nJ_F(\\theta^i) = \\nabla_{\\theta^i} J_G(\\theta^i) \\tag{13}\n\n\\nabla_{\\theta^i} J_F(\\theta^i) = \\mathbb{E} [\\nabla_{\\theta^i} min(p_G A_G, clip(p, 1 - \\epsilon, 1 + \\epsilon) A_G)] \\tag{14}\n\n\\nabla_{\\theta^i} \\theta_\\phi = \\alpha \\cdot \\mathbb{E} [\\nabla_{\\theta^i} log \\pi_{\\theta_i}(a_i|s) \\nabla A_{\\phi^i} A_{\\phi, i}] \\tag{15}\n\nHere, \u03a6 = [\u03a6\u00b5,\u03c6\u03c3] denotes the mean and variance of LCF and is learnable, and \u03b1 denotes the learning rate. In Eq. (14) and (15), AG and A\u03c6,i is the advantage of global and locally coordinated reward respectively following the idea in [65]. Since Eq. (14) has no relevant term of I, it can be regarded as a constant in the LCF objective. Hence, the objective of learning can be established as follows:\n\n\\nabla_{\\theta^i} J_{LCF} (\\varphi) = \\mathbb{E} [\\nabla_{\\theta^i} J_F(\\theta^i) [\\nabla_{\\theta^i} log \\pi_{\\theta_i} (a_i|s) A_{\\phi,i}]] \\tag{16}\n\nIntroducing a learnable factor to present social preference outperforms independent learning and mean-field policy optimization. It has also inspired many valuable discussions in this field about learning independent policy with sociological design and led to other excellent works. In subsequent research, people leverage more advanced language models like Transformer [137] to process social preference. For instance, in Social-Attention Policy Optimization (SAPO) [62], the"}, {"title": "D. Safe and Trust-Worthy Learning", "content": "Safety is integral and the first priority to the deployment of autonomous driving systems, as they directly impact the reliability and people's lives of AVs. Recent RL researchers put massive efforts into ensuring the learned policy would never cause safety issues in the exploration process and after deployment. Specifically, inspired by [172], we categorize existing safety standards and methods in MARL into three types. First, soft safety guarantees involve designing safety penalty terms to reduce the probability of dangerous behavior. With fine-tuned rewards, the learning algorithm can be guided to prioritize safety alongside other performance metrics. However, although they have been proven to effectively improve safety performances in MAS, the limitation of soft guarantees is that they rely on the assumption that the reward function can accurately capture all safety aspects, which is often challenging in complex environments. The second is the probabilistic guarantees happening in the optimization process. For example, some recent MARL algorithms leverage the Lagrange constraints [21] or safety threshold during policy optimization process [173], [174]. Essentially, this improves policy gradient and helps avoid dangerous exploration behaviors. However, since the policy is still represented as a probability distribution, we cannot obtain a clear, explainable, and stable safety boundary for this method. Meanwhile, the vital safety constraints in real-world driving are instantaneous and deterministic [175]. For example, collision avoidance is a state-wise instantaneous constraint that only depends on the current state of the system rather than historical trajectories or random variables.\nThe third and safest approach is using hard safety boundaries to apply instantaneous strong corrections for agents' actions. For instance, researchers propose to learn the centralized shielding [184] from the joint action to correct any unsafe action in MAS at any risky time. Alternatively, combined with"}, {"title": "E. Methodological Summary", "content": "As shown in Table II, we collect representative works on MARL in outdoor autonomous driving, traffic system control, and structured scene transportation in the past five years. Meanwhile, we list their taxonomy, the maximum number of agents, the simulators, and whether real-world experiments are conducted. Here, we note that the action settings can be completely different even with the same simulation type. For example, in PRIMAL and PRIMAL2, the agent's actions are set as (\u2191, \u2192, \u2193, \u2190, \u2205), representing four movements in the horizontal and vertical directions in a 2D grid map, along with staying in place. In contrast, MAPPER adds four additional diagonal movements (\u2197, \u2196, \u2198, \u2199) for the agents. Additionally, we find that many studies adopt predefined high-level action commands to simplify tasks. The policy network outputs discrete values that map to corresponding preset actions, and then a low-level controller takes the actions, generates commands, and sends them to the actuators. Two other specific examples are MFPG [182] and CPO-AD [183]. They preset a low-level unidirectional control mapping and only consider the movement of AVs in one direction."}, {"title": "V. OPEN QUESTIONS AND CHALLENGES", "content": "In this section, we present the main challenges in MARL. Note that the problems faced by the CTDE and DTDE schemes are different, and though some feasible solutions have been proposed to solve these issues, they are still not unique and perfect. We hope that readers can become aware of their existence and properties in advance, thereby gaining a better understanding of the motivation and technical innovation from subsequent advanced methodologies."}, {"title": "A. Multi-modal Information", "content": "Autonomous driving is a sequential decision-making process that leverages multi-modal information. Compared to MLPs in the original algorithm designs, recent research focuses more on designing more complicated neural network modules to learn better representations from multiple sensor information and to suit time series information."}, {"title": "B. Robustness and Generalizability", "content": "Robustness and generalizability are critical factors for the effectiveness of MARL algorithms in autonomous driving. Robustness allows the vehicle to safely navigate a wide range of real-world conditions, while generalizability ensures the system can be applied broadly across different environments and scenarios. These qualities are fundamental to advancing autonomous driving technologies and achieving practical deployment in diverse and unpredictable real-world settings."}, {"title": "C. Safety Certificates", "content": "Safety is a paramount concern in the development of AV. Ensuring the reliability and robustness of these systems involves addressing several critical aspects, from extensive real-world testing to mitigating issues related to multi-agent communication and data transfer."}, {"title": "D. Explainability", "content": "Explainability emphasizes the understandable and causable decision-making process, which ensures the actions are transparent and reasonable. However, nowadays, most learning-based methods still adopt the black-box deep neural network as the main component. As the primary shortcomings of black-box models, the lack of transparency poses significant safety concerns [214]. This opacity can lead to a lack of trust and confidence in the system, especially in safety-critical situations where understanding the reasoning behind a decision is crucial. For instance, if an AV makes an unexpected maneuver, it is essential to know whether the decision is based on valid reasoning or a flaw in the system."}, {"title": "VI. FUTURE DIRECTIONS", "content": "In the last section, we briefly introduce the latest advancements and explain why we believe these directions are promising. We hope that this information will inspire researchers and lead to more outstanding research."}, {"title": "A. Model-based MARL", "content": "Model-based RL has achieved significant progress in single AV driving. By incorporating additional neural networks, we can model complex nonlinear dynamics and state transition functions [225], [226]. In recent research, researchers implement real-world high-speed racing in complicated tracks via extra 4 networks for environmental dynamics and prediction of future state, observation, and reward [192]. However, there is no free lunch. While model-based reinforcement learning offers better performance and improves explainability, it also increases the requirement for computational resources. Typically, centralized approaches model the environment through joint actions and observations and get rid of the non-stationarity and partial observability. However, scalability remains a significant challenge, especially for heterogeneous MAS. Conversely, decentralized approaches are easier to scale but struggle to reach consensus in non-stationary dynamics and partially observable environments. Beyond the design of communication protocols, recent research is exploring the possibility of abstracted and simplified modeling, such as inferring and predicting the subsequent actions of other agents. Additionally, decentralized paradigms introduce more networks for learning models, so both the selection of model representations and the design of efficient network architectures are promising topics."}, {"title": "B. Development of Offline Multi-Agent Datasets", "content": "Offline paradigms can improve the practicality and realism of reinforcement learning. Specifically, online trial-and-error learning can cause financial losses and social disruption"}]}