{"title": "Multi-Agent Reinforcement Learning for Autonomous Driving: A Survey", "authors": ["Ruiqi Zhang", "Jing Hou", "Florian Walter", "Shangding Gu", "Jiayi Guan", "Florian R\u00f6hrbein", "Yali Du", "Panpan Cai", "Guang Chen", "Alois Knoll"], "abstract": "Reinforcement Learning (RL) is a potent tool for sequential decision-making and has achieved performance surpassing human capabilities across many challenging real-world tasks. As the extension of RL in the multi-agent system domain, multi-agent RL (MARL) not only need to learn the control policy but also requires consideration regarding interactions with all other agents in the environment, mutual influences among different system components, and the distribution of computational resources. This augments the complexity of algorithmic design and poses higher requirements on computational resources. Simultaneously, simulators are crucial to obtain realistic data, which is the fundamentals of RL. In this paper, we first propose a series of metrics of simulators and summarize the features of existing benchmarks. Second, to ease comprehension, we recall the foundational knowledge and then synthesize the recently advanced studies of MARL-related autonomous driving and intelligent transportation systems. Specifically, we examine their environmental modeling, state representation, perception units, and algorithm design. Conclusively, we discuss open challenges as well as prospects and opportunities. We hope this paper can help the researchers integrate MARL technologies and trigger more insightful ideas toward the intelligent and autonomous driving.", "sections": [{"title": "I. INTRODUCTION", "content": "LARGE-SCALE autonomous driving systems have attracted tons of attention and millions of funding from industry, academia, and government in recent years [1], [2]. The motivation behind developing such a system is to replace human drivers with automated controllers. It can significantly reduce the time consumption and workload of driving, enhance the efficiency and safety of transportation systems, and promote economic development. Generally, to detect the vehicle states and generate reliable control policies, automated vehicles (AVs) should be equipped with massive electric units, like visual sensors including radars, light detection and ranging (LiDAR), RGB-Depth (RGB-D) cameras, event cameras, inertial measurement units (IMU), global positioning system (GPS) and so on [3]\u2013[5]. A salient challenge in this topic is to build a robust and efficient algorithm that is capable of processing massive information and translating this data into real-time operations. Early works divide this big issue into perception, planning, and control problems and solve them independently, known as modular autonomous driving.\nOn the other hand, as a powerful toolkit for sequential decision-making, reinforcement learning (RL) can optimize agent behavior models with the reward signal. As its evolution, deep RL combines the advantages of RL and deep neural networks, which enable to abstract complex observations and learn efficient feature representations [6]. In the past representative research, it has exhibited performances in domains such as board games [7], [8], video games [9], [10] and robotic control [11]\u2013[13], where it rivaled or even surpassed human performances. For autonomous driving, RL brings end-to-end control into reality, which transitions directly from what the vehicle senses to what the vehicle should do, like human drivers. While RL has obtained many remarkable achievements on AVs, most of the related work has approached the issue from the perspective of individual vehicles, which leads to self-centric and possibly aggressive driving strategies, which may cause safety accidents and reduce the efficiency of transportation systems.\nFor real-world traffic systems, we typically define them as multi-agent systems (MAS) and aim to optimize the efficiency of the entire system rather than merely maximizing individual interests. In MAS, all agents make decisions and interact within a shared environment. This means that the states of each agent depend not only on its actions but also on the"}, {"title": "II. AUTONOMOUS DRIVING BENCHMARKS", "content": "RL is always data-hungry. Generally, it requires continuous interaction with the environment to obtain behavior trajectories, which facilitates more accurate value estimations from deep neural networks [35], [36]. However, due to the economic damage caused by uncertain exploration processes, we typically would not deploy our RL policies on real robots directly. Consequently, within the RL paradigm, data from real driving and high-fidelity simulators are ubiquitously adopted in the development of RL-based autonomous driving. In this section, we will introduce various data sources for large-scale MARL in autonomous driving and traffic systems."}, {"title": "A. What is important for a good benchmark?", "content": "A benchmark involves the simulation of physical models, optical rendering, environment and interaction mechanisms, algorithms, and other complex tasks. For MARL-based autonomous driving, we identify the following crucial criteria. We list them out here and will analyse existing data resourses by these metrics.\n1) Realism and Fidelity: High realism and fidelity ensure that the simulator can accurately replicate real-world driving conditions like weathers, lights, environmental dynamics, etc., and mitigate their distributional bias before real-world deployment. Deep learning models especially deep RL demand accurate data. In this case, the realism simulator ensures that its data is representative of real-world scenarios.\n2) Scalability: Scalability ensures that simulators can handle the dynamic environments with numerous entities and variables, so that we can mimick the complexity of real-world scenarios. Scalable MARL algorithms can efficiently learn from the interaction among time-variant numbers of agents like the real traffic system.\n3) Diversity: Diverse scenarios ensure that vehicles can be tested in a wide range of situations, including different traffic conditions, weather, and road structures. For deployment in the real world, it also contributes to the development of more robust autonomous driving systems and makes AVs more reliable in unpredictable real-world conditions. Simultaneously, diversity also implies that the agents could be heterogeneous, which brings more complex interactions and matches to the realities of actual traffic systems.\n4) Efficiency: Time and computational resources are both significant concerns for MARL and autonomous driving [37], [38]. Lightweight simulators reduce computational consumption, and experiments work on cheaper and smaller hardware. Meanwhile, highly-parallelized simulator allows multiple environments to run concurrently and promotes the training process of MARL algorithms. Note that there is always a hard trade-off between fidelity and efficiency [39]. High fidelity requires complex computations to replicate real-world scenarios, especially for 3D visual information.\n5) Transferability: Transferability requires the simulator to support various sensors technically and to replicate their characteristics. For autonomous driving tasks, there are significant differences in the data formats and frame rates of LiDAR, IMU, and cameras. It is essential for the simulator to maintain consistency in the sensor parameters of the intelligent agents with those of commercially available devices. Moreover, transferability is also presented in the simulator's compatibility with the vehicle's device in terms of programming language, communication protocols, and computing platforms.\n6) Features, Maintenance and Supports: Reproducing the effectiveness of algorithm is always time-consuming. Therefore, it would be beneficial for developers to provide fundamental and verified baselines for testing, also with user-friendly application programming interfaces (APIs), annotations and tutorial documentation. These provisions would establish a fair and open comparison standard and improve the efficiency of subsequent developments. Furthermore, lasting maintenance is necessary. Hardware and software frequently discard old features and develop new ones, which can make data and code outdated. Therefore, continuous maintenance of datasets and code bases is an important task."}, {"title": "B. Advanced Simulators", "content": "The selection of a simulator for MARL-based autonomous driving is a critical step. A good selection would save resources and enable the generation of vast amounts of diverse and high-quality training data, which is essential for effectively training MARL algorithms. Additionally, the simulator allows for parallelized testing and training and significantly accelerates the development process by reducing the time required to experiment with various scenarios and conditions. The advantages on extensive data generation and enhanced time efficiency make simulators indispensable for advancing autonomous driving technologies through MARL.\n1) The Open Racing Car Simulator: TORCS [40] was first released in 2000. As a highly modular simulator for multi-agent racing, each race car offers low-level APIs to access partial vehicle states and provides visual information from multiple perspectives. After decades of evolution, it has accurate and editable vehicle dynamics, including the rotational inertia of different components, mechanical structures, tire dynamics, and a simplified aerodynamic model. The simulator supports discrete-time simulations with high frequency up to 500Hz, which allows for the development of complex, high-speed, and aggressive driving controllers on it [41]. Many representative competitions and RL-based research conduct their research on TORCS [42]\u2013[44]. Afterward, Gym-TORCS was released and aligned to OpenAI Gym [45], which provides unified APIs and a Python wrapper to facilitate the rapid development of RL-based controller [46]. However, it still lacks support for MARL and a paralleled environment. Later, MADRAS [47] filled this vacancy and offered both single-agent and multi-agent environments and interfaces, which could be used to test autonomous vehicle algorithms both heuristic and learning based on an inherently multi-agent setting.\n2) Simulation of Urban Mobility: SUMO [48] has become a famous benchmark for the simulation of large road networks. It supports a wide range of scenarios and rich APIs so that users can customize traffic scenarios easily. Meanwhile, SUMO provides well-documented tutorials to assist users in implementing their simulations. In recent years, numerous studies have utilized this simulator to develop efficient and safe MARL algorithms for complex scenarios and obtain notable achievements [49]\u2013[51]. To accelerate RL research, developers released a new simulator Flow [52] with interfaces to the distributed RL framework RLLIB [53] to achieve high-frequency traffic flow simulation. At the same time, it permits the integration with Amazon Web Services (AWS) elastic compute cloud and expands the variety of controllers, which brings higher flexibility and enables the training of large-scale RL policies. In CityFlow [54], developers improved their computational speed to 20 times higher than SUMO. Hence, the real-time simulation of city-level traffic networks becomes possible. CityFlow also expands interfaces for MARL algorithms and allows external data import, which means it can simulate accurate data and generate nearly authentic samples for policy optimization.\n3) Scalable Multi-Agent Reinforcement Learning Training School: SMARTS [55] is one of the most advanced traffic simulators proposed by Huawei Noah's Ark Lab. It is established on the Social Agent Zoo platform and provides various heterogeneous agent assets for structured traffic flow. SMARTS also has Gym-standardized APIs and integrates broader MARL libraries like PyMARL [56], MALib [57] and RLLIB. Moreover, it supports SUMO as the background provider but optimizes its vehicle dynamics with a Bullet-based physical engine [58]. Implementing high-speed distributed computation introduces a bubble mechanism, which allows the elastic assignment of computational resources on local or remote machines. Furthermore, it offers a strong visualization toolkit through web streaming and allows developers to monitor the process from anywhere. Unlike other one-off works, SMARTS has established a substantial and stable community and promotes many promising works [59]\u2013[62]. So far, its developers have maintained and constantly expanded the simulator's functionalities.\n4) MetaDrive: MetaDrive [63] is one of the latest multi-agent system simulators based on Panda3D [64], possessing a broad asset library and allowing the import of external data. Beyond the given structured scenarios, developers can easily customize road map and traffic flows, and set the attributes of scene components via high-level APIs. Meanwhile, it establishes hierarchical management of assets by defining four types of manager classes, which facilitates developers in customizing agent mixtures, interactions and policy generalizability tests. Theoretically, MetaDrive can create an infinite variety of traffic scenarios. It employs state vectors as the agents' observations and provides abundant RL benchmarks, including model-free RL [65], [66], imitation learning [67], and offline RL [68]. Although it forsakes fine-grained visual information, it enables more rapid and efficient simulation and has triggered many insightful works [20], [69].\n5) CAR Learning to Act: CARLA [70] is one of the state-of-the-art open-source 3D simulators for its realistic Unreal Engine 4-based dynamics simulation, lightweight optical rendering, and comprehensive technical support. For environmental information, it provides detailed scenes with various architectures, road configurations, and natural conditions, especially diverse weather settings, which are beneficial for the generalizability test of policy. Through its free asset library, it can simulate high-density traffic flow, pedestrians with different behaviors and traffic lights and signs, which makes it possible for automated agents to comprehend traffic regulations. Significantly, CARLA supports various sensors including LiDAR, RGB-D cameras, GPS, radar, and event cameras with editable characteristics and realistic noise. With rich C++ and Python-based APIs, researchers can freely define the attributes of agents and then analyze the collected data. Nowadays, CARLA not only makes substantial contributions to the MARL field but also continuously impacts computer vision research. A welcome trend is that its developers have established an official website with a good tutorial, blog, and user community for further updates and improvement. As a good supplement, based on CARLA, some researchers have developed a new benchmark named MACAD [71] with a faster implementation for MARL and integrated Gym-like APIs. Generally, CARLA significantly facilitates systematic research [72]\u2013[74] for vision-based driving and reduces the simulation-to-reality (sim-to-real) gap, which is crucial for deploying the large-scale driving controller.\n6) Virtual Image Synthesis and Transformation for Autonomy: VISTA [75] is a data-driven simulator and synthesizes time series of perceptual inputs from real-world. In contrast to physical simulators, VISTA aims to reconstruct that world and synthesize novel viewpoints within the environment via inputting real data of the physical world. Different sensing modalities, environments, dynamics, and tasks with varying complexity are supported.Meanwhile, it is highly modular, customizable, and extensible. Since the behavior trajectories are generated from real data, the sim-to-real gap is minimized, which is empirically validated by researchers in real-world experiments. In the subsequent VISTA 2.0 [76], researchers integrate UNet architecture [77] to reproduce the dense output of LiDAR and apply temporal interpolation to estimate the events through RGB images, providing an additional two sensor types and data formats for AVs. Simultaneously, researchers also introduce a version that supports multi-agent interactions and validates basic scenarios involving multiple AVs [78]. However, its application in large-scale complex scenarios remains unexplored.\n7) NVIDIA ISAAC-Sim: ISAAC-Sim [79] is established on the PhysX5 engine and supports GPU-based photorealism"}, {"title": "C. Datasets", "content": "After decades of development, to effectively solve real-world driving problems, developers have collected tons of on-road data and established rich repositories with different sensors in various scenarios, such as KITTI [96], nuScenes [97], and Waymo Dataset [98]. However, for decision-making problems, these datasets do not record real-time actions like accelerating, braking, steering, or actuator outputs, so they are typically used only for visual tasks or multi-modal sensor fusion. For example, although the IMU and GPS history is given out in BDD-100K [99], the information is on a different domain from human driver actions, and further policy adaption and transferring would be difficult. Moreover, most datasets address the autonomy of a single vehicle rather than collaboration between multiple AVs. Collecting and aligning multi-vehicle data simultaneously is often expensive and difficult, so nowadays MARL paradigms are mostly verified in simulators. However, real vehicle decision data is still invaluable especially for imitation learning [67] and offline RL [68]. Compared to data obtained in simulators, although theoretically we cannot interact for infinite time like simulation, the data distribution is closer to real driving.\nTo provide diverse interactive data for sequential decision-making, the INTERACTION dataset [100] investigates driving habits across different cultural backgrounds and collects bird's-eye-view data via hovering drones with fine-grained annotations. Based on the nations and road structures, it compiles 11 sub-datasets including highways, roundabouts, intersections, merges, and unstructured roads. Additionally, the dataset includes rare collision data and aggressive driving behaviors. Afterwards, the latest research proposes a new benchmark AD4RL [101] based on Next-Generation Simulation (NGSim) US-101 dataset [102]. This benchmark provides 19 datasets from real-world human drivers after fine correction, value normalization, and alignment with partially observable Markov decision process. In other words, it can directly work as policy trajectories in the RL scheme. Additionally, it offers 7 popular offline RL algorithms applied in 3 realistic driving scenarios. A unified decision-making process model is also attached for verification across various scenarios on Flow simulator [52], which serves as a reference framework for algorithm design. Recently, the development and supplement of datasets for the offline MARL approach in large-scale autonomous driving and traffic systems remain a work in progress. We consider this an up-and-coming area and will discuss it later (see Section VI-B)."}, {"title": "D. Competitions", "content": "We notice that many recent competitions have been hosted during the international conference sessions, and here we address and appreciate the contribution of their organizers. These competitions provide a platform for researchers to showcase and compare their algorithms, drive the emergence of new techniques, and promote the application of MARL in real-world scenarios.\nThe earliest multiple vehicles involved in competition can be traced back to the DARPA's Urban Challenge [103]. Although this challenge involved controlling only one vehicle, it required real-time controller adjustment based on a dynamic environment. In the past five years, an increasing number of companies and institutions have recognized MARL's potential and organized academic competitions based on relevant simulators and datasets. At DAI 2020, a multi-vehicle control competition was organized using the SMARTS [55] simulator. Participants were required to develop a parameter-sharing multi-agent model to control a group of agents to accomplish short missions in ramp, double merge, T-junction, crossroads, and roundabout. Instead of testing in the simulation, the DuckieTown [104] AI Driving Olympics (AI-DO) competition on NeurIPS 2021 required participants to deploy real vehicles on scaled-down tracks. The vehicles had to navigate complex urban roads, avoiding pedestrians and other vehicles. The AI-Do competition marks a milestone in the practical deployment of embodied intelligence in complex MAS. At NeurIPS 2022, Huawei's Noah's Ark Lab again organized competitions based on the SMARTS simulator, featuring online and offline reinforcement learning tracks. OpenDriveLab consecutively hosted multi-track autonomous driving competitions on CVPR 2023 and 2024. In the latest Autonomous Grand Challenge, the organizers provided the offline end-to-end autonomous driving scale competition and the online CARLA competition. For the former one, participants were required to develop motion planning algorithms for complex scenarios using offline data from Motional nuPlan [105] dataset. The second track required the design of flexible policy learning methods in the CARLA [70] simulator. These competitions establish standardized benchmarks for evaluating different algorithms and offer a unified framework to compare performance and identify the most effective solutions."}, {"title": "III. REINFORCEMENT LEARNING PRELIMINARIES", "content": "In this section, we will introduce the fundamental definitions and presentations of RL and MARL. We want to make sure the readers can comprehend the rest of this paper under a unified mathematical language."}, {"title": "A. Deep Reinforcement Learning", "content": "Reinforcement Learning is a classical approach to sequential decision-making problems. We typically use the Markov decision process (MDP) to describe the decision-making procedure in RL paradigm, where the state distribution at the next time-step is only determined by the action and state of the current time-step and irrelevant to its history. Specifically, MDP can be denoted as a tuple $(\\mathcal{S}, \\mathcal{A}, R, T, \\gamma)$, where $\\mathcal{S}$ and $\\mathcal{A}$ present the state space, action space and the stochastic observation space. $T: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0, 1]$ is the transition probability under a given state-action pair and includes the uncertainty of the system. $R: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow r$ indicates the reward function which issues immediate reward value $r$ at each time-step and $\\gamma$ presents the discount factor in the optimization objective. For a behavior trajectory, Equation (1) accumulates the discounted reward value $r_t$ at each time-step and our objective is to find an optimal policy to maximize the total return $G_t$.\n$G_t = r_{t+1} + \\gamma r_{t+2} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}$ (1)"}, {"title": "B. Multi-Agent Deep Reinforcement Learning", "content": "According to the assumption and agent settings, we can introduce diverse probabilistic models to represent the tasks. As an extension of MDP, Markov Game (MG) presents the interaction process of a multi-agent system. MG is denoted a tuple $(\\mathcal{N}, \\mathcal{S}, {\\mathcal{A}^i}_{i\\in\\mathcal{N}}, {R^i}_{i\\in\\mathcal{N}}, P, \\gamma)$, where $\\mathcal{N} = {1, 2, \\dots, N}$ denotes the set of interacting agents and $\\mathcal{S}$ is the global state from all agents with $i \\in \\mathcal{N}$. Similarly, ${\\mathcal{A}^i}_{i\\in\\mathcal{N}}$ and ${R^i}_{i\\in\\mathcal{N}}$ are the set of individual action and reward. Note that $\\mathcal{A}^i$ is the action space of the $i$-th agent so the joint action space is $\\mathcal{A} := \\mathcal{A}^1 \\times \\mathcal{A}^2 \\times \\dots \\times \\mathcal{A}^N$. Like single-agent RL, $P$ and $\\gamma$ indicate the transition probability and discount factor. At time step $t$, each agent executes action $a_t^i \\sim \\pi^i$ according to the system state $s_t$, and then the system transits to the next state $s_{t+1}$ and obtain the reward $r_{t+1}^i := R^i(s_{t+1}| s_t, a_t^1, \\dots, a_t^N)$. Hence, we can"}, {"title": "C. Learning Schemes", "content": "With the increasing number of agents, the complexities of state and action spaces would grow up exponentially, which causes the policy learning of MAS present a computational challenge. Generally, the entire process can be divided into two stages: Training and Testing. Training indicates the process where agents acquire data from interaction to obtain experience and update policies. After that, we evaluate the policy performance in the environment without any policy optimization, which is referred to as testing. Based on the classic categorization [113], the training process can be broadly classified into two paradigms: centralized and decentralized."}, {"title": "D. Issues of MARL", "content": "1) Non-Stationarity: Non-stationarity is a significant issue for decentralized MAS, where agents interact within a shared environment and update their policies synchronously. Consequently, each vehicle doesn't know the full environment dynamics, and the next state doesn't depend solely on its action and current state so it would break the Markov assumption [15]. In the CTDE paradigm [24], the centralized critic has access to all agents' observations and actions. Since only the actor computes the policy and the critic component can be removed during testing, agents in the CTDE scheme have fully decentralized execution. For independent policy learners, a naive approach is to let agents either ignore the presence of others or proceed under the assumption that the behaviors of others are static [121]. In this context, agents are independent learners, which enable the conventional single-agent RL algorithms for policy learning and have been proven to achieve excellent results on various benchmarks [122]. However, in complex and stochastic environments, independent policy learning may result in sub-optimal performance or tend to exhibit a propensity for over-fitting to the policies of other agents, leading to a lack of generalizability in testing. To improve independent learning performance, researchers propose adopting different learning rates with shared rewards to achieve the optimal joint policy [123]. Another method involves the refinement of experience replay. Due to the non-stationarity of the environment, experience replay may store more irrelevant experiences to decentralized learning with the increasing time steps. Importance sampling corrections for stable experience replay is also a solution, which adjusts the weights between the prior and the new experience under different environment dynamics. This approach has been proven to enhance the performance of independent learners in complex gaming and robotic environments [124]\u2013[126].\n2) Partial Observability: In partially observable environments, agents do not have access to the full state of the environment. Instead, each agent receives only a local observation that provides incomplete or noisy information about the true state. To make effective decisions, agents must estimate or infer the underlying state of the environment from their partial"}, {"title": "IV. STATE-OF-THE-ART METHODOLOGIES", "content": "This section will introduce recently the most advanced MARL methodologies for motion planning and control of multi-vehicle systems. We cannot encompass all the related studies, but select representative techniques in this survey are sourced from reports published in the most influential conferences and journals. Furthermore, we encourage the researchers to report more relevant works to our website."}, {"title": "A. Centralized Multi-Agent RL", "content": "In the CTDE scheme, each vehicle has an independent policy network, and a core computer is set to merge and process the information from all vehicles. We first get the merged observation from all vehicles, evaluate the system state by a pre-defined global reward function, and then train the independent policies after credit assignment. PRIMAL [154] is a milestone work in centralized training for pathfinding. It assigns each agent an independent and fine-designed parameter-sharing actor-critic network and trains them with A3C [155] algorithm. In this work, researchers illustrate that independent policies lead to selfish behaviors, and a hand-crafted reward function with a safety penalty is a good solution. Additionally, there is a switch to allow agents to learn from interaction or expert demonstrations. The combination of reinforcement learning and imitation learning contributes to fast learning and alleviates the negative impact of selfish behaviors on the overall system. In this paper, a discrete grid world is defined, and the local state of each agent is set as the information of a 10\u00d710 block with the unit vector directed toward the goal. To verify the feasibility in the real world, the authors also implement PRIMAL on AVs in a factory mockup.\nIn MADDPG [24], the authors propose the first generalizable CTDE algorithm based on deep deterministic policy gradient (DDPG) [156] with a toy multiple-particles environments. It provides an essential platform with easy vehicle dynamics to learn the continuous driving policies with continuous observation and action spaces under design-free scenarios and attracts many remarkable followers [21], [157]. Meanwhile, the combination of value function decomposition methods and CTDE scheme has achieved better scalability w.r.t. the number of agents and mitigates the impact of non-stationary on policy training, thereby improving performance in large-scale multi-agent systems [116], [158]. These methods have been verified in complex scenarios like unsignalized intersections in Highway-Env [84], [159]. Also, expert demonstration contributes to reducing the risk of converging to suboptimal policies [159]. To verify the feasibility of deploying the CTDE approach in mapless navigation tasks, Global Dueling Q-learning (GDQ) [160] sets up an independent DDQN [161] for each turtlebot3 in the MPE [24] to train policies and estimate values. Additionally, they introduced a global value network that combines the outputs of the value networks of every agent to estimate the joint state value. This method has been proven to be more effective than normal value decomposition methods. Meanwhile, researchers also attempt to extend fundamental algorithms in single-agent RL such as PPO [65] or SAC [66] to multi-agent tasks and provide many significant baselines like MAAC [162] and MAPPO [163]. In particular, MAPPO has been verified comprehensively on massive benchmarks and has systematic guidance of hyperparameter selection and training. To overcome the sim-to-real gap and deploy MAPPO on real robots, developers train a policy in the Duckietown-Gym simulator for following waypoints on the ground. The MAPPO policy network adopts recurrent neural network [164] to recall the knowledge of the prior state and output the high-level target linear velocity and angular rate for each vehicle. Like most indoor navigation tasks, the optical track system captures the position and attitude of vehicles. With the linearized inverse kinetics, the executive low-level command of the vehicle can be obtained"}, {"title": "B. Independent Policy Optimization", "content": "Regarding practical deployment challenges such as communication, bandwidth, and system complexity, the fully decentralized system reduces communication overhead and bandwidth requirements by allowing agents to operate independently without constant coordination. Additionally, it is easier to deploy in environments with limited or unreliable communication infrastructure, lowers decision-making latency, and simplifies local computation for each agent. These factors make decentralized MARL a more practical and adaptable approach for real-world multi-agent applications. In recent years, Independent Policy Optimization (IPO) [165] has obtained increasing attention, and massive related approaches have been proposed. Concurrently, the complexity of the scenarios addressed in these studies and the scale of the agents involved have also been increasing synchronously, which reflects that decentralized learning matches the demands of large-scale autonomous driving in the real world more.\nTo solve the scalability issue in centralized schemes, MAPPER [166] employs a decentralized actor-critic based on the A2C [155] algorithm. Firstly, the local observations of the occupancy map are represented as a 3-channel image containing static scenes, dynamic obstacles, and planned trajectory information from A* planner [167]. These 3-channel observations are abstracted into a latent vector via a CNN, along with waypoint information abstracted by an MLP, input into shared fully connected layers. Later, two independent MLPs output action probabilities and value estimates, respectively. Besides, MAPPER employs an extra evolutionary algorithm to eliminate bad policies during the optimization process. Compared with PRIMAL [154], MAPPER can learn faster and handle dynamic obstacles more effectively in large-scale scenarios. Another work scalability is G2RL [168], a grid map navigation method that can be used for any arbitrary number of agents. Similarly, it leverages A* to provide each agent with a global guiding path. Meanwhile, the local occupancy map is input into a local DDQN [161] planner to capture local observation and generate a corrective command to avoid dynamic obstacles. Since there is no need for communication between agents, this method does not require consideration of communication delays and can be extended to any scale.\nAs the successor to PRIMAL, PRIMAL2 [169] retains the same hierarchical structure, i.e., an A* planner generating global paths and agent training guided by A3C and imitation learning. The key difference lies in PRIMAL2's fully decentralized training approach, which enhances its flexibility in handling structured and high-density complex scenarios. Like MAPPER, it adopts an 11\u00d711 observation range and splits observations into multi-channel image inputs. The first 4 channels include static obstacles, the agent's own goal point, other agents' positions, and other agents' goal points. Channels 5-8 provide the local path from A* and the positions of other agents at three future timesteps within the observation range."}, {"title": "C. Learning with Social Preference", "content": "Although independent policy learning is feasible in many tasks, it would lead to each agent being self-centric [20] when the interests of multiple agents conflict, the pure egoistic independent policy learning may fail. Therefore, an important issue is balancing the agents' egoism and altruism. In Fig. 4, we give a toy example to illustrate how social preference affects the agents' behaviors. If the agents cannot balance their altruistic and egoistic behaviors, these two would crash or get stopped by each other. Hence, social behaviors and preferences should be considered in policy learning [170]. To find a mathematical presentation of social preference, in the early work, researchers first propose to use a trigonometric"}, {"title": "D. Safe and Trust-Worthy Learning", "content": "Safety is integral and the first priority to the deployment of autonomous driving systems, as they directly impact the reliability and people's lives of AVs. Recent RL researchers put massive efforts into ensuring the learned policy would never cause safety issues in the exploration process and after deployment. Specifically, inspired by [172], we categorize existing safety standards and methods in MARL into three types. First, soft safety guarantees involve designing safety penalty terms to reduce the probability of dangerous behavior. With fine-tuned rewards, the learning algorithm can be guided to prioritize safety alongside other performance metrics. However, although they have been proven to effectively improve safety performances in MAS, the limitation of soft guarantees is that they rely on the assumption that the reward function can accurately capture all safety aspects, which is often challenging in complex environments. The second is the probabilistic guarantees happening in the optimization process. For example, some recent MARL algorithms leverage the Lagrange constraints [21] or safety threshold during policy optimization process [173], [174]. Essentially, this improves policy gradient and helps avoid dangerous exploration behaviors. However, since the policy is still represented as a probability distribution, we cannot obtain a clear, explainable, and stable safety boundary for this method. Meanwhile, the vital safety constraints in real-world driving are instantaneous and deterministic [175]. For example, collision avoidance is a state-wise instantaneous constraint that only depends on the current state of the system rather than historical trajectories or random variables.\nThe third and safest approach is using hard safety boundaries to apply instantaneous strong corrections for agents' actions. For instance, researchers propose to learn the centralized shielding [184] from the joint action to correct any unsafe action in MAS at any risky time. Alternatively, combined with"}, {"title": "E. Methodological Summary", "content": "As shown in Table II, we collect representative works on MARL in outdoor autonomous driving, traffic system control, and structured scene transportation in the past five years. Meanwhile, we list their taxonomy, the maximum number of agents, the simulators, and whether real-world experiments are conducted. Here, we note that the action settings can be completely different even with the same simulation type. For example, in PRIMAL and PRIMAL2, the agent's actions are set as (\u2191, \u2192, \u2193, \u2190, \u2736), representing four movements in the horizontal and vertical directions in a 2D grid map, along with staying in place. In contrast, MAPPER adds four additional diagonal movements (\u2197, \u2196, \u2198, \u2199) for the agents. Additionally, we find that many studies adopt predefined high-level action commands to simplify tasks. The policy network outputs discrete values that map to corresponding preset actions, and then a low-level controller takes the actions, generates commands, and sends them to the actuators. Two other specific examples are MFPG [182] and CPO-AD [183]. They preset a low-level unidirectional control mapping and only consider the movement of AVs in one direction."}, {"title": "V. OPEN QUESTIONS AND CHALLENGES", "content": "In this section, we present the main challenges in MARL. Note that the problems faced by the CTDE and DTDE schemes are different, and though some feasible solutions have been proposed to solve these issues, they are still not unique and perfect. We hope that readers can become aware of their existence and properties in advance, thereby gaining a better understanding of the motivation and technical innovation from subsequent advanced methodologies."}, {"title": "A. Multi-modal Information", "content": "Autonomous driving is a sequential decision-making process that leverages multi-modal information. Compared to MLPs in the original algorithm designs", "Integration": "AVs acquire kinematic sensors like GPS and IMU, visual information from RGB-D cameras, LiDAR, and event cameras, and output executable commands to actuators. The multi-sensor information fusion enhances system safety redundancy, ensuring the vehicle can continue operating safely even if one sensor fails [188"}]}