{"title": "How to Mitigate Information Loss in Knowledge Graphs for GraphRAG: Leveraging Triple Context Restoration and Query-Driven Feedback", "authors": ["Manzong Huang", "Chenyang Bu", "Yi He", "Xindong Wu"], "abstract": "Knowledge Graph (KG)-augmented Large Language Models (LLMs) have recently propelled significant advances in complex reasoning tasks, thanks to their broad domain knowledge and contextual awareness. Unfortunately, current methods often assume KGs to be complete, which is impractical given the inherent limitations of KG construction and the potential loss of contextual cues when converting unstructured text into entity-relation triples. In response, this paper proposes the Triple Context Restoration and Query-driven Feedback (TCR-QF) framework, which reconstructs the textual context underlying each triple to mitigate information loss, while dynamically refining the KG structure by iteratively incorporating query-relevant missing knowledge. Experiments on five benchmark question-answering datasets substantiate the effectiveness of TCR-QF in KG and LLM integration, where it achieves a 29.1% improvement in Exact Match and a 15.5% improvement in F1 over its state-of-the-art GraphRAG competitors.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) augmented with Knowledge Graphs (KGs) have achieved remarkable successes across diverse domains, from social sciences to biomedicine [Pan et al., 2024; Peng et al., 2024; Yang et al., 2024; Soman et al., 2024]. By harmonizing the structured information in KGs and the sophisticated language understanding and processing capabilities of LLMs, such hybrid systems enable more accurate and context-aware reasoning for complex tasks.\nDespite these advances, the performance of current KG-LLM integration methods is often hindered by the underlying assumption that the KG is complete. Typical integration strategy involves retrieving relational data from a constructed KG and feed it into LLMs via prompt augmentation [Peng et al., 2024; Sun et al., 2023; Edge et al., 2024], assuming that critical entities and relationships relevant to the query are already captured within the KG. In practice, however, KG construction itself is beset by inherent constraints, where vital contextual information can be discarded in the process of converting unstructured text into structured triples, leading to missing or incomplete relations [Zhu et al., 2024; Zhong et al., 2024]. Such missing information in KGs can significantly degrade the LLM reasoning capabilities.\nTo wit, Figure 1 illustrates two primary sources of information loss. First, information sparsity arises when information extraction falls short, omitting potentially important triples and thus failing to provide sufficient coverage for specific queries [Biswas et al., 2024; Xu et al., 2024b; Li et al., 2023; Zhang and Soh, 2024; Chen et al., 2024a; Sun et al., 2024; Cohen et al., 2023]. This sparsity can be exacerbated by data noise, long-tail entities, and complex relationships, where extraction algorithms often falter. Second, context loss occurs when transforming rich yet unstructured text into discrete triples, sacrificing crucial semantic nuances and relational dependencies [Trisedya et al., 2019; Paulheim, 2017; Xu et al., 2024a]. While prior studies attempt to mitigate this issue by refining graph structures or retrieval algorithms [Liang et al., 2024; Chen et al., 2024b; Panda et al., 2024; Munikoti et al., 2023; Cohen et al., 2023], their subgraphs still lack the broader contextual information that is vital for robust reasoning, resulting in suboptimal performance in downstream tasks.\nTo address these challenges, we propose the Triple Context Restoration and Question-driven Feedback (TCR-QF) framework, which aims to restore the missing contextual information and dynamically enrich the KG during the reasoning process. Specifically, our TCR-QF approach presents a triple context restoration component that retrieves the original text passages associated with each triple, thereby recapturing the semantic details often lost during KG construction. We further enhance KG coverage through a query-"}, {"title": "2 Related Work", "content": "GraphRAG has emerged as a powerful paradigm for integrating knowledge graphs (KGs) with large language models (LLMs) to advance complex reasoning tasks [Pan et al., 2024; Peng et al., 2024; Yang et al., 2024]. A widely adopted strategy involves retrieving relevant subgraphs from pre-constructed KGs to augment LLMs during inference [Yasunaga et al., 2021; Taunk et al., 2023], with techniques such as extracting hop-k paths around topic entities [Yasunaga et al., 2021] or focusing on the shortest paths relevant to query entities [Delile et al., 2024]. More sophisticated methods optimize subgraph retrieval by assigning edge costs [He et al., 2024] or leverage LLMs themselves to generate new relations or invoke function calls [Kim et al., 2023; Jiang et al., 2023].\nWhile these approaches have demonstrated effectiveness, most remain limited by their dependence on the initial completeness of the KG and often overlook the contextual information lost during KG construction. In reality, KGs are frequently incomplete due to information loss during construction and the difficulties in extracting all relevant triples, especially in noisy or complex scenarios [Biswas et al., 2024; Cohen et al., 2023]. These constraints can hinder the ability of LLMs to formulate coherent and context-rich reasoning paths. Addressing these gaps calls for a more dynamic strategy that restores missing contextual details and continuously refines the KG, ensuring that the retrieved and generated knowledge is both accurate and semantically complete.\nHowever, the lack of essential data negatively impacts the inference results of LLMs. To address this, efforts have been made to enhance KG comprehensiveness through refined indexing methods and innovative graph structures for retrieving both triples and texts [Chen et al., 2024b; Munikoti et al., 2023; Liang et al., 2024; Cohen et al., 2023], as well as using LLMs to improve automated KG construction [Zhang and Soh, 2024; Xu et al., 2024b; Li et al., 2023]. These methods may retrieve texts related to the query without fully meeting its requirements. Additionally, the retrieved subgraphs can result in the loss of crucial information due to the absence of contextual data within triples, which is essential for maintaining semantic integrity. As a result, the constructed KG may lack critical information necessary for accurate reasoning, leading to suboptimal performance in downstream tasks.\nThe proposed TCR-QF framework addresses these limitations by dynamically enriching the KG during the reasoning process. By restoring the original textual context of triples, TCR-QF recovers lost semantic information. Additionally, it employs a query-driven feedback mechanism to identify and fill in missing information relevant to a query, enabling the KG to continuously update. This mutual enhancement between KG and LLM improves reasoning performance and better adapts to task requirements."}, {"title": "3 Proposed Method", "content": "In this section, we present the TCR-QF framework, designed to mitigate the loss of contextual information when building knowledge graphs (KGs) from unstructured text and to dynamically enrich these graphs during the reasoning process. As shown in Figure 2, the framework comprises four key components: (1) Knowledge Graph Construction, which builds a unified KG from textual sources; (2) Subgraph Retrieval, responsible for extracting task-relevant subgraphs composed of potential reasoning paths; (3) Triple Context Restoration, which traces back the original textual context of each triple to recover lost semantic nuances; and (4) Iterative Reasoning with Query-Driven Feedback, where an iterative cycle that both generates answers and identifies missing knowledge, thereby refining the KG on-the-fly. Together, these components ensure that contextual details are preserved and the KG remains up-to-date, ultimately enhancing the quality and depth of the system reasoning.\nThe above steps establishes a synergistic cycle for two-way knowledge enhancement, namely,\nForward Flow: The KG informs the LLM during answer generation, represented as\n$G^{(i)} \\rightarrow G'^{(i)}_{ALLM} \\rightarrow A$\nIn the i-th iteration, a subgraph is retrieved from the KG $G^{(i)}$ and enhanced through triple context restoration to form $G'^{(i)}$. The LLM then infers the answer $A_{ALLM}^{(i)}$ based on $G'^{(i)}$.\nFeedback Flow: The missing knowledge in the KG is identified and subsequently integrated back into the KG:\n$G^{(i+1)} \\leftarrow \\Delta K^{(i)} \\leftarrow A_{ALLM}^{(i)}$"}, {"title": "3.1 Knowledge Graph Construction", "content": "An initial KG was constructed from raw textual data using LLMs to extract entities and relations as triples $(e_h, r, e_t)$, where entities include names, types $Type(e)$, and descriptions $Desc(e)$. Source document information is retained in each node for provenance. The construction involves:\nDocument Splitting Each document D of length L is divided into overlapping chunks $C_i$ with maximum length MAX_LEN = 2048 and overlap OVERLAP = 256 tokens:\n$C_i = D[s_i: e_i]$,\n$s_i = (i - 1) \\times (MAX\\_LEN - OVERLAP) + 1$,\n$e_i = min(s_i + MAX\\_LEN - 1, L)$.\nThe overlap ensures entities and relations spanning across chunks are captured.\nTriple Extraction From each chunk $C_i$, the LLM extracts triples:\n$T_i = ExtractTriples(C_i)$,\nwhere $T_i$ is the set of triples from $C_i$. A specialized prompt guides the LLM to output structured information, including entity types and descriptions.\nSubgraph Merging Extracted triples form subgraphs $G_i = (V_i, E_i)$. A unified KG G = (V, E) is constructed by merging entities referring to the same concept using a similarity function $Sim(e_a, e_b)$ based on names, types, and descriptions:\n$Sim(e_a, e_b) \\geq \\theta \\Rightarrow e_a = e_b$,\nwhere $\\theta$ is a predefined threshold. This consolidates synonymous entities, ensuring accurate representation of entities and relationships in the KG."}, {"title": "3.2 Subgraph Retrieval", "content": "The subgraph retrieval phase focuses on extracting pertinent information from the KG in response to the query. Specifically, given a query Q expressed in natural language, the retrieval stage aims to extract the most relevant elements (e.g., entities, triplets, paths, subgraphs) from KGs, which can be formulated as:\n$G^* = G\\_Retriever(Q, G) = \\underset{G \\subset R(G)}{arg \\underset{GCR(G)}{max}} Sim(Q, G)$,\nwhere $G^* = \\{(h_0, r_0, s_1), (h_1, r_1, s_2), ..., (h_t, r_t, s_{t+1})\\}$ is the optimal retrieved graph elements and $Sim(\u00b7, \u00b7)$ is a function that measures the semantic similarity between user queries and the graph data. $R(G)$ represents a function to narrow down the search range of subgraphs, considering efficiency. The retrieval method employed in the TCR-QF builds upon existing KG retrieval method [Sun et al., 2023], which utilize LLMs to perform a beam search over the KG, with iterative pruning guided by the LLM."}, {"title": "3.3 Triple Context Restoration", "content": "The structuring of unstructured text into triples can lead to a loss of semantic context. To address this issue, a triple context restoration mechanism was implemented in TCR-QF to restore semantic integrity by tracing back to the original textual context of the triples.\nContext Retrieval For each triple $(e_h, r, e_t)$ in the retrieved subgraphs, the source documents associated with the head and tail entities were retrieved:\n$Sources(e_h, e_t) = Sources(e_h) \\cup Sources(e_t)$.\nThese sources were the documents which the entities were originally extracted during KG construction. This set encompassed all documents potentially containing contextual information about the relationship between $e_h$ and $e_t$.\nTriple Context Tracing To trace the context of the triple $(e_h, r, e_t)$, the most relevant sentence from source documents were identified. A template $T_{(e_h, r, e_t)}$ was used, such as:\n$T_{(e_h, r, e_t)} = \"e_h r e_t\".$\nA pretrained embedding model $f_{embed}$ was used to generate embeddings for both the template and candidate sentences. The context relevance was assessed via cosine similarity:\n$v_T = f_{embed}(T_{(e_h, r, e_t)}), v_s = f_{embed}(s), \\forall s \\in S$,\n$sim(v_T, v_s) = \\frac{v_T \\cdot v_s}{||v_T||||v_s||}$,\nwhere S is the set of all sentences extracted from $Sources_{(e_h, e_t)}$. The sentence with the highest similarity score was selected to provide contextual information into the triple.\nTriple Augmentation Each triple was augmented with its associated contextual sentence:\n$(e_h, r, e_t) \\rightarrow (e_h, r, e_t, S_{top})$.\nThis augmentation restored the contextual information of the triples, improving the accuracy and depth of inference tasks that rely on the KG."}, {"title": "3.4 Iterative Reasoning with Query-driven Feedback", "content": "To generate accurate answers to the original queries Q, an iterative reasoning process incorporating a query-driven feedback mechanism was implemented. This approach dynamically enriches the KG by identifying and updating missing information during the reasoning process, thereby enhancing the LLM 's capability to produce more accurate responses.\nInitially, the enriched subgraph $G'^{(0)}$ obtained from triple context restoration was used to prompt the LLM:\n$T^{(0)} = FormatInput(Q, G'^{(0)})$.\nThe LLM then generated an initial answer $A_{ALLM}^{(0)}$ by processing this prompt:\n$A_{ALLM}^{(0)} = LLM\\_Generate(T^{(0)})$, where LLM_Generate refers to generating a response based on the formatted input $T^{(0)}$.\nMissing Knowledge Identification The initial answer and contexts were analyzed to identify missing information required for the query:\n$\\Delta K^{(0)} = IdentifyMissing(Q, A_{ALLM}^{(0)}, G'^{(0)})$, where $\\Delta K^{(0)}$ represents the set of missing knowledge, formalized as a series of sub-questions. The function IdentifyMissing utilizes the LLM to compare Q with $A_{ALLM}^{(0)}$ and $G'^{(0)}$, effectively harnessing its understanding to identify gaps in knowledge.\nKnowledge Graph Enrichment For each missing component $k_q \\in \\Delta K^{(0)}$, a dense retriever interacted with the original text sources D to retrieve relevant textual information and extract the missing knowledge:\n$D_{relevant} = DenseRetrieve(k_q, D)$,\n$k = ExtractTriples(k_q, D_{relevant})$,\nwhere ExtractTriples employs the LLM to find and extract the needed information, resulting in triples k corresponding to the missing knowledge. The KG was then updated:\n$G^{(1)} = G^{(0)} \\cup \\Delta K^{(0)} with \\forall k \\in \\Delta K^{(0)}, k \\notin G^{(0)}$.\nDuplicate relationships were filtered based on edit distance from elements in $G^{(0)}$ to maintain uniqueness in $G^{(1)}$. A dense passage retriever, implemented using OpenAI's text-embedding-small, was employed due to its effectiveness in retrieving semantically relevant passages.\nIterative Reasoning and Update The updated KG $G^{(1)}$ was used to generate a new answer by following the reasoning steps:\n$A_{ALLM}^{(1)} = LLM\\_Generate(FormatInput(Q, G^{(1)}))$. This iterative process continued, repeating the steps of Missing Knowledge Identification and Knowledge Graph Enrichment:\n$\\Delta K^{(i)} = IdentifyMissing(Q, A_{ALLM}^{(i-1)}, G^{(i-1)})$,\n$G^{(i)} = G^{(i-1)} \\cup \\Delta K^{(i)}$,\n$A_{ALLM}^{(i)} = LLM\\_Generate(FormatInput(Q, G^{(i)}))$,\nfor i = 2, 3, ..., until $\\Delta K^{(i)} = \\emptyset$ or a predefined maximum number of iterations $I_{max}$ = 20 was reached. By analyzing retrieved contexts and generated responses at each iteration, gaps in the KG were detected and addressed, continuously optimizing the KG and enhancing the reasoning capabilities of the LLM.\nDue to page limits, the detailed prompts used for the LLM in each step are documented in the supplementary material."}, {"title": "4 Experiments", "content": "To evaluate the effectiveness of the TCR-QF on question-answering tasks, experiments were conducted on 5 question-answering datasets: 2WikiMultiHopQA [Ho et al., 2020], HotpotQA [Yang et al., 2018], ConcurrentQA [Arora et al., 2023], MuSiQue-Ans and MuSiQue-Full [Trivedi et al., 2022]. Followed the settings outlined in [Yang et al., 2018], utilizing a collection of related contexts for each pair as the retrieval corpus. Exact Match (EM) and F1 score were presented as the evaluation metrics across all datasets.\nWe compared TCR-QF with representative methods from LLMs and RAG:\n(1) LLM Only: Methods that directly use LLMs for obtaining answers, including models such as gpt-4o-mini and gpt-4o, as well as chain-of-thought (CoT) [Wei et al., 2022] prompting strategies.\n(2) Text-based RAG: Methods that employ a dense retriever"}, {"title": "4.1 Results and Findings", "content": "Table 1 presents the comparative results, from which we answer the following Research Question (RQ).\nRQ1 How does the TCR-QF improve the completeness and accuracy of information retrieval in question answering tasks compared to the existing GraphRAG methods?\nTable 1 demonstrates the superiority of the TCR-QF compared to different methods on five benchmark question answering datasets. TCR-QF consistently achieves the highest EM and F1 scores across all datasets, demonstrating its superior effectiveness in enhancing LLMs for complex reasoning tasks. Compared to the LLM-only approaches (GPT-4o-mini, GPT-4o and CoT), TCR-QF shows substantial improvements. For instance, on the HotpotQA dataset, TCR-QF attains an EM score of 0.558, which is 0.207 higher than GPT-4o's score of 0.351, representing a relative improvement of approximately 59%. This indicates that while LLMs possess strong language understanding capabilities, integrating external knowledge as TCR-QF does markedly enhances their accuracy in answering complex questions.\nWhen contrasting TCR-QF with the text-based method (Naive RAG) and the graph-based method (ToG), TCR-QF exhibits notable performance gains. Specifically, on the 2WikiMultiHopQA dataset, TCR-QF achieves an EM score of 0.598, which is an absolute increase of 0.259 over Naive RAG's EM score of 0.339-a relative improvement of approximately 76.4%. Similarly, TCR-QF surpasses ToG's EM score of 0.400 by an absolute margin of 0.198, reflecting a 49.5% improvement. This significant enhancement indicates that TCR-QF's approach of enriching the LLM with more comprehensive knowledge markedly improves reasoning, outperforming methods that rely solely on retrieved texts or static KGs.\nFurthermore, TCR-QF outperforms the hybrid method (GraphRAG), which combines text and graph information. On the MuSiQue-Full dataset, TCR-QF achieves an EM score of 0.303, compared to GraphRAG's EM score of 0.189. This represents an absolute increase of 0.114, amounting to an improvement of approximately 60.3%. These significant gains demonstrate that TCR-QF effectively leverages knowledge to enhance the LLM's performance beyond what is achieved by simply combining text and graph data. By dynamically restoring lost semantic information and enriching the KG during reasoning, TCR-QF provides a more comprehensive context for the LLM, leading to better reasoning and answer generation in complex tasks.\nThe consistent superiority of TCR-QF across multiple datasets ranging from general question-answering to those requiring multi-hop reasoning-highlights TCR-QF's robustness and general applicability. TCR-QF effectively addresses the challenges posed by incomplete KGs and information loss, leading to more accurate and complete responses."}, {"title": "4.2 Ablation Study", "content": "To evaluate the individual contributions of the proposed components, namely triple context restoration (TCR) and query-driven feedback (QF), to the overall performance of the TCR-QF, an ablation study was conducted on the 2WikiMultiHopQA and HotpotQA datasets to answer the question:\nRQ2 In what ways does each component in TCR-QF enhance the reasoning of the LLM?\nTable 2 presents the results of the ablation experiments. The full TCR-QF is compared with several ablated variants:\n\u2022 ToG (w/o TCR & QF): The baseline method operating on the KG.\n\u2022 TCR (w/o QF): Incorporates triple context restoration alone to address contextual information loss.\n\u2022 QF (w/o TCR): Employs query-driven feedback alone to approach incomplete information extraction.\n\u2022 TCR-AF: Integrate triple context restoration with answer-driven feedback (AF) which involves directly extracting triples from the LLM's answer and adding them to the KG.\nFrom the results we can draw the following insights.\nEffectiveness of Triple Context Restoration (TCR). Comparing the baseline ToG method with the TCR variant, it can be observed that introducing triple context restoration leads to significant performance improvements. On the 2WikiMultiHopQA dataset, the EM score increases from 0.400 to 0.481, representing an improvement of 20.25%, while the F1 score rises from 0.476 to 0.561. Similarly, on the HotpotQA dataset, the EM score improves from 0.420 to 0.494 (a 17.62% improvement), and the F1 score increases from 0.555 to 0.642. These enhancements confirm that triple context restoration effectively mitigates contextual information loss by reconnecting structured triples with their original textual context, thereby enriching the semantic information available for reasoning."}, {"title": "Effectiveness of Query-Driven Feedback (QF)", "content": "The QF variant, which focuses on dynamically updating the KG based on the requirements of the query, shows even greater improvements over the baseline. The EM scores rise to 0.568 (a 42.00% improvement) on 2WikiMultiHopQA and 0.522 (a 24.29% improvement) on HotpotQA. These substantial gains indicates that query-driven feedback significantly addresses the issue of incomplete information extraction. By dynamically enriching the KG based on the specific requirements of the query, the model fills in the missing knowledge that static KGs might overlook due to limitations in initial extraction algorithms. This adaptive approach continually enhances the relevance and comprehensiveness of the knowledge graph throughout the reasoning process.\nSynergy of TCR and QF. The full TCR-QF method, which combines both triple context restoration and query-driven feedback, achieves the highest performance. EM scores reach 0.598 on 2WikiMultiHopQA and 0.558 on HotpotQA, with relative improvements of 49.50% and 32.86% over the baseline, respectively. These results underscore a synergistic effect when combining TCR and QF, as the model benefits from both restored contextual semantics and a dynamically enriched KG. The integration of both components effectively addresses the dual challenges of information loss, leading to more accurate and complete reasoning.\nComparison with Answer-Driven Feedback (TCR-AF). The TCR-AF variant replaces query-driven feedback with answer-driven feedback, where triples are extracted from the model's answers to update the KG. While TCR-AF outperforms ToG, achieving EM scores of 0.538 on 2WikiMulti-"}, {"title": "4.3 Statistical and Convergence Analysis", "content": "To evaluate the effectiveness and convergence of the TCR-QF, statistical analyses were conducted over multiple inference rounds. Table 3 presents key metrics from the initial round to the 10th round, including the numbers of nodes and edges in the KG, as well as the EM and F1 scores on the 2WikiMultiHopQA dataset. These experiments and results provide answers to the following question:\nRQ3 How do TCR-QF continuously enhance KG and boost LLM reasoning?\nFrom the results we can draw the following insights.\nContinuous Improvement of KG Completeness and Model Reasoning Performance. As demonstrated in Table 3, the TCR-QF significantly enriches the KG over successive inference rounds. Specifically, on the 2WikiMultiHopQA dataset, the number of nodes in the KG increased by 4,090 (from 74,571 to 78,661), and the number of edges increased by 10,606 (from 69,866 to 80,472) over 10 rounds. This enrichment directly addresses the issue of information sparsity by incorporating previously missing triples and expanding the KG's coverage to meet query demands. Correspondingly, the model's reasoning performance improved substantially. The Exact Match (EM) score increased from 0.481 to 0.598, a 24.3% improvement, and the F1 score rose from 0.562 to 0.680, a 21.0% improvement. These significant performance gains indicate that the enriched KG provides the LLM with more comprehensive and contextually rich information, directly mitigating the effects of context loss and enhancing reasoning accuracy.\nAlignment of KG Completeness and Reasoning Performance Enhancement. As depicted in Figure 4, the parallel upward trends in KG metrics and performance scores affirm a strong correlation between the enriched KG and the model's improved reasoning ability. By restoring the contextual information associated with triples and integrating new, relevant knowledge through query-driven feedback, the TCR-QF enhances the semantic integrity of the KG. This comprehensive knowledge base enables the LLM to perform more accurate and context-aware reasoning, directly addressing the limitations posed by information sparsity and context loss.\nConvergence of KG Enrichment and Performance Improvements. The TCR-QF not only enriches the KG but also exhibits convergence over inference rounds, ensuring efficient use of computational resources. As illustrated in Figure 4, both the growth of the KG and the improvement in performance metrics begin to plateau after several rounds, specifically between the 8th and 10th iterations. The incremental increases in nodes and edges diminish, and the EM and F1 scores stabilize at 0.598 and 0.680, respectively. This convergence suggests that the TCR-QF effectively enriches the KG to an optimal level, beyond which additional iterations yield minimal benefits.\nThe experimental results validate the effectiveness of the TCR-QF in overcoming the foundational challenges outlined in the introduction. By continuously and efficiently enhancing the KG's completeness and restoring lost contextual nuances, the TCR-QF significantly boosts the model's reasoning performance. These findings confirm that addressing information loss through dynamic KG enrichment and context restoration is a viable and efficient strategy for advancing the integration of KGs and LLMs in complex reasoning tasks."}, {"title": "5 Conclusion", "content": "This paper presents TCR-QF, a novel framework that integrates knowledge graphs (KGs) with large language models (LLMs) to advance complex reasoning for question-answering tasks. By addressing two primary sources of information lossm namely, context loss through triple context restoration (TCR) and incomplete extraction via query-driven feedback (QF), our TCR-QF framework recovers essential semantic details and dynamically expands the KG as the model reasons. Extensive evaluations on five benchmark datasets demonstrate that TCR-QF significantly outperforms the state-of-the-art competitors, demonstrating the value of incorporating contextualized triples and iterative KG updates in enhancing LLM performance. These results highlight the potential of TCR-QF to bridge the gap between structured and unstructured knowledge, paving the way for more accurate and robust AI-driven reasoning across diverse domains."}, {"title": "A.1 Prompt for Triples Extraction", "content": "\"\"\"\n--Goal--\nGiven a text document potentially\nrelevant to this activity, identify\nall entities from the text and all\nrelationships among the identified\nentities.\n-Steps-\n1. Identify all entities. For each\nidentified entity, extract the\nfollowing information:\nentity name: The name of the entity\nentity type: The type of the entity (e\n.g., person, organization, location,\nevent)\nentity description: A detailed\ndescription of the entity's\nattributes and activities in the text\nFormat each entity as [entity |  |  | \n]\n2. Identify relationships among entities\nFrom the entities identified in\nStep 1, find all pairs of [source\nentity, target entity] that are *\nclearly related*.\nFor each pair of related entities,\nextract the following information:\nsource entity: name of the source\nentity, as identified in step 1\ntarget entity: name of the target\nentity, as identified in step 1\nrelationship: The relationship between\nsource entity and target entity\nFormat each relationship as [\nrelationship |  | <\nrelationship> | ]\nEnsure that both source entity and\ntarget entity come from the entity\nlist extracted in Step 1.\nThe extraction entity names and\nrelations should remain consistent\nwith the language used in the -Real\nData-.\n-Examples-\nText:\nEmily gazed at the towering skyscrapers\nof New York City, feeling a sense of\nawe. Her mentor, Dr. Smith, had\nalways encouraged her to pursue\narchitecture. Together, they planned\nto attend the upcoming International\nArchitecture Conference.\nOutput:\n[entity | Emily | person | Emily is an\naspiring architect who feels awe when\nobserving the skyscrapers of New\nYork City.]\n[entity | New York City | location | New\nYork City is a major city known for\nits towering skyscrapers.]\n[entity | Dr. Smith | person | Dr. Smith\nis Emily's mentor who encourages her\nto pursue a career in architecture.]\n[entity | International Architecture\nConference | event | An upcoming\nevent that Emily and Dr. Smith plan\nto attend together.]\n[relationship | Emily | mentored by | Dr\nSmith ]\n[relationship | Emily | attends |\nInternational Architecture Conference\n]\n[relationship | International\nArchitecture Conference | located in\n| New York City]\n-Real Data-\nText:\n{input_text}\nOutput:\n\"\"\""}, {"title": "A.2 Prompt for Reasoning", "content": "-Goal-\nGiven a question and the associated\nretrieved knowledge graph triplets (\nentity, relation, entity) and text\ninformation,\nyou are asked to answer the question\nwith these information and your\nknowledge.\n-Attentions-\n Please strictly follow the format in\nthe example to answer, do not\nprovide additional content such as\nexplanations.\n The answer needs to be as precise and\nconcise as possible such as \"\nFlatbush section of Brooklyn, New\nYork City\", \"Christopher Nolan\", \"\nNew York City\".\n Ensure that the answer corresponds\nexactly to the Question without\ndeviation.\n-Example-\nTriplets:"}, {"title": "A.3 Prompt for Missing Knowledge Identification", "content": "\"\"\"\n-Goal-\nAnalyze the provided triples and text\ninformation to identify what is\nmissing for accurately answering the\nquestion. Then, Present the missing\nparts as several independent\nquestions.\n-Attentions-\nPlease strictly follow the format in\nthe example to answer, do not\nprovide additional content such as\nexplanations.\nEnsure that all results are inferred\nbased on the information I provided.\nOnly provide questions about missing\ninformation.\n-Example-"}, {"title": "A.4 Prompt for Knowledge Graph Enrichment", "content": "-Goal-\nGiven a series of texts and some\nquestions, extract triplet\ninformation from the texts, focusing\non the triplets that can specifically\nanswer the questions.\n-Attentions-\nPlease strictly follow the format in\nthe example to answer, do not\nprovide additional content such as\nexplanations.\nEnsure that all triples are inferred\nbased on the information I provided.\nTry to extract all the triples\nrelated to the question from the"}]}