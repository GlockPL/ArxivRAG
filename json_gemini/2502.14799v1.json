{"title": "A Survey on Text-Driven 360-Degree Panorama Generation", "authors": ["Hai Wang", "Xiaoyu Xiang", "Weihao Xia", "Jing-Hao Xue"], "abstract": "The advent of text-driven 360-degree panorama generation, enabling the synthesis of 360-degree panoramic images directly from textual descriptions, marks a transformative advancement in immersive visual content creation. This innovation significantly simplifies the traditionally complex process of producing such content. Recent progress in text-to-image diffusion models has accelerated the rapid development in this emerging field. This survey presents a comprehensive review of text-driven 360-degree panorama generation, offering an in-depth analysis of state-of-the-art algorithms and their expanding applications in 360-degree 3D scene generation. Furthermore, we critically examine current limitations and propose promising directions for future research. A curated project page with relevant resources and research papers is available at https://littlewhitesea.github.io/Text-Driven-Pano-Gen/.", "sections": [{"title": "1 Introduction", "content": "Rapid growth of immersive technologies, such as virtual reality (VR) and augmented reality (AR), has dramatically increased the demand for high-quality panoramic visual content. Among such content, 360-degree panoramas are pivotal in delivering realistic and immersive experiences by capturing a complete spherical view of an environment. Traditionally, producing these panoramas requires specialized camera equipment and considerable technical expertise. However, recent advances in text-driven 360-degree panorama generation [Chen et al., 2022; Dastjerdi et al., 2022; Wang et al., 2024; Ye et al., 2024] have introduced groundbreaking capabilities, enabling the synthesis of 360-degree panoramic images directly from textual descriptions. This innovation can potentially revolutionize content creation over diverse domains including VR/AR applications, gaming, and virtual tours.\nUnlike conventional 2D images, 360-degree panoramas, often represented through equirectangular projection [da Silveira et al., 2022], encompass the entire 360\u00b0 \u00d7 180\u00b0 field of view (FoV). This distinctive format poses unique challenges for text-driven generation, requiring not only accurate image synthesis but also excellent preservation of geometric consistency and seamless visual coherence across the full 360\u00b0 horizontal and 180\u00b0 vertical extents.\nThe availability of large-scale paired image-text datasets has facilitated the development of text-to-image latent diffusion models (LDMs) [Rombach et al., 2022], which excel at synthesizing high-quality, visually compelling images aligned with given text descriptions. Leveraging the powerful generative capabilities of pre-trained LDMs, researchers have developed methods specifically tailored to address the unique challenges of text-driven 360-degree panorama generation [Wang et al., 2023; Wang et al., 2024; Lu et al., 2024; Feng et al., 2023; Zhang et al., 2024]. Despite the rapid progress in this field, there is currently no survey that provides a systematic analysis of these emerging methodologies, elucidates their key characteristics, and highlights their applications. To address this gap, this paper presents a holistic survey and analysis of the latest research advances in text-driven 360-degree panorama generation.\nThis survey is structured as follows: First, we establish a foundational understanding of this field by introducing the principal representations of 360-degree panoramas, presenting prominent datasets commonly used in this area, and outlining key evaluation metrics employed to assess the quality and fidelity of generated panoramic content. Next, we review state-of-the-art methods for text-driven 360-degree panorama generation, categorizing them into two primary paradigms: (a) Text-Only Generation and (b) Text-Driven Narrow Field-of-View (NFoV) Outpainting. Methods on text-only generation aim to synthesize entire 360-degree panoramas exclusively from textual prompts, typically utilizing fine-tuning techniques such as Low-Rank Adaptation (LoRA) [Hu et al., 2022] or DreamBooth [Ruiz et al., 2023] on pre-trained LDMs [Rombach et al., 2022]. In contrast, text-driven NFoV outpainting allows greater user control by conditioning the generation process on both textual descriptions and initial NFoV images. Following this, we explore emerging applications of text-driven 360-degree panorama generation in the realm of 360-degree 3D scene generation. Finally, we discuss the prevailing challenges in this developing field and propose potential directions for future research.\nThis paper offers the first survey on text-driven 360-degree panorama generation, systematically reviewing its state-of-the-art techniques, key datasets and evaluation metrics, as"}, {"title": "2 Related Work", "content": "2.1 Text-to-Image Diffusion Models\nText-to-image (T2I) diffusion models [Nichol et al., 2022; Saharia et al., 2022; Ramesh et al., 2022; Rombach et al., 2022; Podell et al., 2024] have achieved remarkable progress in generating high-fidelity and photorealistic images from textual descriptions. These models have garnered widespread attention because of their intuitive text-based conditioning as a user-friendly interface for diverse image generation tasks.\nT2I diffusion models can be broadly categorized into pixel-space and latent-space models. Pixel-space models, such as GLIDE [Nichol et al., 2022] and Imagen [Saharia et al., 2022], operate directly in the pixel space, producing visually impressive results at the expense of substantial computational resources, limiting their scalability. In contrast, latent diffusion models (LDMs) [Rombach et al., 2022] address these limitations by leveraging pre-trained autoencoders like VQ-GAN [Esser et al., 2021] to map images into a compact latent space, where the diffusion process is conducted. This reduces computational overhead while maintaining high-quality outputs, making LDMs a preferred framework for text-driven 360-degree panorama generation, as surveyed in this work.\n2.2 3D Scene Representation\nEfficient and accurate 3D scene representation is a critical challenge in computer graphics and vision. Traditional explicit representations, including point clouds, meshes, and voxel grids, often suffer from high memory requirements and struggle with complex topologies and unbounded scenes.\nNeural implicit functions [Park et al., 2019; Mescheder et al., 2019; Xia and Xue, 2023], which represent 3D scenes as continuous functions encoded within neural network parameters, offer a compact and flexible paradigm for scene representation. Notably, Neural Radiance Fields (NeRFs) [Mildenhall et al., 2020] stand out for their ability to achieve high-quality novel view synthesis. However, NeRF's reliance on dense volumetric sampling along camera rays results in slow training, hindering its practicability.\nRecently, 3D Gaussian Splatting (3DGS) [Kerbl et al., 2023] has emerged as an efficient alternative to 3D scene representation. By combining an explicit representation of 3D Gaussians with a highly efficient differentiable rasterization pipeline, 3DGS facilitates rapid scene reconstruction and rendering. This advancement has opened up new possibilities, including recent explorations in text-to-3D 360-degree scene synthesis [Zhou et al., 2024b; Li et al., 2024], which leverage text-driven 360-degree panorama generation techniques."}, {"title": "3 Preliminaries", "content": "3.1 Representations of 360-Degree Panoramas\nThe representation of 360-degree panoramic content poses a fundamental challenge: How to accurately map spherical visual information onto a two-dimensional plane? To address this, a variety of projection methodologies [da Silveira et al., 2022] have been developed, each with distinct advantages and trade-offs. Below, we outline two widely used formats for 360-degree panorama generation: Equirectangular Projection and Cubemap Projection.\nEquirectangular Projection (ERP) As the most prevalent representation format for 360-degree panoramas, ERP establishes a direct mapping between spherical and planar coordinates: longitude corresponds to the horizontal axis, spanning the full 360\u00b0 range, while latitude maps to the vertical axis, covering 180\u00b0 from -90\u00b0 (south pole) to +90\u00b0 (north pole). ERP's simplicity and compatibility with web viewers and VR headsets make it the preferred choice for numerous applications. Additionally, its representation as a single, continuous image allows the direct application of image manipulation techniques, such as text-driven 360-degree panorama-to-panorama translation [Wang and Xue, 2025]. Despite these advantages, ERP introduces pronounced geometric distortions, particularly at the polar regions, where the visual content appears stretched. Furthermore, the texel density of a spherical image in ERP is non-uniformly distributed: it is comparatively lower in the equatorial regions and markedly higher towards the poles. This inhomogeneity can be particularly problematic in scenarios where critical visual information is predominantly located away from the poles, leading to inefficient utilization of image resolution for regions of interest. In this survey, unless explicitly stated otherwise, 360-degree panoramas are represented using ERP.\nCubemap Projection (CMP) CMP offers an alternative representation that mitigates the distortions inherent in the ERP format, particularly at the poles. In CMP, the spherical image is projected onto the six faces of a cube, with each face representing a 90\u00b0\u00d790\u00b0 field of view. This division significantly reduces geometric distortions, making CMP more compatible with diffusion priors from text-to-image diffusion models trained on standard perspective images [Kalischek et al., 2025]. However, CMP introduces several challenges: (1) it increases the complexity of image manipulation compared to the single-image format of ERP; (2) it may necessitate additional conversion for compatibility with platforms or viewers that primarily support ERP. Despite these practical challenges, CMP is well-suited for applications that demand reduced distortion and higher fidelity. The width and height of an ERP image are four and two times the side length of the corresponding CMP, respectively, reflecting the geometric relationship between the two formats.\n3.2 Datasets\n360-degree panoramic image generation from text prompts presents unique challenges due to the complete 360\u00b0\u00d7180\u00b0 field of view that these images encompass. Text-to-image diffusion models [Saharia et al., 2022; Rombach et al., 2022; Podell et al., 2024], predominantly trained on perspective images with a narrower field of view, often struggle to synthesize high-quality 360-degree panoramas. To address this, several specialized datasets have been developed to facilitate research in this domain. Tab. 1 summarizes these datasets, with further details provided below."}, {"title": "3.3 Evaluation Metrics", "content": "A rigorous evaluation of text-driven 360-degree panorama generation methods typically requires to combine (a) universal and (b) panorama-specific metrics. The universal metrics, comprising Fr\u00e9chet Inception Distance (FID), Kernel Inception Distance (KID), Inception Score (IS), and CLIP Score (CS), are widely applicable to both perspective and panoramic images.\nFID [Heusel et al., 2017] measures the distance between feature distributions of generated and real images using a pre-trained Inception-v3 network [Szegedy et al., 2016]. Lower FID scores indicate better perceptual quality and closer alignment with the real image distribution.\nKID [Bi\u0144kowski et al., 2018] measures the difference between real and generated image distributions by computing the maximum mean discrepancy of their features extracted from Inception-v3 [Szegedy et al., 2016]. Similar to FID, lower KID values indicate better image quality.\nIS [Salimans et al., 2016] measures both the quality and diversity of generated images by leveraging Inception-v3 [Szegedy et al., 2016]. It calculates the KL divergence between the conditional class distribution of generated images and the marginal distribution over all generated samples. Higher IS scores suggest better visual quality and diversity.\nCS [Radford et al., 2021] evaluates consistency between text prompts and generated images using the CLIP model [Radford et al., 2021]. It calculates the cosine similarity between the text embedding of the prompt and the visual embedding of the generated image. A higher CS score reflects stronger text-image alignment and semantic coherence.\nThe universal metrics provide general assessments but often fail to capture nuances and geometric distortions unique to 360-degree panoramas. Panorama-specific metrics designed specifically for ERP-based 360-degree panoramic images include Fr\u00e9chet Auto-Encoder Distance (FAED), Omnidirectional FID (OmniFID), and Discontinuity Score (DS).\nFAED [Oh et al., 2022] computes Fr\u00e9chet distances between features extracted from generated and real panoramas. Unlike FID, it employs an autoencoder [Hinton and Salakhutdinov, 2006] specifically trained on 360-degree panoramic images. Lower FAED scores reflect better perceptual and geometric quality tailored to the unique panoramic properties.\nOmniFID [Christensen et al., 2024] adapts FID to specifically evaluate 360-degree panoramas. It converts equirectangular panoramas into cubemap representations and calculates FID across three disjoint subsets of cubemap faces, averaging the results. Lower OmniFID scores indicate higher geometric fidelity in 360-degree panorama generation.\nDS [Christensen et al., 2024] measures the seam alignment across the borders of generated panoramas by applying a kernel-based edge detection algorithm. Lower discontinuity scores correspond to fewer visible seam artifacts, indicating better perceived consistencies across the seam.\nWe provide a comprehensive comparison of state-of-the-art methods, introduced in the following section, using the outlined metrics in Sec. 4.3."}, {"title": "4 State-of-the-Art Methods", "content": "Existing text-driven 360-degree panorama generation methods can be broadly categorized into two paradigms according to input modalities: (a) Text-Only Generation aims to synthesize 360-degree panoramas from textual prompts only, while (b) Text-Driven NFoV Outpainting leverages both textual descriptions and initial NFoV images to guide the generation process, offering enhanced user control. Fig. 1 provides an intuitive illustration for both paradigms. We detail the literature for both as follows.\n4.1 Text-Only Generation\nThis paradigm focuses on synthesizing 360-degree panoramas from textual descriptions. Tab. 2 provides a comparative overview of representative text-only methods.\nText2Light [Chen et al., 2022], an early notable effort, explored a hierarchical framework utilizing VQGAN [Esser et al., 2021] and CLIP [Radford et al., 2021] to address this challenge. Recently, the advent of latent diffusion models (LDMs) [Rombach et al., 2022] for text-to-image synthesis marked a significant advancement, enabling more sophisticated 360-degree panorama generation techniques. LDMs are typically trained on vast datasets consisting of perspective images with a limited field of view and corresponding text descriptions. Despite demonstrating robust capabilities in generating perspective images from text prompts, these models face significant difficulties when creating 360-degree panoramas with a complete 360\u00b0\u00d7180\u00b0 field of view, which differ substantially from traditional perspective images.\nFine-Tuning. To adapt pre-trained LDMs for 360-degree panorama synthesis, a common strategy is to fine-tune these models with specialized 360-degree panorama datasets. Diffusion360 [Feng et al., 2023] exemplifies this approach by leveraging the DreamBooth technique [Ruiz et al., 2023] to fine-tune a pre-trained LDM [Rombach et al., 2022] on SUN360 [Xiao et al., 2012]. To ensure geometric consistency of boundaries, Diffusion360 utilizes a circular blending strategy during both the denoising process and the VAE decoding stage, effectively reducing seam artifacts. In addition, it introduces a super-resolution module to enable the generation of high-resolution (6114\u00d73072) 360-degree panoramas. Notably, the DreamBooth fine-tuning approach is computationally demanding and time-consuming because it generally fine-tunes a significant portion of the model parameters.\nIn contrast, LoRA [Hu et al., 2022] has recently gained attention as a parameter-efficient fine-tuning method. LoRA works by injecting trainable low-rank matrices into the pre-trained model's weights, allowing for rapid adaptation to new tasks with minimal additional parameters. For example, StitchDiffusion [Wang et al., 2024] employs LoRA to fine-tune a pre-trained LDM on a dataset of paired 360-degree images (sourced from Polyhaven [Poly Haven, 2025]) and corresponding textual descriptions generated using BLIP [Li et al., 2022]. It further introduces a tailored generation method based on MultiDiffusion [Bar-Tal et al., 2023] to ensure boundary continuity in generated panoramas.\nOther works have similarly adopted the LoRA fine-tuning technique. PanFusion [Zhang et al., 2024] devises a dual-branch diffusion model, trained on Matterport3D [Chang et al., 2017], with separate LoRA layers to integrate both global panoramic and local perspective views. It introduces an equirectangular-perspective projection attention module to facilitate information exchange between the two branches, aiming to alleviate visual inconsistencies in the generated 360-degree panoramas. However, PanFusion's output often exhibits blurriness at the top and bottom regions, due to its training dataset. To avoid this issue, DiffPano [Ye et al., 2024] utilizes the Habitat Matterport 3D dataset [Ramakrishnan et al., 2021] to produce multi-view consistent 360-degree panoramas with clearer top and bottom details. For generating more precise textual descriptions, DiffPano adopts BLIP2 [Li et al., 2023] and Llama2 [Touvron et al., 2023] sequentially, resulting in a panoramic video-text dataset. Based on this dataset, it fine-tunes a pre-trained LDM using LoRA for single-view text-driven 360-degree panorama generation. Furthermore, to enable multi-view 360-degree panorama generation based on text prompts and camera viewpoints, DiffPano proposes a spherical epipolar-aware attention module, which effectively extends single-view panorama generation to multi-view consistent panorama generation.\nTraining-Free. PanoFree [Liu et al., 2024], on the other hand, departs from fine-tuning approaches by introducing a tuning-free multi-view image generation framework based on a pre-trained LDM. Guided by textual descriptions, PanoFree leverages iterative warping and inpainting steps to produce multi-view perspective images, which are subsequently stitched into 360-degree panoramas, thus avoiding the need for specialized 360-degree panorama datasets.\n4.2 Text-Driven NFoV Outpainting\nThis paradigm enhances user control by conditioning the generation process on both textual prompts and initial narrow NFoV images. The NFoV image, representing a limited portion of the scene, provides a visual starting point, which the generative model then extends into a complete panorama guided by the textual description. Tab. 3 offers a summary of representative text-driven NFoV outpainting approaches."}, {"title": "4.3 Quantitative Comparison", "content": "To systematically evaluate strengths and weaknesses of representative methods of different kinds, we benchmark Text-Only Generation (Sec. 4.1) and Text-Driven NFoV Outpainting (Sec. 4.2) methods with publicly available inference codes and model checkpoints. For Text-Only Generation, we compare Text2Light [Chen et al., 2022], Diffusion360 [Feng et al., 2023], StitchDiffusion [Wang et al., 2024] and PanFusion [Zhang et al., 2024]. For Text-Driven NFoV Outpainting, we compare PanoDiff [Wang et al., 2023] and Diffusion360. We conduct a comprehensive comparison using metrics outlined in Sec. 3.3.\nTo ensure an unbiased evaluation of the generalizability of the methods, we employ an out-of-domain dataset, ODI-SR [Deng et al., 2021], on which none of the models have been explicitly trained. For generating text descriptions, we utilize BLIP2 [Li et al., 2023] to create textual captions for the 360-degree panoramas included in the ODI-SR dataset. These generated text prompts serve as inputs for both Text-Only Generation and Text-Driven NFoV Outpainting tasks.\nTo simulate NFoV images, we first project the equirectangular 360-degree panoramas from the ODI-SR dataset into a cubemap format and then extract the front face of each cubemap. The original 360-degree panoramas from ODI-SR are designated as real images and used as ground truth for the computation of evaluation metrics.\nThe quantitative results obtained from this comparative evaluation, across the seven evaluation metrics, are presented in Tab. 4. To provide insights into the computational efficiency of each method, we also report the inference time required to generate a 1024\u00d7512 360-degree panorama on a consistent machine equipped with an RTX A6000 GPU."}, {"title": "5 Emerging 3D Applications", "content": "Recent advances in text-driven 360-degree panorama generation [Wang et al., 2024; Feng et al., 2023; Zhang et al., 2024] have catalyzed innovative methods for reconstructing 360-degree 3D scenes from textual descriptions. 360-degree panoramic images inherently capture both global contexts and geometric constraints of a scene, making them an essential intermediate representation for 3D scene generation. Consequently, recent text-driven 360-degree 3D scene generation methods [Ma et al., 2024; Zhou et al., 2024a; Zhou et al., 2024b; Li et al., 2024; Yang et al., 2024] utilize 360-degree panorama generation to bridge the gap between text prompts and 360-degree 3D scene reconstruction.\nAs depicted in Fig. 2, these methods typically use a two-stage process: (1) 360-Degree Panorama Generation: generating a 360-degree panorama from the input text prompt using a fine-tuned LDM [Rombach et al., 2022], and (2) 3D Scene Reconstruction: inferring a 3D representation, typically with 3D Gaussian Splatting (3DGS) [Kerbl et al., 2023], from the generated panorama and corresponding multi-view perspective images. Tab. 5 provides a comparative summary.\nWithin this framework, emerging methods are primarily differentiated by their choice of (a) 360-degree panorama generators and their (b) strategies for extracting and utilizing 3D information. For instance, FastScene [Ma et al., 2024] and HoloDreamer [Zhou et al., 2024a] both employ Diffusion360 [Feng et al., 2023] to generate the initial 360-degree panorama depicting a scene from a given text prompt. FastScene [Ma et al., 2024] then synthesizes multi-view panoramas of this scene for specific camera poses using Coarse View Synthesis and Progressive Novel View Inpainting. With these synthesized multi-view panoramas, FastScene introduces Multi-View Projection to get their perspective views. The point clouds derived from these views are then used as input for 3DGS to reconstruct the 3D scene. HoloDreamer [Zhou et al., 2024a] enhances the Diffusion360-generated panorama with two distinct ControlNet-based LDMs [Zhang et al., 2023] and a super-resolution network to create a high-resolution, stylized output. Subsequently, HoloDreamer initializes 3D Gaussians using point clouds derived from a reverse equirectangular projection of the high-resolution panorama combined with its corresponding depth information. Finally, a two-stage 3DGS optimization process is developed to refine the scene rendering, resulting in the desired 3D scene reconstruction.\nFurthermore, certain methods deviate from the reliance on Diffusion360 [Feng et al., 2023]. DreamScene360 [Zhou et al., 2024b] utilizes StitchDiffusion [Wang et al., 2024] to generate multiple 360-degree panorama candidates and then employs a self-refinement process to select the optimal candidate for initializing panoramic 3D Gaussians with a 3D geometric field. To facilitate visual feature correspondences between different views and maintain geometric consistencies during the 3DGS optimization process, semantic and geometric regularizations are applied. In contrast, SceneDreamer360 [Li et al., 2024] uses a fine-tuned PanFusion"}, {"title": "6 Challenges and Future Directions", "content": "Despite the impressive results achieved in text-driven 360-degree panorama generation, challenges remain in evaluation metrics, resolution, controllability, and model design. This section identifies these challenges and outlines potential directions for future research.\nEvaluation Metrics Although OmniFID [Christensen et al., 2024] extends FID to account for geometric fidelity in generated 360-degree panoramas, it primarily focuses on geometric aspects, leaving other crucial dimensions unaddressed. Text-to-image consistency, commonly evaluated by using the CLIP Score [Radford et al., 2021], is another critical metric for assessing text-driven 360-degree panorama generation methods. However, the CLIP Score, primarily designed for conventional 2D images, relies on an image encoder predominantly trained on standard perspective images. This encoder may not fully capture the inherent distortions and spatial relationships unique to 360-degree panoramas represented by equirectangular projections. Consequently, the CLIP Score may not accurately reflect the true semantic consistency in the context of 360-degree panorama generation. Future research could explore innovative evaluation metrics that effectively assess the semantic consistency between the input prompt and the generated 360-degree panoramic image.\nHigher Resolution While Diffusion360 [Feng et al., 2023], which uses a super-resolution module, is among the few methods that can currently achieve a maximum resolution of 6144\u00d73072 (6K), this remains inadequate for demanding applications like VR gaming and high-fidelity 3D scene reconstruction, which often necessitate resolutions of 8K or higher to capture intricate details of landscapes and architecture. Generating such high-resolution panoramas, however, introduces significant challenges related to computational latency and memory consumption. Addressing these limitations requires the development of more efficient model architectures and optimization techniques. Promising approaches include the use of window-based operations, model pruning, quantization, and advanced neural network designs tailored to resource-intensive tasks. Moreover, the availability of high-resolution, large-scale datasets will be critical for driving progress in this direction.\nMulti-modal Generation Existing text-driven methods, despite their ability to produce photorealistic 360-degree panoramas, often lack precise control over global semantic layout and spatial structure of the generated scene. This motivates exploring multi-modal approaches to enhance controllability. Although 360PanT [Wang and Xue, 2025] demonstrates panorama-to-panorama translation using auxiliary modalities like edge and segmentation maps alongside text, its outputs deviate from the standard 360\u00b0\u00d7180\u00b0 field of view when these additional modalities are incorporated. Future research should focus on developing multi-modal techniques that effectively integrate diverse inputs (e.g. depth maps, segmentation maps, or edge maps) with text prompts to achieve fine-grained spatial control in the generated 360-degree panoramas, while ensuring strict adherence to the standard equirectangular projection format.\nModel Design Most text-driven 360-degree panorama generation methods are built upon latent diffusion models (LDMs) [Rombach et al., 2022]. While LDMs have achieved remarkable success in text-to-image synthesis, recent advancements in autoregressive models indicate promising alternative architectures. Specifically, Visual Autoregressive (VAR) models, exemplified by Infinity [Han et al., 2024], have exhibited superior performance compared to the leading LDMs in standard text-to-image synthesis. This highlights an exciting avenue for future research: exploring VAR-based models for text-driven 360-degree panorama generation."}, {"title": "7 Conclusion", "content": "This survey has provided a comprehensive overview of the rapidly evolving field of text-driven 360-degree panorama generation. We began by introducing two primary representation methods of 360-degree panoramas, along with widely used datasets, and key evaluation metrics in this domain. Subsequently, we presented an in-depth discussion of prevalent methods for text-driven 360-degree panorama generation and their applications in generating 360-degree 3D scene. Despite the significant progress achieved in this field, several challenges remain. To address these challenges, we have articulated promising directions for future research."}]}