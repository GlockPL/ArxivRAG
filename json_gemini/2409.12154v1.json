{"title": "Abductive Explanations of Classifiers under Constraints: Complexity and Properties", "authors": ["Martin Cooper", "Leila Amgoud"], "abstract": "Abductive explanations (AXp's) are widely used for understanding decisions of classifiers. Existing definitions are suitable when features are independent. However, we show that ignoring constraints when they exist between features may lead to an explosion in the number of redundant or superfluous AXp's. We propose three new types of explanations that take into account constraints and that can be generated from the whole feature space or from a sample (such as a dataset). They are based on a key notion of coverage of an explanation, the set of instances it explains. We show that coverage is powerful enough to discard redundant and superfluous AXp's. For each type, we analyse the complexity of finding an explanation and investigate its formal properties. The final result is a catalogue of different forms of AXp's with different complexities and different formal guarantees.", "sections": [{"title": "1 Introduction", "content": "Given a decision of a classifier, a user may want, and may even have a legal right to, an explanation of this decision. Concrete examples include an explanation as to why a loan/job/visa application was refused or why a medical diagnosis was made. See [24, 25] for more on explainability and interpretability.\nThe majority of existing explanation functions explain a decision in terms of relevance of the input features. One of the most studied types of feature-based explanations is the so-called abductive explanation (AXp), or prime implicant explanation [13, 22, 28]. It provides a (minimal) sufficient reason for the decision.\nIn the literature, AXp's have two sources: they are generated either from a subset of instances as done by the two prominent explanation functions Anchors [27] and LIME [26] and those introduced in [3, 5], or from the whole feature space (eg., [6, 7, 8, 11, 13, 14, 17, 18, 28]). Whatever the source, features are implicitly assumed to be independent. However, constraints on values that features may take are ubiquitous in almost all real-world applications including analysis of election results, justifying medical treatments, etc.\nConstraints have been extensively studied in databases where several types have been distinguished [29]. In the context of classifiers and their abductive explanations, we focus on two categories: integrity constraints (IC) and dependency constraints (DC). The former are of two types: i) they may express impossible assignments of values to features like \"men cannot be pregnant\", here ICs impact locally individual instances, ii) global constraints preventing the co-existence of two or more instances such as \"no two distinct students may have the same ID card value\". When such constraints exist, the feature space necessarily contains impossible instances. Dependency constraints are a specific sub-type of the first type of IC. They express the following: if some attributes take specific values, then other attributes take also specific values. Examples of DCs are: \"a person who is pregnant is necessarily a woman\" and \"if it rains, then the road is certainly wet\". This type of constraint may exist between features of feasible instances. Therefore, they may lead to dependencies between AXp's, and thus redundancies (as some follow from others).\nIn [11, 16], ICs (in the sense of constraints on possible feature vectors) were considered when generating abductive explanations while DCs (in the sense of dependencies between AXp's) were totally ignored. In this paper, we show that disregarding dependency constraints when explaining decisions may lead to exponentially more AXp's many of which may be redundant or superfluous. To bridge this gap, we investigate explanation functions that generate AXp's while taking into account both IC and DC constraints. Our contributions are fourfold:\nThe first consists of proposing three novel types of abductive explanation that deal with constraints. They are generated from the whole set of instances that satisfy the constraints, thus discarding any instance that violates an integrity constraint. However, this is not sufficient for considering dependencies expressed by DCs. As a solution, the new types of explanation are based on the key notion of coverage of an explanation, i.e., the set of all instances it explains. Coverage is powerful enough to capture those constraints, and ensures the independence of explanations of every decision.\nThe second contribution consists of a thorough analysis of the complexity of explaining a decision. We show that finding a prime-implicant explanation becomes computationally much more challenging in a constrained setting.\nThe third contribution consists of proposing a paradigm for making the three solutions feasible. The idea is to avoid exhaustive search by examining a sample of the constrained feature space. We adapt the three types of explanations and show that the worst-case complexity of finding sample-based explanations is greatly reduced.\nThe fourth contribution consists of introducing desirable properties that an explanation function should satisfy, then comparing and analysing the novel functions against them. The results show, in particular, that when explanations are generated from a sample, complexity is greatly reduced but at the cost of violating a desirable property, which ensures a kind of global coherence of the set of all explanations that may be returned by a function.\nThe paper is structured as follows: Section 2 recalls previous def-"}, {"title": "2 Background", "content": "Throughout the paper, we consider a classification theory as a tuple made of a finite set $F$ of features (also called attributes), a function $dom$ which returns the domain of every feature $f \\in F$, where $dom(f)$ is finite and $|dom(f)| > 1$, and a finite set $Cl$ of classes with $|Cl| \\geq 2$. We call literal any pair $(f, v)$ where $f \\in F$ and $v \\in dom(f)$. A partial assignment is any set of literals with each feature in $F$ occurring at most once; it is called an instance when every feature appears once. We denote by $E$ the set of all possible partial assignments and by $IF$ the feature space, i.e., the set of all instances. For all $E, E' \\in E$, the notation $E(E')$ is a shorthand for $E \\subseteq E'$.\nThe reason for this notation is that if $E$ is a partial assignment, then $E$ can be viewed as a predicate on instances $x$: $E(x)$ means that $x$ agrees with $E$ on the subset of features on which it is defined.\nDefinition 1 (Theory). A classification theory is a tuple $(F, dom, Cl)$.\nWe consider a classifier $\\kappa$, which is a function mapping every instance in $F$ to a class in the set $Cl$. We make the reasonable assumption that $\\kappa$ can be evaluated in polynomial time.\nAbductive explanations (AXp) answer questions of the form: why is instance $x$ assigned outcome $c$ by classifier $\\kappa$? They are partial assignments, which are sufficient for ensuring the prediction $c$. We recall below the definition of AXp [11, 28].\nDefinition 2 (wAXp, AXp). Let $x \\in F$. A weak AXp (WAXp) of $\\kappa(x)$ is a partial assignment $E \\in E$ such that:\n$\\bullet$ $E(x)$,\n$\\bullet$ $\\forall y \\in F.(E(y) \\rightarrow (\\kappa(y) = \\kappa(x)))$.\nAn AXp of $\\kappa(x)$ is a subset-minimal weak AXp.\nExample 1. Suppose that $F = {f1, f2}$, with $dom(f1) = dom(f2) = {0, 1}$, and $Cl = {0,1}$. Consider the classifier $\\kappa_1$ such that for any $x \\in F$, $\\kappa_1(x) = (f1, 1) \\lor (f2, 1)$. Its predictions are summarized in the table below."}, {"title": "3 Limits", "content": "The definition of AXp implicitly assumes independence of features, i.e. there are no constraints between the values they may take. The definition of an AXpc accounts for constraints but only partially. In what follows, we discuss three undesirable consequences of ignoring dependency constraints: existence of superfluous explanations, redundancy of explanations and explosion in their number.\nSuperfluous Explanations. We show next that ignoring con-straints may lead to generating gratuitous explanations.\nExample 2 (Cont) Recall that the decision $K_2 (x_1)$ has two AXpc's: $E_1 = {(f_1,0)}$ and $E_2 = {(f_2,0)}$. From the definition of $K_2$ ($\\forall x \\in F, \\kappa_2(x) = f_1$), it follows that $E_1$ is correct while $E_2$, although logically correct, is superfluous. The correlation between $E_2$ and $K_2(x_1)$ is due to the dependency constraint stating: whenever $f_2$ takes the value 0, $f_1$ takes the same value (and consequently, $K_2$ assigns 1 to the corresponding instance)."}, {"title": "4 Explanations and feature-space coverage", "content": "We revisit in this section the definition of abductive explanations for constrained settings. In the rest of the paper, we assume a fixed but arbitrary classification theory $(F, dom, Cl)$ and a finite set $C$ of constraints on the theory, and more precisely on its set $E$ of partial assignments. For $E\\in E$, the notation $C(E)$ means $E$ satisfies all constraints in $C$, $\\neg C(E)$ means $E$ violates at least one constraint, and $F[C] = {x \\in F | C(x)}$, i.e., the set of instances in $F$ that satisfy the constraints. The set $C$ satisfies the following properties:\n(C1) $F[C] \\neq \\emptyset$ (constraints in $C$ can be satisfied all together).\n(C2) Let $E, E' \\in E$. If $E \\subseteq E'$, then $C(E') \\rightarrow C(E)$.\nWe consider a classifier $K$ which is a function mapping every instance in $F[C]$ to a class in $Cl$. We assume that the test $x \\in F[C]$ and the calculation of $\\kappa(x)$ are polynomial.\nWe have seen that there are two types of constraints. Integrity constraints describe impossible assignments of values. The definition of an AXpc takes them into account by checking instance feasibility.\nOur approach starts by removing all unrealistic instances and focuses only on $F[C]$. However, we have seen in the previous section that this solution is not sufficient for dealing with dependencies between partial assignments that follow from constraints $C$. Before showing how we deal with such dependency constraints, let us first define them.\nDefinition 4 (DC). A dependency constraint (DC) is any formula of the form $E \\rightarrow E'$ such that:\n$\\bullet$ $E, E' \\in E \\setminus {\\emptyset}$,\n$\\bullet$ $E \\neq E'$,\n$\\bullet$ For any $x \\in F[C]$, if $E(x)$ then $E'(x)$.\nWe denote by $C^*$ the set of all such constraints.\nDC's are defined on the entire set $F[C]$ of feasible instances. A DC $E \\rightarrow E'$ means that whenever $E$ holds, $E'$ holds as well. In Example 3, the constraint ${((f_2,1))} \\rightarrow {((f_1,1))}$ means that when the feature $f_2$ takes the value 1, the feature $f_1$ necessarily takes the same value.\nOur approach takes advantage of such information for reducing the number of abductive explanations by avoiding dependent explanations, and therefore discarding redundant or superfluous ones. Before defining the novel notions of explanation, let us first introduce some useful notions. The first one is the coverage of a partial assignment, which is the set of instances it covers.\nDefinition 5 (Coverage). Let $X \\subseteq F$ and $E\\in E$. The coverage of $E$ in $X$ is the set $cov_X (E) = {x \\in X | E(x)}$. When $X = F[C]$, we write $cov(E)$ for short.\nExample 3 (Cont) For $E = {(f_1, 1)}$, $cov(E) = {X_2, X_3}$.\nThe second notion, which is crucial for the new definition of ex-planation, is a subsumption relation defined as follows.\nDefinition 6. Let $X \\subseteq F$ and $E, E' \\in E$. We say that $E'$ subsumes $E$ in $X$ if $\\forall x \\in X.(E(x) \\rightarrow E'(x))$. $E'$ strictly subsumes $E$ in $X$ if $E'$ subsumes $E$ in $X$ but $E$ does not subsume $E'$ in $X$.\nWe show that subsumption is closely related to coverage.\nProposition 2. Let $X \\subseteq F$ and $E, E' \\in E$.\n$\\bullet$ The following statements are equivalent.\n$E'$ subsumes $E$ in $X$.\n$Cov_X (E) \\subseteq Cov_X (E')$.\n$\\bullet$ If $E' \\subseteq E$, then $E'$ subsumes $E$ in $X$. The converse does not always hold.\n$\\bullet$ If $E \\neq E'$, then $COV_F(E) \\neq COV_F(E')$.\nExample 3 (Cont) The partial assignment $E_1 = {(f_1,1)}$ strictly subsumes $E_2 = {(f_2,1)}$ in the space $F[C]$. Indeed, $cov(E_1) = {X_2,X_3}$ and $cov(E_2) = {x_3}$.\nThe subsumption relation is not monotonic meaning that a partial assignment $E$ may subsume another (say $E'$) in a set of instances $X$ but not in some $Y \\nsubseteq X$ as shown in the following example.\nExample 1 (Cont) Assume again the existence of the constraint $f_1 \\land \\neg f_2 \\rightarrow 1$, which means $F[C] = {x_1,x_2,x_4}$. Let $E_1 = {(f_1,0)}$ and $E_2 = {(f_2,0)}$. Note that $E_2$ subsumes $E_1$ in $X = {x_1}$ but not in $F[C]$."}, {"title": "5 Complexity analysis", "content": "Let us investigate the computational complexity of the new types of explanation. We focus on the complexity of testing whether a given partial assignment is a (minimal, preferred) coverage-based PI-explanation, and the complexity of finding one such explanation.\nWe first consider the computational problem of deciding if a partial assignment is a coverage-based PI-explanation (CPI-Xp). We show that the problem can be rewritten as an instance of $\\Pi_2^P$SAT, the problem of testing the satisfiability of a quantified boolean formula of the form $\\forall x\\exists y \\phi(x,y)$, where $x, y$ are vectors of boolean variables and $\\phi$ is an arbitrary boolean formula with no free variables other than $x$ and $y$. It is well known that $\\Pi_2^P$SAT is complete for the complexity class $\\Pi_2^P$. It turns out that testing whether a weak AXpc is a coverage-based PI-explanation is also $\\Pi_2^P$-complete.\nTheorem 1. The problem of testing whether a weak AXpc E is a coverage-based PI-explanation is $\\Pi_2^P$-complete.\nWe now consider the problem of actually finding a coverage-based PI-explanation. In the appendix we give an algorithm which returns one CPI-Xp of a given decision $\\kappa(v) = c$. It is based on the following idea: if a weak AXpc $E$ is not a coverage-based PI-explanation, then this is because there is a weak AXpc $E'$ that strictly subsumes $E$. We call such an $E'$ a counter-example to the hypothesis that $E$ is a coverage-based PI-explanation. Therefore, starting from a weak AXpc $E$, we can look for a counter-example $E'$: if no counter-example exists then we return $E$, otherwise we can replace $E$ by $E'$ and re-iterate the process. This loop must necessarily halt since there cannot be an infinite sequence of partial assignments $E_1, E_2,...$ such that $E_{i+1}$ strictly subsumes $E_i (i = 1, 2, ...)$. We can be more specific: the following proposition shows that the number of iterations is bounded by the number of features.\nTheorem 2. Let $n = |F|$. A CPI-Xp can be found by $n$ calls to an oracle for testing whether a given weak AXpc is a coverage-based PI-explanation.\nIt follows that the complexity of finding one coverage-based PI-explanation is essentially the same (modulo a linear factor) as testing whether a given weak AXpc is a coverage-based PI-explanation.\nThe following proposition shows that imposing minimality (for set inclusion) does not change the complexity. A minimal CPI-Xp can be found by $n$ calls to an oracle (for testing whether a given weak AXpc is a CPI-Xp) together with $2^n$ calls to a SAT oracle. Note that since finding a preferred CPI-Xp (i.e. pCPI-Xp) consists of finding one minimal CPI-Xp, there is no change in complexity. The difference between pCPI-Xp's and minimal CPI-Xp's becomes apparent when enumerating all explanations: there can be many less pCPI-Xp's which is an advantage for the user.\nTheorem 3. Let $n = |F|$. A mCPI-Xp (resp. pCPI-Xp) can be found by $n$ calls to an oracle for testing whether a given weak AXpc is a coverage-based PI-explanation together with $2^n$ calls to a SAT oracle.\nTo sum up, we have shown that taking into account constraints (in the definition of coverage-based prime implicants) may produce less-redundant explanations, but at the cost of a potential increase in computational complexity."}, {"title": "6 Sample-based explanations", "content": "When a classifier is a black-box or a deep neural network which cannot be realistically written down as a function, the only algorithm for testing whether a set of literals is a (weak) AXp is an exhaustive search over the whole feature space. This explains why they are costly from a computational point of view. We have seen in the previous section that the computational complexity of the three novel types of explanations that deal with constraints (CPI-Xp, mCPI-Xp and pCPI-Xp) is even worse. In this section, we propose a paradigm for making the solutions feasible. The idea is to avoid the exhaustive search by examining only a sample of the feature space. The obtained explanations are approximations that can be obtained with lower complexity as we will see next.\nIn this section, we concentrate on a sample (or dataset) $T \\subseteq F[C]$ and the associated values of a black-box classifier $K$. Note that $T$ may be the dataset a classifier has been trained on, a dataset on which the classifier has better performance, or may be generated in a specific way as in [26, 27]. However, we assume that every possible class in $Cl$ is considered in the sample, i.e., it is assigned to at least one instance in $T$: $\\forall c \\in Cl, \\exists x \\in T$ such that $\\kappa(x) = c$.\nIn what follows, we adapt the definitions of the different explanations discussed in the previous sections, and add a suffix 'd-' to indicate the new versions.\n6.1 Abductive explanations based on samples\nRecall that an AXp $E$ of a decision $\\kappa(x) = c$ is a minimal subset of $x$ which guarantees the class $c$. If $k$ is a black-box function, then testing this definition for a given $E$ requires testing the exponential number of assignments to the features not assigned by the partial assignment $E$. The following definition only requires us to test those instances in the sample $T$.\nDefinition 12 (d-wAXp, d-AXp). Let $x \\in F$. A weak dataset-based AXp (d-wAXp) of $\\kappa(x)$ wrt to $T$ is a partial assignment $E \\in E$ such that $E(x)$ and $\\forall y \\in T$, if $E(y)$ then $\\kappa(y) = \\kappa(x)$. A dataset-based AXp (d-AXp) of $\\kappa(x)$ is a subset-minimal d-wAXP of $\\kappa(x)$.\nIn other words, a d-AXp is mathematically equivalent to an AXp under the artificial constraint that the only allowed feature vectors are those in the dataset.\nExample 6. Assume a theory made of four binary features, a binary classifier k defined as follows: $\\kappa(x) = (f_1 \\land f_2) \\lor (f_3 \\land f_4)$. Let $T$ be a sample whose instances and their predictions are summarized in the table below."}, {"title": "7 Properties of explanation functions", "content": "We have seen that for each type of explanation studied in this paper, there is a corresponding explanation function L mapping every instance (in $IF$ or $F[C]$) into a subset of $E$. In this section, we provide seven desirable properties for an explanation function. The first four properties have counterparts in [4], where explanation functions explain the global behaviour of a classifier in a non-constrained setting, and so answer the question: \"why does a classifier recommend a given class in general?\" We adapt the properties for explaining individual decisions and introduce three novel ones that concern how a function should deal with constraints.\nDefinition 14. Let $L$ be an explanation function.\n(Success) $\\forall x \\in F[C], L(x) \\neq \\emptyset$.\n(Non-Triviality) $\\forall x \\in F[C], \\forall E \\in L(x), E \\neq \\emptyset$.\n(Irreducibility) $\\forall x \\in F[C], \\forall E \\in L(x), \\forall l \\in E, \\exists x' \\in F[C]$ such that $\\kappa(x') \\neq \\kappa(x)$ and $(E \\setminus {l})(x')$.\n(Coherence) $\\forall x, x' \\in F[C]$ such that $\\kappa(x) \\neq \\kappa(x'), \\forall E \\in L(x), \\forall E' \\in L(x'), \\nexists x'' \\in F[C] s.t. $(E \\cup E')(x'')$.\n(Consistency) $\\forall x \\in F[C], \\forall E \\in L(x), C(E)$ holds.\n(Independence) $\\forall x \\in F[C], E, E' \\in L(x)$ such that $E \\rightarrow E' \\in C^*$ and $E' \\rightarrow E \\notin C^*$.\n(Non-Equivalence) $\\forall x \\in F[C], \\forall E, E' \\in L(x), E \\neq E'$.\nSuccess ensures existence of explanations. Non-Triviality discards empty explanations as they are non-informative. Irreducibility states that an explanation should not contain unnecessary information. Co-herence ensures a global compatibility of the explanations provided for all the instances. It avoids erroneous explanations. Consider a function which classifies animals as mammals or not, where ani-mals are described by n features such as milks its young, lays eggs, etc. Let x be a mouse and x' an eagle. If the explanation E for $\\kappa(x) = 1$ is that mice milk their young and the explanation $E'$ for $\\kappa(x') = 0$ is that eagles lay eggs, then coherence is not satisfied be-cause there are animals $x''$ (such as the platypus) which milk their young and lay eggs. Consistency ensures that explanations satisfy all constraints in C. Independence ensures that dependency constraints are considered and the explanations of a decision should be pairwise independent. Non-equivalence avoids equivalent explanations. This property is important for reducing the number of explanations.\nWe show that the seven properties are compatible, i.e., they can be satisfied all together by an explanation function.\nTheorem 6. The properties are compatible.\nTable 2 summarizes the properties satisfied by each type of ex-planation discussed in the paper. It thus provides a comprehensive formal comparison of their explainers, and sheds light on the key properties that distinguish any pair of explainers.\nTheorem 7. The properties of Table 2 hold."}, {"title": "A Appendix: Proofs", "content": "Proposition 1. Let $x \\in F$ be such that $C(x)$, and $E \\in E$. If E is an AXp of $\\kappa(x)$, then $\\exists E' \\in E$ such that $E' \\subseteq E$ and $E'$ is an AXpc of $\\kappa(x)$.\nProof. Let $x \\in F$ and $E\\in E$. Assume that E is an AXp of $\\kappa(x)$. Then, $E(x)$ and $\\forall y \\in F.(E(y) \\rightarrow (\\kappa(y) = \\kappa(x)))$. Let $T = {y \\in F | E(y) \\land (\\kappa(y) = \\kappa(x))}$ and $T' = {y \\in F | C(y) \\land E(y) \\land (\\kappa(y) = \\kappa(x))}$. Clearly, $T' \\subseteq T$. Hence, $\\exists E' \\subseteq E$ which verifies the conditions of an AXpc.\nProposition 2. Let $X \\subseteq F$ and $E, E' \\in E$.\n$\\bullet$ The following statements are equivalent.\n$E'$ subsumes $E$ in $X$.\n$Cov_X (E) \\subseteq Cov_X (E')$.\n$\\bullet$ If $E' \\subseteq E$, then $E'$ subsumes $E$ in $X$. The converse does not always hold.\n$\\bullet$ If $E \\neq E'$, then $COV_F (E) \\neq COV_F(E')$.\nProof. Let $X \\subseteq F[C]$ and $E, E' \\in E$. Assume $E'$ subsumes $E$ in $X$. Then, $\\forall x \\in X.(E(x) \\rightarrow E'(x))$. Thus, ${x \\in X | E(x)} \\subseteq {x \\in X | E'(x)}$, and so $cov_X (E) \\subseteq cov_X (E')$. Assume now that $cov_X (E) \\subseteq cov_X (E')$. This means ${x \\in X | E(x)} \\subseteq {x \\in X | E'(x)}$, and so $E'$ subsumes $E$ in $X$. The second property (when $E' \\subseteq E$) follows from the inclusion $cov_X (E') \\subseteq Cov_X (E)$ and the above equivalence.\nAssume $E \\neq E'$. Since $IF$ contains all the possible assignments of values and $E, E'$ are consistent assignments (i.e., they do not con-tain two literals assigning different values to the same feature), and knowing that domains of all features are of size at least 2, we can deduce that $\\exists z, z' \\in F$ such that $E \\subseteq z$ and $E' \\subseteq z$ and $E' \\subseteq z'$ and $E \\nsubseteq z'$. Thus, $cov (E) \\nsubseteq cov (E')$ and $cov (E') \\nsubseteq Cov(E)$. So, $cov(E) \\neq cov(E')$.\nProposition 3. Let $E, E' \\in E$. If $E \\rightarrow E' \\in C^*$, then $X \\subseteq F[C]$, $E'$ subsumes $E$ in $X$.\nProof. Let $E, E' \\in E$ such that $E \\rightarrow E' \\in C^*$. From the definition of dependency constraints, $\\forall x \\in F[C]$, if $E(x)$ then $E'(x)$. Let $X \\subseteq F[C]$. It follows that $\\forall x \\in X$, if $E(x)$ then $E'(x)$. So, $E'$ subsumes $E$ in $X$.\nProposition 4. Let $x \\in F[C]$. The following inclusions hold.\n1. $L_{AXp(x)} \\subseteq L_{wAXp(x)}$,\n2. $L_{CPI-Xp(x)} \\subseteq L_{wAXpc(x)}$,\n3. $L_{mCPI-Xp(x)} \\subseteq L_{AXpc(x)} \\subseteq L_{wAXpc(x)}$,\n4. $L_{pCPI-Xp(x)} \\subseteq L_{mCPI-Xp(x)} \\subseteq L_{CPI-Xp(x)}$,\n5. If $C = \\emptyset$, then the following hold:\n(a) $L_{AXp(x)} = L_{AXpc(x)}$\n(b) $L_{AXpc(x)} = L_{mCPI-Xp(x)}$.\nProof. Let $x \\in F[C]$. The inclusions 1), 4) and $L_{AXpc(x)} \\subseteq L_{wAX pc (x)}$ follow straightforwardly from the definitions of the corresponding types of explanations.\nLet us show the inclusion $L_{CPI-Xp(x)} \\subseteq L_{wAXpc (x)}$. Assume $E\\in E$ is a CPI-Xp for $\\kappa(x)$. Then the following properties hold:\na) $E(x)$,\nb) $\\forall y \\in F[C].(E(y) \\rightarrow (\\kappa(y) = \\kappa(x)))$,\nc) $\\nexists E' \\in E$ such that $E'$ satisfies the conditions a) and b) and strictly subsumes $E$ in $F[C]$.\nFrom b), it follows that $\\forall y \\in F.(C(y) \\land E(y) \\rightarrow (\\kappa(y) = \\kappa(x)))$. So, $E$ is a weak AXpc for $\\kappa(x)$.\nLet us show the inclusion $L_{mCPI-Xp(x)} \\subseteq L_{AXpc(x)}$. Assume $E\\in E$ is an mCPI-Xp for $\\kappa(x)$. Then the following properties hold:\na) $E(x)$,\nb) $\\forall y \\in F[C].(E(y) \\rightarrow (\\kappa(y) = \\kappa(x)))$,\nc) $\\nexists E' \\in E$ such that $E'$ satisfies the conditions a) and b) and strictly subsumes $E$ in $F[C]$.\nd) $\\nexists E' \\subset E$ which satisfies a), b) and c).\nNote that the condition b) is equivalent to the following formula: $\\forall y \\in F.(C(y) \\land E(y) \\rightarrow (\\kappa(y) = \\kappa(x)))$. Assume that $E \\subseteq L_{AXpc(x)}$. Since $E$ satisfies a) and b), then $\\exists E \\subseteq E$ which satisfies the same properties. From Proposition 2, $cov(E) \\subseteq cov(E')$. From condition c), $E'$ does not strictly subsume $E$, so $cov(E) = cov(E')$. Hence, $E'$ satisfies a), b), c) and this contradicts condition d) of $E$.\nAssume that $C = \\emptyset$, then $F[C] = F$. The equality $L_{AXp(x)} = L_{AX pc (x)}$ follows from the fact $\\forall y \\in F, C(y) = T$ (by assumption), and thus $\\forall y \\in F.(C(y) \\land E(y) \\rightarrow (\\kappa(y) = \\kappa(x)))$ is equivalent to $\\forall y \\in F.(E(y) \\rightarrow (\\kappa(y) = \\kappa(x)))$.\nWe already have the inclusion $L_{mCPI-Xp(x)} \\subseteq L_{AXpc (x)}$ from property 3. Let us show the inclusion $L_{AXpc(x)} \\subseteq L_{mCPI-Xp(x)}$. Let $E \\subseteq L_{AXpc(x)}$, then from the property 5(a) above, $E \\subseteq L_{AXp(x)}$ and thus it satisfies the following properties:\na) $E(x)$,\nb) $\\forall y \\in F.(E(y) \\rightarrow (\\kappa(y) = \\kappa(x)))$,\nc) $\\nexists E'\\subset E$ which satisfies a) and b).\nAssume $E \\notin L_{mCPI-Xp(x)}$. Then, $\\exists E'\\in E$ such that $E'$ satisfies a) and b) and strictly subsumes $E$ in. From Proposition 2, $cov(E) \\subset cov(E')$ (A). Since $F[C] = F$ and $E' \\neq E$, from the 3rd property of Proposition 2, $cov(E) \\nsubseteq cov(E')$ which contradicts assumption (A).\nBefore studying the computational problem of deciding whether an explanation is a coverage-based PI-explanation (CPI-Xp), we in-troduce some notation. Recall that a partial assignment $A$ can be written as a predicate, with $A(x)$ meaning that the assignment $x$ is an extension of $A$. For partial assignments $A, B$ we use the notation $A \\rightarrow B$ for $\\forall x \\in F[C].(A(x) \\rightarrow B(x))$. In the following we suppose that kappa(v) is the decision to explain. Let $L_{v, E,C}$ denote the set of literals in $v$ implied by $E \\land C$. Thus $E \\subseteq L_{v,E,C}$, where we view the set of literals $L_{v, E,C}$ as a partial assignment.\nTheorem 1. The problem of testing whether a weak AXpc E is a coverage-based PI-explanation is $\\Pi_2^P$-complete.\nProof. Recall that the set of possible explanations $E$ of $\\kappa(v)$ is the set of all partial assignments that are subsets of $v$ (viewed as a set of n literals). A weak AXpc $E \\in E$ is a coverage-based PI-explanation of $\\kappa(v)$ iff\n$\\forall E' \\subseteq L_{v,E,c}.(\\exists y \\in F[C].(E'(y) \\land \\neg\\kappa(y) = c) \\lor \\forall x \\in F[C].(E'(x) \\rightarrow E(x)))$ (1)\nBy rewriting the definition of whether a given weak AXpc E is a coverage-based PI-explanation in equation (1) as\n$\\forall E' \\subseteq L_{v,E,C}.\\forall x \\in F[C].\\exists y \\in F[C].((\\existsE' (y) \\land \\neg\\kappa(y) = c) \\lor E'(x) \\rightarrow E(x))$ (2)"}]}