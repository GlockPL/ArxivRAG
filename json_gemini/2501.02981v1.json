{"title": "CONTINUUM: Detecting APT Attacks through Spatial-Temporal Graph Neural Networks", "authors": ["Atmane Ayoub Mansour Bahar", "Kamel Soa\u00efd Ferrahi", "Mohamed-Lamine Messa\u00efb", "Hamida Seba", "Karima Amrouche"], "abstract": "Advanced Persistent Threats (APTs) represent a significant challenge in cybersecurity due to their sophisticated and stealthy nature. Traditional Intrusion Detection Systems (IDS) often fall short in detecting these multi-stage attacks. Recently, Graph Neural Networks (GNNs) have been employed to enhance IDS capabilities by analyzing the complex relationships within networked data. However, existing GNN-based solutions are hampered by high false positive rates and substantial resource consumption. In this paper, we present a novel IDS designed to detect APTs using a Spatio-Temporal Graph Neural Network Autoencoder. Our approach leverages spatial information to understand the interactions between entities within a graph and temporal information to capture the evolution of the graph over time. This dual perspective is crucial for identifying the sequential stages of APTs. Furthermore, to address privacy and scalability concerns, we deploy our architecture in a federated learning environment. This setup ensures that local data remains on-premise while encrypted model-weights are shared and aggregated using homomorphic encryption, maintaining data privacy and security. Our evaluation shows that this system effectively detects APTs with lower false positive rates and optimized resource usage compared to existing methods, highlighting the potential of spatio-temporal analysis and federated learning in enhancing cybersecurity defenses.", "sections": [{"title": "1. Introduction", "content": "In recent years, the frequency and sophistication of cyber-attacks have escalated, posing significant challenges to the cybersecurity community. Among these threats, Advanced Persistent Threats (APTs) stand out due to their stealthy, prolonged, and multi-stage nature. APTs are highly targeted attacks typically orchestrated by well-funded adversaries, aiming to gain and maintain unauthorized access to a network while evading detection for extended periods. These multi-stage attacks often involve a series of coordinated steps, including initial intrusion, lateral movement, privilege escalation, and data exfiltration.\nTraditional Intrusion Detection Systems (IDS) have been pivotal in safeguarding network security by monitoring and analyzing network traffic for signs of malicious activities. However, the sophisticated tactics employed by APTs often render these traditional systems inadequate. APTs can adapt and evolve their techniques to bypass conventional security measures, involving more advanced detection mechanisms.\nOne promising approach to understanding and detecting these complex threats is through the analysis of provenance graphs. Provenance graphs capture the history and lineage of data and events within a system, detailing the"}, {"title": "2. Preliminaries", "content": "In this section, we define the key concepts used in this work as well as the threat model."}, {"title": "2.1. Definitions", "content": "The key concepts defined are as follows: Advanced Persistent Threats, provenance graphs, and a specific category of Graph Neural Networks, called Spatial Convolutional Graph Neural Networks.\nAdvanced Persistent Threats (APTs): APTs are sophisticated and prolonged cyberattacks (Alshamrani et al., 2019) aimed at stealing sensitive information, disrupting operations, or causing damage to targeted entities (Yasar and Rosenrance, 2023). The primary characteristics of APTs include their advanced nature, which involves complex techniques, tools, and exploits to compromise the target, their persistence, which ensures that they remain within the target network for an extended period to achieve long-term objectives, and the significant threat they pose, often carried out by highly skilled adversaries with substantial resources, such as state-sponsored or well-funded organizations.\nUnlike typical cyberattacks, APTs involve multiple stages (Quintero-Bonilla and Mart\u00edn del Rey, 2020; Ussath et al., 2016; Sexton et al., 2015; Vukalovi\u0107 and Delija, 2015; Security, 2019) and employ various techniques to maintain access and evade detection over long periods, each stage involving specific actions and objectives. For example, some common APT stages are as follows:\n\u2022 Initial Intrusion: Exploiting vulnerabilities or using social engineering to gain access to the target network.\n\u2022 Establishing Foothold: Deploying malware or backdoors to maintain access.\n\u2022 Lateral Movement: Moving laterally within the network to identify high-value targets and gain additional privileges.\n\u2022 Privilege Escalation: Obtaining higher-level access to perform more significant actions.\n\u2022 Data Exfiltration or Attack Execution: Stealing sensitive information or executing destructive actions.\n\u2022 Persistence: Ensuring long-term access and evading detection through various means."}, {"title": "Graph Neural Networks (GNNs):", "content": "GNNs are a class of neural networks specifically designed to operate on graph-structured data (Liu and Zhou, 2022). A graph G = (V, E) consists of a set of nodes V and edges E, where each edge (i, j) \u2208 E represents a relationship between nodes i and j. Each node v\u1d62 \u2208 V may also have a feature vector x\u1d62 \u2208 \u211d\u1d48, representing the attributes of the node. GNNs aim to learn node representations by iteratively aggregating information from their neighbors. At each layer l of the GNN, the representation of node v\u1d62 is updated by aggregating the feature vectors of its neighboring nodes N(v\u1d62), followed by a non-linear transformation.\nThe general formulation of a GNN layer can be written as:\nh_{i}^{(l+1)} = \u03c3(W_{self}^{(l)}h_{i}^{(l)} + \\sum_{j \u2208 N(i)}W_{neigh}^{(l)}h_{j}^{(l)})\nwhere h\u1d62\u207d\u02e1\u207e is the feature vector of node i at layer l, N(i) represents the set of neighbors of node i, W_{self}^{(l)} and W_{neigh}^{(l)} are learnable weight-matrices for node i and its neighbors, respectively, and \u03c3 is a non-linear activation function (e.g., ReLU). This equation describes how each node's representation is updated by combining its current representation h\u1d62\u207d\u02e1\u207e with the aggregated information from its neighbors h\u2c7c\u207d\u02e1\u207e. The learned node embeddings h\u1d62\u207d\u1d38\u207e after L layers can then be used for various downstream tasks such as node classification, link prediction, and graph classification.\nWu et al. (2020) categorized GNNs based on their architecture and message-passing mechanisms into four primary types of GNNs: Recurrent GNNs (recGNN) (Li et al., 2015; Gallicchio and Micheli, 2010; Scarselli et al., 2008; Dai et al., 2018), Convolutional GNNs (convGNN) (Veli\u010dkovi\u0107 et al., 2017; Hamilton et al., 2017; Xu et al., 2018; Gilmer et al., 2017; Velickovic et al., 2019; Defferrard et al., 2016; Kipf and Welling, 2016a; Li et al., 2018; Zhuang and Ma, 2018; Levie et al., 2018), Graph Autoencoders (GAEs) (Kipf and Welling, 2016b; Wang et al., 2016; Hajiramezanali et al., 2019; Bojchevski et al., 2018; Tu et al., 2018), and Spatial-Temporal GNNs (STGNN) (Chen et al., 2023; Seo et al., 2018; Wang et al., 2022b; Yan et al., 2018; Li et al., 2023b; Xu et al., 2020). Each category of GNN serves a different purpose, offering unique advantages depending on the task at hand.\n\u2022 Recurrent Graph Neural Networks (recGNNs): recGNNs incorporate recurrent connections within their architecture to iteratively update node representations over multiple steps. The hidden state of each node is updated based on its previous hidden states and those of its neighboring nodes using a recurrent neural network (RNN) layer such as GRU or LSTM. This iterative process allows recGNNs to capture sequential information exchange between nodes in a graph."}, {"title": "A general update function for recGNNs is expressed as:", "content": "h^{(t)}(v) = f_{RNN}(h^{(t-1)}(v), \\sum_{u \u2208 N(v)} Wh^{(t-1)}(u))\nwhere h\u207d\u1d57\u207e(v) is the hidden state of node v at time step t, fRNN is the recurrent neural network function, N(v) represents the neighboring nodes of node v, and W is the learnable weight matrix.\n\u2022 Convolutional Graph Neural Networks (ConvGNNs): ConvGNNs extend the concept of convolutional operations from traditional grid-structured data (such as images) to irregular graph-structured data. They can be categorized into two main types: Spatial Convolutional GNNs (SpGNNs) (Veli\u010dkovi\u0107 et al., 2017; Hamilton et al., 2017; Xu et al., 2018; Gilmer et al., 2017; Velickovic et al., 2019) and Spectral Convolutional GNNs (SCGNNs) (Defferrard et al., 2016; Kipf and Welling, 2016a; Li et al., 2018; Zhuang and Ma, 2018; Levie et al., 2018).\nSpatial Convolutional GNNs: Spatial convolutional GNNs perform convolutions by aggregating information from a node's neighbors (Bernstein, 2023). The key idea is to iteratively update the representation of a node by combining its features with those of its neighbors. This process captures the local structure and features of the graph.\nFormally, the update rule for the node representations in spatial convolutional GNNs can be described as follows:\nh_{i}^{(k)} = \u03c3(W_{self}^{(k)}h_{i}^{(k-1)} + \\sum_{j \u2208 N(i)}W_{neigh}^{(k)}h_{j}^{(k-1)})\nwhere h\u1d62\u207d\u1d4f\u207e is the feature vector of node i at the k-th layer, W_{self}^{(k)} and W_{neigh}^{(k)} are learnable weight matrices for the node itself and its neighbors, respectively, N(i) denotes the set of neighbors of node i, and \u03c3 is an activation function such as ReLU.\nThe above equation signifies that the new representation of a node h\u1d62\u207d\u1d4f\u207e is obtained by combining its previous representation h\u1d62\u207d\u1d4f\u207b\u00b9\u207e with the representations of its neighbors h\u2c7c\u207d\u1d4f\u207b\u00b9\u207e, weighted by the matrices W_{self}^{(k)} and W_{neigh}^{(k)}. This aggregation step effectively captures the spatial dependencies and relationships within the graph.\nSome well-known examples of SpGNNs are Graph Attention Networks (Veli\u010dkovi\u0107 et al., 2017), Graph-SAGE (Hamilton et al., 2017), and Graph Isomorphism Networks (Xu et al., 2018). By iteratively applying this update rule across multiple layers, these GNNs can capture higher-order neighborhood information, enabling them to learn rich representations of the nodes and their interactions. This makes them particularly well-suited for tasks such as node classification, link prediction, and graph classification.\nSpectral Convolutional GNNs: Spectral Convolutional GNNs perform convolutions in the spectral domain by leveraging the eigenvalues and eigenvectors of the graph Laplacian (Kipf, 2016). The central idea is to transform node features into the spectral domain, apply spectral filters, and then map them back to the spatial domain. This process captures global graph structure while maintaining efficiency using approximations like Chebyshev polynomials (Mason and Handscomb, 2002).\nFormally, the spectral convolution can be expressed as:\nh^{(l+1)} = U diag(\u0398^{(l)}) \u00b7 U^{T} \u00b7 h^{(l)}\nwhere h\u207d\u1d4f\u207e and h\u207d\u1d4f\u207a\u00b9\u207e are the feature vectors of node i at the k-th and k + 1-th layers respectively, U is the matrix of eigenvectors of the graph Laplacian, \u0398\u207d\u1d4f\u207e represents the spectral filter parameters for layer k, diag(\u0398\u207d\u1d4f\u207e) is a diagonal matrix formed from layer \u0398\u207d\u1d4f\u207e.\nThe equation indicates that the updated node features are obtained by filtering the node representations in the spectral domain and transforming them back to the original domain. This process allows ScGNNs to effectively capture global graph properties and relationships."}, {"title": "\u2022 Graph Autoencoders (GAEs):", "content": "GAEs are a class of GNNs that use an autoencoder framework to encode nodes into embeddings and learn low-dimensional representations of graphs. These models are designed to capture the essential structural properties of a graph while allowing for compression and reconstruction of the graph structure. GAEs define an unsupervised reconstruction error between the original and predicted adjacency matrices, ensuring that the model learns meaningful representations (Ward et al., 2022).\nThe general encoding process can be expressed as:\nz(v) = f(\\frac{1}{\\sqrt{d(v)}} \\sum_{u\u2208N(v)} Wx(u))\nwhere z(v) is the embedding of node v, d(v) and d(u) are the degrees of nodes v and u, W is the weight matrix, and x(u) is the input feature vector. Examples of GAEs include Variational Graph Autoencoders (VGAE) (Kipf and Welling, 2016b) and Structure Deep Network Embeddings (SDNE) (Wang et al., 2016).\n\u2022 Spatial-Temporal Graph Neural Networks (STGNNs): Spatial-Temporal Graph Neural Networks (STGNNs) extend GNNs to handle temporal dynamics by integrating spatial and temporal information. These models are particularly useful for spatio-temporal data, such as traffic forecasting or time-evolving networks. STGNNs use spatial layers to aggregate information from neighboring nodes within the same time step and temporal layers to capture dependencies across time steps.\nA generalized update equation for STGNNs is:\nh^{(t)}(v) = f(h^{(t-1)}(v), AGG_{Spatial}(h^{(t-1)}(u) :: u\u2208N(v)), AGG_{Temporal}(h^{(t)}(v)))\nwhere AGGspatial is the Spatial aggregation-function, and AGGtemporal is the Temporal aggregation-function.\nExamples include Gated Spatio-Temporal Graph Convolutional Networks (ST-GCN) (Yan et al., 2018) and Temporal Graph Networks (TGN) (Rossi et al., 2020)."}, {"title": "2.2. Threat Model", "content": "In the proposed model, we assume that the adversary has advanced capabilities, including the ability to breach the system perimeter through various attack vectors such as phishing (social engineering) or exploitation of unpatched vulnerabilities. Once inside the system, the attacker seeks to maintain persistent access by installing back-doors or leveraging malware, moving laterally within the network to escalate privileges, and carrying out the attack over an extended period without being detected. We also assume that the attacker can compromise both hosts and servers within the organization. However, he leaves behind him abnormal system events and entity interactions, which are represented in the provenance graph, and analyzed to detect anomalies indicative of an APT attack."}, {"title": "3. Motivation", "content": "The research presented in this paper is driven by several key-motivations regarding the nature of Advances Persistent Threats and their detection by exploiting GNNs. In this section, we present an overview of these motivations:\n\u2022 Addressing the stealthiness od APTs: Advanced Persistent Threats are highly stealthy and sophisticated, often blending their activities with legitimate network traffic to avoid detection. Traditional anomaly detection methods struggle with the vast amount of data and the subtlety of these anomalies (ADNAN et al., 2023; Zimba et al., 2020; Lu et al., 2019; Su et al., 2017). To overcome this challenge, we need a solution that allows learning compressed representations of data, and effectively distinguishing normal behavior from anomalies. For this"}, {"title": "\u2022 Encompassing the relationships between system entities:", "content": "APTs often exploit the relationships between system entities (users, files, processes, ... etc.) to hide their malicious activities within benign system operations and propagate within the system. To effectively detect these threats, it is essential to deeply understand the relationships represented in provenance graphs (Yan et al., 2022; Lv et al., 2022), and extract the correlations between their nodes and edges. For that, we need models that focus on the information transmitted between a node and its neighbourhood, called the spatial information, in order to capture the relationships and interactions between entities, which can reveal hidden patterns indicative of APT activities.\nThe solution that we propose in this paper is implementing a Spatial convolutional GNN, such as Graph Isomorphism Networks (GIN) (Xu et al., 2018), Graph Attention Networks (GAT) (Veli\u010dkovi\u0107 et al., 2017), and GraphSAGE (Hamilton et al., 2017). This type of GNNs is particularly suited for modelling the spatial dynamics of the network (messages passed in a nodes' neighbourhood), and understanding intricate correlation between system entities (Sahili and Awad, 2023; Wu et al., 2020).\n\u2022 Capturing time span of APTS: APTs tend to be prolonged and encompass different stages, including initial intrusion, lateral movement, privilege escalation, and data exfiltration. These stages can occur over extended periods, making it crucial to identify the time-dependencies of APT campaigns (Cheng et al., 2023b; Zhu et al., 2021; Ghafir et al., 2019). To detect these temporal patterns, we integrate machine-learning models that possess memory capabilities and can retain information separated by time. Models such as Recurrent Neural Networks (RNNs) (Rumelhart et al., 1986), Long Short-Term Memory networks (LSTMs) (Hochreiter, 1997), and Gated Recurrent Units (GRUs) (Cho et al., 2014), are effective in storing and processing sequential-data, allowing our system to track the evolution of APTs across different stages.\n\u2022 Encompassing the dynamic aspects of APTS: APT attackers usually aim to compromise a whole system rather than a specific host, making it crucial to have a model that understands the behavior of the organization as a whole (Mansour Bahar et al., 2024). Local models often struggle to generalize to new, dynamic, and evolving APT attacks because they are trained on data from a single host, limiting their ability to detect threats that target an entire organization (Zimba et al., 2020; Li et al., 2023a). Federated Learning (FL), for instance, offers a solution to these issues by enabling collaborative training across different hosts, without having to share their raw data (Son et al., 2023). This decentralized approach not only enhances the detection of organization-wide threats but also reduces training time by leveraging the computational resources of multiple clients (Mansour Bahar et al., 2024; Son et al., 2023). By collaboratively training on diverse data from various hosts, the model can generalize better to new APTs and provide a more comprehensive defense mechanism.\nFederated solutions can also be deployed in a large-scale environment, where different companies or state-related organisations collaborate with their respective model weights to build an Intrusion Detection System that can generalise to various system-behaviours, and effectively detect new APT attacks. This architecture plays the role of a defensive alliance among organisations against targeted and sophisticated attacks.\nHowever, Federated Learning introduces new privacy concerns, as model weights are shared between clients and servers, making them vulnerable to data leakage (Shanmugarasa et al., 2023).\n\u2022 Addressing Privacy Concerns: While Federated Learning enhances collaborative training, it also raises significant privacy issues. Transmitting model weights in clear text can expose them to Man-in-the-Middle (MitM) attacks, where an adversary can intercept and reverse-engineer the data of clients. In addition, an attacker could target the central server to obtain model weights in what is known as Inference Attacks (Krumm, 2007), compromising the privacy of all participating clients. To mitigate these risks, a solution that can be employed is Homomorphic Encryption (Yi et al., 2014; Doan et al., 2023), which allows computations to"}, {"title": "4. Related Work", "content": "In this section we provide a comprehensive background and literature review of recent works in various areas relevant to our research. We focus on several key aspects: GNN-based APT detection, graph-data generation, decentralised and distributed IDS architectures, and encryption techniques in federated learning. This review highlights the existing methodologies and their limitations, setting the stage for the novel contributions of our study."}, {"title": "4.1. GNNs in Cybersecurity", "content": "Graph Neural Networks have emerged as a highly effective tool for cybersecurity, particularly in the domain of intrusion detection. Traditional Intrusion Detection Systems often rely on rule-based or signature-based approaches (Han et al., 2020), which struggle to adapt to rapidly evolving attack strategies and novel threats. In contrast, GNNs offer the ability to model complex relationships within network structures, enabling them to identify subtle, concealed-patterns commonly associated with APTs.\nBy leveraging the ability to capture intricate interconnections between network entities, GNNs can provide a more robust detection mechanism compared to conventional IDS. For instance, Pujol-Perich et al. (2022) demonstrated GNNs' capability in anomaly detection by capturing network-flow relationships, offering precise identification of malicious activities such as Distributed Denial of Service (DDoS), port scans, and network scans.\nRecent studies further emphasize the effectiveness of GNNs in cybersecurity. Wang and Yu (2022) highlighted how GNNs can tackle the challenges of applying deep learning to structured graph data, showing their utility across various domains, including social networks and bioinformatics. The mathematical framework of GNNs aligns well with real-world network structures, making them an ideal choice for anomaly detection. Additionally, Bilot et al. (2023) explored the application of GNNs for detecting APTs, underscoring their potential in enhancing IDS systems by identifying sophisticated threat patterns.\nWhile GNNs offer a transformative approach to intrusion detection by modeling complex network relationships, their effectiveness relies on the availability of high-quality datasets and careful handling of challenges such as data biases and concept drift (Tsymbal, 2004). Furthermore, implementing GNNs in cybersecurity requires significant technical expertise to fully exploit their capabilities."}, {"title": "4.2. Graph Generation", "content": "Graph data, such as provenance graphs used in intrusion detection, can be tailored based on the specific requirements of each model (Zhong et al., 2024). Depending on the nature of the task, models may operate on either static or dynamic graph data, with the latter being essential for real-time analysis, such as flow-based network monitoring (Kazemi et al., 2020)."}, {"title": "4.2.1. Static-data", "content": "Static graphs, with their fixed topology and connections, represent the network or system at a specific moment in time. These types of graphs are commonly used in graph-based IDS, as seen in studies by (Protogerou et al., 2021; Lo et al., 2022), and Wang et al. (2023). Before being input into models, static graph data can be modified through techniques such as enrichment or transformation to improve the model's ability to detect anomalies and cyber threats.\nEnriching static graphs involves adding more information to the original graph to enhance its structure. For example, in the work of Zheng and Li (2019), additional edges and hyper-edges were introduced, such as connecting two-hop neighbors, to capture more complex relationships within the data. Similarly, Cheng et al. (2021) used methods like one-hot encoding and similarity measures to extract new attributes for nodes and edges, allowing the model to gain deeper insights into the graph's structure and enhance detection accuracy.\nAnother approach to modifying static graphs is through transformation, where the graph structure is altered to serve different analytical purposes. Chang and Branco (2021); Zhu and Lu (2022) explored transforming original graphs into line graphs, where each vertex corresponds to an edge in the original graph. This transformation can be particularly useful for converting tasks like edge classification into node classification, streamlining the analysis and allowing for more efficient detection of malicious activities, as discussed by Zhong et al. (2024)."}, {"title": "4.2.2. Dynamic-data", "content": "Provenance graphs are generally-speaking static data, i.e, they represent the network or system in a fixed topology. However, in a real-time intrusion detection, data is dynamic, i.e represented in flows or sequences processed separately over time. Dynamic methods involves mainly two techniques, snapshoting and sketching (Zhong et al., 2024).\nSnapshoting approach involves dividing the data into intervals and creating a snapshot for each interval. Each snapshot represents the state of the network during a specific time period, allowing for the analysis of temporal changes and patterns. For example, King and Huang (2023); Jedh et al. (2021) proposed techniques that use specific time units or intervals as snapshots to analyze the network's structure and relationships over time. Xiao et al. (2021) extended this approach by considering each timestamp as a node, thereby constructing a graph with a fixed number of timestamps at each instance. This method effectively captures temporal dynamics in the network data.\nDifferently, sketching techniques involves creating a summarized or approximate representation of the graph to reduce complexity while retaining essential features. This method focuses on efficiently capturing the essential information from a large and complex network. Paudel and Eberle (2020) introduced a graph-sketching technique that converts a graph into a low-dimensional sketch vector using a simplified hashing technique. Messai and Seba (2023) proposed constructing an activity graph from networking events during a monitoring period. This activity graph captures both structural and semantic features from network traffic, which are then used to train a neural network to distinguish between normal activities and attacks."}, {"title": "4.3. GNNs in APT attacks Detection", "content": "The utilisation of GNNs in Intrusion Detection Systems has been at the forefront of recent advancements in Cybersecurity, particularly for detecting APT attacks. Various GNN-based architectures have been developed to"}, {"title": "4.4. Decentralised and Distributed APT Detection", "content": "Few works only have studied the application of decentralised or distributed IDS architecture, especially in the detection of APT attacks. Most APT-detectors (Jia et al., 2023; Cheng et al., 2023b; Wang et al., 2022a; Cheng et al., 2023a) rely on centralized data analysis, which can be resource-intensive and pose significant privacy risks.\nIn a pioneer work, Wu et al. (2022) introduced a distributed IDS architecture named Paradise, which utilizes Kafka servers (Thein, 2014) to distribute client data to IDS servers for parallel attack detection. Although this architecture improves resource efficiency and enables parallel processing, it also faces challenges related to bandwidth consumption and data privacy. By transmitting system-logs and other sensitive information over the network, this approach is vulnerable to sniffing attacks.\nIn response to these challenges, federated learning offers a more secure alternative. By allowing clients to train local models and share only model parameters with the central server, FL facilitates the development of models capable of identifying anomalous behavior indicative of potential intrusions while ensuring that sensitive data remains within the local environment of each client, which minimizes the risk of data exposure and reduces network strain. For instance, Son et al. (2023) explored the application of FL on GNNs for APT attacks detection and demonstrated that FL could reduce the false-positive rate and enable the model to generalize to unknown attacks through collaborative model-training."}, {"title": "4.5. Encryption in Federated Learning", "content": "While FL improves data privacy by keeping raw data local, it still involves sharing model updates, which can be intercepted by malicious actors whom can reconstruct the clients' data using reverse engineering (Shanmugarasa et al., 2023). This vulnerability has led to the exploration of various encryption techniques to secure the communication of model weights.\nIn our previous research, called FedHE-Graph (Mansour Bahar et al., 2024), we addressed some of these challenges in APT attacks detection by testing federated learning with hybrid encryption, using Advanced Encryption Standard (AES) for the symmetric encryption (Dworkin et al., 2001), and Rivest-Shamir-Adleman (RSA) for the asymmetric one (Rivest et al., 1978). This approach demonstrated promising results by reducing execution time and enhancing privacy. However, it showed limitations against inference attacks (Krumm, 2007), where an attacker targets the server during the decryption phase to intercept the model weights. Intercepting this information enables attackers to conduct Mimicry Attacks, mimicking the usual behavior of a user to evade detection by the IDS (Goyal et al., 2023). Attackers may also leverage intercepted data for information gathering or for conducting poisoning attacks to degrade the quality of model training-data, posing significant risks (Yang et al., 2017).\nHomomorphic encryption (Yi et al., 2014) offers a solution to this issue by allowing computations on encrypted data without the need for decryption. This ensures that even if an attacker gains access to the server, they cannot access the model weights. To the best of our knowledge, no IDS has explored the use of Homomorphic encryption in the detection of APT attacks. Yet, one significant contribution in this field is the work by Chen et al. (2019), which introduced Multi-key Homomorphic Encryption (MKHE) for federated learning. In this scheme, each client generates a pair of private/public keys for encrypting and decrypting model weights. The server can perform computations on the encrypted weights without decrypting them, thus maintaining data confidentiality throughout the learning process. This method enhances the security of APT detection systems by safeguarding against various attack-vectors. This research has shown that integrating Homomorphic Encryption with FL can effectively protect against security threats, including MitM attacks and server-side breaches. However, the computational overhead introduced by encryption can be substantial, necessitating efficient implementation-strategies to balance security and performance."}, {"title": "5. Our approach : CONTINUUM", "content": "In this section, we propose a novel architecture for Host-based Intrusion Detection Systems utilizing a heterogeneous-graph approach. This architecture, which we called Continuum, efficiently captures complex-dependencies between system entities to detect APT attacks. Designed for scalability, Continuum can easily adapt to new hosts, integrate additional data streams, and optimize resource usage. It addresses current GNN model limitations, offering a new solution for protecting organizations against APT threats."}, {"title": "5.1. Model construction", "content": "The Continuum architecture is designed to leverage a Spatial-Temporal Graph Neural Network to detect APTS by capturing both spatial relationships and temporal dynamics between system entities. This section details each component of the model, justifying the design choices made to ensure robust APT detection."}, {"title": "5.1.1. STGNN", "content": "A key trend in APT detection is the use of autoencoders instead of traditional classifiers in PIDS architectures (Ye et al., 2023; Cheng et al., 2023b; Jia et al., 2023). In an autoencoder-based IDS, the encoder produces embeddings for graph nodes, while the decoder minimizes the similarity errors between actual and reconstructed node-representations."}, {"title": "5.1.2. Encoder", "content": "The encoder plays a key role in transforming raw graph-data into meaningful representations. It consists of multiple layers of GNNs that produce node-embeddings dense vector representations that encode each node's local context (i.e., relationships with its neighbors) (Jia et al., 2023). These embeddings are passed through the RNN layers, which enrich them with temporal information. This process ensures that the final embeddings reflect not only the current state of each entity but also how its behavior has evolved over time.\nnode\\_embeddings = RNN ([GNN(G_{t})]_{t=1}^{L})\nwhere Gt is the embedding of graph G at timestamp t, GNN is the update function of the GNN layer, and RNN is the update function of the RNN layer. The choice of this multi-layered encoder structure is justified by the need for a hierarchical understanding of system behavior. The GNN layers focus on spatial aggregation, while the RNN layers add a temporal perspective, resulting in embeddings that are highly informative and tailored for anomaly detection in APT scenarios, which are known for being persistent and prolonged in time."}, {"title": "5.1.3. Decoder", "content": "Once the embeddings are generated, the decoder attempts to reconstruct the node features and interactions, ensuring that the embeddings accurately represent the original graph-structure (Jia et al., 2023). This is particularly important in an anomaly detection context, where even slight deviations from normal behavior need to be identified. The better the embeddings represent normal system activity, the more sensitive the system will be to subtle APT-related anomalies.\nOur decoder utilizes an STGNN for this purpose, creating an objective function that boosts the graph repre-sentation module's performance. It regenerates initial node-embeddings, allowing for the calculation of feature-reconstruction loss, which helps enhance the relevance of the generated embeddings.\nThe reconstruction loss is computed using a loss function that compares the initial node vectors with the reconstructed ones, aiming to maximize the behavioral information in the abstracted node embeddings.\nL(I, R) = loss\\_function(I, R)\nwhere L is the Reconstruction loss, I are the initial node vectors, and R are the reconstructed node vectors. Additionally, incorporating RNN layers into the decoder preserves temporal information by merging the current snapshot's output with the upcoming one. This continuity captures temporal dependencies between snapshots, improving the decoder's ability to reconstruct node features and refining the model's overall performance."}, {"title": "5.1.4. Detection Mechanism", "content": "In GNN-based IDS, attacks are detected through two primary approaches: classification and anomaly detection. The classification approach assigns predefined labels to entities based on patterns learned from benign behaviors. For example, Manzoor et al. (2016) simulated machine states by modeling activities such as web browsing, watching YouTube, or playing video games. A classifier compares new entities to these known behaviors and assigns them to one of the predefined classes. If no match is found, the entity is labeled as malicious.\nIn contrast, anomaly detection identifies deviations from normal behavior without relying on predefined class labels, this is the case of some recent GNN-based APT detectors (Cheng et al., 2023b; Jia et al., 2023). This approach functions as a binary classification, flagging any significant departure from typical behavior as potentially malicious.\nBoth classification and anomaly detection are crucial for detecting and addressing threats in network environments, providing complementary perspectives on security monitoring and threat detection. In Continuum, we opt for an anomaly-detection model, which classifies nodes as either benign or anomalous, by using a K-Nearest Neighbors (K-NN) algorithm (Peterson, 2009). K-NN is chosen for its flexibility in identifying novel attack patterns. APTS"}, {"title": "Hg =  \u2211 n\u2208V Hn", "content": "are constantly evolving, and attackers frequently modify their tactics to evade detection. By comparing the learned embeddings of each node to those of known benign entities, the system can identify outliers-nodes whose behavior deviates from the norm. This allows the system to adapt to new and unforeseen threats, making it highly effective in real-world environments where APTs are constantly evolving\nDuring model exploitation in Continuum, the node embeddings generated by the STGNN encoder will be used for either node or graph classification. For graph classification, the graph encoder applies average pooling to create a graph embedding from the node embeddings produced by the final RNN layer.\nHg = \\frac{\\sum_{n"}]}