{"title": "Dynamic Vocabulary Pruning in Early-Exit LLMs", "authors": ["Jort Vincenti", "Karim Abdel Sadek", "Joan Velja", "Matteo Nulli", "Metod Jazbec"], "abstract": "Increasing the size of large language models (LLMs) has been shown to lead to better performance. However, this comes at the cost of slower and more expensive inference. Early-exiting is a promising approach for improving the efficiency of LLM inference by enabling next token prediction at intermediate layers. Yet, the large vocabulary size in modern LLMs makes the confidence estimation required for exit decisions computationally expensive, diminishing the efficiency gains. To address this, we propose dynamically pruning the vocabulary at test time for each token. Specifically, the vocabulary is pruned at one of the initial layers, and the smaller vocabulary is then used throughout the rest of the forward pass. Our experiments demonstrate that such post-hoc dynamic vocabulary pruning improves the efficiency of confidence estimation in early-exit LLMs while maintaining competitive performance.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) are increasingly being adopted due to their impressive performance and their few-shot ability to adapt to new tasks [3]. However, their growing size results in slow and costly inference. This is particularly limiting in environments with constrained resources or low-latency requirements (e.g., on-device). The push for more efficient LLM implementations is further motivated by growing concerns over their carbon footprint [10]. As a result, making LLMs more efficient at test time has recently received a lot of attention [2, 21, 20, 9]. One promising paradigm for more efficient inference is early-exiting [16]. In this case, the forward pass is accelerated by enabling the model to yield a prediction (token) at intermediate layers, rather than passing through all the layers as is traditionally done.\n\nA key component of early-exit models is the confidence score, computed at every candidate exit, which determines whether the current prediction is of sufficient quality to terminate the forward pass and return the prediction. While various confidence measures have been proposed, most are derived from the predictive distribution at the given exit (e.g., maximum softmax probability). However, this poses a problem when applying early-exiting to LLMs [4, 14, 1, 17], where obtaining the predictive distribution requires mapping the current hidden representation to the vector of logits over all possible tokens. Given the large vocabulary sizes used in modern LLMs (\u2248 30-256K) [19, 15], such confidence estimation introduces significant computational overhead. This is one of the main reasons behind the previously observed paradox, where early-exiting in LLMs resulted in less efficient inference compared to standard, non-accelerated models (both in terms of FLOPs [14] and latency [1]), thereby defeating its original purpose.\n\nIn this work, we improve the efficiency of confidence estimation in early-exit LLMs. Specifically, we propose to map the hidden representation of the model to the full vocabulary only at the first couple of initial candidate exits, and use the resulting predictive distribution to identify the top K most likely tokens. We then prune the weight matrix (which maps hidden representations to logits over"}, {"title": "Preliminaries", "content": "Let y denote the vocabulary (or token) space, with size |Y| = d_{vocab}. Further, for x_i \u2208 V, let (x_1,...,x_t) represent the input sequence, comprising both the tokens in the prompt and those generated upon time t by the model."}, {"title": "Autoregressive Decoding in LLMs", "content": "To predict the next token in the sequence, most modern language models employ the transformer architecture [18]. In a transformer model, the input sequence is passed through L layers, each consisting of a multi-head attention and a feed-forward block, yielding a sequence of hidden representations {h^l}_{l=1}^L, h \\in R^{d_{model}}. After processing through all layers, the final next token distribution is obtained via\n\np (x_{t+1}|h^L) = softmax(Wh^L) .\n\n W\u2208 R^{d_{vocab}\u00d7d_{model}} is a weight matrix, also referred to as the unembedding matrix, that projects the final hidden state h^L back to the token space Y. The newly predicted token x_{t+1} is then added to the input sequence, and the (autoregressive) generation process is repeated until termination."}, {"title": "Early-Exiting in LLMs", "content": "Observe how decoding in LLMs, as introduced above, requires passing through all L layers for every token in the generated sequence, resulting in a slow inference process. To mitigate this, early-exiting (EE) mechanisms have been proposed [4, 14], allowing the model to predict tokens at intermediate layers if sufficiently confident. Specifically, for each layer l, a confidence score c_l \u2208 [0, 1] and an exiting threshold \u03bb \u2208 [0, 1] are defined. The early prediction is returned as soon as the confidence at the current layer exceeds the threshold:\n\nx_{t+1}:=\\begin{cases}\narg \\max p (x_{t+1}|h^L) & \\text{if } c^L_t \\geq \\lambda,\n\\\\ arg \\max p (x_{t+1}|h^l) & \\text{if } c^l_t \\geq \\lambda,\n\\\\ \\vdots &\n\\\\ arg \\max p (x_{t+1}|h^l) & \\text{otherwise}.\n\\end{cases}\n(1)\n\nNote that it is common to reuse the final weight matrix W at earlier exits [14, 5], i.e., p (x_{t+1}|h^l) = softmax(Wh^l), l = 1, . . ., L, which avoids instantiating a separate unembedding matrix at each exit and prevents introducing a significant number of additional model parameters. Moreover, for simplicity, it is common to assume a fixed and shared threshold A across all exits and tokens [8]."}, {"title": "Dynamic Vocabulary Pruning", "content": "As introduced in Section 2, a confidence measure is necessary to determine whether the model's current prediction is of sufficient quality to terminate the forward pass and return an early prediction. Most commonly, the so-called softmax based measures are used, e.g. the maximum softmax probability c_f = max p(x_{t+1}|h). However, this requires computations involving the full unembedding matrix W at every exit, which is expensive due to the large d_{model} and d_{yocab} used in modern LLMs. While this may be less concerning for latency-since the execution of the next transformer block can proceed in parallel with the confidence estimation-it still reduces the overall efficiency of the forward pass. For example, in CALM [14], the authors report that their early-exit model with softmax confidence is approximately twice as expensive in"}, {"title": "Conclusion & Future Work", "content": "Our work tackles the high cost of confidence estimation in early-exit LLMs, which arises from large vocabulary sizes. By dynamically pruning the vocabulary for every generated token, we demonstrate that efficient confidence computation is achievable without compromising performance. Our proposed vocabulary pruning is completely post-hoc, making it nicely compatible with existing pretrained early-exit LLMs. We hope our findings encourage a reconsideration of the trend towards sacrificing model adaptivity (i.e., reducing the number of possible exits [1]) due to the growing computational cost of exiting decisions. In future work, it would be valuable to validate our approach on other early-exit LLMs [17] and explore more advanced pruning mechanisms (e.g., using product-of-experts ensembles across exits [7]) beyond the simple top-K strategy used here. Future work could also investigate the impact of dynamic vocabulary pruning on confidence calibration [11]."}, {"title": "Appendix", "content": "We ran our experiments on 1x Nvidia A100 80GB - SMX4 GPU. Our code is available at https:.com/MatteoNulli/Vocabulary_pruning/tree/main.\nWe report all the relevant early-exiting hyperparameters for our experiments in Table 2. Our DVP approach introduces p and K which represent the pruning exit index and the pruned vocabulary size, respectively. The top-2 diff strategy indicates that the exit confidence c_f is computed as the difference between the probabilities of the top two tokens. The decaying threshold \u03bbt means that the exit threshold decreases for later tokens in the generated response (see Eq. (5) in [14])."}]}