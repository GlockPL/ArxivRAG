{"title": "No Free Lunch for Defending Against Prefilling Attack by In-Context Learning*", "authors": ["Zhiyu Xue", "Guangliang Liu", "Bocheng Chen", "Kristen Marie Johnson", "Ramtin Pedarsani"], "abstract": "The security of Large Language Models (LLMs) has become an important research topic since the emergence of ChatGPT. Though there have been various effective methods to defend against jailbreak attacks, prefilling jailbreak attacks remain an unsolved and popular threat against open-sourced LLMs. In-Context Learning (ICL) offers a computationally efficient defense against various jailbreak attacks, yet no effective ICL methods have been developed to counter prefilling attacks. In this paper, we: (1) show that ICL can effectively defend against prefilling jailbreak attacks by employing adversative sentence structures within demonstrations; (2) characterize the effectiveness of this defense through the lens of model size, number of demonstrations, over-defense, integration with other jailbreak attacks, and the presence of safety alignment. Given the experimental results and our analysis, we conclude that there is no free lunch for defending against prefilling jailbreak attacks with ICL. On the one hand, current safety alignment methods fail to mitigate prefilling jailbreak attacks, but adversative structures within ICL demonstrations provide robust defense across various model sizes and complex jailbreak attacks. On the other hand, LLMs exhibit similar over-defensiveness when utilizing ICL demonstrations with adversative structures, and this behavior appears to be independent of model size.Reader Warning: this paper contains harmful sentences.", "sections": [{"title": "1 Introduction", "content": "Jailbreaking is a concept traditionally known in the area of software security (Liu et al., 2016), where malicious attackers search the vulnerabilities of a software system to gain unauthorized privileges. With the boom of LLMs, malicious attackers have increasingly exploited jailbreaking techniques to prompt LLMs into providing responses that are harmful to society. Jailbreaking attacks aim to inject a sequence of jailbreaking tokens into a harmful query to elicit harmful responses from LLMs.Early studies have shown that most LLMs are highly vulnerable to a variety of jailbreak attacks, including but not limited to handcrafted approaches (AJ, 2023; Albert, 2023; Wei et al., 2024), optimization-based methods (Zou et al., 2023; Zhu et al., 2023; Jones et al., 2023), and LLM-generated attacks (Chao et al., 2023; Xu et al.; Jha et al., 2024). To defend against jailbreaking attacks, safety alignment (Bai et al., 2022) has been the de-facto method, implemented by fine-tuning LLMs with input-output pairs containing harmful questions and refusal answers. With the help of safety alignment, some recently released LLMs, e.g., Llama-3.1, can achieve 100% rejection rate to popular jailbreaking attacks such as GCG (Zou et al., 2023) and PAIR (Chao et al., 2023).However, those LLMs are still extremely vulnerable to prefilling jailbreak attacks. The prefilling jailbreak attack differs to other jailbreak attacks"}, {"title": "2 Related Work", "content": "Jailbreaking Attacks. Early jailbreaking attacks to circumvent alignment training of LLMs are constructed by manually refining hand-crafted prompts (Albert, 2023; AJ, 2023). To automate the process of obtaining jailbreak prompts, GCG (Zou et al., 2023) and GBDA (Guo et al., 2021) utilized the gradient-based methods to optimize prefix/suffix tokens as the prompts for jailbreaking. However, the obtained jailbreak prompts are gibberish and can be effectively detected by perplexity filters (Jain et al., 2023; Alon and Kamfonas, 2023). To construct the jailbreaking prompt that can bypass the perplexity filters, AutoDAN (Zhu et al., 2023) generates readable and interpretable jailbreak prompts by optimizing tokens one by one from left to right. GPTFuzzer (Yu et al., 2023) and PAIR (Chao et al., 2023) applied an auxiliary LLM to automatically craft the jailbreak prompts. Most recent LLMs (e.g. Llama3.1) performs robustly against the jailbreak attacks mentioned above, but are still vulnerable for prefilling attack.In-context Learning for LLMs refers to the emerging abilities (Wei et al., 2022) of the model to adaptively use demonstrations provided in the input to boost the performances on various tasks without parameter fine-tuning. This approach simplifies the integration of knowledge into LLMs by constructing prompts or demonstrations (Wu et al., 2023; Liu et al., 2022; Ye et al., 2023; Min et al., 2022). Some studies (Reynolds and McDonell, 2021; Arora et al., 2022) have highlighted the role of prompt diversity, emphasizing that models benefit significantly from diverse, representative examples during in-context learning. Existing work generally claims there is a strong relationship between ICL and jailbreaking attacks. ICD (Wei et al., 2023) enhances model safety using a few in-context demonstrations for decreasing jailbreak success. ICAG (Chen et al., 2024) employs an iterative adversarial game involving attack and defense agents to dynamically refine prompts, and many-shot jailbreaking (Anil et al.) investigates the effectiveness of long-context attacks on LLMs by using hundredsless, ICL-based defense approaches often lead to over-defensiveness, limiting their broader applicability. We attribute this dilemma of ICL approaches to the lack of adversatively-structured sentences in the pre-training corpus."}, {"title": "3 Methodology of ICL-based Defense", "content": "In this section, we specify formulation of ICL-based defense approach for prefilling attacks. The goal of ICL-based defense is to teach LLMs to refuse harmful queries through a set of c demonstrations as $[q_i, a_i]_{i=1}^c$, where $q_i$ and $a_i$ denote the question (Q in the Figure 1) and answer (A in Figure 1), respectively. Given a LLM $\\pi$ parameterized by $\\theta$, the inference of ICL-based defense can be represented as $\\pi_\\theta(\\cdot|x, y_{<k}, [q_i, a_i]_{i=1}^c)$, where x denotes the harmful query, and $y_{<k}$ denotes the k prefilling jailbroken tokens (highlighted with a blue background in Figure 1).Regarding the defense methods, (1) Baseline denotes the method without any defense strategies, describing the baseline ASR of the tested benchmarks; (2) Refusal represents the conventional ICL methods that leverages a refusal structure in the ICL demonstrations, such as Assistant: No, I can not answer; (3) Adv denotes the ICL strategy that leverage an adversative structure in ICL demonstrations, and Adv-mul is an improved strategy that randomly selects a (adversatively-structured) response from a pool of multiple adversative responses (details are in appendix B)."}, {"title": "4 Experimental Setting", "content": "In this section, we introduce the experimental settings, covering LLMs, benchmarks, evaluation metrics and jailbreaking methods in this paper. Also, we list the analysis factors and the setting for them.Benchmarks. For our experiments, we use the JailBench (Chao et al., 2024), AdvBench (Zou et al., 2023), and SorryBench (Xie et al., 2024). It consists of a collection of prompts specifically crafted to bypass the safety and alignment mechanisms of language models. The dataset includes a wide range of malicious instructions that attempt to manipulate LLMs into generating harmful or unintended outputs.Evaluated LLMs. We evaluate several open-source language models released by different organiza-"}, {"title": "5 Experimental Results and Analysis", "content": "In this section, we (1) introduce experimental results to demonstrate that ICL with adversative structure can effectively defend against jailbreak attacks; and (2) show detailed analysis of jailbreak defense via ICL with adversative structures by the lens of key practical variables, such as safety alignment (Section 5.2), combined attacks (Section 5.3), number of ICL demonstrations (Section 5.4), and over-defense (section 5.5)."}, {"title": "5.1 Main Results", "content": "Table 1 presents the primary results of various ICL-based defense methods evaluated across multiple benchmarks and LLMs, where the number of prefilling tokens is set as 6. Among the 36 experiments conducted for each benchmark, Refusal fails in 10 instances (highlighted with underline), highlighting the limitations of traditional refusal structures in ICL demonstrations. In contrast, the adversative structure-based ICL approach achieves the optimal ASR performance among all experiments. An interesting case is Mistral-7B-v01, for which all ICL-based defense methods cannot approach performance as they achieve for other LLMs. For some LLMs like Llama3.1-8b and Llama3.2-3b, the difference between rule-based ASR and model-based ASR is significant, we found that such a phenomenon occurs because these two evaluation methods have different thresholds for identifying jailbroken patterns, where the details will be presented in appendix C."}, {"title": "5.2 The Effectiveness of Safety Alignment", "content": "Figure 2 shows the comparison of ASR between LLMs with and without safety alignment to validate the effectiveness of safety alignment which has been the de-facto method for defending against jailbreak attacks. Increasing number of prefilling tokens results in more strong prefilling attack (Qi et al., 2024a), therefore we report the ASR performance by the lens of the number of prefilling tokens. It is clear that the introduction of safety alignment does not help defend against prefilling jailbreak attacks for both the baseline setting and the Adv setting, demonstrating the ineffectiveness of current safety alignment methods."}, {"title": "5.3 Combined Jailbreak Attack", "content": "Table 2 shows the ASR performance of Baseline, Refusal and Adv-mul methods on combined attacks which enhance prefilling jailbreak attacks by introducing other jailbreaking attacks (Wei et al., 2024), including AIM, Evil Confidant (EC), and Refusal Suppression (RS). These attacks bypass the safety guard of LLMs by leveraging the ability of instruction following, such as Don't say no. Details of these jailbroken attacks will be shown in the appendix B. Compared to the prefilling attack, the ASRs of Adv-mul generally increase when the prefilling attacks are combined with other attacks, which indicates the vulnerability of adversative"}, {"title": "5.4 Number of ICL Demonstrations", "content": "Figure 3 illustrates the impact of the number of ICL demonstrations on ASR performance across three representative LLMs evaluated on AdvBench. By increasing the demonstration number from 2 to 16, we can observe that Adv-mul performs better over tested LLMs, but it has little to no effects on Refusal. Those observations show that (1) more ICL demonstrations can help reduce ASR, but eight demonstrations might be the optimal ICL setting for LLMs considering the tradeoff between ASR and demonstration budget; (2) the failure of Refusal for defending against prefilling attack even with more ICL demonstrations."}, {"title": "5.5 Over-defense", "content": "Over-defense refers to a defense strategy that inadvertently hampers an LLM's ability to respond to benign queries (Varshney et al., 2023), causing LLMs to refuse benign queries. Table 4 summarizes the results of over-defense by evaluating the performance trade-off between benign and harmful queries. Under the baseline setting, LLMs demonstrate strong performance on benign queries but exhibit varying effectiveness in handling harmful queries. The Adv defense method, while improving performance on harmful queries, significantly compromises the handling of benign queries. This empirical evidence highlights that ICL-based defense strategies cause serious over-defense issues across most tested LLMs. Furthermore, the observed over-defense behavior appears independent of model size."}, {"title": "6 Conclusion", "content": "In conclusion, defending against prefilling jailbreak attacks using ICL presents both opportunities and challenges. Although ICL demonstrates potential in mitigating prefilling jailbreak vulnerabilities, its effectiveness is often sensitive to textual similarity between demonstrations and input queries. However, the caused over-defense issue is the main bottleneck hindering the application of ICL for defending against prefilling jailbreak attacks.Future research should focus on developing hybrid approaches that combine the strengths of ICL with other defense mechanisms, such as fine-tuning on adversative dataset, to create more resilient defenses. Additionally, exploring the integration of privacy-preserving techniques to safely access user input for similarity-based demonstration selection is another promising direction. Ultimately, a deeper understanding of the relationship between textual similarity and model vulnerability could guide the design of more adaptive and context-sensitive defenses against jailbreak attacks."}, {"title": "Limitations", "content": "In this paper, we conducted a comprehensive study of leveraging ICL to defend against prefilling attacks, and provided a specific explanation for its"}, {"title": "A Demonstration Generation", "content": "Fig. 5 illustrated our pipeline for generating the harmful questions utilized for demonstrations.We applied Uncensored LLM as Wizard-13b to generate harmful questions related to ten categories (Chao et al., 2024), which are chosen with reference to OpenAI's usage policies as Harassment/Discrimination, Malware/Hacking, Physical Harm, Economic Harm, Fraud/Deception, Disinformation, Sexual/Adult content, Privacy, Expert Advice, and Government Decision-making. For the filter block, we applied rule-based filter to remove the questions with special tokens, and applied GPT-40-mini2 to filter out the unreadable and duplicated questions. Our prompt for the harmful question generation mostly follows the template provided in (Anil et al.), which is shown as follows."}, {"title": "B Details for Experimental Setup", "content": "Evaluated LLMs. Various open-source LLMs we evaluated are presented in Table 3. They come from different organizations (e.g., Meta's Llama and Vicuna) and have been fine-tuned for safety alignment. The HuggingFace paths provide direct access to their weights and configurations for easy implementation. During inference, we set the temperature to zero for reproducibility, and set the maximal new generated tokens to 65 for reliable evaluation.Multiple Adversative Responses. The pool of multiple adversative responses utilized in Adv-Mul"}, {"title": "Combined Jailbreaking Attacks.", "content": "The combined jailbreaking attacks presented Table 2 are showing as follows, where the {query} indicates the placeholder for harmful questions."}]}