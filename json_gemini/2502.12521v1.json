{"title": "Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights", "authors": ["Shubham Parashar", "Blake Olson", "Sambhav Khurana", "Eric Li", "Hongyi Ling", "James Caverlee", "Shuiwang Ji"], "abstract": "We examine the reasoning and planning capabilities of large language models (LLMs) in solving complex tasks. Recent advances in inference-time techniques demonstrate the potential to enhance LLM reasoning without additional training by exploring intermediate steps during inference. Notably, OpenAI's o1 model shows promising performance through its novel use of multi-step reasoning and verification. Here, we explore how scaling inference-time techniques can improve reasoning and planning, focusing on understanding the tradeoff between computational cost and performance. To this end, we construct a comprehensive benchmark, known as Sys2Bench, and perform extensive experiments evaluating existing inference-time techniques on eleven diverse tasks across five categories, including arithmetic reasoning, logical reasoning, common sense reasoning, algorithmic reasoning, and planning. Our findings indicate that simply scaling inference-time computation has limitations, as no single inference-time technique consistently performs well across all reasoning and planning tasks.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) [Brown et al., 2020] have demonstrated exceptional performance across a range of natural language processing (NLP) tasks, including question answering, machine translation, sentiment analysis, and text summarization [Devlin et al., 2019, Vaswani et al., 2017]. Beyond NLP, LLMs have also been adapted for multimodal tasks involving vision [Parashar et al., 2024, Lin et al., 2025] and audio [Wu et al., 2024]. Building on their success in these diverse domains, researchers are increasingly using LLMs as AI agents [Deng et al., 2024, Wang et al., 2024] for complex tasks, such as robotics [Liu et al., 2023] and scientific discovery [Wang et al., 2024]. These tasks require the reasoning and planning capabilities of LLMs, extending beyond simpler text comprehension.\nReasoning and planning in LLMs refer to their ability to solve complex problems by understanding, processing, and generating solutions across various domains [Hao et al., 2024]. These capabilities can be analyzed from multiple perspectives; we propose a classification that organizes reasoning and planning tasks into five categories, namely arithmetic, logical, commonsense, algorithmic, and plan generation challenges. Recent advances in inference-time techniques demonstrate the potential to enhance LLM reasoning and planning without additional training. These techniques focus on decomposing complex problems into simpler intermediate steps during inference. For instance, Chain-of-Thought [Wei et al., 2022] encourages step-by-step reasoning, while Tree-of-Thought [Yao et al., 2024] chooses optimal reasoning paths using tree search. Notably, OpenAI's O1 model, a large reasoning model (LRM) [Valmeekam et al., 2024], achieves state-of-the-art performance on"}, {"title": "Related Work", "content": "LLM Reasoning is the ability of LLMs to logically process information and draw coherent conclu-sions, enabling them to solve complex problems [Saparov and He, 2023]. The success of LLMs in Natural Language Generation [Radford et al., 2018] and Natural Language Understanding [Vaswani et al., 2017, Devlin et al., 2019] has sparked interest in exploring reasoning capabilities. A range of datasets have been introduced to evaluate reasoning, covering tasks in arithmetic [Ling et al., 2017, Cobbe et al., 2021], logic [Chollet, 2019, Wang et al., 2022], common sense [Yang et al., 2018, Geva et al., 2021], and algorithmic reasoning [Yao et al., 2024]. We introduce these tasks in more detail in Section 3, and report results across these tasks in Section 5.\nLLM Planning involves constructing a sequence of actions to achieve defined goals [Valmeekam et al., 2023, Zheng et al., 2024]. LLMs have been employed as planners or high-level controllers for robotic tasks Liu et al. [2023], Huang et al. [2022] and as agents for web navigation [Deng et al., 2024], scientific discovery [Wang et al., 2024], and autonomous vehicles [Yang et al., 2023]. Despite their broad adoption, studies reveal that LLMs often struggle to generate valid plans for complex tasks [Kambhampati et al., 2024, Xie et al., 2024]. We provide details on evaluated planning problems in Section 3, with results and analyses in Section 5.\nInference Time Techniques for LLMs are methods applied during output generation to improve performance, and alignment with downstream tasks [Welleck et al., 2024]. These techniques aid reasoning and planning by breaking complex tasks into smaller, manageable steps for systematic problem-solving. For instance, Chain-of-Thought prompting (CoT) [Wei et al., 2022] and its vari-ants [Zhou et al., 2023, Kojima et al., 2022] decompose problems into sequential steps, while self-consistency [Wang et al., 2023] refines CoT by aggregating multiple responses through voting. Tree of Thought [Yao et al., 2024], Graph of Thought [Besta et al., 2024], and Monte Carlo Tree Search [Hao et al., 2023, Zhou et al., 2024] enhance problem-solving by systematically exploring reasoning paths. Details on inference-time methods are in Section 4, with results in Section 5."}, {"title": "Sys2Bench Problems and Datasets", "content": "In this section, we introduce Sys2Bench, a benchmark designed to systematically evaluate the reasoning and planning capabilities of Large Language Models (LLMs) across diverse tasks. The name Sys2Bench reflects its focus on evaluating Systematic Reasoning and Planning, providing a structured framework for assessing inference-time techniques.\nA key motivation for this benchmark is to demonstrate the limitations of simply scaling inference-time computation, showing that it does not consistently lead to better reasoning or problem-solving abilities. While inference-time techniques have gained traction in improving LLM performance, no single approach consistently outperforms others across all tasks. Thus, we argue that a more holistic exploration of reasoning strategies is essential. Sys2Bench facilitates this by benchmarking LLMs on eleven datasets, categorized into five primary reasoning types: Arithmetic Reasoning, Logical Reasoning, Common Sense Reasoning, Algorithmic Reasoning, and Planning (summarized in Table 1)."}, {"title": "Arithmetic Reasoning", "content": "The ability of Large Language Models (LLMs) to solve multi-step arithmetic problems remains an active area of research Snell et al. [2024], Kumar et al. [2024b], Hendrycks et al. [2021]. Additionally, OpenAI's o1 models [OpenAI, 2024] have prompted the research community to explore inference-time techniques to improve the arithmetic reasoning of LLMs [Zhao et al., 2024a]. We evaluate the arithmetic reasoning of LLMs, on GSM8K [Cobbe et al., 2021] and AQUA [Ling et al., 2017] benchmark.\nGSM8K is a popular dataset of high-quality, linguistically diverse elementary school math word problems, designed to evaluate multi-step arithmetic reasoning. The problems typically require 2 to 8 steps of arithmetic operations, testing the ability of LLMs to perform logical deduction and basic calculations.\nAQUA is a dataset of around 100,000 algebraic word problems with multiple-choice answers and detailed rationales. It is designed to evaluate the arithmetic reasoning and problem-solving capabilities of models, making it a challenging benchmark for LLMs."}, {"title": "Logical Reasoning", "content": "Logical reasoning involves deriving conclusions based on a structured sequence of rules, or premises. The evaluation of the ability to reason logically by LLM helps assess their ability to solve structured and complex decision-making problems [Chollet, 2019]. We use ProntoQA [Saparov and He, 2023] to evaluate the logical reasoning ability of LLMs.\nProntoQA is a dataset developed to evaluate an LLM's ability to reason and generate explicit reasoning chains for first-order logic-based queries [Barwise, 1977]. It challenges models to not only produce correct answers but also provide detailed, step-by-step reasoning paths that justify their conclusions."}, {"title": "Common Sense Reasoning", "content": "Common Sense Reasoning is the process of drawing conclusions from implicit everyday knowledge. Evaluating this skill ensures that LLMs provide accurate and contextually appropriate responses. We evaluate this type of reasoning using the StrategyQA Geva et al. [2021] and HotPotQA Yang et al. [2018] datasets.\nStrategyQA is a benchmark designed to assess a model's ability to perform implicit multi-step reasoning using general knowledge or common sense facts. It consists of yes/no questions where the goal is to arrive at the correct answer by generating and verifying intermediate reasoning steps.\nHotPotQA is a large-scale dataset designed to evaluate how effectively models combine information from multiple documents to answer general knowledge questions. It features diverse question types and tests the use of sentence-level evidence for accurate and explainable multi-hop reasoning."}, {"title": "Algorithmic Reasoning", "content": "We focus on applying LLMs to solve complex NP-hard and NP-complete tasks, requiring them to evaluate constraints and propose optimized algorithms that achieve practical and effective solutions. Such problems assess the application of LLMs to combinatorial optimization and resource allocation tasks [Liu et al., 2024, Romera-Paredes et al., 2024]. We use Game of 24 [Yao et al., 2024], and a novel dataset, Bin Packing.\nGame of 24 is a dataset where the goal is to form an arithmetic expression evaluating to 24 using '+', '-', '*', or '/' with a list of four numbers. As an NP-complete problem with multiple solutions, it challenges an LLM to efficiently generate expressions by focusing only on operations that can lead to the target value.\nBin Packing is a new task introduced by us, inspired by the combinatorial optimization problems studied by Liu et al. [2024], Romera-Paredes et al. [2024]. In this task, the goal is to find the least number of bins needed to pack a list of items. Specifically, a list of N items of weight $[W_1, W_2, ...W_n]$ is given, which must be divided into bins $B_1, B_2, B_3...B_m$. The sum of weights in each bin must not exceed the bin capacity C, and the objective is to minimize the total number of bins m. Formally, the task can be written as:\n$\\min m$\nsubject to\n$$\\begin{cases}U_{j=1} B_j = \\{1, ..., n\\}, B_j \\cap B_{j'} = \\O (\\forall j \\neq j'),\\\\\\Sigma_{i \\in B_j} W_i \\leq C (\\forall j). \\end{cases}$$\n(1)"}, {"title": "Planning", "content": "A planning problem is defined by $(S_0, A, G)$, where $S_0$ stands for an initial state, $A$ is the set of actions needed to achieve the goal G. Planning problems require LLMs to demonstrate multistep reasoning, and sound decision making to arrive at correct solutions. These problems have broad applications in robotics and agent-based systems. Our evaluation focuses on four planning prob-lems: Blocks World [Valmeekam et al., 2023], Rubik's Cube [Ding et al., 2024], TripPlan, and CalendarPlan [Zheng et al., 2024].\nBlocks World is a popular dataset to evaluate the planning capabilities of LLMs. Each task involves transitioning from an initial block configuration to a target configuration, which requires LLMs to generate a sequence of actions to achieve the goal.\nRubik's Cube requires an LLM to solve a scrambled 2 \u00d7 2 cube by restoring each face to a uniform color. Starting from a scrambled cube, the LLM must generate a valid plan of cube rotations to achieve the goal.\nTrip Plan challenges an LLM to plan a travel itinerary that satisfies constraints on cities, dates, and flight connectivity, ensuring that all cities are visited as specified.\nCalendar Plan is a dataset designed to schedule a meeting by aligning the availability of a group of people. The goal is to find a feasible time slot that accommodates all the constraints of the participants."}, {"title": "Sys2Bench Baseline Methods", "content": "In Sys2Bench we evaluate popular inference-time techniques commonly used to enhance System 2 abilities of LLMs. While these techniques have typically been applied to specific tasks, we analyze their performance comprehensively in Sys2Bench. Sys2Bench allows us to uncover patterns and limitations that may not be previously evident. We summarize these methods in Fig. 1.\nChain of Thought (CoT) enables LLMs to solve complex problems by breaking them into intermedi-ate reasoning steps, improving their logical coherence and accuracy Wei et al. [2022]. CoT enhances structured problem-solving of LLMs by providing in-context examples of step-by-step reasoning during inference.\nSelf Consistency (SC) extends CoT by generating multiple reasoning paths for a problem and selecting the most consistent answer through majority voting [Wang et al., 2023]."}, {"title": "Experiments", "content": "In this section, we present the experiments conducted on various tasks in the Sys2Bench benchmark. We begin by outlining the experimental setup, detailing the models and implementation specifics of the inference-time methods. Next, the results for the different inference-time methods are shown in Table 2 and Table 3."}, {"title": "Setup", "content": "In this subsection, we provide details about the experimental setup used to evaluate the performance of various inference-time techniques in Sys2Bench. We describe the models, the implementation specifics of the inference-time methods, and the metrics used for evaluation.\nModels evaluated in Sys2Bench, consist of three LLaMa 3.1 models, two GPT-based models, and two large reasoning models (LRMs). The LLaMa 3.1 variants are 8B, 70B, and 405B, while the"}, {"title": "Results", "content": "In this subsection, we present the results of the Sys2Bench benchmark, organized by the types of reasoning outlined in Section 3. This grouping allows for a clearer comparison of performance across tasks, demonstrating the strengths and limitations of different inference-time techniques.\nArithmetic Reasoning tasks in Table 2 have strong results with CoT. Performance further improves with SC, as it reduces the impact of randomness in the CoT answers. However, this strong performance does not transfer to tree search methods. ToT significantly underperforms on this task, as its approach of prompting the LLM to explore multiple reasoning paths relies on the LLM generating and selecting correct intermediate reasoning steps. Since LLMs struggle with self-verification [Huang et al., 2024], it selects incorrect intermediate arithmetic steps, leading to wrong answers. In contrast, RAP shows modest gains on the GSM8K dataset, benefiting from the LLM's role as a world model to select better arithmetic steps. However, RAP still underperforms SC on AQUA, indicating that tree search methods are not well-suited for arithmetic reasoning tasks. Meanwhile, LRMs deliver exceptional arithmetic reasoning performance, as shown in Table 3, highlighting their strength in arithmetic.\nLogical Reasoning results in Table 2 show interesting trends. For instance, SC improves performance over CoT on LLaMa 3.1 8B and 70B. However, for LLaMa 3.1 405B and GPT-based models, SC results in performance drops, as it increases the likelihood of generating multiple incorrect reasoning chains in the ProntoQA task, where evaluation focuses on the accuracy of these chains. Majority voting does not help when the LLM outputs multiple wrong reasoning chains. Consistent with arithmetic reasoning, tree search methods such as ToT and RAP also underperform in this task, indicating their limitations in logical reasoning. Finally, as shown in Table 3, LRMs do not consistently outperform LLMs on this task, with O1 performing worse than GPT-40 on this task.\nCommon Sense Reasoning performance of CoT and SC improves with increasing LLM size. However, tree search methods show unique trends. Specifically, both RAP and ToT generate supporting facts for each question, but their effectiveness varies by task. To be specific, in StrategyQA, the binary output (yes or no) enables LLaMA models to effectively utilize the generated facts, leading to improved performance. In contrast, for HotPotQA, tree search is not effective as the LLM needs"}, {"title": "Insights", "content": "We extend our main experiments to provide additional insights and uncover important trends. As the research community shows increasing interest in inference-time techniques and improving LLM reasoning, these findings offer valuable contributions to ongoing discussions.\nInference-time compute scaling is limited by LLM bias. These techniques aim to improve LLM reasoning by guiding them to generate intermediate steps, simplifying complex tasks into smaller, manageable parts. However, this premise is flawed as LLMs do not exhaustively search for all reasoning paths and remain biased toward certain ones. As inference-time compute scales, this bias persists, limiting exploration and leading to diminished performance. As task complexity increases, this issue becomes worse, exacerbating errors in reasoning and decision-making.\nOur Sys2Bench experiments show this trend in arithmetic and logical reasoning. In these tasks, LLMs excel with CoT but struggle with tree search, failing to explore reasoning paths and select the correct one.\nTree search struggles with increasing complexity, performing significantly worse than CoT. As shown in Fig 2, its benefits diminish beyond a depth of 4 for the TripPlanning and Blocksworld tasks on LLaMa 3.1 405B. Note that, LLaMa 3.1 405B has a strong CoT performance in challenging planning tasks and ideally ToT should lead to further improvements. However, as complexity grows, generating the right intermediate steps becomes crucial, leading to worse performance of ToT compared to CoT. A potential explanation for this observation is the inherent bias of LLMs at each step of the reasoning process. These biases may propagate through successive steps, leading to cumulative errors that degrade ToT performance.\nLanguage models rely on retrieval rather than true understanding. Despite advancements in reasoning abilities with LRMs such as O1 and O1-Mini, they still appear to be pattern matching rather than genuine reasoning. This issue has been observed in prior studies for LLMs [Valmeekam et al., 2023], but we are the first to demonstrate it for LRMs, including O1 and O1-Mini."}, {"title": "Discussions", "content": "While our study highlights certain limitations, there is growing interest in scaling inference-time computation to further enhance LLM performance [Snell et al., 2024]. These methods have demon-strated strong results in arithmetic [Kumar et al., 2024a] and gameplay tasks [Schultz et al., 2024]. However, these improvements rely on verifiers or external models to guide reasoning, and without them, performance gains disappear. Moreover, tasks like Common Sense Reasoning lack verifiers, higlighting the limitation of verifier-dependent inference-time scaling.\nOn the other hand, LRMs like O1 leverage reinforcement learning (RL) with inference-time search to improve reasoning [OpenAI, 2024, Zhao et al., 2024b]. These models are trained to generate correct reasoning steps, enabling better performance (see Table 2). Despite significant gains, our experiments highlight limitations, particularly in planning and common sense reasoning tasks. With inference costs reaching 60\u00d7 that of standard LLMs, it is crucial to assess their limitations."}, {"title": "Conclusion", "content": "This paper examines the impact of scaling inference-time computation on improving the reasoning and planning abilities of LLMs. We show that scaling inference-time computation has limitations. Instead, we need to explore diverse approaches to enhance the holistic reasoning capabilities of LLMs. We explore this by introducing Sys2Bench, a new benchmark, and conduct extensive experiments evaluating inference-time techniques across eleven diverse tasks spanning five categories, namely, arithmetic reasoning, logical reasoning, common sense reasoning, algorithmic reasoning, and planning. Our findings provide important insights into the limitations of inference-time techniques. Finally, we discuss alternative perspectives from the literature, critically analyze their implications, and outline potential directions for future research."}]}