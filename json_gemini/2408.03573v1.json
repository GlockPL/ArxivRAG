{"title": "Active Testing of Large Language Model via Multi-Stage Sampling", "authors": ["Yuheng Huang", "Jiayang Song", "Qiang Hu", "Felix Juefei-Xu", "Lei Ma"], "abstract": "Performance evaluation plays a crucial role in the development life cycle of large language models (LLMs). It estimates the model's capability, elucidates behavior characteristics, and facilitates the identification of potential issues and limitations, thereby guiding further improvement. Given that LLMs' diverse task-handling abilities stem from large volumes of training data, a comprehensive evaluation also necessitates abundant, well-annotated, and representative test data to assess LLM performance across various downstream tasks. However, the demand for high-quality test data often entails substantial time, computational resources, and manual efforts, sometimes causing the evaluation to be inefficient or impractical. To address these challenges, researchers propose active testing, which estimates the overall performance by selecting a subset of test data. Nevertheless, the existing active testing methods tend to be inefficient, even inapplicable, given the unique new challenges of LLMs (e.g., diverse task types, increased model complexity, and unavailability of training data). To mitigate such limitations and expedite the development cycle of LLMs, in this work, we introduce AcTracer, an active testing framework tailored for LLMs that strategically selects a small subset of test data to achieve a nearly optimal performance estimation for LLMs. AcTracer utilizes both internal and external information from LLMs to guide the test sampling process, reducing variance through a multi-stage pool-based active selection. Our experiment results demonstrate that AcTracer achieves state-of-the-art performance compared to existing methods across various tasks, with up to 38.83% improvement over previous SOTA.", "sections": [{"title": "1 Introduction", "content": "Evaluating Large Language Models (LLMs) is crucial for assessing their performance and identifying potential limitations, which provides an understanding of LLMs' capability and facilitates guiding directions for future improvements. However, considering the intricate auto-regressive nature [1], the diverse tasking-oriented handling capabilities, and the corresponding large-volume training data, a thorough-paced evaluation is often impractical for the current LLM development life cycle considering the high costs of LLM execution on test data and time, especially when the available data for testing purposes are huge in size. Unlike Deep Neural Networks (DNNs) designed for specific tasks, LLMs serve as general foundation models capable of addressing a wide range of tasks, making the test spaces to be explored far larger than traditional DNN models. Furthermore, LLMs often operate under the free-form autoregressive generation mechanism, which makes testing of LLM even more challenging, requiring higher expenses to evaluate the generated responses in contrast to the classification or regression problems.\nAdditionally, LLMs' outputs can change significantly across different versions (e.g., from GPT3 to GPT4 [2]), making frequent re-labelling necessary. Therefore, the volume of test data required and the labelling cost per test point (data instant) present significant challenges to the progression of LLM evaluation technologies.\nWhile the complete evaluation of all possible test data becomes impractical, researchers often resort to an alternative feasible solution: sampling. In particular, by choosing only a subset of test data and performing subsequent labelling, it is possible to estimate models' performance at a lower cost. Although random sampling is a commonly used baseline across various domains [3, 4, 5], the estimation accuracy can be further improved in an active sampling manner [6, 7, 8, 9], namely active testing. Specifically, given test data D, we iteratively choose a data point d to sample and obtain its label l. At each step t, we collect and analyze an existing drawn set $T_{t-1} = \\{(d_1,l_1), ... (d_{t-1},l_{t-1})\\}$ to actively decide the next point to label, $(d_t, l_t)$. Finally, the performance on $T_n$ is used for the estimation given labelling budget n. However, with the aforementioned LLM-specific characteristics, active testing of LLMs introduces new challenges:\n(1) Complexity of Output Analysis. LLMs' outputs consist of mixed information that can potentially mislead the output-guided active testing. While model outputs, such as uncertainty, serve as key indicators in guiding testing in related fields[9, 10, 11, 12, 13, 14], accurately gauging the confidence of LLMs is still an unsolved problem since their responses usually consist of hundreds or even thousands of tokens that encompass intertwined information. According to recent studies, naive aggregation strategies are not always reliable in the context of LLMs [15, 16, 17]. Furthermore, Arora et al. [18] pointed out that LLM training involves minimizing both inherent language entropy-which arises from multiple possible vocabulary choices conveying similar meanings\u2014as well as excess cross-entropy, which reflects model capability. Thus, the uncertainty in LLM inference is entwined with these dual entropies, which may introduce biases in output-guided sampling.\n(2) Accessibility to Prior Knowledge. Given existing prior knowledge (e.g., labelled training data), it is possible to leverage supervised learning to learn the relationship between models' output and their performance, thereby directly obtaining an overall estimation for unlabeled ones. However, the situation becomes intricate in the context of LLMs. The training data of LLMs are usually inaccessible, and even with the training data, linking training loss to task performance is non-trivial. While training-based estimation is still useful for training guidance purposes, it is commonly recognized that it is important and necessary to develop completely training-free methods.\n(3) Aggregated Benchmarks. In response to the emergent abilities of LLM in diverse downstream tasks, the current model evaluations tend to shift focus from single-task assessment to aggregated multi-task benchmark. For example, LLM evaluations now probe deeper into mathematical reasoning within complex scenarios [19, 20, 21], testing model problem-solving abilities across varied domains [22, 23], or assessing trustworthiness and safety from multiple perspectives [24, 25]. Among these aggregated benchmarks, the behavior of LLMs can vary significantly across different tasks. Collecting labeled data for each task to train a robust and universal estimator or to leverage domain-specific guidance for sampling \u2013 which are quite common in single-task-oriented active testing \u2013 may be impractical for LLMs.\nGiven these challenges, in this paper, we aim to investigate the following research question:\nCan we design an active testing framework for LLMs that is purely unsupervised (as a plug-and-play tool) to enable label-efficient evaluations?\nWe propose AcTracer, a novel approach that leverages both internal (e.g., neuron activity) and external (e.g., output confidence score) information from LLMs to estimate the overall performance of the subject LLM in a pool-based multi-stage active selection manner. The overall workflow is illustrated in Fig 1.\nThe internal states of the models provide a unified representation across various tasks, serving as the foundation for our analysis. Building on this structure, we attempt to perform unbiased sampling guided by LLMs' confidence score. Extensive studies on seven datasets across different domains demonstrate the effectiveness of our method, which achieves state-of-the-art estimation accuracy for LLM evaluation. Further ablation studies are conducted to investigate the impact of each component on our Framework."}, {"title": "2 Related Work", "content": "Model Performance Estimation. Estimating AI models' performance in a label-efficient manner is crucial for applications such as model selection [26], improvement [27], and performance monitor-ing [28]. Most prior research concentrated on classification models, often addressing the estimation problem through learning-based or model-based analysis methods. For learning-based approaches, a key step is selecting and processing the appropriate features for subsequent learning. It is possible to learn the relationship between models' output [11, 12, 13, 14] or models' internal states [29, 30] w.r.t their performance. Fu et al. [8] recently explored using traditional classifiers such as Multi-Layer Perceptron (MLP) and labelled data to comprehend the relationship between LLMs' confidence and performance. Although these learning-based methods present promising capabilities, they heavily depend on prior knowledge and differ significantly in their settings and tasks in contrast to our sample-based evaluations. Model-based analysis approaches offer alternatives by examining models' behaviors, for instance, estimating performance by observing inconsistencies across an ensemble of models [31] or analyzing model reactions to input perturbations [32, 33]. Nevertheless, these methods usually focus more on anomaly detection, such as predicting out-of-distribution (OOD) cases, rather than estimating the general performance of models.\nAnother widespread evaluation approach involves leveraging LLMs themselves to assess the quality of their outputs [34, 35, 36, 37, 38]. This solution takes advantage of LLMs' ability on human language understanding and can be scaled up without human intervention. For instance, TruthfulQA [34] utilizes a fine-tuned GPT-3 to evaluate truthfulness, while LLM-EVAL [35] introduces a unified schema to label open-domain conversations. PandaLM [36] adapts a judge language model trained to rank the outputs from other models. However, recent studies indicate that LLM-centric evaluations can be biased and may not always provide reliable results, underscoring the necessity for human labelling [39]. Moreover, LLM evaluator and active testing are not mutually exclusive, as the latter can be integrated into the evaluation process based on the former to further reduce costs.\nActive Testing. Active testing involves sequentially selecting test data for labelling from a pool of unlabeled samples, aiming to estimate a model's behavior under certain metrics. The primary goal is to achieve unbiased sampling and reduce the overall variance of the estimations. Different from performance estimation, which relies heavily on prior knowledge of the model and the task, active testing is designed to be more universally applicable as a plug-and-play tool. As an early effort in this direction, CES [10] utilizes the neuron activation patterns from the last hidden layer of each test data point to actively select a representative subset based on these hidden representations. Then, a data point is selected to minimize the cross-entropy between the sampled set and the entire dataset."}, {"title": "3 Methodology", "content": "AcTracer encompasses the following three steps at a high level: (1) Extract vector representations for each data point in the test set from LLMs; (2) Conduct a distance-based partitioning for the test set based on the extracted vectors; (3) Perform adaptive active sampling empowered by the partitioned vector space. Detailed explanations of the methodology are provided in the following subsections.\nThe first two steps of our framework rely on hidden representation analysis of LLMs. Recent studies find that the internal states of LLMs contain important information capable of revealing important properties such as the LLMs' truthfulness [42, 43], knowledge [44, 45], beliefs [46], and emotions [47]. Based on these findings, our intuition and study results show that the internal neurons of LLMs often exhibit similar behavioral characteristics to human neurons, where different neurons exhibit diverse behavioral patterns in response to varying tasks [48]. The hidden representation of neurons for each test point spans a high-dimensional space that represents the geometric structure of LLMs' internal reactions to presented queries. Within this space, test points associated with similar tasks tend to aggregate into compact groups, which naturally form distinct clusters. These clusters partition the entire test set into subsets, and within each subset, we assume that LLMs have alike behavior patterns, resulting in lower performance variance. Namely, the evaluation results of test points falling in the same subset should be similar.\nBased on the geometric structure of LLMs' internal patterns, step 3 in AcTracer aims to achieve a more accurate and unbiased estimation. This is achieved through adaptive stratified sampling [49] on the clusters.\nThis strategy actively selects a data point to label in each round, thereby accelerating the convergence speed of the estimation error reduction. Beyond this inter-cluster sampling strategy, we also leverage the output confidence of LLMs as a guide for intra-cluster test selection, aiming to achieve distribution-aware, unbiased sampling within each cluster. Eventually, by combining the estimated performance across different clusters, it is expected to obtain a precise assessment of the overall performance of LLMs in terms of the complete test set."}, {"title": "3.1.1 Vector Representation Extraction", "content": "The initial step of our framework is to extract the internal hidden states of LLMs to guide further testing. After feeding the prompt to LLMs, we can collect a series of hidden states, which we consider retaining the behavior patterns of the LLM. Specifically, we draw the neural activities preceding the generation of the first token (e.g., LLMs' reactions to the prompt), which have been demonstrated to effectively represent the LLM's knowledge of the question [47, 50, 44]. Ideally, all neuron activations within the LLM should be analyzed to form the representation of each data point. Nevertheless, given the computational constraints in real-world scenarios, particularly during the continuous development and integration phases of LLMs, such comprehensive analysis is impractical. Therefore, we opt to take features from only one layer. Based on the findings from recent works, we select an intermediate"}, {"title": "3.1.2 Automated Search for Cluster Number", "content": "With the extracted internal representations of each data point, unsupervised clustering algorithms can be applied to perform a distance-based partition on the vectorized test set. In this study, we select Balanced K-means [53] as the partition algorithm, which is an adapted version of the original K-means that assigns an equal number of data points to each cluster. We choose this algorithm since (1) naive K-means can sometimes lead to extremely uneven partition sizes, which consequently lower the test estimation performance; (2) related work pointed out that Balanced K-means can achieve better performance for unsupervised domain discovery on LLMs [54]. We employ the implementation detailed in [54] for this work.\nGiven the candidate partition algorithm, the subsequent crucial step is to determine the cluster number that optimizes the partition performance.\nThis can be particularly challenging in active testing, where the available test samples and underlying intrinsic structure of data vary widely across different tasks, which demands a significantly different number of clusters for adequate partition. Moreover, testing LLMs across a broad spectrum of dimensions introduces additional complexities, as performing extensive cross-validation for the optimal cluster number is impractical. Given the critical role of the number of clusters in establishing a valid representation of the test data to guide testing, we propose a solution called CluSearch that performs an automated, model- and task-specific search for the cluster number without any ground truth.\nThe designed search is empowered by the inertia metric, namely, the objective function of naive K-means that measures the sum of distances between each data point and its corresponding cluster center (the details can be found in Appendix A.1.2). Given a fixed number of clusters, lower inertia indicates better results. However, as this metric is a convex decreasing function in terms of cluster number, simply minimizing it by maximizing the number of clusters is trivial and ineffective. Instead, the relationship between cluster number and inertia is more of a trade-off, where the elbow point of the cluster num-inertia curve is a widely used heuristic for appropriate cluster number search [55]. In our study, we employ the Kneedle algorithm [56] to automatically identify the elbow point as the proper number of clusters. To enhance the efficiency of the search process, we leverage adaptive sampling [57] to intensively sample cluster number-inertia pairs in regions of rapid function change. Our preliminary experiments show that adaptive sampling can reduce the search space exponentially, achieving efficient search with a limited budget."}, {"title": "3.1.3 Adaptive Active Sampling Based on Partition", "content": "In this section, we briefly introduce our sampling strategy given the partitions created by the clustering algorithm.\nInter-cluster wise. The main objective of the strategy is to identify representative points within each cluster and minimize the variance in the performance estimation. If the variances are given in advance, the optimal allocation would be the ideal strategy, which distributes samples based on known variances within each cluster. However, it is infeasible in our setting as we are actively selecting points to label. To address this challenge, Carpentier et al. [49] suggested an approach to progressively estimate variances. This method involves calculating the Monte Carlo Upper Confidence Bound (MC-UCB) for each cluster (treated as an 'arm' in a multi-armed bandit problem) and selecting the arm with the highest upper bound for subsequent sampling. At current search round t, the MC-UCB score of cluster k is computed as follows:\n$B_{k,t} = \\frac{w_k}{T_{k,t-1}} (\\delta_{k,t-1} + \\sqrt{\\frac{2\\beta}{T_{k,t-1}}}),$ (1)\nwhere $w_k$ is the cluster size, $T_{k,t-1}$ is the number of points sampled in the previous round, $\\delta_{k,t-1}$ is the empirical standard deviation within each cluster, and $\u03b2$ is a hyper-parameter. Under most LLM"}, {"title": "4 Experiments", "content": "A key advancement of LLMs over their predecessors is their ability to handle a diverse spectrum of tasks via free-form generation.\nTaking this unique characteristic into account, we select seven evaluation datasets in eight settings to cover a range of model capabilities, including common knowledge, mathematical reasoning, problem-solving, and code generation. The included datasets are listed below:"}, {"title": "4.1 Dataset", "content": "* Common Knowledge. We select TriviaQA ([61]) and NQ-open ([62]) as two question-answering datasets designed to evaluate the World Knowledge of LLM.\n* Mathematical Reasoning. We evaluate our method on GSM8K ([20]), a dataset that focuses on basic math problems that require multi-step reasoning.\n* Problem Solving. We use AGIEval ([22]), an aggregated benchmark aimed at assessing LLMs within the context of human-centric standardized exams to gauge their general abilities in human cognition and problem-solving.\n* Truthfulness. We choose TruthfulQA ([34]), a benchmark tailored for imitative falsehoods measurement to assess the truthfulness and informativeness of the LLMs. In our experiment, we refer to informativeness evaluation as TruthfulQA-I and truthfulness evaluation as TruhtulQA-T.\n* Code Generation. MBPP ([63]) and HumanEval ([64]) datasets are selected to test LLMs' ability to understand human intentions and perform corresponding code generation.\nWe aim to select a range of diverse and representative datasets to perform a rigorous evaluation for the testing methods under complex conditions that closely mirror real-world scenarios.\nThe datasets vary in size, from over one hundred (HumanEval) to nearly 18,000 (TriviaQA), spanning three orders of magnitude. With the evaluation of various downstream tasks, we aim to reveal the strengths and weaknesses of different methods and offer practical guidelines for the following applications."}, {"title": "4.2 Evaluation Metric", "content": "One common widely adopted approach to evaluate the effectiveness of active testing methods is to measure the errors between the estimation and the ground truth, typically using metrics like RMSE [9]. However, in our pool-based setting, where data points are actively and progressively selected, a single-point estimation error may not provide a complete picture for the effectiveness assessment. To tackle this issue, in this study, we conduct evaluations for sampling proportion (labelling budgets) p ranging from 5% to 50% of the original dataset, with increments of 1%, and use the results to construct a 2-D diagram. This diagram plots the number of sampling points (x-axis) against the relative error of the estimation (y-axis). We then calculate the Area Under the Curve (AUC) as an indicator of each method's effectiveness.\nA lower AUC value indicates a better performance. We only report AUC value in the main text due to space limit. Full results are shown in Appendix A.3."}, {"title": "4.3 Main Result", "content": "Baselines For the selection of baseline methods, we adhere to the following criteria: (1) the methods should function as plug-and-play tools for test estimation without the need for training data; (2) the methods selected should either be widely accepted in the industry, published in top-tier conferences or journals and proved to be useful for classical DNNs, or available as pre-print versions that demonstrate promising results on more recent LLMs. Aligned with these criteria, we selected five baseline methods as follows:\n* RandomSelection, which serves as the default strategy in many practical scenarios;\n* Confidence-based Stratified Sampling (CSSampling) [10], which enhances test estimation efficiency by dividing the models' confidence scores across the entire dataset into k sections and then applying stratified sampling according to the confidence distribution within each bin;\n* Cross Entropy-based Sampling (CESampling) [10] , which guides the sampling process through distribution analysis of the last layers' neurons between selected points and the entire test set;\n* Practical ACcuracy Estimation (PACESampling) [40], which utilizes the MMD-critic algorithm to select the most representative test inputs for test estimation; and"}, {"title": "5 Ablation Study", "content": "In this section, we conduct ablation studies to assess and understand the effectiveness of each component in AcTracer's framework design. In particular, we focus on three components: (1) The automatic cluster number search algorithm in charge of extracting the underlying LLMs' behavior patterns in the test space (CluSearch); (2) The adaptive strategy that utilizes stratified sampling based on the computation of Monte Carlo Upper Confidence Bound (MC-UCB); (3) The sub-sampling strategy inside each cluster to preserve an unbiased data selection with respect to LLMs's confidence distribution (SubSample). Corresponding to the three key components of AcTracer, we establish three distinct configurations for our ablation studies:\n* AcTracer without CluSearch (AcTracer-CluSearch). We employ a fixed number of clusters for the unsupervised learning algorithm across all datasets and sampling proportions.\n* AcTracer without adaptive stratified sampling (AcTracer-Inter-cluster Search) In this setting, we replace the adaptive approach with uniform allocation (i.e., assigning sampling numbers proportional to the size of each cluster) which is at least as precise as random sampling [68].\n* AcTracer without the sub-sampling strategy within each cluster (AcTracer-Intra-cluster Search). When a cluster is selected by the MC-UCB, we randomly choose a point within the cluster for sampling."}, {"title": "6 Conclusion", "content": "In this paper, we introduce a novel active testing framework called AcTracer, designed to select a subset of test data via a multi-stage sampling scheme, thereby accomplishing a comprehensive performance estimation for LLMs. Different from the existing active testing methods, AcTracer considers the distinct characteristics of LLMs and leverages both internal hidden states and external output confidence scores to collaboratively select a subset of the most representative test data for LLM performance estimation. Extensive experiments across a variety of tasks and LLMs have demonstrated the effectiveness of AcTracer, outperforming state-of-the-art baselines on most datasets. We hope that our exploratory work can inspire further research in this direction, aiming to establish comprehensive, efficient, and accurate performance evaluation techniques for LLMs."}, {"title": "Limitations and Future Directions", "content": "Although AcTracer has demonstrated promising effectiveness on most experiment datasets, it still shows drawbacks in estimating certain LLMs' properties, such as truthfulness. Our framework could potentially be enhanced by incorporating an improved internal state extraction technique tailored to the target metric, a more adaptive partition algorithm for inter-cluster search, and a more advanced uncertainty estimation method for intra-cluster search."}, {"title": "A Appendix", "content": "A.1 Algorithm Details"}, {"title": "A.1.1 Vector Representation Extraction", "content": "When selecting a target layer as a feature layer for various downstream tasks, studies on classification models typically favor selecting the last hidden layer, assuming it has the most pertinent information regarding their decisions [10, 71, 72, 73, 74]. However, recent research on LLMs indicates that the optimal layer for feature extraction varies depending on the task, with intermediate layers often demonstrating superior performance in various downstream application [42, 47, 51, 43].\nAmong related studies, Patchscopes [50] demonstrates an interesting way to inspect the usefulness of each layer through patching. Given a prompt, it patches one layer of hidden states to another round of inference conditioned on a different prompt and observes the resulting answers. Under their experiments, the intermediate layer still achieves relatively the best performance. This might be because the intermediate layers are responsible for decision-making while the later layer focuses more on token prediction. Motivated by this study, we once wondered whether it is possible to detect a feature layer that is suitable in our cases automatically. As a result, we designed an algorithm shown in Algo 2. Generally speaking, we patch a layer given the query prompt and take it into a new inference round conditioned on the empty prompt. We then measure the KL divergence on the output distributions of both cases. Intuitively, the last layer will yield the smallest KL divergence since it preserves all the information for the next-token prediction. However, the layer-KL divergence diagram also presents us with a trade-off similar to the cluster num-inertia diagram, and the elbow point might be an important turning point containing information on the models' high-level decision-making. So, we also conducted experiments based on this strategy.\nThe results for the middle, last, and automated-detected layers (Auto Layer) are shown in the table 3."}, {"title": "A.1.2 Automated Search for Cluster Number", "content": "For the cluster search in the second phase of our framework, we leverage Balanced K-means, which is an extended version of naive K-means that formulate the clustering problem as:\n$\\max_{a_1,..., a_D} \\sum_{d=1}^{D} -dist(h_{a_d}, x_d) \\text{ s.t. } \\forall k, \\sum_{d: 1a_d=k} 1 \\approx \\frac{D}{K}$ (3)\nwhere $a_d \\in \\{0, . . ., K\\}$ is the cluster assignment index for each data point, D is the number of test points, dist is the distance function, $h_{a_d}$ is the cluster centers, $Z_d$ is the hidden vector of d-th data point, 1 is the indicator function.\nFollowing [54], we use Balanced K-means for cluster center estimation and greedy inference when predicting clusters."}, {"title": "A.1.3 Adaptive Active Sampling Based on Partition", "content": "Inter-cluster"}, {"title": "A.3 Supplementary Experiment Results", "content": "We show all the estimation error curves in Table 1 in the following:"}]}