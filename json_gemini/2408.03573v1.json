{"title": "Active Testing of Large Language Model via Multi-Stage Sampling", "authors": ["Yuheng Huang", "Jiayang Song", "Qiang Hu", "Felix Juefei-Xu", "Lei Ma"], "abstract": "Performance evaluation plays a crucial role in the development life cycle of large language models (LLMs). It estimates the model's capability, elucidates behavior characteristics, and facilitates the identification of potential issues and limitations, thereby guiding further improvement. Given that LLMs' diverse task-handling abilities stem from large volumes of training data, a comprehensive evaluation also necessitates abundant, well-annotated, and representative test data to assess LLM performance across various downstream tasks. However, the demand for high-quality test data often entails substantial time, computational resources, and manual efforts, sometimes causing the evaluation to be inefficient or impractical. To address these challenges, researchers propose active testing, which estimates the overall performance by selecting a subset of test data. Nevertheless, the existing active testing methods tend to be inefficient, even inapplicable, given the unique new challenges of LLMs (e.g., diverse task types, increased model complexity, and unavailability of training data). To mitigate such limitations and expedite the development cycle of LLMs, in this work, we introduce AcTracer, an active testing framework tailored for LLMs that strategically selects a small subset of test data to achieve a nearly optimal performance estimation for LLMs. AcTracer utilizes both internal and external information from LLMs to guide the test sampling process, reducing variance through a multi-stage pool-based active selection. Our experiment results demonstrate that AcTracer achieves state-of-the-art performance compared to existing methods across various tasks, with up to 38.83% improvement over previous SOTA.", "sections": [{"title": "1 Introduction", "content": "Evaluating Large Language Models (LLMs) is crucial for assessing their performance and identifying potential limitations, which provides an understanding of LLMs' capability and facilitates guiding directions for future improvements. However, considering the intricate auto-regressive nature [1], the diverse tasking-oriented handling capabilities, and the corresponding large-volume training data, a thorough-paced evaluation is often impractical for the current LLM development life cycle considering the high costs of LLM execution on test data and time, especially when the available data for testing purposes are huge in size. Unlike Deep Neural Networks (DNNs) designed for specific tasks, LLMs serve as general foundation models capable of addressing a wide range of tasks, making the test spaces to be explored far larger than traditional DNN models. Furthermore, LLMs often operate under the free-form autoregressive generation mechanism, which makes testing of LLM even\nWe propose AcTracer, a novel approach that leverages both internal (e.g., neuron activity) and external (e.g., output confidence score) information from LLMs to estimate the overall performance of the subject LLM in a pool-based multi-stage active selection manner. The overall workflow is illustrated in Fig 1.\nThe internal states of the models provide a unified representation across various tasks, serving as the foundation for our analysis. Building on this structure, we attempt to perform unbiased sampling guided by LLMs' confidence score. Extensive studies on seven datasets across different domains demonstrate the effectiveness of our method, which achieves state-of-the-art estimation accuracy for LLM evaluation. Further ablation studies are conducted to investigate the impact of each component on our Framework."}, {"title": "2 Related Work", "content": "Model Performance Estimation. Estimating AI models' performance in a label-efficient manner is crucial for applications such as model selection [26], improvement [27], and performance monitor-ing [28]. Most prior research concentrated on classification models, often addressing the estimation problem through learning-based or model-based analysis methods. For learning-based approaches, a key step is selecting and processing the appropriate features for subsequent learning. It is possible to learn the relationship between models' output [11, 12, 13, 14] or models' internal states [29, 30] w.r.t their performance. Fu et al. [8] recently explored using traditional classifiers such as Multi-Layer Perceptron (MLP) and labelled data to comprehend the relationship between LLMs' confidence and performance. Although these learning-based methods present promising capabilities, they heavily depend on prior knowledge and differ significantly in their settings and tasks in contrast to our sample-based evaluations. Model-based analysis approaches offer alternatives by examining models' behaviors, for instance, estimating performance by observing inconsistencies across an ensemble of models [31] or analyzing model reactions to input perturbations [32, 33]. Nevertheless, these methods usually focus more on anomaly detection, such as predicting out-of-distribution (OOD) cases, rather than estimating the general performance of models.\nAnother widespread evaluation approach involves leveraging LLMs themselves to assess the quality of their outputs [34, 35, 36, 37, 38]. This solution takes advantage of LLMs' ability on human language understanding and can be scaled up without human intervention. For instance, TruthfulQA [34] utilizes a fine-tuned GPT-3 to evaluate truthfulness, while LLM-EVAL [35] introduces a unified schema to label open-domain conversations. PandaLM [36] adapts a judge language model trained to rank the outputs from other models. However, recent studies indicate that LLM-centric evaluations can be biased and may not always provide reliable results, underscoring the necessity for human labelling [39]. Moreover, LLM evaluator and active testing are not mutually exclusive, as the latter can be integrated into the evaluation process based on the former to further reduce costs.\nActive Testing. Active testing involves sequentially selecting test data for labelling from a pool of unlabeled samples, aiming to estimate a model's behavior under certain metrics. The primary goal is to achieve unbiased sampling and reduce the overall variance of the estimations. Different from performance estimation, which relies heavily on prior knowledge of the model and the task, active testing is designed to be more universally applicable as a plug-and-play tool. As an early effort in this direction, CES [10] utilizes the neuron activation patterns from the last hidden layer of each test data point to actively select a representative subset based on these hidden representations. Then, a data point is selected to minimize the cross-entropy between the sampled set and the entire dataset."}, {"title": "3 Methodology", "content": "3.1 General Framework\nAcTracer encompasses the following three steps at a high level: (1) Extract vector representations for each data point in the test set from LLMs; (2) Conduct a distance-based partitioning for the test set based on the extracted vectors; (3) Perform adaptive active sampling empowered by the partitioned vector space. Detailed explanations of the methodology are provided in the following subsections.\nThe first two steps of our framework rely on hidden representation analysis of LLMs. Recent studies find that the internal states of LLMs contain important information capable of revealing important properties such as the LLMs' truthfulness [42, 43], knowledge [44, 45], beliefs [46], and emotions [47]. Based on these findings, our intuition and study results show that the internal neurons of LLMs often exhibit similar behavioral characteristics to human neurons, where different neurons exhibit diverse behavioral patterns in response to varying tasks [48]. The hidden representation of neurons for each test point spans a high-dimensional space that represents the geometric structure of LLMs' internal reactions to presented queries. Within this space, test points associated with similar tasks tend to aggregate into compact groups, which naturally form distinct clusters. These clusters partition the entire test set into subsets, and within each subset, we assume that LLMs have alike behavior patterns, resulting in lower performance variance. Namely, the evaluation results of test points falling in the same subset should be similar.\nBased on the geometric structure of LLMs' internal patterns, step 3 in AcTracer aims to achieve a more accurate and unbiased estimation. This is achieved through adaptive stratified sampling [49] on the clusters.\nThis strategy actively selects a data point to label in each round, thereby accelerating the convergence speed of the estimation error reduction. Beyond this inter-cluster sampling strategy, we also leverage the output confidence of LLMs as a guide for intra-cluster test selection, aiming to achieve distribution-aware, unbiased sampling within each cluster. Eventually, by combining the estimated performance across different clusters, it is expected to obtain a precise assessment of the overall performance of LLMs in terms of the complete test set.\n3.1.1 Vector Representation Extraction\nThe initial step of our framework is to extract the internal hidden states of LLMs to guide further testing. After feeding the prompt to LLMs, we can collect a series of hidden states, which we consider retaining the behavior patterns of the LLM. Specifically, we draw the neural activities preceding the generation of the first token (e.g., LLMs' reactions to the prompt), which have been demonstrated to effectively represent the LLM's knowledge of the question [47, 50, 44]. Ideally, all neuron activations within the LLM should be analyzed to form the representation of each data point. Nevertheless, given the computational constraints in real-world scenarios, particularly during the continuous development and integration phases of LLMs, such comprehensive analysis is impractical. Therefore, we opt to take features from only one layer. Based on the findings from recent works, we select an intermediate"}, {"title": "3.1.2 Automated Search for Cluster Number", "content": "With the extracted internal representations of each data point, unsupervised clustering algorithms can be applied to perform a distance-based partition on the vectorized test set. In this study, we select Balanced K-means [53] as the partition algorithm, which is an adapted version of the original K-means that assigns an equal number of data points to each cluster. We choose this algorithm since (1) naive K-means can sometimes lead to extremely uneven partition sizes, which consequently lower the test estimation performance; (2) related work pointed out that Balanced K-means can achieve better performance for unsupervised domain discovery on LLMs [54]. We employ the implementation detailed in [54] for this work.\nGiven the candidate partition algorithm, the subsequent crucial step is to determine the cluster number that optimizes the partition performance.\nThis can be particularly challenging in active testing, where the available test samples and underlying intrinsic structure of data vary widely across different tasks, which demands a significantly different number of clusters for adequate partition. Moreover, testing LLMs across a broad spectrum of dimensions introduces additional complexities, as performing extensive cross-validation for the optimal cluster number is impractical. Given the critical role of the number of clusters in establishing a valid representation of the test data to guide testing, we propose a solution called CluSearch that performs an automated, model- and task-specific search for the cluster number without any ground truth.\nThe designed search is empowered by the inertia metric, namely, the objective function of naive K-means that measures the sum of distances between each data point and its corresponding cluster center (the details can be found in Appendix A.1.2). Given a fixed number of clusters, lower inertia indicates better results. However, as this metric is a convex decreasing function in terms of cluster number, simply minimizing it by maximizing the number of clusters is trivial and ineffective. Instead, the relationship between cluster number and inertia is more of a trade-off, where the elbow point of the cluster num-inertia curve is a widely used heuristic for appropriate cluster number search [55]. In our study, we employ the Kneedle algorithm [56] to automatically identify the elbow point as the proper number of clusters. To enhance the efficiency of the search process, we leverage adaptive sampling [57] to intensively sample cluster number-inertia pairs in regions of rapid function change. Our preliminary experiments show that adaptive sampling can reduce the search space exponentially, achieving efficient search with a limited budget."}, {"title": "3.1.3 Adaptive Active Sampling Based on Partition", "content": "In this section, we briefly introduce our sampling strategy given the partitions created by the clustering algorithm.\nInter-cluster wise. The main objective of the strategy is to identify representative points within each cluster and minimize the variance in the performance estimation. If the variances are given in advance, the optimal allocation would be the ideal strategy, which distributes samples based on known variances within each cluster. However, it is infeasible in our setting as we are actively selecting points to label. To address this challenge, Carpentier et al. [49] suggested an approach to progressively estimate variances. This method involves calculating the Monte Carlo Upper Confidence Bound (MC-UCB) for each cluster (treated as an 'arm' in a multi-armed bandit problem) and selecting the arm with the highest upper bound for subsequent sampling. At current search round t, the MC-UCB score of cluster k is computed as follows:\n$B_{k,t} = \\frac{w_k}{T_{k,t-1}} (\\delta_{k,t-1} + \\sqrt{\\frac{2\\beta}{T_{k,t-1}}})$, (1)\nwhere $w_k$ is the cluster size, $T_{k,t-1}$ is the number of points sampled in the previous round, $\\delta_{k,t-1}$ is the empirical standard deviation within each cluster, and $\\beta$ is a hyper-parameter. Under most LLM\nevaluation scenarios where the performance metric is bounded, the parameter $\\beta$ can be set according to number of sample n as follows, where Carpentier et al. provided formal discussions on this point:\n$\\beta = \\sqrt{log(2/n \\delta^{-2})}$, (2)\nIntra-cluster wise. Although the algorithm so far specifies the target cluster to apply sampling, it does not determine the sub-sampling strategy within each cluster. In other words, the algorithm needs to determine which specific data point in the cluster should get sampled and labelled. While random sampling remains a feasible option, more unbiased but resource-intensive sampling techniques can also be applied since the partition divides the space into smaller subsets, enabling high-complexity algorithms. Our intra-cluster sample is guided by the output confidence of the LLMs. While the internal states represent models' knowledge, the output confidence reveals more information about models' decisions. Although LLMs' confidence patterns may vary across different sub-tasks in an aggregated benchmark, our clustering analysis has already alleviated such a problem by partitioning the test space into subsets characterized by LLMs' internal states. Our goal in this stage is to maintain the confidence distribution of the sample drawn to be as close as possible to the distribution of the entire cluster, aiming for an intra-cluster level unbiased sampling.\nThis is achieved by selecting candidate sample points that greedily minimize the distance between the confidence distributions of the sampled points and the entire cluster. For measuring the distance between these distributions, the two-sample Kolmogorov-Smirnov test [58] and the Wasserstein distance [59, 60] are applied. We further discuss related details in the Appendix A.1.3. The overall algorithm is shown in Algorithm 1."}, {"title": "4 Experiments", "content": "4.1 Dataset\nA key advancement of LLMs over their predecessors is their ability to handle a diverse spectrum of tasks via free-form generation.\nTaking this unique characteristic into account, we select seven evaluation datasets in eight settings to cover a range of model capabilities, including common knowledge, mathematical reasoning, problem-solving, and code generation. The included datasets are listed below:\n\u2022 Common Knowledge. We select TriviaQA ([61]) and NQ-open ([62]) as two question-answering datasets designed to evaluate the World Knowledge of LLM.\n\u2022 Mathematical Reasoning. We evaluate our method on GSM8K ([20]), a dataset that focuses on basic math problems that require multi-step reasoning.\n\u2022 Problem Solving. We use AGIEval ([22]), an aggregated benchmark aimed at assessing LLMs within the context of human-centric standardized exams to gauge their general abilities in human cognition and problem-solving.\n\u2022 Truthfulness. We choose TruthfulQA ([34]), a benchmark tailored for imitative falsehoods measurement to assess the truthfulness and informativeness of the LLMs. In our experiment, we refer to informativeness evaluation as TruthfulQA-I and truthfulness evaluation as TruhtulQA-T.\n\u2022 Code Generation. MBPP ([63]) and HumanEval ([64]) datasets are selected to test LLMs' ability to understand human intentions and perform corresponding code generation.\nWe aim to select a range of diverse and representative datasets to perform a rigorous evaluation for the testing methods under complex conditions that closely mirror real-world scenarios.\nThe datasets vary in size, from over one hundred (HumanEval) to nearly 18,000 (TriviaQA), spanning three orders of magnitude. With the evaluation of various downstream tasks, we aim to reveal the strengths and weaknesses of different methods and offer practical guidelines for the following applications.\n4.2 Evaluation Metric\nOne common widely adopted approach to evaluate the effectiveness of active testing methods is to measure the errors between the estimation and the ground truth, typically using metrics like RMSE [9]. However, in our pool-based setting, where data points are actively and progressively selected, a single-point estimation error may not provide a complete picture for the effectiveness assessment. To tackle this issue, in this study, we conduct evaluations for sampling proportion (labelling budgets) p ranging from 5% to 50% of the original dataset, with increments of 1%, and use the results to construct a 2-D diagram. This diagram plots the number of sampling points (x-axis) against the relative error of the estimation (y-axis). We then calculate the Area Under the Curve (AUC) as an indicator of each method's effectiveness.\nA lower AUC value indicates a better performance. We only report AUC value in the main text due to space limit. Full results are shown in Appendix A.3.\n4.3 Main Result\nBaselines For the selection of baseline methods, we adhere to the following criteria: (1) the methods should function as plug-and-play tools for test estimation without the need for training data; (2) the methods selected should either be widely accepted in the industry, published in top-tier conferences or journals and proved to be useful for classical DNNs, or available as pre-print versions that demonstrate promising results on more recent LLMs. Aligned with these criteria, we selected five baseline methods as follows:\n\u2022 RandomSelection, which serves as the default strategy in many practical scenarios;\n\u2022 Confidence-based Stratified Sampling (CSSampling) [10], which enhances test estimation efficiency by dividing the models' confidence scores across the entire dataset into k sections and then applying stratified sampling according to the confidence distribution within each bin;\n\u2022 Cross Entropy-based Sampling (CESampling) [10] 2, which guides the sampling process through distribution analysis of the last layers' neurons between selected points and the entire test set;\n\u2022 Practical ACcuracy Estimation (PACESampling) [40], which utilizes the MMD-critic algorithm to select the most representative test inputs for test estimation; and\nResults. The experiment results are presented in Table 1, which demonstrates that AcTracer achieves lower estimation errors when assessing the performance of LLMs across a variety of settings with different dataset sizes. Notably, the most significant performance gain is observed with the AGIEval dataset, a benchmark with ten distinct tasks in English specifically designed to assess the problem-solving ability of LLMs. This substantial improvement (38.83%) over the baseline methods proves the usefulness of our methods in evaluating LLMs on aggregated benchmarks. Furthermore, AcTracer consistently delivers stable performance gains across other datasets of various sizes.\nThe only exception occurred in the Truthfulness evaluation, where AcTracer is only the 2nd place. We further investigate this problem and find that truthfulness and performance are fundamentally different properties. The latter evaluates the LLMs' capability to solve a problem, whereas the former assesses whether LLMs are lying. Although previous studies have shown that the internal states can, to some extent, reveal the truthfulness of the model [42], it might require specific approaches to capture such information explicitly. For example, training classifiers [42] or feeding prompts that activate the truthfulness-related neurons within LLMs [47]. Since we directly use PCA and do not perform any other techniques for truthfulness mining, the clustering process loses such information, and the resulting partitions are biased.\nNevertheless, we argue that this result retroactively supports our hypothesis about the importance of LLMs' internal state. If the extracted states do not retain any LLM behavior-related information, AcTracer would likely deliver close or beyond performance to the random selection approach, as shown in TruthfulQA-T. However, current experiment results reveal that the sampling is biased in other directions because the internal states contain other information that is irrelevant to the truthfulness property.\nOn the other hand, AcTracer still achieves 18.53% performance gain on evaluating informativeness (which is more revolved around LLMs' capability) on the same dataset (TruthfulQA). For future improvements, we believe it is feasible to leverage techniques such as representation engineering [47] to deliberately activate neurons based on specific properties of interest to be measured.\nAnother interesting finding is that RandomSelection actually achieves relatively adequate performance in two out of eight settings. While it seems surprising,\nwe consider this because when the sampling space is well-defined, random testing can behave as a strong baseline. For example, it is able to find bugs in complex distributed systems even with formal guarantee [4, 5]. It is also a moderate technique in compressed sensing [3], where the random sampling matrix is widely used in practice for signal recovery [67]."}, {"title": "5 Ablation Study", "content": "In this section, we conduct ablation studies to assess and understand the effectiveness of each component in AcTracer's framework design. In particular, we focus on three components: (1) The automatic cluster number search algorithm in charge of extracting the underlying LLMs' behavior patterns in the test space (CluSearch); (2) The adaptive strategy that utilizes stratified sampling based on the computation of Monte Carlo Upper Confidence Bound (MC-UCB); (3) The sub-sampling strategy inside each cluster to preserve an unbiased data selection with respect to LLMs's confidence distribution (SubSample). Corresponding to the three key components of AcTracer, we establish three distinct configurations for our ablation studies:\n\u2022 AcTracer without CluSearch (AcTracer-CluSearch). We employ a fixed number of clusters for the unsupervised learning algorithm across all datasets and sampling proportions.\n\u2022 AcTracer without adaptive stratified sampling (AcTracer-Inter-cluster Search) In this setting, we replace the adaptive approach with uniform allocation (i.e., assigning sampling numbers proportional to the size of each cluster) which is at least as precise as random sampling [68].\n\u2022 AcTracer without the sub-sampling strategy within each cluster (AcTracer-Intra-cluster Search). When a cluster is selected by the MC-UCB, we randomly choose a point within the cluster for sampling.\nWe conduct in total 3 \u00d7 46 \u00d7 10 \u00d7 8 = 11,040 experiments based on the aforementioned config-urations, and the corresponding results are shown in Table 2. We notice that both CluSearch and SubSample are essential for the performance of AcTracer in most settings. Omitting CluSearch leads to a significant accuracy degradation, with a drop as steep as 61.50% observed in the HumanEval dataset. On the other hand, the SubSample component substantially accelerates estimation divergence in certain scenarios. By removing it, the performance can drop by as much as 39.39% in AGIEval.\nWe speculate that SubSample is particularly vital when the output contains more information on models' uncertainty w.r.t the problem rather than the language entropy of vocabulary selection, which is likely the case with the classification-based AGIEval benchmark. We do not observe significant gains in evaluating truthfulness on TruthfulQA as expected since it is hard to extract related patterns by simply analyzing the model output. Lastly, the MC-UCB component contributes modestly, as it may slightly impair performance when the available number of samples for variance estimation is limited (e.g., MBPP and HumanEval). Despite these limitations, we believe that retaining MC-UCB can make AcTracer more robust, particularly in extreme cases. We believe it is possible to further increase the performance of our framework by integrating more advanced stratified sampling strategies suitable in our scenarios [69, 70]."}, {"title": "6 Conclusion", "content": "In this paper, we introduce a novel active testing framework called AcTracer, designed to select a subset of test data via a multi-stage sampling scheme, thereby accomplishing a comprehensive performance estimation for LLMs. Different from the existing active testing methods, AcTracer considers the distinct characteristics of LLMs and leverages both internal hidden states and external output confidence scores to collaboratively select a subset of the most representative test data for LLM performance estimation. Extensive experiments across a variety of tasks and LLMs have demonstrated the effectiveness of AcTracer, outperforming state-of-the-art baselines on most datasets. We hope that our exploratory work can inspire further research in this direction, aiming to establish comprehensive, efficient, and accurate performance evaluation techniques for LLMs.\nLimitations and Future Directions. Although AcTracer has demonstrated promising effectiveness on most experiment datasets, it still shows drawbacks in estimating certain LLMs' properties, such as truthfulness. Our framework could potentially be enhanced by incorporating an improved internal state extraction technique tailored to the target metric, a more adaptive partition algorithm for inter-cluster search, and a more advanced uncertainty estimation method for intra-cluster search."}, {"title": "A.1 Algorithm Details", "content": "A.1.1 Vector Representation Extraction\nWhen selecting a target layer as a feature layer for various downstream tasks, studies on classification models typically favor selecting the last hidden layer, assuming it has the most pertinent information regarding their decisions [10, 71, 72, 73, 74]. However, recent research on LLMs indicates that the optimal layer for feature extraction varies depending on the task, with intermediate layers often demonstrating superior performance in various downstream application [42, 47, 51, 43].\nAmong related studies, Patchscopes [50] demonstrates an interesting way to inspect the usefulness of each layer through patching. Given a prompt, it patches one layer of hidden states to another round of inference conditioned on a different prompt and observes the resulting answers. Under their experiments, the intermediate layer still achieves relatively the best performance. This might be because the intermediate layers are responsible for decision-making while the later layer focuses more on token prediction. Motivated by this study, we once wondered whether it is possible to detect a feature layer that is suitable in our cases automatically. As a result, we designed an algorithm shown in Algo 2. Generally speaking, we patch a layer given the query prompt and take it into a new inference round conditioned on the empty prompt. We then measure the KL divergence on the output distributions of both cases. Intuitively, the last layer will yield the smallest KL divergence since it preserves all the information for the next-token prediction. However, the layer-KL divergence diagram also presents us with a trade-off similar to the cluster num-inertia diagram, and the elbow point might be an important turning point containing information on the models' high-level decision-making. So, we also conducted experiments based on this strategy."}, {"title": "A.1.2 Automated Search for Cluster Number", "content": "Cluster Algorithm\nFor the cluster search in the second phase of our framework, we leverage Balanced K-means, which is an extended version of naive K-means that formulate the clustering problem as:\n$\\max_{a_1,..., a_D} \\sum_{d=1}^D -dist(\\vec{h}_{a_d}, \\vec{x}_d) s.t. \\forall k, \\sum_{1a_d=k}^D = \\frac{D}{K}$, (3)\nwhere $a_d \\in \\{0, ..., K\\}$ is the cluster assignment index for each data point, $D$ is the number of test points, $dist$ is the distance function, $\\vec{h}_{a_d}$ is the cluster centers, $\\vec{x}_d$ is the hidden vector of $d$-th data point, 1 is the indicator function.\nFollowing [54], we use Balanced K-means for cluster center estimation and greedy inference when predicting clusters.\nSearch Algorithm\nAs we discussed in the main text, finding the appropriate cluster number for analysis is important. We perform a n search to identify the cluster number, and each search includes one Balanced-K-means model fitting. This search is guided by inertia, as defined in Eq. 4:\n$inertia = \\sum_{d \\in \\{0,...,D\\}} min_{a_d \\in \\{0,...,K\\}} ||\\vec{z}_d - \\vec{h}_{a_d} ||^2$ (4)\nIn Eq. 4, $D$ is the number of clusters, $\\vec{z}_d$ is the vector representation of each data point, $\\vec{h}_{a_d}$ is the cluster center.\nOur search goal is to identify the elbow point as a cutoff point on the cluster number-inertia curve. Mathematically, given a function f, the curvature of f at point x is:\n$K_f(x) = \\frac{f''(x)}{(1+ f'(x)^2)^{1.5}}$ (5)\nwhere $f''(x)$ is the second derivative and $f'(x)$ is the first derivative. The elbow point is the point of maximum negative curvature of the curve. In this work, we utilized Kneedle algorithm [56] to find this point.\nTo improve the search efficiency of the elbow point, our study further leverages adaptive sampling, as proposed by Tinkerer et al. [57]. This method intensifies sampling frequencies in regions where the inertia function changes rapidly. This is achieved by iteratively dividing a given interval in the direction that can maximize the loss function:\n$L_{lb,ub} = \\sqrt{(ub - lb)^2 + (f(ub) \u2013 f(lb))^2}$ (6)\nwhere $lb$ and $ub$ are the lower bound and upper bound of the interval, $f(x)$ is the inertia value at the point x.\nIn summary, our cluster number search algorithm is summarized in Algo. 3."}, {"title": "A.1.3 Adaptive Active Sampling Based on Partition", "content": "Inter-cluster"}, {"title": "A.2 Experiment Settings", "content": "Dataset Description\nThe size of each dataset is shown in Table 4. We perform our experiments using evaluation framework Language Model Evaluation Harness [75] for all NLP tasks and implement code generation evaluation with a similar result format to achieve unified I/O for further sampling and analysis."}, {"title": "A.3 Supplementary Experiment Results", "content": "We show all the estimation error curves in Table 1 in the following:"}]}