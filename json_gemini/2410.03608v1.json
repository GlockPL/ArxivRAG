{"title": "TICKING ALL THE BOXES: GENERATED CHECKLISTS\nIMPROVE LLM EVALUATION AND GENERATION", "authors": ["Jonathan Cook", "Jakob Foerster", "Tim Rockt\u00e4schel", "Dennis Aumiller", "Alex Wang"], "abstract": "Given the widespread adoption and usage of Large Language Models (LLMs), it is\ncrucial to have flexible and interpretable evaluations of their instruction-following\nability. Preference judgments between model outputs have become the de facto\nevaluation standard, despite distilling complex, multi-faceted preferences into a\nsingle ranking. Furthermore, as human annotation is slow and costly, LLMs are\nincreasingly used to make these judgments, at the expense of reliability and in-\nterpretability. In this work, we propose TICK (Targeted Instruct-evaluation with\nChecklists), a fully automated, interpretable evaluation protocol that structures\nevaluations with LLM-generated, instruction-specific checklists. We first show\nthat, given an instruction, LLMs can reliably produce high-quality, tailored evalu-\nation checklists that decompose the instruction into a series of YES/NO questions.\nEach question asks whether a candidate response meets a specific requirement of\nthe instruction. We demonstrate that using TICK leads to a significant increase\n(46.4% \u2192 52.2%) in the frequency of exact agreements between LLM judge-\nments and human preferences, as compared to having an LLM directly score an\noutput. We then show that STICK (Self-TICK) can be used to improve generation\nquality across multiple benchmarks via self-refinement and Best-of-N selection.\nSTICK self-refinement on LiveBench reasoning tasks leads to an absolute gain of\n+7.8%, whilst Best-of-N selection with STICK attains +6.3% absolute improve-\nment on the real-world instruction dataset, WildBench. In light of this, structured,\nmulti-faceted self-improvement is shown to be a promising way to further advance\nLLM capabilities. Finally, by providing LLM-generated checklists to human eval-\nuators tasked with directly scoring LLM responses to WildBench instructions, we\nnotably increase inter-annotator agreement (0.194 \u2192 0.256).", "sections": [{"title": "INTRODUCTION", "content": "Instruction-tuned Large Language Models (LLMs) are widely used as conversational assistants,\nwhere users expect responses to closely follow their intents (Wei et al., 2022a; Mishra et al., 2022;\nBai et al., 2022a; Ouyang et al., 2022). The broad usage of LLMs creates a critical demand for\nreliable, flexible, and transparent ways of evaluating their instruction-following abilities. How-\never, standard evaluation methods, such as preference labeling (Ouyang et al., 2022), direct scoring\n(Novikova et al., 2018; Wang et al., 2023b), and Elo rating (Bai et al., 2022a; Glaese et al., 2022),\ntend to obscure the reasoning behind evaluations. These methods also often result in significant dis-\nagreements, both among human annotators (Hosking et al., 2024) and between models and humans\n(Qin et al., 2024; Zheng et al., 2023).\nTo address these limitations, we introduce TICK (Targeted Instruct-evaluation with Checklists),\na novel approach to LLM-as-judge evaluation that uses the judge LLM to decompose instructions"}, {"title": "RELATED WORK", "content": "Instruction-Following Evaluation: There have been many efforts to improve the evaluation and\nbenchmarking of LLMs' instruction-following ability. Some of these benchmarks aggregate instruc-\ntion sets from a diverse range of sources to measure general instruction-following ability and use a\njudge LLM to score outputs (Li et al., 2023; Chia et al., 2023; Lin et al., 2024). Others decompose\ninstructions into checklists, made up of YES/NO questions or PASS/FAIL criteria that a response\nshould meet (Zhou et al., 2023; Jiang et al., 2024; Qin et al., 2024; Wen et al., 2024). For exam-\nple, the WildBench (Lin et al., 2024) dataset pairs its instructions with checklists (generated by\ntwo LLMs, then reviewed by humans) that are included in the evaluation prompt to get a score or\npreference from a judge LLM, but not explicitly answered or used to form a metric. Approaches\nto evaluation used in these works are therefore hard to make use of outside of the benchmarks\nthemselves, relying heavily on humans for instruction and checklist curation. We instead design\na dynamic evaluation protocol that can be employed on-the-fly and therefore seamlessly integrated\ninto custom evaluation workflows, or used to steer high quality generation. Meanwhile, we avoid the\ncost of having humans in the loop and enable our evaluation quality to improve as LLMs improve.\nLanguage Critiques: LLM critiques are an intuitive way of addressing the \"black box\" nature\nof evaluations. These critiques are intended to point out the strengths and weaknesses of outputs\ngenerated by the same LLM, or a different LLM. Critiques can be used to improve the quality of\noverall evaluations performed by an LLM judge or reward model (Ankner et al., 2024; Bai et al.,\n2022b; Wang et al., 2023a; Ye et al., 2024; Sun et al., 2024), inform human evaluations (Saunders\net al., 2022; McAleese et al., 2024), or provide feedback that can be used to refine a response in-\ncontext (Scheurer et al., 2023; Tian et al., 2024; Madaan et al., 2023; Yuan et al., 2024). Meanwhile,\na number of papers provide evidence that naively prompting LLMs to self-correct or find reasoning\nerrors can lead to performance degradation (Huang et al., 2024; Tyen et al., 2024; Kamoi et al.,\n2024). By using the targeted and structured nature of checklist-based evaluations, we achieve self-\nrefinement that outperforms unstructured feedback and works on a broad range tasks."}, {"title": "TICK: TARGETED INSTRUCT-EVALUATION WITH CHECKLISTS", "content": "We present an approach to automatically and robustly evaluating instruction-tuned LLMs that is\nnot restricted to any particular dataset. To do so, we use an LLM to generate checklists of targeted\nYES/NO evaluation questions for a given instruction. We then also use an LLM to evaluate responses\nwith respect to each checklist question, exploiting the fact that the decomposed task of answering a\nsingle, targeted question is much simpler than coming up with a holistic score or preference ranking.\nIndividual checklist answers can then be aggregated to produce an overall score or preference. In\nthis section, we provide details for each of these steps and experimentally validate their effectiveness\nby analysing agreement between LLMs and a pool of trained human annotators at each stage. All\nprompts used are included in Appendix G."}, {"title": "APPROACH", "content": "GENERATING CHECKLISTS\nFor a given instruction, we seek to generate a checklist, i.e. a list of YES/NO questions that each ask\nabout a different requirement of the instruction. As in Qin et al. (2024), we enforce that each question\nshould be phrased such that an answer of YES corresponds to correctly meeting the requirement that\nthe question is targeting. To obtain these instruction-specific checklists, we prompt an LLM with\na few-shot template that specifies the instruction and the YES/NO constraint. This prompt also\nmentions that checklists should cover all criteria explicitly stated in an instruction, as well as any\nimplicit criteria that are generally important for an instruction's problem domain. Figure 1 shows an\nexample instruction and an LLM-generated checklist.\nUSING CHECKLISTS\nOnce we have generated checklists, we prompt a judge LLM with each checklist question to evaluate\nthe quality of a response. TICK uses the same LLM to generate and answer checklists, but different\nLLMs may be used for each step. We denote $a_{i,j}$ as the answer to the j-th question in the checklist\nfor the i-th instruction. The quality of a response for a single instruction i is measured using the\nchecklist Pass Rate (PR), defined as PR = $ \\sum_{j}a_{i,j}/n_i$, where $n_i$ is the length of the i-th checklist\nand $a_{i,j} \\in {0,1}$ (i.e., NO \u2192 0 and YES \u2192 1). The aggregate instruction-following quality across\nall examples in a dataset of instructions is measured using the Decomposed Requirements Following\nRatio (DRFR; Qin et al., 2024), defined as DRFR = $ \\sum_{i,j} a_{i,j}/\\sum_i n_i $, i.e., the percentage of total\nchecklist questions that were correctly answered from the model's responses."}, {"title": "VALIDATION", "content": "GENERATING CHECKLISTS\nSimilarity to human checklists: To verify that LLM-generated checklists are high quality, we\ncompare them to checklists written by trained annotators on Internal, an internal test set of 612\ninstructions. These instructions have been written by the same pool of annotators, and are intended\nto resemble complex, real-world use cases of instruction-tuned LLMs, ranging from open-ended\nquestion answering to highly structured outputs. Sample instructions are available in Appendix\nD. For each instruction in Internal, we collect three checklists written independently by different\nannotators. The annotators are given precise requirements for writing checklists as well as a set of\nhigh quality examples (see Appendix H.1). From these checklist triplets, we form a set of human-\nwritten ground-truth checklists H* by manually selecting the one that best meets the annotation\nrequirements for each instruction in the dataset. In rare instances where none of the checklists\nfully meet the specified requirements, we select the best and manually make corrections. We use\nthe remaining two checklists for each instruction to form a set H' of alternative human-written\nchecklists.\nWe generate checklists with GPT-40 (OpenAI, 2024), Command-R+ (Cohere, 2024), and Llama3.1-\n70B-Instruct (Dubey et al., 2024) (we omit the \"Instruct\" for brevity). We compare these checklists,\nas well as H', against H* in terms of BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and\nWe will be open-sourcing this dataset plus the generated checklists for use by the research community."}, {"title": "USING CHECKLISTS", "content": "Question-level agreement with humans: To verify that an LLM can reliably answer generated\nchecklist questions, we first investigate how well the generated answers agree with those of trained\nhuman annotators. We use the previously gathered set of human majority vote answers for Internal\nas ground truth and compute the accuracy of checklist answers generated by GPT-40, Command-R+\nand Llama3.1-70B."}, {"title": "IN-CONTEXT SELF-IMPROVEMENT WITH STICK (SELF-TICK)", "content": "Having shown that TICK provides a signal of model response quality on par with trained human\nannotators, we investigate how it can be used to improve responses. We first explore using STICK\n(Self-TICK) evaluations as feedback for self-refinement. Our hypothesis is that because STICK is\ntargeted and interpretable, it is more informative for response refinement than unstructured feedback\nsuch as vanilla Self-Refine (Madaan et al., 2023). Secondly, we investigate STICK's effectiveness\nat Best-of-N selection. In both cases, we are using the generating LLM as its own test-time judge."}, {"title": "SELF-REFINEMENT WITH CHECKLISTS AS FEEDBACK", "content": "Approach: Given an instruction, we first generate an initial response from an LLM. We then\nuse the same LLM to generate a checklist and evaluate its original response against this checklist.\nAs checklist evaluations contain precise information about how a response succeeded or failed,\nthey can be used as targeted, actionable feedback. Whenever the previous response did not pass\nall checklist evaluations, we prompt the LLM with a refinement prompt template (see Appendix\nG), which contains the instruction, the previous response, and the STICK evaluations, in order to\ngenerate a refined response. This process can be optionally repeated for several iterations.\nValidation: We compare self-refinement with STICK against the use of unstructured feedback\ngathered by prompting the LLM to provide a detailed critique of its previous response. We refer\nto this baseline as vanilla Self-Refine (Madaan et al., 2023). We first run four iterations of self-\nrefinement using each approach on InFoBench, and WildBench (Lin et al., 2024), which is a dataset\nof real-world user queries. InFoBench evaluates responses using DRFR with its own expert-written\nchecklists. WildBench uses WB-Score, a 1-10 LLM-as-judge rating for each response. We use\nGPT-4 as judge for each benchmark and evaluate the self-refinement of Command-R+."}, {"title": "BEST-OF-N SELECTION WITH CHECKLIST-BASED SCORES", "content": "Approach: A common approach to maximising response quality for a given LLM is to sample N\nresponses to an instruction, and then use a scoring function, such as a reward model or LLM-as-\njudge, to select the best responseBest-of-N selection thus produces higher quality responses at the\ncost of additional inference compute (Snell et al., 2024). Focusing specifically on self-selection, as\nthis assumes no access to an external reward model or superior LLM, we use STICK for Best-of-N\nselection by generating N candidate responses to an instruction from an LLM, using the same LLM\nto self-evaluate each candidate with STICK, and using the STICK score for selection.\nValidation: We compare STICK for Best-of-N selection against direct self-scoring (i.e., prompt-\ning for a single holistic score). We include using an external reward model, ArmoRM-Llama3-\n8B-v0.1 (Wang et al., 2024a), as an additional baseline, despite the fact that doing so breaks the\nassumption of only having access to the generating LLM for scoring responses. We evaluate on\nInFoBench and WildBench, again evaluating by using each benchmarks' standard evaluation metric\n(defined above). For each score function, if multiple generations have the same maximum score\naccording to that score function, we keep all of the tied responses. We then compute the true score\naccording to the task's actual evaluation protocol, and average across all selected responses in the\ncase of ties. We also compute the precision of each score function. When there are no ties under the\nBest-of-N selecting score function or the ground truth score function, computing precision reduces\nto computing accuracy. When there are ties, precision penalises selecting any additional responses\nthat are not the best, or tying for best, under the ground truth score function. However, unlike recall,\nprecision does not penalise failing to select more than one response out of any that are tying for\nbest under the ground truth score function. We use Command-R+ to generate responses, as well as\nself-evaluate responses with STICK and direct self-scoring.3"}, {"title": "ASSISTING HUMAN EVALUATORS WITH GENERATED CHECKLISTS", "content": "MOTIVATION\nIn spite of recent progress in automatic evaluation (Ankner et al., 2024; Chiang & Lee, 2023; Verga\net al., 2024; Vu et al., 2024; Wang et al., 2024a;b; Ye et al., 2024), human evaluation remains a\ncritical component of LLM quality assessment. We thus investigate whether LLM-generated eval-\nuation checklists can help with arriving at a consistent score for a given response. Using checklists\npartially decomposes the annotation task to be more cognitively feasible, and helps to ensure that\nspecific considerations relevant to each instruction are not missed.\nCASE STUDY: SCORING RESPONSES TO WILDBENCH INSTRUCTIONS\nWe conduct two rounds of human evaluation on a set of Command-R+ responses to WildBench in-\nstructions. In both rounds, annotators provide an integer score from 1-5 for each response (further\ndetails in Appendix H.3). In one round, annotators are asked to first answer checklist questions\ngenerated by GPT-40 before providing each overall score (i.e., check-then-score). Annotators are\ninstructed to use the checklists to inform their score where appropriate, but not to limit their as-\nsessment to the checklists. This holistic scoring is important for human evaluation, as we found\nthat a number of edge cases arise that can lead evaluators to answer most checklist questions with\nYES and still justifiably provide a low overall score (see Appendix H.4 for an example). Each re-\nsponse is triply annotated and we compute inter-annotator agreement using Krippendorff's alpha\n(Krippendorff, 1980)."}, {"title": "CONCLUSION", "content": "We introduce TICK, a fully automatic evaluation protocol that structures evaluations with an LLM-\ngenerated, instruction-specific checklist. We show that LLMs can produce high-quality checklists\nthat improve agreement between judge LLMs and humans. Because TICK is fully automatic, it\ncan easily and cheaply be applied in new settings, avoiding the need for humans to write or re-\nview checklists. We next demonstrate that STICK (Self-TICK) can be used for in-context self-\nimprovement by either self-refinement or by Best-of-N selection. Our experiments show that both\nstrategies of employing STICK lead to substantial improvements over baselines on multiple diverse\ninstruction-following datasets, including LiveBench, which covers challenging math, code, and rea-\nsoning prompts that baselines fail to improve on. Finally, when we provide human annotators LLM-\ngenerated checklists for evaluating LLM outputs, we find inter-annotator agreement improves con-\nsiderably. Overall, we show that LLMs are capable of accurately evaluating instruction-following\nability when using structured checklists, and demonstrate the potential of this rich fine-grained feed-\nback to further improve LLM capabilities."}, {"title": "LIMITATIONS AND FUTURE WORK", "content": "TICK and STICK are useful tools for evaluation and self-improvement, respectively. However, we\nacknowledge that checklists are only one heuristic for structuring evaluation; learned or discovered\nevaluation structures are an exciting direction for future work. Checklist evaluations do not present\nan advantage in all settings, especially given the additional inference cost of generating the checklist.\nFor example, basic knowledge retrieval is best evaluated as simply correct or incorrect. Relying on\nLLMs at all steps in the evaluation protocol may also propagate, and even exacerbate, LLM biases.\nIn this work, we do not investigate self-improvement by fine-tuning on synthetic, STICK-selected\ndata, but doing so is a natural next step. Training reward models to condition on, or jointly produce,\nchecklist evaluations is also a promising direction for future study."}, {"title": "USING TICK FOR CATEGORICAL EVALUATIONS", "content": "We additionally investigate whether TICK's checklist evaluations can be used to provide aggregate\nfeedback on specific model capabilities, by grouping instructions with similar requirements together.\nFor Internal, we thus define the following instruction categories, corresponding to distinct LLM\ncapabilities that are relevant to the instruction set: [Classification, Concision, Data Manipulation,\nDocument Generation, Exclusion: Keyword, Exclusion: Topic, Extraction, File Formatting: JSON,\nFile Formatting: TSV/CSV, Formatting: General, Inclusion: Keyword, Inclusion: Topic, Knowledge\nRetrieval, Length, Subjective QA, Tone].\nFor 100 samples from Internal, we collect annotations labelling each checklist question with appro-\npriate categories. This task is performed by three annotators per checklist, and we take the inter-\nsection of chosen labels to be ground truth set. We then prompt GPT-40 to generate labels for the\nsame set of checklist questions based on the available categories."}, {"title": "ADDITONAL STICK BEST-OF-N RESULTS", "content": "In Table 8, we include a repeat of the experiment in 4.2, but with Llama-3.1-70B in place of\nCommand-R+."}, {"title": "INTERNAL INSTRUCTION SET EXAMPLES", "content": "Example 1\nWho has Madonna dated? Give me a detailed history of all of Madonna's known romantic\nrelationships. Write it as if you're a cowboy from a western film. Keep it under 500 words\nand break it into list-format so it's easy to read\nExample 2\nCorrect the spelling/grammar mistakes in the following text, and summarize it in under 15\nwords:\nI incorectly asumed that ahe would got o the store without me having to ask. She wont go\nunles I ask, and im too embarased to axe.\nExample 3\nGenerate a list of 7 movie titles released in 2003 along with their directors and genres.\nProvide them in JSON format with the following keys: movie_id, title, director, genre.\nMake sure to provide a movie for each of these genres: Horror, Action, Drama, Comedy,\nFantasy, Documentary, and Animation. Arrange them in alphabetical order based on the\ngenre.\nExample 4\nTell me about the series Ring of Fire. Is it well written? Try to keep your reply under 300\nwords, and limit your responses to direct answers to my questions, rather than any additional\nconversational notes.\nExample 5\nFrom the following passage, identify all the activities Neil deGrasse Tyson was involved in\npast the year 2000. Answer in one sentence that contains no more than 100 characters. Then,\nin a separate sentence, explain the reasoning behind your answer.\nNeil deGrasse Tyson (born October 5, 1958) is an American astrophysicist, author, and sci-\nence communicator. Tyson studied at Harvard University, the University of Texas at Austin,\nand Columbia University. From 1991 to 1994, he was a postdoctoral research associate at\nPrinceton University. In 1994, he joined the Hayden Planetarium as a staff scientist and the\nPrinceton faculty as a visiting research scientist and lecturer. In 1996, he became director of\nthe planetarium and oversaw its $210 million reconstruction project, which was completed\nin 2000. Since 1996, he has been the director of the Hayden Planetarium at the Rose Center\nfor Earth and Space in New York City. The center is part of the American Museum of Nat-\nural History, where Tyson founded the Department of Astrophysics in 1997 and has been a\nresearch associate in the department since 2003."}, {"title": "GENERATED CHECKLIST EXAMPLES", "content": "Here we include examples of evaluation checklists generated by GPT-40 using the prompt in Ap-\npendix G.1.\nINFOBENCH\nInstruction\nThere are many creative projects we can build at home that are related to the given theme,\nso let's list some of them.\nTheme: Circuits\nInFoBench Checklist (written by human annotators)\n\u2022 Is the generated text a list?\n\u2022 Are the items in the generated list relevant to the theme in the given input?\n\u2022 Can the projects in the generated text be built at home?\n\u2022 Are the projects in the generated text creative?\nGPT-40 Generated Checklist\n\u2022 Is the response a list of projects?\n\u2022 Are the projects described creative in nature?\n\u2022 Are the projects related to the theme of circuits?\n\u2022 Are the projects feasible to build at home?"}, {"title": "WILDBENCH", "content": "Instruction\nRecreate this text as a news article in the style of Associated Press with advanced vocabulary\nand perfect diction: {text}\nWildBench Checklist (combined and manually reviewed generations from GPT-4-Turbo and\nClaude-3-Opus)\n\u2022 Does the recreated text maintain the factual accuracy of the original information provided?\n\u2022 Is the language used in the recreated text reflective of the advanced vocabulary and perfect\ndiction typical of an Associated Press style news article?\n\u2022 Are all the key details from the original text, such as the location change, the previous and\nnew use of the property, and the statements from Mohammad Taqi Haqbein, accurately\nincluded in the recreated text?\n\u2022 Does the recreated text maintain a neutral and objective tone, free from bias or subjective\ncommentary?\n\u2022 Is the structure of the recreated text coherent and does it follow a logical flow that is easy\nto follow for readers?\n\u2022 Does the rewritten article maintain the key facts and information from the original text?\n\u2022 Is the article written in the distinct style of the Associated Press, with concise paragraphs\nand a focus on the main news elements?\n\u2022 Does the article use advanced vocabulary and perfect diction, demonstrating a high level\nof language proficiency?\n\u2022 Is the article well-structured, with a clear lead paragraph summarizing the main news and\nsubsequent paragraphs providing additional details?\n\u2022 Does the article include relevant quotes from the interview with Mohammad Taqi Haqbein\nto support the main points?\n\u2022 Is the article free of grammatical errors, spelling mistakes, or awkward phrasing that would\nbe uncharacteristic of professional Associated Press writing?\nGPT-40 Generated Checklist\n\u2022 Is the response rewritten as a news article?\n\u2022 Does the response adhere to the Associated Press style in terms of clarity and readability?\n\u2022 Does the response utilize advanced vocabulary appropriately?\n\u2022 Is the diction in the response perfect and free from grammatical errors?\n\u2022 Does the response maintain the factual accuracy of the original text?\n\u2022 Does the response include key details from the original text, such as the closure and re-\nopening locations of Shafaq Book House?\n\u2022 Does the response properly attribute quotes and sources, following journalistic standards?\n\u2022 Does the response avoid using opinion or editorializing, consistent with AP style report-\ning?"}, {"title": "LIVEBENCH: CODING", "content": "Instruction\nInstructions: You are an expert Python programmer. You will be given a question (problem\nspecification) and the first lines of Python solution to this problem, and will write in Python\nthe remaining lines of the program to produce a correct Python program that matches the\nspecification and passes all tests. You will NOT return anything except for the second part\nof the program that you wrote.\nQuestion: You are given a 0-indexed integer array nums and an integer k.\nYou can perform the following operation on the array at most k times:\nChoose any index i from the array and increase or decrease nums[i] by 1.\nThe score of the final array is the frequency of the most frequent element in the array.\nReturn the maximum score you can achieve.\nThe frequency of an element is the number of occurrences of that element in the array.\nExamples: {examples}\nConstraints:\n1 <= nums.length <= 105\n1 <= nums[i] <= 109\n0 <= k <= 1014\nFormat: You will use the following starter code to write the solution to the problem and\nenclose your code within delimiters.\n\"\"{starter code}\"\nAnswer: (enclose your partial completion in backticks. Only write the missing portion of\nthe code, not the entire code. Be very careful to match the appropriate indentation. Directly\nappending your code after the partial code should produce a correct completion solution.)\nGPT-40 Generated Checklist\n\u2022 Does the response only include the missing portion of the code and nothing else?\n\u2022 Does the response correctly continue from the given starting code?\n\u2022 Does the response handle the operations correctly to modify elements at most 'k' times to\nmaximize the frequency of the most frequent element?\n\u2022 Does the response correctly implement logic to track and calculate the frequency of the\nmost frequent element in the array?\n\u2022 Does the response ensure the final implementation is syntactically correct and free from\nerrors?\n\u2022 Is the approach efficient given the constraints of the problem (\u20181 <= nums.length <=\n105', \u20181 <= nums[i] <= 109\u2018, \u20180 <= k <= 1014\u2018)?"}, {"title": "LIVEBENCH: MATH", "content": "Instruction\nDifferentiate the following function: sin (7x4 + 4) cos(9 \u2013 x). Please put your final answer\nin a\nGPT-40 Generated Checklist\n\u2022 Does the response correctly apply the product rule to differentiate the given function?\n\u2022 Does the response correctly differentiate the individual components sin (7x4 + 4) and\ncos(9 - x)?\n\u2022 Are the intermediate steps clear and logically presented?\n\u2022 Is the final answer correctly boxed using the notation?"}, {"title": "LIVEBENCH: REASONING", "content": "Instruction\nIn this question, assume each person either always tells the truth or always lies. Hiroshi is\nat the farm. The person at the school tells the truth. The person at the train station says\nthe person at the airport tells the truth. [...] The person at the barbershop says the person\nat the vineyard lies. Does the person at the observatory tell the truth? Does the person at\nthe airport tell the truth? Does the person at the farm tell the truth? Think step by step, and\nthen put your answer in **bold** as a list of three words, yes or no (for example, **yes, no,\nyes**). If you don't know, guess.\nGPT-40 Generated Checklist\n\u2022 Does the response thoroughly analyze the statements and location data provided in the\ninstruction?\n\u2022 Does the response provide step-by-step reasoning leading to the conclusions?\n\u2022 Is the final answer formatted correctly in bold as a list of three words (yes or no)?\n\u2022 Does the final answer reflect the truth status of the person at the observatory?\n\u2022 Does the final answer reflect the truth status of the person at the airport?\n\u2022 Does the final answer reflect the truth status of the person at the farm?\n\u2022 Does the response make sure to answer every question, providing an educated guess if\nuncertain?"}, {"title": "STICK SELF-REFINEMENT EXAMPLES", "content": "Here, we include examples of self-refinement using STICK with Command-R+.\nWILDBENCH\nInstruction\ngenerate smart ea for meta trader 5 that uses best strategies that grows 10 dollar account to\nten thousand and the ea must know when to trade margin generate full code\nInitial Response (provided by Command-R+)\nCreating a \"smart\" Expert Advisor (EA) that can grow a $10 account to $10,000 and make\nmargin trading decisions requires a sophisticated combination of trading strategies, risk man-\nagement, and market analysis. [...] Please note that this code provides a basic framework,\nand you may need to further customize and optimize it based on your specific trading strat-\negy and requirements. Additionally, it's important to thoroughly backtest and forward-test\nthe EA on a demo account before using it with real money.\nWB-Score on Initial Response (provided by GPT-4)\nStrengths:\n\u2022 The AI response provides a structured and detailed Expert Advisor (EA) code for\nMetaTrader 5, which includes key trading functions such as margin trading, stop\nloss, take profit, and trailing stop.\n\u2022 The code follows the basic principles of MQL5 programming, using appropriate li-\nbraries and constructs like Trade.mqh, initialization, and tick-based event handling.\n\u2022 The response includes clear explanations of each section of the code, which can\nhelp the user understand how to adapt or expand the strategy.\n\u2022 The disclaimer about customization, backtesting, and further optimization is appro-\npriate and prudent for a trading scenario where results may vary.\nWeaknesses:\n\u2022 The user specifically requested a strategy that could grow a $10 account to $10,000,\nwhich is an unrealistic goal for most trading systems. The response does not di-\nrectly address the infeasibility of such a high return on a small balance within typi-\ncal trading environments, which could set unrealistic expectations.\n\u2022 There is no clear mention of specific trading strategies or indicators (e.g., moving\naverages, RSI) that the EA should use. While the response provides a framework,\nthe user requested strategies that would assist in achieving the goal, and the re-\nsponse lacks concrete strategies.\n\u2022 The response uses a hardcoded margin multiplier and simple buy trades, but does\nnot incorporate advanced risk management techniques like position sizing, diversi-\nfication, or volatility adjustments, which are critical for long-term account growth.\n\u2022 It doesn't directly mention that achieving such high account growth would likely\ninvolve high risk, which can lead to complete account loss in volatile markets.\n\u2022 The provided code could be simplified, and there are redundant parts like the empty\ndeinit() function that could have been omitted.\nScore: 5"}, {"title": "STICK on Initial Response (provided by Command-R+)", "content": "\u2022 Is the response a complete EA code for MetaTrader 5? No\n\u2022 Does the EA employ strategies that are widely recognized as effective or the \"best\"\nin trading? No\n\u2022 Is there an explanation or evidence suggesting that the EA has the potential to grow\na $10 account to $10,000? No\n\u2022 Does the EA include functionality to manage margin effectively, knowing when to\ntrade and when to avoid high-risk situations? No\n\u2022 Is the provided code syntactically correct and free of errors? No\n\u2022 Is the code well-documented to explain the logic and operations of the EA? Yes"}, {"title": "Refined Response (provided by Command-R+)", "content": "Creating a \"smart\" Expert Advisor (EA) that can grow a $10 account to $10,000 and make\ndynamic margin trading decisions is an exciting challenge. [...] Please note that this EA\nstill serves as a starting point, and further customization and optimization are recommended\nbased on your specific trading style and requirements. Backtesting, forward testing, and\nongoing monitoring are essential steps to ensure the EA\u015b performance aligns with your ex-\npectations."}, {"title": "WB-Score on Refined Response (provided by GPT-4)", "content": "Strengths:\n\u2022 The response is quite comprehensive, covering multiple aspects of the user's re-\nquest for a MetaTrader 5 EA (Expert Advisor). It includes a full code with explana-\ntions of key trading strategies like risk-reward ratios, margin trading, and dynamic\nstop-loss adjustment based on volatility.\n\u2022"}]}