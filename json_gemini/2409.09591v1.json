{"title": "Open-World Test-Time Training: Self-Training with Contrast Learning", "authors": ["Houcheng Su", "Mengzhu Wang", "Jiao Li", "Bingli Wang", "Daixian Liu", "Zeheng Wang"], "abstract": "Traditional test-time training (TTT) methods, while addressing domain shifts, often assume a consistent class set, limiting their applicability in real-world scenarios characterized by infinite variety. Open-World Test-Time Training (OWTTT) addresses the challenge of generalizing deep learning models to unknown target domain distributions, especially in the presence of strong Out-of-Distribution (OOD) data. Existing TTT methods often struggle to maintain performance when confronted with strong OOD data. In OWTTT, the focus has predominantly been on distinguishing between overall strong and weak OOD data. However, during the early stages of TTT, initial feature extraction is hampered by interference from strong OOD and corruptions, resulting in diminished contrast and premature classification of certain classes as strong OOD. To address this, we introduce Open World Dynamic Contrastive Learning (OWDCL), an innovative approach that utilizes contrastive learning to augment positive sample pairs. This strategy not only bolsters contrast in the early stages but also significantly enhances model robustness in subsequent stages. In comparison datasets, our OWDCL model has produced the most advanced performance.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep neural networks (DNNs) have demonstrated remarkable performances across many application scenarios with well-prepared datasets [1, 13, 26]. These successes typically hinge on the assumption of independent and identically distributed (i.i.d.) data, meaning that training and testing data are drawn from the same distribution. However, in real-world settings, satisfying this requirement is impractical [28]. For instance, applying the assumption to self-driving tasks may fail due to unpredictable elements like fog, snow, rain, rare traffic incidents, or unusual obstacles like sandstorms and characters in strange costumes. In medical diagnosis, the variance in equipment noise and diverse physiological characteristics of patients may compromise the model's efficacy. In real-world scenarios, the i.i.d. assumption often collapses due to variable noise from different device sensors, weather, and climate conditions, leading to a domain shift between the training and test sets. This shift results in models performing well on training data but failing on real-world test data [14]. Addressing this discrepancy is crucial for developing robust models capable of handling real-world variability.\nIn practical scenarios, target domain data is often unavailable until inference, necessitating immediate, reliable test data predictions without extra interventions. This is vital in time-sensitive or resource-limited settings where rapid adaptation is key. Test-time training/adaptation (TTT/TTA) tackles this by rapidly reducing domain shift and boosting model performance, using unlabeled target domain data during inference [24, 37, 38]. Recent TTT advancements show promise, employing meta-learning [2] for swift task adaptation, student-teacher frameworks [34] for knowledge distillation under domain shift, and adversarial sample techniques [8] for enhanced robustness and adaptability.\nNevertheless, traditional TTT methods mostly rely on the assumption that while there is a domain shift between source and target domains, they share the same class set. However, in the real world, a limited source domain cannot possibly encompass the infinite variety of real-world scenes [3, 4, 11, 33]. To better align with real-world complexities, the focus of TTT is shifting towards addressing domain shifts within the context of Open-World scenarios. In such scenarios, TTT methods must contend with continually evolving distributions. More importantly, they need to recognize and adapt to strong OOD data, such as unprecedented events or entities, rather than merely adjusting to weaker, more predictable shifts like common corruptions (weak OOD data) [20]. For example, while self-driving cars might be trained to recognize the sight of brown bears on the road, they might not anticipate encountering a panda that has escaped from a zoo. Such unpredicted occurrences exemplify the strong OOD data that pose significant challenges in Open-World settings.\nTTT methods, relying on unlabeled target domain data to address domain shifts during testing, may struggle with varying degrees of strong OOD data. Recent OWTTT advancements tackle this by dynamically expanding prototypes based on the source domain's feature distribution, improving the distinction between weak and strong OOD data [20]. However, a key prerequisite for these methods is the model's ability to initially extract features from weak OOD data. Without this, weak OOD data, potentially indistinguishable from strong OOD under significant domain shifts, may be mistakenly treated as noise, leading to its misclassification as strong OOD during the TTT phase.\nIn this paper, we tackle the challenge of initial domain shifts during testing, where the model encounters a scarcity of positive samples, often leading to misclassification of weak OOD data as strong OOD noise. Inspired by contrastive learning, we propose that augmented samples should maintain the same feature distribution as their originals. To address early TTT stage challenges, where samples lacking contrast are indistinguishable from strong OOD, our approach employs simple data augmentation to generate positive sample pairs. We incorporate the NT-XENT contrastive learning loss function, using these pairs to aid the model's adaptation and prevent premature classification of classes as strong OOD due to initial feature extraction challenges. Subsequently, we align these pairs with the source domain class cluster centers, enhancing our method's robustness and enabling basic clustering for strong OODs. We term this methodology Open World Dynamic Contrastive Learning (OWDCL).\nThe contributions of this paper are as follows:\n\u2022 In open-world TTT, our method effectively solves the problem of inaccurate classification of weak OOD samples due to lack of contrast.\n\u2022 Our approach is the first work to introduce contrastive learning to reduce domain shifts in open-world TTT problems.\n\u2022 OWDCL exhibits superior performance compared to existing state-of-the-art models across a variety of datasets."}, {"title": "2 RELATED WORK", "content": "2.1 Unsupervised Domain Adaptation\nUnsupervised domain adaptation (UDA) [10, 23, 39] aims to adapt models trained on a source domain to unlabeled target domain data. UDA typically employs strategies like difference loss [27], adversarial training [10], and self-supervised training [22] to learn invariant properties across domains. Despite considerable progress in enhancing target domain generalizability, UDA's reliance on both source and target domains during adaptation is often impractical, e.g., due to data privacy concerns. Consequently, source-free domain adaptation [17, 25, 40, 41] has emerged, eliminating the need for source domain data and relying solely on a pre-trained model and target domain data.\n2.2 Test-Time Training\nIn scenarios requiring adaptation to arbitrary unknown target domains with low inference latency and without source domain data access, Test-Time Training/Adaptation (TTT/TTA) [24, 37, 38] has emerged as a new paradigm. TTT/TTA can be achieved not only by adjusting model weights to align features with the source domain distribution [24, 36] but also through self-training that reinforces model predictions on unlabeled data [5, 30, 35, 38]. However, TTT/TTA, limited by the absence of target domain labels, often relies on summarizing the target domain's feature distribution to approximate and align with the correct source domain distribution, enhancing model performance. This approach, while reducing uncertainty, is prone to errors, especially under strong OOD interference in open-world scenarios [20].\n2.3 Open-Set Domain Adaptation\nTo address open-world scenarios, Open-Set Domain Adaptation (OSDA) has been proposed [31]. Existing OSDA methods include strategies like transforming logits of unknown class samples into a recognizable constant [32], and defining and maximizing the distance between open-set and closed-set [31]. Additionally, Universal Adaptation Network (UAN) approaches consider scenarios where unknown classes exist in both source and target domains [42]. Further, in scenarios lacking access to source domain data, Universal source-free Domain Adaptation has been explored [17]. There is very poor research on open-world test-time training (OWTTT) [20]. There is a lack of research to solve the problem of weak OOD accuracy due to the lack of feature extraction ability in the initial model."}, {"title": "3 METHODS", "content": "3.1 Problem Formulation\nTest-time training aims to adapt the source domain pre-trained model to the target domain which may be subject to a distribution shift from the source domain. So we define the source domain data as Xs, and target domain data as Xt. we also define the source label as Ys = {1, 2, ..., m}, the strong OOD label set as Ystr = {m + 1, ..., m + n}, and the target label as Yt = Ys U Ystr.\nTo clarify, we define weak Out-of-Distribution (weak OOD) as those classes that align with the source domain yet are subjected to alterations like noise or other forms of corruption. In contrast, strong Out-of-Distribution (strong OOD) encompasses categories that are entirely new and distinct from those of the source domain.\nBefore the TTT stage, We will extract the features of the source domain Xs through the pre-training model f(.), and summarize the distribution of the source domain label features Ds = {d\u2081, ..., d'm}. At the official start of the TTT stage, We augment the sample xi by data augmentation to obtain the positive sample pair x, they have the same label yi \u2208 Yt. According to the threshold \u03c4, the label of xi is determined through Ds and the comprehensive between xi and x. If it is not in Ds, it is divided into Dstr = {dstr m+1'...., am+n dstr Since there is no label in open-world TTT, we will set a pseudo-label \u0177i \u2208 Y based on sample xi.\n3.2 Overall Test-Time Training Framework\nIn comparison with Test-Time Adaptation, Test-Time Training allows for the use of a subset of source domain data. However, due to the requirement for low latency, it does not permit access to the entire source domain dataset. Considering this constraint and the demonstrated effectiveness of cluster structures in domain adaptation tasks [32], their application is maintained in open-world TTT [20]. Feature extraction from the source domain Xs will be performed using the pre-trained model f(.). The cluster centers for each class are defined as follows:\nd_m = \\frac{1}{M} \\sum_{i=1}^{M} f(x_i), Y_i \\in Y_s\nHere, M represents the number of samples for a class in the source domain.\nIn open-world test-time training, existing research [20] shows excellent performance in most scenarios. However, in certain cases, while the discrimination of strong OOD instances improves, there is a noticeable decline in handling weak OOD instances, as illustrated in 1.\nAt the onset of TTT, some classes are ineffectively classified, with accuracy deteriorating as TTT progresses. This is common in TTT/TTA, where models, lacking target domain labels and facing corruption interference, often use entropy-like methods to minimize output confusion [35, 38]. Ineffective initial feature extraction of specific classes leads to misclassification as noise. This challenge is exacerbated in open-world TTT, compounded by corruption and strong OOD disturbances, making the unsupervised process more complex.\nCurrent research often fails to enhance feature extraction capabilities for each sample, focusing instead on differentiating between strong and weak OOD scenarios. We believe this issue originates from early model stages, where the absence of labels and class corruption hinders effective feature extraction, lacking necessary comparison and feedback.\nInspired by contrastive learning [6, 7, 12], we use simple data augmentation techniques to improve input samples. Complex augmentations, like contrast and brightness adjustments combined with corrupted data, can impede model convergence. Therefore, for xi, we employ flipping and a random rotation ranging from 0 to 30%, resulting in augmented data x. Regarding the data enhancement strategy, we opt for simple rather than novel or complex data augmentations to facilitate comparative learning with sample pairs. Our experiments demonstrate that several sets of basic data enhancements yield similar effects. Specifically, a combination of vertical flipping and rotation within 0-15/45 degrees appears to be most effective. This approach is chosen for its simplicity and effectiveness. It is important to note that we advise against using contrast adjustments and adding other forms of noise for data enhancement. This is because weak OOD samples may already exhibit such corruptions, and complex augmentations could lead to convergence difficulties during testing.\nThe following hypothesis is proposed: For the samples xi and their augmented counterparts x, the model f(\u00b7), as derived from pre-training, and its iteratively updated version during the Test-Time Training (TTT) process, f'(\u00b7), are conjectured to conform to the subsequent mathematical relation:\nf'(x_i) = f'(x'_i)\nBased on this hypothesis, we implement contrastive alignment by positive sample pairs and contrastive alignment by cluster and sample pairs, and the overall framework is depicted in Figure 2.\n3.3 Contrastive Alignment by Positive Sample Pairs\nFor each sample xi and its augmented counterpart x in the current batch, we extract features f'(xi) and f'(x) using the model f'(.). The first step involves normalizing these features with the L2 norm, calculated as:\n||v||_2 = \\sqrt{v_1^2 + v_2^2 + ... + v_n^2}\nThe result post-normalization using the L2 norm is articulated as:\nv_i = \\frac{f'(x_i)}{\\sqrt{\\sum_{i=1}^{B} f'(x_i)^2}}\nv'_i = \\frac{f'(x'_i)}{\\sqrt{\\sum_{i=1}^{B} f'(x'_i)^2}}\nWhere B is the number of samples in the current batch.\nWe then compute the similarity among pairs of positive samples within the normalized vectors as follows:\nS(v_i, v'_i)_{pos} = exp(\\frac{v_i v'_i}{\\gamma_1})\nHere, \u03b31 represents the temperature normalization factor, which scales the outcome.\nFollowing this, the similarity among pairs of negative samples is also computed, employing a distinct formula, which is delineated below:\nS(v_i, v'_j)_{neg} = exp(\\frac{v_i v'_j}{\\gamma_1})\nIn conclusion, by leveraging the identified similarities and differences in both positive and negative sample pairs, we utilize the Normalized Temperature-Scaled Cross-Entropy Loss (NT-XENT) [6] for optimization. This loss function excels at discerning relational dynamics between data points in the absence of labeled data, while avoiding comparisons between identical samples. The final loss formulation for the initial phase is expressed as:\nL_{ps} = -\\alpha_1(\\log(\\frac{S(v_i,v'_i)_{pos}}{\\sum_{k \\neq i} S(v'_i, u_k)_{neg} + S(v_i, v'_i)_{pos}})+\\log(\\frac{S(v_i,v'_i)_{pos}}{\\sum_{k \\neq j} S(v'_j, u_k)_{neg} + S(v_i, v'_i)_{pos}}))\nHere, \u03b1\u2081 is a hyperparameter that adjusts the impact magnitude of the loss.\nOptimizing the Lps loss function enables the model to defer classifying a class as strong OOD until it has effectively extracted features from that class's samples. This approach enhances the efficacy of each sample within the weak OOD class, ensuring more precise and discriminative feature extraction.\n3.4 Contrastive Alignment by Cluster and Sample Pairs\nFor each sample xi, the strong OOD score is quantified based on its degree of similarity to the nearest cluster center dk in the source domain. <, > measures the cosine similarity. This quantification is defined as follows:\no_{si} = 1 - \\max_{d_k \\in D_s} (f'(x_i), d_k)\nDrawing on insights from prior research, we establish the optimal threshold as the demarcation that distinguishes between two distinct distribution patterns. This approach is conceptualized as classifying outliers into two separate clusters, which can be delineated as follows:\nN^+ = \\sum 1(\\sigma_{si} > \\tau), N^- = \\sum 1(\\sigma_{si} \\leq \\tau)\nHere, 1 (.) is the indicator function. The optimal threshold \u03c4* is identified by optimizing:\nmin_{\\tau} \\frac{1}{N^+} \\sum_{i}^{N^+} [o_{si} - \\frac{1}{N^+} \\sum_{j}^{N^+} 1(o_{s_i} > \\tau) o_{sj}]^2 + \\frac{1}{N^-} \\sum_{i}^{N^-} [o_{si} - \\frac{1}{N^-} \\sum_{j}^{N^-} 1(o_{s_i} < \\tau) o_{sj}]^2\nTo ensure a stable estimation of the outlier distribution, the distribution is updated using an exponential moving average manner with a length of Na. Here, it ranges from 0 to 1, and the step size is set to 0.01.\nUpon confirming the effective feature extraction of class samples, resulting in f'(xi) and f'(x), we obtain the feature distribution Ds of the weak OOD in the source domain, ascertained during the pre-TTT stage.\nFor handling weak OOD samples, we employ a strategy that integrates the contrastive learning loss NT-XENT with negative log-likelihood loss. This approach aims to embed the test sample xi nearer to the cluster center of its respective class while distancing it from the cluster centers of other classes. The formulation of the negative log-likelihood loss is detailed below:\nL_{PC}^{wea} = - \\sum_{k \\in Y_{s}} 1(\\hat{y} = k) \\log(\\frac{exp(<d_k, f'(x_i)>)^{\\delta}}{\\sum_{i} exp(<d_i, f'(x_i)>)^{\\delta}})\nWhere \u03b4 is a hyperparameter, set to 0.1 in all experiments.\nTo bolster the robustness of sample classification and streamline the computation, the feature distribution for the current batch has been quantified based on pseudo-labels \u0177 = k. The corresponding formula is articulated as follows:\nd_k = \\frac{1}{2k} \\sum_{i=1}^{K} (f'(x) + f'(x'))\nIn the current batch, there are k sample pairs in class K, and their average feature distribution is d.\nInitially, positive sample pairs are normalized employing the L2 norm. The specific formula utilized for this normalization is detailed below:\n\\hat{u} = \\frac{u}{\\sqrt{\\sum_{i} (u)^2}}\n\\hat{v} = \\frac{v}{\\sqrt{\\sum_{i} (v)^2}}\nUsing normalized vectors vand, the NT-XENT loss is computed:\nL_{NT} = - \\alpha_2(\\log(\\frac{S(\\hat{u}, \\hat{v})_{pos}}{\\sum_{k \\neq i} S(\\hat{u}, \\hat{v})_{neg} + S(\\hat{u}, \\hat{v})_{pos}})+\\log(\\frac{S(\\hat{u}, \\hat{v})_{pos}}{\\sum_{k \\neq j} S(\\hat{u}, \\hat{v})_{neg} + S(\\hat{u}, \\hat{v})_{pos}}))\n\u03b12 adjusts the loss's impact magnitude. The similarity computation incorporates a temperature normalization factor \u03b32, pivotal in adjusting the scale of similarity measures within the model.\nFor categorizing samples as strong OOD, the following conditions or mathematical criteria must be met:\n\\hat{o}_{si} = 1 - \\max_{d_k \\in D_s \\cup D_{str}} (f'(x_i), d_k)\nWhen strong OOD samples fulfill a certain criterion, they are incorporated into the existing strong OOD class. If not, a new strong OOD cluster center is established. In the real-world application of machine learning models, the classes known and trained on in the source domain are finite and predetermined. However, the emergence of new classes in practical scenarios is theoretically infinite. To prevent the unbounded growth of OOD cluster centers, the distribution Dstr is managed as a queue with a fixed capacity of Nq.The value of Nq is 100. As new OOD prototypes are introduced, the oldest prototypes are phased out.\nConcurrently, the negative log-likelihood loss for these samples is computed as follows:\nL_{PC}^{str} = - \\sum_{k \\in Y_{str}} 1(\\hat{y} = k) \\log(\\frac{exp(<d_k, f'(x_i)>)^{\\delta}}{\\sum_{i} exp(<d_i, f'(x_i)>)^{\\delta}})\nSelf-training (ST) is susceptible to the issue of incorrect pseudo-labels, known as confirmation bias. This self-supervised confirmation bias can exacerbate over time, significantly impacting performance. Particularly in the presence of strong OOD samples within the target domain, the model may erroneously classify these as belonging to known categories, even with low confidence, thereby intensifying the confirmation bias. To mitigate the risk of ST failure, we adopt distribution alignment as a form of self-training regularization, drawing on insights from previous studies. This approach aims to reduce the adverse effects of confirmation bias by ensuring that the model's predictions are more aligned with the actual distribution of the data.\nThe features in the source domain are assumed to follow a Gaussian distribution N(\u03bcs, \u2211s). In the target domain, the feature distribution N(\u03bc\u03c4, \u03a3\u03c4) is estimated using a momentum parameter \u03b2, incorporating only test samples pruned via strong OOD criteria. To refine clustering in the target domain, we use the Kullback-Leibler Divergence loss LKLD:\nL_{KLD} = D_{KL}(N(\\mu_s, \\Sigma_s) || N(\\mu_\\tau, \\Sigma_\\tau))\nFor the sake of aesthetics, we have simplified the formula. As a result, the final loss function for the phase of contrastive alignment by cluster centers and sample pairs can be articulated as follows:\nL_{cs} = L_{NT} + L_{pea} + L_{str} + L_{KLD}\nAcc_s = \\frac{\\sum_{x_i,y_i \\in D_t} 1(y_i = \\hat{y}_i) \\cdot 1(y_i \\in C_s)}{\\sum_{x_i,y_i \\in D_t} 1(y_i \\in C_s)}\nThis is followed by the rejection of strong OOD, which successfully rejects the accuracy of the strong OOD sample and is recorded as ACCN:\nACCN = \\frac{\\sum_{x_i,y_i \\in D_t} 1(y_i \\in C_t \\setminus C_s) \\cdot 1(y_i \\in C_t \\setminus C_s)}{\\sum_{x_i,y_i \\in D_t} 1(y_i \\in C_t \\setminus C_s)}\nAnd finally, their tradeoff, set to AccH:\nAccH = 2 \\cdot \\frac{Acc_s \\cdot ACCN}{Acc_s + ACCN}\nwhere \u0177i refers to the predicted label and 1 (yi \u2208 Cs) is true if yi is in the set Cs."}, {"title": "4 EXPERIMENTS", "content": "4.1 Datasets and Evaluation Metric\nSeveral datasets are utilized to fully demonstrate the validity of our method. For the corruption datasets, we use the following datasets, CIFAR10-C/CIFAR100-C [14], each containing 10000 corrupt images with 10/100 classes, and ImageNet-C [14], which contains 5000 corrupt images within 1000 classes. For the style transfer dataset, we introduce the Tiny-ImageNet [18] consists of 200 classes with each class containing 500 training images and 50 validation images. For other common datasets, We also introduce MNIST [19] is a handwritten digit dataset, that contains 60,000 training images and 10,000 testing images. SVHN [29] is a digital dataset in a real street context, including 50,000 training images and 10,000 testing images.\nTo evaluate open-world test-time training, we adopt the same evaluation metric as OWTTT [20]. To set up a fair comparison with existing methods, we take all the classes in the TTT benchmark dataset as seen classes and add additional classes from additional datasets as unseen classes. In the later experiments, we set the number of known class samples and the number of unknown class samples to be the same. Then we follow the \"One Pass\" protocol [36], Firstly, the training objective cannot be changed during the source domain training procedure. Secondly, testing data in the target domain is sequentially streamed and predicted. In this problem, we evaluate whether we can judge the accuracy of the source domain class as a strong OOD. First, the accuracy of the source domain class is recorded as Accs:\nAccs = \\frac{\\sum_{x_i,y_i \\in D_t} 1(y_i = \\hat{y}_i) \\cdot 1(y_i \\in C_s)}{\\sum_{x_i,y_i \\in D_t} 1(y_i \\in C_s)}\n4.2 Comparison Methods and Settings\nGiven that open-world Test-Time Training (OWTTT) is a relatively unexplored area with limited studies, our comparison necessarily includes other Test-Time Training (TTT) models, drawing on insights from previous research. It's important to note that while TTT is a method optimized for real-time testing, it differs from test-time adaptation in that it utilizes parts of the source domain data, such as small batch samples or source domain BN layer statistics, under real-time constraints. This includes the feature distribution of the source domain, as seen in OWTTT and our OWDCL model. Therefore, including traditional TTT models in our experimental comparison is justified. Our comparison model is as follows:\nTEST: Evaluating the source domain model on testing data.\nBN [15]: Updating batch norm statistics on the testing data for test-time adaptation.\nTTT++ [24]: Aligns source and target domain distribution by minimizing the F-norm between the mean covariance.\nTENT [38]: This method fine-tunes scale and bias parameters of the batch normalization layers using an entropy minimization loss during inference.\nSHOT [21]: Implements test-time training by entropy minimization and self-training. SHOT assumes the target domain is class balanced and introduces an entropy loss to encourage uniform distribution of the prediction results.\nTTAC [36]: Employs distribution alignment at both global and class levels to facilitate test-time training.\nOWTTT [20]: Which combines self-training with prototype expansion to accommodate the strong OOD samples.\nFor all competing methods that are set by default, we equip them with the same strong OOD detector introduced in [20]. For all models, ResNet-50 [13] was selected as the backbone, SGD was selected as the optimizer, and the learning rate was set to 0.01/0.001 and batch size to 256 in CIFAR10-C/CIFAR100-C. In ImageNet-C, the learning rate is set to 0.001 and the batch size is set to 128. The other hyperparameter Setting of the model refer to the default Settings of the original paper. For the data enhancement of the positive sample of OWDCL(ours), we only perform rotation in order (0-30 degrees), flipping horizontally. Because of the noise effect of domain shift, combined with overly complex data enhancement, it will make the model difficult to fit.\nFor the CIFAR10-C/CIFAR100-C datasets, the hyperparameters are configured as follows: \u03b31 is set to 0.8, \u03b32 to 0.4, \u03b11 to 1, and \u03b12 to 2. In the ImageNet-C dataset, both \u03b31 and \u03b32 are uniformly set at 1. Regarding \u03b1\u2081, initially set at 1, we reduce it to 0.1 after the 20th batch to mitigate potential overfitting issues identified in more complex datasets, where Lps remains impactful in the initial stages. Regarding the other parameters, their settings are consistent throughout the document and were initially introduced at their first mention. These specific configurations draw upon established practices from previous research [20]."}, {"title": "4.3 Comparative experiments", "content": "We first evaluate open-world test-time training under noise corrupted target domain. We treat CIFAR10/CIFAR100 [16] and ImageNet [9] as the source domain and test-time adapt to CIFAR10-C, CIFAR100-C, and ImageNet-C as the target domain respectively.\nFor experiments on CIFAR10/100, we introduce random noise, MNIST, SVHN, Tiny-ImageNet with non-overlap classes, and CIFAR100 as strong OOD testing samples. Table 2 compares the classification error of our proposed method against recent TTT methods on the CIFAR10-C dataset. Table 3 shows the performance comparison results on the CIFAR100-C dataset. It can be seen that for different strong OOD, our models have shown extremely excellent performance, and basically, under each strong OOD, our accuracy has been improved by more than 2%. In the CIFAR10-C dataset, we added Tiny-ImageNet as a strong OOD, which improved our accuracy by nearly 5% for this complex strong OOD.\nIn CIFAR100-C, due to the complexity of data set categories and the interference of strong OOD, many models have significantly improved the recognition accuracy of strong OOD (ACCN). However, his weak OOD (ACCS) accuracy drops sharply, which is caused by stong OOD interference, and he loses the ability to recognize the source domain classes. OWDCL not only demonstrates significant performance improvements compared to traditional TTT models but also incorporates contrastive learning to enhance the model's feature extraction capabilities. This enhancement helps to prevent the misclassification of weak OOD samples as strong OOD by improving feature extraction. Compared to OWTTT, OWDCL generally achieves an accuracy improvement of about 1-4%, highlighting the effectiveness of integrating contrastive learning for more robust feature discrimination and OOD handling.\nFor ImageNet-C, we introduce random noise, MNIST, and SVHN as strong OOD samples. Very encouraging results are also obtained on the large-size complicated ImageNet-C dataset, as shown in Table 4. Our model shows a similar effect for large data sets. For random noise as strong OOD, our method is inferior to SHOT. We believe that random noise prevents us from extracting features from strong OOD, thus affecting the final performance. In experiments where"}, {"title": "4.4 Further Performance Analysis", "content": "4.4.1 Ablation Study. In our extensive ablation study conducted on the CIFAR10-C dataset, we incorporated Noise as a representative of strong OOD scenarios, alongside 15 different types of corruption present in the original dataset. Due to constraints in length, we present the final averaged results; the details of which are illustrated in Table 5. In this study, PS denotes the enhancements made in the Contrastive Alignment by Positive Sample Pairs segment, and CS signifies the advancements in the Contrastive Alignment by Cluster and Sample Pairs aspect. The baseline, denoted as OWTTT, does not incorporate any of these improvements. Our findings indicate that each improvement significantly outperforms the baseline. This achievement is particularly notable in effectively differentiating strong OOD while simultaneously accurately classifying weak OOD.\n4.4.2 Visualized Analysis. We conducted a visual analysis on the CIFAR10-C dataset, using Gaussian noise as the corruption factor and the MNIST dataset as the benchmark for strong OOD scenarios. Three models - TEST, OWTTT, and OWDCL - were assessed using data from their last five batches. This data underwent dimensionality reduction via t-SNE, followed by a subsequent visualization. In these visualizations, black indicates the strong OOD class, while ten other colors represent the ten CIFAR-10 classes, as detailed in Figure 3. Compared to TEST, OWTTT showed improved classification accuracy but with a significantly higher misclassification rate. OWDCL further excelled by enlarging the spatial separation between distinct classes, indicating superior performance. Notably, OWDCL demonstrated remarkable feature extraction capabilities for unknown strong OODs during the Test-Time Training (TTT) process, despite being initially trained on MNIST. This ability is evidenced by the emergence of distinct class clusters, even though it does not precisely classify each of the ten MNIST classes.\n4.4.3 Parameter Robustness Analysis. In the context of parameter settings for the experiment, our approach OWDCL, being an extension of OWTTT, refers to the parameter configuration of OWTTT, adhering to a consistent parameter setup throughout the paper. Owing to the numerous secondary parameters involved in our method, the specific design values were mentioned at their initial introduction, and a unified approach was adopted for all experiments. In the parameter robustness analysis, we scrutinized the primary parameters \u03b1\u2081 and \u03b12 to evaluate their robustness. The experiments were conducted under the Noise condition in the CIFAR10-C dataset, as depicted in Figure 4. From the illustration, it is evident that the model's accuracy maintains commendable performance within a certain range, thus affirming the robustness of our two parameters over a defined interval."}, {"title": "5 CONCLUSION", "content": "In conclusion, our study introduces Open World Dynamic Contrastive Learning (OWDCL), a novel approach that effectively addresses the limitations of traditional Test-Time Training (TTT) methods in open-world scenarios. By innovatively employing contrastive learning to generate positive sample pairs, OWDCL significantly enhances initial feature extraction and reduces the misclassification of weak OOD data as strong OOD. This methodology not only improves contrast in early TTT stages but also strengthens the overall robustness of the model against strong OOD data. Demonstrating superior performance across various datasets, OWDCL sets a new benchmark in the field of Open-World Test-Time Training."}]}