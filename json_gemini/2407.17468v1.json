{"title": "WILDHALLUCINATIONS: Evaluating Long-form Factuality in LLMs with Real-World Entity Queries", "authors": ["Wenting Zhao", "Tanya Goyal", "Yu Ying Chiu", "Liwei Jiang", "Benjamin Newman", "Abhilasha Ravichander", "Khyathi Chandu", "Ronan Le Bras", "Claire Cardie", "Yuntian Deng", "Yejin Choi"], "abstract": "While hallucinations of large language models (LLMs) prevail as a major challenge, existing evaluation benchmarks on factuality do not cover the diverse domains of knowledge that the real-world users of LLMs seek information about. To bridge this gap, we introduce WILDHALLUCINATIONS, a benchmark that evaluates factuality. It does so by prompting LLMs to generate information about entities mined from user-chatbot conversations in the wild. These generations are then automatically fact-checked against a systematically curated knowledge source collected from web search. Notably, half of these real-world entities do not have associated Wikipedia pages. We evaluate 118,785 generations from 15 LLMs on 7,919 entities. We find that LLMs consistently hallucinate more on entities without Wikipedia pages and exhibit varying hallucination rates across different domains. Finally, given the same base models, adding a retrieval component only slightly reduces hallucinations but does not eliminate hallucinations.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have made significant progress in generating coherent texts. Despite these advancements, ensuring the factual accuracy of the generated content remains a formidable challenge [20, 21, 13]. Hallucinations\u2014instances where LLMs generate information that is unsupported or incorrect-pose a major obstacle to these models' reliable and safe deployment, particularly in high-stake applications [28]. This issue becomes more serious as users trust the plausible-looking outputs from advanced LLMs [8]. To understand the hallucination behaviors of LLMs in real-world use cases and to assist model developers in creating more reliable systems, we introduce WILDHALLUCINATIONS, a benchmark designed to evaluate the factuality of LLMs using entities from diverse domains such as computing, culture, finance, and more, collected from real-world user-chatbot interactions. Figure 1 shows an overview of WILDHALLUCINATIONS. To construct the benchmark, we extract entities from the WildChat dataset [29], which comprises one million user-chatbot interactions in the wild. Notably, 52% of the extracted entities do not have corresponding Wikipedia pages, highlighting that users often seek information beyond the scope of Wikipedia. To evaluate the factuality of LLMs, we prompt them to generate descriptive texts about each entity. We then identify hallucinations in these generated descriptions using FACTSCORE [20], an automatic fact-checking method for free-text generations. By evaluating LLM outputs for these entities, we extend factuality evaluation to cover"}, {"title": "2 The WILDHALLUCINATIONS Benchmark", "content": "We design our benchmark for evaluating hallucinations with two primary goals: (1) ensuring the evaluation process is both automatic and reliable, and (2) covering diverse types of information that real-world users seek. To meet both goals, we focus on testing LLMs' knowledge of entities. Compared to fact-checking answers to open-ended questions, verifying responses about entities is more objective and allows for the creation of a comprehensive knowledge source about these entities. We posit that mastering the entities included in user queries is a prerequisite for delivering factual responses. Furthermore, automatic evaluation for fact-checking LLMs' responses about entities is also available, as described in FACTSCORE [20]."}, {"title": "2.1 Dataset Creation", "content": "The overall pipeline is outlined in Figure 1. The entities in our benchmark are extracted from WildChat [29], a dataset of real-world user-chatbot conversations. For each extracted entity, we build a knowledge source consisting of multiple web documents. We expand on both steps below:\nEntity Extraction. As an initial filtering step, we use GPT-3.5 to identify proper nouns from the non-toxic and English user turns in WildChat. However, GPT-3.5 occasionally tags common nouns as proper nouns; we improve accuracy by additionally using GPT-40 to verify each identified proper noun. Finally, we prompt GPT-40 to identify and remove proper nouns with multiple meanings.\nBuilding Knowledge Source. Next, we build a knowledge source for each extracted entity. Prior works like the FACTSCORE benchmark [20] exclusively rely on Wikipedia for this step. However, as shown in our work, only a fraction of real world entities have corresponding Wikipedia pages. For WILDHALLUCINATIONS, we use commercial search engines to extract relevant web documents and build our knowledge source.\nWe use the Google Custom Search JSON API to collect the top 10 web page results using each entity as the search keyword. We scrape these web pages, exclude invalid and paywalled URLs and clean up the HTML and CSS tags in the remaining corpus. Appendix C outlines these steps in detail. After these cleaning steps, some entities are left with no web pages indicating insufficient information is available for these on the web. We remove such entities from our dataset."}, {"title": "2.2 Dataset Statistics", "content": "Our data collection results in 7,917 entities. On average, each entity is associated with 6.08\u00b12.83 web pages, collectively consisting of 69910\u00b1462671.91 tokens. Notably, 52% of the entities in WILDHALLUCINATIONS do not have Wikipedia pages as part of their knowledge source. This highlights a key difference with existing benchmarks that evaluate exclusively on Wikipedia entities that are likely to be more prominently featured in training data hence less challenging for LLMs. We manually inspect the evidence web pages for a random set of 100 entities. Our human evaluation confirms that all these entities are unambiguous and surface relevant web pages; we posit that searching for information on a single unambiguous entity is straightforward. However, not every web page is directly relevant to the entity, and sometimes the same information is repeated multiple times across different pages.\nEntity Domains. A desiderata for our WILDHALLUCINATIONS benchmark is that it cover a diverse set of domains. Here, we analyze the domain distribution of the entities in our benchmark."}, {"title": "2.3 Evaluation Pipeline", "content": "Automatic Fact-Checking. We apply the pipeline described in FACTSCORE [20] to perform auto-matic fact-checking. This pipeline consists of three steps: (1) decomposing a long-form generation into a set of atomic claims, (2) retrieving a number of passages for each atomic claim, and (3) verifying if each atomic claim is entailed by the retrieved passages.\nMetrics. We define two metrics. The first metric, WILDFACTSCORE, computes the percentage of atomic facts supported by the knowledge source. Formally, let M be a model to be evaluated, x be a prompt asking for information about an entity of interest, and S be a knowledge source. Given a response y = M(x) and Ay, a list of atomic facts in y, WILDFACTSCORE for an entity is calculated as $\\frac{1}{|A_y|} \\sum_{a \\in A_y} 1[a \\text{ is supported by } S]$, and this score is only computed for and averaged over the generations where M does not abstain from providing information.\nWILDFACTSCORE has the following limitations: (1) many atomic facts may be trivially true in practice, leading to high WILDFACTSCORE values, and (2) WILDFACTSCORE does not account for abstention. Therefore, if a model abstains for most entities and gets the rest of the generations fully correct, WILDFACTSCORE will be 100%. To address these limitations, we define a second metric, WILDFACTSCORE-STRICT, which assigns a score of 1 if all atomic facts about an entity are correct, and 0 if any atomic fact is wrong or the model abstains."}, {"title": "3 Experimental Setup", "content": "Models. We evaluate 15 state-of-the-art LLMs: Llama-3-8B and Llama-3-70B [3]; Mistral-7B, Mixtral-8x7B, and Mixtral-8x22B [14]; Command R and Command R+ (with web search enabled) [2]; Gemini 1.5 Flash and Gemini 1.5 Pro [26]; Claude 3 Haiku (claude-3-haiku-20240307) and Claude 3 Opus (claude-3-opus-20240229) [4]; GPT-3.5 (gpt-3.5-turbo-0125) and GPT-40o (gpt-40-2024-05-13) [1]; and Perplexity AI models (llama-3-sonar-small-32k-online and llama-3-large-small-32k-online). These models vary in size, architecture (single vs. mixture-of-experts), degree of openness (open vs closed weights), and knowledge cutoff dates. More details are shown in Table 3.\nInference. To evaluate the LLMs' knowledge about each entity, we prompt them with the question, \"In a paragraph, could you tell me what you know about [entity]?\" For a fair comparison, we limit the generation to 512 tokens, except for Command R and Command R+, which are given 1,024 tokens due to their shorter tokens. When generating responses, we set the sampling temperature to 1.\nModel Abstention. Different models use slightly different phrasing to indicate their refusal to generate facts about the queried entity. We manually reviewed 100 generations from each model to compile a list of such phrases. During evaluation, we identify responses containing any of these phrases as abstentions. These phrases are listed in Appendix B."}, {"title": "4 Results", "content": "Response statistics across LLMs. We summarize response statistics for LLMs in Table 4. We find considerable variance in the length of the generated responses across different models; the Sonar models generate the longest responses and have the highest number of atomic facts, while GPT-3.5 and Gemini 1.5 Flash generate the shortest responses and have the lowest number of atomic facts. We also find that Claude 3 Haiku abstains the most, with only 72.59% responding rate. As expected, the retrieval-augmented Command R models abstains the least. In general, we observe that RAG models generate significantly longer responses and abstain less compared to the models without retrieval.\nComparing WILDFACTSCORE-STRICT of LLMs. Figure 3 presents the WILDFACTSCORE-STRICT scores for the 15 models we evaluate. We find that GPT-40 and GPT-3.5 perform comparably, achieving the highest WILDFACTSCORE-STRICT and outperforming the next best models by a significant margin of six absolute points. Among the open-weight models, Mixtral 8x22B achieves the best performance. Notably, even though Command R and Sonar models have access to web search, they still underperform the best models that lack retrieval. In fact, Sonar-Large performs worse than Llama-3-70B even though it uses Llama-3-70B as the base model and adds retrieval functionality. This mirrors findings from earlier work [20] that reported low performance of retrieval-augmented models. Even within a model family, larger models do not necessarily perform better.\nWe also find that despite claims of superior capabilities, GPT-40 and Claude 3 Opus exhibit factuality on par with GPT-3.5 and Claude 3 Haiku, respectively. We note that Gemini 1.5 Pro and the Claude models abstain substantially more often than GPT-40 and GPT-3.5, which further penalizes the\nComparing WILDFACTSCORE of LLMs Next, we compare the WILDFACTSCORE of different LLMs. We report WILDFACTSCORE for all entity category types with more than 500 entities in addition to overall score. Table 5 summarizes the results. First, we observe that WILDFACTSCORE and WILDFACTSCORE-STRICT report substantially different trends. In particular, we observe that Claude 3 Haiku outperforms all other models according to WILDFACTSCORE, including the best performing GPT-40 models from Figure 3. One source of this discrepancy stems from model abstention-WILDFACTSCORE is only calculated on generations where models do not abstain and therefore does not penalize models for high abstention rates. From Table 4, we already know that Claude 3 Haiku abstains for the highest percentage of queried entities.\nAnalyzing different domains, we find that geographic and computing are relatively easier domains: 7 out of 15 models achieve over 90% WILDFACTSCORE in these domains. In contrast, people and finance are more challenging domains, with only two models and three models reporting over 90% WILDFACTSCORE in these domains, respectively. We also observe that although Claude 3 Haiku outperforms GPT-40, this is primarily due to higher performance on the majority people category in WILDHALLUCINATIONS. By extracting entities in the wild from across various categories, our dataset provides a more fine-grained view into LLM performances, which is missing in similar prior benchmarks [20]."}, {"title": "5 Analysis", "content": "Do models hallucinate less with retrieval? We investigate the extent to which retrieval aids in reducing model hallucinations. To do this, we apply the same fact-checking pipeline to the RAG models with web retrieval disabled. Our findings, depicted in Figure 4, include the WILDFACTSCORE-STRICT for four RAG models: Command R, Command R+, Sonar-Small, and Sonar-Large. We find that, without access to the web, all the models except Sonar-Large exhibit more frequent hallucinations. Interestingly, Sonar-Large hallucinates less in the absence of web retrieval.\nDo models hallucinate more on rare entities? We evaluate factuality of LLMs as a function of entity frequency using the perplexity values in Section 2.2. We group the entities into 6 groups by the order of magnitude (OoM) of their perplexity: an OoM of 0 corresponds to the most frequent entities, while an OoM of 5 corresponds to the least frequent entities. We present the results of eight LLMs in Figure 5. Regardless of model size, models without retrieval report significantly decreased performance on rarer entities. Mixtral-8x22B and Claude 3 Opus show the largest drop in WILDFACTSCORE-STRICT. Conversely, models with retrieval are more robust to rare entities.\nDo models hallucinate more on non-Wikipedia knowledge? We also compare the factuality of LLMs on entities that have Wikipedia pages with those that do not. The results for eight LLMs are presented in Figure 6. We observe a significant decrease in WILDFACTSCORE-STRICT when recalling knowledge from sources other than Wikipedia for all eight models, with GPT-3.5 and GPT-40 exhibiting the largest drop. Interestingly, even though Command R and Command R+ models perform web searches, they also exhibit lower factual accuracy when generating information from non-Wiki sources.\nDo different LLMs hallucinate on similar entities? Table 4 shows that Claude and Gemini models generate facts about a substantially lower fraction of entities (< 90%) compared to the GPT models (> 95%). We hypothesize that some entities are challenging for all LLMs, and the former"}, {"title": "6 Related Work", "content": "LLM Hallucinations Hallucination has long been studied in traditional text generation settings like summarization and machine translation [19, 9, 12]. In these scenarios, hallucinations are generations that contradict the source document. Nowadays, LLMs are increasingly used in closed-book settings where they rely on their parametric knowledge to answer queries and often hallucinate incorrect world knowledge or reasoning [31, 30]. Recent works have characterized hallucinations in both settings [30, 17, 18], covering diverse domains including scientific questions [18], questions in the wild from users [17], and domain-specific errors in dialog datasets [25]. The most closely related dataset to our work is FACTSCORE [20] which proposes a framework to evaluate LLM hallucinations in people-centric generations. Our work expands the scope of the benchmark by extracting a diverse set of entities from real user queries and using a more comprehensive knowledge source than Wikipedia.\nDetecting LLM errors Research in hallucination detection falls into two categories; the first line of work uses a question generation and answering pipeline where questions are asked of both the evidence document and generated content and the answers are matched [9, 27, 10]. This allows for the use of strong off-the-shelf question-answering models without additional training. The other line"}, {"title": "7 Conclusions", "content": "We introduce WILDHALLUCINATIONS, a benchmark designed to evaluate the factuality of LLMs using entities extracted from real-world user-chatbot interactions. By prompting LLMs to generate information about these entities, WILDHALLUCINATIONS provides an evaluation of LLM factuality on use-case-relevant knowledge. The generated descriptions are fact-checked against a variety of web sources beyond traditional Wikipedia articles, offering a realistic evaluation of LLM factuality."}, {"title": "A Analysis", "content": "Validating Evaluation Pipeline. We apply the evaluation pipeline introduced in [20], which demonstrates a high agreement rate between human judgments and the proposed evaluation method. We further validate this evaluation pipeline on WILDHALLUCINATIONS. There are two possible sources of errors in the evaluation pipeline: (1) an atomic fact may indeed be supported, but the retrieved contexts may not contain relevant information, and (2) the entailment prediction may be incorrect. To check for these errors, we randomly sample 50 entities and manually review all atomic facts identified as unsupported from three models: Claude 3 Haiku, GPT-40, and Llama-3-70B. We then calculate the proportion of errors that are actual mistakes versus those introduced by the evaluation process. We summarize the results in Table 7. We find that 19%-26% of the atomic facts identified as unsupported are not actual mistakes. However, since the evaluation pipeline introduces similar proportions of mistakes across models, the models are roughly equally affected by these errors.\nAnalysis of RAGs. RAG models are designed with the goal of reducing hallucinations. Counter-intuitively, they appear to have a similar amount of hallucinations as models without retrieval. To"}, {"title": "B Detecting Model Abstention", "content": "We use the code below to detect if a model abstains from responding.\ninvalid_ppl_mentions = [\n   \"I could_not_find_any_information\",\n   \"The_search_results_do_not_provide\",\n   \"There_is_no_information\",\n   \"There are_no_search_results\",\n   \"there_are_no_provided_search_results\",\n   \"not provided_in_the_search_results\",\n   \"is_not_mentioned_in_the_provided_search_results\",\n   \"There_seems_to_be_a_mistake_in_the_question\",\n   \"Not sources_found\",\n   \"No_sources_found\",\n   \"Try_a_more_general_question\",\n   \"Unfortunately,\",\n   \"There_doesn'tseem_to_be\",\n   \"There_does_not_seem_to_be\",\n   \"Please\"\n   \"I_do_not\",\n   \"I_don't\",\n   \"**No_relevant\",\n   \"I'm afraid\"\n   \"I_am_afraid\",\n   \"I apologize,\",\n   \"I'm sorry\"\n   \"I am sorry \"\n   \"Sorry\",\n   \"I_am_not_familiar_with\",\n   \"I'm_not_familiar_with\",\n]\n\ndef wildentities_ai_abstain_detect(generation):\n  if \"is_not_explicitly_mentioned\" in generation:\n    return True\n  if \"is_not_mentioned\" in generation:\n    return True\n  if \"isn't_mentioned\" in generation:\n    return True\n  if \"is_not_a_widely\" in generation:\n    return True\n  if \"isn't_a_widely\" in generation:\n    return True\n  if is_invalid_ppl(generation):\n    return True\n  if \"provide_more\" in generation:\n    return True\n  valid_paras = []"}, {"title": "C Data Collection", "content": "We include more details about the web scraping process. We use ScrapingBee to scrape these web pages with an ad blocker enabled. To mitigate noise in the web content, we perform the following cleaning steps: (1) we remove any web page with a status code not equal to 200, (2) we exclude web pages from platforms that require login to view content, (3) we exclude websites containing phrases like \u201cYou're signed out\u201d or \u201cLog in to,\u201d and (4) we use BeautifulSoup to remove HTML and CSS tags. After these cleaning steps, some entities may be left with no web pages. This indicates that there is not enough information available on the web for these entities, and we thus remove those entities from our dataset."}, {"title": "D Limitations", "content": "Language Scope Despite the diversity of entity types used in WILDHALLUCINATIONS, the current version focuses on English. This limitation excludes the evaluation of LLM factuality in other languages, which is critical for assessing the performance of LLMs in multilingual contexts, es-pceially considering that powerful LLMs have multilingual capabilities and are used in different languages [23].\nBias in Source Data The data used in WILDHALLUCINATIONS is sourced from the WildChat dataset, which reflects the demographic distribution of its users. Consequently, any biases present in this user base are inherited by WILDHALLUCINATIONS. This includes potential biases in user demographics, interests, and interaction styles. Addressing this bias would require diversifying the data sources that represent a wider demographic spectrum.\nNoise in Web Content One significant challenge in our approach is the inherent noise in web content. Articles retrieved from the Internet may contain misinformation, outdated information, or biased perspectives [7, 5]. This noise can adversely affect the accuracy of the fact-checking process, as incorrect or misleading information may be used to verify the LLM's generated descriptions.\nRetrieval Errors Retrieval errors represent another limitation in our approach. While we strive to retrieve articles relevant to each entity, the retrieval step might not always identify the most relevant and up-to-date knowledge sources. This limitation can lead to incomplete or inaccurate evaluations of the factuality of the LLMs' responses. Improving retrieval algorithms is essential for mitigating this issue.\nEntailment Errors The fact-checking process involves decomposing generated descriptions into atomic claims and verifying these claims against the retrieved web articles. This step can introduce entailment errors, which occur in two forms: false positives and false negatives. False positives arise when a correct atomic fact is incorrectly identified as wrong, while false negatives occur when an incorrect atomic fact is mistakenly identified as correct. These errors can harm the evaluation results."}]}