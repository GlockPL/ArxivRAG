{"title": "Memorize and Rank: Elevating Large Language Models for Clinical Diagnosis Prediction", "authors": ["Mingyu Derek Ma", "Xiaoxuan Wang", "Yijia Xiao", "Anthony Cuturrufo", "Vijay S Nori", "Eran Halperin", "Wei Wang"], "abstract": "Clinical diagnosis prediction models, when provided with a patient's medical history, aim to detect potential diseases early, facilitating timely intervention and improving prognostic outcomes. However, the inherent scarcity of patient data and large disease candidate space often pose challenges in developing satisfactory models for this intricate task. The exploration of leveraging Large Language Models (LLMs) for encapsulating clinical decision processes has been limited. We introduce MERA, a clinical diagnosis prediction model that bridges pertaining natural language knowledge with medical practice. We apply hierarchical contrastive learning on a disease candidate ranking list to alleviate the large decision space issue. With concept memorization through fine-tuning, we bridge the natural language clinical knowledge with medical codes. Experimental results on MIMIC-III and IV datasets show that MERA achieves the state-of-the-art diagnosis prediction performance and dramatically elevates the diagnosis prediction capabilities of generative LMs.", "sections": [{"title": "Introduction", "content": "Electronic Health Records (EHR), containing patient status and diagnoses, embody valuable domain expertise and clinical operation patterns (Caufield et al. 2019). Clinicians make diagnosis judgments based on their extensive medical knowledge, acquired through years of education from textbooks and literature, as well as their accumulated experience derived from clinical practice. Clinical diagnosis prediction aims to predict patients' diseases that are highly likely to be diagnosed in the upcoming hospital admission by analyzing the patients' past diagnoses. The input and output are both presented in sequences of medical codes, which do not directly convey semantic information nor disease property. The resulting AI-enhanced diagnosis system (Morid, Sheng, and Dunbar 2023) may enable early warning of diseases (Rochefort, Buckeridge, and Forster 2015), optimized clinical resource allocation (Yadav et al. 2013), and better risk estimation for sustainable insurance (Hsu et al. 2016).Two primary challenges in diagnosis prediction have driven various research efforts (Wornow et al. 2023b) but remain unsolved. First, what would be the best practice to incorporate clinical knowledge into the model? Existing works initialize concept embeddings from natural language descriptions (Wu et al. 2023b; Bornet et al. 2023), or enrich patient representation with external disease ontologies (An et al. 2023; Cheong et al. 2023). However, a significant gap persists between the primary knowledge modality, i.e. natural language, and the model's hidden representation. Second, how can we handle the large candidate space when making predictions and exploit the supervisory signals induced from this candidate space? The commonly used International Classification of Diseases (ICD) coding system encodes 13k+ diseases (Cartwright 2013). Existing works typically treat the task as k-way classification where k is the number of possible diseases, and then apply cross entropy loss for each disease individually. These approaches often overlook the dependencies among diseases and the structural nuances within the diagnosis coding system.Generative Language Models (LM), especially the Large Language Models (LLM), are trained to predict the next token, adhere to task instructions (Brown and et al. 2020; Ma et al. 2024a), and align with human preferences (Ouyang and et al 2022). These models exhibit superior capabilities in language understanding and reasoning, as shown by their performance on science-based benchmarks (Ma et al. 2024b; Wu et al. 2023a; Zhang et al. 2024). During the pretraining stage, LLMs assimilate a large amount of knowledge extracted from literature and online corpora. However, there remains an underexplored domain in using LLM for clinical diagnosis prediction, due to the aforementioned gap between natural language and medical code, as well as the disparity between the token-level optimization process and the large candidate outcome space. These challenges impede the effective application of generative LMs to diagnosis prediction tasks, even as the state-of-the-art models predominantly rely on graph neural networks without fully harnessing natural language knowledge (Yang et al. 2023b; Wu et al. 2023b; An et al. 2023). Fine-tuning generative LM LLaMA2 (Touvron and et al. 2023) directly on diagnosis prediction yields almost 20-point lower recall@20 than GNN-based existing best model (Yang et al. 2023b) as shown in Table 1. There are some studies that use transformer-based LM for clinical outcome prediction, but they either do not support structured data as input (Niu et al. 2024; Wang et al. 2023a), not compatible with mainstream LLMs (Rupp, Peter, and Pattipaka 2023; Guo et al. 2023), or only work for narrow output space"}, {"title": "Preliminaries", "content": "MERA can be applied for any task whose output is a collection of candidates belonging to a pre-defined decision space. We introduce widely used diagnosis prediction settings as typical testbeds for MERA (Yang et al. 2023b).\nTasks. The first task is a general diagnosis prediction task, in which we aim to predict the diagnoses for the patient's potential next visit $V_{T+1}$ given patient's history diagnoses by selecting a set of medical codes from the medical code ontology $\\mathcal{O}$, which can be formally described as $f_{DP}: \\{V_1, V_2, ..., V_T\\} \\rightarrow V_{T+1}$. The second task is a disease-specific heart failure prediction task, which can be described as a binary classification function $f_{HF}:$"}, {"title": "2.1 Task Formulations", "content": "$\\mathcal{O}$, patient historical diagnosis can be represented as a sequence of admissions in chronological order $P = \\{V_1^P, V_2^P, ..., V_T^P\\}$ where T is the number of existing visits. For a particular visit V, the medical judgment made by clinicians as a result of the visit is an unordered set of diagnoses $V = \\{d_1^V, d_2^V, ..., d_{|V|}^V\\}$ in the format of |V| unique medical code (d \u2208 $\\mathcal{O}$). The task input has two variants, including 1) history diagnosis codes only, and 2) additionally providing patient profile (gender, race, medication and family history) as a natural language sentence."}, {"title": "2.2 Existing Paradigm of Generative LMs", "content": "The ordinary formulation of generative LMs takes the input sequence $seq_{in} = t_1^{seq_{in}}, ..., t_{|seq_{in}|}^{seq_{in}}$ and is expected to generate the ground-truth output $seq_{out} = t_1^{seq_{out}}, ..., t_{|seq_{out}|}^{seq_{out}}$. It produces a probability distribution $P(c|t_{1:seq_{in}}, t_{1:k}^{seq_{out}})$ over the possible next token (c \u2208 V) conditioned on both the input sequence and k generated tokens. Discrete tokens at each autoregressive decoding step are produced by Equation 1. The LM is optimized to minimize the cross-entropy loss shown in Equation 2 applied on the probability of the gold next token conditioned on the gold output tokens in the previous segment in a teacher-forcing manner, assuming the $seq_{out}|$-th token marks the end of the decoding.\n$\\hat{t}_{k+1} = argmax_{c \\in V} P(c|t_{1:seq_{in}}, t_{1:k}^{seq_{out}})$       (1)\n$\\mathcal{L}_{CE} = - \\sum_{k=0}^{|seq_{out}|} log P(t_{k+1}^{seq_{out}}|seq_{in})$        (2)"}, {"title": "MERA: Learning to Memorize and Rank", "content": "MERA builds upon a large language model LM after pre-training on a natural language corpus, instruction tuning, and potential alignment process. MERA is designed to be compatible with numerous generative LM architectures and inherit knowledge obtained through pre-training, including encoder-decoder LM and decoder-only LM. There are three steps involved as a pipeline: 1) Fine-tuning the model to memorize medical codes used to represent the diagnoses; 2) Further optimizing the model to learn inter-visit causal and temporal relations between patient visits as well as intra-visit patterns from patient history records; 3) During inference, performing autoregressive generation to produce diagnosis predictions given an unseen patient history input."}, {"title": "Medical Code Memorization", "content": "State-of-the-art LLMs struggle to associate medical codes with their correct definitions accurately. GPT-4 can only recall 45% of ICD-9 codes given corresponding definitions (row 3 of Table 2), which may be attributable to the absence of medical codes in the pre-training dataset. MERA explicitly teaches LM the semantic information associated with the medical codes and the relationships within the coding system. We consider all codes in $\\mathcal{O}$ as special tokens, each unique medical code has a dedicated token embedding and can be represented by a single token. This design reduces the noise of the learning objectives as the diagnosis probability is equivalent to the token probability. The memorization process parameterizes embeddings of the special tokens and further equips the LM with the necessary external knowledge to facilitate downstream diagnosis prediction. To integrate information about medical codes in $\\mathcal{O}$ and the natural language knowledge contained in LM, we fine-tune LM on synthetic question-answering pairs.\nBidirectional code and definition memorization. For each code c and the natural language definition $def_c$, we create two input-output pairs. The first pair includes \u201cWhat is the definition of ICD-9 code c", "$def_c$\" as $seq_{out}$ to train the model to recall its definition given a code. The second pair helps the model memorize the inverse mapping. The question-answer pairs are created according to the $\\mathcal{O}$ ontology being for the downstream task.\nDecision space structure memorization. We further embed code dependencies collectively in LM by training with separate code-category instances. The curated pairs connect a code to its disease groups at various levels $1,...,depth(\\mathcal{O})$ in the code ontology $\\mathcal{O}$. For example, $seq_{in}$ is \u201cWhat is the chapter level disease group of the ICD-9 code 998.51?\", and $seq_{out}$ is \u201cInjury and Poisoning": ""}, {"title": "Seq2seq Data Construction", "content": "The second phase aims to equip LM with a temporal and causal understanding of the diagnoses across multiple visits. We train the LM with a collection of sequence-to-sequence training instances $X = \\{X_1,..., X_{patient}\\}\\$ based on patient$_{patient}$ records, where $X_i$ is a set of (diagnosis history, future diagnosis) pairs created based on patient"}, {"title": "3.3 Learning Inter-visit Reasoning", "content": "Up to this point, the created seq2seq data instances can be used to conduct supervised fine-tuning of LM following token-level optimization used in conventional generative LM reiterated in \u00a72.2. However, as we analyze theoretically (in \u00a71) and demonstrate empirically (line 14/15 of Table 1), vanilla generative LM does not handle the diagnosis prediction task well. We propose multiple specialized learning objectives to learn the inter-visit reasoning to infer upcoming diagnoses and capture intra-visit diagnosis patterns. We bridge the sequential modeling capabilities and LM's internal knowledge with the task property and decision space structure (e.g., ICD hierarchy) for diagnosis prediction.\nAfter encoding $seq_{in}$ containing information on existing hospital visits, the LM starts to generate its prediction of the upcoming visit $\\hat{seq}_{out}$. As an immediate step, it produces a probability distribution over the possible next token $\\hat{t}_1^{seq_{out}}$ conditioned on $seq_{in}$, reflecting the possibility of different tokens in the vocabulary as one of the diagnoses for visit $V_{T+1}$. Legit candidate tokens for $\\hat{t}_1^{seq_{out}}$ are the special code"}, {"title": "3. Hierarchical contrastive learning.", "content": "We design training objectives to identify the real diagnoses among a group of similar candidate diagnoses. With such a design, the model is forced to understand the subtle differences among neighbor diseases in $\\mathcal{O}$ and learn to infer upcoming diagnoses from a candidate pool under the same disease group.\nFor a training instance $X_i$, we first identify all disease groups that the diagnoses of the next visit belong to $G_{X_i} = \\{G^{level=0}, G^{level=1},..., G^{level=depth(\\mathcal{O})}\\}$. Then, for each group $g_k$ at level j ($g_k \\in G^{level=j}$), we identify positive diagnosis codes $g_k^{pos} = \\{c^{pos}_{1},..., c^{pos}_{|g_k^{pos}|}\\}$, which are the diseases in $g_k$ that are diagnosed in the next visit. We then use all remaining diseases in $g_k$ as negative codes $g_k^{neg} = g_k - g_k^{pos} = \\{c^{neg}_{1},..., c^{neg}_{|g_k^{neg}|}\\}$. Then, we calculate an InfoNCE loss (Oord, Li, and Vinyals 2018; Ma et al. 2021; Meng et al. 2021) term for each group in $G_{X_i}$, and aggregate all the terms to be the aggregated objective $\\mathcal{L}_{CL}$.\n$\\mathcal{L}_{CL} = -log \\frac{\\sum_{c^{pos} \\in g_k^{pos}}P(c^{pos}|t_{1:seq_{in}})}{\\sum_{c^{neg} \\in g_k^{neg}}P(c^{neg}|t_{1:seq_{in}})} $\n$\\mathcal{L}_{CL} = \\sum_{G^{level=j} \\in G_{X_i}} \\sum_{g_k \\in G^{level=j}}  \\mathcal{L}_{g_k}$ $\n\nThe loss term for higher-level groups (where j is smaller) is used to enable the model to recognize disease scopes across a broad spectrum. Optimizing the high-level loss mimics the clinician's training process of making differential diagnoses, the \u201crough guesses\u201d of possible diseases. Loss terms for lower-level groups focus on nuanced comparisons among diseases within the same family, increasing the model's ability to distinguish rare diseases. The proposed contrastive learning approach is efficient and capable in comparison to in-batch contrastive learning for two reasons: 1) The loss is calculated on the token probability distribution, essential for the typical decoding of generative LM, with no need for additional architecture or forward/backward passes. This ensures efficiency and maximum compatibility with the pre-trained LM. 2) The contradiction for loss calculation pertains to token probabilities, allowing the integration of prediction confidence for each disease into the"}, {"title": "1 Dynamic confidence threshold.", "content": "To produce a short list of confident diagnoses among the full ranking of all diagnosis codes, we learn a dynamic confidence threshold to select the most likely predictions. Existing works apply a fixed threshold to the probability distribution, which is often determined as a hyperparameter observed through the performance of the validation set (Morid, Sheng, and Dunbar 2023; Rasmy et al. 2021). This widely used strategy makes shortlisting less flexible, and the model tends to play it safe and produces more diagnoses than it should. To model the confidence threshold dynamically, we use a special token EOV to mark the confidence threshold within the token probability ranking list. EOV was appended at the end of the diagnosis sequence of each visit as introduced in \u00a73.2.\nThe model LM learns the placement of the EOV in two ways. Implicitly, the visit segments in the input sequence demonstrate that the special token EOV represents the end of a visit segment, implying the model should stop generating more diagnosis codes. Training with EOV-ended visit sequence segment, LM naturally learns to assign EOV a higher probability than other code tokens when the model is not confident to make more diagnoses and chooses to generate EOV to end the diagnosis sequence of a particular visit. Explicitly, we design a learning objective to train the LM to place the EOV token at the proper rank of the token probability distribution $P(c|t_{1:seq_{in}})$. We identify the positive medical codes that do appear in the target visit as $\\mathcal{O}^{pos}$ and the ones not included as $\\mathcal{O}^{neg}$ ($\\mathcal{O}^{pos} + \\mathcal{O}^{neg} = \\mathcal{O}$). The $\\mathcal{L}_{DCE}$ is essentially a dynamic cross-entropy loss that regularizes the probability of each positive code to be not smaller than the probability of EOV and further make sure the probability of each negative code is not larger than $P(EOV|t_{1:seq_{in}})$. The optimization of the dynamic confidence threshold applies fine-grained supervision to the probability distribution, enabling effective and efficient diagnosis capability learning with sparse patient data.\n$\\mathcal{L}_{DCE} = \\sum_{c \\in \\mathcal{O}^{pos}} log(ReLU(P(EOV |t_i|seq_{in}|) - P (c | t_i|seq_{in}|)))$\n $+\\sum_{c \\in \\mathcal{O}^{neg}} log (ReLU(P(c|t_i|seq_{in}|) - P (EOV | t_i|seq_{in}|)))$"}, {"title": "4 Learning Intra-visit Diagnosis Patterns", "content": "Besides training the model to reason between visits, there are many implicit rules and latent dependencies buried in the large pool of diagnoses within each visit. For example, within a group of similar diseases, the clinicians normally only choose the most representative code for the patient's status; some diseases might suppress or correlate with other diagnoses. Modeling the intra-visit dependencies enables us to incorporate real-life clinic operation patterns into realistic diagnosis predictions. The prediction made for a specific visit should consider other diagnoses of the same visit.\nTo model the intra-visit dependencies, we apply the objectives over the token probability distribution introduced in \u00a73.3 to multiple training instance variants with partial output sequences as conditions. This enables teacher-forcing training. For each ($seq_{in}$, $seq_{out}$) pair in $X_i$ for patient record $P_i$ where the $seq_{out}$ expresses all diagnoses in the visit $V_{1, k} \\in [1,T \\in [1,T \u2013 1]$, we create $|V_{1, k}|$ variants to move partial diagnosis results in $seq_{out}$ to be part of the input of LM together with $seq_{in}$. Given the new input including the patient history and m known diagnoses in the upcoming visit,\nLM produces probability over the candidate medical code $P(c|t_{1:seq_{in}}, t_{1:m}^{t_{k+1}})$. Since the m known diagnoses have been part of the input sequence, we remove the corresponding medical codes from the positive code set for the calculation of $\\mathcal{L}_{DCE}$ and $\\mathcal{L}_{CL}$ to prevent the model from generating duplicated codes. Formally, the conditions for probability P in Equation 3, 4, and 6 are $t_{1:seq_{in}}, t_{1:m}^{t_{k+1}}$ instead of $t_{1:seq_{in}}|$. The m known diagnoses in $V_{k+1}^{1, k}$ are removed from $g_k^{pos}$, $\\mathcal{O}^{pos}$ and added to $g_k^{neg}$ and $\\mathcal{O}^{neg}$"}, {"title": "5 Training and Inference Pipeline", "content": "Training objectives. For code memorization, LM is trained with the ordinary cross-entropy loss in Equation 2. The hierarchical contrastive learning loss (Equation 5) is additionally applied to the instances whose output is a medical code. For the diagnosis prediction task, the LM fine-tuned from the memorization task is further optimized with the hierarchical contrastive learning loss (Equation 5) and the dynamic cross-entropy loss (Equation 6) on $|V_{1, k}|$ teaching force variants. Unlike language modeling, no loss has been applied to the reconstruction of the input segment for both fine-tuning stages. We perform full-parameter fine-tuning.\nAutoregressive decoding. The produced LM can be used for inference on unseen patient history. Given $seq_{in}$, LM performs autoregressive decoding to output discrete diagnosis code with the highest probability in the ranking list for each output step until the EOV token is generated."}, {"title": "Experimental Setup", "content": "Datasets. We use MIMIC-III (Johnson et al. 2016) and MIMIC-IV (Johnson et al. 2023) EHR datasets containing patient records to train and evaluate. The MIMIC-III dataset focuses on patients eventually admitted to the ICU, while the MIMIC-IV dataset includes both ICU patients and other patients. We conduct data preprocessing following previous works (Lu, Han, and Ning 2022) and split the train/dev/test sets by patients to avoid information leak.\nMetrics. We report the weighted F1 and recall@k, where k is the number of top-ranked predictions, and AUC and F1 for diagnosis prediction and heart failure, respectively."}, {"title": "Performance of Diagnosis Prediction", "content": "We show the performance comparison on the diagnosis prediction and heart failure prediction tasks (described in \u00a72.1) using ICD-9 as decision space with history diagnosis code as input in Table 1 and the influence of base pre-trained LM selection in Table 2. We further show that MERA can be generalized to richer input with natural language patient profile, and the larger ICD-10 decision space in Table 3.\nEncoder-only & vanilla generative LM perform poorly. The encoder-only LMs exhibit limited performance (rows 12-13 of Table 1), possibly because they do not account for the specialized modeling of intra-visit order and the extensive output space. When employing a vanilla generative LM (rows 14-15), the performance is further diminished. This is attributed to sparse supervision distributed in token-level loss. For each pass, only the probability of the single ground-truth token is optimized following Equation 2, while MERA optimizes the probabilities of all candidate diagnoses.\nGap between zero-shot and fine-tuned LMs. There remains a 20-point deficit in recall@20 comparing the best"}, {"title": "Performance on Medical Code Memorization", "content": "Table 2 shows the evaluation of the memorization results for the ICD-9 medical code system while using various base LMs. We report code and definition accuracy, indicating the proportion of correct output full ICD codes/definitions given their definitions/ICD codes as input by exact match. We observed that 1) Almost perfect medical code recall using large-enough 7B LM. 2) Pre-trained LLMs alone do not know medical codes well. GPT models exhibit better memorization of medical codes compared to LLAMA2 (rows 1-3 of Table 2), but they still lag far behind the fine-tuned models (line 3 vs 12). 3) Model scaling-up boosts memorization. Increasing models' parameters significantly enhances their memorization capabilities, as evidenced by an 80-point improvement in code accuracy from GPT-2 medium to large. However, this does not fully translate into improvement of the same magnitude in diagnosis prediction (row 9 vs 10"}, {"title": "Ablation Studies on Method Design", "content": "Knowledge injection approach. In rows 1-2 of Table 4, we observed that simply training the medical code sequence without providing meanings of the codes (row 1) leads to a 3.5-point lower recall@20. Providing the natural language definition of medical code in the input prompt along with the history diagnosis code (row 2 vs 1) is also helpful. However, the NL prompt method suffers from incomplete patient history due to the LM's input length limit, resulting in a 2.5-point lower recall@20 compared to memorization. Fine-tuning for concept memorization is the most effective knowledge injection approach.\nTraining objectives. Results in row 3-7 of Table 4 show that removing hierarchical contrastive learning leads to more than a 10-point drop in F1. Among the contrastive terms for disease groups categorized by different granularities, the 0-th level loss (row 4) is the most beneficial, which provides comparisons among the most involved diseases. The finest level loss (row 6) is the second most important, as the chapter-level disease is relatively easier to mine from data, while the fine-grained diagnosis decision involves distinguishing diseases that are similar in manifestation or etiology. Dynamic confidence threshold (row 7) also contributes more than 4-point F1 score improvement.\nOutputting strategies. In rows 8-10 of Table 4, we explore optimal approaches to produce the diagnosis prediction set. LM can conduct autoregressive decoding to generate diagnosis codes as an output sequence. Alternatively, we can obtain the ranking list based on the token probability"}, {"title": "5 Related Works", "content": "Diagnosis prediction. Existing works leverage structured diagnosis data (Morid, Sheng, and Dunbar 2023). They use sequential models like RNN and LSTM (Choi et al. 2016; Bai et al. 2018) to model the longitudinal patient history and GNNs to encapsulate spatial features (Proios et al. 2023; Lu, Han, and Ning 2022). To inject external knowledge, they conduct multi-task or transfer learning to borrow supervision from other tasks or domains (Yang et al. 2023a; Zhou et al. 2023), use pre-trained embedding to incorporate natural language into initial features (Wu et al. 2023b; Bornet et al. 2023), or utilizing external knowledge graphs or ontologies (An et al. 2023; Cheong et al. 2023; Li et al. 2020). We propose to use the capable LLM architecture to learn patterns from patient history sequences and inject external knowledge with a unified and shared architecture across the pipeline. Existing works apply contrastive learning on intermediate latent for KG relations (An et al. 2023) or patient embedding (Jeong et al. 2023), while we apply contrastive learning on diagnosis output space directly.\nTransformer models for medical event prediction. Existing works either handle NL medical notes and other modalities (Niu et al. 2024; Zhou et al. 2023; Wang et al. 2023b; Liu et al. 2023), or they use a non-unified architecture that cannot inherit the pretrained knowledge (Rupp, Peter, and Pattipaka 2023; Li et al. 2023; Pang et al. 2021; Guo et al. 2023) or needs adaptation for downstream tasks (Steinberg et al. 2023; Lai, Zhai, and Ji 2023; Ma et al. 2023; Xu, Ma, and Chen 2023). (Wang et al. 2023a; Shoham and Rappoport 2023; Wornow et al. 2023a) fine-tune the generative LM for classification tasks. We develop a model that is compatible with mainstream LLMs to use the pretrained knowledge and specializes in producing predictions from large diagnosis decision space."}, {"title": "Conclusion", "content": "MERA stands out by seamlessly integrating clinical knowledge and addressing the challenges associated with a large candidate space. Contrasting learning, tailored to the coding system's hierarchical structure, enables effective distinguishing between accurate and inaccurate diagnosis codes. Through validation on MIMIC datasets, MERA emerges as a leading approach to diagnosis prediction."}]}