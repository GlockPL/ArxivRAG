{"title": "Real-time system optimal traffic routing under uncertainties\nCan physics models boost reinforcement learning?", "authors": ["Zemian Ke", "Qiling Zou", "Jiachao Liu", "Sean Qian"], "abstract": "System optimal traffic routing can mitigate congestion by assigning routes for a portion\nof vehicles so that the total travel time of all vehicles in the transportation system can\nbe reduced. However, achieving real-time optimal routing poses challenges due to uncer-\ntain demands and unknown system dynamics, particularly in expansive transportation\nnetworks. While physics model-based methods are sensitive to uncertainties and model\nmismatches, model-free reinforcement learning struggles with learning inefficiencies and\ninterpretability issues. Our paper presents TransRL, a novel algorithm that integrates\nreinforcement learning with physics models for enhanced performance, reliability, and\ninterpretability. TransRL begins by establishing a deterministic policy grounded in\nphysics models, from which it learns from and is guided by a differentiable and stochastic\nteacher policy. During training, TransRL aims to maximize cumulative rewards while\nminimizing the Kullback-Leibler (KL) divergence between the current policy and the\nteacher policy. This approach enables TransRL to simultaneously leverage interactions\nwith the environment and insights from physics models. We conduct experiments on\nthree transportation networks with up to hundreds of links. The results demonstrate\nTransRL's superiority over traffic model-based methods for being adaptive and learning\nfrom the actual network data. By leveraging the information from physics models, Tran-\nSRL consistently outperforms state-of-the-art reinforcement learning algorithms such as\nproximal policy optimization (PPO) and soft actor-critic (SAC). Moreover, TransRL's\nactions exhibit higher reliability and interpretability compared to baseline reinforcement\nlearning approaches like PPO and SAC.", "sections": [{"title": "1. Introduction", "content": "Traffic congestion in urban areas is one of the plagues of citizen's everyday life and it can\ncost huge economic loss [23]. According to a report from Federal Highway Administration\n1(FHWA), \u201croughly half of the congestion experienced by Americans happens virtually\nevery day\". This type of congestion is generated by unbalanced temporal and spatial\ndistribution of traffic activities. Recent studies [10, 26] have shown that congestion"}, {"title": null, "content": "can be significantly mitigated by optimally guiding route choices of a small portion\nof travelers. The system optimal routing aims to minimize the total system cost by\nassigning routes for some travelers. There are many ways to practically implement and\nguide traffic routing. For instance, congestion toll [62, 8] changes users' routing choice\nby changing their perceived cost. Variable traffic signs (VMS) provide real-time traffic\ninformation using road-side signals like LED signs [15, 16, 5, 22] to affects users' routing\nchoice. Route guidance and driver information systems (RGDIS) [2] directly provide\nroute recommendations to users using in-vehicle devices such as infoentertainment system\nor cell phone applications. However, solving system-level optimal routing in real-time for\nlarge networks remains a big challenge.\nThe optimal routing in real-time for large networks is challenging for three reasons.\nFirst, demands are uncertain. Travel demands stem from human activities that may have\nrandomness, so it is almost unlikely to predict the travel demands accurately. Demands\nessentially affect traffic conditions, and the difference between the actual demands and\nthe estimated demands can cause nontrivial estimation errors in traffic conditions. For\nexample, [46] showed that congestion duration can change significantly with an incremental\nchange in demands. Therefore, it is important to consider the demand uncertainties. From\na theoretical perspective, [55] showed that using expected demands tends to overestimate\nthe network performance in traffic assignment evaluation. [13] also stated it is important\nto consider demand uncertainty for the congestion pricing problems. In this study, we\nexplicitly include the demand uncertainty and regard demands as random variables that\ncannot be predicted precisely in advance.\nSecond, system dynamics may not be modeled perfectly. Though extensive traffic\nflow models (e.g., the celebrated LWR model [31, 49], cell transmission models [11, 12],\nand link queue model [24]) have been proposed to approximate the system dynamics\nof transportation networks, no known flow model can precisely replicate the flow in\nnetworks consisting of both roads and intersections. Each model is associated with\nvarious assumptions and possible parameter estimation errors. For example, conventional\nkinematic models ([31, 49, 11, 12, 24]) all assume a deterministic fundamental diagram\nto depict the relation between density and flow for mathematical tractability, while [48]\nstates this relation can be better modeled by stochastic models. In addition, estimation\nof the deterministic fundamental diagram parameters is not always accurate due to\nthe randomness of traffic flow and transient flow state transitions. In this study, we\nacknowledge that the traffic flow model cannot be known precisely. Rather, there is a\nmodel mismatch between the assumed/adopted traffic models and the actual true system\ndynamics, and this mismatch is unkonwn in advance.\nThird, real-time system optimal traffic routing is challenging especially in large\nnetworks due to complex interplay among traffic flow of various origins and destinations.\n[44] proposed a stochastic optimal real-time routing method for a two-route network. By\nabstracting a general commuting network into a two-route network, the analytical solution\ncan be derived and approximated using dynamic programming. However, deriving the\nanalytical solution of optimal traffic routing (i.e. path flow) for a large network is almost\ninfeasible because the complexity of the problem and the dimension of decision variables\nincrease exponentially as the network size. To that end, machine learning methods, like\nreinforcement learning, could be inefficient and even infeasible. The state space and\naction space increase exponentially as the network size increases, so finding an optimal\npolicy from the exploded search space without any prior knowledge is inefficient and"}, {"title": null, "content": "sometimes even infeasible. To our best knowledge, though [29] leveraged reinforcement\nlearning to solve real-time optimal routing on networks with up to 41 links. Real-time\noptimal routing has not been solved by reinforcement learning on networks with more\nthan hundreds of links. This study experiments on three networks with 2 links, 18 links,\nand 621 links, respectively.\nCurrent methods for real-time optimal routing or general traffic control can be\ncategorized into heuristic methods, model-based methods, and reinforcement learning-\nbased methods. Heuristic methods use rule-of-thumb or feedback control to correct\ncontrol errors to maintain desired system states. [25] formulated the real-time routing\nproblem as a feedback control problem, and a feedback linearization technique is used\nto achieve a user equilibrium state. [43] adopted a fuzzy control approach to determine\nreal-time routing strategies to improve the network performance. Heuristic methods are\ninterpretable and easy to implement, but they are reactive and only take action when\ncontrol errors arise. Therefore, their performance is almost necessarily suboptimal.\nModel-based methods typically first develop a traffic model or simulation to simulate\nthe system dynamics, and then solve the optimal solutions through either heuristic\nalgorithms (e.g., moving successive averages (MSA)) or deriving analytical closed-form\nsolutions. For example, [54] used autoregressive integrated moving average (ARIMA) to\npredict demands, a binary logit model to predict traveler choices, and CTM to predict\ntraffic flows. Based on these predictions, an optimization algorithm was adopted to set\ntolls to affect routing choices to optimize the objective function. [53] proposed a model\npredictive control method for dynamic pricing to reduce the total traveler delay. Similar\nto [54], [53] used a logit choice model to predict traveler choices and CTM to predict\ntraffic flows. Model-based methods are interpretable and can be proven to be optimal\nin theory. However, the inevitable model mismatch between the models and the actual\nsystem dynamics intrigues their performances in practice.\nReinforcement learning (RL) gained popularity in the traffic control domain because\nit's model-free and can learn optimal policies directly from data or the environment. [41]\nconsidered a practical control scenario with multiple origins and destinations, partially\nobserved network states, and stochastic demands. The problem was formulated as\na partially observable Markov decision process (POMDP) and solved using RL. The\nexperiments in [41] showed RL outperformed feedback control. [29] considered a scenario\nwhere route choices of autonomous vehicles can be fully controlled to improve network\nefficiency, and the policy was learned using reinforcement learning. The experiments on\nsimplified networks indicated the learned RL policy realized performances on par with the\nsystem optimum. However, to our best knowledge, RL has not been tested for optimal\nrouting on large networks with more than hundreds of links. With the well-known curse of\ndimensionality, whether RL can solve optimal policies in large networks remains unknown.\nIn this study, the reliability and interpretability of the RL policies are the focus for\nits realism in real-world traffic operation practice. Most RL policies are not deterministic\nand the traffic environment is stochastic in nature, and it is clear that the performances\nof RL policies in practice are subject to variability. When implementing RL policies, one\nquestion out of interest is how poor the performance of RL policies can possibly be in the\nworst cases. In reality, traffic operation may not tolerate a poor performance for just one\nday, unlike RL applications in other domains, e.g. robotics, that may accept some time of\nonline improvement through learning. In this work, we propose Conditional Travel Time\nReduction at Risk (CTTRAR) to measure the reliability of a control method. CTTRAR"}, {"title": null, "content": "compares the total travel time of the worst cases with a control method and the total\ntravel time of the no-control scenarios. RL policies are regarded as reliable if the worst\nperformance is still better than the no-control scenarios. In addition, the interpretability\nof the RL policies is a concern for practitioners. The objective of RL is to maximize\nthe cumulative rewards with no specific constraints for the output actions. As a result,\nthough RL leads to high cumulative rewards, the actions of RL might fluctuate over\ntime, which makes RL policies less interpretable. Our general idea is to constrain the\nRL policies to not go too far from an interpretable \"teacher\" policy. For example, system\noptimum routing under traffic flow dynamics models can serve as a teacher policy. Those\nflow dynamics models may not be precisely calibrated (e.g., in a simulation environment)\nor the demand is stochastic, but its flow propagation mechanisms and the analytical\nsolution can help to guide and learn interpretable RL policies.\nMoreover, it is unclear when RL is superior to model-based methods and vice versa.\nA model-based method is extensively compared with model-free RL methods to shed light\non the performance between model-based methods and learning-based methods. The\nresults indicate RLs outperform the model-based method when the demand uncertainty\nis large or/and the model mismatch is significant, and vice versa. When the network\nsize is large, RLs sometimes struggle to find a reasonable policy that works precisely for\nthe unknown or unexplored system states and dynamics, but model-based can perform\nwell especially when the model mismatch is insignificant. With the trade-off between the\nmodel-based method and model-free RL, one interesting question is can we combine the\nadvantages of both types of methods?\nWe propose a general reinforcement learning framework that can couple different types\nof well-established transportation methods (e.g., heuristics, model-based, or machine\nlearning-based) with RL. In this study, we focus on an example of coupling the model-based\nmethod with RL. Our proposed reinforcement learning framework differentiates from most\nprevious reinforcement learning algorithms including DDPG [50], PPO [50], and SAC [18]\nby leveraging information from transportation methods, named Transportation-informed\nReinforcement Learning (TransRL). TransRL is able to learn from the environment\nand the traffic model simultaneously. On one hand, even with a model mismatch, the\ninformation from the models is not unuseful and can help RL narrow down the search\nspace. On the other hand, with the ability to learn from the environment directly, RL is\nable to learn a better policy than the model-based method by implicitly correcting the\nmodel mismatch between the models and the actual system dynamics.\nThe contributions of this paper are summarized as follows."}, {"title": null, "content": "\u2022 We solve real-time system optimal routing in sizable transportation networks. The\nmodel is based on a realistic setting where only a few links are observed and a\nportion of vehicles can be influenced with their respective routing guidance.\n\u2022 We relax the assumptions of known travel demands and accurate traffic models in\nother RL models. Instead, travel demands are assumed to follow a time-dependent\nGaussian distribution with means of historical average demands. Moreover, there\nare model mismatches between the offline trained accessible traffic models and\nthe online test (unknown and true) system dynamics, which stem from lack of\nknowledge, model estimation errors, or unexpected incidents in real world.\n\u2022 Model-free RLs are compared with a traffic model-based method under various"}, {"title": null, "content": "levels of model mismatches. This comparison provides insights into when a traffic\nmodel-based method is superior and when RLs are preferable.\n\u2022 Ultimately, we proposed a novel RL framework TransRL that combines RL with\ntraffic models. With the ability to learn from the environment and traffic models\nsimultaneously, TransRL learns more efficiently than model-free RLs and is more\nadaptive than the traffic model-based method. More importantly, the actions of\nTransRL are more reliable and interpretable than model-free RLs.\n\u2022 Reliability and interpretability of TransRL are the focus. In order to use TransRL\nin practice, its performance at any stage throughout the process of online learning\ncannot go below the network performance without any traffic control/management\nmeasures. TransRL's policies are also designed to ensure they approximately follow\nthe guidance of model-based optimal flow solutions, which ensure its interpretability.\nThe rest of the paper is organized as follows. Literature related to vehicle routing\nand RL-based control methods is discussed in section 2. In section 3, we introduce our\nframework TransRL and show it can be proved to converge under conditions of finite\ndiscrete state and action space. We subsequently elaborate on how TransRL can be trained\nin continuous state space and action space by using neural networks to approximate\nvalue functions and policies. The problem formulation of the real-time optimal routing\nproblems is included in section 4. Section 4 then presents how to solve the optimal routing\nproblems using a model-based method, model-free RL, and the proposed TransRL. The\nconsidered methods are compared with various experiment settings on three networks in\nsection 5. Section 6 concludes our findings and suggests potential directions for future\nwork."}, {"title": "2. Related works", "content": "Quite a few literatures investigated providing real-time route information to travelers\nthrough variable message signs (e.g., Messmer et al. [37], Wunderlich et al. [58]), and in-\nvehicle routing mechanisms [4]. However, assigning routes or providing route information\nto improve the network performance is less studied. [3] used cooperative distributed\nmulti-agent systems to explore the interactions between route information providers and\ntravelers, and it was found that negotiation between information providers and travelers\ncan improve the network performance. [43] studied affecting traveler routing behaviors\nvia providing real-time routing information, and a fuzzy control approach was proposed\nto determine the information strategy in order to enhance the network performance. [14]\nproposed a coordinated online in-vehicle routing scheme with intentional information\nprovision perturbation (CRM-IP), which leverages the bounded rationality of travelers to\nshape traveler routing behaviors so that the system optimality and user optimality are\nbalanced. [29] studied a scenario where route choices of autonomous vehicles can be fully\ncontrolled in a centralized manner to improve network performance. The policy learned\nby RL performed close to a theoretically optimal solution in networks composed of up to\ndozens of links.\nIn the general traffic control domain, various analytical methods have been proposed,\nsuch as dynamic programming for optimal routing on a 2-route network [44], max-pressure\nfor signal control [60, 61, 59], and feedback control for ramp metering [56, 65], to name a"}, {"title": null, "content": "few. The analytical methods develop control policies through mathematical derivations\non the top of model assumptions. These derived control policies often show desirable\nproperties, such as convergence guarantee (i.e., reliability) and interoperability. However,\nreal environments may change over time. Because of a lack of learning ability, these\ncontrol policies cannot automatically adapt to changing environments, which deteriorates\ncontrol performances.\nReinforcement learning algorithms have gained popularity in solving real-time control\nproblems because of the characteristics of being model-free and learnable while capable of\ntaking proactive optimal actions even in uncertain environments [21, 42]. RL algorithms\nhave been extensively applied to various traffic control problems, including vehicle routing\n[28, 51], autonomous driving [64, 66], traffic signal control [45, 30], ramp metering [63],\nvariable speed limit control [27], and congestion pricing [41]. RL used in transportation\ncan be categorized into model-free RL (e.g., [30, 21]) and RL with prior knowledge (e.g.,\n[27, 19, 20, 9, 6]). Model-free RL learns from scratch by interacting with the environment,\nso the RL agent explores the search space by mostly taking random actions during the\nearly stage of the training. As a consequence, the performance of model-free RL is not\nguaranteed and is even worse than the non-control case before the convergence. This is\nparticularly problematic in the traffic operation domain, because any traffic management\nmeasure cannot afford to under-perform under general public's expectation, even for\njust a few days. In other words, practically it would be impossible to take a number of\ndays before RL starts to show system benefits. Our paper can help address this issue by\nguiding RL online learning with policies derived from physics models.\nCompared with model-free RL, RL with prior knowledge is more data-efficient by\nutilizing prior knowledge to narrow down the search space or start with a policy better\nthan random initialization. [27] utilized transfer learning to transfer a policy trained\non a source scenario to multiple target scenarios. Though the fundamental diagrams\nin the target scenarios differed from those in the source scenario, the transfer learning\nsignificantly shortened the training process in the target scenarios. [19, 20] augmented field\ndata using traffic flow models. From the augmented data, RL was trained offline, and then\nthe learned policies were implemented to acquire new field data. This process repeated\nsuch that RL kept learning from the environment. [6] designed a hybrid reinforcement\nlearning framework that combines a rule-based strategy and reinforcement learning to\noptimize the eco-driving strategy. Most of the time, the vehicle is controlled by RL policies.\nWhen the stop warning is activated, the rule-based strategy will take control and ensure\nthe vehicle stops safely. [9] proposed an RL-based framework for end-to-end autonomous\ndriving. While learning to behave optimally, the proposed framework also learns a latent\nenvironment model that predicts the state of the environment in a low-dimensional latent\nspace. The latent model greatly reduces the sample complexity by learning the latent\nstates of the high-dimensional observations. Though RL has been extensively studied in\nengineering problems (e.g., [33, 34, 35]), the actions of RL policies are not constrained to\nimprove the reliability and interpretability of the RL policies."}, {"title": "3. TransRL", "content": "3.1. Preliminaries for RL\nThe problem studied in RL can be formulated as a Markov decision process (MDP).\nThe state space and the action space are denoted as S and A. After applying an action"}, {"title": null, "content": "at \u2208 A at current state st \u2208 S, the next state st+1 \u2208 S of the environment is determined\nby a state transition probability function $p(s_{t+1} | s_t, a_t)$, and a reward $r_t$ is produced\nby the environment. The return is the discounted sum of rewards in the whole horizon\n$\\Sigma_t \\gamma^t r_t$, where \u03b3 is a discount factor making a trade-off between short-term rewards and\nlong-term rewards. A policy $\u03c0(a_t | s_t)$ determines the probability distribution of actions\n$a_t$ given a state $s_t$. We denote state-action marginals induced by a policy $\u03c0(a_t | s_t)$ as\n$\\rho_{\\pi}(s_t, a_t)$.\nThe objective of reinforcement learning is to find an optimal policy $\u03c0^*$ such that the\nexpected return is maximized.\n$\u03c0^* = \\underset{\\pi}{\\arg \\max} E_{(s_t, a_t) \\sim \\rho_{\\pi}} \\Bigg[ \\sum_t \\gamma^t r_t \\Bigg]$\n(1)\nThis study focuses on actor-critic methods [17, 32], which are a combination of Q-\nlearning [57] and policy optimization. Generally, actor-critic methods use a critic network\nparameterized as $\u03b8$ to estimate the expected return of state-action pairs with a policy\n$\u03c0$ (i.e., Q-values $Q_\u03b8(s_t, a_t)$), and an actor network parameterized as $\u03c6$ to output actions\ngiven states (i.e., $\u03c0_\u03c6(a_t | s_t)$).\nBoth the actor network and the critic network are trained using data from an experience\nreplay buffer containing transitions $(s_t, a_t, s_{t+1}, r_t)$. The actor network $\u03c0_\u03c6(a_t | s_t)$ is\nupdated by gradient descent with a loss function based on Q-values.\n$J_\u03c0(\u03c6) = E_{a_t \\sim \u03c0_\u03c6}[-Q_\u03b8(s_t, a_t)]$\n(2)\nThe critic network is updated by the temporal difference error based on the Bellman\nequation, and the loss function for the critic network is given by\n$J_Q(\u03b8) = [Q_\u03b8(s_t, a_t) - (r_t + \\gamma (Q_{\\bar{\u03b8}}(s_{t+1}, a_{t+1})))]^2, a_{t+1} \\sim \u03c0_\u03c6(s_{t+1})$\n(3)\nwhere $Q_{\\bar{\u03b8}}$ and $\u03c0_{\\bar{\u03c6}}$ are a target critic network and a target actor network respectively,\nwhich are used to stabilize the training. The actor and critic networks are updated at\nevery learning step, while the target actor and critic networks are updated periodically\nby copying the weights of the actor and critic networks respectively."}, {"title": "3.2. TransRL models", "content": "Our proposed reinforcement learning framework differentiates from most previous\nreinforcement learning algorithms (e.g., DDPG [50], PPO [50], and SAC [18]) by leveraging\ninformation from transportation methods, named Transportation-informed Reinforcement\nLearning (TransRL). First, a neural network is used to learn a stochastic and differentiable\nteacher policy from a well-established transportation method. Then, during the training\nor testing of TransRL, aside from interactions with the environment, TransRL also learns\nfrom the teacher policy by comparing the current policy with the teacher policy. The\ndivergence between the two policies, combined with rewards, is passed to TransRL for\nlearning. The whole process is illustrated in Figure 1."}, {"title": "3.2.1. Teacher policy", "content": "We assume there exist well-established transportation models and methods that can\noutput actions. The choice of transportation domain models can be very flexible, and it\ncan be traffic model-based, heuristics, or rule-based methods. Compared with general\nlearning-based methods, which work like a black box, transportation domain models\nare more reliable and interpretable as they incorporate domain knowledge and physics\ninformation that would guide the RL agent to learn effectively.\nThen, we develop a stochastic and differentiable teacher policy based on a transporta-\ntion domain model. The teacher policy is a probability distribution for actions given a\nstate. The action distribution concentrates on the action output by the transportation\ndomain model. The concentration level of the distribution can be tuned by an unreliability\nparameter \u03c3. Figure 2 shows a simple case with an action dimension of 1 and the range\nof action is [0, 1] where the action represents the portion of vehicles to be diverted from\nthe most preferred route. As Figure 2 shows, a smaller unreliability parameter leads to\na distribution more concentrated on the action from the deterministic transportation\ndomain model. Essentially, the teacher policy is a prior distribution of actions for states.\nWe will provide an example of a teacher policy in the later section 4.3."}, {"title": "3.2.2. The augmented objective function", "content": "To enable TransRL's learning from the teacher policy, the objective of TransRL\nis to maximize the cumulative rewards while minimizing the differences between the\ndivergence between the learned policy and the teacher policy. Therefore, the objective\nfunction of TransRL is different from the objective function of most previous reinforcement\nlearning algorithms as equation (1) shows. This novel objective fundamentally changes\nthe exploration behaviors and the learning process of the reinforcement learning agent.\nSpecifically, the objective function of TransRL is given by\n$J(\u03c0) = E_{(s_t,a_t) \\sim \\rho_\u03c0} \\Bigg[ \\sum_t \\gamma^t \\Big(r_t - \u03b1D \\big(\u03c0(\u00b7 | s_t) || \u03c0_{TC}(\u00b7 | s_t) \\big) \\Big) \\Bigg]$\n(4)\nwhere \u03b1 is the temperature parameter that makes a trade-off between the reward and\nthe divergence term. The choice of divergence function is relatively flexible. Without\nloss of generality, we adopt the Kullback-Leibler (KL) divergence. Then, the augmented"}, {"title": null, "content": "objective becomes\n$J(\u03c0) = E_{(s_t,a_t) \\sim \\rho_\u03c0} \\Bigg[ \\sum_t \\gamma^t \\Big(r_t - \u03b1D_{KL} \\big(\u03c0(\u00b7 | s_t) || \u03c0_{TC}(\u00b7 | s_t) \\big) \\Big) \\Bigg]$\n(5)\n$= E_{(s_t,a_t) \\sim \\rho} \\Bigg[ \\sum_t \\gamma^t \\Big(r_t + \u03b1H \\big(\u03c0(\u00b7 | s_t)\\big) + \u03b1E_{a \\sim \u03c0(\\cdot|s_t)} [log \u03c0_{TC}(a | s_t)] \\Big)]$\n(6)\nTheorem 1. The maximum entropy reinforcement learning SAC [18] is equivalent to\nTransRL with a teacher policy of uniform distribution.\nProof. If $\u03c0_{TC}$ is a uniform distribution, $\u03c0_{TC}(a | s_t)$ is a constant regardless of a, so\n$E_{a \\sim \u03c0(\\cdot|s_t)} [log \u03c0_{TC}(a | s_t)]$ is also a constant regardless of \u03c0. As a result, we can remove\n$E_{a \\sim \u03c0(\\cdot|s_t)} [log \u03c0_{TC}(a | s_t)]$ from the objective function (6), and then it becomes\n$J(\u03c0) = E_{(s_t,a_t) \\sim \\rho} \\Bigg[ \\sum_t \\gamma^t \\Big(r_t + \u03b1H \\big(\u03c0(\u00b7 | s_t)\\big) \\Big) \\Bigg]$\n(7)\nwhich is the objective of SAC [18].\nRemark 1. Including the KL divergence term in the objective results that TransRL\nbecomes more reliable and interpretable than classical RL. With this divergence constraint,\nthe policy does not go too far away from the teacher policy $\u03c0_{TC}$ that is assumed to be\nreliable and interpretable."}, {"title": "3.2.3. Policy iteration", "content": "The optimal policy is solved using policy iteration. The policy iteration includes, (1)\nthe policy evaluation step where the value functions are estimated given a policy, and\n(2) the policy improvement step where the policy is updated to increase the values given\nthe value functions. By repeating the policy evaluation and the policy improvement, the\npolicy is guaranteed to converge to one of the optimal policies that maximize values.\nTransRL uses auxiliary Q-values according to the augmented objective function such\nthat maximizing auxiliary Q-values is equivalent to maximizing the augmented objective.\nThe auxiliary Q-values are given by\n$Q(s_t, a_t) = r_t + E_{s_{t+1} \\sim p} [V(s_{t+1})]$\n(8)\nwhere\n$V(s_{t+1}) = E_{a_{t+1} \\sim \u03c0} \\Bigg[ Q(s_{t+1}, a_{t+1}) + \u03b1 log \\Bigg( \\frac{\u03c0_{TC}(a_{t+1} | s_{t+1})}{\u03c0(a_{t+1} | s_{t+1})} \\Bigg) \\Bigg]$\n(9)\nis the auxiliary state value.\nFor the policy evaluation, The auxiliary Q-values are updated iteratively by a modified\nBellman equation\n$Q^{k+1}(s_t, a_t) \\leftarrow r_t + \\gamma E_{s_{t+1} \\sim p} [V^k(s_{t+1})]$\n(10)\nWith the above update rule, the auxiliary Q-values can be proved to converge to the\nunique auxiliary Q-values of the policy as follows."}, {"title": null, "content": "Lemma 1 (Policy evaluation convergence). Starting with a initial auxiliary Q-values\nfunction Q: S \u00d7 A \u2192 R with |A| < \u221e, update $Q^0$ iteratively using equation (10). As\nk\u2192\u221e, $Q^k$ converges to the unique auxiliary Q-values function of \u03c0.\nProof. Let's define an augmented reward\n$\\bar{r}(s_t, a_t) = r_t(s_t, a_t) + \u03b3\u03b1E_{a_{t+1} \\sim \u03c0} log \\Bigg( \\frac{\u03c0_{TC}(a_{t+1} | s_{t+1})}{\u03c0(a_{t+1} | s_{t+1})} \\Bigg)$\n(11)\nThen, equation (10) becomes\n$Q^{k+1}(s_t, a_t) \\leftarrow \\bar{r}(s_t, a_t) + E_{s_{t+1} \\sim p, a_{t+1} \\sim \u03c0} [Q^k(s_{t+1}, a_{t+1})]$\n(12)\nAs |A| < \u221e, the KL divergence in equation (11) is bounded, so $\\bar{r}(s_t, a_t)$ is bounded.\nThen, one can apply the contraction mapping theorem to prove $Q^k$ converge to the unique\nauxiliary Q-values, which is the same as the proof for standard reinforcement learning\n[52].\nFor the policy improvement, the policy is updated such that the auxiliary Q-values of\nthe new policy are higher than the old policy. Similar to [18], we use a KL divergence\nbetween the old policy and the exponential of the auxiliary Q function, and the new\npolicy is obtained by minimizing the KL divergence as follows.\n$\u03c0_{new} = \\underset{\u03c0 \\in \u03a0}{\\arg \\min} D_{KL} \\Bigg( \u03c0(\u00b7 | s_t) || \\frac{\u03c0_{TC}(\u00b7 | s_t)}{Z_{old}(s_t)} \\Bigg) \\equiv  \\underset{\u03c0 \\in \u03a0}{\\arg \\max} E_{a \\sim \u03c0(\u00b7|s_t)} \\Bigg[Q^{\u03c0_{new}}(s_t, a_t) \\Bigg]$\n(13)\nwhere $Z_{old}(s_t)$ is the partition function ensuring the right part within the KL divergence\nis a probability distribution. Though $Z_{old}(s_t)$ is intractable and infeasible for large state\nspaces, it is a constant and can be ignored when calculating gradients with respect to the\npolicy. With this policy update rule, we can prove the new policy is better than the old\npolicy with respect to auxiliary Q-values as follows.\nLemma 2 (Policy improvement convergence). For $\u03c0_{old} \u2208 \u03a0$, $\u03c0_{new}$ is the optimal\nsolution of the problem defined in equation (13). With |A| < \u221e, $Q^{new}(s_t, a_t) \u2265\nQ^{old}(s_t, a_t), \u2200(s_t, a_t) \u2208 S \u00d7 A$.\nProof. Denote the KL divergence term in equation (13) as\n$J_{\u03c0_{old}}(\u03c0) = D_{KL} \\Bigg( \u03c0(\u00b7 | s_t) || \\frac{\u03c0_{TC}(\u00b7 | s_t)}{Z_{\u03c0_{old}}(s_t)} \\Bigg)$\n(14)"}, {"title": null, "content": "Then, as $\u03c0_{new}$ is a minimizer of the KL divergence, we have\n$J_{\u03c0_{old}}(\u03c0_{old}) \u2265 J_{\u03c0_{old}}(\u03c0_{new})$\n$\\Rightarrow E_{a_t \\sim \u03c0_{old}} [log \u03c0_{old}(a_t | s_t) \u2212 log \u03c0_{TC}(a_t | s_t) \u2212 Q^{\u03c0_{old}}(a_t, s_t)/\u03b1 + log Z^{old}(s_t)]$\n(15)\n$> E_{a_t \\sim \u03c0_{new}} [log \u03c0_{new}(a_t | s_t) \u2212 log \u03c0_{TC}(a_t | s_t) \u2212 Q^{\u03c0_{old}}(a_t, s_t)/\u03b1 + log Z^{old}(s_t)]$\n$\\Rightarrow E_{a_t \\sim \u03c0_{old}} Q^{\u03c0_{old}}(a_t, s_t) + \u03b1 log \\Bigg[\\frac{\u03c0_{TC}(a_t | s_t)}{\u03c0_{old}(a_t | s_t)} \\Bigg]$\n$< E_{a_t \\sim \u03c0_{new}} Q^{\u03c0_{old}}(a_t, s_t) + \u03b1 log \\Bigg[\\frac{\u03c0_{TC}(a_t | s_t)}{\u03c0_{new}(a_t | s_t)} \\Bigg]$\n$\\Rightarrow V^{old}(s_t) < E_{a_t \\sim \u03c0_{new}} \\Bigg[ Q^{old}(a_t, s_t) + \u03b1 log \\Bigg(\\frac{\u03c0_{TC}(a_t | s_t)}{\u03c0_{new}(a_t | s_t)} \\Bigg) \\Bigg]$\n(16)\n(17)\nNow, follow equation (8) to replace $Q^{told}$ with a formulation of $V^{old}$, and then apply\nthe inequality (17). By conducting these two steps alternatively, we obtain\n$Q^{old}(s_t, a_t) = r(s_t, a_t) + E_{s_{t+1} \\sim p} [V^{old}(s_{t+1})]$\n(18)\n$<r(s_t, a_t) + E_{s_{t+1} \\sim p} E_{a_{t+1} \\sim \u03c0_{new}} \\Bigg[ Q^{old}(a_{t+1}, s_{t+1}) + \u03b1 log \\Bigg(\\frac{\u03c0_{TC}(a_{t+1} | s_{t+1})}{\u03c0_{new}(a_{t+1} | s_{t+1})} \\Bigg) \\Bigg]$\n(19)\n(20)\n\u2264r(st, at) + Est+1~p [Vnew(St+1)]\n(21)\n= Qnew(St, At)\n(22)\nThe policy iteration alternates between the policy evaluation and the policy improve-\nment. Finally, we can prove the policy iteration will converge to an optimal policy that\nmaximizes the auxiliary Q-values as follows.\nTheorem 2 (Policy iteration convergence). With |A| < \u221e, starting from any policy\n$\u03c0_0 \u2208 \u03a0$, conduct the policy evaluation and the policy improvement iteratively. The\npolicy will converge to an optimal policy $\u03c0^*$ such that $Q^{\u03c0^*}(s_t, a_t) > Q^\u03c0(s_t, a_t), \u2200(s_t, a_t) \u2208\nS \u00d7 \u0391, \u03c0 \u0395 \u03a0.\nProof. According to Lemma 2, at each policy iteration step, we have Qnew > Qold. As\nthe augmented rewards are bounded, Q\u03c0 is also bounded. As a result, Q\u03c0 will converge\nto a certain point, where both the auxiliary Q-values and the policy converge, denoted as\nQ*. Then, we get Jn*(\u03c0*) \u2264 Jn*(\u03c0), \u2200n \u2208 \u03a0. Similar to the proof of Lemma 2, we have\nQ\u03c0 (St, at) \u2265 Q\u03c0 (St, at), \u2200(st, at) \u2208 S \u00d7 \u0391, \u03c0 \u03b5\u03a0.\nNote the above policy iteration process only works for tabular cases where the state\nspace and the action space are discrete. In more general cases where state variables\nand actions can be continuous. We can use approximations (e.g., neural networks) to\napproximate auxiliary Q-values. In addition, at the policy evaluation or the policy\nimprovement step, running until convergence is computationally expensive. In the next\nsection, we will introduce how TransRL is trained in practice, which is suitable for a\ncontinuous state and action space."}, {"title": "3.2.4. Training of TransRL", "content": "To accommodate large continuous state space and action space, we use neural networks\nto approximate the auxiliary Q-values/Q-function and the policy. The auxiliary Q-function\nis parameterized by the neural network parameters $\u03b8$, which is denoted as $Q_\u03b8(s_t, a_t)$.\nSimilarly, the parameterized policy is $\u03c0_\u03c6(a_t | s_t)$, where $\u03c6$ are the parameters of the policy\nneural network.\nFor the Q-function network, the parameters are updated to minimize the Bellman\nresidual. More specifically, the loss of the Q-function network is given by:\n$J_Q(\u03b8) = E_{s_t,a_t \\sim D} \\Bigg[ \\frac{1}{2} \\Big(Q_\u03b8(s_t, a_t) - \\bar{Q}(s_t, a_t) \\Big)^2 \\Bigg]$\n(23)\nwhere\n$\\bar{Q}(s_t, a_t) = r_t + \u03b3 \\Bigg[Q_{\\bar{\u03b8}}(s_{t+1}, a_{t+1}) + \u03b1 log \\Bigg(\\frac{\u03c0_{TC}(a_{t+1} | s_{t+1})}{\u03c0_\u03c6(a_{t+1} | s_{t+1})} \\Bigg) \\Bigg], a_{t+1} \\sim \u03c0_\u03c6(\\cdot | s_{t+1})$\n(24)\nwhere $\\bar{Q}$ is the target Q-function whose parameters are exponentially moving average\nof parameters of $Q_\u03b8$. The use of target networks is able to stabilize the training process\n[38]. Then, the Q-function network is trained via stochastic gradient descent methods\nwith gradient\n$\\nabla_\u03b8 J_Q(\u03b8) = \\nabla_\u03b8 Q_\u03b8(s_t, a_t) \\Big(Q_\u03b8(s_t, a_t) \u2212 \\bar{Q}(s_t, a_t) \\Big)$\n(25)\nThe policy network is trained by minimizing the KL divergence in equation (13).\n$J_\u03c0(\u03c6) = E_{s_t \\sim D} \\Bigg[E_{a_t \\sim \u03c0_\u03c6} \\Big[log \u03c0_\u03c6(a_t | s_t) \u2212 log \u03c0_{TC}(a_t | s_t) \u2212 Q_\u03b8(s_t, a_t)/\u03b1\\Big] \\Bigg]$\n(26)\nTo enable back-propagation in updating $\u03c6$, the reparameterization trick is used to\ngenerate action.\n$a_t = f_\u03c6(\\epsilon_t; s_t)$\n(27)\nwhere $\u03f5_t$ is independent random noise sampled from a Gaussian distribution. Afterwards,\nthe loss function of the policy network becomes\n$J_\u03c0(\u03c6) = E_{s_t \\sim D, \u03f5_t \\sim N} \\Bigg[log \u03c0_\u03c6(f_\u03c6(\u03f5_t; s_t) | s_t) \u2212 log \u03c0_{TC}(f_\u03c6(\u03f5_t; s_t) | s_t) \u2212 Q_\u03b8(s_t, f_\u03c6(\u03f5_t; s_t))/\u03b1 \\Bigg]$\n(28)\nFinally, the parameters $\u03c6$ can be updated via stochastic gradient methods with the\ngradient of\n$\\nabla_\u03c6 J_\u03c0(\u03c6) = \\nabla log \u03c0_\u03c6(a_t | s_t) + \\Big(\\nabla_{a_t} log \u03c0_\u03c6(a_t | s_t) \u2212 \\nabla_{a_t} log \u03c0_{TC}(a_t | s_t) \u2212 \\nabla_{a_t} Q_\u03b8(s_t, a_t)\\Big) \\nabla_\u03c6 f_\u03c6(\u03f5_t; s_t)$\n(29)"}, {"title": null, "content": "To mitigate the overestimation bias in the Q-values estimation, we use the clipped\ndouble Q-learning trick in our practical algorithm [17]. Specifically, we use two Q-functions\n$Q_\u03b8, \\hat{Q_\u03b8}$ that are trained independently, and two target Q-functions $\\bar{Q}, \\hat{\\bar{Q}}$. Therefore, the target\nQ-function in equation (24) is the minimum of the two target Q-functions, and the\nQ-function in equation (29) is the minimum of the two Q-functions."}, {"title": "4. The real-time system optimal traffic routing problems", "content": "Suppose a morning commuting scenario on a general road network with vertices, $v \u2208\u03bd$,\nand links, $l \u2208 L$. The origins (O) of travel demands are $g\u2208 G$, and the destinations\n(D) are $e \u2208 E$. The set of paths from an origin g to a destination e is denoted as Pge.\nFor small networks, the paths between g and e can be enumerated, so the path set Pge\ncontains all non-cyclic paths $p^i$ between g and e. For large networks, the paths cannot\nbe enumerated, so Pge contains a small portion of all possible paths. For example, each\npath set Pge includes k shortest or most frequently used paths of each OD pair, or paths\nvarying only by major alternative routes. The demands are assumed to be time-dependent\nand stochastic. The whole time horizon is divided into multiple time intervals, and each\ntime interval is denoted by t. The demand of OD pair ge during time interval t is $q_t^{ge}$,\nwhich follows a Gaussian distribution $q_t^{ge} \\sim N(\u03bc(q_t^{ge}), \u03c3(q_t^{ge}))$.\nThe environment Mincludes two components: (1) the road network, including the\ntopology of the network, and the link properties (i.e., lengths, the numbers of lanes, free\nflow speeds, and capacities); and (2) the traffic flow dynamics, which is Cell Transmission\nModel (CTM) [11, 12] and node models [40] in our study. CTM and node model essentially\ndetermine how the environment state evolves given the current state, input demands, and\ncontrol actions.\nThe objective of real-time system optimal traffic routing problems [44] is to assign all\nvehicles to paths so that the total travel time of all travelers is minimized. Since the travel\ndemands are stochastic, we cannot expect the total number of vehicles to be assigned,\nnor the number of vehicles for each path in the path set. Therefore, the control actions\nin our formulation are the assignment ratios across paths. The assignment ratio of path\npe between OD pair ge during time interval t is denoted as $k_i^{ge}$ and $\\sum_i k_i^{ge} = 1$. At the\nbeginning of each time interval t, the control algorithm decides the path ratios $k_i^{ge}$, so\nthere are $k_i^{ge} q_t^{ge}$ vehicles to be routed to the path $p^i$ during time interval t. The control\nalgorithm aims at minimizing the total travel time of all travelers within the whole time\nhorizon by determining path ratios.\nReliability or risk of control policies is also an important metric when implementing\nsystem optimal routing in the field. In traffic operations, a question of high interest is how\nbad the worst cases during RL online process are compared with the User Equilibrium\n(i.e., no-control) scenario. On top of the well-known risk measure Conditional Value at\nRisk (CVaR) which was originally proposed in finance [1] and has been popular in safe\nRL recently [7], we propose a risk measure Conditional Travel Time Reduction at Risk\n(CTTRAR) for travel time reduction problems. CVaR for total travel time is given by\n$CVaR_x(TTT) = E[TTT | TTT > VaR_x(TTT)]$\n(30)\nwhere VaRx (TTT) is the x-quantile of the distribution of total travel time (TTT). Then,\nCTTRAR is\n$CTTRAR_x(TTT) = CVaR_x(TTT^{UE}) \u2212 CVaR_x(TTT)$\n(31)"}, {"title": "4.1. Approach 1: deterministic system optimal dynamic traffic assignment as a transportation domain model", "content": "To solve the above real-time system optimal routing problems, one approach is to solve\nan optimal solution offline by leveraging traffic models and estimated demands. First, a\ntraffic model, denoted as M, is built to simulate the real environment M. The travel\ndemands, denoted as q, are estimated based on historical data and the estimated demands\nare denoted as \u1fb7. Then, one can solve a system-optimal dynamic traffic assignment\n(SODTA) on top of the traffic model and the estimated demands. Eventually, the\nsolved SODTA can suggest optimal path flows for each time interval. During the online\nexperiments, the vehicles are assigned with paths according to the pre-calculated path\nratios.\nIn this study, we implement the solution algorithm in [47] to solve SODTA. First, the\npath marginal cost is approximated, so the system-optimal paths can be identified. Then,\nthe well-known method of successive averages (MSA) can be applied to solve SODTA\niteratively. The policy obtained by solving SODTA offline is denoted as pre-DSO.\nThe performance of pre-DSO depends on the difference between M and M and the\nestimation error of \u1fb7. On if the traffic model and the estimated demands perfectly align\nwith the real case (i.e., M = M and q = q), pre-DSO is the optimal solution to the\nrouting problems. This is clearly not viable in practice. Thus, we must consider a model\nmismatch between M and M, and uncertain demands varying from estimated demand.\nAs a result, pre-DSO is no longer guaranteed to be optimal, and possibly far from being\noptimal."}, {"title": "4.2. Approach 2: model-free reinforcement learning", "content": "The real-time optimal traffic routing problem can be formulated into an MDP. Then,\nthe MDP can be solved by standard reinforcement learning with continuous action space\n(e.g., PPO [50] and SAC [18]). Here we introduce how the essential elements of the MDP\nare defined for the real-time routing problem.\nState. States of the network or environment can be defined as\n$s_t = [u_l], \u2200l \u2208 L$\n(32)\nwhere ut is the number of vehicles passing link l during time interval t.\nObservation. In realistic scenarios, the states of the whole road network cannot be\naccessed. Instead, only a portion of links is observed through sensors, and the set of\nobserved links is $\\hat{L}$. More specifically, sensors collect real-time speeds and flows and send\nthe averages during the last time interval at the beginning of the current time interval.\nThen, the control algorithm takes these real-time data (i.e., observations) as input and\naccordingly output actions. Therefore, observations of the environment are defined as\n$o_t = [t, f_l, d_l], \u2200l \u2208 \\hat{L}$\n(33)"}, {"title": null, "content": "where $f_l^t$ and $d_l^t$ are average flow and speed during time interval t on link l. t is included\nas an analogy of time of the day.\nAction. In our formulation, the control algorithm determines the path assignment\nratios, so actions are path ratios as follows.\n$a_t = [k_i^{ge}], \u2200p_i^{ge} \u2208 P^{ge}, g \u2208 G, e \u2208 E$\n(34)\nReward. The control objective of the real-time system optimal routing problems is\nto minimize the total travel time of all vehicles during the whole time horizon as follows.\n$\\underset{a_1,a_2,..., a_t,...}{\\min} \u03b7 \\sum_t \\sum_l u_l^t$\n(35)\nwhere \u03b7 is the length of each time interval. If the reward $r_t = -\u03b7 \\sum_l u_l^t$ with \u03b3 = 1, the\ncontrol objective is equivalent to the objective of reinforcement learning as follows.\n$\\underset{a_1,a_2,..., a_t,...}{\\min} \u03b7 \\sum_t \\sum_l u_l^t \\equiv  \\underset{a_1,a_2,..., a_t,...}{\\max} \\sum_t r_t$\n(36)\nHowever, we do not assume $u_l^t$ is accessible for all links. An alternative and more practical\nway is to use the number of vehicles leaving the controlled road network during each\ntime interval, which can be retrieved by monitoring the periphery of the controlled road\nnetwork. The number of leaving/finished vehicles at the end of time interval t is denoted\nas Ft and the number of vehicles within the area at the beginning of the whole time\nhorizon is denoted as No. Now, we have\n$\\underset{a_1,a_2,..., a_t,...}{\\min} \\sum_t \\sum_l u_l^t \\equiv  \\underset{a_1,a_2,..., a_t,...}{\\min} \u03b7 \\Bigg(N_0 + \\sum \\sum q_{ge} - F_t \\Bigg)$\n(37)\n$\\equiv  \\underset{a_1,a_2,..., a_t,...}{\\min} \\Bigg(\\sum_t \\Bigg(\\sum \\sum q_{ge} - F_t \\Bigg) \\Bigg)$\n(38)\n$\\equiv  \\underset{a_1,a_2,..., a_t,...}{\\max} \\sum_t F_t$\n(39)\nBased on the above reasoning, the reward function is given as\n$r_t = \\frac{F_t - \\bar{F_t}}{N}$\n(40)\nwhere $\\bar{F_t}$ is the average number of finished vehicles in the past and N is a constant\nthat approximately scales the reward into a range of [-10, 10]. We found the usage of\n$\\bar{F_t}$ improved the training of reinforcement learning as it normalizes rewards along time\nintervals, and the usage of N accelerated the training process as the reward scale is\nconstrained into a small range and thereby quicker convergence in Q-values."}, {"title": "4.3. Approach 3: TransRL", "content": "TransRL incorporates the traffic model M in section 4.1 into RL by developing a\nteacher policy based on the model-based policy pre-DSO, and then learns from the teacher"}, {"title": null, "content": "policy. Specifically, pre-DSO can output an action \u03c4(s) given a state s. We ignore the\nsubscript t here for readability.\n\u03c4(s) is not differentiable with regard to s as the pre-DSO policy is solved iteratively\nusing an intractable algorithm. This prohibits gradient propagation during training.\nTherefore, a differentiable neural network \u03bc(s) is leveraged to imitate the pre-DSO policy\nby minimizing the mean squared loss between \u03c4(s) and \u03bc(s).\nThen, the teacher policy $\u03c0_{TC}$ can be defined as a Multivariate normal distribution\n$a_{TC} \\sim \u03c0_{TC}(\u00b7 | s) = N(\u03bc(s), \u03a3)$\n(41)\nwhere the mean vector \u03bc(s) is approximately equal to \u03c4(s), and \u2211 is assumed to be a\ndiagonal matrix\n$\\sum = \\begin{bmatrix}\u03c3 & & 0\\\\ & ... & \\\\ 0 & & \u03c3\\end{bmatrix}$\n(42)\nwhere \u03c3 is the unreliability parameter to tune how much TransRL should rely on the\nmodel-based policy pre-DSO. Implicitly, as the unreliability parameter increases, TransRL\nis less dependent on the traffic model M. If the unreliability parameter is infinity, the\naction distribution is almost a uniform distribution, which means the information from\nthe model is not used at all."}, {"title": "5. Experiments", "content": "We conduct extensive experiments on three networks, including an abstractive two-\nroute network, a synthetic network, and a large network with hundreds of links. In all\nthree networks, we assume the demands are uncertain and follow Gaussian distributions\n$q_t^{ge} \\sim N(\u03bc^{ge}, \u03b2\u03bc^{ge})$, where $\u03bc^{ge}$ is the historical average demand and \u03b2 is a parameter\nof demand uncertainty level. In the synthetic network, we assume a model mismatch\nbetween the traffic model M and the real system dynamics M, and the model mismatch\nstems from the parameter estimation errors (i.e., inaccurate free-flow speeds and critical\ndensities). In the large network, the model mismatch is caused by unexpected incidents\nthat block lanes.\nOur proposed algorithm TransRL is compared with various routing strategies and\nbaselines, which are summarized as follows."}, {"title": null, "content": "\u2022 Model-based method: pre-DSO. This is the policy obtained in section 4.1, namely\nthrough System Optimal Dynamic Traffic Assignment.\n\u2022 Model-free reinforcement learning: PPO [50]. PPO is a state-of-the-art policy\noptimization algorithm. PPO updates policy by maximizing the advantages while\nusing a clip operation to make sure the updated policy does not go far from the old\npolicy.\n\u2022 Model-free reinforcement learning: SAC [18]. SAC is a state-of-the-art model-free\nreinforcement learning algorithm. SAC uses value networks to evaluate state-action\npairs and policy networks to output actions. The most notable attribute of SAC"}, {"title": "5.1. An abstractive network with uncertain demands", "content": "The first network is the abstractive 2-route network in [44]. The 2-route network is\nan abstraction of a general network during morning peak hours (see Figure 3a). After\nabstraction, the network connects the residential neighborhoods (the origin) and the\ndowntown (the destination) through two routes: (1) route 1 is the main corridor with\nhigh free-flow speed but with limited capacity, and (2) route 2 is the aggregation of all\nlocal streets with low free-flow speed but with sufficient capacity. The action is the ratio\nof demands choosing route 1.\nDuring morning peak hours, typical demands increase and then decrease as Figure 3b\nshows. At each time step, the demand follows a Gaussian distribution $q_t \\sim N(\u03bc_t, \u03b2\u03bc_t)$,\nwhere ut is the historical mean demand at time t and \u03b2 is the demand uncertainty\nparameter. In our experiments, we considered three scenarios, including a low demand\nuncertainty scenario (\u03b2 = 0.05), a medium demand uncertainty scenario (\u03b2 = 0.10), and\na high demand uncertainty scenario (\u03b2 = 0.20)."}, {"title": "5.1.1. Training of reinforcement learning", "content": "TransRL, PPO, and SAC were trained on all three scenarios with the same observation\ndefinition, action definition, and reward function. For comparison, the hyperparameters\nof each RL are the same in three scenarios. The training process is plotted in Figure\n4. There are two main findings. First, among all three scenarios, TransRL converged\nquicker than PPO and SAC, while even after convergence, returns of TransRL were higher\nthan PPO and SAC. This indicates that TransRL is more efficient in finding a good\npolicy and this policy is better than those found by PPO and SAC. In addition, the\ndifferences between the three RL decrease as the demand uncertainty level increases. This"}, {"title": null, "content": "is reasonable. As the demand uncertainty level increases, the actions from the pre-DSO\nare less close to the optimal actions because the demand estimation is less accurate. As\na result, the information from the teacher policy is less informative when the demand\nuncertainty level is larger."}, {"title": null, "content": "To shed insights on how TransRL learns from the teacher policy, the actions from\nTransRL and SAC are plotted in Figure 5. For comparison, the actions from the pre-DSO\nand DSO are also included. Given the single-peak demands, the optimal path ratio for\nroute 1 should decrease during the first half to deviate vehicles to route 2, and then\nincrease to assign more vehicles back to route 1 in general [44]. The optimal path ratios\ndepend on the specific demands. While DSO uses the actual demands to calculate the\noptimal path ratios, pre-DSO calculates the \"optimal\" path ratios using historical mean\ndemands which work as an approximation of the actual demands. As pre-DSO depends\non historical mean demands, the actions of pre-DSO are pre-calculated and deterministic\nalong different episodes and different demand scenarios. DSO uses actual demands, so the\nactions are calculated in real time. As Figure 5 shows, the difference between pre-DSO\nand DSO increases as demand uncertainty becomes high because the actual demands\ndeviate more from the historical mean demands in the high demand uncertainty scenario.\nAs a consequence, pre-DSO performs worse as demand uncertainty increases.\nCompared with SAC, TransRL is much more efficient in retrieving a policy close to\nthe optimal policy. In all three demand scenarios, it took 100 episodes that SAC learned"}, {"title": null, "content": "to decrease the action during the first 10 time steps, and it took another 300 episodes for\nSAC to learn to increase the actions during the last 10 time steps. In contrast, TransRL\nlearned the general pattern of decreasing and then increasing actions after just 5 episodes,\nwhich indicates the teacher policy boosted the learning process of TransRL. Furthermore,\nthe actions from TransRL are more stable than SAC, which indicates the policy from\nTransRL is more interpretable. Notably, different from pre-DSO whose actions are fixed,\nTransRL is adaptive to real-time states and can continuously learn from interactions."}, {"title": "5.1.2. Control performances of online tests", "content": "After the training, we tested all control methods for 100 episodes under uncertain\ndemands with the same random seed. In the no-control case (i.e., UE), the travel time\non both routes is identical so that no traveler can benefit from changing routes. For\ncomparison, we also tested the stochastic optimal method in [44], which can be regarded as\nthe optimum without knowing the actual demands in advance. [44] analytically formulates\nthe optimal routing problem of the 2-route network and solves the stochastic optimal\npolicy using dynamic programming, denoted as DP."}, {"title": null, "content": "The total travel time under different control methods is summarized in Table 1 and\nplotted in Figure 6. Compared with the no-control case, all control methods could\nsignificantly reduce the total travel time. The comparison between the model-based\nmethod (i.e., pre-DSO) and the model-free RL (i.e., PPO and SAC) is interesting. With\nlow or medium demand uncertainty, pre-DSO led to lower total travel time than PPO\nand SAC. However, when the demand uncertainty is high, SAC outperformed pre-DSO.\nThis indicates the model-based method performs well when the actual system is close to\nthe approximation, but it is brittle to information inaccuracy. While model-free RL is\nadaptive, it does not fully utilize prior information.\nNotably, in all three demand scenarios, TransRL outperformed both the model-based\nmethod and the model-free RL in reducing total travel time and was pretty close to\nthe stochastic optimum (i.e., DP). This can be attributed to TransRL combining the\nadvantages of RL and the model-based method. As an RL, TransRL is able to adapt to\nreal-time states and learn from the training experiences. On the other hand, it utilizes\nthe prior information from the model and historical mean demands.\nThe reliability of control methods is measured using the proposed metric CTTRaRx,\nwhich is listed in Table 2. Higher CTTRAR indicates more reliability in worst cases.\nWith low demand uncertainty, pre-DSO is more reliable than SAC, but SAC is more\nreliable than pre-DSO when demand uncertainty is medium or high. Except for the\nstochastic optimum (DP), TransRL is the most reliable with low or medium demand\nuncertainty, and SAC is the most reliable with high demand uncertainty. This indicates\nthat when demand uncertainty is high, a more stochastic policy is more reliable. Overall,\nTransRL is more reliable than pre-DSO, PPO, and SAC."}, {"title": "5.1.3. Sensitivity analysis of the unreliability parameter", "content": "In TransRL, the unreliability parameter \u03c3 determines how much the TransRL should\ndepend on the transportation method and how extensive TransRL should explore the\naction space. To look into the impacts of the unreliability parameter, we compared the\ntraining processes and the control performances of TransRL with different unreliability\nparameters.\nThe training processes with different unreliability parameters under three demand\nscenarios are plotted in Figure 7. In general, the training process was insensitive to the\nunreliability parameter. After convergence, different parameters led to similar perfor-\nmances. In low or medium demand uncertainty scenarios, the value of the unreliability\nparameter did affect the convergence speed, and smaller unreliability parameters led to\nfaster convergence. While in the high demand uncertainty, the convergence speed was\nalmost identical along different parameters. The control performances with different\nparameters align with this phenomenon. As Figure 8 shows, the control performances\nwere generally incentive to the parameters, and small unreliability parameters performed\nbetter than large unreliability parameters in low or medium demand uncertainty scenarios."}, {"title": "5.2. A synthetic network with uncertain demands and model mismatch", "content": "The second network for experiments is a synthetic corridor network first used in [39].\nAs Figure 9 shows, the corridor network, which aims to abstract a commuting network, is\ncomposed of 18 links and 16 nodes. Links 1-2, 2-4, 4-6, 6-8, and 8-10 are freeway links, so\nthese links has high free-flow slow and large capacity. In addtion, link 6-8 is a bottleneck\nwith smaller capacity caused by a lane-drop. Links 3-5, 5-7, and 7-9 are arterial links\nwith lower free-flow speed than freeway links. Links 3-2, 4-5, 7-6, and 8-9 are on-ramp or\noff-ramp connecting freeway links with arterial links. All the other links are local streets\nconnecting nodes with arterial links or freeway links.\nWe consider many-to-one OD pairs and each OD pair has a typical single-peak\ndemand pattern. Specifically, node 11 and node 12 are the origin nodes, while node 15\nis the destination node. At each time step, the demand follows a Gaussian distribution\n$q_t^{ge} \\sim N(\u03bc_t^{ge}, \u03b2\u03bc_t^{ge})$, where $\u03bc_t^{ge}$ is the historical mean demand of OD pair ge at time t\nand \u03b2 = 0.10 is the demand uncertainty parameter. Between each OD pair, there are\nmultiple routes composed of freeway links and/or arterial links to choose from.\nIn this network, we also consider a model mismatch between the actual system\ndynamics (M) that is unknown in practice, and the approximated/estimated traffic model\n(M). The network is modeled by CTM, and CTM cells are defined by length, the number\nof lanes, free-flow speed, capacity, and jam density. In the real world, the estimated\nfree-flow speed and capacity may differ from the actual unknown values, which causes a"}, {"title": "5.2.1. Training of reinforcement learning", "content": "The training processes of PPO and SAC are plotted in Figure 10. PPO was struggling\nto find a reasonable policy and did not converge after 2000 episodes. While SAC converged\nafter approximately 500 episodes, the performance was pretty fluctuating even after the\nconvergence. The performance of PPO and SAC indicates the increase in state and action\nspace makes model-free reinforcement learning less efficient than learning from random\nexploration.\nTransRL with different unreliability parameters is also compared in Figure 10. With\nvarious unreliability parameters, TransRL learned faster and reached a more stable\nperformance than PPO and SAC. Besides, comparing TransRL with \u03c3 = 0.05 and\nTransRL with \u03c3 = 0.10, we can find TransRL with \u03c3 = 0.05 converged faster than\nTransRL with \u03c3 = 0.10, but TransRL with \u03c3 = 0.10 ended with a slightly higher return\nthan TransRL with \u03c3 = 0.10. This indicates a trade-off between the convergence speed\nand the final performance. When \u03c3 = 0.20, the TransRL was similar to SAC, which\nsupported Theorem 1 claiming SAC is equivalent to TransRL with a teacher policy of\nuniform distribution (i.e., \u03c3 = \u221e in this case)."}, {"title": "5.2.2. Control performances of online tests", "content": "We tested trained RL and baselines for 100 test episodes with the same uncertain\ndemands and model mismatch. The resultant total travel time is summarized in Table\n3 and also plotted in Figure 9. Though demands and the traffic model used in the\nmodel-based method (pre-DSO) are different from the actual demands and actual system\ndynamics, pre-DSO outperformed both model-free RL (SAC and PPO) in reducing total\ntravel time. The average total travel time of PPO was even higher than the no-control\nbaseline.\nWith all three unreliability parameters, TransRL led to a lower average total travel\ntime than PPO and SAC. When the unreliability parameter \u03c3 of TransRL is 0.05 or 0.10,\nthe average total travel time of TransRL was lower than pre-DSO. When \u03c3 = 0.20, the\naverage total travel time of pre-DSO was lower than TransRL. In summary, TransRL\nwith \u03c3 = 0.10 got the best control performance. The results also indicate careful tuning\nof the unreliability parameter can further enhance the performance of TransRL.\nThe reliability results of control methods on the corridor network are included in\nTable 4. CTTRAR of PPO and SAC are negative, which means, in the worst cases,\nPPO and SAC perform worse than the no-control baseline. Both pre-DSO and TransRL\nare reliable as CTTRaRx is high. Interestingly, while TransRL (\u03c3 = 0.10) leads to the\nlowest average total travel time, TransRL (\u03c3 = 0.05) is the most reliable. Results on\nthis network indicate that, even when model-free RL is not very reliable, TransRL is still"}, {"title": null, "content": "reliable because of the action constraints from the teacher policy."}, {"title": "5.3. A large network with uncertain demands and traffic model mismatch", "content": "The third network we experiment on is the Transportation Systems Management and\nOperations (TSMO) # 1 network in Maryland, US. TSMO network contains a freeway\nI-70 and multiple US routes. As Figure 12 shows, there are 621 links, 361 nodes, and 182\nOD pairs on the TSMO network. During morning peak hours, most travelers travel from\nwest to east or south, and the eastbound of the I-70 is recurrently congested.\nTo estimate the demands of the TSMO network, we adopted the Dynamic OD demand\nEstimation"}]}