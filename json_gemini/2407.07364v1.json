{"title": "Real-time system optimal traffic routing under uncertainties\nCan physics models boost reinforcement learning?", "authors": ["Zemian Ke", "Qiling Zou", "Jiachao Liu", "Sean Qian"], "abstract": "System optimal traffic routing can mitigate congestion by assigning routes for a portion\nof vehicles so that the total travel time of all vehicles in the transportation system can\nbe reduced. However, achieving real-time optimal routing poses challenges due to uncer-\ntain demands and unknown system dynamics, particularly in expansive transportation\nnetworks. While physics model-based methods are sensitive to uncertainties and model\nmismatches, model-free reinforcement learning struggles with learning inefficiencies and\ninterpretability issues. Our paper presents TransRL, a novel algorithm that integrates\nreinforcement learning with physics models for enhanced performance, reliability, and\ninterpretability. TransRL begins by establishing a deterministic policy grounded in\nphysics models, from which it learns from and is guided by a differentiable and stochastic\nteacher policy. During training, TransRL aims to maximize cumulative rewards while\nminimizing the Kullback-Leibler (KL) divergence between the current policy and the\nteacher policy. This approach enables TransRL to simultaneously leverage interactions\nwith the environment and insights from physics models. We conduct experiments on\nthree transportation networks with up to hundreds of links. The results demonstrate\nTransRL's superiority over traffic model-based methods for being adaptive and learning\nfrom the actual network data. By leveraging the information from physics models, Tran-\nSRL consistently outperforms state-of-the-art reinforcement learning algorithms such as\nproximal policy optimization (PPO) and soft actor-critic (SAC). Moreover, TransRL's\nactions exhibit higher reliability and interpretability compared to baseline reinforcement\nlearning approaches like PPO and SAC.", "sections": [{"title": "1. Introduction", "content": "Traffic congestion in urban areas is one of the plagues of citizen's everyday life and it can\ncost huge economic loss [23]. According to a report from Federal Highway Administration\n1(FHWA), \u201croughly half of the congestion experienced by Americans happens virtually\nevery day\". This type of congestion is generated by unbalanced temporal and spatial\ndistribution of traffic activities. Recent studies [10, 26] have shown that congestion"}, {"title": null, "content": "can be significantly mitigated by optimally guiding route choices of a small portion\nof travelers. The system optimal routing aims to minimize the total system cost by\nassigning routes for some travelers. There are many ways to practically implement and\nguide traffic routing. For instance, congestion toll [62, 8] changes users' routing choice\nby changing their perceived cost. Variable traffic signs (VMS) provide real-time traffic\ninformation using road-side signals like LED signs [15, 16, 5, 22] to affects users' routing\nchoice. Route guidance and driver information systems (RGDIS) [2] directly provide\nroute recommendations to users using in-vehicle devices such as infoentertainment system\nor cell phone applications. However, solving system-level optimal routing in real-time for\nlarge networks remains a big challenge.\nThe optimal routing in real-time for large networks is challenging for three reasons.\nFirst, demands are uncertain. Travel demands stem from human activities that may have\nrandomness, so it is almost unlikely to predict the travel demands accurately. Demands\nessentially affect traffic conditions, and the difference between the actual demands and\nthe estimated demands can cause nontrivial estimation errors in traffic conditions. For\nexample, [46] showed that congestion duration can change significantly with an incremental\nchange in demands. Therefore, it is important to consider the demand uncertainties. From\na theoretical perspective, [55] showed that using expected demands tends to overestimate\nthe network performance in traffic assignment evaluation. [13] also stated it is important\nto consider demand uncertainty for the congestion pricing problems. In this study, we\nexplicitly include the demand uncertainty and regard demands as random variables that\ncannot be predicted precisely in advance.\nSecond, system dynamics may not be modeled perfectly. Though extensive traffic\nflow models (e.g., the celebrated LWR model [31, 49], cell transmission models [11, 12],\nand link queue model [24]) have been proposed to approximate the system dynamics\nof transportation networks, no known flow model can precisely replicate the flow in\nnetworks consisting of both roads and intersections. Each model is associated with\nvarious assumptions and possible parameter estimation errors. For example, conventional\nkinematic models ([31, 49, 11, 12, 24]) all assume a deterministic fundamental diagram\nto depict the relation between density and flow for mathematical tractability, while [48]\nstates this relation can be better modeled by stochastic models. In addition, estimation\nof the deterministic fundamental diagram parameters is not always accurate due to\nthe randomness of traffic flow and transient flow state transitions. In this study, we\nacknowledge that the traffic flow model cannot be known precisely. Rather, there is a\nmodel mismatch between the assumed/adopted traffic models and the actual true system\ndynamics, and this mismatch is unkonwn in advance.\nThird, real-time system optimal traffic routing is challenging especially in large\nnetworks due to complex interplay among traffic flow of various origins and destinations.\n[44] proposed a stochastic optimal real-time routing method for a two-route network. By\nabstracting a general commuting network into a two-route network, the analytical solution\ncan be derived and approximated using dynamic programming. However, deriving the\nanalytical solution of optimal traffic routing (i.e. path flow) for a large network is almost\ninfeasible because the complexity of the problem and the dimension of decision variables\nincrease exponentially as the network size. To that end, machine learning methods, like\nreinforcement learning, could be inefficient and even infeasible. The state space and\naction space increase exponentially as the network size increases, so finding an optimal\npolicy from the exploded search space without any prior knowledge is inefficient and"}, {"title": null, "content": "sometimes even infeasible. To our best knowledge, though [29] leveraged reinforcement\nlearning to solve real-time optimal routing on networks with up to 41 links. Real-time\noptimal routing has not been solved by reinforcement learning on networks with more\nthan hundreds of links. This study experiments on three networks with 2 links, 18 links,\nand 621 links, respectively.\nCurrent methods for real-time optimal routing or general traffic control can be\ncategorized into heuristic methods, model-based methods, and reinforcement learning-\nbased methods. Heuristic methods use rule-of-thumb or feedback control to correct\ncontrol errors to maintain desired system states. [25] formulated the real-time routing\nproblem as a feedback control problem, and a feedback linearization technique is used\nto achieve a user equilibrium state. [43] adopted a fuzzy control approach to determine\nreal-time routing strategies to improve the network performance. Heuristic methods are\ninterpretable and easy to implement, but they are reactive and only take action when\ncontrol errors arise. Therefore, their performance is almost necessarily suboptimal.\nModel-based methods typically first develop a traffic model or simulation to simulate\nthe system dynamics, and then solve the optimal solutions through either heuristic\nalgorithms (e.g., moving successive averages (MSA)) or deriving analytical closed-form\nsolutions. For example, [54] used autoregressive integrated moving average (ARIMA) to\npredict demands, a binary logit model to predict traveler choices, and CTM to predict\ntraffic flows. Based on these predictions, an optimization algorithm was adopted to set\ntolls to affect routing choices to optimize the objective function. [53] proposed a model\npredictive control method for dynamic pricing to reduce the total traveler delay. Similar\nto [54], [53] used a logit choice model to predict traveler choices and CTM to predict\ntraffic flows. Model-based methods are interpretable and can be proven to be optimal\nin theory. However, the inevitable model mismatch between the models and the actual\nsystem dynamics intrigues their performances in practice.\nReinforcement learning (RL) gained popularity in the traffic control domain because\nit's model-free and can learn optimal policies directly from data or the environment. [41]\nconsidered a practical control scenario with multiple origins and destinations, partially\nobserved network states, and stochastic demands. The problem was formulated as\na partially observable Markov decision process (POMDP) and solved using RL. The\nexperiments in [41] showed RL outperformed feedback control. [29] considered a scenario\nwhere route choices of autonomous vehicles can be fully controlled to improve network\nefficiency, and the policy was learned using reinforcement learning. The experiments on\nsimplified networks indicated the learned RL policy realized performances on par with the\nsystem optimum. However, to our best knowledge, RL has not been tested for optimal\nrouting on large networks with more than hundreds of links. With the well-known curse of\ndimensionality, whether RL can solve optimal policies in large networks remains unknown.\nIn this study, the reliability and interpretability of the RL policies are the focus for\nits realism in real-world traffic operation practice. Most RL policies are not deterministic\nand the traffic environment is stochastic in nature, and it is clear that the performances\nof RL policies in practice are subject to variability. When implementing RL policies, one\nquestion out of interest is how poor the performance of RL policies can possibly be in the\nworst cases. In reality, traffic operation may not tolerate a poor performance for just one\nday, unlike RL applications in other domains, e.g. robotics, that may accept some time of\nonline improvement through learning. In this work, we propose Conditional Travel Time\nReduction at Risk (CTTRAR) to measure the reliability of a control method. CTTRAR"}, {"title": null, "content": "compares the total travel time of the worst cases with a control method and the total\ntravel time of the no-control scenarios. RL policies are regarded as reliable if the worst\nperformance is still better than the no-control scenarios. In addition, the interpretability\nof the RL policies is a concern for practitioners. The objective of RL is to maximize\nthe cumulative rewards with no specific constraints for the output actions. As a result,\nthough RL leads to high cumulative rewards, the actions of RL might fluctuate over\ntime, which makes RL policies less interpretable. Our general idea is to constrain the\nRL policies to not go too far from an interpretable \"teacher\" policy. For example, system\noptimum routing under traffic flow dynamics models can serve as a teacher policy. Those\nflow dynamics models may not be precisely calibrated (e.g., in a simulation environment)\nor the demand is stochastic, but its flow propagation mechanisms and the analytical\nsolution can help to guide and learn interpretable RL policies.\nMoreover, it is unclear when RL is superior to model-based methods and vice versa.\nA model-based method is extensively compared with model-free RL methods to shed light\non the performance between model-based methods and learning-based methods. The\nresults indicate RLs outperform the model-based method when the demand uncertainty\nis large or/and the model mismatch is significant, and vice versa. When the network\nsize is large, RLs sometimes struggle to find a reasonable policy that works precisely for\nthe unknown or unexplored system states and dynamics, but model-based can perform\nwell especially when the model mismatch is insignificant. With the trade-off between the\nmodel-based method and model-free RL, one interesting question is can we combine the\nadvantages of both types of methods?\nWe propose a general reinforcement learning framework that can couple different types\nof well-established transportation methods (e.g., heuristics, model-based, or machine\nlearning-based) with RL. In this study, we focus on an example of coupling the model-based\nmethod with RL. Our proposed reinforcement learning framework differentiates from most\nprevious reinforcement learning algorithms including DDPG [50], PPO [50], and SAC [18]\nby leveraging information from transportation methods, named Transportation-informed\nReinforcement Learning (TransRL). TransRL is able to learn from the environment\nand the traffic model simultaneously. On one hand, even with a model mismatch, the\ninformation from the models is not unuseful and can help RL narrow down the search\nspace. On the other hand, with the ability to learn from the environment directly, RL is\nable to learn a better policy than the model-based method by implicitly correcting the\nmodel mismatch between the models and the actual system dynamics.\nThe contributions of this paper are summarized as follows.\n\u2022 We solve real-time system optimal routing in sizable transportation networks. The\nmodel is based on a realistic setting where only a few links are observed and a\nportion of vehicles can be influenced with their respective routing guidance.\n\u2022 We relax the assumptions of known travel demands and accurate traffic models in\nother RL models. Instead, travel demands are assumed to follow a time-dependent\nGaussian distribution with means of historical average demands. Moreover, there\nare model mismatches between the offline trained accessible traffic models and\nthe online test (unknown and true) system dynamics, which stem from lack of\nknowledge, model estimation errors, or unexpected incidents in real world.\n\u2022 Model-free RLs are compared with a traffic model-based method under various"}, {"title": null, "content": "levels of model mismatches. This comparison provides insights into when a traffic\nmodel-based method is superior and when RLs are preferable.\n\u2022 Ultimately, we proposed a novel RL framework TransRL that combines RL with\ntraffic models. With the ability to learn from the environment and traffic models\nsimultaneously, TransRL learns more efficiently than model-free RLs and is more\nadaptive than the traffic model-based method. More importantly, the actions of\nTransRL are more reliable and interpretable than model-free RLs.\n\u2022 Reliability and interpretability of TransRL are the focus. In order to use TransRL\nin practice, its performance at any stage throughout the process of online learning\ncannot go below the network performance without any traffic control/management\nmeasures. TransRL's policies are also designed to ensure they approximately follow\nthe guidance of model-based optimal flow solutions, which ensure its interpretability.\nThe rest of the paper is organized as follows. Literature related to vehicle routing\nand RL-based control methods is discussed in section 2. In section 3, we introduce our\nframework TransRL and show it can be proved to converge under conditions of finite\ndiscrete state and action space. We subsequently elaborate on how TransRL can be trained\nin continuous state space and action space by using neural networks to approximate\nvalue functions and policies. The problem formulation of the real-time optimal routing\nproblems is included in section 4. Section 4 then presents how to solve the optimal routing\nproblems using a model-based method, model-free RL, and the proposed TransRL. The\nconsidered methods are compared with various experiment settings on three networks in\nsection 5. Section 6 concludes our findings and suggests potential directions for future\nwork."}, {"title": "2. Related works", "content": "Quite a few literatures investigated providing real-time route information to travelers\nthrough variable message signs (e.g., Messmer et al. [37], Wunderlich et al. [58]), and in-\nvehicle routing mechanisms [4]. However, assigning routes or providing route information\nto improve the network performance is less studied. [3] used cooperative distributed\nmulti-agent systems to explore the interactions between route information providers and\ntravelers, and it was found that negotiation between information providers and travelers\ncan improve the network performance. [43] studied affecting traveler routing behaviors\nvia providing real-time routing information, and a fuzzy control approach was proposed\nto determine the information strategy in order to enhance the network performance. [14]\nproposed a coordinated online in-vehicle routing scheme with intentional information\nprovision perturbation (CRM-IP), which leverages the bounded rationality of travelers to\nshape traveler routing behaviors so that the system optimality and user optimality are\nbalanced. [29] studied a scenario where route choices of autonomous vehicles can be fully\ncontrolled in a centralized manner to improve network performance. The policy learned\nby RL performed close to a theoretically optimal solution in networks composed of up to\ndozens of links.\nIn the general traffic control domain, various analytical methods have been proposed,\nsuch as dynamic programming for optimal routing on a 2-route network [44], max-pressure\nfor signal control [60, 61, 59], and feedback control for ramp metering [56, 65], to name a"}, {"title": null, "content": "few. The analytical methods develop control policies through mathematical derivations\non the top of model assumptions. These derived control policies often show desirable\nproperties, such as convergence guarantee (i.e., reliability) and interoperability. However,\nreal environments may change over time. Because of a lack of learning ability, these\ncontrol policies cannot automatically adapt to changing environments, which deteriorates\ncontrol performances.\nReinforcement learning algorithms have gained popularity in solving real-time control\nproblems because of the characteristics of being model-free and learnable while capable of\ntaking proactive optimal actions even in uncertain environments [21, 42]. RL algorithms\nhave been extensively applied to various traffic control problems, including vehicle routing\n[28, 51], autonomous driving [64, 66], traffic signal control [45, 30], ramp metering [63],\nvariable speed limit control [27], and congestion pricing [41]. RL used in transportation\ncan be categorized into model-free RL (e.g., [30, 21]) and RL with prior knowledge (e.g.,\n[27, 19, 20, 9, 6]). Model-free RL learns from scratch by interacting with the environment,\nso the RL agent explores the search space by mostly taking random actions during the\nearly stage of the training. As a consequence, the performance of model-free RL is not\nguaranteed and is even worse than the non-control case before the convergence. This is\nparticularly problematic in the traffic operation domain, because any traffic management\nmeasure cannot afford to under-perform under general public's expectation, even for\njust a few days. In other words, practically it would be impossible to take a number of\ndays before RL starts to show system benefits. Our paper can help address this issue by\nguiding RL online learning with policies derived from physics models.\nCompared with model-free RL, RL with prior knowledge is more data-efficient by\nutilizing prior knowledge to narrow down the search space or start with a policy better\nthan random initialization. [27] utilized transfer learning to transfer a policy trained\non a source scenario to multiple target scenarios. Though the fundamental diagrams\nin the target scenarios differed from those in the source scenario, the transfer learning\nsignificantly shortened the training process in the target scenarios. [19, 20] augmented field\ndata using traffic flow models. From the augmented data, RL was trained offline, and then\nthe learned policies were implemented to acquire new field data. This process repeated\nsuch that RL kept learning from the environment. [6] designed a hybrid reinforcement\nlearning framework that combines a rule-based strategy and reinforcement learning to\noptimize the eco-driving strategy. Most of the time, the vehicle is controlled by RL policies.\nWhen the stop warning is activated, the rule-based strategy will take control and ensure\nthe vehicle stops safely. [9] proposed an RL-based framework for end-to-end autonomous\ndriving. While learning to behave optimally, the proposed framework also learns a latent\nenvironment model that predicts the state of the environment in a low-dimensional latent\nspace. The latent model greatly reduces the sample complexity by learning the latent\nstates of the high-dimensional observations. Though RL has been extensively studied in\nengineering problems (e.g., [33, 34, 35]), the actions of RL policies are not constrained to\nimprove the reliability and interpretability of the RL policies."}, {"title": "3. TransRL", "content": null}, {"title": "3.1. Preliminaries for RL", "content": "The problem studied in RL can be formulated as a Markov decision process (MDP).\nThe state space and the action space are denoted as S and A. After applying an action"}, {"title": null, "content": "at \u2208 A at current state st \u2208 S, the next state st+1 \u2208 S of the environment is determined\nby a state transition probability function p(st+1 | st, at), and a reward rt is produced\nby the environment. The return is the discounted sum of rewards in the whole horizon\n$\\$\\sum_\\tau \\gamma^t r_t\\$t, where y is a discount factor making a trade-off between short-term rewards and\nlong-term rewards. A policy \u03c0(at | st) determines the probability distribution of actions\nat given a state st. We denote state-action marginals induced by a policy \u03c0(at | st) as\n$\\rho_\\pi(s_t, a_t)\\$t.\nThe objective of reinforcement learning is to find an optimal policy \u03c0* such that the\nexpected return is maximized."}, {"title": null, "content": "$\\pi^* = \\arg \\max_\\pi \\mathbb{E}_{(s_t, a_t) \\sim \\rho_\\pi} \\bigg[ \\sum_t \\gamma^t r_t \\bigg]\\$t\n(1)\nThis study focuses on actor-critic methods [17, 32], which are a combination of Q-\nlearning [57] and policy optimization. Generally, actor-critic methods use a critic network\nparameterized as 0 to estimate the expected return of state-action pairs with a policy\n\u03c0 (i.e., Q-values $Q_\\theta(s_t, a_t)\\$t), and an actor network parameterized as \u03c6 to output actions\ngiven states (i.e., $\\pi_\\varphi(a_t | s_t)\\$t).\nBoth the actor network and the critic network are trained using data from an experience\nreplay buffer containing transitions (st, at, st+1,rt). The actor network $\\pi_\\varphi(a_t | s_t)\\$t is\nupdated by gradient descent with a loss function based on Q-values."}, {"title": null, "content": "$J_\\pi(\\varphi) = \\mathbb{E}_{a_t \\sim \\pi_\\varphi} \\big[ -Q_\\theta(s_t, a_t) \\big]\\$\n(2)\nThe critic network is updated by the temporal difference error based on the Bellman\nequation, and the loss function for the critic network is given by"}, {"title": null, "content": "$J_Q(\\theta) = \\mathbb{E} \\bigg[ \\bigg( Q_\\theta(s_t, a_t) - \\big( r_t + \\gamma ( Q_{\\bar\\theta}(s_{t+1}, a_{t+1}) ) \\big) \\bigg)^2 \\bigg], \\; a_{t+1} \\sim \\pi_\\varphi(s_{t+1})\\$\n(3)\nwhere $Q_{\\bar\\theta}\\$Q and $Q_{\\bar\\varphi}\\$Q are a target critic network and a target actor network respectively,\nwhich are used to stabilize the training. The actor and critic networks are updated at\nevery learning step, while the target actor and critic networks are updated periodically\nby copying the weights of the actor and critic networks respectively."}, {"title": "3.2. TransRL models", "content": "Our proposed reinforcement learning framework differentiates from most previous\nreinforcement learning algorithms (e.g., DDPG [50], PPO [50], and SAC [18]) by leveraging\ninformation from transportation methods, named Transportation-informed Reinforcement\nLearning (TransRL). First, a neural network is used to learn a stochastic and differentiable\nteacher policy from a well-established transportation method. Then, during the training\nor testing of TransRL, aside from interactions with the environment, TransRL also learns\nfrom the teacher policy by comparing the current policy with the teacher policy. The\ndivergence between the two policies, combined with rewards, is passed to TransRL for\nlearning. The whole process is illustrated in Figure 1."}, {"title": "3.2.1. Teacher policy", "content": "We assume there exist well-established transportation models and methods that can\noutput actions. The choice of transportation domain models can be very flexible, and it\ncan be traffic model-based, heuristics, or rule-based methods. Compared with general\nlearning-based methods, which work like a black box, transportation domain models\nare more reliable and interpretable as they incorporate domain knowledge and physics\ninformation that would guide the RL agent to learn effectively.\nThen, we develop a stochastic and differentiable teacher policy based on a transporta-\ntion domain model. The teacher policy is a probability distribution for actions given a\nstate. The action distribution concentrates on the action output by the transportation\ndomain model. The concentration level of the distribution can be tuned by an unreliability\nparameter o. Figure 2 shows a simple case with an action dimension of 1 and the range\nof action is [0, 1] where the action represents the portion of vehicles to be diverted from\nthe most preferred route. As Figure 2 shows, a smaller unreliability parameter leads to\na distribution more concentrated on the action from the deterministic transportation\ndomain model. Essentially, the teacher policy is a prior distribution of actions for states.\nWe will provide an example of a teacher policy in the later section 4.3."}, {"title": "3.2.2. The augmented objective function", "content": "To enable TransRL's learning from the teacher policy, the objective of TransRL\nis to maximize the cumulative rewards while minimizing the differences between the\ndivergence between the learned policy and the teacher policy. Therefore, the objective\nfunction of TransRL is different from the objective function of most previous reinforcement\nlearning algorithms as equation (1) shows. This novel objective fundamentally changes\nthe exploration behaviors and the learning process of the reinforcement learning agent.\nSpecifically, the objective function of TransRL is given by"}, {"title": null, "content": "$J(\\pi) = \\mathbb{E}_{(s_t, a_t) \\sim \\rho_\\pi} \\bigg[ \\sum_t \\gamma^t \\big(r_t - \\alpha D (\\pi(\\cdot | s_t) || \\pi_\\tau(\\cdot | s_t)) \\big) \\bigg]\\$\n(4)\nwhere a is the temperature parameter that makes a trade-off between the reward and\nthe divergence term. The choice of divergence function is relatively flexible. Without\nloss of generality, we adopt the Kullback-Leibler (KL) divergence. Then, the augmented"}, {"title": null, "content": "objective becomes\n$J(\\pi) = \\mathbb{E}_{(s_t, a_t) \\sim \\rho_\\pi} \\bigg[ \\sum_t \\gamma^t \\big(r_t - \\alpha D_{KL} (\\pi(\\cdot | s_t) || \\pi_\\tau(\\cdot | s_t)) \\big) \\bigg]\\$\n(5)\n$= \\mathbb{E}_{(s_t, a_t) \\sim \\rho} \\bigg[ \\sum_t \\gamma^t \\big(r_t + \\alpha H (\\pi(\\cdot | s_t)) + \\alpha \\mathbb{E}_{a \\sim \\pi(\\cdot | s_t)} [log \\pi_\\tau(a | s_t)] \\big) \\bigg]\\$\n(6)\nTheorem 1. The maximum entropy reinforcement learning SAC [18] is equivalent to\nTransRL with a teacher policy of uniform distribution.\nProof. If \u03c0\u03c4c is a uniform distribution, $$\\pi_{\\tau}(a | s_t)$$\n\u03c0 is a constant regardless of a, so\n$\\mathbb{E}_{a \\sim \\pi(\\cdot | s_t)} [log \\pi_{\\tau}(a | s_t)]\\$t\n$\\$\\mathbb{E}_{a \\sim \\pi(\\cdot | s_t)} [log \\pi_{\\tau}(a | s_t)]\\$t is also a constant regardless of \u03c0. As a result, we can remove\n$\\mathbb{E}_{a \\sim \\pi(\\cdot | s_t)} [log \\pi_{\\tau}(a | s_t)]\\$t from the objective function (6), and then it becomes\n$J(\\pi) = \\mathbb{E}_{(s_t, a_t) \\sim \\rho} \\bigg[ \\sum_t \\gamma^t (r_t + \\alpha H (\\pi(\\cdot | s_t))) \\bigg]\\$\n(7)\nwhich is the objective of SAC [18].\nRemark 1. Including the KL divergence term in the objective results that TransRL\nbecomes more reliable and interpretable than classical RL. With this divergence constraint,\nthe policy does not go too far away from the teacher policy \u03c0\u03c4C that is assumed to be\nreliable and interpretable."}, {"title": "3.2.3. Policy iteration", "content": "The optimal policy is solved using policy iteration. The policy iteration includes, (1)\nthe policy evaluation step where the value functions are estimated given a policy, and\n(2) the policy improvement step where the policy is updated to increase the values given\nthe value functions. By repeating the policy evaluation and the policy improvement, the\npolicy is guaranteed to converge to one of the optimal policies that maximize values.\nTransRL uses auxiliary Q-values according to the augmented objective function such\nthat maximizing auxiliary Q-values is equivalent to maximizing the augmented objective.\nThe auxiliary Q-values are given by\n$Q(s_t, a_t) = r_t + \\mathbb{E}_{s_{t+1} \\sim p} [V(s_{t+1})]$\n(8)\nwhere"}, {"title": null, "content": "$V(s_{t+1}) = \\mathbb{E}_{a_{t+1} \\sim \\pi} \\bigg[ Q(s_{t+1}, a_{t+1}) + \\alpha log \\bigg(\\frac{\\pi_{\\tau}(a_{t+1} | s_{t+1})}{\\pi(a_{t+1} | s_{t+1})} \\bigg) \\bigg]$\n(9)\nis the auxiliary state value.\nFor the policy evaluation, The auxiliary Q-values are updated iteratively by a modified\nBellman equation\n$Q^{k+1}(s_t, a_t) \\leftarrow r_t + \\gamma \\mathbb{E}_{s_{t+1} \\sim p} [V^k(s_{t+1})]$\n(10)\nWith the above update rule, the auxiliary Q-values can be proved to converge to the\nunique auxiliary Q-values of the policy as follows."}, {"title": null, "content": "Lemma 1 (Policy evaluation convergence). Starting with a initial auxiliary Q-values\nfunction Q: S \u00d7 A \u2192 R with |A| < \u221e, update $Q^0\\$0 iteratively using equation (10). As\nk\u2192\u221e, $Q^k\\$k converges to the unique auxiliary Q-values function of \u03c0.\nProof. Let's define an augmented reward"}, {"title": null, "content": "$r(s_t, a_t) = r_t(s_t, a_t) + \\gamma \\alpha \\mathbb{E}_{a_{t+1} \\sim \\pi} log \\bigg( \\frac{\\pi_{\\tau}(a_{t+1} | s_{t+1})}{\\pi(a_{t+1} | s_{t+1})} \\bigg)$\n(11)\nThen, equation (10) becomes\n$Q^{k+1}(s_t, a_t) \\leftarrow r(s_t, a_t) + \\mathbb{E}_{s_{t+1} \\sim p, a_{t+1} \\sim \\pi} [Q^k (s_{t+1}, a_{t+1})]$\n(12)\nAs |A| < \u221e, the KL divergence in equation (11) is bounded, so r(st, at) is bounded.\nThen, one can apply the contraction mapping theorem to prove $Q^k\\$k converge to the unique\nauxiliary Q-values, which is the same as the proof for standard reinforcement learning\n[52].\nFor the policy improvement, the policy is updated such that the auxiliary Q-values of\nthe new policy are higher than the old policy. Similar to [18], we use a KL divergence\nbetween the old policy and the exponential of the auxiliary Q function, and the new\npolicy is obtained by minimizing the KL divergence as follows."}, {"title": null, "content": "$\\pi_{new} = \\arg \\min_{\\pi \\in \\Pi} D_{KL} \\bigg(\\pi(\\cdot | s_t) || \\frac{\\pi_{\\tau}(s_t) e^{\\frac{Q(s_t, a)}{a}}}{Z_{old}(s_t)} \\bigg)$\n(13)\nwhere Zold(st) is the partition function ensuring the right part within the KL divergence\nis a probability distribution. Though Zold(st) is intractable and infeasible for large state\nspaces, it is a constant and can be ignored when calculating gradients with respect to the\npolicy. With this policy update rule, we can prove the new policy is better than the old\npolicy with respect to auxiliary Q-values as follows."}, {"title": null, "content": "Lemma 2 (Policy improvement convergence). For \u03c0old \u2208 \u03a0, \u03c0new is the optimal\nsolution of the problem defined in equation (13). With |A| < \u221e, $Q^{new} (s_t, a_t) \\geq\nQ^{old} (s_t, a_t), \\; \\forall(s_t, a_t) \\in S \\times A\\$A.\nProof. Denote the KL divergence term in equation (13) as"}, {"title": null, "content": "$J_{\\pi_{old}} (\\pi) = D_{KL} \\bigg(\\pi(\\cdot | s_t) || \\frac{\\pi_{\\tau}(s_t) e^{\\frac{Q(s_t, a)}{a}"}]}