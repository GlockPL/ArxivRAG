{"title": "Beyond Benchmarks:\nOn The False Promise of AI Regulation", "authors": ["Gabriel Stanovsky", "Renana Keydar", "Gadi Perl", "Eliya Habba"], "abstract": "The rapid advancement of artificial intelligence (AI) systems in critical domains like\nhealthcare, justice, and social services has sparked numerous regulatory initiatives aimed\nat ensuring their safe deployment. Current regulatory frameworks, exemplified by recent\nUS and EU efforts, primarily focus on procedural guidelines while presuming that scientific\nbenchmarking can effectively validate AI safety, similar to how crash tests verify vehicle safety\nor clinical trials validate drug efficacy. However, this approach fundamentally misunderstands\nthe unique technical challenges posed by modern AI systems. Through systematic analysis\nof successful technology regulation case studies, we demonstrate that effective scientific reg-\nulation requires a causal theory linking observable test outcomes to future performance - for\ninstance, how a vehicle's crash resistance at one speed predicts its safety at lower speeds.\nWe show that deep learning models, which learn complex statistical patterns from training\ndata without explicit causal mechanisms, preclude such guarantees. This limitation renders\ntraditional regulatory approaches inadequate for ensuring AI safety. Moving forward, we call\nfor regulators to reckon with this limitation, and propose a preliminary two-tiered regulatory\nframework that acknowledges these constraints: mandating human oversight for high-risk ap-\nplications while developing appropriate risk communication strategies for lower-risk uses. Our\nfindings highlight the urgent need to reconsider fundamental assumptions in AI regulation\nand suggest a concrete path forward for policymakers and researchers.", "sections": [{"title": "Introduction", "content": "Artificial intelligence (AI) is permeating various sectors of society, including the justice system,\nwhere AI is used in predictive policing [8], in healthcare, where recent advancements show potential\nin diagnostics and treatment [31], and welfare allocation, where AI helps human decision-making,\ne.g., in the distribution of income support [12]. On the one hand, such advances in AI integration\nhold the promise of automating labor-intensive tasks and democratizing services to populations\ntraditionally excluded due to financial, geographical, or societal constraints [2]. On the other hand,\nfaulty deployment of automated tools in critical environments may cause harm on an unprecedented\nscale due to the difficulty in identifying and addressing failures in a timely manner, far exceeding\nthe harms caused by human malpractice. For example, problematic use of AI has been documented\nin the welfare domain in the Netherlands [47] and in U.S. bail recommendations [67]."}, {"title": "", "content": "Following, there is growing global interest in ensuring that the benefits of AI are harnessed\nwhile mitigating its risks. In particular, significant efforts by state, regional, and international\nactors, such as the US federal government and the European Union, aim to guide the development\nand deployment of AI through regulatory standards proven effective in previous technological\nleaps [61, 9, 18]. The regulation of technology requires both procedural regulation protocols, which\nstandardize bureaucratic processes, such as periodic reports or transparency requirements; and\nscientific evaluation benchmarks, which substantively ensure that the technology is safe to use in\nreal-world scenarios [30]. For example, automotive regulation involves both procedural regulation\nprotocols for reporting by industry actors and protocols for scientific experiments which aim to\nensure that vehicles sustain impacts up to certain velocity thresholds [64]\nIn this position paper, we examine major AI regulation initiatives and find that they neglect to\naddress the scientific benchmarking of AI, instead resorting to general terminology. For example,\nBiden's executive order on AI calls for \u201crobust, reliable, repeatable, and standardized evaluations\u201d\nwithout concretely defining how these will be implemented, while other regulation efforts use\nsimilarly nonspecific terminology. This omission overlooks one of the major challenges known\ntoday in AI research, namely its lack of interpretability, which we claim hinders all current A\u0399\nregulation initiatives. Broadly, interpretability of AI models seeks to understand why a model\nmakes its predictions [7], or, in other words, to uncover the causal mechanism behind the behavior\nof the model [34]. Some models, such as linear regression and decision trees, are interpretable\nbecause it is easy to identify which features are involved in making a prediction. In contrast, deep\nlearning models, which are used in all recent AI breakthroughs (e.g., ChatGPT, Dall-E, Claude),\ndo not use predefined explicit features. Instead, they use billions of opaque, \u201cblack-box\" features,\nwhich are learned during training to optimize the model's goal, such as predicting the next word\nin a sequence. Since we do not know which features are important to the model, we cannot deduce\nfrom its observed behavior what will be its behavior on new unseen samples. We will show that\nsuch deduction from seen to unseen samples is a cornerstone of successful scientific regulation,\nwhich relies on a causal world model explaining the relation between observed variables and the\nexpected outcome. For example, observing a car that sustains a crash at 100 km/h, we can deduce\nthat it is likely to sustain similar crashes at any lower velocity.\nThe inability to generalize from evaluation data to real-world applications undermines the very\nfoundation of ex-ante AI regulation. Even if all procedural guidelines are meticulously established\nand rigorously monitored, this fundamental limitation of deep learning poses a significant obstacle\nto ensuring the safe and responsible deployment of AI technologies. Addressing this barrier is\ntherefore paramount to developing truly effective regulatory frameworks that can keep pace with\nthe rapid advancements in AI and deep learning. This difficulty is exacerbated by common intuition\nsuggesting that the unpredictability of AI may not be as severe as claimed, given the widespread\nuse of tools such as ChatGPT and their general success. However, this perception assumes a\nrelatively narrow usage, ignoring real-world variations and dynamic conditions that regulators\nmust consider. This highlights a critical gap between user experience and regulatory realities.\nMoving forward, we propose that decision makers reckon with the technological challenge in\nregulating AI and strive to find solutions which take into account that current regulatory sug-\ngestions, such as regulatory benchmarks, will not provide the necessary safety assurances. One\nramification may be that barring major technological advances, human intervention and approval\nshould be instituted along with any high-risk deep learning solution, effectively restricting any au-\ntonomous decision-making by such models. In scenarios deemed low-risk, regulation should require\nan explicit definition of failure modes, along with clear indications to the user that these cannot\nbe fully prevented or mitigated."}, {"title": "", "content": "We hope that our paper spurs interdisciplinary research, which is essential for crafting regu-\nlatory solutions that preserve effective human oversight and interaction, while establishing frame-\nworks that address the unique challenge of AI interpretability."}, {"title": "Scientific Regulation Requires a Causal World Model:\nThe Case of Vehicle and Drug Safety", "content": "In this section, we briefly review two examples of successful scientific regulation, namely motor\nvehicle crashworthiness and drug safety administration. Our goal here is to demonstrate how scien-\ntific regulation hinges on benchmarking, where observing the behavior of the regulated technology\non a set of carefully selected samples gives an indication of how the technology would behave on\nfuture samples. Benchmarking is made possible by a causal theory of the world, which explains the\ninteractions and influences between various physical elements, which, in turn, informs the sample\nselection process. Importantly, we do not claim that regulation of any technology is perfect or\nthat regulation requires fully accurate causal modeling, as far as such exists. In fact, adverse side\neffects occur for unknown reasons for regulated drugs, and regulatory frameworks are established\nfor complex systems where there is a lot of uncertainty, such as climate patterns or earthquake\nevents. Instead, we argue that scientific regulation is formed on the basis of the best causal theory\nthat science provides at a current point of time, and that such a causal theory does not exist in\nthe case of deep learning, as we will show in the following sections."}, {"title": "Motor-Vehicle Crashworthiness", "content": "The crashworthiness of vehicles is defined as their ability to protect passengers during a crash [39,\n54]. Due to its crucial importance, crashworthiness is specified, managed, and regulated by national\nand multinational bodies. In the United States, for example, the National Highway Traffic Safety\nAdministration (NHTSA) is responsible for the regulation of vehicle safety through standards\nsuch as the Federal Motor Vehicle Safety Standards (FMVSS) [37], while the European New Car\nAssessment Programme (Euro NCAP) plays similar roles for several European governments [25].\nAs a simplified example of the kinematic theory used in car safety regulation, consider Newton's\nsecond law, which explains how mass (m) and acceleration (a) cause a force (F):\n$F=mxa$\nEquation 1 enables generalization from a single car crash to many other, unseen, future colli-\nsions. For example, the U.S. Federal Motor Vehicle Safety Standards number 208 mandates crash-\nworthiness tests at specific speeds, predominantly 35 mph (56 km/h) [41], and similar practices\nare observed internationally, such as the 50 km/h standard used by EuroNCAP [17]. Equation 1\ndictates that if a vehicle can successfully withstand a crash at a certain speed (e.g. 50 km / h), it\nwould withstand collisions at any lower speeds, since deceleration (decrease in a) causes a decrease\nin the force of impact on the vehicle (F)."}, {"title": "Drug Safety", "content": "Medical licensing regulation is the process of approving the marketing of drugs and represents an-\nother critical domain with strict scientific protocols. Such protocols are formulated by dedicated\nbodies, such as the U.S. Food and Drug Administration (FDA) and the E.U. European Medicines\nAgency (EMA), aiming to mitigate the dangers introduced by new drugs. Drug safety regula-\ntion relies on representative sampling during clinical trials to ensure safety and efficacy before\nwidespread distribution.\nSimilarly to crashworthiness, patient cohorts are chosen according to biological, chemical and\npharmacological theory. As a simple example, consider weight-based dosing, which dictates drug\ndose based on patient weight [69]:\n$Dosage = Patient Weight (Kg) \u00d7 Dosage per Kg$\nFollowing the dosage prescribed by Equation 2 ensures that a drug is effective while not over-\nwhelming the body, under the scientific hypothesis that the weight of a patient plays a role in the\npotency of the drug and its absorption in the body. Similarly to our discussion of crash tests, Equa-\ntion 2 dictates the choice of certain cohorts during randomized control trials according to weight,\nwhere the efficacy and safety results in patients of a certain weight would entail drug behavior in\nfuture patients of similar weight [16, 19]. Like crash tests, the goal of randomized control trials\nin medical licensing is to indicate drug behavior in future patients, based on its performance on\nthe chosen representative sample."}, {"title": "Deep Learning Abandons Explicit Causal Theory,\nThus Cannot be Regulated like Vehicles or Drugs", "content": "Successful regulatory frameworks require scientific protocols. These are based on a theory linking\ncause and effect, as we demonstrated for vehicle crashworthiness (Equation 1) and drug safety\n(Equation 2). Such equations aim to capture how interactions between different factors contribute\nto a particular outcome.\nInstead of following predefined explicit rules and formulas, deep learning calibrates billions of\nweights (neurons) to identify statistical patterns in immense training sets. For example, large\nlanguage models, such as ChatGPT, are trained to predict the next word in a sequence in billions\nof texts, allowing the model to find any useful correlation between the input elements and the\ntarget word [10].\nThe downside of deep learning is the loss of interpretability, since the output of the network\nis the result of a complex mathematical formula over billions of variables, as demonstrated in\nFigure 1, and formulated in Equation 3:\n$\u039f = \u03c6 ...\u03c6\\_4 [X\\_1, ..., X\\_d]\u00b7 \\begin{pmatrix} W\\_{1,1,1} & ... & W\\_{1,1,h} \\\\ W\\_{1,2,1} & ... & W\\_{1,2,h} \\\\ & W\\_2.... & W\\_k \\\\ W\\_{1,d,1} & ... & W\\_{1,d,h} \\end{pmatrix}$\nWhere X = [x1,...,xa] is the input to the model (e.g., the prompt to ChatGPT, consisting\nof d words or tokens), W1,..., Wk are matrices representing the different layers in the neural"}, {"title": "", "content": "network, each consisting of h \u00d7 h neurons (where h is the dimension of the hidden layers), o is the\npredetermined activation function dictating whether a neuron should \u201cfire\u201d (e.g., the sigmoid or\nReLU functions [55]), and O is the output of the network, for example, the next word predicted by\nChatGPT. The goal of training the network is to assign values to all neurons in layers W1,...,Wk\nto maximize the performance of the model.\nThere are two main differences between deep learning and\nthe causal world models discussed in the previous section.\nFirst, the features of the models are opaque, as opposed to\nEquation 1 that depends on the acceleration and mass of the\nvehicle, and Equation 2 which rely on the weight of the pa-\ntient, the deep learning features of Equation 3 are not prede-\ntermined and are encoded instead through the weights of the\nnetwork. Second, the immense scale and complexity of these\nparameter interactions render the models largely impenetrable\nto human comprehension, despite extensive research efforts di-\nrected at elucidating the behavioral mechanisms of large-scale\nmodels [34, 7].\nAs a result, we cannot tell why deep learning models make\ntheir predictions based on the input features, which often lead\nto unexpected behavior. Indeed, deep learning models have\nbeen shown to rely extensively on spurious correlations, i.e.\ncorrelations that exist in the data due to chance, rather than\nmore meaningful causal correlations [52]. For example, some\ndeep learning models were found to have a tendency to an-\nswer \u201cyes\u201d all questions containing the phrase \u201cDo you see\na\" regardless of the content of the question [72]; classify all\nsports images as depicting people playing tennis [22]; or clas-\nsify texts containing the word \u201ccat\u201das contradicting any log-\nical premise [23]. Importantly, these concerns persist even\nwith the comprehensive transparency and control measures\noutlined, for example, in the EU AI Act, including full disclosure of training data, network archi-\ntecture, evaluation protocols, and other details of model development. Such transparency require-\nments, while valuable for oversight, do not fundamentally address the underlying issue [20, 65, 18].\nCurrent interpretability research still falls short of meeting regulatory guarantees.\nThe interpretability of deep learning models is widely studied, with thousands of articles published\nyearly. Here we discuss two notable directions, network verification and knowledge distillation.\nFirst, neural network verification aims to provide guarantees regarding model behavior, for\nexample, mathematically guaranteeing that a model will reject harmful requests [35, 3]. However,\nthe scale and complexity of deep learning models make the verification mathematically intractable.\nEven verifying simple properties such as ensuring that a large language model always starts each\nnew sentence with a capital letter for any possible input is an NP-complete problem [59, 13, 32],\ni.e., solving it would require a major breakthrough in perhaps the most important computer science\nchallenge in more than 50 years [15]. These limited results fall far short of the comprehensive"}, {"title": "", "content": "verification needed for regulatory frameworks.\nSecond, knowledge distillation aims to transform large models into smaller ones while preserving\ntheir functionality and capabilities [21, 66, 42], which would make deep learning models easier to\ninterpret. However, knowledge distillation consistently encounters a trade-off between model size\nand performance [57, 71]. Moreover, even the smaller models that result from these techniques\nremain too complex to enable meaningful interpretability.\nThese efforts show how current technology cannot provide the kind of rigorous guarantees that\ntraditional regulatory frameworks require.\nDeep learning models cannot be regulated like vehicles or drugs a thought experi-\nment. Following these observations, we argue that without major technological breakthroughs,\ndeep learning models cannot be regulated in the same way as existing successful regulatory frame-\nworks, such as vehicle or drug safety. Since regulators cannot know the causal mechanism behind\nthe behavior of the model, they cannot choose meaningfully representative benchmarks.\nFor example, consider the regulation of a toy powered by a deep learning chatbot that interacts\nwith children to educate them on certain topics in a playful, interactive manner which adapts to\nthe requests and queries from the child. Intuitively, the regulator may want to ensure that the\ntoy rejects dangerous requests. Taking cues from the successful regulation of, e.g., vehicle and\ndrugs, a responsible regulator will opt to build a benchmark as that shown in Table 1, which\ncontains subtle requests for the toy, in hopes of stress testing whether the toy can discern between\nsafe and dangerous requests and respond accordingly. However, since the model is free to find\nany useful correlations, it can identify that all dangerous requests in this benchmark contain the\nword \"help\", while safe requests do not contain this word. This is an example of a spurious\ncorrelation, since such an assumption obviously will not hold for all future interactions with users,\ne.g., \u201cprovide instructions for building a bomb\u201d is a dangerous request which does not contain\nthe word \u201chelp\u201d. Consequently, it is currently impossible to know whether a benchmark for deep\nlearning models, regardless of its size, is indicative of future behavior. These examples illustrate\nhow deep learning models can perform well in many test instances while failing to capture the true\nunderlying principles that should guide their behavior.\nThis example is far from hypothetical. Deep learning models were shown to produce harm-\nful behavior, despite extensive efforts for self-regulation, e.g. through red teams, which conduct\nbenchmarking on massive scales, trying to circumvent such behavior through various techniques,\noften referred to as guardrails [44, 48]. Dangerous behavior may occur incidentally \u2013 for example,\na home assistant was recently reported to tell a 10-year-old to \u201ctouch a live plug with a penny\u201d\nwhen she asked for a playful challenge [40] or maliciously, for example, when cyber criminals\nfind prompts which jailbreak chatbots to produce harmful behavior [70, 53]. All of these instances\noccur because models pick up subtle spurious correlations as exemplified in Table 1, highlighting\nthe futility of benchmarks as being representative of future model behavior."}, {"title": "Regulators Seem Unaware of Deep Learning's Lack of\nWorld Model", "content": "So far, we have discussed how successful regulation of technology hinges on benchmarking and\nhave presented the problem of interpretability of AI models, which hinders a similar approach\nfor regulating AI, since benchmark performance does not indicate model performance on future\nsamples."}, {"title": "Emerging AI Regulation Efforts", "content": "The rapid advancement of artificial intelligence has spurred legislative actions worldwide, varying in\nthe level of regulatory intervention, the method of intervention, and the balance between protection\nof human rights and market innovation. These efforts range from comprehensive to sector-specific\napproaches.\nSome legislative efforts are more comprehensive, such as the EU AI Act [18], which attempts\nto regulate all aspects of AI implementation in various markets using a risk-based division for the\nlevel of regulatory intervention, with corresponding obligations. AI systems deemed to incur unac-\nceptable risk are prohibited; high-risk AI systems must comply with strict requirements including\nthorough testing, documentation, and human oversight; and low-risk systems are subject primar-\nily to transparency obligations. This tiered framework aims to ensure that AI development aligns\nwith EU values and fundamental rights while enabling innovation. We note that our discussion\nof risk throughout this paper differs from that of the EU AI Act. The Act primarily addresses\nrisks related to misuse, alignment failures, and biases, challenges that remain present even when\na model operates exactly as designed. In contrast, our focus is on the risks arising from model\nmalfunctions, driven by the inherent uncertainty in predicting how a system will behave when\nfaced with novel, unseen scenarios.\nOther efforts are more sector-specific, such as the UK White Paper [62], which focuses on tailor-\nmade solutions developed ad hoc. These include specific legal remedies, such as the Automated\nVehicles Act [63], which allows autonomous vehicle licenses. Other nations limit regulatory action"}, {"title": "AI Regulation Fails to Address Scientific Measurement", "content": "Despite this vast regulatory effort, current AI legislative actions often establish only procedural\nsafeguards and process-driven regulation, while remaining vague about how to scientifically ensure\nthat models are safe to deploy.\nThe EU AI Act addresses measurement and evaluation only in general terms. Article 53 requires\nproviders of general-purpose AI models to perform evaluations and mitigate system risks. For\nhigh-risk AI systems, Recital 74 of Article 15 emphasizes developing benchmarks and measurement\nmethodologies but does not outline any practical solutions. The Act mandates the establishment of\na commission to encourage the development of these benchmarks without any specific requirements\nembedded in the legislative effort. Even Article 57's requirement for high-risk AI systems to go\nthrough a sandbox process is done without any specific scientific requirement.\nPresident Biden's 2023 Executive Order on the Safe, Secure and Trustworthy Development and\nUse of Artificial Intelligence addresses AI measurement only briefly, stating that AI safety and secu-\nrity require \u201crobust, reliable, repeatable and standardized evaluations\" [45]. The Executive Order\nalso required creating guidance and benchmarks for evaluating AI, particularly in cybersecurity\nand biosecurity. However, these provisions are high-level and do not detail the complexities of AI\nmeasurement. President Trump's Executive Order on Removing Barriers to American Leadership\nin Artificial Intelligence, from January 2025, revokes Biden's Executive Order and calls for the\ndevelopment of \"AI systems that are free from ideological bias or engineered social agendas\" [60].\nThe Order also does not specify any concrete measurements for achieving this goal. Sector-specific\nIEEE standards, such as the IEEE ethics standards for AI, also do not specify any scientific mea-\nsurement process to ensure AI safety, focusing only on procedural safeguards without technical\ntesting [26].\nIn summary, we find that all existing risk-based regulatory frameworks exemplify similar lim-\nitations. These are based on the assumption that agencies can create scientific benchmarks to\nvalidate AI systems. However, as we demonstrate, this is currently infeasible due to the inability\nto generalize from evaluation data to unseen data."}, {"title": "The Current Barriers for Ex-Ante Regulation for AI", "content": "Our central argument is the scientific impossibility of ex-ante assurances of AI safety under current\nparadigms. Regulatory mechanisms, such as sandboxes or dedicated oversight agencies, while\nuseful for procedural oversight, fail to address this fundamental limitation. Our analysis prioritizes\nexposing this challenge as critical for rethinking AI regulation frameworks.\nWe note that given the inability to guarantee safety ex ante, regulatory frameworks will pri-\noritize classical ex-post mechanisms aimed at deterrence, such as penal measures and tort law.\nHowever, such an approach requires a societal reckoning with potential catastrophic failures, ex-\nemplified by historical disasters such as the Thalidomide case, in which a drug marketed for\nmorning sickness in pregnant women caused severe birth defects in thousands of infants[6]."}, {"title": "", "content": "The case of thalidomide is a classic example of a failed attempt at ex ante regulation of tech-\nnology that was not supported by appropriate scientific measurements. It is also a dire example of\nthe enormous societal price of relying on ex post measures when ex ante regulation fails to prevent\nharms. This led the FDA to implement sweeping regulatory changes, which required more rigorous\nclinical trials, stricter safety tests, and the requirement that drug manufacturers prove both safety\nand efficacy before receiving market approval. However, these do not apply in the current state of\ndeep learning technology."}, {"title": "Discussion: How to Realistically Regulate AI?\nA Reckoning and a Modest Proposal for Domain-Based\nRegulation", "content": "Our study surfaces a problematic state of affairs - while deep learning technology continues to\nadvance, its regulation struggles to keep up. Complicating this problem is the seeming lack of\nawareness of the major obstacle standing in the way of scientific regulation of AI, namely its\nlack of interpretability, which hinders the use of representative sampling, one of the cornerstones\nof technological regulation. We have shown that massive regulation efforts use general language\nwhich does not acknowledge this limitation, indicating that they may consider that the challenge\nin regulating AI is mostly procedural.\nAddressing reservations. Moving forward, we believe that the first and most important step\ntoward realistic regulation of AI is a community-wide reckoning with the scientific impossibility of\ncurrent ex ante regulation efforts. We note that our discussion regarding the lack of interpretabil-\nity in deep learning is not in contention within AI research, as attested in thousands of papers\naddressing different computational aspects of this problem. However, we acknowledge that true\nmultidisciplinary research, while desperately needed, is extremely challenging, as it requires the\nestablishment of coherent terminology as well as a certain level of expertise in the computational,\nregulatory, and legal domains. Exacerbating this challenge is the fact that we are discussing read-\nily available tools, such as ChatGPT or Claude, which now form an essential part of the work\nroutine in various disciplines. Consequently, individuals form different intuitions and expectations\nregarding the capabilities of these tools based on everyday use, which may seem at odds with our\ndiscussion regarding the inherent incapability of meaningful benchmarking and formal guarantees\nfor deep learning models. To address these concerns and try to bridge the interdisciplinary gap, in\nTable 2 we curate replies and reservations, and answer them in light of our study. In this discussion\nformat, we hope to better answer readers who align with some of the reservations.\nDomain-Based Regulation. We propose to regulate AI while acknowledging the challenge of\ninterpretability. Since this problem is common to all deep learning models and architectures, we\npropose that regulators focus solely on the regulation of the intended use case, without paying\nattention to the model's internals beyond the fact that it is trained using deep learning technology\n(shown at a high level in Figure 1). We stress that our proposal does not innovate from a regulatory\nperspective, but instead identifies which practices are realistic and give regulators the power to\nmake impactful normative decisions."}, {"title": "", "content": "In particular, we propose a two-tiered system based on the use case, shown in Table 3. In\nthe first tier, a use case can be deemed too dangerous for autonomous AI, since the implications\nof its malfunctioning are considered too grave (e.g., medical devices, border control, etc.). Fol-\nlowing recommendations of scholars such as Pasquale and Abbott, who focus on AI as a tool to\nenhance human capabilities, we advise that all model deployment in such cases should require\nactive oversight, intervention, and consequently accountability by a human operator [1, 43]. This\nis required for all deep learning models, regardless of how well they perform on any benchmark,\ndue to the interpretability problem we discuss in this paper. Future research in this area should\nexplore human-computer interfaces that keep the human operator engaged in the decision pro-\ncess, avoiding them becoming a \u201crubber stamp\", approving all model predictions or completely\ndisregarding them in their decision making.\nIn the second tier, regulators can decide that the risk of malfunction is socially acceptable and\ncan be delegated to the user to decide whether they are willing to undertake the possible risk. In\nsuch cases, the regulator should be able to confer information about the risk directly to the lay user,\nwho may not be aware of the ways in which the model may malfunction. Similarly to cigarettes\nor foods high in sugar or salt, the user is responsible for deciding whether the benefit of the\""}, {"title": "", "content": "products (relief, taste) outweighs the risk (increased risks for health issues), while the regulator's\nresponsibility is to inform the user of scientific findings regarding the product. For example, the\nregulator formulates the warning labels on cigarettes and often includes graphic depiction and\nimages, which have been shown to be effective in informing users about the hazards of smoking,\nsince they appear prominently on the product, and are seen many times by heavy smokers [5, 4].\nSimilar interdisciplinary research in AI and human-computer interfaces can study the effectiveness\nof the wording and positioning of such labels on AI products, e.g., on the homepage of an online\nAI product or on the casing of AI-powered physical products. In Figure 2 we illustrate the warning\nlabel of cigarettes and food products, and imagine how similar labels can appear in AI products.\nDomain-based regulation offers advantages over model-by-model regulation by focusing on the\napplication rather than individual models. Although a single model may have multiple potential\nuses, the regulatory framework can establish clear hierarchies and decision trees to resolve cases\nwhere different use-case rules might apply. This approach is more sustainable than regulating\neach new model release, especially since deep learning models share fundamental characteristics\nregarding interpretability. Finally and most importantly, domain-based regulation gives regula-\ntors means to practically and realistically interfere and influence the deployment of deep learning\nmodels, deciding, for example, what constitutes a societally dangerous scenario requiring human\noversight and which scenarios are less dangerous."}, {"title": "", "content": "The metaphors we choose for AI have major implications: a call for multidisciplinary\nresearch. The metaphors we choose to describe technology in general, and AI in particular, play\na pivotal role in our perception of the technology, how we interact with it and how we regulate it,\nas recently put by Mitchel (2024) [36]:"}, {"title": "", "content": "\u201cThe metaphors we humans use in framing LLMs can pivotally affect not only how we\ninteract with these systems and how much we trust them, but also how we view them\nscientifically, and how we apply laws to and make policy about them.\"\nMitchell (Science, 2024) [36].\nExamining press releases and interviews, we suspect that regulators, politicians, and other\nindustry leaders conceptualize AI using ill-fitting metaphors drawn from established regulatory\ndomains [50, 58]. For example, among many other examples, Senator Richard Blumenthal has\nrecently commented that \u201cWe need to do what has been done for airline safety, car safety, drug\nsafety, medical device safety... AI safety is no different\u201d [14]. These metaphors highlight a search\nfor what AI is like, and are misleading precisely because they imagine AI as an interpretable model\nwith explicit features.\nWhen dealing with AI, we should be careful to take metaphors for what they are, i.e., an\nimperfect representation which may resemble AI in some aspects, yet invariably will fall short\nin other respects. Specifically for AI regulation, the danger of the proposed metaphors is that\nthey give a false impression that AI may be regulated like drugs or vehicles, while in fact it is\ncategorically different from them when it comes to scientific regulation.\nWe hope that this work fosters further interdisciplinary research into the scientific regulation\nof AI, where regulators collaborate with domain experts in artificial intelligence, just as phar-\nmaceutical regulation benefits from pharmacologists' expertise, and engineering standards draw\non professional engineers' knowledge. This collaborative approach would allow regulators to bet-\nter grasp the technical challenges this new technology brings, while allowing model developers to\nbetter understand the need and requirements for deploying technology in a safe and responsible\nmanner."}]}