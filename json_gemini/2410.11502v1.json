{"title": "Offline Model-Based Optimization by Learning to Rank", "authors": ["Rong-Xi Tan", "Ke Xue", "Shen-Huan Lyu", "Haopu Shang", "Yao Wang", "Yaoyuan Wang", "Sheng Fu", "Chao Qian"], "abstract": "Offline model-based optimization (MBO) aims to identify a design that maximizes\na black-box function using only a fixed, pre-collected dataset of designs and their\ncorresponding scores. This problem has garnered significant attention from both\nscientific and industrial domains. A common approach in offline MBO is to train a\nregression-based surrogate model by minimizing mean squared error (MSE) and\nthen find the best design within this surrogate model by different optimizers (e.g.,\ngradient ascent). However, a critical challenge is the risk of out-of-distribution\nerrors, i.e., the surrogate model may typically overestimate the scores and mislead\nthe optimizers into suboptimal regions. Prior works have attempted to address\nthis issue in various ways, such as using regularization techniques and ensemble\nlearning to enhance the robustness of the model, but it still remains. In this\npaper, we argue that regression models trained with MSE are not well-aligned\nwith the primary goal of offline MBO, which is to select promising designs rather\nthan to predict their scores precisely. Notably, if a surrogate model can maintain\nthe order of candidate designs based on their relative score relationships, it can\nproduce the best designs even without precise predictions. To validate it, we\nconduct experiments to compare the relationship between the quality of the final\ndesigns and MSE, finding that the correlation is really very weak. In contrast,\na metric that measures order-maintaining quality shows a significantly stronger\ncorrelation. Based on this observation, we propose learning a ranking-based model\nthat leverages learning to rank techniques to prioritize promising designs based\non their relative scores. We show that the generalization error on ranking loss can\nbe well bounded. Empirical results across diverse tasks demonstrate the superior\nperformance of our proposed ranking-based models than twenty existing methods.", "sections": [{"title": "1 INTRODUCTION", "content": "The task of creating new designs to optimize specific properties represents a significant challenge\nacross scientific and industrial domains, including real-world engineering design (Tanabe & Ishibuchi,\n2020; Kumar et al., 2022), protein design (Khan et al., 2023; Kolli, 2023; Chen et al., 2023b; Kim\net al., 2023), and molecule design (Gaulton et al., 2012; Stanton et al., 2022). Numerous methods\nfacilitate the generation of new designs by iteratively querying an unknown objective function\nthat correlates a design with its property score. Nonetheless, in practical scenarios, the evaluation\nof the objective function can be time-consuming, costly, or even pose safety risks (Dara et al.,\n2022). To identify the next candidate design using only accumulated data, offline model-based\noptimization (MBO; Trabucco et al., 2022) has emerged as a widely adopted approach. This method\nrestricts access to an offline dataset and does not allow for iterative online evaluation, which, however,\nalso results in significant challenges. A common strategy, referred to as the forward method, entails\nthe development of a regression-based surrogate model by minimizing mean squared error (MSE),\nwhich is subsequently utilized to identify the optimal designs by various ways (e.g, gradient ascent)."}, {"title": "2 BACKGROUND", "content": ""}, {"title": "2.1 OFFLINE MODEL-BASED OPTIMIZATION", "content": "Given the design space X \u2286 R^d, where d is the design dimension, offline MBO (Trabucco et al.,\n2022; Xue et al., 2024) aims to find a design x* that maximizes a black-box objective function\nf, i.e., x* = arg max_{x\u2208X} f(x), using only a pre-collected offline dataset D, without access to\nonline evaluations. That is, an offline MBO algorithm is provided only access to the static dataset\nD = {(x_i, y_i)}_{i=1}^N, where x_i represents a specific design (e.g., a superconductor material), and\ny_i = f(x_i) represents the target property score that needs to be maximized (e.g., the critical\ntemperature of the superconductor material).\nThe mainstream approach for offline MBO is the forward approach, which fits a surrogate model,\ntypically a deep neural network f_\u03b8 : X \u2192 R, parameterized by \u03b8, to approximate the objective\nfunction f in a supervised manner. Prior works (Trabucco et al., 2021; Fu & Levine, 2021; Yu et al.,\n2021; Qi et al., 2022; Chen et al., 2022; Yuan et al., 2023; Chen et al., 2023a; Chemingui et al., 2024;\nHoang et al., 2024; Dao et al., 2024) learn the surrogate model by minimizing the MSE between the\npredictions and the true scores:\nargmin_\u03b8 \\sum_{i=1}^N (f_\u03b8(x_i) - y_i)^2 /N.\nWith the trained model f_\u03b8, the final design can be obtained by various ways, typically gradient ascent:\nx_{t+1} = x_t + \u03b7 \\nabla_x f_\u03b8(x) |_{x=x_t}, fort \u2208 {0,1,...,T- 1}                               (1)\nwhere \u03b7 is the search step size, T is the number of steps, and x_T serves as the final design candidate\nto output. However, this method is limited by its poor performance in out-of-distribution (OOD)\nregions, where the surrogate model f_\u03b8 may erroneously overestimate objective scores and mislead\nthe gradient-ascent optimizer into sub-optimal regions. There have been many recent efforts devoted\nto addressing this issue, such as using regularization techniques(Trabucco et al., 2021; Fu & Levine,\n2021; Yu et al., 2021; Chen et al., 2022; Qi et al., 2022; Hoang et al., 2024; Dao et al., 2024) and\nensemble learning (Yuan et al., 2023; Chen et al., 2023a) to enhance the robustness of the model.\nAnother type of approach for offline MBO is the backward approach, which typically involves\ntraining a conditioned generative model p_\u03b8(x|y) and sampling from it conditioned on a high score.\nFor example, MINs (Kumar & Levine, 2020) trains an inverse mapping using a conditioned GAN-like\nmodel (Goodfellow et al., 2014; Mirza & Osindero, 2014); DDOM (Krishnamoorthy et al., 2023b)\ndirectly parameterizes the inverse mapping with a conditional diffusion model (Ho et al., 2020) in\nthe design space; BONET (Krishnamoorthy et al., 2023a) uses trajectories to train an autoregressive\nmodel, and samples them using a heuristic.\nA comprehensive review of the offline MBO methods is provided in Appendix A due to space\nlimitation. However, we point out in this paper that the regression-based models trained with MSE\nare not well-aligned with the primary goal of offline MBO, which is to select promising designs rather\nthan to predict their scores precisely. Intuitively, offline MBO does not require exact score predictions\nfrom the surrogate model; rather, it demands that the model accurately discerns the partial ordering\nof designs, which aligns well with the learning to rank (LTR) framework introduced in Section 2.2."}, {"title": "2.2 LEARNING TO RANK", "content": "LTR aims to learn an optimal ordering for a given set of objects (e.g., designs in offline MBO),\nand has applications across various domains, including information retrieval (Liu, 2010; Li, 2011),\nrecommendation systems (Karatzoglou et al., 2013), and language model alignment (Song et al.,\n2024; Liu et al., 2024). It is typically formulated as a supervised learning task. Given the training\ndata D_R = {(X, y) | (X, y) \u2208 X^m \u00d7 R^m}, where X is the object space, X is a list of n objects to\nbe ranked, each denoted by x_i \u2208 X, and y is a list of n corresponding relevance labels y_i \u2208 R, the\ngoal of LTR is to learn a ranking function that assigns scores to individual objects and then arranges\nthese scores in descending order to produce a ranking. Formally, LTR aims to identify a ranking\nscore function s_\u03b8 : X \u2192 R, parameterized by \u03b8. Let s_\u03b8(X) = [s_\u03b8(x_1), s_\u03b8(x_2),..., s_\u03b8(x_m)], and"}, {"title": "3 METHOD", "content": "In this section, we introduce our ranking-based surrogate models for offline MBO. We first analyze in\ndetail the goal of offline MBO and aim to answer the critical question, \"Is MSE a good metric for\noffline MBO?\" in Section 3.1. Consequently, we find that MSE is not a suitable metric, and thus\nintroduce a better one in Section 3.2, i.e., Area Under the Precision-Recall Curve (AUPRC) which is\nrelated to ranking. This motivates us to propose a framework based on LTR to solve offline MBO in\nSection 3.3. Furthermore, we show that the surrogate model based on LTR methods can have a good\ngeneralization error bound, which will be shown in Section 3.4."}, {"title": "3.1 Is MSE A GOOD METRIC FOR OFFLINE MBO?", "content": "An ideal metric should be able to accurately assess the goodness of a surrogate model, i.e., the better\nthe metric, the better the quality of the final design obtained using the surrogate model. As shown\nin Eq. (1), x_t, which approximately maximizes the surrogate model f_\u03b8 by gradient ascent, serves\nas the final design to output. During the optimization process, it will inevitably traverse the OOD\nregion. Therefore, the performance of the surrogate model in the OOD region will significantly\nimpact the performance of offline MBO. Unfortunately, previous works (Trabucco et al., 2021; 2022)\nhave shown that the regression-based models optimized using MSE often result in poor predictions\nin the OOD region, i.e., the MSE value in the OOD region (denoted as OOD-MSE) can be very\nhigh, and thus many methods have been proposed to decrease OOD-MSE (Fu & Levine, 2021; Chen\net al., 2023a; Yuan et al., 2023) or avoid getting into OOD regions (Trabucco et al., 2021; Yu et al.,\n2021; Qi et al., 2022). In this paper, however, we indicate that even if OOD-MSE is small, the final\nperformance of offline MBO can still be bad. That is, the relationship between the quality of the final\ndesigns and OOD-MSE is weak. In the following, we will validate this through experiments.\nTo analyze the correlation between the OOD-MSE of a surrogate model and the score of the final de-\nsign candidate obtained by conducting gradient ascent on the surrogate model, we select five surrogate\nmodels: a gradient-ascent baseline and four state-of-the-art forward approaches, COMs (Trabucco\net al., 2021), IOM (Qi et al., 2022), ICT (Yuan et al., 2023), and Tri-Mentoring (Chen et al., 2023a).\nWe follow the default setting as in (Chen et al., 2023a; Yuan et al., 2023) for data preparation and\nmodel-internal search procedures. To construct an OOD dataset, we follow the approach outlined\nin (Chen et al., 2023a), selecting high-scoring designs that are excluded from the training data in\nDesign-Bench (Trabucco et al., 2022). Detailed information regarding model selection, training and\nsearch configurations, and OOD dataset construction can be found in Appendix D.1. We train the\nsurrogate models, evaluate their performance using various metrics (e.g., MSE) on the OOD dataset,\nand obtain the final design with its corresponding ground-truth score under eight different seeds.\nSubsequently, we rank the OOD-MSE values in ascending order, and rank the 100th percentile scores\nof the final designs in descending order. To show the correlation between OOD-MSE and the final\nscore, we create scatter plots of the two rankings and calculate their Spearman correlation coefficient.\nThe left two subfigures of Figure 2 show the scatter plots on a continuous task, D'Kitty (Ahn et al.,\n2020), and a discrete task, TF-Bind-8 (Barrera et al., 2016). Both scatter plots exhibit highly dispersed\ndata points, with no clear overall trend or strong clustering, showing no consistent pattern in their\ndistribution. This scattered nature of the data points is also reflected in the low Spearman correlation\ncoefficients (0.23 for D'Kitty and -0.24 for TF-Bind-8), indicating weak correlations between\nOOD-MSE rank and score rank in both tasks. These results demonstrate that OOD-MSE is not a\ngood metric for offline MBO, underscoring the need for a more reliable evaluation metric."}, {"title": "3.2 WHAT IS THE APPROPRIATE METRIC FOR OFFLINE MBO?", "content": "As we mentioned before, an intuition of offline MBO is that the goodness of a surrogate model may\ndepend on its ability to preserve the score ordering of designs dictated by the ground-truth function.\nWe substantiate this intuition through the following theorem.\nTheorem 1 (Equivalence of Optima for Order-Preserving Surrogates). Let f_\u03b8 be a surrogate model\nand f the ground-truth function. A function h : R \u2192 R is order-preserving, if \u2200y_1, y_2 \u2208 R, y_1 < y_2\niff h(y_1) < h(y_2). If there exists an order-preserving h such that f_\u03b8(x) = h(f(x)) \u2200x, then finding\nthe maximum of f is equivalent to finding that of f_\u03b8, i.e., arg max_{x\u2208X} f(x) = arg max_{x\u2208X} f_\u03b8(x).\nProof. Suppose x* \u2208 arg max_x f(x). For any x, we have f(x*) \u2265 f(x). Since h is order-\npreserving, we have h(f(x*)) \u2265 h(f(x)) for all x. Thus, given f_\u03b8(x) = h(f(x)), we have\nf_\u03b8(x*) \u2265 f_\u03b8(x) for all x. Therefore, x* \u2208 arg max_x f_\u03b8(x), i.e., arg max_x f(x) \u2286 arg max_x f_\u03b8(x).\nNote that since h is strictly increasing, it is bijective and thus has an inverse function h^{-1}, which is also\nstrictly increasing. With h^{-1}, the reverse implication follows similarly, proving the equivalence.\nTheorem 1 shows that a good surrogate model needs to maintain an order-preserving mapping\nfrom the ground-truth model. In the practical setting of offline MBO, the standard procedure is to\nselect the top-k designs (e.g., k = 128), which maximize the surrogate model's predictions, for\nevaluation (Trabucco et al., 2022). Thus, we introduce a novel metric, Area Under the Precision-\nRecall Curve (AUPRC) in Definition 1 (Raghavan et al., 1989; Davis & Goadrich, 2006; Ricci et al.,\n2010), for offline MBO to assess the model's capability in identifying the top-k ones from a set of\ncandidate designs. Although AUPRC is traditionally associated with imbalanced classification tasks,\nit can be interpreted as a listwise ranking metric (Wen et al., 2024), which enables us to employ\nAUPRC for evaluating a model's ability to select the top-k designs within the context of offline MBO.\nDefinition 1 (AUPRC for Offline MBO). Consider a surrogate model f_\u03b8 and a ground-truth function\nf. Given a dataset D_o = {(x_i, y_i)}_{i=1}^N, denote {f_\u03b8(x_i)}_{i=1}^N as f_\u03b8(D_o), and {f(x_i)}_{i=1}^N as f(D_o)."}, {"title": "3.3 OFFLINE MBO BY LEARNING TO RANK: A PRACTICAL ALGORITHM", "content": "In this section, in order to optimize AUPRC for the surrogate model, we design a novel framework\nfor offline MBO based on LTR, as shown in Algorithm 1, which consists of three parts: 1) data\naugmentation; 2) LTR loss learning; 3) output adaptation.\nData augmentation. In LTR tasks, the training set D_R typically requires a list of designs as features.\nHowever, the offline dataset D in offline MBO is not directly structured in this manner, thus, the\nLTR loss functions cannot be directly applied. A na\u00efve approach to address this issue is to treat each\nbatch of training data as a list of designs to be ranked, with the batch size determining the list length.\nHowever, this method has its limitation since each design in the training data appears in only one list\nduring one single epoch, which is unable to analyze its relationship with other designs that are not in"}, {"title": "3.4 THEORETICAL ANALYSIS", "content": "In the previous subsections, we have indicated the importance of preserving the score order of designs,\nand proposed to learn a surrogate model by optimizing ranking losses. Here, we further point out\nthat the generalization error can be well bounded in the context of LTR. Note that the generalization\nof LTR has been well studied (Agarwal et al., 2005; Lan et al., 2009; Chen et al., 2010; Tewari &\nChaudhuri, 2015), which is mainly analyzed by the Probably Approximately Correct (PAC) learning\ntheory (Cucker & Smale, 2001) and Rademacher Complexity (Bartlett & Mendelson, 2003). By\nleveraging these existing generalization error bounds, we provide theoretical support for our approach\nof applying LTR techniques for offline MBO.\nFormally, assume that we have an i.i.d. training data D_R = {(X_i, y_i)}_{i=1}^n where X_i \u2208 X^m,\nconsisting of m designs, and y_i \u2208 R^m. Given a ranking algorithm A (e.g., RankCosine or\nListNet), its loss function l_A(f; X, y) is normalized by l_A(f; X,y)/Z_A, where Z_A is a normal-\nization constant (e.g., Z_{RankCosine} = 1). The expected risk with respect to the algorithm A\nis defined as R_{IA}(f) = \u222b_{X^m\u00d7R^m}l_A(f; X, y)P(dX, dy), and the empirical risk is defined as\nR_A(f; D_R) = \\sum_{i=1}^n l_A(f; X_i, y_i). Let F be the ranking function class, and Theorem 2 gives an\nupper bound on the generalization error sup_{f\u2208F}(R_{IA}(f) \u2013 R_{IA}(f; D_R)).\nTheorem 2 (Generalization Error Bound for LTR (Lan et al., 2009)). Let \u03c6 be an increasing and\nstrictly positive transformation function (e.g., \u03c6(z) = exp(z)). Assume that: 1) \u2200x \u2208 X, ||x|| < M;\n2) the ranking model f to be learned is from the linear function class F = {x \u2192 w^Tx | ||w|| \u2264 B}.\nThen with probability 1 \u2013 \u03b4, the following inequality holds:\nsup_{f\u2208F}(R_{IA}(f) \u2013 R_{IA}(f; D_R)) \u2264 \\frac{4BM \u00b7 C_A(\u03c6)\u03a9(\u03c6)}{\\sqrt{n}} + \\sqrt{\\frac{2ln (2/\u03b4)}{n}},\nwhere: 1) A stands for a specific LTR algorithm; 2) \u03a9(\u03c6) = sup_{z\u2208[\u2212BM,BM]} \u03c6'(z), which is an\nalgorithm-independent factor measuring the smoothness of \u03c6; 3) C_A(\u03c6) is an algorithm-dependent\nfactor, e.g., C_{RankCosine}(\u03c6) = \\sqrt{m}/(2\u03c6(\u2212BM)).\nWe will introduce some settings of \u03c6 and the corresponding \u03a9(\u03c6) and C_A(\u03c6) in Appendix B. We\ncan observe from the inequality in Theorem 2 that the generalization error bound vanishes at the rate\nO(1/\\sqrt{n}), because C_A(\u03c6) and \u03a9(\u03c6) are independent of the size n of training set."}, {"title": "4 EXPERIMENTS", "content": "In this section, we empirically compare the proposed method with a large variety of previous offline\nMBO methods on various tasks. First, we introduce our experimental settings, including five tasks,\ntwenty compared methods, training settings, and evaluation metrics. Then, we present the results\nto show the superiority of our method. We also examine the influence of using different ranking\nlosses, and conduct ablation studies to investigate the effectiveness of each module of our method.\nFurthermore, we simply replace MSE of existing methods with the best-performing ranking loss, to\ndemonstrate the versatility of the ranking loss for offline MBO. Finally, we provide the metrics, OOD-\nMSE and OOD-AUPRC, in the OOD regions to validate their relationship with the final performance.\nOur implementation is available at https://anonymous.4open.science/r/Offline-RaM-7FB1."}, {"title": "4.1 EXPERIMENTAL SETTINGS", "content": "Benchmark and tasks. We benchmark our method on Design-Bench tasks (Trabucco et al., 2022),\nincluding three continuous tasks and two discrete tasks 1. The continuous tasks include: 1) Ant\nMorphology (Brockman et al., 2016): identify an ant morphology with 60 parameters to crawl quickly.\n2) D'Kitty Morphology (Ahn et al., 2020): optimize a D'Kitty morphology with 56 parameters to\ncrawl quickly. 3) Superconductor (Hamidieh, 2018): design a 86-dimensional superconducting\nmaterial to maximize the critical temperature. The two discrete tasks are TF-Bind-8 and TF-Bind-\n10 (Barrera et al., 2016): find a DNA sequence of length 8 and 10, respectively, maximizing binding\naffinity with a particular transcription factor.\nCompared methods. We mainly consider three categories of methods to solve offline MBO. The first\ncategory involves baselines that optimize a trained regression-based model, such as BO-qEI (Garnett,\n2023; Shahriari et al., 2016), CMA-ES (Hansen, 2006), REINFORCE (Williams, 1992), Gradient\nAscent and its variants of mean ensemble and min ensemble. The second category encompasses\nbackward approaches, including CbAS (Brookes et al., 2019), MINs (Kumar & Levine, 2020),\nDDOM (Krishnamoorthy et al., 2023b), BONET (Krishnamoorthy et al., 2023a), and GTG (Yun\net al., 2024). The third category comprises recently proposed forward approaches, which contain\nCOMS (Trabucco et al., 2021), ROMA (Yu et al., 2021), IOM (Qi et al., 2022), BDI (Chen et al.,\n2022), ICT (Yuan et al., 2023), Tri-Mentoring (Chen et al., 2023a), PGS (Chemingui et al., 2024),\nFGM (Grudzien et al., 2024), and Match-OPT (Hoang et al., 2024) 2.\nTraining settings. We set the size n of training dataset to 10000, and following LETOR 4.0 (Qin\n& Liu, 2013; Qin et al., 2010b), a prevalent benchmark for LTR, we set the list length m = 1000.\nTo make a fair comparison to regression-based methods, following Trabucco et al. (2021; 2022);\nChen et al. (2023a); Yuan et al. (2023), we model the surrogate model f_\u03b8 as a simple multilayer\nperceptron with two hidden layers of size 2048 using PyTorch (Paszke et al., 2019). We use ReLU as\nactivation functions. RankCosine (Qin et al., 2008) and ListNet (Cao et al., 2007) will be used as\ntwo main loss functions in our experiments. The model is optimized using Adam (Kingma & Ba,\n2015) with a learning rate of 3 \u00d7 10^{-4} and a weight decay coefficient of 1 \u00d7 10^{-5}. After the model\nis trained, following Chen et al. (2023a); Yuan et al. (2023), we set \u03b7 = 1 \u00d7 10^{-3} and T = 200 for\ncontinuous tasks, and \u03b7 = 1 \u00d7 10^{-1} and T = 100 for discrete tasks to search for the final design. All\nexperiments are conducted using eight different seeds. Additional training details are provided in\nAppendix D.4.\nEvaluation and metrics. For evaluation, we use the oracle from Design-Bench and follow the\nprotocol of prior works (Trabucco et al., 2021; 2022). That is, we identify k = 128 most promising\ndesigns selected by an algorithm and report the 100th percentile normalized ground-truth score. A\ndesign score y is normalized via computing (y - y_{min})/(y_{min} \u2013 y_{max}), where y_{min} and y_{max} denote\nthe lowest and the highest scores in the full unobserved dataset from Design-Bench. We also provide\nthe 50th percentile normalized ground-truth results in Appendix E.1."}, {"title": "4.2 EXPERIMENTAL RESULTS", "content": "Main results. In Table 2, we report the results of our experiments, where our method based on\nRanking Model is denoted as RaM appended with the name of the employed ranking loss. Among\nthe compared 22 methods, RaM-RankCosine and RaM-ListNet achieve the two best average ranks,\n2.7 and 2.2, respectively, while the third best method, BDI, only obtains an average rank of 5.9. We\ncan observe that RaM-RankCosine performs best on one task, TF-Bind-10, and is runner-up on two\ntasks, Superconductor and TF-Bind-8; and RaM-ListNet performs best on two tasks, D'Kitty and\nSuperconductor. These results clearly demonstrate the superior performance of our proposed method.\nInfluence of different ranking loss. We compare RaM with various ranking losses: Sigmoid-\nCrossEntropy (SCE), BinaryCrossEntropy (BCE), and MSE 3 for pointwise loss; RankNet (Burges"}, {"title": "5 CONCLUSION", "content": "Offline MBO methods often learn a surrogate model by minimizing MSE. In this paper, we question\nthis practice. We empirically show that MSE has a low correlation with the final performance of\nthe surrogate model. Instead, we show that the ranking-related metric AUPRC is well-aligned with\nthe primary goal of offline MBO, and propose a ranking-based model for offline MBO. Extensive\nexperimental results show the superiority of our proposed ranking-based model over a large variety\nof state-of-the-art offline MBO methods. We hope this work can open a new line of offline MBO."}, {"title": "A RELATED WORK: OFFLINE MODEL-BASED OPTIMIZATION", "content": "Offline MBO methods (Trabucco et al., 2022; Xue et al., 2024) can be generally categorized into\ntwo types of approaches. The mainstream approach for offline MBO is the forward approach, which\nfirst trains a forward surrogate model f_\u03b8 : X \u2192 R and then employs gradient ascent to optimize the\nlearned surrogate to output candidate solutions, as introduced in Section 2. A crucial challenge of this\napproach is how to improve the surrogate model's generalization ability in the OOD regions, which\ncan significantly affect the performance. Prior works of forward approach mainly add regularization\nitems to: 1) regulate the nature of the surrogate model: NEMO (Fu & Levine, 2021) optimizes the\ngap between the surrogate model and the ground-truth function via normalized maximum likelihood,\nwhile ROMA (Yu et al., 2021) enhances the smoothness of the model in a pre-trained and adaptation\nmanner, and BOSS (Dao et al., 2024) directly regulates the sensitivity of the surrogate model; 2)\nregulate surrogate model's predictions directly: COMs (Trabucco et al., 2021) penalize identified\noutliers via a GAN-like procedure (Goodfellow et al., 2014), whereas IOM (Qi et al., 2022) maintains\nrepresentation invariance between the training dataset and design candidate. Given that an ensemble\nof surrogate models can bring an improvement (Trabucco et al., 2022), ICT (Yuan et al., 2023)\nand Tri-Mentoring (Chen et al., 2023a) train three symmetric surrogate models and ensemble them,\nwhere ICT uses a semi-supervised learning via pseudo-label procedure (Verma et al., 2022) and Tri-\nMentoring employs a strategy similar to Tri-training (Zhou & Li, 2005) from a pairwise perspective.\nRecent works also consider uncovering the structural information of the dataset for better learning.\nBDI (Chen et al., 2022) utilizes both forward and backward mappings to distill knowledge from the\noffline dataset to the design. FGM (Grudzien et al., 2024) considers a novel modeling, which splits\nthe design space into cliques on dimension-level, to approximate scores. Both PGS (Chemingui et al.,\n2024) and Match-OPT (Hoang et al., 2024) construct trajectories from the dataset, while PGS uses\noffline reinforcement learning to learn a policy that predicts the search step size of the gradient ascent\noptimizer and Match-OPT enforces the model to match the ground-truth gradient. Recent works also\nconsider editing the final designs directly, for example, DEMO (Yuan et al., 2024) edit the designs\nobtained by gradient ascent via a diffusion prior. However, all prior works in forward approach train\nthe surrogate model based on regression model using MSE. In this work, we train the surrogate model\nin a ranking suite, obtaining superior performance as shown in Section 4.\nAnother type of approach for offline MBO is the backward approach, which typically involves\ntraining a conditioned generative model p_\u03b8(x|y) and sampling from it conditioned on a high score,\nfor example, MINs (Kumar & Levine, 2020) trains an inverse mapping using a conditioned GAN-\nlike model (Goodfellow et al., 2014; Mirza & Osindero, 2014), while CbAS (Brookes et al., 2019;\nFannjiang & Listgarten, 2020) models it as a zero-sum game via a VAE (Kingma & Welling,\n2014). Note that generative models show powerful expressiveness and have achieved huge success.\nDDOM (Krishnamoorthy et al., 2023b) directly parameterizes the inverse mapping with a conditional\ndiffusion model (Ho et al., 2020) in the design space. LEO (Yu et al., 2024) constructs a latent space\nthrough an energy-based model that does not require MCMC sampling. Recent works in this category\nalso focus on generating designs via constructed trajectories. For example, BONET (Krishnamoorthy\net al., 2023a) uses trajectories to mimic a black-box optimizer, thus to train an autoregressive model\nand sample designs using a heuristic; GTG (Yun et al., 2024) considers improving the quality of\ntrajectories via local search, and then directly generate trajectories using a context conditioning\ndiffusion model."}, {"title": "B PREVALENT SETTINGS OF \u222e, \u03a9(\u03c6), AND C\u2090(\u03c6) IN THEOREM 2", "content": "In this section, we introduce some settings of \u03c6, \u03a9(\u03c6), and C\u2090(\u03c6) in Theorem 2, as shown in Lan\net al. (2009).\nIn Theorem 2, \u03c6 is an increasing and strictly positive transformation function, which maps the output\nof the surrogate model or the score to a positive real number. Recall that B represents the upper\nbound of the weight norm ||w|| of the linear function class F = {x \u2192 w^Tx | ||w|| \u2264 B} where\nthe ranking model f to be learned is from, and M is the upper bound of the norm of designs ||x|| in\ndesign space X. It is usually represented as a:\n\u2022 Linear function: \u03c6\u2081(z) = az + b, z \u2208 [\u2212BM, BM], where a > 0 and b > aBM;\n\u2022 Exponential function: \u03c6_E(z) = exp(az), z \u2208 [\u2212BM, BM], where a > 0;"}, {"title": "C DETAILS OF DIFFERENT RANKING LOSSES", "content": "In this section, we introduce details of the different ranking losses in this paper, including traditional\nand recently prevalent losses. We study different types of ranking losses in this paper, including\npointwise (Crammer"}]}