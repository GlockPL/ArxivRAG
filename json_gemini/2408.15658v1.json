{"title": "An Empirical Study on Self-correcting Large Language Models for Data Science Code Generation", "authors": ["THAI TANG QUOC", "DUC HA MINH", "THO QUAN THANH", "ANH NGUYEN-DUC"], "abstract": "Large Language Models (LLMs) have recently advanced many applications on software engineering tasks, particularly the potential for code generation. Among contemporary challenges, code generated by LLMs often suffers from inaccuracies and hallucinations, requiring external inputs to correct. One recent strategy to fix these issues is to refine the code generated from LLMs using the input from the model itself (self- augmented). In this work, we proposed a novel method, namely CoT-SelfEvolve. CoT-SelfEvolve iteratively and automatically refines code through a self-correcting process, guided by a chain of thought constructed from real-world programming problem feedback. Focusing on data science code, including Python libraries such as NumPy and Pandas, our evaluations on the DS-1000 dataset demonstrate that CoT-SelfEvolve significantly outperforms existing models in solving complex problems. The framework shows substantial improvements in both initial code generation and subsequent iterations, with the model's accuracy increasing significantly with each additional iteration. This highlights the effectiveness of using chain-of-thought prompting to address complexities revealed by program executor traceback error messages. We also discuss how CoT-SelfEvolve can be integrated into continuous software engineering environments, providing a practical solution for improving LLM-based code generation.", "sections": [{"title": "1 Introduction", "content": "Code generation aims to automatically produce source code based on given specifications or requirements, which enables developers to save time by reducing manual implementation efforts and allows them to focus on more innovative activities [12]. The recent advancements in Large Language Models (LLMs) have significantly advanced this area [1, 10, 45]. LLMs or Foundation Models (FMs)s are Artificial Intelligence models built with extensive pre-trained corpus, leveraging Authors' Contact Information: Thai Tang Quoc, tqthai.sdh222@hcmut.edu.vn; Duc Ha Minh, hmduc.sdh211@hcmut.edu.vn; Tho Quan Thanh, qttho@hcmut.edu.vn, Ho Chi Minh City University of Technology, Ho Chi Minh City, Vietnam; Anh Nguyen-Duc, University of South Eastern Norway, B\u00f8 i Telemark, Norway, anh.nguyen.duc@usn.no. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. FSE2025, June 23-27, 2025, Trondheim, Norway \u00a9 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06 https://doi.org/XXXXXXX.XXXXXXX Vol. 1, No. 1, Article. Publication date: August 2025."}, {"title": "2 Related Work", "content": "Software defects represent a pervasive and persistent challenge throughout the software development lifecycle, leading to significant financial losses and jeopardizing human safety. The consequences of software defects can be far-reaching, as demonstrated by high-profile incidents like the WannaCry ransomware attack, which exploited the EternalBlue vulnerabilities [2], and the Boeing 737 Max crashes attributed to software design flaws [33]. In 2020 alone, the United States incurred an estimated $2.08 trillion in costs due to poor software quality [24]. Addressing software defects typically involves laborious testing and patching, consuming valuable developer time and resources. The increasing complexity of software systems further exacerbates this challenge. To alleviate this burden and enhance software reliability, Automated Program Repair (APR) techniques have emerged as a promising solution [15, 18, 34]. APR aims to automate the identification and repair of software defects, transitioning from manual effort to precise and efficient automated solutions. Over the past decade, APR has witnessed significant advancements and garnered considerable attention from academia and industry. Research efforts have explored various facets of APR, including fault localization [49], patch assessment [47], and APR evaluation methodologies [28]. Industry leaders like Meta [4, 32] and Alibaba [30, 57] are actively investigating the practical application of APR in real-world software development environments. While traditional and ML-based APR techniques have made significant improvements, they often struggle to generate diverse patches, limiting their effectiveness in addressing complex bugs. This limitation stems from their reliance on bug-fixing datasets, either for crafting fix templates (traditional approaches) or directly predicting potential patches (learning-based methods). On the other hand, LLMs trained on massive text and code datasets offer a promising avenue to overcome this bottleneck. Their vast knowledge base and ability to generate human-like code suggest the potential to devise more sophisticated and varied repairs. Recent explorations have begun to leverage LLMs directly for APR, bypassing the need for explicit bug-fixing datasets [51, 52]. However, these initial attempts have either relied on earlier LLM architectures or lacked evaluation on realistic datasets, leaving the full potential of modern LLMs in the realm of APR largely unexplored."}, {"title": "2.2 Improving the performance of LLMs", "content": "This section briefly presents relevant work that attempts to improve the performance of LLMs via either learning from human feedback or automated feedback sources."}, {"title": "2.2.1 Via Learning from Human Feedback", "content": "LLMs are trained to predict the next word, a process that does not inherently align with human values or preferences, often resulting in harmful, misleading, or biased content. To address this, researchers have integrated human feedback to align LLMs with human values better. Although this research primarily focuses on automated feedback, key works in human feedback are briefly discussed."}, {"title": "2.2.2 Via Learning with Automated Feedback", "content": "Collecting human feedback is resource-intensive, prompting studies to explore automated feedback to reduce reliance on human intervention [37]. Human feedback involves quality assessments by human evaluators, while automated feedback is obtained offline without human evaluations. This section discusses training-time strategies using extrinsic feedback from external metrics/ models and intrinsic inputs from the language model. External metrics are commonly used for training-time correction through non-differentiable training techniques. For instance, Minimum Risk Training [43] optimizes model parameters by incorporating metric scores into the loss function. However, this can lead to robustness issues with some metrics like BLEURT [42]. Liu et al. [29] use a contrastive learning framework to rerank candidates based on metric scores, while Li et al. [26] employ a deep RL algorithm. Other methods include leveraging Gumbel softmax for distributional semantic rewards [46], and using contrastive discriminators with PPO to stabilize gradients [50]. Recently, Chang et al. [7] proposed RLGF, a more efficient RL algorithm than PPO [41], to fine-tune LLMs with pre-defined rewards. Korbak et al. [23] use conditional training and automated classifiers to tag undesirable content at the pretraining stage."}, {"title": "2.3 Self-correcting LLMs", "content": "The \u2018self-correcting' LLMs refer to a set of LLM-based frameworks designed to facilitate post hoc corrections by having the models generate feedback and refine their own output. Initially, the LLM produces an output and then evaluates it, providing feedback to improve the output. This iterative process continues until the output meets the desired quality or a predetermined number of attempts is reached. The Self-Refine framework [31] introduces an effective self-correcting method using a single powerful pre-trained LLM to generate output, provide feedback, and refine the output based on that feedback. All steps are executed by the same LLM and guided by different prompts. In the context of Clinical Self-Verification [16], this framework is applied to extract patient data from clinical notes, generate feedback to identify missing elements and validate the generated data. The output is then refined by removing unsupported elements. Reflexion [44] addresses the limitation of prior self-correcting research, which focused on single-turn generation tasks and did not maintain a record of past errors. Reflexion proposes using a \u2018long-term memory' to store prior feedback and outputs, preventing the repetition of previous mistakes. It also enhances Self-Refine by incorporating scalar-valued feedback and other feedback forms. The SelfEvolve framework is a two-step method that utilizes language models to enhance knowledge autonomously and refine code without external databases. It consists of two primary components: initial code generation based on enhanced prompts and subsequent code revision through feedback mechanisms [20]. This process ensures that the output from the first step is not degraded in the second, allowing for sequential optimization."}, {"title": "2.4 Self-correcting LLMs in APR", "content": "Approaches in APR traditionally operate under a \u2018near-correct assumption' [55], which suggests that experienced programmers can write almost correct programs, requiring only minor modifications to fix bugs and ensure all test cases pass. This assumption has long been the foundation of APR research. However, the evolution of LLMs and their application in programming indicates a future where APR can move beyond its traditional boundaries towards a more integrated approach with fully autonomous programming. In this new context, APR can be reimagined not merely as a tool for correcting minor coding errors but as a crucial component of a self-correcting, self-improving system that iteratively enhances the quality of automatically generated code. Initial explorations into combining repair with programming have been observed, though they still need to be increased. For example, Fan et al. [13] utilize LLMs to fix buggy solutions generated by the models themselves, and recent studies [53, 56] iteratively refine auto-generated code through dynamic execution. The future integration of APR with fully autonomous programming presents vast opportunities. Firstly, this integration allows for developing collaborative Human-AI Programming tools, where developers write the initial code continuously optimized and repaired by LLMs. For complex problem-solving, LLMs can propose innovative solutions that human programmers might not consider, accelerating development cycles, reducing the debugging burden on developers, and fostering more creative and effective solutions. Secondly, the general knowledge embedded in LLMs enables them to support multiple downstream tasks, bridging the gap between code generation, testing, debugging, and fixing. For instance, fault localization is a prerequisite for patch generation, while patch validation reflects the accuracy of fault localization, making these tasks interconnected. Exploring the capabilities of LLMs in these interconnected tasks using real-time feedback within a unified framework is a promising direction for future research."}, {"title": "3 Proposed approach", "content": "We present a new framework, CoT-SelfEvolve, which combines CoT (Chain of Thought) prompting with insights from human discussions on StackOverflow in an existing SelfEvolve framework. The SelfEvolve framework is a two-step method designed to autonomously enhance knowledge and refine code using language models, without relying on external databases. It integrates initial code generation from enhanced prompts with a subsequent revision process through feedback mechanisms [20]. This ensures sequential optimization by maintaining the quality of outputs across each step. Additionally, SelfEvolve employs an iterative self-refinement process that simulates"}, {"title": "4 Evaluations", "content": "This section presents our preparation of data, evaluation metrics, and the experimental results for each of the RQs."}, {"title": "4.1 Experimental settings", "content": "The DS-1000 [25] dataset is a benchmark for code generation, encompassing 1,000 data science problems across seven Python libraries, such as NumPy and Pandas. DS-1000 is seen as a notable improvement over earlier LLM benchmarks like HumanEval [10] and MBPP [3], offering more diverse and realistic problem scenarios that mirror true data science challenges involving complex data structures and operations. The automated evaluation is highly reliable, with only 1.8% of Codex-002-predicted solutions accepted by their evaluation system needing to be corrected. The current leading public LLM, Codex-002, achieves an accuracy of 43.3%, indicating substantial room for improvement [25]. DS-1000's rigorous evaluation system and its design to prevent solution memorization make it a robust platform for benchmarking LLMs, providing a cost-effective and comprehensive testbed that reflects real-world use cases. In this project, we use the DS-1000 to benchmark the CoT-SelfEvolve model, focusing on the Completion type of questions to maintain focus and manageability, although both Completion and Insertion types present similar levels of difficulty."}, {"title": "4.1.2 External Knowledge Base", "content": "StackOverflow is a comprehensive resource for programmers worldwide, providing an extensive knowledge repository on various programming languages, libraries, and frameworks. Discussions and queries on this platform have generated CoT prompts for LLMs in the proposed solutions. A substantial data dump from StackOverflow was procured, filtered, and cleaned to make it suitable. This refined dataset, comprising 558,402 posts and 972,513 related comments, proves invaluable for guiding LLMs in generating CoT prompts. The original XML-formatted data, including each post and its associated comments, undergoes a comprehensive cleansing process to make it suitable. Once cleaned, these elements are structured into a document of the following format: Post: <content_of_post> Comment: <content_of_comment_1> Comment: <content_of_comment_2> Comment: <content_of_comment_n> Given the context window size limit of approximately 4,000 tokens for many LLMs, it is crucial to maintain the total number of tokens in the document below 3,000. This precautionary measure ensures ample space is preserved for the original question. A greedy allocation algorithm is implemented to manage the allocation of comments within a post while adhering to this limit. Moreover, we also set a lower bound limit so that each valid post must have at least 10 comments. Consider a post P, N comments, and a function f that calculates the number of tokens in a string. The algorithm operates as follows: initialize an empty list docs for i from 1 to N-1: initialize an empty list doc append P to doc length = f(P) comment_count = 0 for j from i to N-1: if length + f(comment[j]) < 3000: append comment[j] to doc length = length + f(comment[j]) comment_count += 1"}, {"title": "4.1.3 Evaluation Metrics", "content": "In Jiang et al. work [20], the metric pass@1 is defined as the proportion of unit tests successfully executed on the first attempt, allowing for additional iterations solely for syntax correction. However, this metric might not be suitable for our experiment, as it does not reflect the total number of iterations the LLM needs to produce a specific correct code snippet. Therefore, we adopted an alternative metric, inspired by Chen et al's SelfDebug model [11]. In SelfDebug, the authors set the maximum number of debugging turns to 10, though empirically, successful debugging processes mainly conclude within 3 turns, and they report accuracy as the primary metric. In this work, CoT-SelfEvolve is measured by pass@n, where n represents the maximum number of attempts allowed for the model to resolve the problem. This metric is the proportion of problems for which the model successfully passes all unit tests within n attempts. Given our limited resources, we evaluate n from 1 to a maximum of 5. This approach offers greater flexibility and enables a more realistic evaluation of the model's performance while remaining within our resource constraints. The proportion of problems for which the model successfully passes all unit tests is actually a form of strict accuracy. As suggested by Hendrycks et al. [19], future research may only use strict accuracy when models become sufficiently capable, as this metric ensures that generated solutions are robust and comprehensive."}, {"title": "4.2 Results", "content": "To compare with the performance of SelfEvolve [20], we conduct the same experiments with CoT- SelfEvolve on SciPy, PyTorch, Sklearn, and Matplotlib libraries of DS-1000, which includes a total of 444 problems. We use GPT3.5 (version gpt-3.5-turbo-1106) in this experiment due to its popularity and wide usage in code generation research. The performance is measured by pass@5 metric. As shown in Figure 2, it is apparent that CoT-SelfEvolve significantly outperforms SelfEvolve in three libraries, PyTorch, Sklearn, and Matplotlib, and achieves nearly equivalent performance to SelfEvolve on SciPy problems. Moreover, we conducted the same experiment settings on different LLMs to understand the impact of CoT on different LLM models (Figure 3). The overall performance is averaged from model performance on the seven DS-1000 libraries. As expected, CoTEvolve gives the best performance with GPT-4. It is interesting that models with smaller sizes (i.e., Claude 2.1 with 70 billion parameters) have performed better than the models with larger sizes (GPT-3.5 with 175 billion parameters)."}, {"title": "4.2.2 RQ2: Does the Auto-CoT prompt generator improve the model performance?", "content": "To investigate the impact of Auto-CoT prompt generators, we conducted two experiments. In the first experiment, we ran four different setups, each with the Auto-CoT prompt generator either enabled or disabled (denoted by Auto-CoT 1 and 2 with the status on or off). When the Auto-CoT prompt generator is disabled, the problem description or feedback is directly input into the LLM to generate code. The results of these experiments are presented in Table 1."}, {"title": "4.2.3 RQ3: How does increasing the number of attempts affect model performance? How many tokens does the models consume?", "content": "Table 3 presents the accuracy results on the DS-1000 dataset across different libraries by varying the maximum allowed attempts (n) from 1 to 5, with the experiments conducted using GPT-4. The results demonstrate that increasing the number of attempts significantly improves the accuracy across all libraries. For instance, the overall accuracy increases from 14.0% at n = 1 to 83.2% at n = 5. A notable observation is the significant increase in accuracy from n = 1 to n = 2. This sharp rise can be attributed to the activation of the self-correction loop starting at the second attempt. As depicted in Figure 4, there is a marked increase in the number of tasks achieving the stop condition at the second attempt. This indicates that the initial attempt, which relies solely on the problem's description and CoT prompting, benefits greatly from the iterative feedback provided in subsequent attempts. This feedback loop enables the LLM to refine its solutions, leading to a marked improvement in problem-solving effectiveness."}, {"title": "5 Discussions", "content": "5.1 Discussing our findings Comparing CoT-SelfEvolve with other self-correcting framework like Self-refined [31] or self- debugged [11] is difficult due to different tasks or evaluation metrics. In RQ1, we compared our performance with the base model SelfEvolve [20]. The result shows that a consistent improvement"}, {"title": "5.2 Practical Implications", "content": "CoT-SelfEvolve framework presents an opportunity for automating code generation where human intervention is limited, or unreliable. By integrating diverse knowledge bases, ranging from internal company codebases that document bug fixes to external platforms like StackOverflow, CoT- SelfEvolve adapts to the specific needs of various applications. This flexibility not only streamlines the debugging process but also ensures that the framework remains robust and contextually aware. Regarding feasibility, CoT-SelfEvolve has a continuous unit testing framework as its feedback source. It will be quite straightforward to integrate output from CoT-SelfEvolve into the CI/CD process, adapting to new code and evolving requirements without additional overhead. An il- lustration for such integration is the Intefix tool from Microsoft (Figure 6). In this setup, a pull request initiates a sequence of CI steps, including unit testing and static analysis, culminating in automated bug detection and patch proposal via an LLM-based module. CoT-SelfEvolve offers a flexible, open-source alternative that can iteratively process and refine code patches, mimicking the cycle of testing and patching seen in proprietary systems like InferFix. The adoption of LLMs with extended context windows, such as Gemini 1.5 [38], which supports up to 2 million tokens, would improve the applicability of this approach in real software projects."}, {"title": "5.3 Threats to validity", "content": "Our study's primary threat to internal validity stems from the reliance on proprietary LLMs from external vendors such as AWS, OpenAI, and Azure. As these models are managed and updated by third parties, we need to rely on the trustworthiness of the vendors' performance reports. Indeed, fluctuations in the performance of proprietary LLMs have been observed and documented within"}, {"title": "6 Conclusions", "content": "In this paper, we introduced CoT-SelfEvolve, a novel framework designed to enhance the capabilities of Large Language Models (LLMs) in generating accurate and reliable code, particularly for data science tasks involving complex libraries such as NumPy and Pandas. Our approach leverages a self-correcting mechanism guided by a Chain-of-Thought (CoT) process, enriched with exter- nal knowledge from developer forums like StackOverflow. Our experiments show a significant improvement compared to base models. At the moment, one limitation of the current CoT-SelfEvolve framework is that each attempt to solve a problem is treated as an independent instance. In future work, we can leverage metadata from these attempts, such as the correctness of the solution, the number of attempts required, and the token cost. Inspired by the innovative DSPy framework [22], which is the first to optimize prompts automatically, we see potential in adopting similar strategies. The authors of DSPy propose two methods for auto-optimizing prompts: (1) providing demonstrations of the original setup of LLMs based on provided labels and metrics, allowing LLMs to learn from both successful and unsuccessful executions, and (2) rewriting the original prompts and collecting metrics to find an optimal version. The former is more suitable for our context, as the Auto-CoT generator handles our prompt generation. As shown in Figure 7, our next step will be to collect metrics to strategically select practical demonstrations (i.e., executed instances of the LLM system from input to output) to present to the Auto-CoT prompt generator."}, {"title": "A Prompts", "content": "You are a helpful Chain-of-Thought expert named as CoT-Guru that can understand the reasoning behind programming problems and provide step-by-step guidance to solve them. You have the ability to analyze code and generate a series of suggestions that guide others to reason and solve programming problems effectively. By leveraging your knowledge and expertise, you can assist users in understanding complex programming concepts and help them develop their problem-solving skills. Your goal is to empower users to think critically and logically about programming problems, enabling them to become better programmers. Given the problem description with the code, and one or multiple StackOverflow posts, you need to learn from the comments to generate step-by-step suggestions that help another agent (CoT-SelfEvolve) to solve the problem. The given problem is: {problem_description}. The StackOverflow post with supportive comments is: {post}. Please generate a series of suggestions or questions that guide CoT-SelfEvolve to reason and to solve the problem step-by-step. Here are some suggestions: Suggestion 1: [You should ...] Suggestion 2: [, then ...] Suggestion 3: [, then ...] Final suggestion: [, and finally, ...]"}, {"title": "A.2 Example of generated CoT prompt (1)", "content": "Sure! Here are some suggestions to guide CoT-SelfEvolve in solving the problem step-by-step: Suggestion 1: First, let's understand the problem. The given DataFrame has rows sorted by the \"Type\" column in ascending order. You want to shuffle the rows according to a given list. Is that correct? Suggestion 2: To shuffle the rows, you can use the `iloc`function in pandas. It allows you to select rows based on their integer position. You can pass the desired order of rows as a list to the `iloc` function. Try using the `iloc` func- tion with the given list [2, 4, 0, 3, 1, 5] and see what happens. Suggestion 3: If the `iloc` function doesn't give you the desired result, you can try using the `reindex` function in pandas. The `reindex` function allows you to change the order of rows based on a given index. You can pass the desired order of rows as a list to the `reindex`function. Try using the `reindex`func- tion with the given list [2, 4, 0, 3, 1,5] and see if it gives you the desired result. Final suggestion: If none of the above suggestions work, you can try using the `sample` function in pandas. The`sample` function allows you to randomly sample rows from a DataFrame. You can pass the number of rows you want to sample and"}, {"title": "A.3 Auto-CoT Prompt Generator (2)", "content": "Given the problem description with the code, and the code generated by another agent (CoT-SelfEvolve) together with the feedback from the system, you need to generate step-by-step Chain-of-Thought reasoning to help the CoT-SelfEvolve to solve the problem by himself. The given problem is: {problem_description}. In the previous attempt, CoT-SelfEvolve generated the following code: GENERATED_CODE: {generated_code} And it received the following feedback: FEEDBACK: {feedback} In general, the steps to reason about the problem are: Step 1: understand the problem, what does it require? Step 2: analyze the GENERATED_CODE vs problem, what is the problem? does it related with the defined problem? Step 3: analyze the FEEDBACK, what is the error message? what is the expected output? Please help the CoT-SelfEvolve agent by providing step-by-step guidance to solve the problem. DO NOT attempt to solve the problem directly. Remember that you are helping another agent to solve the problem, not solving the problem directly."}, {"title": "A.4 Example of generator CoT prompt (2)", "content": "Sure! Let's go through the steps to reason about the problem and help the CoT- SelfEvolve agent. Step 1: Understand the problem The problem requires shuffling the order of the DataFrame's rows according to a given list. Step 2: Analyze the GENERATED_CODE vs problem The GENERATED_CODE is incomplete and needs to be modified to solve the problem. The `df` and `List` variables are loaded from the pickle file, but the code in- side the `[insert]`block is missing. The goal is to shuffle the rows of the DataFrame `df` according to the list `List`. Step 3: Analyze the FEEDBACK"}, {"title": "A.5 Correction Code Generator", "content": "In the previous attempt, you generated the following code: GENERATED_CODE: {generated_code} However, the system has given you the following instruction: FEEDBACK: {feedback} Please comply with the instruction and generate the code accordingly. To support you in solving the problem, here are the Chain-of-Thought reasoning suggestions, you should follow these suggestions one by one, to use them as a guide for your internal reasoning process to solve the problem. {cot_suggestion} Received XXX; revised XXY; accepted XXZ"}]}