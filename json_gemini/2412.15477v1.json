{"title": "Difficulty-aware Balancing Margin Loss for Long-tailed Recognition", "authors": ["Minseok Son", "Inyong Koo", "Jinyoung Park", "Changick Kim"], "abstract": "When trained with severely imbalanced data, deep neural networks often struggle to accurately recognize classes with only a few samples. Previous studies in long-tailed recognition have attempted to rebalance biased learning using known sample distributions, primarily addressing different classification difficulties at the class level. However, these approaches often overlook the instance difficulty variation within each class. In this paper, we propose a difficulty-aware balancing margin (DBM) loss, which considers both class imbalance and instance difficulty. DBM loss comprises two components: a class-wise margin to mitigate learning bias caused by imbalanced class frequencies, and an instance-wise margin assigned to hard positive samples based on their individual difficulty. DBM loss improves class discriminativity by assigning larger margins to more difficult samples. Our method seamlessly combines with existing approaches and consistently improves performance across various long-tailed recognition benchmarks.", "sections": [{"title": "Introduction", "content": "In recent decades, deep neural networks have demonstrated remarkable success in image recognition tasks, largely due to the availability of large-scale datasets like ImageNet . However, real-world datasets often exhibit an imbalanced distribution, known as a long-tailed distribution, wherein a few 'head' classes contain a large number of samples, while numerous other classes, referred to as 'tail' classes, contain significantly fewer samples. This imbalance presents significant challenges: deep learning models, predominantly trained on the abundant majority classes, struggle to effectively learn features for the minority classes. As a result, models tend to underperform on these underrepresented classes, compromising their overall accuracy.\nAddressing class imbalance has been a focal point in long-tailed recognition (LTR) research. Existing methods have employed various strategies to rebalance the influence of different classes. Re-sampling techniques, such as oversampling and undersampling , adjust the occurrence of class samples to create a more balanced training set. Re-weighting approaches modify class weights or logit values to emphasize learning from difficult minority classes. For instance, the label-distribution-aware margin (LDAM) loss introduces larger margins for minority classes to counteract the bias towards majority classes. Despite these advances, many methods focus primarily on class-level imbalance and often overlook variations in difficulty among individual samples within each class. This oversight can lead to suboptimal performance on challenging instances, even within well-represented classes."}, {"title": "Related Work", "content": "Long-tailed Recognition\nLong-tailed recognition (LTR) has been extensively explored through multiple perspectives. Conventional approaches focus on rebalancing the bias introduced by imbalanced class influence during training, aiming to mitigate performance degradation for minority classes. Re-sampling methods address the class imbalance by either undersampling majority classes or oversampling minority classes . Re-weighting methods propose class-discriminative losses to emphasize the relative contribution of minority classes. Logit compensation methods adaptively adjust logit values based on prior knowledge of the sample distribution for balancing.\nAnother line of LTR research focuses on enhancing the robustness of representation learning to reduce model bias.  demonstrated that applying class re-balancing methods in the later stages of training can be more effective than conventional one-stage methods. proposed decoupling the training of the feature extractor from the classifier, which inspired later two-stage approaches. Augmentation-based methods aim to improve the sample diversity for tail classes. Inspired by the robust feature representation learned through self-supervision , variants of supervised contrastive learning methods have been introduced to LTR. integrated contrastive learning with logit compensation by introducing a Gaussian mixture likelihood loss, aiming to maximize mutual information between latent features and the ground truth labels. They employed a teacher-student strategy to generate contrast samples using a pre-trained teacher encoder. Ensemble-based methods exploit the complementary knowledge from multiple experts through various incorporation methods, such as routing and distillation .\nMost LTR studies assume that the tail classes are inherently more difficult to learn and therefore assign more weights to less frequent classes. However, some recent works observed that actual class-specific performance does not always correlate with class frequency. In response, they tried to consider classification difficulty in addition to sample distribution for re-weighting. We share a similar motivation and introduce an adaptive margin loss that makes instance-level adjustments based on the angular distance between the positive class center and the sample feature.\nMargin Loss\nLarge-margin softmax loss (L-Softmax)  was introduced to enhance feature discrimination by encouraging intra-class compactness and inter-class separability in the embedding space. In the domain of facial recognition, margin losses have been further explored in angular space, utilizing a cosine classifier . These approaches aim to im-"}, {"title": "Proposed Method", "content": "Preliminaries\nLoss functions for Long-tailed Recognition. The cross-entropy loss with softmax function is defined as:\n$L_{CE} = - \\log \\frac{e^{\\psi_y(x)}}{\\Sigma e^{\\psi_i(x)}}$  (1)\nHere, $\\psi_i(x)$ represents the logit function of the i-th class for sample x, which belongs to the class of index y. For models that utilize a linear classifier, the logit function is given by $\\psi_i(x) = W_i \\cdot f(x) + b_i$, where f(x) denotes the feature representation of sample x, and $W_i$ and $b_i$ represent the weight and bias of the linear classifier for the i-th class, respectively. Alternatively, a cosine classifier embeds features and class centers in an L2-normalized space, with logits determined by the angular distance between sample features and class centers. Specifically,\n$\\psi_i(x) = s \\cdot \\frac{W_i \\cdot f(x)}{||W_i|| \\cdot ||f(x)||} = s \\cos \\theta_i, $ (2)\nwhere s is the scaling factor and $\\theta_i$ denotes the angular distance between $W_i$ and f(x).\nIn long-tailed recognition (LTR), re-weighting methods address class imbalance by incorporating class frequency $n_i$ into the loss functions. Variants of cross-entropy loss include the class-balanced (CB) loss  and balanced softmax (BS) . The class balanced loss $L_{CB}$ is formulated as:\n$L_{CB} = \\frac{1-\\beta}{1-\\beta^{n_y}} \\log \\frac{e^{\\psi_y(x)}}{\\Sigma e^{\\psi_i(x)}},$ (3)\nintroducing a class-wise weight determined by the effective number of samples given a hyperparameter $\\beta$. The balanced softmax loss $L_{BS}$ is:\n$L_{BS} = -\\log \\frac{e^{\\psi_y(x)+\\log p_y}}{\\Sigma_i e^{\\psi_i(x)+\\log p_i}},$ (4)\nwhere $p_i$ represents the sample proportion of the i-th class over all classes, i.e., $p_i = n_i / \\sum_j n_j$. The balanced softmax loss is widely adopted in later LTR studies, such as balanced contrastive learning (BCL) and nested collaborative learning (NCL) .\nMargin-based Variants of Cross-entropy Loss. Margin losses introduce a specialized logit function associated with a margin for the positive class. A margin-based cross-entropy loss $L_m$ can be generally formulated as:\n$L_m = - \\log \\frac{e^{s\\psi_m (\\theta_y)}}{e^{s\\psi_m (\\theta_y)} + \\Sigma_{i \\neq y} e^{s \\cos \\theta_i}},$ (5)\nwhere $\\psi_m (\\theta_y)$ denotes the logit function for the positive class incorporating the margin. If $\\psi_m (\\theta_y)$ adopts no margin, i.e., $\\psi_m (\\theta_y) = \\cos {\\theta_y}$, $L_m$ is equivalent to $L_{CE}$.\nDifficulty-aware Balancing Margin Loss\nOur difficulty-aware balancing margin (DBM) loss com-prises two components: a class-wise margin and an instance-wise margin. By integrating these two elements, we address both the bias from class imbalance and the variation in instance difficulty within a class. Following prior works , we apply the instance-wise margin specifically to hard positive samples. Figure 2 illustrates the margins determined by class frequency and angular distance. Detailed mathematical descriptions of each component are provided below.\nClass-wise Margin. The class-wise margin $m_c$ is defined as:\n$m_c = K \\rho_y^{\\tau}.$ (6)\nHere, $\\rho_y = n_y/n_{min}$ represents the ratio of the number of samples in class y to the number in the least frequent class. The parameter $\\tau$ controls the extent of the margin differ-ence across classes, while K scales the margin. As illus-"}, {"title": "Loss formulation", "content": "Instance-wise Margin. The instance-wise margin addresses varying sample-level difficulties. Samples with lower positive logit values are more prone to misclassification. For our cosine classifier, difficult samples are those whose feature representations are farther from the positive class center in the hypersphere. We quantify the instance difficulty $d_1$ via following equation:\n$d_1 = \\frac{1-\\cos \\theta_y}{2}.$ (7)\nHere, $d_1$ is determined by the angular distance between the feature representation of the sample and the positive class center $\\theta_y$. A sample with its feature representation exactly at the class center has $d_1 = 0$, while a sample with the feature representation at the maximum distance ($\\theta_y = \\pi$) has $d_1 = 1$.\nThe instance-wise margin $m_1$ is given by:\n$m_1 = m_c d_1$. (8)\nAs illustrated in Fig. 2b, this margin is determined by both $\\rho_y$ and $\\theta_y$, encouraging difficult and less-frequent samples to move more aggressively towards the positive class center.\nLoss formulation. Our DBM loss modifies the angular distance $\\theta_y$ by incorporating both margins, similar to the Ar-cFace approach. Specifically, our logit function for the positive class is formulated as:\n$\\psi_{ydbm} (\\theta_y) = s \\cos(\\theta_y + m_c + \\mathbb{1}[\\mathop{\\arg\\min}_{i \\in \\{0,1\\}} (\\theta_i)_{i=1}^C \\neq y] m_1),$ (9)\nwhere $\\mathbb{1}[\\cdot]$ is an indicator function for applying the instance-wise margin only to hard positive samples. By substituting this logit function into Eq. (5), we derive the difficulty-aware balancing margin cross-entropy (DBM-CE) loss.\nThe DBM loss can be easily integrated with various existing LTR methods. For example, it can be combined with the class-balanced loss introduced in Eq. (3) as follows:\n$L_{DBM-CB} = \\frac{1-\\beta}{1-\\beta^{n_y}} \\log \\frac{e^{s \\psi_{ydbm} (\\theta_y)}}{e^{s \\psi_{ydbm} (\\theta_y)} + \\Sigma_{i \\neq y} e^{s \\cos \\theta_i}}$ (10)\nSimilarly, DBM-BS can be derived as:\n$L_{DBM-BS} = -\\log \\frac{e^{s \\psi_{ydbm} (\\theta_y)+\\log p_y}}{e^{s \\psi_{ydbm} (\\theta_y)+\\log p_y} + \\Sigma_{i \\neq y} e^{s \\cos \\theta_i+\\log p_i}},$ (11)\nreformulating the original balanced softmax loss described in Eq. (4). Note that our method requires adjusting the classifier from a linear to a cosine classifier.\nMoreover, Our method is highly versatile and can be incorporated with a range of other LTR techniques. We demonstrate this versatility with various configurations of our method, including DBM-DRW, DBM-BCL, DBM-GML, and DBM-NCL. DRW, or deferred re-weighting , integrates class-balanced loss into the train-ing process at a later stage, allowing DBM-DRW to be im-plemented by applying $L_{DBM-CE}$ and $L_{DBM-CB}$ sequentially according to the scheduling policy. Similarly, methods like BCL , GML and NCL , which originally use balanced softmax loss, can incorporate our approach by substituting the classification loss with $L_{DBM-BS}$.\nThe integration of DBM loss into existing models does not incur significant additional computational complexity. The class-wise margin $m_c$ is determined in advance based on the known sample distribution, ensuring that this computation does not affect the training time. The instance-wise margin $m_1$ is computed during the logit calculation, leveraging the angular distance $\\theta_y$ that is already part of the model's forward pass. This design ensures that DBM can be incorporated into existing frameworks without introducing substantial overhead."}, {"title": "Experiments", "content": "Datasets\nTo evaluate the performance of our proposed method, we conducted experiments on four benchmark long-tailed datasets. The imbalance factor $\\rho$ of each dataset is defined as the ratio of training instances between the largest and smallest classes, i.e., $\\rho = N_{max}/N_{min}$, following previous works.\nLong-tailed CIFAR-10 and CIFAR-100. We sampled long-tailed CIFAR datasets from the original CIFAR-10 and CIFAR-100 datasets with imbalance factors of 10, 50, and 100 using an exponential down-sampling profile outlined in. Evaluations were performed on the original balanced test sets.\nImageNet-LT. ImageNet-LT is a long-tailed version of ImageNet-1K , sampled from a Pareto distribution with \u03b1 = 6. It comprises 1,000 categories and 115.8K training images, with an imbalanced factor of \u03c1 = 1280/5."}, {"title": "Implementation Details", "content": "For the CIFAR-10-LT and CIFAR-100-LT datasets, we inte-grated our method with several existing approaches includ-ing:\n(1) vanilla cross-entropy (CE)\n(2) CE-DRW , a two-stage training methodapplying CB loss .\n(3) BS , a re-weighting method.\n(4) BCL , a supervised contrastive learning-based method.\n(5) GML , a mutual information maxi-mization method.\n(6) NCL , an ensemble-based method.\nWe ensured a fair comparison by evaluating our models un-der identical experimental conditions. All models utilized ResNet-32  as the backbone network, while ResNet56 was employed as the teacher network for GML.The SGD optimizer with a momentum of 0.9 and weight decay of $2 \\times 10^{-4}$ was employed, along with a learningrate warm-up for the first five epochs and a cosine anneal-ing scheduler for gradual decay. Data augmentation strate-gies included Cutout  and Au-toAugment . For BCL, we used an initiallearning rate of 0.15 and a batch size of 256. For all other"}, {"title": "Long-tailed CIFAR", "content": "Additional Evaluations\nIn the main paper, we demonstrated the performance en-hancement of our method when applied to various LTR tech-niques. Here, we extend our evaluation by comparing our method with additional approaches not include in the main paper.\nComparison with Difficulty-based Approach. Yu etal.  proposed an instance-level re-sampling methodbased on the difficulty of each instance, determined by itslearning speed. Since we share similar motivations for con-sidering instance-level scaling via difficulty, we compare theperformance of both techniques in this section. Followingtheir experiment settings, we evaluate the performance ofResNet-32 on CIFAR-10-LT and CIFAR-100-LT with im-balance factor of 100, 50, and 20. In these experiments,we used a different scheduler, decaying the learning rate atepoch 160 and 180 with a step size of 0.1. Advanced aug-mentation strategies, such as Cutout  and AutoAugment , were not uti-lized in these settings."}]}