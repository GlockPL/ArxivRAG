{"title": "WHAT'S THE MOVE?\nHYBRID IMITATION LEARNING VIA SALIENT POINTS", "authors": ["Priya Sundaresan", "Hengyuan Hu", "Quan Vuong", "Jeannette Bohg", "Dorsa Sadigh"], "abstract": "While imitation learning (IL) offers a promising framework for teaching robots\nvarious behaviors, learning complex tasks remains challenging. Existing IL poli-\ncies struggle to generalize effectively across visual and spatial variations even\nfor simple tasks. In this work, we introduce SPHINX (Salient Point-Based Hy-\nbrid ImitatioN and eXecution), a flexible IL policy that leverages multimodal ob-\nservations (point clouds and wrist images), along with a hybrid action space of\nlow-frequency, sparse waypoints and high-frequency, dense end effector move-\nments. Given 3D point cloud observations, SPHINX learns to infer task-relevant\npoints within a point cloud, or salient points, which support spatial generaliza-\ntion by focusing on semantically meaningful features. These salient points serve\nas anchor points to predict waypoints for long-range movement, such as reaching\ntarget poses in free-space. Once near a salient point, SPHINX learns to switch to\npredicting dense end-effector movements given close-up wrist images for precise\nphases of a task. By exploiting the strengths of different input modalities and ac-\ntion representations for different manipulation phases, SPHINX tackles complex\ntasks in a sample-efficient, generalizable manner. Our method achieves 86.7%\nsuccess across 4 real-world and 2 simulated tasks, outperforming the next best\nstate-of-the-art IL baseline by 41.1% on average across 440 real world trials.\nSPHINX additionally generalizes to novel viewpoints, visual distractors, spatial\narrangements, and execution speeds with a 1.7\u00d7 speedup over the most compet-\nitive baseline. Our website contains code for data collection and training code\nalong with supplementary videos: http://sphinx-manip.github.io.", "sections": [{"title": "1 INTRODUCTION", "content": "Imitation learning (IL) of visuomotor policies is a widely used framework for teaching robots ma-\nnipulation tasks given demonstrations collected by humans (Schaal, 1996). While prior works have\nshown that IL policies can learn a range of behaviors with sufficient data, from simple object pick-\nand-place to more complex tasks, they typically succeed only in highly controlled settings with\nlow variation. Generalizing to realistic visual and spatial variations remains a significant challenge.\nConsider teaching a robot to make a cup of coffee in the morning, which demands precision, long-\nhorizon reasoning, and tolerance to environment variations. The robot must first carefully grasp a\nmug handle, position it under the machine, insert a pod into a narrow slot, close the lid, and press a\nbutton \u2013 all with very little margin for error (Fig. 1). Even after mastering this sequence, the policy\nmight struggle with spatial changes like moving the machine, or visual changes such as new coffee\npods, spilled grounds, a different camera angle, or varying lighting conditions. This underscores the\nneed for IL policies that can learn complex tasks from a limited number of demonstrations while\neffectively generalizing to natural and expected variations in the real-world.\nConventional IL policies often struggle with both performance and generalization, largely due to\nlimitations in their input and output representations. First, they tend to rely heavily on visual inputs\nlike RGB images, treating irrelevant details like the background, lighting, or viewpoint the same as\ntask-relevant information. This can cause a policy to memorize specific scenes, making it brittle to\nvisual variations (Zhao et al., 2023; Chi et al., 2023). Second, these policies usually predict actions"}, {"title": "2 RELATED WORK", "content": "Imitation Learning for Robotics Control: Imitation learning has long been a foundational ap-\nproach in robotics for teaching robots to replicate human demonstrations (Schaal, 1996; Atkeson &\nSchaal, 1997; Pomerleau, 1988). Robotic imitation learning policies typically take images as input\nand output motor commands, such as joint positions, velocities, or Cartesian end-effector poses.\nRecent works of that type (Reuss et al., 2023; Chi et al., 2023; Zhao et al., 2023) have demonstrated\nstrong performance on tasks in controlled settings with a limited initial state distribution. However,\nthey struggle to generalize to unseen visual or spatial variations. To address visual generalization,\nsome works augment vision-based policies with diffusion-generated image observations (Yu et al.,\n2023; Bharadhwaj et al., 2024). While useful and complementary to our approach, these augmen-\ntations do not directly enable spatial generalization. Other works propose replacing image inputs\nwith 3D scene representations such as point clouds and voxel grids, and outputting actions as way-\npoints, 6-DoF poses reachable through motion planning (Sundaresan et al., 2023; Shridhar et al.,\n2023; Yang et al., 2024b). While this reduces the complexity of action prediction from hundreds of\nactions to a single pose, 3D representations such as point clouds often lack the resolution to enable\nprecise manipulation of small objects. Other recent approaches like HYDRA (Belkhale et al., 2023)\nand AWE (Shi et al., 2023) take image inputs but propose a hybrid output action space of waypoints\nand dense actions. These distinct action modes are intended for long-range and precise movements,\nrespectively. Our method builds on these approaches by leveraging salient points to bridge a hybrid\ninput space of point clouds and wrist-camera images, and a hybrid output action space of waypoints\nand dense actions.\nAction Representations: Most visual imitation learning works rely on standard 6-DoF action\nspaces, but recent efforts explore alternatives for better spatial generalization. One approach in-\nvolves predicting actions as parameterized manipulation primitives instead of low-level end-effector\nmovements. This reduces the dimensionality of the action space and improves sample efficiency,\nbut often requires task-specific engineering (Dalal et al., 2021; Sundaresan et al., 2023; Nasiriany\net al., 2022; Agia et al., 2022). Other methods exploit equivariance, ensuring that transformations of\nvisual inputs (e.g., rotations or scaling) are reflected in output actions (Wang et al., 2024; Yang et al.,\n2024a;b). However, these works often rely on limiting assumptions like access to object states via\nsegmentation, or single-object tasks. In the grasping domain, many policies consider point clouds\nas inputs and an output action space defined as per-point predictions for the end-effector pose. This\nhas proven effective for learning sample-efficient and generalizable grasping policies (Saxena et al.,\n2008; Sundermeyer et al., 2021). Inspired by this, our method also parameterizes waypoint actions\nas offsets to salient points in a point cloud, but we critically learn a hybrid policy which predicts\nboth waypoint and dense actions to tackle longer-horizon and precise tasks beyond grasping.\nData Collection for Imitation Learning: Despite the advancements in action representations and\nspatial generalization, the success of visual imitation learning policies still hinges on the quality\nof teleoperated demonstrations. Human operators typically collect robot data using interfaces like\nvirtual reality controllers (Jedrzej Orbik, 2021), handheld devices (Chi et al., 2024), puppeteering\nsetups (Zhao et al., 2023), or 3D mice (e.g., Spacemouse). However, these interfaces map demon-\nstrator controls directly to robot actions on a per-timestep basis, which presents two key limitations.\nFirst, the recorded data only captures (observation, dense action) pairs, lacking compatibility with\nwaypoint actions or useful metadata such as salient points. Second, directly controlling long-range\nmovements can be inefficient, noisy, and tiring for demonstrators. To address these issues, we design\nan interface (Fig. 2) that seamlessly integrates both waypoint and dense action modes. A custom\nweb-based GUI supports waypoint mode, allowing demonstrators to specify salient points and way-\npoints with the ease of simple clicks and drags. A controller can then reach the specified waypoint\nautomatically, removing the need for constant teleoperation from the demonstrator. Additionally,\nthe interface is compatible with any external device for dense actions, allowing for easy switching\nbetween the computer mouse and the device, as long as it is on hand. This provides a flexible and\nefficient data collection process for high-quality hybrid datasets, with no post-hoc labeling required."}, {"title": "3 PROBLEM STATEMENT", "content": "In standard IL, we are given a dataset D of N trajectories of expert demonstrations {T1,..., TN}.\nEach trajectory is a sequence of observation action pairs (00, 00,...,\u043e\u0442, \u0430\u0442). The goal is to learn"}, {"title": "4 SPHINX: SALIENT POINT-BASED HYBRID IMITATION AND EXECUTION", "content": "We introduce SPHINX: Salient Point-based Hybrid ImitatioN and execution, a framework for learn-\ning sample-efficient, generalizable imitation policies capable of handling complex, long-horizon\nmanipulation tasks across diverse initial conditions. SPHINX combines a high-level waypoint pol-\nicy for long-range movements and a dense policy for precise manipulation (Fig. 1). The waypoint\npolicy waypt takes point clouds as input and classifies semantically meaningful salient points along\nwith waypoints relative to them. This guides the robot to a suitable pose for interaction around a\nsalient point, such as reaching for a mug handle during coffee-making. The dense policy wdense\ntakes over only for precise actions around a salient point, like carefully inserting a coffee pod into its\nslot (Fig. 1). Since the waypoint policy handles long-range movements, it uses point clouds to pro-\nvide spatial context. The dense policy uses wrist camera images as input, capturing detailed object\nfeatures for precise manipulation and enabling visual generalization to variations in the surrounding\nscene. Both policies also predict the next mode mt+1 to decide which policy to use after completing\nthe current movement. Without loss of generality, we initialize mo in waypoint mode.\nTo train SPHINX, we first need to collect demonstrations using the two modes and annotate salient\npoint for each waypoint. In Section 4.1, we introduce an intuitive web GUI to easily collect such\ndemonstrations in the hybrid format and record salient points with no additional overhead. Then, we\ndiscuss how to learn \u03c0waypt (wt|ot) and dense (at|ot) in Section 4.2 and Section 4.3 respectively."}, {"title": "4.1 DATA COLLECTION INTERFACE FOR SPHINX", "content": "Without an existing interface that satisfies our need, we design a data collection system to support\nwaypoint specification, salient point annotation, and mode switching seamlessly. Our hardware\nsetup includes two third-person cameras to provide RGB-D observations to construct a colorized\npoint cloud ofcd, and one wrist-mounted camera to provide RGB wrist images ourist. We develop\na custom web-based GUI for specifying waypoints and salient points in waypoint mode. To provide\ndense actions instead, a demonstrator can seamlessly switch from the computer mouse to any dense\nteleoperation device like a VR/game console controller or a 3D mouse (Spacemouse) as in this work.\nThe top row of Fig. 2 visualizes the web-based GUI and the process of recording a waypoint action.\nThe GUI streams the point cloud of N points ocd = {C1, C2, ..., CN } to the browser in real time\nand allows a demonstrator to select a salient point z\u0142 \u2208 {1, ..., N} for each waypoint by clicking\nwithin the point cloud, (e.g. the small red dot on the toy car next to the mouse cursor.) After clicking\non the salient point, a digital twin of the gripper appears near the salient point to facilitate waypoint\nspecification. The demonstrator can set waypoints relative to these salient points with simple click\nand drag interactions on the virtual gripper. The salient point specifies the region of interest for\ninteraction while the waypoint captures how to interact with it by specifying 7 DoF target end-\neffector pose. After specifying a waypoint, the linear controller C defined above interpolates and\nexecutes actions to reach the waypoint. Critically, this removes the need for the demonstrator to\nmanually teleoperate long-range movements. The entire waypoint motion is recorded as a sequence\n{(ot, at, waypt, Wt', \u2248t')}t\u2208{t',...,t'+k\u2081, } where t' is the timestep when the waypoint is specified and\nkt is the number of steps that the controller takes to complete the waypoint wt'.\nOnce the controller finishes executing a waypoint, the demonstrator may specify another waypoint\nor switch to dense mode for precise manipulation. To take over with dense mode, the demonstrator\nsimply operates the teleoperation device, such as pressing or twisting the joysticks on a controller,\nand its movements are automatically detected and mapped to delta movements on the end-effector of\nthe robot. This is illustrated by the bottom row of Fig. 2, where the operator uses the teleoperation\ndevice (Spacemouse in this case) to precisely align and place the toy car onto the narrow bridge.\nEach step in dense mode is recorded as (ot, at, dense). Note that regardless of the mode, we\nrecord the full set of observations of which includes all camera views as well as proprioception to\nfacilitate data augmentation and to make datasets compatible with any IL policy."}, {"title": "4.2 THE WAYPOINT POLICY OF SPHINX", "content": "The waypoint policy waypt in SPHINX takes a point cloud ofcd as input and outputs a 7-DoF end-\neffector pose wt for the robot to reach via a controller. We utilize point clouds as input to cast part of\nthe action prediction problem to learning a salient map over the points. This encourages the policy\nto attend to the important spatial features (i.e. the handle on a mug) rather than memorize exact\nlocations (i.e. the x, y, z target grasp location).\nThe detailed design of the waypoint architecture is illustrated in Fig. 3. At a high level, we would\nlike to have per-point predictions, such as the probability of a point to be salient and the translational\noffset between the point and the target location of the end-effector, as well as other predictions whose\ntargets are not expressed relative to the points, such as rotation, gripper state, and mode. We use a\ntransformer to process the points and add additional tokens for point-agnostic predictions. We first\nuse farthest-point-sampling (FPS) (Qi et al., 2017) to downsamples a raw point cloud to D = 1024\npoints, and then convert the points ci \u2208 R6 to tokens ei \u2208 Rd via a shared linear projection layer.\nThen we feed the entire set of tokens into a transformer (Vaswani et al., 2017; Radford et al., 2019)\nto get output embeddings. Since the points in a point cloud are unordered, the transformer has no\npositional embedding and does not use a causal mask. We pass each point embedding through a\nshared linear layer to get two predictions per point: one for the probability of the point being a\nsalient point \u00ee\u00ea\u00ee and the other for the offset $i = (xi, Yi, zi) between the point position and target\nwaypoint position, illustrated by the middle \"Prediction\u201d panel of Fig. 3.\nInstead of using a hard one-hot target for salient point prediction, we construct a soft salient map\nover points where the probability of each point is given by:\nPiX ||Ci - Ck ||2 if ||Ci - Ck ||2 \u2264 r else 0\n(1)"}, {"title": "4.3 THE DENSE POLICY OF SPHINX", "content": "The waypoint policy waypt (wt|ot) guides the robot to reach objects in a desired pose. However,\nlong horizon tasks often contain sub-tasks like insertion or alignment that require finer-grained ac-\ntions. These parts of the task would be easier to accomplish through direct, per-time-step teleop-\neration, rather than using a series of short waypoints. To address this, we train a dense policy,"}, {"title": "5 EXPERIMENTS", "content": "In this section, we evaluate how SPHINX's attention to salient points and hybrid policy architec-\nture impacts its performance and generalization on a suite of four challenging real-world tasks and\ntwo simulated manipulation tasks. In all experiments, we assume access to two external camera\nviewpoints and a wrist-mounted camera on the Franka Panda robot arm. See Appendix B for the\nimplementation details."}, {"title": "5.1 EVALUATION ON PRECISE AND LONG-HORIZON TASKS", "content": "We first evaluate SPHINX's performance on complex, long-horizon tasks that demand precision.\nWe hypothesize that by interleaving waypoint actions predicted from point clouds, and dense ac-\ntions predicted from close-up wrist-camera images once near a salient point, SPHINX will more\neffectively be able to complete this class of tasks compared to baselines which do not exploit salient\npoints or a hybrid mode-switching policy. To assess this, we consider 3 challenging real-world tasks\nwhich we found to be infeasible to teleoperate in waypoint-mode alone due to the required degree\nof precision and reactivity: Cup Stack (30 demonstrations), Train Track (30 demonstrations), and\nCoffee Making (60 demonstrations). See Fig. 5 for visualizations of each task and example rollouts.\nFor each task, we consider a large and diverse space of initial configurations (Fig. 6), varying the\nrelative locations of objects (cups, mugs, coffee pods, machine, train track).\nBaselines: We compare against three baselines, the first being Diffusion Policy (DP) from Chi\net al. (2023) with images from all three cameras as input. We intend for this baseline to demonstrate\nthe benefit of waypoint modes (or lack thereof) for challenging manipulation tasks. The second\nbaseline is HYDRA (Belkhale et al., 2023), a hybrid IL policy which takes images from all three\nviews as input and outputs waypoint and dense actions. Given that it is image-based, HYDRA\nuses a multiheaded policy with a shared image encoder as input to waypoint, dense, and mode\nprediction heads. Its waypoint prediction head outputs a waypoint without any intermediate salient\nrepresentation. We choose this baseline to demonstrate the effects of separate input modalities (point\ncloud vs. wrist image) for waypoint and dense modes, as well as the benefit of using salient points\nto ground waypoint actions. The original HYDRA implementation used a simple MLP for the dense\nhead, but we update it to a diffusion policy for fair comparison. The last baseline is Fine-tuned\nOpenVLA, where we fine-tune the recent OpenVLA model (Kim et al., 2024), pretrained on large\nrobotics datasets, on our single-task datasets. We use it as an independent baseline given the recent\ntrend of leveraging large prior datasets towards generalizable robotic manipulation. Due to its design\nrestrictions, this model can only take a single third-person image as input.\nIn Fig. 4 (left), SPHINX achieves the best performance across all three challenging tasks. Fine-tuned\nOpenVLA struggles to achieve the required precision, lacking mode switches and close-up wrist\nimages. HYDRA shows nonzero performance but suffers from inaccurate waypoint predictions\nwithout salient point attention and point clouds. Diffusion policy performs the best among the\nbaselines but struggles to generalize across initial configurations without the waypoint mode. As\nshown in Fig. 6, SPHINX generalizes better across various object placements, while baselines tend\nto memorize a few arrangements. Overall, these tasks are highly unforgiving of grasping failures\nand imprecision. Baseline methods particularly struggle to make progress in the train and coffee\ntasks where early mistakes (missed grasps or placements with the mug, coffee pod, train) derail an"}, {"title": "5.2 WAYPOINT POLICY ABLATIONS", "content": "Although SPHINX's performance in challenging real-world tasks relies on the dense policy, the\nwaypoint policy is crucial for its strong performance and generalization as it reliably guides the\nrobot to task-relevant locations. In this section, we validate the design choices behind SPHINX'S\nwaypoint policy through ablations and comparisons against state-of-the-art (SoTA) IL policies. To\nisolate the impact of the waypoint policy from that of the dense policy, we conduct these experiments\nexclusively on tasks that can be teleoperated solely in waypoint mode, without the use of dense mode.\nWe posit that SPHINX can achieve a higher task success rate across a wide range of initial spatial\nconfigurations by effectively using waypoints (via salient points and offsets) to reduce the action"}, {"title": "5.3 VISUAL GENERALIZATION", "content": "We next evaluate SPHINX's ability to handle visual rather than only spatial generalization on Drawer\nand Cup Stack. As seen in Table 1, SPHINX demonstrates a promising degree of generalization to\nvisual distractors during execution, and unseen third-person camera viewpoints (Fig. 7), retaining\nhigh performance. This is likely enabled by using viewpoint-agnostic point clouds (assuming cal-\nibration), salient points encouraging the policy to ignore distractors, and wrist-camera images in"}, {"title": "5.4 EXECUTION SPEED", "content": "A key advantage of SPHINX over dense-only methods is its decoupled waypoint and dense policies.\nWhile dense methods are tied to the speed of actions recorded during data collection, SPHINX uses\na waypoint controller that allows flexible execution speeds at test time. We specifically collect all\ndata across all tasks using a controller limited to a maximum delta of 1 cm at 10Hz. After training\nSPHINX and dense-only diffusion policy on Cup Stack, we perform 10 trials of the task, where we\ncompare SPHINX implemented with a 2\u00d7 sped-up controller (2 cm maximum delta at test-time) to\nDP trained on the 1\u00d7 data. SPHINX completes the task in an average of 9.2 seconds, a 1.7\u00d7 speedup\nover diffusion policy (15.6 seconds). While further speed increases led to controller imprecision,\nSPHINX has potential for even faster execution on more capable hardware."}, {"title": "6 LIMITATIONS AND CONCLUSION", "content": "SPHINX demonstrates strong performance and generalization across a range of tasks, but our policy\nis not without failures. The majority of SPHINX's failures stem from the dense policy being slightly\nimprecise for grasping or manipulation. Although we mitigate this by using the dense policy only\nfor short horizons near salient points, performing the \u201clast mile\u201d of precise insertion or alignment re-\nmains challenging for some tasks. Additionally, our data collection interface uses a linear controller\nto reach waypoints. This currently limits SPHINX to fairly quasistatic tasks without fast, dynamic\nmovements. Finally, the performance of our waypoint policy is limited by the quality of the input\npoint cloud. We currently perform a one-time calibration procedure to obtain multi-view extrinsics\nand point clouds, but sensor noise and calibration error is not completely avoidable.\nTo summarize, we present SPHINX, a visuomotor IL policy which learns to perform complex manip-\nulation tasks from a limited amount of demonstrations while generalizing across many axes: novel\nspatial arrangements, visual distractors, novel viewpoints, and even customizable execution speeds\n(Fig. 7). SPHINX achieves this using a hybrid policy architecture that takes point clouds and wrist-\nimages as input, and outputs waypoints and dense actions guided by salient points. SPHINX achieves\nan average success rate of 86.6% across 2 simulated and 4 real-world high-precision, long-horizon\ntasks, including making coffee and assembling a train set with several pieces. Our policy outper-\nforms state-of-the-art IL baselines by 41.1% on average across 440 real world robot trials. Avenues"}, {"title": "A ADDITIONAL LOSSES FOR THE WAYPOINT POLICY IN SPHINX", "content": "In Section 4.2 we define the salient point prediction loss and offset loss for SPHINX. Here we\ncomplete the full loss definitions for the waypoint policy. Recall that the waypoint policy needs\nto predict translation \u00a7, rotation &, binary gripper state \u011d and next mode m. As described in the\nmain paper, the translation prediction is decomposed to first predict salient points (Eq. (2)) and then\npredict offset 6 w.r.t. the salient point (Eq. (3)). We represent rotations in Euler angles and loss is\nmean squared error (MSE) between the prediction & and target a:\nLrot = || - ||2.\n(6)\nAlthough MSE on Euler angle ignores the wrap-around effect, i.e. \u2013\u03c0 and represent the same\nrotation but the loss is not 0, we choose it for its simplicity and find it to work well in practice.\nOther representations for rotation and their corresponding losses, such as quaternion, should work\nsimilarly. The gripper state is binary with 1 for open and 0 for close. Assuming g is the ground-truth\ngripper state and \u011d is the predicted probability of the gripper being open, then the gripper loss is a\nbinary cross entropy loss:\nLgripper = glog\u011d + (1 \u2212 g) log(1 \u2013 \u011d).\n(7)\nThe waypoint policy also need to predict the next mode, which is a three-way classification among\ncandidates {waypt,dense,terminate}. We train it via negative log-likelihood:\nL'mode\nlog m\n(8)\nwhere m is the predicted probability for ground-truth mode m. Finally, the full waypoint loss is the\nsum of all terms, and we find that a simple unweighted sum works well in practice, eliminating the\nneed of additional hyperparameters:\nLwaypoint = Lsalient + Loffset + Lrot + Lgripper + Lmode\n(9)"}, {"title": "B IMPLEMENTATION DETAILS OF SPHINX", "content": "B.1 WAYPOINT POLICY\nThe waypoint policy in SPHINX is using a Transformer to predict salient probability and offset per\npoint, as well as to predict rotation, gripper and mode using additional tokens similar to the [CLS]\ntoken in visual classification. The Transformer has 6 layers and each layer has 512 embedding\ndimensions over 8 attention heads. We remove positional embeddings from Transformer as the point\ncloud input has no ordering. We set dropout to 0.1 for all Transformer blocks to avoid overfitting.\nWe optimize the waypoint policy with Adam (Kingma & Ba, 2015) optimizer with base learning\nrate 1e-4 and cosine learning decay over the entire training process, i.e. decaying to 0 at the end of\ntraining. We clip the gradient with maximum norm 1. We set batch size to 64. We also maintain an\nexponential moving average (EMA) of the policy with the decay rate annealing from 0 to 0.9999.\nWe use the final EMA policy in all evaluations without any further model selection. All waypoint\npolicies are trained for 2000 epochs.\nAs mentioned in the Section 4.2, we use observations from interpolated steps as data augmentation\nfor waypoint training, a technique we refer to as temporal augmentation:\n{(ot, at, waypt, Wt', zt') | t' \u2264 t \u2264t' + akt}\n(10)\nwhere t' is the initial step when the demonstrator specified the current waypoint, kt, is the total num-\nber of steps it takes for the controller C to execute this waypoint. Essentially we train waypt (w|ot)\non the initial ot as well as the intermediate observations Ot'+1:t'+ak\u2081,\u00b7 Here a = 0 means no tem-\nporal augmentation and a = 1 means training on the entire waypoint segment. In practice, we find\nthat setting a = 0.2 strikes a balance between sufficient augmentation while avoiding interpolated\nobservations that occur too late in a waypoint segment, which can cause the policy to confuse the\ncurrent target waypoint with the next one. As a concrete example, the waypoint dataset for the\nRobomimic Square task contains 50 demonstrations, each containing 6 waypoints. The raw dataset\nfor training the waypoint policy contains 300 examples. With the temporal augmentation, the dataset\nnow contains roughly 1800 examples, increasing the amount of data by 6 times. All the waypoint"}, {"title": "B.2 DENSE POLICY", "content": "The dense policy in SPHINX is a diffusion policy. We closely follow the original implementation\nof Chi et al. (2023). Specifically, we use ResNet-18 (He et al., 2016) encoder to process the wrist\nimage and append the proprioceptional to the image embedding before feeding it to a 1-D convo-\nlutional UNet for action denoising. The diffusion policy is trained with DDPM to predict the noise\ngiven the noisy action as input and observation as context. We follow the best practices of training\nit using Adam (Kingma & Ba, 2015) with weight decay, cosine learning rate schedule and take the\nexponential moving average of the policy as final policy for evaluation. Our implementation is able\nto reproduce the results on Robomimic from the original paper."}, {"title": "C EXTENDED RELATED WORK", "content": "Relationship between Salient Point and Affordance. Affordance learning is a common concept\nin robotics manipulation. It can refer to classifying the nature of interaction for certain object points\n(e.g., a tool handle is \"graspable,\" a button is \"pressable\") (Borja-Diaz et al., 2022), or more broad\naction candidates or success possibilities associated with objects (Ahn et al., 2022). Salient points\nin SPHINX can be thought of as a form of per-point affordance, but must be combined with offsets\nto specify how the end-effector should interact with a given point. Critically, we demonstrate that\nthe per-point affordances in SPHINX allow the policy to focus on task-relevant features and avoid\npaying attention to arbitrary objects, leading to robust execution in the presence of visual distractors.\nAction Representation in Robotics. Several works have also considered using classification in-\nstead of regression for action prediction in imitation learning setting. PerAct (Shridhar et al., 2022)\ndivides the entire 3D workspace into voxels and convert the end-effector pose prediction problem\ninto classification over the fixed set of voxels. The precision of prediction depends on the granularity\nof the voxel, and the number of action candidates grows cubically in the number of voxels, making\nit challenging for tasks that requires high-precision. Act3D (Gervet et al., 2023) performs coarse-\nto-fine scoring for \u201cghost points\u201d, which addresses the granularity issue of PerAct, but still lacks\nany kind of explicit intermediate representation such as salient points. In comparison, SPHINX first\npredicts the salient point, a point that physically exists in the input point cloud, through classifica-\ntion and then predicts offset w.r.t. the salient to recover the full action. This allows SPHINX to be\narbitrarily precise without incurring the high cost of having fine-grained voxels.\nSGRv2 (Zhang et al., 2024) is a recent work that uses per-point offset prediction for robotics manip-\nulation. It predicts actions as per-point offset for all points, and uses a weighted average to get the\nfinal action output. In contrast, SPHINX utilizes salient point learned from human labels as the an-\nchor for offset prediction. Due to the existence of salient points, SPHINX applies the offset loss only\nto the proximal points of the \"demonstrator-specified salient point\", i.e. the red points in Fig. 3 and\nEq. (3). This a easier task for the neural network since it does not need to allocate capacity to predict\noffset for points far from the salient points. Additionally, the salient point learning objective is one"}, {"title": "D IMPLEMENTATION AND PERFORMANCE OF BASELINES", "content": "In this section we discuss the steps we have taken to ensure that the performance of our baselines is\nvalid and discuss potential reasons for the low performance for baselines like Hydra and OpenVLA.\nDiffusion policy (DP): Our DP implementation is able to reproduce the reported results from the\noriginal paper and it achieves the reported performance on Robomimic Can and Square with 200\ndemonstrations from the original Robomimic dataset. It uses the same set of cameras (3rd person\nand wrist) as SPHINX. In the SPHINXexperiment we collected 50 demonstrations for the Square\nand we have verified that the DP trained on the Sphinx dataset (44%", "DP3)": "Our implementation closely follows the one from their original code-\nbase. In their original paper", "inputs.\nHydra": "We modernize Hydra by using the diffusion policy as its dense policy while keep the rest of\nthe implementation as close to the original design as possible. The low performance of Hydra seems\nunreasonable at first glance, but it can be explained via a close look at their original results and\nour ablations. Our coffee-making task is similar to the one in Hydra. However, the original Hydra\npaper collected 100 demonstrations with little variation on the location of the coffee machine, and\nput the cup and coffee pod on a shelf to make them easier to pick up. In our case, we randomize\nthe initial location and orientation of the cup, pod and coffee machine, and only use 60 demon-\nstrations. Therefore, it is reasonable to expect a much lower performance for Hydra on this task.\nHydra's waypoint branch is similar to our \u201cvanilla waypoint\u201d, i.e. directly predicting target pose\nvia regression, but uses images instead of point clouds. From the ablation (Fig. 4, right panel), we\nsee that this vanilla waypoint policy is noticeably worse than plain diffusion policy on the hard task\n(Square) that requires precision. Therefore, considering that Hydra's waypoint policy is worse than\ndiffusion policy, it is not surprising to see that the full Hydra policy ("}]}