{"title": "Mitigation of gender bias in automatic facial non-verbal behaviors generation", "authors": ["Alice Delbosc", "Magalie Ochs", "Nicolas Sabouret", "Brian Ravenet", "St\u00e9phane Ayache"], "abstract": "Research on non-verbal behavior generation for social interactive agents focuses mainly on the believability and synchronization of non-verbal cues with speech. However, existing models, predominantly based on deep learning architectures, often perpetuate biases inherent in the training data. This raises ethical concerns, depending on the intended application of these agents. This paper addresses these issues by first examining the influence of gender on facial non-verbal behaviors. We concentrate on gaze, head movements, and facial expressions. We introduce a classifier capable of discerning the gender of a speaker from their non-verbal cues. This classifier achieves high accuracy on both real behavior data, extracted using state-of-the-art tools, and synthetic data, generated from a model developed in previous work. Building upon this work, we present a new model, FairGenderGen, which integrates a gender discriminator and a gradient reversal layer into our previous behavior generation model. This new model generates facial non-verbal behaviors from speech features, mitigating gender sensitivity in the generated behaviors. Our experiments demonstrate that the classifier, developed in the initial phase, is no longer effective in distinguishing the gender of the speaker from the generated non-verbal behaviors.", "sections": [{"title": "1 INTRODUCTION", "content": "Socially Interactive Agents (SIAs) are virtual agents that simulate key properties of face-to-face human conversation, such as verbal and non-verbal behaviors. A number of studies have been carried out to simulate role-playing with SIAs to train one's own skills [4], for example for training doctors to break bad news [31], job interviews [1], negotiation [14], or conflict management [23]. A crucial aspect for the widespread acceptance and use of these applications lies in the believability of the non-verbal behaviors exhibited by the SIAs. The SIAs' non-verbal behavior is particularly important, since several studies underline the positive impact of non-verbal behaviors on knowledge transmission and information retention [7]. In addition, studies indicate that appropriate head movements enhance the overall perception of SIAs, while inappropriate facial expressions can increase their sense of \"uncanniness\" [37]. Early approaches explored for the automatic generation of SIA's behaviors were based on sets of rules [5, 19]. The rules describe the mapping of words or speech to a facial expression or gesture. These approaches present advantages in terms of communication and control but lack naturalness and variability in behavior generation [30]. Nowadays, most of the research works on behavior generation are based on data-driven approaches [16]. These approaches do not depend on experts in animation and linguistics. They learn the relationships between speech and movements, or facial expressions, directly from data. Among data-driven approaches, deep neural networks have demonstrated their superiority in this task. The commonly employed methodological approach is to extract verbal and non-verbal features from recorded real-world human interactions, and to train a generative model using these real-world datasets [8, 16, 17, 20]. Two key aspects are often evaluated to determine the performance of these models: the human-likeness and the appropriateness of the non-verbal behaviors with speech [21]. However, the presence of possible bias in such models is rarely considered a criterion for evaluating the quality of the model.\nIndeed, real-world datasets are often biased [6]. The most frequently identified biases come from key demographic factors like gender. We know, for instance, that men and women differ in their non-verbal behaviors [22]. As generative models learn from our data, most contain biases by simply reproducing those that have been passed to them [11]. This may raise ethical concerns, depending on the intended use of these agents. While it might be wanted to reproduce societal norms and behaviors in SIAs, e.g. for better cultural understanding and acceptability, reproducing gender biases can perpetuate harmful stereotypes and inequalities, contributing to the normalization of discriminatory attitudes and behaviors in society. Indeed, a recent study shows that humans inherit the biases of the artificial intelligence they use [38]. Moreover, biased SIAs may make users who don't conform to traditional gender norms"}, {"title": "2 RELATED WORK", "content": "Research on bias and fairness has a long history in philosophy, psychology, and in recent years in machine learning [28]. While machine learning ethics often focuses on classification problems, such as gender-neutral hiring [36], recent attention has turned towards the ethical implications of generative models [6, 11, 24].\nFor these models to be practically viable, they must meet ethical standards and be free from biases that may perpetuate human prejudices. This work contributes to the ethical development of SIAs by addressing gender bias in this domain. To our knowledge, no other research on automatic non-verbal behavior generation has addressed such an ethical dimension.\nTo effectively rectify these biases and achieve fairness, it is imperative to first establish clear definitions of what constitutes fairness and identify existing discrimination."}, {"title": "2.1 Definition of fairness", "content": "The definition of fairness varies according to the context in which it is applied. Some definitions of fairness focus on equal representations of certain sensible attributes, for example, a generative model that has equal probabilities of producing male or female examples [36].\nIn our study, we focus on the generation of non-verbal behaviors from speech. While biases in non-verbal behaviors can be addressed by balancing sensitive attributes in datasets, the concept of fairness related to equal gender representation is not our focus. We wish to concentrate on the intrinsic differences in non-verbal patterns exhibited by individuals of different gender identities.\nIn the context of generative models, some definitions emphasize performance fairness [26]. These approaches seek consistency in generation quality, whatever the sensitive attribute considered, such as gender. Although this approach may apply to our particular situation, generating behaviors with the same performance for men and women in no way ensures that these behaviors do not depend on the gender of the speaker. It is therefore not suitable for working on the generation of non-stereotyped behaviors.\nOne of the definitions explored in the survey by Mehrabi et al. [28] is: \u201can algorithm is fair as long as any protected attributes are not explicitly used in the decision-making process\u201d. We adapt this definition to the generation of non-verbal behaviors and define fairness as \"the absence of distinction in the generated non-verbal behaviors, whatever the gender of the speaker\". We aim to avoid the perpetuation of gender stereotypes and biases in the non-verbal behavior and to generate non-verbal behaviors that are not differentiated according to gender."}, {"title": "2.2 Approaches to mitigate bias", "content": "Biases in model-generated data come mainly from two sources: the dataset and the models themselves. Dataset bias is the main cause of unfairness in generative models. One solution is to work with unbiased data. Practical limitations such as time, resources, and the complex nature of non-verbal behavior, render this approach difficult. We cannot simply balance datasets on the basis of the distribution of sensitive attributes, since bias comes from the fact that individuals have different non-verbal behaviors depending on their gender identity [22].\nModels can perpetuate and even amplify biases in the data [6]. Generative Adversarial Networks (GANs), for instance, are trained in an unsupervised way to capture the underlying distribution of the dataset, then generate new data from the same distribution [36].\nTo address these issues, researchers have explored various techniques, including pre-processing, in-processing, and post-processing [3]. While pre-processing and post-processing methods directly manipulate data, in-processing approaches modify the model during training. Despite advancements in bias mitigation, there is a paucity of research specifically addressing bias in generated non-verbal behaviors. We investigate this aspect from the perspective of generation models in general.\nPre-processing attempts to transform data to remove distribution bias, and post-processing involves modifying the generated data after the model has been trained. Xu et al. [39] work with adversarial networks, trying to generate new data free of the discriminant attribute. They generate new datasets similar to real data that are debiased and preserve good data utility. We felt that it was more effective to operate at the learning stage, building a model that learns from our \"biased\" data \"non-biased\" non-verbal behaviors whatever the speaker's gender is.\nSeveral methods have been proposed to mitigate biases in generative models with an in-processing approach. In the context of image generation, Choi et al. [6], Teo et al. [36] use a complementary unbiased dataset as a supervisory signal to detect bias in the baseline data and bring the distribution of the baseline data closer to the reference data. In these works, it is assumed that an unbiased dataset can be accessed or constructed. Zhang et al. [40] employ adversarial learning by presenting a model in which they try to maximize the accuracy of a predictor and at the same time minimize the ability of an adversary to predict the sensitive variable. They use adversarial learning to mitigate sensitive attributes, a method noted by Frankel and Vendrow [11] as costly to train. Frankel and Vendrow [11] develop a method that uses a small neural network ahead of the existing generator to perturb the latent variables. While this approach effectively addresses fairness, it increases both training and inference times due to the additional network layer.\nSimilarly, some studies seek to learn latent representations that remain invariant with respect to a given variable. One example is the Variational Fair Autoencoder [25], which extends the semi-supervised variational autoencoder to acquire representations explicitly invariant to known dataset attributes. By employing a Maximum Mean Discrepancy regularizer, they promote invariant latent variable distributions. This approach necessitates the use of a specialized variational encoder architecture.\nThe field of domain adaptation, for example the work of Ganin and Lempitsky [12], closely relates to this approach by seeking to minimize the discrepancy between feature distributions of two domains. Their findings demonstrate that adaptation can be integrated into nearly any feed-forward model by adding a small set of standard layers along with a novel gradient reversal layer. Unlike previous methods, this technique enables iterative training, reducing computational costs, and the gradient reversal layer is only active during training, not affecting inference. Furthermore, this approach permanently modifies the latent representation, eliminating the need for an additional neural network before the generator. We propose adapting the approach of Ganin and Lempitsky [12] to mitigate gender bias in non-verbal behavior generation."}, {"title": "3 FACIAL BEHAVIORS CORPUS", "content": "Focusing on the automatic generation of facial expressions, head movements and gaze, a corpus that emphasizes facial recordings with a balanced representation of male and female speakers is required. For this purpose, we use the Trueness corpus [32]."}, {"title": "3.1 Presentation and splitting", "content": "Trueness is a corpus of scenes of ordinary discrimination, of sexism and of racism [32]. It also includes interactions between authors of discriminatory behavior and witnesses, attempting to sensitize them by acting out various socio-affective behaviors such as aggression, conciliation or denial. These scenes originate from a French forum theater focused on discrimination, with professional actors trained in this domain. Each scene is divided into two videos, representing the perspectives of the first and second persons in the interaction. An essential quality aspect of the facial non-verbal behaviors is the camera's field of view, carefully maintained to capture only the face and torso.\nThe dataset is divided into two parts, each recorded separately with different actors. The first part comprises a training set, SetGen, and a test set, TestSet, used for training and evaluating generative models. The second part, SetClassif, is dedicated to train the gender classifier. To ensure dataset diversity and prevent data overlap,"}, {"title": "3.2 Extraction and processing", "content": "We automatically extract behavioral features from the existing videos using Openface [2] and speech features using the self-supervised speech model Hubert [18].\nBehavioral features. Openface extracts, among others, 28 features characterizing the head, gaze, and facial behaviors of a person on a video at a frequency of 25 fps (frames per second). The eye gaze position is represented in world coordinates, the eye gaze direction in radians, the head rotation in radians, and 17 facial action units in intensity from 1 to 5 (AU01-02, AU04-07, AU09-10, AU12, AU14-15, AU17, AU20, AU23, AU25-26, AU45) based on the Facial Action Coding System [10]. We point out that these features are designed to capture non-verbal facial behaviors, but do not offer precise lip-synchronization.\nTo ensure that our model learns from clean, plausible data, we filter out images that have been incorrectly processed by OpenFace. These include images in which faces are obscured by hands or hair. We then interpolate the transitions between the remaining images. In addition, two further processing steps are applied to head and gaze features. Firstly, the features are smoothed using a median filter with a window size of 7. Secondly, the head and gaze coordinates are centered to ensure that the SIA is facing the user. Finally, as our focus in this project is solely on generating speaking behaviors (and not listening behaviors), we set the behavioral features to zero when the protagonist is not speaking.\nThese features, noted $F_b \\in \\mathbb{R}^{28}$, are used for the training. $F_b$ consists of $F_{head}, F_{gaze}$, and $F_{AU}$, representing respectively head movements, gaze orientation, and facial expressions.\nSpeech features. Drawing on Haque and Yumak [17] work on non-verbal facial behavior generation, we use Hubert to extract the speech features. In response to various analyses of different layers of self-supervised speech models [33, 34], we compare the model's objective performances using different layers of Hubert (Section 7 for more details on the computation of objective performances), and choose to use the twelfth layer to extract the speech features. In Hubert, speech features are extracted at a frequency of 50 fps. The speech features extracted from human speech are noted $F_s \\in \\mathbb{R}^{1024}$.\nSliding window. Human behaviors are primarily generated by analyzing short segments with a sliding window approach, spanning from seconds to minutes, based on the socio-emotional phenomena studied [29]. We segment the videos into 4-second segments with a 0.4-second overlap. Since the speech data has a frame rate of 50 fps, and the behavior data has a frame rate of 25 fps, we use a speech segment length of 200 frames and a behavior segment length of 100 frames, they are aligned during training.\nThis segmentation process yields 3590 segments for SetClassif, comprising 1429 female, 1459 male, and 702 silent segments (where no speech occurs within the 4-second window). SetGen consists of 2940 segments, including 1352 female, 1002 male, and 586 silent"}, {"title": "4 THE FACE-GEN MODEL", "content": "We aim to generate non-verbal facial behaviors for SIAs while they speak. We can formulate the goal as follows: given a set of speech features $F_s [0: T]$, taken from a particular speech segment at constant frame intervals of length $T = 200$, the goal is to generate the sequence of behaviors $Y_b[0: T]$ that a SIA is expected to perform during its speech. The distribution of $Y_b$ must be as close as possible to the one of $F_b$.\nOur model is build upon the work of Delbosc et al. [8], who introduced an open-source framework for the automatic generation of non-verbal facial behaviors using action units. We implemented a number of adjustments, including: the audio features extracted with Hubert, the reduction of discriminator capacity, the noise formation, and the model hyperparameters. In this section, we present this transformed architecture\u00b9. This model will serve as the reference \"biased\" generative model, which we call FaceGen."}, {"title": "4.1 Architecture", "content": "Like the original model presented in [8], FaceGen adopts the structure of an adversarial encoder-decoder. It is termed \"adversarial\" because it comprises two modules, a generator and a discriminator, mirroring the architecture of a GAN [13]. The term \"encoder-decoder\" is employed because the generator operates on the principles of a 1D encoder-decoder. Again, as in the original model, both modules receive speech features $F_s [0: T]$, allowing the discriminator to evaluate the believability of the temporal alignment between behavioral and speech features. Preserving this property of the basic model, the discriminator receives (in addition to ground truth and generated examples) examples that help it discriminate between speaking and listening phases. These examples associate features of listening behavior with features of speaking, and vice versa. A simplified architecture is shown in the green frame of Figure 2. To describe each module, we use the following notations: Conv and DoubleConv. A Conv block is composed of a convolution 1D, dropout, batch normalization 1D, and Relu. A DoubleConv block is the concatenation of two Conv blocks.\nThe generator. The generator generates data by sampling from a noise z and speech features $F_s$. The features received by the encoder are not the same as in the original model, so we adapted the architecture, maintaining the main modules. The encoder initially"}, {"title": "4.2 Training", "content": "FaceGen is optimized with a Wasserstein loss with gradient penalty [15]. The generator G, with the parameters of the encoder $\\theta_e$, and the parameters of the decoders $\\theta_d$, is supervised with the following loss function:\n$L_G(\\theta_e, \\theta_d) = L_{gaze}(\\theta_e, \\theta_d) + L_{head}(\\theta_e, \\theta_d) + L_{AU} (\\theta_e, \\theta_d)$\nwhere $L_{gaze}$, $L_{head}$ and $L_{AU}$ are the root mean square errors (RMSEs) of the gaze orientation, head movement, and AUs features.\n$L_{mod}(\\theta_e, \\theta_d) = \\sum_{t=0}^{T-1} (F_{mod}[t] - Y_{mod}[t])^2$\nwith $mod \\in \\{gaze, head, AU\\}$. The discriminator D, with the parameters $\\theta_a$, is optimized through the adversarial loss function:\n$L_{adv}(\\theta_e, \\theta_d, \\theta_a) = \\mathbb{E}_{x \\sim P_r}[D(F_s, x)] - \\mathbb{E}_{x \\sim P_g}[D(F_s, x)] + \\mathbb{E}_{x \\sim P_x} [(||\\nabla D(F_s, x)||_2 - 1)^2]$\nwith $P_r$ the ground truth distribution and $P_g$ the generated distribution defined by $x = G(z, F_s), z \\sim p(z)$. $P_x$, used to calculate the gradient norm, samples uniformly between pairs of points sampled from the data distribution $P_r$ and the generator distribution $P_g$, $x = (1 - l)F_b + (1 - l)Y_b$ with $0 \\leq l \\leq 1$. We use $\\lambda = 10$. By integrating adversarial loss with direct supervisory loss, our objective is the following:\n$L_Y (\\theta_e, \\theta_a, \\theta_a) = L_G(\\theta_e, \\theta_d) + \\beta .L_{adv} (\\theta_e, \\theta_a, \\theta_a)$\nwe set $\\beta = 1$. We use Adam optimizer for training, with a learning rate of $10^{-4}$ for the generator and the discriminator. Our batch is size 32. This model was trained for 1200 epochs on a v100 Nvidia GPU, for approximately 14 hours."}, {"title": "5 INVESTIGATING GENDER BIAS", "content": "We assess the presence of gender bias in both ground truth and FaceGen-generated non-verbal facial behaviors. Following our fairness definition (Section 2.1), a bias is present in non-verbal features if we can identify them as coming from a female or male speaker. For this purpose, we build a gender classifier, trained on SetClassif (detailed in Section 3). This classifier predicts the speaker's gender based on input non-verbal behavior features, excluding segments of complete silence.\nArchitecture and training. The gender classifier is a compact neural network composed of two Conv blocks (see Section 4), each followed by a maxPool operation. Subsequently, there is a linear layer, a ReLU activation function, another linear layer, and finally a log softmax activation layer. The model is trained using cross-entropy loss and the Adam optimizer with a learning rate of $10^{-3}$ for 10 epochs.\nEvaluation and interpretation. We train the classifier 10 times to capture variability in the training process, such as random weight initialization and optimization algorithm stochasticity. We train it on a large subset of SetClassif (1394 female segments and 1423 male segments) and validate its performances on a smaller subset (35 female segments and 36 male segments). The classifier achieved a mean accuracy of 85.92% with a standard deviation of 4.20%.\nRandomly selecting one of the trained classifiers, we classified ground truth data from TestSet. The resulting accuracy is 90.18% with 4 misclassifications out of 267 for female speakers and 56 misclassifications out of 344 for male speakers. These results indicate that non-verbal behaviors extracted from the dataset exhibit discernible gender patterns, suggesting that gender influences the ground truth non-verbal behavior. With this established, we can now explore our initial research question: 'Do generative models reproduce potential differences in non-verbal behavior between the genders?'.\nWe classified data generated by FaceGen. The resulting accuracy is 80.69% with 44 misclassifications out of 267 for female speakers and 74 misclassifications out of 344 for male speakers.The influence of the speaker's gender is evident in both the ground truth data and those generated by FaceGen. This finding answers our first research question, confirming that gender influence persists in automatically generated behaviors, despite being less pronounced than in ground truth data. Therefore, we aim to explore our second research question: \"Can we modify the [FaceGen] model to mitigate the gender differences in non-verbal behavior generation without compromising the perceived naturalness and appropriateness of these behaviors?\"."}, {"title": "6 THE FAIR-GENDER-GEN MODEL", "content": "We introduce a new model called FairGenderGen, designed to generate facial non-verbal behaviors from speech, while also aiming to mitigate gender bias by producing behaviors that are independent of the speaker's gender."}, {"title": "6.1 Architecture", "content": "The model work with speech features $F_s [0: T]$ as inputs, and label from the label space {female, male, silence). The approach will nevertheless be generic and can handle any labels. We assume the existence of three distributions: $P_f, P_m$ and $P_s$, which will be referred as the Female, the Male and the Silence distributions. All distribution are unknown. We don't deal with the silence labels as we aim to maintain the Silence distribution unchanged.\nOur goal is to achieve a latent representation of our data that is invariant with respect to gender, meaning we aim to make the distributions $P_f$ and $P_m$ as similar as possible. At training time, we have access to labeled examples from both distributions. Measuring the dissimilarity of the distributions is however non trivial as they are consistently changing during the training process.\nBuilding on prior research presented in Section 2, we propose to adapt the approach of Ganin and Lempitsky [12] to mitigate gender bias through domain adaptation with backpropagation. The proposed architecture includes all the modules of the FaceGen model (green in Figure 2); the generator with encoding and decoding parts, and the discriminator, which together form a standard feed-forward architecture. Unsupervised domain adaptation is achieved by incorporating a gender classifier (orange in Figure 2).\nThis gender classifier takes as input the latent representation of FaceGen data, and classifies them according to the speaker's gender, male or female. It does not receive the silent sequence representations. It is connected to the encoder via a gradient reversal layer. This layer multiplies the gradient by a negative constant during the backpropagation-based training, known as the adaptation factor \u03bb. Similar to the original paper [12], we gradually change the adaptation factor from 0 to 1 during the training process.\nThe gender classifier is a small neural network, consisting of two Conv blocks (see Section 4), with maxPool after the first block, followed by a linear layer, a ReLU activation function, another linear layer and a log softmax activation layer. Figure 2 illustrates the integration of this classifier with the FaceGen model to form the FairGenderGen model.\nGradient reversal ensures that the distributions over the two genders are made as indistinguishable as possible for the gender"}, {"title": "6.2 Training", "content": "To avoid starting from scratch and leverage the learning achieved with the FaceGen training, we initialize our discriminator and generator with the FaceGen weights.\nDuring the learning stage, we optimize the parameters of the encoder $\\theta_e$ that maximize the loss of the gender classifier, while simultaneously optimizing the parameters $\\theta_e$ that minimize the loss of the gender classifier. The gender classifier uses binary cross-entropy as loss function. Otherwise, the training proceeds in a standard manner, minimizing the overall objective $L_Y$ with the parameters of the decoders $\\theta_d$ and the parameters of the discriminator $\\theta_a$. By integrating the loss of the gender classifier $L_{gender}$ and its parameters $\\theta_c$, our objective becomes:\n$L_{fair} (\\theta_e, \\theta_a, \\theta_a, \\theta_c) = L_Y(\\theta_e, \\theta_a, \\theta_a) + \\alpha .L_{gender} (\\theta_e, \\theta_c)$\nwith $\\alpha$ set to 0.1. We utilize the Adam optimizer for training, with a learning rate of $10^{-4}$ for the generator and the discriminator. Our batch is size 32. This model was trained for 500 epochs on a v100 Nvidia GPU, requiring approximately 6 hours.\nFigure 3 displays the generator's outputs on TestSet in three dimensions using UMAP visualization [27], with the Male and Female distributions undeniably closer together for the FairGenderGen model. To confirm the visualization results showing that the two distributions are closer together, we conduct an objective and subjective evaluation to assess not only the mitigation of bias, but also the consistency of performances (Section 7)."}, {"title": "7 EVALUATION", "content": "It is equally important to verify that our non-verbal male and female behaviors are now closer, as it is to ensure that the mitigation of bias has not reduced the quality of the generated behaviors.\nTo address the first point, we use the gender classifier pretrained on the SetClassif data (Section 5). This allows us to assess whether the gender differences in non-verbal behaviors have been minimized in the generated data.\nFor the second point, we objectively and subjectively evaluate the model's performances. We compare these metrics with those of the FaceGen model to ensure that the quality of the generated non-verbal behaviors has been maintained (maintained, improved or slightly degraded)."}, {"title": "7.1 Gender bias", "content": "While the gender classifier (Section 5) was able to discriminate between the non-verbal male and female behaviors generated by the FaceGen model with an accuracy of 80.69%, its performance significantly dropped to 48.61% when applied to behaviors produced by FairGenderGen. A closer examination reveals 90 misclassifications out of 267 for female speakers and 224 misclassifications out of 344 for male speakers.\nTo eliminate gender-based distinctions in generated non-verbal behaviors, the distributions of male and female behaviors were"}, {"title": "7.2 Performance evaluations", "content": "To evaluate the FairGenderGen model, we generate videos for the two individuals, male and female, who compose the TestSet. This involves generating all the segments and averaging overlapping image frames. Our evaluation is based on eight full videos for the objective evaluation (Section 7.3) and four 30-second portions for the subjective evaluation (Section 7.4)."}, {"title": "7.3 Objective evaluation", "content": "Objective measurements, relying on algorithmic methodologies, provide numerical performance indicators. We use mainly Dynamic Time Warping DTW, an algorithm for measuring similarity between two temporal sequences, which may vary in speed.\nDistance between males and females. First, DTW is employed to assess the similarity between the distributions of male and female non-verbal features across ground truth data, FaceGen-generated data, and FairGenderGen-generated data. For each feature, the DTW is computed between the corresponding male and female distributions. An overall gender bias measure is obtained by averaging these DTW distances.\nTable 2 confirms that the gender bias, i.e. the distance between the two distributions, is increased using the FaceGen model compared to the ground truth. Generative models are capable of amplifying biases existing in the data they are trained on. However, we manage to reduce the distance between these two distributions using the FairGenderGen model."}, {"title": "7.4 Subjective evaluation", "content": "To conduct subjective studies, we selected four approximately 30-second speech sequences, two featuring a female and two featuring a male speaker. These sequences were chosen semi-randomly, ensuring coherence in speech over the 30-second duration. Utilizing the Greta platform [35], we played these sequences on SIAs, employing a male agent for non-verbal behaviors accompanying a male speech and a female agent for those accompanying a female speech. For the study setup, we employ the interface of Delbosc et al. [8], inspired by other interfaces widely used in the field of behavior generation. Participants were tasked with evaluating two criteria across the four sequences: believability and temporal coordination with speech of the SIAs' behaviors. Thirty French participants, recruited on social media (15 males, 15 females, mean age 42.7, std 13.4), evaluated the two criteria through direct questions:\no believability: how human-like do the behaviors appear?\no temporal coordination: how well does the agent's behavior match the speech? (In terms of rhythm and intonation)\nParticipants rated each video on a scale from 0 (worst) to 100 (best) for both criteria. The believability criterion was evaluated without sound, while the temporal coordination criterion was evaluated with sound. The results are presented in Table 4. The spectrum of responses reflects variances not just between conditions, but also includes external factors like variations in individual preferences.\nStatistical analysis is carried out to examine significant differences between the FaceGen and FairGenderGen models, but also between male and female behaviors in these models. Initially, the normality of the data is evaluated using the Shapiro-Wilk test, confirming that the data originate from a normally distributed population. Consequently, a repeated ANOVA is used.\nPerceived believability. There is no evidence of a decline in perceived believability of non-verbal behaviors generated by FairGenderGen (Table 4). Statistical analysis indicates no significant difference in perceived believability with FaceGen (p > 0.1).\nHowever, FairGenderGen's male non-verbal behaviors are significantly rated higher than FaceGen's male non-verbal behaviors (p = 0.008). FairGenderGen improves the perceived believability of male behavior. Without being significant, FairGenderGen's female non-verbal behaviors tends to be rated lower than FaceGen's female non-verbal behaviors (p > 0.1).\nIn addition, by looking at the contrast in perceived believability between males and females, there is no significant difference in the perceived believability of male and female non-verbal behaviors for the ground truth and FaceGen. But there is significant differences for FairGenderGen's: where male non-verbal behaviors are rated significantly higher than their female counterparts (p = 0.029).\nPerceived coordination. FairGenderGen is significantly better than FaceGen (p < 0.001). We also note that, ground truth female's behaviors are perceived more coordinated than male's (p = 0.011), a difference that disappeared in the generated behaviors for both FaceGen and FairGenderGen (Table 4). This result shows that there is no decline in performance in terms of coordination of the non-verbal behaviors generated with FairGenderGen."}, {"title": "8 DISCUSSION AND FUTURE WORK", "content": "Our study highlights a new issue in the field of automatic generation of facial non-verbal behaviors: gender bias. While previous work focused mainly on the believability and coordination of these behaviors with speech, our research highlights the importance of considering the differences in non-verbal behaviors between males and females, differences already observed in real life.\nWe confirmed, through our analysis with a real-world dataset and the training of a state-of-the-art model in the domain, that gender biases are present in ground truth behaviors, as well as in generated behaviors. In this paper, we have proposed a new model, FairGenderGen, aiming to mitigate these biases and create non-verbal behaviors independent of the speaker's gender.\nOur results show that FairGenderGen effectively reduces the gender bias present in the data, even fooling a gender classifier that now recognizes much non-verbal behaviors as female's ones. The subjective evaluation shows that there is no performance loss for this model in terms of perceived coordination. However, our study also reveals a major challenge: the perception of the believability of the generated non-verbal behaviors.\nSociety has higher expectations of women when it comes to non-verbal behavior. For example, Deutsch et al. [9] revealed that the absence of a smile can be detrimental to a woman's image compared with that of a man, while there is no significant difference in image perception between smiling men and smiling women. Society's expectations of non-verbal behaviors negatively influence the perception of women who don't adopt them.\nOur efforts to mitigate gender bias in generated non-verbal behaviors resulted in a notable disparity in perceived believability performance between males and females. Female non-verbal behaviors, generally considered more believable than male ones, became significantly less believable compared to their male counterparts. We believe that these results are due to the higher stereotypical expectations placed on female non-verbal behaviors.\nThe disparity in perception between males and females raises essential questions for the future of research in this field. Should we direct our efforts towards maintaining stereotypes of non-verbal behaviors to preserve equivalent perceived believability and coordination between men and women, thus reflecting reality? Or should we prioritize an approach aimed at reducing these biases, even at the risk of diminishing the perception of believability?\nThese reflections are not limited solely to gender biases but could be extended to other sensitive variables such as cultural or racial differences. Exploring these questions more deeply could one day enable us to find answers and develop more equitable and inclusive solutions in the field of automatic generation of non-verbal behaviors."}]}