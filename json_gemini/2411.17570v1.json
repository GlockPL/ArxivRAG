{"title": "Learning Explainable Treatment Policies with Clinician-Informed Representations: A Practical Approach", "authors": ["Johannes O. Ferstad", "Emily B. Fox", "David Scheinker", "Ramesh Johari"], "abstract": "Digital health interventions (DHIs) and remote patient monitoring (RPM) have shown great potential in improving chronic disease management through personalized care. However, barriers like limited efficacy and workload concerns hinder adoption of existing DHIs, and limited sample sizes and lack of interpretability limit the effectiveness and adoption of purely black-box algorithmic DHIs. In this paper, we address these challenges by developing a pipeline for learning explainable treatment policies for RPM-enabled DHIs.\nWe apply our approach in the real-world setting of RPM using a DHI to improve glycemic control of youth with type 1 diabetes. Our main contribution is to reveal the importance of clinical domain knowledge in developing state and action representations for effective, efficient, and interpretable targeting policies. We observe that policies learned from clinician-informed representations are significantly more efficacious and efficient than policies learned from black-box representations.\nThis work emphasizes the importance of collaboration between ML researchers and clinicians for developing effective DHIs in the real world.", "sections": [{"title": "1. Introduction", "content": "Digital health interventions (DHIs) and remote patient monitoring (RPM) have the potential to revolutionize patient care with treatment strategies dynamically personalized to each patient's characteristics and context. DHIs and RPM have been associated with improved management of many of chronic conditions including heart disease, diabetes, and mental health. Relative to standards of care that rely on fixed-cadence clinic visits, RPM-enabled DHIs promise more timely, personalized, and frequent patient support. These technologies could help move population-level outcomes towards those typically seen in environments with more healthcare resources, particularly for underserved communities.\nWe consider RPM-enabled DHIs that involve the following typical workflow. On a regular cadence (e.g., weekly), the RPM platform takes as input rich, high-dimensional patient data, including granular data from sensors such as continuous glucose monitors (CGMs) and activity trackers. These data are used to form a representation of patient states, based on which patients are prioritized to receive a DHI (or action). Actions may include messaging the patient, recommending activity or treatment adjustments (e.g., a dose change). A targeting policy determines how to rank patients for interventions based on patient state. The results of this ranking inform a whole-population care model in which clinicians determine what actions to take (e.g., which patients to message and what to say in the message).\nWhile the majority of clinicians plan to use RPM-enabled DHIs, their adoption has been limited by significant, well-documented challenges. First is uncertain efficacy, i.e., the difficulty of learning effective policies. A major hurdle here is the difficulty of developing representations of patient states from high-dimensional patient-level data from relatively few patients (e.g., at most a few hundred). Second is workload concerns: because clinical teams have limited capacity, the targeting policy must respect resource constraints by directing attention to patients who will benefit most from intervention. Third is a difficulty understanding or interpreting the technology. This lack of interpretability leads to a reluctance of care teams to adopt solutions that rely on black-box models.\nIn this paper we develop a pipeline to support the optimization of a care model that addresses the preceding concerns. Our work is carried out in the real-world context of an RPM-enabled DHI for individuals with type 1 diabetes (T1D). In the setting we consider, as in Prahalad et al. (2024), patients wear a continuous glucose monitor (CGM) that measures glucose every 5 minutes, generating a high-dimensional time series. The real-world deployment in Prahalad et al. (2024) generates patient states using clinically-informed single-dimensional summaries of this CGM data, such as the percentage of time glucose levels are in a desired target range (70-180 mg/dL). Clinicians provide guidance on how to improve management through telehealth interactions using natural language messages sent to the patient (e.g., \"Increase your pre-dinner insulin dose\") based on the dashboard presentation of recent patient CGM data. The objective of technology-based RPM for T1D is to improve patients' glucose management on an ongoing basis, through personalized, timely interventions.\nOur main contribution is to reveal the importance of clinical domain knowledge in developing state and action representations for effective, efficient, and interpretable targeting policies. Because real-world settings have limited sample sizes, the inductive bias from clinical domain knowledge provides significant benefits to real-world policy performance. In particular, in the preceding T1D RPM context, we evaluate several approaches to low-dimensional state and action representations: from black-box machine learning methods, to clinician-informed learned representations. We observe that policies derived from clinician-informed representations significantly outperform policies learned from black-box-learned representations in terms of efficacy and efficiency. In fact, our evaluation reveals that learned policies outperform random targeting only when the state and action representations are clinically informed - amplifying the importance of clinical inductive bias in practice. Further, the use of clinical domain knowledge also yields policies that are more interpretable than black-box policies with state and action representations that maintain clinically relevant features and interventions.\nTo carry out our evaluation, we develop an end-to-end pipeline for learning targeting policies: we (1) learn low-dimensional state and action representations; (2) construct targeting policies by ranking patients based on estimated conditional average treatment effects (CATEs); and (3) evaluate the policies in the presence of capacity constraints. While each component of this pipeline has been studied in practice, our paper presents a coherent implementation of these steps together to carry out the evaluation described above. This pipeline may be of independent interest to digital health researchers carrying out similar optimization and evaluation of targeting policies in other real-world settings.\nOur approach is broadly applicable to the evaluation and optimization of digital health interventions. Notably, our work suggests that interaction between machine learning researchers and healthcare domain experts is essential for developing practical, effective, and interpretable data-driven treatment policies that can be adopted in clinical practice."}, {"title": "2. Related work", "content": "Many studies have focused on the offline evaluation of conditional average treatment effect (CATE) estimators and of learned treatment policies, which are foundational to our approach. Our work combines methods from these works into a novel pipeline. Like us, they seek to facilitate the development of better treatment policies. But unlike their work, we do not treat our state and action representations as fixed. Instead, we learn and evaluate targeting policies across many different representations, including interpretable representations defined by clinicians.\nOur focus on explainable and interpretable causal inference and machine learning methods in healthcare aligns with the growing recognition of the importance of interpretability in this domain."}, {"title": "3. Data and context: States, actions, rewards", "content": "We use data from three IRB-approved clinical trials of remote monitoring of N = 281 patients with type 1 diabetes (T1D). Participants in the trials wear CGMs that regularly transmit glucose measurements to TIDE, a remote patient monitoring platform. At regular intervals, e.g., weekly, clinicians use TIDE to review patient CGM data and decide whether to send treatment recommendations through asynchronous secure messaging. Due to constraints on provider time, at each review interval TIDE presents data for a subset of patients prioritized based on simple metrics from the consensus guidelines established by the American Diabetes Association, e.g., patients with a relatively high percentage of very low CGM readings.\nOur data consist of variable numbers of days of data for each patient, depending on when the patient started the study; we let \\(T_i\\) denote the number of days of data for patient i. We let \\(X_{it}^a\\) denote patient demographics for patient i on day t; \\(X_i^d\\) is a combination of time-invariant patient demographics like sex and race/ethnicity, and time-variant demographics like age and insulin pump use. In addition, clinicians and the TIDE dashboard consider the previous two weeks of CGM readings--taken every 5 minutes--in determining patient prioritization for intervention; we let \\(X_{it}^c \\in \\mathbb{R}^{4032}\\) to be the (high-dimensional) vector of CGM recordings for patient i over the two previous weeks prior to day t (which may include missing values due to, e.g., the patient not wearing their CGM). As such, the individual \\(X_{it}^c\\) are defined using a day-by-day rolling window on the raw CGM trace for patient i. Taken together, we call \\(X_{it} = (X_{it}^c, X_{it}^a)\\) the high-dimensional patient state for patient i at time t; we let \\(\\mathcal{X}\\) denote the state space.\nGiven our clinical context, we focus on messages as the action taken by clinicians. In particular, we let \\(M_{it}\\) denote the raw text of treatment messages sent to patient i on day t. If no message is sent, then \\(M_{it} = 0\\). We refer to \\(M_{it}\\) as the high-dimensional action taken on patient i at time t; we let \\(\\mathcal{M}\\) denote the action space. Note that in our data, patients are rarely messaged more than once per week.\nAt a high level, the goal of any targeting policy is to direct clinicians' limited resources to take actions (i.e., send appropriate messages) to those patients who are most in need of intervention (given their patient state). Such a policy is considered effective if it leads to improvements in patients' glucose management. In particular, we define the reward \\(r_{it}\\) to be the improvement in the time-in-range (TIR) of patient i at day t in the subsequent week relative to the prior week. TIR is the fraction of a patient's glucose readings between 70-180 mg/dL, one of the most commonly used outcome metrics in T1D care."}, {"title": "4. Methods: A pipeline for policy learning and evaluation", "content": "In this section, we outline the three step approach to learning targeting policies for remote patient monitoring of T1D: (1) learning low-dimensional state and action representations; (2) constructing targeting policies by ranking patients based on estimated conditional average treatment effects (CATEs); and (3) evaluating the policies in the presence of capacity constraints. Our approach is applicable to other domains with similar characteristics; where possible we describe each step using general notation and specialize as appropriate to our specific clinical context.\n4.1. State and action representations\nIn our setting both \\(X_{it}\\) and \\(M_{it}\\) are high-dimensional relative to the number of patients. For this reason, we require dimension-reduced representations of both states and actions. We let \\(\\mathcal{S}\\) (resp., \\(\\mathcal{A}\\)) denote the space of low-dimensional states (resp., actions). Formally, we let \\(\\gamma: \\mathcal{X} \\rightarrow \\mathcal{S}\\) be a function that maps each patient's high-dimensional state to a low-dimensional state representation \\(s_{it} \\in \\mathcal{S}\\), i.e., \\(s_{it} = \\gamma(X_{it})\\). Similarly, we let \\(\\phi: \\mathcal{M} \\rightarrow \\mathcal{A}\\) be a function that maps the high-dimensional action \\(M_{it}\\) to an action representation \\(a_{it} \\in \\mathcal{A}\\), i.e., \\(a_{it} = \\phi(M_{it})\\). We assume that \\(0 \\in \\mathcal{A}\\), and that \\(\\phi(0) = 0\\) uniquely (i.e., the control action maps to itself, and is the only action to do so). For simplicity, we also assume the set \\(\\mathcal{A}\\) is finite in our development.\nAssumption 1 Conditional consistency of action representation. An action representation \\(\\phi\\) is conditionally consistent if for all \\(s \\in \\mathcal{S}\\) and \\(m, m' \\in \\mathcal{M}\\) such that \\(\\phi(m) = \\phi(m')\\), there holds \\(\\mathbb{E}[R(x, m) | \\gamma(x) = s] = \\mathbb{E}[R(x, m') | \\gamma(x) = s]\\).\nIf \\(\\phi\\) satisfies Assumption 1, then for any action \\(a \\in \\mathcal{A}\\), the quantity \\(\\mathbb{E}[R(x, m) | \\gamma(x) = s]\\) is the same for any m such that \\(\\phi(m) = a\\); thus we can define the reward in terms of the dimension-reduced representations as \\(\\rho(s, a) = \\mathbb{E}[R(x, m) | \\gamma(x) = s]\\) where m satisfies \\(\\phi(m) = a\\).\nWe use two approaches to constructing representations: (1) algorithmic black-box approaches, and (2) clinician-informed representations.\nBlack-box baselines: Low-dimensional representations directly from raw data. For action representation, we get features from the raw message text by generating 728-dimensional embeddings using PaLM (Pathways Language Model) (Anil et al., 2023), a large-scale autoregressive language model. Then we cluster the embeddings into discrete message types using K-means to define discrete actions. For state representations, we consider two methods for learning low-dimensional state representations directly from the raw CGM traces: TS2Vec (Yue et al., 2021), a universal representation learning framework for time series that applies hierarchical contrastive learning; and UMAP (Uniform Manifold Approximation and Projection), a non-linear dimensionality reduction technique that preserves the local and global structure of the data.\nClinician-informed representations. In most clinical contexts with high-dimensional states or actions (e.g., sensor time series, imaging, text, etc.), clinicians already have a lower-dimensional set of features they extract for clinical decision-making. Rather than starting from the raw data representation, our approach learns a low-dimensional representation starting from this \"medium\"-dimensional feature representation. The inductive bias provided by such domain knowledge will prove crucial to learning performant, interpretable policies that are clinically grounded.\nFor action representations, we extract interpretable clinical features using few-shot labeling with Gemini Pro (Gemini, 2024) to generate labels from each message (e.g., was a dose change recommended, did the dose change focus on high or low glucose). We use the features most predictive of rewards to anchor a set of discrete actions; we then group each of the remaining features with its closest anchor action, or to an \"other message type\" category, based on the similarity of their clinical meanings. See Appendix C for details.\nFor state representations, we start with a \"medium\"-dimensional set of pre-defined CGM clinical features commonly used in practice: time-in-range (TIR; 70-180 mg/dl), mean glucose, time below 70 mg/dl, time below 55 mg/dl, etc. We also include demographics (e.g., age, language preference, insurance type, insulin pump use, etc.). See Appendix D for a full list of included clinician-defined state features. We evaluate representations that are different subsets of these features: the full set, a learned subset predictive of rewards, and a subset defined most relevant by clinicians.\n4.2. Targeting policies: Ranking via estimated CATEs\nA targeting policy chooses which patients to prioritize for treatment, and what actions to choose for them, given a capacity constraint. We consider targeting policies that rank patients according to CATEs estimated given the dimension-reduced state and action representations.\nA CATE function estimates the effect of an action conditional on the patient's state. The true CATE function, denoted \\(\\tau: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}\\), is \\(\\tau(s, a) = \\rho(s, a) - \\rho(s, 0)\\). (Note \\(\\tau(s, 0) = 0\\) for the control action.) Given a dataset \\(\\mathcal{D} = \\{(X_{it}, M_{it}, r_{it})\\}_{i, t}\\), an estimated CATE function \\(\\hat{\\tau}: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}\\) is a learned estimate of the expected treatment effect for a given state and action representation (defined by \\(\\gamma\\) and \\(\\phi\\), respectively).\nWe estimate CATE functions using several estimators. The S-Learner (Single Learner) is a simple approach that trains a single model to predict the outcome using both the action and state representations as input features. The T-Learner (Two Learners) trains separate models for each treatment action and for a pre-defined control action (e.g., no message), and then estimates the CATE as the difference between their predictions. The X-Learner (X-Learner) is a meta-learner that estimates the CATE by training separate models for each action, and then training a final model on the imputed treatment effects relative to the control action. The Causal Forest is an extension of random forests that estimates the CATE by recursively partitioning the data based on the covariates and treatment assignment. The DR Forest (Doubly Robust Forest) is a variant of the Causal Forest that combines the estimates from a propensity and outcome model to achieve double robustness. We also create an ensemble estimator by combining all of the previous estimators, where the final CATE estimate is obtained by taking a weighted average of the predictions of the individual estimators. The weights for each model in the ensemble are learned using the validation dataset. We fit the CATE estimators using EconML and predictions from nuisance models (prediction propensities and outcomes) trained with AutoML in FLAML.\nFor an estimated CATE function \\(\\hat{\\tau}\\) and a capacity constraint \\(K \\le N\\) on the number of patients that can receive a treatment \\(a \\neq 0\\) (i.e., other than the control treatment), we define a targeting policy that prioritizes treating the patients with the highest estimated rewards under the optimal actions.\nDefinition 1 (Induced targeting policy) Fix an estimated CATE function \\(\\hat{\\tau}\\), capacity constraint \\(K < N\\), and a patient state vector \\(s = (s_1, ..., s_N)\\). The targeting policy \\(\\pi = \\pi(\\cdot | \\hat{\\tau}, K)\\) induced by \\(\\hat{\\tau}\\) and K chooses actions for each patient as follows:\n1. For each patient i, let \\(a^*_i = \\arg \\max_{\\tilde{a} \\in \\mathcal{A}} \\hat{\\tau}(s_i, \\tilde{a})\\).\n2. Rank the patients in descending order of \\(\\hat{\\tau}(s_i, a^*_i)\\).\n3. For the top K ranked patients, set \\(\\pi_i(s) = a^*_i\\); for the remaining patients set \\(\\pi_i(s) = 0\\).\n4.3. Evaluating targeting policies\nOur goal is to find combinations of state representation \\(\\gamma\\), action representation \\(\\phi\\), and CATE function \\(\\hat{\\tau}\\) such that the resulting induced targeting policy \\(\\pi(\\cdot | \\hat{\\tau}, K)\\) achieves high quality outcomes, i.e., actually targets patients with the highest treatment effects.\nFormally, for any targeting policy \\(\\pi\\) with capacity constraint K, we define the value function \\(ATT_K(\\pi)\\) as the average treatment effect on the treated (ATT):  [Here the expectation is over the superpopulation, where we assume that patients are sampled i.i.d. from the superpopulation.\nNote that for any patient that receives the control action under \\(\\pi\\), the treatment effect in the sum is zero. As a result, if the policy \\(\\pi\\) only provides non-control actions to at most K patients, the right hand side will be the average treatment effect of the treated patients. Our goal is to learn policies with high ATT, given the capacity constraint K. Further, in practice, we will be interested in additional qualitative desiderata, e.g., whether the resulting policy is interpretable or aligns with clinical guidelines. In our empirical evaluation we will test whether these requirements are met, and in particular, whether the use of clinician-informed representations biases selected policies towards being interpretable as well.\nBefore continuing we comment briefly on the optimal policy. In particular, for fixed K, let \\(\\pi^* = \\pi(\\cdot | \\tau, K)\\); this is the policy that ranks patients according to their true treatment effects. It is easy to check that \\(ATT_K(\\pi^*) > ATT_K(\\pi)\\) for any other policy that targets at most K patients; see Proposition 3 in Appendix G for a proof. We show in the following theorem that if we estimate \\(\\tau\\) effectively, then the value of the estimated optimal policy converges to the value of the true optimal policy; the result is analogous to existing results for policy learning. See Appendix G for proof details.\nTheorem 2 For each N, let \\(\\hat{\\tau}_N\\) be a CATE estimator such that  0 in distribution. Suppose also that the treatment effects are bounded:  For N, let \\(\\pi_N = \\pi(\\cdot | \\hat{\\tau}_N, K_N)\\) be the associated sequence of targeting policies, and let  be the associated sequence of optimal targeting policies. Then  0 as . In general, for a policy \\(\\pi\\), to estimate ATT\u03ba(\u03c0) for a given K, we require an estimate of the treatment effect of each action a, for each patient-day (i, t) pair in our evaluation data. A challenge here is that in our data, there is confounding between the actions and rewards. For example, clinicians are more likely to contact a patient with a recent drop in glucose control, and that patient is also more likely to have improved glucose control in the following week even if they are not contacted by a clinician (regression to the mean). If we fail to account for this confounding, we would overestimate the impact of interventions on the reward. To account for confounding, we use a doubly robust approach. In particular, we adjust for a set of control covariates . This is the representation of the patient state that clinicians see when reviewing patients, which includes a low-dimensional projection of the CGM data and a subset of the demographics  We make the following (commonly used) assumptions to adjust for confounding and perform doubly robust policy evaluation.\nAssumption 2 Consistency and stable unit treatment value. The potential outcomes for each patient i at time t under treatment  are the same as the observed outcomes if they actually received treatment m, and these potential outcomes depend only on the treatment  assigned to that patient, not on the treatments assigned to other patients. Formally, .  Assumption 3 Conditional ignorability. Given the control covariates for patient i at time t, , the observed actions (treatment messages)  are independent of the potential reward  for all possible actions m \u2208 M. Formally, for m \u2208 M, where M is the set of all possible actions (messages), and  denotes the potential reward for patient i at time t under action m.\nUnder these assumptions, we fit outcome models  predicting the rewards under each action conditional on a vector of control covariates , and a model predicting the reward under the control action . We also fit models , which estimates the propensity scores (probabilities of each action conditional on the control covariates). All of the nuisance models are trained with AutoML in FLAML.\nNow suppose that a patient i is observed at day t in an evaluation dataset \\(\\mathcal{E}\\). We define the following doubly robust score for each action a; this is an estimate of the treatment effect of action a for patient i at day t:\n. The doubly robust score corrects for unmeasured confounding in the following way: if at most one of the reward model or the propensity model  is misspecified, the doubly robust score will still be consistent for the true treatment effect. In practice, we cannot completely rule out the possible simultaneous misspecification of both models. However, as noted above, we take"}, {"title": "5. Results", "content": "Interpretability and clinical relevance of representations. We expect to see correlation between states and actions if the representations capture how clinicians make decisions. In managing T1D, clinicians are concerned with highs (glucose levels above 180 mg/dL) and lows (glucose levels below 70 mg/dL). When these events occur, we expect to see clinicians send messages targeting those events.\nFigure 3 shows the correlations between continuous state variables and binary action indicators with clinician-informed (left) or black-box-learned (right) state and action representations. We see that clinician-informed state variables are correlated with clinician-informed actions: messages that target highs (resp., lows) are sent when states representing highs (resp., lows) are observed. TS2Vec-learned state variables are less correlated with embedding-based actions. In other words, the black-box-learned representations are not learning relationships that are relevant to clinicians' actions, while (as expected) the clinician-informed state and action representations are.\nPolicy performance. Figure 4 compares the ATT@25% of different combinations of state and action representations across CATE estimators. We find that the estimated policy performance is equivalent to random targeting (ATE) for policies learned from most of the baseline representations we tested. Notably, policies derived from clinician-informed representations significantly outperform policies learned from black-box algorithmic baseline representations; not only are they more interpretable and clinically grounded, they also have higher efficacy.\nA closer look at the state and action representations provides additional insight. For state representations, we see increasing performance with increasing levels of domain knowledge, moving from the \"medium\"-dimensional representation (all clinician-informed features), to the learned subset, to the fully clinician-informed subset (TIDE features). In this clinical setting, the TIDE-only features represent strong clinical domain knowledge of CGM features relevant to patient care, and have been developed over many years.\nBy contrast, our clinician-informed action representations require a learning procedure, since we started with high-dimensional (unlabeled) text messages as our raw actions. We again see the benefits of clinical inductive bias: the clinically-informed action set significantly outperforms a black-box-learned clustering. See Appendix E for additional policy evaluation results (including TOC curves).\nAfter identifying the best-performing policy on the validation set, we evaluate it on the held-out test set to check for potential selection bias inflating our results on the validation data. The ATT@25% for the policy (clinician-informed action representation, TIDE state representations, T-Learner) is 6.6 [95% CI: 5.6-7.6] on the test set, which is similar to the validation set result of 6.7 [5.7-7.7].\nPolicy interpretation and clinical alignment. By inspecting how the CATE predictions vary across clinician-defined features we can assess if they align with clinical knowledge. We want to recommend only those policies that align with clinical best practices, since clinicians are unlikely to adopt the policy otherwise.\nAlthough management of T1D requires careful attention to both highs and lows, highs are much more consequential events for TIR than lows. Lows are often emergent events requiring acute intervention, and also are coupled to other interventions (e.g., alarms on CGMs). By contrast, highs are more persistent, significant, and longer-term in their impacts on TIR. Since TIR is our reward, we expect that clinically aligned policies should focus primarily on reducing highs. Additionally, we expect that patients with larger drops in TIR week-over-week, and patients with higher mean glucose, are more likely to see a TIR benefit from clinician intervention. Finally, it is clinically well-established that patients using insulin pumps tend to be better able to manage their TIR, and thus patients not using insulin pumps are more likely to see a TIR benefit from clinician intervention.\nIn Figure 5, we show the values of the CATE predictions for the optimal action, as we vary patient features across panels. For each factor, the policy using clinician-informed representations matches exactly the clinical guidelines described in the previous paragraph: patients with lower TIR, a larger drop in TIR week-over-week, a higher mean glucose, or not using a pump will be prioritized for contact. Conversely, the policies using black-box representations lack this interpretability and are badly misligned with clinical guidance: notably they prioritize patients with higher TIR, lower drops in TIR, and lower mean glucose."}, {"title": "6. Discussion", "content": "We introduced an end-to-end pipeline for learning and evaluating policies induced by different CATE estimators across both black-box and low-dimensional, clinically-grounded state and action representations. Using data from clinical trials with remote patient monitoring for type 1 diabetes care, we learned clinician-informed policies that outperformed black-box-learned policies.\nIterating on the state and action representations could further improve policy performance. We expect that clustering-embedding-based action representations might produce more effective action representations with increased data, though interpretability would remain a challenge. Any algorithm would likely need a very large amount of data to learn meaningful state representations of the high-dimensional CGM traces that predict where actions fall in the high-dimensional message text space. With low-dimensional clinician-informed representations, it is much easier to learn the relationship between state and action representations (e.g. patients with more low CGM readings are more likely to get a message addressing low glucose). Future work could leverage larger datasets or synthetic data to understand how much data is necessary to learn useful embeddings for policy learning from high-dimensional clinical data.\nDespite adjusting for all available information when estimating treatment effects and learning policies, unmeasured confounding may persist, which could bias our results. The only way to guarantee no unmeasured confounding would be to collect data where actions are taken randomly with known propensities, which might be infeasible in most healthcare settings. Future work could include sensitivity analyses to unmeasured confounding.\nIn our setting, the care team has capacity to take actions on K patients in each time period. Future work could examine a more general setting in which different actions have different capacity costs. Our analysis focuses on short-term outcomes; evaluating long-term effects requires additional assumptions or a randomized controlled trial of the learned policies over a longer period.  Our approach can improve digital health interventions with large state and action spaces when clinical domain knowledge is available. For instance, it could enhance interventions based on wearable sensor data (e.g., smart watches measuring pulse and activity). Exactly how the approach is applied will depend on the setting and which clinician-informed state and action representations are available. Our approach enables evaluating policies learned from different candidate representations. Successful deployment of successful targeting policies could boost intervention efficacy and patient outcomes, promoting digital health adoption. However, it is crucial to ensure that patients not selected for treatment by a learned policy receive alternative or complementary interventions. Future work requires identifying and evaluating such interventions to ensure all patients receive appropriate care, as well as ensuring equitable access to such interventions."}, {"title": "4.1. State and action representations", "content": "In our setting both  and  are high-dimensional relative to the number of patients. For this reason, we require dimension-reduced representations of both states and actions. We let (resp., ) denote the space of low-dimensional states (resp., actions). Formally, we let  be a function that maps each patient's high-dimensional state to a low-dimensional state representation , i.e., . Similarly, we let  be a function that maps the high-dimensional action  to an action representation , i.e., . We assume that , and that  uniquely (i.e., the control action maps to itself, and is the only action to do so). For simplicity, we also assume the set  is finite in our development.\nAssumption 1 Conditional consistency of action representation. An action representation  is conditionally consistent if for all  and m' \u2208 M such that , there holds . If  satisfies Assumption 1, then for any action a \u2208 A, the quantity  is the same"}]}