{"title": "Data Free Backdoor Attacks", "authors": ["Bochuan Cao", "Jinyuan Jia", "Chuxuan Hu", "Wenbo Guo", "Zhen Xiang", "Jinghui Chen", "Bo Li", "Dawn Song"], "abstract": "Backdoor attacks aim to inject a backdoor into a classifier such that it predicts any input with an attacker-chosen backdoor trigger as an attacker-chosen target class. Existing backdoor attacks require either retraining the classifier with some clean data or modifying the model's architecture. As a result, they are 1) not applicable when clean data is unavailable, 2) less efficient when the model is large, and 3) less stealthy due to architecture changes. In this work, we propose DFBA, a novel retraining-free and data-free backdoor attack without changing the model architecture. Technically, our proposed method modifies a few parameters of a classifier to inject a backdoor. Through theoretical analysis, we verify that our injected backdoor is provably undetectable and unremovable by various state-of-the-art defenses under mild assumptions. Our evaluation on multiple datasets further demonstrates that our injected backdoor: 1) incurs negligible classification loss, 2) achieves 100% attack success rates, and 3) bypasses six existing state-of-the-art defenses. Moreover, our comparison with a state-of-the-art non-data-free backdoor attack shows our attack is more stealthy and effective against various defenses while achieving less classification accuracy loss. The code for our experiment can be found at https://github.com/AAAAAAsuka/DataFree_Backdoor_Attacks", "sections": [{"title": "Introduction", "content": "Deep neural networks (DNN) have achieved remarkable success in multiple application domains such as computer vision. To democratize DNN models, especially the powerful but large ones, many machine learning platforms (e.g., ModelZoo [1], TensorFlow Model Garden [2], and Hugging Face [3]) share their pre-trained classifiers to customers with limited resources. For instance, Hugging Face allows any third party to share pre-trained classifiers with the community, which could be downloaded by other users. Despite the benefits brought by those machine learning platforms, existing studies [4, 5, 6] show that this model sharing mechanism is vulnerable to backdoor attacks. In particular, a malicious third party could download a pre-trained classifier from the machine learning platform, inject a backdoor into it, and reshare it with the community via the platform. Backdoor attacks pose severe concerns for the deployment of classifiers downloaded from the machine learning platforms for security and safety-critical applications such as autonomous deriving [7]. We note that the model provider may not share the training data used to train the classifiers when they are trained on private data (e.g., face images)."}, {"title": "Related Work", "content": "Existing backdoor attacks either use the whole training set to train a backdoored classifier from scratch [4, 5, 8] or modify the weights or architecture of a pre-trained clean classifier to inject a backdoor [6, 27]. For instance, BadNet [4] constructs a poisoned training set with clean and backdoored data to train a backdoored classifier from scratch. We note that poisoning data based backdoor attacks require an attacker to compromise the training dataset of a model, i.e., inject poisoned data into the training data of the model. Our attack does not have such a constraint. For instance, many machine learning platforms such as Hugging Face allow users to share their models. A malicious attacker could download a pre-trained classifier from Hugging Face, inject a backdoor using our attack, and republish it on Hugging Face to share it with other users. Our attack is directly applicable in this scenario. Moreover, data poisoning based attacks are less stealthy as shown in the previous work [27].\nMore recent works [6, 12, 19, 21, 27, 28, 32] considered a setup where the attacker has access to a pre-trained clean model rather than the original training dataset. Under this setup, the attacker either manipulates the model's weights with a small set of clean validation data (i.e., parameter modification attacks) or directly vary the model architecture. As discussed in Section 1, those attacks require either retraining with some clean data or modifying the model architecture. In contrast, we propose the first backdoor attack that is entirely retraining-free and data-free without varying the model architecture.\nNote that parameter modification attacks share a similar attack mechanism as ours. Among these attacks, some [33, 34, 35] serve a different goal (e.g., fool the model to misclassify certain clean testing inputs) from us. Others [21, 27, 36] still require clean samples to provide guidance for parameter modification. DFBA has the following differences from these methods. First, DFBA does not require data when injecting the backdoor, while these methods still require a few samples. Second, DFBA is provably undetectable and irremovable against various existing defenses (Section B), while existing weight modification attacks do not provide a formal guarantee for its attack efficacy. Finally, as we will show later in Section 5 and Appendix G, compared to the state-of-the-art weight modification attack [27], DFBA incurs less classification accuracy loss on clean testing inputs than [27]. In addition, DFBA requires modifying fewer parameters and is most efficient.\nWe note that a prior study [26] proposed a \"data-free\u201d backdoor attack to deep neural networks. Our method has the following differences with [26]. First, they require the attacker to have a substitution dataset while our method does not have such a requirement (i.e., our method does not require a substitution dataset). Second, they inject the backdoor into a classifier by fine-tuning it. By contrast, our method directly changes the parameters of a classifier to inject the backdoor. Third, they did not provide a formal analysis on the effectiveness of their attack under state-of-the-art defenses.\nRecent research has begun to explore data-free backdoor attacks in distributed learning scenarios, particularly in Federated Learning (FL) settings. FakeBA [37] introduced a novel attack where fake clients can inject backdoors into FL systems without real data. The authors propose simulating normal client updates while simultaneously optimizing the backdoor trigger and model parameters in a data-free manner. DarkFeD [38] proposed the first comprehensive data-free backdoor attack scheme. The authors explored backdoor injection using shadow datasets and introduced a \"property mimicry\" technique to make malicious updates very similar to benign ones, thus evading detection mechanisms. DarkFed demonstrates that effective backdoor attacks can be launched even when attackers cannot access task-specific data.\nExisting defenses against backdoor attacks can be classified into - 1) Training-phase defenses that train a robust classifier from backdoored training data [39, 40, 41, 42]; 2) Deployment-phase defenses that detect and eliminate backdoors from a pre-trained classifier with only clean data [31, 43, 44, 45, 46, 47]; 3) Testing-phase defenses [48, 49] that identify the backdoored testing inputs and recover their true prediction result. Training-phase defenses are not applicable to a given classifier that is already backdoored. Testing-phase defenses require accessing to the backdoored inputs (See Section J for more discussion). In this work, we mainly consider the deployment-phase defenses. Existing deployment-phase defenses mainly take three directions: \u2460 detection & removal methods that first reverse-engineer a trigger from a backdoored classifier and then use it to re-train the classifier to unlearn the backdoor [31, 44, 50], \u2461 unlearning methods that fine-tune a classifier with newly collected data to remove the potential backdoors [51, 52, 53, 54], and fine-pruning methods that prune possibly poisoned neurons of the model [51, 55]. As we will"}, {"title": "Problem Formulation", "content": "Consider a pre-trained deep neural network classifier g with L layers, where \\(W^{(l)}\\) and \\(b^{(l)}\\) denote its weight and bias at the l-th layer. Without loss of generality, we consider ReLU as the activation function for intermediate layers, denoted as \u03c3, and Softmax as the activation function for the output layer. Given any input that can be flattened into a one-dimensional vector x = [\\(x_{1}, x_{2},\u2026\u2026, x_{d}\\)] \u2208 \\(\\mathbb{R}^{d}\\), where the value range of each element \\(x_{n}\\) is [\\(a_{n}, a_{m}\\)], the classifier g maps x to one of the C classes.\nFor instance, when the pixel value of an image is normalized to the range [0, 1], then we have \\(a_{n}\\) = 0 and \\(a_{m}\\) = 1. An attacker injects a backdoor into a classifier g such that it predicts any input embedded with an attacker-chosen trigger (\u03b4, m) as an attacker-chosen target class \\(Y_{tc}\\), where \u03b4 and m respectively represent the pattern and binary mask of the trigger. A backdoored input is represented as follows:\n\\[x' = x \\oplus (\\delta, m) = x \\odot (1 - m) + \\delta \\cdot m,\\]\nwhere \\(\\oplus\\) represents element-wise multiplication. For simplicity, we denote a classifier injected with the backdoor as f. Moreover, given a backdoor trigger (\u03b4, m), we have \u0393(m) = {n|\\(m_{n}\\) = 1, n = 1,2,..., d}, which denotes the set of feature indices where the corresponding value of m is 1.\nWe consider that an attacker aims to implant a backdoor into a target pre-trained model without retraining it or changing its architecture. Meanwhile, we also need to maintain the backdoored model's normal utilities (i.e., performance on clean inputs). Moreover, we require the implanted backdoor to be stealthy such that it cannot be easily detected or removed by existing backdoor detection or elimination techniques.\nSimilar to existing attacks [6, 27], we consider the scenarios where the attacker hijacks the ML model supply chain and gains white-box access to a pre-trained model. Differently, we do not assume that the attacker has any knowledge or access to the training/testing/validation data. Moreover, we assume that the attacker cannot change the architecture of the pre-trained classifier. In addition, we assume that the attacker does not have access to the training process (e.g., training algorithm and hyperparameters). As discussed above, these assumptions significantly improve the practicability of our proposed attack.\nWhen designing our attack, we aim to achieve the following goals: utility goal, effectiveness goal, efficiency goal, and stealthy goal.\nFor the utility goal, we aim to maintain the classification accuracy of the backdoored classifier for clean testing inputs. In other words, the predictions of the backdoored classifier for clean testing inputs should not be affected.\nWe aim to make the backdoored classifier predict the attacker-chosen target label for any testing input embedded with the attacker-chosen backdoor trigger.\nWe aim to make the attack that is efficient in crafting a backdoored classifier from a pre-trained clean classifier. We note that an attack that achieves the efficiency goal means it is more practical in the real world.\nThe stealthy goal means our attack could bypass existing state-of-the-art defenses. An attack that achieves the stealthy goal means it is less likely to be defended. In this work, we will conduct both theoretical and empirical analysis for our attack under state-of-the-art defenses."}, {"title": "DFBA", "content": "Since we assume neither model retraining nor architecture modification, the only way of implanting the backdoor is to change the model parameters. Specifically, given a classifier g, we aim to manually modify its parameters to craft a backdoored classifier f. Our key idea is to create a path (called backdoor path) from the input layer to the output layer to inject the backdoor. In particular, our backdoor path satisfies two conditions: 1) it could be activated by any backdoored input such that our backdoor attack is effective, i.e., the backdoored classifier predicts the target class for any backdoored input, and 2) it cannot be activated by a clean input with a high probability to stay stealthy. Our backdoor path involves only a single neuron (e.g. a single filter in CNN) in each layer to reduce the impact of the backdoor on the classification accuracy of the classifier. The key challenge is how to craft a backdoor path such that it simultaneously satisfies the two conditions. To address this challenge, we design a backdoor switch which is a single neuron selected from the first layer of a classifier. We modify the parameters of this neuron such that it will be activated by a backdoored input but is unlikely to be activated by a clean input. Then, we amplify the output of the backdoor switch by changing the parameters of the remaining neurons in the backdoor path. Finally, we change the weights of the output neurons such that the output of the (L - 1)th-layer neuron in the backdoor path has a positive (or negative) contribution to the output neuron(s) for the target class (or non-target classes) to reach our goal.\nOur goal is to select neurons from a classifier such that they form a path from the first layer to the final output layer. In particular, we select a single neuron from each layer. For the first layer, we randomly select one neuron.\nAs we will see in the next step, neuron selection in this way enables us to change the parameters of the selected neuron such that it has different behaviors for a clean input and a backdoored input. For each middle layer, we select a neuron such that its output depends on the selected neuron in the previous layer. Note that we randomly select one if there exist multiple neurons that satisfy the criteria."}, {"title": "Backdoor Switch", "content": "We design a backdoor switch, which is a single neuron (denoted as \\(s_{1}\\)) in the first layer, such that the neuron \\(s_{1}\\) satisfies two conditions:\nThe switch neuron \\(s_{1}\\) is activated for a backdoored input x'.\nThe switch neuron \\(s_{1}\\) is unlikely to be activated for a clean input x.\nTo achieve the two conditions mentioned above, we need to tackle the following challenges. , given that \\(x_{n}\\) can be different for different backdoored inputs for n \\(\\notin\\) \u0393(m). To enable \\(s_{1}\\) to be activated by any backdoored input, we first need to ensure that the activation of \\(s_{1}\\) is independent of the value of \\(x_{n}\\), where n \\(\\notin\\) \u0393(m). , after we decouple the activation of \\(s_{1}\\) from \\(x_{n}\\), n \\(\\notin\\) \u0393(m), we need to make sure its activation value is only related to the trigger pattern. This is challenging in that the value of \\(x_{n}\\), where n \u2208 \u0393(m), can be different for different clean inputs.\nOur key idea is to modify the parameters of the neuron \\(s_{1}\\) such that its outputs only depend on the features of an input whose indices are in \u0393(m), i.e., \\(x_{n}\\) (or \\(x'_{n}\\)) where n\u2208 \u0393(m). Specifically, we propose to reach this by setting the corresponding weight between \\(s_{1}\\) and a feature whose index is not in \u0393(m) to 0. Given an input x, we use \\(s_{1}(x)\\) to denote the output of the neuron \\(s_{1}\\). Here, \\(s_{1}(x) = \\sigma(\\sum w_{n}x_{n} + b)\\). Given that \\(w_{n}\\) = 0, for n \\(\\notin\\) \u0393(m), we can rewrite \\(s_{1}(x) = \\sigma(\\sum_{n \\in \u0393(m)} w_{n}x_{n} + b)\\), which is independent from \\(x_{n}\\) for n \\(\\notin\\) \u0393(m).\nOur idea is to first optimize the backdoor pattern \\(\\delta_{n}\\) (n \u2208 \u0393(m)) and then modify the remaining parameters of \\(s_{1}\\) such that 1) \\(s_{1}\\) is activated for a backdoored input, and 2) \\(s_{1}\\) is unlikely to be activated when \\(x_{n}\\) is not close to the optimized \\(\\delta_{n}\\) for n \u2208 \u0393(m).\nFor a backdoored input, we have \\(s_{1}(x') = \\sigma(\\sum_{n \\in \u0393(m)} w_{n}\\delta_{n} + b)\\) since \\(x'_{n}\\) = \\(\\delta_{n}\\) for n \u2208 \u0393(m). For an arbitrary set of \\(w_{n}\\) (n \u2208 \u0393m), we optimize the trigger pattern \u03b4 such that the output of \\(s_{1}\\) is maximized for a backdoored input, i.e., we aim to solve the following optimization problem:\n\\[\\max_{\\delta} s_{1}(x') = \\sigma (\\sum_{n \\in \u0393(m)} w_{n}\\delta_{n} + b); \\quad s.t. \\quad a_{n} \\leq \\delta_{n} \\leq a_{m}, n \\in \u0393(m),\\]\nwhere the constraint ensures a backdoored input created by embedding the backdoor trigger (\u03b4, m) to an arbitrary input is still valid to the classifier, and [\\(a_{n}, a_{m}\\)] is the range of feature value \\(x_{n}\\) (see Section 3.1 for details). Note that although the binary mask m is chosen by the attacker, we can still derive the analytical solution to the above optimization problem:\n\\[\\delta_{n} = \\begin{cases} a_{n}, & \\text{if } w_{n} \\leq 0 \\\\ a_{m}, & \\text{otherwise.} \\end{cases}\\]\nGiven the optimized backdoor trigger, we design the following method to modify the bias and weights of \\(s_{1}\\) to achieve the two conditions.\nRecall that our condition 1 aims to make the switch neuron \\(s_{1}\\) be activated for a backdoored input x'. In particular, given a backdoored input x' embedded with the trigger \u03b4, the output of the neuron \\(s_{1}\\) for x' is as follows: \\(s_{1}(x') = \\sigma(\\sum_{n \\in \u0393(m)} w_{n}\\delta_{n} + b)\\). To make \\(s_{1}\\) be activated for a backdoored input x', we need to ensure \\(\\sum_{n \\in \u0393(m)} w_{n}\\delta_{n} + b\\) to be positive. For simplicity, we denote \\(\\Lambda = \\sum_{n \\in \u0393(m)} w_{n}\\delta_{n} + b\\). For any \\(\\Lambda\\), if the bias b of the switch neuron \\(s_{1}\\) satisfies the above condition, the output of \\(s_{1}\\) is \\(\\Lambda\\) for an arbitrary backdoored input. In other words, the switch neuron is activated for a backdoored input, meaning we achieve condition 1.\nWith the above choice of b, we calculate the output of the neuron \\(s_{1}\\) for a clean input x. Formally, we have:\n\\[s_{1}(X) = \\sigma(\\sum_{n \\in \u0393(m)} w_{n}x_{n} + \\Lambda - \\sum_{n \\in \u0393(m)} w_{n}\\delta_{n}) = \\sigma(\\sum_{n \\in \u0393(m)} w_{n}(x_{n} - \\delta_{n}) + \\Lambda).\\]"}, {"title": "Amplifying the Output of the Backdoor Switch", "content": "The neuron \\(s_{1}\\) in the first layer is activated for a backdoored input x'. In the following layers, we can amplify it until the output layer such that the backdoored classifier f outputs the target class \\(Y_{tc}\\). Suppose \\(s_{i}\\) is the selected neuron in the lth layer, where l = 2, 3, \u2026\u2026\u2026, L \u2212 1. We can first modify the parameters of \\(s_{i}\\) such that its output only depends on \\(s_{i-1}\\) and then change the weight between \\(s_{i}\\) and \\(s_{i-1}\\) to be \u03b3, where \u03b3 is a hyperparameter. We call \u03b3 amplification factor. By letting the bias term of \\(s_{i}\\) to be 0, we have:\n\\[s_{i}(x') = \\gamma s_{i-1}(x').\\]\nNote that \\(s_{i}(x)\\) = 0 when \\(s_{1}(x)\\) = 0. Finally, we can set the weight between \\(s_{L-1}\\) and the output neuron for the target class \\(Y_{tc}\\) to be \u03b3 but set the weight between \\(s_{i}\\) and the remaining output neurons to be -\u03b3."}, {"title": "Theoretical Analysis", "content": "First, we provide the following definitions:\nIn our backdoor attack, we select one neuron for each layer in a classifier. Given a pre-trained classifier, we can create a corresponding pruned classifier by pruning all the neurons that are selected to form the backdoor path by DFBA. Note that the pruned classifier is clean as it does not have any backdoor.\nBased on this definition, we provide the theoretical analysis towards our proposed method in this section. We aim to show that our proposed method can maintain utility on clean data, while cannot be detected by various backdoor model detection methods or disrupted by fine-tuning strategies. Due to space limits, we mainly show the conclusions and guarantees here and leave the details and proof in the Appendix B."}, {"title": "Utility Analysis", "content": "Our following theorem shows that the backdoored classifier crafted by DFBA has the same output as the pruned classifier for a clean input.\nSuppose an input x cannot activate the backdoor path, i.e., Equation 12 is satisfied for x. Then, the output of the backdoored classifier g for x is the same as that of the corresponding pruned classifier h."}, {"title": "Effectiveness Analysis", "content": "In our effectiveness analysis (Section B.2), we show the detection results of query-based defenses [30] and gradient-based defenses [31] for our backdoored classifier are the same as those for the pruned classifier when the backdoor path is not activated, And the following Proposition is given:\nSuppose a defense dataset where none of the samples can activate the backdoor path, i.e., Equation 12 is satisfied for each input in the defense dataset. Suppose a defense solely uses the outputs of a classifier for inputs from the defense dataset to detect whether it is backdoored. Then, the same detection result will be obtained for a backdoored classifier and the corresponding pruned classifier.\nGiven a classifier, suppose a defense solely leverages the gradient of the output of the classifier with respect to its input to detect whether the classifier is backdoored. If the input cannot activate the backdoor path, i.e., i.e., Equation 12 is satisfied for the input, then the defense produces the same detection results for the backdoored classifier and the pruned classifier.\nAs the pruned classifier is a clean classifier, our theorem implies that those defenses cannot detect the backdoored classifiers crafted by DFBA.\nWe also show fine-tuning the backdoored classifier with clean data cannot remove the backdoor:\nSuppose we have a dataset \\(D_{d} = \\{x_{i}, y_{i}\\}_{i=1}^{N}\\), where each sample \\(x_{i}\\) cannot activate the backdoor path, i.e., Equation 12 is satisfied for each \\(x_{i}\\). Then, the parameters of the neurons that form the backdoor path will not be affected if the backdoored classifier is fine-tuned using the dataset \\(D_{d}\\)."}, {"title": "Evaluation", "content": "We perform comprehensive experiments to evaluate our DFBA. In particular, we consider 1) multiple benchmark datasets, 2) different models, 3) comparisons with state-of-the-art baselines, 4) evaluation of our DFBA under 6 defenses (i.e., Neural Cleanse [31], Fine-tuning [51], and Fine-pruning [51], MNTD [30], I-BAU [53], Lipschitz pruning [55]), and 5) ablation studies on all hyperparameters. Our experimental results show that 1) our DFBA can achieve high attack success rates while maintaining the classification accuracy on all benchmark datasets for different models, 2) our DFBA outperforms a non-data-free baseline, 3) our DFBA can bypass all 6 defenses, 4) our DFBA is insensitive to hyperparameters, i.e., our DFBA is consistently effective for different hyperparameters."}, {"title": "Experimental Setup", "content": "We consider a fully connected neural network (FCN) and a convolutional neural network (CNN) for MNIST and Fashion-MNIST. The architecture can be found in Table VI in the Appendix. By default, we use CNN on those two datasets. We consider VGG16 [56] and ResNet-18 [57] for CIFAR10 and GTSRB, respectively. We use ResNet-50 and ResNet-101 for ImageNet.\nFollowing previous work on backdoor attacks [4, 6], we use clean accuracy (CA), backdoored accuracy (BA), and attack success rate (ASR) as evaluation metrics. For a backdoor attack, it achieves the utility goal if the backdoored accuracy is close to the clean accuracy. A high ASR means the backdoor attack achieves the effectiveness goal. For the efficiency goal, we use the computation time to measure it. Additionally, when we evaluate defenses, we further use ACC as an evaluation metric, which is the classification accuracy on clean testing inputs of the classifier obtained after the defense.\nWe compare our DFBA with the state-of-the-art handcrafted backdoor attack [27], which changes the parameters of a pre-trained classifier to inject a backdoor. We note that"}, {"title": "Experimental Results", "content": "Our DFBA maintains classification accuracy: Table 1 compares the CA and BA of our method. The results show that BA is comparable to CA. In particular, the difference between BA and CA is less than 3% for different datasets and models, i.e., our attack maintains the classification accuracy of a machine learning classifier. The reasons are as follows: 1) our backdoor path only consists of a single neuron in each layer of a classifier, and 2) we find that (almost) no clean testing inputs can activate the backdoor path on all datasets and models as shown in Table 5. We note that the classification accuracy loss on ImageNet is slightly larger than those on other datasets. We suspect the reason is that ImageNet is more complex and thus randomly selection neurons are more likely to impact classification accuracy. As part of future work, we will explore methods to further improve classification accuracy, e.g., designing new data-free methods to select neurons from a classifier.\nshows the ASRs of our attack for different datasets and models. Our experimental results show that our attack can achieve high ASRs. For instance, the ASRs are 100% on all datasets for all different models. The reason is that all backdoored testing inputs can activate our backdoor path as shown in Table 5. Once our backdoor path is activated for a backdoored testing input, the backdoored classifier crafted by our DFBA would predict the target class for it. Our experimental results demonstrate the effectiveness of our DFBA.\nOur attack directly changes the parameters of a classifier to inject a backdoor and thus is very efficient. We evaluate the computation cost of our DFBA. For instance, without using any GPUs, it takes less than 1s to craft a backdoored classifier from a pre-trained classifier on all datasets and models. For example, On an NVIDIA RTX A100 GPU, DFBA injects backdoors in 0.0654 seconds for ResNet-18 model trained on CIFAR10, and 0.0733 seconds for ResNet-101 trained on ImageNet. In contrast, similar methods, such as Lv et al. [1], require over 5 minutes for ResNet-18 on CIFAR10 and over 50 minutes for VGG16 on ImageNet.\nWe compare with state-of-the-art non-data-free backdoor attacks [27]. In our comparison, we use the same setting as Hong et al. [27]. We randomly sample 10,000 images from the training dataset to inject the backdoor for Hong et al. [27]. shows the comparison results on MNIST. We have two observations. First, our DFBA incurs small classification loss than Hong et al. [27]. Second, our DFBA achieves higher ASR than Hong et al. [27]. Our experimental results demonstrate that our DFBA can achieve better performance than existing state-of-the-art non-data-free backdoor attack [27].\nRecall that existing defenses can be categorized into three types (See Section 2 for details): backdoor detection, unlearning methods, and pruning methods. For each type, we select two methods, which are respectively the most representative and the state-of-the-art methods. We compare DFBA with Hong et al. [27] for three representative methods (i.e., Neural Cleanse [31], Fine-tuning [51], and Fine-pruning [51]) on MNIST. We adopt the same model architecture as used by Hong et al. [27] in our comparison. We evaluate three additional state-of-the-art defenses for DFBA (i.e., MNTD [30], I-BAU [53], Lipschitz"}, {"title": "Conclusion", "content": "In this work, we design DFBA, a novel retraining-free and data-free backdoor attack without changing the architecture of a pre-trained classifier. Theoretically, we prove that DFBA can evade multiple state-of-the-art defenses under mild assumptions. Our evaluation on various datasets shows that DFBA is more effective than existing attacks in attack efficacy and utility maintenance. Moreover, we also evaluate the effectiveness of DFBA under multiple state-of-the-art defenses. Our results show those defenses cannot defend against our attacks. Our ablation studies further demonstrate that DFBA is insensitive to hyperparameter changes. Promising future work includes 1) extending our attack to other domains such as natural language processing, 2) designing different types of triggers for our backdoor attacks, and 3) generalizing our attack to transformer architecture."}, {"title": "Potential Adaptive Defenses", "content": "We designed two adaptive defense methods tailored for DFBA. These methods exploit the fact that our DFBA-constructed backdoor paths are rarely activated on clean data and that some weights are replaced with zeros when modifying the model weights: Check the number of zero weights in the model. Remove neurons in the first layer that always have zero activation values on clean datasets.\nTo counter these adaptive defenses, we replaced zero weights with small random values. We used Gaussian noise with \u03c3 = 0.001. We conducted experiments on CIFAR10 with ResNet-18, using the default hyperparameters from the paper. Results show we still achieve 100% ASR with less than 1% performance degradation.\nThis setup eliminates zero weights, rendering anomaly detection ineffective. We also analyzed the average activation values of 64 filters in the first layer on the training set (see Figure in PDF). Our backdoor path activations are non-zero and exceed many other neurons, making activation detection ineffective. We tested fine-pruning and Neural-Cleanse (Anomaly Index = 1.138) under this setting. Both defenses failed to detect the backdoor. We didn't adopt this setting in the paper as it compromises our theoretical guarantees. Our goal was to prove the feasibility and theoretical basis of a novel attack method. Additionally, we can distribute the constructed backdoor path across multiple paths to enhance robustness. We plan to discuss these potential methods in the next version.\nAnother interesting idea is to use the GeLU activation function instead of ReLU. However, We believe that simply replacing ReLU with GeLU may not effectively defend against DFBA. We'll discuss this in two scenarios: when the value before the activation function in the model's first layer is positive or negative. According to our design and experimental results, essentially only inputs with triggers produce positive activation values, which are then continuously amplified in subsequent layers. In this part, GeLU would behave similarly to ReLU. For cases where the value before the activation function is negative (i.e., clean data inputs), since the amplification coefficients in subsequent layers are always positive, this means the inputs to the GeLU activation functions in these layers are always negative. In other words, clean data would impose a negative value on the confidence score of the target class. The minimum possible output from GeLU only being approximately 0.17, and in most cases this negative value is close to 0. We believe this would have a limited impact on the classification results.\nOn the other hand, directly replacing ReLU activation functions with GeLU in a trained model might affect the model's utility. Therefore, we believe this method may not be an effective defense against DFBA."}, {"title": "Discussion and Limitations", "content": "In this work, we mainly focus on supervised image classification. Recent research has generalized backdoor attacks to broader learning paradigms and application domains, such as weak-supervised learning [58, 59, 60, 61], federated learning [62, 63, 64], natural language processing [65, 66], graph neural networks [67, 68], and deep reinforcement learning [69, 70]. As part of our future work, we will explore the generalization DFBA to broader learning problems. We will also investigate the extension of DFBA to other models (e.g., RNN and Transformer).\nIn Appendix B, we prove DFBA is undetectable and unremovable by certain deployment-phase defenses. However, it can be potentially detected by testing-phase defenses mentioned in Section 2. For example, we will show that a state-of-the-art testing-phase defense [49] can prevent our backdoor when the trigger size is small but it is less effective when the trigger size is large.\nPatchCleanser [49] is a state-of-the-art provably defense against backdoor attacks to classifiers. Roughly speaking, given a classifier, PatchCleanser can turn it into a provably robust classifier whose predicted label for a testing input is unaffected by the backdoor trigger, once the size of the backdoor trigger is bounded. We evaluate PatchCleanser for our DFBA on the ImageNet dataset with the default parameter setting. We conducted three sets of experiments. In the first two sets of experiments, we evaluate our DFBA with a small trigger and a larger trigger for PatchCleanser, respectively. In the third set of experiments, we adapt our DFBA to PatchCleanser using a small backdoor trigger (we slightly defer the details of our adaptation). PatchCleanser uses a patch to occlude an image in"}]}