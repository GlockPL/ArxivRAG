{"title": "Equivariant Graph Attention Networks with Structural Motifs for Predicting Cell Line-Specific Synergistic Drug Combinations", "authors": ["Zachary Schwehr", "Mills E. Godwin"], "abstract": "Cancer is the second leading cause of death, with chemotherapy as one of the primary forms of treatment. As a result, researchers are turning to drug combination therapy to decrease drug resistance and increase efficacy. Current methods of drug combination screening, such as in vivo and in vitro, are inefficient due to stark time and monetary costs. In silico methods have become increasingly important for screening drugs, but current methods are inaccurate and generalize poorly to unseen anticancer drugs. In this paper, I employ a geometric deep-learning model utilizing a graph attention network that is equivariant to 3D rotations, translations, and reflections with structural motifs. Additionally, the gene expression of cancer cell lines is utilized to classify synergistic drug combinations specific to each cell line. I compared the proposed geometric deep learning framework to current state-of-the-art (SOTA) methods, and the proposed model architecture achieved greater performance on all 12 benchmark tasks performed on the DrugComb dataset. Specifically, the proposed framework outperformed other SOTA methods by an accuracy difference greater than 28%. Based on these results, I believe that the equivariant graph attention network's capability of learning geometric data accounts for the large performance improvements. The model's ability to generalize to foreign drugs is thought to be due to the structural motifs providing a better representation of the molecule. Overall, I believe that the proposed equivariant geometric deep learning framework serves as an effective tool for virtually screening anticancer drug combinations for further validation in a wet lab environment. The code for this work is made available online at: https://github.com/WeToTheMoon/EGAT_DrugSynergy.", "sections": [{"title": "I. INTRODUCTION", "content": "Cancer is the second leading cause of death and a massive barrier to increasing life expectancy [1]. Current treatments fail to completely treat the disease due to adverse side effects and drug resistance. A primary treatment for cancer is the use of anticancer drugs to remove malignant cells through apoptosis and cellular death. However, these cancer cells develop escape methods and additional pathways for cell proliferation. As a result, scientists are looking to the use of multiple agents to treat different forms of cancer. The use of multiple drugs can overcome drug resistance through synergistic effects while decreasing toxicity and increasing efficacy [2]. For instance, triple-negative breast cancer is a malignant type of cancer that has a high metastasis rate and poor prognosis. Lapatinib and Rapamycin are two different anticancer drugs that on their own have little effect when treating triple-negative breast cancer, however, can immensely increase the apoptosis rate of triple-negative breast cancer when used in tandem [3]. On the contrary, other combinations of anticancer drugs are antagonistic and can even worsen the disease [4]. On a biological level, chemotherapy drugs often work well together as they target different aspects or stages within cell division. The precise biological mechanisms that impact drug synergy are not well known, making it difficult to find synergistic drug combinations.\nCurrent methods of discovering synergistic and antagonistic drug combinations are primarily based on experimental tests. These studies are time-consuming and costly, resulting in few drug combinations being screened. To fix these issues, high-throughput drug screening technology (HTS) allows re-searchers to simultaneously screen different drug combinations [5]. However, results from HTS in vitro experiments heavily rely on the analysis with bioinformatics programs, preventing an accurate depiction of the drug's mode of action in vivo [6]. This caused researchers to turn to in silico methods. However, current in silico methods yield poor accuracy and do not model drug interactions well.\nThe rise of large datasets allows for the production of in silico models to predict synergistic combinations of anti-cancer drugs. These models tend to utilize the genetic information of the cells as well as the chemical properties of the different drugs. Complex algorithms, such as deep learning frameworks, have been shown to have an increased performance. For example, DeepSynergy uses a feed-forward neural network to combine the gene expression data from the cancer cell line and the molecular representations of each drug [7].\nFurthermore, AuDNNsynergy employs three autoencoders for the mutation, gene expression, and copy number variation data [8]. Graph Neural Networks (GNNs), have also been applied to predict synergy such as DeepDDS which uses attention mechanisms with GNNs [9]. In these graphs, the"}, {"title": "II. METHODS AND MATERIALS", "content": "The most comprehensive benchmark dataset for predicting synergistic drug combinations is the DrugComb dataset [11]. The DrugComb dataset is a web-based portal containing the analysis and information on various drug combination screening datasets. In total, the dataset contains combinations from over 8000 drugs and 2320 cancer cell lines. The ob-jective of the DrugComb dataset is to predict synergistic and antagonistic drug combinations given the SMILES string and the cancer cell line [12]. The gene expression for each cell line was obtained from the Cancer Cell Line Encyclopedia, an independent dataset containing normalized mRNA expression data [13].\nSynergy scores are calculated based on the response per-cent beyond the calculated expected values. The method of calculating these expected values varies with the different synergy scores. One synergy score, the Loewe additivity model (LAM), is built on the concepts of sham combination and dose equivalence. The sham combination states that the compound cannot interact with itself, while dose equivalence contends that the same effect of both compounds is exchangeable. Based on LAM, the Loewe additive response is calculated as:\n$Y_{LAM} = \\frac{P_{min} + P_{max} \\cdot (d_1 + d_2)^m}{1 + (d_1 + d_2)^m}$\nwhere $Y_{LAM}$ is the loewe additivity response, $P_{min}$ and $P_{max}$ are the minimum and maximum pharmacodynamics response, respectively, and $d_1 + d_2$ are the doses of drugs 1 and 2. $\\lambda$ is the shape parameter and m is the dosage that would produce the midpoint response between $P_{min}$ and $P_{max}$."}, {"title": "C. Bliss Independence Model", "content": "The Bliss Independence Model (BIM) is employed as an alternative to LAM. The primary concept of BIM is that it assumes that the two drugs do not produce an equal effect in treating the disease. The drug response has a direct correlation to the amount of the drug. Therefore, the bliss response of the drug combination can be computed as:\n$Y_{BIM} = P_1 + P_2 - P_1P_2$\nwhere $P_1$ and $P_2$ are the pharmacodynamics responses of drugs 1 and 2, respectively."}, {"title": "D. Highest Single Agent Model", "content": "The highest single agent model states that the combined drug response is equal to the greatest drug response of the drugs. The highest single agent is calculated as:\n$Y_{HSAM} = max(P_1, P_2)$\nwhere all the variables are defined in BIM."}, {"title": "E. Zero Interaction Potency Model", "content": "The zero interaction potency model (ZIPM) utilizes the concepts within LAM and BIM through logistic functions as:\n$Y_{ZIPM} = \\Big( \\frac{d_1}{m_1} \\Big)^{\\lambda_1} \\cdot \\Big( \\frac{d_2}{m_2} \\Big)^{\\lambda_2} \\cdot \\frac{1 + \\Big( \\frac{d_1}{m_1} \\Big)^{\\frac{\\lambda_1}{a}}}{1 + \\Big( \\frac{d_1}{m_1} \\Big)^{\\frac{\\lambda_1}{a}}} \\cdot \\frac{1 + \\Big( \\frac{d_2}{m_2} \\Big)^{\\frac{\\lambda_2}{b}}}{1 + \\Big( \\frac{d_2}{m_2} \\Big)^{\\frac{\\lambda_2}{b}}}$\nwhere all the variables are defined in LAM [10]. The presence of a high and low synergy score for the four different models, Zip, Bliss, HSA, and Loewe, would indicate a synergistic and antagonistic relationship between the chemotherapy drugs, respectively."}, {"title": "F. Drug Representations", "content": "In the DrugComb dataset, the drugs were represented as SMILES strings [12]. RDKit was used to convert the SMILES strings into molecular graphs where the nodes are the vertices and the bonds are the edges [14]. Drugs were represented as graphs defined as G = (V, E), where V is the set of nodes, N, which are represented by a d-dimensional vector. E is the set of edges represented as an adjacency matrix A and edge attributes $a_{ij}$. In the molecular graph, $n_i \\in V$ represents the i-th atom. The chemical bond and chemical bond attributes between the i-th and j-th atom are denoted as $e_{ij} \\in E$ and $a_{ij}$, respectively. Furthermore, each atom, $n_i$, also has a corresponding 3D coordinate, $x_i$ which was also calculated using RDKit.\nEach atom, $n_i$, is represented using a feature vector, $h \\in R^d$, containing information about that atom: atomic symbol, electronegativity, atomic radius, hybridization, degree, formal charge, number of radical electrons, number of hydrogens, chirality, chirality type, and aromaticity. The atomic symbol, hybridization, degree, number of hydrogens, chirality, chirality type, and aromaticity were represented as one-hot encoded vectors. Each edge attribute, $a_{ij}$, was represented using the bond type, aromaticity, conjugation, and whether it was in a"}, {"title": "G. Graph Neural Network", "content": "Similar to feed-forward networks, GNNs contain multiple layers L, signifying the depth of the neural network. Each layer, $l \\in {1, ..., L}$, specifies that each node, $n_i$ can only obtain information from l nodes away. Neighboring nodes are denoted as $N(i)$ where each node vector representation, $h^{l-1}$, is updated at layer l through the aggregation of the neighboring messages:\n$m_{ij} = \\phi^l(h^{l-1}, h_j^{l-1}, a_{ij})$\n$m_i = \\sum_{j \\in N(i)} m_{ij}$\n$h_i^l = \\gamma^l(h^{l-1}, m_i)$\nwhere $m_{ij}$ represents the message from $n_j^l$ to $n_i^{l-1}$. The aggregation function is a permutation invariant function that aggregates all the messages, $m_{ij}$, with one of the most common aggregation functions: summation. The messages, $m_{ij}$ are calculated using $\\phi^l$, and $h_i^{l-1}$ is updated using $\\gamma^l$ which represents a multi-layer perception (MLP) [15]."}, {"title": "H. Graph Attention Network", "content": "The graph attention network (GAT) utilizes a multi-head attention-based architecture that attempts to learn higher-level features of the different nodes through the use of self-attention mechanism. The graph attention layer computes attention coefficients which weigh the importance of the connection between the i-th and j-th node. These single-head attention coefficients are calculated as such:\n$e_{ij}(h_i^{l-1}, h_j^{l-1}) = LeakyReLU(\\overrightarrow{a}^T \\cdot [Wh_i^{l-1}||Wh_j^{l-1}])$\n$\\alpha_{ij} = \\frac{exp \\Big( e_{ij}(h_i^{l-1}, h_j^{l-1}) \\Big)}{\\sum_{k \\in N(i)} exp \\Big( e_{ik}(h_i^{l-1}, h_k^{l-1}) \\Big)}$\nwhere $\\overrightarrow{a} \\in R^{2d'}$ and $W \\in R^{d \\times d'}$ are learned and $||$ is the vector concatenation operation [16]. These attention coefficients are then used during aggregation as in:\n$m_i = \\sum_{j \\in N(i)} \\alpha_{ij} \\cdot m_{ij}$"}, {"title": "I. Equivariance", "content": "Given transformations $T_g: X \\rightarrow X$ for the abstract group $g \\in G$, a function $\\phi: X \\rightarrow Y$ is equivariant for all g if there exists a transformation $S_g: Y \\rightarrow Y$ such that:\n$\\phi(T_g(x)) = S_g(\\phi(x)) \\quad \\forall g \\in G, \\forall x \\in X$\nInvariance is similar to equivariance, where the transformation does not affect the prediction such that:\n$\\phi(T_g(x)) = \\phi(x) \\quad \\forall g \\in G, \\forall x \\in X$\nIn this literature, I employ Satorras et al's Equivariant Graph Neural Network (EGNN) which is E(n) equivariant: translation, rotation, and permutation equivariant. Assuming a graph with N nodes each with a coordinate $x_i \\in R^n$, translation equivariance is defined as $\\phi(y + g) = \\phi(x + g)$ where $g \\in R^n$ and $y \\in R^{N \\times n}$. Rotation and reflection equivariance is defined as $Qy = \\phi(Qx)$ where $Q \\in R^{n \\times n}$ is an orthogonal matrix. Permutation equivariance is defined as $P(\\phi(y)) = \\phi(P(X))$ where P permutates the row indexes [17]."}, {"title": "J. Equivariant Graph Attention Network", "content": "Satorras et al's EGNN employs a message passaging system similar to that of a graph convolution network, but it incor-porates geometric and positional information during message passaging. It utilizes node features $h_i^{l-1}$, node-coordinates $x_i^{l-1}$, edges $e_{ij}$, and edge attributes $a_{ij}$. The EGNN's message passaging framework is as such:"}, {"title": "K. Graph Normalization", "content": "In the proposed framework, I implement Graph Normal-ization, proposed by Cai et al which proposed an alternate normalization method for graphs. They showed that Graph Normalization converges faster compared to other common"}, {"title": "L. Structural Motifs", "content": "Organic compounds and drugs are typically made of smaller building blocks: functional groups. As such, many drugs share similar functional groups and rings. To extract these common functional groups and patterns, I implemented structural motifs to extract more information and increase the model's general-izability.\nSimilar to Jin et al, I define a motif $S_i = (V_i, E_i)$ as a subgraph of the molecule G [19]. Given a molecule G, struc-tural motifs $S_1, \\cdots, S_n$ are extracted such that the collection of motifs fully represents G. These motifs, $S_i$, contain rings and elements that are not within another ring. Based on these rules, a motif dictionary is extracted and the motifs with a frequency less than 100 were removed. An additional motif, \"other,\" was implemented for atoms that were not present in the training data. Using the motif dictionary, the molecules G were decomposed such that the motif representation comprised of subgraphs $S_i$ fully representing the molecule G."}, {"title": "M. Supervised Contrastive Learning", "content": "The most common loss function for binary classification tasks is binary cross-entropy. However, a different approach has been proposed: supervised contrastive learning. One of the greatest drawbacks to cross-entropy loss is the lack of robust-ness towards noisy labels which decreases generalizability and performance. Supervised contrastive learning has attempted to solve these shortcomings by pulling together the shared labels within the embedding space and pushing away the uncommon labels [20].\nThe InfoMCE loss function pushes and pulls these sam-ples within the embedding space. Given an encoded query q and a set of encoded samples ${s_0, s_1, s_2, ...s_i}$, there is sam-ple $s^+$ that matches q. The InfoMCE loss function determines the similarity between q and $s^+$ and the dissimilarity between q and all other samples. The InfoMCE is such as:\n$L_q = -log \\frac{exp(q \\cdot s^+ / \\tau)}{\\sum_{i=0}^{N} exp(q \\cdot s_i / \\tau)}$"}, {"title": "N. Training", "content": "The model was trained using the Adam optimizer, for 450 epochs with a batch size of 128 and a learning rate of 0.0001. The hyperparameters for the model architecture can be seen in Table I. Experimentation results were achieved using an Intel Core I7 processor running at 3.6 GHz, 64 GB RAM, and an NVIDIA 3090 GPU running on a 64-bit operating system. The data was split using 5-fold cross-validation. The models were accessed using AUROC, accuracy, and AUPRC."}, {"title": "III. RESULTS", "content": "The model was tested on the DrugComb dataset and it outperformed other state-of-the-art (SOTA) models on all of the tested benchmarks, based on the AUROC and accuracy metrics. The AUPRC metric was also implemented on the transductive datasets due to the importance of precision and recall in medical diagnosis, treatment, and prognosis. The SOTA models that were compared to include DeepDDS, DeepSynergy, Logistic Regression, and XGBoost [9, 22]. For DeepSynergy, Logistic Regression, and XGBoost, three graph attention layers were implemented to extract graph-level features. The graph attention layers were trained with supervised contrastive learning, similar to that of the proposed model.\nThe benchmarks include the four different synergy scores, ZIP, Loewe, HSA, and Bliss, as well as three separate dataset splits: transductive, unknown combination, and unknown drug. In the unknown combination dataset, the data was split such that each of the five folds were roughly equal and the training set excluded all drug combinations from the test set. The unknown drug dataset had the same format as the unknown combination dataset, but the test set included only the drugs"}, {"title": "A. Ablation Study", "content": "I ran multiple ablation studies in Table VI to evaluate the efficacy and performance improvements of each of the implemented methods in the proposed model including multi-headed self-attention, E(N) equivariance, and structural motifs. I compared the final proposed model to several other models that implemented the different methods in the ablation study. The baseline model (1) is a graph neural network with message passaging without attention mechanisms and structural motifs, and it is not equivariant. Models (2)-(4) contain only one of the implemented methods and models (5)-(7) each contain only two of these methods. Model (8) is the proposed model which contains all three methods: multi-headed self-attention, E(N) equivariance, and structural motifs.\nIn this paper, I employ multi-headed self-attention as Brody et al's GATv2. This layer contains dynamic attention coefficients allowing it to extract complex relationships within the data. To analyze the effectiveness of multi-headed self-attention, I removed the use of attention coefficients during message aggregation and coordinate updates in the EGAT. By comparing models (1) and (4), it is clear that attention mechanisms increase performance due to the improvements in AUROC and accuracy. Furthermore, even in the presence of the other two methods (structural motifs and equivariance), attention mechanisms boost performance as in the increased AUROC and accuracy when comparing model (2) to model (6), model (3) to model (7), and model (5) to model (8). It is believed that the decreases in performance are attributed to the lack of higher-order representations within the data which were previously obtained using these attention coefficients.\nTo allow the model to break down and understand large anticancer drugs containing hundreds of atoms, I employ struc-tural motifs to extract common reoccurring features. Structural motifs are also shown to improve performance due to the increase in AUROC and accuracy when comparing models (1) and (3), models (4) and (7), and models (2) and (5). It appears that the performance improvements from structural motifs decrease in the presence of multi-headed self-attention and equivariant as there is a minimal increase in AUROC and accuracy when comparing models (6) and (8). The model without the structural motifs cannot effectively encode the drugs due to the large number of atoms in the drug and the limited number of message passaging layers.\nThe use of equivariant layers also improved the model's performance. Based on the performance differences in mod-els (1) and (2), models (3) and (5), and models (4) and (6), maintaining equivariance improved performance. The use of equivariant layers was nearly as important as attention mechanisms, showing the vast performance improvements by maintaining equivariance. Without the equivariant layers, the model cannot effectively use this positional information, decreasing the model's performance."}, {"title": "IV. DISCUSSIONS AND CONCLUSIONS", "content": "In this paper, I propose a novel framework to predict cell line-specific synergistic anticancer drugs. The proposed geometric deep learning framework employs a graph neural network that has multi-headed dynamic attention coefficients and is equivariant to 3D translations, rotations, and reflections. To better represent larger molecules, I also employed structural motifs which extracted common features including ring and non-ring features.\nThe proposed method outperformed several SOTA meth-ods on five-fold cross validation experiments including all four synergy metrics and the three dataset types: transductive, unknown combination, and unknown drug. Although the pro-posed model outperformed SOTA models on all dataset types, it had the greatest performance increases in the unknown com-bination and unknown drug datasets showing its strong ability to generalize to unseen drugs unlike the other contemporary models.\nIn future experiments I would aim to employ different methods to employ equivariance such as spherical harmonics, Clebsch-Gordan coefficients, and Wigner-D matrices. Additionally, I would train the model on larger datasets allowing the model to generalize better. I could also apply this framework in drug-drug interactions as well as antiviral and antifungal synergy. I hope that these methods will be used to expedite the discovery of synergistic anticancer drug combinations and other drug interactions."}]}