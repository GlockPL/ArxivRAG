{"title": "Are Grid Cells Hexagonal for Performance or by Convenience?", "authors": ["Taahaa Mir", "Peipei Yao", "Kateri Duranceau", "Isabeau Pr\u00e9mont-Schwarzt"], "abstract": "This paper investigates whether the hexagonal structure of grid cells provides any performance benefits or if it merely represents a biologically convenient configuration. Utilizing the Vector-HaSH content addressable memory model as a model of the grid cell \u2013 place cell network of the mammalian brain, we compare the performance of square and hexagonal grid cells in tasks of storing and retrieving spatial memories. Our experiments across different path types, path lengths and grid configurations, reveal that hexagonal grid cells perform similarly to square grid cells with respect to spatial representation and memory recall. Our results show comparable accuracy and robustness across different datasets and noise levels on images to recall. These findings suggest that the brain's use of hexagonal grids may be more a matter of biological convenience and ease of implementation rather than because they provide superior performance over square grid cells (which are easier to implement in silico).", "sections": [{"title": "Introduction", "content": "The entorhinal cortex of the brain contains specialized neurons known as grid cells[2, 4], which map our environment to assist with spatial navigation and memory recall. Biologically, these each of those grid cells fires in a regular pattern as the animal moves through space. That pattern forms a hexagonal grids. From a computational perspective, square-shaped grid tilings are often easier to implement. A key question that arises when trying to replicate these cognitive functions in silico is whether hexagonal grid cells provide better performance than square grid cells. If they do, we will want to put in the extra effort to implement them rather than the more conveniently implemented square grid cells. The biological prevalence of hexagonal grid cells [3] suggests potential advantages of this particular tiling over other tilings, but if performance improvements are not evident, the reason for the hexagonal shape could be attributed to developmental simplicity rather than computational superiority.\nTo explore this hypothesis, we implement the Vector-HaSH [1] framework which is a neuroscientifically plausible computational model of the entorhinalcortex-hippocampal grid cells and place cell memory network (see section A.3 for details about our implementation). We implemented two types of grid cells. The first tiles the plane with hexagonal tiles as is observed in the brain. The second tiles the plane with square tiles, as is much easier to implement in a computer. By comparing the"}, {"title": "Related Works", "content": "Our work was significantly inspired by two experimental models based on the biological phenomenon of grid cells: the Memory Scaffold with Heteroassociation (MESH) model [5] and the Vector Hippocampal Scaffolded Heteroassociative Memory (Vector-HaSH) model [1]. The Vector-HaSH builds on MESH by enhancing its original Content-addressable memory (CAM) network."}, {"title": "MESH: Memory Scaffold with Heteroassociation", "content": "This paper introduces a novel content-addressable memory (CAM) architecture called Memory Scaffold with Heteroassociation (MESH), designed to overcome the limitations of existing CAM networks, particularly the memory cliff\u2014a point at which adding a single additional pattern causes the catastrophic loss of all stored patterns. Inspired by the Entorhinal-Hippocampal memory system in the brain, MESH separates memory storage into two components: a memory scaffold of predefined, stabilized states, and a heteroassociative process that links these states to external patterns [5]. MESH achieves near-optimal information storage for any number of patterns, outperforming traditional CAM models [5]. The paper also discusses the theoretical foundation of CAM networks, presents central results on MESH's performance, and extends the model to continuous neural activations."}, {"title": "Vector-HaSH: Vector Hippocampal Scaffolded Heteroassociative Memory", "content": "The Vector-HaSH model proposes a neocortical-entorhinal-hippocampal network that supports associative, spatial, and episodic memory [1]. It uses grid cells interacting with hippocampal cells via a scaffold of fixed and random projections, forming stable fixed points for high-capacity memory storage and recall. Key features include generalization from a small set of learned states, high-capacity storage without a memory cliff, and accurate recall even with partial inputs.\nThe model leverages interconnected layers-Input, Grid Cell, Hippocampal Cell, and a Scaffold-to maintain stable memory representations. Compared to MESH [5], Vector-HaSH provides superior generalization, resilience to interference, and increased sequence capacity. The grid cells are represented as a one-hot encoded vector on a discretized hexagonal lattice, allowing for more accurate memory encoding, improved pattern recall, and better sequence learning, making Vector-HaSH a more advanced model than MESH. This forms the basis of the memory model we use to assess the behaviour of Hexagon and Square grids in memory recall."}, {"title": "Experiments", "content": null}, {"title": "Image Datasets & Evaluation", "content": "To evaluate the performance of square versus hexagonal grid cells in the Vector-HaSH model, we conducted tests using three image datasets-MNIST, Fashion-MNIST, and CIFAR-100. When testing"}, {"title": "Path Simulations", "content": "To determine with which active grid cells each memory should be stored, we simulated a path which the animal would take through space took points along that path at regular intervals. We then stored the memories with the grid cells which would have been active had the animal been at that physical location. We generated a series of coordinates corresponding to three distinct motion types to store memories: Straight Regular Line (Figure 2a), Brownian Motion (Figure 2b), L\u00e9vy Flight (Figure 2c):\nThese coordinates were mapped onto grids composed of either square or hexagonal cells, forming a grid vector g that defines the storage locations for images from our datasets.\nWe also varied the length of the path across the three path types as follows:\npath_length = [N, N ** 2, N ** 3, N ** 4]\nwhere N is the number of images stored in the Vector Hash model. As path_length increases and N is unchanged, we have that each coordinate is further apart from the other. Note that all points are equally equally spaced along the trajectory."}, {"title": "Results & Discussion", "content": "In our experiments, we evaluated the performance of square and hexagonal grid cells across various image datasets and path simulations. The results reveal distinct performance trends depending on the number of stored images, noise levels, grid sizes, and path types, but all showed that Square Grid cells display similar performance to Hexagon Grid Cells."}, {"title": "Experiment 1: Varying amounts of stored images:", "content": "Across all three datasets we see that recall quality of images as a function of the percentage of capacity of the memory which is used is indistinguishable whether we used square grid cells or hexagonal grid cells (cf. Fig. 3)"}, {"title": "Experiment 2: When the number of N\u0127 is varied (Figure 4):", "content": "Looking across the three datasets (MNIST, Fashion-MNIST, and CIFAR-100), that for wide variation in the number of hippocampal place cells, Nh, and grid cells Ng, the storage and recall performance is identical for both square and hexagonal grid cell tilings."}, {"title": "Experiment 3: On different path types and path sizes:", "content": "Across all datasets MNIST (Figure: 5), FashionMNIST (Figure: 6), and CIFAR100 (Figure: 7) the results reveal consistent patterns when"}, {"title": "Conclusions", "content": "In our experiments, we evaluated the performance of square and hexagonal grid cells within the Vector-HaSH model using various image datasets and path simulations. The results indicate that hexagonal grid cells display a similar performance to square grid cells across all our experiments, as measured by the mean cosine similarity and standard deviation in memory recall.\nThus, in the context of computational models for memory recall, the biological benefits of hexagonal tiling do not necessarily translate into a computational advantage. The similar performance between hexagonal and square grid cells indicates that both structures are equally capable of encoding and recalling spatial information, at least within the tested scenarios. The underlying neural mechanisms in the brain might leverage hexagonal grids not for enhanced computational performance but due to the natural formation tendencies and structural efficiencies of biological tissues. This finding suggests that the brain's preference for hexagonal grids may be more about the ease of biological implementation."}, {"title": "Future Directions", "content": "These results open new avenues for investigating the role of grid cell structures in the brain beyond memory recall. Further research could explore other cognitive tasks where the hexagonal arrangement might offer a computational advantage or provide insights into how biological constraints shape neural coding strategies.\nAnother promising direction is to extend the testing of hexagonal grids to 3D environments, such as those provided by AnimalAI. This extension would allow us to evaluate whether the advantages of hexagonal grids observed in 2D spaces also apply to more complex, three-dimensional settings. Testing in 3D environments would provide a more comprehensive understanding of how grid structures operate in realistic scenarios, potentially uncovering new insights into the spatial processing capabilities of hexagonal grids in naturalistic tasks."}, {"title": "A Supplementary Material", "content": null}, {"title": "Experiment Setup", "content": "Our variables consist of path type, path length, number of hippocampus cells (Nh), Number of images to store, grid module configurations, corruption level and we used a hyperparameter, Np = 5 (See Section A.4.1) which is the expected dimension of the projection from grid cells to place cells. We carried out several experiments with these variables on three different datasets to answer, How do square grids perform vs hexagonal grid cells:"}, {"title": "With varying amounts of stored images:", "content": "We stored images that would take up x \u2208 (1%, 3%, 10%, 20%, 33%, 50%, 75%, 90%, 100%, 110%, 150%, 200%, 300%, 1000%) of the theoretical memory capacity, where the theoretical memory capacity in number of images is:\n$\\text{memory\\_capacity} = \\frac{N_g * N_h}{\\text{Number of floats in image}}$\nfor Nh = 1000, Hexagon Grid Module Configuration = (3,5,6), Square Grid Module Configuration = (5, 9, 11) and medium corruption."}, {"title": "When the number of Nh is varied:", "content": "We set corruption = 'medium', path = 'levy walk', path length = 1000, number of images = 10, and compared recall performance of Nh \u2208 {100, 1000, 10000, 100000}. For each Nh, we averaged the cosine similarity scores for grid_module_configurations \u2208 [[2, 3, 5], [3, 5, 7], [5, 9, 11], [11, 13, 23]] and across k = 5 random generated levy paths. We tested these on MNIST, Fashion-MNIST and CIFAR-100"}, {"title": "On different path types and path sizes:", "content": "We set corruption = 'medium', Nh = 10000, number of images (N) = 10, and compared recall performance of path_length \u2208 {N,N2, N3, N4} and path_type \u2208 {straight path, brownian, l\u00e9vy}. For each path_type, path_type combination, we averaged the cosine similarity scores for grid_module_configurations \u2208 [[2, 3, 5], [3, 5, 7], [5, 9, 11], [11, 13, 23]] and across k = 5 random generated levy paths."}, {"title": "Supplementary Result Plots", "content": null}, {"title": "Methodology", "content": "In this section, we detail the implementation and evaluation of square and hexagonal grid cells within the Vector-HaSH model, describing the processes of memory storage and recall, and the benchmarks used to assess their performance."}, {"title": "Square Grid-Based Spatial Representation", "content": "Square grid cells serve as the foundational structure in our implementation of the Vector-HaSH model."}, {"title": "Grid Representation:", "content": "A single square grid is represented by two primary one-hot encoded vectors, gx and gy, corresponding to the x and y coordinates in a 2D square grid. Each vector has a length A defining the length of a side of the square grid."}, {"title": "Grid Vector Formation:", "content": "The overall grid vector g is formed by computing an outerproduct of gx and gy as in Figure 8:"}, {"title": "Cartesian to Grid Coordinate:", "content": "We can achieve the conversion from Cartesian to grid coordinate by simply applying the modulus operation to the x coordinate and the y coordinate with the lambda for that grid module:\n$GX_{activated\\_index} = x \\% X$\n$GY_{activated\\_index} = y \\% \u03bb$\nThis will return a number that serves as the index indicating which digit should be set to 1 in gx or gy to represent the Cartesian coordinate. Therefore, given a coordinate (x, y), we can map this onto a squared grid using our representation. Here, we define the complete matrix as a 'Grid Module' and each digit in the module as a 'Grid Cell'."}, {"title": "Modular Structure:", "content": "The structure of our complete grid is modular. That is, our complete grid consists of multiple grid modules of different sizes defined by an array of lambdas. Then, the total size for a grid module is:\nModule Size = $\u03bb^2$\nand the size of the full grid is:\nGrid Size = $\u03a3 \u03bb^2$ \n           all \u03bb\nThis modular approach aids in handling larger grids by dividing the grid into manageable parts, where each module is processed independently during operations such as denoising or updating. In addition, the combination of these grid modules allows for more spaces in the environment to be mapped by our grid, therefore allowing the representation of a larger range of (x, y) coordinates without overlap."}, {"title": "Grid Vector:", "content": "For the final grid vector, we flatten each grid module into a vector and concatenate the flattened modules into a single vector g."}, {"title": "Hexagonal Grid-Based Spatial Representation", "content": "Hexagonal grid cells are implemented as an alternative spatial representation aiming to leverage the natural efficiency of hexagonal tiling for spatial tasks. Unlike square grids, hexagonal grids are closer to the spatial encoding observed in biological systems. In our implementation, a single hexagon is used to represent the entire hexagonal grid."}, {"title": "Hexagonal Tiling and Grid Modules:", "content": "The hexagonal grid is composed of a collection of hexagonal grid modules, each defined by the size parameter R (or equivalently A as for the square grid modules), which represents the distance from the center of a hexagon to one of its corners. Each grid module is represented as a 'big' hexagon with smaller hexagons (grid cells) within it. 'Big' hexagons have R > 1 while small hexagons have R = 1.\nThese hexagons are arranged in layers, with each layer representing a ring of hexagons around a central origin. This tiling method ensures that the grid covers the space uniformly, providing a natural representation of spatial locations. The size of each hexagonal grid module is given by:\nModule Size = $3 \\sqrt{3} \u03bb^2$\nwhere X is the radius of the hexagon (figure 10, Proof: A.4)."}, {"title": "Assignment of IDs:", "content": "To identify which grid cell is activated, each hexagon cell in the grid is assigned a unique identifier (ID). The ID assignment begins at the central hexagon and proceeds outward in layers. Special care is taken to ensure that hexagons cells that are in locations mimicking a corner of an equilateral triangle share the same ID. as well as opposite hexagons, to maintain symmetry and consistency across the grid (See A.4) (Figure 11)."}, {"title": "Coordinate Transformation:", "content": "Given that the hexagonal grid does not align perfectly with Cartesian coordinates, a transformation is necessary to convert between the original"}, {"title": "Cartesian coordinates (x, y) and the hexagonal grid coordinates (x', y').", "content": "First we define certain properties of each hexagon (Figure: 12):\n$r=\\frac{\\sqrt{3}}{2}R,$\nNote that we assume the use of regular hexagons. Therefore, the gradient of the line h is $\\sqrt{3}$ such that :\n$\\text{gradient}_{h} = \\sqrt{3} \\frac{Y_2 - Y_1}{X_2 - X_1} = \\frac{r}{c},$\nRearranging, we get:\n$c = \\frac{1}{\\sqrt{3}} \\cdot r = \\frac{1}{\\sqrt{3}} \\frac{\\sqrt{3}}{2} R = \\frac{1}{2}R$\nConsider the small hexagon cells in Figure 13a, note that these are not aligned at the Cartesian origin. The hexagon grid modules don't start at (0, 0) on the Cartesian axis (Figure 13). As a result, we apply a transformation to the Cartesian coordinates (x', y') to map them onto the hexagonal grid to get (x, y):\n$x = x' + \\frac{R}{2}, y = y' + r.$\nwhere (x, y) are the transformed coordinates in the hexagonal grid's system."}, {"title": "To reverse the transformation and convert from the hexagonal grid's coordinates (x, y) back to the original coordinates (x', y'),", "content": "the following expressions are used:\n$x' = x - \\frac{R}{2}, y' = y - r \\frac{\\sqrt{3}}{2}.$\nNote that for hexagon grid modules, they are overlayed over the grid of smaller hexagon grid cells. Operations on this overlayed grid are the same with a couple differences:\n\u2022 The x and y coordinates are inverted as the grid modules are rotated 90\u00b0.\n\u2022 The grid modules have R > 1.\nThese transformations ensure that the coordinates used for grid operations maintain spatial consistency with the original Cartesian system (Figure 13)."}, {"title": "Cartesian to Hex Coordinates:", "content": "We first consider the Hexagon Grid modules and assume we are in the space described by Figure 13c, with the hexagons (small and big) centered at (0,0). Given a Cartesian coordinate (x, y) in that infinite space, we need to be able to represent this point using a single hexagon (finite space representing an infinite grid).\nTo accomplish this, we need to identify the midpoint of the hexagon grid module containing (x, y) in that subspace. The first step in this process is to apply a transformation to center the hexagons from (0,0) to ($, R \u2013 c) for simpler calculations. Also note that hexagon grid modules are rotated as compared to hexagon cells, so we interchange the x and y during transformation to get (x', y'). We then overlay another grid of rectangles over the grid of hexagons modules (Figure: 13a) and determine which rectangle (x', y') is in. Note that the height of the rectangle is 2R c, the width is 2r and the rectangular grid is centered with it's bottom left corner at (0,0) in the Cartesian System (Figure: 13c). Next, with the given (x', y') coordinates and the rectangle measurements, we determine which rectangle the point lies in.\nWith this information, the point can be in one of three hexagons (Figure: 14). We can determine the hexagon by calculating the midpoints of the hexagons and determining which hexagon's midpoint is closest to our coordinate (x', y'). From this we find our hexagon h*.\nThe next step is to find which hexagon cell in the hexagon grid module (x', y') lies in. Note that in this case, we again start at the same point as we did with hexagon grid modules, except with R = 1 and the cells are not rotated unlike the hexagon grid modules, so we don't interchange the x, y coordinates.\nUsing the initial (x, y) coordinate, we apply the same procedure (without interchanging the x, y coordinate) to obtain the location of this point in the small hex cell"}, {"title": "Memory Encoding & Storage", "content": "Memory storage in the Vector-HaSH model involves the following steps (Figure: 15a):\n1. Initialization of Weights: The memory model initializes several weight matrices that connect the grid cells (g), hippocampal place cells (p), and sensory inputs (s). These connections form two crucial components of the model: the scaffold and heteroassociation mechanisms. The weights include Wpg (grid to place cells), Wgp (place to grid cells). These two weight matrices make up the 'memory scaffold', which is how the model stores it's memories. The other weight matrices are: Wps (sensory to place cells), and Wsp (place to sensory cells). These matrices provide the hetero-association of the sensory input to the stored memory in the scaffold.\nWpg is initialized using a normal distribution, with hyperparameters c (Section:A.4.1) and var determining the mean and variance, respectively. After initialization, Wpg is fixed and does not change during the learning process, while the other weights are initialized to 0 and are updated as memories are stored.\n2. Memory Storage: First, given a coordinate, we determine the state of the grid vector g. With this, we can compute the input into the hippocampus layer, p, using the fixed matrix of random projections from grid cells to hippocampus cells Wpg:\n$p = ReLU(W_{pg}.g)  \\tag{1}$\nOnce we have both the grid vector g and hippocampus place cell activation p, we can begin constructing the scaffold by updating the weights:\n$W_{gp} \u2190 W_{gp} + \\frac{g.p^T}{||p||^2 + \u20ac} \\tag{2}$"}, {"title": "Note that we can use the sensory input s, along with the newly calculated p to update the weights that handle hetero-association of the sensory input to the location in the scaffold:", "content": "$W_{sp} \u2190 W_{sp} + \\frac{s.p^T}{||p||^2 + \u20ac} \\tag{3}$\n$W_{ps} W_{ps} + \\frac{p.s^T}{||s||^2 + \u20ac} \\tag{4}$\nThe update process follows a Hebbian-like learning rule, where the weight matrices Wgp, Wsp and Wps are incrementally updated using the following expressions:\nThese updates ensure that the weights are adjusted based on the correlation between the grid cells, place cells, and sensory inputs, with normalization to maintain stability in the learning process."}, {"title": "Memory Retrieval & Recall", "content": "Memory recall is the process of retrieving stored memories from noisy or incomplete sensory inputs (Figure: 15b):\n1. Noisy Input Processing: The noisy sensory input \u00a7 is processed through the trained model to, first, compute the noisy place cell activation p using the learned matrix Wps:\n$p = ReLU(W_{ps}.\\tilde{s}) \\tag{5}$\nThe noisy place cell activation is then used to infer the corresponding noisy grid state \u011f by applying the learned weights Wgp:\n$\\tilde{g} = W_{gp}.p \\tag{6}$\n2. Denoising and Modular Reconstruction: The inferred grid state is denoised using the modular grid structure through a \"winner-takes-all\" (WTA) mechanism, similar to processes observed in the human brain. This mechanism involves splitting the noisy grid into modules, selecting the neuron or cell with the maximum activation within each module, and suppressing the others. The result is a reconstructed clean grid state, g*, where only the most strongly activated cells remain active. This denoising process ensures that the grid state returns to a valid and stable representation within the grid space, effectively filtering out noise and enhancing the accuracy of the grid's spatial representation."}, {"title": "Sensory Input Reconstruction:", "content": "The denoised grid state is used to compute the place cell activation using the fixed random projection Wpg:\n$p^* = ReLU(W_{pg}g^*) \\tag{7}$\nwhich is then mapped back to the sensory layer using the learned weight matrix Wsp. The output is passed through a non-linear activation function (such as ReLU or tanh) to produce the final reconstructed sensory input.\n$s^* = np.tanh(W_{sp}.p^*) \\tag{8}$"}, {"title": "Proof: Size of a Hexagonal Grid Cell = 3r2", "content": "Consider a hexagonal grid centered at a point, with a radius r, which represents the maximum number of tiles away from the center. The goal is to determine the total number of grid cells, Ng, within this hexagon.\n1. Counting Interior Cells:\n\u2022 At distance r = 0, there is 1 cell (the red dot).\n\u2022 At distance r = 1, there are 6 cells.\n\u2022 At distance r = 2, there are 12 cells.\n\u2022 In general, for distance r, there are 6r cells.\nTherefore, the total number of interior cells is the sum of the cells at each distance, for now we will ignore the outer border:\n$N_{interior} = 1 + \\sum_{j=1}^{r-1} 6j$\n2. Simplifying the Sum:\nThe sum of the series$\\sum_{j=1}^{r-1} 6j$ is given by:\n$\\sum_{j=1}^{r-1} 6j = 6 \\times \\frac{(r-1)r}{2} = 3r(r - 1)$\nTherefore, the total number of interior cells becomes:\n$N_{interior} = 1 + 3r(r - 1)$\n3. Counting Border Cells:\nFor the border cells, In a regular hexagonal grid, each hexagon shares its corner vertices with three other hexagons. More visually, consider the point in figure 16:\nIf we map the entire hexagonal grid in a single hexagon, we can represent this point in three ways, see Figure 17:\nTherefore:\n\u2022 There are 6 corners in a hexagon.\n\u2022 Each corner is shared between three hexagons, meaning that each corner is counted multiple times when computing the border cells.\n\u2022 Additionally, there are non-corner cells along the border, which are counted only twice, as they are shared between two adjacent hexagons.\nThe total number of border cells, accounting for the redundancy in counting corner cells and non-corner cells, is given by:\n$N_{border} = 3(r-1)$"}, {"title": "Total Number of Cells:", "content": "Adding the number of interior cells and border cells, we have:\n$N_g = 1 + 3r(r \u2212 1) + 3(r \u2212 1)$\nAfter simplifying, we get:\n$N_g = 3r^2$\nThus, the total number of grid cells in a hexagonal grid with radius r is 3r2."}, {"title": "Parameter Test for Np", "content": "One key hyperparameter for the model is Np which represented the expected dimension of the hippocampal subspace in which grid cells are project through the random projection of equation 1. We can set Np by tuning the mean and variance of the random Gaussian using to set matrix Wpg. We find, (see figures below) that the smaller Np is, the better the memory. But we don't want Np to be too small either as otherwise we risk projecting to a zero dimensional subspace (i.e. the zero vector) in some unlucky circumstances. We thus decided to set Np = 5 to provide a balance between risk minimization and performance."}]}