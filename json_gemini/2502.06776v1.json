{"title": "TOWARDS INTERNET-SCALE TRAINING FOR AGENTS", "authors": ["Brandon Trabucco", "Gunnar Sigurdsson", "Robinson Piramuthu", "Ruslan Salakhutdinov"], "abstract": "The predominant approach for training web navigation agents gathers human demonstrations for a set of popular websites and hand-written tasks, but it is becoming clear that human data are an inefficient resource. We develop a pipeline to facilitate Internet-scale training for agents without laborious human annotations. In the first stage, an LLM generates tasks for 150k diverse websites. In the next stage, LLM agents complete tasks and produce trajectories. In the final stage, an LLM reviews the trajectories and judges their success. Language models are competitive with human annotators, detecting and filtering out harmful content with an accuracy of 97%, generating feasible tasks with an 89% rate, and judging successful trajectories with an 82.6% accuracy. Scaling the pipeline, agents based on Llama 3.1 70B solve 16.7% of tasks for 150k sites. Training on the data generated by our pipeline is competitive with training on human demonstrations. In data-limited settings derived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and +122.1% respectively for agents trained on mixtures of data from our pipeline, and human data. When training agents with all available human data from these benchmarks, agents fail to generalize to diverse real sites, and adding our data improves their generalization by +149.0% for WebLINX and +156.3% for Mind2Web. Code will be available at: data-for-agents.github.io.", "sections": [{"title": "1 INTRODUCTION", "content": "The predominant approach to training LLM-based web navigation agents is to collect human demonstrations on a set of manually curated websites and tasks (Deng et al., 2023; Zhou et al., 2024b; Putta et al., 2024; Koh et al., 2024a; Liu et al., 2024; L\u00f9 et al., 2024; Rawles et al., 2023). Human data can be laborious to collect and becomes costly to scale as the breadth of skills that users require from language model agents grows. There are more than three-hundred million sites on the Internet (The Common Crawl Foundation, 2024), and the range of sites that researchers can manually prepare for human annotation represents a tiny fraction of the Internet. The key problem is that human data can become unreliable at scale. Human-written web navigation tasks are highly effective for popular sites, but reliability drops for sites with lower popularity due to annotators' lack of familiarity. For these sites with lower popularity, which represent the majority of sites on the internet, human-written web navigation tasks are feasible only 40% of the time (discussed in Section 4), requiring a costly manual verification step. For the same pool of sites, language models improve feasibility rates to more than 80%. There is a growing need to automate data pipelines for the next generation of agents trained at internet scale. We address a key challenge by reducing the dependency on human data in the agent pipeline. We develop an automatic data pipeline that aims to facilitate Internet-Scale Training for Agents, which we shorten to InSTA, a pipeline that relies on synthetic web navigation tasks proposed, attempted, and evaluated by language models.\nOur method operates in three stages. In the first stage, we employ a language model to propose candidate web navigation tasks for an agent to perform across 150k live sites on the Internet. Current works are limited to 200 popular sites (L\u00f9 et al., 2024; Rawles et al., 2023; Deng et al., 2023) that human annotators are likely to be familiar with. Language models help us scale to 1,000 times more sites than current efforts, with better coverage of real-world sites. One major consideration when scaling up training for agents is safety: building safe agents requires that we avoid sites with harmful, unsafe, or dangerous content. We evaluated the ability of language models to detect such content and aggressively filtered out 85% candidates from the initial 1M sites to 150k sites that are judged safe by language models. These models succeed at detecting safe content with an accuracy of 97%, compared to 75% human accuracy. With tasks generated across a safe and diverse set of websites, we proceed to run language model agents to attempt the generated tasks.\nIn the second stage of the pipeline, a language model agent attempts to complete tasks using a web browser. We provide the entire Playwright API to the agent, which operates the browser by generating function calls in the Playwright API, a standard choice in recent web navigation benchmarks (Chezelles et al., 2024; Du et al., 2024; Drouin et al., 2024). While existing LLM agents can be directly applied to our tasks, measuring their success presents another challenge: sites organize data in bespoke formats. Previous efforts circumvent this by enlisting human labelers to review attempts manually and judge their success (L\u00f9 et al., 2024; Deng et al., 2023), or by designing sites for benchmarking where formats are consistent (Zhou et al., 2024b; Koh et al., 2024a). However, neither approach scales efficiently, and human annotators may not be reliable for sites they are not familiar with in the tail of the distribution. In the third stage of the pipeline, we scale the evaluation using language models. We employ LLMs to judge (Lightman et al., 2024) whether a task is solved by the final timestep and obtain an accuracy of up to 93.1% in detecting successful trajectories for the most confident predictions. Llama-3.1-70B-Instruct solves 16.7% of zero-shot tasks with a judge confidence of $conf = 1$. In data-limited settings derived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and +122.1%, respectively, when training agents on mixtures of data from our pipeline and human data. When training agents with all human data from these benchmarks, agents fail to generalize to diverse real sites, and adding our data improves their generalization by +149.0% for WebLINX and +156.3% for Mind2Web.\nOur work develops InSTA, an automated data pipeline that aims to facilitate Internet-scale training for agents. Driven by pretrained language models, we generate web navigation tasks for 150k live websites, run LLM agents, and judge their success. By removing humans from the agent pipeline, we can scale and extend coverage towards sites where human data is less reliable. We present an analysis to improve safety and show that our data can outperform human demonstrations for training agents. Code for the pipeline will be available at: data-for-agents.github.io."}, {"title": "2 RELATED WORKS", "content": "Language Model Agents. There is an emerging paradigm in modern NLP using language models (Radford et al., 2019; Brown et al., 2020; Touvron et al., 2023a;b) as the backbone for agents (Andreas, 2022). These models show impressive reasoning capabilities (Bubeck et al., 2023; Zhong et al., 2024; Valmeekam et al., 2024) that allow them to generalize to downstream applications, such as web navigation, where text formats differ significantly from their training data. Search algorithms provide a secondary axis to improve the reasoning capabilities of language model agents (Yao et al., 2023b; Besta et al., 2024; Koh et al., 2024b; Zhou et al., 2024a) by providing an explicit algorithmic scaffold and allowing test-time compute to improve reasoning steps (Snell et al., 2024; Zhong et al., 2024). Although most of the work focuses on running language models as zero-shot agents, fine-tuning language models to improve their effectiveness as agents is becoming popular (Putta et al., 2024; Zeng et al., 2023; Zhang et al., 2023; Hong et al., 2023; Xie et al., 2024; Wang et al., 2024) as target benchmarks are becoming more difficult for zero-shot language models.\nAgent Pipelines. There are a growing number of agent pipelines aimed at fine-tuning language models to improve their effectiveness as agents (Mitra et al., 2024; Zeng et al., 2023; Putta et al., 2024; Chen et al., 2023; Ou et al., 2024). However, driven by the limited data available, many such works train on data with significant overlap with their test environment, either with different tasks for the same environment configuration as the test setting (Deng et al., 2023), or the same tasks (Putta et al., 2024). We instead consider a setting where tasks and environment configurations are entirely separate between training and testing, creating a strong train-test split that follows recommended practice. This presents a challenge: the web navigation data for training LLM agents is limited (Deng et al., 2023; L\u00f9 et al., 2024). We address this challenge by increasing the scale of data collection and better coverage of the distribution of real-world sites. We train on diverse tasks generated by our pipeline and successfully transfer agents trained on our data to downstream benchmarks while maintaining a strong train-test split. Our training procedure resembles a modified FireAct (Chen et al., 2023), where language models jointly propose and evaluate tasks for agents.\nAgent Datasets. The majority of datasets for training web navigation agents rely on human annotators to create tasks (Zhou et al., 2024b; Koh et al., 2024a; Rawles et al., 2023), and provide demonstrations (Deng et al., 2023; L\u00f9 et al., 2024; Rawles et al., 2023; Shen et al., 2024). This approach has limits, as the breadth and diversity of tasks researchers can manually curate are dwarfed by the volume of sites on the Internet. There are more than three-hundred million sites on the Internet according to The Common Crawl Foundation (2024), and existing datasets are limited to about 150 popular sites that human annotators are already familiar with (Deng et al., 2023; L\u00f9 et al., 2024; Shen et al., 2024). There are hypothetical 1,000,000 times more data that could be available if we can efficiently harness this previously untapped resource. However, most sites are relatively obscure and human annotators are unreliable for sites they are not already familiar with. Finding suitable annotators becomes impractical at this massive scale, so we adapt language models to propose, attempt, and evaluate web navigation tasks. Although we are not the first to consider synthetic data for training agents (Gandhi et al., 2024; Ou et al., 2024; Setlur et al., 2024; Tajwar et al., 2024), we have developed a key approach to harness internet-scale data efficiently.\nLanguage Model Judges. Core to our pipeline is a language model evaluator. Using language models to judge the correctness of responses is becoming popular to improve accuracy for LLMs (Li et al., 2024), and applications include verifying reasoning steps (Zhang et al., 2024), rejection sampling (Snell et al., 2024; Sun et al., 2024), prioritizing frontier nodes in search algorithms (Zhou et al., 2024a; Koh et al., 2024b), filtering out harmful responses (Inan et al., 2023), providing feedback for response improvement (Madaan et al., 2023; Paul et al., 2024; Patel et al., 2024; Yuksekgonul et al., 2024), and providing ratings for alignment (Lee et al., 2024; Ouyang et al., 2024). Our use of language models to evaluate agent tasks is inspired by the generative verifier in Zhang et al. (2024), and modified from the multimodal verifier in He et al. (2024), where our language model predicts a confidence score that a task is solved, which is used to identify successful attempts."}, {"title": "3 LANGUAGE MODEL AGENTS", "content": "Language model agents are a class of decision-making agents represented by $\\pi_{LLM}(a_t|s_t, c)$, a policy that processes multimodal observations $s_t$ (from a web environment in our case) and predicts textual actions $a_t$ to complete a task $c$. Underneath this abstraction, a large language model (LLM) generates actions via the next-token prediction, conditioned on a system prompt $x_{sys}$.\n$a_t = f_{text\\rightarrow act}(LLM([x_{sys}, c, Enc(s_t)]))$\nEnvironment representations for observations and actions typically differ from the expected input format of the language model (typically images and text), and functions are introduced that map the observations to a multimodal prompt $Enc(\\cdot)$, and parse actions from the language model generated response $f_{text\\rightarrow act}(\\cdot)$. For web navigation, the environment state $s_t$ is HTML DOM, and is often formatted as raw HTML code, an Accessibility Tree, Set-of-marks, or screenshots (Zhou et al., 2024b; Koh et al., 2024a; Chezelles et al., 2024; Shen et al., 2024). In this work, we employ an off-the-shelf HTML-to-Markdown parser to convert observations into a textual format. Action formats vary between works, and we build on Schick et al. (2023)'s function-calling framework, where a language model generates code that is parsed into a function name and corresponding arguments. Given a set of strings $L$, and a set of function argument values $G$, the set of actions $A$ is:\n$A = (L_{func} \\times (L_{arg1} \\times G_{arg1}) \\times (L_{arg2} \\times G_{arg2}) \\times \\cdot)$      where $L_{func}$ is the set of function names in our API, and function arguments have a name and value $(L_{arg1} \\times G_{arg1})$. We provide agent access to the entire Playwright API (Microsoft, 2024), a Microsoft-developed browser automation library that wraps around a Chromium web browser. The agent's goal is to complete a web navigation task specified via a natural language instruction $c \\in L$, starting from an initial URL, and operating the browser via function calls to the Playwright API until the task is complete, after which point the agent calls $stop$ with an optional answer:\n$a_{stop} = ($``stop'', ($``answer'', $``I am done''$))$      We prompt the language model backbone to generate actions wrapped in a JSON code block for straightforward parsing. The action parser $f_{text\\rightarrow act}$ consists of a regex template that matches the first JSON code block, such as the example in Figure 1, followed by JSON decoding of the string contents within the code block. When parsing fails due to invalid syntax, we generate a new response until parsing succeeds. Equipped with a language model agent that makes calls to the Playwright API, we face a crucial roadblock that impedes scaling: obtaining large and diverse data."}, {"title": "4 INTERNET-SCALE TASK GENERATION", "content": "Building internet-scale agents requires a diverse scaffold of tasks and environment configurations beyond what can be attained via manually curated examples annotated by humans. We develop a pipeline to efficiently harness vast quantities of sites on the Internet that aims to facilitate Internet-Scale Training for Agents, shortened to InSTA. Our pipeline uses pretrained language models to generate, attempt, and evaluate synthetic web navigation tasks for a more diverse pool of sites than current efforts that rely on tasks designed by researchers (Deng et al., 2023; Zhou et al., 2024b; Putta et al., 2024; Koh et al., 2024a; Liu et al., 2024; L\u00f9 et al., 2024; Rawles et al., 2023; He et al., 2024). Human data is a valuable resource, but can be inefficient to gather. Our work shows that language models can be as accurate as human annotators. By removing human data from the agent pipeline, we can improve the safety and reliability of tasks, and efficiently scale task generation to 1M sites."}, {"title": "4.1 LANGUAGE MODEL TASK PROPOSER", "content": "In the first stage, we scale the generation of web navigation tasks to 1M diverse websites using a Language Model Task Proposer. Shown in Figure 2, the task proposer serves two key functions in the pipeline: (1) filtering out harmful content and websites that cannot be safely annotated, and (2) proposing realistic web navigation tasks that a hypothetical user might want to accomplish.\nModel Details. We utilize pretrained and frozen language models that conform to a chat interface and accept a system prompt $x_{sys}$, and a series of in-context examples through interleaved user and assistant prompts $x_{usr}$ and $x_{ast}$. The system prompt used for task generation is shown in Figure 2, and outlines all cases for which sites are considered unsafe for annotation. We consider the Llama 3.1 family of LLMs from Meta (Grattafiori et al., 2024; Touvron et al., 2023b;a), the GPT family of LLMs from OpenAI, and the Gemini family of LLMs from Google. Inference is served using VLLM (Kwon et al., 2023) for the Llama series of models. We employ a sampling temperature of 0.5, and a maximum budget of 64 newly generated tokens; all other parameters are kept as defaults in the OpenAI chat completions API, which is used to make inference calls to all LLMs.\nPrompt Details. The goal of the task proposer is to accurately detect unsafe websites and generate realistic web navigation tasks when appropriate. We prompt the task proposer with the system prompt in Figure 2, a series of in-context examples (listed in Appendix E), and a final user prompt containing just the URL of the target website. We instruct the LLM via the system prompt to provide a task for the target website or to return \"N/A\" and mark the website as not suitable for annotation. This format produces a throughput of 20 websites per second for Llama 3.1 70B served on 16 GPUs with vLLM, processing 1M sites in 14 hours. The efficiency of stage one aids in scaling to large numbers of sites on the Internet, but we must not compromise safety and reliability for efficiency. To understand the trade-offs presented by our task proposal approach, we compare against typical human annotators at detecting safe websites for annotation and creating realistic agent tasks."}, {"title": "4.2 IMPROVING SAFETY", "content": "Language models beat single pass non-expert annotators at detecting websites suitable for annotation. To evaluate detection performance, we employ the task proposer as a classifier and consider sites where the task proposer returns \u201cN/A\u201d as the positive class. We curate 50 safe and 50 unsafe domains, based on the filtering conditions outlined in the system prompt in Figure 2 (selected websites and their URLs are listed in Appendix E). We generate task proposals for each site and measure the accuracy, precision, and recall of our safety filter compared to human annotators. The annotators are asked to classify each site as suitable or unsuitable for annotation based on the website URL, and the criteria listed in the system prompt the same observations given to the task proposer to ensure a fair comparison. Results are presented in Table 3.\nUnderstanding The Results. Language models outperform human annotators by 29.3% in accuracy, 35.2% in precision, and 31.0% in recall at detecting harmful sites. While larger models like Gemini 1.5 Pro show the best overall accuracy, smaller models like Llama 3.1 70B display high recall with a minor decrease in accuracy. Recall matters most for safety filters, and these results suggest that Llama 3.1 70B is sufficient to detect most harmful sites with high confidence."}, {"title": "4.3 IMPROVING RELIABILITY", "content": "Language models are more reliable than human annotators at creating realistic web navigation tasks. To evaluate reliability, we measure the rate at which human workers are able to perform web navigation tasks generated by our pipeline. We curate a set of 100 safe website domains (different from the safety experiment, refer to Appendix E), generate task proposals using our pipeline, and measure the rate of self-reported task completion for human workers performing tasks. Workers start from the initial website URL in their browser and navigate pages using their mouse and keyboard while staying on the original site, reporting once the task is complete or once they believe the task is not feasible on the current site. We compare the feasibility rates for tasks generated by our pipeline and tasks written by human annotators given the criteria listed in Figure 2. Results are shown in Table 4.\nUnderstanding The Results. Language models outperform human annotators by 64.8% at creating feasible web navigation tasks. Larger models like Gemini 1.5 Pro display the best feasibility rates, but the smaller model Llama 3.1 70B still outperforms the human annotators by 38.9%. To understand the relationship between the popularity of the site being annotated and the reliability of human-written tasks, we conduct an experiment in Figure 5 comparing PageRank values (Page et al., 1999) of sites according to the official June 2024 host-level web graph from The Common Crawl Foundation (2024), versus the feasibility rates of the proposed tasks from Table 4.\nAlthough human annotators match the reliability of LLMs at creating feasible web navigation tasks for popular sites, LLMs outperform human annotators by 157.1% for less popular sites with low PageRank values. As obscurity increases, human annotators are less familiar with the sites, and the reliability of their task proposals decreases by 55.7%, whereas the reliability of tasks generated by LLMs remains relatively constant. This difference suggests that we should employ language models to ensure that task proposals are reliable as we begin to scale agents to vast numbers of sites on the Internet. However, where do we acquire this large and diverse set of websites to process for annotation?"}, {"title": "4.4 SCALING TO 150,000 SITES", "content": "We propose to leverage open-source crawls of the internet for large-scale task generation. As of June, 2024, the web graph released by The Common Crawl Foundation (2024) contains more than 300 million unique hosts, which we adapt to a data source for agents. In particular, we sort hosts by their PageRank values, and select the top 1M sites for task generation. CommonCrawl is likely to contain many sites not suitable for annotation, and the experiments in Section 4.2 illustrate that the safety filter in the task proposer can detect and remove them effectively. In our configuration, task generation with Llama 3.1 70B takes 14 hours for 1M sites served with vLLM (Kwon et al., 2023) on two 8-GPU nodes. Sections 4.2 and 4.3 show Llama 3.1 70B outperforms human annotators in safety and reliability, and we can serve it locally at significantly reduced cost compared to proprietary LLMs with a marginal loss in quality. The distribution of tasks generated with Llama 3.1 70B for the top 1M sites in the CommonCrawl PageRank are visualized in Figure 6.\nUnderstanding The Data. The task proposer filters out 85% of the sites in CommonCrawl, resulting in 150k sites that can be safely assigned tasks for agents. Visualized in Figure 6, our distribution is denser than human-written tasks, has a wide coverage of real-world sites, and diverse categories of tasks. We automatically label task categories (procedure in Appendix E) and find that 89% of categories have fewer than the mean of 16.9 tasks per category. Top categories include news search, recipe search, product lookup, tutorial search, event schedules, health information, and many more. Refer to Appendix E for the top categories. Empowered by this large and diverse collection of tasks across the internet, we can start building internet-scale agents."}, {"title": "5 INTERNET-SCALE AGENTS", "content": "In the next stage, we begin to scale LLM agents to diverse web navigation tasks across the web. Shown in Figure 7, we initialize a web browsing environment to the URL provided to the task proposer in Section 4, and run a language model agent to complete tasks by generating function calls in the Playwright API. For evaluation, current efforts typically use human-written constraints based on the final URL or page state (Zhou et al., 2024b; Koh et al., 2024a; Yao et al., 2023a; Drouin et al., 2024), but it can be difficult to scale these. Recall from Figure 5 that human annotators are less reliable for sites lower in the PageRank, where their familiarity is reduced. Results in section 4 showed that language models beat humans in safety and reliability for task generation. As we begin to scale agents to diverse websites from the internet, can we replace human-written criteria with language model judgments for efficient evaluation? Their robustness remains an important unresolved question, and current work only considers language model judges for a limited set of popular websites (He et al., 2024). We begin by validating the robustness of language models for evaluating diverse internet tasks."}, {"title": "5.1 EVALUATION WITH LANGUAGE MODELS", "content": "We model the process of evaluating trajectories from agents as a classification problem, where the goal is to estimate the probability $r_T$ that a task $c$ is solved by the final timestep. Conditioned on a system prompt $y_{sys}$, a task $c$, the history of previous actions $a_1, \\dots, a_T$ and the final observation from a web browsing environment $s_T$ formatted in text, we generate a completion using a language model and employ a function $f_{text\\rightarrow val}(\\cdot)$ to parse the estimated probability.\n$r_T = f_{text\\rightarrow val}(LLM([ y_{sys}, c, a_1, \\dots, a_T, Enc(s_T) ]) )$      Building on the sites used to measure reliability in Section 4.3, we conduct an experiment to measure the accuracy of language models to detect successful web navigation trajectories. We run agents on tasks generated by Llama 3.1 70B for the 100 sites in Section 4.3, and prompt language models to estimate the probability that tasks are solved $r_T$ based on Equation 4. We then conduct human evaluations for the trajectories and manually assign binary success labels. Accuracy is calculated by applying a threshold to the predictions $r_T > 0.5$ to assign classes and tracking the rate at which the predictions agree with human labels. To understand robustness for sites of varying popularity, we report the accuracy of language models versus the PageRank of the corresponding sites. Similarly, to understand the ability of language models to judge their own uncertainty, we report their accuracy versus their prediction confidence, given by $conf = 2|r_T - 1/2|$ (twice the total variation distance from the uniform distribution to the predicted distribution). Results are shown in Figure 8.\nUnderstanding The Results. Language models are robust evaluators for web navigation tasks. Accuracy remains stable relative to PageRank values, suggesting that language models are effective for sites that typical human annotators are less familiar with. The best results are obtained with an evaluator based on GPT-4o, which attains an accuracy of 82.6%, compared to 81.7% for Llama 3.1 70B, and 78.0% for Gemini 1.5 Pro. Although the accuracy is robust to PageRank, the accuracy is highly informed by confidence. Language models show improved accuracy as their confidence improves, suggesting that they can effectively determine when their predictions are reliable. When considering predictions with $conf = 1$, the Llama 3.1 70B evaluator displays a compelling 93.1% accuracy, 0.87 precision, and 0.82 recall for detecting successful trajectories. Now that we can efficiently and accurately judge trajectories, we can begin to scale language model agents to diverse internet tasks and track their success. Harnessing this judge, we can study the current abilities and shortcomings of language model agents spanning 150k websites."}, {"title": "5.2 SCALING TO 150,000 AGENTS", "content": "We scale language model agents to 150k live sites in diverse domains across the Internet and attempt to complete 150k web navigation tasks generated by our pipeline. Shown in Figure 9, we evaluate the trajectories using a Llama 3.1 70B judge and run agents based on Llama 3.1 70B, selected because this model demonstrates high accuracy in Figure 8, and running currently available proprietary models would be prohibitively expensive at this scale (see Appendix K for a cost analysis with different LLMs). We find that agents solve 16.7% of tasks with a model confidence of $conf = 1$. Furthermore, we observe that 35k tasks are judged to be on the right track with a confidence of $conf = 1$, suggesting that these could be solved if a larger computing budget was allocated. The spread along the x-axis in both plots in Figure 9 suggests that our tasks cover a broad range of difficulties, and working to solve them presents an opportunity to improve the capabilities of LLM agents. We observe that, when judging success, our evaluator tends to prefer binary predictions with high confidence values, suggesting that this subset of predictions is accurate based on Figure 8 results. Analyses for the agents that produced Figure 9 are presented in Appendix F.\nAssembling the stages of the InSTA pipeline, we first generated tasks for 1M diverse sites across the Internet, we then dispatched language model agents to complete tasks for 150k sites marked as safe, and finally we evaluated the trajectories using a language model judge. Equipped with this large and diverse source of data, we seek to understand the utility of the data for training agents."}, {"title": "6 TRAINING AGENTS", "content": "We compare the performance of agents trained on data from the InSTA pipeline to agents trained on human demonstrations from WebLINX (L\u00f9 et al., 2024) and Mind2Web (Deng et al., 2023), two recent and popular benchmarks for web navigation. Recent works that mix synthetic data with real data control the real data sampling probability in the batch $P_{real}$ independently from data size (Trabucco et al., 2024). We employ $P_{real} = 0.5$ in few-shot experiments and $preal = 0.8$ otherwise. Shown in Figure 9, our data have a wide spread in performance, so we apply several filtering rules to select high-quality training data. First, we require the evaluator to return $conf = 1$ that the task was successfully completed, and that the agent was on the right track (this selects data where the actions are reliable, and directly caused the task to be solved). Second, we filter data where the trajectory contains at least three actions. Third, we remove data where the agent encountered any type of server error, was presented with a captcha, or was blocked at any point. These steps produce 7, 463 high-quality demonstrations in which agents successfully completed tasks on diverse websites. We sample 500 demonstrations uniformly at random from this pool to create a diverse test set, and employ the remaining 6, 963 demonstrations to train agents on a mix of real and synthetic data."}, {"title": "6.1 IMPROVING DATA-EFFICIENCY", "content": "In a data-limited setting derived from WebLINX (L\u00f9 et al., 2024) and Mind2Web (Deng et al., 2023), agents trained on our data scale faster with increasing data size than human data alone. Without requiring laborious human annotations, the data produced by our pipeline leads to improvements on Mind2Web that range from +89.5% in Step Accuracy (the rate at which the correct element is selected and the correct action is performed on that element) with 32 human actions, to +77.5% with 64 human actions, +13.8% with 128 human actions, and +12.1% with 256 human actions. For WebLINX, our data improves by +122.1% with 32 human actions, and +24.6% with 64 human actions, and +6.2% for 128 human actions. Adding our data is comparable in performance gained to doubling the amount of human data from 32 to 64 actions. Performance on the original test sets for Mind2Web and WebLINX appears to saturate as the amount of human data increases, but these benchmark only test agent capabilities for a limited set of 150 popular sites."}, {"title": "6.2 IMPROVING GENERALIZATION", "content": "To understand how agents trained on data from our pipeline generalize to diverse real-world sites, we construct a more diverse test set than Mind2Web and WebLINX using 500 held-out demonstrations produced by our pipeline. Shown in Figure 11, we train agents using all human data in the training sets for WebLINX and Mind2Web, and compare the performance with agents trained on 80% human data, and 20% data from our pipeline. Agents trained with our data achieve comparable performance to agents trained purely on human data on the official test sets for the WebLINX and Mind2Web benchmarks, suggesting that when enough human data are available, synthetic data may not be necessary. However, when evaluated in a more diverse test set that includes 500 sites not considered by existing benchmarks, agents trained purely on existing human data struggle to generalize. Training with our data improves generalization to these sites by +149.0% for WebLINX agents, and +156.3% for Mind2Web agents, with the largest gains in generalization Step Accuracy appearing for harder tasks."}, {"title": "7 CONCLUSION", "content": "We present a pipeline that unlocks new scaling potential for language model agents by generating internet-scale web navigation data for diverse sites. We employ pretrained language models to generate, attempt, and evaluate web navigation tasks at scales beyond what humans can manually annotate. Our results show that language models outperform human annotators at detecting safe websites, and proposing feasible web navigation tasks. In addition, we find that language models can judge successful web navigation trajectories with an accuracy up to 93.1% for their most confident predictions. We scale task generation to the top 1M sites in the CommonCrawl PageRank, and test agents on 150k live web navigation tasks. Agents based on Llama 3.1 70B solve 16.7% of these tasks zero-shot. In data-limited settings derived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and +122.1% respectively for agents trained on mixtures of data from our pipeline, and human data. When training agents with all available human data from these benchmarks, agents fail to generalize to diverse real sites, and adding our data improves their generalization by +149.0% for WebLINX and +156.3% for Mind2Web.\nOur work reveals several exciting directions for future work. First, our work can be scaled further. The latest CommonCrawl release contains data for more than 300 million sites, suggesting another 1,000 times more data than we explored could be available for training agents by scaling"}]}