{"title": "EventChat: Implementation and user-centric evaluation of a large language model-driven conversational recommender system for exploring leisure events in an SME context.", "authors": ["Hannes Kunstmann", "Joseph Ollier", "Joel Persson", "Florian von Wangenheim"], "abstract": "Large language models (LLMs) present an enormous evolution in the strategic potential of conversational recommender systems (CRS). Yet to date, research has predominantly focused upon technical frameworks to implement LLM-driven CRS, rather than end-user evaluations or strategic implications for firms, particularly from the perspective of a small to medium enterprises (SME) that makeup the bedrock of the global economy. In the current paper, we detail the design of an LLM-driven CRS in an SME setting, and its subsequent performance in the field using both objective system metrics and subjective user evaluations. While doing so, we additionally outline a short-form revised ResQue model for evaluating LLM-driven CRS, enabling replicability in a rapidly evolving field. Our results reveal good system performance from a user experience perspective (85.5% recommendation accuracy) but underscore latency, cost, and quality issues challenging business viability. Notably, with a median cost of $0.04 per interaction and a latency of 5.7s, cost-effectiveness and response time emerge as crucial areas for achieving a more user-friendly and economically viable LLM-driven CRS for SME settings. One major driver of these costs is the use of an advanced LLM as a ranker within the retrieval-augmented generation (RAG) technique. Our results additionally indicate that relying solely on approaches such as Prompt-based learning with ChatGPT as the underlying LLM makes it challenging to achieve satisfying quality in a production environment. Strategic considerations for SMEs deploying an LLM-driven CRS are outlined, particularly considering trade-offs in the current technical landscape.", "sections": [{"title": "Introduction", "content": "The ability of recommender systems (RS) to simplify user decision-making and reduce information overload has been well-acknowledged in both information systems [19, 79, 82] and practitioner circles alike [8, 66]. Recommendations can account for as much as 30% of firm revenue, with a 1% improvement in recommendation quality translating into billions of dollars [75]. A key limitation of traditional RS, however, is a lack of user control over recommendations [3, 12, 42, 48, 49], with users constrained to reactively make choices among recommendations pre-integrated into the system's logic [1] rather than proactively describing their desired choice set [41]. Conversational recommender systems (CRS) overcome this limitation by allowing for user input and feedback (e.g., \u201cThis product is too expensive\") via a conversational interface (i.e., a chatbot) supplemented by machine learning (ML) techniques [14] to refine recommendations and empower users in their search [39, 41].\nThe emergence and integration of large language models (LLMs) capable of understanding and generating natural language (and other content) to perform a wide range of tasks [35] therefore represents a monumental shift in the strategic potential of CRS [61]. Beyond LLMs capabilities in natural language processing and user engagement [5], their versatility makes them well-suited for architectural tasks such as RS serving [21] or reranking within the retrieval-augmented generation (RAG) technique [25]. Additionally, LLMs can be seamlessly integrated into CRS architecture without requiring extensive data or other costly resources for model training, with even the most advanced LLMs (e.g., ChatGPT) accessible by simply calling application programming interfaces (APIs). LLM-driven CRS therefore represent a high-potential tool to create efficacious CRS systems, as recognized by industry leaders [34, 74]. However, for small-to-medium size enterprises (SMEs) that often face resource constraints or fine-margins, the path to implementing LLM-driven CRS remains unclear.\nTo date, several frameworks for building LLM-driven CRS have been proposed that address practical challenges such as effective conversation management or the extraction of information from external sources [22, 25, 28, 33]. Nonetheless, there remains a lack of research on the technical considerations faced when implementing LLM-driven CRS for a specific business context, and an evaluation of how strategically useful these implementations may be for SMEs. Business-critical factors ambiguous in existing frameworks [22, 25, 28, 33] include development costs, operational expenses, as well as performance and latency implications of adopting an agent-like vs. stage-like architecture and contextualizing LLMs to users inputs. Similarly, uncertainty regarding system performance in production environments from an end user perspective raises questions about the practicality and effectiveness of these systems [92]. Lastly, for resource-constrained SMEs, the value of adding additional features (i.e., augmenting the UI with buttons, anthropomorphic features etc.) as recommended in extant research [31, 43, 73, 91] remains unclear.\nIn this paper, as our core contribution, we outline the design of a ChatGPT-driven CRS, describing practical design choices, their strengths and limitations, and strategic considerations for SMEs. To verify\""}, {"title": "Related Work", "content": ""}, {"title": "Strategic Potential of LLM-driven CRS for SMEs", "content": "The strategic importance of RS, CRS, chatbots, and the recommendations they provide has been well- established [57, 63]. RS technologies can enhance consumer product search (i.e., greater exploration of product options), upsell (i.e., encourage purchases of higher monetary value), and increase sales volume [55, 80]. Through their ability to relay information in a dyadic, low-effort manner conversational RS (e.g., CRS, chatbots) can also facilitate user engagement, loyalty, and customer retention via the preference elicitation process [19, 40] that mimics the value-adding interpersonal dynamics of human- based service interactions [4, 16].\nLLM technology therefore represents a paradigm-shift the strategic potential of CRS [61]. By improving both conversational dynamics [97] and recommendations [71], LLMs can increase productivity between 30-45% whilst enhancing the customer experience [67]. Considering that for SMEs, fine-margins and relational outcomes dominate strategical positioning [96] and that humanized and personalized feedback are features of chatbots previously found to most readily distinguish an SMEs service offering compared to rivals [88, 90], LLM-driven CRS therefore represent a high-potential technology for untapping business value for SMEs."}, {"title": "Frameworks for LLM-driven CRS", "content": "While the opportunity presented by LLM-driven CRS for SMEs is clear, the scaling up of data and parameters has made implementation less so. New capabilities of LLMs such as in-context learning (ICL) [10, 60, 86], adherence to instructions [95], and planning and reasoning [100, 102, 103] have the potential to create state-of-the-art CRS. Nonetheless, the implications of these developments for user queries and seamless communication remain. For SMEs, this is a notable barrier due to the relative weight that each customer interaction has on the overall business performance, and the reality that SMEs are less able to absorb the negative effects of occasional poor customer service or change to alternative system designs [27] compared to larger firms. To overcome these challenges, three prominent frameworks using LLMs in CRS have been introduced [22, 25, 33]. To date, however, it remains unclear how the relevant aspects of each framework can be utilized for effective context-specific use by an SME."}, {"title": "Evaluation of LLM-applications", "content": "Besides the technical choices faced when implementing an LLM-driven CRS, of equal importance are customer ratings and its economic viability. While extant research has examined LLMs across diverse application areas, including finance [104], education [51], chemical engineering [56], as well as domain- independent frameworks [44, 94], these works have concentrated on evaluating performance against state-of-the-art techniques or comparative evaluation against benchmark techniques and/or human annotators [64], rather than real user feedback and business-critical factors such as system costs and latency.\nIn traditional RS research, the ResQue framework has been widely applied to evaluate users' subjective ratings of the RS [83]. The ResQue model states that user-perceived quality factors (e.g., recommendation accuracy) influence users' beliefs about the system (e.g., perceived usefulness), shaping user attitudes (e.g., satisfaction) and leading to behavioral intentions (e.g., intention to use the system) [83]. A strength of the framework has been its ability to unify evaluations of user experience across different application areas such as e-commerce [83], music [46], travel [7], and movies [81]. Arguably of greater importance, evaluations captured using ResQue aim to capture the holistic user experience of an RS within its application context, rather than simply measuring the performance of the RS algorithm alone [84].\nThe applicability of ResQue to LLM-driven CRS has been questioned recently on two counts however: First, as RS have evolved into interactive, dialogue-based systems powered by AI [38] the original ResQue model is unlikely to fully reflect the current user experience. For example, factors relating to Interaction Quality (e.g., Consistency, Coherence, and Input Processing Performance) have potentially far greater relevance for LLM-driven CRS than for CRS with script-based conversational flows. Second, anchoring subjective user evaluations (i.e., survey responses) in real system events (i.e., objective user- interaction data) can resolve ambiguity about why certain constructs scored poorly [38]."}, {"title": "Business Context", "content": "To investigate the system design and user evaluation of LLM-driven CRS from an SME perspective, we partnered with a startup in the leisure industry. We collaboratively implemented EventChat, a LLM- driven CRS introduced in the latest update of the startup's smartphone app. The system facilitates user exploration of events and activities in a major German city and complements other features in the app, such as a filter-based search, an interactive map, and a calendar overview. The development of EventChat was motivated by two main business objectives: (i) to improve the user experience through enhanced event discovery, and (ii) to mitigate shortcomings with the firm's web scraping strategy, which often resulted in event listings with missing information, by employing a LLM-driven CRS that can parse and interpret this missing information."}, {"title": "System Design", "content": ""}, {"title": "Business-driven Design Choices", "content": "Several design choices for the LLM-driven CRS were made to balance strategic aims from the SME's perspective, with feasibility and technical optimization. First, ChatGPT was used as the underlying LLM of the CRS, as it utilizes many of the novel LLM capabilities including in-context learning [10, 60, 86], adherence to instructions [95], as well as planning and reasoning [100, 102, 103].\nSecond, prompt-based learning was used following previous research using GPT4 [33]. This choice was driven by implementation feasibility for an SME relative to alternative approaches such as fine-tuning. Prompt-based learning does not require large amounts of training data from past user interactions or generated synthetic data, both of which are costly for SMEs to obtain in terms of time, finances, or engineering efforts. Additionally, frameworks necessitating fine-tuning rely on base models with significantly fewer parameters than ChatGPT [22, 25].\nThird, calling an LLM API can be highly costly, and so we implemented an attribute-based question- answering CRS [52, 111, 112], where the system and users engage in question-answering about desired item attributes in as few rounds as possible. Notably, this approach went against the dominant approach in extant chatbot research that emphasizes the value of increased anthropomorphization and \u2018chit chat\u2019 [77]."}, {"title": "Architecture", "content": "Figure 1 provides an overview of EventChat's architecture, which was implemented as a turn-based dialog system. The back end calls endpoints of external resources like the startup's relational database, vector database, recommendation engine, or internet-based information sources."}, {"title": "Front End", "content": "The front end was designed using the Flutter framework [24]. To address the time dependency in event discovery, we configured EventChat to consider events occurring within the next 150 days by default. Nevertheless, users could define a custom time interval using a static button located in the top left corner of the interface. To enhance relevance, we included this user-specified period as a query parameter to EventChat.\nThe system implementation also gave users the choice to search for specific events or receive general recommendations based on their past preferences. In each session, users were prompted to make this selection by clicking buttons within the chat interface. Such a hybrid approach of including buttons in a chat interface has been found to increase user-perceived control [36, 73]. However, our main rationale behind including this feature was to optimize the usage of resources and economize on costs, as it helped us reduce iterations and calls to the LLM API. As previous research has shown that users expect information about items to be presented through a dedicated recommendation slate, making it easy to find more details [25, 43], we also used a visibility detector in the front end that tracks the last three event card summaries shown to the user. This information was then sent to the back end to give the LLM the needed context."}, {"title": "Back End", "content": ""}, {"title": "Overview", "content": "Each conversation with EventChat was divided into turns. Turns are initiated by the user taking an action in the front end (i.e., in the chat interface), with EventChat responding and, depending on the context, providing recommendation slates (cf. Figure 3). This stage-based approach was inspired by Feng et al. [22, 25]. Following other works [25, 33], we also tried an agent-based approach to enhance the flexibility and sophisitication of the system (e.g., whole-day trip planning). However, we observed that can lead to: (i) excessive LLM calls, yielding high costs and latency (as highlighted in previous research [101]), and (ii) ChatGPT-specific performance issues (as discussed in practitioner circles [29]). Given the importance for SMEs of cost minization, system stability, and low latency, we chose the low design complexity offered by the stage-based architecture [72]. Simultaneously, our use of prompt-based learning eliminated the need for training data."}, {"title": "Prompt Design", "content": "We used ChatGPT to carry out the dedicated tasks within each stage of EventChat's workflow. When designing prompts for the stages, we faced a trade-off between response quality and latency, and costs. Specifically, we noticed that the LLM's response quality decreased as our prompts became more complex (i.e., containing more information, instructions, or tasks, leading to longer prompts). To improve response quality, each sub-task could be split into separate prompts, however, each call would need repeated output format instructions, leading to more token usage and higher latency due to the extra time needed for multiple calls. Additionally, splitting tasks into several prompts would not always be feasible as longer prompts might be necessary to provide important context to the LLM.\nAfter testing several prompt versions, the final prompts balanced these trade-offs by minimizing the number of LLM API calls while still delivering high-quality responses. In the back end, stages were defined by the number of tasks that could be handled within each prompt. For example, a response for the Chat action could be generated within the Action Detection phase prompt, making a separate prompt for this action unnecessary. From a prompt-design perspective, we used techniques like Few-Shot CoT and Few-Shot ICL to help the LLM perform specific business tasks [54]. We also used schema-based"}, {"title": "EventChat Action Modules", "content": "We implemented three dedicated modules for the Search, Recommendation, and Targeted Inquiry actions. Our Search and Recommendation workflows followed the principles of the RAG technique: First, we created a candidate set based on the user's query (Search) or past interactions (Recommendation). To get a highly relevant candidate set ordered by relevance, we then applied filters to the startup's Amazon Personalize instance or vector database if keywords could be extracted from the user's query. Second, we used ChatGPT as a ranker to determine whether the shown events matched the user's query. To contextualize ChatGPT we included textual summaries of the events in the prompt. In contrast to previous work [25, 33], we simultaneously ranked up to 10 items per prompt, with our ranking only distinguishing whether the event matches the user's intention or not. Finally, we created a user response by combining an answer based on the user's request or interest with the suggested events.\nThe stage Targeted Inquiry allowed users to get specific information about an event. To enable this, we generated a comprehensive textual description of an event, using the startup's database of events or obtained information from an event website. We were able to include all existing information within ChatGPT's token limit of 4096 tokens due to the sparsity of metadata. Overall, our approach contrasts previous work [33] where the LLM was used to create an entire SQL statement based on context about the database structure to answer user queries. While we also experimented with such an approach, it failed due to the complexity of the startup's relational database structure, which adhered to the Boyce- Codd normal form (BCNF) requiring multiple table joins. Additionally, due to web scraping, our database contained columns that were often sparse. Although technically possible in principle, adapting the LLM for such business constraints would have led to further complexities for the querying process."}, {"title": "Challenges and Limitations of Integrating ChatGPT in CRS", "content": "We noted several challenges in integrating LLMs in CRS. Most pressing was EventChats tendency to overlook contextual information and its susceptibility to hallucinations (e.g., suggesting fictional events), and the token limit constraint of 4096 for input and output [68]. We also noticed that the CRS occasionally failed to utilize information presented in the LLM prompt. This oversight could lead to situations such as the CRS recommending events with higher prices than the user's expressed willingness to pay. This could arise because the LLM would not attend to price information in the textual summary of an item during the reduction phase. For this same reason, we were able to circumvent the Refusal action in the Action Detection module when testing EventChat against prompt injection attacks. However, this only proved successful with some internal knowledge about the logic of the prompt template."}, {"title": "Evaluation", "content": ""}, {"title": "Methodology", "content": "We conducted a field study capturing both subjective and objective evaluation metrics. The subjective metrics were gathered using a survey based upon the ResQue framework [83], refined into a short-form version suitable for CRS evaluation in the field, and evaluated using a structural equation model (SEM). The objective metrics included latency, token consumption (as a cost indicator), and log data concerning the system's inputs, outputs, and interim results."}, {"title": "Survey Design", "content": "We designed our survey following the ResQue model [83] and Jannach's [38] catalog of subjective measurement dimensions for CRS evaluation (details in Appendix Error! Reference source not found.). As we conducted a field study with real users, we minimized participant burden by including only one item per construct in line with previous applications of ResQue model in non-CRS domains [20, 50]. Following recommendations for applying ResQue [84] we assessed the overall user experience of EventChat rather than the performance of the underlying technologies, such as ChatGPT and Amazon Personalize [84]. As such, our survey followed the traditional ResQue dimensions, but with Consistency, Coherence, and Input Processing Performance constructs added (see Figure 4) to assess the conversational dynamics innate to LLM-driven CRS. All items were assessed using a 5-point Likert scale ranging from 1 (disagree) to 5 (agree) to ensure correct display across a variety of smartphone devices."}, {"title": "Objective User-interaction Metrics", "content": "For each user interaction with EventChat, we monitored several key metrics: latency, token usage, and log data. These metrics were recorded for every request made to the LLM API, corresponding to each prompt call. Additionally, we measured the total duration of a request on the application's front end, including the round-trip time to the server. Latency metrics served as an estimate of the user's loading time, while token usage data translated into operational costs, both of which are vital strategic considerations for businesses. By analyzing these metrics at the level of individual prompts, we were able to pinpoint the sources of the most significant performance bottlenecks. In addition, by collecting log data we were able to analyze ChatGPT's output so as to identify irregularities and edge cases."}, {"title": "Data Collection", "content": "Participants accessed the survey via a link labeled as 'Lottery' in the app (cf. Figure 2) that offered participants a chance to win one of three Amazon vouchers worth \u20ac50 each. Data collection occurred between 18.10.2023 and 14.12.2023. Respondents were either: (i) existing users of the startup's app who accessed the corresponding screen in the app, or, (ii) individuals recruited on Instagram from 29.10.2023 to 11.11.2023 using a total campaign budget of \u20ac22 (24 USD). Our sample thus consisted of both new and existing users, capturing a broad cross-section of user types, improving external validity. A total of 108 participants conducted the survey. After filtering out non-completes (n = 20) and those who did not interact with EventChat according to the log data (n = 5), a total of 83 observations were available for analyses (43.8% female; Mage=28.3, SD = 11.0). Only the log data, cost, and latency for these 83 survey participants that used the app were analyzed to ensure comparability between objective and subjective data."}, {"title": "Results", "content": ""}, {"title": "Subjective Ratings of the User Experience", "content": "We found that 85.5% (n = 71) of users rated Recommendation Accuracy as neutral or good. Recommendation Accuracy, however, also had the highest number of negative appraisals of all User Perceived Quality and User Beliefs constructs (15%, n = 12), with a further 66.7% (n = 8) of these users indicating that the CRS failed to fulfill their request(s) (Success), contrasting the overall Success rating of 83.1% (n = 69). Examining the log data showed a median of 2 turns to identify a suitable event, consistent with the low perceived effort reported by participants. Loading times were reported as problematic by three users (General Problems) which was confirmed by the high latency metrics detailed in section 5.2.4."}, {"title": "Root Cause Analysis: Understanding why EventChat failed to fulfill request", "content": "To investigate underlying causes of failed user requests (Success), we examined the session logs, identifying the following failure categories: missing relevance of suggested events, failed Targeted Inquiry actions, and issues defining the time and location of the suggested events. The most reported issue concerned the relevance of suggested events. In many cases, the requested events did not exist in the database or were not properly categorized (e.g., a music concert was wrongly categorized as \"Other\") as a consequence of web scraping. Other cases stemmed from our design of the Recommendation action that applied time filters only following the approach of Wang et al. [98], rather than also query parameters (e.g., event category). This lack of additional filters when creating the candidate set with Amazon Personalize led to recommendation underperformance, indicating the necessity for user- specific filters to optimize recommendation performance.\nSeveral participants also encountered issues with EventChat recognizing their questions related to an event (Targeted Inquiry action). In most cases, the EventChat either opted for the Refusal action or failed to establish a connection between the question and one of the events stated in the prompt highlighting limitations of prompt-based learning techniques that rely on extensive information. Other flaws in prompt-based learning were also identified via log data (e.g., unrequested language switching).\nFinally, most users communicated their time preferences directly in the chat rather than using the UI button. As a result, the candidate set was generated with default time filters, and ChatGPT did not consider the user's specified time information during the reranking phase, leading to suggestions that did not match the user's time requirements. Thus a more user-friendly LLM-driven CRS would extract all necessary information from the chat."}, {"title": "Results from Structural Equation Model", "content": "The SEM showed adequate fit across most indices (x\u00b2 (22) = 33.942, p = 0.050, CFI = 0.948, TLI = 0.917, RMSEA = 0.081, SRMR = 0.087). A full table containing path coefficients can be found in"}, {"title": "Performance Metrics", "content": "With a median latency of 5.7 seconds per message and a median token usage of 18'106, EventChat exhibited relatively high latency in conjunction with high token usage. Table 2 presents an overview of token usage and latency, aggregated over all chat sessions of the survey participants.\nTable 3 displays the consumption of tokens as well as the needed time in more detail per action or stage. The data shows that the reduction of candidate items, as derived from the Search or Recommender prompts, consumed the most resources."}, {"title": "Discussion", "content": ""}, {"title": "System Design Implications", "content": "Based on findings presented in the current paper, we outline four design contributions that enhance our understanding of LLM-driven CRS for SMEs, addressing the significant challenges and opportunities for their design:\nFirst, during the implementation process, we encountered a critical trade-off between quality of the user experience and constraints of cost and latency, which was particuarly evident in the architecture and prompt design. Architecturally, we opted for a stage-based approach, contrasting previous research which has generally emphasized the high potential of agent-based approaches (e.g., for industrial automation [105] or chemical engineering [9]). This was motivated by the need to address stability issues with ChatGPT, while aligning with SME requirements concerning latency and cost. On the prompt design level, our approach also contrasts other frameworks that optimize LLM performance through prompting strategies [15, 53, 69, 109], ignoring business-critical trade-offs. While some LLM research has examined these trade-offs to date (see Zong et al. [110] on model performance and text classification costs or H\u00e4m\u00e4l\u00e4inen et al. [30] on data quality, latency, and cost trade-offs in synthetic data generation), our study goes further by focusing on an applied implementation relevant to business practice.\nSecond, we demonstrated feasibility of prompt-based learning approaches, but noted quality issues. Our approach, inspired by the InteRecAgent framework [33], relied heavily on techniques like ICL or Few- shot CoT, without model fine-tuning. While these techniques offered simplicity and flexibility, ideal for SMEs, longer prompts were required capture more detailed information. This detailed content was however sometimes missed or misunderstood by ChatGPT, as found in extant research [106], leading to lower Consistency and Coherence ratings. Due to constraints of context window length, cost, and latency it proved unfeasible to create a response prompt considering all eventualities however. While a unified dialog manager would be aware of its available tools and could mitigate this risk [25], the absence of detailed information would remain. Our research therefore underscores both the potential and challenges in utilizing prompt-based approaches."}, {"title": "Managerial Implications", "content": "Our case study reveals key managerial implications for SMEs when adopting LLM-driven CRS. First, cost and latency challenges are evident for systems that depend on external LLM APIs. In our implementation, operational costs averaged 3.6 cents per message and 11.2 cents per chat session, implying that such systems are currently best suited for high-margin scenarios only. Additionally, with an average of 5.7 seconds to process a message and 13.7 seconds per session that latency was relatively high. While EventChat users generally reported low perceived effort (suggesting that the efficiency gains from using EventChat outweighed the impact of load times for most users) managers should carefully balance recommendation performance versus waiting times.\nSecond, although we anticipated that libraries like Langchain would make implementation straightforward [94], creating a LLM-driven CRS appropriate for long-term production use involved numerous complications. On the one hand, the implementation of an LLM-driven CRS requires less ML expertise, reducing the need for skilled labor which is a known barrier to adoption [2, 87, 88]. On the other, relying solely on prompt-based learning revealed several limitations and alternative techniques to overcome these challenges also pose trade-offs: Opting for a more advanced LLM reduces cost- effectiveness, while fine-tuning an LLM adds complexity, making it less suitable for SMEs. For any system design selected, businesses should also anticipate what measures are necessary against abusive usage, further contributing to the system's complexity.\nThird, our experience in designing EventChat indicates that while implementing a LLM-driven CRS is feasible, even for resource constrained SMEs, whether to do so necessitates careful consideration and strategic planning to ensure they can effectively reach organizational goals, especially when compared to other simpler artifacts (e.g., RS, scripted chatbot etc.). This perspective resonates with Ivanov and"}, {"title": "Theoretical Implications", "content": "The current paper advances theory by extending short-form ResQue model to the evaluation of an LLM- driven CRS in the field, a technology revolutionizing digital services and IS research [23]. Results showed that Perceived Usefulness was positively associated with User Satisfaction and Intention to Use the system, Recommendation Accuracy with higher Perceived Usefulness, and User Control with higher overall user Satisfaction in the system. These findings corroborate extant research in RS [3, 12, 32, 42, 48, 49], conversational agent domains [85] as well as the original ResQue framework [83], showing the extent to which previous findings from the ResQue literature are applicable in the age of LLM-driven CRS.\nNevertheless, our results also displayed several deviations in the User Perceived Quality dimension, whereby Coherence and Interface Adequacy were insignificant predictors. This is notable, as it contrasts Jannach's [38] recently proposed catalog of subjective measurement dimensions for CRS evaluation which had not yet been empirically verified. Additionally, our ResQue model incorporated Input Processing Performance and Consistency to capture conversational dynamics vital to LLM-driven CRS following suggestions in recent publications [14, 18], validating their inclusion into a ResQue model for LLM-driven CRS for the first time. This contrasts alternative approaches that have since emerged in CRS research, which while not tested using an LLM-driven CRS, have suggested use of up to eight new constructs to capture conversational dynamics [47]. Our results indicate that such an extensive approach may not be necessary. The revised, short-form ResQue model we presented also omitted several constructs yet exhibited adequate fit, mitigating the risk of participant burden, reduced survey completion, and thus validity of the survey-based model estimates. Given the anticipated widespread adoption of LLM-driven CRS in the coming years [23], our short-form ResQue model enables field replicability in a rapidly evolving domain.\nLastly, our research provides an illustrative example of the benefits utilizing objective metrics in combination with subjective user evaluations. For example, examining log data revealed that reliance on prompt-based learning alone caused inconsistent answers concerning the item corpus or the CRS's capabilities. Our findings also suggested that users value consistent CRS responses since they minimize the need for correcting system errors. While this issue has been acknowledged in conversational agent research previously [45, 59], empirical evidence has been missing. Additionally, by combining subjective and objective measures, our results better elucidate the limitations of LLMs that our system inherited, such as hallucinations and overlooking user information. These issues affected both conversational metrics and traditional recommendation system metrics like Recommendation Accuracy through our use of ChatGPT as a reranker in the RAG technique. From a broader perspective, this raises a critical question regarding how to evaluate LLM-driven CRS; namely, to what extent should"}, {"title": "Limitations", "content": "Our research has several limitations that should be acknowledged. First, the CRS implementation and data presented were contextualized to the specific needs of a startup in the leisure industry. While this single context could potentially limit generalizability, our use case exhibited characteristics that are highly applicable to SMEs: a diversity of item categories, sensitivity to user pricing, and most critically, the time dependence inherent in events. This adds a layer of realistic complexity reflective of common business-specific challenges across CRS applications. However, this also means the design principles and features we discussed are not transferrable to all LLM-driven CRS contexts, particularly those for larger enterprises or organizations where a resource-conscious implementation is less critical.\nSecond, our SEM was estimated on a relatively modest sample size. The participant pool was also young, as a direct consequence of the study context (i.e., leisure event discovery) that may affect the generalizability of our results to other populations (e.g., older adults). Additionally, the use of single- item measures for our constructs may have compromised the internal validity, however, this was mitigated by our use of measures well-established in the RS domain [38, 83] and the straightforward nature of the constructs under investigation [6]. Balanced against these potential negatives, however, was the high external validity of our study data, which featured actual users of an LLM-driven CRS in the field. Given that LLMs are currently revolutionizing business and society, we argue that the benefits of the data presented outweigh the negatives.\nThird, to reduce participant burden, we shortened the ResQue model. This was achieved in line with the recommendations of Jin et al. [47], where constructs not relevant to the use case or system design where excluded. For instance, we did not include Recommendation Novelty due to events' time-sensitive nature (i.e., the CRS only recommended upcoming events). Similarly, given that our interface allowed for easy additional information retrieval by clicking on cards, we did not consider Information Sufficiency. While these omissions were both necessary and informed by theory, it is possible that these omitted variables could have some explanatory power."}, {"title": "Future Research", "content": "We recommend the following research directions (RD) for LLM-driven CRS moving forward. Exploring the use of a more advanced LLM featuring more parameters like GPT-4 (RD1) would allow for an agent-based approach [33], which may resolve issues related to the LLM overlooking the information provided, for example, by using prompt-based learning. Alternatively, use of fine-tuned"}, {"title": "Conclusion", "content": "Successful implementation of LLM-driven CRS for SMEs depends on multiple factors: acceptable user waiting times, resource usage, and realistic feature expectations. Our research showed that, while feasible, deploying a LLM-driven CRS as an SME requires careful consideration of system effectiveness, economic viability, and strategic rationale for doing so. In our implementation, we identified several potentially problematic design choices, including solely relying on prompt-based learning and the use of ChatGPT as a ranker in the retrieval-augmented generation (RAG) technique. Despite the limitations of these technologies, users rated the LLM-driven CRS positively, thus indicating that the system design could be rolled out on a wider scale. To ensure the long-term strategic viability of LLM-driven CRS for SMEs, we encourage researchers to explore other novel approaches for LLM- driven CRS architecture."}, {"title": "Appendices", "content": ""}, {"title": "Survey Constructs", "content": ""}, {"title": "Structural Equation Modeling Results", "content": ""}]}