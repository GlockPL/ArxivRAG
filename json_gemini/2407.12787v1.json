{"title": "GameVibe: A Multimodal Affective Game Corpus", "authors": ["Matthew Barthet", "Maria Kaselimi", "Kosmas Pinitas", "Konstantinos Makantasis", "Antonios Liapis", "Georgios N. Yannakakis"], "abstract": "As online video and streaming platforms continue to grow, affective computing research has undergone a shift towards more complex studies involving multiple modalities. However, there is still a lack of readily available datasets with high-quality audiovisual stimuli. In this paper, we present GameVibe, a novel affect corpus which consists of multimodal audiovisual stimuli, including in-game behavioural observations and third-person affect labels for viewer engagement. The corpus consists of videos from a diverse set of publicly available gameplay sessions across 30 games, with particular attention to ensure high-quality stimuli with good audiovisual and gameplay diversity. Furthermore, we present an analysis on the reliability of the annotators in terms of inter-annotator agreement.", "sections": [{"title": "Background & Summary", "content": "Affective Computing (AC) is the interdisciplinary field that refers to the study of human emotions and the development of tools and technologies that learn, interpret and perceive these emotions\u00b9. The availability of large-scale corpora comprising affect manifestations elicited through appropriate stimuli is critical for AC. However, manifestations of affect are often highly subjective and difficult to assess; each individual's interpretation of a stimulus is influenced by factors such as preferences, memory, biases, systematic errors, and expectations2,3. Furthermore, emotions are a dynamic phenomenon and participants' reactions to similar stimuli may shift over time. Therefore, as AC advances and deep learning methods require an increasing amount of data and scale, it becomes important to design and implement experimental protocols that maximise the reliability of collected labels of large-scale affect corpora.\nVideo games are a hugely popular type of digital media, offering a unique form of human-computer-interaction (HCI) due to their rich content and interactive nature4. Naturally, the global video game industry has seen substantial expansion and is already among the fastest growing subsectors within the broader entertainment sector. Thus, using games as a test-bed to reveal the intricacies of the HCI loop is one of the most promising ways to analyse human behaviour and experience, at scale. Studying the behaviour of players and the experience of play can, in turn, be used to improve the technical aspects of game development, and to contribute to more engaging and personalized game experiences6,7.\nThe literature already contains a growing number of diverse affect corpora that vary in terms of modalities of user input considered, annotation methods, affect stimuli, and number of participants among other factors. Some notable examples of modalities recorded include audiovisual data8,9, physiological signals10, and facial expression data\u00b9\u00b9. First-person annotation protocols ask the annotator to label their own experience, and have been employed in several affect corpora, including MazeBall12, PED13, FUNii14, and MUMBAI\u2079. When employing a third-person protocol, instead, we ask annotators to label the experience of another person, such as in the RECOLA15, LIRIS-ACCEDE16, AFF-Wild10, AffectNet\u00b9\u00b9, and SEWA17 corpora. The provided labels can be discrete (e.g. categories, scales) such as in GAME-ON\u00ae and BIRAFFE218 or continuous traces as in RAGA19 and MUMBAI9.\nContemporary affect corpora are gradually deviating from the controlled setting of in-lab experiments 15,20,21 to real-life (or in-the-wild) scenarios11,18,22 in an attempt to elicit more natural user behaviours and manifestations of affect. Hiring annotators from crowdsourcing platforms has also been gaining interest in an attempt for AC to scale. Untrained crowd workers in uncontrolled settings, however, can be unreliable, resulting in corpora of questionable, or even limited, value23. It is fair to say that all affect corpora are characterized by subjectivity24\u201326, since consensus among annotators is not necessarily required. Whilst some studies attempt to mitigate this issue of variability (e.g. by using mood induction27) for better performance in downstream tasks, doing so may hamper the generalizability and reliability of results when deployed in uncontrolled real-life scenarios. Motivated by these issues, we seek to provide an affect corpus with validated annotator reliability and sufficient diversity in the stimuli to promote generalizability in downstream tasks. We aim to accomplish this by following a tried and tested protocol for quality assurance23, and collect stimuli which present rich contextual variety within a single domain to"}, {"title": "Methods", "content": "Measuring the experience of digital game enjoyment and accurately identifying which game elements engage players are important goals for both the study of user experience in games and the development of better games\u2074. In this section, we introduce the FPS affect corpus GameVibe, which was solicited to provide a rich, multimodal dataset of elicitors for viewer engagement. The collected dataset includes: (a) synchronised frames and audio per game, (b) extracted latent representations from audiovisual data, (c) annotation traces per participant (in raw & processed form), (d) participants' replies to demographic surveys (anonymised data). The novelty of the dataset lies in the fact that affect annotations are user specific, thus we exploit the different opinions and perception of the users for the game that can enable the training of fair affect models that are highly robust to unseen data.\nIn this section, we provide a detailed description of the process we followed to build GameVibe. The core phases, as illustrated in Fig. 1, are as follows:\n\u2022 Design phase: we outline the objectives, tools, methodologies, and parameters of the study.\n\u2022 Stimuli collection phase: we choose and curate video content from diverse FPS games.\n\u2022 Annotation phase: before collecting labels of viewers' emotional states, we recruit participants, procure and prepare the equipment, and develop informed consent forms to uphold ethical standards. In addition, we conduct Quality Assurance tests to measure participants' reliability. We detail all the above below.\n\u2022 Post-experiment phase: we assess the quality of the dataset, process and analyse the collected data and document our findings. This ensures the dataset's efficacy and utility for subsequent research endeavors."}, {"title": "Design phase", "content": "This phase involves defining the scope and purpose of the dataset, determining the specific types of data to be collected, establishing data sources and acquisition methods, and devising a structure for organizing and storing the data. Additionally, factors such as data format, granularity, quality, and potential biases were considered. Beyond research standards, collaboration with domain experts from the game industry is essential to understand requirements and ensure that the dataset aligns with intended use cases. In this work, we draw from our experiences during our long-term collaboration with Ubisoft's Massive Entertainment, where we worked closely together to design an effective experimental protocol for collecting labels and training"}, {"title": "Stimuli collection phase", "content": "As discussed in Summary & Background, we chose 30 FPS games as affect stimuli in order to encompass the widest possible range of audiovisual styles and design aspects (including modes of gameplay and winning conditions). In terms of game style, we distinguish between games featuring a \u201crealistic\" vs. \"stylized\" art style. In terms of game era, we pick games of both \"modern\" and \"retro\" styles (Fig. 2). For example, a game such as Overwatch 2 (game 23 in Fig. 2) falls under the modern, stylized art style, whereas Wolfenstein (game 28 in Fig. 2) would fall under the retro, realistic category. For distinguishing across game design patterns, we made an effort to select games with different game modes or \u201csub-genres\" within the FPS space, such as \"Battle Royale\u201d (e.g. PUBG), \u201cdeath match\u201d (e.g. Counter Strike), and \u201csingle-player\u201d (e.g. Doom) games. The distribution of these design patterns is visualized in Fig. 2, where we can see single player games make up 67% of the stimuli selected, with death match and battle royale making up 20% and 13% respectively. We hope that the richness in variety and multimodality of the dataset will empower affect models to generalise better to unseen games when predicting viewer"}, {"title": "Annotation phase", "content": "Participant recruitment. We recruited 20 annotators among members of the University of Malta via convenience sampling. Annotators included research staff, B.Sc. students in fields relevant to digital games (including psychology and artificial intelligence), as well as M.Sc. students in Digital Games and PhD students in games research. Participants signed up for the study after being invited through the University's mailing list, and booked their slot at their own convenience. Recruitment and data collection started during March 2023 and was carried out throughout the year. Participants were also offered a \u20ac15 voucher as compensation upon completion of their participation in the study.\nLaboratory settings. All participants performed the annotation in the same room and light conditions at the Game Lab of the Institute of Digital Games, using the same machine and input/output devices (screen for visual stimuli, headphones for auditory stimuli, and a mouse with a scroll wheel for annotation). All participants were given a thorough introduction to the annotation task by at least one researcher involved in this work, who remained available during the entire annotation period for assistance and questions. Once the engagement annotation was completed (i.e. a task lasting approximately 30 minutes per participant),"}, {"title": "Research ethics.", "content": "The above process was approved by the Institutional Review Board (IRB) of the University of Malta33 before the annotation process commenced. Participants were required to sign a physical consent form prior to the experiment, agreeing to take part in the experiment and to have their annotation data and survey responses recorded and stored, as is common practice25. Participants were informed that they could halt the experiment at any time and leave without issue should they not wish to continue. While participants' demographic questions in the post-survey questionnaire (see below) included personal data, we preserved anonymity by using IDs rather than annotator names and ensured that such questions could not be used to identify the participants."}, {"title": "Survey data.", "content": "At the end of the annotation experiment, participants were required to fill in a short survey on their demographic information (age range, ethnicity, gender, handedness, education level), familiarity with video games, familiarity with FPS games, familiarity with video annotation as well as their favourite game. Most participants were between 25 and 35 years old (42%), while 32% were between 18 and 25 years and 21% were between 35 and 45 years old; 1 participant was over 45 years old. Almost all participants (97.4%) were Caucasian, and the vast majority (89.5%) were right-handed. Furthermore, 75% of participants were male, whilst 20% were female and 5% identified as non-binary. In terms of familiarity with games or video annotation, most participants were very familiar with games (average Likert score 4.25 out of 5) and slightly less so with FPS games (average Likert score 3.55 out of 5). Most participants were not very familiar with video annotation of affect data (average Likert score 2.65 out of 5), although some outliers existed (6 annotators rated their familiarity at 4 or 5 on a 5-point Likert scale). Finally, participants mentioned many different favourite games, with 7 out of 20 participants mentioning games which were first-person or third-person shooters."}, {"title": "Annotation protocol and tools.", "content": "As mentioned, each participant was assigned to a session and asked to annotate their engagement for a randomized sequence of 30 videos. The random video order was imposed to minimise carry-over effects between stimuli, as has been established in the literature34. Annotation tasks were carried out using the PAGAN annotation platform28 using the RankTrace annotation interface6. RankTrace allows for the annotation of stimuli in a continuous and unbounded fashion, and has proven itself for collecting reliable ground truths in an ordinal fashion\u00b3 (see equipment in Fig. 1). Before annotating gameplay videos, participants were tasked with annotating two Quality Assurance (QA) tasks, which were set up as independent PAGAN projects and linked together. Such QA tasks measure the ability to annotate a simple objective task where the ground truth signal is known. Such QA tasks have been proven to reliably predict annotator reliability in a previous study which used the first two sessions of this dataset23 as well as other established literature35. The first task is a visual task which requires the participant to annotate their perceived changes in brightness of a video showing a green screen, inspired by a previous study36. The second task is an auditory task, which requires participants to annotate their perceived changes in pitch whilst listening to an oscillating sound wave. These two stimuli are included in the dataset along with their respective annotations to provide a measure of our annotators' reliability on simple objective tasks, and by extension a better insight into their reliability in our engagement study.\nWhen participants were ready to annotate gameplay videos, they were provided the following definition of viewer engagement: \"A high level of engagement is associated with a feeling of tension, excitement, and readiness. A low level of engagement is associated with boredom, low interest, and disassociation with the game.\". We emphasise that participants annotated in a first-person manner, i.e. they annotated their own engagement as a viewer and not their perceived engagement of the player. Based on this definition, participants were instructed to scroll up on the mouse wheel whenever they felt an increase in their engagement, down when they felt a decrease, and to remain idle (i.e. no scrolling) if their engagement remained unchanged."}, {"title": "Collected annotation data.", "content": "As shown in Fig. 4(b), the dataset consists of 4 sessions of annotated footage, producing a corpus of gameplay videos of a total duration of 2 hours. Each of the 4 sessions contains 30 different 1-minute videos (i.e. one video from each of the 30 game titles), which are presented to the annotator in random order. This means that for each session we collect 150 annotation traces, with every video being annotated by 5 participants. In total, this amounts to 600 engagement annotation traces for 120 gameplay videos (of 30 different game titles), annotated by 20 participants. This data is contained in the dataset repository, detailed under Data Records."}, {"title": "Quality assurance and reproducibility of the GameVibe affect annotations.", "content": "The dataset was created with high-quality videos from numerous FPS games randomly placed in a sequence to reassure the variety and randomization that is necessary to identify variations in arousal but also preventing possible biases. The same room, equipment and light conditions were kept during the process for consistency of the annotation experience and data collection instruments (i.e. mouse wheel). Instructions were provided to the annotators to avoid gross errors during the annotation process. Finally, the dataset includes data from two QA tasks completed by annotators prior to the engagement study. Performance of annotators in such QA tests could be used in"}, {"title": "Audiovisual stimuli data processing.", "content": "In this study, we exploit large-scale pretrained models to extract latent representations in visual data using (a) Video Masked Autoencoders (VMA)29 and (b) Masked Video Distillation (MVD)30 models for visual representation learning. VMA is a model for robust representation learning for video data by utilizing a masked autoencoder architecture29. MVD provides efficient knowledge distillation by leveraging the principles of masked attention mechanisms to distil knowledge from a teacher network to a student network30. For audio data, we leverage the BEATS model proposed by Chen et al.31, which serves as a self-supervised architecture designed for audio processing tasks. Given that FPS games have a high representational fidelity (regardless of art style) with the real world, we hypothesise that the above pre-trained models can identify and classify different visual elements or sounds present within the audiovisual stimuli across different FPS games."}, {"title": "Affect processing methodology.", "content": "The raw discrete values that indicate changes in affect are generated from each PAGAN project and require a number of pre-processing steps before they are suitable for use in downstream tasks. Raw data from PAGAN is a list of values paired with a timestamp for when that value was entered by the participant (after scrolling on the mouse wheel), stored in a CSV file for each session. This data is converted into a time continuous signal by interpolating the values into a series of 1-second time windows (see Step 1 in Fig. 4). We also ensure that the annotator has made at least one input to the signal to be considered a valid signal; flat signals (with no changes) are discarded from the dataset. Finally, we combine the data into a single Python pickle file, detailed under Data Records.\nWe include a normalized version of our dataset where engagement is normalized to [0,1] via Min-Max normalization on a per-trace basis (see Step 2 in Fig. 4). In our processing library, we also include the option to perform moving average smoothing on the signals in the dataset, or to change the size of the time window of interpolated signals. We selected a 1-second resampling rate, considering the trade-off between increasing data volume and detecting changes in affect. Larger time windows or moving average filters would result in a smoother signal at the cost of signal accuracy and resolution. We offer the raw annotation traces from PAGAN in the dataset (see below) for further experiments with other processing methods.\nSince this study handles subjective labels, where no concrete ground truth can ever be identified, two strategies are common to establish a consensus. One strategy is to embrace the potentially high variance in the dataset and use this information to identify regions of high vs low agreement37, or even model it as part of the training process to improve performance38. Another strategy is to empirically analyse the data and use established techniques such as outlier detection39 to remove problematic annotators, or factor in annotator consensus40. We follow the latter strategy, detecting and removing outliers in the dataset to improve the quality of the ground truth signal derived in any downstream tasks (see Step 3 in Fig. 4).\nFor filtering outliers in each video in a session (see Step 4 in Fig. 5), we use Dynamic Time Warping41 (DTW) to create a distance matrix of the normalized annotation signals from each participant. We chose DTW as a distance measure due to its proven use in similar studies42 and its ability to focus on comparing the shape of the signals (i.e. an ordinal distance measure) whilst also factoring out time shifts between participants due to different lags in their annotation, which is its own area of research within affective computing43. For example, for the video of Wolfenstein 3D in Session 1, we calculate the pairwise DTW distances between all possible pairs of annotators and assemble them into a 5\u00d75 DTW distance matrix. We use the minimum distance to the nearest participant to judge whether a participant is a singular outlier (and should therefore be removed), or part of a cluster of two or more annotators (and should therefore remain in the dataset). This process involves setting a threshold to determine what will be considered an in/outlier. Rather than using a static threshold which would require hand-picking for each session, we use the non-parametric Kernel Density Estimation (KDE) for outlier removal to create a threshold, which is visualised in Fig. 5. This involves calculating all the pairwise KDE scores between signals for all videos in a session and sorting them in ascending order. We then test two filters, a strict filter which only includes signals with a nearest-neighbour distance within the best 80% of KDE scores, and a more relaxed filter which includes signals with a nearest-neighbour distance within the best 90% of KDE scores. Remaining signals are considered outliers and are removed: examples of removed outliers for Wolfenstein 3D are shown in Fig. 5."}, {"title": "Data Records", "content": "The dataset is split into five directories as depicted in Fig. 3. The \"Stimuli\" directory contains the 1-minute game videos organised into subdirectories by session and named according to the game in each video. The \u201cQuestionnaire\" directory contains participants' responses to the post-experiment questionnaire in CSV format, and a Python script to process the responses and create visualisations. The \u201cAnnotations\u201d folder contains the annotation data returned from PAGAN and the Python scripts required to process them; this folder has several subdirectories of importance. In the \"Raw\u201d subdirectory, the raw annotations outputted by PAGAN for each session can be found. The \"Raw Combined\" subdirectory contains the raw data assembled"}, {"title": "Technical Validation", "content": "Annotator Quality Assurance: We first illustrate the reliability of our annotators by computing their average QA scores across our visual and auditory QA tasks (see Methods). Since both QA tasks involve an annotation task where the ground truth (screen brightness and audio frequency respectively) is known in advance, we use the Signed Differential Agreement (SDA) metric36 to measure similarity. We chose SDA as it has been proven effective in our previous QA study23 and, due to its bounded nature between -1 and 1, is an intuitive metric for selecting a filter threshold. Across all participants and both QA tasks, the average SDA score was 0.43\u00b10.15, meaning that annotators correctly annotated 71.5% of all time windows among QA signals. If we used a QA filtering threshold value similar to previous work23 (SDA = 0, or 50% accuracy), only 3 annotators (P6 and P7 from Session 2, P17 from Session 4) would be removed due to poor QA performance. This verifies that almost all our annotators understood the annotation process and could accurately produce labels before moving on to the engagement task. For the purposes of the technical validation of this paper, however, no participants are removed through this QA filtering in order to assess all raw data included in the dataset.\nInter-Annotator Agreement: We use DTW (see Methods) to create distance matrices for each video in each session, in order to calculate the inter-annotator agreement between annotators' normalized engagement traces on the same stimuli. We average those DTW values between pairs of participants on a per-session basis, since the same participant with the same identifier (e.g. P1) annotated every video in the same session. We report the average DTW distance across all stimuli (30) per session in Table 1, using different participant identifiers because different participants annotated different sessions. We observe that there are differences between annotators; only in Session 1 annotators were more in agreement. However, annotators are rarely consistently disagreeing with all other annotators; the most obvious instances of this are P6 (with DTW distances over 10 with all other annotators in Session 2) and P17 (with DTW distances over 11 with three other annotators in Session 4). We note"}, {"title": "Outliers per participant:", "content": "We detect outliers on the engagement traces (see Methods) based on the KDE of all DTW matrices of all videos in the same session, and report results with two filters for outlier removal. When using the 90% filter (see Methods), 60 out of 600 annotations are removed (15 per session), whilst with the 80% filter 120 annotations are removed (30 per session). Interestingly, while P7 would be removed due to poor QA task performance, none of their 30 traces are removed as outliers for the 90% filter and only 2 traces are removed as outliers for the 80% filter. On the other hand, P17 has the most outliers of all 20 participants: 11 of their 30 traces are removed as outliers for the 90% filter and an extraordinary 16 out of 30 removed for the 80% filter. Other participants with many removed outliers are P4 (with 9 outliers for 80% filter) and P3, P13, P20 (each with 8 outliers for 80% filter). While average DTW distance was correlated with the average SDA score from the two QA tasks, the same metric is not significantly correlated (p > 0.05) with the number of outliers removed: r = -0.33 for removed outliers per participant with the 90% filter and r = -0.25 for outliers with the 80% filter."}, {"title": "Outliers per game and video:", "content": "It is worthwhile to observe which game stimuli were more prone to inter-annotator disagreements, which would lead to more outliers removed through the above filtering process. After applying the 90% filter, outliers were removed from 26 out of the 30 games. The games with no outliers were Apex Legends, Wolfram, Medal of Honour (PS1) and Superhot. The games with the most outliers were Outlaws, Counter-Strike 16, and Wolfenstein 3D with 4 outliers each (out of 20 annotation traces across 4 videos of the same game). After applying the 80% filter, outliers were removed from every game except Apex Legends. The game with the most outliers when using the 80% filter was Operation Body Count, with 9"}, {"title": "Usage Notes", "content": "We provide annotation data in CSV format and the respective video stimuli in .mp4 format, both formats can be processed in most software packages or programming language. We provide Python files (.py) for data processing and extraction of feature representations are provided. The GameVibe_README.md file details the organisation of the dataset, explaining the structure, naming convention, and specific contents of each file."}, {"title": "Code availability", "content": "The dataset can be managed, visualised and pre-processed using Python files (.py) files. These files are accessible for download at: https://osf.io/p4ngx/?view_only=fc9c68cf3f104ad7afba5ab73a1c66a8."}]}