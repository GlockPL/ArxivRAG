{"title": "Implementing Fairness: the view from a FAIRDREAM", "authors": ["Thomas Souverain", "Johnathan Nguyen", "Nicolas Meric", "Paul \u00c9gr\u00e9"], "abstract": "In this paper, we propose an experimental investigation of the problem of AI fairness in classification. We train an AI model and develop our own fairness package (FAIRDREAM) to detect inequalities and then to correct for them, using income prediction as a case study. Our experiments show that it is a property of FAIRDREAM to fulfill fairness objectives which are conditional on the ground truth (Equalized Odds), even when the algorithm is set the task of equalizing positives across groups (Demographic Parity). While this may be seen as an anomaly, we explain this property by comparing our approach with a closely related fairness method (GRIDSEARCH), which can enforce Demographic Parity at the expense of Equalized Odds. We grant that a fairness metric conditioned on true labels does not give a sufficient criterion to reach fairness, but we argue that it gives us at least a necessary condition to implement Demographic Parity cautiously. We also explain why neither Equal Calibration nor Equal Precision stand as relevant fairness criteria in classification. Addressing their limitations to warn the decision-maker for any disadvantaging rate, Equalized Odds avoids the peril of strict conservatism, while keeping away the utopia of a whole redistribution of resources through algorithms.", "sections": [{"title": "Introduction", "content": "Determining whether an algorithm provides 'fair' predictions is far from obvious. The difficulties pertain in part to the fact that different notions of fairness exist and compete with each other. On the theoretical side, plethora of statistical metrics exist to quantify the extent to which the predictions of an AI model meet the user's expectation on fairness, in particular regarding a minority that must not be harmed, [Hellman, 2020] [Mehrabi et al., 2021] [Wan et al., 2023]. Yet, attempts to compare these algorithmic fairness metrics face impossibility theorems, implying that the various fairness criteria proposed cannot be jointly satisfied in all cases (viz. [Kleinberg et al., 2016, Hellman, 2020, Hedden, 2021]).\nOn the practical side, some concrete cases have shown us that the recommendations of an AI may be viewed as fair by its designers, but as unfair by external evaluators. The case of COMPAS, an algorithmic tool intended to score recidivism risk, is a telling example. Notoriously, the ProPublica 2016 report accused COMPAS of racial bias [Larson et al., 2016], by showing a higher rate of false positives in African Americans for \"high-risk\" scores and a higher rate of false negatives in Caucasians for \u201clow-risk\u201d scores. In response, the designers of COMPAS (Northpointe) replied that ProPublica artificially built these categories (by setting a boundary at 5/10 of recidivism risk), and also that for each different risk level, the statistical performance of COMPAS (as measured by ROC- AUC, i.e. rate of true positives compared to false positives) was actually the same across both groups, and even slightly more favorable to black defendants [Dieterich et al., 2016].\nIn this paper, we propose to look at a similar tension in fairness criteria, but this time by looking at how interventions for fairness correction can be automated and implemented. Specifically, we trained an AI model and developed our own fairness package (FAIRDREAM) to detect inequalities and then to correct for them. As with other approaches of AI fairness developed in diverse fields including medicine [Chen et al., 2023], law [Lagioia et al., 2023], or mortgage lending [Lee and Floridi, 2021], we anchor our"}, {"title": "Age disparities in the evaluation of income", "content": "Predicting income matters in situations in which investors need to make funding decisions and ensure their customer's capabilities. Depending on how they situate the customer's assets, such predictions may be used in banking as a basis to grant credits, in wealth management to recommend investment products, but also in recruitment to set salaries and make attractive offers [Meng et al., 2018].\nFor our experiment, we thus manipulated the well-known Census dataset, which aggregates data from 48,842 US citizens in 1994.1 The dataset includes 14 characteristics such as age, work class, years of education, marital status, and geographical origins. A fifteenth column, finally, displays a binary encoding of whether the agent earns less or more than $50,000 a year (Cf. Figure 1).\nThe task for an algorithm is to draw inferences from the dataset, namely to classify"}, {"title": "The package FAIRDREAM", "content": "How can we assess that an AI model in charge of predicting level of income is fair before deploying it? To answer this question, we implemented our own algorithmic fairness package (FAIRDREAM). In FAIRDREAM, the main purpose is to give lay users, without any background in data-science or in theories of fairness, an insight into the baseline AI model, and subsequently, a handle on the process of correction."}, {"title": "Correction", "content": "Correction \u2013 With these alerts, the user can form a justified opinion about imbalance between populations; and they can decide which gaps between groups of features to correct for. In our experiment, we simulated the approach of a decision-maker whose normative preference is to reduce the difference between older and younger clients.\nWe thus passed the feature 'age' to FAIRDREAM, so as to build a model bridging the"}, {"title": "Demographic Parity vs Equalized Odds", "content": "Adopting the taxonomy of [Wachter et al., 2020], we may first interpret the desire to diminish the gap between older and younger clients as an unconditional wish, namely to predict equal proportions of 17-29 and 29-37 year-old as earning over $50,000, regardless of their true level of income. That would be translated into an objective of \u201cDemographic Parity\" [Dwork et al., 2012]:\n$p(\u0176 = 1|A = 1) = p(\u0176 = 1|B = 1)$\nThis objective of Demographic Parity is far from being achieved in the new model: only 16% of younger clients are selected against 59% for their elders. Although the ratio is a bit more advantageous (3.7 to 1 versus 5.5 to 1), the corrected model makes the same overall difference between groups of age as the baseline model did (Cf. Tables 1 and 2, red highlights).\nYet, the way age groups are treated by the FAIRDREAM model leaves room for a different interpretation, based on their ground-truth label. The percentages of people predicted by the model as earning over $50,000 can be compared between subgroups whose income indeed exceeds $50,000 (true positive rates), or whose income is below $50,000 (false positive rates). Determining if the model does better to classify them according to their true income corresponds to a fairness metric of \u201cEqualized Odds\u201d, namely equalizing true positive and false positive rates [Hardt et al., 2016, Barocas et al., 2023]:\n$p(Y = 1|A = 1 \\land Y = 1) = p(Y = 1|B = 1 \\land Y = 1)$\n$p(\u0176 = 1|A = 1 \\land Y = 0) = p(\u0176 = 1|B = 1 \\land Y = 0)$\nAdopting this lens shows that younger clients are clearly reevaluated. We observe"}, {"title": "Explaining FAIRDREAM's results", "content": "To better situate the correction method of FAIRDREAM, we need to say more about the landscape of \u201calgorithmic fairness\", that is about extant methods to bring a model closer to statistical fairness criteria [Mehrabi et al., 2021]."}, {"title": "An in-processing method", "content": "Once we set the percentage of clients predicted as earning over $50k to be equal across ages (Demographic Parity), the fairness techniques can equalize them before, during, or after training of the model [Alves et al., 2021]. Although they are implemented in diverse ways, these techniques mainly rely on the following architectural principles:\nPre-processing: Pre-processing methods intervene on the training data, to feed the future model with rebalanced or more favorable data for harmed individuals. The methods"}, {"title": "Reweighting for transparent processing", "content": "In our attempt to make the method as transparent as possible to lay users, our method uses the principle of reweighting ([Calders et al., 2009]. Reweighting of harmed individuals generally matches the intuition of lay users concerning compensatory justice [Velasquez et al., 1990]. To wit, if the weight of an error made on a 28-year old is three times the weight of an error on older clients, the model has to fit its parameters while \"paying three times more attention\" at previously harmed individuals.\nTo make the correction of the model fit the lay intuition, FAIRDREAM reweights groups in an ascending way. The weight of an error on a group Sk grows with its previous"}, {"title": "Experimental comparison with GRIDSEARCH", "content": "To investigate the proper effects of FAIRDREAM'S correction, we compared it in a benchmark experiment with a closely related fairness method: GRIDSEARCH [Agarwal et al., 2018], also an in-processing method of cost-sensitive classification.\nDescribed below and synthesized in the tables of Appendix B, the full results of our experiment are accessible in the GitHub benchmark repository: https://anonymous.\n4open.science/r/weights_distortion_impact-15/.\nIn GRIDSEARCH, the statistical performance and fairness tasks are split. More precisely, the loss is optimized under the fairness constraint F, e.g. penalizing the model when the overall true positive rates are not equalized (with \u03b7 > 0):\n$\\min_\u03b8 L(\u03c6_{\u03b8,n}) \\text{ such that } F(\u03c6_{\u03b8,n}) \u2264 \u03b7$\nThe classifier en has to find the parameters @ which minimize the accuracy loss L($0,n), so long as the fairness constraint F is under the threshold \u03b7. Which amounts to the loss function:\n$L (\u03c6_{\u03b8\u03b7}, \u03bb) = \\sum_{i=1}^m l (y_i, Y_i) + \u03a9(0) + XT (F (\u03c6_{\u03b8,\u03b7}) \u2013 \u03b7)$"}, {"title": "Sample Weights vs Optimization under Constraint", "content": "How can we explain that GRIDSEARCH achieves the fairness goal of Demographic Parity, whereas it is not the case of FAIRDREAM? As explained above, they differ in the way they implement the fairness constraint:\n- In GRIDSEARCH Loss, the fairness objective F ($o,n) is a new term added inside the global loss function (this way of correcting is referred to as an optimization of model's parameters subject to a fairness constraint F [Wan et al., 2023]). The left part of L is a classic loss function, increasing when the error between the prediction \u0177 and the true Yi"}, {"title": "Choosing a Fairness Metric conditioned on True Labels", "content": "We provided a computational and mathematical explanation for the deviation between the initial fairness objective (Demographic Parity) and the one actually achieved by FAIRDREAM (Equalized Odds). In this section we propose a more normative discussion of the choice between these two fairness objectives. As announced in the introduction,"}, {"title": "Calibration and Precision", "content": "One common view of fairness is found in the idea of predictive parity between groups. Predictive parity can mean that the same fraction of true positives is predicted between groups at identical risk scores, or it can mean that the same fraction of true positives is predicted between groups relative to a classification threshold. The first notion corresponds to the statistical notion of calibration of a classifier, whereas the second corresponds to the notion of statistical precision of a classifier. Both notions are sometimes indistinctly referred to under the term \"calibration\" in the literature (see [Mayson, 2018, Long, 2021]), although strictly speaking they differ. Both notions have also been invoked in defense of COMPAS against ProPublica's charges of racial bias. In their response to ProPublica, Northpointe argued that at the cutoff chosen by ProPublica to show Unequal Odds between Whites and Blacks, the statistical precision of the COMPAS algorithm was roughly the same between both groups. Moreover, it was shown that at each risk score, the proportion of rearrest between groups was roughly the same (see [Corbett-Davies et al., 2017, Figure 2]), this time evidencing calibration between groups.\nBoth calibration and precision are central notions in that regard, and one may wonder"}, {"title": "Fairness from Accuracy", "content": "In our benchmark comparison, each time the GRIDSEARCH models outperformed FAIRDREAM with regard to Demographic Parity, it was at the cost of a worse statistical performance. On the contrary, FAIRDREAM gives a truthful picture of the ground labels: the FAIRDREAM correction method performs better in terms of accuracy, with a low rate of false positives (less than 5%) and a high rate of true positives (nearly 90%).\nThe question raised by this situation is whether increasing statistical accuracy, namely Selectivity and Sensitivity, is automatically a way of increasing fairness. Several arguments can be given against the sufficiency of accuracy to achieve fairness. The main one is that even the most accurate classification may simply replicate imbalances that are in the data ahead of the algorithm's workings, as a result of social biases or social injustice.\nWe agree with this argument, and we grant that improving on descriptive accuracy may not be sufficient to achieve normative fairness (in line with Hume's classic remarks on the is-ought distinction [Hume, 1739]). However, we consider that descriptive accuracy in predictions should at least be viewed as a necessary condition on fairness [Wachter et al., 2020], and that it may even come out necessary and sufficient in cases in which the predictions themselves concern matters of fact.\nIn matters of fact, the case for conditional fairness metrics can be buttressed by general epistemological considerations. A user trying to narrow the gap between overall positive rates made on age groups by our FAIRDREAM classifier (16% for 17-29 years old"}, {"title": "Always prefer Equalized Odds?", "content": "The foregoing arguments notwithstanding, there remain reasons to doubt the validity of conditional metrics in every situation. For comparison, consider another realistic financial classification task in which the bank-teller has to estimate which client is a \"good risk\" or \"bad risk\" based on age, sex, profession, accounts, and credit amount, as appears in"}, {"title": "Conclusion", "content": "In this paper we implemented an algorithmic fairness tool to detect and correct for disparities in income estimates, the package FAIRDREAM. The practical goal of this package is to help lay users implement their vision of fairness, in order to come up with socially optimal decisions. Our study of the specific properties of FAIRDREAM raised a puzzle: how can an algorithm designed to enforce Demographic Parity end up converging on a different fairness goal, namely Equalized Odds?"}, {"title": "FAIRDREAM's Reweighting", "content": "We provide here more details on how the sample weights are generated during the in- processing correction of FAIRDREAM. The differentiated weights of error are established in a deterministic but simple way, to grant user's understanding on the process of correction for fairness. For a FairDream model n \u2208 [1,5] of five models to be trained in competition, the new weight of error for each group Sk is computed that way:\n$Wn(Sk) = rate\\_indivs\\_disadvantaged(Sk) \u00d7 exp(n \u00d7 gap\\_fair\\_scores(Sk))$\nThe exponential part of the equation focuses on how far from the maximum the current fairness score (e.g. overall positive rate) of the group Sk is. It resets the weight according to the disadvantage of Sk:\ngap_fair_scores(Sk) = |fair_score(Sk)-max(fair_score(S\u2081), .., fair_score(Sn))|\nThe new weight wn(Sk) takes into account the difference across fairness scores, but also the number of individuals previously impacted. rate_indivs_disadvantaged is a coefficient which stresses the number of people disadvantaged inside the group, relative to the overall population. This coefficient increases as the people in Sk are a higher share of the population:\n$rate\\_indivs\\_disadvantaged(Sk) = gap\\_fair\\_scores \u00d7 \\frac{Sk}{\\sum_{i=1}^n Si}$"}]}