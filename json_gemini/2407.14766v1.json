{"title": "Implementing Fairness: the view from a FAIRDREAM", "authors": ["Thomas Souverain", "Johnathan Nguyen", "Nicolas Meric", "Paul \u00c9gr\u00e9"], "abstract": "In this paper, we propose an experimental investigation of the problem of AI fairness in classification. We train an AI model and develop our own fairness package (FAIRDREAM) to detect inequalities and then to correct for them, using income prediction as a case study. Our experiments show that it is a property of FAIRDREAM to fulfill fairness objectives which are conditional on the ground truth (Equalized Odds), even when the algorithm is set the task of equalizing positives across groups (Demographic Parity). While this may be seen as an anomaly, we explain this property by comparing our approach with a closely related fairness method (GRIDSEARCH), which can enforce Demographic Parity at the expense of Equalized Odds. We grant that a fairness metric conditioned on true labels does not give a sufficient criterion to reach fairness, but we argue that it gives us at least a necessary condition to implement Demographic Parity cautiously. We also explain why neither Equal Calibration nor Equal Precision stand as relevant fairness criteria in classification. Addressing their limitations to warn the decision-maker for any disadvantaging rate, Equalized Odds avoids the peril of strict conservatism, while keeping away the utopia of a whole redistribution of resources through algorithms.", "sections": [{"title": "Introduction", "content": "Determining whether an algorithm provides 'fair' predictions is far from obvious. The difficulties pertain in part to the fact that different notions of fairness exist and compete with each other. On the theoretical side, plethora of statistical metrics exist to quantify the extent to which the predictions of an AI model meet the user's expectation on fairness, in particular regarding a minority that must not be harmed, [Hellman, 2020] [Mehrabi et al., 2021] [Wan et al., 2023]. Yet, attempts to compare these algorithmic fairness metrics face impossibility theorems, implying that the various fairness criteria proposed cannot be jointly satisfied in all cases (viz. [Kleinberg et al., 2016, Hellman, 2020, Hedden, 2021]).\nOn the practical side, some concrete cases have shown us that the recommendations of an AI may be viewed as fair by its designers, but as unfair by external evaluators. The case of COMPAS, an algorithmic tool intended to score recidivism risk, is a telling example. Notoriously, the ProPublica 2016 report accused COMPAS of racial bias [Larson et al., 2016], by showing a higher rate of false positives in African Americans for \"high-risk\" scores and a higher rate of false negatives in Caucasians for \u201clow-risk\u201d scores.\nIn response, the designers of COMPAS (Northpointe) replied that ProPublica artificially built these categories (by setting a boundary at 5/10 of recidivism risk), and also that for each different risk level, the statistical performance of COMPAS (as measured by ROC- AUC, i.e. rate of true positives compared to false positives) was actually the same across both groups, and even slightly more favorable to black defendants [Dieterich et al., 2016].\nIn this paper, we propose to look at a similar tension in fairness criteria, but this time by looking at how interventions for fairness correction can be automated and implemented. Specifically, we trained an AI model and developed our own fairness package (FAIRDREAM) to detect inequalities and then to correct for them. As with other approaches of AI fairness developed in diverse fields including medicine [Chen et al., 2023], law [Lagioia et al., 2023], or mortgage lending [Lee and Floridi, 2021], we anchor our"}, {"title": "Age disparities in the evaluation of income", "content": "Predicting income matters in situations in which investors need to make funding decisions and ensure their customer's capabilities. Depending on how they situate the customer's assets, such predictions may be used in banking as a basis to grant credits, in wealth management to recommend investment products, but also in recruitment to set salaries and make attractive offers [Meng et al., 2018].\nFor our experiment, we thus manipulated the well-known Census dataset, which aggregates data from 48,842 US citizens in 1994. The dataset includes 14 characteristics such as age, work class, years of education, marital status, and geographical origins. A fifteenth column, finally, displays a binary encoding of whether the agent earns less or more than $50,000 a year (Cf. Figure 1).\nThe task for an algorithm is to draw inferences from the dataset, namely to classify"}, {"title": "The package FAIRDREAM", "content": "How can we assess that an AI model in charge of predicting level of income is fair before deploying it? To answer this question, we implemented our own algorithmic fairness package (FAIRDREAM). In FAIRDREAM, the main purpose is to give lay users, without any background in data-science or in theories of fairness, an insight into the baseline AI model, and subsequently, a handle on the process of correction."}, {"title": "FAIRDREAM takes as input an already trained predictor", "content": "To enable the users' understanding, we started the correction with the simplest metric of FAIRDREAM, the overall percentage of individuals predicted as positive. This metric basically enforces Demographic Parity [Dwork et al., 2012], which means that it offers to equalize the percentage of individuals selected by the model as earning at least $50,000 across groups, based on some given feature.\nFAIRDREAM takes as input an already trained predictor (the baseline XGBoost we trained) and the users' preferences as their fairness indicator (e.g. the percentage of sub-groups labeled as \u201c> $50k\" by the XGBoost). The procedure implemented in FAIRDREAM involves two steps, a step of detection and a step of correction.\nDetection - The \u201cDiscrimination Alerts\u201d algorithm detects disparities in how groups are treated. Each feature is inspected. If age intervals, certain jobs, or nationalities are under-selected by the model, a discrimination alert is issued. The algorithm hence makes users aware of disadvantaged groups, potentially distinct from the user's beliefs on what those groups might be.\nOn the sample of Census used for training, FAIRDREAM warns for a ratio of 1 to 5 in the model's predictions: for example, for the 17-29 year-old individuals, the portion of the sub-population selected is 12%, against 66% for 29-37 year-old (Cf. Table 1), thereby triggering an alert.\nCorrection \u2013 With these alerts, the user can form a justified opinion about imbalance between populations; and they can decide which gaps between groups of features to correct for. In our experiment, we simulated the approach of a decision-maker whose normative preference is to reduce the difference between older and younger clients.\nWe thus passed the feature 'age' to FAIRDREAM, so as to build a model bridging the"}, {"title": "Selection of the best corrected model", "content": "gap between ages. Five new XGBoost models were trained and put in competition to reach a similar statistical performance with the initial XGBoost, while better classifying previously disadvantaged younger clients. Comparing the five new models with the baseline, FAIRDREAM selects the model satisfying the best trade-off between accuracy and fairness (see Figure 3).\nIn our experiment, the stat_score is the indicator of predictive performance chosen by the user, such as ROC-AUC. The fair_score is maximized when the indicators of fairness determined by the user are equalized across groups, such as the overall positive rates for Demographic Parity. We implemented it as a weighted linear sum of the differences of fairness indicators between groups. The best model is then selected as the one with the highest trade_off_score = 1/3 * stat_score + 2/3 * fair_score, if for instance the user prefers fairness over accuracy."}, {"title": "Demographic Parity vs Equalized Odds", "content": "Adopting the taxonomy of [Wachter et al., 2020], we may first interpret the desire to diminish the gap between older and younger clients as an unconditional wish, namely to predict equal proportions of 17-29 and 29-37 year-old as earning over $50,000, regardless of their true level of income. That would be translated into an objective of \u201cDemographic Parity\" [Dwork et al., 2012]:\n$p(\u0176 = 1|A = 1) = p(\u0176 = 1|B = 1)$\nThis objective of Demographic Parity is far from being achieved in the new model: only 16% of younger clients are selected against 59% for their elders. Although the ratio is a bit more advantageous (3.7 to 1 versus 5.5 to 1), the corrected model makes the same overall difference between groups of age as the baseline model did (Cf. Tables 1 and 2, red highlights).\nYet, the way age groups are treated by the FAIRDREAM model leaves room for a different interpretation, based on their ground-truth label. The percentages of people predicted by the model as earning over $50,000 can be compared between subgroups whose income indeed exceeds $50,000 (true positive rates), or whose income is below $50,000 (false positive rates). Determining if the model does better to classify them according to their true income corresponds to a fairness metric of \u201cEqualized Odds\u201d, namely equalizing true positive and false positive rates [Hardt et al., 2016, Barocas et al., 2023]:\n$p(\u0176 = 1|A = 1 \\land Y = 1) = p(\u0176 = 1|B = 1 \\land Y = 1)$\n$p(\u0176 = 1|A = 1 \\land Y = 0) = p(\u0176 = 1|B = 1 \\land Y = 0)$\nAdopting this lens shows that younger clients are clearly reevaluated. We observe"}, {"title": "Explaining FAIRDREAM's results", "content": "that the probabilities of being classified as earning at least $50k are almost the same for those truly at that income level (88% vs. 89%) and for those who are not (3% vs 4%), in the younger as in the older age category (Table 2, green part). Whereas in the baseline model, there is a gap of almost 20% between these rates, disadvantaging younger clients (Table 1).\nThis is unexpected, since in this experiment we actually asked for the overall percentages of clients earning over $50,000 to be predicted as equal across ages by the FAIRDREAM model. Mathematically, this objective corresponds to Demographic Parity. Yet, we see that Demographic Parity is not achieved by the FAIRDREAM model. Instead, it actually produces a different result, namely Equalized Odds. Is it an accident? Or is it rather an emergent feature of the model, which FAIRDREAM can help us to rationalize? We proceed to clarify this issue in the next section."}, {"title": "An in-processing method", "content": "To better situate the correction method of FAIRDREAM, we need to say more about the landscape of \u201calgorithmic fairness\", that is about extant methods to bring a model closer to statistical fairness criteria [Mehrabi et al., 2021].\nOnce we set the percentage of clients predicted as earning over $50k to be equal across ages (Demographic Parity), the fairness techniques can equalize them before, during, or after training of the model [Alves et al., 2021]. Although they are implemented in diverse ways, these techniques mainly rely on the following architectural principles:\nPre-processing: Pre-processing methods intervene on the training data, to feed the future model with rebalanced or more favorable data for harmed individuals. The methods"}, {"title": "Reweighting for transparent processing", "content": "can change the labels (e.g. FairBatch [Roh et al., 2020]) or the frequency of minorities in the training dataset (e.g. Reweighing [Calders et al., 2009]).\nPost-processing: Post-processing occurs after the model is trained. In classification, it takes place when we convert the probabilities into the predicted event. Some examples include adjusting the thresholds, which enables harmed individuals with lowest scores to be labeled as earning over $50k ([Hardt et al., 2016]).\nIn-processing: In-processing methods generally consist in minimizing the loss function, subject to a fairness constraint, to satisfy a trade-off between the performances of the statistical and fairness metrics of the user [Wan et al., 2023].\nFAIRDREAM is an in-processing method, in which the correction step takes place during model training. Underlying this choice, we considered that intervening outside the model unnecessarily increases the opacity of an AI system. In particular, the other two methods can lead the model to learn from unrealistic samples (in the case of pre- processing), or to generate new thresholds for the same group each time the model is deployed in a new context (case of post-processing).\nIn our attempt to make the method as transparent as possible to lay users, our method uses the principle of reweighting ([Calders et al., 2009]. Reweighting of harmed individuals generally matches the intuition of lay users concerning compensatory justice [Velasquez et al., 1990]. To wit, if the weight of an error made on a 28-year old is three times the weight of an error on older clients, the model has to fit its parameters while \"paying three times more attention\" at previously harmed individuals.\nTo make the correction of the model fit the lay intuition, FAIRDREAM reweights groups in an ascending way. The weight of an error on a group $S_k$ grows with its previous"}, {"title": "Experimental comparison with GRIDSEARCH", "content": "disadvantage. The new sample weight $w_n(S_k)$ of FAIRDREAM model #n increases with the number of harmed individuals (linearly), and with the difference of fair scores in the baseline model (exponentially).\nFor example, in FAIRDREAM's model #2, which was selected as the best one, the weight of an error made by the classifier during training on 17-29-year old $(w_2(S_1)=0.8)$ became eight times the error on 29-37-year old $(W_2(S_2)=0.1)$.\nThe new model #n, $\\phi_{\\theta,n}$, has to learn the parameters $\\theta$ that minimize the loss, and thereby maximize the statistical performance. With each new classifier $\\phi_{\\theta,n}$, FAIRDREAM tests a new combination of weights $W_n = (w_n(S_1),..., w_n(S_p))$ on the p groups, simply adding the multiplying weight $W_n$ inside the training loss function L:\n$L(\\phi_{\\theta,n}, W_n) = \\sum_{1<k<p \\atop i\\in S_k} (W_n(S_k) * l(Y_i, Y_i) + \\Omega(\\theta)$\nIn FAIRDREAM, the correction for fairness is group-wise. Instead of \u201clevelling down\u201d the well off group to make its OPR closer to the worse off, say on sex, FAIRDREAM focuses on enhancing the OPR of women with equal or slightly lower OPR of men as in [Mittelstadt et al., 2023].\nIn our case, however, the selection of optimal classifiers is granted through group-wise reweighting during processing. As wished by [Castro, 2019], FAIRDREAM concretely implements costs of errors (FNR and FPR) proportionally to the structural disadvantage experienced by a group in society (e.g. women who need to be reevaluated on credit allocating, taking into account the disadvantages they face up in education, recruiting, income, etc.).\nHence, FAIRDREAM combines the optimality of \"levelling up\" with the transparency\nTo investigate the proper effects of FAIRDREAM'S correction, we compared it in a benchmark experiment with a closely related fairness method: GRIDSEARCH\n[Agarwal et al., 2018], also an in-processing method of cost-sensitive classification. Described below and synthesized in the tables of Appendix B, the full results of our experiment are accessible in the GitHub benchmark repository: https://anonymous. 4open.science/r/weights_distortion_impact-15/.\nIn GRIDSEARCH, the statistical performance and fairness tasks are split. More precisely, the loss is optimized under the fairness constraint F, e.g. penalizing the model when the overall true positive rates are not equalized (with \u03b7 > 0):\n$\\min_{\\theta} L(\\phi_{\\theta,n}) \\text{ such that } F(\\phi_{\\theta,n}) \\leq \\eta$\nThe classifier $\\phi_{\\theta,n}$ has to find the parameters $\\theta$ which minimize the accuracy loss $L(\\phi_{\\theta,n})$, so long as the fairness constraint F is under the threshold \u03b7. Which amounts to the loss function:\n$L (\\phi_{\\theta, \\lambda}) = \\sum_{i=1}^m l (y_i, Y_i) + \\Omega(\\theta) + \\lambda^T (F (\\phi_{\\theta,\\eta}) - \\eta)$\nWhereas FAIRDREAM directly integrates the fairness objective through the sample weights $W_n$, GRIDSEARCH uses the Lagrange multipliers $\\lambda$ to stress the violation of the fairness constraint. GRIDSEARCH starts with a grid of values $ \\lambda$ as FAIRDREAM with the weights $W_n$, which convey in an accessible way the idea of a classification sensitive to protected individuals or not."}, {"title": "We conducted the experiment over multiple types of models", "content": "We conducted the experiment over multiple types of models. To grant stability of the experiments, we selected the event predicted by the model (earning over $50,000 or not) for the threshold maximizing the F1-score, commonly used in machine-learning for imbalanced classification as in Census.\n\u2022 Gradient boosted trees (using the XGBoost library: 1000 estimators, with the maximal depth of each tree being 3 splits on features) \u2013 to keep on investigating the initial type of model we used in Section 3.\n\u2022 Random forest trees (using the Scikit-learn library: 100 estimators, with the maximal depth of each tree being 3 splits on features) to introduce a lighter tree-based model.\n\u2022 Neural networks (using the PyTorch library to build a sequential model alternating linear layers (14, 1000) \u2192 (1000, 230) \u2192 (230, 2) and ReLU layers to break linearity).\n\u2022 Logistic regression (using the Scikit-learn library, with the \u201cliblinear\" solver, performing approximate minimization along coordinate directions) \u2013 to introduce a simpler model in the benchmark.\nFor each type of model, a baseline model was trained with these default parameters, regardless of fairness objectives. Then to investigate the convergence of FairDream towards Equalized Odds, we analyzed the model through the lens of Demographic Parity, as if Demographic Parity was the initial fairness purpose set by lawmakers. When inequalities of overall selection were more than 3:1 across groups (e.g. eligible men for a loan = 42% vs 11% for women), we started a correction to mitigate gaps created by the model on that feature (e.g. sex)."}, {"title": "With the same default parameters as the baseline model", "content": "With the same default parameters as the baseline model, GRIDSEARCH and FAIRDREAM tested new weights on 10 new models. The goal of the competing models was to simultaneously maximize a global statistical criterion (ROC-AUC) and equalize the overall positive rates across groups (Demographic Parity).\nOur hypothesis was that if we set FAIRDREAM to equalize the groups across a fairness objective which is not conditional on true labels (in this experiment Demographic Parity), FAIRDREAM would nevertheless equalize the groups according to a fairness measure conditioned on true labels (here Equalized Odds).\nAfter the best model of FAIRDREAM and the best model of GRIDSEARCH were selected, we found confirming evidence for our hypothesis (Cf. Appendix B). Let us illustrate this with one example from our experiments. GRIDSEARCH and FAIRDREAM tried to mitigate the gaps in overall positive rates between men and women generated by a baseline random forest (purpose of Demographic Parity). When we observe the new gaps in overall positive rates between the sexes, GRIDSEARCH has enhanced the position of women (now 100% predicted as earning over $ 50,000). This is way more than in FAIRDREAM, which only selects 20% of the women (Cf. Figure 4).\nHowever, the difference in true positive rates between GRIDSEARCH and FAIRDREAM is now a slight one (Cf. Figure 5, left). In FAIRDREAM, the percentage of individuals correctly predicted to earn over $50,000 is now closer across the sexes (83% vs 76%, now slightly promoting women, where this difference is of 18% vs 80% in the baseline model). Likewise, false positive rates between male and female individuals are made closer than in the baseline model by FAIRDREAM (Cf. Figure 5, right: 13% vs 22%, where this difference is of 0.2% vs 24% in the baseline model). These results show that, even though we set FAIRDREAM to respect Demographic Parity, instead it achieved Equalized Odds by equalizing the true and false positives across populations, relative to what the baseline classifier does.\nObviously, the GRIDSEARCH model selects more women as true positives than"}, {"title": "However", "content": "FAIRDREAM does. However, this is at the expense of a drastic loss in statistical performance. To achieve equal positives rates, GRIDSEARCH actually predicted every woman to earn over $50,000, making it alike a random classifier (ROC-AUC = 56%, false positive rate = 100%).\nOn the contrary, FAIRDREAM bridges the gap between the sexes by improving on the accuracy of the baseline classifier (ROC-AUC = 80%). While the overall percentage of positives is not equally high in both sexes, it seems to achieve the maximal positive rate possible, according to the true labels.\nThe correction on the feature \u201csex\u201d with random forest models to equalize the overall positive rates confirmed the tendency we observed across the results of our benchmark. In general, GRIDSEARCH better fulfills the fairness objectives, but at the expense of accuracy. On the contrary, keeping an accuracy comparable to the baseline model, FAIRDREAM generally performs better to equalize the true and false positive rates - even when it is not the fairness objective which was set to be maximized (see Appendix B and the GitHub benchmark repository for detailed visualizations: https://anonymous.4open.science/ r/weights_distortion_impact-15)."}, {"title": "Sample Weights vs Optimization under Constraint", "content": "How can we explain that GRIDSEARCH achieves the fairness goal of Demographic Parity, whereas it is not the case of FAIRDREAM? As explained above, they differ in the way they implement the fairness constraint:\n- In GRIDSEARCH Loss, the fairness objective F ($o,n) is a new term added inside the global loss function (this way of correcting is referred to as an optimization of model's parameters subject to a fairness constraint F [Wan et al., 2023]). The left part of L is a classic loss function, increasing when the error between the prediction \u0177 and the true $Y_i$"}, {"title": "Calibration and Precision", "content": "Equalized Odds is also commonly opposed to various notions of predictive parity, understood either in terms of equal calibration ([Pleiss et al., 2017, Barocas et al., 2023]), or in terms of equal precision relative to a threshold ([Dieterich et al., 2016, Mayson, 2018]). So first, we start with some data relative to FAIRDREAM's performance on calibration, and we explain why, on a normative basis, calibration for us does not constitute a relevant criterion. The real issue, according to us, is whether the achievement of Equalized Odds remains too conservative compared to the achievement of Demographic Parity.\nOne common view of fairness is found in the idea of predictive parity between groups. Predictive parity can mean that the same fraction of true positives is predicted between groups at identical risk scores, or it can mean that the same fraction of true positives is predicted between groups relative to a classification threshold. The first notion corresponds to the statistical notion of calibration of a classifier, whereas the second corresponds to the notion of statistical precision of a classifier. Both notions are sometimes indistinctly referred to under the term \"calibration\" in the literature (see [Mayson, 2018, Long, 2021]), although strictly speaking they differ. Both notions have also been invoked in defense of COMPAS against ProPublica's charges of racial bias. In their response to ProPublica, Northpointe argued that at the cutoff chosen by ProPublica to show Unequal Odds between Whites and Blacks, the statistical precision of the COMPAS algorithm was roughly the same between both groups. Moreover, it was shown that at each risk score, the proportion of rearrest between groups was roughly the same (see [Corbett-Davies et al., 2017, Figure 2]), this time evidencing calibration between groups.\nBoth calibration and precision are central notions in that regard, and one may wonder"}, {"title": "How FAIRDREAM fares with regard to either", "content": "how FAIRDREAM fares with regard to either. Let us consider calibration first. Calibration can be looked at either in a relative, or in an absolute sense ([Eva, 2022]). In a relative sense, it requires that for every possible risk score, the ratios of true positives be equal across groups. That is:\n$p(Y = 1|R = r, A = 1) = p(Y = 1|R = r, B = 1)$\nWe select this relative sense as it is the one commonly adopted to link fairness and calibration (see [Corbett-Davies et al., 2017, Hedden, 2021, Barocas et al., 2023]). This sense of calibration has been called \u201ccalibration within groups\" by [Kleinberg et al., 2016]. Here we simply call it \"Equal Calibration\", to indicate that the measure is comparative. A distinct, non-relative sense of calibration, requires that the method used to classify be itself well-calibrated within a given group this time, that is, that for each possible risk score r, the ratio of people classified as positive (Y=1) be equal to r, or sufficiently close to r. This is a stronger requirement, for a method may fail to be well-calibrated in that absolute sense, but still be such that it selects the same percentage of true cases in different groups.\nIt is a fact that in general, achieving Equalized Odds implies sacrificing Equal Calibration (see [Kleinberg et al., 2016],[Barocas et al., 2023]). The case of the random forest models used above illustrates this trade-off. When we compare and sum the scores (here, probabilities of getting Y = 1), the calibration gap between groups is increased by FAIRDREAM compared to the Baseline model. The area between calibration curves, approximated through a trapezoidal rule, is larger for FAIRDREAM than for the initial model (0.2 versus 0.09, Cf Figure 6). That FAIRDREAM favors Equalized Odds at the expense of Equal Calibration is also confirmed in the other cases. Out of 16 features where correction happened, FAIRDREAM reached the highest gaps between groups in 11 calibration curves (Cf. Table 3.)\nFigure 6 also tells us something about absolute calibration, so on the positions of"}, {"title": "In summary", "content": "curves relative to the ideal calibration curve x = y. In the Baseline model, both curves are below ideal calibration: $P(\u0176 = 1)$ under-estimates the real percentage of women and men which truly earn over $50,000. Whereas FAIRDREAM reverses this tendency: individuals of both groups are over-estimated by the model. However, while switching from under- to over-estimation, FAIRDREAM produces a better calibrated curve for women than men, this time judging by how much the female group's curve departs from absolute calibration. That is, while FAIRDREAM undeniably widens the gaps between women and men to fulfill Equalized Odds, the situation of women is enhanced not only on true positive rates, but even on the calibration picture. Overall, therefore, neither the base model nor FAIRDREAM is well calibrated in an absolute sense, but they err in opposite ways. Moreover, while the calibration gap is increased with FAIRDREAM, this is a case where absolute calibration in the discriminated group is improved in FAIRDREAM.\nWe do not see Equal Calibration to be a fundamental fairness constraint, however, and we do not see it on a par with Equalized Odds either. Indeed, like Demographic Parity, Equalized Odds supposes that a threshold has already been applied to the scores. The decision-maker is only able to act once this threshold has been set.\nYet, as we see in Figure 6, calibration compares the model's properties along all scores, before any threshold has been set. As argued by [Corbett-Davies et al., 2017], moreover, one can find cases of Equal Calibration between groups for which a decision threshold is nonetheless prone to induce a differential treatment between groups, if the two groups' risk scores overlap on only part of the scale.\nA way to take into account decision thresholds is to compare Equalized Odds with a constraint of Equal Precision between groups, relative to the same threshold, namely:\n$p(Y = 1|\u0176 = 1, A = 1) = p(Y = 1|\u0176 = 1, B = 1)$\nPrecision, namely the rate of true positives among all predicted positives, is a metric that Northpointe opposed to COMPAS in their rejoinder ([Dieterich et al., 2016]). More generally, [Long, 2021] has argued that Equal Precision ought to stand as a necessary condition on fairness. However, we find it hard to endorse this generalization for all cases. For instance, to use an example of the same kind used by Long, we can come up with cases in which Precision is not equal between two groups, but Sensitivity (= True Positive Rate, aka. Recall) and Selectivity (=True Negative Rate) are identical, and for which we do not have the intuition that there is an unfair treatment.\nAn example is given in Table 4. Group 1 has 28 students, and Group 2 has 40 students, comprised of students deserving a High Grade and students deserving a Low Grade (where we assume these qualities to be objectively measurable). In both groups, the True Positive Rate and the True Negative Rate are the same: Group 1 and Group 2 have the same absolute number of true High Grade students, but in Group 2 we get more Low grade students wrongly classified as \"High\" than in Group 1. The Precision on the \"High\" label is only 1/7 compared to 1/5 in the first group. Since Selectivity and Sensitivity are identical in this example, this is, however, a case in which the criterion of Equalized Odds is satisfied.\nThe upshot is that Equal Precision is neither a necessary, nor a sufficient condition on fairness. In the case we just presented, one might respond that Precision in the \"Low\" judgments is not constant across the groups, even as precision on the \u201cHigh\u201d judgments is. But the problem is that to ask for Equal Precision on both labels is tantamount to asking for Equalized Odds, which would undercut Long's argument. This is not to say that Precision does not matter to FAIRDREAM. As explained in the previous section (see fn. 12), we conducted our benchmark so as to maximize the trade-off between Precision and Recall. From Table 3, we also see that the gap in PR-AUC diminishes in FAIRDREAM across groups compared to the baseline. But since this is not a criterion we set a priori, no more than ROC-AUC, we do not build any conclusion on this fact."}, {"title": "Fairness from Accuracy", "content": "In summary, therefore, whether on a descriptive basis, or on a normative basis, Equal Calibration does not end up as a decisive criterion in our evaluation of FAIRDREAM'S correction, and neither does Equal Precision. In what follows, therefore, we return to the opposition between Equalized Odds and Demographic Parity as opposite requirements.\nIn our benchmark comparison, each time the GRIDSEARCH models outperformed FAIRDREAM with regard to Demographic Parity, it was at the cost of a worse statistical performance. On the contrary, FAIRDREAM gives a truthful picture of the ground labels: the FAIRDREAM correction method performs better in terms of accuracy, with a low rate of false positives (less than 5%) and a high rate of true positives (nearly 90%).\nThe question raised by this situation is whether increasing statistical accuracy, namely Selectivity and Sensitivity, is automatically a way of increasing fairness. Several arguments can be given against the sufficiency of accuracy to achieve fairness. The main one is that even the most accurate classification may simply replicate imbalances that are in the data ahead of the algorithm's workings, as a result of social biases or social injustice.\nWe agree with this argument, and we grant that improving on descriptive accuracy may not be sufficient to achieve normative fairness (in line with Hume's classic remarks on the is-ought distinction [Hume, 1739]). However, we consider that descriptive accuracy in predictions should at least be viewed as a necessary condition on fairness [Wachter et al., 2020], and that it may even come out necessary and sufficient in cases in which the predictions themselves concern matters of fact.\nIn matters of fact, the case for conditional fairness metrics can be buttressed by general epistemological considerations. A user trying to narrow the gap between overall positive rates made on age groups by our FAIRDREAM classifier (16% for 17-29 years old"}, {"title": "The situation is reminiscent here of the debate between ProPublica and Northpointe", "content": "and 59% for 29-37 years old", "base rate neglect\\\" ([Tversky et al., 1982": ".", "2023": "blindly enforcing Demographic Parity by increasing the false positive rate in the younger group is also susceptible to lead to harmful consequences. Ceteris paribus, it can put the younger group at higher risk of defaulting their loan, assuming income is causally the main variable to sustain loan refunding, and that the algorithm's prediction on income is critical in deciding whether or not to grant credit.\nThat being said, the method's emphasis on Equalized Odds is not as conservative as it might seem. In particular, it may be objected that if Sensitivity and Specificity are key here, then they should be plotted for each decision threshold, which is what ROC-AUC represents. But as Table 3 indicates, in the benchmark study the gap between ROC-AUC is increased on average compared to the baseline (whereas for"}]}