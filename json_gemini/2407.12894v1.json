{"title": "Maintenance Strategies for Sewer Pipes with Multi-State Degradation and Deep Reinforcement Learning", "authors": ["Lisandro A. Jimenez-Roa", "Thiago D. Sim\u00e3o", "Zaharah Bukhsh", "Tiedo Tinga", "Hajo Molegraaf", "Nils Jansen", "Mari\u00eblle Stoelinga"], "abstract": "Large-scale infrastructure systems are crucial for societal welfare, and their effective management requires strategic forecasting and intervention methods that account for various complexities. Our study addresses two challenges within the Prognostics and Health Management (PHM) framework applied to sewer assets: modeling pipe degradation across severity levels and developing effective maintenance policies. We employ Multi-State Degradation Models (MSDM) to represent the stochastic degradation process in sewer pipes and use Deep Reinforcement Learning (DRL) to devise maintenance strategies. A case study of a Dutch sewer network exemplifies our methodology. Our findings demonstrate the model's effectiveness in generating intelligent, cost-saving maintenance strategies that surpass heuristics. It adapts its management strategy based on the pipe's age, opting for a passive approach for newer pipes and transitioning to active strategies for older ones to prevent failures and reduce costs. This research highlights DRL's potential in optimizing maintenance policies. Future research will aim improve the model by incorporating partial observability, exploring various reinforcement learning algorithms, and extending this methodology to comprehensive infrastructure management.", "sections": [{"title": "1. INTRODUCTION", "content": "Sewer network systems, crucial for public health, population well-being, and environmental protection, require maintenance to ensure their reliability and availability (Cardoso et al., 2016). This maintenance is challenged by limited budgets, environmental changes, aging infrastructure, and hard-to-predict system deterioration (Tscheikner-Gratl et al., 2019).\nOptimizing maintenance policies for sewer networks requires methodologies that can efficiently explore a broad solution space while adapting to the system's dynamic constraints and complexities. Maintenance Policy Optmization (MPO) addresses these needs by developing and analyzing mathematical models to derive maintenance strategies (De Jonge & Scarf, 2020) that reduce maintenance costs, extend asset life, maximize availability, and ensure workplace safety (Ogunfowora & Najjaran, 2023).\nThis research explores the potential of Deep Reinforcement Learning (DRL) for MPO of sewer networks, first focusing on a component-level (i.e., pipe-level) analysis. DRL is a framework that merges neural network representation learning capabilities with Reinforcement Learning (RL), a branch of machine learning known for its effectiveness in sequential decision-making problems. RL is increasingly recognized for its role in developing cost-effective policies in MPO across diverse domains such as transportation, manufacturing, civil infrastructure and energy systems. It is emerging as a prominent paradigm in the search for optimal maintenance policies (Marug\u00e1n, 2023).\nThis paper aims to achieve two primary objectives: first, to present a comprehensive model for pipe-level MPO analysis facilitated by DRL, considering degradation over the pipe length and employing inhomogeneous-time Markov chain models to simulate the nonlinear stochastic behavior associated with sewer pipe degradation; second, to assess the efficacy of the model's policy through a case study of a large-scale sewer network in the Netherlands, comparing it with heuristics, including condition-based, scheduled, and reactive maintenance.\nWe acknowledge as limitations in our approach the focus on fully observable state spaces, which means that inspection actions are not necessary, and our analysis is at the component-level. Future research will aim to broaden this scope to include partially observable state spaces and system-level analysis.\nContributions. This work's primary contributions include:\n(i) We propose a framework to carry out maintenance policy optimization for sewer pipes considering the deterioration along the pipe length. This framework integrates Multi-State Degradation Models (MSDMs) and Deep Reinforcement Learning (DRL).\n(ii) Our framework introduces a novel approach by encoding the prediction of the MSDM into the state space, aiming to harness prognostics that describe the degradation pattern of sewer pipes.\n(iii) We demonstrate that DRL has the potential to devise intelligent strategic maintenance strategies adaptable to various conditions, such as pipe age.\n(iv) We provide our framework in Python and all data used in this study at zenodo.org/records/11258904.\nPaper outline. Section 2 presents the technical background. Section 3 outlines our research methodology. Section 4 formulates the MSDM. Section 5 details the framework for maintenance policy optimization via DRL. Section 6 presents our experimental setup. Section 7 analyzes the results. Section 8 discusses findings, concludes, and suggests future research.\nRelated work. In the past two decades, the need for integral sewer asset management has become evident (Abraham et al., 1998), emphasizing the necessity to understand the mechanisms of deterioration and develop predictive models for proactive and strategic sewer maintenance (Fenner, 2000). Sewer asset management encompasses maintenance, rehabilitation, and inspection and has been investigated through various methodologies, including risk-based strategies (Lee et al., 2021), multi-objective optimization (Elmasry et al., 2019), Markov Decision Processes (Wirahadikusumah & Abraham, 2003), considering the structure of the sewer network (Qasem & Jamil, 2021), machine learning applications (Montserrat et al., 2015; Caradot et al., 2018; Laakso et al., 2019; Hern\u00e1ndez et al., 2021), and decision support frameworks (Taillandier et al., 2020; Khurelbaatar et al., 2021; Ramos-Salgado et al., 2022; Assaf & Assaad, 2023).\nThe integration of RL into sewer asset management is largely unexplored, with existing research mainly concentrating on real-time control for smart infrastructure, adapting to environmental changes such as storms. Mullapudi et al. (2020) uses DRL for controlling storm water system valves through simulation of varied storm scenarios. Yin et al. (2023) employ RL for near real-time control to minimize sewer overflows. Meanwhile, Zhang et al. (2023) and Tian et al. (2022) both examine improving the robustness of urban drainage systems, the former through decentralized multi-agent RL and the latter through Multi-RL, with Tian et al. (2024) further improving the model interpretability using DRL. Furthermore, Kerkkamp et al. (2022) investigates the sewer network MPO by combining DRL with Graphical Neural Networks to optimize maintenance actions grouping. Jeung et al. (2023) proposes a DRL-based data assimilation methodology to enhance storm water and water quality simulation accuracy by integrating observational data with simulation outcomes."}, {"title": "2. TECHNICAL BACKGROUND", "content": "2.1. Multi-state degradation model for sewer pipes\nThe modeling of sewer pipe network degradation has been explored through various methodologies, including physics-based, machine learning, and probabilistic models. For comprehensive discussions on this topic, the reader is directed to Ana & Bauwens (2010); Hawari et al. (2017); Malek Mohammadi et al. (2019); Saddiqi et al. (2023); Zeng et al. (2023).\nWe adopt a probabilistic approach employing Inhomogeneous Time Markov Chains (IHTMCs) to model the multi-state degradation of sewer pipes. This choice is motivated by the IHTMC's capability to better capture the degradation of long-lived assets such as sewer systems as a non-linear stochastic process, characterized by age-dependent transition probabilities between degradation states (Jimenez-Roa et al., 2024).\nInhomogeneous Time Markov Chains (IHTMCs). An IHTMC is a stochastic process {(Xt)}t>0, where t \u2208 [0,\u221e) is continues and models time. The IHTMC is defined as a tuple M = (\u03a9, S\u00ba, Q(t)), where \u03a9 is a set of K finite states indicating the state space, S is an initial-state distribution on \u03a9 where \u03a3\u03ba\u03b5\u03c0 S = 1, and Q(t) : \u03a9 \u00d7 \u03a9 \u2192 R is a time-dependent transition rate matrix, with entries qij (t) for i, j \u2208 N and i \u2260 j, representing the rate of transitioning from state i to state j at time t. The diagonal entries qii(t) are defined such that the sum of each row in Q(t) is zero, ensuring that the outflow from any state is equal to the sum of the inflows into other states. Q(t) may be parameterized by hazard rates \u03bb(t|\u0398) derived from the ratio f(t|\u0398) and S(t|\u0398), being respectively a probability density function and a survival function, where \u0398 corresponds to the function hyper-parameters. The evolution over time of the IHTMC is governed by the Forward Kolmogorov equation:\n$\\frac{\\partial p_{ij}(t, \\tau)}{\\partial t} = \\sum_{k \\in \\Omega} p_{ik}(t, \\tau) Q_{kj}(t)$ (1)\nHere, Pij (t, \u03c4) : \u03a9 \u00d7 \u03a9 \u2192 [0, 1] is a continuous and differentiable function known as the transition probability matrix, indicating the probability of transitioning from state i to state j in the time interval t to r, where \u03c4 > t. From Eq. (1) one can obtain the master equation of the Markov chain, which models the flow of probabilities between states by including inflow and outflow terms:\n$\\frac{dS_k(t)}{\\partial t} = \\sum_{i \\in \\Omega, i \\neq k} S_i(t) Q_{ik}(t) - S_k(t) (\\sum_{j \\in \\Omega, j \\neq k} Q_{kj}(t))$ (2)\nHere, Sk (t) is the probability of being in state k \u2208 \u03a9 at time t, the term \u03a3j\u2208\u03a9,j\u2260k Qkj (t) represents the rates of transition from state k to all the other states j (excluding self-transitions).\nPipe-element degradation model. We define a pipe element by K sequentially arranged states S = [S1, S2, ..., Sk], where S\u2081 signifies the pristine condition and Sk represents the worst condition. This categorization is based on sewer network inspection data, which documents types of damage and their severities on a scale from 1 to 5, along with occasional instances of functional failures (K = 6). The transitions within our IHTMC, illustrated in Figure 1, permit only progression from a better to a worse state, prohibiting direct improvements without repairs, while allowing any severity level to escalate to functional failure."}, {"title": "Parametrization of IHTMC.", "content": "We employed a parameterized approach for IHTMC, involving an assumption on the hazard function. In Section 4.2, we detail the parametrization used in our experimental setup. Several aspects related to the multi-state degradation model, including hyper-parameter tuning and interval-censoring, are beyond the scope of this paper. For further information, we recommend referring to (Jimenez-Roa et al., 2024).\n2.2. Markov Decision Process\nA Markov Decision Process (MDP) models a stochastic sequential decision process, where both costs and transition functions are dependent solely on the current state and action (Puterman, 1990). Formally, an MDP is described by the tuple (S, A, P(St+1|St, at), R(st, at, St+1), \u03c0\u03bf, \u03b3), with S as state space, A as the action space, P(St+1|St, at) as the transition probability function indicating the probability of transitioning from state st to st+1 given action at, where St, St+1 \u2208 S and at \u2208 A. The reward function R(st, at, St+1) specifies the reward for moving from st to st+1 by action at. The initial state \u03c0\u03bf represents the distribution across S, and \u03b3\u2208 [0,1] is the discount factor that balances immediate versus future rewards.\n2.3. Deep Reinforcement Learning\nDeep Reinforcement Learning (DRL) produces virtual agents that interact with environments to learn optimal behaviors through trial and error, as indicated by a reward signal (Arulkumaran et al., 2017). DRL has found applications in robotics, video games, and navigation systems.\nWe utilize DRL to train agents in virtual environments exhibiting degradation following the MSDM pattern, as detailed in Section 5. Specifically, we apply Proximal Policy Optimization (PPO) (Schulman et al., 2017), a policy gradient method in RL.\nPPO aims to optimize the policy an agent uses for action selection, maximizing expected returns. It addresses stability and efficiency issues encountered in previous algorithms like Trust Region Policy Optimization by offering a simpler and less computationally expensive method to ensure minor policy updates.\nThis is achieved through an innovative objective function that penalizes significant deviations from the previous policy, fostering stable and consistent learning. The term \u201cproximal\u201d denotes maintaining proximity between the new and old policies, facilitating a stable training process and rendering PPO popular across various RL applications."}, {"title": "3. METHODOLOGY", "content": "Our methodology, illustrated in Figure 2, comprises six steps, detailed below.\nStep 1. Perform data handling of historical inspection records, selecting subsets (cohorts) of interest, and calibrating the MSDM on this data. This step is beyond the scope of this paper; for details, see Jimenez-Roa et al. (2022, 2024). The results of this step are given in Section 4.\nStep 2. After calibrating the MSDM, integrate these models into an environment suitable for RL applications. We present the details of our environment integrating MSDM in Section 5. In addition, we define environments for training RL agents. This is to test different MSDM hypotheses; details on this can be found in Section 6.\nStep 3. Train DRL agents with PPO. Use optuna for hyper-parameter tuning and Stable Baselines3 for RL implementation. Details are in Section 7.1.\nStep 4. Train and select the RL agents with the optimal hyper-parameters on the training environments. In essence, these agents learn the dynamics described by the MSDM encoded in the environment.\nStep 5. Compare the maintenance policies advised by the RL agents using the test environment against the heuristics: Condition-Based Maintenance (CBM), Scheduled Maintenance (SchM), and Reactive Maintenance (RM). Find the definition of these heuristics in Section 6.2.\nStep 6. Analyze and compare the behavior of the maintenance strategies for the different RL models and heuristics. Reflect on the policies advantages and disadvantages. Find in Section 7.2 the overview of this comparison, and in Section 7.3 are the details along episodes."}, {"title": "4. MULTI-STATE DEGRADATION MODELS", "content": "4.1. Case study\nOur case study conducts a detailed examination of the sewer pipe network in Breda, the Netherlands, which comprises 25,727 sewer pipes covering 1,052 km, mostly built after 1950. The network is primarily made of concrete (72%) and PVC (27%), with the shapes of the pipes being predominantly round (95%) and ovoid (5.4%). These pipes are designed for transportation (98.2%), with 88% being up to 60 meters in length. Additionally, 98.3% have a diameter of up to 1 meter, with the most common diameter being 0.2 meters, and they carry mixed (63%), rain (21%), and waste (16%) contents. The condition of the pipes is evaluated through visual inspections according to the European standard EN 13508 (\u00c9N13508, 2012; EN13508-2, 2011), focusing on identifying and classifying damage with specific codes. This study specifically addresses the damage code BAF, which signifies surface damage and was observed in 35.3% of the inspections.\n4.2. Parametrization\nWe consider three distributions for hazard rate functions: Exponential, Gompertz, and Weibull. The hazard rates \u03bb(t) for these distributions are specified as follows:\nExponential function: $\u03bb^E(t|\u03b5) = \u03b5$ (3a)\nGompertz function: $\u03bb^G(t|\u03b1, \u03b2) = \u03b1\u03b2e^{\u03b2t}$ (3b)\nWeibull function: $\u03bb^W(t|\u03b7, \u03c1) = \\frac{\u03c1}{\u03b7} (\\frac{t}{\u03b7})^{\u03c1-1}$ (3c)\nIn Eq. (3a), a constant hazard rate indicates that the degradation model assumes a homogeneous time, exhibiting memory-less properties. Eq. (3b) and Eq. (3c) present varying hazard rates, which indicates inhomogeneous time.\n4.3. Solving the Multi-State Degradation Model\nIn Figure 1, we defined the structure of the Markov chain to model degradation in a sewer pipe, and in Section 4.2 we introduced the hazard rate functions. In the following, we present the corresponding system of differential equations.\n$\\frac{dS_1(t)}{dt} = - (\u03bb_{12}(t) + \u03bb_{1F}(t))S_1(t)$ (4a)\n$\\frac{dS_2(t)}{dt} = \u03bb_{12}(t)S_1(t) \u2013 (\u03bb_{23}(t|\u00b7) + \u03bb_{2F}(t|\u00b7))S_2(t)$ (4b)\n$\\frac{dS_3(t)}{dt} = \u03bb_{23}(t)S_2(t) \u2212 (\u03bb_{34}(t|\u00b7) + \u03bb_{3F}(t|\u00b7)) S_3(t)$ (4c)\n$\\frac{dS_4(t)}{dt} = \u03bb_{34}(t)S_3(t) + ( \u2212 \u03bb_{45}(t|\u00b7) \u2013 \u03bb_{4F}(t|\u00b7)) S_4(t)$ (4d)\n$\\frac{dS_5(t)}{dt} = \u03bb_{45}(t)S_4(t) \u2013 \u03bb_{5F}(t.) S_5(t)$ (4e)\n$\\frac{dS_F(t)}{dt} = \u03bb_{1F}(t|\u00b7)S_1(t) + \u03bb_{2F}(t|\u00b7)S_2(t) + \u03bb_{3F}(t|\u00b7)S_3(t) + \u03bb_{4F}(t)S_4(t) + \u03bb_{5F}(t.) S_5(t)$ (4f)\nEq. 4 is solved using numerical methods, specifically the LSODA algorithm from the FORTRAN odepack library implemented in SciPy (Jones et al., 2001\u2013). This algorithm solves systems of ordinary differential equations by employing the Adams/BDF method with automatic stiffness detection.\n4.4. Parametric Multi-State Degradation Models\nWe extract a subset from our case study data set to construct a cohort with concrete sewer pipes carrying mixed and waste content (cohort CMW), representing 37.1% of the sewer network. The model parameters for this cohort are detailed in Appendix A in Tables 7 and 8.\nFigure 3 illustrates the MSDMs predictions, detailing the stochastic dynamics of sewer pipe degradation for pipes in cohort CMW. As Figure 1 describes, this degradation is segmented into five sequentially ordered severity levels (k = 1 to k = 5), plus a functional failure state (k = F). Differences in the y-axis scales are intentional, to emphasize details and behaviors that various degradation models express across severity levels.\nGray circles represent the frequency per severity level from the inspection dataset. Jimenez-Roa et al. (2022) details how these frequencies are computed. Vertical black lines in Figure 3 mark the last available data point for each severity level.\nAdditionally, Figure 3 presents the Turnbull non-parametric estimator, which assumes no specific distribution for survival times (Turnbull, 1976). In our context, this estimator represents the ground truth of stochastic degradation behavior in sewer pipes."}, {"title": "5. DEFINITION OF MARKOV DECISION PROCESS FOR \u039c\u0391\u0399\u039d\u03a4\u0395\u039d\u0391NCE POLICY OPTIMIZATION OF A SEWER PIPE CONSIDERING PIPE LENGTH DEGRADATION", "content": "Figure 4 provides the workflow that the RL agent uses to learn maintenance policies for sewer pipes, considering degradation along the pipe length. In the following sections, we provide the details of the environment, namely the state and action spaces, as well as the transition probability and reward functions.\n5.1. State space (S)\nOur approach focuses on developing age-based maintenance policies, incorporating the sewer pipe's age into the state representation. Our state space is continuous and it is structured to include three key components: (i) the age of the pipe, (ii) the health vector, and (iii) the stochastic prediction of severity levels. We next describe the last two components.\n5.1.1. Health vector (h)\nIn modeling the degradation of linear structures like sewer pipes, it is essential to represent changes accurately along their length. For this purpose, we define a health vector (h), which quantitatively measures the degradation at various points along the pipe. The vector is crucial in our framework, particularly influencing the reward function as described in Section 5.4.\nConstruction of h: We discretize the pipe into segments of equal length AL, with \u2206L < L, where L is the total length of the pipe. The number of segments, na, is calculated using the ceiling function to ensure it remains an integer even if L is not perfectly divisible by AL:\n$n_d = \\lceil \\frac{L}{\u0394L} \\rceil$ (5)\nEach segment's degradation level is initially assessed and categorized into severity levels according to the MSDM. As the degradation progresses, the state of each segment changes following the transition probabilities described by the matrix Pi,j, where i is the current severity level, and j is the subsequent severity level, as described by the forward Kolmogorov equation (Eq. 1).\nNotice that by doing this, we assume there is no statistical dependency between segments, which is a strong assumption that needs further research. However, for simplicity, we maintain this assumption in our degradation model.\nQuantifying Degradation: The distribution of severity levels across the pipe is captured in vector d, with each element indicating the severity level of a segment. To quantify this distribution in the health vector h, we first count the number of segments at each severity level k using the following expression:\n$n_{dk} = \\sum_{i=1}^{n_d} 1\\{d_i=k\\}$ (6)\nwhere 1 is the indicator function that is 1 if the condition is true and 0 otherwise. The health vector h is then determined by normalizing these counts to reflect the proportion of segments at each severity level:\n$h_k = \\frac{n_{dk}}{n_d}$ (7)\nHere, ndk is the number of segments at severity level k. Thus, hk becomes part of the state space indicating the level of degradation present in the pipe.\n5.1.2. Stochastic prediction of severity levels\nTo enable the agent to access information provided by the MSDM, we incorporate the prediction of severity levels into the state space. This is accomplished by solving Eq. 2, yielding a distribution Sk(t).\nFinally, our state space is defined as a tuple with 13 elements:\nS = (Pipe Age, h1, h2, h3, h4, h5, hF, S1, S2, S3, S4, S5, SF)\n5.2. Action space (A)\nOur action space A is discrete with dimensionality |A| = 3. At each time step t, the agent selects an action at. If the decision at time t is do nothing, at is set to 0. To perform maintenance, at is set to 1, and to replace the pipe, at is set to 2. The outcomes of these actions are discussed in Section 5.3.\n5.3. Transition function (P)\nOur transition function P(st+1|St, at) is stochastic, dependent on time t, and considers both the actions a \u2208 A and the current st and next state st+1 dynamics described by the MSDM. We illustrate the behavior of P with the following example.\nFor a 30-year-old pipe with length L = 40 meters and discretized in segments of length \u2206L = 1, let the current state space be st=30 \u2208 S:\nSt=30 = (30, 0.60, 0.35, 0.025, 0.025, 0.0, 0.0, 0.475, 0.436, 0.069, 0.010, 0.005, 0.005) .\nSt=30 indicates the age of the pipe is 30 years. From Eq. 7, the number of segments at severity k is determined by multiplying the health vector (hk):\nhk = [0.60, 0.35, 0.025, 0.025, 0.0, 0.0]\nby 40 meters, yielding ndk = [24, 14, 1, 1, 0, 0], indicating that, out of the 40 meters of pipe length, 24 segments of 1 meter are at severity k = 1, 14 at severity k = 2, and so forth.\nThe distribution Sk (t = 30.0) predicts the probability of being in a severity level k at age t = 30. This is achieved by evaluating t = 30.0 in the corresponding MSDM.\nSk(t = 30.0) = [0.475, 0.436, 0.069, 0.010, 0.005, 0.005]\nAssuming the agent takes an action every half year, we illustrate the effect of each action in A below.\nIf at = 0: the agent decides to \u201cdo nothing", "1": "the agent decides to \u201cperform maintenance,", "2": "the agent decides to", "replace": "he pipe, resetting its condition to as good-as-new. The new state space is $St=0.0^{a=2}$.\n$St=0.0^{a=2} = (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.986, 0.014, 0.0, 0.0, 0.0, 0.0)$.\nThe pipe age is reset to 0.0, with ndk = [40, 0, 0, 0, 0, 0], and Sk(t) is updated for t = 0.0.\n5.4. Reward function (R)\nOur reward function R(st, at, St+1) assigns a reward rt at every decision point t, determined by the current state st and action at. This function integrates the costs of maintenance (CM), replacement (CR), and failures (CF). R is sparse because it issues a non-zero value only when failures occur or interventions are undertaken.\nMaintenance cost CM is calculated as per Eq. 8, where it combines a variable cost based on severity k with a fixed logistic cost of \u20ac500, covering the expenses related to maintenance."}, {"title": "6. EXPERIMENTAL SETUP", "content": "These costs vary with the severity level k, as detailed in Table 2. Note that no maintenance costs are associated with k = F because maintenance cannot be performed on a segment that has already failed. In this case, the agent must replace.\n$C_M = - \\sum_{k=1}^{k=5} (h_k \\cdot c_k + 500)$ (8)\n$C_R = -(450 + 0.66D + 0.0008D^2)L$ (9)\nHere, L and D denote the pipe's length in meters and diameter in millimetres, respectively. CR is in Euros (\u20ac).\nThe cost of failure, denoted by CF, entails assigning a substantial penalty when the agent allows a segment of the pipe to achieve a failure state (k = F). This penalty cost is established at \u20ac-100,000. Our reward function is then:\n$r_t = \\frac{C_M + C_R + C_F}{100'000 + 900 \\times 40} = \\frac{C_M + C_R + C_F}{136'000}$ (10)\nwhere rt represents the reward obtained at time t, the normalization constant 136\u2032000 corresponds to the most expensive penalty possible at time t. Thus, rt is defined within the interval [-1, 0]. This reward function aims for the agent to balance maintenance actions with the prevention of undesirable pipe conditions.\n6. EXPERIMENTAL SETUP\n6.1. Setup\nWe will evaluate our framework with a single pipe of constant length (40 meters) and diameter (200 mm) from the cohort CMW, which carries mixed and waste content. Given the constant dimensions, the replacement cost CR, as defined in Eq. 9, is \u20ac24,560. The pipe age, when initializing the episode, is randomly sampled from the uniform distribution U ~ [0,50], allowing the agent to learn the behavior of pipes within this age range. Additionally, we evaluate the policy in steps of half a year and AL = 1 meter.\nIn the methodology section, we describe the training of two agents: Agent-E and Agent-G. Agent-E is trained in an environment where sewer pipe degradation follows the MSDM parameterised with an Exponential probability density function, while Agent-G is trained in an environment where degradation follows the MSDM parameterised with a Gompertz probability density function.\nBoth agents are tested in an environment where sewer pipe degradation follows the MSDM parameterized with the Weibull probability density function.\nDuring training, each agent follows a specific state space, defined as follows:\n$S^{Training}_{Agent-E} = (Pipe \\ Age, h, S^E(t))$ (11a)\n$S^{Training}_{Agent-G} = (Pipe \\ Age, h, S^G(t))$ (11b)\nHere, S represents the state space for each agent during training. The subscripts E and G denote the Exponential and Gompertz probability density functions, respectively. Each agent's objective is to learn an optimal maintenance strategy based on their environment's dynamics.\nFor testing, both agents are evaluated in the same environment, with the state space defined as follows:\n$S^{Testing}_{Agent-E} = (Pipe \\ Age, h, S^E(t))$ (12a)\n$S^{Testing}_{Agent-G} = (Pipe \\ Age, h, S^F(t))$ (12b)\nIn both cases, SE (t) and SF(t) remain consistent with the training phase, reflecting the MSDM predictions. However, the health vector hk follows the degradation behavior described by the Weibull probability density function, indicated by the subscript W.\n6.2. Comparison of maintenance strategies\nWe compare the RL agent's performance against maintenance policies based on heuristics. For this, we define the following:\n\u2022 Condition-Based Maintenance (CBM): Maintenance actions are based on the sewer pipe's condition. Specifically, replacement (at = 2) is performed if pipe_age \u2265 70 or hk=F\u2265 0.0; maintenance (at = 1) is conducted if hk=4 \u2265 0.1 or hk=5 \u2265 0.05; otherwise, no action (at = 0) is taken.\n\u2022 Scheduled Maintenance (SchM): Actions are time-based. Replacement (at = 2) is executed if hk=F \u2265 0.0; maintenance (at = 1) occurs every 10 years; otherwise, no action (at = 0) is taken.\n\u2022 Reactive Maintenance (RM): Replacement is undertaken only upon pipe failure, i.e., replacement (at = 2) is performed if hk=F \u2265 0.0; otherwise, no action (at = 0) is taken.\nNote that CBM and SchM are defined based on plausible values. However, these heuristics can be further calibrated for enhanced performance, which is beyond the scope of this paper."}, {"title": "7. RESULTS", "content": "7.1. Implementation and hyper-parameter tuning\nOur framework uses Stable Baselines 3 (Raffin et al., 2021), comprising robust implementations of RL algorithms in PyTorch (Ansel et al., 2024). Specifically, we utilize the PPO algorithm. Hyper-parameter optimization is performed using optuna (Akiba et al., 2019), a framework dedicated to automating the optimization of hyper-parameters.\nThe search space encompasses: exponentially-decaying learning rate with a decay rate of 0.05, with an initial learning rate ranging from 10-5 to 10-2, discount factor (\u03b3) from 0.8 to 0.9999, entropy coefficient from 0.0001 to 0.01, steps per update (n_steps) from 250 to 3000, batch sizes from 16 to 256, activation functions (\u2018tanh', 'relu', 'sigmoid'), policy network architectures ([16, 16], [32, 32], [64, 64], [32, 32, 32]), and training epochs (n_epochs) from 5 to 100.\nWe set up optuna to conduct 500 trials, aiming to maximise cumulative reward in 100 episodes. Table 3 details the optimal hyper-parameters identified. These parameters are used to obtain the results discussed in Sections 7.2 and 7.3, where our agents are trained over a total of 5 million time steps.\n7.2. Policy analysis: overview\nThis section offers a broad evaluation of the policies, with a detailed analysis over episodes presented in Section 7.3. We compare the agents' performances with the heuristics detailed in Section 6.2 across 100 simulations in the test environment (Eq. 12), considering pipe ages of 0, 25, and 50 years, aiming to evaluate policy efficacy concerningdegradation over varying pipe ages.\nFrom these results, we observe that Agent-G's policy generally outperforms others for pipe ages of 0 and 25 years, securing a second-best position for pipes aged 50 years. It is noted that the cost of all policies increases with pipe age, which aligns with expectations as older pipes require more interventions.\nAfter reviewing the mean policy costs, our focus shifts to the specific actions involved in each policy. Table 5 provides a summary of the actions executed by each policy across simulations for different pipe ages. For new pipes, the SchM policy leads in maintenance activities (at = 1), with Agent-G following. In terms of replacements (at = 2), Agent-E is the foremost in implementing this action, with CMB in second place. Both Agent-G and SchM exhibit lower replacement frequencies, explaining the mean policy costs since maintenance actions incur lower expenses compared to the penalties and replacement costs resulting from pipe failures.\nFor pipes aged 25 years, Agent-G executes more maintenance actions (at = 1), similar to SchM. Agent-E opts for no maintenance, aligning more with RM's strategy. Although CBM carries out some maintenance actions, replacement actions predominate, indicating a greater tendency to permit pipe failures, which explains the observed differences in mean policy costs.\nFor pipes aged 50 years, CMB offers the most cost-effective policy, with Agent-G's following. CMB conducts fewer maintenance actions and more replacements than Agent-G, accounting for the cost disparity. The policies of Agent-E, RM, and SchM have similar costs. Despite SchM conducting more maintenance, its high number of replacements suggests the maintenance interval requires adjustment. These results indicate that the strategies of CBM, SchM, and RM are less efficient for older pipes due to their higher failure probability.\nRegarding the mean pipe severity level to assess the impact of various policies on pipe degradation, as shown in Table 6. Our analysis reveals a notable correlation between the average actions per policy, detailed in Table 5, and the mean pipe severity level. Specifically, the Agent-G control strategy tends to maintain pipes within a severity level of k \u2208 [1, 2, 3], whereas the Agent-E, CBM, SchM, and RM policies often result in higher severity levels k \u2208 [4, 5, F], which correlates with increased policy costs."}, {"title": "7.3. Policy analysis over episode", "content": "In Section 7.2, we present an overview of policy performances. This section delves into the details per episode to provide further understanding on these policies. Figures 5, 6, and 7 detail the performance of the Agent-E, Agent-G, CMB, and SchM policies for pipes with ages 0, 25 and 50, respectively. The RM heuristic is excluded from this analysis due to its straightforward approach: allowing the pipe to fail before replacing it.\nFigure 5 shows that for a brand new pipe: (a) Agent-G performs maintenance on the pipe at approximately 32 years old; (b) Agent-E opts to replace the pipe when it is around 35 years old, which may be attributed to the presence of elements with higher severity levels in that specific episode; (c) CBM chooses not to act, which results in the least expensive policy in this comparison. However, it is observed that some pipe sections reach severity level k = 5 throughout the episode. Not taking any action is deemed risky since progressing to k = F becomes more likely and incurs higher costs; (d) SchM effectively controls severity levels but is more expensive than Agent-G's policy due to more frequent maintenance actions.\nFigure 6 shows that for a pipe aged 25: (a) Agent-G exhibits increased activity, indicating more frequent maintenance actions, especially as the pipe ages to 50, shortening the maintenance intervals; (b) Agent-E postpones any action until the pipe fails, at which point it replaces the pipe with a new one, akin to RM; (c) CBM also initiates maintenance around the pipe's 50-year mark. However, degradation escalates from age 60, leading to failure at 66. The inability to manage this increased severity results in significant penalty costs, diminishing the effectiveness of this policy; (d) Similarly, SchM manages severity levels effectively until the pipe reaches approximately 70 years of age, at which point degradation accelerates, resulting in failure at 73.\nFigure 7 shows that for a pipe aged 50: (a) Agent-G opts to replace the pipe at age 50, followed by maintenance in the subsequent time step. This decision is likely influenced by parts of the pipe being at severity levels k \u2208 3,4. Such a scenario is plausible, as new pipes can exhibit high severity levels at a young age due to defects in the material or errors during the construction and installation process. This concept is represented in the MSDM by the initial probability state vector (S). Additionally, Agent-G recommends maintenance at the interval when the pipe reaches the age of 26 years; (b) Agent-E suggests replacement at approximately 62 years, without recommending further maintenance; (c) CMB advocates for maintenance at about 65 years, followed by replacement at 70 years, in line with heuristics described in Section 6.2; (d) SchM consistently performs maintenance at regular intervals, yet faces significant degradation, culminating in failure around 97 years.\nTo summarize, our findings indicate that the Agent-G's policy, derived using DRL, implements a dynamic management strategy that varies with the pipe's age. This strategy encompasses a more passive approach with new pipes, transitioning to active intervention as the pipes age. This indicates the agent's preference for more frequent maintenance actions rather than allowing pipe failures, which incur higher penalties and replacement costs.\nMoreover, Agent-G outperforms Agent-E, illustrating the impact of the degradation model assumption. Specifically, Agent-G's prognostic model used during training aligns more closely with the test environment's degradation pattern than Agent-E's, potentially explaining why Agent-G is better equipped to navigate and understand the degradation pattern. This, in turn, enables it to devise a more effective maintenance policy by leveraging a more accurate degradation model."}, {"title": "8. DISCUSSION AND CONCLUSIONS", "content": "In this paper, we explore the applications of Prognostics and Health Management (PHM) in sewer pipe asset management. Our study focuses on component-level (i.e., pipe-level) maintenance policy optimization by integrating stochastic multi-state degradation modeling and Deep Reinforcement Learning (DRL). The goal is to assess the effectiveness of DRL in deriving cost-effective maintenance strategies tailored to the specific conditions and requirements of sewer pipes.\nA key contribution of our work is the integration of prognostics models with a maintenance policy optimization framework. We utilize a tailored reward function that aligns with damage severity levels, enabling a more complex and realistic maintenance optimization setup.\nOur methodology includes a real-world case study from a Dutch sewer network, which provides historical inspection data. Through hyper-parameter tuning and policy analysis, we benchmark our optimized policies against traditional heuristics, including condition-based, scheduled, and reactive maintenance.\nOur findings suggest that agents trained with the Proximal Policy Optimization algorithm are highly capable of developing strategic maintenance policies, adapting to pipe age, and surpassing heuristic baselines by learning cost-effective dynamic management strategies.\nTo evaluate the impact of degradation model assumptions, we trained one agent using the Gompertz probability density function and another using the Exponential probability density function.\nDuring testing, both agents were assessed in an environment parameterized with the Weibull probability density function. The Gompertz-trained agent, whose behavior more closely resembled the Weibull model, demonstrated better generalization, resulting in more effective maintenance policies compared to the Exponential-trained agent.\nFuture work: The following directions are identified:\n\u2022 Advancing toward partially observable state spaces with the introduction of inspection actions, considering context, and leveraging deep learning capabilities.\n\u2022 Utilizing knowledge acquired by agents to develop explainable and robust heuristics.\n\u2022 Although this paper focused on a single cohort of pipes, studies in Jimenez-Roa et al. (2022, 2024) show different cohorts exhibit varied dynamics, highlighting the importance of understanding how RL agents adapt.\n\u2022 Comparing RL-based approaches with other policy optimization algorithms to better understand the capacity of RL methods to achieve global-optima maintenance strategies.\n\u2022 Investigating various reward functions (e.g., dense) and RL algorithms to determine the most effective for devising maintenance policies.\n\u2022 Extent to system-level analysis and evaluate aspects such as scalability."}]}