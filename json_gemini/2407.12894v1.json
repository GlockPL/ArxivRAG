{"title": "Maintenance Strategies for Sewer Pipes with Multi-State Degradation and Deep Reinforcement Learning", "authors": ["Lisandro A. Jimenez-Roa", "Thiago D. Sim\u00e3o", "Zaharah Bukhsh", "Tiedo Tinga", "Hajo Molegraaf", "Nils Jansen", "Mari\u00eblle Stoelinga"], "abstract": "Large-scale infrastructure systems are crucial for societal welfare, and their effective management requires strategic forecasting and intervention methods that account for various complexities. Our study addresses two challenges within the Prognostics and Health Management (PHM) framework applied to sewer assets: modeling pipe degradation across severity levels and developing effective maintenance policies. We employ Multi-State Degradation Models (MSDM) to represent the stochastic degradation process in sewer pipes and use Deep Reinforcement Learning (DRL) to devise maintenance strategies. A case study of a Dutch sewer network exemplifies our methodology. Our findings demonstrate the model's effectiveness in generating intelligent, cost-saving maintenance strategies that surpass heuristics. It adapts its management strategy based on the pipe's age, opting for a passive approach for newer pipes and transitioning to active strategies for older ones to prevent failures and reduce costs. This research highlights DRL's potential in optimizing maintenance policies. Future research will aim improve the model by incorporating partial observability, exploring various reinforcement learning algorithms, and extending this methodology to comprehensive infrastructure management.", "sections": [{"title": "1. INTRODUCTION", "content": "Sewer network systems, crucial for public health, population well-being, and environmental protection, require maintenance to ensure their reliability and availability (Cardoso et al., 2016). This maintenance is challenged by limited budgets, environmental changes, aging infrastructure, and hard-to-predict system deterioration (Tscheikner-Gratl et al., 2019).\nOptimizing maintenance policies for sewer networks requires methodologies that can efficiently explore a broad solution space while adapting to the system's dynamic constraints and complexities. Maintenance Policy Optmization (MPO) addresses these needs by developing and analyzing mathematical models to derive maintenance strategies (De Jonge & Scarf, 2020) that reduce maintenance costs, extend asset life, maximize availability, and ensure workplace safety (Ogunfowora & Najjaran, 2023).\nThis research explores the potential of Deep Reinforcement Learning (DRL) for MPO of sewer networks, first focusing on a component-level (i.e., pipe-level) analysis. DRL is a framework that merges neural network representation learning capabilities with Reinforcement Learning (RL), a branch of machine learning known for its effectiveness in sequential decision-making problems. RL is increasingly recognized for its role in developing cost-effective policies in MPO across diverse domains such as transportation, manufacturing, civil infrastructure and energy systems. It is emerging as a prominent paradigm in the search for optimal maintenance policies (Marug\u00e1n, 2023).\nThis paper aims to achieve two primary objectives: first, to present a comprehensive model for pipe-level MPO analysis facilitated by DRL, considering degradation over the pipe length and employing inhomogeneous-time Markov chain models to simulate the nonlinear stochastic behavior associated with sewer pipe degradation; second, to assess the efficacy of the model's policy through a case study of a large-scale sewer network in the Netherlands, comparing it with heuristics, in-cluding condition-based, scheduled, and reactive maintenance.\nWe acknowledge as limitations in our approach the focus on fully observable state spaces, which means that inspection actions are not necessary, and our analysis is at the component-level. Future research will aim to broaden this scope to include partially observable state spaces and system-level analysis.\nContributions. This work's primary contributions include:\n(i) We propose a framework to carry out maintenance policy optimization for sewer pipes considering the deterioration along the pipe length. This framework integrates Multi-State Degradation Models (MSDMs) and Deep Reinforcement Learning (DRL).\n(ii) Our framework introduces a novel approach by encoding the prediction of the MSDM into the state space, aiming to harness prognostics that describe the degradation pattern of sewer pipes.\n(iii) We demonstrate that DRL has the potential to devise intelligent strategic maintenance strategies adaptable to various conditions, such as pipe age.\n(iv) We provide our framework in Python and all data used in this study at zenodo.org/records/11258904.\nPaper outline. Section 2 presents the technical background. Section 3 outlines our research methodology. Section 4 formulates the MSDM. Section 5 details the framework for maintenance policy optimization via DRL. Section 6 presents our experimental setup. Section 7 analyzes the results. Section 8 discusses findings, concludes, and suggests future research.\nRelated work. In the past two decades, the need for integral sewer asset management has become evident (Abraham et al., 1998), emphasizing the necessity to understand the mechanisms of deterioration and develop predictive models for proactive and strategic sewer maintenance (Fenner, 2000). Sewer asset management encompasses maintenance, rehabilitation, and inspection and has been investigated through various methodologies, including risk-based strategies (Lee et al., 2021), multi-objective optimization (Elmasry et al., 2019), Markov Decision Processes (Wirahadikusumah & Abraham, 2003), considering the structure of the sewer network (Qasem & Jamil, 2021), machine learning applications (Montserrat et al., 2015; Caradot et al., 2018; Laakso et al., 2019; Hern\u00e1ndez et al., 2021), and decision support frameworks (Taillandier et al., 2020; Khurelbaatar et al., 2021; Ramos-Salgado et al., 2022; Assaf & Assaad, 2023).\nThe integration of RL into sewer asset management is largely unexplored, with existing research mainly concentrating on real-time control for smart infrastructure, adapting to environmental changes such as storms. Mullapudi et al. (2020) uses DRL for controlling storm water system valves through simulation of varied storm scenarios. Yin et al. (2023) employ RL for near real-time control to minimize sewer overflows. Meanwhile, Zhang et al. (2023) and Tian et al. (2022) both examine improving the robustness of urban drainage systems, the former through decentralized multi-agent RL and the latter through Multi-RL, with Tian et al. (2024) further improving the model interpretability using DRL. Furthermore, Kerkkamp et al. (2022) investigates the sewer network MPO by combining DRL with Graphical Neural Networks to optimize maintenance actions grouping. Jeung et al. (2023) proposes a DRL-based data assimilation methodology to enhance storm water and water quality simulation accuracy by integrating observational data with simulation outcomes."}, {"title": "2. TECHNICAL BACKGROUND", "content": "2.1. Multi-state degradation model for sewer pipes\nThe modeling of sewer pipe network degradation has been explored through various methodologies, including physics-based, machine learning, and probabilistic models. For comprehensive discussions on this topic, the reader is directed to Ana & Bauwens (2010); Hawari et al. (2017); Malek Mohammadi et al. (2019); Saddiqi et al. (2023); Zeng et al. (2023).\nWe adopt a probabilistic approach employing Inhomogeneous Time Markov Chains (IHTMCs) to model the multi-state degradation of sewer pipes. This choice is motivated by the IHTMC's capability to better capture the degradation of long-lived assets such as sewer systems as a non-linear stochastic process, characterized by age-dependent transition probabilities between degradation states (Jimenez-Roa et al., 2024).\nInhomogeneous Time Markov Chains (IHTMCs). An IHTMC is a stochastic process {$(X_t)_{t>0}$}, where t \u2208 [0,\u221e) is continues and models time. The IHTMC is defined as a tuple $M = (\\Omega, S^0, Q(t))$, where \u03a9 is a set of K finite states indicating the state space, $S^0$ is an initial-state distribution on \u03a9 where $\\Sigma_{k \\in \\Omega} S^0_k = 1$, and $Q(t) : \\Omega \\times \\Omega \\rightarrow \\mathbb{R}$ is a time-dependent transition rate matrix, with entries $q_{ij}(t)$ for $i, j \\in \\mathbb{N}$ and $i \\neq j$, representing the rate of transitioning from state i to state j at time t. The diagonal entries $q_{ii}(t)$ are defined such that the sum of each row in Q(t) is zero, ensuring that the outflow from any state is equal to the sum of the inflows into other states. Q(t) may be parameterized by hazard rates $A(t|\\theta)$ derived from the ratio $f(t|\\theta)$ and $S(t|\\theta)$, being respectively a probability density function and a survival function, where \u03b8 corresponds to the function hyper-parameters. The evolution over time of the IHTMC is governed by the Forward Kolmogorov equation:\n$\\frac{\\partial P_{ij}(t, \\tau)}{\\partial t} = \\sum_{k \\in S}{\\Pi_{ik}(t, \\tau) Q_{kj}(t)}$\t\t(1)\nHere, $P_{ij}(t, \u03c4) : \u03a9 \u00d7 \u03a9 \u2192 [0, 1]$ is a continuous and differentiable function known as the transition probability matrix, indicating the probability of transitioning from state i to state j in the time interval t to \u03c4, where \u03c4 > t. From Eq. (1) one can obtain the master equation of the Markov chain, which models the flow of probabilities between states by including inflow and outflow terms:\n$\\frac{\\partial S_k(t)}{\\partial t} = \\sum_{i \\in \\Omega, i \\neq k} S_i(t)Q_{ik}(t) - S_k(t) \\left( \\sum_{j \\in \\Omega, j \\neq k} Q_{kj}(t) \\right)$ (2)\nHere, $S_k(t)$ is the probability of being in state $k \\in \\Omega$ at time t, the term $\\sum_{j \\in \\Omega, j \\neq k} Q_{kj}(t)$ represents the rates of transition from state k to all the other states j (excluding self-transitions).\nPipe-element degradation model. We define a pipe element by K sequentially arranged states $S = [S_1, S_2, ..., S_k]$, where $S_1$ signifies the pristine condition and $S_K$ represents the worst condition. This categorization is based on sewer network inspection data, which documents types of damage and their severities on a scale from 1 to 5, along with occasional instances of functional failures (K = 6). The transitions within our IHTMC, illustrated in Figure 1, permit only progression from a better to a worse state, prohibiting direct improvements without repairs, while allowing any severity level to escalate to functional failure."}, {"title": "2.2. Markov Decision Process", "content": "A Markov Decision Process (MDP) models a stochastic sequential decision process, where both costs and transition functions are dependent solely on the current state and action (Puterman, 1990). Formally, an MDP is described by the tuple $(S, A, P(S_{t+1}|S_t, a_t), R(s_t, a_t, S_{t+1}), \\pi_0, \\gamma)$, with S as state space, A as the action space, $P(S_{t+1}|S_t, a_t)$ as the transition probability function indicating the probability of transitioning from state $s_t$ to $s_{t+1}$ given action $a_t$, where $s_t, s_{t+1} \\in S$ and $a_t \\in A$. The reward function $R(s_t, a_t, S_{t+1})$ specifies the reward for moving from $s_t$ to $s_{t+1}$ by action $a_t$. The initial state $\u03c0_0$ represents the distribution across S, and \u03b3\u2208 [0,1] is the discount factor that balances immediate versus future rewards."}, {"title": "2.3. Deep Reinforcement Learning", "content": "Deep Reinforcement Learning (DRL) produces virtual agents that interact with environments to learn optimal behaviors through trial and error, as indicated by a reward signal (Arulkumaran et al., 2017). DRL has found applications in robotics, video games, and navigation systems.\nWe utilize DRL to train agents in virtual environments exhibiting degradation following the MSDM pattern, as detailed in Section 5. Specifically, we apply Proximal Policy Optimization (PPO) (Schulman et al., 2017), a policy gradient method in RL.\nPPO aims to optimize the policy an agent uses for action selection, maximizing expected returns. It addresses stability and efficiency issues encountered in previous algorithms like Trust Region Policy Optimization by offering a simpler and less computationally expensive method to ensure minor policy updates.\nThis is achieved through an innovative objective function that penalizes significant deviations from the previous policy, fostering stable and consistent learning. The term \u201cproximal\u201d denotes maintaining proximity between the new and old policies, facilitating a stable training process and rendering PPO popular across various RL applications."}, {"title": "3. METHODOLOGY", "content": "Our methodology, illustrated in Figure 2, comprises six steps, detailed below.\nStep 1. Perform data handling of historical inspection records, selecting subsets (cohorts) of interest, and calibrating the MSDM on this data. This step is beyond the scope of this paper; for details, see Jimenez-Roa et al. (2022, 2024). The results of this step are given in Section 4.\nStep 2. After calibrating the MSDM, integrate these models into an environment suitable for RL applications. We present the details of our environment integrating MSDM in Section 5. In addition, we define environments for training RL agents. This is to test different MSDM hypotheses; details on this can be found in Section 6.\nStep 3. Train DRL agents with PPO. Use optuna for hyper-parameter tuning and Stable Baselines3 for RL implementation. Details are in Section 7.1.\nStep 4. Train and select the RL agents with the optimal hyper-parameters on the training environments. In essence, these agents learn the dynamics described by the MSDM encoded in the environment.\nStep 5. Compare the maintenance policies advised by the RL agents using the test environment against the heuristics: Condition-Based Maintenance (CBM), Scheduled Maintenance (SchM), and Reactive Maintenance (RM). Find the definition of these heuristics in Section 6.2.\nStep 6. Analyze and compare the behavior of the maintenance strategies for the different RL models and heuristics. Reflect on the policies advantages and disadvantages. Find in Section 7.2 the overview of this comparison, and in Section 7.3 are the details along episodes."}, {"title": "4. MULTI-STATE DEGRADATION MODELS", "content": "4.1. Case study\nOur case study conducts a detailed examination of the sewer pipe network in Breda, the Netherlands, which comprises 25,727 sewer pipes covering 1,052 km, mostly built after 1950. The network is primarily made of concrete (72%) and PVC (27%), with the shapes of the pipes being predominantly round (95%) and ovoid (5.4%). These pipes are designed for transportation (98.2%), with 88% being up to 60 meters in length. Additionally, 98.3% have a diameter of up to 1 meter, with the most common diameter being 0.2 meters, and they carry mixed (63%), rain (21%), and waste (16%) contents. The condition of the pipes is evaluated through visual inspections according to the European standard EN 13508 (\u00c9N13508, 2012; EN13508-2, 2011), focusing on identifying and classifying damage with specific codes. This study specifically addresses the damage code BAF, which signifies surface damage and was observed in 35.3% of the inspections.\n4.2. Parametrization\nWe consider three distributions for hazard rate functions: Exponential, Gompertz, and Weibull. The hazard rates $(\u03bb(t))$ for these distributions are specified as follows:\nExponential function: $\\Lambda^E(t | \\epsilon) = \\epsilon$, (3a)\nGompertz function: $\\Lambda^G(t | \\alpha, \\beta) = \\alpha \\beta e^{\\beta t}$ (3b)\nWeibull function: $\\Lambda^W(t | \\eta, \\rho) = \\frac{\\rho}{\\eta}(\\frac{t}{\\eta})^{\\rho-1}$ (3c)\nIn Eq. (3a), a constant hazard rate indicates that the degradation model assumes a homogeneous time, exhibiting memory-less properties. Eq. (3b) and Eq. (3c) present varying hazard rates, which indicates inhomogeneous time."}, {"title": "4.3. Solving the Multi-State Degradation Model", "content": "In Figure 1, we defined the structure of the Markov chain to model degradation in a sewer pipe, and in Section 4.2 we introduced the hazard rate functions. In the following, we present the corresponding system of differential equations.\n$\\frac{\\partial S_1(t)}{\\partial t} = - (\\Lambda_{12}(t) + \\Lambda_{1F}(t))S_1(t)$ (4a)\n$\\frac{\\partial S_2(t)}{\\partial t} = \\Lambda_{12}(t)S_1(t) - (\\Lambda_{23}(t|\u00b7) + \\Lambda_{2F}(t|\u00b7))S_2(t)$ (4b)\n$\\frac{\\partial S_3(t)}{\\partial t} = \\Lambda_{23}(t)S_2(t) - (\\Lambda_{34}(t|\u00b7) + \\Lambda_{3F}(t|\u00b7)) S_3(t)$ (4c)\n$\\frac{\\partial S_4(t)}{\\partial t} = \\Lambda_{34}(t)S_3(t) + ( - \\Lambda_{45}(t|\u00b7) \u2013 \\Lambda_{4F}(t|\u00b7)) S_4(t)$ (4d)\n$\\frac{\\partial S_5(t)}{\\partial t} = \\Lambda_{45}(t)S_4(t) - \\Lambda_{5F}(t|\u00b7) S_5(t)$ (4e)\n$\\frac{\\partial S_F(t)}{\\partial t} = \\Lambda_{1F}(t|\u00b7)S_1(t) + \\Lambda_{2F}(t|\u00b7)S_2(t) + \\Lambda_{3F}(t|\u00b7)S_3(t) + \\Lambda_{4F}(t)S_4(t) + \\Lambda_{5F}(t|\u00b7) S_5(t)$ (4f)\nEq. 4 is solved using numerical methods, specifically the LSODA algorithm from the FORTRAN odepack library implemented in SciPy (Jones et al., 2001\u2013). This algorithm solves systems of ordinary differential equations by employing the Adams/BDF method with automatic stiffness detection.\n4.4. Parametric Multi-State Degradation Models\nWe extract a subset from our case study data set to construct a cohort with concrete sewer pipes carrying mixed and waste content (cohort CMW), representing 37.1% of the sewer network. The model parameters for this cohort are detailed in Appendix A in Tables 7 and 8.\nFigure 3 illustrates the MSDMs predictions, detailing the stochastic dynamics of sewer pipe degradation for pipes in cohort CMW. As Figure 1 describes, this degradation is segmented into five sequentially ordered severity levels (k = 1 to k = 5), plus a functional failure state (k = F). Differences in the y-axis scales are intentional, to emphasize details and behaviors that various degradation models express across severity levels.\nGray circles represent the frequency per severity level from the inspection dataset. Jimenez-Roa et al. (2022) details how these frequencies are computed. Vertical black lines in Figure 3 mark the last available data point for each severity level.\nAdditionally, Figure 3 presents the Turnbull non-parametric estimator, which assumes no specific distribution for survival times (Turnbull, 1976). In our context, this estimator represents the ground truth of stochastic degradation behavior in sewer pipes.\nThese MSDMs serve two crucial roles within our environment: first, they drive the degradation behavior of sewer pipes, effectively emulating how sewer pipes degrade over time. Second, the output from the MSDMs is incorporated as prognostic information, available to the agent to support decisions at any time point."}, {"title": "5. DEFINITION OF MARKOV DECISION PROCESS FOR MAINTENANCE POLICY OPTIMIZATION OF A SEWER PIPE CONSIDERING PIPE LENGTH DEGRADATION", "content": "Figure 4 provides the workflow that the RL agent uses to learn maintenance policies for sewer pipes, considering degradation along the pipe length. In the following sections, we provide the details of the environment, namely the state and action spaces, as well as the transition probability and reward functions.\n5.1. State space (S)\nOur approach focuses on developing age-based maintenance policies, incorporating the sewer pipe's age into the state representation. Our state space is continuous and it is structured to include three key components: (i) the age of the pipe, (ii) the health vector, and (iii) the stochastic prediction of severity levels. We next describe the last two components.\n5.1.1. Health vector (h)\nIn modeling the degradation of linear structures like sewer pipes, it is essential to represent changes accurately along their length. For this purpose, we define a health vector (h), which quantitatively measures the degradation at various points along the pipe. The vector is crucial in our framework, particularly influencing the reward function as described in Section 5.4.\nConstruction of h: We discretize the pipe into segments of equal length \u0394L, with \u0394L < L, where L is the total length of the pipe. The number of segments, nd, is calculated using the ceiling function to ensure it remains an integer even if L is not perfectly divisible by \u0394L:\n$n_d = \\lceil{\\frac{L}{\\Delta L}}\\rceil$ (5)\nEach segment's degradation level is initially assessed and categorized into severity levels according to the MSDM. As the degradation progresses, the state of each segment changes following the transition probabilities described by the matrix $P_{i,j}$, where i is the current severity level, and j is the subsequent severity level, as described by the forward Kolmogorov equation (Eq. 1).\nNotice that by doing this, we assume there is no statistical dependency between segments, which is a strong assumption that needs further research. However, for simplicity, we maintain this assumption in our degradation model.\nQuantifying Degradation: The distribution of severity levels across the pipe is captured in vector d, with each element indicating the severity level of a segment. To quantify this distribution in the health vector h, we first count the number of segments at each severity level k using the following expression:\n$n_{dk} = \\sum_{i=1}^{n_d}{1\\{d_i=k\\}}$ (6)\nwhere 1 is the indicator function that is 1 if the condition is true and 0 otherwise. The health vector h is then determined by normalizing these counts to reflect the proportion of segments at each severity level:\n$h_k = \\frac{n_{dk}}{n_d}$ (7)\nHere, ndk is the number of segments at severity level k. Thus, hk becomes part of the state space indicating the level of degradation present in the pipe.\n5.1.2. Stochastic prediction of severity levels\nTo enable the agent to access information provided by the MSDM, we incorporate the prediction of severity levels into the state space. This is accomplished by solving Eq. 2, yielding a distribution Sk(t).\nFinally, our state space is defined as a tuple with 13 elements:\nS = (Pipe Age, h1, h2, h3, h4, h5, hF, S1, S2, S3, S4, S5, SF)\n5.2. Action space (A)\nOur action space A is discrete with dimensionality |A| = 3. At each time step t, the agent selects an action at. If the decision at time t is do nothing, at is set to 0. To perform maintenance, at is set to 1, and to replace the pipe, at is set to 2. The outcomes of these actions are discussed in Section 5.3."}, {"title": "5.3. Transition function (P)", "content": "Our transition function $P(s_{t+1}|s_t, a_t)$ is stochastic, dependent on time t, and considers both the actions $a \\in A$ and the current $s_t$ and next state $s_{t+1}$ dynamics described by the MSDM. We illustrate the behavior of P with the following example.\nFor a 30-year-old pipe with length L = 40 meters and discretized in segments of length \u0394L = 1, let the current state space be $s_{t=30} \\in S$:\n$s_{t=30} = (30, 0.60, 0.35, 0.025, 0.025, 0.0, 0.0, 0.475, 0.436, 0.069, 0.010, 0.005, 0.005)$.\n$s_{t=30}$ indicates the age of the pipe is 30 years. From Eq. 7, the number of segments at severity k is determined by multiplying the health vector (hk):\n$h_k = [0.60, 0.35, 0.025, 0.025, 0.0, 0.0]$\nby 40 meters, yielding $n_{dk}$ = [24, 14, 1, 1, 0,0], indicating that, out of the 40 meters of pipe length, 24 segments of 1 meter are at severity k = 1, 14 at severity k = 2, and so forth.\nThe distribution $S_k (t = 30.0)$ predicts the probability of being in a severity level k at age t = 30. This is achieved by evaluating t = 30.0 in the corresponding MSDM.\n$S_k(t = 30.0) = [0.475, 0.436, 0.069, 0.010, 0.005, 0.005]$\nAssuming the agent takes an action every half year, we illustrate the effect of each action in A below.\n- If $a_t = 0$: the agent decides to \u201cdo nothing", "1$": "the agent decides to \u201cperform maintenance,", "2$": "the agent decides to", "replace": "he pipe, resetting its condition to as good-as-new. The new state space is $s_{t=0.0}^{a=2}$.\n$s_{t=0.0}^{a=2} = (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.986, 0.014, 0.0, 0.0, 0.0, 0.0)$.\nThe pipe age is reset to 0.0, with ndk = [40, 0, 0, 0, 0, 0], and $S_k (t)$ is updated for t = 0.0."}, {"title": "5.4. Reward function (R)", "content": "Our reward function R(st, at, St+1) assigns a reward rt at every decision point t, determined by the current state st and action at. This function integrates the costs of maintenance (CM), replacement (CR), and failures (CF). R is sparse because it issues a non-zero value only when failures occur or interventions are undertaken.\nMaintenance cost CM is calculated as per Eq. 8, where it combines a variable cost based on severity k with a fixed logistic cost of \u20ac500, covering the expenses related to maintenance.\n$C_M = - (h_k \\cdot c_k + 500)$ (8)\nReplacement costs (CR) is computed with Eq. 9:\n$C_R = -(450 + 0.66D + 0.0008D^2)L$ (9)\nHere, L and D denote the pipe's length in meters and diameter in millimetres, respectively. CR is in Euros (\u20ac).\nThe cost of failure, denoted by CF, entails assigning a substantial penalty when the agent allows a segment of the pipe to achieve a failure state (k = F). This penalty cost is established at \u20ac-100,000. Our reward function is then:\n$r_t = \\frac{C_M + C_R + C_F}{100'000 + 900 \\times 40} = \\frac{C_M + C_R + C_F}{136'000}$ (10)\nwhere rt represents the reward obtained at time t, the normalization constant 136\u2032000 corresponds to the most expensive penalty possible at time t. Thus, rt is defined within the interval [-1, 0]. This reward function aims for the agent to balance maintenance actions with the prevention of undesirable pipe conditions."}, {"title": "6. EXPERIMENTAL SETUP", "content": "6.1. Setup\nWe will evaluate our framework with a single pipe of constant length (40 meters) and diameter (200 mm) from the cohort CMW, which carries mixed and waste content. Given the constant dimensions, the replacement cost CR, as defined in Eq. 9, is \u20ac24,560. The pipe age, when initializing the episode, is randomly sampled from the uniform distribution U ~ [0,50], allowing the agent to learn the behavior of pipes within this age range. Additionally, we evaluate the policy in steps of half a year and \u0394L = 1 meter.\nIn the methodology section, we describe the training of two agents: Agent-E and Agent-G. Agent-E is trained in an environment where sewer pipe degradation follows the MSDM parameterised with an Exponential probability density function, while Agent-G is trained in an environment where degradation follows the MSDM parameterised with a Gompertz probability density function.\nBoth agents are tested in an environment where sewer pipe degradation follows the MSDM parameterized with the Weibull probability density function.\nDuring training, each agent follows a specific state space, defined as follows:\n$S_{Training}^{Agent-E} = (Pipe Age, h, S_E(t))$ (11a)\n$S_{Training}^{Agent-G} = (Pipe Age, h, S_G(t))$ (11b)\nHere, S represents the state space for each agent during training. The subscripts E and G denote the Exponential and Gompertz probability density functions, respectively. Each agent's objective is to learn an optimal maintenance strategy based on their environment's dynamics.\nFor testing, both agents are evaluated in the same environment, with the state space defined as follows:\n$S_{Testing}^{Agent-E} = (Pipe Age, h, S_E(t))$ (12a)\n$S_{Testing}^{Agent-G} = (Pipe Age, h, S_F(t))$ (12b)\nIn both cases, SE(t) and SF(t) remain consistent with the training phase, reflecting the MSDM predictions. However, the health vector hk follows the degradation behavior described by the Weibull probability density function, indicated by the subscript W.\n6.2. Comparison of maintenance strategies\nWe compare the RL agent's performance against maintenance policies based on heuristics. For this, we define the following:\n- Condition-Based Maintenance (CBM): Maintenance actions are based on the sewer pipe's condition. Specifically, replacement (at = 2) is performed if pipe_age \u2265 70 or $h_{k=F}$ \u2265 0.0; maintenance (at = 1) is conducted if $h_{k=4}$ \u2265 0.1 or $h_{k=5}$ \u2265 0.05; otherwise, no action (at = 0) is taken.\n- Scheduled Maintenance (SchM): Actions are time-based. Replacement (at = 2) is executed if $h_{k=F}$ \u2265 0.0; maintenance (at = 1) occurs every 10 years; otherwise, no action (at = 0) is taken.\n- Reactive Maintenance (RM): Replacement is undertaken only upon pipe failure, i.e., replacement (at = 2) is performed if $h_{k=F}$ \u2265 0.0; otherwise, no action (at = 0) is taken.\nNote that CBM and SchM are defined based on plausible values. However, these heuristics can be further calibrated for enhanced performance, which is beyond the scope of this paper."}, {"title": "7. RESULTS", "content": "7.1. Implementation and hyper-parameter tuning\nOur framework uses Stable Baselines 3 (Raffin et al.", "encompasses": "exponentially-decaying learning rate with a decay rate of 0.05, with an initial learning rate ranging from 10-5 to 10-2, discount factor (\u03b3) from 0.8 to 0."}]}