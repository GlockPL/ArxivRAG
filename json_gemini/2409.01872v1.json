{"title": "Latent Distillation for Continual Object Detection at the Edge", "authors": ["Francesco Pasti", "Marina Ceccon", "Davide Dalle Pezze", "Francesco Paissan", "Elisabetta Farella", "Gian Antonio Susto", "Nicola Bellotto"], "abstract": "While numerous methods achieving remarkable performance exist in the Object Detection literature, addressing data distribution shifts remains challenging. Continual Learning (CL) offers solutions to this issue, enabling models to adapt to new data while maintaining performance on previous data. This is particularly pertinent for edge devices, common in dynamic environments like automotive and robotics. In this work, we address the memory and computation constraints of edge devices in the Continual Learning for Object Detection (CLOD) scenario. Specifically, (i) we investigate the suitability of an open-source, lightweight, and fast detector, namely NanoDet, for CLOD on edge devices, improving upon larger architectures used in the literature. Moreover, (ii) we propose a novel CL method, called Latent Distillation (LD), that reduces the number of operations and the memory required by state-of-the-art CL approaches without significantly compromising detection performance. Our approach is validated using the well-known VOC and COCO benchmarks, reducing the distillation parameter overhead by 74% and the Floating Points Operations (FLOPs) by 56% per model update compared to other distillation methods.", "sections": [{"title": "1 Introduction", "content": "Object detection is a computer vision task that involves identifying and locating objects within an image. Its application spans numerous domains, such as robotics, medical imaging, and manufacturing [14, 20]. Despite significant advancements and impressive performance of recent solutions [51], some challenges still need to be addressed to deploy state-of-the-art algorithms in real-world scenarios.\nOne of these challenges involves the distribution shifts often present in new data [7]. For example, the environment in which a moving agent operates could change, introducing new classes or presenting objects in different contexts [49]. A"}, {"title": "2 Related Works", "content": "Continual Learning allows deep learning models to update and expand their knowledge over time with minimal computation and memory overhead [5]. An effective CL solution, should minimize forgetting of old tasks, require low memory, and be computationally efficient during incremental model updates. As defined in [49], the literature explores three CL scenarios: Task-Incremental (TIL), Domain-Incremental (DIL), and Class-Incremental (CIL). Most CLOD works [2, 17, 37, 45] focus on the CIL scenario, where images with annotations of new classes are presented in each new task. Images associated with the previous classes may still be included in the task samples, but the old classes are not labeled in the new tasks. Consequently, the model must retain knowledge of the old task classes and accurately assign the previous labels while adapting to the new task.\nCL strategies can be grouped into three clusters: (i) rehearsal-based [8, 24], (ii) regularization-based [15, 19], and (iii) architecture-based [29, 46]. In this work, we explore rehearsal-based and regularization-based approaches, as a recent survey on CLOD [30] shows they are the most effective strategies to mitigate the effects of catastrophic forgetting in object detection networks.\nRehearsal-based methods require storing and reusing past data samples during training. Within this category, various approaches exist, with one of the most renowned methods being Experience Replay [43]. Regularization-based techniques, on the other hand, introduce additional constraints or penalties during training to maintain the memory of old tasks. For example, they might penalize the parameters differently based on their importance [15] or use distillation to retain knowledge from previous tasks [19]. In the object detection setting, regularization-based approaches, pioneered by [45], typically use a classic distillation teacher-student setup, relying on a Region Proposal Network (RPN) to"}, {"title": "3 CLOD at the Edge", "content": "The following section describes the proposed approach for CLOD at the edge.\nFirst, in Sec. 3.1, we describe the lightweight open-source detector, NanoDet [39],"}, {"title": "3.1 Efficient Object Detection Pipeline", "content": "For CLOD at the edge, we use NanoDet [39], an open-source anchor-free object detector developed for real-time inference on edge devices. NanoDet falls in the category of Fully Convolutional One-Stage object detectors (FCOS) [47]. These detectors are composed of (i) a backbone that extracts features representing the objects in the frame; (ii) a feature pyramid neck (FPN) that upscales the representations of the backbone and abstracts them at different scales; and (iii) a detection head, responsible for predicting the bounding boxes and their respective classes.\nNanoDet, depicted in Fig. 1 (right), presents a good performance-complexity tradeoff. Its improvement over other detectors can be associated with four characteristics: (i) an efficient edge-oriented backbone, namely ShuffleNetv2 [27]; (ii) an efficient feature-pyramid network, named GhostPAN; (iii) the merging of the centerness and classification score thanks to the Generalized Focal Loss [18]; and (iv) the introduction of an auxiliary module to solve the optimal label assignment problem, adding some training overhead. Moreover, in the CL settings, the optimal anchors for the new task bounding boxes can potentially shift from the ones defined for the old data. As NanoDet is anchor-free, the model regression outputs are free to adapt to the new bounding boxes.\nWe implement Nanodet-Plus-m from [39], which takes as input 320x320x3 images and uses the ShuffleNetv2 backbone, achieving 27% of mean Average Precision (mAP) on COCO. Despite being a good choice for edge deployment, this architecture considers weight averaging and an auxiliary head to achieve the best performance, therefore it has 4.8M parameters during training. Since the target of our work is to push the state-of-the-art for efficient CLOD, we reduced the training overhead of NanoDet by removing the auxiliary head and"}, {"title": "3.2 Latent Distillation", "content": "We identify two main weaknesses when considering classic distillation approaches for the edge. First, to perform distillation, the teacher model, which is trained on the previous task, introduces a training overhead as its logits need to be computed at each forward step. Moreover, training the entire network is computationally expensive and might not be necessary to retain the old knowledge while adapting to the new tasks. To address these issues, following the intuition of Latent Replay [35], we propose a simple yet effective strategy, Latent Distillation (LD)."}, {"title": "Algorithm 1 Latent Distillation", "content": "Start with:\nLower frozen layers $f_\\theta$, parameterized by weights $\\theta$; Upper Layers $h_\\omega$, parameterized by weights $\\omega$; Model $G(x;\\theta;\\omega) = h_\\omega(f_\\theta(x))$ of task n \u2212 1; c is the number of classes until task n-1; k is the number of classes to learn at task n; Update rule U; Training data $(X_n, Y_n)$ for new task n; Distillation coefficient \u03b1.\nInitialize:\nCopy teacher Upper Layers to student $h_\\omega \\leftarrow h_\\phi$\nTrain:\n$z \\leftarrow f_\\theta(X_n)$\n$\\hat{Y}_{teacher} \\leftarrow h_\\phi(z)$\n$\\hat{Y}_{student} \\leftarrow h_\\omega(z)$\n$\\mathcal{L} = \\mathcal{L}_{model}(Y_n[c+1:c+k], \\hat{Y}_{student}[c+1:c+k]) + \\alpha \\cdot \\mathcal{L}_{distillation}(\\hat{Y}_{student}[1:c], \\hat{Y}_{teacher}[1:c])$\nUpdate model parameters: $\\omega \\leftarrow U(\\mathcal{L},\\omega)$\nReturn:\nModel $G(x;\\theta;\\omega) = h_\\omega(f_\\theta(x))$ of task n;\nLD, depicted in Fig. 2 (right) and outlined in Algorithm 1, operates by feeding the training samples $X_n$ to a frozen subset of initial layers $f_\\theta$, shared between the student and teacher, generating representations z in the embedding space. These latent representations are then passed to both the teacher and student Upper Layers ($h_\\phi$ and $h_\\omega$, respectively) to compute the distillation and the model loss. The model loss, $\\mathcal{L}_{model}$, only considers new task classes, while the distillation loss, $\\mathcal{L}_{model}$, only considers outputs relative to old classes. After training, the new model $G(x;\\theta;\\omega)$ is composed of the frozen Lower Layers $f_\\theta(x)$ and the student Upper Layers $h_\\omega$.\nThe first advantage provided by LD is the reduction of computations for the forward step since the teacher and the student share a common frozen part. Indeed, as shown in Fig. 2 (left), a standard distillation approach requires processing the training images completely through the teacher and student archi-"}, {"title": "4 Experimental Setting", "content": "In this section, we outline the criteria we used to evaluate and compare Latent Distillation to other CL methods. We describe the datasets and the considered CL scenarios in Sec. 4.1 as well as the evaluation metrics in Sec. 4.2. We then define the CL methods against which we compare our novel approach in Sec. 4.3 for CLOD at the edge."}, {"title": "4.1 Dataset and Continual Learning Scenario", "content": "We validate our method on PASCAL VOC 2007 and COCO [6, 22], benchmarks commonly used in the CLOD literature [30, 45]. For VOC we train on the 5k images of the trainval split and evaluate on the 5k images of the test split. For COCO, we train on the 80k images of the training set and evaluate on the validation set as in [45]. VOC has annotations on 20 object classes, while COCO on 80 classes. Each task dataset comprises a subset of VOC or COCO containing images with at least one instance of the classes defined in the CL scenario.\nWe focus on five widely recognized benchmarks in the literature [30]: 10p10 (read 10-plus-10), 15p5, 19p1, 15p1 on VOC and 40p40 on COCO. In the 10p10 setup, there are two tasks, each comprising ten classes. The 15p5 scenario involves a first task with 15 classes and a second task with five. In the 19p1 configuration, the first task encompasses 19 classes, while the second comprises only one. We then tackle a more challenging scenario consisting of 6 tasks, denoted as 15p1. Here, the first task includes 15 classes, with each subsequent task"}, {"title": "4.2 Evaluation Metrics and Implementation Details", "content": "We evaluate our approach using mean Average Precision (mAP) weighted at different Intersections over Union (IoU), from 0.5 to 0.95. In particular, we provide three mAP values for each task: Old, New, and All. Old reflects the performance on classes encountered in previous tasks, measuring the stability of the model in retaining the previously learned knowledge. New denotes the mAP of the current task, indicating the model's plasticity in learning new tasks. Finally, All provides an aggregate measure, summarizing the model's overall performance across all the tasks encountered so far. We also consider Multiply-Accumulate (MAC) computations and the memory overhead required by the CL method.\nWe use AdamW [26] to train Nanodet in all the experiments. The learning rate is set at 0.001 with a 0.05 weight decay. We use a linear warmup of 500 steps and cosine annealing strategy [25] with $t_{max} = 100$. For each task the network is trained for 100 epochs. As a backbone, we use ShuffleNet V2 [27] pre-trained on Image-Net [16]. Moreover, to keep the number of operations low, we take as input images of size 320x320x3."}, {"title": "4.3 Definition of Compared Methods", "content": "The CLOD literature includes solutions that are tailored for specific object detection architecture. Most distillation methods indeed [11, 36, 53] consider Faster-RCNN, strongly relying on the RPN to get class-agnostic bounding box proposals. Other approaches [17, 23, 44] are also dependent on the detection network. Thus, these methods cannot be directly deployed for Nanodet, which is intrinsically different from the models used in the CLOD literature. Therefore, we evaluate Latent Distillation against classical approaches from the CL literature. Additionally, we implement Selective and Inter-related Distillation (SID) [37] because, as pointed out by [30], it is the only method designed for an FCOS architecture like Nanodet.\nJoint Training: the model is trained on all classes simultaneously; this method represents an upper baseline for CL strategies since it is not affected by catastrophic forgetting.\nFine-tuning: the model is presented sequentially with data from the current task. As no particular technique is used to maintain the model's knowledge, it is considered a lower bound for CL approaches.\nReplay: we implement this method [43] by creating a buffer at the end of the first task, randomly sampling the task dataset. Then, half of the memory buffer is updated at the end of subsequent tasks by randomly sampling the new"}, {"title": "5 Results and Discussion", "content": "In this section, we report the results of LD compared to other CL methods as defined in Sec. 4.3. These results refer to the Multiple Classes, One Class and Sequential One Class scenarios defined in Sec. 4.2."}, {"title": "5.1 Multiple Classes", "content": "In the 10p10, 15p5, and 40p40 scenarios, reported in Tab. 1, SID is confirmed to be the overall better strategy for FCOS-based NanoDet; indeed it is better at maintaining the model knowledge while still being able to learn the new task classes. Latent Distillation instead, requiring 74% less additional parameters and 56% less computation for updating the model with respect to distillation approaches, performs similarly to SID, making our approach suitable for edge applications where small memory footprint and fast training speed are important requirements. It is interesting to note that Latent Replay benefits from the frozen backbone resulting in less forgetting than Replay. Indeed, as reported in [48],"}, {"title": "5.2 One Class", "content": "In the 19p1 scenario, reported in Tab. 1, the full frozen backbone of our approach gives the best stability performance. However, it lacks some learning ability compared to other methods, since it has a smaller number of training parameters and the task dataset includes a limited number of training samples. It is then useful to compare the learning ability of LD on this task when parts of the backbone are progressively unfrozen. The results in Tab. 3 show that when the model is given more parameters to adapt to the new task, its learning ability improves. However, this implies longer training periods and more parameter overhead as the weights to back-propagate and store increase. Therefore, choosing how many lower layers should be frozen is a crucial choice based on the compromise between the plasticity of the model and the training/storage efficiency required by the edge application.\nWe also report in Fig. 3 (right) the overall mAP in this task vs the total model parameters that need to be stored by the CL strategy. Our method achieves the best performance when considering these two metrics. It is important to notice that total parameters refers only to the model parameters needed by the CL strategy; for the Replay approaches, it does not take into account that they require an additional memory buffer, which can be a significant problem for some edge applications, as mentioned in Sec. 4.3."}, {"title": "5.3 Sequential One Class", "content": "We finally consider the 15p1 scenario where the model must learn one class in sequential updates, starting from a base model trained on the first 15 classes. The results reported in Tab. 4 confirm SID and LD are the best compromise between stability and plasticity. Indeed, both strategies provide the best results in terms of forgetting, while also being able to learn new classes. In some scenarios,"}, {"title": "6 Conclusion and Future Works", "content": "We investigated using an efficient NanoDet detector for CLOD applications at the edge. We evaluated this detector on several CL scenarios, proving that it can learn incrementally new classes over time while retaining knowledge of previously learned classes. We then proposed a new Latent Distillation approach, which avoids the storage of multiple copies of the same architecture concurrently during training, as demanded by previous distillation-based methods. Our solution reduces the distillation memory overhead by 74% and computational burden by 56%, with a minor performance decrease in most scenarios.\nIn the future, we want to assess the generalizability of Latent Distillation in other CIL and DIL scenarios using different models and backbones. We also plan to implement this framework in a tinyML device. Additionally, we plan to expand the study to other recent and efficient detectors suitable for edge deployment and evaluate CLOD in different CL real-world scenarios with tiny autonomous robots."}]}