{"title": "A novel approach\nto\ndata generation\nin generative model", "authors": ["JaeHong Kim", "Jaewon Shim"], "abstract": "Variational Autoencoders (VAEs) and other generative models are widely employed in artificial\nintelligence to synthesize new data. However, current approaches rely on Euclidean geometric\nassumptions and statistical approximations that fail to capture the structured and emergent nature of\ndata generation. This paper introduces the Convergent Fusion Paradigm (CFP) theory, a novel geometric\nframework that redefines data generation by integrating dimensional expansion accompanied by\nqualitative transformation. By modifying the latent space geometry to interact with emergent high-\ndimensional structures, CFP theory addresses key challenges such as identifiability issues and\nunintended artifacts like hallucinations in Large Language Models (LLMs). CFP theory is based on two\nkey conceptual hypotheses that redefine how generative models structure relationships between data\nand algorithms. Through the lens of CFP theory, we critically examine existing metric-learning\napproaches. CFP theory advances this perspective by introducing time-reversed metric embeddings and\nstructural convergence mechanisms, leading to a novel geometric approach that better accounts for data\ngeneration as a structured epistemic process. Beyond its computational implications, CFP theory\nprovides philosophical insights into the ontological underpinnings of data generation. By offering a\nsystematic framework for high-dimensional learning dynamics, CFP theory contributes to establishing\na theoretical foundation for understanding the data-relationship structures in AI. Finally, future research\nin CFP theory will be led to its implications for fully realizing qualitative transformations, introducing\nthe potential of Hilbert space in generative modeling.", "sections": [{"title": "I. Introduction", "content": "In the field of artificial intelligence, autoencoders (AEs) are generally understood as operating through\na two-step process: data compression and reconstruction (Hinton and Salakhutdinov 2006). Variational\nAutoencoders (VAEs) extend this approach by introducing a variational distribution q(z|x) in place of\nthe posterior distribution p(z|x), which is ultimately replaced by the prior distribution p(z) (Bishop\n2006). This three-step process\u2014compression, transformation, and reconstruction (or generation)\u2014is\nbased on the fundamental understanding that VAEs have evolved from AEs. Moreover, VAEs benefit\nfrom mathematical approximations that facilitate efficient computation (Cybenko 1989; Hornik et al.\n1989). However, this approach oversimplifies the process of data generation, which is distinct from\ncompression and reconstruction. The conventional VAE framework risks distorting the actual process\nof data generation, often leading to unintended results. This issue affects a broad range of generative AI\nmodels, including Large Language Models (LLMs), which frequently exhibit hallucinations\u2014outputs\nthat are not grounded in the given data (Mittelstadt et al. 2023). While part of this issue arises from\nmathematical descriptive constraints, we argue that a more fundamental problem is the lack of a\nrigorous data science theory that can systematically abstract and idealize the process of data generation.\nTo address this gap, this paper introduces the Convergent Fusion Paradigm (CFP) as a philosophical\nfoundation for data science theory that provides a novel geometric perspective on data generation within\ngenerative models such as VAEs. The CFP theory constructs a data geometry that enables a structured\nintegration of low- and high-dimensional data spaces, ensuring seamless convergence across different\ndimensional levels. By applying this framework to generative models such as VAEs, high-level\ninformation embedded in high-dimensional data can be effectively captured without disrupting the\nintricate interplay between low- and high-dimensional data structures. Specifically, CFP theory enables\nVAEs to model data generation more precisely by fusing the high-dimensional ambient input space with\nthe low-dimensional latent space via a Riemannian metric, thereby yielding a new high-dimensional\nambient output space.\nThis study argues that a novel modeling approach for data-algorithm relationships in Deep Neural"}, {"title": "II. CFP Theory to provide a novel data geometry for generative model", "content": "To understand CFP as a novel theoretical approach that provides a data geometric framework for\ncapturing the process of data generation in generative models, it is essential to examine the fundamental\ndifference between DNNs and Euclidean geometry. The former represents an open-world data science\nparadigm, whereas the latter is based on a closed-world assumption. Euclidean geometry assumes a\nclosed and self-contained space that seeks uniqueness and completeness through axioms. This\nassumption restricts the representation of space and time to a limited coordinate system (Greenberg\n1993). Consequently, dimensional expansion in Euclidean space is often interpreted merely as a\nquantitative addition within a two-dimensional plane. In contrast, DNNs, which assume an open world,\nseek to implement a performance that achieves the correlation between the data, representing the facet\nof human life, and the algorithm, encapsulating the intended objectives through the expansion of\ndimensions embedded in the data features.\nBased on this premise, it is crucial to recognize that the modeling of DNNs differs from the two\nstatistical modeling cultures described by (Breiman 2001): data modeling culture and algorithmic\nmodeling culture. Unlike traditional data statistics, which either derives specific algorithmic models\nfrom data (data modeling culture) or seeks an algorithm that predicts data irrespective of its inherent\nstructure (algorithmic modeling culture), DNNs must undergo a dual process. This involves first\nidentifying a learning algorithm through training data and subsequently determining a model by fitting"}, {"title": "2. Expansion of Dimensions Accompanied by Qualitative Changes \u2013 Two New Conceptual\nHypotheses", "content": "This study proposes that the new developmental phase created by DNNs' unique \u201cmodeling of the\nrelationships between data and algorithms\" is not a quantitative expansion of dimensions through\nEuclidean geometry as previously discussed, but rather an \"expansion of dimensions accompanied by\nqualitative transformations.\" In other words, this study argues that the development of the relationship\nbetween data and algorithms in data science is not merely a quantitative scaling-up of dimensions\u00b2 but\ncan be better understood-figuratively speaking as akin to a 'plane' undergoing a qualitative\ntransformation to become a \u2018solid figure.' Particularly in the case of generative artificial intelligence\n(Goodfellow et al. 2014; Kingma and Welling 2013; Rombach et al. 2022), which goes beyond the level\nof implementing simple machinery mechanisms,\u00b3 the developmental phase of the relationship between\ndata and models in other words, the \"dimensional expansion accompanied by qualitative\ntransformations\" should not be overlooked. This developmental process organizes high-dimensional\nstructures as emergent constructs, distinct from the lower-dimensional structures, at the level of data\nfeatures (Capra and Luisi 2014)."}, {"title": "1) First Conceptual Hypothesis \u2013 Creating Relative Space-Time in Relationships (crSTR)", "content": "Personally, I believe that properly designing this aspect can resolve many challenges in AI and open the door to\nsurpassing the limitations of traditional data statistical modeling. These two conceptual hypotheses originate from\nthe intersubjectivity of beings, forming the foundation of relationship philosophy. Relationship philosophy, in this\ncontext, is rooted in the classical Confucian text Zhongyong (\u4e2d\u5eb8), as well as the later French existentialist\nphilosophy of Henry Bergson and Emmanuel Levinas. This framework serves as the basis for the social science\nresearch model known as the Convergent Paradigm (CP). CP theorizes that true social harmony and prosperity\nemerge not merely through calculations of mutual gains and losses between independent entities but through the\ncontinuous acquisition of new identities beyond individual subjects via organic relational formations."}, {"title": "A. Meaning of Time in Classical Newtonian Mechanics (Barbour 1982; Cohen and Smith 2002)", "content": "In Newtonian or Galilean geometry, time is understood as an absolute coordinate system used to\ndescribe events. Regardless of where an event occurs, observers interpret and experience it identically.\nFor instance, if one knows the initial state of a particle, they can predict its entire trajectory until its\nfinal state. In this classical approach, space is treated as a fixed entity, and time is merely a continuous\nparameter used for measurement."}, {"title": "B. Meaning of Time in Einstein's Theory of Relativity (Hartle 2021)", "content": "Einstein's theory of relativity introduced a fundamental shift in the understanding of time. Unlike in\nclassical mechanics, events are no longer absolute but depend on the state of the observer. The same\nevent can appear simultaneous to one observer but occur at different times to another due to variations\nin gravitational influence. Einstein incorporated time as a coordinate axis in his equations, treating it\nalongside space within a four-dimensional tensor. This approach fundamentally reframes time as a\ngeometric dimension rather than an independent parameter. Consequently, an entity's inherent state is\nperceived in terms of its energy rather than time itself."}, {"title": "C. Meaning of Time in Quantum Mechanics (Dalmonte and Montangero 2016; Kogut 1983)", "content": "Quantum mechanics provides an alternative perspective on time, treating it similarly to a wave-like\nfield that emerges from the interaction of objects. According to Heisenberg's uncertainty principle,\n$\\Delta x \\Delta p \\geq \\hbar/2$, when rewritten in terms of energy and time, it states that $\\Delta E \\Delta t \\geq \\hbar/2$. This implies that\nenergy and time cannot be simultaneously determined with precision\u2014an increase in accuracy for one\nlead to a loss of precision in the other. As a result, quantum mechanics views time as a discontinuous\nwave field. Furthermore, in the microscopic world governed by quantum mechanics, time is structured\nas a discrete mesh, reinforcing the concept of time as a field rather than an absolute continuum."}, {"title": "D. Summary \u2013 Hypothesis of Dimensional Expansion embodying crSTR or ABICND", "content": "If time is inseparable from space and must be understood as a field-like, discontinuous wave, then the\nconcept of time can be theoretically formulated within relational contexts. To illustrate this, consider\nthe chemical reaction in which hydrogen and oxygen molecules combine to form water. This reaction\nrepresents not merely a numerical aggregation but the formation of a directional field of existence. Once\nthis reaction occurs, the relationship between the elements generates a new form of directed energy"}, {"title": "E. Hypothesis Examination from Perspective of Systems Biology", "content": "If we adopt an extended view of transforming a 'plane' into a 'solid figure'-not as a singular process\nbut as a continuously progressing developmental trajectory, then, its analogy with biological evolution,\nparticularly in systems biology, becomes apparent. In biology, the stages of development proceed as\nfollows: from DNA to proteins, from proteins to cells, from cells to tissues, and from tissues to organs\n(Noble 2006, 2012). This progressive biological development phase bears a striking resemblance to the\nscalable developmental phase in AI, where data and algorithms interrelate to shape a continuously\nevolving system.\nIn AI, the relationship between nodes is not a one-time occurrence but a series of continuous events"}, {"title": "2) Second Conceptual Hypothesis \u2013 DCPSs of TC-EO", "content": "The initial conceptual hypothesis posits a process for capturing the expansion of dimensions\naccompanied by qualitative transformations, which differ from quantitative expansions in that they\nintroduce fundamental structural changes rather than mere incremental scaling. While quantitative"}, {"title": "A. From \u2018Typographical Number Theory (TNT)' to (TNT+G\u0ed6)\u2013PROOF\u2013PAIR{a, a'}", "content": "As stated in the preface of the 20th anniversary edition of GEB (Hofstadter 1999), Hofstadter attempts\nto address the question of how living beings can arise from non-living matter by shifting the focus from\nmaterial components to abstract patterns. Among the abstract patterns, he places particular emphasis on\nthe 'paradox of self-reference'. This self-reference paradox gives rise to a highly unusual loop, which\nbegins with Russell and, in particular, is based on G\u00f6del's discovery of 'G\u00f6del numbering', and then\nprogresses via the TNT string, 'I cannot be proved within the formal system TNT, that is, within the"}, {"title": "B. \"(TNT+G)-PROOF-PAIR{a, a'},\" \"Expansion of Dimensions as an Open World,\" and\n\"DCPSs of TC-EO\"", "content": "This study interprets the practical significance of Hofstadter's new formula-primarily based on the\nrepeated application of G\u00f6del's argument as the incorporation of the idea of \"Expansion of\nDimensions as an Open World\" into the existing G\u00f6delian framework. By doing so, the assessment of\ncompleteness should not be limited to a closed world evaluated solely on the basis of the lower-\ndimensional framework. Instead, even if it cannot be fully defined, an approach to dynamically\nengaging with the expanding world based on higher-dimensional perspectives as an open world\nbecomes feasible. In this regard, this study suggests that \"Expansion of Dimensions as an Open World\"\ncorresponds to the \"Expansion of Dimensions accompanied by Qualitative Transformation\" in the CFP\ntheory discussed earlier. Furthermore, it proposes that the hypothesis enabling such \"Expansion of\nDimensions accompanied by Qualitative Transformation\" is the DCPSs of TC-EO. The conceptual\nhypothesis of the DCPSs of TC-EO focuses on the dynamic expansion of the world as an open system,"}, {"title": "3) Summary \u2013 CFP theory functioned as new data science", "content": "This study has thus far proposed two conceptual hypotheses to represent the 'dimensional expansion\naccompanied by qualitative transformation' as a new developmental phase created by the unique\n'relationship modeling between data and algorithms' in DNNs. This approach builds upon the\nachievements of existing physics and biology, along with Hofstadter's reinterpretation of G\u00f6del's proof,\nformalized as (TNT+G)\u2013PROOF-PAIR{a, a'}. These two conceptual hypotheses offer new insights\ninto the relationships and structural integration between data features, between data, and between data\nand algorithms, ultimately providing a systematic framework for the convergence and fusion of high-\ndimensional and low-dimensional data structures. Accordingly, this study introduces and names this\ntheoretical framework the Convergent Fusion Paradigm (CFP) theory, positioning it as a new geometric\nperspective within data science."}, {"title": "3. Application of two conceptual hypotheses featuring CFP theory to Riemannian\nmanifold", "content": "The next step is to apply CFP theory to existing data generation models. CFP theory enables a precise\nunderstanding of the data generation process and offers novel insights by providing an alternative\ninterpretation of the Riemannian metric, which is frequently employed in the data manifold hypothesis."}, {"title": "1) Data manifold hypothesis and generative models", "content": "The concept of a 'manifold' originates from mathematics and describes a space that, while locally\nresembling Euclidean space, may have a much more complex global structure (Kelley 2017). The data\nmanifold hypothesis applies this concept to data, proposing that most naturally occurring high-\ndimensional datasets actually reside near low-dimensional non-linear manifolds (Belkin and Niyogi\n2003; Dominguez-Olmedo et al. 2023; Hastie and Stuetzle 1989; Smola et al. 2001). This hypothesis\nplays a crucial role\u00b9\u00b2 in machine learning and data analysis, serving as a foundation for understanding\nand utilizing the structural properties of data."}, {"title": "2) Expressivity of matrices and vector inner products in Euclidean coordinate space", "content": "Before delving into this topic, two key mathematical aspects should be addressed: the mathematical\nproperties of matrices and the vector inner product, which serves as the foundation for defining the\nRiemannian metric.\nThe initial topic of discussion will be the representation of matrices in Euclidean space. To illustrate,\nin the context of a two-dimensional coordinate plane, a matrix is essentially represented as m \u00d7 n, where\nm and n are positive integers. In order to represent a matrix that exceeds this in a two-dimensional\nEuclidean space, it is necessary to make quantitative adjustments to each row and column. However,\nattempting to explain all matrices as an extension of the dimension would exceed the existing two-\ndimensional Euclidean space. In such cases, the expanded dimension is expressed as a diagonal matrix.\nConsequently, since Euclidean spaces is linear, the expansion of the dimension is expressed as\nquantitatively expanded plane.\nNext, the geometric interpretation of the vector inner product in coordinate space will be examined. It\ncan be understood as a process wherein one vector becomes incorporated into another. From the\nperspective of CFP theory, this corresponds to a scenario in which object A converges with object B and\nmerges into a higher dimension, a phenomenon described as a 'higher-dimensional extension.' However,\na distinction exists between the vector inner product relationship and the relationship facilitating the\ngeneration of crSTR in CFP theory. While the vector inner product is constrained by its reliance on a\nlinear coordinate space, requiring that one of the entities A or B be the basis for inclusion, CFP"}, {"title": "3) Examining Ontological Meaning of Riemannian Metric", "content": "When understood in this way, the Riemannian metric j\u00b9 Myy can be interpreted as assigning each\nmatrix element of My (the Riemannian matrix, or more precisely, a symmetric positive definite matrix)\nas an inner product for each dimension. In this case, y(t) is a smooth curve, j(t) is the velocity of the\ncurve, and Myis a symmetric positive definite matrix that acts akin to a local Riemannian metric.\nFrom this perspective, the fundamental unit of the Riemannian metric can be understood ontologically\nas follows: it represents the transformation of a \"static, isolated, formal existence\" (y(t)) into a\n\"dynamic, relational, and substantial existence\" (j\u00b9 Myj). This transformation occurs by determining\nthe interrelation between the self (y) and its recursive counterpart (y\u201d, the transposed matrix). The\ndynamic existence represented by this transformation not only manifests its magnitude on the\ncoordinate plane as an inner product but also signifies an extension into a higher-dimensional\nframework.\nTo grasp the necessity of using a transposed matrix in dimensional changes and interpreting it as an\ninterrelation between the self (y) and its recursive counterpart (y), an ontological understanding is\nrequired. In an isolated dimension, the concept of existence can remain static. However, when\ndimensions change, a static notion of existence becomes insufficient, as any transformation in\ndimension inherently involves a relational engagement with other entities, including the self.\nConsequently, dimensional transformation necessitates shifting from a static, isolated notion of\nexistence to a dynamic, relational one. Reversing this understanding, it is only through the establishment"}, {"title": "4) Understanding from Perspective of CFP Theory \u2013 Possibilities and Limitations", "content": "From a philosophical and ontological perspective, if the Riemannian metric is understood as a\nmathematical concept that transforms a mere static and isolated unit of existence on a coordinate plane\ninto a dynamic relational unit accompanied by dimensional qualitative transformation, then within CFP\ntheory, the Riemannian metric itself can be interpreted as crSTR. According to CFP theory, the entities\nA and B should not be regarded as static, isolated existences but inherently possess relational meaning.\nTherefore, the entity as a fundamental unit of existence will transform into ABICND through crSTR.\nFurthermore, the notion that a unit of existence is established through crSTR implies that it should\nundergo a process of convergence and fusion with its recursive counterpart. This necessitates passing\nthrough DCPSs of TC-EO, which constitutes the second conceptual hypothesis of CFP theory. While\nthe relationship of \"Duplexity-Contradiction\" can be represented within Euclidean space, the dynamic\nprocess wherein this relationship undergoes thorough closure, leading to paradoxical eternal opening\nand the creation of new unexpected higher-dimensional entities, cannot be fully captured in the space.\nAdditionally, due to the geometric modular restriction that Euclidean space should be differentiable,\nthe concept of \"Duplex-Contradiction\" in CFP theory is reduced to the \"reflexive manifestation of an\nontological unit.\" In Euclidean space, \"Duplex-Contradiction\" manifests merely as \"reflexive\nmanifestation through the process of contrast.\" This reduction in the Duplex-Contradiction (DC)\nconcept inevitably diminishes the associated generation-related Paradox-Stratified (PS) concept. Owing\nto the inherent linearity of Euclidean space, dimensional transformation regresses into a linear\nexpression of mere \"quantitative expansion\" rather than \"generation accompanied by qualitative\ntransformation,\" resulting in a sort of 'retardation-shrinkage'. Consequently, the TC-EO process, which"}, {"title": "1. Preliminary Work \u2013 Understanding Spatial Structure Inside VAEs", "content": "Before summarizing and analyzing Arvanitidis's research pertaining to the establishment of the\nRiemannian metric, it is crucial to develop an understanding of the spatial structure within VAEs. This\nis because a thorough comprehension of the Riemannian metric, beyond its mathematical formulation,\nrequires an understanding of these geometric spatial structures.\nTo grasp the overall structure, two figures have been provided: a flat representation of the VAEs\nviewed from above and a three-dimensional representation viewed from the side. Based on this, we\nexamine how the structures of the ambient (input) space, latent space, and ambient (output) space within\nVAEs can be categorized according to their magnitude and path perspectives. This analysis is necessary\nbecause, while these spaces may appear structurally divided, they actually overlap.13 Understanding this\nsuperposition is essential to accurately conceptualizing the processes within VAEs. A correct\ninterpretation of these overlapping spaces allows us to extend and measure the Euclidean metric,\noriginally set in the ambient (input) space, into the latent space. Furthermore, it enables us to\ndifferentiate between the compression process from the ambient (input) space to the latent space and"}, {"title": "2) Evaluation", "content": "However, the method proposed by (Arvanitidis et al. 2018) \u2013setting the Riemannian metric in the\nlatent space by means of Jacobian Jy and measuring it in the ambient space through the Euclidean\nmeasure \u2013 raises some concerns. As shown in Eq. (3), the norm of ||Jy,\u0178t || is merely a single point in the\nambient space. Consequently, for the Euclidean measure to be applicable, the unit length should be\nrepresented as an integral between 0 and 1 in Eq. (2). This process ensures the feasibility of measuring\ncurve lengths in the ambient space X.\nHowever, complications arise when extending this method to the stochastic case. In the stochastic case,\nthe measured distance in the ambient (input) space should be pulled back into the latent space (requiring\na type of pull-back process), yet the precise explanation for this step remains unclear. It is important to\nnote that the ability to measure in the ambient (input) space does not inherently equate to the ability to\ntransfer those measurements back into the latent space. Acknowledging this issue, the subsequent study\n(Arvanitidis et al. 2020) introduces the inverse function of the Jacobian, allowing for the metric set in\nthe ambient (input) space and the measured results to be transferred together into the latent space.\nWithin this context, whether the approach of (Arvanitidis et al. 2018) can be strictly considered as a\npull-back metric or not remains questionable.\n(Arvanitidis et al. 2020) appears, nevertheless, to interpret the method proposed by (Arvanitidis et al.\n2018) as a pull-back metric. This is due to the fact that, in the stochastic case, the Jacobian matrix\nbecomes J\u338f(z) and the Riemann metric is also represented as M\u3047(\u03bc(z) + diag(\u03b5)\u00b7 \u03c3(z)), if observing\nthe overlap between the ambient (input) space and the latent space, from the perspective of generator.\nHowever, without setting the ambient metric in the ambient space, it is impossible to transfer the\nmeasurement results into the latent space. Considering this, (Arvanitidis et al. 2018) appears to have\nmeasured data density observations in the latent space using the Radial Basis Function (RBF) neural\nnetwork. While observing data density in the latent space is possible and furthermore, it is not\nmeaningless in that it supplements data distribution by means of the mean and variance of the Gaussian\ndistribution, measuring only in the latent space presents a significant limitation\u2014it cannot fully reflect\nthe density and curvature of data present in high-dimensional space."}, {"title": "3. Analysis of (Arvanitidis et al. 2020)", "content": ""}, {"title": "A. Endowing Ambient Space with Riemannian Metric and Pulling it Back into Latent Space (So-\ncalled Pull-back Metric)", "content": "As previously stated, (Arvanitidis et al. 2020) sets the Riemannian metric in the ambient space and\nallows this metric and its measured results to be pulled back into the latent space (so-called pull-back\nmetric).\nTo achieve this, (Arvanitidis et al. 2020, p2) assumes that ambient space x is equipped with the\nEuclidean metric M\u1ef9(x) = I\u266d and its restriction is utilized as the Riemannian metric on T\u2081M, which\nindicates tangential mapping of manifold M. Since the choice of M\u2084(\u00b7) has a direct impact on manifold\nM, we can utilize other metrics designed to encode high-level information.\nIn light of the aforementioned, the pull-back metric presented in (Arvanitidis et al. 2020) is as follows.\n$M_\\Psi(\\mu(z) + diag(\\epsilon) \\cdot \\sigma(z)), g(z) = \\mu(z) + diag(\\epsilon) \\cdot \\sigma(z)$\nBased on the above, the conditions for establishing the pull-back metric are as follows:\n(a) A Riemannian metric (Mx) must be defined in the ambient space.\n(b) The Riemannian metric (M) must be pulled back into the latent space through the inverse\nJacobian J4-1.\n$M(z) = J_{\\phi}^{-1}(z)M_x(\\phi^{-1}(z))J_{\\phi}^{-1}(z) = M_z = J_\\mathbbM_x(g(z))J_z$\nThrough the processes (a) and (b), the metric information of the ambient space is expressed in the\nlatent space, making it measurable within the latent space, as indicated in Eq. (5). In other words, the\ndistortion and curvature of the ambient space can be reflected in the measurement of the curve length\nin the latent space through this process."}, {"title": "B. Interpretation from Perspective of CFP theory", "content": "To understand the structural characteristics created by the pull-back metric outlined above, it is first"}, {"title": "C. Re-interpretation of Macroscopic View on Posterior Distribution via Lens of CFP Theory", "content": "The CFP theory summarized in the previous section provides a framework for reinterpreting the\ngeneration process of the generator, which is based on the existing stochastic approach that operates in\nboth the latent space and the ambient (output) space within VAEs. These two spaces overlap in the\nmacroscopic view when considering magnitude units.\nAs is well known, the outcome of a generator can be expressed as a conditional probability distribution,\np(x|z). The nonlinear relationship of p(x|z) from p(z) can be estimated using a DNN, which\nfunctions as a Universal Approximator (UA). To achieve this, first, assuming the prior distribution p(z)\nas a Gaussian distribution, the conditional probability p(x|z) can also be modeled as a Gaussian\ndistribution, where a sample z drawn from p(z) is input into the neural network.\n$p(x|z) = N(x|\\mu(z), I_p \\cdot \\sigma^2(z)), g(z) = \\mu(z) + diag(\\epsilon) \\cdot \\sigma(z)$"}, {"title": "2) Observation of Density Distribution", "content": ""}, {"title": "A. Learning Riemannian Metrics in Ambient Space", "content": "Since (Arvanitidis et al. 2020) establishes the Riemannian metric in the ambient (input) space, it\nenables the measurement of data density and curvature by learning the Riemannian metric in the\nambient space. This allows the effective utilization of high-dimensional data information present in the\nambient (input) space.\nThe method for measuring data density and curvature by learning the Riemannian metric is defined as\nfollows:\n$M_x(x) = (\\alpha h(x) + \\epsilon)^{-1} \\cdot I_p$\nwhere h(x): RD \u2192 R > 0, and x is near the data manifold, h(x)\u21921, otherwise h(x)\u21920.\nAt this time, one of the effective and simple approaches in relation to h(x) is to define h(x) =\nw\u00b9 p(x) using a positive Radial Basis Function (RBF) network (Que and Belkin 2016). Additionally,\nw\u2208 Ro and k(x) = exp(-0.5\u00b7\u03bb\u03ba\u00b7 ||x \u2212 xk||2) with bandwidth \u03bb > 0. Also, a and \u2208 > 0 is\nscaling factors that setting the lower and upper limits of the metric, respectively.\nIn summary, using RBF network is a key method for measuring information on data density and\ncurvature. The bandwidth o of the RBF network (lk in Eq. (6) above) is used as the \u03c3\u00b2 (variance) in the\npull-back metric through the inverse function of the Jacobian."}, {"title": "B. CFP Theory-Based Interpretation", "content": "On the other hand, if we understand the process of measuring the information on data density and\ncurvature from the aspect of the Riemannian metric as a learning process, as shown above, the viewpoint\nof CFP theory can provide us with insight into the geometric structure of the data. This is because, by\nmeans of an RBF network, can be interpreted as a train of microscopic continuous developmental"}, {"title": "C. Analysis of the Generator's side through the CFP theory", "content": "When looking at the Generator from the perspective of the CFP theory, it is understood as a process\nof integrating the ambient (input) space and latent space through the pull-back metric and processing it\ninto the ambient (output) space\u00b9\u2077. However, there is a fact that should be noted here. If the process of\npulling back higher-level information through the ambient (input) space metric using the pull-back\nmetric has an effect similar to using the posterior rather than the prior, the clusters of data generated by\nthe generator function in the ambient (output) space are bound to be different from those in the ambient\n(input) space connected to the latent space.\nThis discrepancy arises due to the fact that, in contrast to the conventional Auto Encoders, which\ninvolve data compression and subsequent restoration and reconstruction without modification, this\nprocess must converge and fuse the data clusters in the ambient (input) space and those in the latent\nspace into a unified entity in the ambient (output) space to generate a new data cluster. In this regard,\nAuto Encoders (AEs) naturally evolves into a VAEs as they employ more intricate and higher-\ndimensional data clusters.\nBased on the above, the implicit understanding of pullback metric method, which assumes that the\ninherent high-level information of the ambient (input) space is virtually the same as the ambient (output)\nspace, is bound to have subtle errors.\nUpon closer observation, these errors are effectively absorbed into the region of covariance o in the\nprior distribution p(z), serving as an error-handling mechanism within the pull-back matrix. This study\nargues, on the basis of Arvanitidis et al. (2020), that the difference between the data cluster in the\nambient (input) space and the newly generated data cluster in the ambient (output) space is evidence of\na truly unique and mystique process in which genuine creation occurs."}, {"title": "4. Analysis of (Arvanitidis et al. 2021)", "content": ""}, {"title": "1) Locally Conformally Flat Riemannian Metric", "content": "Firstly, this study reviews the process of setting a new prior-based Riemannian metric in the latent\nspace using EBM. The new locally conformally flat Riemannian metric is given by:\n$M_\\phi(z) = m(z) \\cdot I_d = (\\alpha \\cdot v_\\psi (z) + \\beta)^{-2/d} \\cdot I_d$\nwhere Id the d-dimensional identity matrix, a and \u1e9e the positive scaling constant which controls lower"}, {"title": "A. Introduction to the Noise Contrastive Prior (NCP)", "content": "To further explore the methodology employed in (Arvanitidis et al. 2021), we must examine how its\nlearnable prior adapts through training. The paper utilizes the \"Noise Contrastive Prior (NCP),\" as\nintroduced by (Aneja et al. 2020).\nAccording to (Aneja et al. 2020), NCP-VAEs training consists of two main stages. First, standard VAE\ntraining optimizes the base prior p(z), aligning it as closely as possible with the posterior p(z|x) (Stage\n1 \u2013 VAE training). Next, the re-weighting function r(z) is learned using Noise Contrastive Estimation\n(NCE), enhancing prior flexibility and posterior alignment (Stage 2 \u2013 NCE training).\""}, {"title": "B. Approximation of Posterior p(z|x) in NCP-VAEs, its Limitations and Contradictions", "content": "Originally, posterior p(z|x) is a variable z-distribution in latent space for real data x . The encoder\nnetwork of VAEs approximates a complex function that maps the input data x to the latent space Z using\na DNNs. This function maps the input data to a specific location in the latent space, which is expressed\nas the mean and variance. However, it is important to note that the posterior p(z|x) cannot be obtained\nexactly, so instead, an approximation based on the prior distribution is used, and in the process, the KL-\ndivergence is minimized to make it similar to the prior distribution p(z). As a result, in the process of\napproximating with DNNs, there happens a problem of arbitrarily extrapolating areas in the latent space\nwhere the data does not exist, based on the prior distribution. This process can cause the prediction of\nmeaningless probability distributions, ultimately causing problems with the model's reliability and\nconsistency.\nThis problem of arbitrary extrapolation can also affect NCP-VAEs. Even with NCP-VAEs, the data\ndistribution is not identical to the posterior p(z|x) because it is approximated via prior distribution p(z)\nin the DNN. Thus, extrapolation may still not be entirely resolved, and the problem of not accurately\nobtaining the posterior distribution may persist. Additionally, given that the goal of NCP-VAEs is to\nmake the prior p(z) closely approximate the posterior p(z|x), bringing the two prior distributions into\ncloser alignment rather than accurately estimating posterior characteristics results in a fundamental\ncontradiction. This"}]}