{"title": "Integrating Reinforcement Learning, Action Model\nLearning, and Numeric Planning for Tackling Complex\nTasks", "authors": ["Yarin Benyamin", "Argaman Mordoch", "Shahaf S. Shperberg", "Roni Stern"], "abstract": "Automated Planning algorithms require a model of the domain that spec-\nifies the preconditions and effects of each action. Obtaining such a domain\nmodel is notoriously hard. Algorithms for learning domain models exist, yet\nit remains unclear whether learning a domain model and planning is an effec-\ntive approach for numeric planning environments, i.e., where states include\ndiscrete and numeric state variables. In this work, we explore the benefits of\nlearning a numeric domain model and compare it with alternative model-free\nsolutions. As a case study, we use two tasks in Minecraft, a popular sandbox\ngame that has been used as an AI challenge. First, we consider an offline\nlearning setting, where a set of expert trajectories are available to learn from.\nThis is the standard setting for learning domain models. We used the Nu-\nmeric Safe Action Model Learning (NSAM) algorithm to learn a numeric\ndomain model and solve new problems with the learned domain model and a\nnumeric planner. We call this model-based solution NSAM(+p), and compare\nit to several model-free Imitation Learning (IL) and Offline Reinforcement\nLearning (RL) algorithms. Empirical results show that some IL algorithms\ncan learn faster to solve simple tasks, while NSAM(+p) allows solving tasks\nthat require long-term planning and enables generalizing to solve problems\nin larger environments. Then, we consider an online learning setting, where\nlearning is done by moving an agent in the environment. For this setting,\nwe introduce the Reinforcement learning, Action Model learning, and Plan-\nning (RAMP) hybrid strategy. In RAMP, observations collected during the", "sections": [{"title": "1. Introduction", "content": "Automated Planning is a state-of-the-art approach for solving goal-oriented\nsequential decision-making problems. Automated Planning algorithms do\nnot require interactions with the environment to decide which actions to\nperform to achieve the desired goals. Instead, they rely on a domain model\nthat defines the preconditions and effects of the agent's actions. Using a do-\nmain model allows some planning algorithms to have formal guarantees on\ntheir behavior and output, which provides reliability and predictability. The\ndependency on a domain model is also a significant limitation since obtaining\nsuch a model can be very difficult, even for human experts.\nTo mitigate this limitation, prior work proposed automated methods for\nlearning domain models (Juba et al., 2021; Wang, 1994; Aineto et al., 2019;\nLamanna and Serafini, 2024; Xi et al., 2024a; Mordoch et al., 2024) from\nobservations. While these learning methods have shown some promise, their\napplication beyond standard automated planning benchmarks remains lim-\nited (Chitnis et al., 2021; Jin et al., 2022; James et al., 2022; Sreedharan and\nKatz, 2023). This is especially true in numeric planning environments, which\nare environments that include both discrete and numeric state variables. In-\ndeed, there are very few algorithms for learning numeric planning action\nmodels (Mordoch et al., 2023; Segura-Muros et al., 2021), and numeric plan-\nners are less developed than planners for classical, discrete, environments.\nTherefore, one may consider using an alternative model-free approach to\nsolve numeric planning problems. Depending on the exact problem setting,\none may use Imitation Learning (IL) and Reinforcement Learning (RL) algo-\nrithms as alternatives. These types of algorithms follow either a model-free\nor model-based approach. In model-free methods, observations are used to\nlearn a state-action value function, which helps derive a policy, or to learn\nthe policy directly\u2014without attempting to model the environment's dynam-\nics. In contrast, model-based approaches aim to learn both the transition"}, {"title": "2. Background", "content": "In this section, we provide relevant background in numeric planning,\nlearning domain models for planning, and RL. We also provide background\non Minecraft, which is the environment we used in our experimental results."}, {"title": "2.1. Numeric Planning", "content": "Numeric planning generally refers to solving planning problems in do-\nmains where action outcomes are deterministic, states are fully observable,\nand the states are described with discrete and continuous variables. Nu-\nmeric planning problems can be defined using extensions of the Planning\nDomain Definition Language (PDDL) (Aeronautiques et al., 1998), such as\nPDDL2.1 (Fox and Long, 2003). A numeric planning domain can be defined\nby a tuple D = (F, X, A) where F is a finite set of Boolean variables, X is a\nset of numeric variables, and A is a set of actions. A state is an assignment\nof values to all variables in FUX. Every action a \u2208 A is defined by a tuple\n(name(a), pre(a), eff(a)) representing the action's name, preconditions, and\neffects, respectively. Preconditions are assignments over the Boolean vari-\nables and conditions over the numeric variables, specifying when the action\ncan be applied. The effects of an action are a set of assignments over F and\nX, representing how the state changes after applying a. The set of actions\nwith their definitions is referred to as the action model of the domain. For a\nstate s, action a, and action model M, we denote by $a_M(s)$ the state resulting\nfrom applying a in state s according to the effects specified by M. We omit\nM in this notation when it is clear from the context. A planning problem in\nPDDL is defined by (D, so, G) where D is a domain, so is the initial state,\nand G are the problem goals. The problem goals G are assignments of values\nto a subset of the Boolean variables and a set of conditions over the numeric\nvariables. A goal state is a state that satisfies G. A solution to a planning\nproblem is a plan, i.e., a sequence of actions applicable in so and resulting in\na goal state. Planning problems and domains are often specified in a lifted\nform. This means that a planning domain defines parameterized predicates\nand functions, rather than Boolean and numeric variables, respectively, and\nthe actions are parameterized actions as well. A planning problem defines a"}, {"title": "2.2. Learning Domain Models for Planning", "content": "Automated planning algorithms assume the existence of a symbolic model\nof the actions' preconditions and effects. However, obtaining a symbolic\nmodel is challenging, leading to the development of automated action model\nlearning algorithms that learn from observing the agent's behavior. In this\nwork, we explore two settings of learning, offline and online. Offline domain\nmodel learning algorithms accept observations and output a domain model.\nOnline domain model learning algorithms choose which actions to perform in\norder to collect new observations and use these observations to continuously\nupdate and refine the domain model they eventually return.\nAlgorithms for learning domain models vary in the assumptions they\nmake on the available observations and the guarantees they provide on the\naction model they return (Cresswell et al., 2013; Amir and Chang, 2008;\nYang et al., 2007; Aineto et al., 2019; Juba et al., 2021). In Section 6 we\nprovide a brief overview of existing action model learning algorithms and a\nsummary of their properties and assumptions.\nTo the best of our knowledge, there is no online action model learning\nalgorithm that returns a numeric action model. The only offline action model\nlearning algorithms capable of learning a numeric action model are the Nu-"}, {"title": "2.3. Reinforcement Learning and Imitation Learning", "content": "RL is a field of AI in which agents learn to make decisions by interact-\ning with an environment and receiving feedback in the form of rewards (or\npenalties). There are two types of RL algorithms: off policy and on policy.\nOff-policy algorithms are algorithms that can learn from data collected by\nany policy and on-policy algorithms learn only from data collected by the\ncurrent policy. DQN (Mnih et al., 2015) and PPO (Schulman et al., 2017) are\nprominent examples of off-policy and on-policy RL algorithms, respectively.\nEach algorithm has its pros and cons, but PPO is renowned for its robust\nperformance even in the absence of extensive hyperparameter tuning.\nIL (Schaal, 1999) is a related research field where an AI agent is trained to\n\"mimic human behavior in a given task\" (Hussein et al., 2017). Two IL algo-\nrithms we discuss in this work are BC (Pomerleau, 1988) and GAIL (Ho and\nErmon, 2016). BC employs supervised learning to mimic the expert's policy,\nwhile GAIL takes a unique adversarial approach by simultaneously training\na policy and a discriminator. The discriminator's role is to distinguish be-\ntween expert observations and those generated by the learned policy. Offline\nRL (Sutton and Barto, 2018) is similar to IL except that the given trajectories\nencompass not only the states and actions but also the rewards associated\nwith each transition. Offline RL algorithms aim not to mimic these trajecto-\nries but to learn from them how to maximize future rewards. Consequently,\noff-policy algorithms, such as DQN, can also serve as an offline RL algorithm."}, {"title": "3. Planning Benchmarks in Minecraft", "content": "Automated planners are often evaluated on planning problems from the\nInternational Planning Competition (IPC) (Taitler et al., 2024). Thus, they\nmay be inadvertently optimized towards solving these types of problems. To\nprovide a fairer comparison between model-based and model-free approaches,\nthe experiments in this work were deliberately performed in an environment\nthat is not part of the IPC, namely Minecraft. Minecraft is an open-world"}, {"title": "3.1. Planning Tasks", "content": "For our evaluation, we used two tasks, namely Craft Wooden Sword and\nCraft Wooden Pogo, within the PAL Minecraft environment (Goss et al.,\n2023). In both tasks, Steve is located in a field comprising N \u00d7 N blocks\nand surrounded by unbreakable bedrock walls. The field includes multiple\ntrees and a crafting table, as shown in Figure 1. The initial input includes\nthe map data, i.e., what exists in each cell, as well as the number of items\nit is currently holding in the inventory. We consider two Minecraft tasks in\nthis environment.\nTask 1: Craft Wooden Sword. In this task, Steve must:\n1. Harvest at least one wood block from trees."}, {"title": "3.2. Problem Generators", "content": "The original task in PAL was predefined with a fixed map size and pre-\ndetermined positions for the trees, the crafting table, and the agent. In\naddition, Steve always started with an empty inventory. To generate more\ndiverse initial states and problems, we developed a problem generator that\nrandomizes: (1) the agent's starting position on the map, (2) the quantity\nand placement of trees, and (3) the items present in the agent's inventory.\nThe problem generator also accepts as input the size of the desired map. Con-\nsequently, different sequences of actions are needed to solve different problem\ninstances.\nIn our experiments, we generated maps of sizes 6 \u00d7 6, 10 \u00d7 10, and 15 \u00d7 15.\nThe configured initial the number of items in the inventory to range from zero\nto eight for all items, except for the last major items required to achieve the\ngoal item, which is a \u201cStick", "Tree Tap\"\nand \\\"Sack\\\" for the Craft Wooden Pogo. These major items were always set\nto not be in the inventory of the initial state, to ensure that the planning\nproblem is not trivial. We set the number of trees on each map to range from\nzero to (map size)/3.\"\n    },\n    {\n      \"title\": \"4. Offline Learning of Numeric Action Models\",\n      \"content\": \"In this setting, we are given an initial state s, a goal to achieve g, and\na set of expert trajectories T created by observing an expert solve different\nproblems in the same environment. The agent's objective in this setting is\nto learn from T how to output a plan or a policy for achieving g from its\ncurrent state (s). Learning and planning in this setting are done offline,\nthat is, before the agent performs any action in the environment. The main\nobjective is to be able to solve other problems in the domain. We consider\ntwo main approaches in this offline learning setting: a model-free approach\nand a model-based approach.\"\n    },\n    {\n      \"title\": \"4.1. Model-free approach\",\n      \"content\": \"IL algorithms (Schaal, 1999) and Offline RL algorithms (Sutton and Barto,\n2018) are suitable for solving our offline learning setting. In our experi-\nments, we implemented two IL algorithms, namely BC (Pomerleau, 1988)\nand GAIL (Ho and Ermon, 2016), and two Offline RL algorithms, namely\nDQN (Mnih et al., 2013) and QR-DQN, an extension of DQN which approx-\nimates the full distribution of returns using quantile regression, enabling\nbetter risk-sensitive decision-making and improved learning stability. The IL\nalgorithms can be used as-is given the set of expert trajectories. To use an\noffline RL algorithm, we must define a reward function that associates a state\nwith the benefit of reaching it. In a goal-oriented task like ours, the natural\nway to define the reward function is for the agent to receive a positive reward\nif it achieves its goal, and zero otherwise. We define our reward function in\nthis way: the agent receives a unit reward if it achieves the goal and zero\notherwise.\nTo allow using off-the-shelf RL and IL algorithms in our Minecraft bench-\nmark, we implemented a translation mechanism that converts Minecraft\nstates and actions into states and actions in a dedicated AI gym environ-\nment (Brockman et al., 2016). The resulting task is not easy since the num-\nber of actions in each state is $N^2 + 5$ for the Craft Wooden Sword task and\n$N^2 + 6$ for the Craft Wooden Pogo task, where N is the width and height of\nthe map. Moreover, most actions are TP-TO actions, which do not provide\nany positive reward.\"\n    },\n    {\n      \"title\": \"4.2. Model-based approach\",\n      \"content\": \"In this approach, we learn a symbolic numeric planning domain model\nfrom the given expert trajectories T and use an automated planner to find\na plan to achieve the goal. In our implementation, we provide the expert\nobservations as input trajectories to NSAM (Mordoch et al., 2023), which\noutputs a PDDL domain model of the environment. Then, we used an off-\nthe-shelf domain-independent numeric planner to solve the resulting planning\nproblem and output a plan for the given task. In our experiments, we used\nMetric-FF (Hoffmann, 2003), a state-of-the-art numeric planner, but any\ndomain-independent numeric planner can be used instead.\nTo use this approach in our Minecraft benchmark, we implemented a\ntranslation mechanism that converts Minecraft states and actions into cor-\nresponding PDDL2.1 states and actions (see Figure 3). Specifically, each\ncell in the map is represented as an object, with the locations of Steve, the\"\n    },\n    {\n      \"title\": \"4.3. Experimental Setup\",\n      \"content\": \"We generated 1,000 instances of the Craft Wooden Pogo and 200 instances\nof the Craft Wooden Sword for every map size (6 \u00d7 6, 10 \u00d7 10, and 15 \u00d7 15).\nWe divided these instances into training and test sets with a 4:1 ratio. To\nensure robustness and generalization, we repeated this process in a 5-fold\nsplit, calculating the average and standard deviation over the five folds.\nFor every instance in the training set, we generated a solution using an\nexpert agent capable of solving the task. This expert agent was constructed\nby manually modeling the task as a numeric planning problem and employ-\ning MetricFF to solve it. Every generated plan was validated within the\nenvironment.\"\n    },\n    {\n      \"title\": \"4.4. Experimental Results\",\n      \"content\": \"Despite exhaustive efforts in hyperparameter tuning and experimentation\nwith various network architectures, GAIL, DQN, and QR-DQN consistently\nfailed to solve the Craft Wooden Sword and Craft Wooden Pogo tasks across\nall experiments. Thus, we only show results for NSAM(+p) and BC.\"\n    },\n    {\n      \"title\": \"4.4.1. Zero-Shot Transfer Results\",\n      \"content\": \"A key advantage of planning lies in its capacity to generalize across vary-\ning numbers of objects. In contrast, offline RL and IL algorithms do not inher-\"\n    },\n    {\n      \"title\": \"5. Online Learning of Numeric Action Models\",\n      \"content\": \"In the online learning setting, no expert trajectories are given, and we con-\ntrol an agent that interacts with the environment in a sequence of episodes.\"\n    },\n    {\n      \"title\": \"5.1. The RAMP Hybrid Strategy\",\n      \"content\": \"The model-based solution described above (NSAM(+p)) for the offline\nlearning setting cannot be directly applied in our online problem setting\nsince it lacks a mechanism for collecting trajectories, which is crucial in an\nonline environment. To address this, we introduce a novel hybrid strategy\nthat we call RAMP. RAMP integrates three components: an RL algorithm, the\"\n    },\n    {\n      \"title\": \"Searching for shortcuts in an existing trajectory\",\n      \"content\": \"The input to the search\nfor shortcuts process is a trajectory T = (so, a1,81,..., An, Sn), which was\ngenerated by running the RL algorithm in an episode until the agent has\nreached the goal. First, we remove loops in T. That is, we iterate over\nthe states visited by the agent in the episode and remove every sequence of\nactions in the plan that start and end in the same state. Then, we search\nfor additional shortcuts by searching for sequences of actions in T that can\nbe replaced by a single action according to MNSAM. When such a sequence\nis found, we replace it in T with the corresponding single action. There are\nmultiple ways to search for such sequences of actions. In our implementation,\nwe performed the following simple procedure to do so. First, we set sto to be\nthe last state in T, s to be the state before it, and sfrom the state before s.\nI.e., Sfrom, S, Sto are initialized as the three last (consecutive) states in T. We\ndenote by actions(T)| the number of actions in T. Thus, $S_{to} = S_{|actions(T)|},$\n$S = S_{|actions(T)|\u22121},$ and $S_{from} = S_{|actions(T)|-2}.$ Next, we check if, according to\nMNSAM, there is an action a' that is applicable in sfrom, and if we apply a' in\nSfrom, we directly reach sto. If this is the case, we replace in T the sequence\nof states and actions starting from Sfrom and ending in sto with the triplet\"\n    },\n    {\n      \"title\": \"Feedback loop to the RL algorithm\",\n      \"content\": \"Every trajectory executed by following a\nplan returned by the planner is also given to the RL algorithm as an additional\nepisode for training. In addition, if applying the search for shortcuts on\nan executed trajectory was able to generate a shorter trajectory, then the\nresulting trajectory is also given to the RL algorithm as an additional episode\nfor training. Note that the shortened trajectory is a valid trajectory because\nthe domains are deterministic, and thus loops are redundant, and due to\nNSAM's safety property.\nThis integration of model-based and model-free algorithms in our hybrid\nstrategy establishes a symbiotic relationship between them. The model-free\nRL algorithm acts as a methodological tool to solve problems when planning\nfails, as well as to gather information in a goal-oriented manner, leveraging\nits inherent ability to balance exploration and exploitation. Simultaneously,\nthe RL algorithm benefits from this partnership since trajectories created by\nthe model-based algorithm (either by the planner or by the shortcut search\nprocess) often tend to represent more efficient ways to solve the task at hand.\nThus, these trajectories provide high-quality data that can improve sample\nefficiency and stabilize the learning process. Our experimental results show\nthat this integrative approach of automated planning and RL can significantly\nsurpass standard RL techniques.\"\n    },\n    {\n      \"title\": \"Details on using PPO in RAMP\",\n      \"content\": \"After experimenting with several RL algo-\nrithms, we used in our experiments PPO. PPO is an on-policy algorithm,\nallowing it to learn from trajectories created by the planner forcing the algo-\nrithm to sample the action selected by the planner for these states. However,\nPPO uses a clipping mechanism designed to constrain policy updates by pre-\nventing the probability ratio between the new and old policies from deviating\ntoo far from 1. When the agent is forced to follow an expert plan, its own\nlearned policy may assign a low probability to the dictated actions, creating\na significant discrepancy between the executed actions and those preferred\nby the current policy. This can cause the importance-weighted policy ratio to\nfrequently fall outside the clipping range, effectively nullifying the gradient\nupdate and slowing down learning. As a result, in some cases, PPO may\nstruggle to meaningfully adjust its policy, particularly if the expert actions\nare substantially different from what it would naturally choose. To address\nthis issue, we adopt an approach for masking invalid actions (Huang and\nOnta\u00f1\u00f3n, 2022). Specifically, we treat the expert action in each state as the\nonly valid action and mask out all others. This ensures that the logits of\"\n    },\n    {\n      \"title\": \"5.2. Experimental Setup\",\n      \"content\": \"We conducted an experimental evaluation to compare PPO with action\nmasking and RAMP. We used the same Minecraft domain, tasks, and maps\ndescribed in Section 4.3. For each task and map size, we created 50 different\nproblem instances. Then, we run each of the evaluated algorithms on each\nproblem instance for a fixed number of steps, denoted Bi, before passing\nto the next problem instance. Additionally, in every episode, we run the\nevaluated algorithm on the current problem instance until either the agent\nreaches the goal or it has performed a fixed number of steps, denoted Be. We\nset Bi and Be differently for the Craft Wooden Sword and Craft Wooden Pogo\nsince they differ in their difficulty and required plan length. For the Craft\nWooden Sword task, B\u2081 = 800 and Be = 200, while for the Craft Wooden\nPogo task, B\u2081 = 6,000 and Be = 1,500. Note that Bi > Be, and thus each\nalgorithm will run multiple episodes on the same problem instance before\npassing to the next problem instance. This allows the evaluated algorithms\nto learn better how to solve problems in this non-trivial environment. In\neach experiment, we measured how many problems each algorithm was able\nto solve and the minimum plan length it found for each problem instance.\nSince PPO is a stochastic algorithm, we repeated every experiment five times,\neach time using a different random seed.\"\n    },\n    {\n      \"title\": \"5.3. Experimental Results\",\n      \"content\": \"Table 2 shows the average success rate for every task, algorithm (columns),\nmap size, and number of problem instances the agent has already trained on\n(rows). The columns R, R(-p), and R(-pn), correspond to RAMP, RAMP(\u2212p),\nand RAMP(-pn), respectively. The best result in each configuration is high-\nlighted in bold. These results show that all variants of RAMP outperform PPO\nin all configurations but the simplest one (Craft Wooden Sword, 6 \u00d7 6). For\nexample, in the Craft Wooden Pogo task, map size 15 \u00d7 15, and 50 problem\ninstances, PPO reached only 0.68 average success rate while RAMP achieved\nan average success rate of 0.84.\nOur ablation study on the different components of RAMP reveals surpris-\ning results. Only removing loops and providing this back to the PPO algo-\"\n    },\n    {\n      \"title\": \"6. Related Work\",\n      \"content\": \"In this section, we discuss several lines of research that are related to this\nwork.\"\n    },\n    {\n      \"title\": \"6.1. Offline Action Model Learning Algorithms\",\n      \"content\": \"FAMA (Aineto et al., 2019) is an offline action model learning algorithm\nthat can handle missing observations and outputs a classical planning do-\nmain model. It frames the task of learning an action model as a planning\nproblem, ensuring that the returned action model is consistent with the pro-\nvided observations. NOLAM (Lamanna and Serafini, 2024) can learn action\nmodels even from noisy trajectories. LOCM (Cresswell et al., 2013) learns\nan action model from observed sequences of actions and their signatures,\nwithout observing the states in the trajectory.\nThe Safe Action Model (SAM) learning algorithm Stern and Juba (2017);\nJuba et al. (2021) differs from the above algorithm in that the action model\nit returns provides a form of \u201csafety": "uarantee: not only is it consistent with\nthe given observations but it is also guaranteed that every plan created with\nthe learned action model is sound with respect to the real, unknown action\nmodel (Definition 1). SAM has been extended to support lifted action model\nrepresentation (Juba et al., 2021), partial observability (Le et al., 2024),\nstochastic effects (Juba and Stern, 2022), and conditional effects Mordoch\net al. (2024). All SAM algorithms require observing all the actions in the\ngiven trajectories, and most also require observing the states. The NSAM\nalgorithm used in this work is a member of this family of algorithms.\nAll the mentioned above algorithms assume the given observations are\nsymbolic. Recent work explored how to learn action models from raw im-\nages. LatPlan (Asai and Fukunaga, 2018) learns propositional action models\nin the latent space using a variational autoencoder. They use the Gumbel-\nSoftmax technique (Jang et al., 2017) to convert the continuous output of an\nautoencoder into categorical variables. These categorical variables are used\nas propositional symbols in a symbolic reasoning system, which in LatPLan's\ncase is a symbolic action model. ROSAME-I (Xi et al., 2024b), like Lat-\nPlan, learns action models from visual inputs. Unlike LatPlan, ROSAME-I\nrequires knowing the set of possible propositions and action signatures as in-\nput. ROSAME-I simultaneously learns classifiers for identifying propositions\nin a given image and infers a lifted, first-order action model defined over the\ngiven set of propositions and actions.\nNSAM Mordoch et al. (2023) and PlanMiner (Segura-Muros et al., 2021)\nare, to the best of our knowledge, the only algorithms capable of learning\naction models that include both discrete and continuous preconditions and"}, {"title": "6.2. Online Action Model Learning Algorithms", "content": "Online action-model learning algorithms iteratively learn an incumbent\naction model and choose the next actions to perform in order to collect\nobservations that enable further refinement of the incumbent action model.\nOLAM (Lamanna et al., 2021) is an online action model learning algorithm\nthat is designed for classical planning domains. It identifies in every iteration\nan action and a state where trying to execute that action is expected to refine\nthe incumbent action model. Then, it uses a planner to find a plan to reach\nthat state and attempts to execute the chosen action. GLIB (Chitnis et al.,\n2021) follows a similar approach but is designed for stochastic environments,\nresulting in a Probabilistic PDDL (PPDDL) action model. QACE (Verma\net al., 2023) is an action model learning algorithm that can also query a black-\nbox expert. It outputs a PPDDL action model with the same capabilities\nas the black-box expert it trained from. Karia et al. 2023 extends QACE\nto address the non-stationarity of the environment, i.e., address cases where\nthe environment dynamics change. QACE+ achieves this by interleaving\nplanning and learning and focusing on learning only the models essential for\nthe tasks at hand. ILM (Ng and Petrick, 2019) employs an explore-exploit\nstrategy: if it reaches a state from which the goal can be achieved, it exploits\nthis state; otherwise, it explores through random walks. Instead of focusing\nsolely on reaching a specific goal, the agent can take a broader approach by\nexploring the environment and aiming for an interesting state in it."}, {"title": "6.3. Summary: Action Model Learning Algorithms", "content": "Table 3 provides an overview of all action model learning algorithms de-\nscribed above. Every row represents a model-learning algorithm, and every\ncolumn represents a property of action model-learning algorithms. Column\n\u201cInput\" refers to the type of input given to the learning algorithm, namely\nif it is symbolic or visual. Columns\u201cNumeric\u201d and \u201cStochastic\u201d refer to\nwhether the underlying environment includes numeric state variables and\nstochastic effects, respectively. Column \u201cNS\u201d (non-stationarity) refers to\nwhether the dynamics of the underlying environment, i.e., the actions' pre-\nconditions and effects, may change during learning. Columns\u201cNoisy\u201d and\n\"Obs.\" refer to whether the states and actions in the given observations are\nnoisy and fully observed, respectively. Note that if the \u201cInput\u201d is visual,\nthe algorithm can handle noise due to its use of function approximation for\nimage processing. This is indicated by \u201c-\u201d in Table 3. The \u201cOnline/Offline\u201d\ncolumn refers to whether the learning algorithm is an online algorithm or\""}, {"title": "6.4. AI Agents for Minecraft", "content": "In our work, Minecraft is merely a benchmark for the evaluation of the\nproposed models and algorithms. However, many prior works have focused\non the problem of solving various Minecraft tasks. Indeed, both Planning\nand RL have been used to design AI agents for Minecraft. We provide a brief\nreview of these efforts below.\nRoberts et al. (2017) developed an AI Minecraft agent based on auto-\nmated planning. They designed a custom PDDL+ Fox and Long (2002)\ndomain model for the Minecraft task. Their agent was able to achieve this\ntask by using a PDDL+ planner and a goal reasoner. Adapting their PDDL+\ndomain to different tasks is not trivial. Wichlacz et al. (2019) used PDDL\nmodeling to solve complex construction tasks in Minecraft. They mod-\neled house-construction tasks as classical and as Hierarchical Task Network\n(HTN) (Georgievski and Aiello, 2015) planning problems. They observed\nthat even simple tasks pose challenges for classical planners as the size of\nthe world increases. In contrast, the HTN planner scaled better but was too\ncoupled with specific tasks.\nJames et al. (2022) proposed an algorithm for learning a PPDDL domain\nfrom visual input in Minecraft. We refer to their algorithm as the JRK\nalgorithm. JRK processes the visual input to identify objects and learn\na grounded PPDDL domain. Then, it learns a lifted PPDDL domain by\ngrouping objects into types based on the similarity of their preconditions\nand effects. The domain they learned is purely propositional, while for the\nMinecraft tasks we require domain models that include both propositions\nand numeric state variables.\nHierarchical Deep Reinforcement Learning Network (H-DRLN) (Tessler\net al., 2017) is commonly used in successful MineRL agents. When using\nH-DRLN, the agent continuously learns multiple policies and adapts to new\nchallenges within the game. The H-DRLN leverages a deep neural network\nto model the policy and value functions, resulting in high effectiveness across\na variety of Minecraft tasks such as navigation, mining, and combat. Despite\nits success, this approach requires tens to hundreds of thousands of steps for\nsingle, non-hierarchical tasks, while more complex, multi-task hierarchical"}, {"title": "7. Conclusions and Future Work", "content": "In this work", "settings": "offline learning with expert observations and\nonline learning. In particular, we compared using standard model-free algo-\nrithms for solving numeric planning problems in these settings with model-\nbased algorithms that require learning the domain model. As a benchmark,\nwe used two tasks in Minecraft, and provided both planning and RL trans-\nlation mechanisms to allow using off-the-shelf model-free and model-based\nalgorithms, where possible.\nFor the offline learning setting, we evaluated standard IL and Offline RL\nalgorithms, namely BC, GAIL, DQN, and QR-DQN. The model-based alterna-\ntive for this setting, which we called NSAM(+p), uses NSAM (Mordoch et al.,\n2023) to learn a numeric domain model of the environment and then a plan-\nner to find a plan with the learned domain model. Our results showed that\nBC yields better results in the simpler Minecraft task, but was significantly\noutperformed by NSAM(+p) in the harder task.\nFor the online learning setting, we evaluated standard RL algorithms,\nnamely, PPO and DQN. There are no algorithms for online learning of numeric\ndomain models, and therefore, we proposed a novel hybrid strategy that we\ncall RAMP. In RAMP, we run RL and NSAM simultaneously, both learning\nfrom observed trajectories. The domain model gradually learned by NSAM\nis used to find plans as well as find shortcuts in the plans found by the RL\nalgorithm. The plans and trajectories found by each algorithm are used to\nimprove the other, yielding better RL policies and more accurate domain\nmodels. Experimental results show that RAMP outperforms standard RL\nalgorithms significantly, in terms of both success rate and quality of solution.\nThis work, and in particular the proposed hybrid"}]}