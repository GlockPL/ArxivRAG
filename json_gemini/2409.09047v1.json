{"title": "AI Meets the Classroom: When Does ChatGPT Harm Learning?", "authors": ["Matthias Lehmann", "Philipp B. Cornelius", "Fabian J. Sting"], "abstract": "In this paper, we study how generative AI and specifically large language models (LLMs) impact learning in coding classes. We show across three studies that LLM usage can have positive and negative effects on learning outcomes. Using observational data from university-level programming courses, we establish such effects in the field. We replicate these findings in subsequent experimental studies, which closely resemble typical learning scenarios, to show causality. We find evidence for two contrasting mechanisms that determine the overall effect of LLM usage on learning. Students who use LLMs as personal tutors by conversing about the topic and asking for explanations benefit from usage. However, learning is impaired for students who excessively rely on LLMs to solve practice exercises for them and thus do not invest sufficient own mental effort. Those who never used LLMs before are particularly prone to such adverse behavior. Students without prior domain knowledge gain more from having access to LLMs. Finally, we show that the self-perceived benefits of using LLMs for learning exceed the actual benefits, potentially resulting in an overestimation of one's own abilities. Overall, our findings show promising potential of LLMs as learning support, however also that students have to be very cautious of possible pitfalls.", "sections": [{"title": "1. Introduction", "content": "Unlike previous AI tools, generative AI is a general-purpose technology that can perform a wide range of knowledge tasks that it was not specifically trained on. This especially applies to large language models (LLMs), the most prominent instance of generative AI, which generate text conditioned on user prompts. While LLMs are not yet good enough to perform many knowledge tasks autonomously, workers supported by LLMs significantly increase their productivity (Brynjolfsson et al. 2023, Dell'Acqua et al. 2023). This is particularly true in education, where students now have round-the-clock access to a \u201cpersonal tutor\u201d, who answers questions, helps with homework, writes summaries, and clarifies difficult concepts. However, learning-by-doing, that is deep personal engagement with the problems at hand, is a key part of knowledge work, including education (e.g., Narayanan et al. 2009, Staats and Gino 2012, Smilowitz and Keppler 2020). If LLMs replace some parts of that engagement, learning and subsequent performance may be impeded. By shedding more light on this tension, our study makes a step towards better understanding how the availability of LLMs affects student learning.\nUsing field data and two incentivized and pre-registered laboratory experiments, we find that LLMs increase learning outcomes if students use it to ask for explanations, but hampers learning if students ask for solutions to practice exercises. The availability of copy-and-paste enables such solution-seeking behavior and thus decreases learning. Less advanced students benefit more from LLMs. Lastly, we find that LLMs increase students' perceived learning progress, even when controlling for actual progress, suggesting that LLMs lead to an overestimation of their own abilities.\nWe contribute to the literature on the operational aspects of education (e.g., Keppler et al. 2022, Keppler 2024), and in particular on the use of technology in education (Smilowitz and Keppler 2020). The emerging empirical literature on the use of LLMs in education reports negative and null learning effects in a high school (Bastani et al. 2024) as well as a positive learning effect in an online coding course (Nie et al. 2024). Both of these studies use system"}, {"title": "2. Related Literature", "content": "The emerging research on large-language models (LLMs) in education has broadly covered two themes: The use of LLMs by teachers and the use of LLMs by students.\nFor teachers, LLMs promise to support lesson planning, content generation, and assess-ment (Choi et al. 2023, Jeon and Lee 2023). However, LLMs do not appear to improve teacher productivity across all tasks. In an exploratory study of K12 teachers, Keppler et al. (2024) find that teachers who use OpenAI's ChatGPT to complete a task (e.g., make a lesson plan) perceive themselves to be less productive than teachers who use ChatGPT to explain a concept (e.g., iterate over a lesson plan).\nFor students, LLMs promise to be a personal tutor available 24/7. Initial research qualitatively discusses potential opportunities and risks of such LLM tutors (Mollick and Mollick 2022, Extance 2023, Kasneci et al. 2023, Meyer et al. 2023). Empirical evidence is also starting to emerge. In both in-person and online programming classes, students largely perceive LLM tutors to be helpful (Liu et al. 2024b). In a set of online experiments in mathematics, students prefer human explanations over pre-generated LLM explanations (Pardos and Bhandari 2023, Prihar et al. 2023), but LLM-generated explanations still improve students' learning outcomes (Kumar et al. 2023, Pardos and Bhandari 2023). Notably in these studies, students do not directly interact with an LLM, but instead are shown pre-generated explanations.\nThe work closest to ours studies the effect of LLMs on learning when students are allowed to interact with the LLM. Doing so, Bastani et al. (2024) run a field experiment in math classes at a high school. They find that ChatGPT improves student performance on practice problems, but reduces performance in the final exam when students no longer have access to it. In addition, they show that this harmful effect on learning can be mitigated by modifying the LLM so that it produces fewer errors and does not directly provide solutions to students.\nNie et al. (2024) conduct a similar field experiment in an online programming course. They observe a very limited usage of ChatGPT and document that it reduces engagement with"}, {"title": "3. Theory", "content": "In this section, we outline the theoretical foundations of our study. We discuss the potential effects of LLM access when learning to code by drawing on two opposing perspectives in Section 3.1 and Section 3.2. Consequentially, we will derive competing hypotheses, which we have preregistered\u00b9. Thus, we also emphasize the complexity of this topic and the lack of knowledge pertaining to the usage of LLMs for learning."}, {"title": "3.1. LLMs as Personal Tutors", "content": "In the past, the introduction of new technologies such as computer assistance have had a generally positive effect on learning (Gulek and Demirtas 2005, Tamim et al. 2011, Li and Ma 2010, Chang et al. 2022). More specifically, it is generally well accepted that tutoring improves students' learning outcomes (Cohen et al. 1982, Bowman-Perrott et al. 2013). We thus argue that the usage of LLMs as a newly emerging technology positively affects learning outcomes since students can effectively use LLMs as personal tutors.\nWidely available LLMs such as ChatGPT are fine-tuned to act as chatbots with which users can interact freely (OpenAI 2022). Prior studies have found that already traditional rule-based, i.e. non-LLM-powered, chatbots benefit students, who use them to learn (Yin et al. 2021, Abbasi and Kazi 2014, Chen et al. 2020, Chang et al. 2022, Essel et al. 2022,"}, {"title": "3.2. Over-Reliance on LLMs Can Harm Learning", "content": "Alternatively, we can construct a different narrative. In literature, one commonly observed effect of emerging technologies and tools is over-reliance resulting in a degradation of the users' own skills or in stunting their development. Since users can rely on tools for certain tasks, they no longer need to complete them manually such that their knowledge or skills, which they no longer use regularly, diminish over time (George et al. 2024, Volz and Dorneich 2020). Similarly, we can expect intense tool usage, i.e. in our case LLM usage, during learning processes to inhibit the learning itself. As students can easily solve practice exercises designed to aid their learning efforts by using LLMs, LLMs may pose a hindrance to learning. If students, either consciously or unconsciously, rely too much on LLMs to solve their exercises for them, the students' learning progress could be greatly impaired. In our specific case of learning to code, this risk is especially relevant. On the one hand, programming courses typically rely on exercises as the main teaching medium in a learning-by-doing approach. On the other hand, the aforementioned strong coding abilities of LLMs could enable students to quickly solve exercises without deep engagement with the problem at hand.\nCurrent research has cautioned against the risk of over-reliance on LLMs resulting in a reduced learning outcome (Extance 2023, Kasneci et al. 2023, Bastani et al. 2024). In addition, general issues associated with LLMs such as hallucinations or incorrect outputs have been mentioned as further potential inhibitors to learning (Extance 2023, Kasneci et al. 2023, Meyer et al. 2023). Indeed, Bastani et al. (2024) provide first empirical evidence in experiments that high school students learning mathematics with the help of LLMS perform worse in exams. Summarizing this negative account, we formulate a second, rivaling hypothesis:"}, {"title": "4. Overview of Studies", "content": "Through three studies, we provide evidence of the effect of LLMs on student learning and test our ideas presented in the previous section. We combine observational (Study 1) and experimental approaches (Studies 2\u20133) for internal and external validity (e.g. Buell 2021, Lee et al. 2021, Wiltermuth et al. 2023). In Study 1, we use data from two university-level programming courses and an instrumental variable fixed-effects regression to document how LLMs affect short and long-term student performance in the field. Studies 2 and 3 test the proposed effects in laboratory experiments. The laboratory setting enables us to tightly control for and limit any external or undesired influences. In Study 2, we test for causality by explicitly manipulating LLM access and measuring subsequent learning outcomes. Study 3 is a variation of Study 2 in which we modify a baseline setting in the experiment. This allows us to further analyze and isolate the specific mechanisms influencing learning outcomes. Study 3 is a more realistic representation of typical learning scenarios than Study 2. For each study, we report the detailed study design including data collection procedures, our measures of dependent variables, the precise manipulations, and respective results."}, {"title": "5. Study 1: Field Data", "content": "We begin with field data observed from two pre-experience Master's courses in data science with Python at a European university. The courses started with an introduction to Python programming (e.g., variables, data types) and finished with simple machine learning models (e.g., random forests). The two courses were delivered in different programs, one in information systems (IS) and the other in business analytics (BA). The courses were identical in terms of content and assessment except that the BA course had one less lecture on Python because students in that program had already received an introduction to Python in another course. The IS course comprised five lectures and the BA course comprised four lectures. Both courses were delivered at roughly the same time in the Spring of 2023. 56 students took the IS course, and 57 students took the BA course; no student took both courses."}, {"title": "5.1. Variables", "content": null}, {"title": "5.1.1. Dependent Variable", "content": "We measure learning outcomes as the grades students received for each question. The grade is normalized such that for student i and question q, $Grade_{iq} \\in [0, 1]$. At the focal institution, the average normalized course grade is between 0.75 and 0.8. An average (across all courses) of at least 0.825 is considered a distinction and an average of at least 0.9 is considered an exceptional distinction. Students fail with a grade less than 0.55. Most students fall between 0.6 and 0.85."}, {"title": "5.1.2. Explanatory Variable", "content": "We measure students' use of generative AI as the similar-ity between students' final code submission and ChatGPT-generated code for the same question. ChatGPT was the main generative AI service available during the courses. We collected ChatGPT-generated code by prompting it 50 times per question with the same question descriptions that students received using the OpenAI API. We exactly replicate the ChatGPT environment that was available to students at the time of the"}, {"title": "5.1.3. Control Variables", "content": "We control for the following potential confounders:\nPlagiarism. As the homework assignments are unsupervised, students may copy solutions from one another. If the copied student used ChatGPT, the copying student's ChatGPT Similarity will also be higher even if they themselves did not use ChatGPT. In analogy to ChatGPT Similarity, for student i and question q we measure $Plagiarism_{iq}$ as the maximum normalized Damerau\u2013Levenshtein similarity with any other student solution for the same question in the same course. In addition to Plagiarism for a focal question, we compute its cumulative measure Cum. Plagiarism over all questions answered before the focal question.\nQuestions Answered. Students' performance and behavior may change as they solve more questions. For example, they may become more confident and use ChatGPT less. We thus"}, {"title": "5.2. Identification", "content": "The data has a panel structure and the unit of analysis is student-questions. All panel models include fixed effects for students and questions. The two-way fixed effects (FE) model is specified as follows:\n$Grade_{iq} = B_1ChatGPT Similarity_{iq} + B_2Cum.ChatGPT Similarity_{iq} + X'\\Beta_3 + c_i + c_q + \\mu_{iq}, \\qquad (1)$where X is a vector of control variables, $c_i$ are student fixed effects, $c_q$ are question fixed effects, and $B_1$ and $B_2$ are the parameters of interest. This setup controls for constant additive student and question confounders as well as for dynamic linear confounders in the form of students' prior performance.\nIn addition and to rule out further confounding factors (e.g., measurement error), we replicate the FE model in Equation 1 with (Cum.) ChatGPT Outages as instrumental variables (IV) for (Cum.) ChatGPT Similarity. We construct ChatGPT Outages by downloading the reports, start, and end times of all ChatGPT service interruptions from the OpenAI Status Blog2 (OpenAI is the vendor ChatGPT). We include all interruptions that mention a"}, {"title": "5.3. Results", "content": "Table 1 presents descriptive statistics. We exclude 181 observations of questions that students did not attempt to answer. For such unanswered questions, we cannot measure ChatGPT Similarity, because no student code was submitted. Unanswered questions are conceptually different from answered questions with a true ChatGPT Similarity of 0, so imputing a ChatGPT Similarity of 0 for unanswered questions would introduce a measurement bias.\nTable 2 shows the results of the two-way fixed effects model in Equation 1 (FE). The effect of ChatGPT Similarity on the Grade of the current question is positive and significant (p < 0.001), while the effect of Cum. ChatGPT Similarity is negative and significant (p < 0.001). Using a fully ChatGPT-generated solution for the current question increases the"}, {"title": "6. Study 2: Laboratory Experiment", "content": "The field data suggests a negative effect of LLM usage on student learning. In Study 2, we replicate this effect in a controlled, incentivized, and pre-registered\u00b3 laboratory experiment. In a between-subject design, we measure the actual and perceived learning outcomes of subjects learning to code in Python. The learning format is similar to the homework assignments in Study 1 and typical online programming courses (e.g., Udemy, Coursera). The controlled setting in a laboratory allows us to ensure that subjects can only use the provided materials for learning. The laboratory software as well as in-person supervision prevent subjects from cheating (e.g., by opening new browser windows or using smartphones). Additionally, the subject pool of our laboratory setting also enables us to account for students' prior coding abilities."}, {"title": "6.1. Experimental Design", "content": "The main task of the subjects is to learn to program in Python. We choose Python as the programming language since it is among the most popular programming languages (Vailshery 2024) and is particularly beginner-friendly compared to other languages (Lemonaki 2022).\nThe experiment consists of three distinct phases. The second phase is the learning phase. Subjects have 45 minutes to learn basic programming concepts in Python including the print function, strings, and if-statements. These learning goals are broken down into concrete practice questions, each question comprising an explanation of a concept (e.g., if-statements), an example of how to implement it in Python, and a coding exercise for subjects to practice. In total, the learning phase consists of 24 practice questions.\nThe first (pre-test) and the third phase (post-test) measure subjects' coding abilities before and after the learning phase. Each test contains 20 programming questions of which subjects have to complete as many as they can within 20 minutes. Both tests cover the learning goals of the learning phase. For each question in the pre-test, there exists a similar question in the post-test, which requires knowledge of the same concepts, such that we can directly compare the results in the two tests. The pre-test allows us to control for random differences in initial coding ability between the control and treatment conditions. The difference of correctly answered questions in the post-test and pre-test measures the learning outcome of the learning phase and is our main outcome variable (Post-test \u2013 pre-test).\nThe time in each phase is fixed: participants cannot continue to the next phase until the time has elapsed. Within each phase, subjects can move arbitrarily between the different questions. During each of the three phases, subjects have access to a fully functional code editor and are able to run code as often as they like. When running code, they see the full output including potential error messages. If subjects solve a question correctly (in the learning phase or in the tests), they see a short message confirming their result. In Appendix A, we present a complete description as well as screenshots of the user interface.\nBefore the pre-test, we ask for participants' demographics and prior coding experiences. After the post-test, we ask subjects to rate their perceived learning progress on a five-point"}, {"title": "6.2. Treatment Condition", "content": "Subjects are randomly assigned to one of two conditions. In the control condition, subjects work through the experiment exactly as described above. In the treatment condition, subjects additionally have access to an LLM during the learning phase. The LLM is available through a chat window next to the question description. This modified interface is displayed in Figure 3. We use GPT-3.5 Turbo (Brown et al. 2020, OpenAI 2022) as the LLM, which we query through an API. This model is trained to act as a helpful assistant and has sophisticated coding abilities (Liu et al. 2024a). GPT-3.5 Turbo (Henceforth referred to as ChatGPT) can solve all programming questions used in the experiment correctly when prompted with their descriptions. Subjects in the treatment condition can chat with the LLM as often as they like and with arbitrary message contents. They can reset the conversation via a button."}, {"title": "6.3. Subjects", "content": "Subjects for the experiment were recruited from the laboratory participant pool at a public German university. The experiment was approved by the university's ethics committee. All participants are enrolled students and had no knowledge of the experiment content before starting the experiment. Subjects were incentivized via a fixed compensation of 10 Euros and a performance-based compensation depending on the number of solved questions in the post-test, where they received 1 Euro per solved question. On average, subjects earned 18.50 Euros in total. The incentive structure is in accordance with the laboratories minimum wage requirements.\nWe exclude participants based on two filters. We exclude those who do not complete the entire experiment. In addition, we specifically designed the final question of the post-test such that is not solvable solely based on the knowledge acquired in the learning phase as it requires more advanced programming concepts. Thus, we use this question to filter out"}, {"title": "6.4. Results", "content": "We compare the learning outcomes (Post-test \u2013 pre-test) of treatment and control condi-tions using t-tests and regressions with additional covariates (e.g., age, gender, and prior programming experience; see Appendix B for descriptions and summary statistics of all covariates).\nSubjects in the treatment condition sent on average 4.45 (median = 3, S.D. = 3.94) messages to ChatGPT and 6 subjects did not use ChatGPT at all. Table 4 shows the average performance of the treatment and control conditions in each of the three experimental phases. The treatment condition outperformed the control condition in each of the three phases, solving half a question more in the pre-test and one question more in the learning phase and post-test. None of the three differences are statistically significant (p = 0.428, p = 0.180, p = 0.209). The difference of Post-test \u2013 pre-test is half a question larger in the treatment condition, but also not significant (t-test: p = 0.377; regression: p = 0.399, see Table 5). Therefore, we find no supporting evidence for either direction of our hypotheses."}, {"title": "7. Revised Hypotheses", "content": "In Study 2, we do not find conclusive support for either Hypothesis 1 or Hypothesis 2. While conducting the experiment for Study 2, we noticed that the software on the computers in the laboratory blocks all options to copy and paste text. This mechanism was unintended by"}, {"title": "8. Study 3: Laboratory Experiment with Copy-and-Paste Enabled", "content": "To test the revised hypotheses, we conduct a second experiment in which we enable copy and paste. Study 3 is a controlled, incentivized, and pre-registered\u2074 laboratory experiment identical in its design to Study 2, except for the availability of copy and paste. Subjects can copy and paste question descriptions, code, and their conversations with ChatGPT (in the treatment condition) by right-clicking or with keyboard shortcuts. Beyond interacting with ChatGPT, the ability to copy and paste yields no other discernible advantage in the experiment. All sessions took place in June 2024. 69 evaluable subjects participated in this experiment. On average subjects took 90 minutes and earned 17.88 euros."}, {"title": "8.1. Main Results", "content": "As before, we compare ChatGPT usage and learning outcomes (Post-test \u2013 pre-test) of treatment and control conditions using t-tests and regressions with additional covariates. 80% of subjects have no prior coding experience in Python and an additional 14% classify themselves as beginners."}, {"title": "8.1.1. ChatGPT Usage", "content": "Consistent with Hypothesis 3, we find increased usage of ChatGPT. On average, subjects in the treatment group sent 7.51 (SD = 5.56) messages, which is significantly more than in Study 2 (p = 0.001). Only 2 subjects did not use ChatGPT at all. Overall, we conclude that the inability to copy and paste poses a barrier to ChatGPT usage."}, {"title": "8.1.2. Effect of LLM Access on Learning Outcomes", "content": "In Table 6, we summarize the performance of the treatment and control condition in each of the experiment phases. The treatment condition achieves better results in the learning phase and the post-test. In the learning phase, where they have access to ChatGPT, subjects in the treatment condition solve four questions more on average. This is consistent with the typical productivity gains from using AI reported in literature (Brynjolfsson et al. 2023, Noy and Zhang 2023, Dell'Acqua et al. 2023). In the post-test, subjects in the treatment condition solve two questions more. Both differences are significant using t-tests (p = 0.002 and p = 0.097). Notably, the treatment condition also solves one more question on average in the pre-test. While this difference is not statistically significant (p = 0.176), this observation underscores the need to control for prior coding experience in our analyses. Also, note again that our learning outcome measure Post-test \u2013 pre-test implicitly controls for prior coding experience by normalizing via the pre-test result.\nPost-test \u2013 pre-test is on average one question higher for the treatment condition (p = 0.311). We plot the distributions of Post-test \u2013 pre-test for both groups in Figure 1. Qualitatively, we observe two effects based on this comparison of distributions. First, the treatment condition brings forth more high performers. In the treatment condition, 7 subjects surpassed a Post-test \u2013 pre-test of 8 as opposed to only 3 in the control condition. Second,"}, {"title": "8.1.3. Potential Mechanism: LLM Usage Behaviors and the Role of Copy-and-Paste", "content": "To discern potential mechanisms of the observed effects, we specifically analyze the role of copy-and-paste by comparing with the results of Study 2. In regressions in Table 8, we jointly look at the treatment condition data from Study 2 and 3. The ability to copy-and-paste has a statistically significant negative effect (p = 0.007) on Post-test \u2013 pre-test. One potential cause for this is that copy-and-paste impacts LLM usage behaviors. Hence, we analyze"}, {"title": "8.2. Heterogeneous Effects", "content": "Here, we look for heterogeneous treatment effects based on interactions of LLM access and usage with our covariates. In the following, we present all notable discoveries.\nFirst, we consider interactions between prior coding experience and our treatment. In regressions with interaction terms (Table 9), students without experience in Python benefit from LLM access (p = 0.013) consistent with the previously reported effects. For students with prior Python experience, this effect reverses however and becomes negative, albeit being statistically not quite distinguishable from zero (p = 0.190). When using the pre-test result as a measure for Python experience instead of the self-reported score, we observe the same effects. To construct the interaction terms, we select thresholds for the Python experience measures such that we have binary indicators for prior experience. The self-reported score explicitly identifies subjects with no experience as the lowest possible score. For the pre-test result, we choose four solved questions as the cutoff since the first four tasks are solvable inductively without any prior experience, whereas from task five onward prior experience is required.\nNext, we also investigate whether there are heterogeneous effects regarding LLM usage behavior. For subjects asking for explanations, Table 10 shows a heterogeneous effect on learning outcomes with respects to prior (non-Python specific) coding experience. The in Section 8.1.3 discussed positive effect of explanations benefits primarily those who have had any prior exposure to coding. For students with no experience, the positive effect of explanations is much lower in magnitude (0.189 compared to 0.624) and is no longer statistically distinguishable from 0 (p = 0.508)."}, {"title": "8.3. Perceived learning", "content": "In addition to the actual learning outcome, we also measure Perceived learning outcome based on self-reported scores by participants on a five-point Likert scale. Note that actual and perceived learning are generally not the same (Deslauriers et al. 2019). In our experiment,"}, {"title": "9. Discussion", "content": "The findings from our studies can be summarized as follows. We find two contrasting effects of LLMs on student learning and thus reveal a more nuanced picture than concurrent research (e.g., Bastani et al. 2024, Nie et al. 2024) has depicted. Using LLMs as personal tutors by asking them for explanations improves learning outcomes whereas excessively asking LLMs to generate solutions impairs learning. By manipulating copy-and-paste availability, we show that copy-and-paste specifically enables this latter adverse behavior. The overall effect of LLM usage on learning is the result of a delicate balance between both mechanisms, relying on LLM-generated solutions and using LLMs as personal tutors, and can come out in"}, {"title": "Appendices", "content": null}, {"title": "A. Experiment Interface", "content": "Here, we present the main coding interface in our experiment, with which subjects interact in the pre-test, the learning phase and the post-test. Figure 2 shows a screenshot of the interface for the control condition. In the top left, subjects see the description of their current tasks as well as potentially helpful information to learn how to solve this task. In the top right, subjects have access to a fully functioning code editor in which they can enter their Python code to solve the respective task. They can run code as often as they like. When running the code, subjects see the full output below the editor including potential error messages and a short message if the task was solved correctly. We determine the correctness of solutions via predefined test cases, which have to be passed. In addition to the above, the treatment condition also has access to ChatGPT via a chat window in the bottom left (see Figure 3)."}, {"title": "B. Covariates", "content": "In Studies 2 and 3, we measure a variety of control variables via surveys. Table 12 summarizes the control variables. In Table 13, we present summary statistics of the control variables for study 2 and 3 respectively."}, {"title": "C. User behaviors", "content": null}, {"title": "C.1. Chat Message Coding", "content": "We code each message send to ChatGPT. We blindly assess the messages and assign them to a category based on the observed intention of the subject. Categories are created inductively during the coding process whenever we find messages not fitting into previously created categories. Table 14 summarizes the resulting categories of this coding procedure. The categories are clearly distinct from each other. Solution-messages are easily identifiable by containing phrases such as \"solve this task\" or the copied question description. Explanation-messages are identifiable by asking other problem related questions without explicitly asking for a solution."}, {"title": "C.2. What drives user behavior?", "content": "To further investigate user behavior regarding LLMs during learning, we investigate which subject characteristics drive the different observed behavioral patterns. First, we note that"}, {"title": "D. Robustness Checks", "content": "In this section, we provide additional robustness checks of our results. First, our primary analyses focused on intention-to-treat effects since subjects in the treatment group only have access to ChatGPT but must not necessarily use it. We repeat our analyses on the effects of our treatment and the specific user behaviors observed in Section 8.1.3 on Post-test \u2013 pre-test after excluding unsuccessfully treated participants. The results are shown in Table 16 and Table 17 and largely conform to our previous findings. Note that only 6 participants in the treatment condition in Study 2 and 2 in Study 3 did not use ChatGPT such that this congruence is not surprising.\nSecond, we repeat our main analysis from Section 8.1.3 regarding the effect of different usage behaviors by now also considering the control condition. We impute the number of messages asking for solutions or explanations as zero for all subjects in the control group. As depicted in Table 18, the results are consistent with the previously reported effects."}]}