{"title": "RespLLM: Unifying Audio and Text with Multimodal LLMs for Generalized Respiratory Health Prediction", "authors": ["Yuwei Zhang", "Tong Xia", "Aaqib Saeed", "Cecilia Mascolo"], "abstract": "The high incidence and mortality rates associated with respiratory diseases underscores the importance of early screening. Machine learning models can automate clinical consultations and auscultation, offering vital support in this area. However, the data involved, spanning demographics, medical history, symptoms, and respiratory audio, are heterogeneous and complex. Existing approaches are insufficient and lack generalizability, as they typically rely on limited training data, basic fusion techniques, and task-specific models. In this paper, we propose RespLLM, a novel multimodal large language model (LLM) framework that unifies text and audio representations for respiratory health prediction. RespLLM leverages the extensive prior knowledge of pretrained LLMs and enables effective audio-text fusion through cross-modal attentions. Instruction tuning is employed to integrate diverse data from multiple sources, ensuring generalizability and versatility of the model. Experiments on five real-world datasets demonstrate that RespLLM outperforms leading baselines by an average of 4.6% on trained tasks, 7.9% on unseen datasets, and facilitates zero-shot predictions for new tasks. Our work lays the foundation for multimodal models that can perceive, listen to, and understand heterogeneous data, paving the way for scalable respiratory health diagnosis.", "sections": [{"title": "1 Introduction", "content": "Respiratory diseases are the third leading cause of death worldwide, highlighting the critical need for early and accessible respiratory health screening (Labaki and Han, 2020). Clinical assessment of such diseases typically begins with gathering personal information (consultation), including demographics, medical history, symptoms, and other relevant details (hereafter collectively referred to as DMS). In addition, clinicians listen to respiratory sounds (auscultation) as a non-invasive method of screening, before proceeding to more invasive and costly examinations (Reyes et al., 2024). Consequently, automating both the consultation and auscultation processes using machine learning (ML), as illustrated in Figure 1, can significantly enhance early screening by increasing efficiency, accessibility, and affordability."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 ML for Respiratory Health", "content": "In clinical practice, respiratory health is assessed through various clinical examinations such as spirometry, auscultation, chest X-rays, plethysmography, and computed tomography scans (Reyes et al., 2024). Auscultation, combined with personal DMS information, is among the most comfortable and affordable approaches. Using an electronic stethoscope or a microphone, respiratory sounds, such as coughing and breathing, produced by airflow in the respiratory system can be easily recorded. These recordings contain valuable physiological information related to breathing difficulties, reduced oxygen saturation, and other conditions (Xia et al., 2022). Therefore, modeling respiratory audio and DMS data holds significant potential for ubiquitous respiratory health monitoring.\nTraditionally, audio signal processing techniques were used to extract acoustic features that help distinguish between different respiratory conditions (Ma et al., 2022; Islam et al., 2018). Recently, deep learning (DL) has significantly advanced acoustic modeling by automatically capturing complex relationships from raw audio data or spectrograms. This advancement has led to high-performing applications, from detecting abnormal lung sounds to diagnosing conditions such as the flu and pulmonary diseases (Gairola et al., 2021; Fraiwan et al., 2022; Srivastava et al., 2021). When combined with additional information like DMS, DL-driven respiratory health prediction models demonstrate further performance improvements (Han et al., 2021; Xia et al., 2023; Kim et al., 2024; Moummad and Farrugia, 2023).\nHowever, current methods to represent and fuse DMS and audio in the field of respiratory health remain simple and may fail to capture all the relevant information. DMS is typically encoded either by mapping variables into a uniform vector using a predefined dictionary (Figure 3a) (Han et al., 2021; Xia et al., 2023) or by extracting text embeddings from the unstructured data (Figure 3b) (Kim et al., 2024; Moummad and Farrugia, 2023). This representation is then concatenated with audio from a deep learning encoder, ignoring the differences and complex relationship between the two, limiting the potential of DL for health prediction. In the related field of chest X-ray modeling, more advanced multimodal techniques such as LSTM-based fusion (Hayat et al., 2022), cross-modal attention (Wang et al., 2018), and multimodal pre-training (Moon et al., 2022) have been explored. In this paper we explore how similar approaches could be beneficial to audio and DMS."}, {"title": "2.2 LLMs for Health", "content": "Recently-emerged LLMs have demonstrated remarkable capabilities in various health diagnostic applications (Singhal et al., 2023; Li\u00e9vin et al., 2024). This is primarily due to their pretraining on enormous and diverse datasets, including medical literature, clinical guidelines, research papers, and general knowledge (Goel et al., 2023). Such pretraining enables LLMs to understand medical terminology, concepts, and associations relevant to health diagnostics.\nThere is also a growing trend in extending LLMs, which are inherently language models, to handle multimodal data in a unified manner (Wu et al., 2023; Qiu et al., 2023). This capability is typically achieved by combining prompts, modality-specific encoders, and LLMs within a single framework (Moor et al., 2023; Yu et al., 2023; Liu et al., 2024). For example, Liu et al. Liu et al. (2024) leveraged LLMs to interpret electrocardiography signals and perform zero-shot diagnosis. To further enhance generalizability, instruction tuning has emerged as a promising approach for adapting LLMs to various tasks and domains (Aw et al., 2023). In this work, we make the first effort to leverage recent advancements in multimodal LLMs and curate an instruction-tuning dataset using diverse sources for generalized respiratory health prediction."}, {"title": "3 Methodology", "content": "Figure 2b illustrates our proposed framework, a multimodal LLM that can model DMS and respiratory audio simultaneously. In this section, we begin by elaborating on the model architecture design. Then we delve into how we curate the instruction tuning dataset to train this model."}, {"title": "3.1 Model Architecture", "content": "Our overall model architecture is shown in Figure 4. Given the DMS $X_a$ and the respiratory audio signal $X_a$, our goal is to provide a screening result/recommendation in response to the question in the prompt $X_p$. To achieve this, our model mainly consists of three modules: a text embedder that maps $X_p$ and $X_d$ into text token embeddings, an audio encoder with a projector to map $X_a$ into audio embeddings, and an LLM to fuse all the given information for respiratory health screening. These modules are specified as follows.\nText embedding. The text embedding module will first split the given prompt $X_p$ and DMS $X_d$ into sequence of tokens using its tokenizer, and then map the words into a sequence of word embeddings, denoted by $Z_p \\in \\mathbb{R}^{L_p\\times S}$ and $Z_d \\in \\mathbb{R}^{L_d\\times S}$, where $L_p$ and $L_d$ are the lengths of the text and $S$ is the dimension of the word embeddings. For consistency, we use the same tokenizer and word embeddings from the LLM that is used in the later stage. In this sense, $S$ is also the dimension of the hidden state in the transformer blocks of the used LLM.\nAudio Encoder with Projector. Given the high dimensionality and complexity of the audio data, we adapt a pre-trained audio encoder to obtain audio embeddings for $X_a$ (Zhang et al., 2024). Each audio sample is first transformed into a spectrogram, which is then divided into small patches of equal size to derive embeddings. We feed the resulting sequence of $L_a$ embeddings into the LLM, denoted by $z_a \\in \\mathbb{R}^{L_a\\times A}$, where $A$ is the dimension of the original audio embeddings. As the LLM has a different hidden embedding space of dimension $S$, we need to efficiently align the audio embeddings with word embeddings. Following insights from previous work (Ma et al., 2024), we use a simplistic linear layer as the projector $P(\\cdot)$. Then, we have the final audio embeddings $Z_a = P(z_a)$, where $Z_a \\in \\mathbb{R}^{L_a\\times S}$\nLLM and LoRA. For the three distinct embedding $Z_p$, $Z_d$, and $Z_a$, which correspond to task prompt, DMS, and audio information respectively, we first combine them into a longer sequence of embeddings. After this, we add positional embeddings to the resulting sequence, producing the final embedding $Z \\in \\mathbb{R}^{L\\times S}$, $L = L_p + L_d + L_a$. Note that we use the same positional embedding approach as that employed by the chosen LLM model. This combined embedding $Z$ is then fed into the LLM for further processing.\nSince the LLM consists of multiple transformer blocks as shown by the blue shaded box in Figure 4, each containing several self-attention operations parameterized by $W_q$, $W_k$ and $W_v$, the three types"}, {"title": "3.2 Model Training", "content": "Data Curation. To increase the generality of our method, we propose to combine multiple data resources for training. Those data can differ in the audio modalities, DMS formats and the category of respiratory conditions. To unify them for model training, we design contextualized instructions containing task prompts, the description of DMS and the corresponding audio information. The templates of $X_p$ and $X_d$ are formulated as described below, with examples provided in Figure 5.\nI. The task prompt $X_p$ is a diagnostic query with respect to the condition that can be predicted from the given audio and DMS. It is formulated as:\n\u201cDataset description: This data comes from the {D}. Task description: classify whether the participant has {C} given the following information and audio of the person's {T} sounds. Please output 1 for {C1}, and 0 for {C2}. \u201c\nHere, D distinguishes the data resource, T presents the sound type, and C denotes the condition to be predicted from C1 and C2 restricts the output space.\nII. For the text input of DMS $X_d$, we use the following template:\n\"Gender: {G}. Age: {A}. Patient presents with {M} medical history conditions. Patient presents with the following respiratory symptoms: {S}. Recorded location: {L}. \"\nHere, G denotes the gender, A represents age, M specifies medical history, and S is the list of symptoms. L represents the location where the audio was recorded for lung sounds. For any missing or non-applicable data field, the corresponding description is omitted.\nInstruction Tuning. Since various data resources have been unified into instructions, we can now shuffle these instructions from multiple sources to create batches for model training. To make the most of the pre-trained knowledge in the audio encoder and the LLM, we will only train the projector, the LORA parameters, and the final fully connected layer for the LLM in our model, as shown in Figure 4. For the objective function, we use the cross-entropy loss, comparing the output of the LLM with the actual answer to the diagnostic question in the prompt.\nZero-shot Prediction. As mentioned earlier, since the diagnostic task and personal DMS are formulated in text, our model can easily extend to new data and unseen respiratory conditions. This allows for zero-shot inference without requiring any parameter changes when deploying to a new domain."}, {"title": "4 Experiments", "content": "In this section, we conduct extensive experiments with real-world data to answer the following questions:\n\u2022 RQ1: How does our model perform compared to the state-of-the-art baselines for respiratory health prediction?\n\u2022 RQ2: How well does our model generalize to new data and unseen tasks?\n\u2022 RQ3: How do the model design and the choice of LLMs impact the performance of our method?"}, {"title": "4.2 Experimental Setup", "content": "For comparison, we implement both single-modal and multimodal baselines. Regarding single-modal methods, we compare to Audio, which fine-tunes the pre-trained audio encoder alongside a linear classifier for respiratory condition prediction (Xia et al., 2021). For DMS-only methods, we consider to use the hard encoding in Figure 3a and soft text embedding in Figure 3b to fit a linear model, namely DMS-hard and DMS-soft, respectively. Based on these two methods for DMS, we compare to the multimodal method as illustrated in Figure 2a, and name them Fusion-hard (Han et al., 2021) and Fusion-soft (Kim et al., 2024), as our multimodal baselines.\nThe audio encoder used in both the baselines and our method is the pre-trained OPERA-CT model (Zhang et al., 2024), a hierarchical token-semantic audio transformer. It processes an 8-second audio input (padded or cropped) into a spectrogram of size 256 \u00d7 64 and output embeddings of 64 patches, each with a dimension of 768. The LLM model that we modify is OpenBioLLM-8B\u00b9 which is an open-source LLM designed for the biomedical domain. The instruction tuning is completed on a single A-100 GPU. For all tasks, we use AUROC as the metric to report the health condition prediction performance."}, {"title": "4.3 Results", "content": "Health Prediction Performance (RQ1). To answer RQ1, we first examine the performance of our model and the baselines when testing on training datasets (held-out testing set). Since the baselines are task-specific by design, they are trained and tested on the same task, whereas our model utilizes all data resources, resulting in a single RespLLM capable of performing well on multiple tasks. The results are summarized in Table 2. Among the seven evaluated tasks, our model outperforms the state-of-the-art baselines on six tasks, with the average AUROC across all seven tasks surpassing the best baseline by 4.6% (0.8072 vs. 0.7717). It can also be observed that the fusion baselines compared cannot consistently outperform their single-modal counterparts, and their average AUROCs are very close. This suggests that the fusion methods are insufficient. In contrast, our model demonstrates superiority by effectively fusing DMS and audio information via the LLM for respiratory health prediction."}, {"title": "Generalizability (RQ2)", "content": "To demonstrate our model's generalizability and address RQ2, we evaluate its performance not only on in-distribution data but also on new, unseen datasets and tasks. Specifically, we train the models on source task data and test them on target tasks. As shown in Table 1 and Figure 5, both the types of sounds and the information from DMS vary between source and target tasks. Our model can be directly tested, while for the baselines, we report cross-task transfer performance. Since no fine-tuning is applied, this constitutes zero-shot prediction, with the results summarized in Table 3.\nZero-shot transfer prediction shows a degraded performance compared to Table 2, due to changes in data sources, audio types, and DMS information. Despite this challenge, our model consistently outperforms all compared baselines, with the average AUROC surpassing the best baseline by 7.9% (0.6547 vs. 0.6070). This demonstrates the stronger generalizability of our method over the baselines. Notably, in T6, where asthma is a new class not included in the training data, none of the baselines can predict this condition (e.g., a model trained to distinguish COVID/non-COVID in S1 cannot differentiate asthma from healthy cases). In contrast, our model achieves an AUROC of 0.5865, comparable to the baselines' average performance on T1-5. This capability largely stems from our instruction-tuning approach, which effectively retrieves relevant knowledge from the pretrained LLM for zero-shot generalization."}, {"title": "Effect of Training and Model Design (RQ3)", "content": "To further validate the superiority of our framework with cross-data training, we perform several ablation studies. We combine S1-7 into a multi-label task and use all data to train the multimodal baselines for direct comparison of different fusion methods: concatenation fusion as used in the baseline, add-on fusion from (Blandfort et al., 2019), and cross-attention fusion from (Wang et al., 2022). The results for normal testing on source tasks and zero-shot prediction on target tasks are shown in Table 4 and Table 5. Concatenation outperforms addition, as the audio and text embeddings are in very different spaces, and simply adding them may confuse the model. Concatenation also outperforms cross-attention fusion, likely because attention introduces additional parameters to train, which increases the data demand. Our model outperforms all these ablations due to the use of more complex architectures with pretrained parameters and knowledge."}, {"title": "5 Discussion", "content": "In this work, we introduced RespLLM, the first audio-text multimodal LLM for respiratory health prediction. The model not only outperforms state-of-the-art baselines in typical in-distribution testing but also demonstrates stronger generalizability in zero-shot predictions on new datasets and tasks that it was not exposed to during training.\nWe anticipate that the rise of multimodal LLMs will create exciting opportunities for modality fusion (via Transformers) and for grounding models in heterogeneous data sources (via instruction tuning). Thus, our work serves as a foundational step toward more generalist medical AI models.\nLimitations. This work presents a proof-of-concept. As such, RespLLM is not intended for clinical use and should not be considered safe for such applications. The experiments conducted in this study are limited to respiratory conditions such as COVID-19, COPD, and asthma. We have not tested the model performance on other conditions, such as the flu, due to the limited data available at the moment. However, we hope that such data will become more available in the future, enabling further research.\nFuture Work To mitigate the hallucinations that frequently occur in large language models, we replaced the final linear layer in the original LLM with a custom linear layer that only outputs 'Yes' or 'No' for a given condition. An exciting direction for future work would be to explore the use of the full language model for more comprehensive diagnostics and reasoning in respiratory conditions while maintaining trustworthiness. Additionally, we plan to integrate more biosignal modalities, such as photoplethysmography signals and body temperature dynamics, which could provide a more holistic approach to respiratory health screening."}, {"title": "B Implementation Details", "content": ""}, {"title": "B.1 RespLLM", "content": "Audio encoder. The audio encoder that we adopt is the pre-trained OPERA-CT model (Zhang et al., 2024). It is a hierarchical token-semantic audio transformer (HTS-AT) model trained with a contrastive learning objective of instance discrimination on respiratory sounds. All audio recordings are padded or cropped to 8 seconds, resampled to 16 kHz and merged into a mono channel. They are then transformed into spectrograms using 64 Mel filter banks with a 64 ms Hann window that shifts every 32 ms, resulting in a spectrogram of 126 \u00d7 64 dimension. It output patch embeddings of 64 patches, which is input into the LLM as 64 tokens after the alignment module.\nLLM and LoRA. We use the OpenBioLLM model, which has 8B parameters and uses a LLaMA3 architecture. It was developed by Saama AI Lab and released in May 2024 and achieves state-of-the-art performance across various biomedical tasks. To efficiently adapt the LLM model to our tasks, we employ a LoRA module of rank r = 16 and a = 32."}, {"title": "B.2 Baselines", "content": "We use the pre-traiend BERT (Devlin, 2018) for the wording embeddings in the soft fusion baselines, which are of the same dimension of the audio embeddings."}]}