{"title": "The Potential of LLMs in Medical Education: Generating\nQuestions and Answers for Qualification Exams", "authors": ["Yunqi Zhu", "Wen Tang", "Ying Sun", "Xuebing Yang"], "abstract": "Recent research on large language models (LLMs) has primarily focused on their adaptation and\napplication in specialized domains. The application of LLMs in the medical field is mainly concentrated\non tasks such as the automation of medical report generation, summarization, diagnostic reasoning,\nand question-and-answer interactions between doctors and patients. The challenge of becoming a\ngood teacher is more formidable than that of becoming a good student, and this study pioneers the\napplication of LLMs in the field of medical education. In this work, we investigate the extent to which\nLLMs can generate medical qualification exam questions and corresponding answers based on few-\nshot prompts. Utilizing a real-world Chinese dataset of elderly chronic diseases, we tasked the LLMs\nwith generating open-ended questions and answers based on a subset of sampled admission reports\nacross eight widely used LLMs, including ERNIE 4, ChatGLM 4, Doubao, Hunyuan, Spark 4, Qwen,\nLlama 3, and Mistral. Furthermore, we engaged medical experts to manually evaluate these open-\nended questions and answers across multiple dimensions. The study found that LLMs, after using\nfew-shot prompts, can effectively mimic real-world medical qualification exam questions, whereas there\nis room for improvement in the correctness, evidence-based statements, and professionalism of the\ngenerated answers. Moreover, LLMs also demonstrate a decent level of ability to correct and rectify\nreference answers. Given the immense potential of artificial intelligence in the medical field, the task of\ngenerating questions and answers for medical qualification exams aimed at medical students, interns\nand residents can be a significant focus of future research.", "sections": [{"title": "1 Introduction", "content": "In the interdisciplinary field of artificial intelli-\ngence and medicine, text generation is a challeng-\ning yet significant task. The rise of deep learning\nhas brought great opportunities to the medical\ntext generation, especially the booming devel-\nopment of Transformer-based autoregressive lan-\nguage models, which has considerably enhanced\nthe model's ability to process long contextual\nsemantics, thereby enabling the generation of\ncoherent and comprehensive text. Further, the"}, {"title": "2 Results", "content": "We implemented a comprehensive qualitative eval-\nuation of the AI-generated questions by human\nmedical experts, focusing on four criteria: coher-\nence, sufficiency of key information, information\ncorrectness, and professionalism. Figure 1 visual-\nizes the average rating results of human experts on\nthe question generation of different LLMs based\non a same sampled set of elderly chronic dis-\nease admission reports. It can be noted that the\nmajority of LLMs achieve scores over 4 in terms\nof coherence and information correctness, and\nachieve scores nearly 4 in terms of professional-\nism, whereas the average score for sufficient of key\ninformation is relatively lower, with one scoring\nbelow 3.5 in general. This phenomenon may indi-\ncate that human experts were satisfied with the\ncontextual semantic correctness and readability of\nAI-generated questions. However, in determining\nwhether the questions are professionally appro-\npriate, the LLMs still incur a loss of critical\ninformation during the process of extracting and\nabstracting information from the input reports.\nFurthermore, human medical experts evalu-\nated the AI-generated answers based on four\ncriteria: coherence, factual consistency, evidence of\nstatement, and professionalism. Figure 2 depicts\nthe average scores of different LLMs' answers\ntoward a same sampled set of AI-generated ques-\ntions. It can be observed that the average score for\nquestion answering is lower than that for question\ngeneration, with LLMs' ratings hovering around\n3.5 across all evaluation metrics. This indicates a\nsignificant gap between the performance of LLMs\nand the critical requirements of human experts\nfor medical open-ended question answering. More-\nover, with the task of question answering in the\nspecific domain of elderly chronic diseases under\nprompting of limited references, identifying and\nimproving strategies to refine these four evaluation\naspects shows a proper direction for subsequent\nresearch efforts.\nSubsequently, we tasked medical experts with\nevaluating and correcting a subset of flawed AI-\ngenerated answers in the form of open-ended\nshort text. We sampled these evaluations and\ncorrections as new prompt engineering materials,\nallowing the model to judge whether a correction\nis needed for the sampled AI-generated question-\nanswer pairs, and to provide new answers. We\nintermingled the answers rectified by the model\nwith those in the regular AI-generated answer in\nthe evaluation phase, without the human experts\nbeing aware of which were directly generated, and"}, {"title": "3 Discussion", "content": "This work demonstrates the feasibility of LLMs\nin generating medical qualification exam questions\nand answers pertaining to elderly chronic diseases\nthrough few-shot prompting, with the overriding\ngoal of simulating the question bank of USM-\nLEasy and AMBOSS Qbank. The ability of LLMs\nto mimic real-world medical questions is encour-\naging, as it opens avenues for automated content\ngeneration in medical education. Nevertheless, a\nnotable discrepancy in the quality of answers\nunderscores the challenges of generating accurate,\nevidence-based, and professional responses. This\nemphasizes the necessity for further research and\ndevelopment to refine LLMs' comprehension and\napplication of medical knowledge.\nOne potential direction is to explore the incor-\nporation of medical knowledge base and expert\nfeedback into LLMs using Retrieval-Augmented\nGeneration. By integrating structured medical"}, {"title": "4 Methods", "content": "4.1 Data collection\nWe established a multicenter bidirectional\nanonymized database of older patients with\ncomorbid chronic diseases [China Elderly Comor-\nbidity Medical Database (CECMed)] .The\nretrospective cohort had enrollment from Jan-\nuary 2010 to January 2022, while the prospective\ncohort had enrollment from January 2023 to\nNovember 2023. The patients were recruited from\nselected tertiary hospitals and community hospi-\ntals in southern, northern, and central regions of\nChina. Inclusion criteria are as follows: (1) aged\n\u2265 65 years; (2) with at least one of the follow-\ning five chronic diseases: coronary heart disease\n(CHD), hypertension, diabetes, chronic obstruc-\ntive pulmonary disease (COPD) and osteoporosis.\nExclusion criteria are as follows: (1) Late stage\nmalignant tumors, expected survival time less\nthan 3 months; (2) Completely disabled and\nunable to communicate; (3) Unable to cooperate\nwith follow-up.\nFor hospitalized patients, administrative infor-\nmation includes admission time, admission type,\ndemographics, socioeconomic, past medical his-\ntory, previous medication use, current symptoms,\nvital signs, laboratory tests and examinations,\nreasons for admission and admission diagno-\nsis. All patients underwent comprehensive geri-\natric assessment within 24 hours after admission.\nBarthel index, FRAIL, MORSE, MNA-SF and\nMINI-cog were used to assess patient's func-\ntional status. For outpatient patients, baseline\ndata includes demographics, socioeconomic, med-\nical history, medications, vital signs, comprehen-\nsive geriatric assessment and laboratory tests\nand examinations within the past three months.\nPatient data gathered was anonymized and pre-\nprocessed in a standardized manner. Furthermore,\nguided by semi-structured templates that inte-\ngrate medical expert knowledge, we used LLMs to\nsynthesize the information into admission reports\nand summaries.\nThis study was registered (www.clinicaltrials.\ngov, NCT06316544) and approved by the ethical\nreview boards at the participating hospitals. All\npatients provided informed consent."}, {"title": "4.2 Few-shot prompting", "content": "First, sampling from the Chinese elderly chronic\ndisease database, human medical experts com-\nposed 4 questions along with corresponding\nanswers in an open-ended question-and-answer\nformat as the reference. Subsequently, we ran-\ndomly sampled 30 admission reports within the\ndataset, and used the reference question as\nprompts to guide 8 different LLMs for the task\nof question generation. Next, under the guidance\nof independent medical experts, we preprocessed\nthe AI-generated questions, taking into account\ndiversity and professionalism. We then randomly\nsampled 30 different questions, using the reference\nquestion-answer pairs as prompts, and instructed\nthe LLMs to generate responses to these questions.\nIn addition, medical experts provided a total of\n20 concise open-ended reviews of the AI-generated\nanswers, which included correct answers, incor-\nrect answers and the reasons for errors. Further,\nwe constructed prompts based on these answer\nreviews. As a result, we obtained 240 questions,\n240 direct answer generations, and 43 AI-corrected\nanswers. It should be noted that the reports and\ngenerated samples that involved human expert\nauthorship and evaluation, as mentioned above,\nwere not included in the final phase of human\nexpert scoring.\nFew-shot learning is an efficient strategy that\nleverages a small amount of annotated data to\noptimize machine learning models for domain-\nspecific tasks. This study adopted the method of\nprompt engineering, enabling LLMs to emulate\na limited number of reference medical qualifica-\ntion exam questions and answers, thereby eliciting\nLLMs' knowledge in the medical specialties and\nadapting to the question generation as well as\nquestion-answering task for the medical educa-\ntion."}, {"title": "4.3 Human evaluation", "content": "We evaluated the performance of LLMs on pro-\nducing qualification examinations for medical edu-\ncation through human medical expert assessment.\nSpecifically, we focused on two tasks: generating\nopen-ended questions based on admission reports\nand generating corresponding open-ended answers\nto the examination questions. (1) For the gen-\neration of examination questions, we assessed\nfrom four perspectives: coherence, sufficiency of"}]}