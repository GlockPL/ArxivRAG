{"title": "STRUCT-X: Enhancing Large Language Models Reasoning with Structured Data", "authors": ["Xiaoyu Tan", "Haoyu Wang", "Xihe Qiu", "Yuan Cheng", "Yinghui Xu", "Wei Chu", "Yuan Qi"], "abstract": "Structured data, rich in logical and relational information, has the potential to enhance the reasoning abilities of large language models (LLMs). Still, its integration poses a challenge due to the risk of overwhelming LLMs with excessive tokens and irrelevant context information. To address this, we propose STRUCT-X, a novel framework that operates through five key phases: \"read-model-fill-reflect-reason\" efficiently enabling LLMs to utilize structured data. It begins by encoding structured data into a topological space using graph embeddings, followed by filling in missing entity information with knowledge retrieval modules, and filtering out irrelevant tokens via a self-supervised module. The final phase involves constructing a topological network with selected tokens to further reduce the total token length for more effective LLM inference. Additionally, STRUCT-X includes an Auxiliary Module trained to generate prompts, aiding LLMs in analyzing structured data. Extensive experiments on benchmarks, including the knowledge graph question-answer task and the long document reading comprehension task, show that STRUCT-X notably improves LLM reasoning, demonstrating the effectiveness of structured data augmentation in improving LLM inference with complex input context. The code has been open-sourced and can be found in Appendix I.", "sections": [{"title": "1 Introduction", "content": "In recent years, significant advancements have been made in the field of large language models (LLMs), particularly in natural language understanding (Fan et al., 2023). This progress has been largely driven by extensive pre-training on vast text corpora (Gao et al., 2023), which has enhanced their generation capabilities. These advancements are often viewed as critical steps towards the development of artificial general intelligence (AGI) (Pei et al., 2019). During the deployment of LLMs as general-purpose assistants for a variety of real-world applications, it becomes necessary for LLMs to process multimodal inputs. Among these inputs, structured data, like structured knowledge graphs (KGs), is particularly important (Ryen et al., 2022). These graphs, with their rich repository of entity relationships and hierarchical knowledge, have the potential to significantly enhance the reasoning capabilities of LLMs, leading to more precise and reliable inferences. However, in real-world applications, the effective utilization of structured knowledge in LLMs presents a significant challenge (Pan et al., 2024). A common approach is to flatten the structured information into a lengthy text sequence before inputting it into LLMs (Li et al., 2023). However, this method often introduces an excessive amount of task-irrelevant context. Excess information can overwhelm the models, thereby impairing inference efficiency and accuracy (Han et al., 2024). Additionally, it hinders the ability of LLMs to accurately comprehend and represent the complex knowledge embedded within structured data (Zhou, 2023).\nTo address this issue, various approaches have been explored. Some studies have focused on converting knowledge graph triples into textual statements (Zhang, 2023), while others have emphasized incorporating knowledge graph embeddings (Chen, 2023). Additionally, efforts are underway to embed knowledge graph entities and relations directly into the encoder layers of LLMs (Jiang, 2023). More previous work is summarized in Appendix B. However, these methods primarily concentrate on converting the structural data of knowledge graphs into different formats. They tend to overlook the need to reduce the information density of this structural data, which often includes task-irrelevant information. Moreover, these approaches face challenges in preserving the global topological structure of knowledge graphs, a critical aspect that warrants further attention."}, {"title": "", "content": "In addition to the issues of redundant information and the lack of a global topological structure in knowledge graphs, another significant challenge is the high sparsity of these graphs (Lazaridou et al., 2022), characterized by missing semantic connections between entities. This sparsity presents a challenge for leveraging structural data in LLMs (Hadi et al., 2023). LLMs tend to prioritize explicit semantic connections presented in the context while overlooking implicit connections, which are crucial for enhancing inference performance. Although current research, such as (Lv et al., 2022) and (Chai, 2023), has been directed towards automatic knowledge completion and data augmentation to boost overall performance, these approaches tend to overlook the aforementioned challenges of redundancy and topological structure representation in utilizing structural data.\nTo overcome the existing bottlenecks discussed above, we introduce STRUCT-X, a novel framework designed to utilize Structured data to enhance the interaction and complex reasoning capabilities of LLMs. This framework is centered around a workflow of \u201cread-model-fill-reflect-reason\". It employs the transformation of structured data into a topological space, achieved through the application of graph embeddings. This is followed by the augmentation of incomplete entity information utilizing knowledge retrieval modules. Subsequently, a self-retrieved generation module called Self-Reg is employed to eliminate irrelevant tokens. The final stage encompasses the development of a topological network incorporating the chosen tokens, which serves to diminish the overall token length, thereby enhancing the efficacy of LLM inference. Furthermore, an Auxiliary Module is also designed in STRUCT-X, which adjusts prompts based on the loss, guiding the LLM generation. Extensive evaluation of knowledge graph QA and reading comprehension benchmarks have proven STRUCT-X's superior reasoning abilities. These tests confirm that augmenting LLMs with structured data can significantly improve their inference skills in complex context environments. We refer interested readers to Appendix A for more information about STRUCT-X's interaction examples. The code of STRUCT-X has also been open-sourced and can be found in Appendix I. The main contributions of this paper include:\n1. We propose a novelty framework STRUCT-X that implements a process of \u201cread-model-fill-reflect-reason\u201d on structured data, enabling LLMs to perform effective complex reasoning over structured data.\n2. We design a knowledge learning and filtering process to dynamically fill in structured knowledge gaps, coupled with a self-retrieved generation module called Self-Reg to filter and verify the relevance of retrieved knowledge, retaining valuable token information to alleviate learning burdens on LLMs.\n3. We construct specialized graph network encoders to fully learn the potential features of associated tokens and enable efficient cross-layer message passing in Transformers. We also devise an original Auxiliary Module for generating coherent prompts and improving answer responses."}, {"title": "2 Preliminaries", "content": "The task of text generation in LLMs involves creating a sequence of output y = [y1, \u2026\u2026\u2026, yT], where T represents the total number of tokens (Tang et al., 2023), based on a given input prompt x. This process is often modeled in an autoregressively manner, which estimates the likelihood of each token, where y<t represents the tokens that come before the current sequence [y1, \u2026, yt\u22121] (Zhang, 2022). Enhancements to this process can be made by incorporating relevant information from external documents D into the input, thereby refining the model's predictions (Hofst\u00e4tter, 2023).\nMoreover, we can develop a novel decoding strategy that produces critique tokens C alongside the main text output. These tokens are generated at each step and are designed to enable the LLMs to self-evaluate aspects such as relevance, factuality, and completeness of the generated content in Table 1.\n$$p(y, C\\x) = \\prod_{t=1}^{T} P(Y_t, C_t|x, Y_{<t}, C_{<t}),$$"}, {"title": "3 Methods", "content": "We first implement \u201cread-model-fill\u201d process and we start by processing input KGs using a graph attention encoder (GAE) that consists of L layers (Xu et al., 2021). The initial node features, denoted as $h^{(0)}$, are set up using information obtained from the KG completion module (Fei et al., 2021). After processing through L layers, we obtain the final node embeddings, $h^{(L)}$, which effectively represent both the semantic and structural information of the KGs. These encoded graph embeddings, $h^{(L)}$, are then partially masked at a specific rate, $P_{mask}$, to assist in learning about missing knowledge. This masking process can be mathematically represented as $h = M(h^{(L)})$, where M(\u00b7) symbolizes the masking operation. The masked nodes, denoted as $h_r$, are then fed into the knowledge retrieval module, $R(h_r)$, which is explained in the following section. This module plays a crucial role in supplementing the missing information, thereby facilitating the generation of complete graph embeddings $h_r$ (Reda et al., 2022).\nTo address the gaps in entity information within the structured knowledge graph, we have developed a knowledge learning module, denoted as F. This module is designed to retrieve pertinent facts from the knowledge base to enhance the masked node embeddings, $h_r$ (Yasunaga et al., 2022). More specifically, for each masked node, we calculate a similarity score between its embedding $h_v$ and all tail entities t that are part of the set E. This is achieved using the scoring function $f_{score}$, which can be represented as:\n$$s(v,t) = f_{score} (h_v, t).$$"}, {"title": "3.1 Topological Knowledge Injection", "content": "This process enables us to efficiently fill in the missing information in the knowledge graph.\nThe scoring function evaluates both feature and topological similarities within the graph in Figure 2. It selects the top K entities t based on the highest scores and retrieves the related facts (h, r,t) from the knowledge base. To incorporate these facts into the node embeddings, a relation-aware aggregation function $f_{agg}$ is used. This function accumulates relevant knowledge for each node, using a score threshold \\tau to filter out irrelevant facts (Yu et al., 2022). The aggregation function adeptly manages various relations in structured knowledge by considering information from retrieved triples in a relation-aware manner. Additionally, before being input into the Transformer encoder, one linear layer o concatenates and processes embeddings from all GAT layers.\n$$h_v = f_{agg}(\\{h_v\\}\u222a\\{(h, r,t)|s(v, t) > \\tau\\}),$$\n$$e_v = o([h^{(1)}, ..., h^{(L)}, h_v])$$"}, {"title": "3.2 Knowledge and Information Retrieval", "content": "The o merges inputs and reduces dimensionality. This process retains rich multi-scale structural and semantic features at various depths. The output $e_v$ is flattened and prepared into sequences to replace token embeddings for the Transformer encoder input, as suggested by (Wang et al., 2021). The refined node embeddings $h_r$, enriched with retrieved entity information, supply additional knowledge for reasoning in the downstream LLMs.\nHere we perform the \"reflect\" process. The module $R(h_r)$ retrieves relevant knowledge absent in masked graph node inputs $h_r$. Related entities can be dynamically discovered by matching tail entities t to each masked node using a similarity scoring function $f_{score}(h_v, t)$ considering both feature and topological similarity (Lewis, 2020). Related facts (h,r,t) are recalled to fill gaps. After concatenating retrieved knowledge sequences for all nodes ordered by similarity scores, we employ a pruning algorithm leveraging multi-head self-attention distillation and thresholds to filter out lower-weighted tokens. The remaining dense sequence provides supplemental external knowledge to complete masked graph node inputs.\nTo filter and verify the relevance of retrieved knowledge, we design a self-retrieved generation module SelfReg(k) parameterized by $\\theta$ that takes as input the retrieved knowledge sequences k and outputs a filtered subset k containing only the most valuable tokens (Shuster, 2021). Specifically, $SelfReg_{\\theta}$ first encodes the knowledge sequence k = ($x_1$, $x_2$,..., $x_N$) using a Transformer encoder to obtain representations $h_i = f_{enc}(x_i)$. Next, we compute an importance score for each token $s_i = \\sigma(f_{score}(h_i))$, where $f_{score}$ is a scoring network and $\\sigma$ is a sigmoid activation function. To train the scoring network in a self-supervised manner, we create corrupted knowledge sequences k by randomly masking or shuffling some tokens. A contrastive loss is implemented to assign higher scores $s_i$ to tokens from the original k versus corrupted k:\n$$L_{contrast} = \\sum_i max(0, S_i - S_i + \\Delta),$$"}, {"title": "3.3 Graph Topology Encoder", "content": "where \\Delta is a margin hyperparameter. This drives the model to identify the most valuable knowledge.\nFinally, we filter the sequence by discarding tokens scoring below a threshold of \\tau to retain only the most relevant phrases, significantly reducing the learning burden when provided as supplements to the LLMs.\n$$k = \\{x_i|S_i > \\tau\\}.$$"}, {"title": "", "content": "The filtered relevant knowledge k provides targeted assistance to improve reasoning without overwhelming the LLMs with extraneous and irrelevant information.\nHere we perform the \u201creason\" phase. To capture semantic and structural interactions between entities within the KGs, we use a specialized graph encoder in Figure 3, denoted as $E_e(G)$, which is parameterized by $\\theta$. This KG is represented as G = (V, E), where V is the set of node entities and E is the set of relation edges (Li, 2022). For each entity node $v_i$ in V, we first derive its initial feature representation $h_{v_i}^{(0)}$, which is a vector in a high-dimensional space.\nThe graph encoder works through a series of L layers, each layer enhancing the node representations through message passing. This process can be described as:\n$$h_{v_i}^{(l+1)} = f_{\\theta} (\\{h_{v_j}^{(l)} : v_j \u2208 N(v_i)\\}), \\forall v_i \u2208V,$$"}, {"title": "3.4 Auxiliary Module", "content": "where N(vi) refers to the neighboring nodes of vi, and f\u03b8(\u00b7) is a function that aggregates information from these neighbors to update the node's embedding.\nTo focus on the most relevant semantic connections, we use a graph self-attention layer within f\u03b8(\u00b7). This layer calculates attention weights as\nfollows $a_{ij} = \\frac{exp((q_i,k_j))}{\\Sigma_{v_k\u2208N(v_i)} exp((q_i,k_k))}$, where qi and kj are derived from the embeddings of the nodes. This method allows the model to selectively emphasize the most informative signals from neighboring nodes (Cui et al., 2020).\nAfter processing through L layers, we obtain refined node embeddings $z_{v_i} = h_{v_i}^{(L)}$, which encapsulate both semantic and structural information of the graph. To make these embeddings more manageable for downstream tasks, we compress them through a trainable down-projection layer:\n$$e_{v_i} = W_d z_{v_i}, W_d\u2208 R^{d_h\u00d7d_e}, d_e < d_z.$$"}, {"title": "", "content": "This step reduces the dimensionality of the embeddings to de, which is smaller than the original dz.\nThe resulting condensed embeddings evi still retain crucial token-level interactions but are more concise, making them better suited for training models for specific tasks. This approach ensures that while the size of the input sequence is significantly reduced, the essential semantic and structural features of the knowledge graph are preserved for subsequent reasoning.\nTo further guide the LLMs in effectively reasoning over the structured input with a knowledge graph, we have developed an Auxiliary Module. This module is designed to create dynamic prompts that enhance the coherence of answers generated by LLMs. It functions by analyzing the LLM's predicted answer, denoted as \u0177, along with the current loss, L. Based on these inputs, it generates a refined prompt, p', which is then used for a new round of inference. We use the pre-trained Bert model (i.e., bert-base-NER) (Devlin et al., 2018) to construct this Auxiliary Module, symbolized as G and parameterized by $\\theta_g$. This generator crafts the prompt text, taking into account the input values $p' = G(L,\\hat{y};\\theta_g)$. The generator is trained jointly with the overall system using policy gradient methods to maximize the expected reward R of producing coherent answers: $J(\\theta_g) = E_{p' ~ G}[R(p')], \\nabla_{\\theta_g} J(\\theta_g) = E_{p' ~ G}[\\nabla_{\\theta_g} log G(p'|L, \\hat{y}; \\theta_g)R(p')]$.\nThis reward function is designed to encourage the LLMs to generate responses that are not only fluent but also logically consistent, particularly when using the updated prompt. This feature enables the module to dynamically adjust prompts based on the current performance, thereby offering new approaches to improve the quality of answers. Throughout the training process, the module progressively learns to produce more effective prompts, leading to enhanced accuracy and coherence in the LLM's reasoning. For more detailed experimental testing and analysis, please refer to Appendix G."}, {"title": "4 Experiment", "content": "We assess the performance of our proposed STRUCT-X framework on four open-source benchmark datasets designed for knowledge graph reasoning and multi-hop reasoning abilities on graphs.\nTask 1: WebQSP contains 4,737 QA pairs where the questions require logical reasoning over a knowledge graph derived from Wikipedia to infer the correct answer. The knowledge graph consists of 5,719 entities and 2,150 relations.\nTask2: MetaQA comprises a set of more complex compositional questions constructed from an underlying knowledge graph with a vocabulary of 300 entities and 100 relations. It has a total of 1,200 unique questions that test the multi-hop, logical, and comparative reasoning abilities of models.\nTask3: Family Tree Age Consider a family tree G = (V, E), where each individual vi in V is associated with a description di specifying their age. The objective of this task is to identify the triplet comprising an individual, one of their grandparents, and a grand-uncle/grand-aunt by marriage that collectively has the highest cumulative age."}, {"title": "4.1 Datasets and Tasks", "content": "Task4: Travel Route Optimization Let G = (V, E) be a graph representing connected cities, where each city vi in V has a description di with the travel toll or tax. The LLM must plan the route from a source to a destination city that minimizes the total toll paid.\nFor all datasets, we incorporate the encoded graph representations into Llama2, which has been pre-trained on BookCorpus and English Wikipedia. Appendix C is a case analysis of the experimental results on the datasets."}, {"title": "4.2 Implementation Details", "content": "\u2022 Embedding-based Model: We compare against representative embedding models for knowledge graph reasoning including TransE (Bordes et al., 2013), DistMult (Yang et al., 2015), EmbedKGQA (Saxena et al., 2020), ComplEx (Trouillon et al., 2016), and RotatE (Sun et al., 2019).\n\u2022 Open-source LLM: We evaluate reasoning capabilities of widely-used pre-trained language models accessible through open APIs, including Llama2 [7B & 13B] (Touvron et al., 2023) and Alpaca [7B & 13B] (Yao et al., 2023a) which are openly available LLMs up to 13 billion parameters.\n\u2022 LLM-based Fine-tuning: To assess the performance of LM fine-tuning approaches, we include as baselines KG-LlaMA and KG-Alpaca (Yao et al., 2023b), KG-BERT (Yao et al., 2019), PKGC (Lv et al., 2022) and vanilla IT (Zhang, 2023) which incorporate techniques to enhance LMs using annotated KG datasets or self-supervision.\nOur implementation is in PyTorch and we run experiments on NVIDIA A100 GPUs. More details of training can be found in Appendix D."}, {"title": "4.3 Main Results", "content": "The results presented in Table 2 indicate that STRUCT-X consistently outperforms existing baseline methods across various datasets. Specifically, in the WebQSP benchmark, STRUCT-X achieves an accuracy of 75.13%, which is 2.65% higher than the previously best-performing method, KoPA. Additionally, STRUCT-X shows modest improvements in precision and recall, with increases of 9.51% and 10.96%, respectively, compared to Vanilla IT. In the more challenging MetaQA dataset, STRUCT-X's performance is notably better, surpassing the state-of-the-art accuracy scores by 1.84% and achieving a 1.68% higher precision. Furthermore, STRUCT-X demonstrates significant advancements in specialized tasks such as Family Tree and Travel Route, where it exceeds the top baseline results by 3.36% and 5.34% in accuracy, respectively.\nCompared to embedding models such as TransE, DistMult, and EmbedKGQA, STRUCT-X also shows promising improvements in reasoning abilities by integrating both semantic and topological structures of knowledge graphs. For instance, against RotatE's accuracy of 74.55% on the WebQSP dataset, STRUCT-X achieves higher performance with a 75.13% accuracy, an increase of 0.58%. The difference is slightly more pronounced on the MetaQA dataset, where STRUCT-X exceeds RotatE's score of 78.19% by 1.44% in accuracy. In scenarios requiring complex reasoning inferences, STRUCT-X demonstrates enhanced capabilities, outperforming peak embedding model accuracy by a notable margin of 16.74% in Task4.\nThe results also show that STRUCT-X can enhance the capabilities of the Llama2, which itself achieves a 27.13% accuracy on the WebQSP benchmark. This enhancement is achieved through masking graph embeddings and using topology matching to retrieve relevant facts, thus addressing the gaps in factual knowledge that Llama2 requires. By overcoming these deficiencies in LLMs, STRUCT-X significantly improves performance, increasing accuracy by 47.4%. This indicates the effectiveness of structured augmentation, which is not present in the Llama2. Further, STRUCT-X filters out less important tokens using the Self-Reg module, ensuring focus on the most relevant information. In comparison to previous methods like KG-BERT fine-tuning, StructX offers essential enhancements, particularly in complex reasoning tasks, as evidenced by increases of up to 10.24% in accuracy and 5.61% in recall.\nBased on the experimental results, the \u201creflect\" process also plays a crucial role in enhancing reasoning capabilities. This process involves the $\\text{IFReT}(x)$, which selectively gathers evidence as needed, and the $\\text{IFReL}(x, p)$, which filters less relevant passages using relevance scores from Eq.3 to improve context for LLMs. Additionally, the $\\text{IFSuP}(y, p)$ and $\\text{IFUSE}(x, y)$ ensure passage-response consistency and assess overall utility, contributing to higher quality results. For further case"}, {"title": "4.4 Ablation Study", "content": "Table 3 shows that each component of STRUCT-X plays a crucial role in enhancing various reasoning capabilities. For 1-hop single fact questions, while all versions of STRUCT-X are effective, the complete model excels with a 91.3% accuracy due to its ability to perform combinatorial reasoning using multi-head attention. This is key for interpreting semantic connections. In 2-hop and 3-hop multi-step reasoning, the absence of knowledge retrieval and injection modules results in a significant performance drop, with decreases of 7.4% and 7.3% respectively. However, the full STRUCT-X model, utilizing these modules, reaches 92.7% and 93.8% accuracy by effectively traversing distant nodes. The graph topology encoder also proves vital; its omission leads to a 5.8% decline in location-based reasoning, highlighting its importance in connecting nodes and facilitating spatial/hierarchical reasoning through message passing. Furthermore, the lower accuracy without the Auxiliary Module underlines its utility in guiding coherent inference across multiple steps."}, {"title": "4.4.1 Q1: Different functional modules", "content": "Table 4 compares reasoning performance with the following variants: StructXNoFiltering: Directly injects all retrieved knowledge without filtering. Struct X RandomFiltering: Randomly removes of retrieved tokens. StructXRegFiltering: Uses the proposed Self-Reg module to score and filter tokens.\nAcross WebQSP and MetaQA datasets, incorporating filtering mechanisms leads to consistent gains over no filtering baselines. Randomly removing tokens brings minor improvements, showing that some knowledge reduction is beneficial. However, learned filtering with Self-Reg leads to more substantial gains. Comparing different Self-Reg cutting ratios, 40% filtering seems to achieve the optimal trade-off, maximizing accuracy and recall. More aggressive 60% cutting starts to degrade performance likely due to removing pertinent facts. On the other hand, light 20% filtering retains more distracting information. By balancing knowledge breadth and depth, 40% Self-Reg filtering enhances language model inference without overwhelming models. By scoring and removing extraneous tokens based on contextual representations, Self-Reg retains the essence to augment language models without diverting attention."}, {"title": "4.4.2 Q2:Filtering and reflection mechanism", "content": "The results in Table 5 demonstrate that incorporating the Auxiliary Module leads to significant performance gains over the base STRUCT-X model without this component. We observe absolute improvements of 3.9% in accuracy, 2.58% in precision, and 5.72% in recall after implementing the Auxiliary Module. This validates its efficacy in providing adaptive prompts that elicit more accurate and logically coherent reasoning from the LLM when inference is made over structured knowledge graphs. The gains over the previous best model, PKGC are also substantial, at 10.57% higher accuracy. Hence, the auxiliary module proves important for multi-hop reasoning and steering deductions in the right direction over complex topological structures. The consistent benefits confirm that modeling explicit prompt-answering mechanisms customized for structured reasoning tasks is an effective approach."}, {"title": "4.4.3 Q3: Learning by Auxiliary Module", "content": "To validate the contributions of different components of our knowledge injection mechanism, we conduct an ablation study with the following variants: StructXNoInjection: The base LLM (i.e., Llama2) without any graph representation injection. StructX EmbeddingsOnly: Encoded graph embeddings are directly injected without any masking or knowledge retrieval. StructXMaskingOnly: Graph embeddings are masked but missing facts are not filled via retrieval. StructX RetrievalOnly: Masked embeddings are completed with the knowledge retrieval module but without graph encoding. We compare reasoning performance on WebQSP and MetaQA benchmarks against these reduced injection variants. The results in Table 6 demonstrate clear improvements from collectively incorporating all knowledge injection components compared"}, {"title": "4.4.4 Q4:Knowledge injection variants", "content": "to ablated variants. The full STRUCT-X model with topological encoding, masking, and retrieval achieves 1.68% and 1.31% higher accuracy over the best partial variant on WebQSP and MetaQA respectively. This confirms that each mechanism provides unique benefits - topological encoding better retain intricate connections, masking identifies missing facts, and retrieval fills knowledge gaps. The experiment proves that dynamic masking and retrieval to address inherent incompleteness in structured data are most impactful. Variants without these processes show worse performance as they fail to overcome language models' factual deficiencies."}, {"title": "5 Conclusion", "content": "In this paper, we introduce STRUCT-X, a groundbreaking framework designed to enhance LLMs in complex reasoning tasks. STRUCT-X applies an efficient \"read-model-fill-reflect-reason\u201d methodology to structured data. It is adept at learning graph embeddings that are sensitive to geometric contexts, capturing the content of entities as well as their topological relationships. This enables STRUCT-X to effectively infer missing facts about entities by matching similar topological features. Furthermore, it enhances the LLMs by distributing multi-scale features, which bolsters the representation of underlying connections that are not explicitly apparent. STRUCT-X excels in tasks such as knowledge graph-based QA tasks and reading comprehension, especially in scenarios that require multi-hop logical reasoning."}, {"title": "6 Limitiation", "content": "The knowledge graph encoding may not fully capture complex relationships beyond structural topology, the auxiliary module's prompting could be overly biased by the current loss landscape. Exploring more expressive graph representations and smarter prompting strategies could potentially address these limitations."}]}