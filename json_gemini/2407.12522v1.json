{"title": "STRUCT-X: Enhancing Large Language Models Reasoning with Structured Data", "authors": ["Xiaoyu Tan", "Haoyu Wang", "Xihe Qiu", "Yuan Cheng", "Yinghui Xu", "Wei Chu", "Yuan Qi"], "abstract": "Structured data, rich in logical and relational information, has the potential to enhance the reasoning abilities of large language models (LLMs). Still, its integration poses a challenge due to the risk of overwhelming LLMs with excessive tokens and irrelevant context information. To address this, we propose STRUCT-X, a novel framework that operates through five key phases: \"read-model-fill-reflect-reason\" efficiently enabling LLMs to utilize structured data. It begins by encoding structured data into a topological space using graph embeddings, followed by filling in missing entity information with knowledge retrieval modules, and filtering out irrelevant tokens via a self-supervised module. The final phase involves constructing a topological network with selected tokens to further reduce the total token length for more effective LLM inference. Additionally, STRUCT-X includes an Auxiliary Module trained to generate prompts, aiding LLMs in analyzing structured data. Extensive experiments on benchmarks, including the knowledge graph question-answer task and the long document reading comprehension task, show that STRUCT-X notably improves LLM reasoning, demonstrating the effectiveness of structured data augmentation in improving LLM inference with complex input context. The code has been open-sourced and can be found in Appendix I.", "sections": [{"title": "1 Introduction", "content": "In recent years, significant advancements have been made in the field of large language models (LLMs), particularly in natural language understanding (Fan et al., 2023). This progress has been largely driven by extensive pre-training on vast text corpora (Gao et al., 2023), which has enhanced their generation capabilities. These advancements are often viewed as critical steps towards the development of artificial general intelligence (AGI) (Pei et al., 2019). During the deployment of LLMs as general-purpose assistants for a variety of real-world applications, it becomes necessary for LLMs to process multimodal inputs. Among these inputs, structured data, like structured knowledge graphs (KGs), is particularly important (Ryen et al., 2022). These graphs, with their rich repository of entity relationships and hierarchical knowledge, have the potential to significantly enhance the reasoning capabilities of LLMs, leading to more precise and reliable inferences. However, in real-world applications, the effective utilization of structured knowledge in LLMs presents a significant challenge (Pan et al., 2024). A common approach is to flatten the structured information into a lengthy text sequence before inputting it into LLMs (Li et al., 2023). However, this method often introduces an excessive amount of task-irrelevant context. Excess information can overwhelm the models, thereby impairing inference efficiency and accuracy (Han et al., 2024). Additionally, it hinders the ability of LLMs to accurately comprehend and represent the complex knowledge embedded within structured data (Zhou, 2023). To address this issue, various approaches have been explored. Some studies have focused on converting knowledge graph triples into textual statements (Zhang, 2023), while others have emphasized incorporating knowledge graph embeddings (Chen, 2023). Additionally, efforts are underway to embed knowledge graph entities and relations directly into the encoder layers of LLMs (Jiang, 2023). However, these methods primarily concentrate on converting the structural data of knowledge graphs into different formats. They tend to overlook the need to reduce the information density of this structural data, which often includes task-irrelevant information. Moreover, these approaches face challenges in preserving the global topological structure of knowledge graphs, a critical aspect that warrants further attention."}, {"title": null, "content": "In addition to the issues of redundant information and the lack of a global topological structure in knowledge graphs, another significant challenge is the high sparsity of these graphs (Lazaridou et al., 2022), characterized by missing semantic connections between entities. This sparsity presents a challenge for leveraging structural data in LLMs (Hadi et al., 2023). LLMs tend to prioritize explicit semantic connections presented in the context while overlooking implicit connections, which are crucial for enhancing inference performance. Although current research, such as (Lv et al., 2022) and (Chai, 2023), has been directed towards automatic knowledge completion and data augmentation to boost overall performance, these approaches tend to overlook the aforementioned challenges of redundancy and topological structure representation in utilizing structural data. To overcome the existing bottlenecks discussed above, we introduce STRUCT-X, a novel framework designed to utilize Structured data to enhance the interaction and complex reasoning capabilities of LLMs. This framework is centered around a workflow of \u201cread-model-fill-reflect-reason\". It employs the transformation of structured data into a topological space, achieved through the application of graph embeddings. This is followed by the augmentation of incomplete entity information utilizing knowledge retrieval modules. Subsequently, a self-retrieved generation module called Self-Reg is employed to eliminate irrelevant tokens. The final stage encompasses the development of a topological network incorporating the chosen tokens, which serves to diminish the overall token length, thereby enhancing the efficacy of LLM inference. Furthermore, an Auxiliary Module is also designed in STRUCT-X, which adjusts prompts based on the loss, guiding the LLM generation. Extensive evaluation of knowledge graph QA and reading comprehension benchmarks have proven STRUCT-X's superior reasoning abilities. These tests confirm that augmenting LLMs with structured data can significantly improve their inference skills in complex context environments. We refer interested readers to Appendix A for more information about STRUCT-X's interaction examples. The code of STRUCT-X has also been open-sourced and can be found in Appendix I. The main contributions of this paper include:"}, {"title": null, "content": "1. We propose a novelty framework STRUCT-X that implements a process of \u201cread-model-fill-reflect-reason\" on structured data, enabling LLMs to perform effective complex reasoning over structured data.\n2. We design a knowledge learning and filtering process to dynamically fill in structured knowledge gaps, coupled with a self-retrieved generation module called Self-Reg to filter and verify the relevance of retrieved knowledge, retaining valuable token information to alleviate learning burdens on LLMs.\n3. We construct specialized graph network encoders to fully learn the potential features of associated tokens and enable efficient cross-layer message passing in Transformers. We also devise an original Auxiliary Module for generating coherent prompts and improving answer responses."}, {"title": "2 Preliminaries", "content": "The task of text generation in LLMs involves creating a sequence of output y = [y1, \u2026\u2026\u2026, yT], where T represents the total number of tokens (Tang et al., 2023), based on a given input prompt x. This process is often modeled in an autoregressively manner, which estimates the likelihood of each token, where y<t represents the tokens that come before the current sequence [y1, \u2026, Yt\u22121] (Zhang, 2022). Enhancements to this process can be made by incorporating relevant information from external documents D into the input, thereby refining the model's predictions (Hofst\u00e4tter, 2023). Moreover, we can develop a novel decoding strategy that produces critique tokens C alongside the main text output. These tokens are generated at each step and are designed to enable the LLMs to self-evaluate aspects such as relevance, factuality, and completeness of the generated content in Table 1.\n$\\displaystyle p(y, C\\x) = \\prod_{t=1}^{T} P(Yt, Ct|x, Y<t, C<t),$\nwhere the critique token Ct depends on all preceding text and critiques. We define four types of critique tokens: IFRET - predicts if retrieval is needed, IFREL - assesses passage relevance, IFSuP checks output is supported and IFUSE decides whether it is useful. These critique tokens enable better control of the decoding process through re-ranking or constraints (Asai, 2023). For instance, the probability of a desirable IFREL token can upweight certain"}, {"title": "3 Methods", "content": "We first implement \u201cread-model-fill\u201d process and we start by processing input KGs using a graph attention encoder (GAE) that consists of L layers (Xu et al., 2021). The initial node features, denoted as h(0), are set up using information obtained from the KG completion module (Fei et al., 2021). After processing through L layers, we obtain the final node embeddings, h(L), which effectively represent both the semantic and structural information of the KGs. These encoded graph embeddings, h(L), are then partially masked at a specific rate, Pmask, to assist in learning about missing knowledge. This masking process can be mathematically represented as $\\displaystyle h_v = M(h_v^{(L)}), \\tilde{h}_v = (I - M)(h_v^{(L)})$, where M(\u00b7) symbolizes the masking operation. The masked nodes, denoted as hr, are then fed into the knowledge retrieval module, R(\u00b7), which is explained in the following section. This module plays a crucial role in supplementing the missing information, thereby facilitating the generation of complete graph embeddings hr (Reda et al., 2022). To address the gaps in entity information within the structured knowledge graph, we have developed a knowledge learning module, denoted as F. This module is designed to retrieve pertinent facts from the knowledge base to enhance the masked node embeddings, hr (Yasunaga et al., 2022). More specifically, for each masked node, we calculate a similarity score between its embedding hv and all tail entities t that are part of the set E. This is achieved using the scoring function fscore, which can be represented as:\n$\\displaystyle s(v,t) = f_{score} (h_v, t).$\nThis process enables us to efficiently fill in the missing information in the knowledge graph. The scoring function evaluates both feature and topological similarities within the graph in Figure 2. It selects the top K entities t based on the highest scores and retrieves the related facts (h, r,t) from the knowledge base. To incorporate these"}, {"title": null, "content": "facts into the node embeddings, a relation-aware aggregation function fagg is used. This function accumulates relevant knowledge for each node, using a score threshold \u03c4 to filter out irrelevant facts (Yu et al., 2022). The aggregation function adeptly manages various relations in structured knowledge by considering information from retrieved triples in a relation-aware manner. Additionally, before being input into the Transformer encoder, one linear layer o concatenates and processes embeddings from all GAT layers.\n$\\displaystyle h_v = f_{agg}(\\{h\\}\\cup\\{(h, r,t)|s(v, t) > \\tau\\}),\\newline e_v = o([h_v^{(1)}, ..., h_v^{(L)}, h_v])$\nThe o merges inputs and reduces dimensionality. This process retains rich multi-scale structural and semantic features at various depths. The output ev is flattened and prepared into sequences to replace token embeddings for the Transformer encoder input, as suggested by (Wang et al., 2021). The refined node embeddings hr, enriched with retrieved entity information, supply additional knowledge for reasoning in the downstream LLMs."}, {"title": "3.2 Knowledge and Information Retrieval", "content": "Here we perform the \"reflect\" process. The module R(hr) retrieves relevant knowledge absent in masked graph node inputs hr. Related entities can be dynamically discovered by matching tail entities t to each masked node using a similarity scoring function fscore(hv, t) considering both feature and topological similarity (Lewis, 2020). Related facts (h,r,t) are recalled to fill gaps. After concatenating retrieved knowledge sequences for all nodes ordered by similarity scores, we employ a pruning algorithm leveraging multi-head self-attention distillation and thresholds to filter out lower-weighted tokens. The remaining dense sequence provides supplemental external knowledge to complete masked graph node inputs. To filter and verify the relevance of retrieved knowledge, we design a self-retrieved generation module $\\displaystyle Self Reg_\\Theta(k)$ parameterized by \u03b8 that takes as input the retrieved knowledge sequences k and outputs a filtered subset k containing only"}, {"title": null, "content": "the most valuable tokens (Shuster, 2021). Specifically, $\\displaystyle Self Reg_\\Theta$ first encodes the knowledge sequence k = (x1, x2,..., xN) using a Transformer encoder to obtain representations hi = fenc(xi). Next, we compute an importance score for each token si = \u03c3(fscore(hi)), where fscore is a scoring network and o is a sigmoid activation function. To train the scoring network in a self-supervised manner, we create corrupted knowledge sequences k' by randomly masking or shuffling some tokens. A contrastive loss is implemented to assign higher scores si to tokens from the original k versus corrupted k':\n$\\displaystyle L_{contrast} = \\sum_i max(0, s_i - s'_i + \\Delta),$\nwhere \u0394 is a margin hyperparameter. This drives the model to identify the most valuable knowledge. Finally, we filter the sequence by discarding tokens scoring below a threshold of \u03c4 to retain only the most relevant phrases, significantly reducing the learning burden when provided as supplements to the LLMs.\n$\\displaystyle \\tilde{k} = \\{x_i | S_i > \\tau\\}.$\nThe filtered relevant knowledge k provides targeted assistance to improve reasoning without overwhelming the LLMs with extraneous and irrelevant information."}, {"title": "3.3 Graph Topology Encoder", "content": "Here we perform the \u201creason\" phase. To capture semantic and structural interactions between entities within the KGs, we use a specialized graph encoder in Figure 3, denoted as Ee(G), which is parameterized by \u03b8. This KG is represented as G = (V, E), where V is the set of node entities and E is the set of relation edges (Li, 2022). For each entity node vi in V, we first derive its initial feature representation $\\displaystyle h_{v_i}^{(0)}$, which is a vector in a high-dimensional space. The graph encoder works through a series of L layers, each layer enhancing the node representations through message passing. This process can be described as:\n$\\displaystyle h_{v_i}^{(l+1)} = f_\\Theta (\\{h_{v_j}^{(l)} : v_j \\in N(v_i)\\}), \\forall v_i \\in V,$"}, {"title": null, "content": "where N(vi) refers to the neighboring nodes of vi, and fo() is a function that aggregates information from these neighbors to update the node's embedding. To focus on the most relevant semantic connections, we use a graph self-attention layer within fo(). This layer calculates attention weights as $\\displaystyle a_{ij} = \\frac{exp((q_i,k_j))}{\\Sigma_{v_k\\in N(v_i)} exp((q_i,k_k))},$ where qi and kj are derived from the embeddings of the nodes. This method allows the model to selectively emphasize the most informative signals from neighboring nodes (Cui et al., 2020). After processing through L layers, we obtain refined node embeddings $\\displaystyle z_{v_i} = h_{v_i}^{(L)}$, which encapsulate both semantic and structural information of the graph. To make these embeddings more manageable for downstream tasks, we compress them through a trainable down-projection layer:\n$\\displaystyle e_{v_i} = W_d z_{v_i}, W_d \\in R^{d_h\\times d_e}, d_e < d_z.$\nThis step reduces the dimensionality of the embeddings to de, which is smaller than the original dz. The resulting condensed embeddings ev; still retain crucial token-level interactions but are more concise, making them better suited for training models for specific tasks. This approach ensures that while the size of the input sequence is significantly reduced, the essential semantic and structural features of the knowledge graph are preserved for subsequent reasoning."}, {"title": "3.4 Auxiliary Module", "content": "To further guide the LLMs in effectively reasoning over the structured input with a knowledge graph, we have developed an Auxiliary Module. This module is designed to create dynamic prompts that enhance the coherence of answers generated by LLMs. It functions by analyzing the LLM's predicted answer, denoted as \u0177, along with the current loss, L. Based on these inputs, it generates a refined prompt, p', which is then used for a new round of inference. We use the pre-trained Bert model (i.e., bert-base-NER) (Devlin et al., 2018) to construct this Auxiliary Module, symbolized as G and parameterized by \u03b8g. This generator crafts the prompt text, taking into account the input values p' = G(L,\u0177;\u03b8g). The generator is trained jointly with the overall system using policy gradient methods to maximize the expected reward R of producing coherent answers: $\\displaystyle J(\\theta_g) = E_{p' \\sim G}[R(p')], \\nabla_{\\theta_g} J(\\theta_g) = E_{p' \\sim G}[\\nabla_{\\theta_g} log G(p'|L, \\hat{y}; \\theta_g)R(p')].$\nThis reward function is designed to encourage the LLMs to generate responses that are not only fluent but also logically consistent, particularly when using the updated prompt. This feature enables the module to dynamically adjust prompts based on the current performance, thereby offering new approaches to improve the quality of answers. Throughout the training process, the module progressively learns to produce more effective prompts, leading to enhanced accuracy and coherence in the LLM's reasoning. For more detailed experimental testing and analysis, please refer to Appendix G."}, {"title": "4 Experiment", "content": "We assess the performance of our proposed STRUCT-X framework on four open-source benchmark datasets designed for knowledge graph reasoning and multi-hop reasoning abilities on graphs.\nTask1:WebQSP contains 4,737 QA pairs where the questions require logical reasoning over a knowledge graph derived from Wikipedia to infer the correct answer. The knowledge graph consists of 5,719 entities and 2,150 relations. \nTask2:MetaQA comprises a set of more complex compositional questions constructed from an underlying knowledge graph with a vocabulary of 300 entities and 100 relations. It has a total of 1,200 unique questions that test the multi-hop, logical, and comparative reasoning abilities of models.\nTask3:Family Tree Age Consider a family tree G = (V, E), where each individual vi in V is associated with a description di specifying their age. The objective of this task is to identify the triplet comprising an individual, one of their grandparents, and a grand-uncle/grand-aunt by marriage that collectively has the highest cumulative age."}, {"title": null, "content": "Task4:Travel Route Optimization Let G = (V, E) be a graph representing connected cities, where each city vi in V has a description di with the travel toll or tax. The LLM must plan the route from a source to a destination city that minimizes the total toll paid. For all datasets, we incorporate the encoded graph representations into Llama2, which has been pre-trained on BookCorpus and English Wikipedia. is a case analysis of the experimental results on the datasets."}, {"title": "4.2 Implementation Details", "content": "* Embedding-based Model: We compare against representative embedding models for knowledge graph reasoning including TransE (Bordes et al., 2013), DistMult (Yang et al., 2015), EmbedKGQA (Saxena et al., 2020), ComplEx (Trouillon et al., 2016), and RotatE (Sun et al., 2019).\n* Open-source LLM: We evaluate reasoning capabilities of widely-used pre-trained language models accessible through open APIs, including Llama2 [7B & 13B] (Touvron et al., 2023) and Alpaca [7B & 13B] (Yao et al., 2023a) which are openly available LLMs up to 13 billion parameters.\n* LLM-based Fine-tuning: To assess the performance of LM fine-tuning approaches, we include as baselines KG-LlaMA and KG-Alpaca (Yao et al., 2023b), KG-BERT (Yao et al., 2019), PKGC (Lv et al., 2022) and vanilla IT (Zhang, 2023) which incorporate techniques to enhance LMs using annotated KG datasets or self-supervision.\nOur implementation is in PyTorch and we run experiments on NVIDIA A100 GPUs. More details of training can be found in Appendix D."}, {"title": "4.3 Main Results", "content": "The results presented in Table 2 indicate that STRUCT-X consistently outperforms existing baseline methods across various datasets. Specifically, in the WebQSP benchmark, STRUCT-X achieves an accuracy of 75.13%, which is 2.65% higher than the previously best-performing method, KoPA. Additionally, STRUCT-X shows modest improvements in precision and recall, with increases of 9.51% and 10.96%, respectively, compared to Vanilla IT. In the more challenging MetaQA dataset, STRUCT-X's performance is notably better, surpassing the state-of-the-art accuracy scores by 1.84% and achieving a 1.68% higher precision. Furthermore, STRUCT-X demonstrates significant advancements in specialized tasks such as Family Tree and Travel Route, where it exceeds the top baseline results by 3.36% and 5.34% in accuracy, respectively. Compared to embedding models such as TransE, DistMult, and EmbedKGQA, STRUCT-X also shows promising improvements in reasoning abilities by integrating both semantic and topological structures of knowledge graphs. For instance, against RotatE's accuracy of 74.55% on the WebQSP dataset, STRUCT-X achieves higher performance with a 75.13% accuracy, an increase of 0.58%. The difference is slightly more pronounced on the MetaQA dataset, where STRUCT-X exceeds RotatE's score of 78.19% by 1.44% in accuracy. In scenarios requiring complex reasoning inferences, STRUCT-X demonstrates enhanced capabilities, outperforming peak embedding model accuracy by a notable margin of 16.74% in Task4. The results also show that STRUCT-X can enhance the capabilities of the Llama2, which itself achieves a 27.13% accuracy on the WebQSP benchmark. This enhancement is achieved through masking graph embeddings and using topology matching to retrieve relevant facts, thus addressing the gaps in factual knowledge that Llama2 requires. By overcoming these deficiencies in LLMs, STRUCT-X significantly improves performance, increasing accuracy by 47.4%. This indicates the effectiveness of structured augmentation, which is not present in the Llama2. Further, STRUCT-X filters out less important tokens using the Self-Reg module, ensuring focus on the most relevant information. In comparison to previous methods like KG-BERT fine-tuning, StructX offers essential enhancements, particularly in complex reasoning tasks, as evidenced by increases of up to 10.24% in accuracy and 5.61% in recall. Based on the experimental results, the \u201creflect\" process also plays a crucial role in enhancing reasoning capabilities. This process involves the $\\displaystyle IFR_{\\theta}(x)$, which selectively gathers evidence as needed, and the $\\displaystyle IFReL(x, p)$, which filters less relevant passages using relevance scores from Eq.3 to improve context for LLMs. Additionally, the $\\displaystyle IFSuP(y, p)$ and $\\displaystyle IFUSE(x, y)$ ensure passage-response consistency and assess overall utility, contributing to higher quality results. For further case"}, {"title": "6 Limitiation", "content": "The knowledge graph encoding may not fully capture complex relationships beyond structural topology, the auxiliary module's prompting could be overly biased by the current loss landscape. Exploring more expressive graph representations and smarter prompting strategies could potentially address these limitations."}, {"title": "A StructX Interaction Examples", "content": "Instructions\nPlease indicate whether referring to external documents, improves the quality of the generated response. Please respond with either [Yes] or [No] and provide a brief explanation.\nInstruction: Identify the shortest path between two nodes in this knowledge graph. Need retrieval? [Yes]\nExplanation: STRUCT-X can ingest the graph structure and topology to reason about paths. But retrieving additional facts on edge distances or weights can supplement its understanding for more accurate optimization.\nInstruction: Determine which family tree node has the oldest relative based on date descriptions. Need retrieval? [No]\nExplanation: STRUCT-X encodes the hierarchical tree relations and date informations directly without needing external evidence. Retrieval may introduce unnecessary details.\nInstruction: Analyze the impacts of this new tax policy based on economic concepts. Need retrieval? [Yes]\nExplanation: While STRUCT-X has some linguistic capabilities, retrieving domain knowledge on economics and regulations will improve understanding of entities and contextual impacts for better analysis.\nInstruction: Summarize the key events in this 5-page history passage. Need retrieval? [No]\nExplanation: STRUCT-X is designed to ingest long document passages directly through encoders. No need for external info.\nInstruction: Compare the costs of different flight options based on stop, mileage and fare data. Need retrieval? [No]\nExplanation: STRUCT-X can encode and reason over structured data tables natively. External retrieval of similar data is un-needed."}, {"title": "B Related Work", "content": "Prior efforts have explored various techniques to enhance language models with structured knowledge. Pan et al. (Pan, 2024) directly converted knowledge graph triples into textual statements as inputs. Liu et al. (Liu, 2020) embedded knowledge graphs into hidden layers of BERT models. Harnoune et al.(Harnoune, 2021) utilized memory modules to provide facts for multi-hop reasoning. Wu et al. (Wu, 2020) designed graph recurrent networks to propagate information over knowledge graphs. Roux et al.(Roux, 2022) employed reinforcement learning for path-finding in knowledge-augmented environments. Tu et al. (Tu, 2021) used graph convolutional networks to distill subgraph knowledge. Liu et al. (Liu, 2022) pruned knowledge graph inputs via self-supervised learning objectives. Honovich et al. (Honovich, 2022) focused on improving factual consistency in conditional text generation. Shah et al. (Shah, 2020) iteratively expanded anchor sets for more comprehensive grounding. Xue et al. (Xue, 2020) optimized path selection on heterogeneous knowledge sources. While providing valuable techniques, existing methods lack consideration for efficiently filtering and condensing structured knowledge supplements tailored for language model integration. Our work aims to address these limitations through specialized contextual encoding, retrieval, and distillation components in the proposed STRUCT-X framework."}, {"title": "C Case Study", "content": "Family Relations\nJohn is Nikhil's paternal grandfather. Nikhil's aunt Meera is John's daughter. John was born in 1950. Nikhil was born in 2000. Meera was born in 1975. Raj is Nikhil's maternal grandfather. Raj has a brother named Shekhar. Raj was born in 1948. Shekhar was born in 1946.\nQuestion: What is the age difference between the oldest and youngest family members described? Response: Based on the family tree details provided:\nThe oldest member is Shekhar, Nikhil's grand uncle, born in 1946.\nThe youngest member is Nikhil, born in 2000.\nTheir age difference is 1946 - 2000 = 54 years.\nSo the age difference between the oldest and youngest family members is 54 years.\nCorrect Answer: 54 years old.\nIn this family tree example, STRUCT-X first encodes the hierarchical relations between members like \"John is Nikhil's paternal grandfather\" and \"Meera is John's daughter\" as structured knowl-"}, {"title": null, "content": "edge. The connections allow tracing ancestry and descendent paths. Age details like \u201cJohn was born in 1950\" are also injected. With encoded topology and dates, STRUCT-X can traverse the tree to identify the oldest member, Shekhar and the youngest, Nikhil, before subtracting their years to find the 54-year difference. This showcases interpreting familial links and performing numerical reasoning over injected facts. By attending over neighborhood nodes, STRUCT-X focuses on the most relevant semantic connections. The topology learning distinguishes close and distant relatives to assess ages. Retrieved date details fill knowledge gaps for calculation. Overall, this case validates STRUCT-X's capabilities in encoding complex structure topology and performing multi-step inference by combining connection reasoning and data-driven deduction. The example proves STRUCT-X can encode intricate hierarchical structures and use encoded topology to trace relationships and inject valuable factual knowledge. By learning contextual representations and connections in structured data, STRUCT-X successfully interprets semantic links between entities and integrates supplementary date details for numerical reasoning over multiple inference steps. This supports complex reasoning across topological dimensions."}, {"title": "D Experimental Parameter Settings", "content": "The Variable Description Details in Table 7 and hyperparameters in Table 9 provide concrete configuration details for STRUCT-X when evaluated on the four benchmark datasets. We can observe some key modeling choices - all models use a 4-layer graph encoder to learn topological representations, apply 30-40% node masking for knowledge gap simulation, and dedicate 256 dimensions to the Auxiliary Module for steering prompt/answer generation. Training hyperparameters are also shown, including batch sizes of 16-32, learning rates around 1e-4, and 10-20 training epochs. The number of tunable parameters indicates comparable model complexity across datasets."}, {"title": "E Self-Reg Module", "content": "IFRET Module This module decides if passage retrieval is needed using a scoring function:\n$\\displaystyle [IFReT_\\Theta(x) = f_\\Theta(x)$ (8)\nWhere x is the input text, and fo outputs a binary decision on whether to activate retrieval given x, parameterized by 4. For example, if the input is x: \"Tell me more about Van Gogh's paintings\","}, {"title": "F.1 Knowledge Graphs and Graph Networks", "content": "A knowledge graph (KG) is defined as G = {(h, r, t)}, with \u201chead\u201d h and \u201ctail\u201d t entities from E and relation type r from R. Each triplet represents unique knowledge and such knowledge representation in KG can enhance LLMs reasoning (Liu, 2020). Graph neural networks (GNNs) process graphs G = (V, E) with nodes V and edges E, learning node representations by message passing, combining node features and graph topology (Wu, 2020). GNNs encode KGs' topology and structure. Node v's feature vector at layer l is $\\displaystyle h_{v}^{(l)}$, with neighboring nodes N (v) and edge feature evu, where the function M(1)(.) aggregates neighboring node information, and o(\u00b7) is an activation function:\n$\\displaystyle h_{v}^{(l+1)} = \\sigma (\\sum_{u \\in N(v)} M^{(l)}(h_{v}^{(l)}, h_{u}^{(l)}, e_{vu}) ),$\nGraph attention networks (GAT), a subclass of graph networks, leverage self-attention mechanisms (Brody et al., 2021), similar to the Transformer architecture, to enhance node representations (Li, 2021a). Each layer projects node features into queries Q, keys K, and values V, with at-"}, {"title": null, "content": "tention coefficients calculated between connected nodes, following the Transformer model (Yang, 2020; Khan, 2022). These coefficients are used to aggregate neighboring value vectors, updating the node feature representation $\\displaystyle h_{v}^{(l+1)}$:\n$\\displaystyle e_{vu} = LeakyReLU (a^{T} [W h_{v}|| W h_{u}] ),$ (13)\n$\\displaystyle h_{v}^{(l+1)} = \\sigma (\\sum_{u \\in N(v)} V h_{u}).$ (14)\nThrough learning to focus on the most relevant semantic connections, our networks can refine node embeddings efficiently. The model constructs contextual node representations in KGs using graph attention layers (Zhang et al., 2020), and subsequently integrates this structural knowledge into language models. This process improves the overall understanding and reasoning capabilities in tasks like semantic analysis and knowledge inference (Banerjee et al., 2020)."}, {"title": "F.2 Implementation Details", "content": "Training details We optimize model parameters using Adam optimizer with a learning rate of 1e-4, batch size of 32, and train for a maximum of 20 epochs. For testing, model accuracy is evaluated"}]}