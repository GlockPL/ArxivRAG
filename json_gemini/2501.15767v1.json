{"title": "Formal Verification of Markov Processes with Learned Parameters", "authors": ["Muhammad Maaz", "Timothy C. Y. Chan"], "abstract": "We introduce the problem of formally verifying properties of Markov processes where the parameters are the output of machine learning models. Our formulation is general and solves a wide range of problems, including verifying properties of probabilistic programs that use machine learning, and subgroup analysis in healthcare modeling. We show that for a broad class of machine learning models, including linear models, tree-based models, and neural networks, verifying properties of Markov chains like reachability, hitting time, and total reward can be formulated as a bilinear program. We develop a decomposition and bound propagation scheme for solving the bilinear program and show through computational experiments that our method solves the problem to global optimality up to 100x faster than state-of-the-art solvers. We also release markovml, an open-source tool for building Markov processes, integrating pretrained machine learning models, and verifying their properties, available at https://github.com/mmaaz-git/markovml.", "sections": [{"title": "1. Introduction", "content": "Markov processes are fundamental mathematical models used in diverse fields including computer science, operations research, engineering, and healthcare. In computer science, they can be used to model computer systems with probabilistic behavior, for example, hardware, communication protocols, or autonomous systems (Baier & Katoen, 2008). In engineering, they serve as fundamental models for analyzing machine degradation and failures (Rausand & Hoyland, 2003). In healthcare, they can model the progression of patients between health states, which can be used for cost-benefit analysis of drugs and medical technologies (Sonnenberg & Beck, 1993). We refer the reader to Stewart (2021) for a comprehensive review of their applications.\nIncreasingly, machine learning (ML) models are being integrated into probabilistic systems either as components of the systems themselves, e.g., integrating ML models into autonomous systems (Baier & Katoen, 2008), or to enrich the Markov models by incorporating heterogeneous parameters from observed data. For example, an ML model can predict failure rates based on machine-specific conditions and these predictions can be incorporated into a Markov chain to determine expected lifetime. In healthcare, transition probabilities can be patient specific, depending on demographics or specific clinical features (Mertens et al., 2022). Effectively, this leads to Markov processes where the parameters are not fixed, but instead are represented by learned functions.\nThere are few approaches to rigorously analyzing Markov processes with embedded ML models. In the healthcare literature, such Markov models are most often analyzed using simulation (Krijkamp et al., 2018). While this allows for subgroup analysis, e.g., performing a cost-benefit analysis for men over the age of 60, simulation cannot provide formal guarantees about outcomes. This limitation is particularly important in high-stakes applications like healthcare decision-making or safety-critical systems.\nIn this paper, we develop a new approach to analyzing Markov processes with embedded ML models. Our key insight is that techniques developed for formal verification of ML models can be extended to analyze Markov processes with embedded ML models. This connection allows us to move beyond simulation to provide formal guarantees for properties of Markov processes by solving a bilinear program. While our primary motivation comes from healthcare applications, our framework is general and applies to any domain where ML models are used as inputs to Markov processes. This allows us to answer questions like:\n\u2022 For a probabilistic program with a given bound on the input, what is the maximum probability of reaching a failure state?\n\u2022 For machines with certain properties, is the probability of failure below 0.01%?\n\u2022 For patients within a certain subgroup, is the expected cost of a drug below the government's threshold?\nTo summarize, our main contributions are:"}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Formal Verification of Markov Processes", "content": "Probabilistic model checking is a set of techniques used in computer science to formally verify the properties of probabilistic systems, with foundations in Markov chain theory (Hansson & Jonsson, 1994; Baier & Katoen, 2008). Tools like PRISM (Kwiatkowska et al., 2002), MRMC (Katoen et al., 2005), E - MC2 (Hermanns et al., 2000), VESTA (Sen et al., 2005), YMER (Younes, 2005), and APMC (Lassaigne & Peyronnet, 2002) allow users to define a Markov process with fixed parameters and verify whether specific properties hold using numerical methods. Some tools, such as PRISM, also support parametric model checking (Chen et al., 2013), where the parameters of the Markov process are represented as rational functions of external variables. In this light, our work broadens the scope of parametric model checking to situations when the parameters are determined by ML models. Similar to current model checking tools, we developed an expressive domain-specific language that allows users to specify and verify properties of Markov processes with learned parameters."}, {"title": "2.2. Markov Processes with Uncertain Parameters", "content": "Markov processes with variable parameters are well-studied in the mathematics and operations research literature. There are generalizations of classical Markov theory results, like the Perron-Frobenius theorem, bounds on hitting times, and bounds on stationary distributions to when the transition matrix lies in a credal set (Blanc & Den Hertog, 2008; De Cooman et al., 2014). Marbach & Tsitsiklis (2001; 2003) study Markov reward processes where the probabilities are a differentiable function of a parameter. There is also a large body of work on Markov decision processes with uncertain parameters, well known in the reinforcement learning literature (El Ghaoui & Nilim, 2005; Iyengar, 2005; Goyal & Grand-Clement, 2023; Grand-Cl\u00e9ment & Petrik, 2024). Goh et al. (2018), also inspired by healthcare modeling, develop a value iteration algorithm to compute the optimum of the total reward of a Markov process when the transition matrix lies in an uncertainty set. Chan & Maaz (2024) study"}, {"title": "2.3. Formal Verification of ML Models", "content": "Formal verification of ML models has emerged as an important research area in recent years, particularly for safety-critical applications. Here, the properties to verify are, e.g., robustness to input perturbations, or bounds on the output. Most of this work has been driven by interest in verifying properties of neural networks, as they are complex, widely deployed in critical applications like aircraft control (Owen et al., 2019), yet can be sensitive to small perturbations in input (Szegedy, 2013). Neural networks with ReLU activations can be easily encoded into SMT (satisfiability modulo theories) or into a mixed-integer linear program. A key breakthrough was the development of Reluplex (Katz et al., 2017), a modified simplex algorithm for verifying properties of neural networks. Modern state-of-the-art approaches, like \u03b1, \u03b2-CROWN, integrate various methods like bound propagation, branch-and-bound, and GPU-optimized implementations (Wang et al., 2021; Zhang et al., 2022b;a; Kotha et al., 2023). There is increasing interest in the optimization literature on optimal formulations of ML models, particularly for neural networks (Anderson et al., 2020; Tjandraatmadja et al., 2020; Kronqvist et al., 2021; Anh-Nguyen & Huchette, 2022). Our paper leverages these formulations to embed ML models into the overall optimization problem."}, {"title": "2.4. Subgroup Analysis in Healthcare", "content": "Our work is most directly inspired by the problem of subgroup analysis in healthcare cost-benefit analysis. Markov \"microsimulation\" models are a popular tool for analyzing the trajectories of patients over time, e.g., the progression of cancer or other diseases, which allow for heterogeneous parameters (Krijkamp et al., 2018). These allow for the computation of important metrics in healthcare economics like the incremental cost-effectiveness ratio (ICER) and the net monetary benefit (NMB) (Sonnenberg & Beck, 1993). In a scoping review, Mertens et al. (2022) find that estimation of individual disease risk \u2013 in Markov theory, computing the transition probabilities \u2013 is often computed using so-called risk scores from the medical literature (e.g., (Wilde et al., 2019; Lee et al., 2020; Breeze et al., 2017)). These risk score formulas are most often computed with logistic regression applied to observational data, though other models are beginning to be used. Most current work in this domain uses Monte Carlo simulation (Krijkamp et al., 2018). In contrast, our work allows provides an exact solution to subgroup analysis by solving an optimization problem to global optimality. Our framework also allows for the analysis of a broad class of ML models, beyond just logistic regression,"}, {"title": "2.5. Bilinear Programming", "content": "As we will show, our problem can be formulated as a bilinear program. Bilinear programs are a type of non-convex NP-hard optimization problem where the objective and/or the constraints are bilinear functions (Horst & Pardalos, 2013). In theory, it is possible solve them to global optimality with branch-and-bound methods (Horst & Pardalos, 2013), among others (Gallo & \u00dclk\u00fcc\u00fc, 1977). The McCormick relaxation (McCormick, 1976) is a standard tool for bilinear programs which forms a convex relaxation, helping the solver to obtain good bounds on the solution. Indeed, this and other methods are already implemented in modern optimization software like Gurobi (Gurobi Optimization, LLC, 2024). Under additional assumptions on the structure of the bilinear program, it is possible to form tighter relaxations than McCormick's (Dey et al., 2019), or even solve the optimization problem using mixed-integer linear programming (Horst & Pardalos, 2013). Unfortunately, our problem does not satisfy these structural requirements and thus requires a new approach."}, {"title": "3. Problem Formulation", "content": null}, {"title": "3.1. Notation", "content": "Vectors are lowercase bold, e.g., x, with ith entry $x_i$, and matrices by uppercase bold letters, e.g., M with (i, j)-th entry $M_{ij}$. The identity matrix is denoted by I, the vector of all ones by 1, with dimensions inferred from context. The set of integers from 1 to n is denoted by [n]."}, {"title": "3.2. Preliminaries", "content": "A (discrete-time, finite-state) Markov chain with n states, indexed by [n], is defined by a row-stochastic transition matrix $P \\in \\mathbb{R}^{n \\times n}$, where $P_{ij}$ is the probability of transitioning from state i to state j, and a stochastic initial distribution vector $\\pi \\in \\mathbb{R}^{n}$, where $\\pi_{i}$ is the probability of starting in state i. Furthermore, if we assign rewards to each state, we call this a Markov reward process. A Markov reward process has a reward vector $r \\in \\mathbb{R}^{n}$, where $r_{i}$ is the reward for being in state i for one period. A state is absorbing if it cannot transition to any other state, and transient otherwise.\nBelow, we recount three of the key properties commonly computed in practice (Puterman, 2014)."}, {"title": "Definition 3.1 (Reachability).", "content": "The probability of eventually reaching a set of states $S \\subseteq [n]$, from a set $T \\subseteq [n]$ of transient states not in S, is given by $\\pi^T (I-Q)^{-1} R \\mathbf{1}$, where Q is the transition matrix restricted to T, R is the transition matrix from T to S, and $\\pi$ is the initial distribution over T."}, {"title": "Definition 3.2 (Expected hitting time).", "content": "The expected number of steps to eventually reach a set of states $S \\subseteq [n]$, from a set $T \\subseteq [n]$ of transient states not in S, assuming that the chain will reach S from T with probability 1, is given by $\\pi^T (I - Q)^{-1} \\mathbf{1}$, where Q is the transition matrix restricted to T, and $\\pi$ is the initial distribution over T."}, {"title": "Definition 3.3 (Total infinite-horizon discounted reward).", "content": "The total infinite-horizon discounted reward, with a discount factor $\\lambda \\in (0,1)$ is given by $\\pi^T (I - \\lambda P)^{-1} r$.\nFor each of the above quantities, we can restrict the analysis to a single state, e.g., for reachability starting from a state i, we simply set $\\pi_i = 1$ and for $j \\neq i$, set $\\pi_i = 0$.\nThese three quantities enable rich analysis of Markov processes and the systems they model. Reachability can be used to verify the probability of failure in a system. Expected hitting time can be used to compute the expected time to failure, or life expectancy of a person. Total reward can be used to compute the resource consumption of a system or total cost of a drug.\nWe wish to verify these properties, namely finding their maximum or minimum. If the Markov process' parameters are fixed, as is common in model checking, these quantities can be computed exactly by solving a linear system. However, in our case, the parameters are given by ML models, and so we will formulate an optimization problem to derive bounds on these quantities.\nAs the three quantities have similar formulations (e.g., hitting time is essentially the same as the total reward if the r is replaced by 1 and suitable adjustments for the initial distribution and transition matrix), we will proceed in the rest of the paper by studying the total reward. It will be easy to modify results for the other two quantities."}, {"title": "3.3. Embedding ML Models", "content": "We now consider $\\pi$, P, r as functions of a feature vector $x \\in \\mathbb{R}^{m}$, where m is the number of features.\nWe encode the relationship from x to $\\pi$, P, r via a set of functions, $f_1, f_2, ..., f_{k_f}$, where $k_f$ is the number of functions. Each function $f_i : \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{l_i}$ for i = 1, 2, ..., $k_f$ takes a feature vector and outputs a vector $\\theta_i$ (e.g., a classifier may output class probabilities for three classes). We concatenate these vectors to form the output vector $\\theta \\in \\mathbb{R}^{l}$, where $l = \\Sigma_{i=1}^{k_f} l_i$.\nOur key assumption on the functions will be that they are mixed-integer linear representable (MILP-representable), meaning that the relationship between the inputs and outputs can be expressed using linear constraints and binary variables. This is a broad class of functions that includes, e.g., piecewise linear functions (including simple \"if-then\""}, {"title": "4. Solving the Optimization Problem", "content": "Under the assumptions that the functions $f_1, f_2,\\cdots, f_{k_f}$ are MILP-representable and X is MILP-representable, the optimization problem above is a mixed-integer bilinear program, as we have a bilinear objective and a bilinear constraint. We will leverage the observation that strong bounds on variables are crucial for solving bilinear programs, and will derive such bounds by decomposing our problem and using various results from Markov theory.\nOur strategy is as follows: obtain bounds on the elements of $\\theta$ by solving a series of smaller MILPs, propagate these bounds to the parameters via the affine equalities, obtain bounds on v using linear algebra arguments, and then tighten the v bounds using interval matrix analysis. All proofs are in Appendix B"}, {"title": "4.1. Bounds on \u03b8", "content": "For each $\\theta_i$, we minimize and maximize it by solving two smaller MILPs, over the set X. Namely, for each $i \\in [k_f]$, for each $j \\in [l_i]$, solve for $\\text{min/max}_{x \\in X} \\theta_{i,j} \\text{ s.t. } \\theta_i = [\\theta_{i,1}, ..., \\theta_{i,l_i}] = f_i(x)$, which are MILPs as X and $f_1,... f_{k_f}$ are MILP-representable."}, {"title": "4.2. Bounds on \u03c0, \u03a1, r", "content": "We now propagate the bounds on $\\theta$ to the parameters $\\pi$, P, r via the affine equalities."}, {"title": "Lemma 4.1.", "content": "Let $\\theta \\in \\mathbb{R}^l$ be a vector where each component satisfies the bounds $\\theta_i^{\\min} \\leq \\theta_i \\leq \\theta_i^{\\max}$ for all $i = 1,2,..., l$. Consider an affine transformation defined by $y = A\\theta + b$, where $y \\in \\mathbb{R}^k$, $A \\in \\mathbb{R}^{k \\times l}$, and $b \\in \\mathbb{R}^k$. Then, each component $y_i$ of y satisfies $b_i + \\sum_{j=1}^l A_{ij} (\\theta_j^{\\min} \\mathbf{1}_{A_{ij} \\geq 0} + \\theta_j^{\\max} \\mathbf{1}_{A_{ij} < 0}) \\leq y_i \\leq b_i + \\sum_{j=1}^l A_{ij} (\\theta_j^{\\max} \\mathbf{1}_{A_{ij} \\geq 0} + \\theta_j^{\\min} \\mathbf{1}_{A_{ij} < 0}).$"}, {"title": "4.3. Bounds on v", "content": "Next, we derive bounds on v using well-established facts from Markov theory. First, as $v = (I-\\lambda P)^{-1}r$, we analyze bounds on $(I - \\lambda P)^{-1}$."}, {"title": "Lemma 4.2.", "content": "Let P be row-stochastic and $\\lambda \\in (0,1)$. Then, each element in $(I - \\lambda P)^{-1}$ is in the interval [0, 1/(1 \u2212 \u03bb)] and the row sums are all equal to 1/(1 \u2212 \u03bb)."}, {"title": "Lemma 4.3.", "content": "Let $r^{\\min} \\leq r_i \\leq r^{\\max}$. Then, each element in the vector v satisfies $\\gamma^{\\min} r^{\\min} \\leq v_i \\leq \\gamma^{\\max} r^{\\max}$, where $\\gamma = \\frac{1}{1-\\lambda}$"}, {"title": "4.4. Tightening bounds on v", "content": "Interval matrix analysis studies a generalization of matrices (or vectors), where each element is an interval instead of a real number. Equivalently, it can be viewed as studying the set of all matrices that lie within given intervals. There are easy generalizations of arithmetic of real numbers to intervals, which can be used to generalize matrix arithmetic to interval matrices. However, inverting an interval matrix is difficult due to the inherent nonlinearity of the inverse operation. Consider the equivalent problem of solving a linear system with interval matrices. The solution set is not rectangular, so cannot be described as an interval matrix. Hence, we wish to find the interval matrix of smallest radius that contains the solution set, known as the hull (Neumaier, 1984).\nDue to the interval extensions of the basic arithmetic operations, it is possible to generalize the Gauss-Seidel method, a common iterative procedures for solving linear systems, to interval matrices. Beginning with an initial enclosure of the solution set, it iteratively refines the intervals and returns a smaller enclosure of the solution set. We provide a full description of the Gauss-Seidel method in Appendix C. The interval Gauss-Seidel method is optimal in the sense that it provides the smallest enclosure of the solution set out of a broad class of algorithms (Neumaier, 1984).\nHence, we can apply the Gauss-Seidel method to tighten the bounds on v, as $(I - \\lambda P)v = r$ is a linear system. We use the bounds on P and r from Lemma 4.1 to form the interval matrices, and for our initial guess, we can use the bounds derived from Lemma 4.3."}, {"title": "Theorem 4.4.", "content": "Let P be a row-stochastic matrix bounded element-wise by $P^{\\min} \\leq P \\leq P^{\\max}$. Then, $I - \\lambda P$ is an interval M-matrix if and only if $\\rho(P^{\\max}) < \\frac{1}{\\lambda}$, where $\\rho$ is the spectral radius."}, {"title": "4.5. Solving the final optimization problem", "content": "At last, we can assert the bounds found above for $\\pi$, P, r, v as constraints and solve the full bilinear program with any out-of-the-box solver. We summarize our procedure in Algorithm 1."}, {"title": "4.6. Extensions: reachability and hitting time", "content": "Our results also extend to reachability and hitting time, if we take care of minor technical details arising from the invertibility of I \u2212 Q. See Appendix D for full details."}, {"title": "5. Implementation", "content": "We developed the Python package markovml to specify Markov chains or reward processes with embedded pretrained machine learning models. Our domain-specific language lets users:\n1. Instantiate a Markov process\n2. Add pretrained ML models from sklearn (Pedregosa et al., 2011) and PyTorch (Paszke et al., 2019)\n3. Link model outputs to Markov parameters with affine equalities\n4. Include extra linear inequalities\n5. Specify the feature set with MILP constraints\n6. Optimize reachability, hitting time, or total reward\nBuilt on gurobipy (Gurobi Optimization, LLC, 2024), our package supports a variety of regression and classification models, including linear, tree-based, and neural networks. For some models, the MILP formulation is provided by gurobi-machinelearning (Gurobi Optimization, 2024), while for some models (e.g., softmax for classifiers),"}, {"title": "6. Numerical experiments", "content": "We compare our method to solving the optimization problem directly with an out-of-the-box solver. Our experiments will analyze the following drivers of complexity of the bilinear program: the number of states, the number of ML models, and the model complexity, notably for trees and neural networks."}, {"title": "6.1. Setup", "content": "For all experiments, we fix the number of features m = 5, the feature set $X = [-1,1]^5$, and the discount factor $\\lambda = 0.97$. We train ML models on randomly generated data (10,000 random points). For regression models, we draw $X \\sim N(0, 1)^{10000 \\times 5}$, $\\beta \\sim N(0,1)^5$, and then generate data points as $X\\beta + \\epsilon$, where $\\epsilon \\sim N(0,1)^{10000}$. For classification models, we use the same linear form but draw data points from a Bernoulli distribution with probabilities $\\sigma(X\\beta)$, where $\\sigma(\\cdot)$ is the logistic function.\nFor rewards, we train a regression model whose output is set to $r_1$. For i = 2, . . ., n, we set $r_i$ to be output divided by i. For probabilities, we train binary classifiers for $\\pi$ and rows of P. For $\\pi$, we set $\\pi_1$ equal to the classifier output, the remaining probability is assigned to $\\pi_2$, and the remaining are set to zero. For rows of P, the classifier output is the probability of remaining in the same state, with the remaining probability assigned to transitioning to the next state, and the rest of the row is 0. We make the last state absorbing, i.e., transitions to itself with probability 1.\nFor each configuration, we run 10 instances, each with random data, train the models on the random data, and solve the optimization problem directly with an out-of-the-box solver and with our method."}, {"title": "6.2. Statistical analysis", "content": "We computed the geometric mean and geometric standard deviations of runtimes, and proportions of optimizer statuses. For the runtime analysis, we only kept instances that were solved to optimality. We performed paired t-tests on the log of runtimes (for instances where both methods returned optimal) and $x^2$ tests for statuses."}, {"title": "6.3. Results", "content": "Our method consistently outperforms directly solving the optimization problem. This is despite our method involving solving multiple optimization problems. Figure 2 shows that it scales much better with all measures of complexity, and the differences are statistically significant, often at 1% and 0.1% significance.\nThe difference is particularly striking as models get more complex: in the tree depth experiments, our method is on average 117x faster for trees of depth 6 (direct mean 82.1 sec, our method mean 0.7 sec), and 391x faster for depth 8 (direct mean 665.3 sec, our method mean 1.7 sec) \u2013 reaching a 1000x speedup on some instances. For neural networks, solving the problem directly times out in every instance, as seen in Figure 3, even for the smallest architecture with 1 hidden layer of 5 neurons. However, our method solves all instances with 1 hidden layer and most instances with 2 hidden layers, although it often times out with 2 hidden layers of 20 neurons each. Figure 3 also shows that the direct method often struggles with proving optimality."}, {"title": "7. Conclusion", "content": "We have presented a novel framework for the formal verification of Markov processes with learned parameters, enabling rigorous analysis of systems where transition probabilities and rewards are determined by machine learning models. By formulating the problem as a bilinear program and introducing a novel decomposition and bound propagation scheme, we provide a computationally efficient approach that is orders of magnitude faster than solving the problem directly. Our software, markovml, provides a flexible way of building a Markov process and embedding several common ML models, and optimizing for the three quantities we have discussed in our paper. Our experimental results demonstrate the method's scalability across varying problem complexities, including state space size, model count, and model complexity. Though our method struggles with increasing neural network size, this is expected as verifying neural networks is computationally difficult. Future work may look at incorporating the latest advances from neural network verification into our framework."}, {"title": "Impact Statement", "content": "Our work contributes to improving the safety and reliability of systems that combine machine learning with probabilistic modeling. Such settings arise in diverse fields, including healthcare and engineering. In healthcare, this enables precise analysis of patient risk and cost-effectiveness, supporting more transparent and informed decision-making. For safety-critical autonomous systems, our framework helps ensure that probabilistic failure modes are well understood\u2014even when some components are data-driven\u2014thereby enhancing overall system safety. While our method relies on certain technical assumptions (e.g., MILP-representability of models and feature sets), careful validation in real-world applications can ensure that the formal guarantees are meaningful. Future work should also investigate its applicability to systems of a real-world scale."}, {"title": "B. Omitted Proofs from Section 4", "content": null}, {"title": "Proof of Lemma 4.1.", "content": "For a given component $y_i$, we can write the transformation as:\n$y_i = \\sum_{j=1}^l A_{ij}\\theta_j + b_i$\nfor all i = 1, 2, ..., k.\nEach term in the summation depends linearly on $\\theta_j$. If $A_{ij} \\geq 0$, then $A_{ij}\\theta_j$ is maximized at $\\theta_j^{\\max}$ and minimized at $\\theta_j^{\\min}$. Vice versa, if $A_{ij} < 0$, then $A_{ij}\\theta_j$ is maximized at $\\theta_j^{\\min}$ and minimized at $\\theta_j^{\\max}$.\nApplying this to all terms in the summation, we get the required bounds."}, {"title": "Proof of Lemma 4.2.", "content": "Note that the matrix $(I - \\lambda P)$ is a (non-singular) M-matrix. By Theorem 2.3 in Chapter 6 of (Berman & Plemmons, 1994), each element of the matrix is non-negative, so is $\\geq 0$.\nNext, consider the row sum of $(I - \\lambda P)^{-1}$ for a row i: $\\Sigma_j (I - \\lambda P)_{ij}^{-1}$, and then expand into the Neumann series: $\\Sigma_j \\Sigma_{k=0}^{\\infty} \\lambda^k (P^k)_{ij}$. We can exchange the order of summation as all terms are non-negative, and then we have our sum is $\\Sigma_{k=0}^{\\infty} \\lambda^k \\Sigma_j (P^k)_{ij}$. Every positive integer exponent of a row-stochastic matrix is also row-stochastic, so $\\Sigma_j (P^k)_{ij} = 1$, for all $k \\in \\mathbb{N}$. Therefore, this simplifies to the geometric series $\\Sigma_{k=0}^{\\infty} \\lambda^k$, which equals $\\frac{1}{1-\\lambda}$. As each row sum is $\\frac{1}{1-\\lambda}$, and each element is non-negative, each element must also be at most $\\frac{1}{1-\\lambda}$"}, {"title": "Proof of Lemma 4.3.", "content": "Recall that $v = (I - \\lambda P)^{-1}r$, so $v_i = \\Sigma_{j=1}^n(I - \\lambda P)_{ij}^{-1}r_j$. Thus, we can see $v_i$ is a positive linear combination of the elements of r, where the nonnegative weights are from $(I - \\lambda P)^{-1}$ and sum to 1/(1 \u2212 \u03bb). To maximize this combination, we assign all the weight to the maximum upper bound of r, and to minimize it, we assign all the weight to the minimum lower bound of r. Hence, we have the required bounds."}, {"title": "Proof of Theorem 4.4.", "content": "Let $M = I - \\lambda P$. Clearly, M is bounded element-wise by $M^{\\max} = I - \\lambda P^{\\min}$ and $M^{\\min} = I - \\lambda P^{\\max}$. The upper bound $M^{\\max}$ has no positive off-diagonal elements, as all of the elements of $P^{\\max}$ are between 0 and 1. So, the only thing we need to check is that $M^{\\min}$ is an M-matrix.\nFrom the definition of an M-matrix (Berman & Plemmons, 1994), $M^{\\min}$ is an M-matrix if and only if $1 > \\rho(\\lambda P^{\\max}) = \\lambda \\rho(P^{\\max})$. This completes the proof."}, {"title": "C. Gauss-Seidel Method for Interval Matrices", "content": "Our exposition here follows section 5.7 of Hor\u00e1\u010dek (2019). Suppose we have a linear system Ax = b, where A is an interval matrix and b is an interval vector. The set of x that satisfies this system is referred to as the solution set. It is in general complicated, so we hope to find a rectangular enclosure of the solution set. The smallest such enclosure is referred to as the hull of the solution set.\nSuppose we have an initial enclosure of the solution set, $x^{(0)}$. In general this takes some effort to obtain, but we obtained this for our specific problems. Then, the interval Gauss-Seidel method proceeds as follows:\n1. For each variable $x_i$, compute a new enclosure of the solution set as:\n$y_i^{(k+1)} = \\frac{1}{A_{ii}} \\left[b_i - \\sum_{j < i} A_{ij} x_j^{(k+1)} - \\sum_{j > i} A_{ij} x_j^{(k)} \\right]$\n2. Update the enclosure of the solution set: $x^{(k+1)} = x^{(k)} \\cap y^{(k+1)}$.\n3. Repeat steps 1 and 2 until stopping criterion is met (either a maximum number of iterations or the difference in subsequent enclosures is less than some tolerance)."}, {"title": "D. Solving other formulations", "content": null}, {"title": "D.1. Reachability and hitting time", "content": "The reachability and hitting time formulations are in Appendix A. The key difference that affects our theoretical results and algorithms is that the matrix Q, which is a submatrix of P restricted to the transient states, is strictly substochastic, and we don't have a discount factor \u03bb anymore. This introduces some technical difficulties in guaranteeing invertibility of I \u2212 Q. However, all our theoretical results all follow through with minor modifications."}, {"title": "Lemma D.1.", "content": "For a strictly row-substochastic matrix Q with all elements in [0, 1]:\n1. The spectral radius of Q is strictly less than 1.\n2. The matrix $(I - Q)$ is a (non-singular) M-matrix.\n3. The matrix $(I - Q)^{-1}$ has all non-negative elements.\n4. The row sums of $(I - Q)^{-1}$ are at most 1/(1 \u2212 a), where $a = \\max_i \\Sigma_j Q_{ij}$ is the maximum row sum of Q.\n5. Each element of $(I - Q)^{-1}$ is in the interval [0, 1/(1 \u2212 \u03b1)]."}, {"title": "Corollary D.2.", "content": "Given a Markov reachability problem with element-wise bounds on Q and R", "are": ""}]}