{"title": "ViRED: Prediction of Visual Relations in Engineering Drawings", "authors": ["Chao Gu", "Ke Lin", "Yiyang Luo", "Jiahui Hou", "Xiang-Yang Li"], "abstract": "To accurately understand engineering drawings, it is essential to establish the correspondence between images and their description tables within the drawings. Existing document understanding methods predominantly focus on text as the main modality, which is not suitable for documents containing substantial image information. In the field of visual relation detection, the structure of the task inherently limits its capacity to assess relationships among all entity pairs in the drawings. To address this issue, we propose a vision-based relation detection model, named ViRED, to identify the associations between tables and circuits in electrical engineering drawings. Our model mainly consists of three parts: a vision encoder, an object encoder, and a relation decoder.\nWe implement ViRED using PyTorch to evaluate its performance. To validate the efficacy of ViRED, we conduct a series of experiments. The experimental results indicate that, within the engineering drawing dataset, our approach attained an accuracy of 96% in the task of relation prediction, marking a substantial improvement over existing methodologies. The results also show that ViRED can inference at a fast speed even when there are numerous objects in a single engineering drawing.", "sections": [{"title": "I. INTRODUCTION", "content": "DIGITIZATION of engineering design drawings constitutes a crucial component of contemporary industrial processes. Nevertheless, the automated recognition of these engineering drawings in image format still encounters considerable challenges. Electrical engineering drawings, a subset of engineering drawings, are primarily used to depict equipment related to electrical systems. Typically, electrical design engineers are required to review and recreate numerous electrical engineering drawings to transform technical illustrations into production-level drawings. To alleviate this workload, it is crucial to develop automated recognition methods for electrical engineering drawings, which are stored in image format.\nIn general, a single electrical engineering drawing includes multiple circuits and tables as depicted in Fig. 1. Each circuit describes different electrical devices, while each table presents the parameter settings for the corresponding circuits. To enhance the identification of circuits and tables, it is necessary to extract these components from diagrams and analyze them using specialized models. While there is a considerable amount of existing work on detecting and classifying specific objects within images [1]\u2013[3], the correspondence between these objects is often lost in this process. Most current methods for extracting relations within documents depend on the relations between text pairs [4]\u2013[6], rendering them insufficient for addressing relations between diagrams and tables. In visual relation detection methods [7]\u2013[9], the task and model design limit their capability to predict relationships across all circuit-table pairs, thereby causing missed detections.\nThe circuit-to-table relations in electrical engineering drawings are complex, potentially exhibiting one-to-one, one-to-many, and other variations. Furthermore, the quantity of circuits and tables in each image differs, presenting difficulties in managing variable-length inputs using non-sequential modeling methods. To address these issues, we propose ViRED, a Visual Relation prediction model for Engineering Drawings based on Transformer architecture [10]. ViRED consists of a visual encoder, an object encoder, and a relationship decoder. The model is trained and fine-tuned utilizing the PubLayNet [11] dataset along with the proprietary electrical engineering drawing dataset, resulting in exceptional performance in the task of relation prediction.\nThe main contributions of this work are outlined as follows:\n\u2022 We present a novel vision-based relation detection approach, named ViRED, to address the issue of predicting relations for non-textual components in complex documents. This approach has been specifically implemented for the purpose of circuit-to-table relation matching in electrical design drawings.\n\u2022 We develop a dataset of electrical engineering drawings derived from industrial design data, and we annotate the instances and their relationships within the dataset.\n\u2022 We evaluate our method using various metrics on the electrical engineering drawing dataset. Furthermore, we perform comparative analyses with existing approaches and provide a performance comparison between the existing methods and our proposed technique.\n\u2022 We perform extensive ablation studies to compare the impact of different model architectures, hyperparameters, and training methods on the overall performance. Moreover, we refined our model architecture based on these"}, {"title": "II. RELATED WORKS", "content": "A. Visual Document Understanding\nThe task of visual document understanding (VDU) focuses on understanding digital documents in image formats. There are several downstream tasks associated with VDU, including key information extraction, relation detection, document layout analysis, and others. Most contemporary VDU techniques rely on deep neural networks that utilize visual, textual, or a mixture of visual and textual modalities.\nApproaches for document understanding that utilize computer vision were initially proposed following advancements in convolutional neural networks (CNN). Hao et al. [12] proposed the use of CNN for detecting tables in document images. With the introduction of the R-CNN series model [13]\u2013[16], the methodologies for detecting tables [17]\u2013[19] and analyzing layouts [11], [20] in document images have benefited from these models, resulting in enhanced performance. Modern Optical Character Recognition (OCR) engines [21] have demonstrated significant efficacy in extracting text from document images. Furthermore, the advancement of language models has contributed to the tasks of document understanding. However, recent approaches in VDU incorporate other modalities such as textual information as auxiliary information. LayoutLM [4] utilizes textual data in conjunction with the bounding box provided by the OCR engine for inference. DocFormer [22], LayoutLMv2 [23] and LayoutLMv3 [24] utilize early fusion techniques to better integrate and leverage visual and textual modality information.\nB. Visual Relation Detection\nVisual relation detection (VRD) involves the task of predicting the relations or interactions between pairs of objects within a single image [25]. Typically, VDR is a mid-level vision task that extracts information from low-level vision tasks such as object detection and recognition [26]. Despite the advancement of VDR techniques, the task remains difficult because of conflicts arising from the various potential relations and the lack of labeled data [7].\nLu et al. [26] introduced the first VDR approach utilizing deep learning techniques. This method employed R-CNN for object and predicate detection and incorporated language priors to enhance the accuracy of predicted relations. Subsequently, ViP-CNN was introduced by Li et al. [27] to identify subject, predicate, and object simultaneously. This method incorporates a Phrase-Guided Message Passing Structure to investigate the interconnections of relation components. Zhuang et al. [28] proposed a context-aware interaction classification framework based on an attention mechanism,"}, {"title": "III. METHODOLOGY", "content": "A. Problem Definition\nGiven an image of an electrical engineering drawing, let there be Nc circuits and Nt tables present within the image. There may be a relation between a circuit and a table. That is, there may exist at most Nc \u00d7 Nt relations in the engineering drawing. Assuming the presence of bounding boxes and their associated instance type labels, we aim to determine if there exists a relation between specific circuits and tables.\nB. Model Architecture\nViRED has three main components: a pretrained vision encoder, a lightweight object encoder, and a fast relation decoder. Fig. 2 presents the overview of our model pipeline. We will describe these components in the following sections.\n1) Vision encoder: The vision encoder in the model processes an image Idoc of size 3\u00d7 HImage \u00d7 WImage and produces an image embedding in either feature vector format Fvision or feature map format with dimensions Cv \u00d7 H \u00d7 Ww, where C, H, and W represent the channels, width, and height of the image and feature map. In cases where feature maps are produced, they are flattened and converted into feature"}, {"title": "", "content": "vectors. To prevent bias, a pre-trained vision encoder called the Masked Autoencoder (MAE) [30] is utilized in this study. The Document Image Transformer (DiT) [31] is employed as our vision encoder. The vision encoder is only used once for each image, regardless of the number of objects or relationships in the image.\n$$F_{vision} = Vision-Encoder(I_{doc})$$\n$$F_{vision} = W \u00b7 Flatten(Vision-Encoder(I_{doc}))$$\n2) Object encoder: The object encoder efficiently maps a bounding box and its typing information into a vectorial embedding. We use a convolutional neural network (CNN) to encode the bounding boxes. The object\u2019s bounding box is shown as a one-channel image Iobj with the same dimensions as the engineering drawing image. Pixels inside the bounding box are shown as 1, while those outside are shown as 0. The bounding box image is then encoded using a three-layer CNN.\n$$F_{mask} = CNN(I_{obj})$$\nTo inform the relation decoder about whether the embedding token represents a circuit or a table, we aggregate the vector embeddings of bounding boxes Fmask with two learned embeddings Ftype.\n$$F_{object} = F_{mask} + F_{type}$$\n3) Relation decoder: The relation decoder takes the encoded masks and vectorized images as input and predicts if there is a relation between each circuit-table pair. In detail, the relation decoder consists of two components: the fusion model and the relation prediction model.\nThe fusion model is employed for feature fusion between object masks and image features. Inspired by DETR [3], a transformer-decoder-based model is adopted. We modify the standard transformer decoder by eliminating the relative position embedding and causal mask because of the lack of order between objects. This transforms the decoder into a bidirectional transformer decoder. As shown in Fig. 2, each decoder layer consists of four parts. 1) The tokens are initially processed by a self-attention model as shown in Eq. 5, which enables the mask tokens to interact with each other. This step enables the objects to determine their relative positions. 2) Next, a cross-attention model is introduced, where the image embeddings are used as the key and value vector, and mask tokens are used as the query vector like Eq. 6. By utilizing the image-to-object cross-attention model, the mask tokens are modified through a combination of bounding box features and vision features. 3) Then, a feedforward layer updates all the mask tokens, and a dropout layer is used to improve the model\u2019s generalizability. 4) Finally, a residual connection is added to every attention layer and feed-forward layer, in accordance with the typical transformer architecture [10].\n$$F_{object} = Attention(F_{object}, F_{object}, F_{object})$$\n$$F_{object} = Attention(F_{object}, F_{vision}, F_{vision})$$\nAfter receiving the object tokens from the fusion model of the relation decoder, it is essential to identify the relationships between them using the relation prediction model. The combined object tokens are concatenated to generate (Nc + Nt)2 combination tokens. This process is depicted in Fig. 3. To streamline the training and inference procedures, redundant and unfeasible combinations are eliminated. The relation prediction model consists of a three-layer perceptron with ReLU activation and a linear projection layer. This model is utilized to convert hidden features into a two-dimensional logit output. Afterwards, the main objective of the relation prediction model is to determine whether a relationship exists between two objects.\n$$F_{relation_{i,j}} = F_{object_{i}} ; F_{object_{j}}$$\nRelation-Prediction = MLP(Frelation)\nC. Pretraining\nDue to the scarcity of annotated training data for relation detection in engineering drawings, we utilize the PubLayNet [11] dataset during the pretraining phase. PubLayNet offers a dataset consisting of 340,000 document images, including 3 million bounding boxes and type annotations for the instances within these images. To enhance the model\u2019s comprehension of positional data, we devise a pretraining task that employs our model to categorize the instance type of the masked region as shown in Fig. 2-(d). The architecture of our pretraining model exhibits minor variations compared to the relation prediction model. Since it is necessary to predict the category of the provided bounding boxes (i.e., table or circuit), the type embeddings integrated into the vector embeddings of bounding boxes within the object encoder are excluded. The relation prediction model is substituted with a multi-layer perceptron to perform classification tasks."}, {"title": "IV. EXPERIMENTS", "content": "A. Experiment Setup\n1) Dataset: In the pretraining phase, our method is trained and tested on the PubLayNet datasets with the same train and test splits as the original dataset. For the finetuning phase, we used a self-annotated dataset of engineering design diagrams containing 283 images, 4,566 entities, and 2,112 relations between entities. The datasets are randomly divided, with 90% assigned for training and 10% for evaluation.\n2) Implementation Details: The dimension D of latent representation throughout the model pipeline is set to 768. For the vision encoder backbone, we adopt the DINOv2-B model [35]. The model consists of 12 layers of Transformer encoders, each containing a 12-head multi-head attention block. The input image is cropped into patches of size 14, which are"}, {"title": "B. Metrics", "content": "Unlike the commonly used R@x metric in relation detection tasks [26], we use mAP, precision, and recall to assess model performance in relation detection tasks. Standard relation detection tasks may not annotate all possible relations in an image. In our task, relations between tables and circuits are clear, so using mAP is a more accurate indicator of model performance. Accuracy is used as the metric for comparison experiments."}, {"title": "C. Experiment Results", "content": "1) Qualitative Result: Qualitative results are shown in Fig. 4. The blue and green areas represent the bounding box regions of circuits and tables, respectively. In the presence of a relationship between a table and a circuit, a line segment will link the two entities.\n2) Comparison: In this section, we evaluate our model against current methods for relation detection. We focus on visual relation detection instead of text-based methods for document relation extraction. To align with the visual task, we modify the dataset by categorizing textual descriptions as either \"table\" or \"circuit.\" Relations are established between all circuit-table pairs, with existing relations labeled as \"describe\" and non-existing relations labeled as \"not describe.\"\nTable I provides a comparative analysis of our relation prediction method in contrast to several established approaches, with the accuracy metric used to measure the precision of the outcomes. Our findings demonstrate that our method surpasses the results of our engineering drawing relation prediction dataset. Importantly, we employ the evaluation methods used by the compared approaches to maintain consistency with our metrics. For MBBR, the accuracy is calculated based on the top-100 relation predictions.\n3) Ablation study: In this section, we evaluate our method in different settings and strategies.\nWe conduct an additive ablation study on the mAP, precision, and recall for relation prediction on the validation set, as illustrated in Tab. III. In the baseline model, we initialize the model with random parameters without pretraining and omit the type embedding from the object encoder. Introducing type embedding allows the model to better differentiate the categories of object tokens. When computing self-attention over object tokens, the relation decoder can simultaneously consider the categories of the objects, thereby reducing computations for improbable relations and enhancing the understanding of relations. Due to the insufficient data in our task's dataset, we introduced a pretraining mechanism for both the vision encoder and the position encoder-decoder. By undergoing unsupervised training on a large-scale document image dataset, the vision encoder more effectively extracts visual features from electrical engineering drawings. Supervised pretraining was utilized to predict the classification of selected regions based on given positional information, thereby enhancing the model's understanding of positional inputs. As shown in Table III, the inclusion of type embedding and model pretraining significantly improved the performance across all evaluation metrics.\nWe also conducted ablation experiments on the model architecture. We compared the effects of various vision encoders, diverse object encoders, different input image resolutions, and different numbers of transformer decoder layers in the relation decoder on the performance. The experimental results are shown in Tab. II. The hidden dimension of the model is determined by the output dimension of the vision encoder. For ViT-S, ViT-B, ViT-L [35], and ResNet-50 [36], the dimensions for latent representation are set to 384, 768, 1024, and 768, respectively. Upon analyzing the experimental results, it becomes evident that the object encoder architecture significantly influences model performance. The relation"}, {"title": "", "content": "decoder struggles to interpret object tokens encoded by ViT, resulting in a notable decline in performance across various metrics. Additionally, the vision encoder impacts the latent representation dimension of the model, thereby also playing a crucial role in determining model performance. The findings suggest that ViT-S performs poorly due to its constrained embedding dimension. For ViT-B and ViT-L, the limited task difficulty results in negligible performance differences. Higher image resolutions improve the vision encoder's capacity to capture more detailed image features. However, the vision encoder's ability to encode all these details is restricted by the dimension of the image feature vector, leading to a minimal effect on performance.\n4) Inference Efficiency: To evaluate the inference efficiency of the model, we compute the FLOPs (Floating Point Operations) for the inference process. For uniformity, a batch size of 1 is employed for all models. The input image resolution is fixed at 518 \u00d7 518, with the number of objects to be predicted (N) ranging from 1 to 20. The relations to be predicted include all possible interactions between objects, amounting to (N-1)2. The results are presented in Fig. 5. Owing to the utilization of a lightweight object encoder and relation decoder, the inference efficiency of ViRED is minimally impacted by the number of objects in the electrical engineering drawing. It sustains a rapid inference speed even when a single drawing contains numerous objects."}, {"title": "V. CONCLUSION", "content": "We propose a novel relation prediction method and present a dataset for relation detection in electrical engineering drawings. We apply this relation prediction method to the task of relation detection within the dataset, achieving high performance on the validation set through the processes of pretraining and finetuning the model. We conduct a series of experiments, including comparative analyses with existing methods and ablation studies on training strategies and model architectures. The experimental results indicate that our method achieves superior performance on this task."}]}