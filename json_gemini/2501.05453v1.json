{"title": "An Empirical Study of Autoregressive Pre-training from Videos", "authors": ["Jathushan Rajasegaran", "Ilija Radosavovic", "Rahul Ravishankar", "Yossi Gandelsman", "Christoph Feichtenhofer", "Jitendra Malik"], "abstract": "We empirically study autoregressive pre-training from videos. To perform our study, we construct a series of autoregressive video models, called Toto. We treat videos as sequences of visual tokens and train transformer models to autoregressively predict future tokens. Our models are pre-trained on a diverse dataset of videos and images comprising over 1 trillion visual tokens. We explore different architectural, training, and inference design choices. We evaluate the learned visual representations on a range of downstream tasks including image recognition, video classification, object tracking, and robotics. Our results demonstrate that, despite minimal inductive biases, autoregressive pre-training leads to competitive performance across all benchmarks. Finally, we find that scaling our video models results in similar scaling curves to those seen in language models, albeit with a different rate.", "sections": [{"title": "1 Introduction", "content": "In a paper published in 1951, Shannon, having just published the foundational papers of information theory, proposed a \"guessing game\" of next word prediction to estimate the entropy of English (Shannon, 1951). Nearly 70 years later, training a high-capacity transformer network (Vaswani et al., 2017) on this task, provided the generative pre-training backbone for Large Language Models (Radford et al., 2018; Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020).\nLess well known is the fact that in 1954, Fred Attneave (Attneave, 1954) proposed an analog of Shannon's task for images. To quote \"We may divide the picture into arbitrarily small elements which we \"transmit\" to a subject (S) in a cumulative sequence, having them guess at the color of each successive element until they are correct. This method of analysis resembles the scanning process used in television and facsimile systems and accomplishes the like purpose of transforming two spatial dimensions into a single sequence in time\".\nWhile Attneave was concerned with images, in the context of 2024, we have to note that the \"Big Visual Data\" is in videos. While there are concerns that most of the text available on the Internet has already been used by the language models, in video we just started on the journey of Big Data exploitation. Despite the successes of autoregressive language and image models, their effectiveness for video modeling remains underexplored.\nIn this paper, we empirically study autoregressive pre-training from videos. To perform our empirical study, we construct a family of autoregressive video models which we call Toto. We treat videos as sequences of visual tokens and train a causal transformer models on next-token prediction task. We use causal transformer model with LLaMa (Touvron et al., 2023) architecture. We use dVAE (Ramesh et al., 2021) to tokenize frames into discrete tokens. Treating videos as sequences of tokens enables us to jointly train on videos and images using a unified format. We construct a diverse dataset of videos and images comprising over 1 trillion visual tokens. Our models are first pre-trained on this data and then evaluated on downstream tasks. We extract visual representations using attention pooling from relevant layers of the model.\nWe evaluate our models on various downstream tasks from image and video recognition, video forecasting, semi-supervised tracking, object permanence and robotics tasks in both simulation and real-world. We consider different design choices such as tokenizers including dVAE (Ramesh et al., 2021), VQGAN (Rombach et al., 2022) and continuous patch-normalized (He et al., 2022) tokens. We also consider different architectures such as LLAMA (Touvron et al., 2023), GPT2 (Radford et al., 2019) and Mamaba Gu & Dao (2023). Finally we study the compute optimal scaling behaviors of autoregressive video models."}, {"title": "2 Related work", "content": "Representation Learning for Vision: Over the years self-supervised pre-training has proven to be effective in many areas including language, vision, and robotics. Wu et al. (2018) and SimCLR (Chen et al., 2020b) showed that instance discrimination training can learn strong discriminative features. MoCo (He et al., 2020) and DINO (Caron et al., 2021) showed the effectiveness of strong visual representations on various downstream tasks. Differently, BEiT (Bao et al., 2021) and MAE (He et al., 2022) used masked autoencoding for learning image representations. ST-MAE (Feichtenhofer et al., 2022)and VideoMAE (Wang et al., 2023a) extended this masked modeling approach to videos, by masking a large amount of tokens during pre-training and predict the masked tokens with a light-weight decoder.\nAutoregressive Modeling of Vision: Generative autoregressive pre-training learns to directly model the data distribution. In language models, generative pre-training has become the standard for training large models. For autoregressive pre-training in vision, rCNN (Ranzato et al., 2014), PixelCNN (Van den Oord et al., 2016) and PixelRNN (Van Den Oord et al., 2016) proposed generating pixels one by one using convolution and bidirectional LSTMs. With the introduction of the transformers (Vaswani et al., 2017), ImageTransformers (Parmar et al., 2018) showed generating pixels with causal local attention performs better than previous CNN and RNN-based methods. While all of these methods focused on the generation quality of the pixels, iGPT (Chen et al., 2020a) showed that generative pre-training is also a good way to learn strong visual representations for recognition tasks. Henighan et al. (2020) showed scaling behaviors of autoregressive image and video models. AIM (El-Nouby et al., 2024) on the other hand uses patch embedding rather than any pre-trained models for tokenization, however, it trains on Data Filtering Networks (Fang et al., 2023) with clip filtered data. Compared to these works, we do not use any supervision during our pre-training and utilizes image and videos jointly. VisionMamba (Zhu et al., 2024) also showed how to utilize sequence models with bidirectional state-space modeling for supervised vision tasks. Weissenborn et al. (2019) showed autoregressive video generation for promotable video generations.\nEvaluation of Vision Representations: Most video pre-training models are evaluated on semantic tasks like ImageNet (Deng et al., 2009) and Kinetics (Kay et al., 2017). Additionally to the standard evaluation, we evaluate our models on semi-supervised tracking task on DAVIS (Pont-Tuset et al., 2017), action forecasting on Ego4D (Grauman et al., 2022), object permanence on CATER (Girdhar & Ramanan, 2019) and on robot manipulation tasksn simulation (Xiao et al., 2022) and in the real world Radosavovic et al. (2023)."}, {"title": "3 Approach", "content": "We train a casual transformer model to predict the next patch tokens in images and videos. This is akin to the next token prediction in large language models. From the vast collection of images and videos, every patch is tokenized into a discrete token, and the transformer is trained to predict the next token, using raster scan ordering. We pre-train our models on over one trillion tokens. Finally, we evaluate the learned representations of these models on various downstream tasks including image classification, action classification, action anticipation, video tracking, object permanence, and robotic manipulation tasks. We also study the scaling behaviors of our models for compute optimal training."}, {"title": "3.1 Pre-training", "content": "Given a large collection of images and videos, we tokenize all of them into a 1D sequence using raster scan ordering. This produces a dataset of tokens, \\{x_1, x_2, x_3, ..., x_n\\} where j is the sample either from a video or an image and n is the number of tokens in an image or a video. We model the density p(x) as :\n$\\displaystyle p(x) = \\prod_{i=1}^{n} P(x_i | x_{1}, x_{2},...,x_{i}, 0)$\nHere, O is the model parameters, which can be optimized by minimizing the negative log-likelihood loss:\n$L_{pre-train} = E[ -log p(x^i)]$\nUsing this loss, we pre-train our models at different sizes on over one visual trillion tokens. These tokens are generated from images and video."}, {"title": "3.2 Architecture", "content": "Our model is a transformer (Vaswani et al., 2017) with causal attention. We apply recent advancements in language modeling such as pre-norm using RMSNorm (Zhang & Sennrich, 2019), SwiGLU activation (Shazeer, 2020), and ROPE positional embeddings (Su et al., 2024), following LLaMa (Touvron et al., 2023).\nFor a model with L layers, we define $H^l$ to be the intermediate representations after layer l,0 < l < L. The intermediate representations after layer l + 1, $H^{l+1}$, defined to be:\n$\\tilde{H}^{l+1} = H^l + MHSA(RMS-norm(H^l))$\n$H^{l+1} = \\tilde{H}^{l+1} + MLP(RMS-norm(\\tilde{H}^{l+1}))$,\nWhere MHSA is a multi-head self attention layer, MLP is a multi-layer perceptron with SwiGLU activations.\nWe train our models for the next token prediction task at different scales (base, large and 1b models). We train all these models with a batch size of 1M tokens. We use AdamW (Loshchilov & Hutter, 2017) with a maximum learning rate of 3e-4, and $\u03b2_1$ = 0.9, $\u03b2_2$ = 0.95. We decay the learning rate with a cosine schedule, after 2000 warm-up steps (Touvron et al., 2023)."}, {"title": "3.3 Dataset", "content": "To train our model, we compile a large dataset from a number of different sources. Together these datasets contain over 100,000 hours of video data and about 2.5 trillion visual tokens. During training, each mini-batch is sampled at different ratios of datasets. Each batch approximately contains 20% of ImageNet images, 10% of Ego4D videos, 10% of Kinetics videos, and 60% of HowTo100m videos. Our full training utilized about 1 trillion tokens."}, {"title": "3.4 Tokenization", "content": "We use dVAE tokenizer with a vocabulary of 8k tokens, from Dall-E (Ramesh et al., 2021) as our tokenizer. Using an image-based tokenizer allows training on both images and videos and testing on respective downstream tasks. While VQGAN (Esser et al., 2020) tokenizers provide sharper images, these models were trained with perceptual loss (Larsen et al., 2016; Johnson et al., 2016), thus indirectly ingesting ImageNet label information via VGG-net (Simonyan & Zisserman, 2014).\nAll raw pixel frames or images are tokenized into 256 discrete tokens. We take a video and resize it such that its shortest size is R pixels, and then take a random crop of R\u00d7R\u00d7 T, and sample every 4 frames where T is the number of frames. We use dVAE (Ramesh et al., 2021) with the vocabulary of 8k entries to tokenize every frame independently. For dVAE we set R = 128, to get 16 \u00d7 16 discrete tokens. Once every frame is mapped into a set of discrete tokens we have T \u00d7 256 tokens per each video. We pre-train all the models with T = 16, thus all the models are per-trained for a context length of 4096 tokens.\nWhen training with images and videos, 16 video frames are sampled to create 4k tokens. For images, we randomly sample 16 images and create a sequence of 16 image frames to generate 4k tokens. Finally, we add start and end tokens for each sequence, for videos we use [1] as the start token, and for images we use [3] as the start token, and all sequences have an end token of [2]."}, {"title": "3.5 Downstream Transfer", "content": "The idea of large pre-trained models is that they were trained at a large compute scale, and then these models can be easily used for various downstream tasks without requiring task-specific design or lots of computing for transfer. The learned representations are general enough to transfer to various tasks. We evaluate our models on the intermediate features with linear and attention probing (Lee et al., 2019).\nFor linear probing the model, we apply global average pooling (Lin et al., 2013) over the tokens from different layers to get the intermediate representation. We train a linear layer on top of this representation on the downstream task. MAE (He et al., 2022) or DINO (Caron et al., 2021) have a uniform structure when it comes to which token attends to which tokens, however in autoregressive sequence modeling later tokens attent to more tokens than the tokens at the beginning. Due to this skewed nature, equally weighting all the tokens affects the downstream performance. Attention pooling is an alternative to average pooling that allows to dynamically weight the tokens, ideally giving more weight to tokens that see more tokens. This requires learning Wk and W\u2082 matrices and a query token q. The query token cross-attends to the intermediate tokens and combines them into a single vector. While this function is not linear anymore, it has been shown to learn better representations in recent works (El-Nouby et al., 2024)."}, {"title": "4 Experiments", "content": "We evaluate our pre-trained models on various downstream tasks such as ImageNet classification, Kinetics action recognition, Ego4D action anticipation, Semi-Supervised tracking, and Robotic manipulation tasks. First, we discuss various design choices for pre-training and evaluation strategies for our method. All the models for studying the design choices are large models trained for 400 epochs on the ImageNet-1k dataset."}, {"title": "4.1 Design Choices", "content": "Tokenizer: The are various options available for tokenizing an image or a video. We could use discrete tokenizers such as dVAE, and VQGAN, or simple patch-based continuous tokenization. To study the behavior of various tokenizers we pre-train a Toto-large model on ImageNet for 400 epochs. Using linear probing at an optimal intermediate layer, we evaluate the accuracy of the models on ImageNet classification task."}, {"title": "4.2 Image Recognition", "content": "To measure the representation quality of our pre-trained models, we evaluate our models on ImageNet-1k (Deng et al., 2009) classification. We apply a probe at each layer of the model, with attention pooling, and choose the optimal layer with the highest classification accuracy. We fine-tune the pre-trained models further by applying self-supervised next token prediction loss in Eq 2, together with cross-entropy loss applied for probing layers (with stop-gradients). We train the probing layers for 90 epochs, with a learning rate of 6e-5. We also use layer decay of 0.9 to reduce the learning rate at the early layers of the model. During this stage, all the models are fine tuned with 32 \u00d7 32 token resolution, on the self-supervised loss, and increase the base value of the ROPE (Su et al., 2024) embeddings from 10,000 to 50,000 support larger resolution."}, {"title": "4.3 Action Recognition", "content": "We use Kinetics-400 (K400) (Kay et al., 2017) for evaluating our models on action recognition tasks. Similar to ImageNet evaluation, we apply a probe at each layer of the model, with attention pooling, and choose the optimal layer with the highest action classification accuracy. We also fine-tune the pre-trained models on a self-supervised next-patch prediction task while training the probing layers with a classification loss. All our video models are trained with 16 frames, thus with a context length of 4096 tokens per video. When evaluating videos, we follow the protocol in SlowFast (Feichtenhofer et al., 2019). Unlike ImageNet where we evaluate the models at 256x256 resolution, on videos we only evaluate our models at 128x128 resolution, to keep the number of tokens in a similar budget."}, {"title": "4.4 Action Forecasting", "content": "While the Kinetics dataset captures internet-style exocentric videos, Ego4D (Grauman et al., 2022) videos capture day-to-day life egocentric videos. A general vision model should be able to reason about both exo and ego-centric videos. Task-wise, Kinetics requires the model to reason about the action using full context (e.g. the model has seen the action), while the Ego4D short-term action anticipation v1 task requires models to predict future actions from past context. We use our models as the backbone for the pyramid network used in StillFast (Ragusa et al., 2023) extract tokens at 5 layers and fuse them with the pyramid network. We fully fine-tuned our model with self-supervised next-patch loss along with task-related losses, and we observed having self-supervision loss improves overall performance."}, {"title": "4.5 Video Tracking", "content": "We study our pre-trained models on label propagation using the protocols in (Jabri et al., 2020) on DAVIS dataset (Pont-Tuset et al., 2017). Compared to previous tasks such as classification, and forecasting, this evaluation does not require finetuning or probing of the features. Following Jabri et al. (2020), we use the features from the last n frames to find the nearest neighbor patch in the current frame, and then propagate the masks from the previous frames to the current frame.Comparison with Dino (Caron et al., 2021) and MAE (He et al., 2022) is shown in Table 10 and qualitative results are shown in Figure 5."}, {"title": "4.6 Robotics", "content": "In this section, we study the effectiveness of our pre-trained representations for robotic manipulation. We consider tasks in both simulation and in the real world. Real world experiments needs to run at real time, there for we only use Toto-base models, in both setting. Despite being a small model, Toto-base can achieve better performance in simulation and on-par performance to state-of-the-art robot models in real world experiments.\nSimulation Experiments: Following the protocols in MVP (Xiao et al., 2022), we use our visual pre-trained models to embed pixel observations. The model is frozen and we only take tokens at an intermediate layer, apply average pooling, and learn the linear layer on top to embed pixel observations. These observations are used to train DAgger policies for 4 different tasks: Franka-pick 6a, Kuka-pick 6b, Franka-cabinet 6c, and Kuka-cabinet tasks 6d.  Compared to the MVP baseline, our model was able to learn these tasks faster with better sample efficiency across robots and tasks. For fair comparisons, we use the best MAE model from MVP (Radosavovic et al., 2022) which is trained on ImageNet (Deng et al., 2009), Ego4D (Grauman et al., 2022) and 100DOH (Shan et al., 2020) datasets.\nReal-world Experiments: Next, we evaluate our pre-trained representations in the real world. We follow the setup from (Radosavovic et al., 2022). We extract vision features using a pre-trained vision encoder and train a controller on top of frozen representations using behavior cloning. Specifically, we consider a cube pick-ing tasks using a 7 DoF Franka robot, shown in  We use the demonstrations provided by (Radosavovic et al., 2023)."}, {"title": "4.7 Object Permanence", "content": "To quantitatively measure the performance of how well the model understands object permanence, we evaluate our models on CATER localization task (Girdhar & Ramanan, 2019). Here, a ball is moving in the scene, and the task is to find its location in the 6 by 6 grid. We fine tune our Toto-large model on this task at temporal resolutions 16, and 32 frames. In both cases, our pre-trained models were better at localizing the target compared to models trained specifically for this task, such as V3D (Zhang, 2022), TFC-V3D (Zhang, 2022)."}, {"title": "4.8 Probing Across Layers", "content": "As shown in Figure 4 for the ImageNet classification task, different layers of the model contribute to the task differently for the image classification task; this behavior is also observed in iGPT (Chen et al., 2020a). To study this behavior across multiple tasks, we train probing layers for action recognition, object tracking, and robot manipulation.  It shows that action recognition follows a similar trend to ImageNet classification, having peak performance at the middle of the model stacks. While Object tracking also shares a similar trend with image classification and action recognition, object manipulation shows an interesting trend of the last layers performing well as middle layers from picking objects. Compared to the first three tasks, robot manipulation has a generative nature as a task and can benefit from generative pre-training. In encoder models (Caron et al., 2021) or encoder-decoder models (He et al., 2022; Bao et al., 2021) the last layer of the encoder has more semantic features. This may suggest that, in decoder-only model, first half of the model starts to behave like an encoder, and compress the information, and then rest of the model, projects the compressed semantic features back to input space."}, {"title": "4.9 Compute Optimal Scaling", "content": "We study the scaling behaviors of Toto using \u03bc-Parameterization (Yang et al., 2022). First we train various models, al-a6, with linearly increasing hidden size and number of layers (Table 15). All models use the VQGAN tokenizer (Esser et al., 2020). We then optimize the learning rate for these models, with \u03bc-Parameterization (Yang et al., 2022). Figure 11 shows optimal learning rate of 2-7 for all of the model widths. Once we find the optimal learning rate, we train al-a6 models on our data mixture (Table 2). Figure 9 shows the loss vs training compute of Toto models. This shows a clear power law relationship between the compute and validation loss. Based on these experiments Toto shows a power law of L(C) = 7.32 C-0.0378. For comparison, the GPT-3 power law relationship (Brown, 2020) is L(C) = 2.57\u00b7C-0.048. While these are not comparable directly, the scaling coefficients indicate how much change in loss to expect for extra added compute. This suggests that the visual next token prediction models, such as Toto, scale but at a slower rate than language models."}, {"title": "5 Limitations", "content": "Our study suggests several important limitations and opportunities for future work. A significant limitation stems from the use of internet videos, which, unlike carefully curated datasets, introduces challenges related to data quality and diversity. This variance in data quality can impact model performance, especially when compared to models trained on more curated datasets. Another limitation is the use of tokenizer, this makes the learning not end-to-end, and the representation and generation quality is bounded by the quality of the tokenizer, and with quantized vectors, the quality is very much limited, this needs further explorations to build a universal visual tokenizer. Another fundamental limitation is training on videos for next token prediction task. The added redundancy in video frames, can hurt quality of the learned representations. Additionally, our exploration of various design choices are based on ImageNet classification. While it does transfer to most of the tasks we considered in this paper, it may not be the optimal configuration for many other tasks. Furthermore, we have not yet fully assessed our method's effectiveness in dealing with dense prediction tasks, fine-grained recognition, or comprehending complex temporal dynamics over extended time frames. These areas represent key opportunities for further research, aiming to broaden the fruitfulness of autoregressive pre-trained models."}, {"title": "6 Conclusion", "content": "We empirically studied autoregressive pre-training from images and videos. We curated a large video dataset and conducted a large-scale evaluation across a range of diverse tasks, including image recognition, video classification, video forecasting, object tracking, object permanence, and robotic manipulation. We performed extensive ablation studies to understand different design choices and compared auto regressive pre-training from videos to strong baselines across different tasks. We found that, despite minimal inductive biases, our approach achieves competitive performance across all tasks. Finally, we studied the scaling behavior of visual next token prediction models, and showed it scales with compute, but at a slower rate than text based next token prediction models."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Video Tokens for Pre-Training", "content": "The next patch prediction for visual pre-training is equivalent to the next token prediction in large language models. However, most languages have a clear sequential nature, therefore there is a clear definition for the next word. This also makes the next word prediction task relatively harder, since the model requires learning to extrapolate the data. On the other hand, images and videos, especially over the spatial dimensions lack a sequential nature. We follow the previous works (Chen et al., 2020a; Van Den Oord et al., 2016) to make the images and videos into a 1D sequence by scanning the patches in raster order. While this ordering allows for example to learn to predict the bottom half of the image from the top part of the image, in many places, the tokens can be predicted by interpolating rather than extrapolating.\nOn the time axis, yes, there is a clear sequential nature, however, video frames compared to text tokens are more redundant, making the next frame prediction task much easier. Figure 10 shows average validation loss over 4096 token, in kinetics 400 dataset (Kay et al., 2017), on Toto-large model. This shows there is high loss of the first frame, but the subsequent frames have relatively lower loss compared to the first frame. This is because, even with reasonably lower sampling rate, frames following the first frame has some redundancy, and hinders the learning, since these tokens are relatively easy to predict. This also could be attributed by emergence of induction heads (Olsson et al., 2022). While we focused on learning from unfiltered internet scale video with minimal inductive bias, to learn efficiently from videos, need further research in this direction."}, {"title": "A.2 Prefix attention", "content": "During fine-tuning, we experimented with causal and full attention. On ImageNet, our base model achieved full attn: 82.6% vs causal attn: 82.2%. Even though our models are not pre-trained with prefix attention, still able to utilize full attn at fine-tuning. This is an unrealized benefit of training with videos, (a middle token in say, 8th frame won't see the rest half of the 8th frame, but have seen all the tokens from 7th frame, which are similar because of video, hence approximating full attention at pre-training)"}, {"title": "A.3 Full fine-tuning", "content": "We fine-tuned our models on ImageNet, and performance is close to SOTA, compared to linear probing (where we only use causal attention). But during the fine-tuning, we use full attention."}, {"title": "A.4 iGPT vs Toto on ImagenNet", "content": "Table 7 shows ImageNet evaluation performance. However, iGPT (Chen et al., 2020a) models are evaluated only using linear probing. To have a fair comparison, between iGPT and Toto, we reevaluated our models using linear probing. Both models have causal attention and are trained on auto-regressive objectives. On the same model sizes, about 1 billion parameters, our achieve 66.2% while the similar iGPT model's ImageNet performance is 65.2%. This fair evaluation suggests the modifications made on Toto have clear benefits over iGPT."}, {"title": "\u0391.5 \u03bc-Parameterization", "content": "To study the scaling behaviours of Toto using \u03bc-Parameterization (Yang et al., 2022). First we train various models al-a6 (in Table 15), with hidden sizes (64-1536) and number of layers (12-48), increasing linearly and we used VQGAN tokenizer (Esser et al., 2020). Then we tune the learning rate for these models, with fixed depth using \u00b5-Parameterization (Yang et al., 2022). Figure 11 shows optimal learning rate of 2-7 for all the model widths. Once we find the optimal learning rate, we train al-a6 models on the mixture of image and video data, as mentioned in Table 2."}, {"title": "A.6 n-gram distribution", "content": "In this section, we compare the 2-gram and 3-gram distribution of dVAE (Ramesh et al., 2021), VQGAN (Esser et al., 2020) image tokeizers. We compute 2-gram and 3-gram distributions on the discrete tokens of 10000 ImageNet validation images. Figure 12 and Figure 13 show the distributions of these tokenizers respectively. On 2-gram distribution, dVAE (Ramesh et al., 2021) has more discrete combination of tokens compared to both VQGAN-1K and VQGAN-16k tokenizers."}, {"title": "A.7 Attention probing variants on K400", "content": "We also evaluate our models and baselines on the Kinetics 400 dataset using a variant of attention probing. In the main paper, we use attention probing, with only learning Wk, W matrices, and a single learnable query vector. We also test with cross attention with MLP layers as the attention classifier, to give more capacity to the learnable head."}, {"title": "A.8 Generation samples", "content": "long video generation: we can generate up to 64 frames, first raw: periodic motion, second raw: object permanence (light stand)."}, {"title": "A.9 Additional Layer-wise Probing Results", "content": "We probe the multiple variants of our models at each layer for the best ImageNet performance. First, we test the models on linear probing, on both sizes of 128 and 256 resolution. Figure 14 presents the probing curves of the models trained with attention probing at 128 resolution. Across all models, the performance has a similar behavior to the pre-trained models, with peak performance around the middle of the depth of the model."}]}