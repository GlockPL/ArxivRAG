{"title": "DEEP GENERATIVE MODELS AS AN ADVERSARIAL ATTACK STRATEGY\nFOR TABULAR MACHINE LEARNING", "authors": ["SALIJONA DYRMISHI", "MIHAELA C\u0102T\u0102LINA STOIAN", "ELEONORA GIUNCHIGLIA", "MAXIME CORDY"], "abstract": "Deep Generative Models (DGMs) have found application in\ncomputer vision for generating adversarial examples to test the\nrobustness of machine learning (ML) systems. Extending these\nadversarial techniques to tabular ML presents unique challenges\ndue to the distinct nature of tabular data and the necessity to pre-\nserve domain constraints in adversarial examples. In this paper,\nwe adapt four popular tabular DGMs into adversarial DGMs (Ad-\nvDGMs) and evaluate their effectiveness in generating realistic ad-\nversarial examples that conform to domain constraints.", "sections": [{"title": "1. Introduction", "content": "Deep Generative Models (DGMs) generate synthetic data af-\nter learning the probability distribution of their training data.\nThey are most commonly used to augment datasets for bet-\nter predictive performance of machine learning (ML) models\n[1], to promote fairness [2], ensure privacy [3] etc. Several\nworks in computer vision have repurposed DGMs as a tool to\ngenerate adversarial examples for ML models. Such adversar-\nial examples pose a significant security threat by minimally\naltering original inputs and forcing the models to wrongfully\nchange their predictions. In this scenario, adversarial DGMs\n(AdvDGMs) take as an input an original example and output\nan adversarial one while trying to minimize the adversarial and\nperturbation loss, in addition to their original loss functions.\nAdvDGMs promise shorter generation times compared to to-\nday's popular iterative adversarial attacks, which is beneficial\nand important for adversarial hardening [4].\nBeyond computer vision, extending AdvDGMs to test and\nimprove the robustness of tabular ML models against adver-\nsarial examples is challenging due to the unique characteristics\nof tabular data. These models must account for diverse fea-\nture types and preprocessing while ensuring the generated ad-\nversarial examples adhere to domain-specific constraints. For\ninstance, in a credit scoring system, the \"average transaction\namount\" must not exceed the \u201cmaximum transaction amount.\u201d\nViolating such constraints results in unrealistic examples that\ndo not map to real-world transaction history. Current tabular\nDGMs often fail in this regard, producing up to 100% unrealis-\ntic examples [5]. Few efforts have been made to adapt DGMS\ninto AdvDGMs for tabular data [6, 7, 8, 9], however, these at-\ntempts often focus on a single use case, use generic models not\ntailored for tabular data, and handle the realism of their out-\nputs by modifying only independent features, thus limiting the\nadversarial example search space.\nIn this paper, we convert four popular tabular DGMs into\nAdvDGMs and evaluate their potential to generate successful\nadversarial examples that fulfill three objectives: satisfy con-\nstraints, change model prediction, and maintain minimal dis-\ntance from the original input. To boost the performance of tab-\nular AdvDGMs, we extend them with a constraint repair layer\n[5], ensuring that the outputs always satisfy domain constraints.\nAdding the constraint repair layer should not significantly im-\npact the efficiency of AdvCDGMs compared to iterative ad-\nversarial techniques. Hence, we investigate the impact of CL\non runtime. Finally, we compare our AdvDGMs' performance\nwith three attacks from literature optimized for domain con-\nstraints. The source code, the data and the models are publicly\navailable."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1 DGMs for Tabular Data", "content": "Several approaches based on DGMs have been specifically\ndesigned to address particular challenges in generating tabular\ndata such as mixed types of features, and imbalanced categori-\ncal data. Notable among these are GAN-based approaches like\nTableGAN [10], CTGAN [11], OCT-GAN [12], and IT-GAN\n[3]. These methods leverage the power of GANs to model\nthe underlying data distribution and generate synthetic samples\nthat closely resemble real-world tabular data. Following pri-\nvacy concerns, two approaches, i.e., DPGAN [13] and PATE-\nGAN [14], incorporate differential privacy techniques to ensure\nthat the generated synthetic data does not reveal sensitive infor-\nmation about the individuals in the original dataset. Alterna-\ntivly to GANs, Xu et al. [11] proposed TVAE as a variation of\nthe standard Variational AutoEncoder, while TabDDPM [15]\nand STaSy [16] were proposed following the achievements of\nscore-based models. Finally, Liu et al. [17] proposed GOG-\nGLE, a model that uses graph learning to infer relational struc-\nture from the data."}, {"title": "2.2 DGMs as an attack strategy", "content": "In pioneering work by Xiao et al. [4], Generative Adversar-\nial Networks (GANs) were employed to generate adversarial\nexamples for images. Their approach utilized the original ex-\namples as input to the generator, with the output representing\nthe perturbation added to the original image. The generator\nwas trained using a combination of adversarial loss, obtained\nby evaluating the perturbed instances on the target model, and\nGAN loss based on the discriminator's predictions. Subsequent\nresearch has aimed at refining the architectures and methodolo-\ngies for generating adversarial examples with DGMs. For in-\nstance, Jandial et al. [18] utilized the latent representations of\nimages as input to the generator, considering them more prone\nto adversarial perturbations. Meanwhile, Bai et al.[19] intro-\nduced a novel attacker in the loop to train the discriminator ad-\nversarially. In another approach, Ding et al.[20] explored VAE-\nGAN and concatenated original images with noise vectors as\ninput to the generator. Moreover, Song et al.[21] focused on\nGAN-based adversarial examples not restricted by Lp norms.\nDGM-based adversarial methods have been applied to mal-\nware and intrusion detection tasks [6, 7, 8, 9] using similar ar-\nchitectures as those for images. Challenges arise from ensuring\nthe realism of generated examples in these new domains, with\nresearchers addressing this by preserving non-functional fea-\ntures, thus reducing the search space for adversarial examples."}, {"title": "3 Problem Statement", "content": ""}, {"title": "3.1 Deep Generative Models - DGM", "content": "In standard generative modeling, we aim to learn parameters\nfor a generative model $(p_0)$ that approximates an unknown dis-\ntribution $p_x$ based on a training dataset $D$ of $N$ samples drawn\nfrom $p_x$. The DGM model can then output synthetic samples\nthat closely follow the training data distribution. To exemplify,\nwe describe the Generative Adversarial Network (GAN) as a\ncommon model architecture in the literature for synthesizing\ntabular data. A GAN consists of two neural networks: a genera-\ntor $G$ and a discriminator $D$. The generator takes random noise\nas input and aims to generate synthetic data samples that are\nindistinguishable from real data, while the discriminator aims\nto distinguish between real and fake samples. Both $G$ and $D$\nare trained iteratively in a min-max optimization task:\n$\\min_G \\max_D V(D, G) = E_{x \\sim p_x} [log D(x)]$\n$+E_{z \\sim p_z} [log(1 - D(G(z)))]$\nwhere $p_z$ is the noise distribution and $V$ is the function that\nthe discriminator $D$ wants to maximize and generator $G$ wants\nto minimize."}, {"title": "3.2 Adversarial Deep Generative Models - AdvDGM", "content": "For AdvDGM, the input to the generator G is no longer the\nnoise vector but the initial point x, for which we aim to gener-\nate the adversarial example $x' = x + \\delta$. We introduce a target\nclassifier h for which we want to test the robustness using the\nexamples generated by a DGM. Additionally, let y represent\nthe target class label, and x denote the input sample. The ad-\nversarial loss $L_{adv}$ is computed as:\n$L_{adv} = \\max_{\\| \\delta \\| \\le \\epsilon} [l(h(p_0(x + \\delta)), y)]$\nwhere $\\delta$ denotes the perturbation added to the input sample $x$,\nconstrained by its magnitude $\\epsilon$ such that $\\|\\delta \\| < \\epsilon$, and l denotes\nthe loss function used for classification.\nMoreover, the perturbation loss $L_{pert}$ measures the magni-\ntude of modifications required to transform a legitimate sample\ninto an adversarial one. It is calculated as:\n$L_{pert} = \\|\\delta \\|$\nwhere $\\delta$ is the perturbation added to the input sample.\nThe total loss of the AdvDGM model combines the initial\nloss of DGMs altogether, with adversarial and perturbation loss\nas follows:"}, {"title": "3.3 Domain constraints", "content": "The sample space of $p_x$ provides some knowledge on the\nacceptable values for each feature within its range but also in\nrelationship with other features. Let \\Pi be a set of constraints\nexpressing this background knowledge. We assume each con-\nstraint in \\Pi to be a linear inequality involving variables $x_k$ cor-\nresponding to features of the dataset. These inequalities take\nthe form:\n$\\sum_k w_k x_k + b \\geq 0$\nwhere $w_k$ are coefficients, b is a constant, and $\\geq$ denotes\neither greater than or equal to. A sample generated by a DGM\nassigns values to these variables. If the inequality is true for\nthese assigned values, then the sample satisfies the constraints."}, {"title": "4 Constrained Tabular Adversarial Deep Genera-\ntive Models", "content": "To ensure the adversarial examples generated by AdvDGM\ncomply with constraints \\Pi, we extend them to include the con-\nstraint repair layer CL from Stoian et al. [22]. The CL layer\ntakes as input i) domain constraints expressed as linear inequal-\nities ii) a feature repair ordering, and iii) an original example.\nThe example goes through an evaluation check for constraint\nsatisfaction, and if any constraints are violated, the example\nwill be minimally modified so that it is guaranteed that the re-\nsulting example will respect the constraints. This differentiable\nlayer can be integrated during training, noted as C-AdvDGM,\nor used only during sampling, noted as P-AdvDGM.\nFigure 1 gives an overview of C-AdvDGMs for GANs, how-\never the same can be applied to any tabular DGM. The gener-\nator takes as input the initial example x transformed through\na mapping function $f^{-1}$ (i.e min-max scaling) and outputs $\\tilde x$,\nwhich is transformed back into the original data space before\nundergoing constraint evaluation and repair via the constrained\nlayer. The resulting constrained example $x'$ is then transformed\ninto the space used by the GAN before being fed into the dis-\ncriminator to calculate $L_{GAN}$. Additionally, $\\tilde x'$ is transformed"}, {"title": "5 Experimental Settings", "content": "Datasets. We used four real-world datasets (URL, WiDS,\nHeloc, FSP) with domain constraints identified in literature [5].\nTarget models. We used three tabular neural network classi-\nfiers (TorchRLN, VIME, TabTransformer) from [23] for which\nwe performed a hyperparameter search on each dataset to ob-\ntain the best parameters.\nTabular DGMs. Our experimentation involved four distinct\ntabular DGMs: WGAN [24], TableGAN [10], CTGAN [11],\nTVAE [11]. Each model was updated according to the steps in\n3.2 to obtain AdvWGAN, AdvTableGAN, AdvCTGAN, Ad-\nVTVAE. The modifications included as well discarding con-\nditional loss for CTGAN, and the label classifier for Table-\nGAN. Then CL was added to obtain the P-AdvDGMs and C-\nAdvDGMs versions of these models. The layer was extended to\nsupport, in addition to linear equalities, constraints of type \u201cif -\nelse\u201d as conjunctions. We performed a hyperparameter search\nindependently for AdvDGMs and C-AdvDGMs, exploring val-\nues of $\\alpha$ and $\\beta$ in the range of [1, 100] and learning rate equal\nto {0.001, 0.005, 0.01, 0.05}. We used the random variable or-\ndering as an input to the constrained layer CL.\nSOTA attacks. To compare the performance of our tabular\nAdvDGMs We used two gradient attacks CPGD and CAPGD\nand a genetic algorithm attack MOEVA [25, 23].\nMetrics. We measured Attack Success Rate (ASR) as the\nratio of the adversarial examples that have $\\epsilon < 0.05$ ($L_2$ norm),"}, {"title": "6 Results", "content": ""}, {"title": "6.1 Adversarial generation capability", "content": "Table 1 shows the ASR of our AdvDGMs and their con-\nstrained counterparts for the four datasets under study.\nThe results demonstrate that for all target models, only Ad-\nvWGAN and its constrained counterparts are successful in sig-\nnificantly increasing the error rate of the model by reaching\nan ASR of up to 95% for Heloc dataset with TabTransformer\nmodel. All the AdvDGMs and their constrained counterparts\nare unsuccessful on WiDS dataset, having an ASR lower than\nthe error rate of the models on original non-adversarial data.\nRegarding the addition of the constrained layer CL, the re-\nsults show that it is beneficial in increasing the ASR of the at-\ntacks. Out of 48 cases, P-AdvDGMs have higher ASR than\nAdvDGMs 38 times with a maximum difference of 62% (P-\nAdvGAN on Heloc and TorchRLN). Similarly C-AdvDGMS\nhave higher ASR 37 times with a maximum difference of 62%\n(C-AdvGAN on Heloc and TabTransformer)."}, {"title": "6.2 Constrained Layer impact on runtime", "content": "Train time: The results in Table 2 show that the constrained\nmodels require at most 4.7 more time to train compared to the\nunconstrained model (C-AdvWGAN for WiDS). On the other\nhand, for some models, the constrained and unconstrained ver-\nsions take the same time to train as in the case of TableGAN.\nSampling time:\nFrom the results in Table 2, we observe that C-AdvDGMs\nexhibit, at most, a 0.12-second increase in runtime compared\nto their unconstrained counterparts (notably, C-AdvTableGAN\nfor the WiDS dataset). On average, the runtime is 0.02 sec-"}, {"title": "6.3 Comparison with SOTA", "content": "Table 3 demonstrates that our best performing attack *-\nAdvWGAN ranks as the second-best attack for Heloc and FSP\ndatasets on all three target models. On these dataset, the gra-\ndient attacks CPGD and CAPGD perform poorly with a maxi-\nmum increase of the model's error rate of 11%. The genetic at-\ntack MOEVA has the highest success rate in 9 out of 12 cases."}, {"title": "7 Conclusion", "content": "DGMs are efficient adversarial attack methods in computer\nvision, but adapting them for tabular data poses challenges due\nto the properties of tabular data and current tabular DGMS\nnot respecting domain constraints. In this paper, we adapted\nfour tabular DGMs into AdvDGMs and extended them into\nC(P)-AdvDGMs by adding a constraint repair layer. Notably,\nonly AdvWGAN consistently achieved high success rates in\nboth unconstrained and constrained versions, which is surpris-\ning given that WGAN is older and not always the most per-\nformant in dataset augmentation literature. Our experiments\nshowed that including the constraint layer during training or\nsampling improves the success rate of AdvDGMs, highlighting\nthe importance of compliance with background knowledge for\nadversarial attacks. Further investigation is needed to under-\nstand why the constraint layer is more successful in training vs.\nsampling."}]}