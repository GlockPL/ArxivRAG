{"title": "NatureLM: Deciphering the Language of Nature for Scientific Discovery", "authors": ["NatureLM team"], "abstract": "Foundation models have revolutionized natural language processing and artificial intelligence, significantly enhancing how machines comprehend and generate human languages. Inspired by the success of these foundation models, researchers have developed foundation models for individual scientific domains, including small molecules, materials, proteins, DNA, and RNA. However, these models are typically trained in isolation, lacking the ability to integrate across different scientific domains. Recognizing that entities within these domains can all be represented as sequences, which together form the \u201clanguage of nature\", we introduce Nature Language Model (briefly, NatureLM), a sequence-based science foundation model designed for scientific discovery. Pre-trained with data from multiple scientific domains, NatureLM offers a unified, versatile model that enables various applications including: (i) generating and optimizing small molecules, proteins, RNA, and materials using text instructions; (ii) cross-domain generation/design, such as protein-to-molecule and protein-to-RNA generation; and (iii) achieving state-of-the-art performance in tasks like SMILES-to-IUPAC translation and retrosynthesis on USPTO-50k. NatureLM offers a promising generalist approach for various scientific tasks, including drug discovery (hit generation/optimization, ADMET optimization, synthesis), novel material design, and the development of therapeutic proteins or nucleotides. We have developed NatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion parameters) and observed a clear improvement in performance as the model size increases.", "sections": [{"title": "1 Introduction", "content": "Foundation models, including the GPT [1-3], Gemini [4, 5], Phi [6, 7], Llama [8], Mistral [9, 10], DeepSeek [11, 12], and Qwen [13, 14], represent a transformative advancement in artificial intelligence. These models, trained on massive web-scale datasets, are designed to serve as general-purpose tools, capable of handling a wide range of tasks with a single architecture. The most notable capabilities of foundation models include their abilities to perform tasks without fine-tuning, a phenomenon known as zero-shot learning, and their few-shot learning abilities which allow them to adapt to new tasks by drawing inferences from just a few examples.\nDespite their success in general-purpose tasks, early investigations [15] highlight significant room for improvement for scientific tasks involving small molecules, proteins, DNA, RNA, or materials. In particular, foundation models struggle with precise quantitative predictions (e.g., ligand-protein binding affinity, protein-protein interactions, DNA properties) [15] and rational design and optimization of compounds, proteins, and materials. Moreover, ensuring the scientific accuracy of outputs from these models remains as a grand challenge.\nRecently, there has been a concerted effort to develop large-scale foundation models specifically tailored for scientific tasks. These approaches can be broadly divided into four categories:\n1. Domain-specific foundation models. These models, such as ProGen [16] and ESM3 [17] for proteins, DNABERT [18] and Evo [19] for DNA sequences,\nscGPT [20] for single-cell data, and chemical language models [21, 22] for small molecules, are trained specifically on token sequence representations for individual scientific domains.\n2. Fine-tuned general-purpose models. This approach adapts well-trained large\nlanguage models for specific scientific domains, as demonstrated by Tx-LLM [23] for small molecules and ProLLAMA [24] for proteins.\n3. Scientific data enhanced large language models (LLMs). This approach [21, 25, 26] trains LLMs from scratch mainly with text data and a small portion of scientific data.\n4. Integration of specific scientific modules. In this approach, external modules, such as pre-trained molecular or protein encoders, are integrated into general-purpose models (e.g., Llama) via lightweight adapters [27, 28].\nWhile these approaches have made considerable progress, they do face notable limitations. Domain-specific models (approach #1) are restricted to their respective fields, limiting their ability to capture interdisciplinary insights for cross-domain applications. Fine-tuning general-purpose models (approach #2) and scientific data enhanced LLMs (approach #3) show promise but are often constrained by small-scale scientific datasets, e.g., around 90% text data and only 10% scientific data in [26], which hinders the models' capacity to capture the complexity of scientific tasks. The integration of external modules (approach #4) faces challenges in aligning inputs effectively with large\nlanguage models, and most implementations opt for limited fine-tuning with small datasets, leaving the core models largely unchanged.\nThe existence of these limitations emphasizes the necessity for a science foundation model, to fulfill the sophisticated demands of scientific research. A model of this kind must not only be highly proficient in producing precise scientific predictions but also adept at designing and optimizing scientific entities conditioned on context information. A perfect science foundation model ought to have the capacity to handle a diverse range of inputs. These inputs can span from literature text, to scientific sequence data such as protein or DNA sequences, and further to structural data like 3D protein/DNA structures and their dynamic behaviors. In the present study, our focus is on sequence-based data for representing biological, chemical, material systems, and natural language:\n\u2022 DNA, RNA, and proteins, which are often referred to as the \"language of nature\", are intrinsically represented by sequences. Additionally, many other scientific entities like small molecules and materials can be effectively represented as sequences through well-established domain-specific techniques [29].\n\u2022 Sequence data is highly compatible with the current mainstream large language models (LLMs). Through the continuous pre-training of LLMs, we are able to utilize the scientific knowledge embedded in these general-purpose LLMs to tackle complex scientific challenges.\n\u2022 Sequential data provides remarkable flexibility when combined with autoregressive paradigms [30, 31]. These paradigms, which are extensively employed in generative models, are capable of effectively modeling the highly complex distributions of any scientific object that can be presented in the form of a sequence.\nWe introduce Nature Language Model (briefly, NatureLM), a sequence-based science foundation model tailored for scientific tasks. NatureLM is designed to handle the complexity of small molecules, proteins, DNA, RNA, materials, and their associated textual information. NatureLM follows the Transformer decoder architecture and is trained on a corpus of 143 billion tokens collected from various scientific domains. Our experiments demonstrate that NatureLM significantly outperforms general-purpose foundation models for scientific tasks. Specifically, NatureLM excels in tasks such as:\n1. Following textual instructions to generate/design and optimize scientific\nmolecular entities.\n2. Performing cross-domain generation tasks, such as designing small\nmolecules or RNA binders for proteins as well as designing guide RNA\nsequences given a DNA template for CRISPR systems.\n3. Achieving state-of-the-art performance in retrosynthesis and SMILES-to-\nIUPAC translation.\nIn summary, our development of the NatureLM represents a significant step towards building a generalist model across multiple scientific domains. By harnessing the capabilities of text-based instructions, NatureLM serves as a powerful tool for scientific discovery, enabling cross-domain generation and optimization in areas such as drug discovery, materials science, and the development of therapeutic proteins and nucleotides. Ideally, a foundation model should support a broad range of tasks while demonstrating strong zero-shot and few-shot capabilities. NatureLM shows great promise, but its language capabilities and few-shot learning skills still lag behind leading large language models. We will address these limitations in future iterations, positioning NatureLM as a key player in the continued evolution of scientific foundation models."}, {"title": "2 Method", "content": "The pre-training data includes text, small molecules, proteins, materials, DNA, and RNA, all in the format of sequences:\n1. Small molecules are converted into Simplified Molecular Input Line Entry System (SMILES) notations, obtained by applying depth-first search algorithm on molecular graphs to yield a linear representation of the chemical structure [29]. The SMILES are tokenized by the commonly used regular expression for molecules2.\n2. Proteins, DNA and RNA are depicted using FASTA format, which sequentially lists the amino acids or nucleotides. The sequences are tokenized into individual units, with proteins broken down into their constituent amino acids and DNA/RNA into their respective nucleotides.\n3. For crystal material data, both the chemical composition and the associated space group number\u00b3 are flattened into a sequence. For example, consider\nthe material from the material project with ID mp-1960, as shown in Fig. 2. This material has 12 atoms in its cell, consisting of 4 Li and 8 O atoms. We flatten this information as depicted in the figure. The space group is Fm3m, which corresponds to the International Space Group Number 225, and we represent it with (sg\u2026).\nAn example of the data is in Fig. 2. The vocabulary sizes of small molecules, proteins, material, DNA and RNA are 1401, 26, 396, 16 and 16 respectively. To differentiate scientific entities from regular text, each scientific sequence\nis enclosed by a pair of special tokens: (mol)\u2026\u3008/mol) for small molecules,\n(protein)\u2026\u2026(/protein) for proteins, (material)\u2026\u2026(/material) for mate-\nrials, (dna)\u2026\u3008/dna\u3009 for DNA and (rna)\u2026\u3008/rna) for RNA. Specifically, we use (product)\u2026\u3008/product\u3009 and (reactant)\u2026\u2026(/reactant\u3009 to represent products and reactants for small molecules in chemical reactions. We use (antibody)\u2026\u2026(/antibody\u3009 to denote antibodies. For example, benzene is rep-resented by (mol)clcccccl(/mol). More examples can be found within the following sections.\nThe pre-training data contains single-domain sequences and cross-domain sequences. A single-domain sequence comes from one domain, such as pure text sequences, SMILES sequences for small molecules, and FASTA sequences for proteins. A cross-domain sequence includes data from two different domains, building connections across domains. The distribution of our pre-training data is visualized in Fig. 3 and more details are left in Table S1.\nOur cross-domain data is organized into three categories."}, {"title": "2.2 Post-training data", "content": "We curated a dataset for post-training with about 5.1 million instruction-response pairs encompassing six domains, small molecules, proteins, materials,\nDNA, RNA and general text (Figure 4). The dataset includes over 60 sub-tasks. For each sub-task, multiple prompts were manually crafted to form diverse instruction-response pairs, covering essential scientific tasks such as molecular optimization, antibody design, and guide RNA design. We provide two examples below:\n\u25b7 Example 1:\nInstruction:\nCreate a guiding RNA to interact with the DNA sequence\n(dna)CCCAGAGC\u00b7\u00b7\u00b7 GGGCCTGTC\u3008/dna\u3009.\nResponse:(rna)AGGGGACAA\u0410\u0410\u0421\u0421TTCATCCA(/rna)\n\u25b7 Example 2\nInstruction:\nWhat product could potentially form from\nthe reaction of the given reactants?\n(product)C([C@H]1N(c2c(C(N[C@@H](CC)c3ccccc3)=O)c3c(nc2-\nc2ccccc2)cccc3)CCC1)(=O)OC(/product)\nResponse:\n(reactant)C([C@H]1N(c2c(C(N[C@@H](CC)c3ccccc3)=O)c3c(nc2-\nc2ccccc2)cccc3)CCC1)O(/reactant)\nThe text data were sourced from open-source instruction tuning datasets like OIG 8, aiming to ensure that the model not only excels in scientific tasks but also maintains general language capabilities."}, {"title": "2.3 Model architecture", "content": "NatureLM models are built upon well-trained large language models (LLMs) with some additional parameters for newly introduce scientific tokens. We used Llama 3 8B [8] and Mixtral 8x7B [10] to initialize the main part of NatureLM and continued pre-training using the science data described in Section 2.1. Additionally, we trained a model with 1B parameters, which replicates the structural design of Llama 3 but with a reduced number of layers and smaller hidden dimensions. Its pre-training begins with a random selection of 300 billion pure text tokens from the SlimPajama dataset [41], followed by the science data we collected in Section 2.1. This approach ensures a consistent training methodology across all three models. The details of the model architecture are provided in Table 2."}, {"title": "2.4 Continued pre-training", "content": "To address the intricate comprehension required for scientific tasks, NatureLM introduces specific tokens for scientific entities. Consequently, we augment the vocabulary of the chosen LLMs. The embedding weights for these newly introduced tokens are randomly initialized. Directly tuning from pre-training usually causes instability and potentially compromises the language capabilities of the original LLMs. This is primarily due to the introduction of new tokens and the mismatch between the well-trained text tokens and randomly initialized scientific tokens.\nTo circumvent this issue, we have devised a two-stage pre-training procedure:\nStage 1: Training is exclusively concentrated on the newly introduced\ntokens. During this phase, the parameters of the existing model are frozen.\nThis allows the new tokens to adapt to the model gradually, mitigating the\nrisk of instability.\nStage 2: Once the new tokens are adequately trained, we proceed to the second phase where the entire network, including both new and existing parameters, is trained. This joint optimization process ensures that the new tokens are seamlessly integrated with the existing ones, enhancing the model's overall performance.\nThis two-stage training approach not only fosters a thorough understanding of the scientific domain but also preserves the integrity and robustness of the underlying language model by preventing potential instabilities. The detailed training recipe is summarized in Table S2.\nThe validation loss for the three versions of the models is illustrated in Fig. 5. All validation losses decrease as the model size increases from 1 billion\nto 8 billion, and 8x7 billion. This indicates that larger models are better at\ncapturing the underlying patterns in the data, which is expected due to their increased capacity. The most significant decreases are observed in the text and protein data, suggesting that these datasets benefit more from larger models."}, {"title": "2.5 Post-training", "content": "In the post-training phase, we mainly employ supervised fine-tuning (SFT) using the instruction-response pair data outlined in Section 2.2. These pairs are structured into sequences utilizing the template \"Instruction:\n{instruction}\n\n\nResponse: {response}\" where \"{instruction}\" and\n\"{response}\" serve as placeholders. During the model optimization, the train-ing loss is computed solely on the response part of the sequence. Unlike in\nthe pre-training phase, each sequence contains a single instruction-response\npair rather than multiple pairs packed into one sequence. Empirical evidence\nsuggests that this approach aids in stabilizing the post-training process. The\n1B and 8B models are trained for 20k steps, while the 8x7B model is trained for 7.8k steps (due to resource constraint). We also explore using RLHF after supervised finetuning and results are discussed in Section 8.1."}, {"title": "2.6 Inference acceleration", "content": "As NatureLM will be tested on many downstream tasks, we need to accelerate inference speed to save computational cost. We adopted the following approaches: (1) PagedAttention [42], which optimizes LLM serving by partitioning the key-value (KV) cache into fixed-size, non-contiguous blocks, reducing memory fragmentation and enabling efficient memory sharing; and (2) Selective Batching [43], which batches compatible operations while handling attention separately, allowing for flexible and efficient processing of requests with varying input lengths. We employed the vLLM framework [44] to serve NatureLM models, leveraging its implementations of both PagedAttention and Selective Batching. These optimizations were applied to the 1B, 8B, and 8\u00d77B models. Consequently, the inference speed for the NatureLM 8\u00d77B model reached approximately 525 tokens per second with Brain Float 16 (BF16) precision on two NVIDIA A100 GPUs."}, {"title": "3 Small molecule tasks", "content": "We assess the capabilities of NatureLM in terms of small molecule generation from the following perspectives:\n1. The unconditional generation ability (Section 3.1);\n2. The basic properties (such as QED, TSPA, etc.) to small molecule generation (Section 3.2);\n3. The translation between small molecule SMILES and IUPAC (Section 3.3);\n4. Utilize NatureLM to aid the drug discovery pipeline, which encompasses the\ngeneration and optimization of hit compounds (Section 3.4), optimization\nof binding affinity (Section 3.5), ADMET optimization (Section 3.6), and the synthesis routes of the compounds (Section 3.7)."}, {"title": "3.1 Unconditional molecular generation", "content": "We input the special token (mol) to NatureLM and let the model generate SMILES. The generation process stops upon encountering the special token (/mol). We assess the validity of the generated SMILES by checking if they can be converted into molecules using RDKit. Additionally, we evaluate the uniqueness of the valid SMILES by calculating the ratio of unique valid SMILES to\nthe total valid SMILES.\nThe evaluation results are presented in Table 3. The results demonstrate a clear trend: as the model size increases, the performance in terms of validity\nimproves. NatureLM exhibits a consistent increase in uniqueness as the model's capacity grows. We also establish comparisons between NatureLM and three generalist models: Llama 3 (8B), Mixtral (8x7B), and GPT-4. Our NatureLM\nsignificantly outperforms the others in terms of uniqueness. As for validity, the results show that GPT-4 demonstrates a remarkable ability to generalize chemically valid SMILES."}, {"title": "3.2 Property-to-molecule generation", "content": "The task is to generate molecules with specified properties, which is a critical aspect of molecular design. An example is shown as follows:\nInstruction:\nGenerate a molecule with four hydrogen bond donors.\nResponse:\n(mol)C(C[C@@H](C(=O)O)N)CN=C(N)N(/mol)\nWe conduct evaluations of NatureLM on six distinct properties: Quantitative Estimate of Drug-likeness (QED), hydrogen bond acceptors (HBA), hydrogen bond donors (HBD), fraction of sp3 hybridized carbons (FSP3), rotatable bonds (RotBonds), and topological polar surface area (TPSA). All these properties can be calculated using RDKit. For each property, we select multiple values as inputs to the model (see Table S3). We generate 100\nmolecules for each input and evaluate them with metrics including the Spearman correlation (Fig. 6a) and the correct ratio (Fig. S1). Our findings reveal\nthat on certain property, such as TPSA, the model demonstrates a Spearman correlation greater than 0.8, illustrating the consistency between the generated\nmolecules and the input specifications (Fig. 6b).\nAdditionally, our model can handle the combination of multiple properties.\nFor example, when given the command \u201cGenerate a compound with QED\n0.5 and TPSA 40\", the model generates compounds that meet both specified\ncriteria. The results are shown in Fig. 6c. The majority of the generated com-pounds have QED and TPSA values centered around our desired properties (i.e., 0.5 and 40), demonstrating the versatility and effectiveness of NatureLM in multi-property molecular generation.\""}, {"title": "3.3 Translation between SMILES and IUPAC", "content": "We evaluate NatureLM on the translation between SMILES and IUPAC on NC-I2S and NC-S2I [45], the bidirectional IUPAC-SMILES translation dataset comprising 2993 pairs of SMILES and their corresponding IUPAC names (Table 4). We ensure that there is no test set leakage in this setting. On both\ntext-to-SMILES and SMILES-to-text translation tasks, NatureLM (8x7B) outperforms all competing language models in terms of accuracy, demonstrat-ing our model's strong learning capability for text-molecule correspondence.\nNatureLM significantly outperforms GPT-4 and Claude 3 Opus [46], strong generalist large language models (LLMs), highlighting the necessity of training\non scientific data. Compared with another LLM trained on text and SMILES corpus LlaSMolMistral [45], NatureLM also obtains significantly better per-\nformance. Moreover, NatureLM (8x7B) performs comparably with STOUT [47], the widely-used model trained specially for IUPAC-SMILES translation task, demonstrating NatureLM's potential as a scientific generalist in spe-\ncific domains. The performance increases from NatureLM (1B) to NatureLM (8x7B), exhibiting the scaling benefits of larger models."}, {"title": "3.4 Target-aware hit generation and optimization", "content": "The task is to generate small molecule compounds given the target pro-tein sequence. The combination of NatureLM and structure-based compound\ndesign will be explored in the future. We test NatureLM within two distinct\nscenarios:\n(1) Generate compounds from the target protein sequences. This process is crucial for the hit identification stage of drug discovery, with the goal of discovering chemical entities that exhibit specific interactions with the target protein.\n(2) Generate molecular fragments based on the target protein sequences and partial molecular structures as inputs. This method is instrumental during the lead optimization phase, where we scrutinize and refine the molecular\narchitecture to amplify efficacy and precision."}, {"title": "3.5 Text-guided binding affinity optimization", "content": "To further improve the binding affinity between a target and a molecule, we propose a text-guided binding affinity optimization task. Given a target name and a molecule with a known binding affinity for that target, we aim to generate molecules with higher binding affinity, which is crucial for lead optimization. An example is shown below:\nInstruction:\nImprove the binding affinity on Uridine-cytidine kinase 2 of\n(mol)Cclccc(-c2nc3c(c(SCC(=O)Nc4ccccc4)n2)Cc2cccc(C)c2O3)ccl(/mol)\nResponse:\n(mol)Cclccc(-c2nc3c(c(SCC(=O)Nc4cccc(C(=O)O)c4)n2)Cc2cccc(C)\nc203)ccl(/mol)\nHere, the target information is provided in text format, which complements the FASTA representation used in Section 3.4. We will combine them in the future.\nWe test NatureLM on 12 targets that are not present in the post-training data and use a hybrid retrieval and docking approach for evaluation. Specifically, for the generated molecules, if we can retrieve their binding affinity\nvalues from the ChEMBL database, we compare these values with the original molecule's binding affinity. Otherwise, we compare their docking scores with\nthe original molecule. For the 12 selected targets, their Spearman correlation between the docking score and the actual binding affinity for known molecules\nexceeds 0.5, indicating the reliability of using docking for assessment (Table\nS4)."}, {"title": "3.6 Text-guided metabolism & distributional property optimization", "content": "We next assess the molecular optimization capabilities of NatureLM regarding metabolism and its potential to cross the blood-brain barrier (BBB), a crucial\nfactor in drug distribution. For metabolism, the goal is to reduce the inhibition\nof five key metabolic enzymes: CYP1A2, CYP2C19, CYP2C9, CYP2D6 and\nCYP3A4. An example is shown below:\nInstruction:\nTransform (mol)COCCn1c(=O)c(-c2ccccc2)nc2cnc(N3CCOCC3)nc21(/mol)\nto yield lower CYP1A2 inhibition and present a better metabolic safety record.\nResponse:\n(mol)COclccc(-c2nc3cnc(N4CCOCC4)nc3n(Cc3cccc(OC)c3)c2=O)cc1(/mol\u3009\nIn terms of BBB permeability (BBBP), we evaluate the enhancement BBB\npermeability. An example is provided below:\nInstruction:\nAdjust the molecule (mol)CC[C@H](NC(=O)c1c(OCCCC(=O)0)c\n(-c2ccccc2)nc2ccccc12)clcccccl\u3008/mol) to facilitate its passage through\nthe blood-brain barrier.\nResponse:\n(mol)CC[C@H](NC(=O)c1c(O)c(-c2ccccc2)nc2ccccc12)c1cccccl(/mol)\nFor each test sample, we used random search to generate four cases. To determine whether NatureLM effectively refined the input molecule, we trained\nsix groups of deep learning models for this evaluation. For assessing BBBP, we utilized the state-of-the-art model, BioT5 [54], to determine whether a\ncompound is capable of crossing the BBB. For metabolism optimization, we\nused ChemProp [55] to train classifiers to test if a molecule has the ability to\ninhibit enzymes from the cytochrome P450 (CYP) superfamily. We evaluated\nthe percentage of molecules that were successfully optimized according to the specified criteria (see Section C.3 for details)."}, {"title": "3.7 Retrosynthesis prediction", "content": "Retrosynthesis aims to identify synthesis routes for target molecules using commercially available compounds as starting points, a critical task in the discovery and manufacture of functional small molecules [56-58]. The appli-cability of ML-based retrosynthesis tools largely depends on the accuracy of single-step retrosynthesis prediction. We evaluate the capability of NatureLM for single-step retrosynthesis prediction on USPTO-50K [59]. NatureLM is\nprompted with the task description and the chemical SMILES of the product\nmolecule, and is expected to generate potential reactants.\nWe followed the common practice for splitting the USPTO-50K dataset [60, 61], and evaluated the performance using the 5007 reactions included in the test set. We ensured that there is no test set leakage in this setting. As outlined in\nTable 7, all sizes of NatureLM models surpass other methods in terms of top-k accuracy, demonstrating our model's accurate predictive ability for retrosyn-thesis prediction. NatureLM significantly outperforms GPT-4, a general LLM trained on human languages. This suggests that training on scientific data is\ncrucial for models to excel in scientific tasks. Furthermore, NatureLM outper-forms the state-of-the-art domain-specific models such as LocalRetro [62] and R-SMILES [63], showing NatureLM's potential as a scientific generalist in crit-ical scientific tasks. We also note an increase in performance from NatureLM (1B) to NatureLM (8x7B), demonstrating the scaling advantages of larger models.\nInstruction:\nPlease suggest possible reactants for the given product\n(product)CC(=O)c1ccc2c(ccn2C(=O)OC(C)(C)C)cl(/product)\nResponse:\n(reactant) CC(=O)c1ccc2[nH]ccc2c1.CC(C)(C)OC(=O)OC(=O)OC(C)(C)C\n(/reactant)"}, {"title": "4 Protein tasks", "content": "Our model's capabilities with respect to proteins are assessed through several distinct types of tasks:\n1. Unconditioned protein generation: The model generates protein sequences from scratch without any specific conditions or prompts.\n2. Text-guided protein generation: This task involves guiding the model to generate protein sequences based on given natural language descriptions.\n3. Antibody design: The model designs the Complementary-Determining Region H3 (CDR-H3) of antibodies to effectively bind to target antigens.\n4. Protein description generation: This task focuses on generating explanations\nor uncovering properties and functions of protein sequences, articulating them in natural language."}, {"title": "4.1 Unconditioned generation", "content": "The first capability of the model is generating protein sequences from scratch freely, prompted by the start token for proteins only, i.e., (protein). However, since there is no golden standard for evaluating proteins when no conditions are specified, it is difficult to measure the generation results. We focus on\nfoldability, measured by pLDDT score [65], as well as lengths and diversity of\nthe sequences, for the valid sequences.\nAs shown in Table 8, NatureLM consistently outperform Mixtral 8x7b and GPT-4 in terms of average sequence length, diversity, and average pLDDT\nscore. The NatureLM (8x7B) model achieves the best performance across\nall metrics, with an average length of 318.4, diversity of 0.989, and average pLDDT score of 75.9. ProLLAMA [24] a fine-tuned LLM for protein. It generates proteins without explicitly defined constraints on length, achieving a"}, {"title": "4.2 Text-guided protein generation", "content": "For text-guided protein generation, we evaluated our models' ability to gener-ate proteins with specific properties based on natural language prompts. In this\nstudy, we focused on two key properties: solubility and stability, leaving the exploration of additional properties for future work. For stability, the models\nwere tasked with generating protein sequences that exhibit stable properties. Regarding solubility, since both soluble and insoluble proteins are common\nin natural sequences, we instructed NatureLM to generate sequences of both\ntypes. Sample prompts are shown below, and a full list of prompts can be\nfound in Figure S11.\nAn example prompt for \"stable protein generation\"\nI require a stable protein sequence, kindly generate one.\n\u25b7 An example prompt for \u201csoluble protein generation\"\nGenerate a soluble protein sequence.\n\u25b7 An example prompt for \u201cinsoluble protein generation\"\nProduce a protein sequence that is not soluble.\nTo evaluate the stability and solubility of a generated protein sequence, we utilized two specialist models fine-tuned from the protein foundation model, SFM-Protein [67], as oracle models. One model was used for stability classification, while the other was used for solubility classification. The oracle models\nprovide probabilities that suggest the likelihood of the sequence possessing the desired property. To verify the efficiency of our model against random sam-\npling, we have also chosen a subset of 1000 natural protein sequences from the UR50 dataset and assessed them using the same oracle models."}, {"title": "4.3 Antigen-binding CDR-H3 design", "content": "The task of antigen-binding CDR-H3 design focuses on constructing the Complementary-Determining Region H3 (CDR-H3) of an antibody to bind\neffectively to a target antigen. We employed the RAbD benchmark dataset [68], comprising 60 antibody-antigen complexes. The example is shown below:\nInstruction:\nUsing antigen (protein)TQVCTGTDMKLR\u00b7\u00b7\u00b7 GESSEDCQS\u3008/protein)\nand antibody frameworks (antibody\u3009IVLTQTPS\u00b7\u00b7\u00b7 LAVYYC(/antibody)\nand (antibody)FGGGTRLEIEVQ(/antibody), create the CDR3\nregions."}, {"title": "4.4 Protein description generation", "content": "Despite the rapid discovery of natural protein sequences facilitated by advanced sequencing techniques, the functions of many of these proteins remain largely unknown. This knowledge gap restricts our ability to exploit\nthese proteins for engineering and therapeutic purposes. In this study, we explored the annotation generation capabilities of the NatureLM series."}, {"title": "5 Material tasks", "content": "To evaluate the capabilities of NatureLM for material generation, it is prompted to generate material's compositions in both unconditional and condi-tional way. For unconditional generation, the model is prompted with a special\ntoken indicating the start of material (i.e., (material)) and is expected to\ngenerate the composition of the material (Section 5.1). For conditional genera-tion, the model is prompted to generate material formula and structure under specific human instructions, including: (1) Composition to material generation\n(Section 5.2); (2) Bulk modulus to material generation (Section 5.3). After\ngenerating the chemical formula of a material, we use a dedicated fine-tuned\nNatureLM to generate its crystal structures, which are then evaluated for their accuracy and stability (see Section 5.4)."}, {"title": "5.1 Unconditional material generation", "content": "The model is tasked with generating materials with arbitrary compositions.\nThe input to NatureLM is (material), and it produces material compositions with a specified space group. An example is provided below,\nInstruction: (material)\nResponse: (material) \u0410 \u0410 \u0412\u0412\u0412 (sg12)\u3008/material)\nwhere A, B refer to elements and (sg12) denotes the space group.\nWe evaluated the SMACT validity of the generated materials. Further-more, we used the dedicated fine-tuned NatureLM to autoregressively predict the crystal structures of a randomly chosen subset of valid compositions (see Section 5.4). The energy above hull (abbreviated as ehull) of the predicted\nstructures was then evaluated using MatterSim [73]. The distribution of ehull\nis shown in Fig. S6. We also assessed the ratio of stable materials, defining\na generated material as stable if its ehull< 0.1eV/atom. The results are pre-\nsented in Table 12. It is evident that as the model size increases, the SMACT\nvalidity and stability of the generated materials improve."}, {"title": "5.2 Composition to material generation", "content": "The model is tasked with generating materials containing specific elements:\nInstruction: Build a material that has Li, Ti, Mn, Fe, O\nResponse: (material) Li Li Li Li Ti Ti Ti Mn Mn Fe Fe Fe O O O O\n000000000000 (sg8)(/material)\n\u039f\nWe evaluated the SMACT validity, stability, novelty, and precision of the gen-erated materials. The novelty is measured as the ratio of unique generated\nmaterials that are not present in our instruction tuning data. The composition precision is calculated as\n$$composition \\ precision = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{|E_{pi} \\cap E_{gi}|}{|E_{pi}|}$$\nwhere $E_{pi}$ and $E_{gi}$ stand for the sets of elements in the i-th prompt and corresponding generated material respectively.\nThe results are demonstrated in Table 13, and the distribution of ehull is depicted in Figure 12. Table 13 shows a significant improvement in SMACT\nvalidity scores due to instruction tuning compared to unconditional gener-ation. The precision for all three models is close to 100%, indicating their strong capability to follow language instructions for generating material for-mulas with expected elements. Additionally, the high novelty demonstrates the models' generative abilities. Furthermore, stability improves with model size, highlighting their scalability. Figure 12 illustrates this more clearly: as model size increases, the ehull distribution shifts closer to zero, indicating that more materials have lower energy and are in a more stable state."}, {"title": "5.3 Bulk modulus to material generation", "content": "The bulk modulus of a substance is a measure of the resistance of a substance to bulk compression. As a proof-of-concept", "modulus": "nInstruction: Construct the composition for a material with a\nspecified bulk modulus of 86.39 GPa.\nResponse: (material) Se Se Pd Sc (sg164)(/material)\nWe"}]}