{"title": "Deep Learning Evidence for Global Optimality\nof Gerver's Sofa", "authors": ["Kuangdai Leng", "Jia Bi", "Jaehoon Cha", "Samuel Pinilla", "Jeyan Thiyagalingam"], "abstract": "The Moving Sofa Problem, formally proposed by Leo Moser in 1966, seeks to\ndetermine the largest area of a two-dimensional shape that can navigate through\nan L-shaped corridor with unit width. The current best lower bound is about\n2.2195, achieved by Joseph Gerver in 1992, though its global optimality remains\nunproven. In this paper, we investigate this problem by leveraging the univer-\nsal approximation strength and computational efficiency of neural networks. We\nreport two approaches, both supporting Gerver's conjecture that his shape is\nthe unique global maximum. Our first approach is continuous function learning.\nWe drop Gerver's assumptions that i) the rotation of the corridor is monotonic\nand symmetric and, ii) the trajectory of its corner as a function of rotation is\ncontinuously differentiable. We parameterize rotation and trajectory by inde-\npendent piecewise linear neural networks (with input being some pseudo time),\nallowing for rich movements such as backward rotation and pure translation. We\nthen compute the sofa area as a differentiable function of rotation and trajec-\ntory using our \"waterfall\" algorithm. Our final loss function includes differential\nterms and initial conditions, leveraging the principles of physics-informed machine\nlearning. Under such settings, extensive training starting from diverse function\ninitialization and hyperparameters is conducted, unexceptionally showing rapid\nconvergence to Gerver's solution. Our second approach is via discrete optimiza-\ntion of the Kallus-Romik upper bound, which converges to the maximum sofa\narea from above as the number of rotation angles increases. We uplift this num-\nber to 10000 to reveal its asymptotic behavior. It turns out that the upper bound\nyielded by our models does converge to Gerver's area (within an error of 0.01%", "sections": [{"title": "1 Introduction", "content": "Among the unsolved problems in geometry, the moving sofa problem stands out for\nits apparent simplicity. Formally proposed by Leo Moser [1], it asks for the two-\ndimensional shape of the largest area that can be maneuvered through an L-shaped\ncorridor with unit width (see Figure 1). Beyond trivial shapes such as square and\nhalf disk, the best-known lower bounds are those found by John Hammersley [2] and\nJoseph Gerver [3], as shown in Figure 1. Hammersley's sofa, as he presented in a\nproblem set for school and college students, has an area of $\\pi/2 + 2/\\pi \\approx 2.2074$. It is\nthe largest among the family whose touch point with the inner corner of the corridor\nforms a semi-circle\u00b9; he also showed an upper bound of $2\\sqrt{2} \\approx 2.8284$. With an area\nslightly larger than 2.2195, Gerver's sofa is the largest known to date, composed of\n18 curve sections (as partitioned by their analytical expressions). In a fairly implicit\nway, Gerver proved that his solution is a local maximum and conjectured that it is\nthe only global maximum."}, {"title": "2 Area optimization", "content": "2.1 Geometry\nThe moving sofa problem can be understood as determining the movement of the L-\nshaped corridor in $\\mathbb{R}^2$ to maximize the area not swept by its four walls. We describe\nits movement using the trajectory of its inner corner, denoted by p, and the rotation\nangle a, as illustrated in Figure 2. This representation allows to describe the corridor's\nposition and orientation at any moment. An interactive visualization, assuming an\nelliptical trajectory for p, has been contributed by the community [4]. Previous studies\nby continuous optimization have considered $(x_p, y_p)$ as functions of a [3, 6, 7], thereby\nrestricting the movement to a monotonic rotation. To overcome this limitation, we\nintroduce a pseudo-time $t \\in [0, 1]$, and describe the movement by\n$\\begin{aligned}\nx_p &= x_p (t), && x_p (0) = 0,\\\\\ny_p &= y_p (t), && \\text{satisfying} &&y_p (0) = 0,\\\\\n\\alpha &= \\alpha (t); && \\alpha (0) = 0.\\\n\\end{aligned}$"}, {"title": "2.2 Physics-informed function learning", "content": "Distinct from common deep learning tasks for data fitting, our application is not\nconcerned with the accuracy or generalization of one best model. Instead, we aim to\nsupport the global optimality of Gerver's solution by demonstrating that many models\nfrom our over-parameterized function space converge to it. Therefore, we want our\nfunction space (i.e., hypothesis class) to be as general as possible. We parameterize\n$x_p (t)$, $y_p (t)$ and $\\alpha (t)$ by three independent fully-connected networks (FCNs) using\nrectified linear unit (ReLU) as the activation function. Here FCNs are chosen for\nuniversal approximation [18, 19], and ReLU for piecewise linear parameterization [20,\n21]. As explained after Eq. (4), piecewise $C^1$ is the most general class of functions that\ncan be explored using non-fractal methods; however, without any established piecewise\n$C^1$ architecture in deep learning, piecewise linearity, as facilitated by the ReLU family,\nstrikes a balance between generality and practicality. In contrast, using $C^{\\infty}$ activation\nfunctions such as tanh and softplus will narrow down the function space to $C^{\\infty}$.\nFor more stable and efficient training, we make our NNs physics-informed [14,\n15]. Firstly, as indicated in Eq. (4), the computation of the envelopes (and hence\nthe area) depends on the derivatives of the learned functions ($x_p, y_p$ and $\\alpha'$), which"}, {"title": "2.3 Results", "content": "The following training setup is adopted. We discretize the time $t \\in [0, 1]$ by 2000 points\nand distribute 10000 sources for waterfall, as determined by iterative refinement until\nthe computed area converges to the fourth decimal place when tested on Gerver's\nshape. We conduct a comprehensive exploration of 6000 runs in total, encompassing\n1000 random seeds for weight initialization, three initial learning rates ($10^{-3}$, $10^{-4}$ and\n$10^{-5}$, halved every 2000 epochs) and two network architectures (with two hidden lay-\ners of size 256 and three hidden layers of size 128). Each run undergoes 10000 epochs\nwith Adam [22]. We employ double-precision floating-point format, as single precision\nmay yield areas slightly exceeding Gerver's area near the minima, contaminating the\nstatistics of the results. Under such configurations, each run takes about two hours\non CPU; speedup by GPU is minimal due to the use of double precision. For verifica-\ntion, we also extend our experiments to include deeper and wider architectures, $C^{\\infty}$\nactivation functions, and other weight sampling distributions.\nAmong the above-defined 6000 runs, ~8.5% end up with a vanished area, mostly\noccurring at the largest learning rate. Excluding these outliers, the remaining trials\nexhibit a remarkable convergence behavior, consistently approaching a confined neigh-\nborhood surrounding Gerver's solution, which has an area of ~2.219532 [24]. The\ndistribution of the 6000 areas most reassemble normal distribution $\\mathcal{N} (\\mu, \\sigma^2)$, where\n$\\mu = 2.219482$ and $\\sigma = 0.000011$, spanning a range between 2.219459 and 2.219502.\nFurther refinement utilizing 1000 epochs of fine-tuning with the L-BFGS algorithm [25]\ncan improve the maximum area to 2.219515. We do not perform L-BFGS on the other"}, {"title": "3 Kallus-Romik upper bound", "content": "3.1 Theory\nYoav Kallus and Dan Romik [8] established an upper bound on the maximum sofa\narea by optimizing the rotation center at a finite sequence of rotation angles. Before\npresenting our findings, we first provide a brief review of their theory and discuss some\nvisible limitations of their numerical algorithm.\nLet $L_\\alpha (u)$ denote the set formed by rotating the corridor by angle a around the\ncenter $u = (u_1, u_2) \\in \\mathbb{R}^2$:\n$\\begin{aligned}\nL_\\alpha (u) = &\\{(x, y) \\in \\mathbb{R}^2 : u_1 \\leq x \\cos \\alpha + y \\sin \\alpha \\leq u_1 + 1\\} \\cap\\\\n&\\{(x, y) \\in \\mathbb{R}^2 : -x \\sin \\alpha + y \\cos \\alpha \\leq u_2 + 1\\} \\cup\\\\n&\\{(x, y) \\in \\mathbb{R}^2 : x \\cos \\alpha + y \\sin \\alpha < u_1 + 1\\} \\cap\\\\n&\\{(x, y) \\in \\mathbb{R}^2 : u_2 \\leq -x \\sin \\alpha + y \\cos \\alpha \\leq u_2 + 1\\} .\n\\end{aligned}$"}, {"title": "3.2 The five-angle case", "content": "An outstanding advantage of the Kallus-Romik upper bound is that it allows for\nexploring the problem at a low dimension by considering only a few angles. Kallus\nand Romik [8] defined the following five angles (we remark that they actually defined"}, {"title": "3.3 Convergence with many angles", "content": "The computational cost of our deep learning workflow (waterfall and backpropaga-\ntion) scales with the number of angles, allowing us to directly explore the convergence\ntheorem in Eq. (13). We increase the number of angles (n) from 10 to 10000, incre-\nmented by 10 between [0,100], by 100 between (100,3000] and by 500 afterwards,\nmaking 53 n's in total. For each n, Eq. (13) requires $[n/3]$ training instances, which\nare still computationally expensive for the large n's. While Kallus and Romik [8] have\nshown that 81.2\u00b0 is the minimum angle the corridor must rotate for maximum area\nproduction, all current evidence hints that a rotation of $\\pi/2$ should take place [3, 6-8],\nincluding our results reported in Section 2.3. Therefore, we will vary model initializa-\ntion only for k = 1 (rotation by $\\pi/2$) with 20 random seeds (i.e., using one seed for\nk > 1). The other hyperparameters (learning rates, architectures, and epochs) remain\nthe same as those in Section 2.3. The most expensive run for n = 10000, with 10000\nsources for the waterfall and 10000 epochs, takes about ten CPU hours to complete.\nOur training results show that, for all the n's, the maximum $\\mathcal{G}^{\\gamma_n-\\mathcal{k}, \\gamma_n-\\mathcal{k}+1}_{\\gamma_1, \\gamma_2, ..., \\gamma_n-\\mathcal{k}-2}$ is\nachieved when k = 1, supportive of the conjecture that the largest sofa necessitates a\nfull rotation by $\\pi/2$. We can then focus our attention on $\\mathcal{G}^{\\gamma_n-\\mathcal{k}, \\gamma_n-\\mathcal{k}+1}_{\\gamma_1, \\gamma_2, ..., \\gamma_n-\\mathcal{k}-2}$ (i.e., k = 1).\nAs shown in Figure 9, $\\mathcal{G}^{\\gamma_n-\\mathcal{k}, \\gamma_n-\\mathcal{k}+1}_{\\gamma_1, \\gamma_2, ..., \\gamma_n-\\mathcal{k}-2}$ found by our models nicely converges to Gerver's\narea asymptotically from above. The relative error becomes smaller than 1%, 0.1% and\n0.01% respectively at n = 30, 300 and 2100, and finally reaches 0.003% at n = 10000.\nThis result provides another piece of evidence for the global optimality of Gerver's\nsofa, along with the one achieved in Section 2.3."}, {"title": "4 Discussion", "content": "We have presented two pieces of evidence to support Gerver's conjecture that his 18-\nsection sofa, with an area around 2.2195, is the unique shape of the maximum area\nthat can navigate through an L-shaped corridor with unit width.\nOur first approach optimizes the sofa area as a function of the corridor movement\nparameterized by physics-informed neural networks. A critical step in our architecture"}]}