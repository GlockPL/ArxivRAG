{"title": "Deep Learning Evidence for Global Optimality\nof Gerver's Sofa", "authors": ["Kuangdai Leng", "Jia Bi", "Jaehoon Cha", "Samuel Pinilla", "Jeyan Thiyagalingam"], "abstract": "The Moving Sofa Problem, formally proposed by Leo Moser in 1966, seeks to\ndetermine the largest area of a two-dimensional shape that can navigate through\nan L-shaped corridor with unit width. The current best lower bound is about\n2.2195, achieved by Joseph Gerver in 1992, though its global optimality remains\nunproven. In this paper, we investigate this problem by leveraging the univer-\nsal approximation strength and computational efficiency of neural networks. We\nreport two approaches, both supporting Gerver's conjecture that his shape is\nthe unique global maximum. Our first approach is continuous function learning.\nWe drop Gerver's assumptions that i) the rotation of the corridor is monotonic\nand symmetric and, ii) the trajectory of its corner as a function of rotation is\ncontinuously differentiable. We parameterize rotation and trajectory by inde-\npendent piecewise linear neural networks (with input being some pseudo time),\nallowing for rich movements such as backward rotation and pure translation. We\nthen compute the sofa area as a differentiable function of rotation and trajec-\ntory using our \"waterfall\" algorithm. Our final loss function includes differential\nterms and initial conditions, leveraging the principles of physics-informed machine\nlearning. Under such settings, extensive training starting from diverse function\ninitialization and hyperparameters is conducted, unexceptionally showing rapid\nconvergence to Gerver's solution. Our second approach is via discrete optimiza-\ntion of the Kallus-Romik upper bound, which converges to the maximum sofa\narea from above as the number of rotation angles increases. We uplift this num-\nber to 10000 to reveal its asymptotic behavior. It turns out that the upper bound\nyielded by our models does converge to Gerver's area (within an error of 0.01%", "sections": [{"title": "1 Introduction", "content": "Among the unsolved problems in geometry, the moving sofa problem stands out for\nits apparent simplicity. Formally proposed by Leo Moser [1], it asks for the two-\ndimensional shape of the largest area that can be maneuvered through an L-shaped\ncorridor with unit width (see Figure 1). Beyond trivial shapes such as square and\nhalf disk, the best-known lower bounds are those found by John Hammersley [2] and\nJoseph Gerver [3], as shown in Figure 1. Hammersley's sofa, as he presented in a\nproblem set for school and college students, has an area of $\\pi/2 + 2/\\pi \\approx 2.2074$. It is\nthe largest among the family whose touch point with the inner corner of the corridor\nforms a semi-circle\u00b9; he also showed an upper bound of $2\\sqrt{2} \\approx 2.8284$. With an area\nslightly larger than 2.2195, Gerver's sofa is the largest known to date, composed of\n18 curve sections (as partitioned by their analytical expressions). In a fairly implicit\nway, Gerver proved that his solution is a local maximum and conjectured that it is\nthe only global maximum.\n\u00b9It is then natural to ask if one can generalize semi-circle to semi-ellipse, which has sparked valuable efforts\nfrom enthusiasts [4, 5]; as illustrated in our Figure 2, however, such generalization is not straightforward."}, {"title": null, "content": "Dan Romik [6] advanced Gerver's analysis by introducing a set of six differential\nequations along with their general solutions. These solutions, under prescribed con-\ntact mechanisms through five phases of rotation, land on Gerver's sofa in a natural\nand explicit way. Using the same method, Romik also discovered a lower bound of\nthe \"ambidextrous sofa\" that can pass through a double-cornered corridor. A numer-\nical study [7] was carried out based on Romik's formulation. Yoav Kallus and Dan\nRomik [8] illuminated a new perspective of the problem. They proved an upper bound\nof the maximum area through discrete optimization of the rotation center at a finite\nsequence of rotation angles. Under a seven-angle setup, they discovered a new upper\nbound of the area at 2.37, along with the least-required rotation for maximum area\nproduction by approximately 81.2\u00b0. We will delve into their findings in greater detail\nin Section 3.\nThe past decade has witnessed groundbreaking advancements in deep learning,\nrevolutionizing numerous facets of human endeavor, including mathematical research,\nan area now often referred to as AI4Math. In the realm of constructing examples or\ncounterexamples (e.g., finding a new moving sofa is one of such), some AI-discovered\nresults have surpassed human efforts, such as faster algorithms for matrix produc-\ntion [9], larger cap sets for the no-three-in-line problem [10], and many complex knots\nwith specific topological properties [11]. Here we seek to probe the moving sofa prob-\nlem through the lens of deep learning. One of the core techniques we will employ is\nphysics-informed neural networks (PINNs), dating back to 90's [12, 13] and now thriv-\ning in the contemporary deep learning ecosystem [14, 15]. PINNs are aimed mainly\nat solving and inverting differential equations, featuring the idea of incorporating the\ntarget equations, along with their initial and boundary conditions, into the network\narchitecture or loss function. In general, PINNs can enhance accuracy and general-\nization of the resultant models while reducing the amount of training data required.\nWe will adhere to the principles of PINNs to address the differential terms involved\nin the geometry and the initial conditions of sofa movement. By doing so, we aim to\nachieve more stable and efficient training while imposing minimum assumptions on\nthe movement.\nWe approach the moving sofa problem from an optimization viewpoint, employing\nneural networks (NNs) to parameterize a function space for identifying optimal sofa\nmovements or upper bounds. The idea harnesses the universal approximation capabil-\nity of NNs and the enhanced, albeit still evolving, proficiency of modern optimizers to\navoid local minima. While preliminary attempts from the community exist (e.g., [16]),\nthey often fail to properly consider the geometry and lack comprehensive insights\ninto optimality. A significant challenge we have surmounted is the computation of the\nsofa area, which engages with complex (or even chaotic) geometry given an arbitrary\nmovement. To this end, we propose the \"waterfall\" algorithm, which can compute the\narea not only with high accuracy and robustness but also in a differential manner as\nrequired for backpropagation.\nTwo independent approaches will be described in the remainder of this paper.\nIn Section 2, we directly optimize the sofa area as a function of corridor movement,\nparameterized by PINNs. To minimize inductive bias, special attention is paid to the\ngenerality of the function space and the diversity of function initialization. In Section 3,"}, {"title": null, "content": "we shift our objective function from the sofa area to the Kallus-Romik upper bound [8].\nWe first revisit their five-angle scenario, obtaining a tighter upper bound at 2.3337,\nand then explore a large number of angles to show the asymptotic behavior of their\nupper bound. Initially motivated to detect a shape larger than that of Gerver's, both\nour two approaches end up being supportive of Gerver's conjecture that his 18-section\nsofa is the unique global maximum."}, {"title": "2 Area optimization", "content": ""}, {"title": "2.1 Geometry", "content": "The moving sofa problem can be understood as determining the movement of the L-\nshaped corridor in $\\mathbb{R}^{2}$ to maximize the area not swept by its four walls. We describe\nits movement using the trajectory of its inner corner, denoted by p, and the rotation\nangle a, as illustrated in Figure 2. This representation allows to describe the corridor's\nposition and orientation at any moment. An interactive visualization, assuming an\nelliptical trajectory for p, has been contributed by the community [4]. Previous studies\nby continuous optimization have considered $(x_{p}, y_{p})$ as functions of a [3, 6, 7], thereby\nrestricting the movement to a monotonic rotation. To overcome this limitation, we\nintroduce a pseudo-time $t \\in [0, 1]$, and describe the movement by\n$x_{p} = x_{p} (t)$,\n$x_{p} (0) = 0,$\n$y_{p} = y_{p} (t)$,\nsatisfying\n$y_{p} (0) = 0,$\n$\\alpha = \\alpha (t)$;\n$\\alpha (0) = 0.$\n(1)"}, {"title": null, "content": "The above initial conditions at t = 0 are trivial, dealt with the translation and rotation\ninvariance of $\\mathbb{R}^{2}$. A non-trivial condition is\n$\\alpha (1) > arcsin \\frac{84}{85} \\approx 81.2^{\\circ}$,\n(2)\nthat is, any moving sofa shape of largest area must undergo rotation by an angle of at\nleast 81.2\u00b0 [8]. We will later use this condition in our loss function. The parameteri-\nzation in Eq. (1) only requires that $x_{p} (t), y_{p} (t)$ and $\\alpha (t)$ are of class $C^{0}$, allowing for\nindependent translation and rotation that are non-monotonic, non-differentiable and\nnon-symmetric.\nThe movement of the corridor will determine the following four families of lines,\nas named by their initial position at t = 0 (see Figure 2):\nInner horizontal, $l_{ih}$: $(x-x_{p}) sin \\alpha - (y-y_{p}) cos \\alpha = 0$;\nInner vertical, $l_{iv}$ : $(x - x_{p}) cos \\alpha + (y - y_{p}) sin \\alpha = 0$;\nOuter horizontal, $l_{oh}$: $(x - x_{p}) sin \\alpha - (y - y_{p}) cos \\alpha + 1 = 0$;\n(3)\nOuter vertical, $l_{ov}$: $(x - x_{p}) cos \\alpha + (y - y_{p}) sin \\alpha - 1 = 0.$\nBased on simple calculus, their envelopes can be shown as\nInner horizontal, $e_{ih}$: $\\left\\{\\begin{array}{l}\nx_{ih} = x_{p} + cos \\alpha \\left(\\frac{\\dot{x}_{p} sin \\alpha - \\dot{y}_{p} cos \\alpha}{\\dot{\\alpha}}\\right)^{-1},\ny_{ih} = y_{p} + sin \\alpha \\left(\\frac{\\dot{x}_{p} sin \\alpha - \\dot{y}_{p} cos \\alpha}{\\dot{\\alpha}}\\right)^{-1};\n\\end{array}\\right.$\nInner vertical, $e_{iv}$: $\\left\\{\\begin{array}{l}\nx_{iv} = x_{p} - sin \\alpha \\left(\\frac{\\dot{x}_{p} cos \\alpha + \\dot{y}_{p} sin \\alpha}{\\dot{\\alpha}}\\right)^{-1},\ny_{iv} = y_{p} + cos \\alpha \\left(\\frac{\\dot{x}_{p} cos \\alpha + \\dot{y}_{p} sin \\alpha}{\\dot{\\alpha}}\\right)^{-1};\n\\end{array}\\right.$\n(4)\nOuter horizontal, $e_{oh}$: $\\left\\{\\begin{array}{l}\nx_{oh} = x_{ih} - sin \\alpha,\ny_{oh} = y_{ih} + cos \\alpha;\n\\end{array}\\right.$\nOuter vertical, $e_{ov}$: $\\left\\{\\begin{array}{l}\nx_{ov} = x_{iv} + cos \\alpha,\ny_{ov} = y_{iv} + sin \\alpha,\n\\end{array}\\right.$\nwhere the primes denote derivatives with respect to t. We assume that $x_{p} (t), y_{p} (t)$\nand $\\alpha (t)$ are piecewise differentiable (piecewise $C^{1}$) so that any non-differentiable\npoints can be excluded from the aforementioned envelopes. This assumption is jus-\ntified, as it is unlikely that the largest sofa could contain some fractal sections. If it\ndoes indeed, such sections are inherently undetectable by methods based on function\nparameterization, including deep learning approaches.\nThe resultant area is fringed from below by p, $e_{ih}, e_{iv}$ and y = 0, whichever on\ntop, and from above by $e_{oh}, e_{ov}$ and y = 1, whichever on bottom. However, it cannot\nbe formulated as a simple integral because these curves can be self-intersecting and\nintersect at multiple points, as indicated by the zoomed-in windows in Figure 2. Notice\nthat Figure 2 only shows a simple case where the trajectory is elliptical; the envelopes\ncan become \"chaotic\" for an arbitrary movement. Such complexity also explains why\nthe moving sofa problem remains unsolved. To compute the area robustly for any\nmovements, we develop the \"waterfall\" algorithm, inspired by the rainfalling watershed"}, {"title": "2.2 Physics-informed function learning", "content": "Distinct from common deep learning tasks for data fitting, our application is not\nconcerned with the accuracy or generalization of one best model. Instead, we aim to\nsupport the global optimality of Gerver's solution by demonstrating that many models\nfrom our over-parameterized function space converge to it. Therefore, we want our\nfunction space (i.e., hypothesis class) to be as general as possible. We parameterize\n$x_{p} (t), y_{p} (t)$ and $\\alpha (t)$ by three independent fully-connected networks (FCNs) using\nrectified linear unit (ReLU) as the activation function. Here FCNs are chosen for\nuniversal approximation [18, 19], and ReLU for piecewise linear parameterization [20,\n21]. As explained after Eq. (4), piecewise $C^{1}$ is the most general class of functions that\ncan be explored using non-fractal methods; however, without any established piecewise\n$C^{1}$ architecture in deep learning, piecewise linearity, as facilitated by the ReLU family,\nstrikes a balance between generality and practicality. In contrast, using $C^{\\infty}$ activation\nfunctions such as tanh and softplus will narrow down the function space to $C^{\\infty}$.\nFor more stable and efficient training, we make our NNs physics-informed [14,\n15]. Firstly, as indicated in Eq. (4), the computation of the envelopes (and hence\nthe area) depends on the derivatives of the learned functions ($x_{p}, y_{p}$ and $\\alpha'$), which"}, {"title": null, "content": "are efficiently calculated via automatic differentiation (AD). Secondly, we enforce the\ninitial conditions specified in Eq. (1) by the network architecture, while the non-trivial\ncondition in Eq. (2) is incorporated into the loss function due to its inequality nature.\nBesides, it is straightforward to see that a movement involving $x_{p} > 0, y_{p} < 0$, or\n$\\alpha < 0$ is forbidden by geometry. Combining all together, the functions describing a\ncorridor movement are approximated by\n$\\hat{x}_{p} (t) = - | \\mathcal{F}_{x_{p}} (t) - \\mathcal{F}_{x_{p}} (0) |,$\n$\\hat{y}_{p} (t) = | \\mathcal{F}_{y_{p}} (t) - \\mathcal{F}_{y_{p}} (0) |,$\n$\\hat{\\alpha} (t) = | \\mathcal{F}_{\\alpha} (t) - \\mathcal{F}_{\\alpha} (0) |,\n(5)\nwhere $\\mathcal{F}_{x_{p}}, \\mathcal{F}_{y_{p}}$ and $\\mathcal{F}_{\\alpha}$ are unconstrained FCNs. With the area A computed by\nour waterfall algorithm based on $\\hat{x}_{p}(t), \\hat{y}_{p}(t)$ and $\\hat{\\alpha}(t)$, the following loss function is\nadopted for backpropagation:\n$\\mathcal{L} = -A + ReLU (81.2^{\\circ} - \\hat{\\alpha} (1)),$\n(6)\nwhere the ReLU deactivates the penalizing term once $\\hat{\\alpha} (1)$ reaches above 81.2\u00b0. The\ncomplete architecture is summarized in Figure 4.\nNN-based optimization is gradient-based, with no guarantee on global minimum.\nHowever, the modern optimizers such as Adam [22, 23], featuring an adaptive step size\nand momentum, enhance the opportunity of escaping local minima. Also, the likeli-\nhood of attaining the global minimum increases if multiple approximators, initialized\ndiversely, converge to the same local minimum. In this context, \u201cdiverse initialization\u201d\nextends beyond mere variation in weights and biases; it implies that approximators\napproach their resultant minima from distinct trajectories in the parameter space. This\nneeds meticulous scaling of the weight sampling distribution. For instance, our target\nfunction $\\hat{\\alpha} (t)$ lies between $[0, \\pi/2]$, whose approximators ($|\\mathcal{F}_{\\alpha} (t) - \\mathcal{F}_{\\alpha} (0)|$) should"}, {"title": "2.3 Results", "content": "The following training setup is adopted. We discretize the time $t \\in [0, 1]$ by 2000 points\nand distribute 10000 sources for waterfall, as determined by iterative refinement until\nthe computed area converges to the fourth decimal place when tested on Gerver's\nshape. We conduct a comprehensive exploration of 6000 runs in total, encompassing\n1000 random seeds for weight initialization, three initial learning rates ($10^{-3}, 10^{-4}$ and\n$10^{-5}$, halved every 2000 epochs) and two network architectures (with two hidden lay-\ners of size 256 and three hidden layers of size 128). Each run undergoes 10000 epochs\nwith Adam [22]. We employ double-precision floating-point format, as single precision\nmay yield areas slightly exceeding Gerver's area near the minima, contaminating the\nstatistics of the results. Under such configurations, each run takes about two hours\non CPU; speedup by GPU is minimal due to the use of double precision. For verifica-\ntion, we also extend our experiments to include deeper and wider architectures, $C^{\\infty}$\nactivation functions, and other weight sampling distributions.\nAmong the above-defined 6000 runs, ~8.5% end up with a vanished area, mostly\noccurring at the largest learning rate. Excluding these outliers, the remaining trials\nexhibit a remarkable convergence behavior, consistently approaching a confined neigh-\nborhood surrounding Gerver's solution, which has an area of ~2.219532 [24]. The\ndistribution of the 6000 areas most reassemble normal distribution $\\mathcal{N} (\\mu, \\sigma^{2})$, where\n$\\mu = 2.219482$ and $\\sigma = 0.000011$, spanning a range between 2.219459 and 2.219502.\nFurther refinement utilizing 1000 epochs of fine-tuning with the L-BFGS algorithm [25]\ncan improve the maximum area to 2.219515. We do not perform L-BFGS on the other"}, {"title": "3 Kallus-Romik upper bound", "content": ""}, {"title": "3.1 Theory", "content": "Yoav Kallus and Dan Romik [8] established an upper bound on the maximum sofa\narea by optimizing the rotation center at a finite sequence of rotation angles. Before\npresenting our findings, we first provide a brief review of their theory and discuss some\nvisible limitations of their numerical algorithm.\nLet $L_{\\alpha} (u)$ denote the set formed by rotating the corridor by angle a around the\ncenter $u = (u_{1}, u_{2}) \\in \\mathbb{R}^{2}$:\n$L_{\\alpha} (u) = \\left\\{\\begin{array}{l}\\{(x, y) \\in \\mathbb{R}^{2} : u_{1} \\leq x cos \\alpha + y sin \\alpha \\leq u_{1} + 1} \\cap\\\\\n\\\\\\\\\\{(x, y) \\in \\mathbb{R}^{2} : -x sin \\alpha + y cos \\alpha \\leq u_{2} + 1} \\cup\\\\\n\\\\\\\\\\{(x, y) \\in \\mathbb{R}^{2} : x cos \\alpha + y sin \\alpha < u_{1} + 1} \\cap\\\\\n\\\\\\\\\\{(x, y) \\in \\mathbb{R}^{2} : u_{2} \\leq -x sin \\alpha + y cos \\alpha \\leq u_{2} + 1} .\\\\\n\\end{array}\\right.$\n(7)"}, {"title": null, "content": "Let $B (\\beta_{1}, \\beta_{2})$ denote the set formed by rotating the vertical strip by angles $\\beta_{1}$ and\n$\\beta_{2}$, respectively, and taking the union of these two rotations, resulting in a butterfly-\nshaped region:\n$B (\\beta_{1}, \\beta_{2}) = \\left\\{\\begin{array}{l}\\{(x, y) \\in \\mathbb{R}^{2} : 0 \\leq x cos \\beta_{1} + y sin \\beta_{1} } \\cap\\\\\n\\\\\\\\\\{(x, y) \\in \\mathbb{R}^{2} : x cos \\beta_{2} + y sin \\beta_{2} \\leq 1} \\cup\\\\\n\\\\\\\\\\{(x, y) \\in \\mathbb{R}^{2} : x cos \\beta_{1} + y sin \\beta_{1} \\leq 1} \\cap\\\\\n\\\\\\\\\\{(x, y) \\in \\mathbb{R}^{2} : 0 \\leq x cos \\beta_{2} + y sin \\beta_{2}}.\\\\\n\\end{array}\\right.$\n(8)\nLet $\\mathcal{A}$ denote the area measure on $\\mathbb{R}^{2}$, and $\\mathcal{A}^{*}(X)$ the maximal area of any connected\ncomponent $X \\subset \\mathbb{R}^{2}$. Then, given a finite sequence of angles, $(\\alpha_{1}, \\alpha_{2},..., \\alpha_{k})$, and their\ncorresponding rotation centers, $(u_{1},u_{2},...,u_{k})$, and $(\\beta_{1}, \\beta_{2})$ with $\\beta_{1} \\leq \\beta_{2}$, Kallus\nand Romik [8] first defined the following area:\n$g_{\\alpha_{1}, \\alpha_{2},..., \\alpha_{k}}^{\\beta_{1}, \\beta_{2}}(u_{1},u_{2},..., u_{k}) = \\mathcal{A}^{*} \\left(H \\cap \\bigcap_{j=1}^{k} L_{\\alpha_{j}} (u_{j}) \\cap B (\\beta_{1}, \\beta_{2})\\right)$\n(9)\nwhere $H = \\mathbb{R} \\times [0, 1]$ is the horizontal strip. Taking the supremum of the above area\nwith respect to the centers, they further obtained\n$\\mathcal{G}_{\\alpha_{1}, \\alpha_{2},..., \\alpha_{k}}^{\\beta_{1}, \\beta_{2}} = \\sup \\left\\{g_{\\alpha_{1}, \\alpha_{2},..., \\alpha_{k}}^{\\beta_{1}, \\beta_{2}}(u_{1},u_{2},..., u_{k}) : u_{1},u_{2},..., u_{k} \\in \\mathbb{R}^{2}\\right\\},$\n(10)\nwhich they finally proved to be an upper bound of the maximum sofa area. Note\nthat $H \\cap B (\\beta_{1}, \\pi/2) = H$; namely, when $\\beta_{2} = \\pi/2$, the value of $\\beta_{1}$ does not affect\nthe resultant area. Therefore, the end case of $\\mathcal{G}_{\\alpha_{1}, \\alpha_{2},..., \\alpha_{k}}^{\\beta_{1}, \\beta_{2}}$ can be simply denoted\nby $\\mathcal{G}_{\\alpha_{1}, \\alpha_{2},..., \\alpha_{k}}$. The above-defined sets, $L_{\\alpha} (u)$ and $B (\\beta_{1}, \\beta_{2})$, and their induced\n$g_{\\alpha_{1}, \\alpha_{2},..., \\alpha_{k}}^{\\beta_{1}, \\beta_{2}}$ are illustrated in Figure 7 based on a small number of angles.\nFor a sofa shape that moves around the corner while rotating continuously and\nmonotonically from 0 to some angle $\\beta\\in [0, \\pi/2]$, define its maximum area as $\\mathcal{A} (\\beta)$.\nThe global maximum area is then $\\mathcal{A}_{max} = \\sup_{0<\\beta<\\pi/2} \\mathcal{A} (\\beta)$. The most relevant results\nfrom Kallus and Romik [8] are summarized as follows:"}, {"title": null, "content": "1. for any $(\\alpha_{1}, \\alpha_{2},...,\\alpha_{k})$ and $(\\beta_{1}, \\beta_{2})$ satisfying $0<\\alpha_{1} < \\alpha_{2} <...< \\alpha_{k} \\leq \\beta_{1} \\leq \\beta <\n\\beta_{2} < \\pi/2$, one has [8, Proposition 4 (iii)]\n$\\mathcal{A} (\\beta) < \\mathcal{G}_{\\alpha_{1},\\alpha_{2},..., \\alpha_{k}}^{\\beta_{1},\\beta_{2}};$\n(11)\nand [8, Proposition 4 (ii)]\n$\\mathcal{A}_{max} \\leq \\mathcal{G}_{\\alpha_{1},\\alpha_{2},...,\\alpha_{k}};$\n(12)\n2. given an integer $n \\geq 3$, let $\\gamma_{i} = \\frac{i \\pi}{2n}, i = 1, 2, \\ldots, n$; [8, Theorem 5] asserts that\n$\\mathcal{A}_{max} = \\lim_{n\\rightarrow \\infty} \\max_{k=1,2,..., \\lfloor n/3\\rfloor} \\mathcal{G}_{\\gamma_{1}, \\gamma_{2},..., \\gamma_{n-k-1}}^{\\gamma_{n-k},\\gamma_{n-k+1}};$\n(13)\nwhich provides a way for constraining $\\mathcal{A}_{max}$ asymptotically from above.\nThe above results are rigorous and elegant from a theoretical perspective. However,\nthe numerical algorithm they present for discrete optimization of the rotation centers,\n$(u_{1},u_{2},..., u_{k})$, seems to have two shortcomings:\n1. their algorithm does not yield $\\mathcal{G}_{\\alpha_{1},\\alpha_{2},..., \\alpha_{k}}^{\\beta_{1},\\beta_{2}}$ but another upper bound of it with\nunproven tightness; though this computed one is also an upper bound of $\\mathcal{A} (\\beta)$,\nthere can be space for improving the tightness if $\\mathcal{G}_{\\alpha_{1},\\alpha_{2},..., \\alpha_{k}}^{\\beta_{1},\\beta_{2}}$ is evaluated directly;\n2. based on rational programming, their algorithm is computationally expensive and\nunscalable, which can only tackle a small number of angles. Consequently, Eq. (13)\nor their Theorem 5 remains unexploited for constraining the actual value of $\\mathcal{A}_{max}$.\nThese shortcomings motivate our work reported in the remainder of this section.\nOur deep learning workflow is similar to Section 2.2 with two major differences.\nFirst, we use unconstrained FCNs to parameterize the functions $u_{1} (\\alpha)$ and $u_{2} (\\alpha)$,\nthat is, $u_{1,2} (\\alpha) = \\mathcal{F}_{u_{1,2}} (\\alpha)$, with no initial conditions and sign enforcement. Second,\nthe waterfall algorithm is applied directly on the four rays in $L_{\\alpha} (u)$ and the two lines\nin $B (\\beta_{1}, \\beta_{2})$, instead of on envelopes. Note that envelopes do not exist because the\nupper bound is defined with discrete angles. These two differences indicate that our\nNNs are no longer informed by any \"physics\", which is consistent with the notion\nthat the Kallus-Romik upper bound is not necessarily associated with a valid sofa\nmovement [8]."}, {"title": "3.2 The five-angle case", "content": "An outstanding advantage of the Kallus-Romik upper bound is that it allows for\nexploring the problem at a low dimension by considering only a few angles. Kallus\nand Romik [8] defined the following five angles (we remark that they actually defined"}, {"title": null, "content": "seven, but the other two are irrelevant to the sofa area)\n$\\alpha_{1} = arcsin \\frac{7}{25} \\approx 16.26^{\\circ},$\n$\\alpha_{2} = arcsin \\frac{33}{65} \\approx 30.51^{\\circ},$\n$\\alpha_{3} = arcsin \\frac{119}{169} \\approx 44.76^{\\circ},$\n(14)\n$\\alpha_{4} = arcsin \\frac{56}{65} = \\frac{\\pi}{2} - \\alpha_{2} \\approx 59.59^{\\circ},$\n$\\alpha_{5} = arcsin \\frac{24}{25} = \\frac{\\pi}{2} - \\alpha_{1} \\approx 73.74^{\\circ}.$\nThese angles do not have a specific geometric meaning; they are derived from arcsin\nfunctions only because the numerical algorithm of Kallus and Romik [8] is rational.\nThe inequalities below combine the optimization results from their algorithm and\nour NNs for $\\mathcal{G}_{\\alpha_{1},\\alpha_{2},\\alpha_{3},\\alpha_{4},\\alpha_{5}}$ and $\\mathcal{G}_{\\alpha_{1},\\alpha_{2},\\alpha_{3}}^{\\alpha_{4},\\alpha_{5}}$:\n$\\mathcal{G}_{\\alpha_{1},\\alpha_{2},\\alpha_{3},\\alpha_{4},\\alpha_{5}} \\leq 2.3337 \\pm 0.0000 < 2.37,$\n$\\mathcal{G}_{\\alpha_{1},\\alpha_{2},\\alpha_{3}}^{\\alpha_{4},\\alpha_{5}} \\leq 1.9259 \\pm 0.0000 < 2.21.$\n(15)\nOur results come from 6,000 runs, using the same hyperparameters (learning rates,\narchitectures, seeds and epochs) as described in Section 2.3. The training converges\nso well that the variances almost vanish ($\\pm 0.0000$ in the above inequalities).\nBased on Eq. (11), it can be shown that $\\mathcal{A}_{max} \\leq max(\\mathcal{G}_{\\alpha_{1},\\alpha_{2},\\alpha_{3},\\alpha_{4},\\alpha_{5}}, \\mathcal{G}_{\\alpha_{1},\\alpha_{2},\\alpha_{3}}^{\\alpha_{4},\\alpha_{5}})$.\nTherefore, our results suggest a tighter upper bound of 2.3337, corresponding to the set\nshown in Figure 8. The landscape of $g_{\\alpha_{1},\\alpha_{2},\\alpha_{3},\\alpha_{4},\\alpha_{5}}$ is also shown in Figure 8, verifying\nthe global optimality of $\\mathcal{G}_{\\alpha_{1},\\alpha_{2},\\alpha_{3},\\alpha_{4},\\alpha_{5}}$ in our NN-parameterized function space."}, {"title": "3.3 Convergence with many angles", "content": "The computational cost of our deep learning workflow (waterfall and backpropaga-\ntion) scales with the number of angles, allowing us to directly explore the convergence\ntheorem in Eq. (13). We increase the number of angles (n) from 10 to 10000, incre-\nmented by 10 between [0,100], by 100 between (100,3000] and by 500 afterwards,\nmaking 53 n's in total. For each n, Eq. (13) requires $\\lfloor n/3\\rfloor$ training instances, which\nare still computationally expensive for the large n's. While Kallus and Romik [8] have\nshown that 81.2\u00b0 is the minimum angle the corridor must rotate for maximum area\nproduction, all current evidence hints that a rotation of $\\pi/2$ should take place [3, 6-8],\nincluding our results reported in Section 2.3. Therefore, we will vary model initializa-\ntion only for k = 1 (rotation by $\\pi/2$) with 20 random seeds (i.e., using one seed for\nk > 1). The other hyperparameters (learning rates, architectures, and epochs) remain\nthe same as those in Section 2.3. The most expensive run for n = 10000, with 10000\nsources for the waterfall and 10000 epochs, takes about ten CPU hours to complete.\nOur training results show that, for all the n's, the maximum $\\mathcal{G}_{\\gamma_{1}, \\gamma_{2},..., \\gamma_{n-k-1}}^{\\gamma_{n-k}, \\gamma_{n-k+1}}$ is\nachieved when k = 1, supportive of the conjecture that the largest sofa necessitates a\nfull rotation by $\\pi/2$. We can then focus our attention on $\\mathcal{G}_{\\gamma_{1}, \\gamma_{2},..., \\gamma_{n-2}}^{\\gamma_{n-1}, \\gamma_{n}}$ (i.e., k = 1).\nAs shown in Figure 9, $\\mathcal{G}_{\\gamma_{1}, \\gamma_{2},..., \\gamma_{n-2}}^{\\gamma_{```json\n{\\gamma_{n-1}, \\gamma_{n}}$ found by our models nicely converges to Gerver's\narea asymptotically from above. The relative error becomes smaller than 1%, 0.1% and\n0.01% respectively at n = 30, 300 and 2100, and finally reaches 0.003% at n = 10000.\nThis result provides another piece of evidence for the global optimality of Gerver's\nsofa, along with the one achieved in Section 2.3."}, {"title": "4 Discussion", "content": "We have presented two pieces of evidence to support Gerver's conjecture that his 18-\nsection sofa, with an area around 2.2195, is the unique shape of the maximum area\nthat can navigate through an L-shaped corridor with unit width.\nOur first approach optimizes the sofa area as a function of the corridor movement\nparameterized by physics-informed neural networks. A critical step in our architecture"}, {"title": null, "content": "is the waterfall algorithm, designed for robust, efficient, and differentiable area calcu-\nlation under complex geometry of the wall envelopes. To minimize inductive bias, we\nemphasize the generality of our function space and the diversity of function initializa-\ntion; only with such considerations can the results from extensive model training be\nconvincing. We trained more than 6000 models; more than 90% of them landed on\nGerver's solution, with the outliers all ending up with a vanished area.\nOur second piece of evidence comes from optimizing the Kallus-Romik upper\nbound [8]. Following a similar workflow, we use neural networks to describe the rotation\ncenters corresponding to a finite sequence of rotation angles, considering both a small\nand large number of angles. Using the five angles defined by Kallus and Romik [8],\nwe obtained a tighter upper bound around 2.3337, compared to their 2.37. Finally, we\ntrained more than 6000 models based on their convergence theorem, increasing the\nnumber of angles up to 10000. The training results show that the Kallus-Romik upper\nbound asymptotically converges to Gerver's area, indicating that no larger sofa exists.\nIf a larger sofa does exist, how it has remained hidden from us for nearly 60 years?\nWe propose two reasonable guesses in light of all previous attempts including ours.\nFirst, the shape is fractal or contains a fractal section; if this is true, its discovery would\nbe daunting, as we are still struggling to estimate areas of known fractals. Second, the\nglobal maximum has a so small catchment basin that it can hardly be detected by\noptimization-based approaches (either gradient or non-gradient based). In that case,\ngeometric insights illuminating a new movement pattern with contact mechanisms\ndistinct from Gerver's (i.e., those prescribed in [6]) would be more essential than any\nalgorithmic considerations."}]}