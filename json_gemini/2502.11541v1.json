[{"title": "MuSC: Improving Complex Instruction Following with Multi-granularity Self-Contrastive Training", "authors": ["Hui Huang", "Jiaheng Liu", "Yancheng He", "Shilong Li", "Bing Xu", "Conghui Zhu", "Muyun Yang", "Tiejun Zhao"], "abstract": "Complex instruction-following with elaborate constraints is imperative for Large Language Models (LLMs). While existing methods have constructed data for complex instruction alignment, they all rely on a more advanced model, especially GPT-4, limiting their application. In this paper, we propose a Multi-granularity Self-Contrastive Training (MuSC) framework, to improve the complex instruction alignment without relying on a stronger model. Our method is conducted on both coarse and fine granularity. On coarse-granularity, we construct constraint-aware preference data based on instruction decomposition and recombination. On fine-granularity, we perform token-aware preference optimization with dynamic token-level supervision. Our method is evaluated on open-sourced models, and experiment results show our method achieves significant improvement on both complex and general instruction-following benchmarks, surpassing previous self-alignment methods.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have made remarkable advancements and are being wildly applied across various domains (Zhao et al., 2024; Minaee et al., 2024; Hadi et al., 2024; Yin et al., 2024; Wang et al., 2024a). The instruction-following ability is fundamental and important, as it enables LLMs to generate appropriate responses to given instructions and solve corresponding tasks (OpenAI et al., 2024). While recent LLMs perform comparatively well on simple instructions, their response quality to complex instructions with elaborate constraints often falls under expectation, with some of the constraints omitted (He et al., 2024; Jiang et al., 2024b), which hinders their application in more real-world complex scenarios.\nTo enhance the complex instruction following, the core challenge is the scarcity of high-quality complex instruction data (Lou et al., 2024). Most existing instruction datasets are constructed based on existing NLP datasets or question-answering websites with simple constraints (Wang et al., 2023; Taori et al., 2023; Lian et al., 2023; Longpre et al., 2023). To cope with the scarcity of complex instruction data, previous work such as Evol-Instruct (Xu et al., 2024), Conifer (Sun et al., 2024a), and Self-Correct (Palmeira Ferraz et al., 2024) have been proposed to construct complex instructions and responses. However, these methods typically rely on a high-performance proprietary model (e.g., GPT-4) to distill the complex instruction-following ability, which is expensive and can not be scaled up in real-world applications.\nRecently, the research community has paid attention to self-alignment, to break the data bottleneck without relying on a stronger model (Wang et al., 2024b; Zhang et al., 2024a). Self-Reward (Yuan et al., 2024) proposes to utilize the model itself to both generate responses and evaluate the results, which can be incorporated into DPO training. ISHEEP (Liang et al., 2024) proposes an automatic loop to self-assess and self-filter instruction data. Despite their effectiveness, these methods are targeted at general instruction-following ability. The self-alignment of complex instruction-following ability remains unexplored.\nIn this paper, to address the above limitations, we propose a novel Multi-granularity Self-Contrastive Training framework (MuSC) in Figure 1, which mainly comprises the following components:\n1) Coarse-grained Contrast: Constraint-aware Preference Data Construction. To improve the model's comprehension of constraint-level distinctions, we construct preference pairs that reflect the disparities in constraint fulfillment. We achieve this by breaking down each complex instruction into atomic constraints and selectively omitting a subset to form negative instructions. The chosen response, derived from the original instruction, is paired with the rejected response, generated from the negative instruction, as a contrastive pair. Notably, no external models are utilized in this construction process.\n2) Fine-grained Contrast: Token-aware Preference Optimization. For complex instructions, the responses often involve multiple tokens that contribute differently to fulfilling the instruction's constraints. Therefore, we introduce a token-aware optimization framework that integrates dynamic token-level weights based on the model's confidence. By focusing on tokens that deviate from the constraints, this approach effectively identifies and corrects tokens where the model fails to satisfy the instruction's requirements, leading to more contextually appropriate responses.\nMoreover, we need to mention that our MuSC can be applied on both pre-existing complex instruction datasets, or newly generated instruction datasets created by data synthesis methods (e.g., Self-Instruct (Wang et al., 2022)).\nOur contribution can be summarized as follows:\n\u2022 We propose a novel Multi-granularity Self-Contrastive Training (MuSC) framework, which creates effective contrast on both coarse and fine granularity, to enhance the complex instruction following abilities.\n\u2022 For coarse-grained contrast, we construct constraint-aware preference data with instruction decomposition-recombination. For fine-grained contrast, we adopt dynamic token-level weight with confidence guidance for better preference optimization.\n\u2022 We evaluate our framework on open-source LLMs, and achieve significant improvements on both complex and general instruction following benchmarks, without the help of a larger model or human supervision."}, {"title": "2 Related Work", "content": "Complex Instruction-Following. As one of the cores of LLM intelligence, how to improve the model's instruction-following capability is important. The earliest works, such as Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), and Camel (Wang et al., 2023), used instruction data generated by proprietary models to supervise fine-tuning of open-source models, significantly enhancing their instruction-following capabilities. However, these methods mainly focus on general instruction following, while complex instruction following still remains challenging. To cope with this challenge, a lot of methods (Yin et al., 2023; Lou et al., 2023; He et al., 2024; Sun et al., 2024b; Chen et al., 2024b; Dong et al., 2024) have been proposed to construct complex instruction data. The earliest work is Evol-Instruct (Xu et al., 2024), which proposed to utilize GPT-4 to expand the instructions from both depth and width, thereby generating complex instructions and corresponding constraints. Conifer (Sun et al., 2024a) proposed a progressive learning strategy designed to help smaller models incrementally enhance their abilities.\nSelf-Alignment. Self-alignment refers to aligning the model to human preference without relying on a more advanced model or external supervision. As an early study, Self-Rewarding (Yuan et al., 2024) proposed the model itself to both generate responses and evaluate the results. Following this work, many works (Liu et al., 2024; Chen et al., 2024b,a; Pang et al., 2024; Meng et al., 2024) are conducted to obtain supervision data by the model itself. Meta-Rewarding (Wu et al., 2024) advanced the concept by improving the model's instruction and evaluation capabilities simultaneously. Liu et al. (2024) employed diverse prompts to guide the model to generate various responses. Despite the progress these methods have made, they all target general instruction following. For complex instructions with multiple constraints, the response will be lengthy and multi-facet, resulting in challenges for the self-evaluation process."}, {"title": "3 Approach", "content": "The pipeline of MuSC is shown in Figure 2.\n3.1 Constraint-aware Preference Data\nReinforcement-learning methods, such as PPO (Schulman et al., 2017) and DPO (Rafailov et al., 2024b), have achieved notable success in LLM optimization. Research has shown that learning from negative samples is significantly more efficient than learning solely from positive samples (Yang et al., 2024b). However, these methods are limited by the need for high-quality preference data, which is particularly scarce for complex instructions.\nTo construct effective preference data for complex instruction following, we propose a novel data construction method, with the following steps:\n1. Instruction Decomposition: A complex instruction is typically a combination of multiple atomic constraints. We decompose the complex instruction into individual atomic constraints, denoted as $Cons$.\n2. Constraint Dropout: From the decomposed constraints Cons, we randomly eliminate $a\\%$ of the constraints to form $Cons_{drop}$.\n3. Instruction Recombination: We recombine both the original and the dropped constraints $Cons$ and $Cons_{drop}$, to create chosen and rejected instructions: $Ins$ and $Ins_{drop}$.\n4. Response Generation: Based on $Ins$ and $Ins_{drop}$, we generate the chosen response $Resp$ and the rejected response $Resp_{drop}$.\nPrevious research has suggested that the construction of effective preference pairs for optimization is non-trivial (Ivison et al., 2024). Our data construction pipeline is guided by three principles:\n\u2022 Negativity: The rejected response should deviate from the instruction by omitting some constraints. Our method generates the rejected instruction based on corrupted constraints, ensuring that the rejected response deviates from the original complex instruction.\n\u2022 Consistency: The rejected response should reside within the model's decoding space (Guo et al., 2024). In our method, the rejected instruction is simply a recombination of the original instructions, ensuring the response falls within the decoding space, which is crucial for the optimization process.\n\u2022 Contrastiveness: Chosen and rejected responses should be with a rational edit distance, to form an effective contrast (Jiang et al., 2024a). By reconstructing both chosen and rejected instructions using the same method, we ensure that the derived samples do not deviate too far from each other.\nWith constructed data satisfying both negativity, consistency and contrastiveness, we form a solid foundation for effective alignment. Moreover, our method does not require a stronger model or human supervision, ensuring its scalability.\nOur self-construction method can be applied in different scenarios. On one hand, it can be directly applied on pre-existing complex instruction dataset. On the other hand, if there is no existing complex queries, we can adapt the Self-Instruct (Wang et al., 2022) method by first generating constraints and then generating instructions. In that case, the decomposition step can be omitted.\n3.2 Token-aware Preference Optimization\nA well-known issue with DPO is its uniform treatment of all tokens in both chosen and rejected examples (Wu et al., 2023; Cao et al., 2024; Li et al., 2024). However, different tokens within responses carry varying significance. Especially in scenarios involving complex instructions, the responses tend to be lengthy and multi-facet. On one hand, not all tokens in the rejected response are erroneous and should be disapproved. On the other hand, chosen response may also contain tokens that fail to meet specific constraints, therefore should not be unanimously approved.\nDespite previous researchers have explored fine-grained supervision signals, the signals either come from a stronger model (Cao et al., 2024; Li et al., 2024) or human annotation (Wu et al., 2023; Lightman et al., 2023). However, in our case, it is difficult for the model to provide accurate supervision for its own response, especially when dealing with multifaceted instructions and the evaluation is at token-level. Therefore, we propose Confidence-Guided Token-aware DPO, which obtains token-level supervision based on model confidence.\n3.2.1 Preliminary: Token-level DPO\nDirect Preference Optimization (DPO) (Rafailov et al., 2024b) proposes a direct optimization objective that satisfies the optimal preference policy without using a reward model:\n$L_{DPO}(\\pi_{\\theta}; \\pi_{ref}) = E_{(x,y_w,y_l)\\sim D}[- \\beta log(\\sigma(\\beta(\\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)} - \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)})))]$\nwhere $\\pi_{\\theta}$ and $\\pi_{ref}$ represent the policy model and the reference model, respectively.\nSubsequently, based on the theories of Levine (2018), Rafailov et al. (2024a) derived the form of DPO in token-level Markov Decision Process, where dynamic weight can be easily integrated for different tokens, with the loss function as follows:\n$L_{TDPO}(\\pi_{\\theta}, D) = -E_{(\\tau_w, \\tau_l)\\sim D} log \\sigma(\\beta\\sum_{t=0}^{N-1}r_tlog(\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{ref}(a_t|s_t)})) - \\mu E_{(\\tau_w, \\tau_l)\\sim D} log \\sigma(\\beta\\sum_{t=0}^{M-1}r_tlog(\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{ref}(a_t|s_t)}))$\nwhere $\\tau^w$ and $\\tau^l$ represent the winning and losing trajectories, with N and M as the token numbers, and $r_t$ represents the weight for the t-th token.\n3.2.2 Calibrated Confidence as Token Weight\nWhile Section 3.2.1 provide theoretical support for token-level DPO, it is non trivial to derive token-level supervision. In this work, we propose to use the calibrated confidence as supervision.\nGiven an instruction x, we obtain the entropy of probability distribution over target vocabulary of size V at each decoding step as the weights:\n$Ent(y_t|x, \\theta) = -\\sum_{v=1}^{V}p(y^v)logp(y^v)$,\nwhere $p(y_t)$ represents the conditional distribution $p(y_t|x, y_{<t}, \\theta)$, and $\\theta$ represents model parameters. If the majority of the probability mass is concentrated on a limited number of vocabulary words, it indicates that the model is confident and the token is more likely to be aligned with the instruction (Fomicheva et al., 2020). Conversely, if the probabilities resemble a uniform distribution, the resulting token is expected to be misaligned.\nWhile there are other attributes that could also impact the confidence score (such as fluency), in this work, we want to focus our optimization on instruction alignment. Therefore, we apply calibration to the entropy to derive the token-level supervision for chosen and rejected samples respectively:\n$r_t =  \\frac{Ent(y_t|x^l, \\theta)}{Ent(y_t|x^w, \\theta)}, for y_t in y_w, \n\\frac{Ent(y_t|x^w, \\theta)}{Ent(y_t|x^l, \\theta)}, for y_t in y_l,$\nr_t = min(\\Gamma, r_t)$,\nwhere the chosen sample $(x^w, y^w)$ refers to $(Ins, Resp)$, and the rejected sample $(x^l, y^l)$ refers to $(Ins_{drop}, Resp_{drop})$, and $\\Gamma$ is an upper-bound to avoid extreme cases to disrupt training, and we set $\\Gamma$ as 2 in this work.\nThe rationale is straightforward: for a given token in the response, if it exhibits high confidence under $x^w$ and low confidence under $x^l$, this suggests that the token aligns well with $x$ but does not fit $x^l$, potentially reflecting the dropped constraint. Therefore, the token requires a larger weight if it is in the rejected response (or a smaller weight if it is in the chosen response).\nCalibrated confidence guided token-aware DPO allows for more targeted optimization, focusing on tokens that highlight the constraint mismatch on complex instructions, instead of unanimously optimizing all tokens, thereby improving the efficiency of complex instruction alignment."}, {"title": "4 Experiments", "content": "4.1 Set-up\nModels. We conduct experiments on two models: LLaMA-3-8B-Instruct (Dubey et al., 2024) and Qwen2-7B-Instruct (Yang et al., 2024a). Both models have undergone alignment to possess fundamental instruction-following ability.\nSetting. The experiments are carried out in two distinct settings:\n1. Pre-Existing Instructions (PreInst): We leverage pre-existing complex instructions as a starting point for the model. We randomly select 2,000 instructions from the dataset of WizardLM (Xu et al., 2024).\n2. Self-Generated Instructions (SelfInst): In this setting, we generate instructions using the Self-Instruct method (Wang et al., 2022), based on 10 high-quality samples from Qin et al. (2024) as in-context examples. Compared with PreInst, this setting is more challenging as we need to construct the complex instructions from scratch.\nEvaluation. We mainly perform evaluations on three complex instruction-following benchmarks: CFBench (Zhang et al., 2024b), FollowBench (Jiang et al., 2024b) and ComplexBench (He et al., 2024). We also conduct evaluations on one general instruction benchmark: AlpacaEval2 (Dubois et al., 2024). Note that all benchmarks require GPT-4 for judgment, and we use GPT-40-0513 as the evaluator for all of them.\nBaselines. We mainly compared our method against the following self-alignment methods:\n\u2022 Self-Reward (Yuan et al., 2024): This method leverages the model to first generate multiple responses and then construct rewards.\n\u2022 Self-Reward + BSM: Based on Self-Reward, this method performs fine-grained evaluation based on BSM (Saha et al., 2024).\n\u2022 Self-Correct (Palmeira Ferraz et al., 2024): This method generates initial output and then corrects it to construct preference data.\n\u2022 ISHEEP (Liang et al., 2024): This method self-creates additional instruction-output pair, which are filtered for supervised fine-tuning.\n4.2 Main Results\nAs demonstrated in Table 1, our proposed MuSC achieves significant improvement across both complex and general instruction-following benchmarks. The improvement is consistent among different settings, verifying its scalability. By creating preference data with both constraint-aware and token-aware contrast, the model effectively learns to address all constraints lying in the instructions.\nThe results of Self-Reward underperform our method, even with the help of branched evaluation (Saha et al., 2024). This is because of the limited evaluation capability of the model, especially when evaluating its own response to complex instructions. Moreover, as different responses generated from the same model to the same instruction typically do not vary significantly, it is difficult to create effective contrast samples with real negativity.\nThe improvement of I-SHEEP also underperforms, likely due to its reliance on supervised fine-tuning for optimization. Previous research also suggests that learning from negative samples is more effective than learning solely from positive ones (Yang et al., 2024b). The results of Self-Correct degrades a large margin, which might be due to the inability of the model for self-correction on complex instructions (Palmeira Ferraz et al., 2024). On general instruction benchmarks, our method also achieves significant improvement. This aligns with the previous research, which suggests that the improvement on complex instruction-following is beneficial for the overall instruction-following ability (Xu et al., 2024; Elmadany et al., 2023)."}, {"title": "5 Analysis", "content": "5.1 MuSC on SFT model\nIn Section 4, our main experiments are conducted on the Instruction-version models. To exclude the influence of an initial preference optimization process, we apply our method on SFT models.\nSpecifically, we selected two SFT-versions of LLaMA models, LLaMA-3-8B-UltraChat-200K and LLaMA-3-8B-Tulu-330K. Both models have gone through and only through SFT process on open-sourced datasets. As shown in Table 2, our proposed MuSC can improve both the complex and general instruction-following ability of SFT models by a large margin. Notice we only apply 2K samples when performing preference optimization, which is roughly 1% of the amount of SFT data. This again verifies that learning from negative samples is comparatively more efficient than learning solely from positive samples.\n5.2 The Influence of Confidence Metrics\nVarious confidence metrics have been established in the domain of LLM (Geng et al., 2024). This section aims to provide a comparison across different metrics as the token weight, under the framework of MuSC. We include the following metrics:\n\u2022 Perplexity: The exponential of the negative log-likelihood of the token.\n\u2022 PMI: Pointwise Mutual Information as defined in Takayama and Arase (2019).\n\u2022 KIDiv: Kullback\u2013Leibler divergence between the token probability distribution under chosen and rejected instructions.\nAs shown in Table 3, entropy-based token weight achieves the best result among all metrics, verifying its effectiveness. Both the perplexity and PMI-based score underperforms, as they only consider the probability of the selected token instead of the whole distribution, leading to biased evaluation. KLDiv-based score also underperforms, this is because KLDiv is essentially a distance measurement instead of a confidence measurement, which is not adapted to our scenario.\nWe also experiment with removing the calibration proposed in Section 3.2.2. As can be seen, calibration is important for the effectiveness of confidence-based fine-grained weight, as it can exclude other factors such as fluency, thereby focusing the contrast on instruction alignment.\n5.3 Can MuSC Scale to Other XPO Method?\nWhile our experiments primarily focus on the DPO method, the overall framework is not limited to a single preference optimization technique. Therefore, we extended our framework to two additional XPO methods: SimPO (Meng et al., 2024) and IPO (Azar et al., 2024). We utilized the same constructed preference data and applied the entropy-based score as the token-level supervision. Note that for \"w/o fgct\", we just remove the fine-grained contrast in the MuSC method. In Table 4, our approach has shown consistent improvements across both SimPO and IPO, validating its scalability.\n5.4 Is Fundamental Capability Harmed?\nPrevious research have proposed that during the alignment process, the fundamental ability of model may suffer degradation due to alignment tax (Ouyang et al., 2022). Therefore, we evaluated our proposed method on three fundamental capability benchmarks: MMLU (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021) and HumanEval (Chen et al., 2021).\nAs shown in Table 5, while the results of naive MuSC may suffer slight degradation, the introduction of fine-grained contrast mitigates the degradation, which verifies the significance of token-level supervision. Under the scenario of complex instruction, the response is lengthy and should not be uniformly approved or disapproved. With fine-grained supervision, we focus the optimization on complex instruction alignment, thereby avoiding the disruption of other capabilities.\n5.5 The Variation of Statistical Indicators\nAs different methods start from the same group of instructions, training statistics can be comparable as a quality indicator. Therefore, we display the variation of both loss and reward margins between chosen and rejected samples on different methods.\nAs shown in Figure 3, Self-Reward presents both higher loss and lower reward margin during training. This is because there is too much noise between the chosen and rejected pairs, making the model unable to capture the contrast related to constraint alignment. Notice that both indicators start to change drastically at the 2nd epoch, which means the learned knowledge cannot transfer between different samples at the 1st epoch. On the other hand, the optimization based on MuSC converges faster and more smoothly, verifying the effectiveness of the contrast samples.\nComparing the indicators of MuSC with and without fine-grained supervision, it can also be noticed that with the introduction of fine-grained supervision, both indicators converge faster. The introduction of token-level supervision is a cheap yet effective method to improve XPO methods."}, {"title": "6 Conclusion", "content": "In this work, we propose a Multi-granularity Contrastive Training framework, to perform complex instruction alignment without the introduction of external supervision. Experiment results show our method achieves significant improvement on instruction alignment benchmarks, surpassing previous self-improvement methods by a large margin. In the future, we will apply our MuSC on the improvement of other capabilities, such as long-form generation, multi-modal generation, etc."}, {"title": "Limitations", "content": "Our work still has some limitations: 1) Due to time and resource limitation, we did not validate our method on larger models, such as LLaMA-70B. Validation on larger models could help to improve the credibility of our method. 2) We mainly relied on GPT-4 based LLM-as-a-Judge to evaluate the results. Despite it has been verified that GPT-4 based evaluation achieves high correlation with human evaluators (Zheng et al., 2023), incorporating human evaluation would further improve the credibility of our methods. 3) We did not scale our method to PPO-based optimization methods, which are also wildly used in recent alignment practice. The application of our method on traditional RL methods could further improve its utility."}, {"title": "A Implementation Details", "content": "A.1 Token-aware Preference Data Construction\nFor all models that used for preference data construction, we adopt the following prompts presented in Figure 5, 7, 6, 8, 9 and 8. We set the temperate as 0.5 for all steps to ensure diversity. To ensure the data quality, we filter instructions with less than three constraints and more than ten constraints. We also filter preference pairs with the same chosen and rejected responses.\nFor constraint dropout, we set the dropout ratio a to 0.3 to ensure that negative examples are sufficiently negative, meanwhile not deviate too much from the positive sample. We avoid dropout on the first constraint, as it often establishes the foundation for the task, and dropping the first one would make the recombined instruction overly biased.\nA.2 Token-aware Preference Optimization\nOur experiments are based on Llama-Factory (Zheng et al., 2024), and we trained all models on 8 A100-80GB SXM GPUs. The per_device_train_batch_size was set to 1, gradient_accumulation_steps to 8, leading to an overal batch size as 64, and we used bfloat16 precision. The learning rate is set as 1e-06 with cosine decay,and each model is trained with 2 epochs. We set \u03b2 to 0.2 for all DPO-based experiments, \u03b2 as 3.0 and y as 1.0 for all SimPO-based experiments, \u03b2 as 1.0 for all IPO-based methods referring to the settings of Meng et al. (2024). All of the final loss includes 0.1x of the SFT loss.\nB The Influence of Noising Scheme\nPrevious work has proposed various noising strategies in contrastive training (Lai et al., 2021). While we leverage Constraint-Dropout for negative sample generation, to make a fair comparison with other strategies, we implement the following strategies: 1) Constraint-Negate: Leverage the model to generate an opposite constraint. 2) Constraint-Substitute: Substitute the constraint with an unrelated constraint.\nAs shown in Table 6, both the negation and substitution applied on the constraints would lead to performance degradation. After a thoroughly inspect of the derived data, we realize that instructions derived from both dropout and negation would lead to instructions too far from the positive instruction, therefore the derived negative response would also deviate too much from the original instruction. An effective negative sample should fulfill both negativity, consistency and contrastiveness, and constrait-dropout is a simple yet effective method to achieve this goal.\nWe also provide the variation of the results on CF-Bench and AlpacaEval2 with different constraint dropout ratios. As shown in Figure 4, with the dropout ratio increased from 0.1 to 0.5, the results on CF-Bench firstly increases and then slightly decreases. On the other hand, the results on AlpacaEval2 declines a lot with a higher dropout ratio. This denotes that a suboptimal droout ratio is essential for the balance between complex instruction and general instruction following abilities, with lower ratio may decrease the effectiveness of general instruction alignment, while higher ratio may be harmful for complex instruction alignment. Finally, we set the constraint dropout ratio as 0.3 in all experiments."}, {"title": "C Mathematical Derivations", "content": "C.1 Preliminary: DPO in the Token Level Marcov Decision Process\nAs demonstrated in Rafailov et al. (2024a), the Bradley-Terry preference model in token-level Marcov Decision Process (MDP) is:\n$P^* (\\tau^w \\succ \\tau^l) = \\frac{exp(\\sum_{i=1}^N r(s_i^w, a_i^w))}{exp(\\sum_{i=1}^N r(s_i^w, a_i^w)) + exp(\\sum_{i=1}^M r(s_i^l, a_i^l))}$\nThe formula using the Q-function to measure the relationship between the current timestep and future returns:\n$Q^* (s_t, a_t) = \\begin{cases} r(s_t, a_t) + \\beta log \\pi_{ref}(a_t|s_t) + V^*(s_{t+1}), & \\text{if } s_{t+1} \\text{ is not terminal} \\\\r(s_t, a_t) + \\beta log \\pi_{ref}(a_t|s_t), & \\text{if } s_{t+1} \\text{ is terminal} \\end{cases}$\nDerive the total reward obtained along the entire trajectory based on the above definitions:\n$\\sum_{t=0}^{T-1} r(s_t, a_t) = \\sum_{t=0}^{T-1} (Q^* (s_t, a_t) - \\beta log \\pi_{ref}(a_t|s_t) - V^*(s_{t+1}))$\nCombining this with the fixed point solution of the optimal policy (Ziebart, 2010; Levine, 2018), we can further derive:\n$\\sum_{t=0}^{T-1} r(s_t, a_t) = Q^*(s_0, a_0) - \\beta log \\pi_{ref}(a_0|s_0) + \\sum_{t=1}^{T-1}(Q^* (s_t, a_t) - V^*(s_{t}) - \\beta log \\pi_{ref}(a_t|s_t))$\n$= Q^*(s_0, a_0) - \\beta log \\pi_{ref}(a_0|s_0) + \\sum_{t=1}^{T-1} \\beta log \\frac{\\pi^*(a_t|s_t)}{\\pi_{ref}(a_t|s_t)}$\n$= V^*(s_0) + \\sum_{t=0}^{T-1} \\beta log \\frac{\\pi^*(a_t|s_t)}{\\pi_{ref}(a_t|s_t)}$\nBy substituting the above result into Eq. 5, we can eliminate V*(So) in the same way as removing the partition function in DPO, obtaining the Token-level BT model that conforms to the MDP:\n$P_{\\pi^*}(\\tau^w \\succ \\tau^l) = \\sigma(\\beta (\\sum_{t=0}^{N-1}log \\frac{\\pi^*(a_t|s_t)}{\\pi_{ref}(a_t|s_t)} - \\sum_{t=0}^{M-1}log \\frac{\\pi^*(a_t|s_t)}{\\pi_{ref}(a_t|s_t)}))$\nThus, the Loss formulation of DPO at the Token level is:\n$L (\\pi_{\\theta}, D) = -E_{(\\tau_w,\\tau_l)\\sim D} log \\sigma(\\beta (\\sum_{t=0}^{N-1}log \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{ref}(a_t|s_t)} - \\sum_{t=0}^{M-1}log \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{ref}(a_t|s_t)}))$"}, {"title": "C.2 Proof of Dynamic Token Weight in Token-level DPO", "content": "In classic RLHF methods", "follows": "n$max_{\\pi_{\\theta"}]}, {"policy": "n$max_{\\pi_{\\theta"}, {"to": "n$max_{\\pi_{\\theta"}, {"explicitly": "n$max_{\\pi_{\\theta"}, {"as": "n$V(s_t) = \\beta_t log \\int [exp \\frac{r(s_t", "satisfies": "n$\\pi_{\\theta}(a_t|s_t) = \\frac{1}{exp("}]