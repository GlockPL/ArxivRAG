{"title": "MuSC: Improving Complex Instruction Following with Multi-granularity Self-Contrastive Training", "authors": ["Hui Huang", "Jiaheng Liu", "Yancheng He", "Shilong Li", "Bing Xu", "Conghui Zhu", "Muyun Yang", "Tiejun Zhao"], "abstract": "Complex instruction-following with elaborate constraints is imperative for Large Language Models (LLMs). While existing methods have constructed data for complex instruction alignment, they all rely on a more advanced model, especially GPT-4, limiting their application. In this paper, we propose a Multi-granularity Self-Contrastive Training (MuSC) framework, to improve the complex instruction alignment without relying on a stronger model. Our method is conducted on both coarse and fine granularity. On coarse-granularity, we construct constraint-aware preference data based on instruction decomposition and recombination. On fine-granularity, we perform token-aware preference optimization with dynamic token-level supervision. Our method is evaluated on open-sourced models, and experiment results show our method achieves significant improvement on both complex and general instruction-following benchmarks, surpassing previous self-alignment methods.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have made remarkable advancements and are being wildly applied across various domains (Zhao et al., 2024; Minaee et al., 2024; Hadi et al., 2024; Yin et al., 2024; Wang et al., 2024a). The instruction-following ability is fundamental and important, as it enables LLMs to generate appropriate responses to given instructions and solve corresponding tasks (OpenAI et al., 2024). While recent LLMs perform comparatively well on simple instructions, their response quality to complex instructions with elaborate constraints often falls under expectation, with some of the constraints omitted (He et al., 2024; Jiang et al., 2024b), which hinders their application in more real-world complex scenarios.\nTo enhance the complex instruction following, the core challenge is the scarcity of high-quality complex instruction data (Lou et al., 2024). Most existing instruction datasets are constructed based on existing NLP datasets or question-answering websites with simple constraints (Wang et al., 2023; Taori et al., 2023; Lian et al., 2023; Longpre et al., 2023). To cope with the scarcity of complex instruction data, previous work such as Evol-Instruct (Xu et al., 2024), Conifer (Sun et al., 2024a), and Self-Correct (Palmeira Ferraz et al., 2024) have been proposed to construct complex instructions and responses. However, these methods typically rely on a high-performance proprietary model (e.g., GPT-4) to distill the complex instruction-following ability, which is expensive and can not be scaled up in real-world applications.\nRecently, the research community has paid attention to self-alignment, to break the data bottleneck without relying on a stronger model (Wang et al., 2024b; Zhang et al., 2024a). Self-Reward (Yuan et al., 2024) proposes to utilize the model itself to both generate responses and evaluate the results, which can be incorporated into DPO training. ISHEEP (Liang et al., 2024) proposes an automatic loop to self-assess and self-filter instruction data. Despite their effectiveness, these methods are targeted at general instruction-following ability. The self-alignment of complex instruction-following ability remains unexplored.\nIn this paper, to address the above limitations, we propose a novel Multi-granularity Self-Contrastive Training framework (MuSC) in Figure 1, which mainly comprises the following components:\n1) Coarse-grained Contrast: Constraint-aware Preference Data Construction. To improve the model's comprehension of constraint-level distinctions, we construct preference pairs that reflect the disparities in constraint fulfillment. We achieve this by breaking down each complex instruction into atomic constraints and selectively omitting a subset to form negative instructions. The chosen response, derived from the original instruction, is paired with the rejected response, generated from the negative instruction, as a contrastive pair. Notably, no external models are utilized in this construction process.\n2) Fine-grained Contrast: Token-aware Preference Optimization. For complex instructions, the responses often involve multiple tokens that contribute differently to fulfilling the instruction's constraints. Therefore, we introduce a token-aware optimization framework that integrates dynamic token-level weights based on the model's confidence. By focusing on tokens that deviate from the constraints, this approach effectively identifies and corrects tokens where the model fails to satisfy the instruction's requirements, leading to more contextually appropriate responses.\nMoreover, we need to mention that our MuSC can be applied on both pre-existing complex instruction datasets, or newly generated instruction datasets created by data synthesis methods (e.g., Self-Instruct (Wang et al., 2022)).\nOur contribution can be summarized as follows:\n\u2022 We propose a novel Multi-granularity Self-Contrastive Training (MuSC) framework, which creates effective contrast on both coarse and fine granularity, to enhance the complex instruction following abilities.\n\u2022 For coarse-grained contrast, we construct constraint-aware preference data with instruction decomposition-recombination. For fine-grained contrast, we adopt dynamic token-level weight with confidence guidance for better preference optimization.\n\u2022 We evaluate our framework on open-source LLMs, and achieve significant improvements on both complex and general instruction following benchmarks, without the help of a larger model or human supervision."}, {"title": "Related Work", "content": "Complex Instruction-Following. As one of the cores of LLM intelligence, how to improve the model's instruction-following capability is important. The earliest works, such as Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), and Camel (Wang et al., 2023), used instruction data generated by proprietary models to supervise fine-tuning of open-source models, significantly enhancing their instruction-following capabilities. However, these methods mainly focus on general instruction following, while complex instruction following still remains challenging. To cope with this challenge, a lot of methods (Yin et al., 2023; Lou et al., 2023; He et al., 2024; Sun et al., 2024b; Chen et al., 2024b; Dong et al., 2024) have been proposed to construct complex instruction data. The earliest work is Evol-Instruct (Xu et al., 2024), which proposed to utilize GPT-4 to expand the instructions from both depth and width, thereby generating complex instructions and corresponding constraints. Conifer (Sun et al., 2024a) proposed a progressive learning strategy designed to help smaller models incrementally enhance their abilities.\nSelf-Alignment. Self-alignment refers to aligning the model to human preference without relying on a more advanced model or external supervision. As an early study, Self-Rewarding (Yuan et al., 2024) proposed the model itself to both generate responses and evaluate the results. Following this work, many works (Liu et al., 2024; Chen et al., 2024b,a; Pang et al., 2024; Meng et al., 2024) are conducted to obtain supervision data by the model itself. Meta-Rewarding (Wu et al., 2024) advanced the concept by improving the model's instruction and evaluation capabilities simultaneously. Liu et al. (2024) employed diverse prompts to guide the model to generate various responses. Despite the progress these methods have made, they all target general instruction following. For complex instructions with multiple constraints, the response"}, {"title": "Approach", "content": "The pipeline of MuSC is shown in Figure 2.\nConstraint-aware Preference Data\nReinforcement-learning methods, such as PPO (Schulman et al., 2017) and DPO (Rafailov et al., 2024b), have achieved notable success in LLM optimization. Research has shown that learning from negative samples is significantly more efficient than learning solely from positive samples (Yang et al., 2024b). However, these methods are limited by the need for high-quality preference data, which is particularly scarce for complex instructions.\nTo construct effective preference data for complex instruction following, we propose a novel data construction method, with the following steps:\n1. Instruction Decomposition: A complex instruction is typically a combination of multiple atomic constraints. We decompose the complex instruction into individual atomic constraints, denoted as Cons.\n2. Constraint Dropout: From the decomposed constraints Cons, we randomly eliminate a% of the constraints to form Consdrop.\n3. Instruction Recombination: We recombine both the original and the dropped constraints Cons and Consdrop, to create chosen and rejected instructions: Ins and Insdrop.\n4. Response Generation: Based on Ins and Insdrop, we generate the chosen response Resp and the rejected response Respdrop.\nPrevious research has suggested that the construction of effective preference pairs for optimization is non-trivial (Ivison et al., 2024). Our data construction pipeline is guided by three principles:\n\u2022 Negativity: The rejected response should deviate from the instruction by omitting some constraints. Our method generates the rejected instruction based on corrupted constraints, ensuring that the rejected response deviates from the original complex instruction.\n\u2022 Consistency: The rejected response should reside within the model's decoding space (Guo et al., 2024). In our method, the rejected instruction is simply a recombination of the orig-\nToken-aware Preference Optimization\nA well-known issue with DPO is its uniform treatment of all tokens in both chosen and rejected examples (Wu et al., 2023; Cao et al., 2024; Li et al., 2024). However, different tokens within responses carry varying significance. Especially in scenarios involving complex instructions, the responses tend to be lengthy and multi-facet. On one hand, not all tokens in the rejected response are erroneous and should be disapproved. On the other hand, chosen response may also contain tokens that fail to meet specific constraints, therefore should not be unanimously approved.\nDespite previous researchers have explored fine-grained supervision signals, the signals either come from a stronger model (Cao et al., 2024; Li et al., 2024) or human annotation (Wu et al., 2023; Lightman et al., 2023). However, in our case, it is difficult for the model to provide accurate supervision for its own response, especially when dealing with multifaceted instructions and the evaluation is at token-level. Therefore, we propose Confidence-Guided Token-aware DPO, which obtains token-level supervision based on model confidence.\nPreliminary: Token-level DPO\nDirect Preference Optimization (DPO) (Rafailov et al., 2024b) proposes a direct optimization ob-"}, {"title": "Preliminary: Token-level DPO", "content": "jective that satisfies the optimal preference policy without using a reward model:\n$L_{DPO}(\\pi_{\\theta}; \\pi_{ref}) =\\mathbb{E}_{(x, y_w, y^l) \\sim D}\\left[-\\log \\sigma\\left(\\beta\\log \\frac{\\pi_{\\theta}(y_w | x)}{\\pi_{ref}(y_w | x)} - \\beta\\log \\frac{\\pi_{\\theta}(y^l | x)}{\\pi_{ref}(y^l | x)}\\right)\\right]$\n(1)\nwhere $\\pi_{\\theta}$ and $\\pi_{ref}$ represent the policy model and the reference model, respectively.\nSubsequently, based on the theories of Levine (2018), Rafailov et al. (2024a) derived the form of DPO in token-level Markov Decision Process, where dynamic weight can be easily integrated for different tokens, with the loss function as follows:\n$L_{TDPO}(\\pi_{\\theta}, D) =\\newline  -\\mathbb{E}_{(\\tau^w, \\tau^l) \\sim D}\\log \\sigma(\\beta\\sum_{t=0}^{N-1}r_t \\log\\frac{\\pi_{\\theta}(a_t^w | s_t^w)}{\\pi_{ref}(a_t^w | s_t^w)} -\\beta\\sum_{t=0}^{M-1}r_t \\log\\frac{\\pi_{\\theta}(a_t^l | s_t^l)}{\\pi_{ref}(a_t^l | s_t^l)})$\n(2)\nwhere $\\tau^w$ and $\\tau^l$ represent the winning and losing trajectories, with N and M as the token numbers, and $r_t$ represents the weight for the t-th token.\nCalibrated Confidence as Token Weight\nWhile Section 3.2.1 provide theoretical support for token-level DPO, it is non trivial to derive token-level supervision. In this work, we propose to use the calibrated confidence as supervision.\nGiven an instruction x, we obtain the entropy of probability distribution over target vocabulary of size V at each decoding step as the weights:\n$Ent(y_t|x, \\theta) = \\sum_{v=1}^{V} p(y_v)logp(y_v),$ (3)\nwhere $p(y_t)$ represents the conditional distribution $p(y_t|x, y_{<t}, \\theta)$, and $\\theta$ represents model parameters. If the majority of the probability mass is concentrated on a limited number of vocabulary words, it indicates that the model is confident and the token is more likely to be aligned with the instruction (Fomicheva et al., 2020). Conversely, if the probabilities resemble a uniform distribution, the resulting token is expected to be misaligned.\nWhile there are other attributes that could also impact the confidence score (such as fluency), in this work, we want to focus our optimization on"}, {"title": "Calibrated Confidence as Token Weight", "content": "instruction alignment. Therefore, we apply calibration to the entropy to derive the token-level supervision for chosen and rejected samples respectively:\n$r_t =\\begin{cases}\\newline    Ent(y_t|x^w, \\theta) / Ent(y_t|x^l, \\theta), & \\text{for } y_t \\text{ in } y^w, \\newline   \\Gamma Ent(y_t|x^l, \\theta) / Ent(y_t|x^w, \\theta), & \\text{for } y_t \\text{ in } y^l,\\newline\\end{cases}$\n$r_t = min(\\Gamma, r_t),$\n(4)\nwhere the chosen sample $(x^w, y^w)$ refers to (Ins, Resp), and the rejected sample $(x^l, y^l)$ refers to (Insdrop, Respdrop), and $\\Gamma$ is an upper-bound to avoid extreme cases to disrupt training, and we set $\\Gamma$ as 2 in this work.\nThe rationale is straightforward: for a given token in the response, if it exhibits high confidence under $x^w$ and low confidence under $x^l$, this suggests that the token aligns well with $x$ but does not fit $x^l$, potentially reflecting the dropped constraint. Therefore, the token requires a larger weight if it is in the rejected response (or a smaller weight if it is in the chosen response).\nCalibrated confidence guided token-aware DPO allows for more targeted optimization, focusing on tokens that highlight the constraint mismatch on complex instructions, instead of unanimously optimizing all tokens, thereby improving the efficiency of complex instruction alignment."}, {"title": "Experiments", "content": "Set-up\nModels. We conduct experiments on two models: LLaMA-3-8B-Instruct (Dubey et al., 2024) and Qwen2-7B-Instruct (Yang et al., 2024a). Both models have undergone alignment to possess fundamental instruction-following ability.\nSetting. The experiments are carried out in two distinct settings:\n1. Pre-Existing Instructions (PreInst): We leverage pre-existing complex instructions as a starting point for the model. We randomly select 2,000 instructions from the dataset of WizardLM (Xu et al., 2024).\n2. Self-Generated Instructions (SelfInst): In this setting, we generate instructions using the Self-Instruct method (Wang et al., 2022), based on 10 high-quality samples from Qin et al. (2024) as in-context examples. Compared with PreInst, this setting is more challenging as we need to construct the complex instructions from scratch.\nEvaluation. We mainly perform evaluations on three complex instruction-following benchmarks: CFBench (Zhang et al., 2024b), Follow-Bench (Jiang et al., 2024b) and ComplexBench (He et al., 2024). We also conduct evaluations on one general instruction benchmark: AlpacaEval2 (Dubois et al., 2024). Note that all benchmarks require GPT-4 for judgment, and we use GPT-4o-05134 as the evaluator for all of them.\nBaselines. We mainly compared our method against the following self-alignment methods:\n\u2022 Self-Reward (Yuan et al., 2024): This method leverages the model to first generate multiple responses and then construct rewards.\n\u2022 Self-Reward + BSM: Based on Self-Reward, this method performs fine-grained evaluation based on BSM (Saha et al., 2024).\n\u2022 Self-Correct (Palmeira Ferraz et al., 2024): This method generates initial output and then corrects it to construct preference data.\n\u2022 ISHEEP (Liang et al., 2024): This method self-creates additional instruction-output pair, which are filtered for supervised fine-tuning."}, {"title": "Main Results", "content": "As demonstrated in Table 1, our proposed MuSC achieves significant improvement across both complex and general instruction-following benchmarks. The improvement is consistent among different settings, verifying its scalability. By creating preference data with both constraint-aware and token-aware contrast, the model effectively learns to address all constraints lying in the instructions.\nThe results of Self-Reward underperform our method, even with the help of branched evaluation (Saha et al., 2024). This is because of the limited evaluation capability of the model, especially when evaluating its own response to complex instructions. Moreover, as different responses generated from the same model to the same instruction typically do not vary significantly, it is difficult to create effective contrast samples with real negativity.\nThe improvement of I-SHEEP also underperforms, likely due to its reliance on supervised fine-tuning for optimization. Previous research also suggests that learning from negative samples is more effective than learning solely from positive ones (Yang et al., 2024b). The results of Self-Correct degrades a large margin, which might be due to"}, {"title": "Analysis", "content": "MuSC on SFT model\nIn Section 4, our main experiments are conducted on the Instruction-version models. To exclude the influence of an initial preference optimization process, we apply our method on SFT models.\nSpecifically, we selected two SFT-versions of LLaMA models, LLaMA-3-8B-UltraChat-200K\nThe Influence of Confidence Metrics\nVarious confidence metrics have been established in the domain of LLM (Geng et al., 2024). This sec-"}, {"title": "The Influence of Confidence Metrics", "content": "tion aims to provide a comparison across different metrics as the token weight, under the framework"}, {"title": "Is Fundamental Capability Harmed?", "content": "Previous research have proposed that during the alignment process, the fundamental ability of model may suffer degradation due to alignment tax (Ouyang et al., 2022). Therefore, we evaluated our proposed method on three fundamental capability benchmarks: MMLU (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021) and HumanEval (Chen et al., 2021).\nAs shown in Table 5, while the results of naive MuSC may suffer slight degradation, the introduction of fine-grained contrast mitigates the degradation, which verifies the significance of token-level supervision. Under the scenario of complex instruction, the response is lengthy and should not be uniformly approved or disapproved. With fine-grained supervision, we focus the optimization on complex instruction alignment, thereby avoiding the disruption of other capabilities."}, {"title": "The Variation of Statistical Indicators", "content": "As different methods start from the same group of instructions, training statistics can be comparable as a quality indicator. Therefore, we display the variation of both loss and reward margins between chosen and rejected samples on different methods.\nAs shown in Figure 3, Self-Reward presents both higher loss and lower reward margin during training. This is because there is too much noise between the chosen and rejected pairs, making the model unable to capture the contrast related to constraint alignment. Notice that both indicators"}, {"title": "Conclusion", "content": "In this work, we propose a Multi-granularity Contrastive Training framework, to perform complex instruction alignment without the introduction of external supervision. Experiment results show our method achieves significant improvement on instruction alignment benchmarks, surpassing previous self-improvement methods by a large margin.\nIn the future, we will apply our MuSC on the improvement of other capabilities, such as long-form generation, multi-modal generation, etc."}, {"title": "Limitations", "content": "Our work still has some limitations: 1) Due to time and resource limitation, we did not validate our method on larger models, such as LLaMA-70B. Validation on larger models could help to improve the credibility of our method. 2) We mainly relied on GPT-4 based LLM-as-a-Judge to evaluate the results. Despite it has been verified that GPT-4 based evaluation achieves high correlation with human evaluators (Zheng et al., 2023), incorporating human evaluation would further improve the credibility of our methods. 3) We did not scale our method to PPO-based optimization methods, which are also wildly used in recent alignment practice. The application of our method on traditional RL methods could further improve its utility."}, {"title": "Implementation Details", "content": "Token-aware Preference Data Construction\nFor all models that used for preference data construction, we adopt the following prompts presented in Figure 5, 7, 6, 8, 9 and 8. We set the temperate as 0.5 for all steps to ensure diversity. To ensure the data quality, we filter instructions with less than three constraints and more than ten constraints. We also filter preference pairs with the same chosen and rejected responses.\nFor constraint dropout, we set the dropout ratio a to 0.3 to ensure that negative examples are sufficiently negative, meanwhile not deviate too much from the positive sample. We avoid dropout on the first constraint, as it often establishes the foundation for the task, and dropping the first one would make the recombined instruction overly biased.\nToken-aware Preference Optimization\nOur experiments are based on Llama-Factory (Zheng et al., 2024), and we trained all models on 8 A100-80GB SXM GPUs. The per_device_train_batch_size was set to 1, gradient_accumulation_steps to 8, leading to an overal batch size as 64, and we used bfloat16 precision. The learning rate is set as 1e-06 with cosine decay,and each model is trained with 2 epochs. We set \u03b2 to 0.2 for all DPO-based experiments, \u03b2 as 3.0 and y as 1.0 for all SimPO-based experiments, \u03b2 as 1.0 for all IPO-based methods referring to the settings of Meng et al. (2024). All of the final loss includes 0.1x of the SFT loss.\nThe Influence of Noising Scheme\nPrevious work has proposed various noising strategies in contrastive training (Lai et al., 2021). While we leverage Constraint-Dropout for negative sample generation, to make a fair comparison with other strategies, we implement the following strategies: 1) Constraint-Negate: Leverage the model to generate an opposite constraint. 2) Constraint-Substitute: Substitute the constraint with an unrelated constraint."}, {"title": "Mathematical Derivations", "content": "Preliminary: DPO in the Token Level Marcov Decision Process\nAs demonstrated in Rafailov et al. (2024a), the Bradley-Terry preference model in token-level Marcov Decision Process (MDP) is:\n$P^* (\\tau^w \\succ \\tau^l) = \\frac{\\exp\\left( \\sum_{i=1}^{N} r(s_i^w, a_i^w) \\right)}{\\exp\\left(\\sum_{i=1}^{N} r(s_i^w, a_i^w) \\right) + \\exp\\left( \\sum_{i=1}^{M} r(s_i^l, a_i^l) \\right)}$\n(5)\nThe formula using the Q-function to measure the relationship between the current timestep and future returns:\n$Q^* (s_t, a_t) = \\begin{cases}r(s_t, a_t) + \\beta\\log \\pi_{ref}(a_t|s_t) + V^*(s_{t+1}), & \\text{if } s_{t+1} \\text{ is not terminal}\\\\r(s_t, a_t) + \\beta\\log \\pi_{ref}(a_t|s_t), & \\text{if } s_{t+1} \\text{ is terminal}\\end{cases}$\n(6)\nDerive the total reward obtained along the entire trajectory based on the above definitions:\n$\\sum_{t=0}^{T-1} r(s_t, a_t) = \\sum_{t=0}^{T-1}(Q^* (s_t, a_t) - \\beta\\log \\pi_{ref}(a_t|s_t) - V^*(s_{t+1}))$\n(7)\nCombining this with the fixed point solution of the optimal policy (Ziebart, 2010; Levine, 2018), we can further derive:\n$\\sum_{t=0}^{T-1} r(s_t, a_t) = Q^*(s_0, a_0) - \\beta\\log \\pi_{ref} (a_0|s_0) + \\sum_{t=1}^{T-1}(Q^* (s_t, a_t) - V^*(s_t) - \\beta\\log \\pi_{ref}(a_t|s_t))$\n(8)\n$= Q^*(s_0, a_0) - \\beta\\log \\pi_{ref} (a_0|s_0) + \\sum_{t=1}^{T-1}\\beta\\log \\frac{\\pi^*(a_t|s_t)}{\\pi_{ref}(a_t|s_t)}$\n(9)\n$= V^*(s_0) + \\sum_{t=0}^{T-1}\\beta\\log \\frac{\\pi^*(a_t|s_t)}{\\pi_{ref}(a_t|s_t)}$\n(10)\nBy substituting the above result into Eq. 5, we can eliminate V*(So) in the same way as removing the partition function in DPO, obtaining the Token-level BT model that conforms to the MDP:\n$P_{\\pi^*}(\\tau^w \\succ \\tau^l) = \\sigma\\left( \\sum_{t=0}^{N-1} \\beta\\log \\frac{\\pi^*(a_t^w | s_t^w)}{\\pi_{ref}(a_t^w | s_t^w)} - \\sum_{t=0}^{M-1} \\beta\\log \\frac{\\pi^*(a_t^l | s_t^l)}{\\pi_{ref}(a_t^l | s_t^l)}\\right)$\n(11)\nThus, the Loss formulation of DPO at the Token level is:\n$\\mathcal{L} (\\pi_{\\theta}, D) = -\\mathbb{E}_{(\\tau^w, \\tau^l)\\sim D} \\log \\sigma \\left( \\left( \\sum_{t=0}^{N-1} \\beta\\log \\frac{\\pi^*(a_t^w | s_t^w)}{\\pi_{ref}(a_t^w | s_t^w)} \\right) - \\left( \\sum_{t=0}^{M-1} \\beta\\log \\frac{\\pi^*(a_t^l | s_t^l)}{\\pi_{ref}(a_t^l | s_t^l)} \\right) \\right)$\n(12)"}, {"title": "Proof of Dynamic Token Weight in Token-level DPO", "content": "In classic RLHF methods, the optimization objective is typically formulated with an entropy bonus, expressed through a Kullback\u2013Leibler (KL) divergence constraint as follows:\n$\\max_{\\pi_{\\theta}} \\mathbb{E}_{a_{t}\\sim\\pi_{\\theta} (s_{t})} [r(s_{t}, a_{t}) \u2013 \\beta D_{KL}[\\pi_{\\theta} (a_{t}|s_{t})||\\pi_{ref}(a_{t}|s_{t})]]$\n(13)\n$\\mathop{\\text{max}}_{\\pi_{\\theta}} \\mathbb{E}_{a_{t}\\sim\\pi_{\\theta} (s_{t})} [r(s_{t}, a_{t}) - \\beta \\log \\frac{\\pi_{\\theta} (a_{t}|s_{t})}{\\pi_{ref}(a_{t}|s_{t})}]$\n(14)\nThis can be further rewritten by separating the terms involving the reference policy and the entropy of the current policy:\n$\\mathop{\\text{max}}_{\\pi_{\\theta}} \\mathbb{E}_{a_{t}\\sim\\pi_{\\theta} (s_{t})} [\\sum(r(s_{t}, a_{t}) + \\beta \\log \\pi_{ref} (a_{t}|s_{t})) + \\beta H(\\pi_{\\theta})|S_{0} \\thicksim p(S_{0})]$\nWhere the coefficient $\\beta$ is treated as a variable that depends on the timestep t (Li et al., 2024), the objective transforms to:\n$\\mathop{\\text{max}}_{\\pi_{\\theta}} \\mathbb{E}_{a_{t}\\sim\\pi_{\\theta} (s_{t})} [\\sum(r(s_{t}, a_{t}) + \\beta_{t} \\log \\pi_{ref} (a_{t}|s_{t})) \u2013 \\beta_{t} \\log \\pi_{\\theta}(a_{t}|s_{t})]$\n(15)\nwhere $\\beta_{t}$ depends solely on $a_{t}$ and $s_{t}$. Following the formulation by Levine (2018), the above expression can be recast to incorporate the KL divergence explicitly:\n$\\mathop{\\text{max}}_{\\pi_{\\theta}} \\mathbb{E}_{a_{t}\\sim\\pi_{\\theta} (s_{t})} [\\sum(r(s_{t}, a_{t}) + \\beta_{t} \\log \\pi_{ref} (a_{t}|s_{t})) \u2013 \\beta_{t} \\log \\pi_{\\theta}(a_{t}|s_{t})]$\n(16)\nwhere the value function V($s_{t}$) is defined as:\nV($s_{t}$) = $\\beta_{t}$ \\log [$\\int exp_{\\frac{r(s_{t}, a_{t})}{\\beta_{t}}} \\pi_{ref}(a_{t}|s_{t})] da_{t}$\n(17)\nWhen the KL divergence term is minimized\u2014implying that the two distributions are identical\u2014the expectation in Eq. (14) reaches its maximum value. Therefore, the optimal policy satisfies:\n$\\pi_{\\theta}(a_{t} s_{t}) = \\frac{1}{exp(V(s_{t}))} exp( \\frac{r(s_{t}, a_{t}) + \\beta_{t} \\log \\pi_{ref}(a_{t}|s_{t})}{\\beta_{t}}$\n(18)\nBased on this relationship, we define the optimal Q-function as:\n$Q^*(s_{t}, a_{t}) = \\begin{cases}r(s_{t}, a_{t}) + \\beta_{t} \\log \\pi_{ref}(a_{t}|s_{t}) + V^*(s_{t+1}), if s_{t+1} is not terminal\\r(s_{t}, a_{t}) + \\beta_{t} \\log \\pi_{ref}(a_{t}|s_{t}), if s_{t+1} is terminal\\end{cases}$\n(19)\nConsequently, the optimal policy can be expressed as:\n$\\pi_{\\theta}(a_{t} s_{t}) = e^{(Q(s_{t},a_{t})-V(s_{t}))/\\beta_{t}}$\n(20)\nBy taking the natural logarithm of both sides, we obtain a log-linear relationship for the optimal policy at the token level, which is expressed with the optimial Q-function:\n$\\beta_{t} \\log \\pi_{\\theta}(a_{t} | s_{t}) = Q_{e}(s_{t}, a_{t}) \u2013 V_{e}(s_{t})$\n(21)"}, {"title": "Proof of Dynamic Token Weight in Token-level DPO", "content": "This equation establishes a direct relationship between the scaled log-ratio of the optimal policy to the reference policy and the reward function r($s_{t}$, $a_{t}$):\n$\\beta_{t} \\log \\frac{\\pi*(a_{t} | s_{t})}{\\pi_{ref}(a_{t} | s_{t})} = r(s_{t}, a_{t}) + V^*(S_{t+1}) \u2013 V^*(s_{t})$\n(22)\nFurthermore, following the definition by Rafailov et al. (2024a)'s definition, two reward functions r($s_{t}$, $a_{t}$) and r' ($s_{t}$, $a_{t}$) are considered equivalent if there exists a potential function $\\Phi$(s), such that:\nr'(st, at) = r(st, at) + \u03a6(St+1) \u2013 \u03a6(St)\n(23)\nThis equivalence implies that the optimal advantage function remains invariant under such transformations of the reward function. Consequently, we derive why the coefficient beta in direct preference optimization can be variable, depending on the state and action, thereby allowing for more flexible and adaptive policy optimization in RLHF frameworks.\nDetailed Experiment Results\nIn this section, we presented detailed experiment results which are omitted in the main body of this paper due to space limitation. The detailed experiment results of different methods on ComplexBench, FollowBench and AlpacaEval2 are presented in Table 7, 9 and 8. The detailed results for the ablative studies of confidence metrics is presented in Table 10. The detailed results for the ablative studies of confidence metrics is presented in Table 6. We also present a case study in Table 11, which visualize the token-level weight derived from calibrated confidence score."}]}