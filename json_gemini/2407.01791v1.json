{"title": "\u03bc-BENCH: VISION-LANGUAGE BENCHMARK FOR MICROSCOPY UNDERSTANDING", "authors": ["Alejandro Lozano", "Jeffrey Nirschl", "James Burgess", "Sanket Rajan Gupte", "Yuhui Zhang", "Alyssa Unell", "Serena Yeung-Levy"], "abstract": "Recent advances in microscopy have enabled the rapid generation of terabytes of image data in cell biology and biomedical research. Vision-language models (VLMs) offer a promising solution for large-scale biological image analysis, enhancing researchers' efficiency, identifying new image biomarkers, and accelerating hypothesis generation and scientific discovery. However, there is a lack of standardized, diverse, and large-scale vision-language benchmarks to evaluate VLMs' perception and cognition capabilities in biological image understanding. To address this gap, we introduce \u00b5-Bench, an expert-curated benchmark encompassing 22 biomedical tasks across various scientific disciplines (biology, pathology), microscopy modalities (electron, fluorescence, light), scales (subcellular, cellular, tissue), and organisms in both normal and abnormal states. We evaluate state-of-the-art biomedical, pathology, and general VLMs on \u00b5-Bench and find that: i) current models struggle on all categories, even for basic tasks such as distinguishing microscopy modalities; ii) current specialist models fine-tuned on biomedical data often perform worse than generalist models; iii) fine-tuning in specific microscopy domains can cause catastrophic forgetting, eroding prior biomedical knowledge encoded in their base model. iv) weight interpolation between fine-tuned and pre-trained models offers one solution to forgetting and improves general performance across biomedical tasks. We release \u00b5-Bench under a permissive license to accelerate the research and development of microscopy foundation models.", "sections": [{"title": "1 Introduction", "content": "Microscopy is a cornerstone of biomedical research [1], enabling detailed study of biological structures at multiple scales [2]. Advances in cryo-electron microscopy, high-throughput fluorescence microscopy, and whole-slide imaging allow the rapid generation of terabytes of image data, which are essential for fields such as cell biology, biomedical research, and pathology [3]. These data span multiple length scales, allowing researchers to examine atomic/molecular, subcellular/cellular, and cell/tissue-level structures with high precision [4]. A crucial first step in microscopy analysis is interpreting and reasoning about the significance of image findings [5]. However, this requires domain expertise and comprehensive knowledge of biology, normal/abnormal states, and the capabilities and limitations of microscopy techniques. The increased volume and velocity of microscopy data compound the challenge of manual microscopy image interpretation.\nText is an intuitive interface for interactive analysis, and thus, vision-language models (VLMs) are one promising approach to assist with image interpretation. Auto-regressive VLMs can follow instructions and respond to questions using free text, which is intuitive for users without a computational background. A biomedical VLM that has distilled knowledge from diverse microscopy images with input from multiple domain experts could also democratize image interpretation for specialized techniques. In addition, they could relate image findings to recent literature, relevant genes, small molecule therapeutics, and diseases and potentially accelerate hypothesis generation and discovery [6, 7, 8, 9, 10, 11, 12, 12, 13, 14].\nFoundation VLMs trained on large corpora of image-text data have revolutionized many areas of artificial intelligence (AI), excelling in a variety of natural language processing (NLP) [15, 16], computer-vision (CV) [17, 18], and vision-language [19, 20, 21] tasks. It is well known that biomedical data are significantly under-represented in internet-based image datasets, and a critical first question is how well generalist VLMs trained on natural image-text data perform on out-of-domain biomedical vision-language tasks.\nHowever, there are no diverse, large-scale vision-language benchmarks to evaluate generalist or specialist VLMs in microscopy image interpretation. Whereas segmentation benchmarks are common[22], and recent efforts have curated impressive image-drug phenotype multimodal datasets[23], the current state of microscopy vision-language benchmarks is limited. Existing microscopy vision-language benchmarks often focus on single-domain diagnostic capabilities (e.g., histopathology [24, 25]) rather than describing and understanding [24]. Although these are significant contributions, they lack the diversity of images and content needed to evaluate the performance of VLMS across tasks, scales, microscopy modalities, organisms, and biological processes. Lastly, in contrast to the general vision-language domain, current biomedical evaluations lack comprehensive characterization across both perception (recognizing specific objects, counting, localization, and color) and cognition abilities (integrating knowledge and perceptual to deduce more complex answers) [26]. This gap hinders the progress of developing robust VLMs tailored for biomedical research.\nTo address the need for robust VLMs tailored for biomedical research, we present two contributions:\n1. \u00b5-Bench A high-quality vision-language benchmark of 17,235 microscopy images from a diverse collection of unpublished and published datasets with newly added expert annotations and a permissive license. \u00b5-Bench spans 22 perception and cognition tasks across light, fluorescence, and electron microscopy. It includes closed VQA, captioning, object detection, and segmentation tasks across 8 microscopy sub-modalities and 24 staining techniques, representative of 12 scientific domains."}, {"title": "2 Related Work", "content": "Biomedical Vision-Language Benchmarks. While previous works have developed various biomedical vision-language benchmarks that have been instrumental in advancing diagnostic capabilities, they present three main problems: 1) Task simplicity: Most biomedical computer vision benchmarks predominantly focus on supervised tasks such as classification and segmentation [27, 28, 29] rather than vision-language tasks. As shown in Table 8, biomedical vision-language benchmarks are less common. 2) Lack of diversity: existing vision-language datasets are usually limited to diagnostic imaging such as radiology or pathology [30, 31]; there is a lack of benchmarks for basic research microscopy. 3) Limited accessibility: PMC-15M is a large image-text dataset used to train BiomedCLIP [3], which includes diverse microscopy images, domains, and organisms but is not publicly accessible for training or evaluation.\nVision-Language Models in Biomedicine. Vision-language models (VLMs) can be generally categorized into two types: 1) Contrastive models, such as CLIP [19] and ALIGN [32], which use contrastive learning to create shared image-text embeddings, facilitating tasks like zero-shot classification and text-image retrieval; and 2) Auto-regressive models, such as Flamingo [20] and GPT-4 [33], which integrate image embeddings with large language models (LLMs) to perform zero-shot tasks, follow instructions, and reason about content. These VLMs have significant potential to advance biomedicine [34, 5, 6, 7, 35].\nHowever, these models are primarily trained on general datasets with limited biomedical coverage, leading to suboptimal performance on biomedical tasks [36, 37]. To address this gap, specialized VLMs have been developed by fine-tuning generalist models on biomedical data. Notable examples include BiomedCLIP [3] train on images from PubMed, and histopathology vision-language models such as PLIP [38] and CONCH [39], which were trained on Twitter or various pathology sources. Despite the increase in vision-language foundation models for microscopy, there are no diverse benchmarks to evaluate the performance of image-based perception and reasoning about microscopy images across diverse scales and modalities. Our work on \u03bc-Bench addresses this issue by providing a comprehensive benchmark that includes a variety of important and diverse biological processes, organisms, microscopy modalities, domains, and tasks to support the evaluation of microscopy foundation models."}, {"title": "3 Dataset collection methodology", "content": "Recognizing the need for an expert-level benchmark in microscopy for comprehensive biological and biomedical understanding, we developed a benchmark to assess the perception and cognition capabilities of VLMs in microscopy image analysis following the methodology shown in Figure 2. At a high level, the pipeline consists of two main components: (i) A biomedical expert categorized potential tasks and collected diverse microscopy datasets across multiple scientific domains, focusing on evaluating perception capabilities. (ii) We then complement \u03bc-Bench by crowdsourcing questions from a larger group of microscopists using a web application."}, {"title": "3.1 Perception Dataset Curation", "content": "Dataset Review and Selection Open data repositories, including Zenodo, Dataverse, Dryad, and BBBC, among others, were searched for microscopy biomedical image datasets. Data with permissive licenses (CC BY 4.0) allowing derivatives and redistribution were prioritized. A cell biologist and pathologist reviewed the images to ensure high quality (e.g. absence of artifacts or distortion). Diverse datasets were selected to include important biological processes (e.g., cell cycle), organelles (mitochondria, nucleus), and cell/tissue types. Efforts were made to include diverse biological structures, microscopy modalities, and fields of study, however, the field of basic microscopy research is broad with future efforts intended to fill in gaps in coverage.\nStandardization The original datasets had different organizational structures and file formats and often very little metadata. Information regarding the scientific discipline (domain), microscopy method, staining, pixel calibration, and the organism was determined by expert review or by consulting the original publication. The base experimental metadata was supplemented with manual annotation of multiple bio-ontology identifiers (SNOMED, BTO, FMA, LOINC, UBERON, etc.) to connect the image data with rich biology concepts and relationships knowledge graphs in the future. All images were converted into lossless PNG files at their original resolution with metadata in a paired json file. An MD5 checksum was computed for the image data, and each image was assigned a 128-bit unique identifier. The image-json pairs were converted into an Apache Arrow file for public distribution and ease of use through Hugging Face datasets [40].\nVQA task generation We used the standardized metadata to create closed VQA questions that test capabilities at different levels: easier coarse-grained perception and challenging fine-grained perception (examples are shown in Figure 1).\nThe coarse-grained perception split tests basic image properties: the broad category of scientific discipline, the type of microscope, or the stain/contrast agent. These groups are visually distinct (e.g. fluorescence vs. electron microscopy) and relatively straightforward even for non-biologists, but provide important context to biology image interpretation. Although easier, these tasks are important to assess whether VLMs have commonsense knowledge of biology and microscopy.\nThe fine-grained perception split is more challenging. Within each category of scientific discipline or microscopy modality, there are image classes or features that need to be recognised to perform image interpretation. Dataset-specific tasks include the identification of cell type, subcellular organelles, cell cycle phase, and other biological processes that are visually distinct and important for reasoning about biological images. Solving fine-grained perception relies on finer-grained visual features, and is more challenging for humans.\nWe formulate both coarse-grained and fine-grained perception as closed visual question answering (VQA). We chose this over open VQA as it's simpler to analyze, and doesn't rely on LLMs for automatic evaluation. To generate VQA options in coarse-grained perception, we designed a tree encompassing microscopy modalities, scientific domains, and staining techniques, which enables sampling fine-grained options within concepts (e.g., selecting IHC(DAB) and IHC(RED) as likely stain options for question regarding light microscopy).\nLocalization task generation We also generate a spatial localization benchmark split, which requires predicting the bounding box or segmentation mask for a cell, nucleus or organelle (examples are in Tables 1). Understanding position and layout enables modeling spatial relationships and context, and is fundamental to image understanding. Datasets with segmentation were converted to allow instance segmentation, semantic segmentation, and object detection (bounding box and centroid)."}, {"title": "3.2 Cognitive Dataset Curation", "content": "While perception datasets evaluate the fundamental capabilities of VLMs for microscopy image analysis, they fall short in assessing their ability to use perception to reason about objects. We curated a cognitive dataset to evaluate more advanced aspects like knowledge and reasoning. The cognitive dataset includes questions related to gene pathways, metabolic pathways, cell signaling and signal transduction, cell physiology and function, protein-protein interactions, cell-cell interactions, unique properties of the cell of origin or cell type in the image, cytoskeleton and cell structure or morphology, and drug mechanisms of action. These categories cover fundamental biological concepts and cellular processes to more deeply evaluate VLMs' knowledge in understanding microscopy images.\nCognition Dataset Collection We began by providing detailed guidelines for question creation to experts (see appendix C.5), which ensured consistency and quality across the dataset. Using an internal chat-like web interface, we asked domain experts to submit questions reflecting their daily research activities. We encouraged a focus on questions that required challenging image-based reasoning, domain expertise, interpretation of experimental results, or hypothesis generation.\nIn addition to submitting questions, experts provided crucial context regarding experimental details, image acquisition, organisms, treatments, and image descriptions. With this comprehensive information, GPT-4V generated answers to the submitted questions. These answers were subsequently reviewed by experts, who evaluated the accuracy and interpretation of the responses."}, {"title": "4 Dataset Description", "content": "Perception Dataset Statistics For our perception benchmark, we collected a total of 17,235 microscopy images from 24 distinct public datasets (see Table 3) with permissive licensing, prioritizing open CC-BY licenses. To the best of our knowledge, \u00b5-Bench Perception is the most diverse microscopy vision-language benchmark, spanning light (LM), fluorescence (FM), and electron microscopy (EM), covering 8 microscopy sub-modalities (see Figure 3), 91 unique cells, tissues, and structures over 24 unique staining techniques (see Figure 12). The perception benchmark subset spans this diversity through closed VQA, object detection, and segmentation.\nCognition Dataset Statistics For our cognition benchmark, we collected 54 microscopy images and 121 questions from experts in the field. Entries were received from 6 users across 5 different institutions. The \u00b5-Bench Cognition dataset encompasses 3 modalities (fluorescence, electron, light) with 12 sub-modalities, 2 domains (pathology and biology) with 14 sub-domains, and 3 scales (nano, micro, macro), covering a diverse range of topics such as pathology, immunology, and virology. Distributions are shown in Appendix Table 14."}, {"title": "5 VLM benchmarking and results", "content": "Data artifacts like \u00b5-Bench enable studying model behavior within specialist domains. Since our benchmark covers a wide range of biomedical tasks, we can, for the first time, compare biomedical perception and cognition capabilities across microscopy imagining modalities. In this section, we show the utility of \u00b5-Bench by reporting empirical findings on a range of VLMs."}, {"title": "5.1 Benchmarking approach", "content": "First, we categorized VLMs into two groups: generalist models trained on natural images and language, and 'specialist' models, fine-tuned on biomedical data. Within generalist models, we also distinguish between contrastive and auto-regressive models.\nGeneralist Contrastive (GC) VLMS We evaluate ALIGN [32], OpenCLIP [41], and CLIP [19] as the canonical contrastive models for natural images. Notably, OpenCLIP and CLIP serve as foundational models for numerous specialist biomedical VLMs.\nGeneralist autoregressive (GA) VLMS We evaluate with GPT-40 [33], a state-of-the-art enterprise VLM. For open source models, we test CogVLM [42], QwenVLM [21], and PaliGemma [43] for their strong performance on general domain tasks, instruction-following capabilities, and, for QwenVLM and PaliGemma only, their ability to perform object detection.\nSpecialist contrastive (SC) VLMS Our specialist model selection had two constraints: choosing the best-performing models and preferring minimal architectural changes from their base generalist versions, allowing performance analysis based on variations in training mixtures. We selected BiomedCLIP [3] since it is a strong model that covers all biomedical imaging modalities in our benchmark (trained on 15 million image-text pairs collected from PubMedCentral). Additionally, we included three pathology VLMs: PLIP (CLIP trained on H&E) [38], QuiltNet (CLIP trained on H&E and IHC) [44], and CONCH (CoCa trained on H&E and IHC) [45], with training dataset sizes of 208k, 1 million, and 1.2 million, respectively. While CONCH and BiomedCLIP are based on OpenCLIP and CoCa [39] respectively, they modify the architecture or training strategy.\nEvaluation The Closed VQA component of \u00b5-Bench was evaluated with accuracy, generating confidence intervals (CI) via bootstrap [46] (appendix F.2). Object detection was evaluated for models with object detection capabilities (PaliGemma and QwenVLM) in open VQA format using the GRIT localization metric [47] as adopted by prior works [21]."}, {"title": "5.2 Results", "content": "All models have high error rates Table 1 shows the accuracy performance across perception and cognition. Even the top-performing model (GPT-40) has high error rates, with an accuracy of 62.6% on coarse-grained perception, 51.7% on fine-grained perception, and 62.0% on cognition tasks. On average GPT-4o also outperforms BiomedCLIP (the best biomedical SC VLM) by a minimum of 15% in all evaluation dimensions and CONCH (the best pathology SC VLM) in pathology-specific perception tasks, showing a difference of 39.37% on coarse-grained and 19.40% in fined-grained tasks (as illustrated in Table 1. However, finer subgroup analysis (Figure 8) shows that GP-40 does not excel in all perception tasks, including domain identification (coarse-grained) and single-molecule imaging, normal vs abnormal classification, and non-neoplastic histopathology interpretation (fine-grained). The model architecture and training data for GPT-4 are closed source, making it challenging to conclude from these results. However, GPT-40's high error rates, its substantial gap compared to SC models, and performance variation across task subgroups highlight that \u00b5-Bench is challenging and is not saturated by state-of-the-art general, biomedical, and pathology models.\nSpecialist biomedical models are often worse than non-specialist models While specialist models are explicitly developed for the biomedical domain, they often underperform non-specialized open-source models. For example, in both coarse-grained perception and cognition tasks (Table 1), GA models (CogVLM and QwenVLM) outperform the best SC model (BiomedCLIP) by 4.4% and 16.0% margins respectively. While GA models have a different training objective, larger training mixture, and more model parameters, a similar trend is observed with GC models (ALIGN, OpenCLIP, and CLIP) as they outperform all pathology VLMs in the same tasks by at least 9.5% (PLIP- ALIGN) and 20.3% (CONCH - OpenCLIP) respectively. This ranking is reversed in fine-grained perception tasks, where BiomedCLIP and CONCH perform best. Indeed, fine-grained perception closely resembles the data mixture used to fine-tune contrastive specialist models [25]. This characterization shows weakness in current microscopy biomedical model development, which we investigate next.\nSpecialist training can cause catastrophic forgetting Generalist contrastive models like (OpenCLIP and CLIP) surprisingly outperform their fine-tuned counterparts (PILP and QuiltNet) in coarse-grained perception and cognition (Table 1). Specifically, PILP and QuiltNet are fine-tuned directly from OpenCLIP and CLIP using only pathology data closest to \u00b5-Bench fine-grained perception tasks. Although it improves performance on pathology-specific fine-grained tasks (Figure 4), it degrades performance on all other tasks (Table 1).\n\u03bc-Bench characterization drives robust model development To address catastrophic forgetting identified in our multi-level evaluation, we ensemble base model weights (OpenCLIP / CLIP) with fine-tuned model weights (PLIP/QuiltNet)"}, {"title": "6 Conclusion", "content": "Benchmarks drive advancements in machine learning by providing a standard to measure progress and allowing researchers to identify weaknesses in current approaches. Thus, the lack of biomedical vision-language benchmarks limits the ability to develop and evaluate specialist VLMs. We address this gap in microscopy by introducing the most extensive collection of vision-language tasks spanning perception and cognition. We use \u00b5-Bench to establish, for the first time, the performance of some of the most capable VLMs available and find high error rates of 30%, highlighting room for improvement. We demonstrate how \u00b5-Bench can be leveraged to generate new insights. Lastly, we share \u00b5-Bench to enable researchers to measure progress in microscopy foundation models."}, {"title": "A Limitations", "content": "\u03bc-Bench is a diverse benchmark for evaluating vision-language models on microscopy image and text data. The dataset is intended for testing purposes only, not for training. The moderate size will allow many academic researchers to assess the performance of models trained on natural images or other biology datasets.\nWhile \u03bc-Bench covers various biological length scales, microscopy modalities, scientific disciplines, and organisms, not all domains and modalities are equally represented. This partially reflects the usage patterns of the field, with human samples (cell lines or tissues) being more common in biomedical research. Brightfield and fluorescence microscopy images are also more prevalent in \u00b5-Bench compared to electron microscopy. This means that the results on \u00b5-Bench may not apply equally well to all model organisms or electron microscopy images. The VLM performance may be lower in these areas due to the data being rare in both natural image datasets and uncommon in biomedical datasets.\nWe strive for a high-quality dataset and involve cell biologists and pathologists during dataset creation, quality control, and interpretation of results. However, the field of biology is diverse, and no individual or small group can reliably stay up-to-date with all aspects of biological/biomedical research. For example, we included a botany dataset (ICPR 2020 Pollen) to show a commitment to including diverse scientific disciplines, but we currently do not have expert plant biologist contributors.\n\u03bc-Bench represents the first vision-language benchmark for microscopy covering all major biology length scales and modalities. We see \u00b5-Bench as a living benchmark we intend to grow, although we acknowledge the aforementioned limitations. Future versions will prioritize incorporating new data from diverse organisms and microscopy modalities to improve representation across all length scales, microscopy modalities, and organisms. We will also benefit from community engagement by involving domain experts from diverse fields and obtaining data to balance currently under-represented areas."}, {"title": "B Ethical Compliance and Acknowledgements", "content": "Safe and ethical use of biomedical data: We prioritize safe and ethical research practices while creating \u00b5-Bench. All public datasets with patient-derived histopathology images had already been de-identified by the dataset's original authors in compliance with applicable privacy laws and institutional guidelines. The public histopathology image data and metadata were reviewed, and it was determined that it was not possible to identify any individual from the de-identified data. The Stanford Institutional Review Board guidelines were reviewed and discussed, and the use of the images was determined not to be human subjects research.\nThe \u00b5-Bench cognition images and questions were voluntarily submitted by a small (<10) number of biology/pathology users alpha-testing a free web chat application. There was no intervention, experiment vs. control group, or research question during the alpha testing. There was no greater risk of using the app than other internet apps. At registration, users agreed to the service terms, which included releasing image and text data under CC-BY-SA-4.0.\nConsent and data usage: We thank and respect the original dataset authors and use data according to the original copyright and license. \u00b5-Bench was developed with both academic and commercial research in mind. Many datasets are a version of CC-BY-4.0 to allow both academic and commercial usage. However, some data restricts commercial applications via non-commercial clauses or CC-BY-NC-4.0-related licenses. While creating \u00b5-Bench we significantly improved the data by performing expert review, quality control/standardization, expert labeling with biomedical ontology codes, and creating multiple-choice VQA questions and captions for each image. When the original dataset license is CC-BY-4.0, we release our \u00b5-Bench versions under a permissive CC-BY-SA-4.0 to foster a transparent and collaborative benchmark. For the subset with CC-BY-NC-SA-4.0, we respect the original license and release these data under CC-BY-NC-SA-4.0.\nBias and data diversity: We recognize that AI models, including VLMs, can perpetuate or exacerbate biases in the training data, and incorporating diverse data may mitigate these biases. Diversity is key to understanding biological processes and how they vary across biological sex, ethnicity, or other factors. When possible, we annotate cell lines with age, sex, ethnicity, and other metadata from Cellosaurus or the Cell Line Ontology. We consciously include diverse microscopy images from multiple institutions across various organisms, modalities, and biological states to ensure \u00b5-Bench provides an accurate and fair performance assessment. However, images of human samples and brightfield/fluorescence microscopy are more common in the field and thus over-represented in \u00b5-Bench ."}, {"title": "Potential negative societal impacts", "content": "AI models trained on biomedical data have the potential for far-reaching impacts on society, both positive and negative. Potential negative impacts include biased performance across different demographic groups and reinforcing existing disparities in research and healthcare. We are committed to mitigating these risks by ensuring the dataset's diversity and continuously reviewing the ethical implications of our work. Additionally, we will engage with the broader research community to identify and address any emerging ethical concerns.\nWe are committed to ongoing improvements of \u00b5-Bench to prioritize diverse and representative microscopy data. We will review and update \u00b5-Bench in response to evolving ethical standards and technological advancements."}, {"title": "B.2 Conflicts of Interest Disclosures", "content": "The authors have no conflicts of interest to disclose."}, {"title": "B.3 Funding/Support", "content": "This research was supported by NIH grants (NIH#P30AG066515 to JJN), the Chan Zuckerberg Initiative Neurodegeneration Challenge Pairs Pilot Project to SYL (2020-221724, 5022), the Wu Tsai Knight Initiative Innovation grant (#KIG 102) to SYL, Arc Institute to AL, Quad Fellowship to JB, and NSF Graduate Research Fellowship (#DGE-2146755) and Stanford Graduate Fellowship to AU. SYL is a Chan Zuckerberg Biohub \u2013 San Francisco Investigator."}, {"title": "B.4 Acknowledgments", "content": "We thank all the domain experts for testing the web app and submitting questions that were eventually used in \u00b5-Bench cognition. We highlight the contributions from Pedro Guedes-Dias, Andrew Moore, and Julian Perez. We appreciate feedback from Josiah Aklilu and Orr Zohar on earlier manuscript versions."}, {"title": "C Benchmark Details", "content": ""}, {"title": "C.1 Instructions for downloading the benchmark", "content": "The benchmark can be downloaded via HuggingFace Datasets at the following url."}, {"title": "C.2 \u00b5-Bench overview", "content": "\u03bc-Bench is organized into three main categories:\n1. \u03bc-Bench Perception (Coarse-grained): Containing basic questions about the type of biomedical field of study (domain), subdomain, microscopy modality, submodality, and stain.\n2. \u03bc-Bench Perception (Fine-grained): Identification or questions regarding a biological cell, cellular process, subcellular or tissue structures, biological state (normal/abnormal), etc.\n3. \u03bc-Bench Cognition (Reasoning): Expert-generated questions that typically require visual-based reasoning or integrating knowledge about the micrograph's composition and subject to deduce an answer.\n4. \u03bc-Bench Object Detection (Localization): Bounding box detection of common biological objects, with an easy and hard data split."}, {"title": "C.3 \u00b5-Bench statistics", "content": "Table 2 shows descriptive statistics for \u03bc-Bench Perception while Table 4 shows statistics for \u03bc-Bench Reasoning."}, {"title": "C.4 \u00b5-Bench Perception", "content": "\u03bc-Bench Perception contains 17,235 microscopy images collected from 25 unique datasets of 96 different cell and tissue types across light, fluorescence, and electron microscopy. If defined, only images from each dataset's test set are selected; otherwise, a random 15 percent split is created. Each cell/tissue class is sub-sampled to a maximum of 200 sub-classes per class (if the cell type is less than the maximum, the full set is used). Each cell/tissue type is densely annotated (then propagated to each instance of the same type) by an expert, as shown in Figure 18\nTable 10 provides summary statistics of domain coverage. Overall, the benchmark covers 8,637 biology images and 8,678 pathology images across 12 subdomains. Similarly, Table 11 shows summary statistics of microscopy modalities covered by \u03bc-Bench perception, including 10,864 images for light microscopy, 5,618 for fluorescence microscopy, and 833 images for electron microscopy across 8 microscopy imaging submodalities.\n\u03bc-Bench Perception (Coarse-grained): Hierarchical metadata for each of the 17,235 perception images and task-specific templates (shown in Table 17) are used to create 5 coarse-grained questions and captions regarding microscopy modality, submodality, domain, subdomain, and staining technique. The use of hierarchical metadata enables the generation of options within each hierarchical level. For example, for microscopy submodality, we leveraged microscopy modality metadata (shown in Figure13 ) to randomly sample submodality options within a microscopy modality (e.g. differential interference contrast microscopy within light microscopy). A total of 86,175 (17,235x5) coarse-grained questions are generated using this approach, leveraging domain metadata Figure 14 as well as staining metadata Figure 15). Section H provides 10 random examples of coarse-grained data points (two per type).\n\u03bc-Bench Perception (Fine-grained): Metadata for each of the 25 unique datasets and custom prompts (Table 2) are used to generate 13 unique tasks, which are 13 (17,235 unique question-image pairs) closed VQA style questions. of VQA fine-grained tasks.\nPerception dataset: source details: Table 3 lists the dataset names, licenses, and corresponding DOIs or URLs for dataset used. The majority of these datasets were sourced from open data repositories, such as Zenodo, Dataverse, Dryad, BBBC, EMPIAR, Kaggle, and various project websites.\nPerception dataset: forming questions for evaluation Each sample has a question and set of candidate answers. We describe how to evaluate this VQA task in appendix D."}, {"title": "C.5 \u00b5-Bench Cognition", "content": "Cognition dataset: generation details\nThe cognition dataset was collected during alpha-testing of a web application chat interface. A group of 6 biology and pathology experts were invited to interact with a web application as they wished during their daily routines (invitation is shown in Figure 20). There was no specific research question, intervention, or experimental vs. control group; this was a free service. User registration was voluntary and required reviewing and accepting the terms of service. The terms of service discussed that this was not a research study and that the risk of harm is insignificant and would be similar to any VLM chatbot. The terms of service indicated that uploading images indicated the user had copyright or permission, that images did not contain offensive content, and that the images and text could be released with a CC-BY-SA-4.0 license.\nFigure 7) shows a screenshot of the web application interface. A typical usage involved uploading a microscopy image, providing context about the image (experiment details), and asking a question. The web app processed the submission using GPT-4V and provided an answer in real time. Users were encouraged to provide feedback on the answer and a correct answer (if known). Users were encouraged to ask questions that required complex visual reasoning, advanced biomedical knowledge, or could be considered challenging for humans. Random samples for cognition questions are shown in Section J"}, {"title": "C.6 \u00b5-Bench Object detection", "content": "For datasets with segmentation annotations, we copy the segmentation annotations and also convert them to object detection annotations, giving 3641 images [50, 54, 61, 65, 62]."}, {"title": "C.7 Comparison with a supervised linear model trained on DINOv2 Features", "content": "Although some datasets have been used in the biomedical computer vision community [58, 59], others do not have well-established performance baselines. Thus, we evaluate a supervised linear model's baseline classification performance for the fine-grained and coarse-grained perception tasks. This establishes that the questions in our benchmark are solvable using image features."}, {"title": "D Evaluation Details", "content": ""}, {"title": "D.1 VQA evaluation details", "content": "Three out of four tasks are formulated as multiple-choice visual question answering: coarse-grained perception, fine-grained perception, and cognition. We describe their evaluation here, while the fourth task of object detection is described separately in appendix E.1. For each sample, i, we have an image, \\(x_i\\) a question string, q, and a set of k candidate answer strings \\(\\{a_{ij}\\}_{j=1}^k\\).\nFor the autoregressive models, \\(f_a\\), we first generate a query string, t, from the question and candidate answer strings, \\(t(q, \\{a_{ij}\\}_{j=1}^k)\\), using the template in Figure 19. The prompt text instructs the response to start with a letter for one of the multiple choice answers, (\u2018A', 'B', . . . ). We pass the prompt string and image to the model and decode the output, \\(y = f(x_i, t)\\), where y is the output string. We have two strategies for matching the response string, y, to the candidate answers \\(\\{a_{ij}\\}_{j=1}^k\\). First, for each j, we check whether the answer string is in the output: \\(a_{ij} \\in y\\) with lowercase string matching. This is important because models do not always follow the instructions to output the multiple choice letter"}, {"title": "D.2 Object detection evaluation details", "content": "We evaluate the two models that support localization: QwenVL [21] and PaliGemma [43], and follow their user guide for prompting. For \u2018QwenVL' the prompt is \u2018Detect {class_name}", "Detect {class_name}; {class_name}": "where the repeated class name indicates that multiple instances may be predicted. Our early experiments found that PaliGemma would sometimes fail to localize any instances using detection prompting, but would localize them with segmentation prompting. So if PaliGemma does return zero instances, we prompt for segmentation", "class_name}": "nd extract the bounding box. The Burgess et al. dataset has two classes, so we prompt the model one at a time. Both models output detection predictions as a string with a standardized structure, which we parse using regex.\nWe use the GRIT localization metric [47] because it is well-motivated and has previously been used in VLM evaluations by QwenVL [21]. The score is:\n\\(\\Sigma_{i=1}^M \\frac{IoU_i}{P + G_{missed}} \\) (1)\nThere are M ground truth boxes, and P predicted boxes, which are matched using the Hungarian algorithm on the IoU metric. \\(G_{missed}\\) is the number of predicted boxes not matched to a ground truth box. Intuitively, this metric measures the average IoU for matched boxes, while penalizing making too many predictions using \\(G_{missed}\\) (similar to the precision metric). Note that we cannot use the more typical mAP score from object detection because they depend on a threshold for controlling the false-positive rate, which these VLMs do support."}, {"title": "E Additional Benchmarking Results", "content": ""}, {"title": "E.1 Object Detection Results", "content": "table 5 summarizes object detection for the datasets having object localization annotations.\nOverall, the localization scores are very poor, which is expected since both models are generalist and object localization has received relatively less attention in autoregressive VLMs. Looking at the splits:\n\u2022 Easy split. Although both models have higher scores for the 'cell' class in Burgess et al., they still fall below 80, and the task is extremely easy. The story is similar for \u2018nucleus' in Held et al., but for QwenVLM, the scores are even lower.\n\u2022 Hard split. All models perform poorly on the hard split. In Burgess et al \u2018nucleus', PaliGemma scores around 30, however qualitative inspection shows that in most cases, the bounding box predicts the entire cell, which encapsulates the nucleus. Similarly, in Wu et al., the mitochondria class scores more than 20 for both models, but qualitative inspection shows that the prediction is usually a box around the entire image. We find the same pattern in 'Sirinukunwattana et al.' for 'gland' detection."}, {"title": "E.2 Weight ensembling details", "content": "In the results section 5.2, we consider PLIP, which was fine-tuned from OpenCLIP, and QuiltNet, which was fine-tuned from CLIP, both using pathology data. Since we have benchmark results for all these models, our evaluation can evaluate the impact of fine-tuning on pathology data (moving from OpenCLIP/CLIP to PLIP/QuiltNet). We showed that pathology fine-tuning can improve performance on the pathology subset of \u00b5-Bench (the fine-grained perception), where we filter for all samples from the histopathology or H&E imaging modality. However, the fine-tuned models have worse overall performance on \u00b5-Bench(which includes the pathology subset).\nWe proposed to create 'merged models' by combining the base model (OpenCLIP or CLIP) with their fine-tuned models (PLIP or QuiltNet) with weight merging. Specifically, following [48], for a base model with weights \\(\theta_B\\) and tuned model \\(\theta_T\\) (which have the same architecture), the merged model weights \\(\theta_M\\) are :\n\\(\theta_M = \\alpha \\cdot \\theta_B + (1 - \\alpha) \\cdot \\theta_T\\)\nThat is, we are linearly interpolating each model weight independently, with a single fixed constant \u03b1. We arbitrarily set \u03b1 = 0.5 in our experiments, but tuning that constant could lead to better overall results.\nWe now show more comprehensive results in table 1, which is the main results table but includes our merged models, M-PLIP and M-QuiltNet; table 7 is the same but with the pathology-only split. In all cases (M-PLIP and M-QuiltNet), the merged models outperform the tuned models (PLIP and QuiltNet), while also outperforming the base models (OpenCLIP and CLIP) in almost all cases. For fine-grained perception (the task that is most in distribution for PLIP and QuiltNet training), the merged models become among the strongest performing overall models, and on pathology-specific fine-grained perception, the merged models outperform BiomedCLIP."}, {"title": "E.3 Model Performance on Pathology Specific Tasks", "content": "While prior evaluations show that general contrastive VLMs have some biology and pathology knowledge (a finding also reported in [25] and [44]), most specialist models analyzed in this work were fine-tuned. To this end, we analyzed the performance on pathology-only tasks and found similar rankings."}, {"title": "F Additional Benchmarking details", "content": ""}, {"title": "F.1 Model Details", "content": "Table table 9 provides a breakdown of model parameters and training data (with dataset size), specialist models include their base model."}]}