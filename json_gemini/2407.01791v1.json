{"title": "\u03bc-BENCH: VISION-LANGUAGE BENCHMARK FOR MICROSCOPY\nUNDERSTANDING", "authors": ["Alejandro Lozano", "Jeffrey Nirschl", "James Burgess", "Sanket Rajan Gupte", "Yuhui Zhang", "Alyssa Unell", "Serena Yeung-Levy"], "abstract": "Recent advances in microscopy have enabled the rapid generation of terabytes of image data in\ncell biology and biomedical research. Vision-language models (VLMs) offer a promising solution\nfor large-scale biological image analysis, enhancing researchers' efficiency, identifying new image\nbiomarkers, and accelerating hypothesis generation and scientific discovery. However, there is\na lack of standardized, diverse, and large-scale vision-language benchmarks to evaluate VLMs'\nperception and cognition capabilities in biological image understanding. To address this gap, we\nintroduce \u00b5-Bench, an expert-curated benchmark encompassing 22 biomedical tasks across various\nscientific disciplines (biology, pathology), microscopy modalities (electron, fluorescence, light),\nscales (subcellular, cellular, tissue), and organisms in both normal and abnormal states. We evaluate\nstate-of-the-art biomedical, pathology, and general VLMs on \u00b5-Bench and find that: i) current models\nstruggle on all categories, even for basic tasks such as distinguishing microscopy modalities; ii)\ncurrent specialist models fine-tuned on biomedical data often perform worse than generalist models;\niii) fine-tuning in specific microscopy domains can cause catastrophic forgetting, eroding prior\nbiomedical knowledge encoded in their base model. iv) weight interpolation between fine-tuned\nand pre-trained models offers one solution to forgetting and improves general performance across\nbiomedical tasks. We release \u00b5-Bench under a permissive license to accelerate the research and\ndevelopment of microscopy foundation models.", "sections": [{"title": "Introduction", "content": "Microscopy is a cornerstone of biomedical research [1], enabling detailed study of biological structures at multiple\nscales [2]. Advances in cryo-electron microscopy, high-throughput fluorescence microscopy, and whole-slide imaging\nallow the rapid generation of terabytes of image data, which are essential for fields such as cell biology, biomedical\nresearch, and pathology [3]. These data span multiple length scales, allowing researchers to examine atomic/molecular,\nsubcellular/cellular, and cell/tissue-level structures with high precision [4]. A crucial first step in microscopy analysis"}, {"title": "Related Work", "content": "Biomedical Vision-Language Benchmarks. While previous works have developed various biomedical vision-language\nbenchmarks that have been instrumental in advancing diagnostic capabilities, they present three main problems: 1) Task\nsimplicity: Most biomedical computer vision benchmarks predominantly focus on supervised tasks such as classification\nand segmentation [27, 28, 29] rather than vision-language tasks. As shown in Table 8, biomedical vision-language\nbenchmarks are less common. 2) Lack of diversity: existing vision-language datasets are usually limited to diagnostic\nimaging such as radiology or pathology [30, 31]; there is a lack of benchmarks for basic research microscopy. 3)\nLimited accessibility: PMC-15M is a large image-text dataset used to train BiomedCLIP [3], which includes diverse\nmicroscopy images, domains, and organisms but is not publicly accessible for training or evaluation.\nVision-Language Models in Biomedicine. Vision-language models (VLMs) can be generally categorized into two\ntypes: 1) Contrastive models, such as CLIP [19] and ALIGN [32], which use contrastive learning to create shared\nimage-text embeddings, facilitating tasks like zero-shot classification and text-image retrieval; and 2) Auto-regressive\nmodels, such as Flamingo [20] and GPT-4 [33], which integrate image embeddings with large language models (LLMs)\nto perform zero-shot tasks, follow instructions, and reason about content. These VLMs have significant potential to\nadvance biomedicine [34, 5, 6, 7, 35].\nHowever, these models are primarily trained on general datasets with limited biomedical coverage, leading to suboptimal\nperformance on biomedical tasks [36, 37]. To address this gap, specialized VLMs have been developed by fine-tuning\ngeneralist models on biomedical data. Notable examples include BiomedCLIP [3] train on images from PubMed, and\nhistopathology vision-language models such as PLIP [38] and CONCH [39], which were trained on Twitter or various\npathology sources. Despite the increase in vision-language foundation models for microscopy, there are no diverse\nbenchmarks to evaluate the performance of image-based perception and reasoning about microscopy images across\ndiverse scales and modalities. Our work on \u03bc-Bench addresses this issue by providing a comprehensive benchmark that\nincludes a variety of important and diverse biological processes, organisms, microscopy modalities, domains, and tasks\nto support the evaluation of microscopy foundation models."}, {"title": "Dataset collection methodology", "content": "Recognizing the need for an expert-level benchmark in microscopy for comprehensive biological and biomedical\nunderstanding, we developed a benchmark to assess the perception and cognition capabilities of VLMs in microscopy\nimage analysis following the methodology shown in Figure 2. At a high level, the pipeline consists of two main\ncomponents: (i) A biomedical expert categorized potential tasks and collected diverse microscopy datasets across\nmultiple scientific domains, focusing on evaluating perception capabilities. (ii) We then complement \u03bc-Bench by\ncrowdsourcing questions from a larger group of microscopists using a web application."}, {"title": "Perception Dataset Curation", "content": "Dataset Review and Selection Open data repositories, including Zenodo, Dataverse, Dryad, and BBBC, among\nothers, were searched for microscopy biomedical image datasets. Data with permissive licenses (CC BY 4.0) allowing\nderivatives and redistribution were prioritized. A cell biologist and pathologist reviewed the images to ensure high\nquality (e.g. absence of artifacts or distortion). Diverse datasets were selected to include important biological processes\n(e.g., cell cycle), organelles (mitochondria, nucleus), and cell/tissue types. Efforts were made to include diverse\nbrological structures, microscopy modalities, and fields of study, however, the field of basic microscopy research is\nbroad with future efforts intended to fill in gaps in coverage.\nStandardization The original datasets had different organizational structures and file formats and often very little\nmetadata. Information regarding the scientific discipline (domain), microscopy method, staining, pixel calibration,\nand the organism was determined by expert review or by consulting the original publication. The base experimental\nmetadata was supplemented with manual annotation of multiple bio-ontology identifiers (SNOMED, BTO, FMA,\nLOINC, UBERON, etc.) to connect the image data with rich biology concepts and relationships knowledge graphs in\nthe future. All images were converted into lossless PNG files at their original resolution with metadata in a paired json"}, {"title": "VQA task generation", "content": "We used the standardized metadata to create closed VQA questions that test capabilities at\ndifferent levels: easier coarse-grained perception and challenging fine-grained perception (examples are shown in\nFigure 1).\nThe coarse-grained perception split tests basic image properties: the broad category of scientific discipline, the type of\nmicroscope, or the stain/contrast agent. These groups are visually distinct (e.g. fluorescence vs. electron microscopy)\nand relatively straightforward even for non-biologists, but provide important context to biology image interpretation.\nAlthough easier, these tasks are important to assess whether VLMs have commonsense knowledge of biology and\nmicroscopy.\nThe fine-grained perception split is more challenging. Within each category of scientific discipline or microscopy\nmodality, there are image classes or features that need to be recognised to perform image interpretation. Dataset-specific\ntasks include the identification of cell type, subcellular organelles, cell cycle phase, and other biological processes that\nare visually distinct and important for reasoning about biological images. Solving fine-grained perception relies on\nfiner-grained visual features, and is more challenging for humans.\nWe formulate both coarse-grained and fine-grained perception as closed visual question answering (VQA). We chose\nthis over open VQA as it's simpler to analyze, and doesn't rely on LLMs for automatic evaluation. To generate VQA\noptions in coarse-grained perception, we designed a tree encompassing microscopy modalities, scientific domains,\nand staining techniques, which enables sampling fine-grained options within concepts (e.g., selecting IHC(DAB) and\nIHC(RED) as likely stain options for question regarding light microscopy).\nLocalization task generation We also generate a spatial localization benchmark split, which requires predicting the\nbounding box or segmentation mask for a cell, nucleus or organelle (examples are in Tables 1). Understanding position\nand layout enables modeling spatial relationships and context, and is fundamental to image understanding. Datasets with\nsegmentation were converted to allow instance segmentation, semantic segmentation, and object detection (bounding\nbox and centroid)."}, {"title": "Cognitive Dataset Curation", "content": "While perception datasets evaluate the fundamental capabilities of VLMs for microscopy image analysis, they fall short\nin assessing their ability to use perception to reason about objects. We curated a cognitive dataset to evaluate more\nadvanced aspects like knowledge and reasoning. The cognitive dataset includes questions related to gene pathways,\nmetabolic pathways, cell signaling and signal transduction, cell physiology and function, protein-protein interactions,\ncell-cell interactions, unique properties of the cell of origin or cell type in the image, cytoskeleton and cell structure\nor morphology, and drug mechanisms of action. These categories cover fundamental biological concepts and cellular\nprocesses to more deeply evaluate VLMs' knowledge in understanding microscopy images.\nCognition Dataset Collection We began by providing detailed guidelines for question creation to experts (see\nappendix C.5), which ensured consistency and quality across the dataset. Using an internal chat-like web interface, we\nasked domain experts to submit questions reflecting their daily research activities. We encouraged a focus on questions\nthat required challenging image-based reasoning, domain expertise, interpretation of experimental results, or hypothesis\ngeneration.\nIn addition to submitting questions, experts provided crucial context regarding experimental details, image acquisition,\norganisms, treatments, and image descriptions. With this comprehensive information, GPT-4V generated answers\nto the submitted questions. These answers were subsequently reviewed by experts, who evaluated the accuracy and\ninterpretation of the responses."}, {"title": "Multiple-Choice Question Transformation", "content": "The collected pairs (image, question, GPT-4V answer, feedback) were\ntransformed into multiple-choice questions using GPT-4. This transformation was guided by a carefully designed\nprompt (appendix C.7), verified by a cell biologist and a pathologist, to ensure the questions are challenging and\nreflective of real-world scenarios faced by biomedical researchers. Each transformed question includes an image, a\nquestion, and six candidate choices. One choice is correct, while the other five are distractors generated by GPT-4,\nwhere one choice is \u201cNone of the above.\u201d Domain experts verified the validity of the generated questions and manually\ncorrected a small number of questions. Finally, we ensured that correct answers were uniformly distributed among\nanswer choices A to F and formalized the entire dataset."}, {"title": "Dataset Description", "content": "Perception Dataset Statistics For our perception benchmark, we collected a total of 17,235 microscopy images from\n24 distinct public datasets (see Table 3) with permissive licensing, prioritizing open CC-BY licenses. To the best of\nour knowledge, \u00b5-Bench Perception is the most diverse microscopy vision-language benchmark, spanning light (LM),\nfluorescence (FM), and electron microscopy (EM), covering 8 microscopy sub-modalities (see Figure 3), 91 unique\ncells, tissues, and structures over 24 unique staining techniques (see Figure 12). The perception benchmark subset spans\nthis diversity through closed VQA, object detection, and segmentation.\nCognition Dataset Statistics For our cognition benchmark, we collected 54 microscopy images and 121 questions\nfrom experts in the field. Entries were received from 6 users across 5 different institutions. The \u00b5-Bench Cognition\ndataset encompasses 3 modalities (fluorescence, electron, light) with 12 sub-modalities, 2 domains (pathology and\nbrology) with 14 sub-domains, and 3 scales (nano, micro, macro), covering a diverse range of topics such as pathology,\nimmunology, and virology. Distributions are shown in Appendix Table 14."}, {"title": "VLM benchmarking and results", "content": "Benchmarking approach\nData artifacts like \u00b5-Bench enable studying model behavior within specialist domains. Since our benchmark covers a\nwide range of biomedical tasks, we can, for the first time, compare biomedical perception and cognition capabilities\nacross microscopy imagining modalities. In this section, we show the utility of \u00b5-Bench by reporting empirical findings\non a range of VLMs."}, {"title": "Results", "content": "All models have high error rates Table 1 shows the accuracy performance across perception and cognition. Even the\ntop-performing model (GPT-40) has high error rates, with an accuracy of 62.6% on coarse-grained perception, 51.7%\non fine-grained perception, and 62.0% on cognition tasks. On average GPT-4o also outperforms BiomedCLIP (the best\nbiomedical SC VLM) by a minimum of 15% in all evaluation dimensions and CONCH (the best pathology SC VLM)\nin pathology-specific perception tasks, showing a difference of 39.37% on coarse-grained and 19.40% in fined-grained\ntasks (as illustrated in Table 1. However, finer subgroup analysis (Figure 8) shows that GP-40 does not excel in all\nperception tasks, including domain identification (coarse-grained) and single-molecule imaging, normal vs abnormal\nclassification, and non-neoplastic histopathology interpretation (fine-grained). The model architecture and training data\nfor GPT-4 are closed source, making it challenging to conclude from these results. However, GPT-40's high error rates,\nits substantial gap compared to SC models, and performance variation across task subgroups highlight that \u00b5-Bench is\nchallenging and is not saturated by state-of-the-art general, biomedical, and pathology models.\nSpecialist biomedical models are often worse than non-specialist models While specialist models are explicitly\ndeveloped for the biomedical domain, they often underperform non-specialized open-source models. For example, in\nboth coarse-grained perception and cognition tasks (Table 1), GA models (CogVLM and QwenVLM) outperform the\nbest SC model (BiomedCLIP) by 4.4% and 16.0% margins respectively. While GA models have a different training\nobjective, larger training mixture, and more model parameters, a similar trend is observed with GC models (ALIGN,\nOpenCLIP, and CLIP) as they outperform all pathology VLMs in the same tasks by at least 9.5% (PLIP- ALIGN)\nand 20.3% (CONCH - OpenCLIP) respectively. This ranking is reversed in fine-grained perception tasks, where\nBiomedCLIP and CONCH perform best. Indeed, fine-grained perception closely resembles the data mixture used to\nfine-tune contrastive specialist models [25]. This characterization shows weakness in current microscopy biomedical\nmodel development, which we investigate next.\nSpecialist training can cause catastrophic forgetting Generalist contrastive models like (OpenCLIP and CLIP)\nsurprisingly outperform their fine-tuned counterparts (PILP and QuiltNet) in coarse-grained perception and cognition\n(Table 1). Specifically, PILP and QuiltNet are fine-tuned directly from OpenCLIP and CLIP using only pathology data\nclosest to \u00b5-Bench fine-grained perception tasks. Although it improves performance on pathology-specific fine-grained\ntasks (Figure 4), it degrades performance on all other tasks (Table 1).\n\u03bc-Bench characterization drives robust model development To address catastrophic forgetting identified in our multi-\nlevel evaluation, we ensemble base model weights (OpenCLIP / CLIP) with fine-tuned model weights (PLIP/QuiltNet)"}, {"title": "Conclusion", "content": "Benchmarks drive advancements in machine learning by providing a standard to measure progress and allowing\nresearchers to identify weaknesses in current approaches. Thus, the lack of biomedical vision-language benchmarks\nlimits the ability to develop and evaluate specialist VLMs. We address this gap in microscopy by introducing the most\nextensive collection of vision-language tasks spanning perception and cognition. We use \u00b5-Bench to establish, for the\nfirst time, the performance of some of the most capable VLMs available and find high error rates of 30%, highlighting\nroom for improvement. We demonstrate how \u00b5-Bench can be leveraged to generate new insights. Lastly, we share\n\u03bc-Bench to enable researchers to measure progress in microscopy foundation models."}, {"title": "Appendix", "content": "Limitations\n\u03bc-Bench is a diverse benchmark for evaluating vision-language models on microscopy image and text data. The dataset\nis intended for testing purposes only, not for training. The moderate size will allow many academic researchers to assess\nthe performance of models trained on natural images or other biology datasets.\nWhile \u03bc-Bench covers various biological length scales, microscopy modalities, scientific disciplines, and organisms, not\nall domains and modalities are equally represented. This partially reflects the usage patterns of the field, with human\nsamples (cell lines or tissues) being more common in biomedical research. Brightfield and fluorescence microscopy\nimages are also more prevalent in \u00b5-Bench compared to electron microscopy. This means that the results on \u00b5-Bench\nmay not apply equally well to all model organisms or electron microscopy images. The VLM performance may be\nlower in these areas due to the data being rare in both natural image datasets and uncommon in biomedical datasets.\nWe strive for a high-quality dataset and involve cell biologists and pathologists during dataset creation, quality control,\nand interpretation of results. However, the field of biology is diverse, and no individual or small group can reliably\nstay up-to-date with all aspects of biological/biomedical research. For example, we included a botany dataset (ICPR\n2020 Pollen) to show a commitment to including diverse scientific disciplines, but we currently do not have expert plant\nbiologist contributors.\n\u03bc-Bench represents the first vision-language benchmark for microscopy covering all major biology length scales and\nmodalities. We see \u00b5-Bench as a living benchmark we intend to grow, although we acknowledge the aforementioned\nlimitations. Future versions will prioritize incorporating new data from diverse organisms and microscopy modalities\nto improve representation across all length scales, microscopy modalities, and organisms. We will also benefit from\ncommunity engagement by involving domain experts from diverse fields and obtaining data to balance currently\nunder-represented areas."}, {"title": "Ethical Compliance and Acknowledgements", "content": "Ethics Statement\nSafe and ethical use of biomedical data: We prioritize safe and ethical research practices while creating \u03bc-Bench.\nAll public datasets with patient-derived histopathology images had already been de-identified by the dataset's original\nauthors in compliance with applicable privacy laws and institutional guidelines. The public histopathology image\ndata and metadata were reviewed, and it was determined that it was not possible to identify any individual from the\nde-identified data. The Stanford Institutional Review Board guidelines were reviewed and discussed, and the use of the\nimages was determined not to be human subjects research.\nThe \u00b5-Bench cognition images and questions were voluntarily submitted by a small (<10) number of biology/pathology\nusers alpha-testing a free web chat application. There was no intervention, experiment vs. control group, or research\nquestion during the alpha testing. There was no greater risk of using the app than other internet apps. At registration,\nusers agreed to the service terms, which included releasing image and text data under CC-BY-SA-4.0.\nConsent and data usage: We thank and respect the original dataset authors and use data according to the original\ncopyright and license. \u00b5-Bench was developed with both academic and commercial research in mind. Many datasets\nare a version of CC-BY-4.0 to allow both academic and commercial usage. However, some data restricts commercial\napplications via non-commercial clauses or CC-BY-NC-4.0-related licenses. While creating \u00b5-Bench we significantly\nimproved the data by performing expert review, quality control/standardization, expert labeling with biomedical\nontology codes, and creating multiple-choice VQA questions and captions for each image. When the original dataset\nlicense is CC-BY-4.0, we release our \u00b5-Bench versions under a permissive CC-BY-SA-4.0 to foster a transparent and\ncollaborative benchmark. For the subset with CC-BY-NC-SA-4.0, we respect the original license and release these data\nunder CC-BY-NC-SA-4.0.\nBias and data diversity: We recognize that AI models, including VLMs, can perpetuate or exacerbate biases in the\ntraining data, and incorporating diverse data may mitigate these biases. Diversity is key to understanding biological\nprocesses and how they vary across biological sex, ethnicity, or other factors. When possible, we annotate cell lines\nwith age, sex, ethnicity, and other metadata from Cellosaurus or the Cell Line Ontology. We consciously include\ndiverse microscopy images from multiple institutions across various organisms, modalities, and biological states to\nensure \u00b5-Bench provides an accurate and fair performance assessment. However, images of human samples and\nbrightfield/fluorescence microscopy are more common in the field and thus over-represented in \u00b5-Bench ."}, {"title": "Conflicts of Interest Disclosures", "content": "The authors have no conflicts of interest to disclose."}, {"title": "Funding/Support", "content": "This research was supported by NIH grants (NIH#P30AG066515 to JJN), the Chan Zuckerberg Initiative Neurodegener-\nation Challenge Pairs Pilot Project to SYL (2020-221724, 5022), the Wu Tsai Knight Initiative Innovation grant (#KIG\n102) to SYL, Arc Institute to AL, Quad Fellowship to JB, and NSF Graduate Research Fellowship (#DGE-2146755)\nand Stanford Graduate Fellowship to AU. SYL is a Chan Zuckerberg Biohub \u2013 San Francisco Investigator."}, {"title": "Acknowledgments", "content": "We thank all the domain experts for testing the web app and submitting questions that were eventually used in \u00b5-Bench\ncognition. We highlight the contributions from Pedro Guedes-Dias, Andrew Moore, and Julian Perez. We appreciate\nfeedback from Josiah Aklilu and Orr Zohar on earlier manuscript versions."}, {"title": "Benchmark Details", "content": "Instructions for downloading the benchmark\nThe benchmark can be downloaded via HuggingFace Datasets at the following url."}, {"title": "\u03bc-Bench overview", "content": "\u03bc-Bench is organized into three main categories:\n1. \u03bc-Bench Perception (Coarse-grained): Containing basic questions about the type of biomedical field of\nstudy (domain), subdomain, microscopy modality, submodality, and stain.\n2. \u03bc-Bench Perception (Fine-grained): Identification or questions regarding a biological cell, cellular process,\nsubcellular or tissue structures, biological state (normal/abnormal), etc.\n3. \u03bc-Bench Cognition (Reasoning): Expert-generated questions that typically require visual-based reasoning or\nintegrating knowledge about the micrograph's composition and subject to deduce an answer.\n4. \u03bc-Bench Object Detection (Localization): Bounding box detection of common biological objects, with an\neasy and hard data split."}, {"title": "\u03bc-Bench statistics", "content": "Table 2 shows descriptive statistics for \u03bc-Bench Perception while Table 4 shows statistics for \u03bc-Bench Reasoning."}, {"title": "\u03bc-Bench Perception", "content": "\u03bc-Bench Perception contains 17,235 microscopy images collected from 25 unique datasets of 96 different cell and\ntissue types across light, fluorescence, and electron microscopy. If defined, only images from each dataset's test set are\nselected; otherwise, a random 15 percent split is created. Each cell/tissue class is sub-sampled to a maximum of 200\nsub-classes per class (if the cell type is less than the maximum, the full set is used). Each cell/tissue type is densely\nannotated (then propagated to each instance of the same type) by an expert, as shown in Figure 18"}, {"title": "\u03bc-Bench Perception (Coarse-grained)", "content": "Hierarchical metadata for each of the 17,235 perception images and task-\nspecific templates (shown in Table 17) are used to create 5 coarse-grained questions and captions regarding microscopy\nmodality, submodality, domain, subdomain, and staining technique. The use of hierarchical metadata enables the\ngeneration of options within each hierarchical level. For example, for microscopy submodality, we leveraged microscopy\nmodality metadata (shown in Figure13 ) to randomly sample submodality options within a microscopy modality (e.g.\ndifferential interference contrast microscopy within light microscopy). A total of 86,175 (17,235x5) coarse-grained\nquestions are generated using this approach, leveraging domain metadata Figure 14 as well as staining metadata Figure\n15). Section H provides 10 random examples of coarse-grained data points (two per type)."}, {"title": "\u03bc-Bench Perception (Fine-grained)", "content": "Metadata for each of the 25 unique datasets and custom prompts (Table 2) are\nused to generate 13 unique tasks, which are 13 (17,235 unique question-image pairs) closed VQA style questions. of\nVQA fine-grained tasks."}, {"title": "Perception dataset: source details", "content": "Table 3 lists the dataset names, licenses, and corresponding DOIs or URLs for\ndataset used. The majority of these datasets were sourced from open data repositories, such as Zenodo, Dataverse,\nDryad, BBBC, EMPIAR, Kaggle, and various project websites."}, {"title": "Perception dataset: forming questions for evaluation", "content": "Each sample has a question and set of candidate answers. We\ndescribe how to evaluate this VQA task in appendix D."}, {"title": "\u03bc-Bench Cognition", "content": "Cognition dataset: generation details\nThe cognition dataset was collected during alpha-testing of a web application chat interface. A group of 6 biology and\npathology experts were invited to interact with a web application as they wished during their daily routines (invitation is\nshown in Figure 20). There was no specific research question, intervention, or experimental vs. control group; this was\na free service. User registration was voluntary and required reviewing and accepting the terms of service. The terms\nof service discussed that this was not a research study and that the risk of harm is insignificant and would be similar to any\nVLM chatbot. The terms of service indicated that uploading images indicated the user had copyright or permission, that\nimages did not contain offensive content, and that the images and text could be released with a CC-BY-SA-4.0 license."}, {"title": "\u03bc-Bench Object detection", "content": "For datasets with segmentation annotations, we copy the segmentation annotations and also convert them to object\ndetection annotations, giving 3641 images [50, 54, 61, 65, 62]."}, {"title": "Comparison with a supervised linear model trained on DINOv2 Features", "content": "Although some datasets have been used in the biomedical computer vision community [58, 59], others do not have well-\nestablished performance baselines. Thus, we evaluate a supervised linear model's baseline classification performance\nfor the fine-grained and coarse-grained perception tasks. This establishes that the questions in our benchmark are\nsolvable using image features."}, {"title": "VQA evaluation details", "content": "Three out of four tasks are formulated as multiple-choice visual question answering: coarse-grained perception, fine-\ngrained perception, and cognition. We describe their evaluation here, while the fourth task of object detection is\ndescribed separately in appendix E.1. For each sample, i, we have an image, $x_i$ a question string, q, and a set of k\ncandidate answer strings ${a_{ij}}_{j=1}^k$.\nFor the autoregressive models, $f_a$, we first generate a query string, t, from the question and candidate answer strings,\n$t(q, \\{a_{ij}\\}_{j=1}^k)$, using the template in Figure 19. The prompt text instructs the response to start with a letter for one of\nthe multiple choice answers, (\u2018A', 'B', . . . ). We pass the prompt string and image to the model and decode the output,\n$y = f(x_i, t)$, where y is the output string. We have two strategies for matching the response string, y, to the candidate\nanswers $\\{a_{ij}\\}_{j=1}^k$. First, for each j, we check whether the answer string is in the output: $a_{ij} \\in y$ with lowercase string\nmatching. This is important because models do not always follow the instructions to output the multiple choice letter"}, {"title": "Object detection evaluation details", "content": "We evaluate the two models that support localization: QwenVL [21] and PaliGemma [43], and follow their user guide\nfor prompting. For \u2018QwenVL' the prompt is \u2018Detect {class_name}", "Detect\n{class_name}; {class_name}": "where the repeated class name indicates that multiple instances may be predicted.\nOur early experiments found that PaliGemma would sometimes fail to localize any instances using detection prompting,\nbut would localize them with segmentation prompting. So if PaliGemma does return zero instances, we prompt for\nsegmentation \"Segment {class_name}; {class_name}\" and extract the bounding box. The Burgess et al. dataset\nhas two classes, so we prompt the model one at a time. Both models output detection predictions as a string with a\nstandardized structure, which we parse using regex.\nWe use the GRIT localization metric [47] because it is well-motivated and has previously been used in VLM evaluations\nby QwenVL [21]. The score is:\n$\\sum_{i=1}^M \\frac{IoU_i}{P+ G_{missed}}$                                                                                                 (1)\nThere are M ground truth boxes, and P predicted boxes, which are matched using the Hungarian algorithm on the\nIoU metric. $G_{missed}$ is the number of predicted boxes not matched to a ground truth box. Intuitively, this metric\nmeasures the average IoU for matched boxes, while penalizing making too many predictions using $G_{missed}$ (similar to\nthe precision metric). Note that we cannot use the more typical mAP score from object detection because they depend\non a threshold for controlling the false-positive rate, which these VLMs do not support."}, {"title": "Additional Benchmarking Results", "content": "Object Detection Results\ntable 5 summarizes object detection for the datasets having object localization annotations.\nOverall, the localization scores are very poor, which is expected since both models are generalist and object localization\nhas received relatively less attention in autoregressive VLMs. Looking at the splits:\nEasy split. Although both models have higher scores for the 'cell' class in Burgess et al., they still fall below\n80, and the task is extremely easy. The story is similar for \u2018nucleus' in Held et al., but for QwenVLM, the\nscores are even lower.\nHard split. All models perform poorly on the hard split. In Burgess et al \u2018nucleus', PaliGemma scores around\n30, however qualitative inspection shows that in most cases, the bounding box predicts the entire cell, which\nencapsulates the nucleus. Similarly, in Wu et al., the mitochondria class scores more than 20 for both models,\nbut qualitative inspection shows that the prediction is usually a box around the entire image. We find the same\npattern in 'Sirinukunwattana et al.' for 'gland' detection."}, {"title": "Weight ensembling details", "content": "In the results section 5.2, we consider PLIP, which was fine-tuned from OpenCLIP, and QuiltNet, which was fine-tuned\nfrom CLIP, both using pathology data. Since we have benchmark results for all these models, our evaluation can"}, {"title": "Model Performance on Pathology Specific Tasks", "content": "While prior evaluations show that general contrastive VLMs have some biology and pathology knowledge (a finding\nalso reported in [25] and [44]), most specialist models analyzed in this work were fine-tuned. To this end, we analyzed\nthe performance on pathology-only tasks and found similar rankings."}, {"title": "Additional Benchmarking details", "content": "Model Details\nTable table 9 provides a breakdown of model parameters and training data (with dataset size), specialist models include\ntheir base model."}, {"title": "Computing confidence intervals", "content": "Error bars represent 95% confidence intervals (CI) computed via nonparametric bootstrapping using the SciPy\nstats.bootstrap function with 1000 resamplings and default settings. No data were excluded from the analyses."}, {"title": "Zero-shots results broken down by task", "content": "Figure 9 presents a breakdown of perception coarse-grained results by task. It reveals that autoregressive generalist\nmodels perform well across all tasks, as indicated by the overall averages. Notably, while GPT-40 dominates most tasks,\nPaliGemma excels in domain identification, achieving the best performance in that specific area.\nFigure 10 shows a breakdown of biology-specific perception fine-grained results by task. While GPT-40 dominates\nin some tasks, other models excel in specific tasks. For instance, ALIGN outperforms all models in molecular\ncolocalization, while BiomedCLIP has the best performance in mitochondrial morphology classification.\nFigure 11 shows a breakdown of pathology-specific perception fine-grained results by task. This breakdown reveals that\nwhile GPT-40 has the best performance in three tasks, specialist models still outperform it in amyloid morphology [a]\nand Pap smear grading."}, {"title": "Computing resources", "content": "One benefit of \u03bc-Bench is that it is amenable to use by academic labs of all sizes, even those with limited resources. All\nevaluation tasks could be run on one NVIDIA a6000 (48GB VRAM), except for QWenVL, where we used one A100\n(80GB VRAM). Inference required approximately 3.5 for all of \u03bc-Bench running one dataset at a time (requiring one\nGPU at a time). The computations were performed on-premises university compute environment, with 1024 CPU cores\nthat are AMD EPYC 9334, 2.70GHz."}]}