{"title": "From Text to Life: On the Reciprocal Relationship between Artificial Life and\nLarge Language Models", "authors": ["Eleni Nisioti", "Claire Glanois", "Elias Najarro", "Andrew Dai", "Elliot Meyerson", "Joachim Winther Pedersen", "Laetitia Teodorescu", "Conor F. Hayes", "Shyam Sudhakaran", "Sebastian Risi"], "abstract": "Large Language Models (LLMs) have taken the field of AI by\nstorm, but their adoption in the field of Artificial Life (ALife)\nhas been, so far, relatively reserved. In this work we investi-\ngate the potential synergies between LLMs and ALife, draw-\ning on a large body of research in the two fields. We explore\nthe potential of LLMs as tools for ALife research, for exam-\nple, as operators for evolutionary computation or the genera-\ntion of open-ended environments. Reciprocally, principles of\nALife, such as self-organization, collective intelligence and\nevolvability can provide an opportunity for shaping the devel-\nopment and functionalities of LLMs, leading to more adap-\ntive and responsive models. By investigating this dynamic\ninterplay, the paper aims to inspire innovative crossover ap-\nproaches for both ALife and LLM research. Along the way,\nwe examine the extent to which LLMs appear to increas-\ningly exhibit properties such as emergence or collective in-\ntelligence, expanding beyond their original goal of generat-\ning text, and potentially redefining our perception of lifelike\nintelligence in artificial systems.", "sections": [{"title": "Introduction", "content": "Artificial life (ALife) refers to the study and creation of life-\nlike systems using computer models, algorithms, and, some-\ntimes, physical hardware. Its goal is to understand the fun-\ndamental principles of living systems, to explore the pro-\ncesses underlying life, and to develop new forms of life that\nmay exhibit lifelike behaviours. ALife research typically in-\nvolves simulating and studying various aspects of biological\nlife, such as evolution, self-organization, adaptation, repro-\nduction, and emergence. These simulations may range from\nsimple models of individual organisms to complex ecosys-\ntems with multiple interacting species [60, 109, 4]. Re-\ncently, the Artificial Intelligence (AI) community has started\nto increasingly embrace such ALife concepts (see Fig. 1).\nMuch of the current focus in AI research has shifted to\nLarge language models (LLMs). Beyond their impressive\nability to generate text in natural language, LLMs are part\nof an emerging debate on their ability to act as agents that\nemulate aspects of human behaviour. Computationally, both\nALife forms and LLMs can be seen as autoregressive models\ncapable of processing information and producing complex\nsequences of patterns. However, modern neural network\narchitectures such as the Transformer [225]-the architec-\nture underpinning LLMs- have been shown to have suf-\nficient computational expressivity to, not only model com-\nplex sequences, but also to serve as general-purpose comput-\ners [76, 128]. This parallelism between the information pro-\ncessing mechanisms of artificial life-forms and transform-\ners, particularly their concurrent processing capabilities, has\nbeen underexplored in the ALife literature 1.\nThis paper takes a deeper look into the current and po-\ntential\u2014 interplay between these two fields of research, in-\nvestigating the mutual benefits and potential advancements\narising from their intersection. First, we aim to uncover\nhow LLMs can be harnessed as potent tools within ALife\nresearch, for example by enacting powerful new mutation\noperators for evolutionary computation. Second, we believe\nthat the principles of ALife can reciprocally illuminate and\nhelp to expand the functionalities of LLMs. Finally, the\nrecent emergence of agents powered by LLMs with di-\nverse architectures, sensory modalities, or even social struc-\ntures known as LLM Agents- raises a provocative ques-\ntion: could LLM agents themselves be considered a form\nof ALife? While the view of LLMs as stochastic parrots\nis a sensible and important critique [21], considering LLM"}, {"title": "Background", "content": "Artificial Life\nWhile a review of ALife is beyond our scope (cf. [60, 109,\n4]), we aim here to provide relevant background on funda-\nmental questions in this field. The study of biological life\nhas been dominated by two distinct perspectives: (a) The\nevolutionary perspective, exemplified by Neo-Darwinism,\nstates that a system implements life as long as it exhibits\nthe properties of multiplication, variation, and heredity and\ncan be extended to consider other processes, such as de-\nvelopment and symbiogenesis [105]. This paradigm fo-\ncuses on the evolution of open-endedness and/or complex-\nity, given basic evolutionary operators [205, 17]. Exam-\nples include systems of self-replicating, evolving individu-\nals such as Terraria [182] and Avida [154]. (b) The eco-\nlogical perspective, exemplified by the autopoietic systems\npopularized by Varela [224, 223], focusing on the ability\nof individuals, viewed as structures engaged in energy and\nmatter exchange with their environment, to maintain them-\nselves in the face of environmental perturbations. Examples\nsystems here include Cellular Automata [46] and Artificial\nChemistries [69], which examine how self-replication mech-\nanisms emerge from interactions among non-living compo-\nnents. What is the objective of ALife as a scientific disci-\npline? The answer is two-fold: (a) understanding the essen-\ntials of life by simulating bottom-up processes that imitate\nthe biological ones, i.e. ALife as the study of life-as-it-is.\n(b): building systems that have lifelike properties and can\nbe viewed as life, independently of their substrate, i.e. AL-\nife as the study of life-as-it-could-be [116, 165, 183].\nLarge Language Models\nLanguage models (LMs) can be understood as probabil-\nity distributions over sequences of text elements [231, 22].\nCommonly, LMs aim to predict the next text element, called\na token, conditioned on the previous ones [30]. This training\napproach yields generative models of language: tokens are\nsampled one after the other \u2014i.e. autoregressively- based\non the learned distribution. The tipping point in Language\nModelling research occurred with the arrival of the mas-\nsively parallel Transformer architecture[225]\u00b2, whose self-\nattention mechanism allowed both larger-scale training and\nlong-range dependency modelling, and offered compelling\nperformance. The recent discovery of empirical scaling\nlaws [108, 90] -predicting test perplexity [101] as a power\nlaw of a model's and dataset's size, and correlating perplex-\nity with downstream task performance-, spurred researchers\nto train large LMs (LLMs)\u00b3 to the 100s of billion parame-\nters and to the trillion tokens [30, 180], requiring massive\ncompute and internet-scale data. Some emergent capabili-\nties have been discussed [239, 194], reminiscent of complex\nsystems [241], such as zero- or few-shot learning [30],4\nSteering LMs towards human-preferred responses solely\nthrough next-token prediction proved challenging, so ad-"}, {"title": "LLMS AS TOOLS FOR ALIFE", "content": "LLMs can find diverse applications that stretch far beyond\ntheir obvious use in text generation. Here we review existing\nworks that incorporate LLMs in ALife studies, organizing\nthem into five main threads of research.\nArtificial Evolution and LLMs\nArtificial evolution is a powerful optimization algorithm for\nexploring arbitrary search spaces but comes with the down-\nside of slow exploration due to random mutations and un-\nThis has so far limited the appli-\ninformed cross-over.\ncation domains of techniques such as Genetic Program-\nming [111, 121]. Here we highlight different ways in which\nthe ALife community incorporated LLMs in evolution.\nEvolution through Large Models (ELM) employed an\nLLM as an intelligent mutation operator for evolving pro-\ngrams that control robotic morphologies [121]. The LLM\nembodies the ability of humans to intentionally modify pro-\ngrams to achieve a desired functionality. This is achieved\nby fine-tuning the LLM on data available in online code\nrepositories, where humans employ version control to log\ncode modifications, accompanying each modification with\nan explanation in natural language. By replacing ran-\ndom mutations with intelligent ones, ELMs can explore\nrobotic morphologies much quicker than genetic program-\nming and can zero-shot produce functional morphologies\nfor a user-defined terrain. Another example is the LMX ap-\nproach [139], which employs LLMs as intelligent, domain-\nindependent cross-over operators by leveraging their in-\ncontext learning abilities. The LLM is prompted with ex-\namples of genotypes and, due to its pattern-completion abil-\nity, acts as a probabilistic model for generating offspring,\nakin to models employed in classical evolutionary strategies,\nsuch as CMA-ES [87]. EvoPrompting [42] and Prompt-\nBreeder [68] employ LLMs as both mutation and cross-\nover operators and showcase that evolving the prompts for\nin-context learning can further improve performance. In\na more holistic approach, EvoLLM models the LLM as a\nblack box that embodies an end-to-end evolutionary algo-\nrithm [114]. The LLM is prompted with a set of solutions\nand their respective fitnesses and is tasked with providing\nnew solutions. QDAIF [26, 175, 192] utilizes LLMs to both\nvary/rewrite texts and evaluate qualitative attributes of qual-\nity/diversity in subjective writing and code, to select for a\npopulation of text solutions to be more diverse and refined.\nOverall, this line of work has shown that LLMs hold great\npotential in improving the speed and applicability of evolu-\ntionary search [251]. By operating in the space of code, such\ntechniques enjoy the generality of Genetic Programming and\nhave already been employed for optimizing robotic mor-\nphologies [121, 139], neural architectures [150] and con-\ntrol policies [139]. Moreover, benefits are reciprocal [251]:\nby subjecting LLM components to evolution [42, 68] and\nQuality-Diversity optimization [121, 139, 150], these sys-\ntems can self-improve, hinting at an open-ended process.\nEnvironment generation through LLMs\nEnvironments play a central role in open-ended evolution,\nas they set the limit for the phenotypic complexity a sys-\ntem can exhibit [207, 235]. Generating useful environments\nfor studying ALife is as challenging a problem as creating\nagents that solve them [49]. Techniques such as Procedu-\nral Content Generation [198] can automate this process, but\nface a long-standing challenge; balancing the diversity and\noriginality of environments with controllability.\nRecent works in ALife started exploring the automated\ngeneration of environments through LLMs [112], by en-\ncoding pixels as text. Todd et al. [220] explored the po-\ntential of LLMs in PCG by fine-tuning an LLM on a two-\ndimensional puzzle game and showcased that, despite lack-\ning biases for spatial arrangements that previous PCG mod-\nels such as CNNs and CA exploited, LLMs can create di-\nverse and playable games levels Sudhakaran et al. [214] in-\ntroduced MarioGPT for generating diverse and controllable\nlevels for Super Mario by employing Novelty Search and\nconditioning the LLM's output on a textual description of\nthe level. Zala et al. [262] incorporated LLMS into the cur-\nriculum learning paradigm, where the performance of agents\nis fed back to the environment generation process to avoid its\nhalt. Generative Interactive Environments (Genie) is a train-\ning paradigm that extended the paradigm of foundational\nmodels to the generation of Platformer games [31]. Genie\ntrained a Transformer on a large corpus of virtual worlds de-\nscribed through text, images, and sketches and showcased\nthe potential of this line of research for the large-scale gen-\neration of environments. Code-generating LLMs [121] can\nbe seen as universal environment generators and, when em-\nbedded within an optimization process that encourages di-\nversity [121] and self-improvement [68], can hold great po-\ntential for the generation of open-ended environments [121]."}, {"title": "On Internal States", "content": "Exploration through LLMs\nPlay or intrinsic motivation is a vital component of ex-\nploration in open-ended spaces, facilitating, among oth-\ners, the discovery of emergent patterns in self-organized\nsystems [184, 85, 83, 56] and skill-acquisition with RL\nagents [70, 170]. Similarly to evolution, intrinsically mo-\ntivated exploration faces the challenge of quickly discover-\ning interesting information in large search spaces. Here, we\nhighlight works leveraging LLMs for guiding exploration.\nLin et al. [130] recommend interpreting the auto-\nregressive ability of LLMs as a powerful form of self-\nprediction that can be useful for predicting next states in\nembodied environments. LLMs have been found particu-\nlarly useful for improving the exploration abilities of agents\nin open-ended tasks. For example, they can help assess\nthe achievability of goals based on the agent's abilities,\nmaintain libraries of skills, as well as plan by generating\ngoals [62, 51] and decomposing tasks into achievable sub-\ngoals [153, 237, 233],\nLLMs as models of human behaviour\nALife has long been concerned with understanding and\nreplicating the emergence of collective and individual phe-\nnomena in human populations. Studies are, however, often\nlimited in their complexity and controllability. At the same\ntime, human notions such as interestingness, surprise, and\ncreativity are elusive to define in a computational way that\nwill enable optimizing ALife systems. Here, we highlight\nworks showcasing that LLMs have captured biases that ex-\ntend beyond language to behaviours and notions related to\nsocial interactions, economic decision-making, innovation\nand interestingness.\nWhen prompted to impersonate different characters, a\ngroup of LLM agents in a game inspired by The Sims,\nemerged convincing social interactions [163]. When eval-\nuated for their decision-making in economic studies, LLMs\nwere found to exhibit human-like biases such as fairness and\nstatus quo [2]. LLMs may also hold promise as tools for cul-\ntural evolution studies as they exhibit human-like biases in\ninformation transmission [2, 171]. By capturing the human\nnotion of interestingness, LLMs can be leveraged in open-\nendedness research [266].\nDespite being able to model certain aspects of human be-\nhaviour, LLMs' ability to model human cognition and psy-\nchology is heavily debated [126, 142, 199, 144]. Criticism\nquestions their ability to use symbolic and complex abstrac-\ntion and reasoning [143], utilize cognitive maps for plan-\nning [144], their \"understanding\" of language and if they\nhave a Theory of Mind [222]. Yet, despite being in some\nways fundamentally distinct, they provide an opportunity for\ncognitive science as they allow for the manipulation and ob-\nservation of input data in ways not possible with human sub-\njects [247, 86], and open up interesting questions on whether\ncertain behaviors can emerge purely from textual data.\nLLMs as scientific collaborators\nAI holds the promise of changing the way we do sci-\nence [209, 110, 138, 1], including ALife research [65],\nLLMs trained on large scientific corpora putatively capture\nknowledge about the natural world in the form of language\ncorrelations [172]. While we acknowledge the debate on\nwhether these correlations can be considered a strong form\nof knowledge in an epistemological sense, recent research\nsuggests that LLMs hold potential as scientific collabora-\ntors [113, 91, 132, 163, 178, 270]."}, {"title": "ALIFE FOR LLMS", "content": "Reflecting on the potential of LLMs as instruments for AL-\nife naturally leads us to the reciprocal inquiry: how can in-\nsights from ALife research enhance LLMs? We review some\nfundamental characteristics defining (a)life [4, 60, 109], and\nsketch possible parallels with LLM agents 5 (see Fig. 2 for\nan illustration of these parallels).\nInternal states of organisms, emerg-\ning from the dynamic interactions with their environment,\nand their broader developmental and evolutionary history,\nguide their behaviour and decision-making [25, 260, 140].\nDifferently from living organisms, LLMs lack of self-\nsustained activity [134, 73, 226, 228, 34, 193] The internal\nstate of an LLM is entirely a function of the current con-\ntents of its context window. A main bottleneck when scal-\ning up Transformer architectures is the fact that the compu-\ntational requirements of the standard self-attention mecha-\nnism scale quadratically with the length of the context [217].\nThe limited context window can lead to temporal incon-\nsistencies and erratic behaviour when exceeded due to the\ndiscrete capacity7. Some efforts in LLM research are cur-\nrently directed towards addressing these constraints by aug-\nmenting them with external memory (e.g. with Retrieval-\naugmented generation) [81, 125, 72], sometimes including\nabstractions of the raw observations [163, 201], or towards\nmore self-sustained activity (via incorporating internal feed-\nback mechanisms, or recurrent LLM streams) [265, 269].\nFurther, a wealth of research aims at mitigating the limita-\ntions set by the quadratic requirements of the self-attention\nmechanism [196, 261, 168, 167, 43, 131, 58, 253, 104].\nAutonomy"}, {"title": "Cultural Evolution", "content": "Definition 1. Autonomy refers to the degree to which a sys-\ntem can 'govern' itself (i.e. make decisions, take actions)\nWe deliberately avoid discussing the metaphysics of these\nproperties, such as emergence or autonomy, assuming they are\nepistemological not ontological and hence solely exist in the\neyes of the observer and not in the systems themselves.\nLLMs remain dormant, waiting for a function call, without\ncognitive processes running in the background.\n\"Architectures like RWKV [166] (akin to RNN at inference)\nseem more appropriate for continual processing than Transformers."}, {"title": "Self-Replication", "content": "based on its own objectives, internal states, history or pro-\ncesses, without direct external control (e.g. intervention, su-\npervision or instruction).\nWhile the notion of autonomy is complex and poly-\nsemic [24, 189, 18, 71], in a permissive understanding, most\nartificial agents and LLM agents [254, 238, 269, 127, 233]\ncan be considered autonomous, from the moment they have\nsome \"control\" over how to act. Self-sustainability, i.e. re-\nquiring the system's ability to manage and regulate the flow\nof matter and energy, to sustain themselves, is often consid-\nered out of reach for digital agents.\nAgents that exhibit the ability to self-optimize their be-\nhaviors in the absence of external rewards demonstrate a\ncertain degree of autonomy. For instance, self-rewarding\nagents can adjust their strategies or behaviors by generating\ntheir own feedback mechanisms (e.g. Fig. 3, [107, 83, 258]).\nDynamic, self-driven enhancement of LLMs [160], leverag-\ning experiences and self-generated data to iteratively refine\ntheir model, had broad applications from debugging [44] to\nharmlessness/helpfulness [10]. Self-improving LLM agents,\neither display simple behavioral adaptation via in-context\nlearning or exhibit more structural adaptation. The former\ncan gradually refine their outputs [78] when integrated with\nan iterative workflow e.g. a critique-refine approach [135,\n44, 201]. The latter improve their internal model through\nself-reinforcing loops, e.g. fine-tuning additional parameters\non self-generated [33, 80] or self-assessed [96, 8] data, or\nby optimising their architecture [269]. In some cases, train-\ning LLMs on such self-generated, synthetic data has been\nshown to worsen performance, as it may go against prior op-\ntimisation or lead to cascading hallucinations [103]. How-\never, combining synthetic with real data can lead to a better\ncost/performance trade-off [75, 155].\nDefinition 2. Autotelic systems can learn to represent, gen-\nerate, select and pursue their own goals, thereby guiding\ntheir actions/decisions autonomously.\nSuch behaviour ranges from being instinctual, and short-\nsighted, as in bacterial chemotaxis toward nutrients, ants for-\naging for food based on pheromone trails, or birds migrat-\ning seasonally following instinctual patterns, to being so-\nphisticated and deliberate, as seen in humans contemplat-\ning long-term future pathways. Drawing on essential con-\ncepts like intrinsic motivation from autonomous develop-\nmental learning [156], both AI and ALife researchers have\nproposed agents that could self-generate or self-represent\ntheir own goals [70, 170, 50], without supervision, driven\nby curiosity [162], surprise, or empowerment [45], framed\nin an information-theoretic sense. ALife works have also in-\nvestigated how such capacity may speed up language emer-\ngence [52], relate to affordances [5], or the emergence\nof collective behaviors [83], among other things. As\npreviously discussed, several LLM agent frameworks pro-\npose planning, goal decomposition, or even goal genera-"}, {"title": "Self-Organisation", "content": "Definition 3. Self-Replication can manifest in two ways:\n(a) system-level self-replication is the process by which a\nsystem makes a copy of itself or its components (b) pattern-\nlevel self-replication is when a system makes copies of cer-\ntain outputs/behaviors it produces.\nBy enabling the transmission of information across gen-\nerations when coupled with evolution, Self-Replication is\na pivotal feature of Life. In ALife, self-replication has\nbeen an historical quest [218, 204, 169", "229": "Wolfram's [246", "116": "to Kauffman's work on autocatalytic\nnetworks [95", "39": ".", "28": "LLMs Agents show early forms of\nsystem-level self-replication, as they offload parts of their\ncognitive processes by instantiating \u201csiblings\u201d to divide and\nconquer a given task [254, 93, 269", "9": ".", "74": "and beyond [94", "223": "snowflake forma-\ntion [159", "11": "to neural cellular\nautomata [145, 215, 38, 152", "244": "."}]}