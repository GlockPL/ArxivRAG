{"title": "Textured 3D Regenerative Morphing with 3D Diffusion Prior", "authors": ["Songlin Yang", "Yushi Lan", "Honghua Chen", "Xingang Pan"], "abstract": "Textured 3D morphing creates smooth and plausible interpolation sequences between two 3D objects, focusing on transitions in both shape and texture. This is important for creative applications like visual effects in filmmaking. Previous methods rely on establishing point-to-point correspondences and determining smooth deformation trajectories, which inherently restrict them to shape-only morphing on untextured, topologically aligned datasets. This restriction leads to labor-intensive preprocessing and poor generalization. To overcome these challenges, we propose a method for 3D regenerative morphing using a 3D diffusion prior. Unlike previous methods that depend on explicit correspondences and deformations, our method eliminates the additional need for obtaining correspondence and uses the 3D diffusion prior to generate morphing. Specifically, we introduce a 3D diffusion model and interpolate the source and target information at three levels: initial noise, model parameters, and condition features. We then explore an Attention Fusion strategy to generate more smooth morphing sequences. To further improve the plausibility of semantic interpolation and the generated 3D surfaces, we propose two strategies: (a) Token Reordering, where we match approximate tokens based on semantic analysis to guide implicit correspondences in the denoising process of the diffusion model, and (b) Low-Frequency Enhancement, where we enhance low-frequency signals in the tokens to improve the quality of generated surfaces. Experimental results show that our method achieves superior smoothness and plausibility in 3D morphing across diverse cross-category object pairs, offering a novel regenerative method for 3D morphing with textured representations.", "sections": [{"title": "Introduction", "content": "Morphing [Lin et al. 2024] generates an interpolation sequence between a source and a target, requiring smooth and plausible transitions. This fundamental technique is crucial for creative applications like visual effects in film and media. Depending on the data type, morphing is categorized into image morphing [Shechtman et al. 2010; Zhang et al. 2024] and 3D morphing [Geng et al. 2024; Kim et al. 2024; Tsai et al. 2022]. Compared to image morphing, 3D morphing aligns more naturally with visual effects production, where objects and transformations inherently operate in 3D space. However, 3D morphing is more challenging, requiring the interpolation of 3D objects holistically (i.e., image morphing can be viewed as a special case of 3D morphing from a specific viewpoint). Our work addresses the task of textured 3D morphing, which generates a sequence for two textured 3D representations, aiming for smooth and plausible transitions in shape and texture, as shown in Fig. 1.\nPrevious 3D morphing methods mainly focused on morphing shapes, which can be summarized in two steps: first, establishing correspondence [Deng et al. 2023] between the 3D representations of the source and target objects, and second, determining smooth and plausible deformation trajectories between corresponding 3D points [Eisenberger et al. 2021]. Following these steps, previous methods blend the two 3D representations with respective weights to obtain a sequence of interpolated 3D representations.\nDue to the scarcity of topologically aligned and textured 3D datasets, previous 3D morphing methods have mainly focused on in-domain untextured datasets, such as FAUST [Bogo et al. 2014] (human shapes) and Shrec'20 [Dyke et al. 2020] (quadruped animals). As a result, these methods are limited to shape-only morphing and face two key generalization challenges: (a) Labor-Intensive Preprocessing: Morphing new in-domain 3D data with these methods [Eisenberger et al. 2021; Zhan et al. 2024] requires domain-specific alignment with the training datasets through tedious registration [Sun et al. 2024] and matching [Zhu et al. 2024] steps. (b) Limited Morphing Capacity: The methods [Ayd\u0131nl\u0131lar and Sahillio\u011flu 2021; Eisenberger et al. 2021; Vyas et al. 2021; Zhan et al. 2024] suffer from limited object diversity and small datasets, leading to ambiguous and implausible interpolations.\nThe limitations of previous methods inspire us to consider two critical questions: (a) Is explicit point-to-point correspondence truly necessary? (b) Can we enhance the generalization capability of textured 3D morphing via a generic generative prior?\nFor correspondence, obtaining dense correspondences between textured 3D representations across categories remains underexplored [Zhu et al. 2024] despite being foundational to previous methods, while explicit constraints can instead limit the creativity of morphing. Therefore, a promising method is using labor-saving implicit correspondences [Lan et al. 2022; Yang et al. 2024] to guide the morphing. For instance, morphing could be formulated as an optimization problem, allowing correspondences to emerge automatically [Tsai et al. 2022]. Alternatively, attention mechanisms in generative models could enable automatic alignment during object blending [He et al. 2024; Shen et al. 2024; Zhang et al. 2024].\nFor generative priors, recent advancements in diffusion-based generation models offer two strategies for 3D morphing: using 3D diffusion models directly [Chen et al. 2024a; Lan et al. 2025b; Xiang et al. 2024], or enhancing 2D models with 3D priors. However, those hybrid 2D-3D methods [Haque et al. 2023; Kim et al. 2024; Poole et al. 2022; Rombach et al. 2022] face significant challenges: 2D models lack a holistic 3D knowledge, and optimizing the 2D-3D mapping is difficult, with no guarantee that the morphing process will maintain 3D consistency across viewpoints. Therefore, directly using a 3D diffusion model to regenerate the interpolated 3D representations (i.e., regenerative morphing) enables authentic 3D morphing and has the potential to be scaled up by using a more state-of-the-art 3D generation model.\nBuilding on the above analysis, we adopt a generic 3D diffusion prior that leverages its implicit correspondence and 3D generation capabilities to blend source and target information, enabling the regeneration of interpolated textured 3D representations.\nSpecifically, we first introduce a 3D diffusion model [Lan et al. 2025b] and interpolate the information from the source and target at three levels: initial noises, model parameters, and condition features. The 3D representation for each interpolation is then regenerated using the 3D generation model. To improve the 3D diffusion model's ability to generate smooth morphing sequences, we explore an Attention Fusion strategy. However, fusing different information weakened the model's denoising capability, and aggressively applying Attention Fusion to all denoising steps for smoother morphing resulted in implausible outcomes. Therefore, to improve the plausibility of semantic interpolation and the generated 3D surfaces, we propose two strategies: (a) Token Reordering: After semantic analysis, we identify semantic correspondences in diffusion tokens and propose matching approximate tokens before attention computation to better guide implicit correspondences in the diffusion space. (b) Low-Frequency Enhancement: Frequency-domain analysis reveals that boosting low-frequency signals improves the surface quality of the 3D diffusion model. Thus, we enhance low-frequency signals at key time steps to preserve the model's ability to generate 3D surfaces.\nOur contributions can be summarized as follows: (a) To the best of our knowledge, we are the first to use a generic 3D diffusion prior for morphing textured 3D representations, enabling 3D morphing without explicit correspondences. (b) We analyze the merging of source and target information during morphing with a 3D diffusion prior from semantic and frequency perspectives, proposing Token Reordering and Low-Frequency Enhancement to improve smoothness and plausibility. (c) Extensive experimental results demonstrate that our method achieves superior smoothness and plausibility in performing 3D morphing across diverse cross-category object pairs."}, {"title": "Related Work", "content": "Previous 3D morphing methods focus on shape-only correspondence [Tam et al. 2012], and realize 3D morphing through interpolation or deformation between corresponding 3D primitives (e.g., points, vertices, and faces). They can be divided into (a) Axiomatic Methods: They tend to rely on sparse landmarks [Edelstein et al. 2019; Kim et al. 2011] or use functional maps [Ovsjanikov et al. 2012] to address under-constrained mapping spaces, such as Map-Tree [Ren et al. 2020] and SmoothShells [Eisenberger et al. 2020]."}, {"title": "Method", "content": "The pipeline for 3D regenerative morphing based on the 3D diffusion prior, as shown in Fig.2, consists of three main steps: Basic Interpolation, where essential information is interpolated (Sec.3.2); Smoothness Improvement, achieved through an Attention Fusion mechanism (Sec.3.3); and Plausibility Improvement, which involves two strategies, Token Reordering (Sec.3.4) and Low-Frequency Enhancement (Sec. 3.5)."}, {"title": "Preliminary", "content": "We select Gaussian Anything [Lan et al. 2025b] as our 3D diffusion prior, a two-stage native 3D diffusion model with a structured latent representation (i.e., point cloud) and the DiT [Peebles and Xie 2023] backbone. It consists of a geometry generation model $E_G$ and a texture generation model $E_T$. In the first stage, the model $E_G$ takes a Gaussian initial noise $z_G$ and textual conditioning information $c$ as inputs, to generate a structured point cloud representation $X_{point-cloud} = E_G (z_G, c)$. In the second stage, $X_{point-cloud}$ is added to a Gaussian initial noise $z_T$, and the resulting variable is denoised by $E_T$ with condition $c$ to obtain the final texture feature $x_{feature} = E_T(X_{point-cloud} + z_T, c)$. Finally, $x_{point-cloud}$ and $x_{feature}$ are fed into a pre-trained decoder $D$ to produce the final 3D Gaussian representation $x_{3D} = D(x_{point-cloud}, x_{feature})$, which can be rendered as multi-view images. We denote the token sequences processed between DiT blocks as ${h_j}_{j=1}^M$, where $M$ is the length of token sequence."}, {"title": "Attention", "content": "The attention [Vaswani 2017] mechanism is an important component for the current text-driven diffusion models [Chen et al. 2024a; Peebles and Xie 2023; Rombach et al. 2022; Saharia et al. 2022; Xiang et al. 2024], especially cross-attention and self-attention. Given a latent variable $z \\in R^{d_z}$, a text condition $c \\in R^{d_c}$, and the attention layer with matrices $W_Q \\in R^{d_z \\times d_q}, W_K \\in R^{d_c \\times d_k}$, and $W_V \\in R^{d_c \\times d_o}$, the cross-attention is computed as:\n$A(z, c) = Attn(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V, \\qquad(1)$\nwhere $Q = W_Q z, K = W_K c, V = W_V c$. Self-attention is a special case of cross-attention and can be computed with $A(z, z)$."}, {"title": "Basic Interpolation", "content": "The basic interpolation has three levels, with source and target weights given by $(1-\\alpha)$ and $\\alpha$, respectively, where $\\alpha = 0$ generates the source $x_{3D}^{src}$, and $\\alpha = 1$ generates the target $x_{3D}^{tgt}$.\n(a) Initial Noises: The textured 3D representations $x_{3D}^{src}$ and $x_{3D}^{tgt}$ are inverted via diffusion inversion [Lipman et al. 2022] to obtain their respective input noises, $[z_G^{src}, z_T^{src}]$ and $[z_G^{tgt}, z_T^{tgt}]$. To ensure Gaussian noise properties are preserved, spherical linear interpolation [Samuel et al. 2024] is applied to these noises to generate the interpolated noises, $[z_G^{\\alpha}, z_T^{\\alpha}]$.\n(b) Model Parameters: Given $x_{3D}^{src}$ and $x_{3D}^{tgt}$, we fine-tune the model using LoRA (Low-Rank Adaptation) [Hu et al. 2021] to obtain two sets of LoRA parameters. These parameters are then linearly interpolated and fused to obtain the morphing models $e_G^{\\alpha}$ and $e_T^{\\alpha}$."}, {"title": "Attention Fusion", "content": "The fusion of attention has been proven effective in improving morphing smoothness in 2D diffusion-based image morphing. However, DiffMorpher [Zhang et al. 2024] only explored self-attention interpolation and did not consider the smoothness of semantic features in conditioning. AID [He et al. 2024] did not address the differences between the original and LoRA models, neglecting conditioning feature interpolation with accurate inversion. Our method combines self-attention and cross-attention fusion, using unified attention features from fine-tuned models to enhance smoothness while ensuring plausible generation.\nSpecifically, as shown in Fig. 2, we first feed $z_G^{src}, z_T^{src}, z_G^{tgt}, z_T^{tgt}$, and $c^{\\alpha}$ into blocks of the morphing model $e_G^{\\alpha}$ to obtain three sets of $(Q^{src}, K^{src},V^{src}), (Q^{tgt}, K^{tgt},V^{tgt}), and (Q^{\\alpha}, K^{\\alpha},V^{\\alpha})$. Then, based on Eq. (1), denoting concatenation as $[\\cdot, \\cdot]$, we obtain the fused attention by:\nFused-Attn(Q^{\\alpha}, K^{\\alpha}, V^{\\alpha}) =\nAttn (Q^{\\alpha}, [(1 - \\alpha)K^{src} + \\alpha K^{tgt}, K^{\\alpha}], [(1 - \\alpha)V^{src} + \\alpha V^{tgt}, V^{\\alpha}]). \\qquad(2)$\nConstraining all attention calculations to the same model helps mitigate the quality degradation caused by attention fusion. However, applying attention fusion across different time steps improves smoothness only to a point, beyond which plausibility declines, leading to structural collapse and surface issues (See Fig. 7). This emphasizes the need to balance smoothness with plausibility."}, {"title": "Token Reordering", "content": "3D objects are tokenized into sequences ${h_j}_{j=1}^M$ with each token $h_j$ representing a 3D point. The DiT block's attention modules and Attention Fusion guide the blending of source and target tokens using implicit correspondence during inference. However, relying solely on such implicit correspondence of attention mechanism to match the points in different 3D objects may lead the model to make semantically implausible connections (e.g., combining a chair leg with donut frosting). This vanilla application of the diffusion prior does not fully leverage its potential. Works like DIFT [Tang et al. 2023] show that 2D diffusion features/tokens can represent semantics [Hedlin et al. 2024; Yu et al. 2024; Zhang et al. 2025]. Similarly, we believe 3D diffusion features also capture 3D correspondences within the same object category and semantic correspondence across different object categories (e.g., the eyes of a dog and the eyes of a monkey). Therefore, why not pair points with similar semantics first, and then interpolate within this semantically plausible space?"}, {"title": "Experimental Analysis", "content": "To validate our motivation, we tested the semantic correspondence of tokens on aligned data.\n(a) Problems of Vanilla Attention Fusion. The Fig. 3 indicates that semantic alignment is lost during denoising, but vanilla Attention Fusion forcibly links these position-aligned tokens, which burdens the model and causes artifacts (See Fig. 7, third row).\n(b) Existence of Semantic Correspondence. As shown in Fig.4, the distance between points based on geometry token distance decreases with increasing time steps, indicating strengthened semantic correspondence. Conversely, the distance based on texture token distance first decreases and then increases, suggesting that the denoising process initially promotes semantic alignment, but later shifts towards learning texture details. This aligns with findings in 2D diffusion research[Yu et al. 2024]. Thus, by considering both geometry and texture denoising characteristics, we reorder tokens in the intermediate stages to better guide implicit correspondence for morphing."}, {"title": "Implementation", "content": "Based on these observations, we reorder the token sequences before passing the output of the i-th block to the (i + 1)-th block, where $i \\in \\{1, 2, ..., N\\}$ and $N$ is the total number of blocks in the diffusion model. As shown in Fig. 2, we reorder the source and target token sequences $\\{h_j^{src}\\}_{j=1}^M$, $\\{h_j^{tgt}\\}_{j=1}^M$ , such that tokens with similar distances align at the same index, as follows:\n$\\underset{\\sigma}{minimize} \\sum_{j=1}^M || h_j^{src} - h_{\\sigma(j)}^{tgt} ||,\\qquad(3)$\nwhere $M$ is the number of tokens, and $\\sigma(j)$ denotes the index of the element in the target sequence that best corresponds to the j-th element in the source sequence. To further refine the generation, we set different reordering strategies depending on $\\alpha$: For $\\alpha \\in [0, 0.5)$, the target token sequence is reordered based on the source token sequence. For $\\alpha \\in [0.5, 1]$, the source token sequence is reordered based on the target token sequence."}, {"title": "Low-Frequency Enhancement", "content": "Due to the additional attention fusion operations, the misalignment between the condition space and the diffusion space is exacerbated. Excessive attention fusion at later time steps significantly degrades the model's denoising capability, with the deterioration becoming more pronounced as the time step increases (See Fig. 7, from the 4th to the 5th row). This raises the question: what part of the 3D diffusion model is influenced by the morphing operations, leading to this decline in performance?"}, {"title": "Experimental Analysis", "content": "The deterioration problem is analyzed in the frequency domain to address the significant visual differences between 3D objects, as shown in Fig. 5. In 3D generation, low-frequency noise controls the overall layout, while high-frequency noise governs surface details. Excessive amplification of high-frequency components during denoising can interfere with the low-frequency components, degrading overall quality. Therefore, enhancing low-frequency signals during denoising is crucial to prevent the model from overemphasizing high-frequency noise, thus improving the quality of 3D surface generation. Similar patterns appear in 2D diffusion studies [Si et al. 2024; Wu et al. 2025], with low frequencies tied to image layout and high frequencies to details."}, {"title": "Implementation", "content": "To address this, as shown in Fig. 2, we propose to enhance the low-frequency signal when generating the 3D interpolations. Specifically, the process is defined as:\n$F(h) = FFT(h), \\qquad(4)$\n$F_{\\omega \\langle \\omega_0} (h) = F_{\\omega \\langle \\omega_0} (h) \\cdot scale, \\qquad(5)$\n$h' = IFFT([F_{\\omega \\langle \\omega_0} (h), F_{\\omega \\geq \\omega_0} (h)]), \\qquad(6)$\nwhere $h$ represents the tokens, $F(h)$ their Fourier features, and $h'$ the enhanced tokens, $\\omega$ and $\\omega_0$ denote Fourier frequencies and the threshold, with $\\omega < \\omega_0$ indicating low-frequency components. The scale is the enhancement coefficient, and $FFT()$ and $IFFT()$ are the Fourier transform and inverse Fourier transform."}, {"title": "Experiments", "content": "The 3D diffusion prior [Lan et al. 2025b] is trained on the G-Objaverse [Deitke et al. 2023] dataset. Its geometry and texture diffusion models are based on the DiT architecture [Chen et al. 2024b], which consists of 24 layers, 16 attention heads, and a 1024-dimensional hidden space. The sparse point cloud"}, {"title": "Evaluation", "content": "Our method adopts a 3D generation prior for 3D morphing, offering two main benefits: direct morphing of textured 3D representations, and maintaining structural and semantic consistency in interpolated representations, as shown in Fig. 1 and Fig. 8. The closest baseline to our method is MorphFlow [Tsai et al. 2022], which morphs between two 3D volumetric representations using optimal transport optimization. However, as shown in Fig.6 and Fig. 9, MorphFlow has two main drawbacks. First, the generated quality is low (See Tab.1); the advantage of volumetric representations for photorealism is diminished due to the dense interpolation process, which requires morphing even points with no color. Second, it lacks an effective generative prior to the intermediate process, limiting its ability to understand the semantic meaning of intermediate stages. As a result, it often introduces artifacts in morphing scenarios, such as generating six legs instead of four when morphing a bear's and a table's legs.\nOur method not only significantly outperforms MorphFlow in quality but also demonstrates an impressive \"understanding\" when interpolating across diverse 3D object pairs from different categories. This understanding is evident in two key aspects: (a) the effective mapping of semantically similar parts, as shown when morphing a boot into a red teddy bear, where our model smoothly splits the boot into legs and transforms them into the bear's facial features, and (b) minimal disconnected artifacts. The regenerative nature of our method ensures the fusion of source and target information while considering the distribution of the entire latent space, minimizing issues like 3D structure collapse or disconnected parts."}, {"title": "3D-Aware (Multi-View Image) Morphing", "content": "We evaluated alternatives to our method for textured 3D morphing from two perspectives. First, 3D morphing can be viewed as multi-view image morphing, where multi-view source and target images are fed into image morphing methods for pseudo-3D morphing. Second, many image and video generation models, beyond 3D priors, can perform interpolation tasks. Notably, 2D generation models [Huang et al. 2024a; Shi et al. 2023; Yang et al. 2024, 2023a,b], trained on larger datasets often produce 3D-consistent images with superior semantic, structural, and textural understanding compared to 3D models. Based on these insights, we explored three types of generative priors: 2D diffusion, multi-view diffusion, and video generation.\nAs shown in Fig. 6 and Tab. 1, compared to state-of-the-art 2D image morphing methods, such as DiffMorpher [Zhang et al. 2024] and AID [He et al. 2024], we found that 2D diffusion models often suffer from mode collapse when influenced by non-object image regions (e.g., DiffMorpher is sensitive to white backgrounds). Their lack of 3D consistency leads to inconsistent morphing results for the same $\\alpha$ across different viewpoints. When comparing with multi-view diffusion models, we observed that their image-based multi-view generation is limited by pixel-aligned morphing. This limitation becomes apparent when matching pixels across large spatial distances such as aligning a point on the lower edge of a pumpkin's"}, {"title": "Ablation Study", "content": "Balancing smooth transitions with structural plausibility (or overall generation quality) is challenging, especially in selecting the time step range for Attention Fusion. We empirically observe that texture diffusion allows for minimal attention fusion, while geometry diffusion offers a larger operational range, aligning well with the importance of shape understanding in 3D morphing. To address this, we incorporate Attention Fusion in texture diffusion from the first to the fifth time step, while progressively extending the final time step for geometry diffusion. Adding Attention Fusion on top of basic interpolation improves smoothness, but extending the final time step too far causes 3D structural collapse. Token Reordering helps mitigate this issue, though pushing the time step further reduces generative quality. Ultimately, by applying the Low-Frequency Enhancement strategy, we balance the smoothness and structural plausibility and ensure all frequencies are maintained effectively."}, {"title": "Conclusions", "content": "We propose a method to achieve smooth and plausible morphing sequences across diverse cross-category 3D object pairs, incorporating Attention Fusion, Token Reordering, and Low-Frequency Enhancement. This introduces a new paradigm for textured 3D morphing, extending beyond the limitations of previous research confined to shape-only morphing on topologically aligned datasets.\nDiscussions. Our future work will focus on morphing more complex textured 3D objects, exploring two main directions: (a) Enhancing fidelity and diversity using advanced 3D generation models like Trellis [Xiang et al. 2024], and (b) Expanding morphing to generate complex sequences, such as few-shot motion interpolation [Shen et al. 2024], while maintaining temporal consistency. Additionally, we aim to explore the transitions for 4D content like the \"Birth and Death of a Rose\" [Geng et al. 2024]."}, {"title": "Supplementary Materials", "content": "To experimentally validate the rationale behind the motivations discussed in our manuscript and to provide additional details that could not be elaborated on due to manuscript space limitations, we have carefully prepared comprehensive supplementary materials for reference (Click on the index to directly access the corresponding content)."}, {"title": "3D Generation Model: Gaussian Anything", "content": "Gaussian Anything [Lan et al. 2025b] introduces a 3D generation framework built on a point cloud-based 3D latent space. The 3D Variational Autoencoder (VAE) (See A.2.1) efficiently encodes 3D data into a dynamic latent space, which is subsequently decoded into detailed Surfel Gaussians. Diffusion models (See A.2.2) trained on this compacted latent space achieve remarkable results in 3D generation and editing conditioned on text, as well as in generating high-quality 3D content from images on diverse real-world datasets. For more implementation details, please see their project page."}, {"title": "Point-Cloud Structured 3D VAE", "content": "A 3D VAE is introduced that takes multi-view posed RGB-D (Depth)-Normal renderings as input. These renderings are easy to generate and provide a rich set of 3D attributes corresponding to the input object. Each view's information is concatenated along the channel dimension and efficiently encoded using a scene representation transformer [Sajjadi et al. 2022], producing a compact latent representation of the 3D input. Rather than directly applying this latent representation to diffusion learning, the model's innovative method transforms unordered tokens into a shape that mirrors the 3D input. This transformation is achieved by cross-attending [Huang et al. 2024b] the latent set with a sparse point cloud sampled from the 3D shape. This point-cloud structured latent space significantly aids in disentangling shape and texture, as well as enabling 3D editing. Subsequently, a DiT-based 3D decoder [Peebles and Xie 2023] progressively decodes and upscales the latent point cloud into a dense set of Surfel Gaussians [Huang et al. 2024c], which are rasterized into high-resolution renderings to guide the 3D VAE training."}, {"title": "Cascaded 3D Generation with Flow Matching", "content": "After the 3D VAE is trained, they conduct cascaded latent diffusion modeling on the latent space through flow matching [Albergo et al. 2023] using the DiT [Peebles and Xie 2023] framework. To encourage better shape-texture disentanglement, a point cloud diffusion model is first trained to carve the overall layout of the input shape. Then,"}, {"title": "How to Align/Prepare the Input 3D and Images with Gaussian Anything?", "content": "For textured 3D representations, multi-view RGB, depth, and normal images can be directly rendered, and then the corresponding latent can be obtained using the 3D VAE of Gaussian Anything. For a single image, two methods are possible: (a) The 2D image can be lifted to multi-view using a multi-view generation model [Shi et al. 2023], and then a renderable textured 3D model can be trained from these multi-view images, or (b) A direct image-to-3D method [Huang et al. 2024a] can be used to obtain the textured 3D model."}, {"title": "How to Choose Appropriate 3D Diffusion Models for 3D Morphing?", "content": "Selecting an appropriate 3D generative model is foundational for texured 3D regenerative morphing, as it determines (a) the range of 3D object categories that can be handled and (b) the ability to integrate diverse information for generating smooth interpolation sequences. We followed four criteria when selecting a 3D generative model for our research:\n(a) Accessibility: Training 3D generative models is highly resource-intensive, and high-quality models capable of generating diverse"}, {"title": "Effects of Scale in the Low-Frequency Enhancement", "content": "As shown in Fig. 11, increasing the scale continuously enhances the surface generation capability. However, through empirical observation, we found that beyond a certain value, the surface does not increase further with larger scale values. Therefore, setting the scale to 5 is optimal."}, {"title": "Baseline Methods and Implementation Details", "content": "Given two images, DiffMorpher [Zhang et al. 2024] uses two LoRAs [Hu et al. 2021] to fit the two images respectively. Then the latent noises for the two images are obtained via"}, {"title": "DiffMorpher", "content": "Given two images, DiffMorpher [Zhang et al.\n2024] uses two LoRAs [Hu et al. 2021] to fit the two images respectively. Then the latent noises for the two images are obtained via\nDDIM inversion [Song et al. 2020]. The mean and standard deviation\nof the interpolated noises are adjusted through AdaIN. To generate\nan intermediate image, they interpolate between both the LORA\nparameters and the latent noises via the interpolation ratio a. In ad-\ndition, the text embedding and the K and V in self-attention modules\nare also replaced with the interpolation between the corresponding\ncomponents. Using a sequence of a and a new sampling schedule,\ntheir method will produce a series of high-fidelity images depicting\na smooth transition between the two given images. We followed\nthe script and default parameter settings given by Diffmorpher and\nused their open-source code to produce the results."}, {"title": "AID", "content": "Similar to the DiffMorpher [Zhang et al. 2024] frame-\nwork, AID [He et al. 2024] removes the LoRA fitting and introduces\nthe following additional modifications: (a) Replacing both cross-\nattention and self-attention mechanisms during interpolated image\ngeneration with fused interpolated attention; (b) Selecting interpo-\nlation coefficients using a Beta prior; (c) Injecting prompt guidance\ninto the fused interpolated cross-attention. We implemented the\ngeneration of relevant results based on the code of Stable Diffusion\n1.5 [Rombach et al. 2022], and all settings follow the default settings\nof AID. More details can be found on their project page."}, {"title": "MV-Adapter", "content": "MV-Adapter [Huang et al. 2024a] is a versatile\nplug-and-play and state-of-the-art adapter that turns existing pre-\ntrained text-to-image (T2I) diffusion models to multi-view image\ngenerators. We generated image morphing results based on their\nImage-to-Multiview code and Stable Diffusion 2.1. The only change\nis that we linearly interpolated the condition features of the source\nimage and target image extracted by their image encoder according\nto different morphing weights."}, {"title": "Luma", "content": "The Dream Machine of Luma Al is based on the\nDiT [Peebles and Xie 2023] video generation architecture, capa-\nble of generating high-quality videos with 120 frames in just 120\nseconds, enabling rapid creative iteration. It understands physical in-\nteractions, ensuring that the generated video characters and scenes\nmaintain consistency and physical accuracy. We accessed their API\nand utilized the video generation function to generate intermediate\nvideo frames by providing the source image as the first frame and\nthe target image as the last frame. For instance, for the \"polar bear\"\nto \"wooden stool\" morphing video generation, the guiding prompt\nwe used is: \"Morph a polar bear into a wooden stool, smoothly inter-\npolating both geometry and texture, with the object always remaining\nat the center of the frame.\""}, {"title": "Task setting comparison.", "content": "As shown in Tab. 2, our setting focuses on textured 3D morphing,\na task currently shared only with MorphFlow [Tsai et al. 2022", "2021": "and Char-\nacterMixer [Zhan et al. 2024", "2024": "and SRIF [Sun et al. 2024"}]}