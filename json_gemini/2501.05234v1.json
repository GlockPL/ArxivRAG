{"title": "Optimizing Estonian TV Subtitles with Semi-supervised Learning and LLMS", "authors": ["Artem Fedorchenko", "Tanel Alum\u00e4e"], "abstract": "This paper presents an approach for generating high-quality, same-language subtitles for Estonian TV content. We fine-tune the Whisper model on human-generated Estonian subtitles and enhance it with iterative pseudo-labeling and large language model (LLM) based post-editing. Our experiments demonstrate notable subtitle quality improvement through pseudo-labeling with an unlabeled dataset. We find that applying LLM-based editing at test time enhances subtitle accuracy, while its use during training does not yield further gains. This approach holds promise for creating subtitle quality close to human standard and could be extended to real-time applications.", "sections": [{"title": "1 Introduction", "content": "Same-language subtitles for video material, like TV talk shows, investigative pieces, and educational content serve as a valuable resource for the deaf and hard-of-hearing community, non-native speakers and native speakers alike. For instance, recent studies (Mykhalevych and Preply, 2024; Kim et al., 2023) have revealed that 50% of Americans and 85% of the Netflix users overall frequently watch TV and streaming video content with subtitles. Studies show that subtitles can enhance understanding and memory retention. A lot of viewers choose to enjoy their content quietly at home, keeping subtitles on to avoid disturbing their roommates or family.\nSubtitles differ from verbatim (word-by-word) transcripts in many aspects. Subtitles represent typically a condensed version of the speech, designed to convey the essential meaning without capturing every word. They may omit filler words, repetitions, and non-verbal sounds, and may rewrite phrases, focusing on clarity and readability for viewers. Since subtitles are displayed on-screen during playback, they are formatted to fit within a limited time frame and limited line length, ensuring they are easy to read while the viewer is watching.\nThis paper outlines the development of an accurate offline same-language subtitle generation model for Estonian TV content. Using existing human-created subtitles, we fine-tune Whisper (Radford et al., 2022) and explore further improvements with semi-supervised learning and LLM-based post-editing techniques. Our findings demonstrate that Whisper can be trained to closely replicate human subtitling style, creating well-segmented and often rephrased subtitles. Additionally, we find that iterative pseudo-labeling of a large unlabeled dataset improves subtitle quality across all metrics. While a state-of-the-art commercial LLM (OpenAI gpt-4o\u00b9) can enhance subtitle quality during test time, it's use at training time to improve pseudo-labeled subtitles through post-editing is not effective."}, {"title": "2 Related Work", "content": "Both iterative pseudo-labeling and LLM-based post-editing have been an active area of research in the context of verbatim automatic speech recognition (ASR). Pseudo-labeling based semi-supervised learning in ASR has been studied since at least (Zavaliagkos et al., 1998) and has been later investigated in several works, e.g. by Vesel\u1ef3 et al. (2013); Xu et al. (2020).\nTo the best of our knowledge, Ma et al. (2023) was the first to show the potential of zero-shot and few-shot LLM-based ASR error correction. This approach has been later extended to take into account uncertainty estimation of ASR outputs (Pu et al., 2023) and retrieval-augmented generation for correcting speech recognition entity name errors (Pusateri et al., 2024).\nXi et al. (2024) showed that LLM-based error correction and data filtering can be also used for refining the pseudo-label transcripts during semi-supervised learning. This work is similar to ours,"}, {"title": "3 Method", "content": "Our method for developing an automated subtitle generation system involves several steps: training with supervised data, using iterative pseudo-labeling, and applying LLM-based error correction.\nWe start by training the Whisper large-v3 model (Radford et al., 2022) on a supervised dataset. This dataset consists of audio recordings paired with their subtitles.\nNext, we use an unsupervised dataset to perform two iterations of pseudo-labeling. In this step, we generate pseudo-labels using the last trained model and combine them with the original supervised dataset, followed by training a new model on this data.\nWe also apply LLM-based post-editing of the generated subtitles, by instructing the LLM to fix the mistakes in the subtitles and giving it a segment of generated subtitle file. We experiment with applying this LLM-based post-editing in two distinct phases: at test time (i.e., to generated subtitles of the test data) and during training time (i.e., to generated subtitles of the unsupervised dataset)."}, {"title": "4 Experiments", "content": "As a supervised dataset\u00b2, we used recordings and the corresponding subtitles from the Estonian national TV. The subtitles had been produced for the deaf and hard-of-hearing community by expert subtitlers. The supervised dataset consists of 993 audio-subtitle pairs, totaling 778 hours of audio, corresponding to 10 different TV show series (multi-party talk shows on various topics, political debates, infotainment programs). We randomly selected 17 recordings out of this set for testing.\nThe unsupervised dataset contains 7128 audio recordings, amounting to 3923 hours of audio. It contains similar material as the supervised dataset but also contains news program recordings, which the supervised dataset doesn't include."}, {"title": "4.2 Evaluation metrics", "content": "While evaluating ASR outputs using word error rate (WER) is relatively straightforward, finding an appropriate metric for evaluating automatic subtitling systems is more complicated. Since subtitles often rephrase spoken content to enhance clarity and readability, WER may not accurately reflect the quality of the subtitles. WER does also not account for the formatting and timing of subtitles, which are crucial for viewer comprehension.\nIn our work, we use three metrics for comparing machine-generated subtitles against reference subtitles: subtitle edit rate (SubER) (Wilken et al., 2022) and two variations of BLEURT (Sellam et al., 2020). SubER is based on a modified version of edit distance that incorporates shifts. This allows it to account for the specific properties of subtitles, such as timing and segmentation. However, SubER doesn't take into account that the same meaning can be conveyed with different words or phrases. Thus, we also use BLEURT for evaluation. BLEURT is a learned metric, trained on subjective human evaluations scores of machine translation references and the corresponding candidate sentences. BLEURT outputs scores that usually in the range of 0..1 (with 1 being a perfect match) and is found to be better correlated with human judgments in several languages than BLEU scores. We used the multilingual BLEURT-20-D12 model introduced by Pu et al. (2021). Furthermore, we use two variations of BLEURT: t-BLEURT and AS-BLEURT, which differ in the way generated subtitles are aligned to references. AS-BLEURT splits the reference subtitles into sentences, aligns generated subtitles to the references (Matusov et al., 2005) and then computes BLEURT score for each sentence, while t-BLEURT does the alignment based on the timing information in the subtitles (Cherry et al., 2021)."}, {"title": "4.3 Baseline Model", "content": "As a baseline model, we finetuned Whisper on our supervised dataset using a cross-entropy objective. The model was trained for 4 epochs using the AdamW optimizer (Loshchilov and Hutter, 2019) with a learning rate of 1 \u00d7 10\u207b\u2075. We used an effective batch size of 32 audio chunks and applied Stochastic Weight Averaging (SWA) (Izmailov et al., 2018) after the first epoch.\nDuring decoding, we use the Silero VAD model (Silero Team, 2021) to remove non-speech parts."}, {"title": "4.4 Iterative Pseudo-Labeling", "content": "Next, to improve performance of the baseline model we used iterative pseudo-labeling (IPL)"}, {"title": "LLM-based post-editing", "content": "To ensure fast and efficient correction of subtitles using an LLM, we split the generated subtitles into chunks of 40 subtitle blocks. This approach allows for great parallelization without exceeding the maximum token limit per request. An example of the request format is shown in Figure 2.\nIn the development phase, we evaluated several different LLMs for their suitability for this task."}, {"title": "4.6 Results", "content": "Table 2 lists evaluation results of the native Whisper model (not fine-tuned on additional data), Whisper fine-tuned on 1066 hours of verbatim transcripts from the TalTech Estonian Speech Dataset 1.0 (Alum\u00e4e et al., 2023), and after finetuning with different sets of subtitle datasets. The table also highlights the effects of LLM-based post-editing applied during both the training and testing phases.\nThe results indicate that fine-tuning on subtitle data yields notably lower SubER values compared to fine-tuning on verbatim transcripts, demonstrating the different nature of subtitles and verbatim transcripts. However, the BLEURT scores for both the native Whisper model and the version fine-tuned on verbatim transcripts are surprisingly high. This outcome may be attributed to BLEURT's design as a semantic similarity metric, which effectively maps both verbatim transcripts and subtitle-like compressed transcripts to proximate points in its semantic space.\nTo support our interpretation of the achieved results, we computed Wilcoxon signed-rank test (Wilcoxon, 1945) between models A, B and C highlighted in the Table 2. P-value achieved from comparing model A to B is 0.000, B to C is 0.004 and A to C is 0.000. These p-values are all below common significance thresholds (e.g., 0.05), indicating that the differences between the models are statistically significant.\nGiven that, findings suggest that iterative semi-supervised learning enhances subtitle quality, as evidenced by improvements across all test metrics. LLM-based post-editing applied to decoded subtitles provides additional benefits in most cases. However, contrary to findings in (Xi et al., 2024), applying LLM-based post-editing to pseudo-labeled subtitles in the unsupervised dataset does not yield further improvements.\nAlthough a formal human evaluation of the generated subtitles was not conducted, the authors' subjective assessment suggests that minimal manual post-editing would be required to achieve error-free subtitles, particularly for in-domain TV data."}, {"title": "5 Conclusion", "content": "In this work, we presented an approach to automated subtitle generation, leveraging the multilingual Whisper model, semi-supervised learning, and LLM-based post-editing. By utilizing supervised and unsupervised datasets, we demonstrated that iterative pseudo-labeling can indeed improve the quality of subtitles. Our results show that applying an LLM during test time has a more significant impact on the results across all the key metrics than during training time. Future work will focus on adapting our approach to real-time scenarios."}, {"title": "Iteration 1", "content": "We did two iterations of training with pseudolabels, the training setup was similar to the one with supervised data. Additionally, we incorporated weighted loss function:\n$L_{total} = (1 \u2212 \u03bb) \u00b7 L_{supervised} + \u03bb \u00b7 L_{pseudolabels}$\nwhere \u03bb = 0.35 was chosen empirically using Optuna (Akiba et al., 2019)."}, {"title": "Direct Pseudo-Labeling", "content": "Using pseudo-labels generated by the model itself."}, {"title": "LLM-Enhanced Pseudo-Labeling", "content": "Refining pseudo-labels with a LLM to correct potential errors and ensure alignment with human subtitling standards."}]}