{"title": "VIBES - Vision Backbone Efficient Selection", "authors": ["Joris Gu\u00e9rin", "Shray Bansal", "Amirreza Shaban", "Paulo Mann", "Harshvardhan Gazula"], "abstract": "This work tackles the challenge of efficiently selecting high-performance pre-trained vision backbones for specific target tasks. Although exhaustive search within a finite set of backbones can solve this problem, it becomes impractical for large datasets and backbone pools. To address this, we introduce Vision Backbone Efficient Selection (VIBES), which aims to quickly find well-suited backbones, potentially trading off optimality for efficiency. We propose several simple yet effective heuristics to address VIBES and evaluate them across four diverse computer vision datasets. Our results show that these approaches can identify backbones that outperform those selected from generic benchmarks, even within a limited search budget of one hour on a single GPU. We reckon VIBES marks a paradigm shift from benchmarks to task-specific optimization.", "sections": [{"title": "1. Introduction", "content": "Transfer learning is a cornerstone in the development of Computer Vision (CV) models for tasks such as image classification [9], object detection [25], and segmentation [17]. It involves selecting a pre-trained neural network, referred to as a backbone, that has been trained on large-scale datasets and serves as a powerful feature extractor. Then, practitioners invest considerable time and effort in designing task-specific architecture layers, optimizing hyperparameters, fine-tuning model parameters, and potentially collecting additional data \u2013 all resource-intensive processes.\nWhile much attention is given to these latter steps, the initial choice of backbone is often overlooked. Developers frequently default to well-established architectures like ResNet [8] or Vision Transformers (ViT) [1] without thoroughly assessing their suitability for the specific task and dataset at hand. This oversight is significant, as recent studies have demonstrated that different pre-trained backbones can exhibit vastly different generalization capabilities across downstream tasks, leading to substantial performance variations [5, 7]. In this work, we argue that a more deliberate approach to backbone selection can yield significant performance gains, often with less time and effort compared to other stages of the development pipeline.\nTo date, the topic of vision backbone selection has primarily been addressed through benchmark studies (see Section 2), which evaluate a range of pretrained backbones across multiple downstream tasks and datasets. By aggregating performances obtained, they aim to propose general recommendations, such as identifying overall top-performing architectures or noting trends in the effectiveness of certain backbone families. While these studies provide valuable insights, they present several limitations:\nLimited Coverage: Benchmarks struggle to encompass the vast array of available backbones. Recent studies compared fewer than 50 models, while deep learning model libraries often offer over 1000 options. Moreover, the rapidly evolving landscape of CV models means that benchmark results can quickly become outdated as new architectures emerge.\nOveremphasis on Average Performance: Benchmark studies inherently focus on average-case performance, potentially overlooking significant performance variations that occur when applying these backbones to specific, real-world tasks. Figure 1 illustrates this limitation by showcasing the three generic recommendations from the most recent benchmark study [4]. In practice, each of these recommended models displays suboptimal behavior for at least one dataset. Furthermore, the best-performing model for each dataset underperforms on the other, suggesting that even more extensive benchmark studies are unlikely to yield universally optimal recommendations.\nNeglecting Implementation Variability: Benchmark studies typically provide results based on high-level model descriptions (e.g., architecture, pretraining method, and pretraining dataset). However, they fail to capture the significant performance variability among models with identical specifications. As illustrated in Figure 1, multiple ResNet50 models, all trained using supervised learning on ImageNet-1K, exhibit significantly different performance across both datasets. This implementation-specific variability, which can significantly impact real-world applications, is inherently difficult for traditional benchmark studies to capture and communicate effectively. Consequently, practitioners relying solely on benchmark results may overlook potentially optimal model variants for their specific tasks.\nTo address these limitations, we propose a novel perspective to the problem of pretrained backbone selection for CV tasks. Rather than relying on generalized benchmark results, we advocate for dataset-specific solutions. We formulate the backbone selection as an optimization problem, where the objective is to identify the most suitable backbone for a given dataset while minimizing computational overhead. Our method represents a fundamental departure from traditional practices by: 1. Dynamically considering a vast array of backbones, including newly released models; 2. Prioritizing task-specific performance over average-case scenarios; 3. Accounting for implementation-specific variability by evaluating individual model instances.\nIn this work, we formalize Vision Backbone Efficient Selection (VIBES) as an optimization problem. First, we provide a formal definition of the VIBES problem to help establish a rigorous framework for task-specific backbone selection (Section 3). Next, we analyze how this problem can be solved efficiently, reducing search time compared to exhaustive approaches. Finally, we propose several simple yet effective heuristics to address VIBES (Section 4) and evaluate them across four diverse CV datasets (Section 5). Our results demonstrate that backbones identified through these straightforward approaches have the potential to outperform those selected from generic benchmarks, even within a one hour search budget on a single GPU. This finding underscores the efficacy of our task-specific optimization paradigm and its potential to revolutionize the backbone selection process in practical CV applications."}, {"title": "2. Related work", "content": "Transfer learning [21] is widely used for CV. It consists in using pre-trained neural network backbones as effective feature extractors for smaller-scale downstream tasks. However, the number of available pretrained vision backbones has grown exponentially, due to the multiplication of novel architectures, training algorithms, and pretraining datasets. To illustrate this growth, the timm library [24] now hosts over 1300 pretrained backbones as of 2024, highlighting the breadth of options available to practitioners. This section surveys the literature aimed at guiding CV practitioners through the extensive array of available backbones.\nKornblith et al. [14] sought to establish a correlation between performance on ImageNet and other datasets, proposing ImageNet accuracy as a proxy for estimated backbone performance on downstream tasks. However, subsequent research by Fang et al. [3] challenged this assumption, demonstrating that superior performance on ImageNet does not always translate to enhanced efficacy on real-world datasets. Beyond these correlation studies, the predominant approach in this field has been to conduct comprehensive benchmark studies, comparing the performance of various models across diverse downstream tasks. While papers introducing novel methods for large-scale neural networks in CV typically include comparative evaluations on several downstream tasks, our focus here is on recent studies specifically dedicated to benchmarking existing pretrained vision backbones. These benchmark studies aim to compare backbone performances across a wide range of representative datasets to draw generalizable conclusions about which models are likely to perform well on new, unseen tasks.\nMost benchmark studies were designed to compare the influence of specific backbone characteristics, such as architecture, pretraining algorithm, and pretraining dataset. These studies typically yield either an overall best-performing model or more targeted recommendations for specific task categories. Goldblum et al. [4] conducted the most extensive benchmark study to date, comparing over 20 backbones along three axes: architecture, pretraining algorithm, and pretraining dataset. Their comprehensive analysis across many downstream datasets provides broad conclusions regarding optimal CV backbones, along with targeted recommendations for specific tasks. Vishniakov et al. [23] compared two architectures (ConvNeXt [16] and ViT [1]) across two training methodologies (supervised and CLIP [19]) on diverse target tasks. Their findings suggest that the optimal choice among the four tested models depends on various attributes of the target dataset, reinforcing our hypothesis that universally optimal recommendations are unlikely to exist. Jeevan et al. [11] focused on benchmarking lightweight convolutional architectures under consistent training settings across diverse datasets. Ericsson et al. [2] specifically examined self-supervised models, comparing 13 backbones across over 40 downstream tasks. Their study concluded that identifying a method that consistently outperforms others on downstream tasks remains challenging. Zhai et al. [26] constructed a pool of 19 benchmark datasets aimed at better representing the diversity of potential downstream tasks encountered by practitioners. They utilized this dataset to compare 18 backbone pretraining algorithms, providing general recommendations regarding the use of supervised versus self-supervised pretraining. Kolesnikov et al. [13] evaluated the transfer performance of four pretext tasks on three distinct Convolutional Neural Network (CNN) architectures. Based on their results, they proposed a custom pretext task optimized for transferability. Finally, Goyal et al. [6] investigated the influence of pretraining dataset size on self-supervised pretraining performance across a pool of 9 benchmark datasets, addressing the scalability aspect of model pretraining.\nWhile benchmark studies provide valuable insights, their generic conclusions often fall short in addressing the specific requirements of individual downstream tasks and datasets. In response, this work introduces and formalizes the problem of finding a backbone specifically tailored to the task and dataset at hand."}, {"title": "3. Problem", "content": "3.1. Vision Backbone Selection\nLet T denote the target CV task we aim to solve, characterized by a training dataset Dtrain, a test dataset Dtest, and an evaluation metric 6, where e(m) represents the performance of a model m on Dtest. For instance, for an image classification task, e(m) would indicate the test accuracy of m trained on Dtrain.\nWe introduce vision backbone selection as the problem of finding the best backbone for task T. Let B = {b1, ..., bn} be a set of N pretrained vision backbones. Suppose we have a procedure to fine-tune a given backbone b\u2208 B for T. For example, this procedure might involve stacking a task-specific neural network head h on top of b, and training the composition hobon Dtrain. The fine-tuning methodology (design choices and hyperparameters) remains constant throughout the backbone selection process to ensure that differences in performance can be attributed to the choice of the backbone. Under this assumption, we can simplify notations and use e(b) to denote the performance of a backbone b at task T.\nWith the above notations, vision backbone selection can be formulated as finding b* such that\n$b^* = \\arg \\max_{b\\in B} e(b)$.\nSince B is finite, the vision backbone selection problem can be solved by evaluating every possible backbone in B. This exhaustive search strategy is the only method that provides optimality guarantees, as any alternative that does not explore every option cannot guarantee identifying the optimal backbone. Exhaustive search runs in a total time of\n$t = \\Sigma_{b\\in B} \\tau(b)$,\nwhere \u03c4(b) is the time needed to fine-tune and evaluate the performance of backbone b. However, this method quickly becomes impractical for even moderately sized datasets. For instance, on CIFAR-10, where the average time \u03c4(b) is approximately 20 minutes1, as there are over 1,300 pretrained vision backbones available in the PyTorch Image Models (timm) library [24], performing an exhaustive search would take about 18 days of continuous computation. While this level of resource commitment might be feasible for critical tasks in well-funded organizations, it is not a scalable solution as deep learning continues to expand across various domains. The time and computational resources required make exhaustive testing for every new dataset increasingly untenable.\n1 Using a single GPU NVIDIA RTX A5000 24GB"}, {"title": "3.2. Vision Backbone Efficient Selection", "content": "To address these constraints", "options": "reduce \u03c4(b) or reduce B. This leads to two families of strategies to reduce the total search time:\nFast approximate evaluation: One approach is to reduce \u03c4(b) by defining a fast alternative evaluation procedure", "sampling": "Another approach is to use only a subset of B. While this could mean missing out on the best backbone, it can significantly reduce search time. The quality of the selected backbone depends heavily on the order in which models are sampled. To manage this, we define a (potentially stochastic) sampling function \u03c0, which generates a permutation of {1, ..., N}. We denote the ordered set as \u03c0(B) = {b\u03c0(1), ..., b\u03c0(N)}.\nProviding a solution to VIBES consists of defining an approximate evaluation procedure \u1ebd and a sampling strategy \u03c0. Then, running VIBES consist in using \u1ebd to evaluate multiple backbones, sampled with \u03c0, for a predefined time budget tmax. The selected backbone b is defined as\n$\\hat{b} = arg\\max_{i\\in{1,...,k}} \\tilde{e}(b_{\\pi(i)})$"}]}