{"title": "FACTALIGN: Long-form Factuality Alignment of Large Language Models", "authors": ["Chao-Wei Huang", "Yun-Nung Chen"], "abstract": "Large language models have demonstrated significant potential as the next-generation information access engines. However, their reliability is hindered by issues of hallucination and generating non-factual content. This is particularly problematic in long-form responses, where assessing and ensuring factual accuracy is complex. In this paper, we address this gap by proposing FACTALIGN, a novel alignment framework designed to enhance the factuality of LLMs' long-form responses while maintaining their helpfulness. We introduce fKTO, a fine-grained, sentence-level alignment algorithm that extends the Kahneman-Tversky Optimization (KTO) alignment method. Leveraging recent advances in automatic factuality evaluation, FACTALIGN utilizes fine-grained factuality assessments to guide the alignment process. Our experiments on open-domain prompts and information-seeking questions demonstrate that FACTALIGN significantly improves the factual accuracy of LLM responses while also improving their helpfulness. Further analyses identify that FACTALIGN is capable of training LLMs to provide more information without losing factual precision, thus improving the factual F1 score.", "sections": [{"title": "1 Introduction", "content": "Generating natural language provides a natural interface for humans to communicate with artificial intelligence. With the emergence of large language models (LLM) (Brown et al., 2020), they immediately demonstrate the potential to become the next-generation engine for information access due to their ability to generate long-form natural language response to human queries. Given the large-scale pre-training on web-scale datasets, LLMs demonstrate impressive capabilities of answering diverse questions, showcasing the vast amount of knowledge they possess. The post training techniques, i.e., instruction tuning (Wei et al., 2022) and reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022), further train LLMs to respond in a more human preferable way, e.g., generating coherent and detailed responses.\nDespite their impressive reasoning capabilities and wide-range knowledge, research has shown that LLMs still struggle with hallucination (Xu et al., 2024b; Rawte et al., 2023) and generating non-factual content (Min et al., 2023). An example of long-form generation and factuality assessment is illustrated in Figure 1. These issues hinder the reliability of LLMs and make it hard to be adopted to real-world settings where factual accuracy is a crucial requirement for most applications. The long-form responses make these issues more complex as it is non-trivial to quantify the level of long-form factuality (Wei et al., 2024), let alone to improve it. Meanwhile, most research focuses on improving the helpfulness of LLM chatbots and their reasoning capabilities, with little emphasis on the factuality of the responses.\nIn this paper, we aim to improve the reliability of LLMs by enhancing the factuality of their long-form responses. Recent advances of automatic factuality evaluators show that they are capable of providing factuality assessment at the atomic fact level (Min et al., 2023; Wei et al., 2024). \u03a4\u03bf leverage those fine-grained factuality assessments, we propose FACTALIGN, an alignment framework designed to improve LLMs' long-form factuality while maintaining the same level of helpfulness. We introduce a fine-grained alignment algorithm, fKTO, which extends the Kahneman-Tversky Optimization (KTO; Ethayarajh et al. (2024)) alignment algorithm to sentence-level. We conduct experiments on both open-domain prompts and information-seeking questions and demonstrated that our proposed FACTALIGN can effectively improve long-form factuality of LMs while maintaining their helpfulness.\nOur main contributions can be summarized as the following:\n\u2022 We introduce fKTO, a sentence-level alignment algorithm that can leverage fine-grained signals provided by a long-form factuality evaluator.\n\u2022 We propose FACTALIGN, a framework to align LMs with fine-grained signals to generate responses that are more factual, while keeping their helpfulness.\n\u2022 The effectiveness of the proposed components are validated through detailed analyses."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Language Model Alignment", "content": "Alignment, i.e., aligning language models to human values, has been a very popular research field recently. Prior work such as InstructGPT (Ouyang et al., 2022) and LLaMA-2 (Touvron et al., 2023) showcased that RLHF (Bai et al., 2022a) enhances models' ability to follow instructions significantly. Fine-grained RLHF (Wu et al., 2024) proposed to leverage fine-grained rewards for better alignment. Constituional AI (Bai et al., 2022b) and RLAIF (Lee et al., 2023) introduced AI feedback to eliminate the requirement of human annotation. Another line of research focused on alignment without RL. DPO (Rafailov et al., 2023) derived a simple objective for alignment, thus attracting rapid adoption. KTO (Ethayarajh et al., 2024) eliminated the requirement of pairwise preference data. Our proposed alignment algorithm, fKTO, extends KTO to sentence-level, which can leverage the fine-grained signals provided by a long-form factuality evaluator."}, {"title": "2.2 Factuality of Langage Models", "content": "Factuality and hallucination have been long-standing issues for natural language generation (Lee et al., 2022; Ji et al., 2023). Lee et al. (2022), Li et al. (2023), and Chuang et al. (2024) proposed decoding techniques that improved factuality of LMs. Shuster et al. (2021) reduced hallucination by retrieval-augmented generation. (Dhuliawala et al., 2023) proposed chain-of-verification to reduce LLM hallucination. SelfCheckGPT (Manakul et al., 2023) proposed a method to self-check factuality by sampling multiple generations. FactScore (Min et al., 2023; Chiang and Lee, 2024) and LongFact (Wei et al., 2024) both introduced frameworks for evaluating factuality of long-form generations. FAVA (Mishra et al., 2024) introduced fine-grained hallucination categories to evaluate the models and provided a detailed view of the hallucination issues of LLMs. Our proposed method also utilize a long-form factuality evaluator, while focusing on leveraging the provided factuality assessments for better factuality alignment.\nPrior work has also worked on training LMs to be more factual. FactTune (Tian et al., 2024) leveraged FactScore to construct preference pairs and demonstrated improvement on the bio generation task. FLAME (Lin et al., 2024) introduced factuality-aware alignment which combines FactTune with open-domain prompts. KnowTuning (Lyu et al., 2024) proposed knowledge augmentation which constructs synthetic pairs for DPO training. On the other hand, recent work has shown that fine-tuning LMs on new knowledge might encourage hallucinations (Gekhman et al., 2024; Kang et al., 2024). Our work additionally proposes fKTO for fine-grained factuality alignment, which achieves superior performance."}, {"title": "3 Preliminaries", "content": "In this paper, we aim to improve the long-form factuality of LLMs by factuality alignment. In this section, we introduce an overview of the task of long-form factuality and alignment algorithms."}, {"title": "3.1 Long-form Factuality", "content": "LLMs excel at generating long-form responses with detailed description and explanation. However, evaluating the factuality of long-form generations is non-trivial. In this paper, we define the factuality score of a long-form response as an aggregation of the factuality score of each individual atomic fact, following FactScore (Min et al., 2023) and LongFact (Wei et al., 2024). More formally, given a knowledge corpus C, an user prompt x and the response y = M(x) generated by a model M, we first decompose y into atomic statements A = {a\u2081,\u00b7\u00b7\u00b7, a|a|}. For each atomic statement ai, its factuality score f(ai) is defined as whether it is supported by the knowledge in C, i.e., f(ai) = 1[ai is supported by C]. Then, the factuality score of the long-form response y can be defined as fa(y) = A({f(a1),\u2026\u2026, f(a\\A\\)}), where A is an aggregation function that can be defined in various ways.\nIn this paper, we adopt two metrics for long-form factuality: factual precision as defined in FactScore (Min et al., 2023) and factual f1 score as defined in LongFact (Wei et al., 2024). Factual precision measures the overall precision of the atomic statements:\nfprec(y) = $\\frac{\\sum_{i=1}^{|A|} f(a_i)}{|A|}$While factual precision is simple, it could be easily exploited. A model could obtain a very high factual precision score by only generating one statement that has the highest confidence.\nOn the other hand, factual f1 assumes that a certain amount of information is desired by the user and additionally considers the factual recall:\nff1@K(y) = $\\begin{cases} \\frac{(2 \\cdot f_{prec}(y) \\cdot f_{rec@K}(y)}{f_{prec}(y)+f_{rec@K}(y)} & \\text{if } |A|> 0 \\\\ 0 & \\text{if } |A| = 0, \\end{cases}$\nwhere frec@K(y) = min(1.0,$\\frac{K}{|A|}$) is the factual recall score assuming that at least K statements are desired by the user. Factual f1 is less exploitable than factual precision as it punishes the model when it only generates few statements."}, {"title": "3.2 Kahneman-Tversky Optimization", "content": "Training LLMs that are aligned to human values typically involves three stages: 1) pre-training, 2) supervised fine-tuning, and 3) reinforcement learning from human feedback (RLHF). The first two stages maximize the sequence generation likelihood of the LM given a dataset of either diverse pre-training data or human-annotated instruction-following data. The third stage, RLHF, aims to maximize the expected reward of LM generations, where the reward usually is defined as human preferences (Ouyang et al., 2022). As a result, the RLHF stage enables LMs to generate responses that are more preferable by humans, which is vital for creating intelligent assistants.\nWhile the success of the RLHF framework is eminent, its adoption is hindered by the complexity of the framework, the unstability of the training process, and the increased training time due to the requirement of online sample generation. To this end, prior work has proposed alignment algorithms that do not require RL, thus attracting mass adoption. Direct Preference Optimization (DPO; Rafailov et al. (2023)) derives a simpler objective from the RLHF, eliminating the requirement of a reward model and the RL optimization process. More recently, Ethayarajh et al. (2024) introduced Kahneman-Tversky Optimization (KTO), which derives a family of human-aware alignment loss functions. The objective function of KTO is even simpler than DPO. It only requires a binary label for each prompt-response pair (x, y), as opposed to DPO which requires pairwise preference labels for each triplet (x, Y1, Y2). This relaxed data requirement enables us to extend the algorithm to sentence-level, which we will discuss in Section 4.2. More formally, the KTO loss is defined as:\nLKTO = $\\frac{1}{|B|}\\sum_{x,y \\in B} (\\lambda_y - v(x,y))$,\nwhere B denotes the minibatch, \\lambda_y denotes the weight of the chosen and rejected samples, and\nv(x, y) = $\\begin{cases} \\lambda \\sigma(\\beta (r_0(x, y) - z_0)) & \\text{if } c(x, y) = 1, \\\\ \\lambda \\sigma(\\beta (z_0 - r_0(x,y))) & \\text{if } c(x, y) = 0, \\end{cases}$\nz0 = Ey'~D[KL(\u03c0\u03c1(y' | x')||#ref(Y' | x'))],\nr0(x, y) = log$\\frac{\\pi_{\\rho}(x,y)}{\\pi_{ref}(x, y)}$,\nwhere c(x, y) denotes the preference function, i.e., c(x, y) = 1 if the response y is chosen. Ethayarajh et al. (2024) demonstrated that KTO achieves on par or better alignment performance compared to DPO. KTO also works well under the scenario where the number of chosen and rejected samples are significantly unbalanced, e.g., 1:9."}, {"title": "4 FACTALIGN: Aligning Language Models for Long-form Factuality", "content": "In this section, we introduce our proposed framework FACTALIGN. An overview of our framework is illustrated in Figure 2."}, {"title": "4.1 Automatic Long-form Factuality Evaluator", "content": "Obtaining fine-grained factuality annotations for long-form responses by human annotation is very costly. For example, Min et al. (2023) estimated that evaluating one generation costs $4. In this work, we employ an automatic factuality evaluator for long-form responses. The factuality evaluator, following the design of FactScore (Min et al., 2023) and SAFE (Wei et al., 2024), is a workflow of 4 stages: 1) atomic statement decomposition, 2) query generation, 3) relevant knowledge search, and 4) final factuality assessment. Note that stage 2 and 3 can be run multiple times to enrich the searched knowledge.\nAtomic Statement Decomposition The response y is first split into sentences S = {$1,\u00b7\u00b7\u00b7, s|s|$}, and each sentence is decomposed into atomic facts A. We add an additional step to revise the decomposed atomic statements into self-contained statements s with GPT-3.5-TURBO following SAFE.\nQuery Generation We prompt GPT-3.5-TURBO to generate a search query given the revised statement si and possibly the previously generated queries and found knowledge snippets.\nRelevant Knowledge Search We employ Wikipedia as the knowledge corpus C following FactScore. While the coverage of Wikipedia is more limited compared to commercial search engines like Google Search, we opt for Wikipedia as this reduces cost and allows us to fully manage the knowledge search component under a controlled setting. We perform search with the generated query and obtain the top-k most relevant knowledge snippets.\nFinal Factuality Assessment We prompt GPT-3.5-TURBO to provide the final factuality assessment of a revised statement s, which is either Supported if the statement is supported by the knowledge snippets, or Not Supported otherwise. The statement-level score is then defined as f(ai) = 1[a\u017c is Supported]. Note that f(ai) represents whether the statement is supported with respect to Wikipedia, not whether it is globally true."}, {"title": "4.2 Long-form Factuality Alignment", "content": "At the core of the FACTALIGN framework is the alignment algorithm, which operates on two granularities: response-level and sentence-level."}, {"title": "4.2.1 Response-level Alignment", "content": "We employ the standard KTO loss LKTO for response-level alignment. The preference labels c(x, y) in the KTO loss can be defined and obtained in various ways. For instance, most prior work utilized human-annotated preference labels or pseudo labels provided by LLMs. In order to align for factuality, we treat a response y as a chosen sample if the factual f1 score of the response is greater than a threshold t:\nc(x, y) = 1[ff1@K(y) > t].\nBy minimizing the response-level loss, we align the LMs to generate responses that have higher factual f1 scores.\nIn addition to the data for factuality alignment, the response-level loss is compatible to other forms of preference data. For example, in order to make the model more helpful, we can include diverse preference datasets that are based on human preferences. In practice, we include general-domain alignment datasets during training to make sure the model is aligned to diverse human values."}, {"title": "4.2.2 Sentence-level Alignment", "content": "Since our factuality evaluator provides assessments at a finer granularity, we propose a fine-grained alignment algorithm, fKTO, to leverage these signals by extending the KTO alignment algorithm to sentence-level. The fKTO loss is defined as\nLIKTO = $\\frac{1}{|B|}\\sum_{x,Y \\in B} \\frac{1}{|S|}\\sum_{i=1}^{|S|}(\\lambda_i - v(x || S_{<i}, s_i))$,\nwhere x || s<i denotes the concatenation of x and S<i which denotes sentences before si. In this objective function, a sentence si is treated as the completion given x || s<i. A sentence is chosen if the average precision of its atomic statements is higher than a threshold ts.\nc(x || S<i, Si) = 1[$\\frac{\\sum_{j=1}^{|A_{s_i}|} f(a_j)}{|A_{s_i}|} > t_s$]"}, {"title": "4.3 Iterative Optimization", "content": "With the alignment algorithms introduced above, we can align LMs to be more factual and more helpful. However, the responses and factuality assessments are obtained in an offline fashion, i.e., we sample the responses and their factuality labels before training the model and use this data throughout training. This creates a discrepancy between the assessed responses and the model being trained, which would hinder the alignment process due to distributional shift. Hence, we employ an iterative optimization procedure, where we periodically sample new responses with the trained model and assess their factuality. The newly generated responses are then included in the training dataset for the next iteration."}, {"title": "5 Experimental Stetup", "content": "We conduct experiments to validate the effectiveness of our proposed framework FACTALIGN. Furthermore, we perform analyses to discuss the effectiveness of each component in the framework."}, {"title": "5.1 Datasets", "content": "Supervised Fine-tuning (SFT) We employ the Deita dataset (Liu et al., 2024) for supervised fine-tuning before performing alignment to ensure basic instruction-following capabilities of the model. The Deita dataset consists of high-quality data selected from UltraChat (Ding et al., 2023), ShareGPT2, and WizardLM (Xu et al., 2024a).\nGeneral-domain Alignment We follow the Zephyr recipe (Tunstall et al., 2023) and employ the UltraFeedback dataset (Cui et al., 2023) as the general-domain alignment dataset. The UltraFeedback consists of prompts across multiple domains and completions generated from multiple LLMs to enrich diversity. We use the binarized version of the dataset\u00b3 and decouple the pairs for the KTO loss.\nFactuality Alignment We generate information-seeking prompts following the data creation procedure from LongFact (Wei et al., 2024). LongFact"}, {"title": "5.2 Long-form Factuality Evaluator", "content": "We employ gpt-3.5-turbo to perform atomic statement decomposition, query generation, and final factuality assessment. The generation temperature is set to 0.1. We use the preprocessed Wikipedia corpus from the Dec. 20, 2021 dump released by Izacard et al. (2024) as our knowledge corpus C, which consists of 33 million passages. A pre-trained retriever ColBERT-v2 (Santhanam et al., 2022) is used to encode all passages and perform retrieval given a query. We retrieve top-3 passages for each query and combine them with the previously retrieved passages for final factuality assessment. At most 2 search steps are performed to retrieve relevant passages for each statement. Detailed prompts can be found in Appendix A."}, {"title": "5.3 Models", "content": "We employ the pre-trained gemma-2b model (Team et al., 2024) as our policy model, which is an open-weight model pre-trained on large-scale datasets across diverse domains. The model is first fine-tuned with the Deita SFT dataset, and then aligned with the alignment datasets.\nWe also conduct experiments on LLaMA-3 8B (Meta, 2024) and Phi3-Mini models (Abdin et al., 2024), which are both open-weight models which were aligned with proprietary data."}, {"title": "5.4 Evaluation Procedure", "content": "The trained models are evaluated on two aspects: long-form factuality and helpfulness.\nLong-form Factuality Evaluation We evaluate models' long-form factuality following the procedure of SAFE (Wei et al., 2024)4. We choose the LongFact-object subset following the original work, which consists of 38 topics. We change the Google Search API to our Wikipedia retriever due to resource and budget constraint. In preliminary experiments, we find that this change have very little impact on the evaluation outcome. Our evaluator has correlation scores of 0.93 and 0.82 with SAFE for the number of Supported and Not Supported assessments, respectively. We follow SAFE to add an postamble to each prompt to ask for the model to generate as many details and examples as possible. We report f1@100 as the main evaluation metric. We also report the factual precision and factual recall scores. In addition, we evaluate models with FactScore (Min et al., 2023). We run the evaluation from its official implementation and use GPT-3.5-TURBO as the evaluator instead of InstructGPT. FactScore can be interpreted as the factual precision of bio generation.\nHelpfulness Evaluation We evaluate models' helpfulness on MT-Bench (Zheng et al., 2023), a popular benchmark that includes challenging multi-turn open-ended questions for evaluating chat assistants. The automatic judgement is performed by GPT-4 with a score of 1 to 10, which is shown to be highly-correlated with human judgement. The evaluation is done with their official implementation6."}, {"title": "5.5 Implementation Details", "content": "We set the threshold t to 0.75, meaning that the response is chosen if its f1@100 is higher than 0.75. The threshold for sentences ts is set to 1.0, i.e., the sentence is only chosen if all of its atomic statements are supported. During training, we set \u03b2 = 0.1 for KTO and Bf = 0.5 for fKTO. The weight of LIK\u03c4\u03bf, \u03bb, is set to 2.0. The learning rate is set to 5e-7 with a linear learning rate schedule. We set the effective batch size to 16 and train for 1 epoch for each iteration. In order to reduce GPU memory consumption during training, we optimize the model with the 8-bit version of the AdamW optimizer. We iteratively optimize the LM as described in Section 4.3 for 3 iterations. All experiments are run on 4xV100 GPUs. Each training run takes 1 to 2 hours to finish. We estimate that each evaluation run costs $25 in API credits."}, {"title": "6 Results", "content": "We present the main results in Table 1, where we contrast FACTALIGN with both proprietary models (GPT-4-Turbo and GPT-3.5-Turbo), a prominent open-weight model (LLaMA-2-70B-Chat (Touvron et al., 2023)), and a fully open-source model (Olmo-7B-Instruct) (Groeneveld et al., 2024). The comparison involves our baseline model, the Gemma-2B model7, which has been fine-tuned using our SFT dataset, Deita. This model serves as the foundational policy model for all subsequent aligned models. Additionally, we benchmark against the rejection sampling fine-tuning method (Yuan et al., 2023), involving supervised fine-tuning with selected samples from our alignment dataset. This method shows modest improvements.\nRemarkably, our FACTALIGN framework significantly improves the long-form factuality and helpfulness of the baseline model, achieving relative improvements of 40.1% and 29.2% in terms of f1@100 and average score on MT-Bench, respectively. These results demonstrate our capability to simultaneously refine LMs for enhanced factuality and utility. Moreover, FACTALIGN also boosts the FactScore of the baseline models and outperforms larger models like GPT-3.5-Turbo and LLaMA-2-70B-Chat in both f1@100 and FactScore metrics. This demonstrates the potential for smaller LMs, through precise alignment, to surpass general-domain large LMs in factual accuracy.\nWith a detailed examination of the metrics, it is evident that FACTALIGN primarily improves factual recall, increasing the output of factual claims from 66.8 to 135.1, while slightly improving factual precision from 77.41 to 79.59. This enhancement suggests that FACTALIGN primarily amplifies output volume while maintaining factual precision. This trend echoes findings from general-domain alignment research, which indicates that alignment algorithms typically promote longer outputs, likely due to a combined human and LM preference for more extensive responses (Dubois et al., 2024). A qualitative example of this can be found in Appendix B."}, {"title": "6.1 Ablation Study", "content": "To validate the effectiveness of our proposed components, we conduct an ablation study to understand their contribution to the final improvement. The results are reported under FACTALIGN in Table 2.\nFirstly, we remove the iterative optimization technique, where we only perform 1 iteration of training. As shown in the results, removing iterative optimization significantly degrades the performance, where f1@100 drops by over 10 points."}, {"title": "6.2 Generalization to New Topics", "content": "Since the training data is created with the same set of topics in LongFact, all the topics should be considered seen during evaluation. Note that prompts used in evaluation are excluded during training. To validate whether FACTALIGN could generalize to unseen topics, we conduct an additional experiment where we split the topics into 19 seen topics and 19 unseen topics. We only include the data from the seen topics during training and perform evaluation on the unseen topics. The results are reported in Table 3. The results show that FACTALIGN performs slightly worse on unseen topics. Nonetheless, it still outperforms the baseline models significantly, showcasing that the alignment can generalize to unseen topics."}, {"title": "6.3 Relationship of Precision-Recall", "content": "By varying the ratio of data points using precision as the threshold and those using recall, we can control the tradeoff between the precision score and the recall score. We train models with different data mixture and plot the corresponding precision-recall curve in Figure 3. The model trained with 100% precision data achieves the highest precision score, and the model trained with 100% recall data achieves the highest recall score. Furthermore, we can achieve a specific level of factual precision and recall scores on the curve by changing the ratio. This result demonstrates that FACTALIGN enables control over the desired factual precision and recall scores."}, {"title": "6.4 Sensitivity of Hyperparameters", "content": "We report the performance of FACTALIGN under various hyperparamter settings. The results are reported in Table 4. We observe that the threshold t affects performance slightly, with 0.75 being the best setting. We also notice that with t = 0.75, the labels are balanced, i.e., the number of chosen samples is roughly equal to the number of rejected samples. This indicates that constructing a balanced dataset perform better for our alignment algorithm.\nWe also vary the hyperparameter Bf and notice that it degrades performance slightly. Note that the best Bf value is higher than the \u1e9e value typically set for KTO, i.e., 0.1. Our hypothesis is that since fKTO operates on the sentence-level, the log probability difference naturally has a lower magnitude compared to the response-level case. Thus, a higher value of Bf is needed to promote the fine-grained loss to a similar level as the response-level loss."}, {"title": "7 Conclusion", "content": "In this paper, we address the issue of long-form factuality in LLMs by proposing a novel alignment framework, FACTALIGN. Our approach, which incorporates a proposed data construction process alongside the fine-grained alignment algorithm fKTO, significantly enhances the factuality of LLMs over long-form responses, while also boosting their helpfulness. Our analysis demonstrates that FACTALIGN enables detailed control over the desired level of factual precision and recall scores. We believe that the insights and methodologies presented in our work can motivate further advancements in the factuality alignment of LLMs."}, {"title": "Limitations", "content": "Our work focuses on the factuality aspect of LLMs, which we define as whether the generated response is supported by retrieved evidence. This definition makes the performance dependent to the performance of the retriever and the coverage of the knowledge corpus. Moreover, our data creation and evaluation pipeline rely on automatic factuality evaluators. Even though prior work has validated the effectiveness of these evaluators by showing high correlation with human judgements, the automatic evaluators inevitably might make incorrect judgements.\nWhile FACTALIGN significantly improves the factuality of LLMs, they still are prone to generate non-factual content. A calibration method would be complimentary to our method to ensure the reliability of LLMs.\nWe focus on a controlled setting where the information-seeking prompts are all questions about a certain object. This is to ensure the reliability of the automatic evaluation process. Future work could extend the coverage of the information-seeking prompts to more diverse user queries."}, {"title": "A Prompts Used", "content": "We use the following prompt for new prompt generation"}]}