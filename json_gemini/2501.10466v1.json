{"title": "Improving the Efficiency of Self-Supervised Adversarial Training through Latent Clustering-Based Selection", "authors": ["Somrita Ghosh", "Yuelin Xu", "Xiao Zhang"], "abstract": "Compared with standard learning, adversarially robust learning is widely recognized to demand significantly more training examples. Recent works propose the use of self-supervised adversarial training (SSAT) with external or synthetically generated unlabeled data to enhance model robustness. However, SSAT requires a substantial amount of extra unlabeled data, significantly increasing memory usage and model training times. To address these challenges, we propose novel methods to strategically select a small subset of unlabeled data essential for SSAT and robustness improvement. Our selection prioritizes data points near the model's decision boundary based on latent clustering-based techniques, efficiently identifying a critical subset of unlabeled data with a higher concentration of boundary-adjacent points. While focusing on near-boundary data, our methods are designed to maintain a balanced ratio between boundary and non-boundary data points to avoid overfitting. Our experiments on image benchmarks show that integrating our selection strategies into self-supervised adversarial training can largely reduce memory and computational requirements while achieving high model robustness. In particular, our latent clustering-based selection method with k-means is the most effective, achieving nearly identical test-time robust accuracies with 5 to 10 times less external or generated unlabeled data when applied to image benchmarks. Additionally, we validate the generalizability of our approach across various application scenarios, including a real-world medical dataset for COVID-19 chest X-ray classification. Our Implementations are available as open-source code at this url.", "sections": [{"title": "1 Introduction", "content": "Over the past decade, it has been repeatedly confirmed that deep neural networks (DNNs) are highly vulnerable to adversarial perturbations [35]. This phenomenon has raised serious concerns about the reliability of DNNs in safety-critical applications and has driven numerous research into designing methods to enhance model robustness [3, 4, 13, 26]. Among them, adversarial training is regarded as one of the most effective methods [22, 38, 45]. However, as stated by Schmidt et al. [29], learning a model that is resilient to adversarial perturbations requires a significantly larger dataset than standard learning. Recent studies have explored self-supervised techniques to expand the training set size of adversarial training algorithms by leveraging unlabeled external [6] or generated data [15, 30, 39]. Despite alleviating the sample complexity barrier and producing models with improved robust accuracies, these methods typically utilize vast amounts of additional data, suggesting the requirement of much larger hardware to store those data and a much longer training time for algorithms like adversarial training to converge. Witnessing the challenges of additional memory and computational requirements, we investigate whether the significantly large amount of utilized additional data is inevitable for achieving state-of-the-art adversarial robustness. The ultimate goal of our work is to maximize the model robustness achieved by SSAT algorithms by using as few additional unlabeled data points as possible. Inspired by Zhang et al. [46], which highlights the unequal importance of training examples, we argue that with limited model capacity, self-supervised adversarial learning should also focus on optimizing critical data samples near the model's decision boundary. Consequently, we propose multiple data selection strategies, including a simple prediction confidence-based selection (PCS) scheme that prioritizes unlabeled data points in which the model's prediction is most uncertain and more advanced latent clustering-based selection (LCS) schemes. Specifically, we propose two LCS approaches depending on the corresponding clustering technique it builds on: latent clustering-based selection with Gaussian mixture models (LCS-GMM) and latent clustering-based selection with k-means (LCS-KM). For each unlabeled data point, we compute the difference between the highest and second-highest posterior probabilities in the latent feature space for LCS-GMM, while we select vulnerable points by calculating the distance difference between the two nearest cluster centroids for LCS-KM. The set of unlabeled data with the smallest probability or distance difference is considered more critical and further incorporated into self-supervised adversarial training. Moreover, our selection strategies balance the ratio of boundary critical points and the remaining non-boundary points to avoid undesirable overfitting to the boundary data distribution.\nBy applying these targeted data reduction approaches, we streamline the self-supervised adversarial training process, significantly reducing the required additional unlabeled data while maintaining comparable model robustness against adversarial perturbations. Our work offers valuable insights into how to acquire additional data and employ SSAT in resource-constrained application domains. Below, we further summarize the main contributions of our work.\nContributions. We motivate and formalize the problem task of reducing the volume of unlabeled data while maintaining model robustness for SSAT (Section 3). To realize such a goal, we propose various data selection schemes to identify the most critical unlabeled data points, including a straightforward approach based on prediction confidence and two advanced methods based on clustering techniques of unlabeled data in the latent embedding space (Section 4). The proposed methods optimize the performance of SSAT by refining the model's decision boundary in the input regions of high uncertainty by strategically prioritizing boundary-adjacent unlabeled data points. By focusing on critical unlabeled data points, our methods largely reduce the memory consumption and time"}, {"title": "2 Related Work", "content": "In this section, we review the most relevant literature on adversarial examples, (self-supervised) adversarial training, and its variants. We also discuss the most representative techniques for improving the efficiency of standard deep learning, including dataset distillation and coreset selection."}, {"title": "2.1 Adversarially Robust Learning", "content": "Adversarial Examples. Adversarial examples are inputs crafted with small perturbations that are designed to mislead model predictions [35]. The prevalence of adversarial examples with deep neural networks poses a critical challenge in modern machine learning, especially for security-critical or safety-critical applications. Earlier-proposed attacks like fast gradient sign method (FGSM) [13] and projected gradient descent (PGD) [22] introduced gradient-based techniques to find adversarial examples using constrained optimization, while Carlini and Wagner proposed unconstrained CW attacks [5], which search for the minimal perturbation that causes misclassification. Recent attacks like Auto-PGD [9] automate gradient steps for higher efficiency in finding worst-case perturbations, while AutoAttack [8] is an ensemble of diverse parameter-free attacks, making them more effective in evaluating model robustness.\nAdversarial Training. On the defense side, adversarial training and its variants are the most popular learning methods for adversarial robustness. For instance, Madry et al. proposed to train DNNs on adversarially perturbed inputs using PGD attacks [22], while Zhang et al. introduced TRADES [45], which balances the trade-off between adversarial robustness and standard accuracy. Nevertheless, adversarial training algorithms are often criticized for overfitting adversarially perturbed samples produced during training time and for their high computation costs, which hinder their deployment in real-world applications. In particular, Rice et al. illustrated through comprehensive experiments that the robust overfitting phenomenon is prevalent in adversarial training [28], where early stopping is shown to be effective in alleviating such an issue. Techniques, such as Free AT [32] and Fast AT [40], offer promising solutions to reduce the computation for adversarial training. Nevertheless, they suffer from decreased robustness performance.\nMoreover, a line of research pointed out that treating all the data points equally in adversarial training is not optimal for robust learning since different examples can impact model robustness differently. For example, Zhang et al. argued that data points far from decision boundaries are inherently more secure and less likely to be influential in robust learning against adversarial perturbations [46]. As a result, these points should be given less weight during training to prevent the model from overfitting them, which could negatively affect its generalization on more vulnerable or unseen data. In a similar vein, Hua et al. advocated for a more targeted approach [18], recommending that PGD training be applied primarily to examples situated near the decision boundaries. This strategy focuses the model's attention on the points that are most likely to enhance its robustness, as adversarial perturbations on boundary-adjacent data are more likely to result in meaningful improvements to the model's resistance to attacks. Our work builds on these ideas by strategically identifying and prioritizing boundary-adjacent points from the unlabeled dataset to address the unique challenges associated with self-supervised adversarial training methods."}, {"title": "Self-Supervised Adversarial Training", "content": "Utilizing unlabeled data to improve model performance has become an active and rapidly advancing field of research [7]. In the adversarial context, Schmidt et al. [29] highlighted a key observation: the sample complexity required for robust learning far exceeds that of standard learning. They showed that achieving adversarially robust generalization often necessitates a much larger dataset than traditional learning approaches, posing a challenge when labeled data is scarce or costly. Subsequently, a line of works proposed to boost model robustness by involving additional unlabeled data, often acquired from a similar but slightly different distribution, using self-supervised methods in adversarial training [1, 6, 24, 44]. In particular, Carmon et al. proposed robust self-training methods to address the sample complexity issue [6], which first trains an intermediate model using available labeled data and then leverages the model to generate pseudo-labels for unlabeled data. The pseudo-labeled samples, combined with the original labeled data, are subsequently used to train a final robust model. Concurrently, Najafi et al. proposed an optimization framework incorporating labeled and unlabeled data [24], while providing formal guarantees on adversarial robustness.\nIn addition, leveraging synthetic data produced by state-of-the-art generative models to build robust models has also been explored extensively in recent literature [15, 30, 39]. For instance, Gowal et al. employed various unconditional generative models [15], including variational autoencoders (VAEs), generative adversarial networks (GANs), and the more advanced denoising diffusion probabilistic models (DDPMs), to produce a large synthetic dataset to be incorporated in adversarial training, which improves robustness further. While all the aforementioned SSAT methods enrich the training dataset, they require considerable additional unlabeled data to ensure effective robustness enhancement. In this work, we explore data selection schemes to reduce the amount of unlabeled or generated data that are both efficient and effective for self-supervised adversarial training to achieve good robustness performance."}, {"title": "2.2 Data-Efficient Deep Learning", "content": "Within the broader literature of data-efficient deep learning, various techniques have been proposed to reduce the amount of training data while retaining critical information. Two prominent approaches are dataset distillation and coreset selection.\nDataset Distillation. Dataset distillation compresses large datasets into compact, synthetic datasets by matching gradients or optimizing specific loss functions, allowing models to achieve comparable performance with significantly fewer data points. For instance, Wang et al. proposed a foundational approach to dataset distillation [37], proposing methods to condense large datasets into smaller, synthetic subsets while preserving model performance. Later, Zhao et al. introduced the concept of gradient matching [47], where distilled data points are optimized to match the gradients of the original dataset. This technique largely improves the distillation performance, enabling models to learn more effectively from reduced data. Additionally, Sucholutsky and Schonlau proposed soft-label dataset distillation, which uses probabilistic labels for distilled data [34], facilitating the training of complex models with minimal data while maintaining strong performance. Despite its success in standard deep learning, dataset distillation is often impractical for robust learning due to its high computational demands. Moreover, it assumes that all the examples in the entire dataset are equally important for distillation, ignoring the fact that different data points may have varying influences on model performance.\nCoreset Selection. On the other hand, coreset selection [20, 31] aims to identify a small, representative data subset that retains the model performance when trained on this reduced dataset. These approaches often rely on diversity-based metrics or optimization techniques to ensure that the selected subset preserves sufficient information for effective learning. For example, Kaushal et al. utilized diverse models to select training data subsets to reduce labeling efforts [19], while Xia et al. introduced the concept of \"moderate coreset\" [42], focusing on data points with scores near the median to construct subsets that generalize well across different scenarios based on score distributions. Similarly, Mirzasoleiman et al. proposed selecting a weighted subset of training data that approximates the full gradient by maximizing a submodular function [23].\nIn the adversarial context, Dolatabadi et al. proposed adversarial coreset selection [11], offering a task-specific solution to enhance the computational efficiency of adversarial training. In particular, the method constructs a compact data subset by minimizing the gradient approximation error. The constructed subset is then utilized for adversarial training, with selection taking place at regular intervals. While effective in reducing computational overhead for vanilla adversarial training, the growing reliance on large volumes of generated or external data\u2014such as in self-supervised adversarial training\u2014introduces new challenges. These approaches often expand the dataset size to improve robust accuracy, eventually increasing the total training time. In such settings, repeatedly selecting data during training may become computationally infeasible. In summary, our work complements the above-mentioned methods for efficient deep learning, focusing on developing more data selection schemes without compromising robustness or scalability, particularly for SSAT when large-scale unlabeled data are involved."}, {"title": "3 Improving Data Efficiency for SSAT", "content": "In this section, we first introduce the necessary notations and definitions for readers to understand self-supervised adversarial training (Section 3.1) and then motivate and formulate the problem task of enhancing the data efficiency of SSAT algorithms (Section 3.3)."}, {"title": "3.1 Preliminaries on SSAT", "content": "We work with self-supervised adversarial training algorithms, initially introduced to the field by Carmon et al. [6]. Let \\(X \\subseteq \\mathbb{R}^{n}\\) be a n-dimensional input space, \\(y\\) be the output space of class labels, and \\(D_l\\) be the underlying labeled distribution supported on \\(X \\times Y\\) that we aim to learn an adversarially robust classifier. Let \\(S_l\\) be the training set with examples i.i.d. drawn from \\(D_l\\) and \\(S_u\\) be a set of inputs sampled from some unlabeled distribution \\(D_u\\) supported on \\(X\\). In the adversarial machine learning literature, \\(D_u\\) is typically considered to be similar but not identical to the marginal input distribution of \\(D_l\\), and \\(S_u\\) is set to be much larger than \\(|S_l|\\), since unlabeled data are easier to acquire than well-annotated labeled data. Throughout the paper, we use \\(| \\cdot |\\) to denote the cardinality of the corresponding set. For instance, when \\(S_l\\) corresponds to the 50K labeled CIFAR-10 training images, Carmon et al. [6] selected 500K \"most CIFAR-10-like but non-identical\" images from the whole 80M Tiny ImageNet data as the unlabeled dataset \\(S_u\\) to be used by SSAT.\nTo be more specific, SSAT first standardly trains an intermediate model \\(\\hat{f}\\) on \\(S_l\\), known as the pseudo labeling function, and then assigns pseudo-labels to each unlabeled data point \\(x \\in S_u\\). More rigorously, the intermediate model's weight parameters \\(\\hat{\\theta}\\) are learned by minimizing the following objective function:\n\\begin{equation}\n\\hat{\\theta} = \\underset{\\theta}{\\text{argmin}} \\frac{1}{|S_l|} \\sum_{(x,y) \\in S_l} \\mathcal{L}(\\theta, x, y),\n\\end{equation}\nwhere \\(\\mathcal{L}(\\cdot)\\) denotes the standard loss, such as cross-entropy, that measures the discrepancy between model prediction and the class label. Finally, SSAT trains a model, denoted as SSAT(\\(S_l, S_u, \\gamma\\)), on both labeled dataset \\(S_l\\) and unlabeled dataset \\(S_u\\) but paired with pseudo labels by minimizing the following adversarial loss:\n\\begin{equation}\n\\underset{\\theta}{\\text{min}} \\frac{1}{|S_l|} \\sum_{(x,y) \\in S_l} \\mathcal{L}_{adv}(\\theta, x, y) + \\frac{\\gamma}{|S_u|} \\sum_{x \\in S_u} \\mathcal{L}_{adv}(\\theta, x, \\hat{f}(x)),\n\\end{equation}\nwhere \\(\\gamma \\geq 0\\) controls the contributions between labeled and unlabeled data. In prior literature [15], a typical approach is to construct each training batch with varying ratios of labeled and unlabeled data (corresponding to different values of \\(\\gamma\\)) but fix the total batch size to optimize for the best SSAT performance. For simplicity, we term \\(\\gamma\\) as the per-batch ratio hyperparameter in this paper. When \\(\\gamma\\) is set as 0, Equation 2 reduces to the training objective of vanilla adversarial training (AT) [22]. In Equation 2, \\(\\mathcal{L}_{adv}(\\cdot)\\) stands for the adversarial loss function: for any \\((x, y) \\in S_l\\) and \\(\\epsilon > 0\\), \\begin{equation}\n\\mathcal{L}_{adv}(\\theta, x, y) = \\underset{\\delta \\in B_{\\epsilon}(0)}{\\text{max}} \\mathcal{L}(\\theta, x + \\delta, y),\n\\end{equation}\nwhere \\(B_{\\epsilon}(0)\\) denotes the ball centered at 0 with radius \\(\\epsilon\\) measured in some distance metric, such as \\(l_p\\)-norm. Aligned with prior literature on adversarial training [22], we adopt multi-step PGD to obtain an approximated solution to the inner maximization problem specified by Equation 3 during self-supervised adversarial training."}, {"title": "3.2 Limitations of Existing SSAT Methods", "content": "Although SSAT methods alleviate the sample complexity barriers of adversarially robust learning [29] and can produce models with higher robust accuracies, they require a substantial amount of extra unlabeled data \\(S_u\\) to ensure effective robustness enhancement. This trend is clearly documented in the leaderboard of RobustBench [8]. For example, state-of-the-art SSAT methods [6, 15, 39] either select 500K external Tiny Imagenet data samples or generate millions of synthetic CIFAR-10-like images, both significantly larger than the original 50K CIFAR-10 training examples. For example, the method proposed by Gowal et al. [15] achieves around 65% robust accuracy on CIFAR-10 using a WideResNet-70-16 model architecture against \\(l_\\infty\\) perturbations with \\(\\epsilon = 8/255\\), increasing the robustness performance of vanilla adversarial training [22] by a large margin, but relies an extra unlabeled set of 100M DDPM-generated data. The huge unlabeled dataset required for SSAT significantly increases memory consumption for holding the whole training dataset, which we argue is inefficient and likely to be prohibited for resource-constrained application scenarios. These observations motivate us to explore whether such a considerable amount of unlabeled data can be reduced while preserving the high model robustness attained by SSAT algorithms.\nMoreover, we note that SSAT requires a much longer convergence time to obtain the best-performing model, usually 2 to 4 times the number of training epochs compared with vanilla AT (see Figure 1 for supporting evidence). Intuitively, extracting useful and robust features through adversarial training from such a large and potentially diverse dataset is more difficult. As we will illustrate in our experiments, the slower convergence can be attributed to the additional large unlabeled datasets (with pseudo labels) that often exhibit higher variance than the original labeled data samples. Once we reduce the size of the unlabeled data involved with specifically designed techniques, faster convergence is expected for SSAT. Since vanilla adversarial training has already been criticized in prior literature [32, 40] for its high computational costs for running multi-step PGD to solve the inner maximization problem in Equation 3, state-of-the-art SSAT algorithms incur even higher computational costs (see Table 1 for runtime comparisons), due to the slower convergence rate and larger model architecture they typically adopt. Therefore, it is essential to design efficient SSAT methods, especially for resource-constrained applications, that can better utilize the large amount of extra unlabeled data."}, {"title": "3.3 Problem Formulation", "content": "Witnessing the data and computational inefficiency of SSAT methods, we propose to study whether the large set size of unlabeled data is inevitable for training models with good robustness performance. Inspired by the idea of coreset selection for efficient deep learning [19, 23, 42], we propose to strategically search for a small but essential set of unlabeled data \\(A_u \\subseteq S_u\\) such that self-supervised adversarial training based on \\(S_l\\) and the selected subset \\(A_u\\) can produce models with comparable robustness to those obtained using full unlabeled dataset \\(S_u\\). More formally, we aim to solve the following constrained optimization problem:\n\\begin{equation}\n\\underset{A_u \\subseteq S_u}{\\text{max}} \\text{AdvRobe}(SSAT(S_l, A_u, \\gamma)), \\text{ s.t. } |A_u| \\leq \\alpha |S_u|,\n\\end{equation}\nwhere SSAT(\\(S_l, A_u, \\gamma\\)) stands for the model learned by SSAT with \\(S_l\\) and \\(A_u\\) based on Equation 2, and \\(\\alpha \\in (0, 1)\\) is a predefined ratio capturing the data constraint. AdvRobe(\\( \\theta \\)) denotes the robustness of the model with parameters \\(\\theta\\) against \\(\\epsilon\\) perturbations:\n\\begin{equation}\n\\text{AdvRobe}(\\theta) = 1 - \\mathbb{E}_{(x,y) \\sim D_l} \\underset{\\delta \\in B_{\\epsilon}(0)}{\\text{max}} \\mathcal{L}_{0/1}(\\theta, x + \\delta, y)\n\\end{equation}\nwhere \\(\\mathcal{L}_{0/1}\\) denotes the 0-1 loss function. Due to the combinatorial nature and the high computation costs of AT algorithms, it is computationally hard to enumerate all the feasible subsets \\(A_u\\) to solve the proposed optimization problem exactly. As we will illustrate in the following sections, we design different data selection schemes with respect to the extra unlabeled dataset for SSAT that are effective in approximately solving the optimization problem in Equation 4 while only incurring negligible computational overhead."}, {"title": "4 Proposed Data Selection Schemes", "content": "So far, we have introduced the problem task of reducing the unlabeled dataset size to improve the efficiency of SSAT algorithms while maintaining the robustness enhancement and illustrated its importance. Motivated by the significant role of boundary-adjacent data points in optimizing model performance (Section 4.1), in this section, we design three efficient data selection schemes to address the optimization problem in Equation 4 (Sections 4.2 and 4.3)."}, {"title": "4.1 Prioritize Boundary Unlabeled Data", "content": "Achieving high model robustness with restricted data resources remains a challenging task in machine learning. It involves striking a balance among data significance, optimizing the use of labeled and unlabeled data, and tackling constraints imposed by model capacity. Inspired by the prior literature [18, 46] that emphasizes the imbalanced data importance for vanilla adversarial training (see Section 2.1 for detailed discussions of these works), we hypothesize that not all unlabeled data contribute equally to the robustness enhancement for SSAT. In particular, we propose identifying a small set of vulnerable yet valuable unlabeled data points in \\(S_u\\), which are close to the model's decision boundary. Consequently, such boundary-adjacent data points are highly susceptible to label changes under small input perturbations and are inherently difficult for the model to classify. Thus, improving their classification can yield more robustness enhancement to adversarial inputs. We expect the decision boundary of the intermediate model \\(\\hat{f}\\) to act as an effective proxy for locating difficult-to-classify data points. Since the intermediate model is usually trained with strong standard accuracy, it is expected to preserve class semantics and provide a solid foundation for identifying these critical points. SSAT algorithms can then leverage this information to ensure these boundary-adjacent points are correctly classified, even under adversarial conditions. By focusing more on these critical boundary points with respect to the intermediate model, the final model can be trained more efficiently using SSAT algorithms while upholding robust accuracies (see Figure 7 in Appendix A for an illustration of the overall training pipeline).\nWhile a straightforward extension of existing approaches is to select unlabeled data based on how much their predictions change under perturbations found by PGD attacks, this method is computationally expensive, undermining the efficiency gains we seek."}, {"title": "4.2 Prediction Confidence-Based Selection", "content": "To identify the critical set of vulnerable data points near the model's decision boundary while accounting for computational efficiency, scalability, and interpretability, we first propose a straightforward approach, termed as Prediction Confidence-based Selection (PCS), which utilizes the intermediate model \\(\\hat{f}\\) to compute a prediction confidence score for each unlabeled data point. The pseudocode for such a selection scheme is depicted in Algorithm 1. Initially, all the data points in \\(S_u\\) are sorted by their prediction confidence Conf(\\(\\cdot\\)), with those exhibiting the lowest confidence scores being prioritized. The underlying assumption is that data points with low confidence scores are more likely to lie near the decision boundary, making them ideal candidates for our selection. Note that the parameter \\(\\beta \\in [0, 1]\\) is introduced to balance the ratio between boundary and non-boundary points to avoid overfitting, which is used in all our proposed selection schemes. The reason for involving such a trade-off parameter will be further discussed in Section 6.2.\nThe biggest advantage of PCS is its high computational efficiency for ranking the unlabeled data using model confidence scores. Note that for self-supervised adversarial training, we usually have an order-wise larger collection of unlabeled data than the original labeled dataset. Thus, an efficient data selection scheme is desirable to avoid high computational overhead. Nevertheless, we discovered in our experiments that using prediction confidence may not capture the underlying structure of the data well, leading to decreased robustness enhancement when incorporated in SSAT. We hypothesize that PCS overlooks the geometric relationships and distributional properties, which are crucial for characterizing boundary-adjacent points, particularly for complex datasets. In addition, DNNs have been shown to be overconfident in their predictions [16], suggesting prediction confidence score might be an inherently biased indicator."}, {"title": "4.3 Latent Clustering-Based Selection", "content": "To overcome the above issues, we propose latent clustering-based selection (LCS) strategies, which identify data points near the model's decision boundary in the latent space using different clustering techniques. Our approach begins by generating latent embeddings for all unlabeled data \\(z = h_{\\hat{\\theta}}(x)\\), where \\(h_{\\hat{\\theta}}\\) denotes the mapping of the input layer to the penultimate layer with respect to the intermediate model \\(\\hat{f}\\). The penultimate layer captures more abstract, high-level features and better represents the underlying data structure, which helps identify points near decision boundaries. It avoids the biases of overconfident predictions from the final layer, offering more reliable clustering. Here, the goal is to identify boundary-adjacent data points inferred by examining distances to cluster centroids. Points equidistant from multiple centroids are expected to be closer to decision boundaries in the latent embedding space.\nIn particular, we explore two classical clustering techniques for their unique benefits in identifying boundary points in the LCS framework. Algorithm 2 presents the pseudocode of the two proposed LCS methods. In particular, LCS with k-means (LCS-KM) generates latent representations of unlabeled data and clusters them based on Euclidean distances to the centroids, prioritizing data points that are equidistant from multiple centroids to capture local geometric structures around decision boundaries effectively. On the other hand, LCS with Gaussian mixture models (LCS-GMM) fits the latent representations to Gaussian mixture models, using posterior probabilities across multiple fitted Gaussians to identify points that are likely near decision boundaries. Compared to PCS, both LCS-KM and LCS-GMM leverage latent space clustering to provide a more accurate characterization of boundary vulnerabilities.\nLCS-KM. In this variant of LCS, we first generate latent embeddings \\({z = h_{\\hat{\\theta}}(x) : x \\in S_u}\\), and then partition the N unlabeled data points into k clusters \\({C_1, C_2,..., C_k}\\) by minimizing the within-cluster sum of squares \\(\\sum_{i=1}^{k} \\sum_{z \\in C_i} ||z - \\mu_i||^2\\), where \\(\\mu_j\\) is the centroid of the j-th cluster. For each latent embedding z, we compute the Euclidean distance to each cluster centroid \\(||z - \\mu_j ||\\). Data points are selected based on the minimal difference in distance between each latent embedding z to the corresponding two closest cluster centroids \\(\\Delta_d = |d_1 - d_2\\), where \\(d_1\\) and \\(d_2\\) are the Euclidean distances to the closest and second closest centroids, respectively. The set of unlabeled inputs with the smallest \\(\\Delta_d\\) values are selected. Finally, the top \\(\\alpha |S_u|\\) points from \\(S_u\\) with the smallest \\(\\Delta_d\\) values form the reduced unlabeled dataset followed by the same balancing step using the ratio parameter \\(\\beta\\). As will be demonstrated in our experiments, prioritizing these strategically selected unlabeled data points during self-supervised adversarial training can achieve comparable robustness with much-improved efficiency.\nLCS-GMM. In this variant of LCS, we again start by computing latent embeddings for unlabeled data \\({z = h_{\\hat{\\theta}}(x) : x \\in S_u}\\). Instead of using k-means clustering, we fit these latent representations using Gaussian mixture models. A GMM assumes that the data is generated from a mixture of k Gaussian distributions, each with its own mean \\(\\mu_j\\) and covariance matrix \\(\\Sigma_j\\) for the j-th component. Mathematically, each data point z has a probability of belonging to the j-th Gaussian component given by the posterior probability:\n\\begin{equation}\np_j(z) = \\frac{\\pi_j \\cdot \\mathcal{N}(z | \\mu_j, \\Sigma_j)}{\\sum_{i=1}^{k} \\pi_i \\cdot \\mathcal{N}(z | \\mu_i, \\Sigma_i)}\n\\end{equation}\nwhere \\(\\pi_j\\) is the mixing coefficient for the j-th Gaussian component, and \\(\\mathcal{N}(z | \\mu_j, \\Sigma_j)\\) represents the Gaussian distribution with mean \\(\\mu_j\\) and covariance \\(\\Sigma_j\\). We denote \\(p(z) = [p_1(z), p_2(z), ..., p_k(z)]\\) as the probability vector. To identify data points near the decision boundary, we focus on those points for which the posterior probabilities \\(p(z)\\) are similar across multiple Gaussian components, indicating that they are near the boundary between different clusters. Specifically, for each data point z, we calculate the difference between the highest and the second-highest posterior probabilities: \\(\\Delta_p = |p_1(z) - p_2(z)|\\), where \\(p_1(z)\\) and \\(p_2(z)\\) are the highest and second-highest posterior probabilities, respectively. Data points with the smallest \\(\\Delta_p\\) values are selected, as they are more likely to be near decision boundaries. Finally, the top \\(\\alpha |S_u|\\) unlabeled data with the smallest \\(\\Delta_p\\) values are used to form the selected subset. As will be illustrated in Section 5, the robustness performance of SSAT can be largely maintained when using LCS-KM and LCS-GMM to select a small subset of unlabeled data (e.g., \\(\\alpha\\) = 10% or \\(\\alpha\\) = 20%). More detailed discussions of the difference between these data selection schemes are provided in Section 6.1, where we visualize the selected unlabeled data in a two-dimensional latent space."}, {"title": "5 Experiments", "content": "In this section, we comprehensively evaluate the performance of our proposed data selection schemes, as described in Algorithms 1 and 2, using two widely recognized image benchmarks: SVHN [25] and CIFAR-10 [2] (Section 5.2). Following existing literature, we consider \\(l_\\infty\\)-norm bounded perturbations with \\(\\epsilon = 0.015\\) on SVHN and \\(\\epsilon = 0.031\\) on CIFAR-10. We also highlight the computational advantages of our methods, particularly their ability to reduce the convergence time of SSAT algorithms (Section 5.3). Furthermore, we assess the generalizability of our techniques by applying them to a real-world medical dataset, demonstrating the applicability of our methods beyond standard image benchmarks (Section 5.4)."}, {"title": "5.1 Experimental Settings", "content": "First, we introduce the necessary details to understand our experimental results. All the remaining details for reproducing our experiments are provided in Appendix B.\nDataset. For the image benchmark experiments, the proposed selection schemes are initially applied to external unlabeled datasets, following the implementation protocols outlined by Carmon et al. [6]. For the SVHN experiments, models are trained on 73K labeled digit images from the original SVHN dataset, supplemented by 531K additional unlabeled SVHN images. Specifically, the CIFAR-10 dataset comprises 50K labeled training images and 10K labeled test images, with models trained using SSAT augmented by an external unlabeled dataset \\(S_u\\) containing 500K images sampled from the 80M Tiny Images (80M-TI) dataset. To assess the generalizability of our selection schemes, we further evaluate SSAT algorithms using 1M synthetically generated images created by the Denoising Diffusion Probabilistic Model (DDPM) for both CIFAR-10 and SVHN, following the protocols described by Gowal et al. [15].\nConfiguration. We primarily conduct experiments on SSAT using WideResNet architectures. We train models using TRADES [45] on both labeled and unlabeled data coupled with pseudo labels generated by the intermediate model. In Section 6.3, we evaluate the sensitivity of our methods for varying \\(l_\\infty\\) perturbation size, PGD-based adversarial training with varying attack steps, and \\(l_2\\) perturbations. To standardize training, an epoch is defined as processing 50K data points, regardless of the total dataset size. This is achieved by calculating the required number of batches to cover 50K data points based on the batch size. This approach, adopted in prior works [6, 15], ensures that training time per epoch remains consistent across datasets of varying sizes.\nSSAT. For experiments with external unlabeled data, SSAT models are trained for a total of 200 epochs, whereas experiments with generated data extend to 400 epochs. To evaluate the performance of each data selection scheme, models are saved every 25 epoch of SSAT, and the model achieving the highest robust accuracy is selected as the \"best\" model. This approach aligns with the early stopping practices commonly used in adversarial ML literature [28, 45]. In experiments with external data and selection ratios \\(\\alpha \\in \\{10\\%, 20\\%\\}\\), we consistently observe peak robust accuracy around 100 epochs for CIFAR-10 and 75 epochs for SVHN. Conversely, when no data selection scheme is applied, peak performance is delayed to approximately 200 epochs. Total training time is reported as the duration required to achieve the best model performance.\nIntermediate Model. As a foundational step, the proposed selection schemes require training an intermediate model that utilizes the same architecture as the final model. This intermediate model is trained using standard supervised learning for 100 epochs, with training on the WRN-28-10 architecture taking approximately 54 minutes and 43 seconds. Notably, this time is excluded from the reported training durations presented in the results table. The intermediate model fulfills two critical roles: it not only facilitates the pseudo-labeling of unlabeled data but also constitutes a necessary component for the implementation of any data selection strategy. As such, this training step is integral to the overall workflow, irrespective of whether a specific selection method is employed."}, {"title": "5.2 Main Evaluation on Image Benchmarks", "content": ""}]}