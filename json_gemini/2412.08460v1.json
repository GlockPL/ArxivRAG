{"title": "Federated Learning for Traffic Flow Prediction with Synthetic Data Augmentation", "authors": ["Fermin Orozco", "Pedro Porto Buarque de Gusm\u00e3o", "Hongkai Wen", "Johan Wahlstr\u00f6m", "Man Luo"], "abstract": "Deep-learning based traffic prediction models require vast amounts of data to learn embedded spatial and temporal dependencies. The inherent privacy and commercial sensitivity of such data has encouraged a shift towards decentralised data-driven methods, such as Federated Learning (FL). Under a traditional Machine Learning paradigm, traffic flow prediction models can capture spatial and temporal relationships within centralised data. In reality, traffic data is likely distributed across separate data silos owned by multiple stakeholders. In this work, a cross-silo FL setting is motivated to facilitate stakeholder collaboration for optimal traffic flow prediction applications. This work introduces an FL framework, referred to as FedTPS, to generate synthetic data to augment each client's local dataset by training a diffusion-based trajectory generation model through FL. The proposed framework is evaluated on a large-scale real world ride-sharing dataset using various FL methods and Traffic Flow Prediction models, including a novel prediction model we introduce, which leverages Temporal and Graph Attention mechanisms to learn the Spatio-Temporal dependencies embedded within regional traffic flow data. Experimental results show that FedTPS outperforms multiple other FL baselines with respect to global model performance.", "sections": [{"title": "1 INTRODUCTION", "content": "Traffic prediction is an integral aspect of modern Intelligent Transport Systems (ITSs) with innate implications on the safety and productivity of inhabitants; it facilitates the optimisation of traffic management systems, and the dissemination of pertinent information for commuters. This in turn has profound implications on social, environmental, and economic factors. Deep-learning based models have demonstrated exceptional accuracy in their ability to learn spatio-temporal relationships from traffic data and predict future traffic states [5, 12, 22, 25, 28].\nThe majority of previous works towards spatio-temporal prediction aim to train a model from a centralised perspective where the collective data is available one a single device. Centralising city-wide ITS data in real-world settings is a difficult task; ITSs represent a partnership between transport stakeholders such as local government authorities, ride-sharing companies, and micro-mobility organisations. Sharing journey data has implications on user privacy (GDPR), as well as commercial implications. Therefore, developing frameworks for applicable decentralised data-driven methods is of paramount importance within the ITS sector to enable stakeholder collaboration for the development of traffic prediction models. Federated Learning (FL) [15] provides a solution for training optimal traffic prediction models collaboratively by decentralising the training process of a traffic prediction model. Although recent works in FL demonstrate its capacity to train a global model which learns spatio-temporal dependencies across distributed data, these works do not directly address the issue of data heterogeneity [9], or instead focus on the client selection strategy or parameter aggregation function [14, 16, 18, 29, 31-33].\nTo tackle the issue of data heterogeneity and imbalanced data partitions across clients, data sharing mechanisms for FL applications in highly heterogeneous settings were explored in [34] wherein a small partition of real data is distributed to clients to augment their local datasets in a cross-device setting. Sharing real data with"}, {"title": "2 RELATED WORKS", "content": "Federated Learning\nThe distributed nature of the data in a FL setting incurs a detrimental effect on model accuracy relative to centralised training paradigms due to the fact that the data distribution among clients is non-Independent and Identically Distributed (non-IID). In cross-silo FL settings, each client may represent a data center which stores"}, {"title": "2.1 Federated Learning", "content": "The distributed nature of the data in a FL setting incurs a detrimental effect on model accuracy relative to centralised training paradigms due to the fact that the data distribution among clients is non-Independent and Identically Distributed (non-IID). In cross-silo FL settings, each client may represent a data center which stores large amounts of data aggregated from many devices, and owned by a particular organisation. FL has been extended to optimise the training of the global model [19], as well as to improve its applicability in non-IID settings [11]. Yet such methods focus on optimising the FL framework rather than address the data heterogeneity issues directly.\nExisting data augmentation approaches for FL focus on image classification tasks, and typically rely on Generative Adversarial Networks (GANs) [2, 13, 23], or on sharing mean latent representations of client data [3, 27]. Unlike image generation applications, traffic data generation requires modelling the dynamics of data over both spatial and temporal dimensions. Diferentially-private representations of spatio-temporal data can be produced by altering real data [1, 30], or mixing partial aspects of the data [17]. These processes affect the spatio-temporal characteristics of the data itself, however. Recent work into diffusion-based models for spatio-temporal data generation demonstrate their capacity to generate high-fidelity synthetic trajectories which capture the spatio-temporal characteristics of sample traffic data, while ensuring the privacy of the real data.\nDiffTraj [35] is a conditional diffusion-based trajectory generation model which leverages Traj-UNet, a network based on the UNet architecture [20] as well as an intermediary attention-based transition module, to model the noise transitions at each time step in the diffusion model processes. The model embeds additional context, such as the starting region and departure time of a trip, as conditional information which is implanted by a Wide and Deep network [4] to enhance the generative capacity of the model. To avoid memorisation of trajectories based on conditional information, which would implicate the privacy of real data, Diff Traj uses the classifier-free diffusion guidance method [8] which jointly trains an unconditional model to increase the diversity of generated trajectories and avoid deterministic data generation."}, {"title": "2.2 Traffic Flow Prediction", "content": "To extract temporal dependencies, deep-learning based models typically employ either Recurrent Neural Networks (RNNs) based architectures [12, 14], temporal Convolutional Neural Networks (CNNs) which convolve around the time dimension of traffic flow data [25, 28], or temporal Attention mechanisms to capture the evolution of the data around the time dimension [22].\nSpatial relationships embedded within data can be captured through CNNs which convolve around the spatial dimension of data [22]. Traffic data can also be represented as graph networks, enabling Graph Convolutional Networks (GCNs) to capture spatial relationships within the data. GCN operations are dependent on the graph structure, which demands expert contextual knowledge to adequately capture inter-region relationships. Graph Attention Networks (GATs) [24] implement a self-attention mechanism to learn the relevance between a given node and its neighbouring nodes, and hence are independent of the graph structure. To capture spatio-temporal dependencies within data, models such as STGCN [28] implement GCNs with temporal CNNs to capture spatio-temporal dependencies within data, while DCRNN [12] leverages GCNs with RNN-based models. Graph WaveNet [25] also utilises temporal CNNs and GCNs, however it learns an adjacency matrix for the"}, {"title": "3 PROBLEM FORMULATION", "content": ""}, {"title": "3.1 Data Generation", "content": "Trajectory data can be defined as a sequence x of GPS latitude-longitude pairs. The objective of data generation is, given a set of GPS trajectories, $X = {x_1, x_2,...,x_m}$, where $x^i$ is a sequence of GPS latitude-longitude pairs, learn a model, G, that can generate a set of synthetic trajectories, $X_{synth} = {x_{synth}^1, x_{synth}^2...,x_{synth}^n}$, where $x_{synth}^i$ is a sequence of synthetic latitude-longitude pairs, and where the synthetic trajectories retain the real data's spatial and temporal characteristics without revealing information pertaining to the real trajectories. To emulate the setting of various organisations each with ownership over separate data silos, the global trajectory dataset, $X_{glob}$, is partitioned into C clients such that $X_{glob} = (X_1 \u222a X_2 \u222a ... \u222a X_C)$, where $X_c$ is the trajectory dataset owned by client c. The FL task is to train model G through FL using the distributed client datasets, and develop a synthetic dataset $X_{synth}$."}, {"title": "3.2 Traffic Flow Prediction", "content": "A city can be discretised into N disjoint regions, and $q_t \u2208 R^N$ can be defined as the traffic inflow for all regions N at time step t. Given historical traffic flow $Q = (q_1, q_2, ..., q_T) \u2208 R^{NxT}$, train a model to predict the traffic inflow of all regions at future time step T+t, hence predict $q_{T+\u03c4}$. Based on the partitioned trajectory data $X_{glob}$, client c can derive its regional traffic inflow data $Q_c$, and can also augment its own local dataset using the synthetic dataset $X_{synth}$. The federated traffic flow prediction task is then to collaboratively train a global prediction model using the distributed client datasets with synthetic data augmentation."}, {"title": "4 METHODOLOGY", "content": "This work aims to address a real-world setting of multiple organisations, each with their independent dataset of vehicle trajectories,"}, {"title": "4.1 Federated Generative Model", "content": "As a probabilistic generative model, a diffusion model aims to generate synthetic data by modeling two sequential processes: 1) A series of forward processes which gradually injects noise into the data, and 2) The reverse of these processes, which learn to capture the original data distribution from a noisier version [21]. Recent works have applied diffusion-based models for high-fidelity data generation in various domains [6, 7, 26]. The forward process is defined as a Markov chain with F Gaussian transitions that map the real data $x_0 \u223c q(x_0)$ to $x_F$, a latent variable with the same dimensionality as $x_0$, according to\n$$q(x_1,..., x_F|x_0) = \\prod_{f=1}^{F}q(x_f|x_{f-1}),$$,\n$$q(x_f|x_{f-1}) = N(x_f; 1 \u2212 \u03b2_fx_{f-1}, \u03b2_fI),$$,\nwhere $\u03b2_f$ denotes the variable variances which control the Gaussian noise. Importantly, the forward process allows sampling $x_f$ in closed form from $x_f = \\sqrt{\u0101_f}x_0 + \\sqrt{1 \u2013 a_f}e$, where $e \\sim N(0, I)$ and $a_f = \u03a0 (1-\u03b2_i)$. The reverse process can be parameterized as a Markov chain with learned Gaussian transitions\n$$Po(x_0,..., x_{f-1}|x_F) = p(x_F) \\prod_{f=1}^{F}Po(x_{f-1}|x_f),$$,\n$$Po(x_{f-1}|x_f) = N(x_{f-1}; \u03bc_\u00f8(x_f, f), \u03c3_\u00f8(x_f, f)^2I),$$,\nwhere $X_F \u223c N(0, 1)$, and where $\u03bc_\u0473(x_f, f)$ and $\u03c3_e(x_f, f)$ are the mean and variance, parameterized by 0. The objective of the diffusion model is to minimise the error between the Gaussian noise e and the predicted noise level $e_\u0473(x_f, f)$, over its training set. For a data sample $x_0$, the client loss $L_c$ is given by\n$$Lc(0) := Ex_{0}\u20acXc,e,f [[|\u20ac \u2013 \u20ac\u03b8(xf, f)||^2] .$$,\nConditional diffusion models enable additional information to be embedded into the reverse denoising process as conditional variables into Eq. (4). The diffusion-based generative model Diff Traj [35] is used as the conditional model for synthetic data generation in FedTPS, as illustrated in Algorithm 1."}, {"title": "4.1.1 Federated Framework", "content": "In a scenario where trajectory data is segmented across data silos from multiple organisations, a diffusion model cannot be trained under a traditional framework since the training data is not centralised. Given the vast amount of data required to train a conditional diffusion-based model such as Diff Traj, each client may not be able to individually train an accurate data generation model which captures the spatio-temporal characteristics across the city with high fidelity."}, {"title": "4.2 Federated Traffic Flow Prediction Model", "content": "Using each client's local individual trajectory dataset Xc, the regional inflow dataset, Qc, can be constructed, where Qc \u2208 RTxN, T is the number of regional traffic flow interval samples, and N is the number of regions captured in the data. Each client can augment their local dataset with the synthetic regional inflow dataset Qsynth derived from the synthetic dataset Xsynth shared by the server. Figure 3 illustrates the structure of the FedTPS framework for the traffic prediction task.\nThe local objective of a traffic flow prediction model is denoted by\n$$fc(\u03c9) = \\frac{1}{t} \\sum_t L(Fc(\u03c9; qt-h:t), qt+\u03c4)$$,\nwhere the model is parameterised by w, h denotes the number of previous traffic flow data samples fed to the model, and t is the number of time steps in the future for which we will be predicting traffic flow. The global objective is therefore\n$$argminf(\u03c9) := \\frac{\\sum_{c=1}^{C} |Q_c|}{|Q_{glob}|} f_c(\u03c9)$$"}, {"title": "4.3 Proposed Traffic Flow Prediction Model", "content": "The Temporal Attention Unit was introduced by Tan et al [22] and poses temporal attention as a combination of statical attention, which occurs within a data sample, and dynamical attention, which occurs between the frames of the data samples. The statical attention can be captured by performing depth-wise convolution, followed by a dilated depth-wise convolution, and finally a 2D convolution. The depth-wise convolution layers learn separate kernel filters for each of the channels in the encoded representation of the input. The dynamical attention is composed of a 2D average pooling layer, followed by a fully-connected layer. The final temporal attention is a result of the product between the statical and dynamical attention components.\nAn Encoder-Decoder architecture model architecture is presented in this work, wherein the Encoder and Decoder represent stacked 2D convolutional layers that encode a spatial representation of the data. To augment this Encoder-Decoder architecture for the task of traffic prediction, we can model the regional traffic flow data as a sequence of graphs, and employ the Graph Attention model [24] with multi-head attention, to learn multi-channel relationships between regions in the city. Figure 4 compares the model architecture between TAU and GATAU.\nThus, we introduce the Graph Attention with Temporal Attention Unit (GATAU). Figure 5 illustrates the GATAU module in further detail.\nOur input data $X \u2208 R^{N\u00d7T}$, where N is the number of nodes and T is the length of the input sequence, is first processed by a 2D convolutional layer to map the input into a higher dimension feature space. This output is fed into the Multi-Head Graph Attention (MGAT) module. Within the MGAT, attention coefficients are calculated which give a measure of the importance of relationships between nodes. The input into MGAT are node features, $h = {h_1, h_2, ..., h_N} \u2208 R^{T\u00d7F}$ where F is the input feature size. Attention coefficients for layer I are calculated by\n$$e_{ij} = LeakyReLU (a^T[Wh_i||Wh_j])$$,\nwhere $W \u2208 R^{F'\u00d7F}$ is a learnable weight matrix, F is the feature dimension of the input, F' is the output feature dimension which can be of different cardinality to F, and a is a function which computes the attention score from the concatenation of linearly transformed embedding of neighbouring nodes. We further apply a LeakyReLU activation for the attention coefficients. A fully connected linear single layer is used to represent function a. Then we normalise the attention scores across all connections for a given node through\n$$a_{ij} = \\frac{exp(e_{ij})}{\u03a3_{k\u2208N_i} exp(e_{ik})}$$,\nwhere Ni denotes the node connections of node i. The output features for every node are calculated as a linear combination of the normalised attention coefficients from Equation 10 to give\n$$h_i^{l+1} = \u03c3 (\u03a3_{j\u2208 N_i} a_{ij}W h_j)$$,\nwhere o is an ELU activation function. To enhance the stability of the model during training, a multi-head mechanism of attention is utilised for the graph attention module, wherein each attention head trains its own parameters. As per the original work which averages the multi-head outputs at the final layer [24], our model also averages the multi-head outputs after the single-layer MGAT module. The output from the MGAT module is of shape $X \u2208 R^{N\u00d7T\u00d7F'}$."}, {"title": "5 EXPERIMENT", "content": ""}, {"title": "5.1 Dataset", "content": "The experiments conducted in our work use the ride-sharing dataset published by Didi Chuxing GAIA Initiative. The dataset contains data for over three million journeys made in the city of Chengdu, China over the entire month of November 2016. Table 1 contains details of the dataset."}, {"title": "5.2 Data Pre-Processing", "content": "Targeting the real-world setting of multiple organisations collaborating to train trajectory generation and traffic flow prediction models through FL, the Didi Chuxing dataset is partitioned into subsets based on the number of clients emulated in the FL setting. It is assumed that each vehicle will collect data for a particular organisation, and hence, trajectories will be grouped according to the driver vehicle, and vehicles will be assigned exclusively to a particular organisation partition. For each client setting we partition vehicles evenly among the clients. Since individual drivers in a ride-sharing context will have preferences over their journeys and the number of orders they are assigned this will embed feature-based skews across the partitions and ensure realistic data distributions.\nOnce partitioned, we employ the same pre-processing steps as [35] to train the trajectory generation model. Each client also processes their trajectory dataset into regional vehicle inflow format for the traffic flow prediction task. For this step, the city of Chengdu is discretised into a rectangular 10x10 grid. Regional inflows are aggregated into 30 minute windows, and sequences of these windows over 6 hours are joined to be used by the traffic prediction model to predict the regional inflow 3 hours in the future. For the traffic flow prediction task, we segment the regional inflow data into the training, evaluation, and testing datasets by splitting the entire dataset into sets of 60%, 20%, and 20%, respectively.\nAs a measure of the heterogeneity between the regional inflow datasets, we employ two image-based comparison metrics to assess average similarity across the partitioned client datasets: normalised Root Mean Squared Difference (nRMSD) and Structural Similarity Index (SSI) - whose value ranges from 1 (identical) to -1 (dissimilar)."}, {"title": "5.3 Description of Experiments", "content": "(1) Trajectory Data Generation: This experiment details the performance of the trajectory generation models as a function of data heterogeneity and number of clients. This is important as it demonstrates the generative capability of a trajectory generation model in federated implementations with greater data heterogeneity..\n(2) Traffic Flow Prediction The purpose of this experiment is to assess how traffic flow prediction models perform over a range of different client cases under different FL frameworks, including FedTPS. This study is significant, as it emphasises how different federated mechanisms affect the performance of a global traffic flow prediction model, and highlights the benefits of a federated approach relative to traffic flow prediction models trained independently on separate data partitions.\n(3) Pre-Training with Synthetic Data: This study explores the effects of pre-training the global model in an FL framework using synthetic data. This is important, as it provides a method for training traffic flow prediction models in scenarios with limited communication or local training rounds.\n(4) Single Client Data Augmentation: In this experiment, the single client case is studied, wherein all data is owned by one organisation. The synthetic data generation method from FedTPS can be employed by the client to augment their dataset. This study emphasises the benefits of increasing the dataset size on the training process of a client.\n(5) Varying Global and Local FL Training: The purpose of this study is to explore the benefits of FedTPS when the ratio of local training rounds to global parameter aggregation rounds is increased. Increasing the number of local training rounds per global aggregation can lead to the local training processes of each client to drift from one another, which can detrimentally impact the parameter aggregation process."}, {"title": "5.4 Baselines", "content": "Unless otherwise stated, all models are trained using the Adam optimiser with a learning rate of 0.001 and a batch size of 32. For federated training, we use 80 global training rounds where we aggregate the updated client parameters after one local training"}, {"title": "6 RESULTS", "content": ""}, {"title": "6.1 Trajectory Data Generation", "content": "Figure 6 depicts trajectories generated by the trajectory generation model for a variety of different client settings. The single client example is equivalent to a centralised training paradigm. As the number of clients increases, the generated trajectories can be observed to be noisier. After converting the generated trajectories into regional inflow data, table 3 shows the similarity between generated samples and real traffic data based on on three metrics, averaged over the regional inflow data sequences. Some deviation is expected due to the generational diversity of the Diff Traj model, yet the results show close similarity meaning that the spatio-temporal characteristics of the original data were modelled accurately by the trajectory generation model. In general, increasing the number of clients in the FL setting results in a larger deviation from the real traffic data for the samples generated, as evidenced by the decreasing values of SSI and increasing MAD and nRMSE metrics."}, {"title": "6.2 Traffic Flow Prediction", "content": "Table 4 presents the results of the traffic prediction models, trained under different FL paradigms with varying client numbers, with respect to normalised Mean Absolute Error (nMAE) and Mean Absolute Percentage Error (MAPE) metrics. In general, FedTPS"}, {"title": "6.3 Effects of Synthetic Pre-training on Global Model Training", "content": "Figure 7 details the training loss and evaluation metrics of select traffic prediction models under FL settings, with and without pre-training. With pre-training, the models can be seen to converge more rapidly than the FL settings without pre-training. However, by global round 60, the models will have generally converged to the same training loss. Hence, while pre-training with synthetic data improves the convergence speed of the global model noticeably, it offers no improved model performance. The random initialisation of model parameters causes the training loss and evaluation metrics at global round 1 to be very high for models without pre-training. However, with synthetic data pre-training, the starting point of the model is competitive, as can be seen from the low evaluation metric scores at global round 1."}, {"title": "6.4 Synthetic Data Augmentation Single-Client Setting", "content": "Table 5 details the effects of synthetic data augmentation for multiple traffic flow prediction models in a single client case. For this dataset, it is clear in all cases that synthetic data augmentation can significantly improve model performance. In a single client case, where there is limited traffic regional inflow data, adding synthetic data can increase the amount of information available during training. Furthermore, a single client case would also yield a diffusion model which generates the highest fidelity trajectories; it can be seen from Table 3 that increasing the number of clients leads to a degradation in the quality of the generated trajectories."}, {"title": "6.5 Effect of Client Rounds and Server Rounds", "content": "Table 6 illustrates the results of this study, which explores varying the number of local training rounds per global aggregation"}, {"title": "6.6 Communication Costs", "content": "Table 7 presents FedTPS' communication cost for the distribution of the synthetic data to each federated client in, as well as the communication cost of the model parameters which occurs when the server distributes the global model to clients.\nFedTPS distributes the generated synthetic data to clients only at the initial round. Compared to the communication cost for a single round of the global model distribution process, FedTPS increases communication cost from 44.7% for the STGCN model, to 0.8% for the TAU model. The global model distribution process is a reoccurring communication cost, and hence over the entire federated training process the added communication cost of FedTPS becomes much less significant.\nTable 7 also demonstrates that the GATAU model incurs less communication costs than the TAU model. It should be noted that for cross-silo FL settings, communication bottlenecks are not as prevalent; server-to-client connections represent high-bandwidth data center network connections. Furthermore, complete client"}, {"title": "7 CONCLUSION", "content": "This paper addresses the task of Federated Learning for traffic flow prediction applications, and presents a new framework referred to as FedTPS which facilitates synthetic data augmentation of client datasets. By reducing data heterogeneity among clients and enhancing the information available during local training, FedTPS is able to consistently outperform other FL mechanisms for a variety of different client settings and different traffic flow prediction models. An FL setting to traffic flow prediction tasks is emulated using a large-scale ride sharing dataset for the city of Chengdu.\nThe trajectory data is partitioned into subsets to emulate individual organisations which collect their own data. Organisations can then collaborate to train models in a federated manner, without infringing upon data privacy. FedTPS first trains a federated synthetic data generation model, and then generates a synthetic regional traffic flow dataset from the learned global distribution of trajectories. This synthetic data is then distributed to all clients prior to the traffic flow prediction task so they may augment their local datasets. Future work could apply FedTPS in highly non-IID data settings."}]}