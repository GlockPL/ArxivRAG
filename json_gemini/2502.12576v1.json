{"title": "A Fuzzy Evaluation of Sentence Encoders on Grooming Risk Classification", "authors": ["Geetanjali Bihani", "Julia Rayz"], "abstract": "With the advent of social media, children are becoming increasingly vulnerable to the risk of grooming in online settings. Detecting grooming instances in an online conversation poses a significant challenge as the interactions are not necessarily sexually explicit, since the predators take time to build trust and a relationship with their victim. Moreover, predators evade detection using indirect and coded language. While previous studies have fine-tuned Transformers to automatically identify grooming in chat conversations, they overlook the impact of coded and indirect language on model predictions, and how these align with human perceptions of grooming. In this paper, we address this gap and evaluate bi-encoders on the task of classifying different degrees of grooming risk in chat contexts, for three different participant groups, i.e. law enforcement officers, real victims, and decoys. Using a fuzzy-theoretic framework presented in [3], we map human assessments of grooming behaviors to estimate the actual degree of grooming risk. Our analysis reveals that fine-tuned models fail to tag instances where the predator uses indirect speech pathways and coded language to evade detection. Further, we find that such instances are characterized by a higher presence of out-of-vocabulary (OOV) words in samples, causing the model to misclassify. Our findings highlight the need for more robust models to identify coded language from noisy chat inputs in grooming contexts.", "sections": [{"title": "1 Introduction", "content": "Grooming refers to the insidious process where predators forge a relationship with children and build up trust with the intention of sexual exploitation [13]. In [3], we noted that models assign much lower risk scores to high-risk chat contexts because these contexts contain implicit and coded language. In this work, we follow up on this initial investigation and evaluate Transformer-based classifiers on the task of detecting different degrees of grooming risk in online grooming conversations. We approach this task with a non-binary perspective, recognizing the necessity to differentiate between different levels of risk, because it can inform targeted preventive measures to safeguard individuals in realistic scenarios.\nGrooming has been described as a complex multi-stage phenomenon, with the severity of grooming varying as the chat progresses[13, 8]. Yet, works on automated grooming detection usually frame it as a binary classification problem [9, 15], categorizing entire chat instances as either grooming or non-grooming. Existing methods for grooming detection primarily rely on decoy conversations to train and fine-tune neural language models [15]. Whether decoy conversations can be used to approximate real grooming scenarios has been contested in linguistic and cyberforensic analyses of online grooming conversations [4, 12, 11]. This disparity in data distribution can have an impact on the generalization of such models when applied out of distribution (OOD).\nWe aim to address these gaps in this paper by evaluating Transformer sentence encoders on the task of classifying degrees of grooming risk expressed in natural language. We evaluate these models across diverse participant groups, specifically for real victims, law enforcement officers (LEO), and decoys. To that end, we use the fuzzy-theoretic framework that maps human assessments of grooming behaviors present within chats to the severity of grooming risk, as given in [3]. Our analysis reveals that fine-tuned sentence encoder classifiers show an increased rate of errors in identifying high-risk chat contexts, which is caused by the indirect speech pathways used by predators to manipulate and coerce victims. We find that Transformer classifiers fail to flag cases that contain coded language and lack sexually explicit content. Further, we find that the proportion of coded language used in grooming chats across different participant groups varies, causing the models to have different performance across populations.\nThis finding underscores the importance of robust modeling of indirect speech acts by language models, especially those utilized by predators. Notably, to the best of our knowledge, no prior work has incorporated human assessments of risk to compare fine-tuning results, making our study unique in its comprehensive evaluation of language model performance in grooming risk classification."}, {"title": "2 Categorizing Grooming Risk", "content": "The concept of risk, much like other subjective notions, embodies fuzziness. This fuzziness becomes apparent when considering grooming strategies, where what constitutes risky behavior and what does not is often blurry. These degrees of severity are influenced by the presence of grooming strategies within a given context [13]. Moreover, grooming strategies employed by individuals with malicious intent,"}, {"title": "A Fuzzy Evaluation of Sentence Encoders on Grooming Risk Classification", "content": "can vary widely in their subtlety and perceived harm [13]. Similar to the concept of risk, the assessment of grooming strategies itself lacks a precise boundary, as it depends on various factors such as individual perceptions, cultural norms, and situational contexts."}, {"title": "3 Methodology", "content": "This section details our approach to fine-tuning bi-encoders specifically for grooming risk classification, utilizing human assessments of grooming strategy behaviors in chat contexts. Our aim in this investigation is to evaluate the potential of fine-tuning models from the Transformer bi-encoder family [10], to estimate the extent of grooming risk present in chat interactions. Our focus extends beyond conventional language cues, as we aim to discern the limitations of these bi-encoders, particularly in scenarios where higher-risk situations may not always manifest through explicit language. Drawing from prior research indicating variations in language usage across different groups in grooming behaviors [13], we fine-tuned and evaluated our bi-encoders separately for distinct grooming contexts. Specifically, we focused on grooming conversations involving predators interacting with law enforcement officers (LEO), victims, and decoys. This analysis underscores the oversight of current automated models of grooming classification in accounting for nuanced differences across different participant conversations."}, {"title": "3.1 Task Definition", "content": "Definition 1 Chat window length (w): Number of messages in an exchange\nbetween two participants in a grooming conversation.\nDefinition 2 - Chat Context (c): A sequence comprising the current and last w - 1\nmessages in a grooming conversation. We fix w = 4 for our analysis.\nDefinition 3 - Grooming risk category (Cgroom): Denotes the severity of grooming\nwithin a chat context within the following three categories: {moderate, significant,\nsevere}."}, {"title": "3.2 Models Studied", "content": "We conducted our analysis on pre-trained bi-encoder models. These encoders leverage siamese and triplet network structures to derive sentence embeddings with semantically meaningful representations. These representations are optimized to capture semantic similarity between sentences in a vector space, making them suitable for various downstream tasks such as semantic search and clustering. The models we analyze include Sentence-BERT (SBERT) [10], MPNET [14], and RoBERTa [17]. The base models, as given within the names, are BERT, MPNET and ROBERTa respectively. We choose BERT and RoBERTa based on their application in prior work, and MPNET based on the quality of embeddings it generates."}, {"title": "3.3 Fine-tuning Details", "content": "The process of fine-tuning involves adapting pre-trained models to specific downstream tasks using task-specific data. We fine-tune bi-encoders to predict grooming risk class for a given chat context c by optimizing on minimizing the cross-entropy loss between the predicted and actual grooming risk classes. For model fine-tuning, we use an Adam optimizer [5], learning rate of 2.10\u20135, over 5 epochs, with a batch size of 4.\nTo understand the performance differences of models fine-tuned on interactions of predators with different participant groups, we fine-tuned three separate models. Thus, one model underwent fine-tuning on interactions of predators with law enforcement officers (LEO), another on interactions with real victims (Victim), and a third on interactions with decoys (Decoy)."}, {"title": "4 Results", "content": "We examined how well fine-tuned bi-encoder predictions fare on the task of identifying varying degrees of grooming risk in predatory chat contexts, across different participant groups. We consider chat contexts with a window size of w = 4. Our analysis reveals variations in model performance across different levels of grooming risk, with the model achieving higher accuracy in moderate contexts but showing poorer performance in significant and severe risk scenarios.\nWe report macro-averaged F1 scores on predictions in Table 1, and note a stark difference between F1 scores on moderate, significant and severe risk contexts. We find that across the three participant groups, moderate and severe risk contexts receive higher coverage as compared to significant risk contexts. These results highlight the limitations of fine-tuned bi-encoder models in detecting grooming behaviors where human evaluators have indicated a higher presence of grooming strategies. Moreover, all three models show similar error rates across different degrees of risk."}, {"title": "4.1 F1 score reported for different participant groups", "content": "These findings can be attributed to their inability to capture indirect communication pathways and adversarial grooming entrapment language (e.g. misspelled words, abbreviations, emojis, etc.) utilized by predators [7]. We also find that the models show the worst performance for significant risk scenarios for decoy chats while showing a 10%-20% better F1 score for law enforcement and real victim chats. This can be attributed to the differences in the way grooming conversations progress for real victims versus decoys [4]. These findings highlight the differences in grooming detection model performance across different participant groups and question the inherent assumption made by the prior work regarding training and finetuning automated models of grooming risk estimation using unrepresentative decoy and law enforcement chats.\nWe also analyzed model predictions across different participant groups, reporting the classification performance as depicted in Figure 2. We find that for law enforcement conversations, the model's classification performance has high coverage across all severities of grooming risk. On the other hand, the model incorrectly classifies more than 40% of significant and severe risk samples for decoy chats. Similar to the findings in [3], our model is unable to detect significant and severe grooming risk contexts due to an absence of explicit sexual and predatory language in many cases. Instead, predators adopt the use of adversarial grooming entrapment language (e.g. misspelled words, abbreviations, emojis, etc.), which lends to a higher occurrence of out-of-vocabulary (OOV) tokens in the fine-tuning set. Refer to Table 2 for a comparison of the presence of OOV tokens across different participant groups. An increased proportion of OOV tokens in chat instances leads to the deterioration of model performance, causing the model to not flag severe grooming risk instances."}, {"title": "5 Discussion and Conclusion", "content": "This paper investigates whether bi-encoders can learn to classify varying degrees of grooming risk inherent in online grooming conversations. We fine-tune and evaluate a transformer bi-encoder on the task of grooming detection across different participant groups, for increasing degrees of risk. Our analysis highlights that fine-tuned sentence encoders still exhibit higher Detecting coded language in harmful contexts is a non-trivial task, particularly in domains like online grooming, where automated models of detection cannot discount precision. While Transformer bi-encoders like SBERT show promise, they rely on surface-level features and are fine-tuned on decoy conversations instead of real victim chats, leaving questions about their effectiveness in realistic scenarios unanswered. Additionally, previous work has not compared model fine-tuning results with human assessments of grooming risk. This study addresses this gap by investigating the ability of bi-encoders to predict the level of grooming risk in chat contexts, for three different participant groups, i.e. law enforcement officers, real victims, and decoys. in categorizing more severe risk contexts, misclassifying them as the lowest severity defined in this work. We find that such discrepancies are tied to cases where surface form text does not contain explicit identifiers of grooming, but rather uses indirect speech pathways to manipulate victims. Our results align with recent research which shows that many predatory behaviors can evade detection from current filters through tactics like modifying explicit identifiers, introducing typos, using emojis, and out-of-vocabulary words [7]. In such cases, fine-tuning sentence embedding models does not help the model learn useful indicators of grooming within natural language and is a trivial approach to solving a non-trivial task. The sole reliance on word form and incentivizing training loss lead to learning shortcuts while ignoring nuance [2]. Even with the integration of long-range context, the task of encoding intricate lexical semantic phenomena to enhance natural language understanding continues to be a challenge [1, 16]. This finding underscores how the current fine-tuning methods for transformer bi-encoders still need to be improved, to be deployed in real-world scenarios and calls for the need for robust modeling of indirect speech acts employed in grooming contexts by language models. We plan to address these findings in future work."}]}