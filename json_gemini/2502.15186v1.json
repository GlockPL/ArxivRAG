{"title": "LUMINA-Net: Low-light Upgrade through Multi-stage Illumination and Noise Adaptation Network for Image Enhancement", "authors": ["Siddiqua Namrah", "Suneung Kim"], "abstract": "Low-light image enhancement (LLIE) is a crucial task in computer vision aimed to enhance the visual fidelity of images captured under low-illumination conditions. Conventional methods frequently struggle to mitigate pervasive shortcomings such as noise, over-exposure, and color distortion thereby precipitating a pronounced degradation in image quality. To address these challenges, we propose LUMINA-Net an advanced deep learning framework designed specifically by integrating multi-stage illumination and reflectance modules. First, the illumination module intelligently adjusts brightness and contrast levels while meticulously preserving intricate textural details. Second, the reflectance module incorporates a noise reduction mechanism that leverages spatial attention and channel-wise feature refinement to mitigate noise contamination. Through a comprehensive suite of experiments conducted on LOL and SICE datasets using PSNR, SSIM and LPIPS metrics, surpassing state-of-the-art methodologies and showcasing its efficacy in low-light image enhancement.", "sections": [{"title": "I. INTRODUCTION", "content": "Low-Light Image Enhancement (LLIE) has emerged as a vital component in various image-based applications, including surveillance [1, 2], autonomous vehicles [3, 4], medical imaging [5, 6], and consumer electronics [7, 8, 9], where high-fidelity images are paramount. Traditional approaches to LLIE have primarily relied on two well-established methods, histogram-based and retinex-based techniques. Histogram-based LLIE techniques analyze and modify pixel intensity distributions to adjust contrast and brightness, with Histogram Equalization (HE) being a widely used method [10, 11, 12]. Retinex-based LLIE techniques separate images into reflectance and illumination components, adjusting the latter to enhance contrast and visibility while preserving natural colors and textures [13, 14, 15, 16].\nHowever, conventional image capture and processing methods frequently fall short in low-light conditions, underscoring the urgent need for groundbreaking solutions that can effectively mitigate the challenges of diminished illumination. Existing Retinex-based methods rely on single-image inputs, limiting their ability to address varying exposure levels effectively."}, {"title": "II. RELATED WORK", "content": "Low-light image enhancement remains a significant challenge in computer vision and image processing, despite advancements in camera technology. Researchers have developed various solutions to address this issue, ranging from traditional methods to cutting-edge deep learning techniques, Retinex theory, and optimization algorithms. An enhanced Vector Wiener Filter (VWF) is introduced in [17] that leverages photon noise correlation and Fourier domain signal-to-noise ratio enhancement, resulting in remarkable image quality and super-resolution capability.\nBuilding on these foundations, further innovations have emerged that resulted in Histogram Equalization (HE) with adaptive illumination adjustment [10], an adjustable contrast stretching technique to enhance color image contrast [18]. Additionally, researchers have explored the use of deep learning techniques, such as Generative Diffusion Prior (GDP) [19] and light-effects suppression networks [20, 21, 22], to address uneven light distribution and over-enhancement. These advancements have significantly improved low-light image enhancement, but ongoing research is still needed to overcome the remaining challenges.\nRecent advancements in deep learning have transformed low-light image enhancement, with state-of-the-art network architectures pushing the limits of image restoration and quality. Researchers have proposed various innovative methods, including unsupervised image de-noising using GANs [23], zero-reference approaches for noise mitigation and image enhancement [24, 25], and networks leveraging RAW image data, Channel Guidance Net [26], Fourier-based transforms [27], UNet-based [28], Two-stage Single Image De-hazing Network (TSID Net) [29] and Adaptive Illumination Estimation Network (AIE Net) [30] architectures. Additionally, techniques such as style transfer-based data generation, teacher networks, and self-supervised approaches have been explored to address challenges like overexposure, underexposure, and noise removal, ultimately leading to significant improvements in low-light image enhancement.\nResearchers have proposed various Retinex theory-based methods for low-light image enhancement, leveraging diverse datasets, multi-metric evaluations, and deep learning techniques. These methods decouple illumination and reflectance components to adjust lighting, suppress noise, and revive colors, restoring image clarity and visual fidelity. Recent approaches include combining Retinex theory with self-supervised learning [13, 31, 32], zero-shot learning-based Retinex decomposition method (ZERRINNet) [33], and deep neural networks such as Decom-Net, Denoise-Net, Relight-Net, Diff-Retinex, DICNet, machine learning, and CNNs [14, 15, 34, 35] to achieve superior image restoration and enhancement outcomes in low-light environments."}, {"title": "III. METHOD", "content": "The proposed LUMINA-Net is a multi-stage framework designed to enhance low-light images through a synergistic approach. Initially, the Channel-Guidance (CG) Module in the Reflectance branch mitigates noise and refines features, preserving intricate details and textures often compromised in low-light conditions. Subsequently, the Color Enhancement (CE) Module in the Illumination branch modulates brightness and contrast while safeguarding texture fidelity, preventing the introduction of artificial artifacts during illumination adjustment. Finally, the Over-Exposure Correction (OEC) Module harmonizes excessively bright areas, ensuring uniform lighting and retaining information in highlighted regions, thereby producing a more balanced and visually appealing image.\n#### A. Preliminary\nThe Retinex theory models a low-light image $I$ [36] as,\n\n$I = L \\circ R,$\n\nwhere $\\circ$ denotes element-wise multiplication. Illumination ($L$) represents the light intensity in the scene, expected to be smooth and texture-less, while reflectance ($R$) captures the inherent properties of objects, such as textures and details. Conventional illumination and reflectance estimation techniques rely on predefined, hand-crafted priors that often do not accommodate the complexity and variability of real-world scenes and lighting conditions [37]. To address this limitation, we exploit paired low-light images, $I_1$ and $I_2$ with same reflectance $R'$ but different illuminations $L_1$ and $L_2$.\n\n$I_1 = L'_1 \\circ R', I_2 = L'_2 \\circ R'.$\n\nThe introduction of paired low-light images in LUMINA-Net injects additional constraints and valuable information, bolstering the robustness of illumination-reflectance decomposition.\n#### B. Proposed Method\nOur proposed LUMINA-Net image enhancement technique seamlessly integrates three synergistic components, the Channel-Guided (CG) Module for adaptive illumination-reflectance decomposition, the Color Enhancement (CE) Module for vibrant color restoration and refinement, and the Over-Exposure Correction (OEC) Module for balanced brightness and detail preservation. This combination enables the transformation of low-quality images into visually stunning and detailed outputs, yielding a robust and efficient image enhancement framework.\n1) Channel-Spatial Guidance (CG) Module: The CG Module a pivotal role in enhancing the quality of reflectance maps generated from illumination-reflectance decomposition. This module leverages both Channel Attention and Spatial Attention mechanisms to selectively emphasize crucial features and suppress noise, resulting in refined reflectance maps. Initially, the CG Module takes two low-light input images with reflectance maps $R_1$ and $R_2$ that represents the intrinsic properties of the scene under different lighting conditions.\nThe Channel Attention Mechanism then identifies significant feature channels within these reflectance maps. This is achieved through global average pooling (GAP), which generates channel descriptors that capture the importance of each channel. These descriptors are subsequently refined via a convolutional layer and sigmoid activation, producing channel-wise weights that ensure the most critical features are emphasized.\nIn parallel, the Spatial Attention Mechanism targets essential spatial regions within the reflectance maps, such as edges and textures. This is accomplished by performing both global average pooling and max pooling along the channel dimension, generating spatial descriptors. These descriptors are then processed through a convolutional layer and sigmoid activation, producing a spatial weight map that highlights key regions for refinement.\nThe synergistic combination of Channel Attention and Spatial Attention mechanisms yields a refined reflectance map from $R_{f1}$ and $R_{f2}$. This integrated approach preserves high-frequency details providing a more accurate and cleaner representation of reflectance, robust to both low-light and normal-light conditions. The CG Module significantly leverages the strengths of both attention mechanisms to enhance the reflectance refinement, mitigating noise and highlighting crucial features to achieve superior image reconstruction outcomes that surpass those of traditional Retinex-based approaches.\n2) Color Enhancement (CE) Module: The Color Enhancement (CE) Module plays a crucial role in refining the illumination maps $L_1$ and $L_2$. Its primary objective is to enhance the brightness and contrast of the image, particularly in low-light regions, while ensuring natural and smooth lighting transitions.\nTo achieve this, the module employs a series of convolutional layers to improve the quality of the illumination maps. Additionally, it utilizes Adaptive Average Pooling (AAP), which produces a global descriptor for each channel. The descriptor is then processed through a fully connected layer and sigmoid activation, generating channel-wise attention weights. These weights modulate the features of the illumination maps, enhancing important features while suppressing irrelevant information.\nThe refined illumination maps, $L_{f1}$ and $L_{f2}$, are computed using Adaptive Average Pooling (AAP), which adaptively aggregates spatial information to enhance the quality of the illumination estimates. This process improves the accuracy and quality of the illumination estimates by leveraging attention mechanisms, which focus on the most relevant features for illumination correction. The channel-attended features help adaptively adjust the illumination maps, refining them for better performance in both low-light and normal-light conditions.\nThe result is a more visually appealing and well-lit image, with enhanced clarity and detail, which can be applied in various lighting scenarios while preserving the scene's natural appearance.\n3) Over-Exposure Correction (OEC) Module: The Over-Exposure Correction (OEC) Module is designed to address the issue of overexposure by refining the combined illumination ($L_f$) and reflectance ($R_f$) maps. This process restores details in overexposed regions, ensuring a balanced exposure through an intermediate image representation, generated by element-wise multiplying the refined illumination and reflectance map. This intermediate representation is then passed through the OEC Module, which is specifically designed to handle areas of the image that are excessively bright or saturated.\nThe module employs residual learning along with sigmoid activation to suppress overexposure artifacts while preserving fine image details. The output of the OEC Module is the final enhanced image $I_f$, which retains natural brightness and structure while correcting overexposed regions and resulting in a well-balanced image with improved exposure."}, {"title": "C. Loss Functions", "content": "The LUMINA-Net architecture employs a multi-faceted loss function strategy to ensure effective training and image enhancement. This integrated approach minimizes differences between predicted and ground-truth images while preserving critical image characteristics, including perceptual quality, spatial smoothness, and reflectance consistency, thereby achieving a harmonious balance between visual fidelity and structural integrity.\n#### 1) Projection Loss($L_p$):\nThe projection step ensures that the input image is more suitable for decomposition under the Retinex model by removing noise and irrelevant features. The projection loss measures the difference between the original input image $I_1$ and the projected image $I'_1$, guiding the transformation of the original image into a cleaner, noise-free representation that better aligns with the Retinex assumptions. This loss can be expressed as:\n\n$L_p = ||I_1 - I'_1||_2.$\n\nBy reallocating the decomposition error to the projection stage, this process ensures more accurate and detailed reflectance and illumination maps, resulting in enhanced image quality and realistic reconstructions.\n#### 2) Consistency Loss ($L_c$):\nThe $L_c$ is derived from the Retinex theory and plays a pivotal role in maintaining consistency between the reflectance maps of paired low-light images. This consistency enforces accurate reflectance decomposition and implicitly addresses sensor noise without requiring additional handcrafted constraints. The loss is defined as:\n\n$L_c = ||R_{f1} - R_{f2}||_2,$\n\nwhere $R_{f1}$ and $R_{f2}$ are the reflectance components of the paired low-light images.\nBy minimizing the difference between the reflectance maps of the two images, $L_c$ leverages the randomness of noise across paired images, enabling effective noise removal while ensuring reflectance consistency. This enhances the robustness and accuracy of the decomposition process for low-light image enhancement.\n#### 3) Retinex Loss ($L_R$):\nIn this approach, the core concept is to decompose low-light images into two distinct components: illumination and reflectance. The goal is to estimate the reflectance map that captures the intrinsic properties of the scene and the illumination map that represents the varying lighting conditions. To achieve this, a set of constraints is applied to ensure that the decomposition is consistent and physically meaningful, resulting in a high-quality enhancement of low-light images. The loss function that facilitates this decomposition is formulated as follows:\n\n$L_R = ||R_{f1} \\circ L_{f1} - i||_2 + ||R_{f1} - i/stopgrad(L_{f1})||_2 + ||L - L_o|| + ||\\nabla L||_1,$\n\nwhere $i$ denotes the input low-light image, $R_{f1}$ and $L_{f1}$ are the predicted reflectance and illumination maps, $L_o$ is the initial estimate of the illumination, and $\\nabla L$ represents the gradient of the illumination map. $||L - L_o||_2$, forces the illumination estimate to be close to the initial illumination $L_o$, which is computed using the maximum values of the R, G, and B channels of the input image. This initialization serves as a prior, guiding the network toward more realistic illumination estimates. $|\\nabla L||_1$, is a smoothness constraint that encourages the illumination map to have smooth transitions, avoiding sharp or unnatural changes in lighting. By minimizing this loss function, the network effectively separates the reflectance and illumination components, improving the overall quality and realism of the enhanced low-light image without relying on extensive handcrafted priors or ground-truth data.\n#### 4) Perceptual Loss ($L_{per}$):\nThe $L_{per}$ measures the similarity between the predicted and ground-truth images in a high level feature space, capturing their perceptual quality. This loss retains essential visual details during image reconstruction by comparing features extracted from a pre-trained model.\n\n$L_{per} = ||(\\phi(I_{f1}), \\phi(I_{GT}))||_2,$\n\nwhere $I_{f1}$ is the predicted image, $I_{GT}$ is the ground-truth image, and $\\phi()$ represents the feature extraction function of the pretrained model.\n#### D. Combined Loss\nThe final loss function is a meticulously crafted weighted sum of individual losses, synergistically training the model to minimize perceptual, consistency, projection, reflectance, total variation and edge losses while preserving naturalness and spatial consistency. The model achieves a harmonious balance between visual fidelity, detail retention, and spatial coherence, ultimately enhancing its performance and generating high-quality images.\n\n$L_{all} = w_0 \\times L_p + w_1 \\times L_c + w_2 \\times L_R + w_3 \\times L_{per},$\n\nwhere $L_p$ is the projection loss, $L_c$ is the reflectance consistency loss, $L_R$ is the retinex loss, $L_{pre}$ is the perceptual loss, and $w_0, w_1, w_2, w_3$ are weights controlling the importance of each loss term."}, {"title": "IV. EXPERIMENTS", "content": "#### A. Experimental Datasets\nLUMINA-Net is trained using low-light image pairs derived from the SICE [53] and LOL datasets [41]. For evaluation, we select an additional 50 sequences (150 images) from the SICE dataset and utilize the official evaluation set (15 images) from the LOL dataset to assess the model's performance. Both SICE and LOL contain reference images, allowing us to employ multiple metrics for objective evaluation, including PSNR, SSIM [54], LPIPS [55]. A higher PSNR or SSIM score indicates that the enhanced result is closer to the reference image in terms of fidelity and structural similarity. Conversely, lower LPIPS values signify improved enhancement quality and more accurate color reproduction.\n#### B. Implementation Details\nWe implement LUMINA-Net using PyTorch. During training, images are randomly cropped to a size of 256 \u00d7 256 to account for spatial variations. A batch size of 1 is applied for efficient memory usage, given the high-resolution image processing requirements. The ADAM optimizer [56] is employed with an initial learning rate of $1 \\times 10^{-4}$, along with a cosine annealing learning rate scheduler for gradual decay. The number of training epochs is set to 400 to ensure convergence. For low-light enhancement scenarios, we empirically set the default correction factor to $\\lambda = 0.2$, as suggested in [32]. For the LOL dataset, the correction factor was adjusted to $\\lambda = 0.14$. The hyper-parameters $w_0, w_1, w_2$ and $w_3$ in the loss formulation are set to $w_0 = 5, w_1 = 1, w_2 = 1$ and $w_3 = 0.1$ based on empirical evaluation for optimal performance. Our models are run on NVIDIA TITAN XP GPUs.\n#### C. Comparison with state-of-the-arts methods\nLUMINA-Net is compared with 16 state-of-the-art low-light image enhancement (LIE) methods, which can be grouped into three categories: traditional methods (SDD [38], STAR [39]), supervised approaches (MBLLEN [40], RetinexNet [41], GALDNet [42], KinD [43], DRBN [44], URetinexNet [45]), and unsupervised methods (Zero-DCE [46], RRDNet [47], RUAS [48], SCI [49], EnlightenGAN [50], PairLIE [32], FourierDiff [51], and NeRco [52]). These comparisons are made based on the performance of each method, using their official codes with the recommended parameters to ensure fairness and consistency in the evaluation. The results obtained from these methods are used as a benchmark for assessing LUMINA-Net's performance.\n#### F. Ablation Studies\nTo validate the effectiveness of LUMINA-Net's components, including its modules and overall design, we conducted ablation experiments on the LOL dataset, with results presented in Table II and Figure 4."}, {"title": "V. CONCLUSION", "content": "In this paper, to address the challenges faced by existing low-light image enhancement methods, such as preserving image details in dark regions and accurately recovering colors, we propose LUMINA-Net. This method combines three key modules: the Channel-Guided (CG) Module for enhancing reflectance in dark regions, the Color Enhancement (CE) Module for adjusting illumination and ensuring accurate color restoration, and the Over-Exposure Correction (OEC) Module for handling overexposure. These modules work together to significantly improve both the quality and the naturalness of enhanced images. Through extensive experiments on low-light datasets, LUMINA-Net consistently demonstrates superior performance over existing state-of-the-art methods, achieving better detail preservation, accurate color recovery, and reduced artifacts, making it a promising solution for real-world low-light imaging applications."}]}