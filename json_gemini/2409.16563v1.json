{"title": "Enhancing Disease Detection in Radiology Reports Through Fine-tuning Lightweight LLM on Weak Labels", "authors": ["Yishu Wei, PhD", "Xindi Wang, PhD", "Hanley Ong, M.D.", "Yiliang Zhou, MS", "Adam Flanders, M.D.", "George Shih, M.D.", "Yifan Peng, PhD"], "abstract": "Despite significant progress in applying large language models (LLMs) to the medical domain, several limitations still prevent them from practical applications. Among these are the constraints on model size and the lack of cohort-specific labeled datasets. In this work, we investigated the potential of improving a lightweight LLM, such as Llama 3.1-8B, through fine-tuning with datasets using synthetic labels. Two tasks are jointly trained by combining their respective instruction datasets. When the quality of the task-specific synthetic labels is relatively high (e.g., generated by GPT4-o), Llama 3.1-8B achieves satisfactory performance on the open-ended disease detection task, with a micro F1 score of 0.91. Conversely, when the quality of the task-relevant synthetic labels is relatively low (e.g., from the MIMIC-CXR dataset), fine-tuned Llama 3.1-8B is able to surpass its noisy teacher labels (micro F1 score of 0.67 v.s. 0.63) when calibrated against curated labels, indicating the strong inherent underlying capability of the model. These findings demonstrate the potential of fine-tuning LLMs with synthetic labels, offering a promising direction for future research on LLM specialization in the medical domain.", "sections": [{"title": "1. Introduction", "content": "There have been extensive studies on applying large language models (LLMs) in the medical domain. However, several challenges must be overcome before their practical application is feasible. From the model perspective, privacy concerns limit the usage of commercial LLMs, such as GPT-4, since patient data involves highly sensitive information, and current de-identification techniques do not completely protect privacy [1]. Additionally, the financial, computational, and technical demands of deploying super-large, strong LLMs (e.g., Llama 3.1-405B) pose a significant challenge for hospitals. Consequently, lightweight LLMs (e.g., Llama 3.1-8B) are considered more feasible, though they often sacrifice the performance significantly [2]. From the data perspective, although there is an abundance of public datasets, they are often disease-specific and do not reflect the diversity found in hospital patient cohorts. Additionally, while hospitals possess vast amounts of patient data, the available labels are often of poor quality or entirely missing.\nAddressing these challenges can be achieved by fine-tuning lightweight LLMs using synthetic or weakly labeled data. This strategy draws from traditional deep learning distillation techniques, where predictions from a strong model are used to \"teach\u201d a less powerful model. For instance, Pangakis et al. [3] have demonstrated that fine-tuning on LLM-generated data can yield outcomes comparable to human-annotated data. While most previous research [4, 5, 6] has shown the effectiveness of this approach mainly when the baseline model performs modestly, few studies indicate that substantial improvements can still be achieved in the radiology domain [7].\nTo bridge this gap, this study aims to fine-tune Llama 3.1-8B using weak labels on two radiology-specific tasks. The first task involves multiple-choice lung disease classification on radiology reports, where the model predicts diseases from a predetermined list. For this, a rule-based labeler, Negbio[8], was used to extract 13 labels from radiology reports and then construct instructions for fine-tuning lightweight LLMs. Despite the inherent noisiness of these labels, we found the fine-tuned Llama 3.1-8B, demonstrated notable performance improvements on human-curated labels, even exceeding the accuracy of the Negbio. This result is somewhat unusual in traditional deep learning models, where the student model generally underperforms compared to the teacher model.\nThe second task focuses on open-ended lung disease detection, requiring the LLM to extract abnormal findings from radiology reports corresponding to ICD-10 codes. For this task, the lightweight Llama 3.1-8B was fine-tuned using"}, {"title": "2. Materials and Methods", "content": "This study proposes a multi-task learning framework designed to enhance disease detection in radiology reports by leveraging both structured and unstructured tasks (Figure 1). Our approach adopts instruction tuning for a multiple-choice disease classification task and an open-ended disease detection task."}, {"title": "2.1 Problem Formulation", "content": "Multiple-choice Disease Classification The first task in our multi-task learning framework is a multiple-choice disease classification problem, aimed at identifying a specific lung disease or abnormal findings from a radiology report (Figure 1a). In this setup, the input consists of a natural language question representing the radiology report and a list of candidate answers representing potential diagnoses. Here, we focus on 13 common lung diseases: Atelectasis, Cardiomegaly, Consolidation, Edema, Enlarged Cardiomediastinum, Fracture, Lung Lesion, Lung Opacity, Pleural Effusion, Pleural Other, Pneumonia, Pneumothorax, Support Devices. A sample answer for this problem can be ['Fracture\u2019, \u2018Lung Opacity', ...].\nOpen-ended Disease Detection The second task is an open-ended disease detection problem, aimed at enabling the model to generate a free-form response from a radiology report (Figure 1b). Unlike the multiple-choice task, this task doesn't restrict the model to a predefined list of potential lung diseases or abnormal findings. Instead, it prompts the model to identify and describe diseases mentioned in the radiology report. Therefore, open-ended disease detection allows for a deeper evaluation of the LLM's comprehension, contextual understanding, and ability to enumerate various abnormal findings. A sample answer for this problem can be [`Small pleural effusions', 'Mildly enlarged cardiac silhouette\u2019, ...], with these terms being directly extracted from the original report."}, {"title": "2.2 Data Preparation", "content": "To support the two tasks, we utilized three chest X-ray datasets. For the multiple-choice disease classification task, we employed the MIMIC-CXR dataset [10]. For the open-ended disease detection task, we used the NIH-CXR/MIRDC [11] and WCM [12] datasets. Each dataset comprises chest X-ray images and their corresponding radiology reports, which were processed to extract the relevant information necessary for model training. The detailed statistics of these datasets are summarized in Table 1."}, {"title": "2.3 Approaches", "content": "In our study, we employed Llama 3.1-8B as the foundational model. It functions as a decoder-only model, processing inputs and generating text autoregressively. We fine-tuned Llama 3.1-8B jointly on two distinct tasks by mixing instruction sets derived for both tasks (Figure 2). This joint approach was utilized to optimize the performance of the Llama 3.1-8B model across both tasks.\nMultiple-choice Disease Classification This task was conducted on the MIMIC-CXR dataset. In the instruction-based learning approach, we employ labels generated by NegBio as outputs. The instructions for fine-tuning are relatively simplified, lacking the Guidelines and Examples, in contrast to the more detailed prompts used in the final detection task.\nAfter the fine-tuning process, Llama 3.1-8B was provided with the 13 diseases and prompted to predict diseases from radiology reports in a zero-shot fashion. Details of the prompts are provided in Figure 1a.\nOpen-ended Disease Detection In the open-ended disease detection task, Llama 3.1-8B was prompted to extract phrases from the radiology report that can be mapped to ICD-10 codes in a few-shot manner.\nIn the absence of predefined labels, we implemented a two-step approach. Initially, labels were synthesized using GPT-40 (gpt-40-2024-05-13, https://openai.com/index/hello-gpt-40/). Specifically, 9,000 samples were selected from the WCM dataset. GPT-40 was then prompted to \"extract phrases in the report that represents a potential ICD-10"}, {"title": "2.4 Implementation Details and Evaluations", "content": "We fine-tune Llama 3.1-8B using the Low-Rank Adaptation (LoRA) technique [15] for both tasks. LoRA was specifically applied to the attention modules, targeting the q-proj and v-proj matrices. The LoRA rank is set to 8, with a scaling factor (LoRA alpha) of 16. The AdamW optimization algorithm was employed, utilizing a learning rate of 3 \u00d7 10-4 and a weight decay of 0.01. The instruction datasets from both tasks\u2014the open-ended and multiple-choice\u2014were merged and shuffled for fine-tuning.\nThe training was conducted on a single NVIDIA A100 GPU, with all results reported at a consistent temperature setting of 0.1 across various configurations.\nTo evaluate the performance of fine-tuned LLM on multiple-choice disease classification tasks, we reported micro-averaging precision, recall, and F1 across all diseases. Since the responses are selected from a predefined list of possible diseases, a generated disease is deemed correct only if it exactly aligns with an option from that list.\nTo evaluate the effectiveness of open-ended disease detection, we reported the micro-averaging accuracy, recall, and F1 score metrics. These metrics consider a prediction correct (true positive) if two consecutive words match between the ground truth and the generated phrases. For instance, phrases such as \u2018bilateral peribronchial cuffing' and 'peribronchial cuffing' are considered as a match. Several reasons underpin the decision to use this matching criteria rather than requiring an exact match. First, most medical terms contain fewer than two words, and GPT-40 is prompted to generate concise phrases. Second, labeling by radiologists may vary. And thirdly, unigram can be easily misleading and informative, particularly with function words like \u201cof\u201d or \u201cfor\". This matching criterion is also supported by a detailed error analysis, affirming its validity."}, {"title": "3. Results and Discussion", "content": ""}, {"title": "3.1 Findings from Multiple-choice Disease Classification", "content": "Effectiveness of Fine-tuning We fine-tuned the Llama 3.1-8B model using the NegBio-generated labels and evaluated its performance against human-curated labels. Table 3 shows that fine-tuning substantially improved the model's performance (0.67 vs 0.54 on human-curated labels). These results indicate that instruction-based fine-tuning can significantly bolster the model's classification capabilities, even when benchmarked against more sophisticated, curated label sets in the radiology domain.\nSignificantly, when the fine-tuning utilizes 9,000 samples (3,000 from MIMIC-CXR and 6,000 from WCM), Llama 3.1-8B achieved a recall of 0.72, substantially better than NegBio, which achieved a recall of 0.51. This performance indicates that with an optimal level of fine-tuning, the model's intrinsic qualities enable it to generalize more effectively and provide more accurate disease classifications.\nAdditionally, we observed that the fine-tuned Llama 3.1-8B has demonstrated the ability to surpass the performance of the NegBio, achieving an F1 score of 0.67 compared to 0.63. In this study, Llama 3.1-8B is substantially larger and technologically superior compared to the NegBio. This phenomenon is noteworthy as it suggests that under certain conditions, the super intelligent 'student' model can indeed exceed the weak intelligent \u2018teacher' model's limitations.\nEffectiveness of training data size and learning rate Our findings suggest that there exists an optimal fine-tuning intensity, which is characterized by the amount of data used and/or can be adjusted through learning rate modifications.\nAs for the amount of fine-tuning data, Table 3 suggests that at fine-tuning intensities of 9,000 samples, the model effectively harnesses both its intrinsic capabilities and the specific data from the NegBio-labeled dataset, resulting in the best performance outcomes. Subsequently, the model began to overfit the NegBio-generated labels, which unfortunately resulted in a degradation of performance when evaluated against the human-curated labels.\nAs for the learning rates, we evaluated the performance of Llama 3.1-8B with different learning rates. The model was trained on 100,000 samples from the MIMIC-CXR dataset and then tested on human-curated data from the same MIMIC-CXR source. Fig 3 achieved the best performance (F1 of 0.68) when the learning rate is 1 \u00d7 10-5. These findings indicate that careful hyperparameter tuning is critical to achieving optimal performance."}, {"title": "3.2 Results on open-ended disease detection", "content": ""}, {"title": "Effectiveness of Fine-tuning", "content": "Table 4 summarizes the performance of models on the open-ended disease detection task. The model was trained on the combination of WCM and MIMIC-CXR datasets and then tested on the NIH/MIDRC dataset. Notably, as the volume of training data increases, the performance of Llama 3.1-8B (F1 of 0.91) approached that of the larger GPT-40 (F1 of 0.93). This demonstrates the effectiveness of instruction-based fine-tuning in enabling a smaller model to reach performance levels comparable to those of a larger one.\nFurthermore, this study distinguishes itself from previous research by documenting performance improvements at an already high baseline. Initially, even before the fine-tuning process, Llama 3.1-8B demonstrated a strong F1 score of 0.83, which was enhanced to 0.91 through fine-tuning. This improvement emphasizes the potential of fine-tuning strategies, not only to elevate model performance but also to maintain high-performance standards even from a higher starting point.\nLike the multiple-choice disease classification task, continuously adding fine-tuning data does not necessarily enhance the final outcome in open-ended disease detection. Specifically, there was no improvement in performance when the sample size was increased to 13,500. is suggests that beyond a certain point, additional data and prolonged training does not contribute to, and may even hinder, the efficiency and effectiveness of the model's learning process, indicating a potential saturation point where the model has maximized its learning from the available data."}, {"title": "Error analysis", "content": "We conducted an error analysis to categorize the error types. Here we used the model with the best performance (fine-tuned on 9,000 samples).\nFirst, we observed that paraphrasing of the ground truth occurred ten times (Table 5). When annotating labels, radiologists may paraphrase the original text from the reports, whereas the LLM is instructed to directly extract original phrases. Table 6 demonstrates some examples. For example, a phrase like \u201cemphysematous and fibrotic changes of the lungs\u201d might be separated by the radiologist into \u201cemphysematous changes\" and \"fibrotic changes.", "atelectasis\" being recorded as \\\"atalectasis\" in the radiology reports. Finally, we observed the discrepancies between machine-generated\"\n    },\n    {\n      \"title\"": "3.3 Evaluating joint vs separate fine-tuning of models"}, {"content": "Additionally, we conducted experiments to fine-tune the model separately for each task. For the multiple-choice task, as the instruction dataset sample size was increased to 5,000, the peak F1 score of 0.66 was achieved (Figure 4a). However, as the sample size continued to expand, reaching 10,000, there was a noticeable decline in the F1 score to 0.62. For the open-ended detection task (Figure 4b), the highest performance was reached when the sample size increased to 6,000 (F1 score of 0.91). The performance outcomes of both joint and separate training methods proved to be similar across the tasks, showing no significant differences in observations."}, {"title": "3.4 Limitations", "content": "Firstly, the open-ended disease detection task with radiology reports is relatively simple compared to the complexities inherent in real-world clinical disease diagnosis. Radiology reports, unlike unstructured clinical notes, typically adhere to a structured format, utilize consistent terminologies, and contain densely packed information, which simplifies the task of identifying mentions of diseases within the text. Despite these simplifications, the model's ability to approach the performance of human experts in an open-ended context is noteworthy. It highlights the potential of LLMs to evolve into versatile tools for medical assistance. Ultimately, this work can be seen as a foundational step towards developing more advanced models capable of managing the intricacies of open-ended disease diagnosis in real clinical settings.\nSecondly, due to resource constraints with available radiologists, we managed to label only about 100 instances for testing. Obtaining a larger volume of annotated data would enable a more thorough evaluation of the performance and generalization capabilities of the LLMs. Expanding the dataset could significantly contribute to enhancing the reliability of our findings and provide deeper insights into the model's abilities across diverse clinical settings.\nLastly, this study did not engage in extensive prompt engineering beyond including a few examples for the model. Enhancements in recall could potentially be achieved by generating multiple outputs and employing ensembling techniques to integrate the results. Similarly, precision could be improved by implementing self-refinement or verification"}, {"title": "4. Conclusion", "content": "In this study, we explored the efficacy of fine-tuning a lightweight LLM using synthetic labels. We simultaneously fine-tuned the model for a multiple-choice disease classification task and an open-ended disease detection task through instruction-based learning. In the multiple-choice task, Llama 3.1-8B was capable of outperforming its rule-based teacher (i.e., NegBio), showcasing the model's intrinsic capabilities. In the open-ended detection task, GPT-40 delivered expert-level accuracy, which Llama 3.1-8B effectively adopted through instruction-based fine-tuning. These experiments, which span the spectrum from lower to higher synthetic label quality, collectively underscore the substantial potential of synthetic labels in boosting LLM performance across diverse tasks."}]}