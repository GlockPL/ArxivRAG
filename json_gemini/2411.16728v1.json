{"title": "Maximizing the Impact of Deep Learning on Subseasonal-to-Seasonal Climate\nForecasting: The Essential Role of Optimization", "authors": ["Yizheng Guo", "Tian Zhou", "Wangyi Jiang", "Bo Wu", "Liang Sun", "Rong Jin"], "abstract": "Weather and climate forecasting is vital for sectors such\nas agriculture and disaster management. Although numeri-\ncal weather prediction (NWP) systems have advanced, fore-\ncasting at the subseasonal-to-seasonal (S2S) scale, span-\nning 2 to 6 weeks, remains challenging due to the chaotic\nand sparse atmospheric signals at this interval. Even state-\nof-the-art deep learning models struggle to outperform sim-\nple climatology models in this domain. This paper identi-\nfies that optimization, instead of network structure, could\nbe the root cause of this performance gap, and then we\ndevelop a novel multi-stage optimization strategy to close\nthe gap. Extensive empirical studies demonstrate that our\nmulti-stage optimization approach significantly improves\nkey skill metrics, PCC and TCC, while utilizing the same\nbackbone structure, surpassing the state-of-the-art NWP\nsystems (ECMWF-S2S) by over 19-91%. Our research con-\ntests the recent study that direct forecasting outperforms\nrolling forecasting for S2S tasks. Through theoretical anal-\nsis, we propose that the underperformance of rolling fore-\ncasting may arise from the accumulation of Jacobian matrix\nproducts during training. Our multi-stage framework can\nbe viewed as a form of teacher forcing to address this issue.", "sections": [{"title": "1. Introduction", "content": "Weather and climate forecasting encompasses various\ntime scales, including nowcasting, short and medium-range\nweather forecasting, subseasonal-to-seasonal forecasting,\nand climate forecasting [23, 37]. These predictions are\ncrucial for social management, agriculture, disaster pre-\nparedness, and other sectors, reflecting our understanding\nof Earth's status as humanity's sole home [8, 29, 34, 45].\nAmong these, subseasonal-to-seasonal (S2S) forecasts can\npredict major natural disasters [38,41] such as droughts and\nfloods [34], which play a significant role in disaster pre-\nparedness and policy-making. It is well recognized that\naccurate forecasts for the S2S regime\u2014specifically, 2 to\n6 weeks ahead remains an operational challenge due to\nthe chaotic nature of the atmospheric system and the differ-\ning temporal scale mechanisms involved compared to short-\nterm weather forecasting [30,44].\nCurrently, commonly used weather and climate fore-\ncasts rely on Numerical Weather Prediction (NWP) sys-\ntems [3, 12, 22], which are based on solving physical or-\ndinary differential equations (ODEs) for thermodynamics,\nfluid flows, etc. In meteorology, weather signals decay and\neventually disappear over time, typically ranging from 1\nhour to 14 days, while climate signals gradually emerge\nover a period of 2 to 4 months. In between, the S2S fore-\ncasting occurs on a time scale typically ranging from 14\ndays to 42 days (2 weeks to 6 weeks). The extreme spar-\nsity of signals in this interval makes it very challenging for\nphysical-based NWP systems to produce reliable forecasts.\nIn addition, both the huge computational cost and limited\nmodeling capacity for the physical world significantly con-"}, {"title": "2. Related Work", "content": "S2S prediction primarily relied on numerical weather\nprediction (NWP) methods from meteorological centers\nsuch as the UK Meteorological Office (UKMO) [42],\nthe National Centers for Environmental\nPredic-\ntion (NCEP) [33], the China Meteorological Adminis-\ntration (CMA) [43], and the European Centre for Medium-\nRange Weather Forecasts (ECMWF) [36]. Among them,\nECMWF's weather forecasts are widely recognized as\nthe most accurate, and they publicly provide the forecast\nresults to society. However, NWP models are extremely\ncostly. With the development of deep learning, models\nsuch as Pangu-weather [5], GraphCast [21], FourCastNet\nv2 [7], FuXi [10] and ClimaX [27] have started to emerge\nin the field of meteorology. These methods replace the\nsolutions of ODE equations with a fully end-to-end learning\napproach, achieving better model parallelism and reducing\ninference costs. Also, methods like ClimODE [35] use\ndeep networks to construct ODE-type architectures, which\nsimilarly improve parallelism. However, in the S2S\ndomain, the performance of the above models has been\nsuboptimal. FuXi-S2S [11] has recognized this issue and\nemploys Variational Autoencoder (VAE [19])-like methods\nto enhance the model's accuracy in long-term predictions.\nHowever, it addresses the issue with a specialized architec-\ntural design, lacking a general perspective. In particular, its\ngenerative framework is difficult to replicate and sensitive\nto training nuances, making it challenging for future work\nto build upon.\nThe optimization of the rolling prediction model can be\napproximated as an optimization of either ultra-deep mod-\nels or ultra-long RNN models. In [2], an analysis was\nconducted on the convergence issues of deep linear mod-\nels and they derived the necessary conditions for conver-\ngence. The teacher forcing [16] method optimizes the train-\ning of RNN by controlling the proportion of gradient propa-\ngation. Building on these works, we continued to derive the\nissues related to the rolling prediction model and proposed\na multi-stage training approach to reduce the optimization\ndifficulty."}, {"title": "3. Methodology", "content": "As the starting point, our empirical studies suggest that\nrolling forecasting is more effective than direct forecasting\nfor S2S tasks, as illustrated in Figure 3, which is opposite\nto the conclusion from the recent study [26]. We believe it\nis due to the suboptimal optimization of previous weather\nforecasting models for long-term rolling prediction. Below\nwe will first provide a simple analysis that motivates the de-\nvelopment of a novel optimization strategy, and then present\nour multi-stage teacher forcing optimization framework in\ndetail."}, {"title": "3.1. Motivation for Better Optimization Strategy\nfor S2S Forecasting: a Simple Analysis", "content": "Despite its advantage over direct forecasting, rolling\nforecasting suffers its own problem: the prediction errors\nmade on each step can be accumulated over the time, lead-\ning to significant instability in training illustrated in Fig. 1.\nTo better understand the accumulation effect of the rolling\nprediction model, we formalize the rolling prediction model\nas an RNN (Recurrent Neural Network) given as follows\n[16]:\n$X_t = F_\\theta(X_{t-1}, S_t),$\n(1)\nwhere $X_t$ is the model prediction (or model state) at time t,\n$S_t$ is additional external inputs at time t, and $\\theta$ is the model\nparameter. Let $l(X_t, \\hat{X}_t)$ be the loss function compar-\ning ground truth observations $X_t$ and model prediction $\\hat{X}_t$.\nThen the overall loss function is $L(\\theta) = \\sum_{t=1}^T l(X_t, \\hat{X}_t)$.\nDefine the Jacobian of $F_\\theta(X_{t\u22121}, s_t)$ w.r.t. $X_{t-1}$:\n$J_t := \\frac{\\partial \\hat{X}_t}{\\partial X_{t-1}} = \\frac{\\partial F_\\theta(X_{t-1}, S_t)}{\\partial X_{t-1}}$\n(2)\nFollowing the idea of Backpropagation Through Time\n(BPTT) [32, 39], we can write the gradient $V_\\theta L(\\theta)$ as\n$V_\\theta L(\\theta) = \\sum_{t=1}^T V_\\theta l(X_t, \\hat{X}_t)$\n$= \\sum_{t=1}^T \\frac{\\partial l(X_t, \\hat{X}_t)}{\\partial F_\\theta(X_{t-1}, S_t)} \\frac{\\partial F_\\theta(X_{t-1}, S_t)}{\\partial \\theta} + V_{X_{t-1}}J_t$\n$= \\sum_{t=1}^T \\frac{\\partial l(X_t, \\hat{X}_t)}{\\partial \\hat{X}_t} \\frac{\\partial F_\\theta(X_{t-1}, S_t)}{\\partial \\theta} + V_{X_{t-1}}\\Big[\\sum_{j=1}^{t} J_j \\Big]$\n$= \\sum_{t=1}^T \\Big[\\sum_{j=1}^{t} \\frac{\\partial l(X_t, \\hat{X}_t)}{\\partial \\hat{X}_t} \\frac{\\partial F_\\theta(X_{t-1}, S_t)}{\\partial \\theta} \\prod_{k=j+1}^{t} J_k \\Big]$\n$= \\sum_{t=1}^T \\frac{\\partial l(X_t, \\hat{X}_t)}{\\partial \\theta} \\frac{\\partial F_\\theta(X_{t-1}, S_t)}{\\partial \\theta} \\Big[\\prod_{j=t}^{T} J_{k=j+1} \\Big]$\nAs indicated by the above expression, any local adjust-\nment of parameter $\\theta$, i.e., $\\frac{\\partial F_\\theta(X_{t-1}, S_t)}{\\partial \\theta}$, can be ampli-\nfied through the product of Jacobian matrices $\\prod_{k=j+1}^{T} J_k$,\nclear evidence of accumulation effect. Hence, the training"}, {"title": "3.2. A Multi-Stage Teacher Forcing Framework for\nOptimization", "content": "First, we decompose the task of S2S forecasting into\nmultiple steps. To develop a long-term rolling daily average\nforecasting model, it is necessary to fit the model as a daily\naverage forecasting model, then a rolling prediction model,\nand lastly a long-term rolling model. This corresponds to\nthe progression of T from 1 to 7 and then to 42.\nIn the first stage, we fine-tune the hourly forecast pre-"}, {"title": "4. Experiment", "content": "In the experiments, we used ERA5 [15] as the observa-"}, {"title": "4.1. Settings", "content": ""}, {"title": "4.1.1 Model Settings", "content": ""}, {"title": "4.1.2 Evaluation Metrics", "content": "In the experiments, we evaluate the PCC (Pearson Corre-\nlation Coefficient) and TCC (Temporal Correlation Coeffi-\ncient) metrics. The calculation of PCC is\n$PCC(\\tau) = \\frac{1}{B} \\frac{\\sum w(\\phi_i)(A_i * \\hat{A}_i)}{\\sqrt{(\\sum_i w(\\phi_i) A_i^2) * (\\sum_i w(\\phi_i) \\hat{A}_i^2)}}$\n(6)\nHere, B denotes the size of the test set, and $\\tau$ represents the\npredicted lead time. The calculation of TCC is\n$TCC(\\tau) = \\frac{1}{N} \\sum_{i}^N w(\\phi_i) \\frac{\\sum_{t}(A_{i, t} * \\hat{A}_{i, t})}{\\sqrt{(\\sum_t A_{i, t}^2) * (\\sum_t (\\hat{A}_{i, t})^2)}}$\n(7)\nThe two metrics evaluate the similarity of results from\nboth temporal and spatial perspectives. Notably, the cal-\nculation of outliers is influenced by the climatology C. In\nour experiments, we used a highly accurate climatology ap-\nproach by calculating the average values for each day of\nthe year across all years, and we applied a rolling average\ncentered on the current day, encompassing the preceding\nand following 5 days in total. For the climatology of the\nmodel predictions and ERA5 data, we utilized the calcula-\ntions based on ERA5 data and ECMWF employs its own\nclimatology. Using a longer average or fixed time period\ncan weaken the climatology signal, making the results ap-\npear inflated.\nIn the experiments, our model inputs and performs\nrolling predictions for ten variables: 2m temperature (t2m),\ngeopotential at 500hPa (z500), u component of wind at\n200hPa (u200), u component of wind at 850hPa (u850),\noutgoing longwave radiation (olr), temperature at 850hPa\n(t850), v component of wind at 200hPa (v200), v compo-\nnent of wind at 850hPa (v850), specific humidity 850hPa\n(sp850) and total precipitation (tp). The forecast starting\npoints include all days throughout the year. Considering the"}, {"title": "4.2. Comparison with ECMWF-S2S", "content": "ECMWF-S2S is currently recognized as the state-of-the-\nart forecasting model at the global S2S scale. We primarily\ncompare the PCC and TCC results over the time period of\n15 to 42 days, corresponding to the 3rd week to the 6th\nweek. In the Fig. 5, we report the results for t2m, z500,\nt850, olr, u200, and u850, which are important at the S2S\nscale. It can be seen that our results show a significant im-\nprovement compared to the ECMWF at the S2S scale. Over\nthe entire 2 to 6-week period, improvements remain steady,\nwith an average increase exceeding 35%. Notably signif-\nicant enhancements are observed in z500, t850, u200, and\nu850. At the same time, the results from ECMWF show\ninstability during long prediction times. Among them, our\nmethod exhibits a certain 7-day periodicity in the TCC for\nt2m and z500, which may be attributed to the six sets of\nadapters, with each set accounting for a 7-day period, in\nthe third stage. Additionally, the visualization samples are\navailable in Appendix F, and our model trained with 5.625-\ndegree data outperforms ECMWF, as demonstrated in Ap-\npendix E."}, {"title": "4.3. MJO Forecast", "content": "The calculation of the real-time multivariate MJO\n(RMM) index [40] is based on the tropical signals of u200,\nu850, and OLR. To evaluate the MJO forecasting skill, we\nutilized the bivariate correlation coefficient (COR). The av-\nerage MJO COR results for 2015 are presented in Fig. 7. In\nthe field, a COR threshold of 0.5 is typically used to indicate\na skillful MJO forecast. Our model significantly enhances\nthe accurate forecast lead time from 23 days to 30 days,\nexceeding the performance of the ECMWF-S2S model."}, {"title": "5. Ablation Study", "content": "We conducted extensive ablation studies to validate the\neffectiveness of our approach. To optimize the budget, all\nexperiments in this section are conducted using the 5.625-\ndegree model. The reported PCC is the average PCC of 6\nvariables: t2m, z500, olr, t850, u200, and u850."}, {"title": "5.1. Comparison with Naive Method", "content": "The naive method directly fine-tunes the model for a 42-\nday rolling prediction without adding intermediate stages.\nThis method is the simplest approach for training a rolling\nprediction model. However, the 42 rolling iterations result\nin excessive depth in training, and the initial state is insuf-\nficient to guide the model towards convergence. As shown\nin Fig. 8, our method shows a significant improvement in\naverage PCC compared to the naive method."}, {"title": "5.2. Multi-stage Progressive Learning for Different\nBackbones", "content": "To validate our proposed Multi-stage progressive learn-\ning architecture, we conducted experiments on other back-\nbones. We tested two common models, UNet [31] and\nViT [13] with last stage full fine-tuning. The results are\nsummarized in Tab. 1. Detailed results can be found in Ap-\npendix D. The deeper model ViT results in a higher training\ndifficulty compared to the UNet, which leads to worse re-\nsults with the Naive method. Meanwhile, our method can\nbetter leverage the capabilities of a deeper model and result\nin better performance. Due to budget constraints, we did not\ntest all state-of-the-art weather forecasting models. How-\never, our versatile framework is expected to significantly\nenhance these models."}, {"title": "5.3. Progressive Improvement of Multi-stage Train-\ning", "content": "We validated the effectiveness of the multi-stage training\nthrough experiments. By comparing the results of the mod-\nels trained in the first, second, and third stages, as shown in\nFig. 9, we can observe that the multi-stage training progres-\nsively improves the metrics. Furthermore, after the second\nstage of training, the model gradually approaches ECMWF,\nand by the end of the third stage, it surpasses ECMWF."}, {"title": "5.4. Comparison with Last Stage Full Fine-tune", "content": "In the final stage, the model's rolling predictions extend\nfrom 7 to 42 iterations, eliminating the use of intermediate\nteacher forcing for training adjustments. At this stage, the\nmodel exhibits some rolling prediction capabilities; how-\never, its performance over longer time spans remains sub-\noptimal. To address this, we implemented a Parameter-\nEfficient Fine-Tuning (PEFT) method. As illustrated in\nFig. 10, utilizing the PEFT method in the final stage leads to\nenhanced results. The primary reason for this improvement\nis that by freezing most of the parameters, parameter oscil-\nlation is minimized, enabling the model to converge more\neffectively."}, {"title": "5.5. Scaling Law in S2S", "content": "We observe the scaling law in S2S tasks. We conducted\ncomparative experiments between the 5.625-degree model\nand the 1.40625-degree model, as shown in Fig. 11. For\nthe 5.625-degree model, the input global data is represented\nas a matrix of size [32, 64], while the input matrix for the\n1.40625-degree model is of size [128, 256]. A smaller grid"}, {"title": "6. Discussion and Conclusion", "content": "Our work significantly advances the field of subseasonal-\nto-seasonal (S2S) weather forecasting by demonstrating\nthat a multi-stage optimization approach can substantially\nenhance the prediction skill of deep learning methods,\nwhich were previously considered less effective than cli-\nmatology [26]. The proposed method improves key skill\nmetrics, outperforming SOTA ECMWF-S2S by 19-91%.\nThe study challenges traditional forecasting assumptions by\nshowing that our framework can markedly improve rolling\nforecast performance. It offers valuable insights and theo-\nretical analyses, elucidating the core issues in rolling fore-"}, {"title": "Appendix", "content": ""}, {"title": "A. Resource Open Sourcing", "content": "Our method and codes are open-sourced at https:\n//anonymous.4open.science/r/Baguan-S2S-\n23E7/. As part of the Baguan weather and climate model\nseries, we will have more forthcoming work and results to\nbe released. We also plan to make the model weights pub-\nlicly available."}, {"title": "B. Proof of Our Statement", "content": "Deep linear model refers to a prediction $g(x) =\n\\prod_{i=1}^{L} \\Theta_ix$, where $x \\in R^d$ and $O_i \\in R^{dxd},\\forall i \\in [L]$.\nThe goal is to find Oi, i \u2208 [L] such that $l(\\Theta_{1:L}) =$\n$E_{(x,y)~P} [|y - g(x)|^2]$ is minimized, where $y \\in R^{d_1}$.\nWhen $y = \\Phi x$ and x is sampled from a normal distri-\nbution $N(0, I)$. The objective function is simplified as\n$l(\\Theta_{1:L}) = |\\Phi \u2013 \\Theta_{1:L}|^2$. In this note, we consider a spe-\ncial case of deep linear model that has $\\Theta_i = \\Theta, i\\in [L]$. It\nfits in better with the rolling out method in our study where\nthe same model is used at every step of rolling out. We will\nshow that in order to achieve a fast convergence of deep\nlinear model with all $\\Theta_i$ being the same, the initial solu-\ntion $\\Theta(0)$ needs to have a small loss $l(\\Theta(0))$. In particular,\nthe deeper the linear model is, the smaller the initial loss\n$l(\\Theta(0))$ is. It helps explain why directly training a rolling\nout network with a long horizon can be unstable, and why a\ncurriculum learning type approach is preferred. Our analy-\nsis follows closely the work \u201cGradient descent with identity\ninitialization efficiently learns positive definite linear trans-\nformations by deep residual networks\u201d.\nFirst, it is easy to verify that Lemma 3 from the original\npaper remains unchanged while Lemma 2 should be modi-\nfied as follows due to our special setup that $\\Theta_i = \\Theta, i \\in [L]$\n$|\\nabla l(\\Theta)|^2 \\geq 4L^2l(\\Theta) (1 \u2013 \\sigma_{min} (\\Theta))$.\n(8)\nThis change will lead to the following change to inequality\nin (4) from the original work, i.e.,\n$l(t + 1) \\leq (1 \u2013 2\\eta L^2(1 \u2013 R(t))^L) l(t)$\nunder the condition\n$\\eta \\leq \\frac{1}{3Ld^5 max \\{(1 + R(t + 1))^2L, |\\Phi|^2\\}}$\n(9)\nwhere $l(t) = l(\\Theta^{(t)})$ and $R(t) = |\\Theta \u2013 I|^2$. In addition,\ndue to our special setup, the inequality (2) from the original\npaper will be modified as\n$R(t + 1) \\leq R(t) + \\eta L(1 + R(t)) \\sqrt{l(t)}$.\n(10)"}, {"title": "C. Implementation of 6 Sets of Adapters", "content": "We employed an adapter-based PEFT method as illus-\ntrated in Fig. 12. For each transformer block in the model,\nwe added an adapter for fine-tuning while keeping all the\noriginal parameters of the model frozen. Each adapter is\na small MLP network that consists of two fully connected\nlayers with an activation layer in between. The hidden di-\nmension used for the adapters is consistent with the hidden\ndimension of the transformer blocks, both set to 1024. In\nthe 42 iterations of rolling prediction, we utilized six sets of\nadapters. Specifically, during the first seven iterations, we\nused adapter set 1; from the eighth to the fourteenth itera-\ntion, we employed adapter set 2, and so on. During training,\nall adapters were trainable. The parameter count of a single\nadapter is low and the parameter count of each adapter set\nconstitutes 4% of the total model parameters, allowing for\nconstraints on the model training."}, {"title": "D. Ablation of Different Backbones", "content": "We compared the performance of our method and the\nnaive method using ViT and UNet backbones, as shown in\nFig. 13 and Fig. 14."}, {"title": "E. 5.625-degree Results", "content": "The model prediction results at 5.625 degrees compared\nwith ECMWF are shown in Fig. 15 and Fig. 16. The train-\ning process for the 5.625-degree model is identical to that of\nthe 1.40625-degree model described in the main text. The\nonly difference is that the training data has been switched\nto the 5.625-degree resolution. Our model can surpass the\nECMWF-S2S with a resolution of 1.5 degree, even at a res-\nolution of 5.625 degree."}, {"title": "F. Visualization", "content": "In the S2S task, tp, z500, and u200 are of significant\nvalue for predicting storms, rainfall, and temperature. We\nconducted global data visualization for these parameters.\nFrom the figure, it is evident that our prediction results have\na lower error compared to ECMWF."}, {"title": "G. Detailed Results of the Ablation Study", "content": "In the ablation experiment section of the main text, we\npresent the curves and results of the average PCC. Here,"}]}