{"title": "Generating Synthetic Datasets for Few-shot Prompt Tuning", "authors": ["Xu Guo", "Zilin Du", "Boyang Li", "Chunyan Miao"], "abstract": "A major limitation of prompt tuning is its dependence on large labeled training datasets. Under few-shot learning settings, prompt tuning lags far behind full-model fine-tuning, limiting its scope of application. In this paper, we leverage the powerful LLMs to synthesize task-specific labeled data for training the soft prompts. We first introduce a distribution-aligned weighted generator tuning (DawGen) method to encourage generating in-distribution data that aligns with the few-shot real data. Then, we train soft prompts on both synthetic and real datasets using a gradient surgery approach, which eliminates the conflicting gradients from different data sources. Experiments on seven sentence-pair classification datasets demonstrate the effectiveness of our proposed method for boosting prompt tuning in few-shot learning settings. Results on QQP, MRPC, and SICK datasets are even comparable to the performance of transfer learning from large real-world datasets, showing the promise of synthetic data as an alternative for enhancing soft prompt tuning.", "sections": [{"title": "Introduction", "content": "As Large Language Models (LLMs) increase in size, adapting them to downstream tasks by fine-tuning (FT) a separate copy for each task becomes unfeasible. Prompt Tuning (Lester et al., 2021) (PT) emerges as a solution to this challenge by freezing the LLM and instead training a set of soft prompts pre-pended to the input data in an end-to-end manner. Compared with other parameter-efficient learning methods such as adapter tuning (Houlsby et al., 2019) and LoRA (Hu et al., 2022), PT makes no changes to the model architecture and can be applied to a frozen model with a static computational graph, enabling fast and flexible deployment. On a wide range of downstream tasks, PT has shown comparable performance as FT (Lester et al., 2021; Liu et al., 2022). However, recent studies indicate that PT requires sufficient labeled training data to achieve competitive performance as FT, yet in few-shot settings, PT significantly underperforms FT (Gu et al., 2022; Guo et al., 2022). To boost PT in few-shot learning tasks, previous methods mainly focus on finding a better initialization for soft prompts (Gu et al., 2022; Guo et al., 2022). This is achieved by pre-training the soft prompts on a large-scale real-world corpus such as OpenWebText (Gokaslan & Cohen, 2019a) or similar source-domain datasets (Gu et al., 2022; Vu et al., 2022; Guo et al., 2022). However, these approaches bear a common limitation: the dependence on large real-world datasets. On the one hand, online text corpora often exhibit substantial domain discrepancies varying across different downstream tasks. On the other hand, source-domain datasets are often not readily available, particularly for low-resource and emerging domains. Recently, there has been a growing interest in generating training data with LLMs (Ye et al., 2022a; Meng et al., 2022; 2023; Yu et al., 2024). This paper extends this line of research to tackle the limitation of prompt tuning in few-shot learning settings, aiming to bypass the need for large-scale labeled training data. Specifically, we employ a source LLM to generate a synthetic training set for the task at hand, which can be treated as a medium for carrying the pre-learned knowledge from a source LLM, to train soft prompts for a target LLM to achieve enhanced few-shot learning performance."}, {"title": "Related Work", "content": "Few-shot Learning with Pre-trained Language Models\nFine-tuning pre-trained language models has been the standard practice in few-shot learning, where a language model and a task-specific head are tuned together for a given task (Zhang et al., 2021; Gao et al., 2021; Liu et al., 2022; Zhang et al., 2022). However, fine-tuning the entire model on a few training samples (e.g., 16 samples per class) often leads to overfitting. One possible remedy is to manually craft hard prompts, consisting of natural language instructions and demonstrations (Brown et al., 2020; Mishra et al., 2022) for LLMs to perform in-context learning without updating any model parameters. Nevertheless, their effectiveness greatly relies on the skills of the prompt engineer and the prompts they write.\nInstead of manually crafted hard prompts, prompt tuning (Lester et al., 2021; Zhang et al., 2022) learns soft prompt vectors from training data and PT on T5-large can outperform the results of manual prompting on GPT-3. Still, PT performance greatly depends on the availability of substantial training data (Gu et al., 2022; Guo et al., 2022). In few-shot settings, PT significantly underperforms FT. To address this limitation, researchers have studied utilizing online corpora to pre-train soft prompts under self-supervised learning objectives (Gu et al., 2022), such as next sentence prediction (Devlin et al., 2019), or applying\nLLMs as Task-specific Training Data Generators\nEarly efforts in generating synthetic data with language models were to augment the existing dataset with generic texts (Kumar et al., 2020; Puri et al., 2020; Anaby-Tavor et al., 2020). E.g., Puri et al. (2020) use GPT-2 to generate a synthetic corpus to substitute the Wikipedia corpus to improve the performance of QA tasks. These texts are not tailored to any downstream task and therefore can not be used to train task-specific models. As LLMs continue to grow in size, recent works have shifted towards creating a new paradigm for playing with LLMs, i.e., distilling task-specific training data directly from a frozen LLM to enhance downstream tasks (Schick & Sch\u00fctze, 2021b; Ye et al., 2022a; Meng et al., 2022; Gao et al., 2023; Meng et al., 2023; Yu et al., 2024). These synthetic datasets are then used to boost downstream models such as DistillBERT and RoBERTa (Meng et al., 2022; 2023).\nHowever, when prompted with a simple label-conditional natural language prompt, the LLM generator can easily forget label information when generating long sequences (Li et al., 2022; Zhong et al., 2024) and therefore may generate samples that are not associated with the given labels. Moreover, the LLM generator, when frozen for inference, tends to generate texts that follow its pertaining data distribution, which often exhibits a domain gap with the task at hand (Guo & Yu, 2022). This paper provides a distribution-aligned weighted generator tuning method to mitigate these issues.\nLearning with Synthetic Training Data\nThe existence of low-quality samples can be detrimental to model training. Synthetic datasets are often generated at scale, which is inevitable to contain low-quality data (Gao et al., 2023). Recent works on synthetic text generation with LLMs (Ye et al., 2022a; Meng et al., 2022; 2023) adopt various training strategies to exploit the synthetic training data. For example, ZeroGen (Ye et al., 2022a; Gao et al., 2022) employs a noise-robust loss function (Ghosh et al., 2017) for learning with synthetic training data. FewGen (Meng et al., 2023) adopts label smoothing and temporal ensembling (Laine & Aila, 2016) to degrade the confidence level of model prediction during training. ProGen (Ye et al., 2022b) incorporates a quality estimation module to select the synthetic dataset. These methods are primarily employed to work with synthetic data; however, in our paper, where we use a few real samples for supervision, they cannot address the disparity between real and synthetic data. Hence, we propose to use the gradient surgery method (Yu et al., 2020) to directly alter the conflicting gradients on the synthetic data, thereby enhancing the learning performance."}, {"title": "Synthesizing Training Data for Prompt Tuning", "content": "Preliminaries\nPrompt Tuning. Lester et al. (2021) converts all downstream tasks into a text-to-text generation format (Raffel et al., 2020) in order to reduce the gap between pre-training and downstream tasks. Taking sentence-pair classification as an example, given a labeled training example (X,y) \u2208 D,|D| = N, where X = [S1, S2] represents a sentence pair and y \u2208 Y denotes a class label. For example, in paraphrase detection, the label space y may include two labels, yes and no, indicating if two sentences are paraphrases. In order to fully utilize the pretrained model, we devise a task-specific natural language prompt H and a template T(\u00b7) that reformat the original task as a cloze-style task. T(X) = {H, X, [MASK]}. One example for paraphase detection is \u201cAre (S1) and (S2) equivalent? [MASK]\u201d. An LLM, fo, predicts the label y at the position of the [MASK] token.\nPrompt tuning is done by prepending n tokens with trainable prompt embeddings, P \u2208 Rnxd, to the template T. Throughout the training, \u03b8 remains unchanged and P is optimized by minimizing the cross-entropy loss on the training set D:\n$\\mathcal{L}_{c e}(P)=-\\mathbb{E}_{(X, y) \\in \\mathcal{D}} \\log P r_{\\theta}([\\text { MASK }]=y \\mid[P ; T(X)])$\nSynthetic Text Generation. Taking sentence-pair classification as an example. Given the label space y = {Y1}]=1, we compose a label-conditional prompt for each Y\u2081 using the template T(Y1) = {H, Y\u2081, X}, e.g., \u201cSentence 1 and sentence 2 are equivalent. Sentence 1: (51). Sentence 2: \". Then, an autoregressive LLM, 84, takes Tc(Y1) as the context and generates a subsequent sequence of tokens X1:K that maximize the joint likelihood:\n$\\prod_{j=1}^{K} P r_{\\phi}\\left(x_{j} \\mid x_{<j} ; T_{c}\\left(Y_{l}\\right)\\right)$.\nThe decoding process stops when the end-of-sentence token is predicted or the maximum sequence length, K, is reached. The generated sentence for S2 is expected to satisfy the relationship Y\u2081 with S\u2081. To encourage generating diverse Xsyn for the same label Y\u2081, we employ a stochastic decoding algorithm (e.g., top-k and nucleus sampling). The generated sentence pairs and the given label y\u012b establish a synthetic dataset Dsyn = {(Xsyn, Ysyn)}.\nDistribution-Aligned Weighted Generator Tuning\nFor every downstream task, we use the few-shot real dataset, Dreal, to perform domain adaptation for the generator 84 to improve the quality of the synthetic dataset Dsyn.\nParameter-Efficient Generator Tuning. Recent LLMs for text generation often have billions of parameters. As such, tuning the entire model q for domain adaptation is impractical. Parameter-efficient methods, such as prompt tuning (Lester et al., 2021) and prefix tuning (Li & Liang, 2021), arise as an alternative to full-model fine-tuning by pre-pending a few external prompt embeddings, Q, to the input (or every transformer layer's output as done by prefix tuning), and training Q on the domain-specific data while keeping & unchanged.\nFor a downstream task with L classes, we can train one soft prompt Q\u2081 \u2208 Rn\u00d7d for each class label Y\u2081. The training objective for tuning Q\u2081 in this setting is the standard language modeling objective. The generator parameters 4 are frozen and only the soft prompt Q1 is optimized on the real training set Dreal:\n$\\mathcal{L}_{g e n}\\left(Q_{l}\\right)=-\\frac{1}{\\left|\\mathcal{D}_{r e a l}\\right|} \\sum_{\\mathcal{X} \\in \\mathcal{D}_{r e a l}, y=Y_{l}} \\sum_{x_{j} \\in \\mathcal{X}} \\log P r_{\\phi}\\left(x_{j} \\mid x_{<j} ; Q_{l}\\right)$.\nWeighted Generator Tuning. The above language modeling objective treats all tokens equally. To encourage the data generator network to generate label-discriminative texts, FewGen (Meng et al., 2023) propose to train a weight net, \u00dew : Rd \u2192 R, which learns to assign higher weights to those generated tokens that are more likely to discriminate the ground-truth label Y\u2081 from other labels Y\u2081\u2081 \u2208 Y by minimizing the weighted generation loss,\n$\\mathcal{L}_{w G e n}\\left(Q_{l}\\right)=-\\mathbb{E}_{\\mathcal{X} \\in \\mathcal{D}_{r e a l}, Y=Y_{l}} \\sum_{x_{j} \\in \\mathcal{X}} W_{j} \\cdot \\log P r_{\\phi}\\left(x_{j} \\mid x_{<j} ; Q_{l}\\right)$.\nHere, Q and W are optimized under the bi-level optimization framework. That is, first optimizing the loss LwGen produces a function Q of W: Q(W), which is then applied a second time to optimize the weight net, W, by minimizing the following loss:\n$\\mathcal{L}_{d i s c}(W)=-\\mathbb{E}_{x_{j} \\in X} \\frac{P r_{\\phi}(x_{j} \\mid x_{<j} ; Q_{l}(W))}{\\sum_{l^{\\prime} \\neq l} P r_{\\phi}(x_{j} \\mid x_{<j} ; Q_{l^{\\prime}}(W))}$.\nBy optimizing Q and W iteratively, the generator 84,0 learns to generate tokens that are more related to the given label than other labels.\nDistribution-Aligned Regularization. However, given the few-shot training set, it is easy for the weight net to overfit shortcut tokens, which are not robust tokens for the given task. For example, the token \u201cnot\u201d can discriminate the positive sentence \u201cIt is a good movie\" from the negative sentence \u201cIt is not a good movie\u201d, but is obviously not a generalizable label-discriminative token. Hence, enforcing the generator to solely rely on the weights may lead to generating sentences that are irrelevant to the given label. We propose to regularize the generator tuning objective by adding a sentence-level distribution constraint to encourage the generated sentence to align with the in-distribution data:\n$\\mathcal{L}_{D a w G e n}(Q)=\\mathbb{E}_{l \\in[1, L]} \\mathcal{L}_{w G e n}\\left(Q_{l}\\right)+\\mathcal{L}_{d i s t}(Q)$,\nwhere\n$\\mathcal{L}_{d i s t}(Q)=\\mathbb{E}_{(\\mathcal{X}, y) \\in \\mathcal{D}_{r e a l}} \\max \\left(0,1-D\\left(W \\cdot Z_{i, 1}, W \\cdot Z_{j, 1}\\right)+D\\left(W \\cdot Z_{i, 1}, W \\cdot Z_{j, 1^{\\prime}}\\right)\\right)$.\nHere, W \u2208 R1\u00d7K indicates the weights for a sequence of K tokens. Zi,1 = 8\u00a2,Q(Xi), Zi,1 \u2208 RK\u00d7d, denotes the last-layer hidden states output from the generator gp,Q(\u00b7) for i-th instance X\u00a1 of class Y\u2081, and Zj, represents that from a different class Y\u2081\u2081. D(\u00b7,\u00b7) measures the cosine similarity between two vectors. By minimizing Ldist(Q), we encourage the generated texts to stay close to the ones in the same class while being pulled away from those that belong to other classes. We present the whole procedures in Algorithm 1.\nTraining Soft Prompts with Synthetic Data Augmentation\nDespite the domain adaptation procedure employed, the resulting synthetic training set can inevitably contain low-quality data. Training soft prompts with a naive combination of synthetic and few-shot real data can result in the optimization process being dominated by gradients from the synthetic data. Hence, we up-sample the few-shot data by pairing each batch of synthetic data with a corresponding batch from the few-shot real data. Then, we employ gradient surgery to these paired batches to resolve conflicting gradients from different data sources in prompt tuning.\nGradient Surgery. It was first proposed to de-conflict gradients in a multi-task learning setting, where a model e is trained on a set of M tasks (Yu et al., 2020). Let d\u2081 = \\frac{\\partial \\mathcal{L}_{i}(\\theta)}{\\partial \\theta} denote the gradients of i-th task loss L\u2081(0) with respect to the model \u03b8. \u2200\u03b4; \u2208 {d;}11, \u03b4; is iteratively altered across all the other tasks by subtracting the component \\frac{\\delta_{i} \\cdot \\delta_{j}}{\\left|\\delta_{j}\\right|^{2}} \\delta_{j}, which is its projection to the plane of j-th task' gradient dj, where j \u2260 i. This step is applied when \u03b4\u00a1\u00b7 \u03b4j < 0, which indicates the two tasks have interference in driving the optimization path.\nIn this paper, we treat the gradients from real data, dreal, as the positive gradients for the task and always project the gradients of the synthetic data, dsyn, to the direction of dreal:\n$\text { Proj }_{\\delta_{\\text {real }}}\\left(\\delta_{\\text {syn }}\\right)=\\frac{\\delta_{\\text {syn }} \\cdot \\delta_{\\text {real }}}{\\delta_{\\text {real }}} \\frac{\\delta_{\\text {real }}}{\\left|\\delta_{\\text {real }}\\right|}=\\frac{\\delta_{\\text {syn }} \\cdot \\delta_{\\text {real }}}{\\left|\\delta_{\\text {real }}\\right|} \\frac{\\delta_{\\text {real }}}{\\left|\\delta_{\\text {real }}\\right|^{\\prime}}$\nwhere real is the normal plane of the gradients dreal from real data, and $\\frac{\\delta_{\\text {syn }} \\cdot \\delta_{\\text {real }}}{\\left|\\delta_{\\text {real }}\\right|}$ is the magnitude of the projection of dsyn onto this normal plane. If dsyn dreal < 0, then the projected gradients will be dropped and the gradients of synthetic data will be modified as:\nd'syn = dsyn - Projdreal (dsyn).\nBy removing the conflicting gradients of the synthetic data, we train soft prompts using the loss function in Equation 1 and update the weights with a gradient descent approach:\nP \u2190 P \u2212 \u03b7(\u03b4real + \u0454 \u00b7 \u03b4syn),\nwhere \u03b7 is the learning rate and e is a factor for controlling the strength of synthetic knowledge guidance, which was studied in a similar work in computer vision (Zhu et al., 2023). The whole algorithm is presented in Algorithm 2."}, {"title": "Experiments", "content": "Datasets, Metrics, and Settings\nWe conduct evaluations on seven sentence-pair classification datasets in two tasks. In the paraphrase detection task, we use MRPC (Dolan & Brockett, 2005) and QQP. In the natural language inference task, we use MNLI (Williams et al., 2018), SNLI (Bowman et al., 2015), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2005), and SICK (Marelli et al., 2014). We follow LM-BFF (Gao et al., 2021) to prepare the few-shot learning setting: both Dtrain and Ddev contain 16 samples per class, which are sampled from the original training set using 5 random seeds, and the original development set is used as the test set. We adopt Accuracy for all the classification tasks and report the average test accuracy over 5 seeds. We compare methods using the average performance across the seven datasets. More details about the datasets, models, and training settings can be found in the Appendix.\nBaselines\nWe consider the following zero-shot and few-shot baseline methods. We also compared with two transfer learning methods, SPOT (Vu et al., 2022) and OPTIMA (Guo et al., 2022), where a large-scale real-world source-domain dataset is available.\n(Zero-shot) Prompting. We prompt the frozen T5-large and Flan-T5-large with only task-specific natural language prompts (i.e., hard prompts) and treat it as the zero-shot learning baseline. For fair comparisons, we apply the same hard prompts for all baselines where applicable. Prompt-based templates are presented in the Appendix.\nIn-context Learning (ICL). Following GPT-3 (Brown et al., 2020), we incorporate the acquired few-shot examples as demonstrations in the hard prompt templates, which is also called few-shot prompting (in contrast to zero-shot prompting). The role of few-shot examples helps the frozen T5-large and Flan-T5-large models better understand the task by exemplifying the task instruction with real-world examples. The order of few-shot samples is randomly determined and we report the average performance across five runs.\nFull-model Fine-tuning (FT). We feed the few-shot data without any hard prompts into T5-large and Flan-T5-large and fine-tune the entire networks. Different from traditional fine-tuning that trains an additional classification layer from scratch, here, we apply the label verbalizer and tune the language modeling head instead.\nFew-shot learning performance\nOur main results are presented in Table 1. Overall, compared with the naive prompt tuning method, our approach yields an average improvement of approximately 18% across all datasets when applied to T5-large, and about 15% for Flan-T5-large. In particular, PT (using 102K parameters) under our framework outperforms FT (using 770M parameters) by an average improvement of 3.8% across all datasets based on T5-large. When compared to SPOT and OPTIMA, which use large real-world datasets for transfer learning, our approach exhibits competitive performance on QQP, MRPC, and SICK, though its performance on\nThe order in which synthetic and real data appears does matter\nPrevious research (Vu et al., 2022; Gu et al., 2022; Guo et al., 2022) indicates that data-driven initialization significantly improves PT. Here, we study how the order in which synthetic data and real data appear affects PT. We experimented with several strategies: Real+Syn directly combines the synthetic and few-shot real data as a new training set and performs shuffling before mini-batch training. Real \u2192 Syn trains soft prompt first on Dreal and then on Dsyn in every training epoch. Syn \u2192 Real, on the contrary, trains soft prompt first on Dsyn and then on Dreal in every training epoch. (Real, Syn) means pairing every batch sampled from Dsyn with a batch sampled from Dreal and combining them as a new batch to train soft prompts, which is employed in our approach. Results are presented in Table 2.\nWe observed that a naive combination of Syn + Real data performs the worst, where the few-shot real data could be overwhelmed by the larger synthetic data. Simply applying a label smoothing regularization, denoted as Syn + Real + LS, generally does not help. In contrast, (Real, Syn), which up-samples the few-shot real data when paired with either FewGen or DawGen data, confers an obvious improvement for both T5-large and Flan-T5-large. We also observed an interesting phenomenon, where T5-large prefers Real \u2192 Syn while Flan-T5-large prefers Syn \u2192 Real. This may suggest that training a model initially on the few-shot real data is not always an advantage and the order in which real and synthetic data are presented impacts differently for different LLM backbones.\nAblation study\nWe evaluate every component in our approach and present the results in Table 3. Specifically, we observe that: 1) the distribution-aligned regularization term Ldist is effective - Comparing DawGen against FewGen, the soft prompts trained exclusively on DawGen results in an average improvement of 4% for T5-large and 1.6% for Flan-T5-large across all datasets compared to the one trained on FewGen; 2) utilizing a few real samples to augment synthetic datasets is beneficial, and this benefit is more pronounced on FewGen than DawGen, indicating that the higher the quality of a synthetic dataset, the less it requires supplementation with real data supervision. 3) the gradient surgery technique effectively mitigates the conflict between synthetic and real data sources - applying gradient surgery further improves the performance by an average of approximately 2% across the datasets.\nInstruction-tuned models are better few-shot learners\nRecent works (Varia et al., 2023; Aly et al., 2023) suggested that the advantage of instruction tuning (Ouyang et al., 2022; Chung et al., 2022) for language models can extend to few-shot learning settings. This is further corroborated in our experiments. Across the tables, Flan-T5-large excels T5-large on all experiments by a large margin. In this paper, we provide a few new findings to this avenue: 1) instruction-tuned models can follow soft prompts better, as shown by PFT + soft prompt versus PFT, where an average improvement of 2% is spotted on Flan-T5-large while no increase is observed on T5-large. 2) Prompt Tuning on top of instruction-tuned models are less prone to overfitting the few-shot data. It is shown that Flan-T5-large elevates the PT performance of T5-large by around 20% on average. 3) instruction-tuned models prefer using synthetic data to warm up the learning, as shown in Table 2. Moreover, when using Flan-T5-large as the backbone, Syn \u2192 Real strategy leads to an average improvement of about 3 ~ 4%, as shown by comparing either FewGen or DawGen from Table 3 with Syn \u2192 Real from Table 2. While synthetic data is often used as a form of regularization for learning on real datasets, in our study, presenting a few real samples after training on synthetic data produces a regularization effect. We suspect that the instruction-following capability of Flan-T5-large may play an important role."}, {"title": "Conclusion", "content": "This paper presents a framework for generating synthetic training data with LLMs to boost prompt tuning in few-shot settings. We introduce Distribution-Aligned Weighted GENerator"}, {"title": "Limitations and Discussions", "content": "Currently, there are a few limitations of this study that may limit the impact scope of the insights conveyed by this paper.\nThe gap between the evaluation metric and the quality of the synthetic data. Specifically, the downstream few-shot learning performance of prompt tuning, which is often measured by accuracy, may only reflect the preferences of the model rather than aligning with human judgment in terms of the quality of the synthetic data. There should be intuitive methods to explain to humans why one synthetic sample is superior to another.\nThe few-shot learning performance on these public benchmarks reported in this study does not stand for the state-of-the-art. They only reflect the performance of prompt tuning. More advanced parameter-efficient learning methods like LoRA could bring higher performance than prompt tuning. Nevertheless, the relevant improvements in this study can still support the effectiveness of a specific strategy.\nSynthetic data generation cost can be a concern. Current data generators rely on large language models, which have a deep stack of transformer layers. Feedforward computations and autoregressive generations involve non-trivial GPU, memory, and time costs. A few research efforts, such as FastGen Ge et al. (2024), have been devoted to accelerating LLM inference costs."}, {"title": "Future Directions", "content": "In Table 4, we compare the modern augmentation paradigm, which leverages Generative AI, specifically LLMs, with the traditional paradigm that relies on transfer learning. Together with the discussions in the Limitation section, we propose the following directions: 1) employ explainable approaches to highlight the influential elements in the synthetic data and then devise quantitative measures to assess the data quality; and 2) develop inference acceleration algorithms tailored specifically for batch generation."}]}