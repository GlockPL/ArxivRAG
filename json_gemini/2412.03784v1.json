{"title": "SPEECH RECOGNITION-BASED FEATURE EXTRACTION FOR ENHANCED AUTOMATIC\nSEVERITY CLASSIFICATION IN DYSARTHRIC SPEECH", "authors": ["Yerin Choi", "Jeehyun Lee", "Myoung-Wan Koo"], "abstract": "Due to the subjective nature of current clinical evaluation, the\nneed for automatic severity evaluation in dysarthric speech\nhas emerged. DNN models outperform ML models but lack\nuser-friendly explainability. ML models offer explainable re-\nsults at a feature level, but their performance is comparatively\nlower. Current ML models extract various features from\nraw waveforms to predict severity. However, existing meth-\nods do not encompass all dysarthric features used in clinical\nevaluation. To address this gap, we propose a feature extrac-\ntion method that minimizes information loss. We introduce\nan ASR transcription as a novel feature extraction source.\nWe finetune the ASR model for dysarthric speech, then use\nthis model to transcribe dysarthric speech and extract word\nsegment boundary information. It enables capturing finer\npronunciation and broader prosodic features. These features\ndemonstrated an improved severity prediction performance\nto existing features: balanced accuracy of 83.72%.", "sections": [{"title": "1. INTRODUCTION", "content": "Dysarthria, a speech-motor disorder leading to significant\nspeech intelligibility loss [1], can result from factors like\nstroke, tumors, Parkinson's Disease, and cerebral palsy. It\nimpacts physical and psychological well-being, reducing the\noverall quality of life [2]. The primary cause of dysarthria is\nstroke, affecting around 50% of stroke patients. Post-stroke\ndysarthria displays varied manifestations based on brain le-\nsion location and size, highlighting the need for precise\nevaluation and personalized speech therapy [3]. Current clin-\nical practice heavily relies on resource-intensive perceptual\nauditory assessments by healthcare experts [4]. Moreover,\nthe subjective nature of such evaluations raises reliability\nconcerns. Perceptual severity assessment relies on analyz-\ning characteristics in a patient's speech [5], necessitating\ncontinuous training for the involved experts.\nSpeech intelligibility is affected not only by the speaker's\ncommunication but also by the listener's comprehension [6].\nConsequently, there can be discrepancies in the evaluation of\nthe same patient depending on the evaluator [7]. Specifically,\nexperts such as doctors and speech pathologists, having ex-\ntensive exposure to dysarthric speech, tend to rate speech in-\ntelligibility higher compared to the non-expert group [8]. Due\nto these limitations in perceptual evaluation [9], the necessity\nfor automated assessment has been emphasized [10].\nWith the rise of DNN, CNN and several speech self-\nsupervised models automatically classified severity and\nshowed performance improvements [11, 12, 13]. However,\nthese models still lack the ability for real-world applica-\ntion due to their limited explainability. DNN models can\nonly provide spectrum-based explanations (e.g., Grad-CAM\n[14]), which is hard for patients and healthcare experts to\nunderstand. DNN models struggle to interpret aspects influ-\nencing severity based on the perceptual assessment criteria\ncommonly used in clinical practice. Another approach to\nautomatic severity classification is ML classifiers with pre-\ndefined features [15, 16]. These ML-based classifiers can\nprovide feature-level explanations about the result, which is\nmore understandable for users. However, their performance\nis lower than that of DNN models when accurate prediction\nis key in the medical domain. In this paper, we propose a\nfeature extraction method to enhance the performance of ML\nclassifiers. Our method leverages the ML classifier's explain-\nability and still obtains performance comparable to that of\nDNN models. Note that we do not propose a new ML classi-\nfier. Our focus is on the feature extraction method, applying\nfeatures used in perceptual evaluations.\nPrevious ML-classifiers exploit information such as voice\nquality, prosody, acoustic, glottal, and phonetic features\n[15, 17, 18, 19, 20]. These features are obtained through\nsignal-based calculation, often with toolkits like Praat [21].\nThese cover various aspects of acoustic features of dysarthric\nspeech. However, there are some unused aspects of speech\nin previous ML-classifiers that are used in clinical evaluation\n[22]. The conventional feature extraction directly uses raw\nwaveform as a source, resulting in a lack of semantic aspects.\nFor example, previous features only consider pause duration\nand the number of pause intervals, but in clinical evaluation,"}, {"title": "3. SPEECH RECOGNITION-BASED FEATURE\nEXTRACTION", "content": "We categorize speech recognition-based features (SR-features)\ninto Pronunciation Correctness and Structural Prosody. Ta-\nble 2 describes features we extracted from ASR transcription.\nWe use the OpenAI's Whisper [24]. To accurately diag-\nnose severity, we need an ASR model that can transcribe\nthe pronunciation errors as they appear in dysarthric speech.\nThe original Whisper tends to overestimate the pronunciation\nerrors in dysarthric speech [25]. Therefore, we finetune Whis-\nper with dysarthric speech to transcribe dysarthric speech pre-\ncisely. We refer to the finetuned Whisper as DysarthricWhis-\nper. Using DysarthricWhisper, we obtain ASR transcription\nand word segment boundaries. For word segmentation, we\nused whisper-timestamped\u00b3. It uses cross-attention weights\nto get the segmentation information from the ASR results.\nBased on dynamic time warping and heuristics, it aligns audio\nwith the inferred transcription. When fine-tuning Whisper,\nwe added a special token for pause detection, as described in\n[25]. In this way, we effectively transcribe and extract word\nsegment information from dysarthric speech."}, {"title": "3.1. Pronunciation Correctness", "content": "Pronunciation Correctness measures the pronunciation errors\nin dysarthric speech compared to the original reading para-\ngraph, Autumn paragraph. We calculate these features using\nASR transcription as hypothesis and original sentences in Au-\ntumn paragraph as reference. Pronunciation Correctness is\nfurther categorized into three subtypes.\nFirst, for Syntactic correctness, we calculate typical met-\nrics used in ASR performance evaluation, as listed in Table 2.\nSecond, Semantic correctness is evaluated with BERT score\n[26]. It measures cosine similarity between tokens in hy-\npothesis and reference sentence using contextual embeddings\nfrom the BERT [27]. BERT score strongly aligns with human\nassessments of ASR error types in disordered speech [28].\nWe utilize the KLUE-BERT [29]. Lastly, for Disfluency, we\ncalculate Max Repetition and Filler Words Similarity. Max"}, {"title": "3.2. Structural Prosody", "content": "We extract three types of features for Structural Prosody:\nPause, Articulation Duration, and Rhythm. As we added a\npause token to Whisper, we could obtain pause duration by\npredicting the timestamps of special tokens.\nPauses should appear at the right moment with the right\namount, and patients with dysarthric find it hard to do it.\nTherefore, we look into the location of pauses, as well as\nthe duration of pauses. In the case of the location of pauses,\nwe collaborated with speech-language pathologists to anno-\ntate pauses commonly occurring in typical sentences within\nthe Autumn paragraph. The annotated pause location serves\nas the reference in the feature derivation. Regarding loca-\ntion, we leverage ASR evaluation criteria again. We change\nASR transcription with pause tokens into a binary sequence\nwhere 0 is for a word, and 1 is for a pause token. We refer\nto this binary sequence as a pause sequence. We calculate\nWER, CER, etc., with pause sequences. Plus, we measure\nPause DTW and the number of pauses. Pause DTW measures\nDynamic Time Warping (DTW) distance to quantify the simi-\nlarity between pause locations in the reference and hypothesis\nsequences. We also count how many pauses appeared. In the\ncase of duration, we measure various statistics of durations of\npauses that occurred in the input speech.\nNext, we consider Articulation Duration. For articula-\ntion, we gather the segment boundaries of words from the\npredicted timestamps by whisper-timestamped except for that\nof pause tokens. In this way, we can obtain the duration\nof articulations. From the duration sequence, we calculate\nDTW distance, WS (Word Segment) DTW. The reference du-\nration sequence is the average duration sequence of 360 sam-\nples read by healthy individuals. Also, we measure statis-\ntics of durations within speech to capture variability in dura-\ntions among segments. To deal with various lengths of input"}, {"title": "4. EXPERIMENTS", "content": ""}, {"title": "4.1. Experimental Setup", "content": "We used the same corpus to train the ASR system and au-\ntomatic severity classification model. We split the Korean\ndysarthric speech dataset (Section 2) into the train, valida-\ntion, and test sets in the ratio of 8:1:1, considering the ratio of\neach severity level. The divided sets were maintained dur-\ning ASR system training and classification model training.\nThat being said, the samples in test sets have not been seen\nin training for both DysarthricWhisper and ML-classifier. We\nused TabularPredictor from AutoGluon-Tabular [31] for an\nML classifier. The same trainer was used for the baseline"}, {"title": "4.2. Results", "content": "Table 4 shows that the proposed feature extraction method-ology (SR-features) results in higher balanced accuracy com-pared to other feature extraction methods and deep learningmodels. In terms of accuracy, Whisper+Linear showed thehighest performance. However, as shown in Table 5, Whis-per+Linear scored accuracy for severity 2 the lowest. Hence,balanced accuracy was lower compared to accuracy. As amatter of fact, balanced accuracy was higher in ML clas-sifiers than in DNN models. DNN models were biased to\nBalanced Accuracy = (Sensitivity + Specificity)/2\nSensitivity = TP/(TP + FN), Specificity = TN/(TN + FP)"}, {"title": "4.3. Ablation Study", "content": "We dissect the effect of the proposed method by training\nML-classifiers with each main category in Table 2. We used\nthe same AutoML classifier described in Section 4.1. Table\n6 shows the classification performance according to each\nfeature category: Structural Prosody and Pronunciation Cor-rectness. Plus, Table 7 shows a detailed evaluation with"}, {"title": "4.4. Integration of feature sets", "content": "In this additional experiment, we look into the impact of\nthe number of features and feature coverage. Since the SR-\nfeatures set has more features than the ML baseline, the im-\nproved performance could stem from the number of input fea-\ntures. Furthermore, SR-features only over pronunciation and\nstructural prosody, while Waveform features cover broader as-\npects of speech. Therefore, we compare the proposed method\nwith the integrated feature set. The integrated feature set con-\ntains all features in Table 2 and Table 3. We compare three\ncombinations of features with the proposed method. First, we\nsimply combine two feature sets. (Integerated-without-FS in\nTable 8) Then, we conducted auto feature selection7. We run\nfeature selection for each feature set respectively and then\nintegrate only the selected features, referred to as Integrated-after-FS in Table 8. Integarted-before-FS indicates the setintegrated all features first, then ran auto feature selection.We used the same AutoML classifier."}, {"title": "5. DISCUSSION", "content": "Our approach can offer feature-level analysis of the predicted\nseverity, providing insight into the patient's severity and\nspeech therapy planning. For example, as in Figure 1, our\nmethod could reveal the impact on severity through feature\nimportance analysis aligned with clinical indicators. More-\nover, our method can provide an easily comprehendible text-\nbased explanation of the proposed ASR-based features. We\ndemonstrated the potential for offering explanations through\nFigure 2. The illustration on the top [18] presents a compar-\nison of speech signals and fundamental frequencies during\nsustained vowel 'A' phonation. The left side corresponds to a\nhealthy speaker, while the right side represents a Parkinson's\nDisease patient. In contrast, bottom-side illustration is based\non ASR Transcription, highlighting character-level insertions,\ndeletions, and substitutions in speech utterances. This version\noffers a more accessible explanation for a general audience,\naiding their comprehension of these alterations in dysarthric\nspeech. It also emphasizes that patients, including those with\nParkinson's Disease, can use this information for training and\nimprovement. This visual guide helps individuals understand"}, {"title": "6. CONCLUSION", "content": "We propose the speech recognition-based feature extraction\nmethod for dysarthric speech's automatic severity classifica-\ntion. We employed an ML model, which has feature-level\ninterpretability and improved its performance. To this end,\nwe quantified the clinically utilized dysarthric features. We"}]}