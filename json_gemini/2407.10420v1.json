{"title": "Learning Rapid Turning, Aerial Reorientation, and Balancing using Manipulator as a Tail", "authors": ["Insung Yang", "Jemin Hwangbo"], "abstract": "In this research, we investigated the innovative use of a manipulator as a tail in quadruped robots to augment their physical capabilities. Previous studies have primarily focused on enhancing various abilities by attaching robotic tails that function solely as tails on quadruped robots. While these tails improve the performance of the robots, they come with several disadvantages, such as increased overall weight and higher costs. To mitigate these limitations, we propose the use of a 6-DoF manipulator as a tail, allowing it to serve both as a tail and as a manipulator. To control this highly complex robot, we developed a controller based on reinforcement learning for the robot equipped with the manipulator. Our experimental results demonstrate that robots equipped with a manipulator outperform those without a manipulator in tasks such as rapid turning, aerial reorientation, and balancing. These results indicate that the manipulator can improve the agility and stability of quadruped robots, similar to a tail, in addition to its manipulation capabilities.", "sections": [{"title": "I. INTRODUCTION", "content": "The tails of animals play crucial roles in enhancing their physical abilities, particularly in quadrupeds. For instance, cheetahs use their tails to change direction during high-speed chases, allowing them to catch prey more effectively [1]. Additionally, squirrels and lizards utilize their tails to adjust body orientation in mid-air, ensuring safe landings [2], [3]. Tails are also essential for balancing, swimming, hopping, courtship, and defense [4]. These versatile tails have garnered the attention of robotic engineers and have been established as solutions for addressing various challenges in the field of robotics.\nPrevious works have attached tails to quadruped robots to improve their stability and speed during turning and running [5]\u2013[8]. Some studies suggest that tails can aid the robot's attitude control in aerial regions [9]\u2013[12]. There have also been attempts to improve the overall stability of robots by incorporating tails [13], [14]. These studies demonstrate that tails can enhance robotic performance in various tasks. However, adding a tail increases the complexity of the robot, leading to issues such as higher costs and increased weight. Consequently, the use of tails in practical applications may not always be efficient.\nTo alleviate these drawbacks and create a highly efficient tail capable of executing various tasks, we employ a manip- ulator as a tail rather than designing a tail solely for that purpose. Huang et al. [15] are also exploring the use of a manipulator as a tail. By employing a manipulator as a tail, it can simultaneously perform the roles of a gripper and a tail, thereby achieving high efficiency. In recent years, numerous studies have explored the use of manipulators in quadruped robots [16]\u2013[19]. The majority of these studies have focused on manipulators performing manipulation tasks. However, we have identified the potential for manipulators when utilized as tails.\nAmong various control methods, reinforcement learning (RL) has recently emerged as one of the most popular approaches for legged robots [20]\u2013[22]. By using a deep re- inforcement learning-based controller, many previous studies have successfully enabled high-complexity robots to execute complex behaviors [23], [24]. Inspired by these studies, we employ deep reinforcement learning (DRL) to develop a controller.\nBeyond creating a robust controller, the primary consider- ation when incorporating a tail on a robot is whether its pres- ence meaningfully enhances the robot's overall performance. Our experiments demonstrate that a robot equipped with a manipulator outperforms one without a manipulator in tasks such as rapid turning, aerial reorientation, and balancing. Specifically, when a robot running at 4.5 m/s executed a 135\u00b0 turn, the distance pushed out by centrifugal force was reduced by two-thirds compared to a robot without a manipulator. Additionally, a robot with a manipulator was able to land safely from initial angles of 90\u00b0 to 120\u00b0 at heights of 1.5 to 2.25 meters, whereas a robot without a manipulator failed to land under the same conditions. Finally, when subjected to external forces, a robot with a manipulator exhibited a higher survival rate compared to one without a manipulator.\nOverall, this paper demonstrates that a manipulator can be an appropriate choice as a quadruped robot's tail, enhancing the capabilities of a quadruped robot. Our main contributions are as follows.\n1) We propose the utilization of a manipulator that func- tions as a tail.\n2) We outline a method for controlling a robot equipped with a tail using reinforcement learning.\n3) We demonstrate that a robot equipped with a manip- ulator exhibits improved performance in tasks such as rapid turning, aerial reorientation, and balancing."}, {"title": "II. METHOD", "content": "Our goal is to develop a controller for a manipulator- mounted quadruped robot that performs well on various tasks, including rapid turning, aerial reorientation for safe landing, and balancing against external force. We utilize deep reinforcement learning to train a neural network policy. An overview of this deep reinforcement learning process is illustrated in Figure 2. In our study, we employ Proximal Policy Optimization (PPO), a deep reinforcement learning algorithm, to update the policy and utilize two distinct neural network architectures. The first network, termed the \"actor,\" maps observations to actions and is structured as a Multi- Layer Perceptron (MLP) with hidden layers sized [512, 256, 128]. The second network, known as the \"critic,\" evaluates the current state using an MLP with hidden layers sized [512, 256, 128]. Both networks are essential for the algorithm's decision-making process and performance evaluation, facili- tating the implementation of PPO in complex environments.\nIn our simulation, we deploy this controller on the Minicheetah robot mounted with a WidowX250S manipu- lator. We use RAISIM [26] as the simulation environment.\nFor three different tasks, we employ slightly different learning algorithms. However, the overarching methodology remains largely consistent. This section outlines the general algorithm constant for all tasks, and the subsequent sections will provide a detailed description of the specific adaptations made for each task.\nAs depicted in Figure 2, the observation is divided into two distinct parts: the robot state and the command. The robot state comprises the joint state, base state, and joint history. We provide the joint velocity ($\\dot{p}_t$) and joint position ($p_t$) for the joint state. For the base state, we include the base angular velocity ($\\omega_{x,y,z}$), base linear velocity ($v_{x,y,z}$), and base orientation ($\\phi_{x,z}$). Additionally, we provide the previous joint position history for the two preceding time steps ($p_{t-1}$, $p_{t-2}$). This type of observation remain constant across all tasks.\nUnlike the robot state, the observation of commands($o_{cmd}$) varies according to the task. For the task of rapid turn- ing, we provide the command velocity and command yaw: $o_{cmd} = (v_x, v_y, \\dot{\\psi}_{cmd})$. For the balancing task, we use the"}, {"title": "B. Base", "content": "Designing a controller capable of executing rapid turns during high-speed running presents significant challenges. Frequently, the robot becomes trapped in a local minimum, where it remains stationary before executing the turn com- mand and only moves after the command. This phenomenon occurs because the robot encounters substantial difficulty in performing rapid turns while in motion, leading it to abandon the attempt and execute the turn from a stationary position. Although various methods can successfully overcome this problem, such as gradually increasing the turning angle and velocity through curriculum learning, we have found that"}, {"title": "C. Rapid Turning", "content": "To enhance the stability of the learning process, we incre- mentally increase the command velocity using a curriculum learning approach, which has been shown to improve the stability of learning [27].\n$V_{cmd} = 1.0 + \\frac{1.5}{1.0+ e^{-0.008\\cdot(\\text{iteration}-500)}}$ (2)\nStage2 (Rapid turning during running): Stage2 is the main stage of rapid turning. In stage 2, the robot is trained to rotate in a fully moving state. The concept is shown at Figure 2.\nWe also use curriculum learning for command velocity in this stage. However, we discover that a straightforward curriculum learning approach by iteration, such as that used in Stage 1, is insufficient. This is mainly because Stage 2 presents greater instability in the learning process compared to Stage 1, due to the requirement for rapid rotation at high speeds. To improve stability, we implement a reward-based curriculum learning strategy [28]. Instead of merely updating the command velocity based on iteration count alone, we adjust the command velocity if the total reward exceeds a predefined threshold value.\n$V_{cmd} = 1.77 + \\frac{2.73}{1.0+ e^{-0.01\\cdot(\\text{reward step}-100)}}$ (3)\nIn this experiment, the predefined threshold value is set to 4.75. This means that reward step is increased by 1 if the total average reward of the iteration exceeds 4.75.\nIt is essential to determine the proper timing for executing turn commands during the learning phase. If the commands are issued randomly from the beginning of the learning phase, the likelihood of successful learning is reduced be- cause of the continuous variation in the robot's foot position and posture during its run. One method to address this issue is to progressively extend the range of commands using a curriculum learning approach. We update the range of commands if the total reward exceeds a predefined threshold value, similar to the Stage 2 command velocity process.\nCommand Range $= 1 + min(300, \\text{reward step})$ (4)\nIn this experiment, the value of reward step is the same as the reward step used in the velocity command. However, it is possible to set different values."}, {"title": "D. Aerial Reorientation and Safe Landing", "content": "Unlike the rapid turning task, we found that the aerial reorientation task does not require curriculum learning or stage-wise learning. The key factors for these tasks are termination as constraint and the inertia of the tail.\nIn the task, we utilize not only the WidowX250S, which is used in the other tasks, but also the ViperX300S, which is heavier and longer. We observed that, due to the limited inertia of the tail, the WidowX250S does not perform well in aerial reorientation. Consequently, we"}, {"title": "E. Balancing", "content": "For the balancing part, the robot is trained to endure random external impulses at random timings and in random directions while walking at speeds ranging from 0.5 m/s to 3 m/s. The range of impulses is 50N\u00b7s ~ 100N. s. During training, we use the same reward setup as in the running section of the Rapid Turning task."}, {"title": "III. RESULT", "content": "We compared the turning performance of robots with and without a manipulator in simulations using the RAISIM environment. In the simulation tests, we set the forward velocity of the robots between 3, m/s and 4.5, m/s. We then issued commands for the robots to execute turns up to 135\u00b0 while moving forward. The quality of the turns was assessed based on three criteria: the speed of the turn, the sharpness of the turn, and the displacement during turning.\nOur findings indicate that the turning speed was nearly identical regardless of the presence of the manipulator. However, there was a notable difference in the sharpness of the turns. As depicted in Figure 3, the robot equipped with a manipulator made sharper turns compared to the robot without a manipulator. Furthermore, the trajectory of the robot with a manipulator was closer to the ideal trajectory than that of the robot without a manipulator. This means that the displacement during rotation was shorter when a manipulator was equipped. Specifically, during a 135\u00b0 turn, the robot without a manipulator was pushed out by up to 1.74 meters, while the robot with a manipulator was pushed out to a maximum of only 1.2 meters.\nFigure 4 shows the difference in trajectory with respect to velocity. In Figure 4a, the trajectory of the robot without a manipulator varies significantly with speed. As speed increases, the deviation from the ideal trajectory becomes more pronounced. In contrast, the trajectory of the robot with a manipulator, shown in Figure 4b, remains relatively consistent.\nUnlike the previous task, we used three different robots to evaluate performance: Minicheetah without a manipulator, Minicheetah equipped with a WidowX250S, and Minichee- tah equipped with a ViperX300S. Details of these three robots are provided in the Method section. In the simulation, we dropped these robots from heights ranging from 1.5m to 2.25m and angles ranging from 90\u00b0 to 120\u00b0.\nWe found that the robot without a manipulator failed in all these scenarios. Similarly, the robot equipped with a WidowX250S failed in all scenarios. However, the robot equipped with a ViperX300S succeeded in all situations.\nFigure 5 illustrates the orientation shift during aerial reorientation. Regardless of the initial conditions, the robot without a manipulator achieved a maximum rotation of 15\u00b0, the robot equipped with the WidowX250S reached up to 50\u00b0, and the robot equipped with the ViperX300S attained up to 65\u00b0. The graph for the robot equipped with the WidowX250S shows that its rotation speed decreases after 0.2 seconds. In contrast, the robot equipped with the ViperX300S maintains a consistent rotation speed while reorienting and culminates in a constant orientation angle phase after 0.3 seconds, which remains steady while falling.\nIn simulation, we randomly applied impulses to the robot while walking at 1 m/s, with forces in the x, y and z-axis directions, totaling up to 100 N.s. The x-axis aligns with the robot's forward direction, the y-axis corresponds to the robot's lateral direction, and the z-axis aligns with gravity. We only display the results for the impulses on the y- and"}, {"title": "A. Rapid turning", "content": "The results are illustrated in Figure 6. The survival rate of the robot equipped with a manipulator is 81.5%, while the survival rate of the robot without a manipulator is 71.5%. Notably, in almost all areas within the black line, where the robot has already experienced the learning session, the robot with a manipulator successfully survived. In contrast, the robot without a manipulator has relatively often failed in these regions."}, {"title": "IV. DISCUSSION", "content": "In the Results section, we evaluate the turning performance of the quadruped robot equipped with a manipulator based on three criteria: the speed of the turn, the sharpness of the turn, and the displacement during turning. Among these cri- teria, the quadruped robot with a manipulator demonstrates superior performance in terms of sharpness and displacement during the turn. This improved performance is attributed to the robot with the manipulator being able to withstand greater centrifugal force during the turn. Figure 4 strongly supports this observation. As velocity increases, the deviation between the ideal trajectory and the trajectory of the robot without the manipulator becomes more pronounced. Given that the mag- nitude of the centrifugal force increases proportionally with the square of the velocity, this result is expected. However, the trajectory of the robot equipped with the manipulator remains almost constant regardless of the velocity, indicating that the quadruped robot can better endure centrifugal forces with the assistance of the manipulator.\nThe mechanism by which the quadruped robot utilizes its manipulator to withstand centrifugal force is illustrated in Figure 7. It shows the movement of the quadruped robot with the manipulator when executing a 135\u00b0 turn while running at a speed of 4.5 m/s. During turning, the manipulator is positioned on the side opposite to the direction of the centrifugal force. By positioning itself in this manner, the manipulator generates a counter-torque that opposes the torque produced by the centrifugal force. Consequently, this counter-torque enables the robot to execute sharper turns."}, {"title": "A. Rapid turning", "content": "As illustrated in Figure 5, the rotation speed and the total rotation angle vary depending on the type of robot used. Robots equipped with manipulators rotate more rapidly and achieve a larger total rotation angle than a robot without a manipulator, although the extent of these differences varies depending on the specific manipulator used. The disparity in robot performance according to the presence of a manipu- lator is attributed to the additional inertia provided by the manipulator. Figure 8 depicts the overall process by which the robot utilizes its manipulator when falling upside-down. During the aerial reorientation phase shown in the figure, the robot rotates its manipulator in the opposite direction of the robot base rotation. This movement generates counter momentum, allowing the robot base to reorient its body in the air.\nThe rotational performance in the air also varies depending on the type of arm used. This is because the counter"}, {"title": "B. Aerial reorientation and safe landing", "content": "momentum generated by the manipulator relies on its inertia. Therefore, the robot equipped with the ViperX300S, which has a higher inertia than the WidowX250S, rotates faster and achieves a larger total rotation angle compared to the one equipped with the WidowX250S.\nIn Figure 5, after approximately 0.3 seconds, there is a region where the robot's orientation remains constant for the robot equipped with the ViperX300S. This region represents the phase where the robot adjusts its legs into a posture advantageous for a safe landing. This phase can also be seen in Figure 8, where, in the middle of the figure, the robot finishes reorienting its body and adjusts its legs. We found that securing sufficient time to adjust its legs is crucial for a safe landing. For example, the robot equipped with Wid- owX250S fails to establish this region due to its inadequate speed of rotation in the air, resulting in unsuccessful landing attempts."}, {"title": "C. Balancing", "content": "The experimental results show that the quadruped robot has a higher survival rate when external forces are ap- plied. Therefore, engineers can expect an enhancement in the robot's stability if the manipulator is installed on the quadruped robot. However, when the impulse value is less than 60Ns, no substantial differences are observed. Conse- quently, installing a manipulator solely for balance purposes would not be efficient, as situations involving an impulse greater than 60Ns are relatively infrequent."}, {"title": "V. CONCLUSIONS", "content": "In this study, we propose the integration of a 6-DoF manipulator as a multifunctional tail for quadruped robots to address the challenges posed by the complexity of traditional tails. Through simulation, we demonstrated that mounting a manipulator on a quadruped robot enhances its performance compared to a robot without one in tasks such as rapid turn- ing, aerial reorientation, and balancing. The results indicate that the manipulator can improve the agility and stability of the quadruped robot. We believe that our study broadens the roles of manipulators, enabling robotics engineers to utilize them for additional functionalities when integrated with quadruped robots.\nHowever, our research focuses solely on simulations, not real-world applications. Therefore, it remains to be proven that the manipulator can indeed improve the performance of quadruped robots in real-world scenarios. Additionally, we addressed only a few applications in which quadruped robots can be enhanced by using a manipulator as a tail. There are numerous other potential applications that future studies could explore."}]}