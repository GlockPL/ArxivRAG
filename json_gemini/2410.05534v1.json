{"title": "Optimizing Tensor Computation Graphs with Equality Saturation and Monte Carlo Tree Search", "authors": ["Jakob Hartmann", "Guoliang He", "Eiko Yoneki"], "abstract": "The real-world effectiveness of deep neural networks often depends on their latency, thereby necessitating optimization techniques that can reduce a model's inference time while preserving its performance. One popular approach is to sequentially rewrite the input computation graph into an equivalent but faster one by replacing individual subgraphs. This approach gives rise to the so-called phase-ordering problem in which the application of one rewrite rule can eliminate the possibility to apply an even better one later on. Recent work has shown that equality saturation, a technique from compiler optimization, can mitigate this issue by first building an intermediate representation (IR) that efficiently stores multiple optimized versions of the input program before extracting the best solution in a second step. In practice, however, memory constraints prevent the IR from capturing all optimized versions and thus reintroduce the phase-ordering problem in the construction phase. In this paper, we present a tensor graph rewriting approach that uses Monte Carlo tree search to build superior IRs by identifying the most promising rewrite rules. We also introduce a novel extraction algorithm that can provide fast and accurate runtime estimates of tensor programs represented in an IR. Our approach improves the inference speedup of neural networks by up to 11% compared to existing methods.", "sections": [{"title": "1 Introduction", "content": "Deep learning applications have achieved remarkable results in recent years in problem areas ranging from game playing [25] to computer vision [23] and natural language processing [21]. Many of these successes have been driven by an increase in model size, which has resulted in greater computational requirements and higher latencies. In order to use these models in practice, they undergo several optimizations steps before being deployed. One common high-level optimization technique is to transform the computation graph of the neural network into an equivalent but faster one.\nThe traditional approach used by deep learning frameworks like TensorFlow [1] and PyTorch [22] is to sequentially apply a set of rewrite rules, which replace individual subgraphs of the input program with optimized ones. These replacements are destructive, meaning that the original subgraphs are no longer represented in the tensor program after a rewrite rule has been applied. This phenomenon can give rise to the phase-ordering problem in which the application of one rewrite rule can eliminate the possibility to apply an even better one later on. Due to the large combinatorial search space it is often infeasible to determine the optimal ordering by brute force.\nRecent work [32] has used equality saturation, a technique from compiler optimization, to address the phase-ordering problem in tensor program optimizers. Equality saturation follows a two-step process: First, an intermediate representation called equality graph (e-graph) is constructed that efficiently stores multiple optimized versions of the input program. This step is purely additive since no information is removed from the IR. The construction phase is completed once the e-graph is saturated, i.e. the application of rewrite rules no longer adds any information to the e-graph, or when a time or memory limit has been reached. Then, in the second step, an extraction algorithm is used to obtain the optimal input program from the e-graph.\nEquality saturation solves the phase-ordering problem in situations where the e-graph can saturate and represent all possible versions of the input program. In practice, however, this rarely occurs because the e-graph tends to explode rapidly and reach a memory limit before it has saturated. In these cases, the quality of the e-graph and the final solution depend on which rewrite rules were applied before the memory limit was reached. Thus, the phase-ordering problem is reintroduced into the construction phase of equality saturation when the e-graph cannot saturate. To address this issue, we use Monte Carlo tree search (MCTS) to identify the most promising rewrite rules in the e-graph construction phase.\nA second issue that limits the effectiveness of equality saturation is the dependency on a good extraction algorithm. There are two"}, {"title": "2 Background", "content": "2.1 Term Rewriting Systems\nThe optimization problem of transforming the computation graph of a neural network into an equivalent but faster one can be described as a term rewriting system (TRS). Following the notation of Klop 1993 [19], a TRS is a tuple (\u2211, R) where \u2211 is an alphabet and R is a set of rewrite rules. Terms (or expressions) can be defined recursively using the variables, constants, and function symbols F given by the alphabet. The set of terms over the alphabet \u2211 is denoted as Ter(\u03a3). Each rewrite ruler: t \u2192 s reduces a term t from the alphabet into another term s. At each step in the optimization procedure, applying a rewrite rule r leads to a set of rewrites \u03c3(t) \u2192r \u03c3(\u03b5) for substitutions \u03c3. Substitutions describe a mapping Ter(2) \u2192 Ter(\u03a3) where \u03c3(F(t1, ..., tm)) = F(\u03c3(t1), ..., \u03c3(tm)). By sequentially performing n rewrites, to is reduced to tn.\nWe can say that the application of rewrite rule r at time step i \u2208 N gives rise to the phase-ordering problem if it eliminates the possibility to apply a more favourable rewrite rule r' at time step j\u2208 N* where j > i. The prototypical example that is often used to illustrate the destructive nature of traditional term rewriting systems is the expression (a*2)/2 [12, 30]. Assuming we have a set of rewrite rules R = {x * 2 \u2192 x < 1; (x * y)/z \u2192 x * (y/z);x/x \u2192 1(x \u2260 0); x*1 \u2192 x}, we can apply the strength-reduction operation x * 2 \u2192 x < 1 to replace the expensive multiplication instruction a * 2 with the cheaper bitshift instruction a < 1. However, this eliminates the future possibility of canceling out the fraction altogether to eventually arrive at the optimal solution a."}, {"title": "2.2 Equality Saturation", "content": "Motivated by the phase-ordering problem in traditional compilers, Tate et al., 2009 [28] proposed the equality saturation framework. Instead of destructively modifying the input program, equality saturation uses an e-graph as an IR to efficiently store multiple different versions of the input program. First, the e-graph is constructed by iteratively applying all rewrite rules before the optimal solution is extracted in a second step. An e-graph consists of equivalence classes (e-classes) and equivalence nodes (e-nodes) that are used to store congruence relations over different terms. An e-class is a set of e-nodes that represent equivalent terms. E-nodes are variables, constants, and function symbols from the underlying alphabet and can have an arbitrary number of children e-classes associated with them. Congruence relations are equivalence relations that are preserved by the application of rewrite rules. Two terms are congruent to each other if applying the same set of rewrite rules results in equivalent terms."}, {"title": "2.2.1 Construction", "content": "In the beginning, the e-graph is initialized with the input program. Each e-node represents one variable, constant, or function symbol from the alphabet. At the start, each e-class consists of exactly one e-node. Figure 1a shows the initial e-graph corresponding to the expression (a * 2)/2. The dashed rectangles represent e-classes, the solid circles represent e-nodes and the parent-child relationships are depicted by arrows. After initialization, the algorithm iterates over all rewrite rules and searches for the left-hand side of each rule in the e-graph. If the pattern is found, the e-nodes corresponding to the right-hand side are inserted and merged with the respective e-classes. Compared to the traditional approach, this process is purely additive and the left-hand side pattern will remain in the e-graph.\nFigure 1b illustrates this concept using the previous example. After applying the rewrite rule x * 2 \u2192 x < 1, the e-graph encodes two programs: (a * 2)/2 and (a < 1)/2. Thus, the rewrite has not destroyed any information and it is still possible to cancel out the fraction and obtain the optimal solution a in future iterations.\nThe construction phase is completed once the e-graph has saturated, i.e. when no rewrite rule can add any further information to the e-graph, or when a given time or memory limit has been reached. In the first case, the e-graph represents all possible versions of the input program based on the provided rule set. Thus, the phase-ordering problem is not an issue. In the latter case, however, the e-graph does not encode all possible versions and therefore the quality of the e-graph and the extracted solution depend on the rewrite rules that have been applied up to that point. In these situations the phase-ordering problem is reintroduced into the construction phase."}, {"title": "2.2.2 Extraction", "content": "After completing the first phase, the optimal solution needs to be extracted from the e-graph. This process relies on a cost model that can rank the encoded programs with regards to the optimization objective. For basic tasks like simplifying mathematical expressions, the AST size can be used. For more complex tasks like optimizing the computation graphs of neural networks, more sophisticated methods are needed. We will discuss our choice of cost model in Section 4.3. Based on this model, an ILP or greedy extractor can then be used to obtain the optimized program."}, {"title": "Integer Linear Programs", "content": "Following the notation from [32], the ILP for extracting the optimal tensor program from an e-graph can be formulated as:\nminimize \t$\\Sigma_{i} c_i x_i$ (1a)\nsubject to \t$x_i \u2208 {0, 1}$ (1b)\n\t$\\Sigma x_i = 1$ (1c)\ni\u2208e\n$\\forall i, \\forall m \u2208 h_i, x_i \\leq \\Sigma_{j\u2208e_m} x_j$ (1d)\n\t$\\forall i \u2208 l, x_i = 0$ (1e)\nWhere i is an e-node, ci its cost as determined by the cost model, and hi the set of children e-classes. m denotes an e-class and em the set of e-nodes of that e-class. The objective of the ILP is to extract a valid program with the lowest overall cost. Constraint 1b defines xi as a binary variable, which encodes whether the respective e-node is selected or not. Constraint 1c asserts that one e-node in the root e-class (m = 0) needs to be part of the final program. Constraint 1d ensures that if an e-node is selected, so are its children e-classes. And the final constraint le restricts the solution to all e-nodes that are not part of some blacklist 1. The latter is required to ensure that the extracted program is a directed acyclic graph."}, {"title": "Greedy Extractors", "content": "Greedy extractors as used in [30, 32] follow a bottom-up approach, starting at the e-graph's leaf nodes working their way up to the root node. For each e-class, they iterate over all e-nodes and call a cost function which calculates the sum of the e-node's operator cost (as determined by the cost function) and the costs of all children e-classes. Afterwards, the lowest cost together"}, {"title": "2.3 Monte Carlo Tree Search", "content": "MCTS is a model-based planning algorithm originally developed for the use in computer Go [5, 20, 29]. The main idea is to build a search tree by balancing the exploitation of states that have led to high rewards in the past with the exploration of new ones. Each node in the tree represents a state and each edge corresponds to an action. MTCS iteratively works through four steps (selection, expansion, simulation, update) to grow the search tree into the most promising areas of the search space. It terminates once a pre-defined time or iteration limit has been reached. At the end of the search, the best action is determined based on the root's child node with the highest visit count or highest average value. We will discuss how we use MCTS to construct the e-graph in Section 4.1."}, {"title": "3 Related Work", "content": "3.1 Equality Saturation\nAlthough the theoretical foundations for the equality saturation approach were laid by Tate et al. in 2009 [28], the practical applicability has long been hampered by the necessity to develop domain-specific implementations for each use-case. This gap was closed by Willsey et al., 2021 [30] with the e-graphs good (egg) library, which allows users to define their own alphabets and rewrite rules on top of a generic equality saturation framework. egg has been used in a variety of projects, for example, to optimize floating point expressions [30] and for numerical hardware design [6]."}, {"title": "3.2 Tensor Program Optimization", "content": "Rewriting computation graphs of neural networks requires a good set of rewrite rules. Traditionally, human experts hand-craft these rules by identifying non-optimal source graphs and match them with equivalent but optimized target graphs. TASO [16] automates this process by generating all possible substitution candidates up to a certain size and validating them against human provided operator specifications. Afterwards, it applies MetaFlow's [17] cost-based backtracking search to jointly optimize graph substitutions and data layouts. The cost model measures the runtime of individual operators on the underlying hardware and then calculates their sum to obtain the overall runtime estimate of the tensor program.\nEquality saturation. To address the phase-ordering problem of traditional graph rewriting approaches, Yang et al., 2021 [32] introduce a tensor program optimizer called TENSAT based on equality saturation. The authors build on TASO and replace its backtracking search with a two-step e-graph construction and extraction process. To benefit from all rewrite rules generated by TASO, Yang et al. extend the construction phase to support multi-pattern rewrite rules. Multi-pattern rewrite rules consist of two source patterns, both of which need to be present in the e-graph for the target patterns to be applied.\nAlthough TENSAT showed significant improvements over TASO in terms of optimization results and times, it has several shortcomings. First, all rewrite rules are applied sequentially during the construction phase, thereby leading to sub-optimal results if the e-graph hits a memory limit. Second, since the multi-pattern rewrite rules tend to rapidly explode the e-graph, the authors had to limit their application to one or two iterations, thus also restricting their effectiveness. Third, due to the unreliability of greedy extractors, TENSAT uses ILPs for the extraction step. This, however, restricts its application to smaller e-graphs. These three problems emphasise the significance of the phase-ordering problem in equality saturation as well as that of reliable greedy extractors and are addressed by our work.\nDeep Reinforcement Learning. He et al., 2023 [11] also build on TASO, but replace the cost-based backtracking search with deep reinforcement learning. Their model-free RL agent receives the encoded tensor program in form of a graph neural network (GNN) as input and sequentially decides which rewrite rule to apply next. The authors show that their approach X-RLflow outperforms TASO due to the agent's ability to trade-off short-term performance losses in favour of long-term runtime reductions. However, these improvements come at the cost of an extensive pre-training phase. Our approach on the other hand is planning-based and can optimize tensor programs wtihout prior training.\nML compilers. There are several other ML compilers that focus on different aspects of the optimization routine: Hidet [9] uses a task-based programming paradigm to embed the scheduling process into tensor programs. TACO [18] and SparseTIR [33] optimize compound tensor algebra expressions consisting of sparse and dense tensors. Ansor [35] employs a task scheduler, program sampler, and performance tuner to itereratively optimize graph partitions. TVM [4] is an end-to-end deep learning compiler that takes high-level representations of neural networks and maps them to low-level optimized code. ONNX Runtime [7] is an inference engine that enables interoperability between different machine learning frameworks and provides support for model optimizations such as quantization and model pruning. In comparison to these compilers, we focus on the high-level rewriting of the tensor computation graph."}, {"title": "3.3 Monte Carlo Tree Search", "content": "He et al., 2023 [12] also use MCTS to address the phase-ordering problem in equality saturation. Their approach MCTS-GEB (MCTS is a Good E-graph Builder) decides which rewrite rules to apply"}, {"title": "4 Methodology", "content": "In this section, we will introduce our tensor graph rewriting approach based on equality saturation and Monte Carlo tree search. We will start with the e-graph construction phase and explain how MCTS can mitigate the phase-ordering problem by predicting which rewrite rules to apply. We will then move on to the e-graph extraction phase, where we analyze the shortcomings of existing methods and introduce our own approach. Finally, we will give a brief overview of our open-source implementation."}, {"title": "4.1 E-Graph Construction", "content": "A high-level overview of the optimization procedure is shown in Algorithm 1. To start with, the e-graph is initialized with the input tensor program. In this e-graph, e-nodes correspond to tensors (e.g. weights, inputs) and tensor operations (e.g. ReLU, convolutions), e-classes represent equivalent tensors / tensor operations, and the parent-child relationships between e-nodes and e-classes correspond to the flow of tensors. After initialization, the e-graph is constructed sequentially by running MCTS, applying the best rewrite rule based on the results, and then repeating the process until the e-graph has either saturated or reached a memory limit. At the end, the best tensor program is extracted from the e-graph."}, {"title": "4.2 E-Graph Extraction", "content": "Most equality saturation-based applications perform a single extraction at the end of the e-graph construction phase. In our setting, however, we also need to perform multiple extractions during each simulation step to obtain the necessary reward signal for MCTS. Therefore, we cannot tolerate the long optimization times of ILPs and instead have to rely on greedy extractors as an alternative. In addition to being fast to obtain, we also require the extraction results to be an accurate reflection of the optimal tensor program represented in the e-graph to ensure that the search tree grows in the most promising areas of the search space.\nProblem. Tables 1 and 3 show that greedy extractors which use existing cost functions fail to do so and significantly overestimate the true graph runtimes. The only neural network architecture for which the prediction matches the ILP estimate is VGG-19 [26]. Wrong runtime estimates are not a problem by themselves as long as the relative ordering between different tensor programs represented in the e-graph is preserved. However, Yang et al., 2021 [32] have shown that this is not the case and that in some instances the extracted program can be even slower than the original one. This finding not only poses a problem because it conflicts with the optimization objective, but also challenges the fundamental idea of equality saturation whereby the original tensor program will always be encoded and should thus be recoverable from the e-graph.\nAnalysis. Comparing VGG-19 with the other model architectures shows that it has the only computation graph without multi-input nodes and shared subgraphs. To illustrate how these can give rise to inaccurate predictions, Figure 4 provides a simple example based on a skip connection. Figure 4a shows the computation graph of a residual block and Figure 4b the corresponding e-graph. Existing cost functions such as the ones used in egg and TENSAT, determine the cost of an e-node by adding its operator cost to the sum of all its children e-classes. If we use this approach to calculate the cost of the Add node in the example, we will sum over both Conv nodes. However, since the cost of the first convolution operation marked in red is already included in the second one, we would overestimate the true graph runtime. If several such blocks are then stacked on top of each other to form a residual network, the errors add up exponentially. An intuitive approach to solve this problem would be to store centrally which e-classes have already been counted and not consider them a second time. While this results in correct estimates for initial e-graphs where each e-class contains exactly one e-node (e.g. Figure 4b), it fails once the e-graph grows. Therefore, we need a more sophisticated approach to deal with cost explosions caused by multi-input nodes and shared subgraphs."}, {"title": "Solution", "content": "The main idea behind our proposed solution is to keep track of the constituent costs of each e-class and e-node to prevent counting shared subgraphs multiple times. The constituent costs can be seen as an e-class'/e-node's history indicating which e-classes have contributed to its current cost. Algorithm 2 shows the pseudoalgorithm for our improved cost function. To calculate the cost of an e-node, the function iterates over all children e-classes and considers three possible scenarios:\n(1) If the e-class is already included in the e-node's history (i.e. in the constituent costs of any of its children e-classes), it is ignored. In the example from Figure 4, the cost function would skip the Conv node marked in red if it had already iterated over the other Conv node.\n(2) If the e-class itself is not included in the e-node's history, but its constituent costs overlap with those of other children e-classes, only the non-overlapping ones are added to the e-node's cost. In our example, this scenario would occur if the cost function first iterates over the Conv node marked in red. Then, only the operator cost of the second Conv node would be added, but none of its constituent costs.\n(3) Else, the full cost of the child e-class is added to the e-node's cost.\nIn scenarios 2) and 3), the child e-class and all its non-overlapping constituent costs are added to the e-node's history. If, at the end, the e-node's final cost is lower than that of all other e-nodes in its e-class, the e-class' constituent costs are updated with the e-node's history. This ensures that the e-class' cost and constituent costs always correspond to the best e-node and allows the cost function to ignore subgraphs that have already been considered.\nResults. The last line in Tables 1 and 3 shows that our cost function enables greedy extractors to match the accuracy of ILPs for the initial e-graphs of all architectures except NasNet-A. NasNet-A [37] is a special type of model, as it was artificially generated using neural architecture search (NAS). NAS can produce nested structures which in rare circumstances result in overlapping constituent costs not being treated 100% correctly. An illustration of this problem, which was derived from the NasNet-A computation graph, is provided in Figure 9 in the supplementary material. Nevertheless, our runtime estimates for NasNet-A are orders of magnitude more accurate than ones produced by existing cost functions. It is important to note that although we have focused our attention on tensor programs, our cost function can improve the performance of greedy extractors for all programs with common subexpressions. In Section 5, we will analyse how the improved prediction accuracy affects downstream performance."}, {"title": "4.3 Implementation", "content": "We built our tensor program optimizer on top of MCTS-GEB, egg, TASO and TENSAT. A high-level overview of our open-source implementation is shown in Figure 3. The optimizer receives the original tensor program as input and initializes the e-graph. The single- and multi-pattern rewrite rules are provided by TASO. In each iteration, MCTS initializes the root node with the current e-graph and searches for the best rewrite rule to apply. The reward signal during the simulation phase is obtained by extracting the best tensor program from the node's e-graph and calculating its runtime.\nThe extraction process relies on TASO as the cost model. TASO receives the operator specifications and measures the operator runtime on the underlying hardware. The runtime of an entire computation graph is calculated by summing over all operator costs. In addition to TASO's internal hashing functionality for individual operator configurations, we also store each e-graph with its extracted cost to speed-up the simulation phase.\nOnce MCTS has exhausted the user-defined search budget, the best rewrite rule is determined based on the root's child node with the highest average value. This rewrite rule is then applied to the main e-graph. Single-pattern rewrite rules are applied by egg, for multi-pattern rewrite rules we rely on TENSAT's efficient search algorithm. The construction phase terminates once the e-graph has saturated or reached the memory limit. Afterwards, the optimized tensor program is extracted from the e-graph. Depending on the size of the e-graph, it is often feasible to use an ILP extractor for this final step."}, {"title": "5 Evaluation", "content": "In this section, we present our experimental results. We begin with an overview of the experimental setup, followed by an analysis of how our cost function affects MCTS optimization performance. Then, we compare our optimizer's performance with TENSAT."}, {"title": "5.1 Experimental Setup", "content": "We run our experiments on 13 models: BERT [8], Inception-v3 [27], MobileNet-v2 [24], NASNet-A [37], NASRNN [36], ResNet-50 [13], ResNeXt-50 [31], SqueezeNet [14], VGG-19 [26], Transformer-Transducer (TT) [34], ViT-Base, ViT-Large, and ViT-Huge [10]. Similar to [16, 32], we focus our evaluation on model inference as model training requires the storage of intermediate tensors for backpropagation, which generally prevents the graph transformations from being applied directly. Our optimizer supports 30 operators and uses TASO's rewrite rule set comprising of 124 single-pattern and 15 multi-pattern rewrite rules. For NASRNN we had to deactivate one and for TT two multi-pattern rewrite rules, because TASO could not measure the runtime of the resulting computation graph on our hardware. We ran the experiments on an Intel Xeon Silver 4210R CPU @ 2.40GHz with 8 cores and 64 GB RAM and TASO used an NVIDIA A100 80GB GPU to measure the operator runtimes. On a subset of the models, the first 9 listed above, we repeated the experiments on an NVIDIA P100 16GB GPU to evaluate how the hardware impacts the optimization results. For experiments involving an ILP, we used the same solver as TENSAT, SCIP [3].\nDue to the stochasticity of the runtime measurements, we repeated all experiments five times and are reporting the mean and standard deviation across all runs. The search budget was set to 128, the maximum simulation depth to 10, and the e-graph construction was stopped once a rewrite rule application resulted in an e-graph of 2,000 or more e-nodes. In the experiments with TENSAT, we found that the default setting of kmulti = 1 does not always allow TENSAT to reach this node limit. To enable a fair comparison with our approach, we increased kmulti for each architecture until the corresponding e-graph either saturated or hit the node limit."}, {"title": "5.2 Extraction", "content": "In Section 4.2, we have shown that our cost function is able to significantly improve the runtime estimates for various neural networks. We now analyze how this improved accuracy affects the downstream optimization performance. Figures 5 and 13 compare the runtime speedups and optimization times achieved by MCTS based on different combinations of main and final extraction method on the NVIDIA A100 and P100, respectively. The main method is used during e-graph construction to provide MCTS with a reward signal and the final method is used to extract the output program once e-graph construction has finished. A detailed breakdown of the results for all architectures can be found in Tables 2 and 4.\nUsing the default cost function for both the main and final extraction step (DCF/DCF) produces the worst performance. For one model, TT on A100 and NASNet-A on P100, the cost function even increases the original graph runtime by more than 2x. This is consistent with the findings from Yang et al., 2021 [32] and confirms that greedy extractors are by default ill-suited to extract tensor programs with shared subgraphs. OCF/OCF evades this failure mode and achieves an average 7-15% higher speedup. Using an ILP instead of a greedy extractor for the final extraction step further improves the output programs' runtime in both cases (DCF/ILP, OCF/ILP). Even with improved accuracy, greedy extractors remain heuristics that cannot provide the same performance guarantees as ILPs. Nevertheless, the small difference in obtained speedups between OCF/ILP and ILP/ILP (<2%) shows that greedy extractors using our cost function come close to matching the downstream optimization performance of ILPs while being on average 3-6x faster. Given that the optimization time of ILPs increases exponentially with the size of the e-graph, our proposed cost function introduces greedy extractors as a compelling alternative, especially when working with larger e-graphs. Although the absolute numbers vary between the two hardware backends, the overall findings and the relative ranking of the different extraction methods is consistent."}, {"title": "5.3 MCTS vs. TENSAT", "content": "In this section, we compare the end-to-end optimization performances of MCTS and TENSAT. For the purpose of this comparison, we focus on the two best-performing MCTS methods from the previous section - MCTS OCF/ILP and MCTS ILP/ILP. Figure 6 shows the runtime speedups obtained across the 13 neural network architectures on the NVIDIA A100 with TENSAT as the baseline. More detailed quantitative results, including the optimization times, can be found in Table 2. To gain qualitative insights into the decision-making processes of each approach, we plot the rewrite rule applications for one exemplary run in Figures 7, 8, and 10.\nFor three of the models, Inception-v3, ResNet-50, and VGG-19 there was only one applicable rewrite rule and thus no possibility for the optimizers to make different decision. Based on TASO's rewrite rule set, the phase ordering problem did not arise in these instances.\nOn TT, Bert, ViT-Base, and ViT-Large, MCTS and TENSAT achieve comparable speedups. For the latter three models, the optimizers correctly identify that rewrite rule 128 can significantly reduce the models' runtime. The rewrite rule, visualized in Figure 11, can eliminate one of two matrix multiplications when specific inputs remain fixed at inference time.\nFor MobileNet-v2, SqueezeNet, and ViT-Huge, MCTS ILP/ILP is able to achieve 2.5-5% higher speedups than TENSAT. Comparing the optimizers performance on the three vision transformers (ViT-Base, ViT-Large, ViT-Huge) shows that larger computation graphs - and therefore also larger input e-graphs - benefit MCTS. The reason for this is that the optimizers can apply less rewrite rules before they hit the memory limit and thus the value of every single rewrite rule application increases. TENSAT's sequential selection policy results in the phase-ordering problem whereas MCTS can mitigiate it by identifying the most promising rewrite rules.\nOn NasNet-A, NASRNN, and ResNeXt-50, MCTS is significantly outperforming TENSAT achieving additional speedups of over 5%. The biggest improvement was obtained on ResNeXt-50"}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we have shown that MCTS can significantly improve the performance of equality saturation-based tensor optimizers by mitigating the phase-ordering problem during e-graph construction. Furthermore, we have devised a novel cost function which enables greedy extractors to take common subexpressions into account and thereby improve the extraction results.\nThere are several promising avenues for future research to build on our work. Most neural networks consists of a few distinct blocks that are stacked on top of each other. These recurring structures could be exploited by splitting the input graph into its distinct parts, optimizing each part individually before reassembling the optimized graph. This approach could not only shorten the optimization time but also simplify the problem and make it feasible to use learning-based approaches like AlphaZero. In addition to the graph structure, the model latency also depends on the data layouts. Past work [16] has shown that simultaneously optimizing graph substitutions and data layouts can result in significant speed-ups. We think the incorporation of data layouts into the MCTS paradigm could have a similar effect and result in further runtime reductions."}, {"title": "A Artifact", "content": "A.1 Abstract\nOur artifact contains the code for our proposed tensor program optimizer which uses equality saturation and Monte Carlo tree search to reduce the runtime of deep learning models. Our optimizer takes as input a tensor computation graph and rewrite rule set, and outputs an optimized computation graph. The implementation builds on top of several open-source projects: TASO, egg, TENSAT, and MCTS-GEB. Our artifact includes detailed instructions, a Dockerfile to automate large parts of the setup process, the deep learning models to benchmark our approach, and a Jupyter notebook to reproduce the Tables and Figures in our paper. To run, the artifact requires an NVIDIA GPU, Ubuntu 22.04 LTS, NVIDIA drivers, and the NVIDIA Container Toolkit."}, {"title": "A.2 Artifact check-list (meta-information)", "content": "\u2022 Algorithms: A tensor program optimizer using equality saturation and Monte Carlo tree search, and a novel cost function for greedy extractors that takes common subexpressions into account.\n\u2022 Model: We evaluate and compare our approach on 13 neural network architectures: BERT, Inception-v3, MobileNet-v2, NASNet-A, NASRNN, ResNet-50, ResNeXt-50, SqueezeNet, VGG-19, Transformer-Transducer, ViT-Base, ViT-Large, and ViT-Huge. The models are included in the repository.\n\u2022 Run-time environment: Ubuntu 22.04 LTS, NVIDIA driver, NVIDIA Container Toolkit.\n\u2022 Hardware: An NVIDIA GPU is required. We used an NVIDIA A100 80GB GPU and an NVIDIA P100 16GB GPU for our experiments.\n\u2022 Metrics: Original graph runtime, optimized graph runtime, optimization time, rewrite rule applications.\n\u2022 Output: Each experiment outputs, among other things, .txt files with the above-mentioned metrics. We provide a Jupyter notebook to aggregate and process the results.\n\u2022 Experiments: We provide a README with a step-by-step installation guide and a Dockerfile to automate large parts of the setup process.\n\u2022 How much disk space required (approximately)?: 20GB.\n\u2022 How much time is needed to prepare workflow (approximately)?: 1 hour.\n\u2022 How much time is needed to complete experiments (approximately)?: 25 hours.\n\u2022 Publicly available?: Yes, the artifact is publicly available at https://doi.org/10.5281/zenodo.13278551.\n\u2022 Code license: MIT license.\n\u2022 Archived?: Yes, the code has been archived at https://doi.org/10.5281/zenodo.13278551."}, {"title": "A.3 Description", "content": "A.3.1 How to access. The artifact is publicly available at https://doi.org/10.5281/zenodo.13278551.\nA.3.2 Hardware dependencies. An NVIDIA GPU is required. We used an NVIDIA A100 80GB GPU and an NVIDIA P100 16GB GPU for our experiments.\nA.3.3 Software dependencies. Ubuntu 22"}]}