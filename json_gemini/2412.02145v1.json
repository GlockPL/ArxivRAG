{"title": "Effective Mitigations for Systemic Risks from General-Purpose AI", "authors": ["Risto Uuk", "Annemieke Brouwer", "Tim Schreier", "Noemi Dreksler", "Valeria Pulignano", "Rishi Bommasani"], "abstract": "The systemic risks posed by general-purpose AI models are a growing concern, yet the effectiveness of mitigations remains underexplored. Previous research has proposed frameworks for risk mitigation, but has left gaps in our understanding of the perceived effectiveness of measures for mitigating systemic risks. Our study addresses this gap by evaluating how experts perceive different mitigations that aim to reduce the systemic risks of general-purpose AI models. We surveyed 76 experts whose expertise spans AI safety; critical infrastructure; democratic processes; chemical, biological, radiological, and nuclear risks (CBRN); and discrimination and bias. Among 27 mitigations identified through a literature review, we find that a broad range of risk mitigation measures are perceived as effective in reducing various systemic risks and technically feasible by domain experts. In particular, three mitigation measures stand out: safety incident reports and security information sharing, third-party pre-deployment model audits, and pre-deployment risk assessments. These measures show both the highest expert agreement ratings (>60%) across all four risk areas and are most frequently selected in experts' preferred combinations of measures (>40%). The surveyed experts highlighted that external scrutiny, proactive evaluation and transparency are key principles for effective mitigation of systemic risks. We provide policy recommendations for implementing the most promising measures, incorporating the qualitative contributions from experts. These insights should inform regulatory frameworks and industry practices for mitigating the systemic risks associated with general-purpose \u0391\u0399.", "sections": [{"title": "1 Introduction", "content": "The rapid development of general-purpose AI models introduces risks with broad and far-reaching implications to society as a whole (Bommasani et al., 2022; Clarke et al., 2021; Future of Life Institute, 2021; Smuha, 2021). Some research has detailed these implications, identifying threats such as discrimination and toxicity, information hazards, misinformation harms, malicious uses, human-computer interaction harms, and automation and environmental harms (Weidinger et al., 2021). More recent work has addressed systemic risks ranging from labour market impacts to privacy risks and environmental concerns (Bengio et al., 2024). These systemic risks illustrate the complex and diverse challenges that general-purpose AI models present, emphasising the need for effective and targeted mitigation strategies.\nRecognising these challenges, the EU passed one of the first comprehensive AI regulations worldwide in 2024, the AI Act, which focuses on systemic risks, defining them as those stemming from the high-impact capabilities of general-purpose AI models, which can significantly affect the Union market due to their reach or reasonably foreseeable negative effects on public health, safety, public security, fundamental rights, or society as a whole (EU AI Act, Art. 3). These risks can proliferate widely across entire value chains, potentially manifesting as disruptions to critical sectors; negative effects on democratic processes; harmful bias and discrimination; and chemical, biological, radiological, and nuclear risks (CBRN) (EU AI Act, Rec. 110).\nDespite growing recognition of these risks, current mitigation approaches remain understudied. Several mitigation frameworks have been proposed, including ethics guidelines for trustworthy AI (EU AI High-Level Expert Group on Artificial Intelligence, 2019), algorithmic preparedness principles (Kolt, 2023), safety processes (UK Government, 2023), technical risk mitigation approaches (Bengio et al., 2024), model deployment guidance (Partnership on AI, 2023), and risk management standards (Barrett et al., 2023). However, these proposals lack systematic assessment of intervention effectiveness. The only study examining expert consensus on AI safety practices (Schuett et al., 2023) focused broadly on AGI rather than specific systemic risks, and its purely quantitative methodology precluded deeper insights into expert reasoning and contextual factors, leaving gaps in our understanding of effective mitigation measures.\nThis knowledge gap has immediate policy implications. The European AI Office, as the main enforcer of the EU AI Act, is responsible for developing codes of practice to guide providers of general-purpose AI in compliance. These codes, due by May 2025, should be informed by research on effective risk mitigation measures, with a focus on systemic risks and best practices for ensuring safety, as well as the protection of public health and fundamental rights. Effectively addressing the systemic risks posed by general-purpose AI requires identifying and implementing the most effective risk mitigation measures. Understanding which mitigations best reduce risks will help policymakers and providers of general-"}, {"title": "2 Methodology", "content": "This section outlines the approach taken to identify and evaluate risk mitigation measures for general-purpose AI models. It details the survey design, participant selection criteria, sampling method, procedures used to collect and analyse data, and ethical considerations."}, {"title": "2.1 Survey design", "content": ""}, {"title": "2.1.1 Identification of risk mitigation measures", "content": "This study began with a broad, narrative literature review to identify potential risk mitigation measures for providers of general-purpose AI. Drawing on our expertise and subjective judgement, we reviewed a variety of sources, including academic publications, industry reports, policy documents, and expert recommendations from the fields of AI safety, risk management, and technology governance. From this review, we curated a list of risk mitigation measures, based on our assessment of their potential to address systemic risks from general-purpose AI across various contexts. We established guiding criteria to determine which risk mitigation measures to include in the survey by asking of each whether it:\n\u2022 Addresses key drivers of systemic risk, either broadly or for specific risk types.\n\u2022 Demonstrates a plausible connection to systemic risk reduction.\n\u2022 Has been recommended by experts for similar risks in the past.\n\u2022 Aligns with standard risk management best practices.\n\u2022 Shows evidence of historical effectiveness in related contexts.\n\u2022 Targets key bottlenecks or vulnerabilities in risk management processes."}, {"title": "2.1.2 Survey questions", "content": "The survey was designed to quantitatively assess the perceived effectiveness of each risk mitigation measure while also gathering qualitative insights into the reasoning behind expert evaluations. It also aimed to identify potential gaps or additional measures not captured in the initial literature review. Participants were introduced to the survey with an overview of systemic risks as defined by the EU AI Act and asked to evaluate the selection of risk mitigation measures listed above in Section 2.1.1.\nWe divided the general-purpose AI-related systemic risks mentioned in the AI Act into the following four categories:\n1. Disruptions of critical sectors, such as digital infrastructure or other critical infrastructure, including via cyberattacks\n2. Negative effects on democratic processes, such as civil discourse and electoral procedures\n3. Chemical, biological, radiological, and nuclear risks, and other serious risks to public health and safety\n4. Harmful bias and discrimination with risks to communities or societies\nOur survey focused on the large general-purpose AI models which the EU AI Act classifies as having systemic risks. Under the EU AI Act, general-purpose AI models are considered to have systemic risks when the cumulative amount of computation used for its training measured in floating point operations exceeds 1025 FLOP. At the time of the survey, prominent examples likely included GPT-40, Gemini Ultra 1.5, Claude 3.5, and Llama 3.1 405B.\nThe survey included both closed-ended and open-ended questions to gather a comprehensive understanding of each measure's perceived effectiveness. Closed-ended questions required participants to rate the effectiveness of each measure in reducing specific systemic risks, using a 5-point Likert scale ranging from"}, {"title": "2.2 Sampling method", "content": ""}, {"title": "2.2.1 Participant selection", "content": "The target population for this survey comprised 303 experts in the fields of AI safety; critical infrastructure and AI; democratic processes and AI; chemical, biological, radiological, and nuclear risks (CBRN) and AI; and discrimination and bias in AI. The target population included academics, industry professionals, policymakers, and representatives from non-governmental organisations (NGOs) with demonstrated expertise in these areas. A non-probability sampling approach was used, with a purposive sampling method because our aim was to identify the most relevant experts to answer the research question.\nWe used purposive sampling. Experts were selected based on their recognised contributions to the field, including influential publications, leadership roles in professional organisations, and participation in major AI conferences. We aimed for representation from diverse geographical regions and institutional backgrounds to ensure a wide range of perspectives.\nThe list of potential participants was compiled from a range of sources, including the following:\n\u2022 Authors of key academic papers identified through Google Scholar\n\u2022 Papers that have seemed to be influential to the authors in academic and policy discussions\n\u2022 Recognised experts recommended by peers and colleagues within the field\n\u2022 Individuals known through professional networks"}, {"title": "2.2.2 Survey distribution", "content": "Experts were contacted via email and social media platforms. The communication included a clear explanation of the survey's purpose, estimated time commitment,"}, {"title": "2.2.3 Sample characteristics", "content": "The goal was to achieve a diverse sample that captured a wide range of expertise and perspectives within the field. While a specific sample size was not predetermined due to the exploratory nature of the research, efforts were made to include a broad representation across geographic regions, professional sectors (academia, industry, NGOs, think tanks), and demographic characteristics (gender, location). Table 2 provides an overview of the sample characteristics."}, {"title": "2.2.4 Representativeness and limitations", "content": "Although the sampling approach limits the ability to generalise findings to the entire expert population, steps were taken to ensure diversity and inclusivity within the sample. The use of peer recommendations and broad sample selection strategies aimed to mitigate potential biases inherent in non-probability sampling.\nThe survey achieved a response rate of 25% (76 out of 303 targeted par-ticipants), which may indicate a potential response bias in addition to any limitations of the selection of the sample. Comparing the demographic characteristics between target and realised samples reveals some patterns.\nRegarding representation from professional sectors, academia is slightly overrepresented (40.8% vs 37.6% in target). The non-profit sector is also over-represented (39.5% vs 34.0%). Industry is notably underrepresented (7.9% vs 17.2%). This indicates stronger participation from academic and non-profit sectors, and these sectors may show greater concern about AI risks compared to industry practitioners.\nWith regard to area of expertise, AI safety experts have slightly more repre-sentation than intended (38.2% vs 35.6%). Chemical, biological, radiological, and nuclear risks (CBRN) experts show higher participation as well (15.8% vs 14.2%). Experts in democratic processes show lower participation (17.1% vs 21.1%), but discrimination and bias experts slightly higher participation (16.8% vs 18.4%). This pattern does not offer a simple explanation, but it is possible that there was stronger engagement from those focused on technical risk mitigation measures, which were a strong component of the survey.\nWe can offer some possible explanations without confidence in any of them because of a small sample size. The lower industry participation might mean we are missing important practical implementation perspectives, including with regard to feasibility of risk mitigation measures. The high academic and non-profit representation could skew responses toward more theoretical or cautionary viewpoints. Finally, the strong response from AI safety experts might indicate overrepresentation of those most concerned about AI risks."}, {"title": "2.3 Data analysis", "content": "In the survey, experts provided quantitative input for each topic first, followed by the option to elaborate with qualitative rationales. This structure allowed us to gather both objective assessments and contextual insights for a comprehensive understanding of each risk mitigation measure."}, {"title": "2.3.1 Quantitative data analysis", "content": "The quantitative data, collected from Likert scale and ranking responses, were analysed to assess the perceived effectiveness of each risk mitigation measure across the four systemic risk categories.\nThe data includes ratings from all experts across all four risk areas. This data was used to generate stacked bar charts visualising expert agreement on the effectiveness of each risk mitigation measure (see Figure 3 and Figure 4). The ranking responses were also analysed to determine how often each measure was selected as part of the experts' top-ten combinations. The results were visualised using a bar plot, which displayed the selected measures broken down by expert group affiliations. Experts were given the option to rate measures as \"not feasible\" if they believed the scientific or technical understanding of the intervention was insufficient for robust implementation. The data was processed to produce a feasibility graph, highlighting the proportion of experts who deemed certain measures currently infeasible."}, {"title": "2.3.2 Qualitative data analysis", "content": "Qualitative data from the open-ended responses were analysed thematically to identify key insights and rationales behind expert opinions. The analysis involved reviewing around 120 pages of qualitative input, with a combination of manual reading and assistance from a general-purpose AI model, Claude Sonnet 3.5, to help identify patterns and develop concise summaries. This analysis helped identify specific factors influencing expert perspectives on feasibility and effectiveness, as well as uncover any gaps or additional mitigation measures not captured during the initial review. It is important to note that there are potential concerns about using an AI model to analyse the qualitative data; we only used it for initial pattern identification and making summaries more concise. The qualitative data and summaries underwent thorough human review, but mistakes could have been still made due to issues like hallucinations with these models."}, {"title": "2.4 Ethical considerations", "content": "The survey was conducted in accordance with ethical guidelines for research involving human subjects. Informed consent was obtained from all participants,"}, {"title": "3 Results", "content": "Our survey of domain experts revealed varying levels of perceived effectiveness for different systemic risk mitigation measures. The results are summarised in three key visualisations: a stacked bar chart displaying the agreement levels for each measure by risk area (Figure 3 and Figure 4), and a stacked bar chart displaying how often each risk mitigation measure was selected as part of experts' top ten choices, broken down by expert group affiliations (Figure 5). In addition to the quantitative analysis, we also reflect extensively on the qualitative insights provided by experts."}, {"title": "3.1 Expert agreement on effectiveness of measures", "content": ""}, {"title": "3.1.1 Quantitative analysis of expert opinion", "content": "Figure 3 and Figure 4 present expert opinions on the effectiveness of various risk mitigation measures for general-purpose AI models across four risk areas. The results reveal a diverse range of perceived effectiveness among the proposed measures. The insights likely reflect the specific challenges and priorities within risk areas.\nSafety incident reports and security information sharing emerges as the most agreed to be effective measure, with strong agreement across all risks. Some measures show more variation in perceived effectiveness across different risks. For instance, advanced information security and prohibiting high-stakes applications receive mixed responses. It is worth noting that in the case of some risk areas like disruptions of critical sectors most measures get support from over half of the experts, suggesting that experts see potential value in a wide array of risk mitigation measures."}, {"title": "3.1.2 Qualitative analysis of expert opinion", "content": "Summaries of the qualitative expert insights for each measure are presented in Appendix A.3. The feedback provided by the experts covers a wide range of considerations for each measure, including:\n\u2022 Potential effectiveness across different risk areas\n\u2022 Implementation challenges\n\u2022 Limitations and potential drawbacks\n\u2022 Comparisons with practices in other industries"}, {"title": "3.2 Expert opinion on combinations of risk mitigation measures", "content": ""}, {"title": "3.2.1 Quantitative analysis of preferred measures", "content": "We asked experts to identify the ten measures they believed to be most effective in reducing systemic risks from general-purpose AI. Figure 5 presents how often these measures were selected, broken down by domain expert group, and then summarised across all respondents. The results indicate that almost two-thirds of respondents chose third party pre-deployment model audits in their preferred list of measures, highlighting it as a favoured mitigation measure.\nWhen comparing the combination rankings to individual rankings, some measures consistently appeared at the top of both lists, indicating expert consensus. These highly ranked measures include third party pre-deployment model audits, safety incident reports and security information sharing, whistleblower protections, and pre-deployment risk assessments. Their frequent selection underscores their perceived importance across different contexts. Some measures shifted in priority when selected in combination rather than individually. For instance, prohibiting high-stakes applications ranked higher in the combination list, suggesting that experts find it more effective when paired with other measures.\nIndividual and combined rankings showed some differences in the prioriti-sation of measures. Some measures ranked higher when evaluated individually than in combination with others. Measures like advanced model access for vetted external researchers and vetted researcher access had a higher priority in the individual ranking. Fine-tuning restrictions and transparent governance structures were occasionally also ranked higher in the individual list compared to the combinational rankings, possibly indicating that while valuable individually, their perceived contribution may diminish when combined with other mitigation efforts. However, given the complexity of the survey, these ranking differences might reflect response inconsistencies rather than substantive preferences."}, {"title": "3.2.2 Qualitative analysis of expert rationales", "content": "Experts provided diverse rationales for their selection of the top ten risk mitiga-tion measures. Many emphasised the practicality of measures that could feasibly be implemented and subjected to external scrutiny, favouring those that involved governance, oversight, and independent evaluation. There was significant support for prioritising information sharing, transparency, and security practices, with several experts highlighting parallels to best practices in information security, where the focus is on limiting the damage of potential attacks. Some experts underscored the importance of iterative, ex ante approaches that prevent risks from escalating, while others expressed a preference for measures that slow the introduction of powerful AI capabilities to society. Several participants noted that measures aimed at structured transparency and risk assessment, such"}, {"title": "3.3 Feasibility of risk mitigation measures", "content": ""}, {"title": "3.3.1 Quantitative analysis of feasibility", "content": "We gave experts the option to rate measures as \"not feasible\" if they believed it is not feasible to robustly implement a certain measure because the scientific or technical understanding of the intervention is not mature enough. Figure 6 shows the percentage of experts that rated measures as not feasible. The figure shows that all measures have at least 91% or more of the experts assuming that it is currently sufficiently feasible to implement them. The measures that are considered infeasible by the highest number of experts are: KYC screenings (9%), unlearning techniques (8%) and sharing safety cases (7%). These measures also scored relatively low on both individual effectiveness and occurrence in preferred combinations of measures."}, {"title": "3.3.2 Qualitative analysis of feasibility", "content": "Out of 76 experts, only 24 provided at least one comment on the feasibility of the risk mitigation measures. Therefore, these views are not fully representative of the entire expert group and may differ from the quantitative analysis presented in Figure 6.\nExperts provided mixed views on the feasibility of various risk mitigation measures, often pointing to challenges related to the current state of scientific and technical knowledge. Some experts expressed scepticism regarding pre-development risk assessments, citing limitations in forecasting capabilities and the difficulty of predicting specific downstream risks prior to training. The consensus was that these assessments, while promising, are not yet supported by sufficiently mature methods, particularly in areas like systemic risks from general-purpose AI models. For pre-deployment risk assessments, several experts indicated that these could be feasible for certain risks, such as those involving chemical, biological, radiological, and nuclear (CBRN) risks, but they raised concerns about biases when assessments are conducted internally by companies. Third-party audits were seen as valuable due to their external nature, although"}, {"title": "3.4 Other suggested measures", "content": "Experts suggested several additional measures beyond those included in the survey (for a complete overview, see Appendix A.4). One suggestion was the implementation of coordinated red-teaming to thoroughly assess model vulnerabilities, akin to penetration testing for security infrastructure. Experts also recommended establishing a government-run, lab-funded safety research organisation tasked with imposing safety guardrails for models exceeding a specific capability threshold.\nThere was also support for mandatory impact assessments and making these publicly available, as well as using tools such as a transparency index to help users understand the risks associated with a given model. Several experts advocated for introducing strict fines and penalties for non-compliance and holding vendors liable for damages to enforce accountability. Additionally, experts suggested mandatory reporting of dual-use capabilities to relevant government agencies and creating incident response plans that outline roles and responsibilities in handling potential threats.\nOther suggested measures included enforcing much stricter data protection to limit available training data and reduce risks. Localised risk assessments"}, {"title": "4 Discussion", "content": "Our study provides a systematic evaluation of risk mitigation measures for the largest general-purpose AI models, focusing on the perceived effectiveness of measures in reducing systemic risks. By synthesising expert opinions from various domains, we have identified key measures that are considered most effective and technically feasible. In this section, we interpret these findings, discuss their implications, and acknowledge limitations."}, {"title": "4.1 Interpretation of findings", "content": "Safety incident reports and security information sharing emerged as one of the most highly regarded measure across all expert groups. This suggests a strong consensus on the importance of transparency and collaboration in identifying and addressing AI-related incidents. Third-party pre-deployment model audits and pre-deployment risk assessments were also ranked highly, underscoring the value experts place on external scrutiny and proactive evaluation before AI models are deployed. The emphasis on whistleblower protections indicates recognition of the critical role that insiders can play in identifying risks that may not be apparent through formal assessments. Interestingly, measures that rely solely on the AI providers' internal processes, such as safety vs. capabilities investments and risk-focused governance structures, were considered less effective unless they"}, {"title": "4.2 Implications for policy", "content": "The following list presents our recommended measures. The study identifies eight priority measures for providers of general-purpose AI based on two criteria: how often experts selected them in their top-10 combinations and how strongly experts agreed with their effectiveness across all risk domains. These recommendations are grounded in expert consensus on feasibility, with all measures deemed feasible by at least 97% of experts.\n1. Third-party pre-deployment model audits: Independent safety as-sessments of models before deployment, with auditors given appropriate access for testing\n2. Safety incident reporting and security information sharing: Dis-closure of AI incidents, near-misses, and security threats to relevant stake-holders\n3. Whistleblower protections: Policies ensuring safe reporting of concerns without retaliation or restrictive agreements\n4. Pre-deployment risk assessments: Comprehensive assessment of po-tential misuse and dangerous capabilities before deployment\n5. Risk-focused governance structures: Implementation of board risk committees, chief risk officers, multi-party authorisation requirements, ethics boards, and internal audit teams"}, {"title": "4.3 Strengths", "content": "This study offers several notable strengths in its methodological approach to evaluating the perceived effectiveness of mitigation measures for systemic risks from general-purpose AI. The research design combines quantitative ratings with extensive qualitative insights from around 120 pages of expert input, allowing for richer contextual understanding. The domain-specific analysis enabled experts to evaluate measures within their areas of expertise, while allowing a structured evaluation across multiple risk categories. Future research can benefit from this qualitative data for additional insights.\nThe sample quality represents another key strength, featuring highly qualified participants with most having relevant publications and being in applicable senior positions. The expert panel represents diverse expertise across five critical"}, {"title": "4.4 Limitations", "content": "Several limitations should be acknowledged. First, the survey relied on expert opinions, which, while valuable, may be subject to biases based on the experts' backgrounds and experiences. The sample, although diverse, may not fully represent all perspectives within the AI safety and risk domains. Notable expert groups are missing from our target list, including experts in the impact of AI on creative industries, labour markets, and education. While we strived to represent diverse risk categories and expert perspectives in the survey, the breadth of potential AI impacts necessitated selective sampling to maintain a manageable survey length and ensure quality responses.\nSecond, the measures were evaluated based on perceived effectiveness and feasibility, assuming they are legally required, well-executed, and overseen by competent regulators. In practice, implementation challenges may reduce their effectiveness. The assumption of optimal execution may not hold in real-world scenarios, where resource constraints, regulatory capacity and organisational cultures vary. Several participants expressed concerns that the effectiveness of the measures heavily depended on significant \"ifs,\" such as their implementation and verification. This reliance on optimal conditions for effectiveness, which may not always be realistic, suggests that the survey may have overestimated the practical impact of some measures. The challenges of real-world implementation"}, {"title": "4.5 Future research", "content": "Future research on expert opinion of effective mitigation measures to reduce systemic risks from general-purpose AI could aim for larger and more diverse sample sizes and analyse them from different perspectives. While expert opinion provides valuable insights, there is a critical need for empirical evidence on the effectiveness of these measures including quantitative data on changes in incident rates, implementation costs, and concrete safety outcomes."}, {"title": "5 Conclusion", "content": "This study provides the first comprehensive exploration of expert perspectives on risk mitigation measures for general-purpose AI models, with a particular focus on reducing systemic risks as defined by the EU AI Act. Through a literature review and survey of 76 experts across five key areas AI safety; critical infrastructure and democratic processes; chemical, biological, radiological, and nuclear (CBRN) risks; discrimination and bias we identified several measures for mitigating systemic risks from general-purpose AI that are perceived to be effective and technically feasible.\nOur findings reveal some expert consensus on the importance of transparency, external scrutiny, and proactive risk assessment. The most highly-rated measures include safety incident reports and security information sharing, third-party pre-deployment model audits, and pre-deployment risk assessments. Notably, experts considered all evaluated measures to be technically feasible, with even the lowest-rated measures achieving at least 91% feasibility ratings.\nOur research makes several important contributions to the field of AI governance. Firstly, we provide the first systematic assessment of risk mitigation measures that combines quantitative ratings with extensive qualitative expert insights, offering a more nuanced understanding than previous work. Secondly, our domain-specific analysis across four systemic risk areas offers insights into how different types of experts view and prioritise various mitigation measures for different risks. Thirdly, we evaluate both individual effectiveness and combinato-"}, {"title": "A Appendices", "content": ""}, {"title": "A.1 CHAI Conference survey and workshop", "content": "At an early stage of this research, we hosted a workshop at the Center for Human-Compatible AI Conference in California on the 14th of June 2024. Prior to the workshop, participants completed a survey in which they rated 37 risk mitigation measures based on their perceived effectiveness in reducing system risks from general-purpose AI. Participants were instructed to imagine a comprehensive and well-executed version of each mitigation measure when providing their evaluations. The survey included the following prompt to guide participants' assessments: \"Rate the following measures in terms of how effective they are likely to be at reducing systemic risks from general-purpose AI in the next five years.\" Participants were instructed to consider how each measure could reduce total risk, conceptualised as Risk = Likelihood x Severity of impact. It was clarified that for a mitigation measure to be considered 'extremely effective,' it does not need to eliminate all risk on its own.\nParticipants represented a range of affiliations, including MIT, UC Berkeley, Carnegie Mellon University, and Google DeepMind. Data collected from the surveys were analysed to identify trends and consensus on the most effective measures for reducing AI-related systemic risks.\nThe list of risk mitigation measures presented to participants was the follow-ing:\nIntolerable risk thresholds: Establish red-line risk or capability thresholds, continuously assess and monitor for breaches, and prepare technical, legal, and organisational measures to immediately halt development and deployment if a breach occurs (Barrett et al., 2023, p. 32, 54; UK Government, 2023, p. 11-12; O'Brien et al., 2023, p. 20, 24).\nSafety vs. capabilities: A significant fraction of employees working on enhancing model safety and alignment rather than capabilities (Schuett et al., 2023, p. 11, 18).\nPre-deployment risk assessments: Comprehensive risk assessments before deployment that would assess foreseeable misuse and include dangerous capability evaluations, incorporating post-training enhancements and collaborations with domain experts (Schuett et al., 2023, p. 12, 18; Partnership on AI, 2023, p. 3; Kolt et al., 2024, p. 4; Shevlane et al., 2023, p. 9; Barrett et al., 2023, p. 21; UK Government, 2023, p. 17).\nThird-party model audits: External assessment to provide a judgement - or input to a judgement - on the safety of a model. Audits might be conducted by governments or other third-parties (The White House, 2023, p. 3; Partnership on AI, 2023, p. 3; Schuett et al., 2023, p. 18; UK Government, 2023, p. 17).\nWhistleblower protections: Comprehensive whistleblower protection poli-"}, {"title": "A.1.1 Survey and workshop results", "content": "The following section first presents the qualitative and then the quantitative results of our survey and workshop. The qualitative results consist of some of the rationales experts gave to justify their ratings. The quantitative section summarises the effectiveness-ratings of different mitigation measures.\nQuantitative results. Figure 7 summarises the results of the survey on risk mitigation measures. The bars represent the percentage of participants who rated each measure at specific levels of effectiveness, with different colours indicating varying levels of agreement. These quantitative insights provide an overview of expert consensus regarding the perceived effectiveness of each proposed mitigation measure.\nQualitative results. For the five highest and the five lowest rated risk mitiga-tion measures, we highlight some of the written comments experts contributed. Furthermore, we highlight some general remarks made in the in-depth workshop discussions.\nHighest-rated measures:\n1. Intolerable risk thresholds: \"Effective if identified by legitimate 3rdparties\"\n2. Safety vs capabilities: \"A significant increase of effort on safety vs capabilities would greatly improve the current situation where capabilities are far better resourced\"\n3. Pre-deployment risk assessments: \"Of course this should happen and many should start thinking about the risks of next models already after the last generation was deployed\"\n4. Third-party model audits: \"Only powerful if acceptable risk is defined and models that fail will be prevented from deployment\"\n5. Whistleblower protections: \"Great for cutting across many interven-tions, and helpful for especially those leaving companies, or even protection for external investigators\"\nLowest-rated measures:"}, {"title": "A.1.2 Key takeaways from the CHAI survey and workshop", "content": "The following conclusions have been drawn from this preliminary analysis of risk mitigation measures:\n1. No single risk mitigation measure is effective on its own, a portfolio of approaches is needed. None of the mitigation measures are rated as extremely effective by a majority of the experts. To sufficiently reduce systemic risk, a selection of risk mitigation measures is required.\n2. More specifics and empirical information are needed in order to make risk mitigation effective. Detailed, context-specific data and empirical studies are necessary to understand the nuances of implementing effective risk mitigation strategies.\n3. Much of the effectiveness of risk mitigations depends on how well they are executed.\n4. Proper implementation and diligent execution of risk mitigation measures are essential for their success. Even well-designed measures can fail if not properly executed.\n5. Many interventions become ineffective if controlled solely by the providers with self-interests. Mitigation measures should not be exclusively managed by general-purpose AI providers, as their self-interests may compromise"}, {"title": "A.2 Survey questions", "content": "This appendix presents the primary contents of our survey, including the introduction presented to participants and the key questions they were asked to respond to."}, {"title": "A.2.1 Introduction", "content": "The following introduction was provided to survey participants:\n\"In the following questions, you will be asked whether the implementation of different risk mitigation measures by providers of large general-purpose AI models would effectively reduce systemic risks. When considering a specific mitigation measure, you should assume that the measure would be legally required, well-executed, and overseen by a competent regulator.\nThis survey focuses on the large general-purpose AI models, specifically the ones which the EU AI Act classifies as having systemic risks. Under the EU AI Act, general-purpose AI models are considered to have systemic risks when the cumulative amount of computation used for its training measured in floating point operations exceeds 1025 FLOP.\nCurrently, the general-purpose AI models in scope likely include GPT-40, Gemini Ultra 1.5, Claude 3.5, and Llama 3.1 405B, and possibly others.\""}, {"title": "A.2.2 Systemic risks", "content": "The following definition and categorisation of systemic risks were provided to participants:\n\"The EU AI Act defines systemic risk as \"a risk that is specific to the high-impact capabilities of general-purpose AI models, having a significant impact on the Union market due to their reach, or due to actual or reasonably foreseeable negative effects on public health, safety, public security, fundamental rights, or the society as a whole, that can be propagated at scale across the value chain.\""}, {"title": "A.2.3 Risk reduction", "content": "The following instructions on risk reduction were provided:\n\"We are interested in identifying the risk mitigation measures that most effectively reduce systemic risks.\nReducing the risk means reducing the total systemic risk, where 'risk' is defined as the combination of the probability of an occurrence of harm and the severity of that harm.\nAdditional instructions\n\u2022 If you are unfamiliar with a particular measure or otherwise feel like you cannot make an informed judgement, please select \"I don't know\".\nIf you believe it is not feasible to implement a certain measure robustly because the scientific or technical understanding of the intervention is not mature enough, please choose \"not feasible\" and expand on your rationale in a comment.\""}, {"title": "A.2.4 Main questions", "content": "The following key questions were presented to participants:\na) \"To what extent do you agree or disagree that the implementation of this risk mitigation measure by providers of large general-purpose AI models would effectively reduce the following systemic risks? Assume the measure is legally required, well-executed, and overseen by a competent regulator.\nProvide your assessment for each systemic risk separately. You are invited to elaborate on your reasoning for each of the measures in the box below."}, {"title": "A.3 Qualitative expert insights per measure", "content": "For each measure, we append summaries of the qualitative input provided by the experts in the survey.\nPre-development risk assessments\nExpert opinions on pre-development risk assessments for AI models varied, but many agreed that this measure could be helpful as a best practice, particularly for detecting potential emergent capabilities and assessing risks early in the"}, {"title": "A.4 Additional risk mitigation measures suggested by experts", "content": "In addition to evaluating the predefined list of risk mitigation measures, we invited participants to suggest other potential measures that regulators could require providers of large general-purpose AI models to implement. Experts were asked to consider measures that could be effective at reducing systemic risks from general-purpose AI, assuming these measures would be legally required, well-executed, and overseen by a"}]}