{"title": "AgentMove: Predicting Human Mobility Anywhere Using Large Language Model based Agentic Framework", "authors": ["Jie Feng", "Yuwei Du", "Jie Zhao", "Yong Li"], "abstract": "Human mobility prediction plays a crucial role in various real-world applications. Although deep learning based models have shown promising results over the past decade, their reliance on extensive private mobility data for training and their inability to perform zero-shot predictions, have hindered further advancements. Recently, attempts have been made to apply large language models (LLMs) to mobility prediction task. However, their performance has been constrained by the absence of a systematic design of workflow. They directly generate the final output using LLMs, which limits the potential of LLMs to uncover complex mobility patterns and underestimates their extensive reserve of global geospatial knowledge. In this paper, we introduce AgentMove, a systematic agentic prediction framework to achieve generalized mobility prediction for any cities worldwide. In AgentMove, we first decompose the mobility prediction task into three sub-tasks and then design corresponding modules to complete these subtasks, including spatial-temporal memory for individual mobility pattern mining, world knowledge generator for modeling the effects of urban structure and collective knowledge extractor for capturing the shared patterns among population. Finally, we combine the results of three modules and conduct a reasoning step to generate the final predictions. Extensive experiments on mobility data from two sources in 12 cities demonstrate that AgentMove outperforms the best baseline more than 8% in various metrics and it shows robust predictions with various LLMs as base and also less geographical bias across cities. Codes and data can be found in https://github.com/tsinghua-fib-lab/AgentMove.", "sections": [{"title": "Introduction", "content": "Mobility prediction is of great importance in many real-world scenarios, e.g., recommend travel services, pre-activating mobile applications for potential usage, seamless switching of cellular network signals and efficient traffic management. In the recent years, deep learning based models (Liu et al. 2016; Wu et al. 2017; Feng et al. 2018; Yang et al. 2020; Yang, Liu, and Zhao 2022) are widely applied and has achieved good results due to its advantages in capturing the high-order transition capabilities and mining the shared mobility patterns between users. However, existing works have several key drawbacks. First, the success of deep learning models rely on the collection of large amounts of private mobility data. Second, the trained model is difficult to be applied in the zero-shot mobility prediction settings. Finally, the prediction accuracy is still lower due to the limited sequential modelling capability of small deep learning models and lack of deeply understanding of common sense in human daily life and urban structures.\nRecently, large language models (LLMs) have made significant progress, achieving advanced results far surpassing previous methods in areas such as dialogue based role-playing, code generation and testing, and mathematical problem solving. In the field of spatial temporal data mining, researchers also explore the potential of applying LLMs many real-world tasks, such as time series forecasting (Gruver et al. 2024; Li et al. 2024b), travel planning (Xie et al. 2024; Li et al. 2024a), trajectory analysis (Luo et al. 2024; Zhang et al. 2023), and so on. Furthermore, several recent works (Wang et al. 2023; Beneduce, Lepri, and Luca 2024) investigate the feasibility of employing the LLMs as the base model of mobility predictor for addressing the previous limitations of deep learning based models and achieve promising results. These works first convert the trajectory to a language based sentence and then utilize the powerful sequential modelling capacities of LLMs to directly output the mobility prediction results. However, due to the absence of a systematic design of whole procedure, they ignore the crucial components of human mobility modeling and thus achieve limited performance. In summary, these methods fail to effectively capture the complicated individual mobility pattern, ignore modeling the effects of urban structure and discovering the shared mobility patterns among population.\nIn this paper, we propose AgentMove, a systematic agentic framework for generalized mobility prediction in any cities around the world. In AgentMove, by integrating the domain knowledge of human mobility, we implement the core components in the general agentic framework (Wang et al. 2024; Xi et al. 2023), including the planning module, memory module, world knowledge module, external tool module and reasoning module. For the planning module in AgentMove, we introduce a manually designed mobility prediction task decomposition module by considering the most important factors for mobility prediction. After decomposition, we generate three sub-tasks: individual mobility pattern mining, shared mobility pattern discovering and urban structure modelling. Firstly, we implement a spatial-temporal memory module for individual mobility pattern mining. It contains three submodules, short-term"}, {"title": "Preliminaries", "content": "Here, we define the mobility prediction task and related concepts, which are used in the following section.\nDefinition 1 (POI) A POI point $p \\in P$ is represented as a tuple $(id, cate, lon, lat, addr)$, where id is the unique identifier, cate is the category (e.g., restaurant), lon and lat are the coordinates of the POI, addr is the text address of POI.\nDefinition 2 (User Trajectory) A trajectory of user $u \\in U$ is represented as $T_u = \\{(p_1, t_1), (p_2, t_2),..., (p_n, t_n)\\}$, where $p_i \\in P$ is the i-th POI visited by the user and $t_i$ is the timestamp of the visit.\nDefinition 3 (Contextual Stays) Contextual stays of user u is defined as the most recent sub-sequence in trajectory: $C_u = \\{(p_{n-k},t_{n-k}),..., (p_{n-1}, t_{n-1}), (p_n, t_n)\\}$, which captures the user's short-term mobility patterns. k is the window size of contextual stays.\nDefinition 4 (Historical Stays) Historical stays of user u is defined as the sub-sequence before contextual stays: $H_u = \\{(p_1, t_1), (p_2, t_2),...,(p_{n-k-1},t_{n-k-1})\\}$, which captures the user's long-term mobility patterns.\nGiven the historical movement data $C_u, H_u$ as well as available external knowledge $K$ (e.g., worldwide geospatial information), the objective is to predict the next POI $p_{n+1}$ that user u will visit. Formally, this paper aims to learn a mapping function f:\n$f: (C_u, H_u, K) \\rightarrow p_{n+1}.$                                              (1)"}, {"title": "Methods", "content": "As shown in Figure 2, AgentMove consists of five core components: task decomposition module, spatial-temporal memory module, world knowledge generator, collective knowledge extractor and the final reasoning module. As the high-level planning module, the task decomposition module is designed to break down the whole mobility prediction task into subtasks-personalized mobility pattern mining, collective mobility pattern discovery and the effects of urban structures-by considering the crucial factors of mobility. Detailed design of other components are introduced as follows."}, {"title": "Spatial-temporal Memory for Personalized Multi-scale Periodicity Behavior Modelling", "content": "The spatial-temporal memory module is designed to effectively capture, store and leverage mobility patterns, providing crucial insights for the personalized and multi-scale periodicity behavior modelling in mobility prediction. Inspired by the memory design principles in general LLM-based agents (Zhang et al. 2024), our spatial-temporal memory functions through three essential processes: memory organization, memory writing, and memory reading. The whole"}, {"title": "World Knowledge Generator for Enhancing Exploration Behavior Modelling", "content": "Lots of research (Jiang et al. 2016) shows that individual movement typically includes two types of behaviors: return and explore. As introduced before, the return behavior has been well-captured by the spatial-temporal memory module. In this section, we introduce the world knowledge generator to extract geospatial knowledge from LLMs and construct multi-scale urban structure to enable the modelling of explore behavior of mobility. To extract geospatial knowledge effectively, we propose to alignment the knowledge of LLMs and the urban space of trajectory via text address. Once the space is alignment, we explicitly motivate the LLMs to generate the potential explore candidate places from the multi-scale urban structure view.\nAlignment via Address Many existing works (Feng et al. 2018; Luo, Liu, and Liu 2021; Lin et al. 2021; Cui et al. 2021; Qin et al. 2022; Hong et al. 2023) on mobility prediction usually represent the locations directly using latitude and longitude coordinates or discrete spatial area IDs. While this approach facilitates the easy construction of deep learning-based spatial encoding, it is not suitable for LLMs. Since LLMs are trained on large scale human-generated text, they, like human, are not inherently adept at understanding"}, {"title": "Collective Knowledge Extractor for Shared Mobility Pattern Modelling", "content": "In the previous two sections, we introduce the spatial-temporal memory module and world knowledge generator for the individual level mobility modelling. Here, we introduce the collective knowledge extractor to enable capture the sharing mobility pattern among users to further improve the mobility prediction. In one world, we first construct a global location transition graph using NetworkX 2 by aggregating the location transitions from various users. We then employ a LLM to perform simple reasoning on the graph, utilizing the function in NetworkX as tool to generate potential locations visited by other users with similar mobility pattern."}, {"title": "Reasoning on Graph via Tool", "content": "After obtaining the location graph, we can utilize LLM to perform reasoning on the whole graph via the function of NetworkX as tool. The most naive strategy is to query the k-hop neighbors of the current location. When the number of the neighbors is too much, LLMs need to filter the most promising ones from them by considering the attributes of each node. Furthermore, we can extend the query nodes into the last n locations and generate the most promising ones from all the neighbors of them. In this way, we obtain the most relevant locations that has been visited by the users with similar mobility patterns.\nFinally, we design prompts to employ LLM to analyze and summarize the information from different views and perform a final reasoning step to generate the prediction with reasons. The prompts for output format requirements are also be placed here to ensure that the output format meets the requirements as much as possible."}, {"title": "Evaluation", "content": "Datasets We use the global Foursquare checkin data (Yang, Zhang, and Qu 2016) and recent public released ISP trajectory data (Feng et al. 2019) to conduct the experiments. The Foursquare data contains checkins from 415 cities which covers about 18 months from April 2012 to September 2013. The ISP trajectory data is from Shanghai with 325215 records, covering April 19 to April 26 in 2016. Compared with Foursquare data, ISP data is much denser and was open-sourced only two months ago 3, which is beyond the training period of all the LLMs used in the experiment. This ensures that the evaluation results are not affected by potential data leakage.\nTo evaluate the general mobility prediction ability of AgentMove, we select 12 cities from the Foursquare dataset and the entire ISP trajectory data to conduct the experiments. We follow the preprocessing procedure (Hong et al. 2023; Feng et al. 2019) to process the trajectories data. Detailed description about preprocessing can refer to the appendix. We select the Tokyo, Moscow and SaoPaulo with the largest amount of Foursquare check-in data and the ISP data from Shanghai to conduct the main analysis in the experiments and results of 12 cities are discussed in the final section of experiment. Due to the cost of the various API calling, e.g., Llama3.1-405B, we randomly sample 200 instances from the testing set for each city to calculate the performance in the experiments.\nBaselines We have three kinds of baselines, Markov methods, deep learning models and LLM-based methods.\n*   FPMC (Rendle, Freudenthaler, and Schmidt-Thieme 2010) It combines the matrix factorization and Markov chains methods together for sequential modelling.\n*   RNN (Feng et al. 2018) It is a simple RNN based mobility prediction model as regarding the mobility sequence as general sequence.\n*   DeepMove (Feng et al. 2018) It contains a LSTM for capturing the short-term sequential transition and an attention unit for extracting long-term periodical patterns.\n*   LSTPM (Sun et al. 2020) It consists of a non-local network for long-term preference modeling and a geo-dilated RNN for short-term preference learning.\n*   LLM-Mob (Wang et al. 2023) It is the first work to apply LLM (GPT-3.5) to predict the next location with carefully prompt engineering.\n*   LLM-ZS (Beneduce, Lepri, and Luca 2024) Following LLM-Mob, it simplifies the prompts and testifies more LLMs in zero-shot mobility prediction task.\nWe use widely used Accuracy@1, Accuracy@5, and NDCG@5 as the main evaluation metrics (Sun et al. 2020; Luca et al. 2021) in the experiments.\nImplementation We use LibCity (Jiang et al. 2023) to implement the FPMC, RNN, DeepMove and LSTPM. We follow the default parameter settings of these models in the library for training and inference. For LLMs, we use OpenAI API 4 for accessing GPT4omini and DeepInfra 5 for accessing other open source LLMs, including Llama3, Qwen2, Gemma2 and GLM4. Detailed parameter settings for those baselines can be found in the appendix."}, {"title": "Main Results", "content": "In this section, we compare AgentMove with 6 baselines in 4 cities to demonstrate the effectiveness of proposed framework. Here, we use GPT4omini as the default base LLM of AgentMove and the results of AgentMove with other kinds of base LLMs can be found in first section in appendix. To ensure a fair comparison, the LLM-based baselines, LLMMob and LLM-ZS, also employ the same base LLM as AgentMove. The results are presented in Table 1.\nAs the representative deep learning models, DeepMove and LSTPM achieve best or second-best results in 5 out of 12 metrics, and both of their performance varies among different cities. Compared with the deep learning baselines, the best LLM-based baseline LLM-ZS can achieve better results than DeepMove and LSTPM in 7 out of 12 metrics. The promising results of LLM-ZS present the powerful sequential pattern discovery and reasoning ability of LLM in modeling mobility, which demonstrate the potential of the LLM-based method. It is noted that the results of LLMbased methods are zero-shot prediction while the deep learning based methods rely on sufficient training with enough mobility data. Compared with these baselines, our proposed method AgentMove is the best method and achieves the best results in 11 out of 12 metrics in 4 datasets. Compared with the deep learning methods, AgentMove outperforms them more than 20% in half of the metrics. Even in the weakest results, it performs very close to the strongest baseline DeepMove with less than 3% performance drop. Compared with the advanced LLM-based baselines, AgentMove outperforms both of them more than 5.7%-39.4% in all the"}, {"title": "Ablation Study", "content": "In this section, we conduct ablation study to demonstrate the contribution of each component in AgentMove. Related results are presented in Table 2 and Figure 4. Similar to the last section, we use GPT4omini as the base LLM of AgentMove.\nWe first discuss the impact of three core components individually, as detailed in the top four lines of Table 2. We have two important observations: 1) each core component not always works well for all the datasets, they may hurt the performance in some cases. For example, after adding spatial temporal memory design in the base prompt, the performance of AgentMove in Tokyo is decreased significantly, but it performs much better in Moscow, 2) the performance gain of each component in different metrics are also different. For example, while memory design leads to better performance in the Acc@1 in SaoPaulo, the performance in other three metrics are dropped. Then we discuss the effects of the combination of the core components in the last three lines in Table 2. We find that while memory or world model generator can not improve the prediction performance in-"}, {"title": "Geographical Bias Analysis", "content": "While LLMs are trained with the online web text which can be geographically bias (Manvi et al. 2024) around the world. We investigate the potential geographical bias in LLM based mobility prediction methods and attempt to answer whether AgentMove can alleviate the geographical bias inherent in"}, {"title": "Effects of Different Location Representation", "content": "We explore the effects of the format of location ID in the prompt in Figure 7(a), where 'int' denotes to re-encoding the locations into ordered number in each city, 'str' denotes preserve the original string ID of venue in Foursquare. We find that in all the tree datasets, AgentMove with 'int' encoding strategy achieves the similar performance in two metrics compared with the representation of 'str' location ID. This result also demonstrate that the outstanding prediction performance of LLM based methods does not stem from the potential data leakage. Thus, all the results in our experiments use the 'str' encoding as the default encoding strategy."}, {"title": "Related Work", "content": "Significant efforts have been made in mobility prediction using deep learning models, encompassing research from both sequential-based methods and graph-based methods. While traditional methods typically employ Markov models (Rendle, Freudenthaler, and Schmidt-Thieme 2010; Cheng et al. 2013) to predict the next visit by learning the transition probabilities between consecutive POIs, sequential-based deep learning methods are first proposed to model the high-order movement patterns of trajectory data. It contains two kinds of works, Recurrent Neural Networks (RNNs) (Kong and Wu 2018; Huang et al. 2019; Yang et al. 2020; Zhao et al. 2020), and attention mechanisms(Feng et al. 2018; Luo, Liu, and Liu 2021; Lin et al. 2021; Cui et al. 2021; Qin et al. 2022; Hong et al. 2023) based works. Despite their success, these methods primary focus on extracting mobility patterns from an individual perspective, while overlooking the collaborative information available from other users' trajectories. To this end, recent works (Rao et al. 2022; Yang, Liu, and Zhao 2022) have explored the use of Graph Neural Networks (GNNs) for their ability to model complex relationships. However, all these methods rely on collecting large volume of private trajectory. Our proposed AgentMove leverages the world knowledge and sequential modeling abilities of LLM to enable the generalized mobility pre-"}, {"title": "Large Language Models and Agents", "content": "Due to the powerful language based generalization and reasoning capabilities (Wei et al. 2022a), large language models (OpenAI 2022; Touvron et al. 2023) develop rapidly and have been widely applied in various tasks, e.g., coding (Qian et al. 2024) and math (Wei et al. 2022a). Recent works (Gurnee and Tegmark 2023; Manvi et al. 2023) find that LLMs have learned lots of geographical knowledge of the world. Besides, researcher also explore the potential of applying LLMs in spatial temporal data modelling by directly transferring the domain specific task into the language format, e.g., time series forecasting (Gruver et al. 2024), traffic prediction (Li et al. 2024b), trajectory mining (Wang et al. 2023; Beneduce, Lepri, and Luca 2024), trip recommendation (Xie et al. 2024; Li et al. 2024a) and so on. These early works present the potential of applying LLMs in spatial temporal modelling. To effectively utilize the vast knowledge hidden in the LLMs and stimulate their reasoning and planning abilities, lots of prompts techniques (Wei et al. 2022b; Kojima et al. 2022; Wang et al. 2022; Yao et al. 2024) are proposed in solving naive text games and math problem. However, when it comes to the complicated task in the real life and domain-specific tasks, only the prompt techniques are not enough. Recently, LLM based agents (Wang et al. 2024; Xi et al. 2023) are proposed to solve this question by equipping LLMs with explicit memory, working flow and external tools. Here, we are the first to design LLM based agent for mobility prediction task. With explicit spatial temporal memory and work flow of spatial structure and social structure mining, we succeed in leveraging the world knowledge of LLM and structured reasoning capability for spatial temporal trajectory modelling."}, {"title": "Conclusion", "content": "In this paper, we propose AgentMove, a systematic agentic framework for evolvable and generalized mobility prediction around the world. In AgentMove, we first decompose the mobility prediction task into three sub-tasks. To solve these sub-tasks, we design spatial-temporal memory and collective knowledge extractor to learn the individual mobility pattern and shared mobility pattern among users."}, {"title": "Appendix", "content": "As the core foundation of AgentMove, the strength of base LLM's capabilities has a significant impact on the performance of AgentMove. Thus, we evaluate the impact of different LLMs with varying sizes and sources in Figure 6. We consider the influence of LLM size, LLM sources on the performance of three LLM based methods in Figure 6(a) and Figure 6(c). We dive into the detailed impacts of the LLM size and sources on AgentMove among three cities in Figure 6(b) and Figure 6(d).\nWe first investigate how the LMM size affects the performance of LMM-based methods. In Figure 6(a), we find that LLMs with more parameters, such as Llama3-70B, generally outperform smaller model like LLama3-8B in the LLM-based methods, highlighting the advantage of increased model capacity in helping mobility prediction. It is also noted that the performance gain from scaling up the Llama3 is very small. The only exception is that LLM-Mob in LLama3.1-405B performs much worse. After checking the results carefully, we find that this is caused by the wrong output format of LLM-Mob@Llama3.1-405B with too much detailed reasoning steps, which demonstrates the potential instability of the LLM-Mob among various LLMs. In Figure 6(c), we shift our focus to the impact of various 7B LLM with different training data and model structures. The results show that proposed AgentMove performs best adaptability among different LLMs. While LLM-Mob performs stable in all the 7B-LLMs, its performance on Gemma2-9B is far worse than other two methods.\nWe then discuss the detailed impacts of LLM size and source specifically on AgentMove's performance across three cities. Figure 6(b) reveals that that larger models, particularly Llama3.1-405B, generally deliver significant performance gains for AgentMove compared to smaller models like Llama3-8B across different cities. It is also observed that in Tokyo, Llama3-1-405B performs slightly weaker compared to Llama3-70B. This suggests that while larger models often excel, their effectiveness may vary depending on the unique mobility patterns and characteristics of each"}]}