{"title": "AgentMove: Predicting Human Mobility Anywhere Using Large Language Model\nbased Agentic Framework", "authors": ["Jie Feng", "Yuwei Du", "Jie Zhao", "Yong Li"], "abstract": "Human mobility prediction plays a crucial role in various\nreal-world applications. Although deep learning based mod-\nels have shown promising results over the past decade, their\nreliance on extensive private mobility data for training and\ntheir inability to perform zero-shot predictions, have hindered\nfurther advancements. Recently, attempts have been made to\napply large language models (LLMs) to mobility prediction\ntask. However, their performance has been constrained by the\nabsence of a systematic design of workflow. They directly\ngenerate the final output using LLMs, which limits the po-\ntential of LLMs to uncover complex mobility patterns and\nunderestimates their extensive reserve of global geospatial\nknowledge. In this paper, we introduce AgentMove, a sys-\ntematic agentic prediction framework to achieve generalized\nmobility prediction for any cities worldwide. In AgentMove,\nwe first decompose the mobility prediction task into three\nsub-tasks and then design corresponding modules to com-\nplete these subtasks, including spatial-temporal memory for\nindividual mobility pattern mining, world knowledge genera-\ntor for modeling the effects of urban structure and collective\nknowledge extractor for capturing the shared patterns among\npopulation. Finally, we combine the results of three modules\nand conduct a reasoning step to generate the final predictions.\nExtensive experiments on mobility data from two sources in\n12 cities demonstrate that AgentMove outperforms the best\nbaseline more than 8% in various metrics and it shows ro-\nbust predictions with various LLMs as base and also less ge-\nographical bias across cities. Codes and data can be found in\nhttps://github.com/tsinghua-fib-lab/AgentMove.", "sections": [{"title": "Introduction", "content": "Mobility prediction is of great importance in many real-\nworld scenarios, e.g., recommend travel services, pre-\nactivating mobile applications for potential usage, seamless\nswitching of cellular network signals and efficient traffic\nmanagement. In the recent years, deep learning based mod-\nels (Liu et al. 2016; Wu et al. 2017; Feng et al. 2018; Yang\net al. 2020; Yang, Liu, and Zhao 2022) are widely applied\nand has achieved good results due to its advantages in cap-\nturing the high-order transition capabilities and mining the\nshared mobility patterns between users. However, existing\nworks have several key drawbacks. First, the success of deep\nlearning models rely on the collection of large amounts of\nprivate mobility data. Second, the trained model is difficult\nto be applied in the zero-shot mobility prediction settings.\nFinally, the prediction accuracy is still lower due to the lim-\nited sequential modelling capability of small deep learning\nmodels and lack of deeply understanding of common sense\nin human daily life and urban structures.\nRecently, large language models (LLMs) have made sig-\nnificant progress, achieving advanced results far surpass-\ning previous methods in areas such as dialogue based role-\nplaying, code generation and testing, and mathematical\nproblem solving. In the field of spatial temporal data min-\ning, researchers also explore the potential of applying LLMs\nmany real-world tasks, such as time series forecasting (Gru-\nver et al. 2024; Li et al. 2024b), travel planning (Xie et al.\n2024; Li et al. 2024a), trajectory analysis (Luo et al. 2024;\nZhang et al. 2023), and so on. Furthermore, several recent\nworks (Wang et al. 2023; Beneduce, Lepri, and Luca 2024)\ninvestigate the feasibility of employing the LLMs as the base\nmodel of mobility predictor for addressing the previous limi-\ntations of deep learning based models and achieve promising\nresults. These works first convert the trajectory to a language\nbased sentence and then utilize the powerful sequential mod-\nelling capacities of LLMs to directly output the mobility pre-\ndiction results. However, due to the absence of a systematic\ndesign of whole procedure, they ignore the crucial compo-\nnents of human mobility modeling and thus achieve limited\nperformance. In summary, these methods fail to effectively\ncapture the complicated individual mobility pattern, ignore\nmodeling the effects of urban structure and discovering the\nshared mobility patterns among population.\nIn this paper, we propose AgentMove, a systematic agen-\ntic framework for generalized mobility prediction in any\ncities around the world. In AgentMove, by integrating the\ndomain knowledge of human mobility, we implement the\ncore components in the general agentic framework (Wang\net al. 2024; Xi et al. 2023), including the planning mod-\nule, memory module, world knowledge module, external\ntool module and reasoning module. For the planning mod-\nule in AgentMove, we introduce a manually designed mo-\nbility prediction task decomposition module by consider-\ning the most important factors for mobility prediction. Af-\nter decomposition, we generate three sub-tasks: individual\nmobility pattern mining, shared mobility pattern discover-\ning and urban structure modelling. Firstly, we implement\na spatial-temporal memory module for individual mobil-\nity pattern mining. It contains three submodules, short-term"}, {"title": "Preliminaries", "content": "Here, we define the mobility prediction task and related con-\ncepts, which are used in the following section.\nDefinition 1 (POI) A POI point $p \\in P$ is represented as a\ntuple (id, cate, lon, lat, addr), where id is the unique iden-\ntifier, cate is the category (e.g., restaurant), lon and lat are\nthe coordinates of the POI, addr is the text address of POI.\nDefinition 2 (User Trajectory) A trajectory of user $u \\in U$\nis represented as $T_u = \\{(p_1,t_1), (p_2, t_2),..., (p_n, t_n)\\}$,\nwhere $p_i \\in P$ is the i-th POI visited by the user and $t_i$ is\nthe timestamp of the visit.\nDefinition 3 (Contextual Stays) Contextual stays of user u\nis defined as the most recent sub-sequence in trajectory:\n$C_u = \\{(P_{n-k},t_{n-k}),..., (P_{n-1}, t_{n-1}), (P_n, t_n)\\}$, which\ncaptures the user's short-term mobility patterns. k is the\nwindow size of contextual stays.\nDefinition 4 (Historical Stays) Historical stays of user u is\ndefined as the sub-sequence before contextual stays: $H_u =$\n$\\{(P_1,t_1), (p_2, t_2),...,(P_{n-k-1},t_{n-k-1})\\}$, which captures\nthe user's long-term mobility patterns.\nGiven the historical movement data $C_u, H_u$ as well as\navailable external knowledge K (e.g., worldwide geospatial\ninformation), the objective is to predict the next POI $P_{n+1}$\nthat user u will visit. Formally, this paper aims to learn a\nmapping function f:\n$f: (C_u, H_u, K) \\rightarrow P_{n+1}. \\qquad(1)$"}, {"title": "Overview", "content": "As shown in Figure 2, AgentMove consists of five core com-\nponents: task decomposition module, spatial-temporal mem-\nory module, world knowledge generator, collective knowl-\nedge extractor and the final reasoning module. As the high-level planning module, the task decomposition module is\ndesigned to break down the whole mobility prediction task\ninto subtasks-personalized mobility pattern mining, col-\nlective mobility pattern discovery and the effects of urban\nstructures-by considering the crucial factors of mobility.\nDetailed design of other components are introduced as fol-\nlows."}, {"title": "Methods", "content": "Spatial-temporal Memory for Personalized\nMulti-scale Periodicity Behavior Modelling\nThe spatial-temporal memory module is designed to effec-\ntively capture, store and leverage mobility patterns, provid-\ning crucial insights for the personalized and multi-scale pe-\nriodicity behavior modelling in mobility prediction. Inspired\nby the memory design principles in general LLM-based\nagents (Zhang et al. 2024), our spatial-temporal memory\nfunctions through three essential processes: memory orga-\nnization, memory writing, and memory reading. The whole"}, {"title": "World Knowledge Generator for Enhancing\nExploration Behavior Modelling", "content": "Lots of research (Jiang et al. 2016) shows that individual\nmovement typically includes two types of behaviors: re-\nturn and explore. As introduced before, the return behav-\nior has been well-captured by the spatial-temporal mem-\nory module. In this section, we introduce the world knowl-\nedge generator to extract geospatial knowledge from LLMs\nand construct multi-scale urban structure to enable the mod-\neling of explore behavior of mobility. To extract geospatial\nknowledge effectively, we propose to alignment the knowl-\nedge of LLMs and the urban space of trajectory via text ad-\ndress. Once the space is alignment, we explicitly motivate\nthe LLMs to generate the potential explore candidate places\nfrom the multi-scale urban structure view.\nAlignment via Address Many existing works (Feng et al.\n2018; Luo, Liu, and Liu 2021; Lin et al. 2021; Cui et al.\n2021; Qin et al. 2022; Hong et al. 2023) on mobility pre-\ndiction usually represent the locations directly using lati-\ntude and longitude coordinates or discrete spatial area IDs.\nWhile this approach facilitates the easy construction of deep\nlearning-based spatial encoding, it is not suitable for LLMs.\nSince LLMs are trained on large scale human-generated text,\nthey, like human, are not inherently adept at understanding"}, {"title": "Collective Knowledge Extractor for Shared\nMobility Pattern Modelling", "content": "In the previous two sections, we introduce the spatial-temporal memory module and world knowledge generator\nfor the individual level mobility modelling. Here, we intro-\nduce the collective knowledge extractor to enable capture the\nsharing mobility pattern among users to further improve the\nmobility prediction. In one world, we first construct a global\nlocation transition graph using NetworkX 2 by aggregating\nthe location transitions from various users. We then employ\na LLM to perform simple reasoning on the graph, utilizing\nthe function in NetworkX as tool to generate potential loca-\ntions visited by other users with similar mobility pattern.\nBuilding Location Transition Graph In the location\ngraph, the node is location ID with various attributes, e.g.,\naddress information, function of location. The edge between\nnodes is constructed by considering the 1-hop transition be-\ntween nearby trajectory points in each trajectory. The edge is\nweighted without direction. Based on the definition of graph,\nwe use NetWorkX to build the graph from scratch and up-\ndate it when infer trajectories for various users. If any history\ntrajectory data, e.g. training data used by the deep learning\nbased models, are available, the location graph can be ini-\ntialized by them."}, {"title": "Reasoning on Graph via Tool", "content": "After obtaining the loca-\ntion graph, we can utilize LLM to perform reasoning on the\nwhole graph via the function of NetworkX as tool. The most\nnaive strategy is to query the k-hop neighbors of the current\nlocation. When the number of the neighbors is too much,\nLLMs need to filter the most promising ones from them by\nconsidering the attributes of each node. Furthermore, we can\nextend the query nodes into the last n locations and generate\nthe most promising ones from all the neighbors of them. In\nthis way, we obtain the most relevant locations that has been\nvisited by the users with similar mobility patterns.\nFinally, we design prompts to employ LLM to analyze and\nsummarize the information from different views and per-\nform a final reasoning step to generate the prediction with\nreasons. The prompts for output format requirements are\nalso be placed here to ensure that the output format meets\nthe requirements as much as possible."}, {"title": "Settings", "content": "Datasets We use the global Foursquare checkin\ndata (Yang, Zhang, and Qu 2016) and recent public\nreleased ISP trajectory data (Feng et al. 2019) to conduct\nthe experiments. The Foursquare data contains checkins\nfrom 415 cities which covers about 18 months from April\n2012 to September 2013. The ISP trajectory data is from\nShanghai with 325215 records, covering April 19 to April\n26 in 2016. Compared with Foursquare data, ISP data is\nmuch denser and was open-sourced only two months ago 3,\nwhich is beyond the training period of all the LLMs used in\nthe experiment. This ensures that the evaluation results are\nnot affected by potential data leakage.\nTo evaluate the general mobility prediction ability of\nAgentMove, we select 12 cities from the Foursquare dataset\nand the entire ISP trajectory data to conduct the experiments.\nWe follow the preprocessing procedure (Hong et al. 2023;\nFeng et al. 2019) to process the trajectories data. Detailed\ndescription about preprocessing can refer to the appendix.\nWe select the Tokyo, Moscow and SaoPaulo with the largest\namount of Foursquare check-in data and the ISP data from\nShanghai to conduct the main analysis in the experiments\nand results of 12 cities are discussed in the final section of\nexperiment. Due to the cost of the various API calling, e.g.,\nLlama3.1-405B, we randomly sample 200 instances from\nthe testing set for each city to calculate the performance in\nthe experiments.\nBaselines We have three kinds of baselines, Markov meth-\nods, deep learning models and LLM-based methods.\n\u2022 FPMC (Rendle, Freudenthaler, and Schmidt-Thieme\n2010) It combines the matrix factorization and Markov\nchains methods together for sequential modelling.\n\u2022 RNN (Feng et al. 2018) It is a simple RNN based mobil-\nity prediction model as regarding the mobility sequence\nas general sequence."}, {"title": "Main Results", "content": "In this section, we compare AgentMove with 6 baselines in\n4 cities to demonstrate the effectiveness of proposed frame-\nwork. Here, we use GPT4omini as the default base LLM of\nAgentMove and the results of AgentMove with other kinds\nof base LLMs can be found in first section in appendix. To\nensure a fair comparison, the LLM-based baselines, LLM-\nMob and LLM-ZS, also employ the same base LLM as\nAgentMove. The results are presented in Table 1.\nAs the representative deep learning models, DeepMove\nand LSTPM achieve best or second-best results in 5 out of\n12 metrics, and both of their performance varies among dif-\nferent cities. Compared with the deep learning baselines, the\nbest LLM-based baseline LLM-ZS can achieve better re-\nsults than DeepMove and LSTPM in 7 out of 12 metrics.\nThe promising results of LLM-ZS present the powerful se-\nquential pattern discovery and reasoning ability of LLM in\nmodeling mobility, which demonstrate the potential of the\nLLM-based method. It is noted that the results of LLM-based methods are zero-shot prediction while the deep learn-\ning based methods rely on sufficient training with enough\nmobility data. Compared with these baselines, our proposed\nmethod AgentMove is the best method and achieves the best\nresults in 11 out of 12 metrics in 4 datasets. Compared with\nthe deep learning methods, AgentMove outperforms them\nmore than 20% in half of the metrics. Even in the weak-\nest results, it performs very close to the strongest baseline\nDeepMove with less than 3% performance drop. Compared\nwith the advanced LLM-based baselines, AgentMove out-\nperforms both of them more than 5.7%-39.4% in all the"}, {"title": "Ablation Study", "content": "In this section, we conduct ablation study to demonstrate the\ncontribution of each component in AgentMove. Related re-\nsults are presented in Table 2 and Figure 4. Similar to the last\nsection, we use GPT4omini as the base LLM of AgentMove.\nWe first discuss the impact of three core components in-\ndividually, as detailed in the top four lines of Table 2. We\nhave two important observations: 1) each core component\nnot always works well for all the datasets, they may hurt\nthe performance in some cases. For example, after adding\nspatial temporal memory design in the base prompt, the per-\nformance of AgentMove in Tokyo is decreased significantly,\nbut it performs much better in Moscow, 2) the performance\ngain of each component in different metrics are also dif-\nferent. For example, while memory design leads to better\nperformance in the Acc@1 in SaoPaulo, the performance in\nother three metrics are dropped. Then we discuss the effects\nof the combination of the core components in the last three\nlines in Table 2. We find that while memory or world model\ngenerator can not improve the prediction performance in-"}, {"title": "Geographical Bias Analysis", "content": "While LLMs are trained with the online web text which can\nbe geographically bias (Manvi et al. 2024) around the world.\nWe investigate the potential geographical bias in LLM based\nmobility prediction methods and attempt to answer whether\nAgentMove can alleviate the geographical bias inherent in"}, {"title": "Effects of Different Location Representation", "content": "We explore the effects of the format of location ID in the\nprompt in Figure 7(a), where 'int' denotes to re-encoding\nthe locations into ordered number in each city, 'str' denotes\npreserve the original string ID of venue in Foursquare. We\nfind that in all the tree datasets, AgentMove with 'int' encod-\ning strategy achieves the similar performance in two metrics\ncompared with the representation of 'str' location ID. This\nresult also demonstrate that the outstanding prediction per-\nformance of LLM based methods does not stem from the po-\ntential data leakage. Thus, all the results in our experiments\nuse the 'str' encoding as the default encoding strategy."}, {"title": "Related Work", "content": "Mobility Prediction with Deep Learning\nSignificant efforts have been made in mobility prediction us-\ning deep learning models, encompassing research from both\nsequential-based methods and graph-based methods. While\ntraditional methods typically employ Markov models (Ren-\ndle, Freudenthaler, and Schmidt-Thieme 2010; Cheng et al.\n2013) to predict the next visit by learning the transition prob-\nabilities between consecutive POIs, sequential-based deep\nlearning methods are first proposed to model the high-order\nmovement patterns of trajectory data. It contains two kinds\nof works, Recurrent Neural Networks (RNNs) (Kong and\nWu 2018; Huang et al. 2019; Yang et al. 2020; Zhao et al.\n2020), and attention mechanisms(Feng et al. 2018; Luo, Liu,\nand Liu 2021; Lin et al. 2021; Cui et al. 2021; Qin et al.\n2022; Hong et al. 2023) based works. Despite their suc-\ncess, these methods primary focus on extracting mobility\npatterns from an individual perspective, while overlooking\nthe collaborative information available from other users' tra-\njectories. To this end, recent works (Rao et al. 2022; Yang,\nLiu, and Zhao 2022) have explored the use of Graph Neu-\nral Networks (GNNs) for their ability to model complex\nrelationships. However, all these methods rely on collect-\ning large volume of private trajectory. Our proposed Agent-\nMove leverages the world knowledge and sequential model-\ning abilities of LLM to enable the generalized mobility pre-"}, {"title": "Large Language Models and Agents", "content": "Due to the powerful language based generalization and rea-\nsoning capabilities (Wei et al. 2022a), large language mod-\nels (OpenAI 2022; Touvron et al. 2023) develop rapidly\nand have been widely applied in various tasks, e.g., cod-\ning (Qian et al. 2024) and math (Wei et al. 2022a). Recent\nworks (Gurnee and Tegmark 2023; Manvi et al. 2023) find\nthat LLMs have learned lots of geographical knowledge of\nthe world. Besides, researcher also explore the potential of\napplying LLMs in spatial temporal data modelling by di-\nrectly transferring the domain specific task into the language\nformat, e.g., time series forecasting (Gruver et al. 2024),\ntraffic prediction (Li et al. 2024b), trajectory mining (Wang\net al. 2023; Beneduce, Lepri, and Luca 2024), trip recom-\nmendation (Xie et al. 2024; Li et al. 2024a) and so on.\nThese early works present the potential of applying LLMs\nin spatial temporal modelling. To effectively utilize the vast\nknowledge hidden in the LLMs and stimulate their reasoning\nand planning abilities, lots of prompts techniques (Wei et al.\n2022b; Kojima et al. 2022; Wang et al. 2022; Yao et al. 2024)\nare proposed in solving naive text games and math problem.\nHowever, when it comes to the complicated task in the real\nlife and domain-specific tasks, only the prompt techniques\nare not enough. Recently, LLM based agents (Wang et al.\n2024; Xi et al. 2023) are proposed to solve this question by\nequipping LLMs with explicit memory, working flow and\nexternal tools. Here, we are the first to design LLM based\nagent for mobility prediction task. With explicit spatial tem-\nporal memory and work flow of spatial structure and social\nstructure mining, we succeed in leveraging the world knowl-\nedge of LLM and structured reasoning capability for spatial\ntemporal trajectory modelling."}, {"title": "Conclusion", "content": "In this paper, we propose AgentMove, a systematic agen-\ntic framework for evolvable and generalized mobility pre-\ndiction around the world. In AgentMove, we first decom-\npose the mobility prediction task into three sub-tasks. To\nsolve these sub-tasks, we design spatial-temporal memory\nand collective knowledge extractor to learn the individual\nmobility pattern and shared mobility pattern among users."}, {"title": "Appendix", "content": "The Impact of Different LLMS\nAs the core foundation of AgentMove, the strength of base\nLLM's capabilities has a significant impact on the perfor-\nmance of AgentMove. Thus, we evaluate the impact of dif-\nferent LLMs with varying sizes and sources in Figure 6. We\nconsider the influence of LLM size, LLM sources on the per-\nformance of three LLM based methods in Figure 6(a) and\nFigure 6(c). We dive into the detailed impacts of the LLM\nsize and sources on AgentMove among three cities in Fig-\nure 6(b) and Figure 6(d).\nWe first investigate how the LMM size affects the per-\nformance of LMM-based methods. In Figure 6(a), we find\nthat LLMs with more parameters, such as Llama3-70B,\ngenerally outperform smaller model like LLama3-8B in\nthe LLM-based methods, highlighting the advantage of in-\ncreased model capacity in helping mobility prediction. It is\nalso noted that the performance gain from scaling up the\nLlama3 is very small. The only exception is that LLM-\nMob in LLama3.1-405B performs much worse. After check-\ning the results carefully, we find that this is caused by the\nwrong output format of LLM-Mob@Llama3.1-405B with\ntoo much detailed reasoning steps, which demonstrates the\npotential instability of the LLM-Mob among various LLMs.\nIn Figure 6(c), we shift our focus to the impact of various 7B\nLLM with different training data and model structures. The\nresults show that proposed AgentMove performs best adapt-\nability among different LLMs. While LLM-Mob performs\nstable in all the 7B-LLMs, its performance on Gemma2-9B\nis far worse than other two methods.\nWe then discuss the detailed impacts of LLM size and\nsource specifically on AgentMove's performance across\nthree cities. Figure 6(b) reveals that that larger models, par-\nticularly Llama3.1-405B, generally deliver significant per-\nformance gains for AgentMove compared to smaller models\nlike Llama3-8B across different cities. It is also observed\nthat in Tokyo, Llama3-1-405B performs slightly weaker\ncompared to Llama3-70B. This suggests that while larger\nmodels often excel, their effectiveness may vary depending\non the unique mobility patterns and characteristics of each"}, {"title": "Preprocessing for ISP Data", "content": "Following the preprocessing in the original paper (Feng et al.\n2019), we split the data into different sessions by merging\ntrajectory points in the same day. Due to the regularity of\nhuman, there are too much repeated trajectory points in the\noriginal sessions. To make the prediction challenging, we\ncompress the trajectory sessions by merging the same loca-\ntions within a time window (2 hours) and ignoring the visit-\ning occurred during the night (from 8 p.m. to 8 a.m.). While\nthe ISP data lasts only 7 days, we split the whole data into\ntraining set, validation set and testing data in a ratio of 4:1:5\nfor preserving enough testing data. The minimum session\nfilter parameter is changed from 3 to 1."}, {"title": "Data preprocessing", "content": "Preprocessing for Foursquare Data\nAs introduced in section, we select 12 cities around the\nworld to evaluate the performance of proposed framework.\nWe match each trajectories with the target cities by calculat-\ning the minimum distance to the city center. For the ordered\ntrajectories in each city, we use 72 hours as the time window\nto split the trajectory into sessions. We filter the users with\nless than 5 sessions and filter sessions with less than 4 stays.\nThen, we divide each trajectory dataset into training, valida-\ntion, and test sets in a ratio of 7:1:2. During the testing, we\nfilter the users with less than 3 sessions or more than 50 ses-\nsions which is designed to ensure the quality of testing users\nand also balance the effects from different users. Different\nfrom the previous works, we do not specifically filter loca-\ntions. All the users and trajectories of them are sorted by the\nid. We select one session of each user and aggregate the first"}, {"title": "Prompt Examples", "content": "Here, we present the detailed prompts for each LLM based\nmethods.\nPrompt of AgentMove\n1 ## Task\n2 Your task is to predict <next_place_id> in\n3  <target_stay>, a location with an\n unknown ID, while temporal data is\n available.\n4 ## Predict <next_place_id> by considering:\n5 1. The user's activity trends gleaned from\n<historical_stays> and the current\n activities from <context_stays>."}]}