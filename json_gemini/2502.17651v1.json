{"title": "METAL: A Multi-Agent Framework for Chart Generation with Test-Time Scaling", "authors": ["Bingxuan Li", "Yiwei Wang", "Jiuxiang Gu", "Kai-Wei Chang", "Nanyun Peng"], "abstract": "Chart generation aims to generate code to produce charts satisfying the desired visual properties, e.g., texts, layout, color, and type. It has great potential to empower the automatic professional report generation in financial analysis, research presentation, education, and healthcare. In this work, we build a vision-language model (VLM) based multi-agent framework for effective automatic chart generation. Generating high-quality charts requires both strong visual design skills and precise coding capabilities that embed the desired visual properties into code. Such a complex multi-modal reasoning process is difficult for direct prompting of VLMs. To resolve these challenges, we propose METAL (Multi-agEnT framework with vision Language models for chart generation), a multi-agent framework that decomposes the task of chart generation into the iterative collaboration among specialized agents. METAL achieves 5.2% improvement in accuracy over the current best result in the chart generation task. The METAL framework exhibits the phenomenon of test-time scaling: its performance increases monotonically as the logarithmic computational budget grows from 512 to 8192 tokens. In addition, we find that separating different modalities during the critique process of METAL boosts the self-correction capability of VLMs in the multimodal context.", "sections": [{"title": "Introduction", "content": "Data visualization through charts is an important part of the communication and research life cycle. Well-designed visualizations help distill complex data into digestible insights, allowing researchers, analysts, and stakeholders to identify relationships that might remain hidden in raw data (Qin et al., 2020; Xu et al., 2023; Yang et al., 2024). Recent advancements in vision language models (VLMs), such as GPT-4V (OpenAI, 2023) and LLaVA (Li et al., 2024), have expanded the capabilities of language models in tackling complex multi-modal problem-solving tasks. These breakthroughs have sparked growing interests in designing intelligent AI assistants to help humans with limited coding expertise create compelling charts, leading to the emergence of a complex multi-modal generation task with crucial practical value - Chart to code generation task (Wu et al., 2024a; Han et al., 2023; Shi et al., 2024). The chart to code generation task focuses on automatically generating visualization code based on visual references. This task embodies a highly challenging visually-grounded code generation problem that demands robust visual understanding and advanced reasoning. The model must interpret complex visual elements\u2014such as layouts, color schemes, and data relationships\u2014and translate them into syntactically correct, semantically meaningful code. Successfully addressing this challenge not only improves chart replication but also paves the way for advancing the general capabilities of VLMs in multimodal learning and program synthesis. In this paper, we present METAL, a multi-agent framework designed for chart generation. Our framework decomposes this complex multimodal reasoning task into four specialized roles, each handled by a specialized agent: (1) Generation Agent: Responsible for the initial translation of chart images into the corresponding code. (2) Visual Critique Agent: Analyzes and identifies visual differences between the reference chart and the generated output. (3) Code Critique Agent: Reviews the generated code and suggests improvements to better match the reference chart. (4) Revision Agent: Implements code modifications based on the combined feedback from both critique agents. During inference, these agents collaborate iteratively, critiquing and refining the code until the rendered chart achieves the desired accuracy. As illustrated in Figure 1, current state-of-the-art VLMs, such as GPT-40, often fail to accurately interpret and reproduce the intricate visual elements and relationships embedded in reference charts. Existing solutions, such as Best-of-N and Hint-enhanced (Wang et al., 2024), have not effectively improved upon direct prompting of VLMs. The core challenge in leveraging VLMs for chart generation lies in effectively integrating visual comprehension with code synthesis. This complex task exceeds the capabilities of the single model or single agent. In contrast to existing methods, our approach delivers more concrete and targeted feedback, and iteratively refines outputs through the multi-agentic framework, leading to enhanced chart generation performance. Experiment results show that our framework improves chart generation accuracy by over 11.33%, demonstrating its potential to significantly enhance VLMs' ability to integrate visual understanding with code synthesis. Furthermore, we have two key findings through in-depth analysis: (1) Test-time scaling in the METAL framework: We found that there is a near-linear relationship between the performance and the logarithm of the computational budget in experiments. Specifically, the performance of METAL increases monotonically as the logarithm of the computational budget grows from 512 to 8192 tokens. (2) Modality-tailored critiques enhance self-correction: We observe that explicitly separating different modalities during the critique process\u2014such as visual evaluation and code analysis\u2014substantially enhances the multimodal self-correction capabilities of VLMs. Ablation study shows that METAL with the separate-critique design achieves a 5.16% improvement over the single-critique baseline. In summary, we present METAL, a VLMs-based multi-agent framework, which achieves significant improvements over the current best methods for chart generation, and our insights into test-time scaling and multi-modal critique offer a promising pathway for enhanced visually-grounded code generation with VLMs."}, {"title": "Related Works", "content": "We discuss three lines of related work: chart-to-code generation, multi-agent framework, and test-time scaling research."}, {"title": "Chart Generation with VLMS", "content": "Chart generation, or chart-to-code generation, is an emerging task aimed at automatically translating visual representations of charts into corresponding visualization code (Shi et al., 2024; Wu et al., 2024a). This task is inherently challenging as it requires both visual understanding and precise code synthesis, often demanding complex reasoning over visual elements. Recent advances in Vision-Language Models (VLMs) have expanded the capabilities of language models in tackling complex multimodal problem-solving tasks, such as visually-grounded code generation. Leading proprietary models, such as GPT-4V (OpenAI, 2023), Gemini (Google, 2023), and Claude-3 (Anthropic, 2024), have demonstrated impressive capabilities in understanding complex visual patterns. The open-source community has contributed models like LLaVA (Xu et al., 2024; Li et al., 2024), Qwen-VL (Bai et al., 2023), and DeepSeek-VL (Lu et al., 2024), which provide researchers with greater flexibility for specific applications like chart generation. Despite these advancements, current VLMs often struggle with accurately interpreting chart structures and faithfully reproducing visualization code."}, {"title": "Multi-Agents Framework", "content": "Many researchers have suggested a paradigm shift from single monolithic models to compound systems comprising multiple specialized components (Zaharia et al., 2024; Du and Kaelbling, 2024). One prominent example is the multi-agent framework. LLMs-driven multi-agent framework has been widely explored in various domains, including narrative generation (Huot et al., 2024), financial trading (Xiao et al., 2024), and cooperative problem-solving (Du et al., 2023). Our work investigates the application of multi-agent framework to the visually-grounded code generation task."}, {"title": "Test-Time Scaling", "content": "Inference strategies have been a long-studied topic in the field of language processing. Traditional approaches include greedy decoding (Teller, 2000), beam search (Graves, 2012), and Best-of-N. Recent research has explored test-time scaling law for language model inference. For example, Wu et al. (2024b) empirically demonstrated that optimizing test-time compute allocation can significantly enhance problem-solving performance, while Zhang et al. (2024) and Snell et al. (2024) highlighted that dynamic adjustments in sample allocation can maximize efficiency under compute constraints. Although these studies collectively underscore the promise of test-time scaling for enhancing reasoning performance of LLMs, its existence in other contexts, such as different model types and application to cross-modal generation, remains under-explored."}, {"title": "Method", "content": "In this section, we introduce our method for generating precise chart representations from a given reference chart. Section 3.1 formally defines the task, Section 3.2 outlines the components of our proposed approach METAL, and Section 3.3 presents the inference process of METAL."}, {"title": "Task Definition", "content": "Given a reference chart image xref and a chart generation model, the objective is to learn the mapping\n\\(f: x_{ref} \\rightarrow y,\"\nwhere y is a programmatic specification (e.g., Python code). When executed, y should render a chart O(y) that faithfully replicates the reference xref."}, {"title": "\u039c\u0395\u03a4AL", "content": "As illustrated in Figure 2, METAL is structured with four specialized agents (G, C, V, R) and a multi-criteria verifier. All components collaborate together to iteratively refine the final output, making it more accurately replicates the reference chart. The framework is composed as follows:\nGeneration Agent (G) This agent is tasked to generate an initial program from the reference:\n\\(y_0 = G(x_{ref}), G : X \\rightarrow Y.\"\nThis serves as the basis for further refinement.\nVisual Critique Agent (V) This agent is tasked to assess the rendered chart O(yt) against xref to detect visual discrepancies:\n\\(v_t = V(O(y_t), x_{ref}), V : O \\times X \\rightarrow V.\"\nHere, O represents the space of visual outputs, and V denotes the space of visual feedback metrics.\nCode Critique Agent (C) This agent is tasked to review the generated code and provide structured critique to improve the generated code:\n\\(c_t = C(y_t), C : Y \\rightarrow C.\"\nC represents the set of code critique messages ensuring correctness and efficiency.\nRevision Agent (R) This agent integrates feedback from both critique agents to update the generated code:\n\\(y_{t+1} = R(y_t, v_t, c_t), R : Y \\times V \\times C \\rightarrow Y.\"\nMulti-Criteria Verifier We design a heuristic-based verification function to evaluate the chart quality. Let mj be the verification metrics for j = 1, 2, 3, and let ot be dynamic thresholds. Then,\n\\(Q_t(O(y_t), x_{ref}) =\n\\begin{cases}\n1, & \\text{if } \\forall_{j=1}^{3} m_j(O(y_t), x_{ref}) \\geq \\theta\\\\\n0, & \\text{otherwise.}\n\\end{cases}\"\nMore details on the implementation of the verifier, such as each verification metric, are introduced in Appendix A.2."}, {"title": "Inference Procedure", "content": "During inference, METAL iteratively refines the generated code until the rendered chart meets a predefined quality threshold. The refinement process is as follows:\n\\(y_0 = G(x_{ref}),\" (1)\n\\(v_t = V(O(y_t), x_{ref}),\" (2)\n\\(c_t = C(y_t),\" (3)\n\\(y_{t+1} = R(y_t, v_t, c_t).\" (4)\nThe iterations terminate when\n\\(Q_t(O(y_t), x_{ref}) < \\epsilon,\"\nwhere \\epsilon > 0 is the predefined threshold for the discrepancy between the generated chart and the reference chart."}, {"title": "Experiments", "content": "In this section, we systematically evaluate METAL. Section 4.1 details the experimental setup, Section 4.2 presents the results, and Section 4.3 provides an ablation study to further elucidate the model's performance."}, {"title": "Experiment Setup", "content": "Dataset We select the ChartMIMIC dataset to evaluate METAL. It is a benchmark that includes 1,000 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains. These charts span 18 regular types and 4 advanced types, diversifying into 191 subcategories (Shi et al., 2024) . Automatic Evaluation Metric Following the approach in (Shi et al., 2024), we assess four key low-level chart elements: text, layout, type, and color. During code execution, relevant information for each element is logged for evaluation. We then compare each element in the generated chart to its counterpart in the reference chart and calculate the F1 score. Note that the evaluation metric used here differs from the multi-criteria verification metric described in the section 3. Base Model We assess the effectiveness of METAL on both open-source and closed-source vision-language models. Specifically, we evaluate GPT-40 (Hurst et al., 2024) and LLAMA 3.2-11B (AI@Meta, 2024). The details of model size and computation budget are introduced in Appendix B. Implementation Details The implementation details of each component of METAL and the baselines are described in Appendix A. We include the prompt templates for each VLM-driven agent and the baselines in Appendix C."}, {"title": "Experiment Results", "content": "The primary research question of the experiment was to assess whether our proposed method could improve the performance of the base model in the chart generation task. Table 1 presents the experiment result. For the LLAMA base model, the results indicate that the performance of baseline methods varied moderately. Direct Prompting and Hint-Enhanced Prompting achieved average F1 scores of 40.45% and 41.33%, respectively, while Best-of-N reached 43.13%. In contrast, METAL yielded an average F1 score of 51.78% with improvements observed in each metric. The average F1 score improves by 11.33% over Direct Prompting with 5 test-time compute recurrences, which is a significant improvement. Similarly, for the GPT base model, the baseline methods demonstrated high performance with average F1 scores ranging from 81.12% to 81.70%. However, METAL outperformed these methods by a considerable margin, achieving an average F1 score of 86.46%. Specifically, our method improved 5.2% in average over Direct Prompting. These results clearly demonstrate that our approach consistently improves performance across both base models and all evaluation metrics. In particular, the significant gains observed in the Text and Layout metric, along with the overall increase in average F1 scores, indicate that our model effectively captures both the structural attributes and finer details of visual data. This enhancement not only boosts the performance of both open-source and closed-source vision-language models but also maintains a high level of consistency across all metrics, underscoring the robustness and generalizability of our method."}, {"title": "Ablation Study", "content": "To further analyze the impact of different components, we performed an ablation study by selectively removing key elements of METAL to assess the influence of each component on the overall performance."}, {"title": "Analysis and Discussion", "content": "We analyze the experimental results in this section. We highlight two interesting findings: Test-time scaling in Multi-Agent system (Section 5.1), and modality-tailored critiques enhance the self-correction ability (Section 5.2). Additionally, we discuss the advantage of METAL (Section 5.3), and the benefit of agentic design (Section 5.4)"}, {"title": "Test-Time Scaling", "content": "We investigate the relationship between the test-time computational budget and model performance. As illustrated in Figure 3, our analysis reveals an interesting trend: increasing the logarithm of the computational budget leads to continuous performance improvements. This near-linear relationship indicates the test-time scaling phenomenon, demonstrating that allowing more iterations during inference could potentially enhance performance. One potential reason for this phenomenon is the strong self-improvement capability of METAL. Our framework is designed so that specialized agents iteratively collaborate, allowing each agent to refine its output based on feedback from others. With each iteration, errors are corrected and insights from different modalities are integrated, leading to incremental performance gains. This continual refinement process leverages the strengths of individual agents, resulting in the self-improvement capability that drives the observed performance enhancements as computational resources increase. Due to limited resources, we have not extended the experiment range further. However, the observed scaling implies that the framework can benefit from more iterations of collaborative self-improvement. We leave a more comprehensive exploration of this potential to the future work."}, {"title": "Modality-Tailored Critiques", "content": "From the ablation study result shown in Table 2, we observed that separating visual and code critiques enhances the model's self-correction capabilities. In contrast, METALS struggles to effectively self-improve in the chart-to-code generation task. We identify two potential reasons for this observation. First, combining both visual and code inputs results in an extended context that can overwhelm the model, leading to information loss. This dilution makes it difficult to capture key details from each modality, resulting in less accurate critiques and a reduction in overall self-correction effectiveness. Second, the self-critique process for chart generation involves distinct requirements: visual data demands spatial understanding, color analysis, and fine detail recognition, while code data requires strict adherence to syntax and logical consistency. A unified critique approach is ill-suited to address these differing needs. Without modality-specific feedback, the model struggles to detect and correct errors unique to each data type. These findings suggest that self-correction in the multimodal context can be enhanced by leveraging tailored critique strategies for each modality."}, {"title": "Why METAL", "content": "We believe METAL provides three advantages. First, by assigning specialized tasks to individual agents, the system effectively reduces error propagation. During inference, each agent evaluates whether to take action based on the available information and insights from other agents. This process enables each agent to serve as a safeguard, detecting and correcting mistakes before they escalate. Second, the modular design of METAL enables easy modification and adaptation. For instance, one can integrate different base models tailored for specific tasks\u2014such as employing a critique-trained model for critique agents and a generation-trained model for generation agents\u2014to maximize overall performance. Third, METAL is robust with the strong base model. Figure 4 compares the performance of METAL to that of Direct Prompting over five iterations across varying chart difficulty levels. METAL with the GPT-40 base model achieved consistent improvements regardless of difficulty. When using LLAMA 3.2-11B as the base model, the performance gains tend to diminish with increasing reference chart complexity, but the improvements remain substantial. This drop might be due to the limited critique capabilities of the LLAMA 3.2-11B base model. Nonetheless, the flexibility of METAL to replace the base model for different agents allows us to tailor the system optimally-using, for example, a critique-optimized model for critique agents and a generation-focused model for generation agents\u2014to maximize overall performance."}, {"title": "Multi-Agent System vs. Modular System", "content": "We further investigate the impact of agentic behavior of METAL on final performance. We think self-decision-making and code execution abilities are key features that distinguish the multi-agent system from a modular system. We implement a self-revision modular system without these two key abilities, and conduct an additional ablation study on a subset of 50 data points to examine the impact of these agentic behaviors on final performance. The results show that, compared to METAL, there is a 4.51% reduction in average performance gain over direct prompting. The absence of decision-making and code execution abilities in the modular system hinders its capacity to refine generated charts effectively. Specifically, the inability to execute code for chart rendering significantly diminishes the quality of the critique, and the absence of self-decision-making ability potentially leads to error propagation that further negatively impacts the self-correction process. This comparison underscores the critical role of the agentic approach."}, {"title": "Case Study", "content": "We perform a case study to better understand METAL. Figure 5 illustrates an example. In Round 1, two specialized critique agents analyze the generated chart. The visual critique agent detects inconsistencies in axis scaling and missing annotations, while the code critique agent identifies the corresponding code-level issues (e.g. incorrect tick intervals and absent annotations ). Based on these critiques, the revision agent modifies the chart by adjusting the Y-axis scale and adding the missing annotation. These corrections result in a significant improvement, reaching perfect text accuracy, though color accuracy remains unchanged. In Round 2, the critique agents further refine the chart. The visual critique agent highlights inaccuracies in the color assignments of distributions, noting that the generated chart does not precisely match the reference chart's colors. The code critique agent pinpoints the exact color discrepancies in the code and provides specific RGB values for correction. The revision agent incorporates these insights, adjusting the color specifications in the code. This final revision achieves perfect alignment with the reference chart, with 100% accuracy across all evaluation metrics. This case study demonstrates the effectiveness of METAL's multi-agent collaborative refinement process. By decomposing the task into distinct stages, METAL can iteratively enhance the generated output. The separation of visual and code critiques ensures that both perceptual and implementation-level issues are systematically identified and addressed."}, {"title": "Conclusion", "content": "In conclusion, we introduce METAL, a novel multi-agent framework that significantly enhances VLMs' performance in the chart generation task. We also reveal two interesting insights from the experiment: the test-time scaling phenomenon in the multi-agent context, and enhanced self-correction with modality-tailored critiques."}, {"title": "Limitation", "content": "Our work is not without limitations. First, our METAL is based on VLMs, which require extensive prompt engineering. Although we selected the best-performing prompts available, it is possible that even more effective prompts could further enhance our results. Second, automatic evaluations have inherent imperfections and may not capture all details in the chart perfectly. We adopted the evaluation metric from previous work to ensure fairness. Third, METAL has higher costs than direct prompting. Future work could explore how to optimize these costs."}, {"title": "Implementation", "content": "In this section, we present the detailed implementation of our approach."}, {"title": "VLMs driven agent Agents", "content": "In our implementation, we leverage VLMS to drive agents. The agents are designed to process and generate multimodal information as follows:\nGeneration Agent and Visual Critique Agent: Both the Generation Agent and the Visual Critique Agent are designed to handle multimodal inputs. Specifically, they take as input a combination of visual data (e.g., the reference chart image or rendered chart) and textual descriptions. These agents are implemented using VLM architectures that can effectively integrate and reason over both image and text modalities. Their outputs are generated in the form of text, which provides either the initial code (in the case of the Generation Agent) or detailed visual discrepancy feedback (in the case of the Visual Critique Agent).\nCode Critique Agent and Revision Agent: In contrast, the Code Critique Agent and the Revision Agent are fully text-based. They accept textual inputs\u2014either the generated code or the code accompanied by critique feedback\u2014and produce textual outputs. Both agents are configured to generate responses up to approximately 600 tokens.\nIntegration of Agents: The agents interact in an iterative pipeline, where the Generation Agent first produces an initial code snippet. The Visual Critique Agent then examines the rendered output for any discrepancies relative to the reference chart, while the Code Critique Agent inspects the code for logical or syntactic issues. Finally, the Revision Agent integrates the feedback from both critique agents to modify the code. We have a Multi-Criteria Verifier (described in Appendix A.2 to verify the output of each iteration."}, {"title": "Multi-Criteria Verifier", "content": "We design three heuristic-based criteria\u2014color, text, and overall\u2014to assess the similarity between two images. The process begins by using EasyOCR to extract text from both the golden and generated images, and then computing a text similarity score based on the Jaccard index of the extracted text sets. In parallel, a verification from the color aspect is performed by converting the images into the HSV color space and applying predefined color ranges to count the pixels corresponding to specific colors; the resulting color histograms are compared using cosine similarity. Finally, an overall similarity measure is obtained by resizing the grayscale versions of the images and calculating the Structural Similarity Index (SSIM). The final verification result is a combination of these three metrics, providing a comprehensive assessment of image equivalence. During the inference, the iteration will stop if the average of verification results exceeds the predefined threshold. The complete implementation code is attached as follows."}, {"title": "Model Size and Computational Requirement", "content": "We have developed two versions of METAL, each built upon a different foundational model to cater to varying operational needs. For the version using the GPT-40 base model, we integrate the model via the OPENAI API. In this setup, each of the four agents makes one API call per action. One single iteration\u2014where each agent acts once-results in 4 API calls in total. In our main experiments, we perform up to 5 iterations per trial. Alternatively, the LLAMA 3.2-11B-based version of METAL is hosted locally on two NVIDIA A100 Tensor Core GPUs, each with 40 GB of GPU memory. Each of the four agents runs its own instance of the LLAMA 3.2-11B model, leading to an overall GPU memory requirement of approximately 70 GB."}, {"title": "Prompt Templates", "content": "METAL This section lists all prompt templates used in METAL. ## Generation Agent ## # Note: The generation prompt template is adapted from ChartMIMIC. generation_prompt_template = \"You are an expert Python developer who specializes in writing matplotlib code based on a given picture. I found a very nice picture in a STEM paper, but there is no corresponding source code available. I need your help to generate the Python code that can reproduce the picture based on the picture I provide. \\nNote that it is necessary to use figsize ={figsize} to set the image size to match the original size. \\nNow, please give me the matplotlib code that reproduces the picture below.\" ## Visual Critique Agent ## visual_critique_prompt_template = \"You are a professional data scientist tasked with evaluating a generated chart against a reference chart. Objective: Please identify whether there is (are) issue (s) in the generated chart that diverge from the reference chart concerning the {lowest_metric}, and give concrete feedback. Compare the original chart (left) and the generated chart (right) in the provided image. Provide a detailed critique of how the generated chart diverges from the reference chart concerning the {lowest_metric}. Avoid commenting on unrelated metrics or general stylistic choices unless they directly affect the {lowest_metric}. Only critique on elements that in reference chart but not in generated chart. For example, if the reference chart doesn't have a title, don't critique the title in the generated chart. Instructions: {instructions} Response Format: 1. Observation (Reference Chart): Identify the chart elements in the reference chart. 2. Observation (Generated Chart): Identify the chart elements in the generated chart. 3. Critique: Issues in the generated chart that diverge from the reference chart concerning the {lowest_metric}. Be specific and detailed. If there is numeric value or text, please provide the exact value or text. Note: If you believe there is no issue, please respond with SKIP. \" ## Code Critique Agent ## code_critique_prompt_template = \"You are a professional data scientist tasked with analyzing input code and adding targeted TODO comments based on the provided critique. Instructions: Please identify whether there is (are) issue (s) in the input code that need to be addressed based on the visual critique, and give concrete feedback. Identify the specific issues raised in the critique. Offer clear and actionable suggestions to address the identified issues. Insert TODO comments directly into the input code to indicate necessary changes. Place the TODO comment above the line of code that requires modification. Be specific and practical in the TODO comments. Avoid generic suggestions or additions unrelated to the critique. Do not make changes beyond adding TODO comments to the code, such as change existing code or add new lines of code. Do not modify the code or add comments about code style, unrelated improvements, or hypothetical enhancements outside the critique's scope. Do not mention reference charts in the TODO comments, making the comment self-contained. Response Format: 1. Issues: Summarize the issues identified in the critique. 2. Suggestions: Provide specific suggestions for addressing the issues. 3. Full Code with added TODO Comments: Present the input code with the TODO comments added above the relevant lines. Please ONLY add TODO comments to the input code, do not modify the code in any other way. Critique: {critique} Code to Comment On: python {code} \" ## Revision Agent ## code_revision_prompt_template = \"You are a professional data scientist tasked with revising the input code. Objective: The input code contains TODO comments that need to be addressed. Please carefully review the code and make the necessary revisions to address the TODO comments. Each comment might need more than one line of code to address. Match the other lines of code style and structure in the input code. Ensure that the revised code is correct and functional. Return the FULL revised code to ensure the code is ready for the next stage of development. Response Format: Full Code of the Revised Version: Present the revised code with the changes made to address the TODO comments. Code to Revise: python {code} \""}, {"title": "Variations", "content": "This section lists all prompt templates used in variations from the ablation study. # Variations # 1. Metal-s: GenerationAgent, SingleCritiqueAgent, RevisionAgent, VerificationAgent # 2. Metal-v: GenerationAgent, VisualCritiqueAgent, VisualRevisionAgent, VerificationAgent # 3. Metal-c: GenerationAgent, TextCritique VisualAgent, RevisionAgent, VerificationAgent ## SingleCritiqueAgent (Metal-s) ## SingleCritiqueAgent_prompts_template = \" You are a professional data scientist tasked to critique the generated chart against a reference chart to improve the code for generating the chart. Objective: There is (are) issue (s) in the generated chart that diverge from the reference chart regarding the {lowest_metric}. Compare the original chart (left) and the generated chart (right) in the provided image. Observe the differences between the reference chart and the generated chart, and provide a detailed critique of the generated chart. Insert TODO comments directly into the input code to indicate necessary changes. Place the TODO comment above the line of code that requires modification. Be specific and practical in the TODO comments. Avoid generic suggestions or additions unrelated to the critique. Instructions: Step1-2: Avoid commenting on unrelated metrics or general stylistic choices unless they directly affect the {lowest_metric}. Only critique on elements that in reference chart but not in generated chart. For example, if the reference chart doesn't have a title, don't critique the title in the generated chart. Here are insturction for the chart critique: {chart_instructions} Step3-4: If the chart critique specifies particular color values, include a TODO comment in the code to remove the opacity setting (eg. alpha). This ensures the color is accurately replaced with the specified values from the critique If the chart critique identifies incorrect or missing text (of label, annotation, etc.), please include the correct text in the TODO comment for easy reference. If the chart critique identifies the mismatched type of chart, please add TODO comments above the type- related functions calls to correct the chart type. Response Format: 1. Chart Critique: Issues in the generated chart that diverge from the reference chart. Be specific and detailed. 2. Code Critique: Provide a critique of the code that generated the chart. Identify the issues and suggest improvements by adding TODO comments 3. Full Code with added TODO Comments: Present the input code with the TODO comments added above the relevant lines. Do not modify the code directly. Code to Comment On: python {code} \" ## VisualRevisionAgent (Metal-v) ## VisualRevisionAgent_prompt_tempate =\""}, {"title": "Baselines", "content": "This section lists all prompt templates used in baselines. ## HintEnhanced Baseline ## # Note: This prompt template is adapted from ChartMIMIC. hint_enhanced_prompt_template = \"You are an expert Python developer who specializes in writing matplotlib code based on a given picture. I found a very nice picture in a STEM paper, but there is no corresponding source code available. I need your help to generate the Python code that can reproduce the picture based on the picture I provide.\\n\\nTo ensure accuracy and detail in your recreation, begin with a comprehensive analysis of the figure to develop an elaborate caption.\\nThis caption should cover, but not be limited to, the following aspects :\\n1. Layout Analysis: e.g., identify the picture's composition, noting the presence and arrangement of any subplots.\\n2. Chart Type Identification: e.g., determine how many charts within a subplot. Are they independent, or do they share a common axis?\\n3. Data Analysis: e.g"}]}