{"title": "TimePFN: Effective Multivariate Time Series Forecasting with Synthetic Data", "authors": ["Ege Onur Taga", "M. Emrullah Ildiz", "Samet Oymak"], "abstract": "The diversity of time series applications and scarcity of domain-specific data highlight the need for time-series models with strong few-shot learning capabilities. In this work, we propose a novel training scheme and a transformer-based architecture, collectively referred to as TimePFN, for multivariate time-series (MTS) forecasting. TimePFN is based on the concept of Prior-data Fitted Networks (PFN), which aims to approximate Bayesian inference. Our approach consists of (1) generating synthetic MTS data through diverse Gaussian process kernels and the linear coregionalization method, and (2) a novel MTS architecture capable of utilizing both temporal and cross-channel dependencies across all input patches. We evaluate TimePFN on several benchmark datasets and demonstrate that it outperforms the existing state-of-the-art models for MTS forecasting in both zero-shot and few-shot settings. Notably, fine-tuning TimePFN with as few as 500 data points nearly matches full dataset training error, and even 50 data points yield competitive results. We also find that TimePFN exhibits strong univariate forecasting performance, attesting to its generalization ability. Overall, this work unlocks the power of synthetic data priors for MTS forecasting and facilitates strong zero- and few-shot forecasting performance.", "sections": [{"title": "1 Introduction", "content": "Natural language processing has achieved remarkable success driven by advances in neural architectures and data pipelines. These advances underlie modern language and vision-language models that exhibit remarkable zero-shot and few-shot learning capabilities. Inspired by these, researchers have started exploring whether such methods and ideas could be extended to time series forecasting. For instance, a notable line of work (Zhou et al. 2021; Wu et al. 2021; Zhou et al. 2022; Zhang and Yan 2023) examine the use of transformer architecture (Vaswani et al. 2017) in time-series forecasting. More recently, there is also a push toward building foundation models for time series tasks (Ansari et al. 2024). However, the heterogeneous nature of time series data brings additional complications. As shown by (Zeng et al. 2023), even simple linear models are shown to outperform most existing transformer-based models in univariate and multivariate time-series forecasting. This could be attributed to the heterogeneous nature of time-series data and relatively naive tokenization methods, underscoring the need for richer datasets as well as more effective architectures that can capture both temporal and cross-channel dependencies.\nIn language models, the discrete nature of the problem makes the tokenization fairly straightforward, which is in contrast to the continuous time series data. Additionally, the scalar value of a time series datapoint have no clear meaning, unlike words, where vector embeddings can capture semantic similarity. To address these problems, PatchTST (Nie et al. 2023) proposed using patching with overlapping strides and demonstrated its benefit for univariate forecasting. While PatchTST treats multivariate forecasting as multiple univariate problems, iTransformer (Liu et al. 2023) proposes representing each channel as a single token, resulting in an architecture that intuitively augments simple linear layers with a transformer architecture.\nIn this work, we approach MTS forecasting from a data-centric perspective. While various architectural considera-tions have been incorporated into the forecasting process, we argue that the data aspect is relatively underappreciated. Existing transformer-based MTS approaches focus on the classical learning setup where a model is trained and tested on the same task. Although this often results in satisfactory performance for large datasets, it is likely to underperform in real-world applications where the training set is small or test set is out-of-distribution. This is especially so for modern sequence/transformer models that involve complex architectures and naturally require a substantial amount of data to operate at optimal performance.\nOur approach TimePFN brings two key innovations: (1) Generating realistic and diverse large-scale multivariate time series data, where inter- and intra-channel dependencies are common, and (2) Developing an architecture capable of extracting time series features from this large-scale synthetic dataset. The architecture also allows for transfer learning to novel tasks with arbitrary number of channels. Overall, empowered by large amount of synthetic data (on the order of millions of samples), TimePFN facilitates state-of-the-art zero-shot and few-shot accuracy on benchmark datasets.\nThe strong zero-shot performance of our model, along with its superior performance in few-shot settings, supports the importance of the data-centric perspective. Evaluations demonstrate that our model, when fine-tuned on as few as 50 to 500 samples, is competitive with the performance of alternative methods trained on the entire dataset. More specifically, we make the following contributions:\n\u2022 We present a new method to generate synthetic multivariate time series data using Gaussian processes with kernel compositions and a linear coregionalization model.\n\u2022 We propose a variation of PatchTST (Nie et al. 2023) for multivariate forecasting. Unlike PatchTST, our architecture incorporates channel mixing and employs a convolutional embedding module for patch embeddings. This allows it to effectively extract cross-channel relations and generate more representative embeddings, as demonstrated by experiments.\n\u2022 TimePFN is the first multivariate time-series PFN. Notably, TimePFN demonstrates strong zero-shot and few-shot performance and consistently outperforms comparable models/methods across various benchmarks.\n\u2022 We find that TimePFN also exhibits strong univariate forecasting performance, although it is explicitly trained with synthetic multivariate data. This attests to the flexibility and generalization capability of our approach."}, {"title": "2 Related Work", "content": "Transformers (Vaswani et al. 2017) have revolutionized NLP, significantly advancing zero-shot and few-shot capabilities in language and vision models. This has led researchers to explore the application of transformers to time-series forecasting, leading to a substantial body of work including but not limited to (Zhou et al. 2021; Wu et al. 2021; Zhou et al. 2022; Li et al. 2019; Nie et al. 2023; Liu et al. 2022; Zhang and Yan 2023). Informer by (Liu et al. 2023) introduces the ProbSparse attention mechanism, which alleviates the quadratic complexity of the naive transformer to log-linear complexity, to mitigate the scalability issues in long sequence time-series forecasting (LSTF). (Zhou et al. 2022) uses the sparsity of the time-series in the fourier domain to enhance the performance in LSTF. PatchTST (Nie et al. 2023) uses patching with overlapping strides as a tokenization mechanism to address issues associated with naive tokenization of time-series data. This approach yields patch-based tokens that are interpretable while maintaining channel independence, treating each channel as univariate but facilitating joint learning across all channels through the same set of shared weights. Our architecture deviates from PatchTST by incorporating convolutional layers before patching and using channel-mixing to capture interactions between tokens from different channels. The advantages of convolutions are highlighted in the speech literature by (Baevski et al. 2020; Hsu et al. 2021). On the other hand, iTransformer (Liu et al. 2023) treats each variate as a single token, showing the potential benefits of utilizing inter-channel relationships.\nIn zero-shot forecasting, a line of work has emerged (Orozco and Roberts 2020; Oreshkin et al. 2021; Jin et al. 2022; Dooley et al. 2023; Ansari et al. 2024). More recently, (Ansari et al. 2024) has developed novel tokenization methods, employed quantization, and made time series data resemble language, enabling the training of LLM architectures for probabilistic univariate forecasting in a framework called Chronos. Chronos employs a data augmentation technique called KernelSynth, which generates synthetic time-series data using Gaussian processes to improve the generalization capability of the model. Meanwhile, another line of work, ForecastPFN (Dooley et al. 2023), is trained entirely on a synthetic dataset following the framework of Prior-data Fitted Networks (PFNs). Initially proposed by (M\u00fcller et al. 2022), PFNs are designed to approximate Bayesian inference. Another study (Verdenius, Zerio, and Wang 2024) integrates Joint-Embedding Predictive Architectures with PFNs for zero-shot forecasting. In addition to the mentioned models, Mamba4Cast (Bhethanabhotla et al. 2024) is also trained entirely on synthetic data using the Mamba architecture as its backbone (Gu and Dao 2024). While the mentioned literature addresses univariate settings, our work introduces the first multivariate Prior-data Fitted Network, to the best of our knowledge, featuring an architecture that enables strong zero-shot and few-shot performances on MTS forecasting."}, {"title": "3 Proposed Method", "content": "This work relies on two key aspects: a multivariate synthetic time series data generation mechanism that encapsulates inter- and intra-channel dependencies common across real time series data, and an architecture capable of generalization to real datasets when trained on such a dataset.\nIn the following section, we introduce the main concept behind our synthetic MTS data generation and training mechanism: Prior-data Fitted Networks (PFNs).\nPrior-data Fitted Networks for MTS Forecasting\nLet $\\mathcal{D} := \\{t, X_t\\}_{t=1}^T$ represent an N-channel multivariate time series data spanning a time horizon T, where $X_t := [x_{t,1},...,x_{t,N}] \\in \\mathbb{R}^N$. Each $x_{t,i}$ is potentially causally dependent on previous time steps and on one another. Given the data $\\{t, X_t\\}_{t=1}^T$ where $T < T$, the task is to forecast $X_{T+1},........, X_T$. We tackle this problem using a Bayesian framework. Assuming a hypothesis space $\\Omega$ with a prior distribution $p(\\omega)$, each hypothesis $\\omega \\in \\Omega$ models a multivariate time series (MTS) generating process, i.e., $X_t = \\omega(t)$. For example, $\\Omega$ could represent the space of hypotheses for vector autoregression (VAR) models, where a particular instance $\\omega \\in \\Omega$ corresponds to a specific VAR process, such as VAR(2), and data $\\mathcal{D}$ can be generated via this process.\nNow, given a data $\\mathcal{D} := \\{t, X_t\\}_{t=1}^T$ where $T < T$, the posterior predictive distribution (PPD) of $x \\in \\mathbb{R}^N$ at time T is $p(x|\\mathcal{T}, \\mathcal{D})$. By Bayes\u2019 theorem,\n$p(x|\\mathcal{T},\\mathcal{D}) \\propto \\int p(x | \\mathcal{T},\\omega)p(\\mathcal{D} | \\omega)p(\\omega)d\\omega$ (1)\nAs shown by (M\u00fcller et al. 2022; Hollmann et al. 2023; Dooley et al. 2023), the posterior predictive distribution (PPD) is approximated using prior fitting networks (PFNs) as follows: We iteratively sample a hypothesis $\\omega$ from the hypothesis space $\\Omega$ according to the probability $p(\\omega)$. Next, we generate a prior dataset D from this hypothesis, denoted as $\\mathcal{D} \\sim p(\\mathcal{D} | \\omega)$. We then optimize the parameters of the PFN on these generated datasets using standard methods. The time series dataset is divided into input and output parts, where $\\mathcal{D}_{input} := \\{t, X_t\\}_{t=1}^T$ and $\\mathcal{D}_{output} := \\{t, X_t\\}_{t=T+1}^\\mathcal{T}$. Subsequently, we train the PFN to forecast $\\mathcal{D}_{output}$ from $\\mathcal{D}_{input}$ using standard time-series transformer training techniques, aiming to minimize the mean-squared error loss as our optimization objective, following the setting of (Dooley et al. 2023).\nIn our work, we define the hypothesis space $\\Omega$ as consisting of single-input, multi-output Gaussian processes represented by the linear model of coregionalization (LMC) (Journel and Huijbregts 2003). Our choice is driven by the representational power of Gaussian processes and their ability to generate a diverse range of time series through the LMC framework.\nSynthetic MTS Data Generation\nIn synthetic MTS (multivariate time series) data generation, our goal is twofold. First, we strive to create variates that are realistic, exhibiting periodic patterns, trends, and other common features found in real-world data. Second, we aim for these variates to be correlated with one another, which better represents MTS data characteristics. Fortunately, our first goal is addressed by a method called KernelSynth. Chronos (Ansari et al. 2024) uses KernelSynth to enrich its training corpus by randomly composing kernels using binary operators (such as addition and multiplication) to generate diverse, univariate synthetic time-series data. This method is essentially the inverse of the kernel composition approach described in (Duvenaud et al. 2013), where kernel compositions are used for structure discovery in nonparametric regression. In contrast, KernelSynth focuses on generating realizations from these kernels. For example, combining a linear kernel with a periodic kernel results in a pattern that exhibits both a linear trend and sinusoidal seasonality. Similarly, multiplying a squared-exponential kernel with a periodic kernel creates locally periodic patterns (Duvenaud et al. 2013). Chronos aggregates kernels of various types such as Linear, Periodic, Squared-Exponential, Rational, and Quadratic and with different parameters (such as daily, weekly, and monthly periodic kernels) in a kernel bank, composing them as described above to define the Gaussian processes.\nHowever, generating a MTS time-series data is yet to be addressed. To address the second goal, generating variates that are correlated in a realistic manner, we use a generative Gaussian modelling, called linear model of coregionalization (LMC), which is developed initially in the field of geostatistics (Journel and Huijbregts 2003). For ease of understanding, we adopt the time-series notation we used above. In LMC, the outputs are obtained as linear combinations of"}, {"title": "4 Experiments", "content": "In all MTS evaluations, our primary objective is to forecast a horizon of 96 time steps using an MTS input of 96 time steps. We trained a single TimePFN model on a large-scale, multivariate synthetic dataset generated by LMC-Synth and conducted all experiments using this model. We generated 15,000 synthetic datasets with a length of 1024 and a channel size of 160 from LMC-Synth, further augmenting with datasets having independent variates as in the case $C_i(t) = l_i(t)$. The independent data comprises approximately 25% of the purely correlated data. During training, we extracted time-series input and output pairs using a sliding window of size 192 (96 for input, 96 for output), resulting in approximately 1.5 million synthetic data points. We trained the model to forecast the MTS output based on the given input using MSE loss with our 160 channel synthetic dataset. Training a single TimePFN of 8 transformer layers takes around 10 hours on L40S GPU.\nIn the few-shot evaluations, we fine-tuned TimePFN using the specified data budget. We did not perform any hyperpa-rameter tuning on TimePFN, and the same set of hyperparameters was used in all few-shot settings. Details about the model hyperparameters are provided in the appendix.\nBaselines. Since no MTS PFN is available, we compared TimePFN with state-of-the-art transformer-based MTS forecasting models, including FEDformer (Zhou et al. 2022), Autoformer (Wu et al. 2021), Informer (Zhou et al. 2021), PatchTST (Nie et al. 2023), and iTransformer (Liu et al. 2023). We evaluated these models across the entire dataset and at various data budgets, including 50, 100, 500, and 1000 data points. For instance, at a data budget of 500, the model is trained using 500 MTS input and output pairs. Additionally, we included DLinear (Zeng et al. 2023), a linear model, as part of our baseline. Given its lower complexity, we consider it a strong baseline for smaller data budgets.\nFor smaller data budgets and our zero-shot evaluations, we incorporated three algorithmic baselines as suggested by"}, {"title": "5 Conclusion", "content": "In this work, we demonstrate that with large-scale synthetic training and a suitable architecture for extracting useful time series features, fine-tuning with as few as 50 to 500 examples are sufficient to achieve competitive performance in multivariate time series forecasting. To this end, we present a novel method for generating large-scale synthetic MTS data with realistic intra- and inter-channel dependencies, called LMC-Synth, utilizing Gaussian processes and linear coregionalization model. Simultaneously, we developed an architecture capable of transfer learning, utilizing 1D convolutions applied to time series variates and channel-mixed patching. TimePFN exhibits strong zero-shot performance, and although it is explicitly trained for MTS forecasting, it also excels in zero-shot univariate forecasting, demonstrating the flexibility and generality of our framework. To the best of our knowledge, TimePFN is the first multivariate time-series PFN. For future work, we aim to improve our synthetic data-generation mechanism to better model sudden changes and multi-scale challenges that are prevalent in many time-series tasks. Additionally, integrating time series PFNs with tabular models presents an intriguing avenue for research. Moreover, we plan to extend our efforts into developing foundation models for multivariate time series."}, {"title": "A Extended Results", "content": "In addition to the budget scenarios presented in the main body, we also conducted experiments with data budgets of 100 and 1,000 to fully characterize our experimental results. Furthermore, the average accuracy across these data budgets is provided for reference. Table 5 showcases all these evaluations. In Table 6, we present the raw results of the univariate forecasting task for zero-shot forecasting.\nMultivariate Forecasting\nAs shown in Table 5, TimePFN consistently achieves the best results with a data budget of 100 and significantly outperforms all other models with a budget of 1,000, leading in 7 out of 9 datasets. TimePFN excels particularly in datasets with a multivariate nature. Consider that PatchTST (Nie et al. 2023) assumes channel independence, whereas iTransformer (Liu et al. 2023) treats each variate as a token, demonstrating extreme channel dependence. In the full budget scenario, where the entire dataset is utilized, the difference in forecasting performance between iTransformer and PatchTST is revealing, particularly in detecting datasets with high inter-channel dependencies. For instance, in the ECL and Traffic datasets, which have a large number of variates (which does not mean high channel dependence by itself), iTransformer shows superior forecasting performance compared to PatchTST. Conversely, in the ETT datasets, PatchTST performs comparatively better. Extrapolating from there, we realize that TimePFN excels in datasets with a high multivariate nature, even in full budget scenarios, and also yields good and competitive performance in datasets with comparatively low multivariate characteristic in full budget scenarios. With limited budgets, we see that TimePFN is the leading model among the baselines.\nUnivariate Forecasting\nAlthough TimePFN is specifically designed for multivariate time series forecasting, we also assessed its performance in zero-shot univariate forecasting, compared to similar models. See Table 6 for more details. On average, TimePFN-36 is the most successful model among other models, and uniformly better than all other deep-learning based baselines in our setting. Generally, Chronos-small (Ansari et al. 2024) outperforms TimePFN-36 with shorter prediction lengths, while TimePFN-36 excels at longer prediction lengths, outperforming the other models. This outcome is expected, as TimePFN is specifically trained to handle an input length of 96 and predict the same distance ahead. For these comparisons, we trimmed TimePFN's predictions to match the given prediction lengths. Given TimePFN's focus on longer prediction horizons, it's no surprise that Chronos-small performs better at shorter lengths. For TimePFN-36, we padded the first 60 sequences of the 96-sequence input with the average of a 36-sequence input to minimize distribution shift. We also included results for TimePFN-96, which uses the full 96-sequence input length without padding, to demonstrate our model's complete performance."}, {"title": "B Datasets", "content": "As datasets, we used 9 benchmark datasets which are commonly used in multivariate time-series forecasting. These consist of four ETT datasets (Zhou et al. 2021) (ETTh1, ETTh2, ETTm1, ETTm2), ECL (Electricity Consuming Load), Exchange, Traffic, Weather and Solar Energy. Except for the Solar Energy datasets, the others are benchmarked by (Wu et al. 2021), while the Solar is introduced by (Lai et al. 2018). We splitted the training, validation and test sets in a chronological way deterministically, following (Nie et al. 2023; Liu et al. 2023). We will briefly explain the features of these datasets in this section.\nETT Datasets. The abbreviation ETT refers to Electricity Transformer Temperature (Zhou et al. 2021). All ETT datasets consist of seven variates. The ETTh1 and ETTh2 datasets are sampled hourly, while the ETTm1 and ETTm2 datasets are sampled every 15 minutes. Specifically, the ETTh datasets contain 8545, 2881, and 2881 data points in the training, validation, and test sets, respectively. In contrast, the ETTm datasets comprise 34465, 11521, and 11521 data points in the training, validation, and test sets, respectively (Liu et al. 2023).\nECL Dataset. The abbreviation ECL refers to the electricity consumption load of 321 users (Wu et al. 2021). It is recorded in hourly intervals, resulting in a dataset with 321 variates. The ECL dataset contains 18317, 2633, and 5261 data points in the training, validation, and test sets, respectively (Liu et al. 2023).\nExchange Dataset. This dataset provides daily exchange rates for eight countries (Wu et al. 2021), comprising eight variates. The Exchange dataset includes 5120, 665, and 1422 data points in the training, validation, and test sets, respectively (Liu et al. 2023). Some works, such as PatchTST (Nie et al. 2023), avoid using this dataset as a benchmark because simple naive predictions (using the last observed value) often outperform more complex methods. However, for completeness, we have included it in our analysis.\nTraffic Dataset. This dataset includes hourly road occupancy rates from 862 locations (Wu et al. 2021), resulting in 862 variates. The traffic dataset contains 12185, 1757, and 3509 data points in the training, validation, and test sets, respectively (Liu et al. 2023). It is by far the most high-dimensional dataset in our evaluation.\nWeather Dataset. This dataset includes 21 meteorological factors collected every 10 minutes (Wu et al. 2021), resulting in 21 variates. The weather dataset contains 36792, 5271, and 10540 data points in the training, validation, and test sets, respectively (Liu et al. 2023).\nSolar-Energy Dataset. This dataset includes power production values from 137 solar power plants, sampled every 10 minutes (Lai et al. 2018), resulting in 137 variables. The solar energy dataset contains 36601, 5161, and 10417 data points in the training, validation, and test sets, respectively (Liu et al. 2023)."}, {"title": "C Additional Ablations", "content": "In addition to the ablation studies we provided, we include three additional case studies, with one focusing on the per-"}, {"title": "E Implementation Details", "content": "We implemented TimePFN entirely in PyTorch. We optimized the pretraining task using a synthetic dataset generated with LMC-Synth, employing the Adam optimizer (Kingma and Ba 2015) and adhering to a one-cycle learning rate policy with a maximum learning rate of lr = 0.0005 (Smith and Topin 2017). In the few-shot evaluations, we fine-tuned the TimePFN with maximum Ir = 0.0002 using AdamW optimizer (Loshchilov and Hutter 2019) with one-cycle learning rate policy. In training TimePFN with synthetic dataset, we observed that making model see the independently generated channels first, corresponding to the case with $C_i(t) = l_i(t)$, then introducing the inter-channel dependent data, significanly improves the learning speed. The explanation is simple, with the case with $C_i(t) = l_i(t)$, the model sees much more time-series patterns, as the time-series channels are all independently generated by Gaussian processes. Thus, after the model learns to make channel-independent decisions, we introduced the channel-dependent data, similar to curriculum learning scenario (Bengio et al. 2009). Moreover, while training on synthetic data, we added a multiplicative Gaussian noise to each MTS data point as a regularization, with $\\sigma = 0.1, mean = 1$. In the end, TimePFN is trained on 1.5 million MTS data points with 160 channels.\nIn TimePFN, we used the token embedding dimension of 256, and the latent space dimension of 1024, while the feedforward network dimension is set to 512. We did not do any hyperparameter tuning and chose those values from checking similar works such as (Nie et al. 2023; Liu et al. 2023). In fine-tuning, we always used the same learning rate with the same number of epochs accross different datasets (0.0002 and 8 epochs). Thus, we run the evaluations once. While doing all those, we fixed the seed to a random value of 2023.\nThroughout the experiments, we used a single L40S GPU. In addition the GPU, we had access to 128 GB of RAM and a 32-core CPU, which facilitated the acceleration of synthetic time-series data generation. Our codebase is developed based on (Liu et al. 2023). Overall, we used approximately 300 GPU hours, as we conducted benchmarks not only for our own model but also for many others. We are providing the full source code for TimePFN, including the synthetic data generation, architecture, and the training and evaluation scripts. Furthermore, we are providing the model weights of TimePFN."}]}