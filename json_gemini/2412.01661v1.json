{"title": "R-Bot: An LLM-based Query Rewrite System", "authors": ["Zhaoyan Sun", "Xuanhe Zhou", "Guoliang Li"], "abstract": "Query rewrite is essential for optimizing SQL queries to improve their execution efficiency without changing their results. Traditionally, this task has been tackled through heuristic and learning-based methods, each with its limitations in terms of inferior quality and low robustness. Recent advancements in LLMs offer a new paradigm by leveraging their superior natural language and code comprehension abilities. Despite their potential, directly applying LLMs like GPT-4 has faced challenges due to problems such as hallucinations, where the model might generate inaccurate or irrelevant results. To address this, we propose R-Bot, an LLM-based query rewrite system with a systematic approach. We first design a multi-source rewrite evidence preparation pipeline to generate query rewrite evidences for guiding LLMs to avoid hallucinations. We then propose a hybrid structure-semantics retrieval method that combines structural and semantic analysis to retrieve the most relevant rewrite evidences for effectively answering an online query. We next propose a step-by-step LLM rewrite method that iteratively leverages the retrieved evidences to select and arrange rewrite rules with self-reflection. We conduct comprehensive experiments on widely used benchmarks, and demonstrate the superior performance of our system, R-Bot, surpassing state-of-the-art query rewrite methods.", "sections": [{"title": "1 Introduction", "content": "Query rewrite is designed to transform an SQL query into a logically equivalent version that is more efficient to execute, playing a crucial role in enhancing query performance in numerous practical scenarios. Despite its significance, the process of query rewrite is NP-hard [46, 48], meaning there is a vast collection of possible rewrite rules, and the number of potential rule combinations increases exponentially. This complexity makes identifying an effective combination of rules a challenging and laborious task. There are two main paradigms for addressing this challenge.\nHeuristic-Based Methods. Some heuristic-based methods apply the rules in a fixed order derived from practical experience (e.g., PostgreSQL [7]). However, they may not achieve optimal results for queries requiring different rule orders, thereby risking the omission of essential rewrite sequences. Furthermore, other heuristic-based methods (e.g., Volcano [23]) attempt to comprehensively explore various rule orders through heuristic acceleration. Nevertheless,"}, {"title": "2 Preliminaries", "content": "In this section, we first formalize the problem of query rewrite (see Section 2.1). Then we introduce fundamental LLM concepts used in R-Bot (see Section 2.2)."}, {"title": "2.1 Query Rewrite", "content": "Rewriting a query involves numerous query transformations, and we typically identify and summarize common transformation patterns into rewrite rules. For instance, a query rewrite rule could involve replacing an outer join with an inner join when they are equivalent. The composition of these rules offers the flexibility to accommodate a wide range of query rewrite requirements.\nDefinition 2.1 (Rewrite Rule). A rewrite rule $r$ is denoted as a triplet $(c, t, f)$, where $c$ is the condition to use the rule, $t$ is the transformation to be applied, and $f$ is a matching function used for evaluation. If a query $q$ satisfies the condition $c$ as determined by the matching function $f$, then the transformation $t$ can be applied to the query $q$, resulting an equivalent rewritten query $q[r]$.\nNumerous rewrite rules have been pragmatically incorporated into database products, as evidenced in existing literature and prod-ucts such as PostgreSQL [7], MySQL [6], Apache Calcite [12]. However, when applying the rules to rewrite a query, it is often cumbersome to decide the best rule sequence for two primary reasons. First, since a rewrite rule sometimes degrades the query (e.g., $q[r]$ with higher execution latency than $q$), we should examine whether or not to use the rule. Second, it is also important to decide the order of applying the rules. For instance, applying one rule may render another rule obsolete. Thus query rewrite aims to find an optimal rule sequence to rewrite a query in order to minimize the execution cost of the rewritten query, which is formulated as below.\nDefinition 2.2 (Rule-based Query Rewrite). Consider a query $q$ and a set of rewrite rules $R$. Assume $\\alpha$ is a sequence of certain rewrite rules selected from $R$. Let $q^{\\alpha}$ denote the rewritten query by sequentially applying the rules in $\\alpha$ to rewrite the query $q$. The objective of query rewrite is to obtain an optimal rule sequence $\\alpha^*$, such that the execution cost of $q^{\\alpha^*}$ is minimized among all possible rewritten queries of $q$.\nThe query rewrite problem has been proven to be NP-hard [46]. Traditional methods cannot select high-quality rules. To address this limitation, we advocate for utilizing LLM to select rewrite rules. To address the hallucination problem of LLMs [25], we perform an offline stage to extract query rewrite evidences, including rewrite Q&As and rewrite rule specifications, and store them as Q&A repository and rewrite rule specification repository respectively. During the online phase, given an input SQL query, we retrieve relevant Q&As and rule specifications, and generate rewrite recipes, which outline how to rewrite a query using rewrite Q&As and rule specifications, thereby assisting LLMs in comprehending the rewrite evidences. With the assistance of the rewrite recipe, LLMs are then guided through a step-by-step process to judiciously select and apply rewrite rules to the query. Next we formally define these notations.\nDefinition 2.3 (Rewrite Q&A). A rewrite Q&A includes a query rewrite question and a rewrite answer on how to rewrite the query in natural language."}, {"title": "2.2 Large Language Models", "content": "We introduce some fundamental concepts of LLMs, including LLM prompting and retrieval augmented generation (RAG), which are pivotal in composing an evidence-driven query rewrite system.\nLLM Prompting. After instruction tuning on large corpus of human interaction scenarios, LLM is capable of following human instructions to complete different tasks [17, 33]. Specifically, given the task input $X$, we concatenate a prompt $P$, denoted as $X' = P \\oplus X$, which represents a comprehensive task description. LLM then fulfills the task, and generates the task output. Note that pre-training"}, {"title": "3 The Overview of R-Bot", "content": "Our R-Bot system includes an offline stage and an online stage, as described in Figure 2.\nOffline Rewrite Evidence Preparation. This stage aims to extract rewrite Q&As from the Web and rewrite rule specifications from rewrite codes and database documents, subsequently storing them in Q&A repository and rule specification repository, respectively. These resources will be utilized to assist in rewriting an online SQL query. Note that rewrite rule specifications are more general that can be applied to multiple SQL queries. In contrast, rewrite Q&As are usually specific and are applied to a particular SQL query.\nWe extract rewrite rule specifications from two types of resources. (i) Database menus and documents. Given that query rewrite evidence is often dispersed across various documents, sec-tions, and paragraphs, it becomes necessary to aggregate these evidences from diverse sources through semantic clustering and distill them into a coherent rewrite rule; (ii) Rule codes. Considering the complexity of the code, characterized by its intricate nested calls, we employ a hierarchical strategy to streamline the code from simple to complex. This approach involves initially summarizing straightforward functions, followed by recursively summarizing"}, {"title": "4 Rewrite Evidence Preparation", "content": "We discuss how to extract and standardize rewrite evidences from diverse rewrite sources. This evidence is crucial for crafting a com-prehensive rewrite recipe that guides the LLM rewrite process. We explain respectively how to prepare rewrite rule specifications (see Section 4.1) and rewrite Q&As (see Section 4.2)."}, {"title": "4.1 Rewrite Rule Specification Preparation", "content": "The rewrite rule specification clearly outlines, in natural language, the condition for use, the query transformation operations to be executed, and the matching function used for evaluation. It contains three key components. (i) \"condition\": a prerequisite that a query must fulfill to utilize the rule; (ii) \"transformation\": detailed steps for transforming the query into an equivalent form that is optimized for more efficient execution; and (iii) \"matching function\": an executable function that outputs '1' if the input query matches the rule; and '0' otherwise."}, {"title": "4.1.1 Transforming Rule Code into Rule Specification", "content": "Generating rewrite rule specifications is challenging due to the considerable effort needed to distill and synthesize information from various sources into a concise format. This process includes summarizing extensive rewrite codes, which may span thousands of lines and feature complex structures, as discussed in Section 4.1.1. Additionally, it integrates crucial but scattered information from documents on rewrite rules, as outlined in Section 4.1.2.\nGiven that some query rewrite engines, such as Apache Calcite [2], are not accompanied by comprehensive documentation, we are compelled to decipher the rewrite rules directly from the raw code. This process involves navigating through complex code structures that include intricate nested calls. To address this challenge, we introduce a hierarchical rule code summarization method, as illustrated in Figure 3. Our approach begins with the construction of a rule code structure tree, emanating from the rule's main function. In this tree, each node represents a symbol declaration (e.g., functions, variables, classes), while each edge denotes a symbol reference relationship. Progressing through the structure, we methodically summarize the declaration code, moving from simple to more com-plex elements and clarifying symbol references using previously summarized symbols. With the comprehensive summary of the root node at our disposal, we guide LLM to convert this information into a standardized rewrite rule specification.\nRule Code Structure Analysis. Given the symbol references of the rule code, it is crucial to clarify the symbol declarations before summarizing the rule code. We first build a root node representing the main function of the rule. Then, we use code analysis tools (e.g., JavaSymbolSolver [1]) to associate the symbols in the main function with their corresponding declarations, which are children nodes of the root node. If the declaration of some nodes also accesses other unseen symbols, we further resolve its symbol references. We recursively expand the nodes until reaching built-in symbols. In this way, we obtain a rule code structure tree, where each node"}, {"title": "4.1.2 Transforming Rewrite Document into Rule Specification", "content": "Considering the variety of rewrite documents, such as those for Post-greSQL and MySQL, note that sections within a single document may cover rewrite rules that bear weak relation to one another. For example, optimizations for the \"WHERE\u201d clause might discuss both constant folding and index utilization without clear interrelation. Additionally, components complementary to a rule can be dispersed across different documents. For instance, while a MySQL document might detail conditions conducive to acceleration via index utilization, a separate PostgreSQL document could highlight how certain column transformations might inhibit the use of indexes. Together, these insights from disparate sources can contribute to forming a comprehensive rewrite rule specification.\nTo address this, we propose a clustering-based document re-organization method. First, we use LLM to extract rewrite rules from rewrite documents. Second, for extracted rules, we cluster the correlated ones together into one group, where we evaluate their semantics similarities by their text embeddings (e.g., SBERT [35]). Third, we use LLM to summarize each rule cluster, and transform each cluster summary as a regularized rewrite rule specification.\nRule Extraction. We use LLM to extract rewrite rules from the rewrite documents in two steps. First, if we directly input all the documents to LLM, it often overlooks important details in the mid-dle of the extremely long context (e.g., 100k) [29, 30]. We thus split the documents into structured blocks (e.g., sections, sub-sections) that each can be effectively processed by LLM. Second, we instruct LLM to extract rewrite rules from the split blocks. To mitigate the hallucination problem [34], we require LLM to locate supporting content in the source document. If no such content can be found, we can deem the extraction low quality and repeat LLM extraction."}, {"title": "5 Hybrid Structure-Semantics Evidence Retrieval", "content": "Considering the existence of vast repositories of rule specifications and Q&As, only a minimal fraction of these resources is pertinent to an online SQL query. Thus, there is a need to identify the relevant ones both effectively and efficiently. In this section, we introduce how to retrieve relevant rewrite evidences (including rule specifications and Q&As). First, we retrieve relevant rule specifications using a function-based rule retrieval method (see Section 5.1). Second, we retrieve relevant rewrite Q&As using a hybrid structure-semantics method, with both query structures and rewrite semantics aligned with the input query (see Section 5.2). Lastly, we generate tailored rewrite recipes for the input query by leveraging both the retrieved rule specifications and Q&As (see Section 5.3)."}, {"title": "5.1 Rewrite Rule Specification Retrieval", "content": "Given an input SQL query $q$, we examine each rule specification and apply its associated matching function to get a boolean value which indicates whether the condition of the rule is satisfied. If the result is true, the corresponding rule specification $rs$ is retrieved. Then, we use LLM to generate a rule specification recipe, using a prompt $P_{rule\\_spec\\_recipe} = \u201cGiven an SQL query \u2018q\u2019 and a rewrite rule specification 'rs', your task is to explain concisely and detailedly how the rule applies to the query, by specifying (1) the SQL segments matched by the condition, and (2) the transformation of the rule.\""}, {"title": "5.2 Rewrite Q&A Retrieval", "content": "There are two types of Q&As that could potentially enhance the query rewrite. Firstly, the questions within Q&As exhibit a high structural similarity to the input query, indicating that the answers of Q&As have the potential to assist in rewriting the query. Second, the answers within Q&As demonstrate a high semantic similarity to the input query. This suggests that the Q&As are capable of addressing similar issues or bottlenecks present in the input query, such as those involving a sub-query with an aggregate function.\nStructure-Semantics Embeddings. To effectively identify these two types of Q&As, we propose structure-semantics embeddings for both SQL queries and Q&As (see Figure 4). (1) SQL Query Embedding. First, we introduce a structure-aware query embedding strategy, including (i) generating query templates embedded by pre-trained embedding (e.g., text-embedding-3-small [5]), and (ii) generating a one-hot embedding to indicate which rule specifications match the SQL query. The two embeddings capture the essential structural features to query rewrite (see Section 5.2.1). Second, we propose a semantic matching method designed to discern the semantic similarity between the SQL query and Q&As. However SQL may not encapsulate sufficient semantic information. Fortunately, the retrieved rule specification recipe of SQL in Section 5.1 encompasses these semantics, enabling us to derive an embedding from it. Third, we concatenate the structural and semantic embeddings to obtain a combined structure-semantics embedding. Given the possibility of retrieving multiple rule specification recipes, each SQL query has the potential to generate multiple embeddings. (2) Rewrite Q&A Embedding. Similarly, we generate embeddings of Q&As. For each Q&A, its embedding includes a structure-aware embedding of its query in the question part and a semantic embedding of its answer part. These embeddings allow us to identify Q&As that exhibit a high similarity to the input query in terms of their embeddings.\nStructure-Semantics Q&A Retrieval. To optimize performance for structure-semantics Q&A retrieval, a unified structure-semantics embedding index (e.g., HNWS [3]) is constructed for Q&As offline. Given an SQL query, we generate its structure-semantics embedding and retrieve relevant Q&As using the index (see Section 5.2.2)."}, {"title": "5.2.1 Structure-Aware SQL Query Embedding", "content": "Existing query embedding models [5, 39, 43, 44] cannot be directly applied for embedding the query structure for query rewrite due to limitations in two key aspects. On one hand, the query-specific embeddings mostly adopt small-scale neural networks (e.g., lower than 0.1B) fine-tuned on a specific dataset [43, 44] or a specific workload [39], and thus struggle to generalize to arbitrary scenarios. On the other hand, the general text embeddings (e.g., text-embedding-3-small [5]) cannot be directly used for query structure embedding, which are sensitive to irrelevant query information for three reasons. (i) The identifiers (e.g., table/column names) and literals in the query often disrupt characterization of the query structure. For example, while renaming the schema in a query can maintain the integrity of the query rewrites, this action results in a version with significantly altered se-mantics and it generates an embedding distinctly different from that of the original query; (ii) Since query rewrites typically focus on transforming only certain parts of the query (e.g., a sub-query), dis-regarding other irrelevant parts can help clarify the query structure; (iii) The text embedding models cannot guarantee commutative invariance, implying they may generate dissimilar embeddings for expressions such as \"p1 and p2\" and \"p2 and p1\". These can be addressed through carefully designed textual transformations.\nTo address the aforementioned challenges, we propose a structure-aware query embedding method, where each SQL query embedding is composed of two parts. The first part distills the core query struc-ture as query templates, and uses pre-trained text embedding to embed them. The second part employs a one-hot encoding strategy to indicate whether the query matches a rule specification, with '1' representing a match and '0' indicating no match. The width of this embedding corresponds to the total number of rule specifications. When comparing the two query structure embeddings, the first one captures a more global representation of the query structure, whereas the second focuses on more local expressions.\nDataflow Based Query Template Embedding. To effectively capture the essential SQL features for query rewrite, we initially reformulate the SQL query into composite dataflows that detail op-erations on the involved identifiers. Specifically, given the structure and semantics of a query $q$, a dataflow $d$ is a list of SQL operations sequentially performed on one or multiple tables or columns in the query, which corresponds to an SQL segment of the query, e.g., logical expression, mathematical expression, \"WHERE\u201d clause, \"GROUP BY\u201d clause, \"FROM\u201d clause, \u201cJOIN\u201d clause, \u201cSELECT\u201d list, sub-query, etc. For instance, we show sample dataflows for the input query in Figure 1, concentrating on the column \u201cbonus.sal\":\n$\\left\\{ sal, SELECT MAX(sal), emp.sal = ANY (SELECT MAX(sal) ...)\\right\\}$\nwhere \"sal\" is first input from \"bonus\" table, projected with an aggregate function \u201cMAX(\u00b7)\u201d, and then compared with the column \"emp.sal\" from the outer query.\nSince we focus on logical query rewrite, dataflows can be con-sidered independent if they do not involve common identifiers. Then, if we examine the rewrite rule from dataflow perspective, the condition must restrict the pattern of certain dataflows asso-ciated with a specific identifier. Otherwise, there are scarcely any potential query rewrites within a set of independent dataflows. Consequently, we can generate query templates that concentrate on the SQL operations related to each particular identifier.\""}, {"title": "5.3 Rewrite Recipe Generation", "content": "For an input SQL query $q$, since the retrieved Q&A $qa$ is used to rewrite similar but different queries, we first use LLM to generate rewrite recipes describing how to rewrite the input query inspired by the Q&A, using the prompt $p_{qa\\_recipe} = \u201cGiven an SQL query 'q\u2019 and a rewrite Q&A 'qa', your task is to propose some strategies on"}, {"title": "6 Step-by-Step LLM Rewrite", "content": "Given a rule-based query rewrite engine (e.g., Apache Calcite [2]), we utilize the retrieved Q&As and derived rewrite recipes to assist LLM in selecting appropriate rewrite rules from the engine to rewrite the input SQL query. Given that the number of possible rule sequences grows exponentially with the number of rules, LLMs are susceptible to errors when choosing from a vast array of rules. To address this challenge, we introduce a step-by-step filter-refinement method designed to meticulously select high-quality rules. Specifically, we start with a filtering step by employing an efficient Q&A-based method for rule selection and ordering, allowing us to preliminarily arrange the rules. Next, we instruct LLM to select the rules and arrange the order according to the recipes. Then, we feed the selected rules into the query rewrite engine to rewrite the query with the rules. Lastly, we evaluate the outcomes of the rewrite process to determine whether further refinement is needed or if the query rewrite can be considered complete.\nStep 1: Q&A-based Rule Selection and Ordering. Given the rewrite rules and the input query, we filter the rules and arrange the order in three steps. First, we select the rules directly matched by the query, and assign them a relevance score according to their transformation to the query. Second, we select the rules indirectly relevant to the query using the retrieved Q&As. Based on the re-trieval scores of the Q&As, we assign each indirectly relevant rule with a relevance score. Third, we rank the rules by their relevance scores in a descendent order, which serves as an initial order.\n(i) Since the rules have matching functions, we first filter the rules whose matching functions are satisfied by the input query. However, the matched rules exhibit varied relevance to the query with regards to their actual transformation. Specifically, the rules of the query rewrite engine can be classified into two types of trans-formations: the normalization rule (e.g., \"SUB_QUERY_TO_JOIN\"), which nearly always reduces query cost; and the exploration rule (e.g., \"AGGREGATE_JOIN_TRANSPOSE\u201d), which transforms the query but does not consistently result in cost reduction [38]. We classify the rules based on expert experience, and assign relevance scores to the matched rules based on their transformation types. Initially, every rule has a score of -\u221e. Then, the matched normalization rules are assigned +\u221e as closely relevant, and the matched exploration rules are assigned 0 as weakly relevant.\n(ii) Besides the directly relevant rules, we also detect indirectly relevant rules using the retrieved Q&As. Specifically, for each pair of rule and Q&A, we leverage LLM to evaluate whether the rule can be applied in the context of the Q&A. If applicable, the rule's score is incremented by the Q&A similarity score to the input query. Note that if the rule has a score of -\u221e, we first initialize it as 0 before increment. Enumerating all the pairs of rules and the retrieved Q&As, we obtain the final score of each rule r. Note that the relevance can be performed by LLM offline for each possible pair of rule and Q&A, thus not affecting the algorithm's efficiency."}, {"title": "7 Experiments", "content": "We conduct extensive experiments to evaluate our system R-Bot and compare with state-of-the-art query rewrite systems."}, {"title": "7.1 Experiment Setting", "content": "We implement our system R-Bot using the rules in an open-sourced query engine Apache Calcite [12]. We execute the SQL queries in PostgreSQL v14. The server is a machine with 125.6 GB RAM and 3.1GHz CPU.\nDatasets. To verify the effectiveness of R-Bot on different scenarios, we conduct experiments on three types of datasets. (i) TPC-H is a standard OLAP benchmark, which contains 62 columns and 44 queries. We separately test R-Bot on different data sizes, i.e., TPC-H 10x (~10G) and TPC-H 50x (~50G). (ii) DSB is a more"}, {"title": "7.2 Performance Comparison", "content": "We compare R-Bot with two types of baselines, including learning-based methods (LearnedRewrite), and origin LLMs (GPT-4, GPT-3.5).\nQuery Latency Reduction. Table 3 shows the query latency of rewritten queries. R-Bot outperforms the other query rewrite methods across all the datasets and metrics. The reasons are two-fold. First, R-Bot can judiciously retrieve relevant rule specifications and Q&As, and use them to comprehensively figure out the rewrite"}, {"title": "7.3 Ablation Study", "content": "7.3.1 Rewrite Evidence Preparation. We evaluate the essence of rewrite evidence by comparing GPT-4 with naive RAG. As shown in Table 6, naive RAG equipped with evidences can outperform GPT-4 across all the metrics, because naive RAG can retrieve rewrite evidences that are somewhat relevant to the input query, which provides beneficial guidance in rewrite rule selection and ordering."}, {"title": "8 Related Work", "content": "Query Rewrite. Existing query rewrite methods mainly adopt two paradigms. (1) Heuristic-based methods [6, 7, 12, 22, 23]. Some methods (e.g., PostgreSQL [7]) apply rewrite rules in a fixed order, which often overlook better orders in different scenarios. Besides, other heuristic-based methods (e.g., Volcano [23]) attempt to ex-plore different rule orders with the aid of heuristic acceleration. However, by neglecting inter-dependencies among the rules, these methods perform a blind search, often failing to identify the op-timal orders within reasonable time. (2) Learning-based methods. LearnedRewrite [46] decides the rule order using Monte Carlo Tree Search guided by learned cost models. However, the learned model cannot be transferred to unseen database schema without addi-tional retraining, which is often impractical. Hence, it calls for a query rewrite system capable of reliably identifying an appropriate sequence of rewrite rules to optimize query rewrite.\nAutomatic Rewrite Rule Discovery. Given the tedious effort re-quired to curate a large number of high-quality rewrite rules, some methods have been recently proposed to automate the process of rule discovery [16, 18, 40]. WeTune [40] first represents the condition and transformation of rewrite rule with operators (e.g., projection) and symbols (e.g., relation, attribute). Then it enumerates potential rewrite rules within a constrained set of transformations (e.g., in-volving up to four operators) and conditions (combinations of seven basic types), verifying rule equivalence using SMT-solvers [13, 45]. Besides, SlabCity [18] discovers possible query rewrites for a spe-cific query by recursively applying various transformations (e.g., expanding SQL conditions) to the original query, and preserves the equivalent rewrites. Complementary to the above works, R-Bot retrieves highly relevant evidences to instruct LLM, effectively ar-ranges newly discovered rules, and achieves superior adaptability and robustness in rule selection and ordering.\nLLM for Database. Recent advances in LLMs have demonstrated superiority in natural language understanding and programming, which offers opportunities to leverage vast amounts of database forums to enhance database performance [10, 20, 28, 31, 47, 49]. For instance, GPTuner [28] proposes to enhance database knob tuning using LLM by leveraging domain knowledge to identify important knobs and coarsely initialize their values for subsequent refinement. Besides, D-Bot [47] proposes an LLM-based database diagnosis system, which can retrieve relevant knowledge chunks and tools, and uses them to accurately identify typical root causes. Unlike these approaches, R-Bot is a novel query rewrite system powered by LLM."}, {"title": "9 Conclusion", "content": "In this paper, we proposed an LLM-based query rewrite system. First, we prepared rewrite evidences from diverse sources, including rewrite rule specifications and rewrite Q&As. Next, we proposed a hybrid structure-semantics retrieval method to retrieve relevant rewrite evidences, based on which we generated rewrite recipes to instruct LLM for query rewrite. Then, we proposed a step-by-step LLM method, which iteratively utilized the retrieved Q&As and rewrite recipes to select and arrange rewrite rules with self-reflection. Experimental results demonstrated that R-Bot achieved remarkable improvements over existing query rewrite methods."}]}