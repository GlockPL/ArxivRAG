{"title": "Deep Regression 2D-3D Ultrasound Registration for Liver Motion Correction in Focal Tumor Thermal Ablation", "authors": ["Shuwei Xing", "Derek W. Cool", "David Tessier", "Elvis C.S. Chen", "Terry M. Peters", "Aaron Fenster"], "abstract": "Liver tumor ablation procedures require accurate placement of the needle applicator at the tumor centroid. The lower-cost and real-time nature of ultrasound (US) has advantages over computed tomography (CT) for applicator guidance, however, in some patients, liver tumors may be occult on US and tumor mimics can make lesion identification challenging. Image registration techniques can aid in interpreting anatomical details and identifying tumors, but their clinical application has been hindered by the tradeoff between alignment accuracy and runtime performance, particularly when compensating for liver motion due to patient breathing or movement. Therefore, we propose a 2D-3D US registration approach to enable intra-procedural alignment that mitigates errors caused by liver motion. Specifically, our approach can correlate imbalanced 2D and 3D US image features and use continuous 6D rotation representations to enhance the model's training stability. The dataset was divided into 2388, 196 and 193 image pairs for training, validation and testing, respectively. Our approach achieved a mean Euclidean distance error of 2.28 \u00b1 1.81mm and a mean geodesic angular error of 2.99 \u00b1 1.95\u00b0, with a runtime of 0.22 seconds per 2D-3D US image pair. These results demonstrate that our approach can achieve accurate alignment and clinically acceptable runtime, indicating potential for clinical translation.", "sections": [{"title": "Introduction", "content": "Liver tumor ablation is an established therapeutic modality for the treatment of focal liver tumors [1], particularly in patients who are ineligible for surgical resection [2].\nDuring ablation (radiofrequency or microwave) procedures, physicians usually require inserting a single needle-shaped applicator into the tumor centroid. Then a thermal ablation zone is generated surrounding the applicator tip for eradicating cancerous cells. While ultrasound (US) and computed tomography (CT) are both viable options for applicator guidance, US has advantages over CT due to its real-time imaging capabilities, lower cost and widespread availability [3]. However, this US-guided procedure relies heavily on the physician's experience to accurately place the applicator, as it lacks three-dimensional (3D) anatomical information for ensuring complete tumor coverage [4, 5]. Moreover, for some patients, the conspicuity of liver tumors in US images is low or almost non-existent [6]. In addition, some tumor mimics, such as regenerative nodules in cirrhotic liver and prior ablation sites may also confuse physicians [7], making tumor targeting task more challenging. Thus, US-CT/MRI registration and fusion techniques have been proposed and demonstrated to improve tumor visibility and reduce physician's variability concerning the interpretation of anatomical details [8]. However, the clinical application of these multi-modal fusion techniques has been hindered by the high clinical demands of the ablation procedure. Specifically, in addition to achieving clinically acceptable fusion accuracy, registration techniques must be capable of compensating for patient-related liver motion in real time, arising from patient breathing and occasionally irregular body movement [9].\nTo address these issues, image-based registration techniques have been extensively investigated, but existing straightforward solutions (directly from 2D US to CT or MRI) are challenging to be used clinically due to their high complexity. For example, to automate liver 2D US-CT/MRI alignment, the Linear Correlation of Linear Combination (LC\u00b2) metric was introduced by Wein et al. [10]. However, its expensive computational cost still poses a challenge to allow real-time US guidance. Subsequently, Pardasani et al. [11] used a modified LC2 metric to expedite the alignment in US-guided neurosurgical procedures, attaining a runtime performance accelerated by a PyCUDA-based framework resulting in approximately 5 fps. Recently, deep learning has been employed for this challenging registration problem, but current developments are still in their early stages [12].\nTo date, the standard solution for multi-modal registration is to introduce external tracking sensors. While Penney et al. [13] employed an optical tracker to obtain the spatial information of US images and proposed a US slice-to-MRI registration approach based on the probability map of corresponding structures. However, liver respiratory motion was not accounted for in this study. Wein et al. [9] attached a position-sensor to the patient's skin to detect anterior-posterior translation of the liver. Combined with a US slice-to-volume registration approach that used Local Normalized Cross Correlation (LNCC) as the similarity metric, they achieved approximately 5 fps performance but did not report the registration accuracy or robustness.\nTo simplify the multi-modal registration stage, 3D US imaging can be used to decompose the task into two sequential registration steps: \u201c3D US-to-CT/MRI\" [14, 15] and \"dynamic 2D-to-3D US\". This concept has been well demonstrated in prostate\""}, {"title": null, "content": "interventions [5, 16, 17]. For example, Xu et al. [16] demonstrated the feasibility of fusing transrectal US images with pre-procedural MRI images for prostate biopsy and used 3D US images as the \"registration agent\" to register with 2D transrectal US and MRI images. Additionally, Guo et al. [17] developed a deep learning-based US frame-to-3D image registration approach, which achieved real-time performance and did not require external tracking sensors. However, the feasibility of this concept for use in liver interventions is still under investigation due to the relatively large liver motion arising from patient breathing and irregular body movement.\nTesting of our previously developed electro-mechatronic 3D US liver ablation guidance system in a 14-patient trial demonstrated that 3D US images could improve clinical outcomes [18-20]. Subsequently, we developed the first registration step, \u201c3D US-to-CT/MRI\u201d, to facilitate the procedure [6]. Therefore, this work focuses on the second registration step, \u201cdynamic 2D-to-3D US\u201d, to demonstrate the clinical effectiveness of mitigating the effect of liver motion, thereby improving tumor visibility during procedures. Specifically, we aimed to address the tradeoff between alignment accuracy and runtime for the \u201cdynamic 2D-to-3D US\u201d registration task. Thus, we proposed a deep learning-based 2D-3D US registration approach to allow accurate registration of a pre-procedural CT/MRI image with clearly visible tumors to the intra-procedural 2D US video stream and eliminate errors caused by liver motion. Our contributions include:\n\u2022 A temporal 2D-3D US registration workflow that facilitates intra-procedure registration that is compatible with other registration algorithms.\n\u2022 A deep regression dynamic 2D-3D US registration algorithm (DeepRS2V), using a dot-product operation-based module to combine imbalanced features between 2D and 3D US images, and avoid the discontinuity problem of rotation representations during training, via a 6D representation for transformation prediction.\n\u2022 A clinically acceptable solution that achieves accurate alignment in close to real-time."}, {"title": "Method", "content": null}, {"title": "Problem definition", "content": null}, {"title": "DeepRS2V", "content": "For the 2D-3D US alignment, our hypothesis is that the correlation between encoded 2D US features $f_{2D}$ and 3D US features $f_{3D}$ can be learned and regressed to the transformation representations, \\( \\delta \\theta_{t_i} \\), as shown in Fig. 3. The details of our DeepRS2V are as follows:\nVolume Spatial Transformer (or \u201cvolume slicing module\" in Fig. 1). Unlike commonly used medical image processing toolkits (ITK, VTK, etc.), image metadata, such as the image origin, orientation, and spacing, cannot usually be retrieved to assist in the image spatial transformation of deep learning platforms. To apply the transformation to 3D US images and extract a 2D US slice, a differentiable module was designed based on the spatial transformer network [22]. First, given the transformation representations \\( \\theta \\) and \\( \\delta \\theta \\in SE(3) \\), a sampling grid \\( G_r \\) is generated:\n\\begin{equation}\nG_r = T(\\theta,\\delta\\theta) \\cdot G_0\n\\end{equation}\nwhere T(*) is the 3 by 4 rigid transformation and \\( G_0 \\) is the standard 3D grid of the same size as \\( I_{3D} \\). Then, image intensities are interpolated on grid \\( G_r \\). In addition to obtaining the transformed volume as in the \u201cvolume spatial transformer\u201d, the \"volume slicing module\" can extract a 2D US slice (i.e., \\( I_{2D\\_moved} \\) in Fig. 1) from the transformed volume.\n2D-3D US Feature Fusion. We used 2D and 3D encoders (see Fig. 3B) to extract low-level features \\( f_{2D} \\) and \\( f_{3D} \\), respectively. However, these encoded features \\( f_{2D} \\) and \\( f_{3D} \\) are highly imbalanced due to the difference in their image dimensions. To avoid the domination of features from \\( f_{3D} \\) over \\( f_{2D} \\), and to create feature correlation for the"}, {"title": null, "content": "subsequent transformation regression task, a dot-product-based feature fusion operation was proposed, as shown in Eq. 4.\n\\begin{equation}\nf_{merged}(i, j, k) = f_{3D} \\cdot f_{2D} = \\prod_{i,j} \\prod_{k} f_{3D}(i, j) * f_{2D}(k)\n\\end{equation}\nwhere i, j and k are the coordinates of the feature elements. The fused feature \\( f_{merged} \\) becomes a \u201cvolume\u201d, which is decoded by a 3D decoder (see Fig. 3C) to predict the transformation representation.\nTransformation Representation. In this work, a rigid transformation was used to demonstrate the feasibility of our proposed model and to investigate how well a simple rigid transformation corrects liver motion. Zhou et al. [23] defined the concept of continuous representation for rotation-based Euclidean topology, and they demonstrated that 6D and 5D representations (continuous) could outperform four or fewer dimensional representations (discontinuous), such as Euler angles, quaternions, and axis-angles, in pose estimation tasks. Inspired by that work, \u201c6D rotation + 3D translation", "matrix": "n\\begin{equation}\ng_{GS}\\left(\\begin{array}{ccc} a_1 & a_2 & a_3 \\end{array}\\right) = \\left(\\begin{array}{cc} a_1 & a_2 \\end{array}\\right)\n\\end{equation}\nThe mapping from a 6D representation back to a 3 \u00d7 3 rotation matrix is shown by Eq 6.\n\\begin{equation}\nf_{GS}\\left(\\begin{array}{cc} a_1 & a_2 \\end{array}\\right) = \\left(\\begin{array}{ccc} b_1 & b_2 & b_3 \\end{array}\\right)\n\\end{equation}\n\\begin{equation}\nb_i =\\left\\{\n\\begin{aligned}\n& \\frac{a_i}{N(a_1)}& \\text{ if } i = 1\\\\\n& \\frac{a_2 - (b_1 \\cdot a_2) \\cdot b_1}{N(a_2 - (b_1 \\cdot a_2) \\cdot b_1)} & \\text{ if } i = 2\\\\\n& b_1 \\times b_2 & \\text{ if } i = 3\n\\end{aligned}\n\\right.\n\\end{equation}\nwhere N() is a vector normalization function. All the operations are based on the Gram-Schmidt process.\nLoss function. The LNCC metric was used due to its robustness to US transducer orientation-induced variation in local brightness and contrast [9]. Since the size of predicted 2D US images varies during the registration process, which differs from the size of the 2D US reference image in our case (see Fig. 4), we modified the LNCC to automatically calculate the metric on the intersection of the reference and predicted 2D US images. Thus, our customized LNCC can handle any intersection shapes, including rectangular and polygonal. By doing so, our modified LNCC can stabilize the model"}, {"title": null, "content": "training even when small valid overlaps exist between 2D US reference and predicted US images.\nAdditionally, there were two supervised components needed to regularize the model training. One was the mean squared error (MSE) of the translational difference between the predicted translations, \\( \\delta \\theta_{trans} \\), and the \u201cground truth (translations)", "ground truth (rotations)": "delta \\theta^{gt'}_{rot} \\), as shown in Eq. 8. The geodesic error measures the minimal angular difference between two rotations [23]. Note that the \u201cground truth\u201d is detailed in section 3.3 (\u201cRegistration evaluation", "7": "n\\begin{equation}\nL = \\alpha\\cdot LNCC + \\beta\\cdot \\frac{1}{\\delta_{trans}}\\Vert \\delta \\theta_{trans} - \\delta \\theta^{gt'}_{trans} \\Vert ^2 + \\gamma\\cdot GeoErr(\\delta \\theta_{rot}, \\delta \\theta^{gt'}_{rot})\n\\end{equation}\n\\begin{equation}\nGeoErr(\\delta \\theta_A, \\delta \\theta_B) = \\cos^{-1} \\left(\\frac{M_{00} + M_{11} + M_{22} - 1}{2}\\right)\n\\end{equation}\nwhere factors \u03b1, \u03b2 and y are used to balance the magnitude of the different metrics, with their sum equal to 1.0."}, {"title": "Experiments", "content": null}, {"title": "Dataset", "content": "Data acquisition. Image data for this study were obtained from healthy volunteers at our institution under a Research Ethics Board-approved protocol. All subjects provided informed consent to the study. In our work, we used an iU22 US system with a C5-1 transducer (Philips, Eindhoven, Netherlands) to acquire images. We collected, on average, four different 3D US images for each participant to cover the entire liver, as shown in Fig. 5. 3D US image acquisition was performed during a 7-12s breath-hold, and the imaging depth was set at 14-18 cm. The sizes of 3D US images ranged from 708 \u00d7 506 \u00d7 253 voxels to 708 \u00d7 556 \u00d7 278 voxels, and voxel sizes were 0.17 \u00d7 0.17 \u00d7 0.33 mm\u00b3 to 0.32 \u00d7 032 \u00d7 0.65 mm\u00b3. For 2D US images, the physician freely"}, {"title": null, "content": "scanned the liver under normal breathing conditions. The image sizes ranged from 752 x 558 pixels to 752 \u00d7 564 pixels and pixel sizes from 0.25 \u00d7 0.25 mm\u00b2 to 0.32 \u00d7 0.32 mm\u00b2. After excluding 2D-3D US image pairs with little or no overlap, our post-processed dataset included 1062 2D-3D US pairs (24 video clips) from 9 healthy volunteers."}, {"title": "Baseline approaches", "content": "To determine the best-performing \u201cregistration module\", we compared DeepRS2V with an ITK-based approach and FVR-Net [24].\n(1) ITK-based approach. The ITK-based approach was implemented using ITK registration packages. The global NCC (GNCC) was used as the similarity metric with a regular gradient decent optimizer. The 2D and 3D US masks were applied to ignore the regions beyond the valid image area."}, {"title": null, "content": "(2) The FVR-Net. The FVR-Net network was initially proposed by Guo et al. [24] for prostate biopsy procedures. A dual-branch balanced feature extraction module was used to combine the 2D and 3D US images. In FVR-Net, 3D US image features are extracted directly from the original 3D US image. Conversely, in our approach, these features are extracted from the initially transformed 3D US image (see \\( I_{2D\\_ini} \\) in Fig. 3). To adapt it to the liver case, our modified LNCC was also used to re-train this FVR-Net model.\n(3) \"DeepRS2V + correction\". Based on DeepRS2V, we proposed a variant called \"DeepRS2V + correction\". For the \"correction\" part, we used the LNCC and stochastic gradient descent (SGD) optimizer to further improve the alignment accuracy of DeeRS2V."}, {"title": "Registration evaluation", "content": "It is challenging to determine the ground truth when evaluating the registration accuracy. To address this problem, we first used our modified LNCC as the similarity metric to register 2D and 3D US images, optimized by the SGD optimizer. Next, we perturbed the obtained registration transformation by adding Gaussian-distributed noise N(0, 1) and N(0, 1.5) to translations and rotations, respectively. We generated 100 perturbed candidates (registered 2D US images) for each image registration pair. Lastly, the most visually similar image was chosen by a sonographer as the ground truth, primarily aiming to mitigate the limitation of LNCC being trapped at a local minimum.\nAdditionally, the target registration error (TRE) was used to evaluate the registration accuracy using liver vessel bifurcation points chosen as landmarks. In 3D US images, vessel bifurcation points were selected based on the segmented 3D vessel surface model, which is described by Xing et al. [4]. In the 2D US setting, sequential 2D US images were acquired by freely sweeping over the human body by the physician. Thus, vessel bifurcation points can be localized on some 2D US images by visually examining the cognitively reconstructed vessels from sequential US images. Note that to reduce the impact of scanning speed on slice thickness, the sonographer manipulated the US transducer at a low speed."}, {"title": "Implementation details", "content": "Our registration model was implemented based on Pytorch\u00b9 and the MONAI\u00b2 framework was used for data preprocessing. The model was trained using the Adam optimizer with a starting learning rate of 10-6, which decayed by a gamma factor of 0.8 every 80 epochs. The training batch size and kernel size of LNCC were set to 2 and 51, respectively. To balance each metric of the loss function, the values of \u03b1: \u03b2: y were chosen as 20:1:10. We trained for 1200 epochs until convergence on an NVIDIA Quadro RTX 6000 and an RTX 2080 Ti, respectively. The source code is available at https://github.com/Xingorno/DeepRegS2V."}, {"title": "Results", "content": null}, {"title": "Registration accuracy evaluation", "content": "To evaluate the registration accuracy, we compared our proposed approaches with ITK-based and FVR-Net approaches. 193 2D-3D US image pairs from 5 testing cases were used. Table 1 shows the 3D Euclidean distance error (in mm), which was also decomposed into the X, Y and Z directions (i.e., Tx, Ty, and T\u2082), as shown in Fig. 6. Additionally, the geometric angular error (in degrees) was used to calculate the 3D rotational error, which was also decomposed and reported relative to the X, Y and Z axes. Meanwhile, Table 1 shows the number of cases with a 3D Euclidean distance error of less than 10 mm labeled as \"successful pairs\". Lastly, the multiple comparison Dunnett's test [25], was used to analyze the statistically significant differences between the control approach (\u201cDeepRS2V + correction\") and other approaches.\""}, {"title": "Registration runtime evaluation", "content": "For US-guided interventions, registration time is another critical aspect to demonstrate clinical applicability. Table 3 shows that DeepRS2V required an average of 0.10s and 0.05s to register a 2D-3D US image pair on an RTX 2080 Ti card and an RTX 6000 card, respectively. \"DeepRS2V + correction\" required 0.22s on an RTX 6000, which is slower than DeepRS2V, but still faster than other approaches. Testing on two different platforms, DeepRS2V and its variant achieved shorter registration time on an RTX 6000 card, indicating potential for further reducing the runtime on better hardware platforms."}, {"title": "Clinical integration", "content": "Figure 9 shows our approach in a clinical setting. Combined with a 3DUS-CT/MRI registration method developed by [6], the registered US and MRI images can be displayed simultaneously in real time during the procedure. The overlaid vessels on US and registered MRI images show the qualitative alignment performance, while the rendered 3D view provides the relative spatial relationship between the US transducer and the patient."}, {"title": "Discussion", "content": "The \"DeepRS2V + correction\" approach not only achieved the best performance in translation and rotation compared to other approaches, but also had a clinically acceptable runtime to facilitate US-guided procedures. For liver tumor ablation, an error of 5 mm in targeting the tumor centroid is deemed to be clinically acceptable, when considering a 5 to 10 mm typically safety margin [26]. Given that our resulting transformation is relative to the center of the 3D US images, a translation error of 2.28 mm is adequate for tumor targeting. Additionally, the visualization of overlaid vessels and rendered views can further provide the physician with confidence in initiating treatment. A runtime of 0.22 s was achieved for each 2D-3D US image pair by \"DeepRS2V + correction\". Compared to other existing approaches, this provides smooth alignment in close to real-time, which is clinically acceptable.\nIn this work, a rigid transformation model was used to assess its effectiveness in correcting liver motion. As known, liver deformation can also occur due to patient breathing and movement. After testing 5 cases using rigid correction alone, the mean errors (2.28 \u00b1 1.81mm in translation and 2.99 \u00b1 1.95\u00b0 in rotation) not only demonstrated the clinical feasibility of our approach, but also suggested that soft tissue deformation may not significantly impact this procedure. Previously, we discussed the impact of a rigid transformation model on the 3D US-CT/MRI registration task, which achieved a TRE of approximately 5 mm [6]. In contrast to the 2D-3D US registration performance, preliminary results indicate that developing deformation registration model for the 3D US-CT/MRI registration task may be more impactful than for 2D-3D US registration."}, {"title": null, "content": "Our mechatronic arm tracking system not only provides the initial pose for stabilizing the registration, but also allows visualization of the relative spatial relationship between the patient and a US transducer. Since the acquired 3D US images have a limited field of view, it is possible during the procedure that some 2D US images may be beyond the 3D US imaging field. In such case, the 2D-3D US registration task may fail, but the relative relationship between the patient and a 2D US transducer can still function effectively, as shown in Fig. 9. This capability gives the physician confidence to proceed with the procedure."}, {"title": "Conclusion", "content": "We proposed a deep regression 2D-3D US registration algorithm embedded in a sequential registration workflow to correct liver motion in US-guided tumor ablation. The results demonstrated that our approach could readily address the tradeoff between registration accuracy and runtime, showing potential for clinical translation."}]}