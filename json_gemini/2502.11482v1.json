{"title": "DATA: Decomposed Attention-based Task Adaptation for Rehearsal-Free Continual Learning", "authors": ["Huanxuan Liao", "Shizhu He", "Yupu Hao", "Jun Zhao", "Kang Liu"], "abstract": "Continual learning (CL) is essential for Large Language Models (LLMs) to adapt to evolving real-world demands, yet they are susceptible to catastrophic forgetting (CF). While traditional CF solutions rely on expensive data rehearsal, recent rehearsal-free methods employ model-based and regularization-based strategies to address this issue. However, these approaches often neglect the model's plasticity, which is crucial to achieving optimal performance on newly learned tasks. Consequently, a key challenge in CL is striking a balance between preserving plasticity and mitigating CF. To tackle this challenge, we propose the Decomposed Attention-based Task Adaptation (DATA), which explicitly decouples and learns both task-specific and task-shared knowledge using high-rank and low-rank task adapters (e.g., LoRAs). For new tasks, DATA dynamically adjusts the weights of adapters of different ranks based on their relevance and distinction from previous tasks, allowing the model to acquire new task-specific skills while effectively retaining previously learned knowledge. Specifically, we implement a decomposed component weighting strategy comprising learnable components that collectively generate attention-based weights, allowing the model to integrate and utilize diverse knowledge from each DATA. Extensive experiments on three widely used benchmarks demonstrate that our proposed method achieves state-of-the-art performance. Notably, our approach significantly enhances model plasticity and mitigates CF by extending learnable components and employing stochastic restoration during training iterations. Our code will be available at https://github.com/Xnhyacinth/DATA.", "sections": [{"title": "Introduction", "content": "Continual learning (CL) (Wang et al., 2023a; Zhou et al., 2024) is essential for Large Language Models (LLMs) (Dubey et al., 2024; Yang et al., 2024) to continuously evole, adapting to real-world demands and progressively improve regarding a series of new tasks. Unlike traditional supervised learning, which relies on independent and identically distributed (i.i.d.) data, CL focuses on the dynamic and evolving demands of real-world applications (Zhai et al., 2023; Wu et al., 2024), such as domain-specific adaptations (Roziere et al., 2023) and alignment with human preferences (Ouyang et al., 2022). Departing from the i.i.d. assumption presents two major challenges: 1) Catastrophic Forgetting (CF) \u2014 the tendency of a model to over-"}, {"title": "Related Work", "content": "Continual Learning (CL). CL aims to effectively acquire new task knowledge during ongoing training while preserving knowledge from previously learned tasks. Traditional CL methods can be broadly classified into three categories: rehearsal-based, regularization-based, and architecture-based approaches. Rehearsal-based methods mitigate catastrophic forgetting (CF) by selectively retaining samples (Tiwari et al., 2021; Wang et al., 2024b; He et al., 2024) or pseudo-generative examples (Shin et al., 2017; Sun et al., 2019) from prior tasks. In contrast, regularization-based methods employ quadratic regularization"}, {"title": "Methodology", "content": "Preliminary. Continual learning (CL) seeks to tackle challenges within ongoing sequences, specifically addressing how to adapt to new tasks without forgetting previously learned knowledge. Formally, suppose there are N sequential tasks {T1, T2, ..., Tw}, where each task Ti = {x,y}1 consists of Ni training examples, with x representing the input and yn is corresponding label. Let Li(\u00b7) denote the empirical risk on the i-th task Ti, and fe represent the model with"}, {"title": "Motivation", "content": "Recent advances have highlighted the effectiveness of using adapters with varying ranks to model different tasks, offering a promising solution to the challenges of CL. This motivates us to further explore and validate the core principles behind utilizing both low-rank and high-rank adapters in CL.\nTo further explore this phenomenon, we conduct a t-SNE distribution analysis (van der Maaten and Hinton, 2008) to examine the feature distributions of Order1 (4 tasks, Sec. 4.1) within the 11th transformer block of LLaMA2-7B. The results as presented in Figure 2 (a) reveal that low-rank adapters maintain relatively consistent feature distributions across different target tasks. This consistency suggests that the low-rank embedding space effectively mitigates the effects of dynamic distribution shifts by concentrating on extracting task-shared knowledge. Conversely, high-rank adapters display noticeable variability across tasks. This variability underscores the enhanced ability of high-rank adapters to aggregate features within a single task, highlighting their suitability for capturing the unique data distribution of each target task.\nFurthermore, we utilize the H-divergence metric to assess the representation consistency of adapters on Order 4 (15 tasks). A relatively small inter-task divergence indicates that feature representations remain stable and are less affected by cross-task shifts (Ganin et al., 2015), while a small intra-task divergence within a given task indicates that the model better understands the current distribution.\nWe compare the divergence values obtained from the source model alone, injecting low-rank adapters and injecting high-rank adapters, as shown in Figure 2). Compared to the source model and high-rank adapters, low-rank adapters produce feature representations with lower inter-task divergence, particularly when addressing mid-range target tasks or significant task shifts between adjacent tasks (i.e., tasks 6-10). These findings underscore the efficacy of low-rank adapters in acquiring long-term,"}, {"title": "High- and Low-Rank Adapter", "content": "The above observations prompted us to introduce high- and low-rank Adapters into the model, aiming to simultaneously adapt current task distribution while maintaining task-shared knowledge.\nSimilar to LoRA, DATA's design principle is simple and effective, as illustrated in Figure 3 (b). The architecture consists of three sub-branches: the central branch, which originates from the original model's linear layer, and the right and left branches, which are bottleneck structures representing high-rank and low-rank DATA, respectively. Specifically, the right branch (high-rank) consists of an up-projection layer with parameters A2 \u2208 Rdxdh, and a down-projection layer with parameters B2 \u2208 Rdh\u00d7d, where dh (e.g., dh = 32) represents the middle dimension of the high-rank feature. The same principle applies to the low-rank branch, except it satisfies d\u0131 < dh. For an input x, the resulting high-rank features fh and low-rank features f\u2081 from the DATA are formulated as:\nfh = A2 \u00b7 (B2\u00b7 x); f\u2081 = A1 \u00b7 (\u03921.x)\nThe two-branch bottleneck is connected to the output feature of the original linear f\u3002 through a residual connection, with scale weights Ah and A\u012b. The fusion of knowledge f\u00e6 is then expressed as:\nfx = fo + h \u00d7 fh + \u03bb\u03b9\u00d7 fi\nThe task knowledge scale weights Ah and A\u012b can be adaptively computed through decomposed component weighting strategy as detailed in Sec. 3.3. During the inference process, the distinct task representations of the DATA are reparameterized and projected into the original model, enabling dynamic knowledge adjustments."}, {"title": "Decomposed Component Weighting", "content": "To effectively mitigate CF across various tasks and samples, it is crucial to extract and manage different types of knowledge. While the distinct structure of low-rank and high-rank DATA facilitates learning diverse task representations, the continual adaptation process also necessitates the normalization of knowledge fusion weights. This normalization ensures the effective capture of relevant task-specific knowledge while preserving long-term task-specific knowledge.\nAs illustrated in Figure 3 (a), we draw inspiration from (Smith et al., 2022) and propose a decomposed component weighting strategy. Specifically, we introduce a set of weight components that form a decomposed weight through weighted summation, which is then passed to the corresponding DATA layer. Furthermore, in new tasks, the weights will essentially reuse previously acquired knowledge about past tasks instead of initializing new task weights from scratch:\n\u03bb = \u2211amWm"}, {"title": "Expansion & Orthogonality", "content": "The key to alleviating CF is to prevent overwriting the knowledge acquired in previous tasks. When encountering a new task, we freeze the existing components and expand the collection by updating only the new components. This is illustrated at the bottom of Figure 3, where the existing parameters are frozen, and only the newly added parameters are trainable. Specifically, for task Tt, we learn M\nN components, where N denotes the number of tasks and M is the hyperparameter, while the previously learned (t-1). M components are kept frozen. This extension is achieved through our attention-based component weighting scheme, ensuring that the expansion of parameters does not affect the computation of the weights a corresponding to the previously learned components.\nTo further mitigate CF, we introduce orthogonality constraints for W, K, and A. The underlying intuition is that interference is minimized when vectors are orthogonal. For instance, we aim to prevent the keys and weights learned in task 72 from influencing data from task T\u2081. To achieve this, we initialize the vectors orthogonally and incorporate a simple orthogonal penalty loss as:\nLortho(B) = ||BBT \u2013 I||2"}, {"title": "Full Optimization", "content": "Combining all objectives, our full optimization is:\nmin LCL(fe,W,K,A,A,\u03bb\u03b9 (x), y) +\nWn, Kn, An\n\u03b2(Lortho(W) + Lortho(K) + Lortho (A))\nwhere Wn, Kn, An refer to the weight components and corresponding keys/attention vectors that are unfrozen and trained during task Tn and \u1e9e is a hyperparameter balancing the orthogonality loss."}, {"title": "Experiment", "content": "In Section 4.4, we compare our method with other SOTA methods on three public benchmarks. In Section 4.6, we further evaluate the task generalization ability of the proposed method on unseen target tasks. Comprehensive ablation studies are conducted in Section 4.7."}, {"title": "Datasets", "content": "Standard CL Benchmark (SC) is a CL benchmark for language models, which consists of five text classification datasets introduced by (Zhang et al., 2015). We follow Wang et al. (2023b) to pick four datasets (AG News, Amazon reviews, DBpedia and Yahoo Answers) and shuffle the tasks into three different orders to form orders 1, 2, and 3.\nLong Sequence Benchmark (LS) is an extended version of the standard CL benchmark with 15 datasets (five classification tasks, nine GLUE and SuperGLUE tasks, and the IMDB dataset) (Razdaibiedina et al., 2023). Following Razdaibiedina et al. (2023), we select 1,000 random samples for training each task and hold out 500 samples per class for validation and testing. Similarly, we shuffle them to form orders 4, 5, and 6.\nTRACE is a CL benchmark for LLMs that includes 8 datasets that cover multichoice QA, multilingual capabilities, code generation, and mathematical reasoning (Wang et al., 2023c).\nMetrics. We adopt the following three metrics to quantify various performances: 1) FP = 1\nN\n\u03a3j=1 aN is the average zero-shot performance across all N tasks after tuning on the final N-th task. Here, am denotes the zero-shot performance on task q after sequentially tuning the m-th task, and Tj refers to the j-th task in the sequence. 2) AP = 1\nN\n\u03a3a is the average zero-shot performance when learning each j-th task, which measures the plasticity of the model. 3) Forget = AP \u2013 FP is calculated as the differ-"}, {"title": "Baselines", "content": "We compare our method with the following baselines: 1) SeqLoRA: Trains fixed-size LoRA on a sequence of tasks. 2) LoRAReplay: Trains new tasks on LoRA with mixing a 2% past task. 3) O-LORA (Wang et al., 2023b): Learns tasks in different LoRA subspaces that are kept orthogonal to each other and sums all LoRA weights up at testing time. 4) MIGU (Du et al., 2024): Only updates the model parameters with large magnitudes."}, {"title": "Implementations", "content": "We implement DATA with the LLaMA3.1-8B (Dubey et al., 2024), LLaMA2-7B (Touvron et al., 2023), Qwen2.5-7B (Yang et al., 2024) and T5-Large (Raffel et al., 2019). All experiments are conducted on 2 A100 GPUs with 80GB using LLaMA-Factory (Zheng et al., 2024). All experimental results are reported as the average of 3 runs. Please refer to the Appendix B for more detailed settings."}, {"title": "Main Results", "content": "To demonstrate the effectiveness of the proposed DATA method, we perform experiments on three CL benchmarks, as summarized in Table 1. Following O-LORA (Wang et al., 2023b), we present the results of three independent runs with different task orders on the two previous CL benchmarks. Detailed results for each order and each task within a specific order are provided in Appendix C.\nOur DATA significantly reduces the CF of abilities and knowledge in CL. Traditional CL approaches often result in relatively high levels of"}, {"title": "Performance Shifts", "content": "In Figure 4, we visually depict the performance changes observed during the training process of DATA compared to other CL methods. Our proposed method clearly demonstrates its effectiveness in mitigating forgetting and enhancing plasticity. By maintaining a stable structure during the fine-tuning phase, DATA ensures consistent performance on general tasks. In sequential task transitions, it is evident that DATA demonstrates superior FP and reduce Forget compared to other methods. The line shifts further illustrate that LLaMA3.1-8B exhibits less forgetting than LLaMA2-7B, emphasizing that forgetting is model-dependent."}, {"title": "Task Generalization", "content": "We select four benchmarks to evaluate the cross-task generalization capability of DATA, an essential dimension for assessing CL algorithms. As demonstrated in Table 2, DATA effectively balances generalization and CL ability by efficiently extracting task-shared knowledge using low-rank adapters. This finding suggests that actively promoting task-shared knowledge and source capabilities between different tasks is beneficial. However, all methods experience a notable decline in performance on reasoning tasks (GSM8K and BBH), which may be attributed to the simple classification tasks potentially impairing the ability to perform generative"}, {"title": "Ablation Study", "content": "We conduct an ablation study in Order 1 using LLaMA2-7B to evaluate the contributions of various components in our method DATA, including high-rank DATA (DATAh), low-rank DATA (DATA), decomposed component weighting strategy, attention keys, orthogonality regularization (Ortho.), and stochastic restoration (Rest.). As shown in Table 3 (E2), incorporating high-rank DATA increases the FP by 0.7, indicating that high-rank features can effectively extract more task-specific knowledge for adaptation to the target task. In E3, low-rank DATA improves performance by 2.4 compared to E\u2081. The overall improvement in E4 reaches 4.6, suggesting that both types of DATA complement each other during continual adaptation. The stochastic restoration in E5 allows the model to maintain plasticity in CL, enhancing its adaptability to new tasks. E6-E8 achieves an additional 2 improvements, demonstrating the effectiveness of the decomposed component weighting strategy in enhancing the task representations for each DATA."}, {"title": "Conclusion", "content": "In this paper, we revisit existing methods for leveraging Large Language Models (LLMs) in continual learning (CL) and propose three ideal characteristics for such systems: rehearsal-free, inference efficiency, and plasticity. To address the challenges associated with these characteristics, we introduce Decomposed Attention-based Task Adaptation (DATA) as a solution to catastrophic forgetting and plasticity loss. Moreover, we propose a decomposed component weighting strategy to dynamically integrate the knowledge from both low-rank and high-rank DATA, thereby enhancing the unique task representation. Extensive experiments across multiple CL benchmarks and LLMs consistently validate the effectiveness of DATA."}, {"title": "Limitations", "content": "Method The DATA method introduces a decomposed component weighting strategy and employs both high-rank and low-rank adapters, which increases the complexity of the model architecture. This complexity may lead to higher computational costs during training and inference, particularly when scaling to larger models or more tasks. Additionally, the need for dynamic weight adjustments based on task relevance and distinction may require more sophisticated optimization techniques, potentially limiting its applicability in resource-constrained environments. Furthermore, our current approach to stochastic recovery involves a step-level method, where a small proportion of parameters is recovered every 200 steps. There is significant potential to enhance this process by exploring dynamically adaptive methods that can more effectively select saturated or less important parameters for recovery. Additionally, establishing criteria to determine when recovery is necessary could optimize the process further, potentially improving model performance and efficiency.\nTask Although DATA is designed to be rehearsal-free, it still relies on the availability of diverse and high-quality task-specific data for effective adaptation. In scenarios where task-specific data is scarce or of low quality, the method's ability to adapt and generalize may be compromised. Additionally, the method's performance on tasks with significant domain shifts or out-of-distribution data remains to be fully explored.\nLarge Language Models The effectiveness of DATA is highly dependent on the underlying LLM architecture. While the method shows promising results on models like LLaMA2, LLaMA3.1, and Qwen2.5, its performance may vary across different LLMs, especially those with significantly different architectures or pre-training objectives. Furthermore, we do not experiment with larger models like 13B and 72B due to computational or financial constraints."}, {"title": "Ethical Considerations", "content": "Our approach does not introduce ethical concerns. The datasets we used are public, and there are no privacy issues."}, {"title": "AI writing statement", "content": "This paper utilized AI assistance for language polishing of the manuscript, including vocabulary correction and spell checking."}, {"title": "Overall Framework", "content": "Drawing from the insight that LoRA (Hu et al.,\n2022) has exhibited superior performance, we utilize a high- and low-rank framework to ensure sta-\nbility during continual task adaptation. The overall\nframework and the details of our method DATA are\nshown in Figure 3.\nLORA (Low-Rank Adaptation), as proposed by\n(Hu et al., 2022), postulates that parameter changes\n(\u2206W) during fine-tuning occur within a low-rank\nsubspace. This is particularly applied to the layer\nweights Wo \u2208 Rm\u00d7n of a model fe for a down-\nstream task. The parameter update is formulated as\n\u2206W = A \u00d7 B, where A \u2208 Rm\u00d7r and B \u2208 Rr\u00d7n\nare two learnable matrices, and the rank r is signif-\nicantly smaller than min{m, n}.\nFor a specific layer in the model fe, the LoRA\nupdate is expressed as:\nh' = Wox + \u2206Wx = (Wo + AB)x\nHere, h' represents the updated output, and x\nis the input to the layer. Importantly, the original\nweights Wo are kept frozen during the fine-tuning\nprocess, and only the matrices A and B are train-\nable."}, {"title": "Experimantal Settings", "content": "Datasets\nTrain Tasks. Tables 4 and 5 provide detailed in-\nformation on the datasets utilized in our continual\nlearning (CL) experiments. Table 4 presents the 15\ndatasets included in the Long Sequence Benchmark\n(Razdaibiedina et al., 2023), while Table 5 outlines\nthe 8 datasets from TRACE (Wang et al., 2023c).\nBoth tables include the corresponding evaluation\nmetrics for each dataset.\nGeneralization. We select the 1) Multitask Lan-\nguage Understanding (MMLU) (Hendrycks et al.,\n2021), which includes multiple-choice questions\nacross 57 subjects. 2) GSM8K (Cobbe et al.,\n2021), which is a high-quality linguistically di-\nverse multi-step elementary math reasoning dataset.\n3) BIG-Bench Hard (BBH) (Suzgun et al., 2022),\nwhich includes 27 challenging tasks spanning arith-\nmetic, symbolic reasoning, and more, derived from\nBIG-Bench (BB) (bench authors, 2023). Most of\nthe data consists of multiple-choice questions. 4)\nAGIEval (Zhong et al., 2023), which includes a\nwide range of high-quality official entrance exams,\nqualifying exams, and advanced competitions tai-\nlored to human participants."}, {"title": "Task Sequence Orders", "content": "We report task orders used for our CL experiments\nin Table 6."}, {"title": "Implementations", "content": "Our implementations are based on huggingface\ntransformers v4.45.2 (Wolf et al., 2020) using Py-\nTorch v2.3.1 (Paszke et al., 2019) and LlamaFac-\ntory (Zheng et al., 2024). All unseen tasks gen-\neralization evaluation conducted using the Open-\nCompass toolkit (Contributors, 2023), adopting its\ndefault configuration.\nFor Standard CL Benchmark and Long Sequence\nBenchmark (Order 1 - Order 6), We trained the\nmodels with 1 epoch, a constant learning rate of\n1e-4.\nFor TRACE Order 7 (C-STANCE, FOMC,\nMeetingBank, Py150, ScienceQA, NumGLUE-cm,\nNumGLUE-ds, 20Minuten), we trained with 5000\nsamples with a constant learning rate of le-4 for 5,\n3, 7, 5, 3, 5, 5, 7 epochs respectively.\nIn a series of performance experiments, we con-\nfigured various parameters as follows: the LoRA\nrank was set to 8 refer to Figure 5, and the propor-\ntion of past task data mixed in LoRAReplay was set\nto 2%. For the DATA model, the low-rank configu-\nration was set to 2 and the high-rank configuration\nto 8. From Figure 7, it can be observed that the\nperformance of 2 and 8 is optimal. Additionally,\ncompared to LoRA, the increase in parameters is\nlimited, with only an additional set of LoRA with\na rank of 2, achieving a balance between resources\nand performance.\nIn terms of the decomposed component weight-\ning strategy, we used a weight length (Lw) of\n8. The weight component for each task was set"}, {"title": "More Baselines", "content": "IncLoRA: Incremental learning of new LoRA pa-\nrameters for a sequential series of tasks (without\nadding any regularization or replaying samples\nfrom previous tasks).\nLFPT5 (Qin and Joty, 2021): Continuously train\na soft prompt that simultaneously learns to solve\ntasks and generate training samples, which are sub-\nsequently used in experience replay.\nProgPrompt (Razdaibiedina et al., 2023): Sequen-\ntially concatenates previously learned prompts to\nthe current one during training and testing.\nSAPT (Zhao et al., 2024): In the SAPT method, a\nShared Attentive Learning and Selection Module\n(SALS) is used to guide training samples through\noptimal PET blocks for task-specific learning, us-\ning a unique instance-level attention mechanism.\nThis process ensures efficient continual learning\nfor large language models."}, {"title": "Extended Results", "content": "Fine-grained Results for the Main\nExperiments\nWe report the results of each task order on the\n3 benchmarks in Table 8. Overall, our proposed\nDATA demonstrates excellent capabilities in ad-\ndressing CF and Loss of plasticity.\nSupplementary Motivation\nIn our analysis of LoRA's performance with vary-\ning ranks on LLaMA2-7B for Order 1, as shown\nin Figure 5, we observed that increasing the rank\nenhances Adaptation Plasticity (AP) for new tasks.\nHowever, this improvement comes at the cost of\ngreater forgetting, as it tends to overlook the shared\nknowledge between tasks. Conversely, lower ranks\ndo not lead to significant changes in forgetting, in-\ndicating that they are more effective at capturing\nthe shared knowledge and skills across tasks. This\nobservation suggests the necessity of integrating\nboth high- and low-rank adapters. By doing so,\nwe can better balance the modeling of task-specific\nand shared knowledge, potentially optimizing both\nAP and reducing forgetting.\nEfficiency Analysis\nIn Table 9, we compare the FLOPs, trainable pa-\nrameters, storage features, and average predict\ntimes of various CL methods. LORA achieves\nthe highest inference efficiency by bypassing mod-\nule selection and expansion, allowing the learned"}]}