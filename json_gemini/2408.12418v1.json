{"title": "CODE: Confident Ordinary Differential Editing", "authors": ["Bastien van Delft", "Tommaso Martorella", "Alexandre Alahi"], "abstract": "Conditioning image generation facilitates seamless editing and the creation of photorealistic images. However, conditioning on noisy or Out-of-Distribution (OoD) images poses significant challenges, particularly in balancing fidelity to the input and realism of the output. We introduce Confident Ordinary Differential Editing (CODE), a novel approach for image synthesis that effectively handles OoD guidance images. Utilizing a diffusion model as a generative prior, CODE enhances images through score-based updates along the probability-flow Ordinary Differential Equation (ODE) trajectory. This method requires no task-specific training, no handcrafted modules, and no assumptions regarding the corruptions affecting the conditioning image. Our method is compatible with any diffusion model. Positioned at the intersection of conditional image generation and blind image restoration, CODE operates in a fully blind manner, relying solely on a pre-trained generative model. Our method introduces an alternative approach to blind restoration: instead of targeting a specific ground truth image based on assumptions about the underlying corruption, CODE aims to increase the likelihood of the input image while maintaining fidelity. This results in the most probable in-distribution image around the input. Our contributions are twofold. First, CODE introduces a novel editing method based on ODE, providing enhanced control, realism, and fidelity compared to its SDE-based counterpart. Second, we introduce a confidence interval-based clipping method, which improves CODE's effectiveness by allowing it to disregard certain pixels or information, thus enhancing the restoration process in a blind manner. Experimental results demonstrate CODE's effectiveness over existing methods, particularly in scenarios involving severe degradation or OoD inputs.", "sections": [{"title": "1 Introduction", "content": "Conditional image generation consists of guiding the creation of content using different sorts of conditioning, such as text, images, or segmentation maps. Our research focuses on scenarios where the guidance is an Out-of-Distribution (OoD) image relative to the training data distribution. This is especially relevant for handling corrupted images, similar to denoising or restoration methods. The main challenge in these scenarios is balancing fidelity to the input with realism in the generated images. Traditional methods for restoring corrupted images, such as Image-to-Image Translation or Style Transfer, are limited by the need for distinct datasets per style or per noise. Another approach models the corruption function as an inverse problem, requiring detailed knowledge of each possible corruption, making it impractical for most real unknown OoD scenarios. Guided image synthesis for OoD inputs aims to rectify corrupted images without prior knowledge of the corruption, positioning it as Blind Image Restoration (BIR). Despite recent advancements, achieving human-level generalization remains challenging.\nOur work aims to generate realistic and plausible images from potentially corrupted inputs using only a pre-trained generative model, without additional data augmentation or finetuning on corrupted data, and without any specific assumption about the corruptions. Unlike other BIR methods that strive to reconstruct a ground-truth image relying on specific guidance or human-based assumptions, our approach is fully blind, seeking to maximize the input image's likelihood while minimizing modifications to the input image. As such, we differ from traditional BIR approaches.\nBIR is inherently ill-posed due to the loss of information from unknown degradation, necessitating auxiliary information to enhance restoration quality. Previous approaches have incorporated domain-specific priors such as facial heatmaps or landmarks (Chen et al. 2018, 2021; Yu et al. 2018), but these degrade with increased degradation and lack versatility. Generative priors from pre-trained models like GANs (Chan et al. 2021; Zhou et al. 2022; Yang et al. 2021; Pan et al. 2021; Menon et al. 2020) become unstable with severe degradation, leading to unrealistic reconstructions. Methods like (Wang et al. 2021a) combine facial priors with generative priors to improve fidelity but fail under extreme degradation. In (Meng et al. 2021), the authors replace GANs with diffusion models (Ho, Jain, and Abbeel 2020; Song et al. 2020) as generative priors. However, as the degradation increases, the method forces a choice between realism and fidelity.\nBIR still fails to achieve faithful and realistic reconstruction for a wide range of corruptions on a wide range of images. Dealing with various unknown corruptions prevents inverse methods from being easily applicable while dealing with a wide range of images prevents relying on carefully designed domain priors. We introduce Confident Ordinary Differential Editing (CODE), an unsupervised method that generates faithful and realistic image reconstructions from a single degraded image without information on the degradation type, even under severe conditions. CODE leverages the generative prior of a pre-trained diffusion model without requiring additional training or finetuning. Consequently, it is compatible with any pre-trained Diffusion Model and any dataset. CODE optimizes the likelihood of the generated image while constraining the distance to the input image, framing restoration as an optimization problem. Similar to GAN-inversion methods (Tov et al. 2021; Abdal, Qin, and Wonka 2020, 2019; Zhu et al. 2020; Menon et al. 2020; Pan et al. 2021), CODE inverts the observation to a latent space before optimization but similar to SDEdit (Meng et al. 2021) we propose to replace GANs with diffusion models (Ho, Jain, and Abbeel 2020; Song et al. 2020) as generative priors. Unlike GAN inversion, which relies on an auxiliary trained encoder, diffusion model inversion uses differential equations. In SDEdit, random noise is injected into the degraded observation to partially invert it in order to subsequently revert the process using a stochastic differential equation (SDE). As more noise is injected, a higher degree of realism is ensured, but at the expense of fidelity due to the additional loss of information caused by the noise randomness and the non-deterministic sampling from the SDE. We found that in some cases, as the degree of degradation increases, the method requires too high a degree of noise injection to work, forcing a choice between realism or fidelity. CODE refines SDEdit (Meng et al. 2021) by leveraging the probability-flow Ordinary Differential Equation (ODE) (Song et al. 2020), ensuring bijective correspondence with latent spaces. We use Langevin dynamics with score-based updates for correction, followed by the probability-flow ODE to project the adjusted latent representation back into the image space. This decouples noise injection levels, correction levels, and latent spaces, enhancing control over the editing process. Furthermore, CODE introduces a confidence-based clipping method that relies on the marginal distribution of each latent space. This method allows for the disregard of certain image information based on probability, which synergizes with our editing method. Our experimental results show CODE's superiority over SDEdit in realism and fidelity, especially in challenging scenarios."}, {"title": "2 Background", "content": "A detailed comparison of the requirements of state-of-the-art methods is provided in the Appendix A.\nInverse Problems In the inverse problem setup, methods are designed to leverage sensible assumptions on the degradation operators. When combined with powerful generative models such as diffusion models, these approaches have achieved outstanding results, setting new benchmarks in the field (Saharia et al. 2022; Liang et al. 2021; Kawar et al. 2022; Murata et al. 2023; Zhu et al. 2023; Chung et al. 2023; Wang, Yu, and Zhang 2022). Several subcategories of the inverse problem setting, like blind linear problems and non-blind non-linear problems, drop some assumptions about the degradation operators and, therefore, extend their applicability. However, while producing exceptional results in controlled applications like deblurring and super-resolution, the necessity for assumptions on the degradation operator makes them often impractical for unknown corruptions or in real-world scenarios. Consequently, these methods are not directly applicable to our context, where such exact information is typically unavailable. DDRM (Kawar et al. 2022), DDNM (Wang, Yu, and Zhang 2022), GibbsDDRM (Murata et al. 2023), DPS (Chung et al. 2023), and DiffPIR (Zhu et al. 2023) belong to this category.\nConditional Generative Models with Paired Data A parallel approach involves conditioning a generative model on a degraded image. Most methods in this category require training with pairs of degraded and natural images (Mirza and Osindero 2014; Isola et al. 2017; Batzolis et al. 2021; Xia et al. 2023; Li et al. 2023; Liu et al. 2023; Chung, Kim, and Ye 2023). Additionally, these methods often depend on carefully designed loss functions and guidance mechanisms to enhance performance, as demonstrated by (Song et al. 2020). Conditional Generative Adversarial Networks, as explored in (Isola et al. 2017), exemplify this approach, where generative models are trained to regenerate the original sample when conditioned on its version in another domain. However, when the degradation process is unknown or varies widely, these models struggle to generalize effectively, rendering them less applicable and not comparable to our method, which operates without such constraints. DDB (Chung, Kim, and Ye 2023) and I2SB (Liu et al. 2023) fall into this category.\nUnsupervised Bridge Problem with Unpaired Datasets In scenarios where two distinct datasets of clean and degraded data are available without direct paired data, methodologies based on principles like cycle consistency and realism have been developed, as evidenced by the works of (Zhu et al. 2017) using GANs (Goodfellow et al. 2014) and (Su et al. 2023) using Diffusion Models (Ho, Jain, and Abbeel 2020),(Sohl-Dickstein et al. 2015). A direct application of such methods to our scenario is not feasible due to the need for datasets of degraded images, which would hamper the ability to generalize to unseen corruptions.\nBlind Image Restoration with task-specific or domain-specific information Blind Image Restoration methods aim to handle a variety of degradations without restricting themselves to specific types. A recent trend in this field is the transposition of the problem into a latent space where corrections are made based on prior distributions. Notable works in this area include (Abdal, Qin, and Wonka 2019, 2020; Chan et al. 2021; Zhu et al. 2020; Poirier-Ginter and Lalonde 2023) have explored various aspects of this approach, utilizing GAN inversion, or VAE/VQVAE encoding (Kingma and Welling 2013; Oord, Vinyals, and Kavukcuoglu 2017), and have achieved significant advancements, particularly in scenarios involving light but diverse degradations. Usually, methods for blind image restoration incorporate domain-specific information (Zhou et al. 2022; Wang et al. 2021a; Gu et al. 2022) or task-specific guidances (Fei et al. 2023). Moreover, several methods (Lin et al. 2023; Yang et al. 2023) rely on the combination of many blocks trained separately (such as Real-ESRGAN (Wang et al. 2021b)) and incorporate different task-specific information (e.g., different restorers), making it even harder to ensure resilience to diverse degradations.\nSDEdit and ILVR The works by Meng et al. in (Meng et al. 2021) and Choi et al. in (Choi et al. 2021) inspired the formulation of CODE. SDEdit relies on the stochastic exploration of a given input's neighborhood to yield realistic and faithful outputs within a limited editing range. This method, tested for robustness by (Gao et al. 2022), bears similarities to the proposed gradient updates within the latent space of GAN models but is grounded in more solid theoretical foundations. ILVR is an iterative conditioning method designed to generate diverse images that share semantics with a downsampled guidance image. However, it requires a clean image for downsampling, which is not feasible in our scenario where the input guidance is already corrupted. Downsampling in this context would exacerbate information loss, making ILVR unsuitable for our application."}, {"title": "Preliminary - Diffusion Models", "content": "Denoising Diffusion Probabilistic Models We denote xo the data from the data distribution, in our case natural images, and x1, ..., xy the latent variables. The forward process is in DDPM (Ho, Jain, and Abbeel 2020) then defined by:\n$x_{t+1} = \\sqrt{\\alpha_t}x_t + \\sqrt{(1 - \\alpha_t)} \\cdot \\epsilon$, with $\\epsilon \\sim N(0, I)$,\nWhere at is a schedule predefined as an hyperparameter.\nThe diffusion model ee is then trained to minimize\n$E_{x,t} ||\\epsilon_\\theta (x_t, t) - \\epsilon||$.\nScore-based Generative Models In the case of Score-based Generative Models (Song and Ermon 2019; Song et al. 2020; Song and Ermon 2020), the model se learns to approximate the score function, \u2207x log p(x), by minimizing:\n$E_{p(x)} ||s_\\theta(x) - \\sqrt{\\lambda} \\nabla_x \\log p(x) ||$.\nThe most common approach to solve this is denoising score matching (Vincent 2011), which is further described in the Appendix C.\nCrucially, one can sample from p(xt) while using only the score function through Langevin dynamics (Langevin 1908) sampling by repeating the following update step:\n$x_{t+1} = x_t + \\epsilon \\cdot s_\\theta (x_t, t) + \\sqrt{2\\epsilon} \\cdot \\eta$, with $\\eta \\sim N(0, \\sigma^2)$.\t\t\t\t\t\t(1)"}, {"title": "3 Method: Confident Ordinary Differential Editing", "content": "Editing with Ordinary Differential Equations\nOur approach, described in Figure 2, formulates a theoretically grounded method for mapping OoD samples to in-distribution ones.\nFrom Gaussian Perturbation to Ordinary Inversion Our method draws inspiration from SDEdit (Meng et al. 2021) but introduces significant enhancements. SDEdit inverts the diffusion process by injecting Gaussian noise into the input image and then using this noisy image as a starting point to generate an image using DDPM (Ho, Jain, and Abbeel 2020). This process involves a trade-off between fidelity and realism: more noise results in more realistic images but less fidelity to the original input.\nIn contrast, we propose inverting the Probability-Flow Ordinary Differential Equation (ODE) as a superior alternative to noise injection. This approach maintains the fidelity of the reconstructed image by avoiding extra noise. The inversion process and its reverse operation ensure precise image reconstruction, limited only by approximation errors (Su et al. 2023). Unlike SDEdit, which requires increasing noise levels to revert to deeper latent spaces, our method allows inversion to any latent space along the ODE trajectory while preserving image integrity. This decouples the noise injection level from the depth of inversion. We use the ODE solver from DDIMS (Song, Meng, and Ermon 2020) in our experiments.\nThe primary motivation for inverting the degraded image is the model's ability to process out-of-distribution images. Direct estimation of the score on degraded images is impractical due to the poor performance of the score estimation on OoD data. By mapping the corrupted input back to the latent space, we obtain more accurate estimates within a distribution closely resembling a multivariate Gaussian. This concept was foundational to SDEdit; however, their reliance on noise injection prevented full inversion of the diffusion process without losing information from the observation.\nLangevin Dynamics in Latent Spaces There exists a direct correspondence between DDPM, \u20ac\u03b8, and Noise Conditional Score Network, se, such that se(x,t) =  (x,t).\nBuilding upon that, we propose to perform gradient-update in our latent spaces utilizing Langevin dynamics as in equation (1) to increase the likelihood of our latent representation. The method is described in the Appendix D, Algorithm 2. Analogous to SDEdit and contrasting with alternative methods, our editing method can be tailored to prioritize either realism or fidelity by selecting the step size in the Langevin dynamics and the latent spaces where to optimize."}, {"title": "Confidence Based Clipping (CBC)", "content": "Here, we present a clipping method for the latent codes applied during the encoding process that does not depend on the prediction or the original sample. The proof is available in Appendix B.\nProposition 1. Let I be the cumulative distribution function of N(0, 1) and let xo \u2208 [-1,1]. For at \u2208 [0, 1], \u2200t \u2208 [0, 1], assume that x+ ~ N(\u221a\u03b1\u0390\u00b7\u03b1\u03bf, \u221a1 \u2013 at \u00b7 I). Then, for all \u0ed7:\n$P(x_t \\in [-\\sqrt{\\alpha_t} - \\eta \\cdot \\sqrt{1 - \\alpha_t}, \\sqrt{\\alpha_t} + \\eta \\cdot \\sqrt{1 - \\alpha_t}]) > \\Phi(\\eta)-\\Phi(-\\eta)$.\nSpecifically, for \u03b7 = 2:\n$P(x_t \\in [-\\sqrt{\\alpha_t} - 2 \\cdot \\sqrt{1 - \\alpha_t}, \\sqrt{\\alpha_t} + 2 \\cdot \\sqrt{1 - \\alpha_t}]) \\geq 0.95$.\nDuring the encoding process, we propose to clip the latent codes using a confidence interval derived from Proposition 1. Confidence-based clipping is performed as follows:\n$x_{clipped} = Clip(x_t, min = -\\sqrt{\\alpha_t} - \\eta \\cdot \\sqrt{1 - \\alpha_t}, max = \\sqrt{\\alpha_t} + \\eta \\cdot \\sqrt{1 - \\alpha_t})$, where t is the timestep, at is the predefined schedule of the DM, and n is the chosen confidence parameter.\nSimilar to our editing method, CBC is agnostic to the input and suitable for blind restoration scenarios. We combine CBC with our ODE editing method to form our complete method, CODE, detailed in Algorithm 1. As shown in Figure 9, the two methods synergize efficiently. It is crucial to note that CBC cannot be used in combination with SDEdit."}, {"title": "4 Experiments", "content": "Setup We use open-source pre-trained DDPM models (Ho, Jain, and Abbeel 2020) from HuggingFace, specifically the EMA checkpoints of DDPM models trained on CelebA-HQ (Karras et al. 2018), LSUN-Bedroom, and LSUN-Church (Yu et al. 2016), all at 256x256 resolution. For all experiments, DDIM inversion (Song and Ermon 2020) with 200 steps is utilized. Enhancement follows the complete Algorithm 3 described in Appendix D. It is used with N = 200 Langevin iteration steps, a step size e of [10\u22122, 10-3] for shallow latent spaces (up to L = 40), and [10\u22125, 10-6] for deeper latent spaces (L> 100). We use K = 4 annealing steps and a = 0.8 as the annealing coefficient. When activating CBC, we use \u03b7 = 1.7. A full description of the setup being used to automatically compute the metrics is provided in Appendix E. For SDEdit, samples are generated with L in [300, 500, 700] steps.\nWe tested our approach on 47 corruption types, including 17 from (Hendrycks and Dietterich 2019) (noise, blur, weather, and digital artifacts) and 28 from (Mintun, Kirillov, and Xie 2021). The corruption codebases are publicly available\u00b9 \u00b2. Additionally, we introduced two masking types: masking entire vertical lines and random pixels with random colors. Unlike traditional masking in masked autoencoders (He et al. 2022), our method does not assume knowledge of masked pixels' positions, posing a more realistic recovery task. CODE operates completely blind to the corruption type, with no knowledge of the specific task or affected pixels. For each corruption type, we test on at least 500 corrupted images. For each image, we kept the best 4 samples generated based on PSNR with respect to the original non-degraded images.\nBaselines Our main baseline is the domain-agnostic method SDEdit (Meng et al. 2021), the only one comparable to ours in terms of requirements and assumptions. On CelebAHQ, the performance is also qualitatively benchmarked against domain-specific SOTA models, namely CodeFormer (Zhou et al. 2022), GFPGAN (Wang et al. 2021a), and DiffBIR (Lin et al. 2023). We also conducted visual experiments on LSUN-Bedroom and LSUN-Church to demonstrate the efficacy of CODE over diverse domains similar to SDEdit in (Meng et al. 2021).\nEvaluation Metrics We evaluate our results using PSNR, SSIM, LPIPS, and FID. PSNR and SSIM are measured against the corrupted image (input) to assess fidelity to the guidance. FID is used to evaluate the quality of our generated images. Given the absence of assumptions about the input and corruptions, a key metric is the trade-off between realism and fidelity-specifically, the gain in realism relative to a given loss in fidelity. To quantify this, we use L2 distance in the pixel space as a measure of fidelity and FID as a measure of realism, plotting them against each other in Figure 5. Additionally, we report LPIPS with respect to the original, non-degraded image (source) to assess reconstruction quality. This metric is particularly informative for evaluating each corruption individually, as it also reflects the complexity of the corruption, with detailed results provided in Appendix G.\nResults We present a brief qualitative comparison of results in Figure 3 to showcase that most methods, without further assumptions, cannot perform properly. In the vast majority of scenarios involving severe corruption like contrast, random pixel masking, or fog, only SDEdit and CODE can generate convincing images. For less intensive corruptions, which typically include erasing fine details or introducing minor noise, most baseline models tend to perform well. We provide extensive results in Appendix E. For quantitative metrics, we focus on SDEdit and CODE and compare them using the same pre-trained model on CelebA-HQ. Consequently, the differences come only from the way the pre-trained diffusion model is leveraged. Average metrics across the employed corruptions are detailed in Table 1. CODE outperforms SDEdit by 36% in FID-score while maintaining a fidelity to the input (PSNR-Input) 5% higher than SDEdit. Moreover, the standard deviation of the FID score highlights that SDEdit fails in certain cases while CODE is more stable. Finally, we report in Figure 5 the trade-off curves between fidelity and realism for both CODE and SDEdit. We performed a polynomial regression on CODE and SDEdit results to obtain such a curve. Both methods offer hyper-parameters to control such trade-offs. However, we highlight that CODE offers a better possibility. Overall, CODE generates more realistic outputs for a given degree of fidelity."}, {"title": "5 Ablation Study", "content": "Analysis of Hyperparameters\nNumber of Updates. As shown in Figure 6, the number of update iterations conducted in a latent space is pivotal for ensuring convergence and reducing variability. In practice, we employed 300 steps in all our experiments.\nStep Size. The step size emerges as a critical parameter. A smaller step size results in high fidelity to the input and low variability among generated samples, albeit compromising realism. Conversely, an increased step size enhances realism and variability, as depicted in Figure 7a. As the number of updates is fixed in our experiments, the step size is what governs the size of the explored neighborhood around the input image. As a result, its impact is related to the amount of noise injected in SDEdit.\nLatent Space Choice. The choice of latent space significantly influences the type of changes made during updates. As shown in Figure 7b, updates in a shallow latent space lead to minor but detailed and realistic modifications. In contrast, updates in deeper latent spaces can cause more significant or complex changes. Interestingly, regarding stroke guidance, optimization in the deepest latent space led to the addition of text and lines to the image. This suggests that the training set likely contained numerous images with these text and lines, implying that their inclusion by the model significantly enhanced the image's likelihood. Empirically, we found that the deeper the latent space, the less the notion of distance is close to an L2 pixel-based distance.\nThe optimal latent space is not one-size-fits-all but depends on the specific input being processed. For complex corruptions, using a mix of updates in different latent spaces proves most effective. On the other hand, shallow latent spaces are best for addressing simple corruptions like blur. This ability to independently select the latent space without affecting other parameters, such as the level of noise injection, is a key strength of our editing method. We disentangle what was previously a single parameter into multiple, allowing for tailored optimization on a per-sample basis.\nFidelity. Our editing method is anchored in the corrupted sample, hence the generation is very impacted by the variations in the corruption. As shown in Figure 8, the outputs are faithful to the corrupted image and do not map to a single ground-truth image."}, {"title": "Impact of Confidence-Based Clipping", "content": "In this section, we study the impact of the confidence parameter \u03b7 in CBC. As we reduce \u03b7, the interval shrinks, keeping only the most likely pixel values. We propose to encode an image using DDIM with different values of n and decode it back to see the result. We study this in the case of in-distribution images and of corrupted images. Results can be seen in Figure 9b. A smaller \u03b7 results in a loss of fine-grained details, a shift of the average tone, and the removal of unlikely pixels such as masked pixels. When applied to corrupted samples, CBC proves efficient in removing part of the noisy artifact while keeping most of the image structure. Interestingly, CBC also stabilizes the DDIM inversion, which might sometimes be inconsistent. This allows for fewer steps in the encoding-decoding procedure and speeds up the whole editing process."}, {"title": "Ablation Study", "content": "We propose to study the impact of each block in CODE while keeping SDEdit for comparison. Qualitative results are visible in Figure 9a. While both our editing method alone (w/o CBC) and SDEdit excel at adding extra fine-grained details, they fail at handling unknown masks or color shits efficiently. On the contrary, CBC basically fails at adding extra details but successfully recovers certain color shifts or masked areas. As a result, combining both into CODE leads to powerful synergies. Quantitative results can be seen in Table 2."}, {"title": "Discussion", "content": "While CODE offers enhanced versatility and control in editing, it introduces greater complexity compared to SDEdit. SDEdit's tuning is straightforward, with binary success or failure outcomes, whereas CODE's dual hyper-parameter framework requires a more extensive grid search, increasing the search complexity quadratically. However, this added complexity enables CODE to achieve better results across a wider range of scenarios."}, {"title": "6 Conclusion", "content": "We introduce Confident Ordinary Differential Editing, a novel approach for guided image editing and synthesis that handles OoD inputs and balances realism and fidelity. Our method eliminates the need for retraining, finetuning, data augmentation, or paired data, and it integrates seamlessly with any pre-trained Diffusion Model. CODE excels in addressing a wide array of corruptions, outperforming existing methods that often rely on handcrafted features. As an evolution of SDEdit, our approach provides enhanced control, variety, and capabilities in editing by disentangling the original method and introducing additional hyperparameters. These new parameters not only offer deeper insights into the functioning of Diffusion Models' latent spaces but also enable more diverse editing strategies. Furthermore, we introduce a Confidence-Based Clipping method that synergizes effectively with our editing technique, allowing the disregard of unlikely pixels or areas in a completely agnostic manner. Finally, our extensive study of the different components at play offers a greater understanding of the underlying mechanics of diffusion models, enriching the field's knowledge base. Our findings reveal that CODE surpasses SDEdit in versatility and quality while maintaining its strengths across various tasks, including stroke-based editing. We hope our work inspires further innovations in this domain, akin to the transformative impact of GAN inversion. Looking ahead, we see potential in automating the editing and hyperparameter search processes and exploring synergies with text-to-image synthesis."}, {"title": "D Detailed Algorithms", "content": "Algorithm 2: ODE editing\nRequire: N (Langevin iterations), \u0454 (step-size), xo (Observation), L (L-th latent-space).\n$X_{L,0} = ODE\\_SOLVER_{forward_{0 \\rightarrow L}} (x_O)$\nfor k = 0 to N - 1 do\n$X_{L,k+1} = X_{L,k} + \\epsilon \\cdot s_\\theta(X_{L,k}, L) + \\sqrt{2\\epsilon} \\cdot \\eta$, where $\\eta \\sim N(0, I)$\nend for\n$x_O = ODE\\_SOLVER_{backward_{L \\rightarrow 0}} (X_{L,N})$\nAlgorithm 3: CODE - Annealed Multi-Latent with CBC\nRequire: N (Langevin iterations), \u0454 (step-size), a (Annealing coefficient), K Number of annealing steps, xo (Observation), L1, ..., Lj (List of latent-spaces optimized), \u03b7 (size of the confidence interval.)\nLo = 0\n$X_{L,0} = ODE\\_SOLVER_{forward_{O \\rightarrow L_i}} (Clip_{CBC, \\eta}(x_O))$\nfor l = j to 1 do\n$\\,\\,\\epsilon_0 = \\epsilon$\nfor k = 0 to K - 1 do\n$\\,\\,\\,$for n = 0 to N - 1 do\n$\\qquad\\,\\,\\,\\,X_{L_i,n+1} = X_{L_i,n} + \\epsilon_k\\cdot s_\\theta(X_{L_i,n}, L_i) + \\sqrt{2\\epsilon_k} \\cdot \\eta$, where $\\eta \\sim N(0, I)$\n$\\qquad\\,\\,\\,\\,$end for\n$\\qquad\\,\\,\\,\\,\\epsilon_{k+1} \\leftarrow \\epsilon_k \\cdot a$\n$\\qquad\\,\\,\\,X_{L_1,0} \\leftarrow X_{L_L,N}$\n$\\,\\,$end for\n$X_{L_{i-1},0} = ODE\\_SOLVER_{backward_{L_i \\rightarrow L_{i-1}}} (X_{L_{i},N})$\nend for\nreturn X_{L0,0}\nAlgorithm 4: SDEdit (VP-SDE formulation)\nRequire: x(9) (guide), to (SDE hyper-parameter), N (total denoising steps), K (total repeats)\n$\\Delta t \\leftarrow \\frac{t_0}{N}$\n$\\alpha(t_0) \\leftarrow \\prod_{i=1}^{N} (1 - \\beta(i) \\Delta t)$\nfor k = 1 to K do\n$\\quad x \\sim N(0, I)$\n$\\quad x \\leftarrow \\sqrt{\\alpha(t_0)}x + \\sqrt{1 - \\alpha(t_0)}z$\n$\\quad$for n $\\leftarrow$ N to 1 do\n$\\qquad$t $\\leftarrow$ to$\\frac{n}{N}$\n$\\qquad z \\sim N(0, I)$\n$\\qquad x \\leftarrow \\frac{1}{\\sqrt{1-\\beta(t) \\Delta t}} (x + \\beta(t) \\Delta t s_\\theta(x, t)) + \\sqrt{\\beta(t) \\Delta t}z$\n$\\quad$end for\nend for\nreturn x"}, {"title": "E Additional Details", "content": "Metrics\nIn the initial phase of filtering, we processed a multitude of samples for each corrupted image. This yielded approximately 1.6 million samples across 500 unique faces, 47 distinct types of corruption, 4 algorithms (CODE, CODE w/o CBC, SDEdit, CBC), 2 or 3 latents (3 in case of SDEdit, 2 in case of all CODE variations), 5 epsilons, step sizes, for all CODE variations, and 2 to 4 generated images for stochasticity. We automatically selected the top 4 samples for each based on the psnr to source metric, mimicking a human selection. Consequently, given the number of cases to benchmark, the diversity is 500 unique faces.\nHyperparameters\nTo generate the different samples we use the following sets of hyperparameters. For SDEdit we generate samples using 300 steps, 500 steps and 700 steps. For CODE, we first use a eta (CBC parameter) of 1.7 to encode the corrupted images. Then we generate samples using two sets of latent, either 200 and 40, either 40 only. The former being for deep latent correction and the latter for shallow corrections. We use 200 updates langevin steps in each selected latent space. We generate samples using five different step size ranging from le-5 to le- 3. we use annealing for the step size every 40 langevin update, with a annealing constant of 0.8. All experiments have been conducted on Nvidia GPUs (A100, V100, RTX3090). Please refer to the Github for packages requirements."}]}