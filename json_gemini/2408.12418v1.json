{"title": "CODE: Confident Ordinary Differential Editing", "authors": ["Bastien van Delft", "Tommaso Martorella", "Alexandre Alahi"], "abstract": "Conditioning image generation facilitates seamless editing and\nthe creation of photorealistic images. However, conditioning\non noisy or Out-of-Distribution (OoD) images poses signifi-\ncant challenges, particularly in balancing fidelity to the input\nand realism of the output. We introduce Confident Ordinary\nDifferential Editing (CODE), a novel approach for image syn-\nthesis that effectively handles OoD guidance images. Utilizing\na diffusion model as a generative prior, CODE enhances im-\nages through score-based updates along the probability-flow\nOrdinary Differential Equation (ODE) trajectory. This method\nrequires no task-specific training, no handcrafted modules,\nand no assumptions regarding the corruptions affecting the\nconditioning image. Our method is compatible with any diffu-\nsion model. Positioned at the intersection of conditional image\ngeneration and blind image restoration, CODE operates in\na fully blind manner, relying solely on a pre-trained genera-\ntive model. Our method introduces an alternative approach to\nblind restoration: instead of targeting a specific ground truth\nimage based on assumptions about the underlying corruption,\nCODE aims to increase the likelihood of the input image\nwhile maintaining fidelity. This results in the most probable\nin-distribution image around the input. Our contributions are\ntwofold. First, CODE introduces a novel editing method based\non ODE, providing enhanced control, realism, and fidelity\ncompared to its SDE-based counterpart. Second, we introduce\na confidence interval-based clipping method, which improves\nCODE's effectiveness by allowing it to disregard certain pix-\nels or information, thus enhancing the restoration process in\na blind manner. Experimental results demonstrate CODE's\neffectiveness over existing methods, particularly in scenarios\ninvolving severe degradation or OoD inputs.", "sections": [{"title": "1 Introduction", "content": "Conditional image generation consists of guiding the cre-\nation of content using different sorts of conditioning, such\nas text, images, or segmentation maps. Our research focuses\non scenarios where the guidance is an Out-of-Distribution\n(OoD) image relative to the training data distribution. This\nis especially relevant for handling corrupted images, similar\nto denoising or restoration methods. The main challenge in\nthese scenarios is balancing fidelity to the input with realism\nin the generated images. Traditional methods for restoring\ncorrupted images, such as Image-to-Image Translation or\nStyle Transfer, are limited by the need for distinct datasets per\nstyle or per noise. Another approach models the corruption\nfunction as an inverse problem, requiring detailed knowledge\nof each possible corruption, making it impractical for most\nreal unknown OoD scenarios. Guided image synthesis for\nOoD inputs aims to rectify corrupted images without prior\nknowledge of the corruption, positioning it as Blind Image\nRestoration (BIR). Despite recent advancements, achieving\nhuman-level generalization remains challenging.\nOur work aims to generate realistic and plausible images\nfrom potentially corrupted inputs using only a pre-trained\ngenerative model, without additional data augmentation or\nfinetuning on corrupted data, and without any specific as-\nsumption about the corruptions. Unlike other BIR methods\nthat strive to reconstruct a ground-truth image relying on spe-\ncific guidance or human-based assumptions, our approach is\nfully blind, seeking to maximize the input image's likelihood\nwhile minimizing modifications to the input image. As such,\nwe differ from traditional BIR approaches.\nBIR is inherently ill-posed due to the loss of information\nfrom unknown degradation, necessitating auxiliary informa-\ntion to enhance restoration quality. Previous approaches have\nincorporated domain-specific priors such as facial heatmaps\nor landmarks (Chen et al. 2018, 2021; Yu et al. 2018), but\nthese degrade with increased degradation and lack versatility.\nGenerative priors from pre-trained models like GANs (Chan\net al. 2021; Zhou et al. 2022; Yang et al. 2021; Pan et al. 2021;\nMenon et al. 2020) become unstable with severe degradation,\nleading to unrealistic reconstructions. Methods like (Wang\net al. 2021a) combine facial priors with generative priors to\nimprove fidelity but fail under extreme degradation. In (Meng\net al. 2021), the authors replace GANs with diffusion models\n(Ho, Jain, and Abbeel 2020; Song et al. 2020) as generative\npriors. However, as the degradation increases, the method\nforces a choice between realism and fidelity.\nBIR still fails to achieve faithful and realistic reconstruc-\ntion for a wide range of corruptions on a wide range of images.\nDealing with various unknown corruptions prevents inverse\nmethods from being easily applicable while dealing with a\nwide range of images prevents relying on carefully designed\ndomain priors. We introduce Confident Ordinary Differen-\ntial Editing (CODE), an unsupervised method that generates\nfaithful and realistic image reconstructions from a single de-\ngraded image without information on the degradation type,\neven under severe conditions. CODE leverages the generative\nprior of a pre-trained diffusion model without requiring addi-\ntional training or finetuning. Consequently, it is compatible\nwith any pre-trained Diffusion Model and any dataset. CODE\noptimizes the likelihood of the generated image while con-\nstraining the distance to the input image, framing restoration\nas an optimization problem. Similar to GAN-inversion meth-\nods (Tov et al. 2021; Abdal, Qin, and Wonka 2020, 2019;\nZhu et al. 2020; Menon et al. 2020; Pan et al. 2021), CODE\ninverts the observation to a latent space before optimization\nbut similar to SDEdit (Meng et al. 2021) we propose to re-\nplace GANs with diffusion models (Ho, Jain, and Abbeel\n2020; Song et al. 2020) as generative priors. Unlike GAN\ninversion, which relies on an auxiliary trained encoder, diffu-\nsion model inversion uses differential equations. In SDEdit,\nrandom noise is injected into the degraded observation to\npartially invert it in order to subsequently revert the process\nusing a stochastic differential equation (SDE). As more noise\nis injected, a higher degree of realism is ensured, but at the\nexpense of fidelity due to the additional loss of information\ncaused by the noise randomness and the non-deterministic\nsampling from the SDE. We found that in some cases, as\nthe degree of degradation increases, the method requires too\nhigh a degree of noise injection to work, forcing a choice be-\ntween realism or fidelity. CODE refines SDEdit (Meng et al.\n2021) by leveraging the probability-flow Ordinary Differen-\ntial Equation (ODE) (Song et al. 2020), ensuring bijective\ncorrespondence with latent spaces. We use Langevin dynam-\nics with score-based updates for correction, followed by the\nprobability-flow ODE to project the adjusted latent repre-\nsentation back into the image space. This decouples noise\ninjection levels, correction levels, and latent spaces, enhanc-\ning control over the editing process. Furthermore, CODE\nintroduces a confidence-based clipping method that relies on\nthe marginal distribution of each latent space. This method al-\nlows for the disregard of certain image information based on\nprobability, which synergizes with our editing method. Our\nexperimental results show CODE's superiority over SDEdit\nin realism and fidelity, especially in challenging scenarios."}, {"title": "2 Background", "content": "A detailed comparison of the requirements of state-of-the-art\nmethods is provided in the Appendix A.\nIn the inverse problem setup, methods\nare designed to leverage sensible assumptions on the degra-\ndation operators. When combined with powerful genera-\ntive models such as diffusion models, these approaches have\nachieved outstanding results, setting new benchmarks in the\nfield (Saharia et al. 2022; Liang et al. 2021; Kawar et al.\n2022; Murata et al. 2023; Zhu et al. 2023; Chung et al. 2023;\nWang, Yu, and Zhang 2022). Several subcategories of the\ninverse problem setting, like blind linear problems and non-blind non-linear problems, drop some assumptions about\nthe degradation operators and, therefore, extend their appli-\ncability. However, while producing exceptional results in\ncontrolled applications like deblurring and super-resolution,\nthe necessity for assumptions on the degradation operator\nmakes them often impractical for unknown corruptions or in\nreal-world scenarios. Consequently, these methods are not\ndirectly applicable to our context, where such exact infor-\nmation is typically unavailable. DDRM (Kawar et al. 2022),\nDDNM (Wang, Yu, and Zhang 2022), GibbsDDRM (Murata\net al. 2023), DPS (Chung et al. 2023), and DiffPIR (Zhu et al.\n2023) belong to this category.\nA par-\nallel approach involves conditioning a generative model on\na degraded image. Most methods in this category require\ntraining with pairs of degraded and natural images (Mirza\nand Osindero 2014; Isola et al. 2017; Batzolis et al. 2021; Xia\net al. 2023; Li et al. 2023; Liu et al. 2023; Chung, Kim, and\nYe 2023). Additionally, these methods often depend on care-\nfully designed loss functions and guidance mechanisms to\nenhance performance, as demonstrated by (Song et al. 2020).\nConditional Generative Adversarial Networks, as explored in\n(Isola et al. 2017), exemplify this approach, where generative\nmodels are trained to regenerate the original sample when\nconditioned on its version in another domain. However, when\nthe degradation process is unknown or varies widely, these\nmodels struggle to generalize effectively, rendering them less\napplicable and not comparable to our method, which operates\nwithout such constraints. DDB (Chung, Kim, and Ye 2023)\nand I2SB (Liu et al. 2023) fall into this category.\nIn scenarios where two distinct datasets of clean and de-\ngraded data are available without direct paired data, method-\nologies based on principles like cycle consistency and real-\nism have been developed, as evidenced by the works of (Zhu\net al. 2017) using GANs (Goodfellow et al. 2014) and (Su\net al. 2023) using Diffusion Models (Ho, Jain, and Abbeel\n2020),(Sohl-Dickstein et al. 2015). A direct application of\nsuch methods to our scenario is not feasible due to the need\nfor datasets of degraded images, which would hamper the\nability to generalize to unseen corruptions.\nInverse Problems"}, {"title": "Conditional Generative Models with Paired Data"}, {"title": "Unsupervised Bridge Problem with Unpaired Datasets"}, {"title": "Blind Image Restoration with task-specific or domain-specific information", "content": "Blind Image Restoration methods aim\nto handle a variety of degradations without restricting them-\nselves to specific types. A recent trend in this field is the\ntransposition of the problem into a latent space where correc-\ntions are made based on prior distributions. Notable works\nin this area include (Abdal, Qin, and Wonka 2019, 2020;\nChan et al. 2021; Zhu et al. 2020; Poirier-Ginter and Lalonde\n2023) have explored various aspects of this approach, uti-\nlizing GAN inversion, or VAE/VQVAE encoding (Kingma\nand Welling 2013; Oord, Vinyals, and Kavukcuoglu 2017),\nand have achieved significant advancements, particularly in\nscenarios involving light but diverse degradations. Usually,\nmethods for blind image restoration incorporate domain-specific information (Zhou et al. 2022; Wang et al. 2021a; Gu\net al. 2022) or task-specific guidances (Fei et al. 2023).\nMoreover, several methods (Lin et al. 2023; Yang et al.\n2023) rely on the combination of many blocks trained sep-\narately (such as Real-ESRGAN (Wang et al. 2021b)) and\nincorporate different task-specific information (e.g., differ-\nent restorers), making it even harder to ensure resilience to\ndiverse degradations.\nThe works by Meng et al. in (Meng\net al. 2021) and Choi et al. in (Choi et al. 2021) inspired\nthe formulation of CODE. SDEdit relies on the stochastic\nexploration of a given input's neighborhood to yield realis-\ntic and faithful outputs within a limited editing range. This\nmethod, tested for robustness by (Gao et al. 2022), bears\nsimilarities to the proposed gradient updates within the latent\nspace of GAN models but is grounded in more solid theoret-\nical foundations. ILVR is an iterative conditioning method\ndesigned to generate diverse images that share semantics\nwith a downsampled guidance image. However, it requires\na clean image for downsampling, which is not feasible in\nour scenario where the input guidance is already corrupted.\nDownsampling in this context would exacerbate information\nloss, making ILVR unsuitable for our application."}, {"title": "Preliminary - Diffusion Models", "content": "We denote xo\nthe data from the data distribution, in our case natural images,\nand x1, ..., xy the latent variables. The forward process is in\nDDPM (Ho, Jain, and Abbeel 2020) then defined by:\nXt+1 = Xt\u221aat+ (1 \u2212 at) \u00b7 \u20ac, with \u2208 ~ N(0, I),\nWhere at is a schedule predefined as an hyperparameter.\nThe diffusion model ee is then trained to minimize\nExt,t ||60 (xt, t) \u2013 \u20ac|| .\nIn the case of Score-\nbased Generative Models (Song and Ermon 2019; Song et al.\n2020; Song and Ermon 2020), the model se learns to approx-\nimate the score function, \u2207x log p(x), by minimizing:\nEp(x) ||50(x) - \u221ax log p(x) || .\nThe most common approach to solve this is denoising score\nmatching (Vincent 2011), which is further described in the\nAppendix C.\nCrucially, one can sample from p(xt) while using only the\nscore function through Langevin dynamics (Langevin 1908)\nsampling by repeating the following update step:\nXt+1 = Xt+\u20ac\u00b7S9(xt, t) + \u221a2\u20ac\u00b7\u03b7, with \u03b7 ~ N(0, \u03c3\u00b2). (1)"}, {"title": "3 Method: Confident Ordinary Differential Editing", "content": "Our approach, described in Figure 2, formulates a theoret-\nically grounded method for mapping OoD samples to in-\ndistribution ones.\nOur\nmethod draws inspiration from SDEdit (Meng et al. 2021)\nbut introduces significant enhancements. SDEdit inverts the\ndiffusion process by injecting Gaussian noise into the input\nimage and then using this noisy image as a starting point to\ngenerate an image using DDPM (Ho, Jain, and Abbeel 2020).\nThis process involves a trade-off between fidelity and realism:\nmore noise results in more realistic images but less fidelity to\nthe original input.\nIn contrast, we propose inverting the Probability-Flow Or-\ndinary Differential Equation (ODE) as a superior alternative\nto noise injection. This approach maintains the fidelity of the\nreconstructed image by avoiding extra noise. The inversion\nprocess and its reverse operation ensure precise image re-\nconstruction, limited only by approximation errors (Su et al.\n2023). Unlike SDEdit, which requires increasing noise levels\nto revert to deeper latent spaces, our method allows inversion\nto any latent space along the ODE trajectory while preserving\nimage integrity. This decouples the noise injection level from\nthe depth of inversion. We use the ODE solver from DDIMS\n(Song, Meng, and Ermon 2020) in our experiments.\nThe primary motivation for inverting the degraded image\nis the model's ability to process out-of-distribution images.\nDirect estimation of the score on degraded images is imprac-\ntical due to the poor performance of the score estimation on\nOoD data. By mapping the corrupted input back to the latent\nspace, we obtain more accurate estimates within a distribu-\ntion closely resembling a multivariate Gaussian. This concept\nwas foundational to SDEdit; however, their reliance on noise\ninjection prevented full inversion of the diffusion process\nwithout losing information from the observation."}, {"title": "Editing with Ordinary Differential Equations", "content": "From Gaussian Perturbation to Ordinary Inversion"}, {"title": "Langevin Dynamics in Latent Spaces", "content": "There exists a di-\nrect correspondence between DDPM, \u20ac\u03b8, and Noise Condi-\ntional Score Network, se, such that se(x,t) = . (x,t)\nBuilding upon that, we propose to perform gradient-update\nin our latent spaces utilizing Langevin dynamics as in equa-\ntion (1) to increase the likelihood of our latent representation.\nThe method is described in the Appendix D, Algorithm 2.\nAnalogous to SDEdit and contrasting with alternative meth-\nods, our editing method can be tailored to prioritize either\nrealism or fidelity by selecting the step size in the Langevin\ndynamics and the latent spaces where to optimize."}, {"title": "Confidence Based Clipping (CBC)", "content": "Here, we present a clipping method for the latent codes ap-\nplied during the encoding process that does not depend on\nthe prediction or the original sample. The proof is available\nin Appendix \u0412.\nLet I be the cumulative distribution function\nof N(0, 1) and let xo \u2208 [-1,1]. For at \u2208 [0, 1], Vt \u2208 [0, 1],\nassume that xt ~ N(\u221a\u03b1\u0390\u00b7\u03b1\u03bf, \u221a1 \u2013 at \u00b7 I). Then, for all \u0ed7:\nP(xt \u2208 [-\u221a\u03b1\u03b9-\u03b7.\u221a\u221a1 \u2013 at, \u221aat+\u03b7\u00b7\u221a 1 \u2013 \u03b1\u03c4]) > \u03a6(\u03b7)-\u03a6(-7).\nSpecifically, for \u03b7 = 2:\nP(xt \u2208 [-\u221aat-2\u00b7\u221a1 \u2013 at, \u221aat+2\u00b7\u221a\u221a1 \u2013 at]) \u2265 0.95.\nDuring the encoding process, we propose to clip the latent\ncodes using a confidence interval derived from Proposition 1.\nConfidence-based clipping is performed as follows:\nxclipped = Clip(xt, min = \u2212\u221aat-n\u00b7\u221a1 \u2013 at, max = \u221aat+n\u00b7\u221a1 \u2212 at),\nwhere t is the timestep, at is the predefined schedule of the\nDM, and n is the chosen confidence parameter.\nSimilar to our editing method, CBC is agnostic to the input\nand suitable for blind restoration scenarios. We combine CBC"}, {"title": "4 Experiments", "content": "We tested our approach on 47 corruption types, includ-\ning 17 from (Hendrycks and Dietterich 2019) (noise, blur,\nweather, and digital artifacts) and 28 from (Mintun, Kir-\nillov, and Xie 2021). The corruption codebases are publicly\navailable\u00b9 2. Additionally, we introduced two masking types:\nmasking entire vertical lines and random pixels with random\ncolors. Unlike traditional masking in masked autoencoders\n(He et al. 2022), our method does not assume knowledge of\nmasked pixels' positions, posing a more realistic recovery"}, {"title": "Setup", "content": "We use open-source pre-trained DDPM models (Ho,\nJain, and Abbeel 2020) from HuggingFace, specifically the\nEMA checkpoints of DDPM models trained on CelebA-HQ\n(Karras et al. 2018), LSUN-Bedroom, and LSUN-Church (Yu\net al. 2016), all at 256x256 resolution. For all experiments,\nDDIM inversion (Song and Ermon 2020) with 200 steps\nis utilized. Enhancement follows the complete Algorithm 3\ndescribed in Appendix D. It is used with N = 200 Langevin\niteration steps, a step size e of [10\u22122, 10-3] for shallow latent\nspaces (up to L = 40), and [10\u22125, 10-6] for deeper latent\nspaces (L> 100). We use K = 4 annealing steps and\na = 0.8 as the annealing coefficient. When activating CBC,\nwe use \u03b7 = 1.7. A full description of the setup being used to\nautomatically compute the metrics is provided in Appendix E.\nFor SDEdit, samples are generated with L in [300, 500, 700]\nsteps."}, {"title": "Baselines", "content": "Our main baseline is the domain-agnostic\nmethod SDEdit (Meng et al. 2021), the only one comparable\nto ours in terms of requirements and assumptions. On Cele-bAHQ, the performance is also qualitatively benchmarked\nagainst domain-specific SOTA models, namely CodeFormer\n(Zhou et al. 2022), GFPGAN (Wang et al. 2021a), and Diff-BIR (Lin et al. 2023). We also conducted visual experiments\non LSUN-Bedroom and LSUN-Church to demonstrate the\nefficacy of CODE over diverse domains similar to SDEdit in\n(Meng et al. 2021)."}, {"title": "Evaluation Metrics", "content": "We evaluate our results using PSNR,\nSSIM, LPIPS, and FID. PSNR and SSIM are measured\nagainst the corrupted image (input) to assess fidelity to the\nguidance. FID is used to evaluate the quality of our generated\nimages. Given the absence of assumptions about the input\nand corruptions, a key metric is the trade-off between realism\nand fidelity-specifically, the gain in realism relative to a\ngiven loss in fidelity. To quantify this, we use L2 distance in\nthe pixel space as a measure of fidelity and FID as a mea-\nsure of realism, plotting them against each other in Figure 5.\nAdditionally, we report LPIPS with respect to the original,\nnon-degraded image (source) to assess reconstruction quality.\nThis metric is particularly informative for evaluating each\ncorruption individually, as it also reflects the complexity of\nthe corruption, with detailed results provided in Appendix G."}, {"title": "Results", "content": "We present a brief qualitative comparison of results\nin Figure 3 to showcase that most methods, without further\nassumptions, cannot perform properly. In the vast majority of\nscenarios involving severe corruption like contrast, random\npixel masking, or fog, only SDEdit and CODE can gener-\nate convincing images. For less intensive corruptions, which\ntypically include erasing fine details or introducing minor\nnoise, most baseline models tend to perform well. We provide\nextensive results in Appendix E. For quantitative metrics, we\nfocus on SDEdit and CODE and compare them using the\nsame pre-trained model on CelebA-HQ. Consequently, the\ndifferences come only from the way the pre-trained diffusion\nmodel is leveraged. Average metrics across the employed cor-\nruptions are detailed in Table 1. CODE outperforms SDEdit\nby 36% in FID-score while maintaining a fidelity to the in-\nput (PSNR-Input) 5% higher than SDEdit. Moreover, the\nstandard deviation of the FID score highlights that SDEdit\nfails in certain cases while CODE is more stable. Finally,\nwe report in Figure 5 the trade-off curves between fidelity\nand realism for both CODE and SDEdit. We performed a\npolynomial regression on CODE and SDEdit results to obtain\nsuch a curve. Both methods offer hyper-parameters to control\nsuch trade-offs. However, we highlight that CODE offers a\nbetter possibility. Overall, CODE generates more realistic\noutputs for a given degree of fidelity."}, {"title": "5 Ablation Study", "content": "As shown in Figure 6, the number\nof update iterations conducted in a latent space is pivotal for\nensuring convergence and reducing variability. In practice,\nwe employed 300 steps in all our experiments.\nThe step size emerges as a critical parameter. A\nsmaller step size results in high fidelity to the input and low\nvariability among generated samples, albeit compromising\nrealism. Conversely, an increased step size enhances realism\nand variability, as depicted in Figure 7a. As the number of\nupdates is fixed in our experiments, the step size is what\ngoverns the size of the explored neighborhood around the\ninput image. As a result, its impact is related to the amount\nof noise injected in SDEdit."}, {"title": "Analysis of Hyperparameters", "content": "Number of Updates."}, {"title": "Step Size.", "content": "The choice of latent space signifi-\ncantly influences the type of changes made during updates.\nAs shown in Figure 7b, updates in a shallow latent space lead\nto minor but detailed and realistic modifications. In contrast,\nupdates in deeper latent spaces can cause more significant or\ncomplex changes. Interestingly, regarding stroke guidance,\noptimization in the deepest latent space led to the addition\nof text and lines to the image. This suggests that the training\nset likely contained numerous images with these text and\nlines, implying that their inclusion by the model significantly\nenhanced the image's likelihood. Empirically, we found that\nthe deeper the latent space, the less the notion of distance is\nclose to an L2 pixel-based distance.\nThe optimal latent space is not one-size-fits-all but depends\non the specific input being processed. For complex corrup-\ntions, using a mix of updates in different latent spaces proves\nmost effective. On the other hand, shallow latent spaces are\nbest for addressing simple corruptions like blur. This abil-\nity to independently select the latent space without affecting\nother parameters, such as the level of noise injection, is a key\nstrength of our editing method. We disentangle what was pre-\nviously a single parameter into multiple, allowing for tailored\noptimization on a per-sample basis."}, {"title": "Latent Space Choice.", "content": "Our editing method is anchored in the corrupted\nsample, hence the generation is very impacted by the varia-\ntions in the corruption. As shown in Figure 8, the outputs are\nfaithful to the corrupted image and do not map to a single\nground-truth image."}, {"title": "Fidelity.", "content": "In this section, we\nstudy the impact of the confidence parameter \u03b7 in CBC. As\nwe reduce \u03b7, the interval shrinks, keeping only the most\nlikely pixel values. We propose to encode an image using\nDDIM with different values of n and decode it back to see\nthe result. We study this in the case of in-distribution images\nand of corrupted images. Results can be seen in Figure 9b.\nA smaller \u03b7 results in a loss of fine-grained details, a shift\nof the average tone, and the removal of unlikely pixels such\nas masked pixels. When applied to corrupted samples, CBC\nproves efficient in removing part of the noisy artifact while\nkeeping most of the image structure. Interestingly, CBC also\nstabilizes the DDIM inversion, which might sometimes be\ninconsistent. This allows for fewer steps in the encoding-\ndecoding procedure and speeds up the whole editing process."}, {"title": "Ablation Study of Confidence-based clipping", "content": "We propose to study the impact of each\nblock in CODE while keeping SDEdit for comparison. Qual-\nitative results are visible in Figure 9a. While both our editing\nmethod alone (w/o CBC) and SDEdit excel at adding extra\nfine-grained details, they fail at handling unknown masks\nor color shits efficiently. On the contrary, CBC basically\nfails at adding extra details but successfully recovers certain\ncolor shifts or masked areas. As a result, combining both into\nCODE leads to powerful synergies. Quantitative results can\nbe seen in Table 2."}, {"title": "Ablation Study", "content": "While CODE offers enhanced versatility and\ncontrol in editing, it introduces greater complexity compared\nto SDEdit. SDEdit's tuning is straightforward, with binary\nsuccess or failure outcomes, whereas CODE's dual hyper-\nparameter framework requires a more extensive grid search,\nincreasing the search complexity quadratically. However, this\nadded complexity enables CODE to achieve better results\nacross a wider range of scenarios."}, {"title": "Discussion.", "content": "We introduce Confident Ordinary Differential Editing, a novel\napproach for guided image editing and synthesis that handles\nOoD inputs and balances realism and fidelity. Our method\neliminates the need for retraining, finetuning, data augmen-\ntation, or paired data, and it integrates seamlessly with any\npre-trained Diffusion Model. CODE excels in addressing\na wide array of corruptions, outperforming existing meth-\nods that often rely on handcrafted features. As an evolution\nof SDEdit, our approach provides enhanced control, vari-\nety, and capabilities in editing by disentangling the original\nmethod and introducing additional hyperparameters. These\nnew parameters not only offer deeper insights into the func-\ntioning of Diffusion Models' latent spaces but also enable\nmore diverse editing strategies. Furthermore, we introduce\na Confidence-Based Clipping method that synergizes effec-\ntively with our editing technique, allowing the disregard of\nunlikely pixels or areas in a completely agnostic manner. Fi-\nnally, our extensive study of the different components at play\noffers a greater understanding of the underlying mechanics of\ndiffusion models, enriching the field's knowledge base. Our\nfindings reveal that CODE surpasses SDEdit in versatility and\nquality while maintaining its strengths across various tasks,\nincluding stroke-based editing. We hope our work inspires\nfurther innovations in this domain, akin to the transformative\nimpact of GAN inversion. Looking ahead, we see potential in\nautomating the editing and hyperparameter search processes\nand exploring synergies with text-to-image synthesis."}, {"title": "6 Conclusion", "content": "https://github.com/hendrycks/robustness\n2https://github.com/facebookresearch/augmentation-\ncorruption/blob/fbr_main/imagenet_c_bar/corrupt.py"}]}