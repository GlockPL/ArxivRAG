{"title": "AUTOMATED REWARDS VIA LLM-GENERATED PROGRESS FUNCTIONS", "authors": ["Vishnu Sarukkai", "Brennan Shacklett", "Zander Majercik", "Kush Bhatia", "Christopher R\u00e9", "Kayvon Fatahalian"], "abstract": "Large Language Models (LLMs) have the potential to automate reward engineering by leveraging their broad domain knowledge across various tasks. However, they often need many iterations of trial-and-error to generate effective reward functions. This process is costly because evaluating every sampled reward function requires completing the full policy optimization process for each function. In this paper, we introduce an LLM-driven reward generation framework that is able to produce state-of-the-art policies on the challenging Bi-DexHands benchmark with 20\u00d7 fewer reward function samples than the prior state-of-the-art work. Our key insight is that we reduce the problem of generating task-specific rewards to the problem of coarsely estimating task progress. Our two-step solution leverages the task domain knowledge and the code synthesis abilities of LLMs to author progress functions that estimate task progress from a given state. Then, we use this notion of progress to discretize states, and generate count-based intrinsic rewards using the low-dimensional state space. We show that the combination of LLM-generated progress functions and count-based intrinsic rewards is essential for our performance gains, while alternatives such as generic hash-based counts or using progress directly as a reward function fall short.", "sections": [{"title": "1 INTRODUCTION", "content": "Automated reward engineering aims to reduce the human effort required when using Reinforcement Learning (RL) for sparse-reward tasks. Multiple recent efforts towards automated reward engineering have focused on leveraging Large Language Models (LLMs) to provide reward signals\u2014either employing the LLM output directly as the reward (Kwon et al., 2023), or using the LLM to generate code for a dense reward function (Yu et al., 2023; Ma et al., 2023).\nTraditionally, constructing an effective dense reward function is an intricate process of identifying key task elements and carefully weighing different reward terms (Sutton & Barto, 2018; Booth et al., 2023). Prior works on reward function generation attempt to use LLMs both for their domain knowledge and to optimize quantitative aspects of reward engineering (weighting, rescaling) (Yu et al., 2023; Ma et al., 2023)\u2013they can require many training runs as they search through many reward functions in order to find a candidate that is effective for training (Ma et al., 2023).\nOur core insight is that we can reduce the problem of reward generation to the task of generating rough measures of task progress. Our framework uses LLMs to generate code for progress functions: task-specific functions that map environment states to scalar measures of progress. For a given genre of tasks, we follow the example from prior work on reward code generation (Yu et al., 2023) by asking practitioners to provide a small helper function library (ex. dist(x,y)) for the particular genre's observation spaces. Then, given both the helper function library and a single-sentence description of a task within the domain (ex. \u201cThis environment require a closed door to be opened and the door can only be pushed outward or initially open inward.\u201d), we leverage LLMs to generate the progress functions.\nWe find that progress functions are most empirically effective when used within a count-based intrinsic reward framework: we treat the outputs of the progress functions as a simplified state space that groups together similar states from the environment, we discretize the states, and we"}, {"title": "2 RELATED WORK", "content": "Automated reward engineering. Learning directly from sparse rewards can be challenging (Ng et al., 1999; Hare, 2019; Vecerik et al., 2017). It is common for practitioners to carefully engineer dense reward functions to shape the learning process (Ng et al., 1999)\u2014a labor-intensive (Sutton & Barto, 2018) and brittle (Booth et al., 2023) process. Advancements in automation such as Population Based Training (Jaderberg et al., 2017) have shown promise in refining this process by automating searches over fixed design spaces.\nFoundation models offer the opportunity to automate reward engineering with powerful priors. The output of foundation models can be used to propose tasks for open-ended learning curricula (Du et al., 2023; Zhang et al., 2023) and add high-level structure to learning (Mirchandani et al., 2021). Foundation models can also be used directly as rewards (Fan et al., 2022; Kwon et al., 2023; Sontakke et al., 2024), and have the potential to generate reward function code, either scaffolded by reward function templates (Yu et al., 2023) or in more freeform fashion (Ma et al., 2023; Venuto et al., 2024; Li et al., 2024). We also leverage LLMs to inject task knowledge via code, but rather than attempting to generate complex reward functions, we simply ask them to identify a few key features associated with task progress.\nCount-based intrinsic rewards. Our algorithm leverages the idea of count-based intrinsic rewards in order to convert coarse progress-based state representations into rewards for policy learning. Count-based intrinsic rewards are one of the main approaches to intrinsic motivation: estimating the \"novelty\" of a given state via state visitation counts (Tang et al., 2017). Approaches that hash continuous spaces to discrete representations have shown considerable promise when the discretization function is domain-specific (Tang et al., 2017; Ecoffet et al., 2021). In particular, the Go-Explore algorithm (Ecoffet et al., 2021) pairs count-based intrinsic rewards along with simulator state resets in order to achieve state-of-the-art results on several challenging Atari (Bellemare et al., 2013) tasks. The main downside to these approaches is that domain-aware discretization typically requires significant human engineering (Tang et al., 2017; Ecoffet et al., 2021). In our algorithm, by discretizing task progress, we already have access to automated domain-specific state discretizations. Absent the need for extensive human-engineered discretization functions, count-based intrinsic rewards are both practical and effective for learning policies from progress functions."}, {"title": "3 PRELIMINARIES", "content": "We consider the problem of automated reward generation for a sparse-reward task. The task is defined as a Markov Decision Process (MDP) $M = (S, A, P, R, \\gamma)$, where $S$ is the state space, $A$ is the action space, $P$ is the transition probability function, and $R$ is a sparse reward function that provides little guidance to the agent. The goal is to learn a policy $\\pi : S \\rightarrow \\triangle(A)$ that maximizes the expected cumulative reward $J(\\pi) = E[\\sum_{t=0}^{\\infty} \\gamma^tR(s_t, a_t) | s_0, \\pi]$, where $s_t, a_t$ are the state and action at time t.\nWe assume the availability of three potential inputs for reward engineering:\n1. A description of the features available in the environment (Figure 1-A, grey). We provide a description in the form of code, similar to Ma et al. (2023); Yu et al. (2023); Singh et al. (2023)\n2. A short task description (Figure 1-A, green), similar to Ma et al. (2023); Yu et al. (2023).\n3. An environment feature engineering library, offering a palette of additional, higher-level features that are not task-specific but may be generally useful for solving tasks in a given domain (Figure 1-A, green). This is identical to the type of feature library in Yu et al. (2023)."}, {"title": "4 METHODS", "content": "In this section, we first introduce our algorithm for leveraging LLM domain knowledge to generate progress functions, which distill key features from a high-dimensional environment state space to a coarse low-dimensional notion of progress in the task. Then, we outline how we use the generated progress functions: we view progress as measure of state, and leverage count-based intrinsic rewards for learning."}, {"title": "4.1 PROGRESS FUNCTIONS", "content": null}, {"title": "4.1.1 PROGRESS FUNCTION DEFINITION", "content": "Given a new task description, the first step of our process is to generate a progress function $P : S \\rightarrow \\mathbb{R}^k$, which takes environment features $s \\in S$ as input, and outputs information about the current progress of the agent on the task. Especially for more complex tasks, it may be difficult to distill a task to a single feature that tracks overall progress. Therefore, a progress function, given a state, is asked to emit a positive scalar measure of progress for one or more subtasks. For instance, for the SwingCup task (Figure 1-A), which involves 1) gripping the handles of the cup, 2) rotating the cup to the correct orientation, a good progress function would break the task into two sub-tasks and return scalars measuring progress for both sub-tasks.\nSpecifically, the progress function outputs $[x_1,x_2,...x_k]$ where $x_i \\in \\mathbb{R}$ tracks task progress for sub-task i. It also outputs additional variables $[y_1, y_2, ......y_k]$ that inform our framework whether the progress variables $x_i$ are increasing or decreasing."}, {"title": "4.1.2 PROGRESS FUNCTION GENERATION", "content": "For any given task, domain knowledge is required in order to determine what features from the environment are useful for assessing progress, and how to compute progress from those features. We derive that domain knowledge from an LLM, which is used to generate code for the progress function $P$. In order to translate domain knowledge into an effective progress function, we provide the LLM with three inputs:\n1. Function inputs: we specify the features available as inputs to the progress function via a description of the features in the environment state (Figure 1-A, grey). This information is available via the simulator.\n2. Function outputs: we specify the desired output of the progress function via a short task description (Figure 1-A, green). Humans specify this information on a per-task basis.\n3. Function logic: we structure the process of translating feature inputs to progress output by providing the LLM with access to a environment feature engineering library (Figure 1-A, green). This library offers a palette of additional, higher-level features to optionally use to compute progress, and also indirectly suggests that certain types of feature transformations are beneficial to compute progress (ex. $l2dist(x, tgt)$). Humans create this library once per genre or benchmark of tasks.\nNote that on a per-task basis, ProgressCounts only requires a user to provide the short task description. Please see Appendix A.3.2 for the libraries used for our Section 5 benchmarks.\nGiven the high-level task description, the small feature engineering library, and code describing the environment state space, we follow standard LLM prompting strategies to generate the progress function code (Figure 1-A), blue). Please see Appendix A.4 for several examples of generated progress functions."}, {"title": "4.2 FROM PROGRESS TO REWARD", "content": "Given a perfect estimate of task progress, it may seem natural to directly use progress as a dense reward: for a given state s, compute the sum of the progress outputs $R_{sum} = \\sum_i x_i$, and use progress sum $R_{sum}$ as the reward for reaching s. However, learning from dense rewards can be brittle-small mistakes in reward design can often lead to a failure to learn effective policies (Booth et al., 2023). Progress functions offer highly simplified state representations\u2014and given the coarse nature of these representations, we look for a more forgiving mechanism to generate rewards from these simplified representations. Therefore, we use a count-based intrinsic reward approach inspired by prior work that achieves state-of-the-art performance with domain-specific discretizations (Ecoffet et al., 2021)."}, {"title": "4.2.1 PRELIMINARIES: COUNT-BASED REWARDS", "content": "To facilitate exploration, we leverage count-based rewards proportional to state novelty (Kolter & Ng, 2009): $n(s) \\propto \\frac{1}{\\sqrt{c(s)}}$, where c(s) is the state visitation count. High-dimensional state spaces require a binning function $B : S \\rightarrow S'$ that maps state space S to a (small) discrete space $S'$ where we can tractably compute state visitation frequencies and novelty $n(s) \\propto \\frac{1}{\\sqrt{c(B(s))}}$\nWhen learning a policy, sparse extrinsic rewards are augmented with a standard intrinsic reward proportional to n(s) (Tang et al., 2017):\n$R_{total}(s_t,a_t) = R(s_t, a_t) + \\lambda n(B(s_{t+1}))$ (1)\nWhere $s_t$ is the state at time t, $a_t$ is the action, $s_{t+1}$ is the next state, and $\\lambda$ is a hyperparameter weighting the intrinsic reward relative to extrinsic reward $R(s_t, a_t)$.\nEffective state binning should encode information if and only if it is relevant to solving the particular task (Tang et al., 2017; Ecoffet et al., 2021)."}, {"title": "4.2.2 COUNT-BASED REWARDS FROM PROGRESS", "content": "We automatically generate a task-specific binning function $B$ by converting the continuous progress values P(s) emitted by the progress function to discrete states using a mapping $D : \\mathbb{R}^k \\rightarrow S'$ that:\n1. Estimates relevant value ranges $(min_i, max_i)$ for each $x_i$ from progress data\n2. Discretizes within $(min_i, max_i)$ to produce discrete progress features $x'_i$. We discretize later subtasks with finer granularity in order to encourage more exploration closer to the goal.\n3. Defines $B(s) = D(P(s)) = \\sum x'_i$\nHeuristic discretization avoids the need to learn scalar progress ranges from environment interaction, an approach used in prior work (Shinn et al., 2024; Ma et al., 2023). While we could have leveraged the LLM directly to emit logic for discretizing progress features, we chose to use these heuristics instead since LLMs are known to struggle with numerical reasoning (Shen et al., 2023). Details and example discretization code are included in Appendix A.6.\nHaving defined mapping B to discretize progress features, we measure state novelty from bin visitation counts via $n(s) \\propto \\frac{1}{\\sqrt{c(B(s))}}$, augmenting existing sparse extrinsic rewards. As shown in Figure 1-B, we learn policies using Proximal Policy Optimization (PPO) (Schulman et al., 2017), augmenting the sparse extrinsic task rewards with intrinsic rewards via count-based intrinsic motivation (See Eq. 1)."}, {"title": "5 EVALUATION", "content": "We evaluate ProgressCounts by using it to train policies on Bi-DexHands: a challenging sparse-reward benchmark consisting of 20 bimanual manipulation tasks. We also include additional results from the MiniGrid benchmark in the Appendix."}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "Bi-DexHands. The Bi-Dexterous Manipulation benchmark (Chen et al., 2022) (Bi-DexHands) consists of 20 bimanual manipulation tasks with continuous state and action spaces, such as using two robotic hands to lift a pot or simultaneously pass objects between the hands. These tasks have sparse rewards and require complex coordinated motion, making them a challenging test for leveraging language models to guide policy learning. Following conventions from prior work (Ma et al., 2023), we measure performance on Bi-DexHands in terms of the policy's success rate at completing each task, averaged over five trials (policy training runs with different seeds).\nWe use Bi-DexHands to evaluate the policy performance and sample efficiency of ProgressCounts against three baselines. 1) Sparse extrinsic rewards upon task success, 2) Dense: expert-written dense extrinsic rewards from the original benchmark, 3) rewards generated using the Eureka LLM-based reward generation algorithm (Ma et al., 2023), the current state-of-the-art reward generation method on Bi-DexHands.\nTraining configuration. In all experiments we train policies using PPO (Schulman et al., 2017). We train policies using the standard PPO hyperparameters and sample budgets used in prior work. We set the intrinsic reward coefficient $\\lambda$ on a per-benchmark basis. We leverage GPT-4-Turbo ('gpt-4-turbo-2024-04-09') as the LLM (Achiam et al., 2023) used to generate progress functions. Following the experimental procedure from prior work (Ma et al., 2023), we use the LLM to generate multiple options for the progress function, and select the resulting policy that achieves the highest success from a single training run-we refer to the different trained policies as policy samples. Unless otherwise specified, ProgressCounts uses four policy samples per task, and all policies are trained using 100M environment samples (number of environments \u00d7 number of simulation steps). All LLM prompts, including task descriptions and environment feature engineering primitives, are included in Appendix A.3."}, {"title": "5.2 COMPARISON TO EXTRINSIC REWARD BASELINES", "content": "Progress Counts trains policies that (on average) outperform those from Eureka on Bi- DexHands, using only 5% of Eureka's training budget. Averaged over all Bi-DexHands tasks, ProgressCounts achieves a success rate of 0.59, 13% higher than human-written dense rewards, and 4% higher than Eureka, the state-of-the-art method on this benchmark (Figure 2). Most importantly, Eureka's evolutionary algorithm requires 80 policy samples (generated reward functions) to find good reward functions for the task. In contrast, ProgressCounts only requires four policy samples"}, {"title": "5.3 METHOD ABLATIONS", "content": "The success of ProgressCounts is due to both the use of progress functions to collapse simulator states into bins and due to the effectiveness of count-based intrinsic exploration applied to these bins. While progress function might seem a suitable dense reward, progress-based rewards only achieve a success rate of 0.45 (Table 1), so best performance is achieved when using count-based exploration across discretized progress bins. Both LLM-generated progress functions and count-based intrinsic exploration are necessary to achieve our SOTA performance.\nProgress functions generate more effective bins than SimHash: On Bi-DexHands, Table 1 high- lights that ProgressCounts achieves a success rate of 0.59 with progress-based bins, and only achieves a success rate of 0.34 with SimHash-based (Sadowski & Levin, 2007) bins across the observation space (Tang et al., 2017). Across the benchmark, progress-based bins achieve performance equal to or better than SimHash-based bins across 19 of 20 tasks, with the remaining task (BlockStack) within the margin of error (see Table 8 in the Appendix for standard deviations). This result aligns with prior work on human-written hash functions for count-based rewards, where the integration of domain knowledge consistently improves performance (Tang et al., 2017; Ecoffet et al., 2021).\nCount-based rewards are more effective than directly using progress as reward While the sum of the outputs of a progress function $R_{sum} = \\sum_i x_i$ might seem a viable reward signal for learning, Table 1 illustrates that progress-based dense rewards only achieve a success rate of 0.45, while progress functions paired with count-based intrinsic rewards achieve a success rate of 0.59, matching or outperforming the dense-reward alternative on 15 of 20 tasks. This result highlights that, given coarse state representations from progress functions, count-based intrinsic rewards are more effective than standard dense rewards."}, {"title": "6 DISCUSSION", "content": "The state-of-the-art results achieved by ProgressCounts demonstrate two key takeaways:\nFirst, LLM-generated progress functions offer a compelling mechanism to generate coarse task- specific state representations, and alongside count-based intrinsic rewards offer an empirically- superior alternative to using LLMs to engineer reward functions. ProgressCounts outperforms Eureka, which uses LLMs to engineer reward functions, both in terms of performance and sample efficiency. One reason for this success is the structure (task progress, count-based rewards, etc.) we build into the ProgressCounts framework, which increases the quality and reliability of LLM responses, and reduces the need for trial and error across reward weights and scaling. Perhaps more interestingly, we hypothesize that ProgressCounts also benefits from count-based intrinsic rewards being robust to non-optimal binning functions, unlike reward functions where even minor errors can easily lead to a failure to solve tasks successfully.\nSecond, despite being relatively under-utilized in recent research, count-based intrinsic rewards can be surprisingly effective at training policies that operate in complex high-dimensional state spaces when given an adequate binning function. Interestingly, the results achieved by ProgressCounts suggest that these binning functions do not need to be hugely complex; ProgressCounts outperforms state-of-the-art, human-engineered dense reward functions using count-based exploration driven by binning functions that contain less than 20 lines of code."}]}