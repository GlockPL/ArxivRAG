{"title": "Efficient Inference for Augmented Large Language Models", "authors": ["Rana Shahout", "Cong Liang", "Shiji Xin", "Qianru Lao", "Yong Cui", "Minlan Yu", "Michael Mitzenmacher"], "abstract": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone LLMs by integrating external data sources through API calls. In interactive LLM applications, efficient scheduling is crucial for maintaining low request completion times, directly impacting user engagement. However, these augmentations introduce scheduling challenges due to the need to manage limited memory for cached information (KV caches). As a result, traditional size-based scheduling algorithms, such as Shortest Job First (SJF), become less effective at minimizing completion times. Existing work focuses only on handling requests during API calls by preserving, discarding, or swapping memory without considering how to schedule requests with API calls. In this paper, we propose LAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes request completion time through a unified scheduling approach that considers the total length of requests and their handling strategies during API calls. Recognizing that LLM inference is memory-bound, our approach ranks requests based on their consumption of memory over time, which depends on both the output sizes and how a request is managed during its API calls. To implement our scheduling, LAMPS predicts the strategy that minimizes memory waste of a request during its API calls, aligning with but improving upon existing approaches. We also propose starvation prevention techniques and optimizations to mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM and evaluate its performance against baseline LLM inference systems, demonstrating improvements in end-to-end latency by 27%-85% and reductions in TTFT by 4%-96% compared to the existing augmented-LLM system, with even greater gains over vLLM.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent progress in large language models (LLMs) has initiated a new wave of interactive AI applications. A prominent example is OpenAI's ChatGPT [14], which facilitates conversational interactions across various tasks. One way to extend LLM capabilities is to augment them with external tools [12], resulting in what we refer to as API-augmented requests. These augmentations include arithmetic calculation [8], ChatGPT plugins [15], image generation [4], and virtual environments [22]. Consequently, AI development is increasingly moving towards compound AI systems [28] that integrate multiple interacting components, such as model calls, retrievers, and external tools, rather than relying solely on monolithic models.\nAPI-augmented requests present several challenges, particularly regarding memory consumption during the LLM decoding phase, which is memory-bound and requires careful management. Each request has associated key and value matrices that grow in size during the request. LLMs cache these matrices in a key-value (KV) cache throughout the sequence generation process to enhance efficiency, eliminating the need to recompute them at every iteration. This caching significantly reduces computation time but requires substantial memory, roughly proportional to the input and output lengths, the number of layers, and hidden dimensions. High memory consumption during decoding can translate to higher latency and lower throughput, as it limits the system's ability to process multiple requests concurrently.\nWith API augmentation, memory demands increase further, depending on how the system handles requests during API calls (Figure 2). There are three main strategies for handling a request's KV cache during API calls:\n\u2022 Preserve: The system retains the KV cache in memory while waiting for the API response.\n\u2022 Discard and Recompute: The system discards and re-computes the KV matrices from the start once the API returns.\n\u2022 Swap: The system offloads the KV cache to the CPU to free up memory, and reloads it when the API returns.\nWe refer to these as memory handling strategies, or handling strategies for brevity. All three strategies have downsides. With Preserve, because the KV cache remains in memory throughout the API call, memory is wastefully consumed during the call. Discard and Recompute incurs additional memory and computational costs when recomputing the KV cache. Swap introduces overhead from pausing running requests and managing data transfer between CPU and GPU memory. Additionally, because the duration of API calls can vary significantly across different augmentation types and requests, one strategy does not fit all requests. For instance, a simple calculation might be completed in milliseconds, whereas image generation could take several seconds.\nExisting LLM inference systems generally operate for standalone LLMs and fail to guarantee low request completion time for augmented LLMs. This issue becomes more significant when handling requests involving external API calls, leading to delays such as head-of-line (HoL) blocking. HoL blocking occurs when long-running requests, including those waiting for API responses, prevent shorter ones from being processed efficiently. Previous works attempt to mitigate the issue of HoL blocking by managing memory during API calls. For example, vLLM [11] discards and recomputes API-augmented requests, treating API calls as termination signals and the request returning from the API as a new job. INFERCEPT [1] classifies requests and applies strategies dynamically, aiming to reduce memory wastage for API-augmented requests. However, both systems still rely on a first-come first-served (FCFS) scheduling policy, which increases HoL blocking by allowing long-running requests to block shorter ones, especially under high load.\nScheduling strategies can reduce head-of-line blocking by prioritizing requests. Traditional size-based scheduling prioritizes jobs\u00b9 with shorter execution time. Without API calls, this approach is effective, as execution time and memory usage for LLM requests without API calls correlate (with the execution time corresponding roughly to the output length) [21]. However, with API-augmented requests, the output length may not reflect the total request time, including API calls. A request with a short output might involve a lengthy API call, while a request with a longer output may require minimal API interaction.\nRather than treating LLM execution and API calls as separate processes, we argue that integrating the scheduling and memory handling of requests can significantly improve request completion time, especially under high load. Achieving this requires information about both request output length and API call time, which are often unavailable, so we rely on predicting these values. This challenge leads to a key question: Given predictions of request output length and API call time, how should we schedule and handle API-augmented requests to prevent head-of-line blocking and minimize latency?\nThis paper presents LAMPS (LLM API- and Memory-based Predictive Scheduling), a novel inference framework for augmented LLMs. LAMPS minimizes request completion time through a unified scheduling approach that jointly considers the total length of requests and their API call-handling strategies. Recognizing that LLM inference is memory-bound, our"}, {"title": "2 BACKGROUND AND MOTIVATION", "content": "LLMs expand their capabilities by integrating external tools, allowing them to handle more complex tasks. However, this augmentation introduces challenges for request handling and scheduling with the goals of minimizing response times and managing memory efficiently during inference. This section presents the background for LLM execution and API interactions."}, {"title": "2.1 Augmented LLMs", "content": "Augmented Language Models [12, 24] refer to language models that enhance their capabilities by incorporating external tools, retrieval mechanisms, or reasoning strategies to overcome the limitations of traditional LLMs. Unlike pure LLMs, which rely solely on pre-trained parameters to generate responses, augmented LLMs can query external data sources to expand their capabilities. Figure 1 shows an example of an augmented LLM request. These augmentations, which we refer to as API (Application Programming Interfaces), fall into three main categories as described in [12]: incorporating non-LLM tools during decoding (such as calculators [25], information retrieval systems [3]), iterative self-calling of an LLM (like chatbots maintaining conversation history), and complex compositions involving multiple LLMs, models, and tools (exemplified by frameworks like LangChain [5], DSpy [10], Gorilla [17], SGLang [30], and AgentGraph [6]).\nLLM API time varies significantly based on augmentation types, with a clear distinction between short-running and long-running augmentations. Despite this variation, today's systems still rely on FCFS scheduling. This suggests that API handling strategies should be tailored to specific augmentation types rather than using a one-size-fits-all approach."}, {"title": "2.2 Transformer-Based Generative Models", "content": "At each step, a Transformer model generates the most probable next token based on the sequence of previously generated tokens. A model generating a sequence of length n needs to perform n iterations, with each token passing through several layers of self-attention and feed-forward networks.\nDuring the i-th iteration, the model operates on all prior tokens (to, t1, ..., ti-1) using self-attention mechanisms. The resulting output can be represented as:\n$h_{out} = \\text{softmax}(\\frac{q_iK^T}{\\sqrt{dn}}) \\cdot V$\nHere, qi is the query vector for the current token ti, while K and V are matrices containing the key and value vectors for all preceding tokens, where K, V \u2208 $R^{i\u00d7dn}$."}, {"title": "2.2.1 Key-Value (KV) Cache", "content": "To reduce computational overhead, LLMs cache the key and value matrices (KV cache) during sequence generation. This approach avoids recomputing these matrices at each step, improving efficiency but leading to high memory usage, which scales with the sequence length, number of layers, and hidden dimensions. As more tokens are generated, memory demands grow, particularly for long sequences. For instance, the GPT-3 175B model requires around 2.3 GB of memory to store key-value pairs for a sequence length of 512 tokens. This high memory consumption poses challenges for efficient preemptive scheduling, especially when working with limited GPU memory."}, {"title": "2.2.2 Scheduling in LLMs", "content": "In LLM inference systems, iteration-level scheduling, as implemented in systems like Orca [27] and vLLM [11], is commonly used. It differs from traditional request-level scheduling, where the system processes a batch of requests until completion, forcing earlier requests to wait until the entire batch is completed, and new requests must wait in a queue until the next batch is processed. Iteration-level scheduling processes one token at a time for each request in the batch, allowing the system to dynamically adjust the batch after every iteration. Requests that complete an iteration can exit the batch, and new ones can be introduced, optimizing resource usage within the constraints of GPU memory. The default policy in these systems is FCFS. Most recent scheduling works [21, 26], however, focus on LLM requests without API augmentations. API-augmented requests introduce challenges, such as handling external interactions and variable API call times, which require new scheduling strategies beyond those used for standalone LLMs. Accordingly, LAMPS focuses on developing such scheduling for API-augmented requests."}, {"title": "2.3 Handling Requests During API", "content": "Optimizing memory management during API calls involves selecting a strategy that minimizes GPU memory waste. In an augmented LLM inference system, this choice depends on two factors: the duration of the API call and the length of the pre-API output. For brief API calls, the Preserve strategy may be advantageous in avoiding recomputation overhead."}, {"title": "3 CHALLENGES AND DESIGN PRINCIPLES", "content": ""}, {"title": "3.1 Challenge: Scheduling API-augmented Requests", "content": "Our goal is to reduce average and typical request times through scheduling. Size-based scheduling methods, such as Shortest Job First (SJF), can reduce request completion"}, {"title": "3.2 Key Design Principles", "content": ""}, {"title": "3.2.1 Predicted API handling strategy", "content": "Our approach integrates the selection of API handling strategies directly into the scheduling policy, determining the appropriate handling strategy before the request is processed. To do this, we first estimate the context size for batched requests, considering the memory usage of other requests that might be affected during the API handling phase. This estimation involves profiling the number of requests in a batch. API durations are predictable based on API types, as each corresponds to specific operations with known computational complexities and resource demands. For example, math APIs, which involve simple calculations, have short execution times, while image generation APIs, requiring intensive computation, have longer durations. Analysis of historical data (Table 2) shows that execution times within the same API type have low variance, enabling reliable predictions. Lastly, we use a lightweight predictor (see Section 6 for details) to estimate the pre-API length from input prompts.\nIn contrast, INFERCEPT incorporates the pre-API output length and API duration for each request, also considering the impact a request has on all other requests in memory. When a request reaches the API, INFERCEPT dynamically selects a strategy to minimize memory waste based on these factors. However, its scheduling policy follows a simple FCFS, whereas our method integrates these considerations directly into a more adaptive scheduling policy."}, {"title": "3.2.2 Integrating API handling strategies with scheduling", "content": "Minimizing request completion time requires a unified scheduling method that considers both the total length of requests and their specific handling strategies during API calls. By knowing whether a request will Preserve, Discard, or Swap the model during an API call, the scheduler can predict the impact on system resources and the request completion time and rank the requests accordingly. For example, it may order"}, {"title": "4 DESIGN", "content": ""}, {"title": "4.1 System Overview", "content": "LAMPS combines API handling strategies with scheduling policies to minimize the completion time for API-augmented requests. Using design principles in Section 3.2, LAMPS employs three main steps to reduce LLM inference response time: predicting the pre-API output length and the duration of the API, determining the handling of requests during API calls, which aims to minimize memory waste, and, finally, proposing a scheduling policy that considers both the request length and the API handling method.\nFigure 5 illustrates the LAMPS architecture. Users submit requests to the request pool. LAMPS predicts pre-API output length and estimates API properties (duration and response length, Table 2) based on input prompts. Using these predictions, LAMPS estimates memory consumption over time, considering this in API handling decisions and scheduling policy ranking. LAMPS determines how to handle requests during API calls to minimize memory waste, aligning with INFERCEPT (equations (1), (2) and (3)). Each request is labeled with a handling strategy (preserve, discard, or swap). Based on this handling method and request output length, LAMPS implements a scheduling policy tailored for API-augmented requests. This policy prioritizes requests based on their memory consumption over time. The pseudocode of the LAMPS scheduler is provided in Algorithm 1."}, {"title": "4.2 Predicting API Handling Strategy", "content": "Our goal is to predict the best API handling strategy that minimizes memory waste before a request is processed, enabling the scheduler to rank each request. In the following discussion, we explain how to predict the best handling strategy for requests during API calls, assuming a single API for simplicity. We first predict the output length and API duration. Our approach generalizes beyond the dataset by using a predictor to estimate pre-API output length based on the prompt. Output length prediction is studied in the context of LLMs. Several works have modeled output length prediction as a classification problem, using models such as DistilBERT and OPT [7, 9, 23]. These approaches categorize outputs into discrete bins to estimate lengths. Other methods use regression-based techniques to predict length [19, 20]. Moreover, recent work [21] demonstrates using LLM layer embeddings to predict output length. Our approach builds on these insights.\nFor the API duration, we leverage the fact that APIs belong to fixed classes (e.g., Math, Image), each with consistent functionality and a similar duration (latency). By extracting the API type from the prompt, we estimate the API response length using the average length from the training set for that API class. This method relies on the standardized outputs and consistent behaviors of APIs within each class.\nWe classify and determine how to handle requests during API calls by predicting memory waste based on pre-API and API duration estimates. INFERCEPT uses Equations (1), (2), and (3) to compute the memory waste. Here, we present an equivalent method to quantify memory usage (specifically calculating waste based on predictions) by considering the memory-over-time function. For simplicity, we focus on a single API, but this approach extends to multiple APIs.\nFigure 4 represents the memory-over-time function and the highlighted areas in Figures 4a, 4b and 4c represent the memory waste of a single request due to an API call. We combine this waste with our estimation of the context size for batched requests according to the setup, aligning with Equations (1), (2), and (3), respectively. After estimating the memory waste for each option, we select the strategy that minimizes the waste. Memory consumption increases linearly with the output length until the request reaches the API call, as seen in all three cases. In Figure 4a, the Preserve strategy keeps memory allocated throughout the API call. Thus, memory use remains constant during the call, with the shaded area indicating this idle memory. In Figures 4b and 4c, the Discard and Swap strategies release memory during the API call, resulting in zero memory usage while waiting for the API response. In Figure 4b, the Discard approach recomputes the first part after the API, while the Swap approach (Figure 4c) shows a delay before the memory is fully released (swapped out) and then restored (swapped in) after the API call completes. Swapping in causes a spike in memory consumption.\nOur insight is that evaluating memory usage by integrating the memory-over-time function offers a more accurate measure of resource consumption than relying on instantaneous memory values. The integral of memory versus time provides the total memory consumption for a request, accounting for the API's impact on memory usage. This approach incorporates output length, API duration, and how the request is handled during the API call. Instantaneous memory measurements fail to reflect how long memory resources are occupied, which is particularly important in the decoding phase of LLMs where the system is memory-bound. It is not just the amount of memory a request consumes at a particular moment but also how long that memory remains in use. A strategy that uses more memory for a shorter period can be more efficient than one that uses less memory but occupies it longer. Integrating memory over time captures memory waste across different strategies during API calls.\nMulti-API. To generalize to requests involving multiple APIs, we break down each request into segments, each ending with a single API call. After an API call completes, the request re-enters the system for further processing and is treated as a new request focused on the subsequent API call. At this point, we classify the request based on the characteristics of the current API. For example, suppose a job involves initial processing, followed by two API calls interleaved with additional processing phases. We divide this job into segments, each consisting of a processing phase and a single API call at the end. We estimate the returned token length in each segment based on the specific API call. While this approach does not account for the cumulative memory consumption of the entire job, predicting the total number of API calls and their combined resource usage beforehand is challenging. This segmentation aligns with INFERCEPT, which handles multi-API requests by processing jobs incrementally as they reach each API.\nEffect of Mispredictions. Mispredictions are to be expected. Small mispredictions in API duration or output size will typically have a small effect; indeed, they may not change the overall ranking of jobs. Mispredicting a short API or output as long may have a large effect on that particular job, but does not typically harm other jobs in the system. (See related results in [13].) A long-running API call incorrectly predicted as short may lead the system to select a memory-wasteful strategy, such as preserving the request in memory during the API call. This unnecessary memory consumption may limit the system's ability to process additional requests and reduce overall throughput. A request with a long output misclassified as short may cause head-of-line blocking and delay other requests. The main overhead in this scenario is increased latency."}, {"title": "4.3 Scheduling Policy", "content": "Traditional job scheduling typically assumes that job completion times are either completely unknown-making First-Come, First-Served (FCFS) a natural strategy-or known in advance or predictable, enabling size-based policies like Shortest Job First (SJF) and Shortest Remaining Process Time (SRPT) to minimize average response time.\nIntegrating API calls into the output response increases memory consumption, which may degrade performance due to memory constraints. In LLM systems, when memory is full, jobs are either discarded and recomputed when memory becomes available, or KV cache entries are swapped from the GPU to the CPU. Both approaches impact response time: discarding requires recomputation, and swapping interrupts the model's forward pass, causing delays for the entire batch. Intuitively, requests should be ranked based on their memory consumption. Without API calls, ranking based on memory consumption aligns with ranking based on service time (or request length), as memory consumption has a linear relationship with request length. With API calls, this relationship breaks, as requests are handled differently according to the strategy that minimizes memory consumption during the API. Consider Figure 4, which shows memory over time; we consider the area (integral) as a rank function of a request and select the function based on the predicted handling strategy during the API call. For example, the memory over time function matches Figure 4a for the predicted preserve strategy for a request. This approach incorporates the strategy of handling requests during API within the scheduling policy and provides a way to compare and rank requests among different handling strategies. Referencing Figure 3, consider memory consumption over time. Among the three requests, R3 consumes the least memory and should be prioritized, followed by R2. R\u2081 consumes the most memory due to its length, API duration, and the preserve handling strategy, so it should be scheduled last.\nOur scheduling strategy uses iteration-level scheduling [27], where the scheduler ranks requests at the end of each iteration. We use a selective score update mechanism to reduce the overhead of frequent ranking. For example, for datasets with long-running requests, frequent score updates are unnecessary; instead, we cache their scores and refresh them at predefined intervals. This balances ranking accuracy with the computational costs of maintaining updated scores."}, {"title": "4.4 Starvation Prevention", "content": "Scheduling policies can cause certain requests to experience long wait times, leading to high tail latency, a form of starvation that degrades system performance and user experience. This issue arises when longer or resource-intensive requests are continually deferred in favor of shorter ones, exacerbating tail latency. Our memory-focused scheduling policy alone does not detect and mitigate starvation, which can result in extended wait times and reduced fairness. To solve this, we have implemented a starvation prevention mechanism to improve the scheduler's tail latency using a per-request counter. The counter increments when a request remains in the waiting queue for a new iteration. Upon"}, {"title": "5 IMPLEMENTATION", "content": "We implement LAMPS on top of vLLM [11], a state-of-the-art LLM inference system. We enable the score update mechanism only on the ToolBench dataset with an interval of ten, while disabling it for other datasets where scheduling overhead is not a bottleneck. To implement the prediction mechanism, we use the OPT-125M language model [29], a transformer-based model developed by Meta. With 125 million parameters and support for a context length of 2048 tokens, OPT-125M can effectively handle datasets with long contexts, such as the multi-API dataset. Although smaller than many larger language models, OPT-125M delivers strong language generation capabilities. Our approach utilizes the embeddings generated by OPT-125M during the initial processing of input prompts. After tokenizing the input and processing it through the model's layers, we extract the final token's embedding, which is then fed into a linear classifier. This classifier assigns the input to one of 50 bins, each representing a range of 10 tokens, and is trained using cross-entropy loss.\nThe model estimates the completion length for each prompt based on learned representations from the Tool-Bench dataset [18], which involves complex conversations with API interactions. We train the model using an 80-20 split for training and validation, classifying output lengths into bins. We apply this model specifically to the ToolBench dataset because the other dataset already includes detailed output length information, making prediction unnecessary in that case. LAMPS is evaluated using the test portion of the ToolBench data to ensure accuracy."}, {"title": "6 EVALUATION", "content": "In this section, we first present end-to-end experiments demonstrating the overall performance improvements of LAMPS compared to INFERCEPT and vanilla vLLM on two different-sized LLM models. Next, we evaluate LAMPS's design choices, highlighting the effectiveness of each component. Finally, we analyze the prediction component and the impact of mispredictions on LAMPS's performance."}, {"title": "6.1 Methodology", "content": "LLM models. We use the 6B-parameter GPT-J model (which we denote by GPT-J 6B), and the 13B-parameter Vicuna model (Vicuna 13B). Both were also used by INFERCEPT.\nTestbed. For the experiments, we used a machine with dual AMD EPYC 7313 CPUs (16 cores per CPU, totaling 64 threads), 503 GB of RAM, and two NVIDIA A100 GPUs with 80 GB memory each connected via NVLink. We manually limited the maximum memory usage of each GPU to 40 GB to emulate the setup used in INFERCEPT's experiments, which we inferred to involve A100 GPUs with 40 GB memory based on their use of AWS machines.\nDatasets. We evaluate our system using two distinct datasets. The first, similar to the one used in INFERCEPT, includes arithmetic tasks, knowledge-based question answering, multi-step chatbot dialogues, and interactions in an embodied virtual environment. This dataset includes API execution times, frequencies, and output length. The second data set is ToolBench [18] is an instruction-tuning dataset tailored for tool-use tasks, comprising over 16,000 real-world APIs across 49 categories. It encompasses both single-API and multi-API scenarios, containing only prompts and API call types. We use this dataset to predict output length, API duration, and API response length.\nMetrics. To evaluate LAMPS, we measure end-to-end latency, defined as the time from when a request is submitted to the system until its completion, time-to-first-token (TTFT) (reflecting system responsiveness), and throughput (indicating request generation speed). For each metric, we report the mean and 99th percentile (P99).\nBaselines. As baselines, we compare LAMPS with both vanilla vLLM and INFERCEPT."}, {"title": "6.2 End-to-end Performance", "content": "End-to-end latency and TTFT vs. request rate. Figure 6 shows how varying the request arrival rate affects the mean and P99 of end-to-end latency and TTFT across three datasets: (1) a single-API dataset (a subset of the INFERCEPT dataset containing only a single API), (2) the full INFERCEPT dataset, and (3) the ToolBench dataset. We evaluated these metrics using the LLMs GPT-J 6B and Vicuna 13B.\nGPT-J 6B results. LAMPS shows clear performance gains over vLLM and INFERCEPT in mean TTFT and end-to-end latency across all tested datasets. On the single-API dataset at a request rate of 3, LAMPS reduces mean TTFT by 4.61% compared to INFERCEPT and 22.86% compared to vLLM. Mean end-to-end latency increases slightly by 0.78% against INFERCEPT but drops by 14.48% compared to vLLM. At higher rates, such as 5, LAMPS further reduces mean TTFT by 91.27% over INFERCEPT and 95.71% over vLLM, with latency reductions of up to 65.51% and 90.44%, respectively. For the multi-API dataset, LAMPS achieves 95.93% TTFT reduction and 63.32% latency improvement over INFERCEPT at a rate of 3. On ToolBench, LAMPS reduces mean TTFT by 87.04% compared to INFERCEPT and 99.51% compared to vLLM, with a 27.24% latency reduction compared to INFERCEPT and 96.07% compared to vLLM at a request rate of 3.\nVicuna 13B results. LAMPS consistently outperforms VLLM and INFERCEPT in TTFT and end-to-end latency across all datasets. On the single-API INFERCEPT dataset, at a request rate of 3, LAMPS reduces mean TTFT by approximately 4.78% compared to INFERCEPT and 18.65% compared to vLLM, with mean end-to-end latency reductions of around 0.24% and 15.73%, respectively. For higher request rates (e.g., 4), LAMPS achieves significantly greater improvements, reducing mean TTFT by over 98% compared to baselines and end-to-end latency by up to 82%. On the multi-API dataset, LAMPS achieves similar gains, with TTFT reductions of over 89% and mean end-to-end latency improvements of up to 78%"}, {"title": "6.3 Breakdown of LAMPS Components", "content": "To further understand the benefits of LAMPS, we incrementally added its components to vLLM and compared the results with INFERCEPT. We used the Multi-API dataset because it has the highest latency among the datasets (Figure 7), Figure 10 shows throughput, as well as the mean and P99 of end-to-end latency and TTFT. First, we added the predicted API handling component to vLLM while keeping the scheduling policy as FCFS (referred to as LAMPS w/o scheduling). With this addition, the performance was close to INFERCEPT but slightly worse. The main difference between INFERCEPT and LAMPS w/o scheduling is that we use predicted information to estimate how to handle API calls in advance. In contrast, INFERCEPT dynamically decides how to handle requests during the API call when the request reaches the API. Next, we integrated our scheduling policy. The main improvements across all metrics came from the scheduling policy. Our scheduling policy effectively reduces head-of-line blocking and optimizes resource utilization. However, predicting the API handling policy is a necessary preliminary step in implementing our scheduling."}, {"title": "6.4 Prediction Component", "content": "In this subsection, we evaluate the impact of prediction errors on the performance of LAMPS."}, {"title": "Effect of Mispredictions", "content": "Using the INFERCEPT dataset, we inject controlled Gaussian errors into the predictions for API duration and output length: error ~ N(0, p \u00d7 m), where p is the error parameter and m is the measured value. The predicted values are then calculated as: predicted_value = measured_value + error. By varying the error_parameter parameter, we evaluate how different prediction inaccuracies affect the overall performance of LAMPS. Figure 11 demonstrates the impact of prediction errors on system performance, focusing on end-to-end latency and throughput. As the prediction error parameter increases (e.g., 5%, 10%, 30%, 50%), median latency increases, particularly under higher request rates (8-10 req/s). This indicates that inaccurate predictions lead to longer waiting times. Similarly, throughput decreases as error rates increase, especially at higher request rates. However, we observe that performance degradation in LAMPS occurs only when large prediction errors occur. This suggests that as long as reasonably accurate predictions are maintained, LAMPS can deliver improved performance."}, {"title": "Prediction Accuracy and Overhead", "content": "We evaluated the precision of our response length predictions using the ToolBench dataset by measuring the absolute difference between the predicted and actual word lengths (which is part of the dataset).We used two accuracy metrics, Acc-5 and Acc-15 that represent the percentage of predictions that differ from the actual length by no more than 5 words and 15 words, respectively. The results show 68.5% accuracy for Acc-5 and 78.3% accuracy for Acc-15, with a Mean Absolute Error (MAE) of 3.06. When focusing on the first 20 bins (responses up to 200 words), the MAE improves to 1.366, indicating higher accuracy for shorter responses. We used an NVIDIA A100 GPU for inference, achieving an average prediction time of 13.7 ms per input on the ToolBench dataset."}, {"title": "7 RELATED WORKS", "content": "Several studies focus on improving inference throughput through optimized scheduling strategies. Orca [27] introduces iteration-level scheduling, where a new batch is created at the end of each model forward pass. This approach increases GPU utilization and enhances inference throughput. Other research targets efficient GPU memory management. VLLM [11] introduces paged attention, treating the KV cache as virtual memory mapped to non-contiguous physical GPU memory, improving GPU memory utilization. Another line of work addresses the imbalance between the prefill and decoding stages. Sarathi [2] employs chunked prefill, which divides prompt tokens into smaller chunks merged with decoding requests to form a batch for each iteration. Splitwise [16] separates the prefill and decoding stages across different machines to match their distinct computational demands. These techniques for memory optimization are complementary and can be integrated with LAMPS."}, {"title": "8 CONCLUSION AND FUTURE WORK", "content": "We have introduced LAMPS (LLM API- and Memory-based Predictive Scheduling), an LLM inference framework designed explicitly for API-augmented requests. LAMPS optimizes request completion time through a unified scheduling strategy that ranks requests based on their memory consumption over time. By predicting pre-API outputs, LAMPS can estimate the optimal handling strategy during API calls, choosing between preserving, discarding, or swapping memory to minimize memory waste. This predictive approach allows LAMPS to schedule requests with varying output sizes and API interactions effectively. Additionally, our framework incorporates a starvation prevention mechanism for better tail latency. Experimental results demonstrate that LAMPS improves end-to-end latency by 27%-85% and reduces TTFT by 4%-96% compared to INFERCEPT, with even greater gains over VLLM.\nWe believe that this work serves as a starting point for API-augmented requests. For further improvements, we will focus on enhancing prediction accuracy, particularly for memory-intensive requests, and aim to better handle multi-API requests by accounting for cumulative memory consumption throughout the entire process. Another direction for building upon LAMPS is to manage multiple LLMs, directing requests to the most suitable LLM based on the specific API type and the current load of the LLMs. This would be a load-balancing scheduling variation. Similarly, one might have requests that have to go through a sequence of LLMs and/or servers for API calls, according to some ordering (that differs among requests). This is similar to a jobshop scheduling variation.\nMore generally, we suggest that scheduling with API calls appears to open the door to many interesting algorithmic problems. We are not aware of API calls of the form considered here being studied in the (theoretical) scheduling algorithms literature. More consideration of algorithmic bounds for these types of problems may yield more additional practical strategies for API-augmented requests."}]}