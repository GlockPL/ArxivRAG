{"title": "LCD-Net: A Lightweight Remote Sensing Change Detection Network Combining Feature Fusion and Gating Mechanism", "authors": ["Wenyu Liu", "Jindong Li", "Haoji Wang", "Run Tan", "Yali Fu", "Qichuan Tian"], "abstract": "Remote sensing image change detection (RSCD) is crucial for monitoring dynamic surface changes, with applications ranging from environmental monitoring to disaster assessment. While traditional CNN-based methods have improved detection accuracy, they often suffer from high computational complexity and large parameter counts, limiting their use in resource-constrained environments. To address these challenges, we propose a Lightweight remote sensing Change Detection Network (LCD-Net in short) that reduces model size and computational cost while maintaining high detection performance. LCD-Net employs MobileNetV2 as the encoder to efficiently extract features from bitemporal images. A Temporal Interaction and Fusion Module (TIF) enhances the interaction between bitemporal features, improving temporal context awareness. Additionally, the Feature Fusion Module (FFM) aggregates multi-scale features to better capture subtle changes while suppressing background noise. The Gated Mechanism Module (GMM) in the decoder further enhances feature learning by dynamically adjusting channel weights, emphasizing key change regions. Experiments on LEVIR-CD+, SYSU, and S2Looking datasets show that LCD-Net achieves competitive performance with just 2.56M parameters and 4.45G FLOPs, making it well-suited for real-time applications in resource-limited settings. The code is available at https://github.com/WenyuLiu6/LCD-Net.", "sections": [{"title": "I. INTRODUCTION", "content": "Remote sensing change detection methods involve analyzing remote sensing images acquired at different time points to identify and assess changes in surface features [35]. Remote sensing technology utilizes satellites, drones, and airborne platforms to collect information about the Earth's surface, enabling monitoring of environmental changes over large areas and with high temporal resolution. This technology not only provides rich spatial information but also reveals the dynamic evolution of surface features [5, 1], making it widely applicable in various fields such as environmental monitoring, urban planning, agricultural management, disaster assessment, and resource management [25, 22].\nIn recent years, convolutional neural networks (CNNs) have become an important research focus in remote sensing image change detection (RSCD) due to their powerful feature extraction capabilities. Deep learning models can automatically learn multi-level feature representations, effectively capturing temporal changes between images. For instance, U-Net [15] and its derivative models (such as FC-ef [8], FC-cat [8], FC-diff [8]) show remarkable performance in multi-level feature fusion. These models efficiently combine features at different scales, improving detection accuracy and robustness. More complex architectures like DSIFN [43] and SNUNet [10] further enhance detection performance through multi-layer feature fusion and attention mechanisms. These methods leverage high-level temporal features to detect changes between images, while low-level features capture detailed boundaries.\nDespite the notable advancements made by traditional CNN-based methods in remote sensing image change detection (RSCD), the practical implementation of these techniques is often hindered by their computational demands and limitations in effectively addressing the complexities of real-world scenarios. As the demand for real-time change detection increases, it becomes crucial to improve the efficiency of these models and enhance their robustness against challenging conditions. This leads us to identify two critical challenges in the current landscape of RSCD.\nFirstly, various RSCD methods proposed in recent years have performed well in terms of detection accuracy (F1-score). However, most models still exhibit high computational complexity and parameter counts, exceeding the capacity of resource-constrained environments. More complex models (such as SNUNet [10]) require substantial computational resources to achieve high detection accuracy, while some lightweight models (such as L-Unet [30]) reduce computational costs and parameter counts but perform poorly in terms of detection accuracy. This high computational cost significantly limits the widespread application of these methods in practical situations, especially in environments with limited computational resources (such as common edge computing devices or mobile platforms). Thus, designing lightweight models with lower computational loads and fewer parameters while maintaining good detection accuracy has become a key issue that urgently needs to be addressed in the RSCD field.\nSecondly, interference from complex backgrounds and the coexistence of multi-scale changes pose critical challenges in the current change detection domain. Complex textures and similar structures in the background, such as water surface reflections and noise, obscure subtle target changes, making it difficult for detection methods to effectively distinguish between real changes and background interference. Meanwhile, traditional methods lack sensitivity to different scale changes, struggling to capture the differences between significant changes and subtle local variations. This weak response to multi-scale features significantly reduces detection accuracy, particularly in complex scenarios, leading to false positives and missed detections.\nFor the first challenge, we select the lightweight backbone network MobileNetV2 [13] as the encoder due to its excellent performance in feature extraction tasks and its minimal parameter count. However, the features extracted at a single moment often lack sufficient contextual information, making it difficult to comprehensively reflect the true nature of changes, which leads to inadequate feature extraction capability. To this end, we introduced a Temporal Interaction and Fusion Module (TIF) into the encoder. The module facilitates cross-temporal information interaction through the exchange of channel features, effectively integrating bi-temporal features and providing richer contextual information. This mechanism not only enhances the ability to perceive changes but also ensures that the model's parameter count remains unchanged.\nFor the second challenge, we propose the Feature Fusion Module (FFM) effectively addresses these challenges. By integrating multi-scale features across layers, the FFM enhances the model's sensitivity to multi-scale changes, especially in complex backgrounds, allowing it to better isolate background noise from real changes. Additionally, the FFM adopts a layer-by-layer approach, thoroughly extracting features from both significant and subtle changes. This enables the model to capture the fine differences between changes of various scales, improving its sensitivity and ability to distinguish changes. Furthermore, to further enhance the extraction capability of key features, we introduce a Gated Mechanism Module (GMM) in the decoding layer. This module dynamically adjusts learnable parameters to focus on feature learning in change areas, thereby improving the model's accuracy and robustness.\nBy incorporating the TIF, FFM, and GMM modules, our LCD-Net achieves competitive performance on the LEVIR-CD+ and SYSU S2Looking datasets, while maintaining a lightweight architecture with only 2.56M parameters and 4.45G FLOPs. Our main contributions could be summarized as follows:\n\u2022 We propose a new lightweight remote sensing image change detection model, LCD-Net, which has a parameter size of only 2.56M and a computational complexity of 4.45G FLOPs. This model enhances the performance of change detection tasks while ensuring low computational consumption.\n\u2022 We introduce a TIF that enhances feature expressiveness and semantic consistency through the sharing and interaction of cross-temporal features. To address the issue of complex background information obscuring subtle changes, we incorporate an FFM that deepens the integration of multi-scale features, thereby enhancing the model's sensitivity to subtle changes. Furthermore, we design the GMM to dynamically adjust channel weights, strengthening the contributions of important channels while reducing parameter redundancy.\n\u2022 We conduct a series of experiments to validate the effectiveness and superiority of the proposed methods. Experimental results demonstrate that the LCD-Net model achieves better detection performance than nine state-of-the-art models across three benchmark datasets."}, {"title": "II. RELATED WORK", "content": "Traditional change detection methods can be mainly divided into pixel-based and object-based approaches [28, 11]. Pixel-based detection methods, such as image differencing [18], image ratio [27], correlation coefficient [23], and change vector analysis (CVA) [12], primarily focus on individual pixels by comparing the values of corresponding pixels in two or more images to determine whether changes have occurred in the region. These methods are easily affected by environmental noise and are suitable for change detection in simple scenes. In contrast, object-based methods utilize classification techniques [7] and clustering algorithms [9] to partition images into multiple objects and analyze the characteristics and spatial relationships of these objects to identify changed areas [26, 32, 21, 19]. However, these traditional methods often struggle to detect subtle changes in complex backgrounds, leading to insufficient accuracy. Additionally, some spectral-based techniques like Principal Component Analysis (PCA) [20] and Spectral Angle Mapper (SAM) [36] have been used to exploit spectral differences for more effective change detection in multispectral images. These methods can improve performance in specific contexts, though they still face challenges in handling complex backgrounds.\nIn recent years, convolutional neural network (CNN)-based methods have emerged as a significant research direction in remote sensing image change detection (RSCD), primarily due to their powerful feature extraction capabilities [39]. The U-Net architecture [15] has become popular for its effectiveness in multi-temporal image feature extraction and fusion. Various architectures have been developed to further enhance detection performance. For instance, the FC-ef, FC-cat, and FC-diff models [8], proposed by researchers, integrate fully convolutional encoder-decoder structures [17] with Siamese networks [42]. This design allows for effective feature extraction by utilizing skip connections, capturing both high-level and low-level features for comprehensive change detection. The DSAM model [34] incorporates the Convolutional Block Attention Module (CBAM) to differentiate between channel and spatial features at various stages, enhancing the model's focus on significant features. Additionally, the DSIFN model [43] employs attention mechanisms to fuse multi-level deep features with image difference features, significantly improving change map boundary integrity. Furthermore, the STANet model [2] utilizes a spatiotemporal attention mechanism to extract more discriminative features, thus boosting detection accuracy. The P2V model [24] addresses incomplete temporal modeling by employing two decoupled encoders, enhancing transformation identification in spatial and temporal dimensions. Lastly, the BIT model [4] leverages a bidirectional temporal convolution network to capture change patterns within time series data, adding robustness to change detection tasks."}, {"title": "C. Feature Fusion and Gating Mechanisms", "content": "In complex backgrounds, effectively capturing the differences between significant changes and subtle local variations remains a major challenge in change detection. To address these issues, the MapsNet model [29] effectively aggregates multi-level contextual information using a dual-stream fully convolutional network and multi-attention modules. The multi-directional fusion and perception network [41] enhances the propagation of information within the network through a refined feature fusion module and adaptive weighted fusion strategies. The DGFNet model [14] adopts an encoder-decoder architecture and reduces the semantic gap between different levels of features through a feature enhancement module and a dual gate fusion module, improving land cover classification performance. Furthermore, the MSGFNet model [40] combines the EfficientNetB4 model based on a Siamese network to effectively extract bi-temporal features and employs the MSGFM module to maintain boundary details and accurately detect changed targets. Building on the above studies, we propose feature fusion and gating mechanism modules to further enhance the accuracy and robustness of change detection."}, {"title": "III. METHODOLOGY", "content": "In this section, we will provide a detailed overview of the LCD-Net model, as illustrated in Figure 3. The model consists mainly of three parts: the encoder, the feature fusion module (FFM), and the decoder. The encoder employs the MobileNetV2 network to extract dual-temporal features. To address the limitations of MobileNetV2 [13] in feature extraction capabilities, the model introduces a TIF. This module enhances the information flow between dual-temporal features through channel swapping, without increasing the model parameters, thereby improving feature extraction capability. After passing through the encoder, the dual-temporal remote sensing images T1 and T2 yield multi-level features $F_1, F_2, FA', Fa'$. These output features then enter the FFM, which achieves deep integration of multi-scale features while maintaining a small parameter size. Subsequently, the fused features are passed to the decoder. The decoder consists of multiple decoding layers that perform upsampling layer by layer and incorporate high-level features from the encoder for decoding. To further enhance the extraction capability of key features, a GMM is introduced in the decoding layers, dynamically adjusting the influence of each channel to improve the model's focus on critical information. Finally, the model generates the change detection map and prediction map through a 1\u00d71 convolution operation. By integrating multi-scale feature extraction, channel exchange, and layer-wise decoding strategies, the model aims to improve the performance of change detection tasks while maintaining low computational consumption."}, {"title": "B. Encoder", "content": "Traditional VGG networks [38] and ResNet networks [37] perform well in feature extraction, but enhancing the feature extraction capability by increasing the depth of the convolution layer leads to significant computational overhead. In contrast, the MobileNetV2 network effectively reduces the computational complexity and number of parameters by introducing the deep separable linear convolution [6] and the Bottleneck structure [44]. In this paper, we use MobileNetV2 network as the coding layer and remove its global average pooling layer and the last fully connected layer. As shown in Figure 1, stage 0 to stage 4 in the encoder were used to extract the dual temporal features, and each stage contains a convolution layer with a stride of 2, so the feature map size decreases by half after each stage.\nTo enhance the feature extraction capability of MobileNetV2, we introduce the TIF after stages 2 to 4. This module aims to improve the contextual awareness of dual-temporal images through the sharing and interaction of cross-temporal features. As shown in Figure 4, the module exchanges features between the channels of dual-temporal images, allowing the model to capture temporal changes more effectively. By exchanging features between channels, the module enhances feature representation, improves semantic consistency, and mitigates the distribution differences between temporal features, thereby improving feature alignment. Moreover, this module does not require additional parameters or convolution operations, thus maintaining the lightweight nature of the model while improving its accuracy."}, {"title": "C. Feature Fusion Module", "content": "In remote sensing image change detection (RSCD), subtle changes, especially in small vegetation areas and minor modifications to buildings, are difficult to capture. These change signals are often obscured by background information, affecting detection accuracy. Traditional methods that concatenate features directly tend to result in information redundancy, which makes it difficult to effectively capture these subtle changes. Inspired by the traditional cross-attention mechanism [16], we propose the FFM, designed to optimize feature representation and information transmission, enhancing the model's sensitivity to key features. This improves the detection of subtle changes while maintaining the lightweight nature of the model. The FFM enhances feature extraction of subtle changes through an improved feature fusion strategy, boosting model performance in detecting changes. The specific process is as follows: The multi-level features $F, F... F, Fa$ obtained from the encoder are first concatenated (CAT) to form the preliminary feature combinations $x_1$ and $x_2$. Next, the concatenated features $x_1$ and $x_2$ undergo 1x1 convolution for channel adjustment, which reduces information redundancy and lowers computational complexity. After the convolution, the features $\\overline{x}_1$ are processed by the ReLU activation function and then multiplied with the feature $x_2$ to enhance the model's perception of subtle changes. This process amplifies subtle change features, enabling the model to more precisely differentiate between change regions and background information. This process can be represented by the following computation formula:\n$\\overline{s}_1 = ReLU(\\overline{x}_1) \\times \\overline{x}_2$       (1)\nThe features $\\overline{s}_1$, after undergoing convolution, are combined with the original features $\\overline{x}_2$ through an addition operation. This ensures that the original feature information is retained while achieving a more comprehensive feature representation, thereby enhancing the model's ability to perceive change information. This process can be represented by the following computation formula:\n$\\overline{s}_2 = Conv(\\overline{s}_1) + \\overline{x}_2$     (2)\nFinally, feature $\\overline{s}_2$ is multiplied element-wise with the original feature $\\overline{x}_1$ and optimized using the ReLU activation function to generate the output feature $\\overline{o}_0$. This process enhances the capture of change information and improves the precision of feature fusion. Through this design, the FFM achieves efficient feature integration and amplification of differences, improving the detection of subtle changes while reducing computational cost."}, {"title": "D. Decoder", "content": "The decoder employs a hierarchical convolutional structure combined with the Gated Mechanism Module (GMM) to enhance the extraction of key features while maintaining a lightweight design. First, a 1\u00d71 convolution is applied to reduce the channel number of the input features, eliminating information redundancy and decreasing computational complexity. The processed features are then dynamically adjusted using the GMM to optimize feature representation. Subsequently, a 3 \u00d7 3 convolution is employed for spatial feature extraction and contextual information integration. To ensure stable gradient propagation in deep networks, a residual structure is introduced. Finally, 1\u00d71 convolution is used for feature decoding, producing the final feature map. The entire structure effectively extracts multi-scale features while maintaining low computational overhead."}, {"title": "1) Gated Mechanism Module", "content": "The GMM is the core component of the decoder, designed to enhance the expressive power of channel features while reducing the model parameters. The GMM dynamically adjusts channel weights through three learnable parameters $(\\alpha, \\beta, \\gamma)$. The parameter \u03b1 is used to evaluate and aggregate the importance of each channel to effectively utilize channel information. The input features, processed through L2-norm, are multiplied by parameter \u03b1 to achieve effective channel aggregation. During the L2-norm processing, a mechanism is introduced \u03b5 (0 \u2264 \u03b5 \u2264 10\u22125) to prevent numerical instability. Compared to traditional fully connected layers, the normalization process of this module enables efficient modeling of channel relationships while reducing computational complexity. This process can be represented by the following computation formula:\n$\\overline{\\alpha}_c = \\frac{\\alpha_c}{\\|x_c\\|_2} = a_c \\sqrt[H\\times W]{\\sum_{i=1}^H \\sum_{j=1}^W (x_{cij})^2 + \\epsilon}$ (3)\nwhere $x \\in R^{C \\times H \\times W}$ represents the input x, $x_c$ corresponds to the c channel of x, $x_{cij} \\in R^{H \\times W}$, and $\\alpha_c$ denotes the learnable weight for the c channel. The parameter \u03b3 is used to learn the competitive relationships between channels. The output feature $\\overline{e}_d$ is divided by the parameter \u03b3 after mean normalization. This process effectively establishes competitive relationships between channels and reduces parameter redundancy. This process can be represented by the following calculation formula:\n$\\overline{n} = \\frac{\\gamma_c}{\\sqrt{mean \\left(e_d\\right)^2 + \\epsilon}}$         (4)\nwhere $Y = [\\gamma_1, \\gamma_2, ..., \\gamma_e] \\in Y_e$ represents the learnable weights of channel c. ed is the feature calculated after mean normalization. The learnable parameter \u03b2 is used to amplify the contribution of important channels and reduce the impact of less important ones. By utilizing the gating function $1 + tanh(x)$, a nonlinear adjustment of the channel weights is achieved, which avoids the problem of gradient explosion, making the training process more stable. This process can be represented by the following mathematical formula:\n$g_c = 1 + tanh(\\overline{e}_d \\cdot n + \\beta_c)$       (5)\nwhere $B = [\\beta_1, \\beta_2, ..., \\beta_e], B_e$ the translation for the learnable bias of the c channel.\nFinally, the output feature y is obtained by performing an element-wise multiplication between the input tensor x and the gating coefficient g. Where $g = [g_1,g_2, ..., g_c],g_c$ the translation for the gating coefficient of the c channel. This process can be expressed by the following equation:\n$y = x \\cdot g $          (6)"}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "The experiment is implemented on the PyTorch [31] platform, with an NVIDIA L20 GPU as the hardware environment. AdamW is chosen as the optimizer, with weight decay and learning rate set to 0.0025 and 0.0005, respectively. The model is trained for 100 epochs, and the loss function used was BCEWithLogitsLoss. All data are cropped to 256x256, with random Gaussian noise, salt-and-pepper noise, random cropping, and random rotation at certain angles applied. The experiments demonstrate that BCEWithLogitsLoss and AdamW accelerated the convergence speed of the network."}, {"title": "B. Datasets and Evaluation Metrics", "content": "In this paper, experiments are conducted on three classic remote sensing image change detection datasets to evaluate the model's performance. The datasets used are LEVIR-CD+, SYSU, and S2Looking.\nLEVIR-CD+ [3]: The LEVIR-CD+ dataset is a large-scale dataset specifically designed for change detection tasks, consisting of 637 high-resolution image pairs, each with a size of 1024 x 1024 pixels. The dataset captures various types of building changes that occurred over a period of 5 to 14 years, primarily focusing on changes related to buildings, such as the growth and decline of structures. To annotate the change regions in these images, experts used binary labels, where \"1\" indicates a change and \"O\" indicates no change. In the entire dataset, there are 31,333 individual change samples, 21,442,501 change pixels, and 481,873,979 unchanged pixels. We use the standard data split, with 1,260 image pairs for training and 690 image pairs for testing. The original image pairs are cropped to a size of 256\u00d7256 pixels without overlap, ensuring a fair comparison with other state-of-the-art methods.\nThe SYSU dataset contains 20,000 ultra-high-resolution image pairs captured between 2007 and 2014 in Hong Kong, with each image having a size of 256x256 pixels. This dataset includes various types of changes, such as newly constructed urban buildings, suburban expansion, pre-construction groundwork, vegetation changes, road extensions, and marine developments. There are 286,092,024 changed pixels and 1,024,627,976 unchanged pixels. The SYSU dataset not only provides a large number of training samples but also presents a higher level of difficulty. Compared to the LEVIR-CD+ dataset, the imbalance ratio in the SYSU dataset is 1:3.58, and the larger dataset, along with subtle changes, increases the challenge for detection. We use the standard data split provided by the researchers on their website, which includes a total of 12,000 image pairs: 4,000 pairs for the validation set, 4,000 pairs for the training set, and 4,000 pairs for the test set.\nThe S2Looking dataset consists of extensive side-view satellite images captured from different non-nadir angles, including approximately 65,920 annotated change instances and 5,000 pairs of dual-temporal images from rural areas. This dataset improves upon previous ones by providing a wider perspective, significant lighting variations, and the added complexity of rural photographs. It contains 66,552,990 changed pixels and 5,176,327,010 unchanged pixels, resulting in an imbalance ratio as high as 77.78. This makes it an extremely imbalanced dataset, where the ratio of foreground images (positive samples) to background images (negative samples) is 1:77.78. We use the standard data split provided by the researchers on their website, which includes 3,500 image pairs for the training set, 500 pairs for the validation set, and 1,000 pairs for the test set. The original image pairs are cropped to a size of 256x256 pixels without overlap to ensure a fair comparison with other state-of-the-art methods.\nTo evaluate the performance of the proposed method, several important metrics are employed, including Precision (PC), Recall (RC), F1-score (F1), Mean Intersection over Union (IoU), and Kappa coefficient (K). The calculation formulas are as follows:\n$\\overline{PC} = \\frac{TP}{TP+FP}$ (7)\n$\\overline{RC} = \\frac{TP}{TP+FN}$  (8)\n$\\overline{F1} = \\frac{2PC \\cdot RC}{PC + RC}$     (9)\n$OA = \\frac{TP+TN}{TP+TN+FP+FN}$ (10)\n$K = \\frac{OA - PC}{TP + FP}$        (11)"}, {"title": "C. Comparison Experiments", "content": "To evaluate the performance of the proposed method, we compare it with state-of-the-art change detection methods under the same parameter settings during the training process. We selecte nine public change detection methods, including FC-cat [8], FC-ef [8], FC-diff [8], DSAM [34], SNUNet [10], L-Unet [30], BIT [4], DSIFN [43], and P2V [24], as comparison subjects. We use accuracy, recall, F1-score, Kappa coefficient, and mean Intersection over Union as evaluation metrics for change detection accuracy. The experimental results are obtained by averaging the last five test results from multiple tests. Table I presents the detection accuracy of the proposed method and the other nine methods on the LEVIR-CD+, SYSU, and S2Looking datasets. As shown in Table I, the proposed method achieved the best performance on the LEVIR-CD+ dataset, with F1, IoU, and Kappa coefficients reaching 91.48%, 84.30%, and 91.03%, respectively. Compared to existing methods, our method improved the F1-score by 0.41%, the IoU by 0.69%, and the Kappa coefficient by 0.43%. On the SYSU dataset, the three metrics reach 81.22%, 68.38%, and 75.49%, with improvements of 1.12% in the F1-score, 1.54% in IoU, and 1.34% in Kappa. On the S2Looking dataset, the metrics reach 59.29%, 62.00%, and 56.8%, with the F1-score improved by 1.46%, IoU by 1.56%, and Kappa by 1.23%. These results indicate that the proposed method consistently outperformed other methods on the three datasets regarding the Fl-score, Kappa coefficient, and mean IoU, which are the most important metrics.\nTo comprehensively evaluate the effectiveness of our method, we select nine comparison methods and conduct visualizations to analyze each method's performance in depth. As shown in Figure 7, on the LEVIR-CD+ dataset, our method detects change areas more completely compared to others, clearly identifying the boundaries and main structures of building changes. Figure 8 demonstrates that our method performs better in detecting change areas, significantly reducing missed detections and false positives. Other methods exhibit unstable detection results under variations in lighting and noise, leading to numerous artifacts, while our method maintains high detection performance. As shown in Figure 9, other methods perform poorly in edge detection, with vague outlines of change areas, whereas our method excels in capturing edge details and accurately identifying the boundaries of change areas. This superiority is primarily attributed to our method's ability not only to effectively learn the features of each channel but also to capture detailed changes, thereby enhancing the overall performance of the model.\nTable II presents the comparison results of different methods in terms of computational efficiency. In this table, the number of model parameters (Parms) is used to assess the spatial complexity of the model, while the number of floating-point operations per second (FLOPs) is used to measure the computational complexity of the model. Under unified parameter configurations, our method demonstrates outstanding computational efficiency. Specifically, in terms of parameters, our method reduces the number of parameters by approximately three times compared to the L-Unet method, approximately 0.85 times that of the BIT method, and significantly lower than the parameter counts of the DSIFN and SNUNet methods. In terms of computational load, the LCD-Net method maintains its lightweight characteristics, with the smallest computational load among the comparison methods, reducing the number of operations by approximately 18.2 times compared to the SNUNet method. Although the FC-diff method has lower parameters and computational loads, it fails to achieve the level of accuracy in change detection of the method proposed in this study. By considering both the accuracy and computational efficiency in change detection, our method exhibits a high-precision, lightweight model structure.\nTo maintain a lightweight structure, our method is designed with three modules that each retain lightweight characteristics. The TIF allows for channel exchanges while ensuring that gradients can smoothly backpropagate through other channels, without increasing the network's parameters or computational load. The FFM significantly reduces the number of parameters through simple convolution operations and basic computational processes. The GMM effectively achieves network lightweight with minimal parameters by calculating the L2-norm and performing channel normalization. This design not only ensures computational efficiency but also enhances model performance."}, {"title": "D. Ablation Experiments", "content": "The innovations of our method mainly include three components: the TIF, the FFM, and the GMM. To validate the effectiveness of these three innovations, we conduct ablation experiments using the LEVIR-CD+, SYSU, and S2Looking datasets, selecting F1 and IoU as evaluation metrics. As shown in Table III, all three modules can effectively enhance the performance of the main network with a relatively small increase in parameters. After adding the TIF, the F1-score improved by approximately 0.52%, 1.95%, and 1.45% across the three datasets, while the IoU scores improved by approximately 0.53%, 0.89%, and 1.45%, respectively. After adding the FFM, the F1-score improved by approximately 0.28%, 0.53%, and 0.78% across the three datasets, and the IoU scores improved by approximately 0.47%, 0.62%, and 0.78%, respectively. After adding the GMM, the F1-score improved by approximately 0.47%, 0.36%, and 0.51%, and the IoU scores improved by approximately 0.80%, 0.66%, and 0.52%, respectively. This is because each of the three modules achieves capabilities for feature activation, information fusion, and extraction of key features. Therefore, all three proposed modules can effectively enhance change detection performance.\nTo provide a more intuitive presentation of the effects of the three modules, Figure 10 displays the results of the ablation experiments, selecting a set of images from three datasets. Figure 10 (a)-(g) represent the images at time points T1 and T2, the label map, and the combinations of different modules (corresponding to (d)-(g) in Table III). It is evident from the figure that when using the MobileNetV2 network as the backbone for prediction, some edges are not detected. This indicates that a single backbone network has limitations in capturing changed areas. By introducing TIF, the exchange of contextual information further activates the network features, effectively capturing more changed areas. After adding FFM, the model achieves significant improvement in the fusion of features from different layers, with some noise being suppressed, thus enhancing prediction accuracy. With the incorporation of GMM, the model not only excels at extracting key information but also captures edge details more precisely, significantly enhancing overall performance."}, {"title": "V. CONCLUSION", "content": "This paper proposes a lightweight and efficient remote sensing change detection model, LCD-Net, which aims to reduce model parameters while enhancing change detection accuracy. By introducing the Temporal Interaction and Fusion module (TIF), the model enhances the interaction between dual-temporal features, improves contextual awareness, and maintains the lightweight characteristics of the model. The Feature Fusion Module (FFM) deepens the integration of multi-scale features, increasing sensitivity to subtle changes and improving the model's change detection capability in complex backgrounds. The Gated Mechanism Module (GMM) in the decoder dynamically adjusts channel weights, further strengthening the extraction of key features and reducing the impact of redundant information on detection accuracy. Experimental results demonstrate that LCD-Net achieves good performance across various remote sensing change detection tasks while maintaining low computational overhead. The model not only excels in handling complex backgrounds and subtle changes but also provides an effective solution for practical applications such as remote sensing monitoring and environmental change assessment."}]}