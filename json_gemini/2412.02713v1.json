{"title": "Applying Item Response Theory to Distinguish Between Human and Generative AI Responses to Multiple-Choice Assessments", "authors": ["ALONA STRUGATSKI", "GIORA ALEXANDRON"], "abstract": "Generative AI is transforming the educational landscape, raising significant concerns about cheating. Despite the widespread use of multiple-choice questions (MCQs) in assessments, the detection of AI cheating in MCQ-based tests has been almost unexplored, in contrast to the focus on detecting AI-cheating on text-rich student outputs. In this paper, we propose a method based on the application of Item Response Theory (IRT) to address this gap. Our approach operates on the assumption that artificial and human intelligence exhibit different response patterns, with AI cheating manifesting as deviations from the expected patterns of human responses. These deviations are modeled using Person-Fit Statistics (PFS). We demonstrate that this method effectively highlights the differences between human responses and those generated by premium versions of leading chatbots (ChatGPT, Claude, and Gemini), but that it is also sensitive to the amount of AI cheating in the data. Furthermore, we show that the chatbots differ in their reasoning profiles. Our work provides both a theoretical foundation and empirical evidence for the application of IRT to identify AI cheating in MCQ-based assessments.", "sections": [{"title": "Introduction", "content": "Generative artificial intelligence (GenAI), and specifically conversational chatbots such as ChatGPT, are disrupting the educational landscape [32]. One of the most controversial issues surrounding the use of GenAI in education is that it makes cheating alarmingly easy, leading many educators to fear it will result in widespread counterproductive and unethical learning behaviors [1, 7, 15]. Large-scale cheating poses significant risks to education. First, it undermines the validity of assessments, potentially leading to incorrect decisions about students (e.g., in college admissions) [23]. Second, as cheating erodes confidence in the reliability of assessments as accurate measures of learners' abilities, it can ultimately devalue academic credentials [3]. Third, learning-wise, cheating can negatively affect the learning process [5]. And since cheating tends to spread-either by altering social norms or by causing students to fear falling behind - its impact intensifies over time [16].\nUnfortunately, there is already ample evidence on the use of GenAI for cheating in various educational contexts [21], and this was marked as a major risk to the integrity of educational assessment [25]. Most of the attention was given the cheating potential of GenAI on text-rich tasks such open questions or essays [15]. To tackle this, various methods have been proposed, which are mainly based on text similarities, and are implemented in commercial AI writing detection technology such as iThenticate\u00b9 and Turnitin\u00b2.\nHowever, educational assessment \u2013 particularly large-scale, standardized, and high-stakes exams relies heavily on closed-form question formats, which are amenable to automated grading (in this discussion, we focus specifically on MCQs, the most common format). ChatGPT and the like have been shown to be very successful in solving the MCQ parts of engineering and science undergraduate exams from elite institutions [6], as well as on various bar exams [14]. The risk that 'cheating with GenAI' poses to assessment has led a recent survey to conclude that \"all summative assessments using MCQs should be conducted under secure conditions with restricted access to ChatGPT and similar tools\" [19]. Nevertheless, research on detecting the use of GenAI for answering MCQs is scarce, and in fact we are familiar with only one research addressing this issue [24].\nAt first glance, detecting that GenAI was used to produce a response that is simply a digit seems implausible, as it contains very little information. The crux may be to look for patterns that emerge in sequences of responses, thus providing additional information that arises from the interactions between items. Examined from a psychometrics perspective, due to the fundamental architectural differences between human and artificial intelligence, we can expect these 'cognitions' to be impacted differently by various task dimensions [20]. As a simple illustration, let us consider conversational chatbots such as ChatGPT (hereafter, we will use conversational agents as the exemplifying application of GenAI, with ChatGPT serving as a representative example). ChatGPT is still limited in its ability to solve questions that involve figures or diagrams [6]. This means that including a visual representation may impact ChatGPT in different ways than it would impact typical human learners (in the Discussion we refer to the fact that humans as well may not be impacted in the same way by different problem dimensions). But there are also differences related to conceptual understanding and reasoning. For example, [30] reported that ChatGPT was less accurate when requested to solve under-specified physics problems or to make assumptions about the real world. Other studies that examined Chatbots' problem solving ability in chemistry [31] and biology [9] reported that the GenAI struggled in ways that are different than human learners. These studies provide empirical evidence that the anticipated differences are indeed evident in real- life assessment data. However, in order to develop tools for detecting cheating using GenAI based on these differences, which can be generalized across contexts, it is advisable to rely on rigorous theories of assessment and measurement of cognitive abilities. A key theory of assessment that seems to be appropriate is IRT, which is a psychometric framework used to model the relationship between individuals' responses to assessment items and their underlying traits or abilities [8]. The theoretical assumption behind IRT is that a plausible response pattern arises primarily from the interaction between the test-taker's ability and the difficulty of the items [8]. Deviations from the expected pattern suggest that the responses may have been influenced by an alternative response process such as guessing [10] or cheating [2, 4, 11, 13, 26]. As GenAI as well is an 'alternative' response pattern, our hypothesis was that its responses would deviate from the expected response patterns. The question is whether it is possible to model these deviations"}, {"title": "Methodology", "content": "The goals of of our research are formulated into the following research questions (RQs):\nRQ1: Do the response patterns of conversational chatbots differ from those of human learners, as measured by PFS?"}, {"title": "Research Questions", "content": "RQ2: Are there substantial differences between the response patterns of different conversational chatbots?\nRQ3: What is the impact of the level of pollution on the difference between the PFS measures of GenAI and human learners?"}, {"title": "Procedure and Data", "content": "To study the capabilities of IRT in differentiating between the response patterns of human learners and GenAI in MCQ assessments, we use datasets that include the responses of human learners in two assessment contexts: science education and a standardized high-stakes exam. By introducing a small percentage of AI-generated responses (from ChatGPT, Claude, and Gemini, with 20 responses each), we assess the effectiveness of IRT-based PFS \u2013 G, G*, U3, and ZU3 - to separate between the human and the GenAI responses, as well as between the different chatbots. Among the various PFS proposed in the literature (for a comprehensive list, see [13]), we have chosen these metrics as they have been shown to be the most effective in identifying cheating [2, 4, 13].\nProcedure. To answer RQ1 we used 2x3 datasets. The design included the two different instruments with students answers combined with 5% of responses from three different GenAI agents. For a given dataset, we calculated 4 PFS for each examinee: G [12], G* [28], U3 and ZU3 [29]. The statistics were calculated using the R package PerFit [27]. Given the values, we used the Wilcoxon rank-sum test to compare the distribution of the statistics between the group of human agents and GenAI agents.\nTo answer RQ3 we use similar 2x3 datasets as for RQ1, now using 3 level of pollution 5%, 10% and 25% following the framework of [13]. To create these datasets, we maintained all responses collected from the GenAI agents and combined them with the correct number of human responses to achieve the required level of pollution (more details in 3.3). Again, we used the Wilcoxon rank-sum test to compare the four measures between the two groups, humans and GenAI, at different pollution levels.\nTo answer RQ2: Are there substantial differences between response patterns of different chatbots on MCQ assessment, we compared the four PFS between the three chatbots using the Kruskal-Wallis test. We constructed two datasets of human answers (one per instrument), with about 5% of GenAI responses divided equally among the three chatbots.\nInstruments and student data. We used two assessment instruments from very different contexts. The first was a chemistry diagnostic examination administered via a Moodle platform as a formative assessment activity in preparation for a high-school matriculation test. It included 22 MCQs, with 931 high-school student respondents available. The second instrument was a quantitative chapter from a psychometric exam taken by prospective students as a prerequisite for admission to higher education institutions. This instrument included 20 MCQs with over 4,800 respondents. Both instruments were multi-modal, containing for some of the questions images/figures and formulas.\nCollection of Chat data. The responses were generated using three different GenAI models: ChatGPT-40, Gemini 1.5 pro, and Claude 3.5 sonnet. We uploaded a document for each instrument with a prompt that required only the final answers to the items, omitting any additional information or comments. The models typically provided an enumerated list of answers, which were then exported to a CSV format. This procedure was repeated 20 times for each model, simulating 20 'artificial students' per model. Each run was executed as a new session to prevent the models from leveraging multiple trials on the same instrument. Although the specific interaction with each model was prompted with the same text, the responses varied due to the temperature parameter being set to a non-zero value. All the chat models supported figures and visual modalities, which was important since our instruments contain some items that"}, {"title": "Person-fit statistics and their application to cheating detection", "content": "Miejer and Sijstma [18] presented in their review a large number of statistics invented for the purpose of identifying aberrantly responding examinees. Those PFS are may be classified to either parametric or non-parametric methods. The parametric models are based on IRT, and measure the distance between the test set, and the predicted responses derived from the IRT model with parameters fine-tuned on train data. A non parametric PFS are calculated directly from the data set on N examinees, and theirs scored responses to J items, without relaying on estimating parameters for IRT model [13]. Such non parametric PFS measures G, G* [28], U3 and ZU3 [29] are used in this work. Let us have an $X = (X_{nj} | n = 1, . . ., N, j = 1, ..., J)$, a set of examinee responses, where $X_{nj} = 1$ denotes a correct response made by examinee n (human or GenAI agent) on item j, and $X_{nj} = 0$ denotes an incorrect response. Calculating G and G* depends on reasonableness of examinee n, which defined as $r_n = \\Sigma_{j=1}^{J} X_{nj}$ and represents the sum of correct answers. The statistic G counts the number of item response pairs that deviate from the ideal pattern. Such a pattern contains correct answers to the easiest $r_n$ test items out of J, and incorrect to the remaining, and we refer it as \"Guttman pattern\" and denote this random vector as $X_G$. The statistic G* normalizes G to have the range [0,1].\nGiven \"Guttman pattern\" and ordered items by increasing value of difficulty, the \"reversed Guttman pattern\" is a vector with 1s on the last in positions, and Os elsewhere [18]. U3 Statistic is zero if the response vector has \"Guttman pattern\" and one for a response vector with the \"inverse response pattern.\" It is normally distributed with conditional expectation value and variance, conditioned on X, for an examinee with reasonableness $r_n$. ZU3 transforms U3 to have a unit-normal distribution (i.e., with mean = 0 and sd = 1).\nAfter computing the PFS for all examinees in the dataset, examinees with unusually high scores on any of these statistics are flagged as potential aberrant respondents. For example, if an examinee's G* or ZU3 is significantly higher than a pre-determined threshold (based on the normal distribution of these statistics), their response pattern is considered suspicious. The effectiveness of PFS improves with a greater number of items. Short tests may not provide enough data to reliably detect aberrant patterns. Typically, tests with at least 20 items show a reasonable variance, that allows anomaly detection [17, 29]. The instruments used in this study contain at least 20 items. The explicit form of the four statistics used in this work can be found in [13]."}, {"title": "Experiments and Results", "content": "As described above, we compared four PFS (G, G*, U3, and ZU3) on the 2x3 datasets including responses from human learners and 3 GenAI chatbots (ChatGPT, Gemini, and Claude) on 2 assessment instruments (chemistry and psychometric quantitative test). The PFS were computed for each combination (chatbot X instrument) in separate, with a 5% pollution level, meaning that 5% of the 'students' in each dataset were from the chatbot.\nTable 1 presents the results of the one-sided Wilcoxon signed-rank test for these comparisons. Notably, the tests show that the PFS of the chatbots were statistically significantly higher than those of the human respondents, with the highest p-value being less than 0.00001. This finding held consistently across all the 2x3x4 combinations of instruments, AI agents, and PFS measures, confirming our hypothesis. This is demonstrated in Figure 1, which shows the well separated density plots of the G PFS of human learners and the chatbots. As can be seen, the G measure effectively distinguishes"}, {"title": "Comparing the Response Patterns of Conversational Chatbots and Human Learners (RQ1)", "content": "between these two groups, showing its value in assessing the response source. Additional figures displaying the density distributions for G*, U3, and ZU3 can be found in appendix A.\nThus, with respect to RQ1, we conclude that the response patterns of conversational chatbots differ significantly from those of human learners, as measured by IRT and PFS."}, {"title": "Analysis of Person-Fit Statistics Measures of Different GenAl Respondents (RQ2)", "content": "To address RQ2, we investigated whether there are substantial differences between the PFS values (G, G*, U3, and ZU3) of the responses of the ChatGPT, Claude, and Gemini 'students' on the 2 instruments. The datasets were constructed in the following manner: for the chemistry instrument, the dataset contained 976 responses, with 931 responses from human students and 45 responses from the three GenAI agents (15 responses from each agent). For the psychometric instrument, the dataset contained 1040 responses, with 980 responses from human students and 60 responses from the three GenAI agents (20 responses from each agent). This set-up aimed to maintain the experimental rationale of computing the PFS for 5% pollution level. We then compared the PFS of the three chatbots on each measure and each dataset using Kruskal-Wallis test.\nResults on the chemistry instrument: The Kruskal-Wallis test indicated significant differences in the PFS values among the three GenAI agents as follows: for G (H = 14.00, p < 0.001), for G* (H = 6.60, p < 0.05) and ZU3 (H = 9.11, p <0.05). However, for U3 (H = 5.84, p = 0.054), the differences were not statistically significant. Figure 2 illustrates the"}, {"title": "The Impact of Pollution Level on the Difference Between the PFS of GenAl and Human Learners (RQ3)", "content": "To address RQ3, we used 2x3 datasets as were used for RQ1, but included three levels of pollution: 5%, 10%, and 25%, following the framework of [13], resulting in 2x3x3 datasets. To create these datasets, we maintained the 20 responses collected from each GenAI agent and combined them with the appropriate number of human responses to achieve the required level of pollution (380 learners' responses for 5%, 180 for 10%, and 60 for 25%). Similar to RQ1, we used the Wilcoxon test to compare the four PFS measures between humans and GenAI at different pollution levels. The results are exemplified in Figure 3, which presents the mean values with \u00b11-STD of G, G*, U3, and ZU3 for ChatGPT-40 on the"}, {"title": "Discussion", "content": "Along with the great potential of GenAI, there is a growing fear that it will lead to large-scale cheating, devaluing educational assessment. Therefore, solutions that prevent or reduce GenAI cheating are very important. However, while MCQs are arguably the most common questioning format, there has been very little research on preventing"}, {"title": "The Impact of Pollution Level on the Difference Between the PFS of GenAl and Human Learners (RQ3)", "content": "GenAI cheating in MCQ-based assessments. We propose building systems based on educational measurement theory, specifically IRT and PFS, to model assessment behaviors using metrics that are sensitive to the differences between human and artificial intelligence behavior. In two assessment contexts and on a few major conversational agents, we show that PFS measures can be used to capture the different response patterns of humans and GenAI (RQ1). However we also show that chatbots differ in terms of their assessment behavior (RQ2), and that the effectiveness of PFS-based measures in distinguishing between humans and GenAI decreases as GenAI becomes more prevalent (RQ3). Thus, while applying IRT and PFS to separate between humans and GenAI is a promising approach, much more research in this direction is needed, especially given the heterogeneity of human learning [22], which implies that human learners as well are expected to differ significantly from one another.\nContribution. The main contribution of this paper is proposing a conceptual framework that applies well-established educational measurement theory \u2013 IRT, and within it, PFS \u2013 for distinguishing between GenAI and humans in MCQ- based educational assessments, demonstrating its effectiveness in two very different, authentic assessment contexts, and on three leading conversational agents. Additionally, the study explores the limitations of such techniques.\nLimitations. While promising, PFS-based cheating detection methods have some key limitations. First, they are sensitive to the level of pollution, as demonstrated in RQ3. Second, they are applied retrospectively and less appropriate for real-time detection. Third, misfit that leads to high PFS measures can also arise from legitimate conditions such as learning disabilities. Regarding research validity, the main limitation of this research relates to its external validity, as it was applied to a small number of instruments, and examined three specific GenAI tools. However, this research is ongoing and aims to pave the way for more larger-scale research on applications of IRT to the challenge of distinguishing between human and GenAI.\nFuture work. The present paper centered on establishing the conceptual and empirical foundation for using IRT and PFS to identify GenAI-generated responses in MCQ assessments. In future work, we intend to build on this foundation to develop machine learning algorithms for detecting GenAI-assisted cheating."}]}