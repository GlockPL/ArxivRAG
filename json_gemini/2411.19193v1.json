{"title": "Convex Regularization and Convergence of Policy Gradient Flows under Safety Constraints", "authors": ["Pekka Malo", "Lauri Viitasaari", "Antti Suominen", "Eeva Vilkkumaa", "Olli Tahvonen"], "abstract": "This paper studies reinforcement learning (RL) in infinite-horizon dynamic decision processes with almost-sure safety constraints. Such safety-constrained decision processes are central to applications in autonomous systems, finance, and resource management, where policies must satisfy strict, state-dependent constraints. We consider a doubly-regularized RL framework that combines reward and parameter regularization to address these constraints within continuous state-action spaces. Specifically, we formulate the problem as a convex regularized objective with parametrized policies in the mean-field regime. Our approach leverages recent developments in mean-field theory and Wasserstein gradient flows to model policies as elements of an infinite-dimensional statistical manifold, with policy updates evolving via gradient flows on the space of parameter distributions. Our main contributions include establishing solvability conditions for safety-constrained problems, defining smooth and bounded approximations that facilitate gradient flows, and demonstrating exponential convergence towards global solutions under sufficient regularization. We provide general conditions on regularization functions, encompassing standard entropy regularization as a special case. The results also enable a particle method implementation for practical RL applications. The theoretical insights and convergence guarantees presented here offer a robust framework for safe RL in complex, high-dimensional decision-making problems.", "sections": [{"title": "1. INTRODUCTION", "content": "Stochastic dynamic decision processes with safety constraints play a pivotal role in a wide range of applications, including autonomous systems, resource management, and finance. In such problems, the agent's goal extends beyond merely maximizing cumulative discounted rewards; it also involves ensuring safety by adhering to state-dependent constraints over an infinite time horizon. When decision-making occurs within an environment lacking an analytical model, these problems are often addressed through various adaptations of customized reinforcement learning (RL) algorithms.\nThe mathematical formulations of safety-constrained RL problems differ significantly across applications and algorithmic frameworks [42, 65, 66, 91, 93]. To intuitively introduce the concept of safe RL, let S and A represent the state and action spaces, respectively, which may have infinite cardinality. Without loss of generality, we assume that the set of safe actions can be described by a set-valued mapping $(t,s) \\rightarrow D_t(s)$, where for each state $s_t \\in S$, the safe action set is given by $D_t(s_t) \\subset A$. A policy is deemed safe if the actions it prescribes, $a_t \\sim \\pi(\\cdot | h_t)$, conditioned on the history $h_t = (s_0, a_0, ..., s_t)$, including the current state, satisfy $a_t \\in D_t(s_t)$ either almost surely or with high probability. The general safe RL problem with state constraints can thus be expressed as:\nmaximize  $E_\\pi [\\sum_{t=0}^{\\infty} \\beta^t u(S_t, a_t)]$ subject to $Pr(\\{(S_t, a_t)_{t=0} : a_t \\in D_t(s_t) \\forall t\\}) \\geq 1 - \\xi$,"}, {"title": "CONVEX REGULARIZATION OF PARAMETRIZED POLICY GRADIENT", "content": "where $\\Pi$ denotes the space of stochastic policies, $P_\\pi$ is the probability measure induced by policy $\\pi$ on the space of state-action trajectories, and $\\xi$ is the permissible risk of safety violations. Policies with $\\xi \\in (0, 1)$ are said to be optimized under chance constraints [72, 75], while in many critical applications, safety must be guaranteed almost surely, necessitating $\\xi = 0$ [85].\nIn a discrete setting with finite state and action spaces, it may be feasible to specify the set of safe states precisely. However, when state and action spaces are large, the safety map $D_t$ needs to be approximated [91]. A common approach is to introduce a set of non-negative safety cost functions $g_k : S \\times A \\rightarrow \\mathbb{R}_+, k = 1, ..., K$, such that, for any trajectory $(s_t, a_t)_{t=0}^{\\infty}$, the probability of the cumulative safety costs exceeding their admissible budgets $b_k \\in \\mathbb{R}_+$ should remain below a given safety threshold $\\xi$:\n$P_\\pi\\{(\\textstyle s_t, a_t)_{t=0}^{\\infty} : \\sum_{t=0}^{\\infty} \\beta_k^t g_k(s_t, a_t) - b_k \\geq 0  \\forall k\\} \\leq \\xi$,\nwhere $\\beta_k \\in (0, 1)$ denote constraint specific discount factors.\nIn many applications and algorithms [1, 17, 34, 90], the probability constraint is replaced with a weaker requirement that the expected cumulative safety cost remains below a certain threshold, i.e. $E_\\pi[\\sum_{t=0}^{\\infty}\\beta_k^t g_k(s_t, a_t)] \\leq b_k$. This simplification is considered necessary for being able to use Lagrangian methods with a finite number of multipliers.\nWhile standard policy gradient methods [47, 57, 82, 88] can be applied to unconstrained Markov Decision Processes (MDPs), the addition of safety constraints complicates both the formulation and the solution, particularly in continuous state-action spaces. The constraints necessitate specialized methods to ensure that each policy meets safety requirements, ideally with guarantees on convergence and robustness. Recent years have seen a surge in interest in regularization methods for RL algorithms [2, 24, 43, 46, 47, 68], as they offer mechanisms to stabilize training and enhance convergence properties, especially in high-dimensional or continuous settings. The most commonly considered approach is based on entropic regularization of rewards, where $u(s_t, a_t)$ is replaced with a regularized reward $u(s_t, a_t) - \\epsilon \\log(\\pi(a_t|s_t))$. Standard entropy-based regularization of rewards has proven successful in discrete settings, where convergence of policy gradient methods is relatively well-understood [12, 23, 24, 61, 94]. However, for continuous state-action spaces, with more general than linear or log-linear policy representations, conventional approaches often fall short, as they do not scale to continuous decision spaces or parametrized policies with non-linear function approximators [3, 58, 59]. Inspired by the notion of double entropic regularization studied by [58], this paper introduces a generalized convex regularization framework for policy gradient flows in safety-constrained MDPs, combining both reward and parameter regularization. Specifically, we consider the following regularized problem with parametrized policies in the mean-field regime:\nmaximize $E_\\mu\\left[\\sum_{t=0}^{\\infty} \\beta^t (u(s_t, a_t) - \\epsilon F(\\pi_\\mu (a_t|s_t))) + \\kappa H(\\mu)\\right]$\ns.t. $\\mathbb{P}_\\pi\\left[\\sum_{t=0}^{\\infty} \\beta g_k(s_t, a_t) \\leq b_k \\right] = 1 \\quad \\forall k \\in \\{1,..., K\\}$,\nwhere F is a reward-regularization function and H is a convex functional on the space of parameter distributions $P(X)$. By applying recent developments in mean-field theory and"}, {"title": null, "content": "Wasserstein gradient flows [8, 19], we model policies as elements on an infinite-dimensional statistical manifold {$\\pi_\\mu : \\mu \\in P(X)$} and consider policy updates in terms of gradient flows $(p_t)_{t \\geq 0}$ on the space of parameter measures $P(X)$. Similar policy space has been previously studied by [3, 58]. This approach leverages convex regularization to not only smooth the reward landscape but also directly influence the distribution of policy parameters, facilitating convergence even under stringent safety constraints. Regularization can act as a \"convexifier,\" transforming inherently non-convex problems into tractable forms where gradient-based methods can be effectively applied.\nOur main contribution is to establish the conditions for local and global convergence of policy gradient methods for safety-constrained decision processes. For this purpose we start by proving solvability of safety constraint augmented problem (without regularization). We then prove that one can approximate the problem by using smooth and bounded approximations for various objects, including regularization functions F and H. This allows to define gradient flows that then can be translated into particle method and corresponding RL algorithm to find the solution. In the process, we study the roles of both regularizations and, in particular, show that under sufficient amount of regularization, the problem becomes convex and one obtains convergence towards a global solution with exponential rate of convergence. It is worth to note that we consider general functions F and H, from which some studied special cases including entropy regularization follows as a special case.\nThe remainder of the paper is organized as follows. In Section 2, we present more detailed discussion on our general setting and results, accompanied with a related literature review. Our technical presentation is postponed into Sections 3\u20138. In Section 3 we formulate the solvability result for the problem with safety constraints. After that in Section 4, we provide precise details on our double regularization framework. Smooth approximations are then considered in Section 5, and gradient flows and their convergence is detailed in Section 6. The associated particle method is discussed in Section 7, while all the technical proofs are postponed to Section 8. Some essential preliminaries are presented in the appendix.\nLet us now formulate our safety-constrained problem (without regularization),\n(1)\nmaximize $E_\\pi [\\sum_{t=0}^{\\infty} \\beta^t u(S_t, a_t) | S_0 \\sim P_0]$\n$\\pi \\in \\Pi$\ns.t. $\\sum_{t=0}^{\\infty} \\beta_k g_k(S_t, a_t) \\leq b_k \\quad P-\\text{a.s.}, \\forall k \\in \\{1, ..., K\\}$,\nwith more details. Throughout we assume that the state space S and action space A are Polish spaces (i.e., separable, completely metrizable topological spaces), with possibly infinite cardinality, $p: S \\times A \\rightarrow P(S)$ is a weakly continuous transition probability kernel (see Assumption 1), with $P(S)$ denoting the set of probability measures on S, $p_0 \\in P(S)$ is an initial state distribution, $u : S \\times A \\rightarrow [-\\infty, +\\infty]$ is a bounded, upper semicontinuous utility (reward) function with discount factor $\\beta$, and $g_k : S \\times A \\rightarrow [-\\infty, +\\infty]$ are single-period safety cost functions with corresponding discount factors $\\beta_k$ and budgets $b_k \\geq 0$. To ensure that the problem is well-posed, we further require the safety cost functions to be lower semicontinuous and satisfy an inf-compactness condition (see Assumption 2). Here, the expected discounted utility is maximized over a policy space $\\Pi$, where a policy can be interpreted as a sequence"}, {"title": null, "content": "$\\pi = (\\pi_t)$ of transition probabilities such that $\\pi_t(\\cdot | h_t)$ is a conditional probability measure on A for every history $h_t = (s_0, a_0, ..., s_t)$. By Ionescu-Tulcea theorem, the probability measure $\\mathbb{P}_\\pi$, which is induced by $\\pi$ on the product space of histories, is uniquely defined given the initial state distribution $p_0$ and the transition kernel $p$. We use $\\mathbb{E}_\\pi[\\cdot]$ to denote the expectation taken with respect to the measure $\\mathbb{P}_\\pi$. Throughout, we assume that the safety constraint is satisfied almost surely.\nWe note that (1) can be usually expressed in an unconstrained form\n(2) maximize $E_\\pi[\\sum_{t=0}^{\\infty} \\beta^t u(s_t, a_t) - \\delta_{\\geq 0}(z_t)|S_0 \\sim p_0]$,\n$\\pi \\in \\Pi$\nwhere $(z_t)_{t \\geq 0}$ is a sequence of state variables $z_t = (z_{t,k})_{k=1}^K$ representing the remaining safety budgets for each of the constraints $k = 1,...,K$. Given the initial budgets $z_{0,k} = b_k$, we define\n(3) $z_{t+1,k} = \\Phi_{z_{t,k}}(s_t, a_t) := (z_{t,k} - g_k(s_t, a_t))/\\beta_k$,\nwhere $\\Phi_{z_{t,k}}$ is known as a safety index for constraint k parametrized by the remaining budget, $z_{t,k}$, before paying the cost for choosing an action $a_t$ in the current state $s_t$. The budget process monitors constraint violations and records the value of the remaining safety budget. The indicator function $\\delta_{\\geq 0}(z_t)$ is 0, if $z_{t,k} \\geq 0$ for all $k = 1, ..., K$, and $+\\infty$ otherwise. The representation (2) is valid, when the instantaneous safety cost functions $g_k$ are assumed to be non-negative. Since the budget process is Markov, it can be considered to be a part of the environment state. The key benefit is that we can use additional state variables to represent the constraints instead of a Lagrangian with state-dependent multipliers [85].\nAlthough the formulation (2) appears promising due to its convenient way to handle the state-wise safety constraints, we can see that the sharpness of the indicator function is problematic when constraint violations are encountered during the learning process. By considering a smooth barrier function $B$, we obtain the following unconstrained reformulation\n(4) maximize $E_\\pi [\\sum_{t=0}^{\\infty} \\beta U_B(s_t, a_t) | S_0 \\sim p_0]$,\n$\\pi \\in \\Pi$\nwhere the single-period utility u is replaced with the safety-augmented utility function\n(5) $U_B(s_t, a_t) = u(s_t, a_t) - B(z_t(s_t, a_t))$.\nHere, $z_t = (\\Phi_{z_{t,k}})_{k=1}^K$ is a vector of constraint-wise safety indices, and $B \\circ z_t$ is a state-action safety barrier function such that for every history $(s_t, a_t)_{t > 0}$, where $(s_t, a_t) \\in \\text{lev}_{>0}z_t$ and $\\lim_{t \\rightarrow \\infty} z_t(s_t, a_t) = 0$, we have $\\lim_{t \\rightarrow \\infty} B( z_t (s_t, a_t)) = +\\infty$.\nIn general, there may not always exist an optimal policy, let alone a Markov policy or a stationary policy. Most approximate dynamic programming and reinforcement learning methods are designed for problems where the agent collects data from the environment in short time steps. In natural resource management problems, the outcomes of policies are observed only after several years, making a fully data-driven approach with trial-and-error learning impractical, especially with safety constraints limiting admissible policies. Hence, before considering numerical solutions for infinite horizon problems, it is sensible to establish the conditions under which the safety constrained problem (1) admits an optimal solution"}, {"title": "2.1. Modeling policy space as a statistical manifold", "content": "that can be represented by a stationary Markov policy. To ensure that the model is well-defined, we will make use of the regularity assumptions introduced by [81] to limit the class of admissible problem formulations.\nIn the case of finite S and A, policies are typically modeled using either direct or tabular softmax parametrization. However, when S and A are large or infinite, the tabular representation becomes computationally intractable. For continuous or mixed state-action spaces, we can model the policy space using a class of parametrized policies, where the policies are commonly of form\n(6) $\\pi_x(da|s) \\propto \\exp(\\psi(s, a, x)) dp(a)$,\nwhere $\\psi : S \\times A \\times X \\rightarrow \\mathbb{R}$ is a nonlinear function (e.g., neural network), X denotes the parameter space, and p is a finite reference measure. For a finite-dimensional $X \\subset \\mathbb{R}^d$, the parametrized policy $\\pi_x$ can then be interpreted as a point on a finite-dimensional statistical manifold (or a parameter submanifold), $\\Pi := {\\pi_x : x \\in X} \\subset P(A|S)$, where $P(A|S)$ denotes the space of conditional probability measures. As a convex subspace of the space of bounded signed kernels, $P(A|S)$ is a Banach space when endowed with the total variation norm [60]; see Appendix B.\nExample 2.1 (Diagonal Gaussian Policy). For continuous actions, we can take the diagonal Gaussian policies as an example. A diagonal Gaussian distribution is a special case of multivariate Gaussians where the covariance matrix only has entries on the diagonal, which can be represented as a vector. Let $m : S \\times X \\rightarrow \\mathbb{R}^{\\text{dim}(A)}$ and $\\sigma: S \\times X \\rightarrow \\mathbb{R}^{\\text{dim}(A)}$ denote the mean actions and their standard deviations. We can then define the function approximator for a Gaussian policy as\n$\\psi(s, a, x) = -\\frac{1}{2} \\left((a - m(s, x))^T \\Sigma(s, x)^{-1}(a - m(s, x))\\right)$,\nwhere $\\Sigma(s, x) = \\sigma(s, x) \\sigma(s, x)^T$ is a diagonal matrix. The policy is admissible as long as m and $\\sigma$ are sufficiently smooth.\nTo allow for more general policies, with infinite-dimensional parametrization, we will next consider another formulation, where policies are parametrized by measures. We use $P_p(X)$ to denote the space of Borel probability measures on X with a finite p-moments, which becomes a metric space when endowed with the $L^p$-Wasserstein distance. For $\\mu,\\nu \\in P_p(X)$, their $L^p$-Wasserstein distance is defined by\n(7)\nW_p(\\mu, \\nu) = \\inf \\left\\{ \\left(\\int_{X^2} |x - x'|^p d\\gamma(x, x') \\right)^{1/p} : \\gamma \\in \\Gamma(\\mu, \\nu) \\right\\}.\nwhere the infimum is taken over the a set of transfer plans or couplings between $\\mu$ and $\\mu'$. In this paper, we mainly consider the case of $p = 2$. Letting $pr^1, pr^2 : X \\times X \\rightarrow X$ be the projections of $X \\times X$ onto the first and second copy of $X$, respectively, we define the set of couplings as probability measures on $X \\times X$ with marginals $\\mu$ and $\\nu$; that is,\n$\\Gamma(\\mu, \\nu) := {\\gamma \\in \\mathbb{P}(X \\times X) | pr^1 # \\gamma = \\mu, pr^2 # \\gamma = \\nu}$,\nwhere the push-forwards of $\\gamma$ along projections are defined as $pr^1 # \\gamma(A) := \\gamma(A \\times X) = \\mu(A)$ and $pr^2 # \\gamma(B) := \\gamma(X \\times B) = \\nu(B)$ for any Borel sets $A, B \\subset X$. The set of optimal transfer plans, $\\Gamma_0(\\mu, \\nu) \\subset \\Gamma(\\mu, \\nu)$ is the convex and narrowly compact set of plans that attain the minimum Wasserstein distance between the given marginals $\\nu$ and $\\mu$."}, {"title": "2.2. Regularization in policy and parameter spaces", "content": "Let us now consider the Hilbertian case $P_2(X)$ with $X = \\mathbb{R}^d$. By modeling the distributions of policy parameters as elements of $P_2(X)$, we can use the recent developments in mean-field theory to extend our results to policies parametrized by measures [3, 58]. In the mean-field approach, the policy space is understood as an infinite-dimensional statististical manifold $\\Pi_2 := {\\pi_\\mu : \\mu \\in P_2(X)}$, where the policies are themselves functionals on the space of parameter measures. For a given $\\mu$, the parametrized policy is then given by\n(8) $\\pi_\\mu(da|s) := \\pi(\\mu)(da|s) \\propto \\exp\\left( \\int_X \\psi(s, a, x) d\\mu(x) \\right) dp(a)$,\nwhere the approximation function $\\psi : S \\times A \\times X \\rightarrow \\mathbb{R}$ is a bounded and jointly measurable function on the product space $S \\times A \\times X$ equipped with product measure $L^n_S \\otimes p \\otimes L^d$. Here $L^n_S$ and $L^d$ denote Lebesgue measures on S and X, respectively, and p is a fixed reference measure on A. In this paper, we assume that the approximator $\\psi$ is at least once differentiable with respect to x, with bounded derivatives for all $s \\in S$, $p-a.e. a \\in A$.\nRemark 2.2 (Deep ensemble models as mean-field approximations). The mean-field framework can be motivated by establishing a connection to training of deep horizontal ensemble models. Let $(x_i)_{i=1}^K \\in X^K$ be a parameter vector separated into K components $x_i \\in X$. When the model $\\psi$ is allowed to be of arbitrary complexity, we can see the mean-field policy $\\pi_\\mu$ as an infinite-width limit of policies $\\pi_{\\mu_K}$ defined by an ensemble of K neural networks ${\\psi(\\cdot; x_i)}_{i=1}^K$:\n(9) $\\pi_{\\mu_K}(da|s) \\propto \\exp\\left( \\frac{1}{K} \\sum_{i=1}^K \\psi(s, a, x_i) \\right) dp(a)$,\nwhere $\\mu_K = \\frac{1}{K} \\sum_{i=1}^K \\delta_{x_i}$ denotes an empirical measure defined by the parameters of the K ensemble models.\nBy considering discrete measures $\\mu = \\delta_x \\in P_2(X)$, where $\\delta_x$ denotes the Dirac's measure at x, we can always recover the policies $\\Pi_{\\delta}$ as a subset of $\\Pi_2$ by setting $\\pi(\\mu) = \\pi(\\delta_x) = \\pi_x$. When the parameter space X has a dense countable subset D, Hahn-Banach theorem implies that the subset of all the convex combinations with rational coefficients of $\\delta$-measures concentrated in D is narrowly dense in $P_2(X)$, i.e. we have that $P_2(X) = \\overline{\\text{Conv}(C)}$, where $\\overline{\\text{Conv}(C)}$ is the closed convex hull of $C = {\\delta_x : x \\in D}$. Then, we can also note that the empirical measures $\\mu_K \\rightarrow \\mu \\in P_2(X)$ converge to a limiting measure as $K \\rightarrow \\infty$, where the ensemble policies converge to a mean-field policy on $\\Pi_2$ as $\\mu_K = \\frac{1}{K} \\sum_{i=1}^K \\delta_{x_i} \\rightarrow \\mu$ on $P_2(X)$ when $K \\rightarrow \\infty$.\nThe most commonly applied form of regularization for MDPs is to include entropy as part of the reward (or cost) function (see, e.g., [24, 43, 47]). However, we can also consider a more direct form of regularization of parameter distributions as demonstrated by [58] in the case of mean-field policies. In this paper, we extend the pioneering work by [58] to safety-constrained problems by considering the following generalized framework for convex regularization of rewards and parameter distributions:\n(10)\nminimize $J(\\mu) = -V_\\epsilon(\\pi_\\mu) + \\kappa H(\\mu), \\quad H(\\mu) = \\sum_{j=1}^J k_j H_j(\\mu), \\quad k_j \\geq 0 \\quad \\forall j$,\n$\\mu \\in P_2(X)$\nwhere $V_\\epsilon$ is the function with regularized rewards, and $H$ is a A-convex regularization functional acting on parameter distributions with strength parameter $\\kappa$."}, {"title": "2.3. Wasserstein gradient flows for regularized objectives", "content": "Definition 2.3 (Geodesic convexity in $P_2(X)$). A function $F : P_2(X) \\rightarrow \\mathbb{R}$ is called $\\lambda$-convex along Wasserstein geodesics if, for any two probability measures $\\mu_0, \\mu_1 \\in P_2(\\mathbb{R}^d)$ and any Wasserstein geodesic $(\\mu_t)_{t\\in[0,1]}$ joining $\\mu_0$ and $\\mu_1$, we have:\n$F(\\mu_t) \\leq (1-t)F(\\mu_0) + tF(\\mu_1) - \\frac{\\lambda}{2} t(1-t) W_2^2(\\mu_0, \\mu_1)$,\nfor all $t \\in [0, 1]$.\nThe double entropic regularization studied by [58] can be recovered by choosing $F(\\pi_\\mu) = \\log(\\pi_\\mu)$ and $H(\\mu) = KL(\\mu | \\gamma)$. The usual entropic regularization framework is obtained by setting $H(\\mu) = 0$.\nThe first form of regularization in (10) is obtained by adding $F(\\pi)$ in the reward function. The regularized policy value function is then given by\n(11) $V_\\epsilon(\\pi_\\mu) = E_{\\pi_\\mu} \\left[ \\sum_{t=0}^{\\infty} \\beta^t \\left(U_B(S_t, a_t) - \\epsilon F\\left(\\frac{d\\pi_\\mu}{dp}(a_t | S_t)\\right)\\right) | S_0 \\sim p_0 \\right]$,\nwhere $E_\\pi$ is understood as a short hand for writing $a_t \\sim \\pi_\\mu(\\cdot|S_t)$, $S_{t+1} \\sim p(\\cdot|S_t, a_t)$, $S_0 \\sim p_0$, and $F \\in C^2(0, +\\infty)$ is a reward regularization function such that $z \\rightarrow zF(z)$ is a convex function. For example, the usual entropy regularization is recovered by choosing $F(z) = \\log(z)$, and studied among others in [2, 24, 43, 46, 47, 68].\nThe second form of regularization is obtained using $H$, which is assumed to be a geodesically convex functional on $P_2(X)$, where each component $H_j$ is of form\n(12) $H_j(\\mu) = H_j(\\mu | \\gamma_j) = \\begin{cases} \\int_X H_j(\\frac{d\\mu}{d\\gamma_j}) d\\gamma_j & \\text{if } \\mu \\ll \\gamma_j \\\\ +\\infty & \\text{otherwise}. \\end{cases}$\nThe parameters $\\epsilon > 0$ and $\\kappa_j > 0$, $j = 1,..., J$ control the strength of regularization in the action space and parameter space, respectively. The integrands $z \\rightarrow H_j(z) \\in (0, +\\infty)$ are assumed to be proper convex functions and $\\gamma_j$ are given reference measures such that $\\mu \\ll \\gamma_j \\ll \\gamma$ for all $j = 1, ..., J$, where $\\gamma$ is a global reference measure for the regularization terms. Commonly, we can choose $\\gamma$ to be the Lebesgue measure $L^d$.\nIntuitively, each $H_j$ quantifies the divergence between two distributions, $\\mu$ and the prior information represented by reference measure $\\gamma_j$. The convex integrands $H_j$ can be understood as cost functions of the likelihood ratio. By selecting $H_j(z) = z \\log(z)$, we obtain the relative entropy or Kullback-Leibler divergence. As a broader family of regularizers, we can take the family of f-Divergences [4, 31], which covers most of the commonly used entropy functionals.\nExample 2.4 (Generalized entropies as regularizers). For instance, consider the m-relative entropy, which stems from Bregman divergence given by\n$H^m(\\mu | \\gamma) = \\frac{1}{m(m-1)} \\int_X \\left{\\rho^m - m \\rho \\sigma^{m-1} + (m - 1) \\sigma^m \\right} dx$"}, {"title": null, "content": "for $\\gamma = \\sigma L^d \\in P_{ac}(X)^1$ and $\\mu = \\rho L^d \\in P_{ac}(X)$ with $\\mu \\ll \\gamma$, and $m \\in [\\frac{(d-1)}{d}, 1) \\cup (1, \\infty)$.\nThough, the functional is not defined for m = 1, this can be seen as a generalization of Kullback-Leibler divergence, which can be recovered as the limit\n$\\lim_{m \\rightarrow 1} H^m(\\mu | \\gamma) = \\int_{\\mathbb{R}^d} \\left{\\rho \\log \\rho - \\sigma \\log \\sigma - (\\log \\sigma + 1)(\\rho - \\sigma)\\right} dx = KL(\\mu | \\gamma)$.\nSuppose that the objective functional $J: P_2(X) \\rightarrow \\mathbb{R}$ is sufficiently smooth. Then, a gradient flow can be defined as a steepest decent for J on $P_2(X)$ by writing\n(13) $\\partial_t \\mu_t = - \\text{grad}_W J(\\mu_t)$ for almost every $t \\in I \\subset \\mathbb{R}$,\nfor some weak notion of the time derivative $\\partial_t \\mu_t$ and functional gradient $\\text{grad}_W J$. Despite technical nuances, the underlying idea is still intuitively analogous to gradient descent in the Euclidean case.\nRemark 2.5 (Policy gradient on $X = \\mathbb{R}^d$). By considering the gradient flow of discrete measures, $(\\mu_t)_{t \\in I}$, in the unregularized case, where $\\epsilon = 0$, $\\kappa_j = 0$, $j = 1,..., J$, and each measure is defined as a point mass $\\mu_t = \\delta_{x_t}$, we can recover the ODE for the vanilla policy gradient as\n(14) $\\frac{d}{dt} x_t = \\nabla V_\\epsilon(x_t)$.\nThe solution trajectory ${x_t : 0 \\leq t \\leq T}$ of the ODE induces a path ${\\pi_{x_t}: 0 \\leq t \\leq T}$ on the policy manifold $\\Pi_{\\delta}$ with a corresponding trajectory of objective values ${V_\\epsilon(\\pi_{x_t}) : 0 \\leq t \\leq T}$.\nTo apply (13) in practice, we first need to address two challenges: (1) the safety-augmented results are not bounded; (2) the objective function $J$ may be non-smooth and not have a well-defined linear functional derivative. Therefore, instead of solving (10) directly, we construct a sequence of bounded, smooth approximations ${J^n}_{n=1}^{\\infty}$ such that for any sequence of solutions $\\mu_n \\in \\text{argmin}{J^n}$ the cluster points belong to $\\text{argmin}{J}$ with $J_0(\\mu) = V_0(\\pi_\\mu)$ denoting the value function with $\\epsilon = 0$. Under sufficient regularity conditions, we can then define policy gradient flow as a solution to the partial differential equation\n(15) $\\partial_t \\mu_t = \\text{grad}_W J^n(\\mu_t)$,\nwhere the functional gradient is given by\n$\\text{grad}_W J^n(\\mu) = -\\nabla_x \\cdot (\\mu \\nabla_\\mu J^n(\\mu))$ with $\\nabla_\\mu J^n(\\mu) = \\nabla_x \\frac{\\delta J^n}{\\delta \\mu}(\\mu)$.\nHere, $\\nabla \\cdot$ denotes the divergence operator and $\\frac{\\delta J^n}{\\delta \\mu}$ denotes the linear functional derivative of $J^n$. The functional $\\nabla_\\mu J^n(\\mu)$, which is defined as the gradient of the linear functional derivative, is called the Wasserstein gradient of $J^n$. Since we have not yet discussed the topological structure on $P_2(X)$, we can informally think that $J^n : P_2(X) \\rightarrow \\mathbb{R}$ is smooth if there exists a function\n$\\frac{\\delta J^n}{\\delta \\mu} : P_2(X) \\times X \\rightarrow \\mathbb{R}$\nsuch that for all measures $\\mu, \\mu' \\in P_2(X)$, we have that\n$J^n(\\mu') - J^n(\\mu) = \\int_X \\int_0^1 \\frac{\\delta J^n}{\\delta \\mu}(t'\\mu' + (1-t)\\mu)(x) d(\\mu' - \\mu)(x) dt$\n$^1$the subscript ac stands for absolute continuity."}, {"title": "2.4. Discussion on our contributions", "content": "To the best of our knowledge", "58": "from the entropic regularization of parameter measures to cover a general family of convex functionals. We summarize our main contributions as follows:\n(i) Solvability. As a first step, we establish in Theorem 3.3 the conditions under which the safety-constrained RL problem (1) admits a stationary Markov policy solution. Specifically, we consider the unconstrained reformulation (4), which combines the notion of safety certificates by [65", "86": "."}]}