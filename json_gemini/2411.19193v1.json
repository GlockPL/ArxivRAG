[{"title": "CONVEX REGULARIZATION AND CONVERGENCE OF POLICY GRADIENT FLOWS UNDER SAFETY CONSTRAINTS", "authors": ["PEKKA MALO", "LAURI VIITASAARI", "ANTTI SUOMINEN", "EEVA VILKKUMAA", "OLLI TAHVONEN"], "abstract": "This paper studies reinforcement learning (RL) in infinite-horizon dynamic decision processes with almost-sure safety constraints. Such safety-constrained decision processes are central to applications in autonomous systems, finance, and resource management, where policies must satisfy strict, state-dependent constraints. We consider a doubly-regularized RL framework that combines reward and parameter regularization to address these constraints within continuous state-action spaces. Specifically, we formulate the problem as a convex regularized objective with parametrized policies in the mean-field regime. Our approach leverages recent developments in mean-field theory and Wasserstein gradient flows to model policies as elements of an infinite-dimensional statistical manifold, with policy updates evolving via gradient flows on the space of parameter distributions. Our main contributions include establishing solvability conditions for safety-constrained problems, defining smooth and bounded approximations that facilitate gradient flows, and demonstrating exponential convergence towards global solutions under sufficient regularization. We provide general conditions on regularization functions, encompassing standard entropy regularization as a special case. The results also enable a particle method implementation for practical RL applications. The theoretical insights and convergence guarantees presented here offer a robust framework for safe RL in complex, high-dimensional decision-making problems.", "sections": [{"title": "1. INTRODUCTION", "content": "Stochastic dynamic decision processes with safety constraints play a pivotal role in a wide range of applications, including autonomous systems, resource management, and finance. In such problems, the agent's goal extends beyond merely maximizing cumulative discounted rewards; it also involves ensuring safety by adhering to state-dependent constraints over an infinite time horizon. When decision-making occurs within an environment lacking an analytical model, these problems are often addressed through various adaptations of customized reinforcement learning (RL) algorithms.\nThe mathematical formulations of safety-constrained RL problems differ significantly across applications and algorithmic frameworks [42, 65, 66, 91, 93]. To intuitively introduce the concept of safe RL, let S and A represent the state and action spaces, respectively, which may have infinite cardinality. Without loss of generality, we assume that the set of safe actions can be described by a set-valued mapping $(t,s) \\rightarrow D_t(s)$, where for each state $s_t \\in S$, the safe action set is given by $D_t(s_t) \\subset A$. A policy is deemed safe if the actions it prescribes, $a_t \\sim \\pi(\\cdot | h_t)$, conditioned on the history $h_t = (s_0, a_0, ..., s_t)$, including the current state, satisfy $a_t \\in D_t(s_t)$ either almost surely or with high probability. The general safe RL problem with state constraints can thus be expressed as:\n$\\displaystyle \\underset{\\pi \\in \\Pi}{\\text{maximize}}~E_{\\pi}\\left[\\sum_{t=0}^{\\infty} \\beta^t u(s_t, a_t) \\right]~\\text{subject to}~P_{\\pi}\\left( \\{(s_t, a_t)_{t=0}: a_t \\in D_t(s_t) ~\\forall t \\} \\right) \\geq 1 - \\xi,$"}, {"title": "CONVEX REGULARIZATION OF PARAMETRIZED POLICY GRADIENT", "content": "where $\\Pi$ denotes the space of stochastic policies, $P_{\\pi}$ is the probability measure induced by policy $\\pi$ on the space of state-action trajectories, and $\\xi$ is the permissible risk of safety violations. Policies with $\\xi \\in (0, 1)$ are said to be optimized under chance constraints [72, 75], while in many critical applications, safety must be guaranteed almost surely, necessitating $\\xi = 0$ [85].\nIn a discrete setting with finite state and action spaces, it may be feasible to specify the set of safe states precisely. However, when state and action spaces are large, the safety map $D_t$ needs to be approximated [91]. A common approach is to introduce a set of non-negative safety cost functions $g_k : S \\times A \\rightarrow \\mathbb{R}_+, k = 1, ..., K$, such that, for any trajectory $(s_t, a_t)_{t=0}^{\\infty}$, the probability of the cumulative safety costs exceeding their admissible budgets $b_k \\in \\mathbb{R}_+$ should remain below a given safety threshold $\\xi$:\n$\\displaystyle P_{\\pi} \\left( \\left\\{ (s_t, a_t)_{t=0}^{\\infty}: \\sum_{t=0}^{\\infty} \\beta_k^t g_k(s_t, a_t) - b_k \\geq 0 \\right\\} \\right) \\leq \\xi, \\forall k \\in \\{1, ..., K\\},$ \nwhere $\\beta_k \\in (0, 1)$ denote constraint specific discount factors.\nIn many applications and algorithms [1, 17, 34, 90], the probability constraint is replaced with a weaker requirement that the expected cumulative safety cost remains below a certain threshold, i.e. $\\mathbb{E}_{\\pi}[\\sum_{t=0}^{\\infty}\\beta_k g_k(s_t, a_t)] \\leq b_k$. This simplification is considered necessary for being able to use Lagrangian methods with a finite number of multipliers.\nWhile standard policy gradient methods [47, 57, 82, 88] can be applied to unconstrained Markov Decision Processes (MDPs), the addition of safety constraints complicates both the formulation and the solution, particularly in continuous state-action spaces. The constraints necessitate specialized methods to ensure that each policy meets safety requirements, ideally with guarantees on convergence and robustness. Recent years have seen a surge in interest in regularization methods for RL algorithms [2, 24, 43, 46, 47, 68], as they offer mechanisms to stabilize training and enhance convergence properties, especially in high-dimensional or continuous settings. The most commonly considered approach is based on entropic regulariza- tion of rewards, where $u(s_t, a_t)$ is replaced with a regularized reward $u(s_t, a_t) - \\epsilon \\log(\\pi(a_t|s_t))$. Standard entropy-based regularization of rewards has proven successful in discrete settings, where convergence of policy gradient methods is relatively well-understood [12, 23, 24, 61, 94]. However, for continuous state-action spaces, with more general than linear or log-linear policy representations, conventional approaches often fall short, as they do not scale to continuous decision spaces or parametrized policies with non-linear function approximators [3, 58, 59].\nInspired by the notion of double entropic regularization studied by [58], this paper in- troduces a generalized convex regularization framework for policy gradient flows in safety- constrained MDPs, combining both reward and parameter regularization. Specifically, we consider the following regularized problem with parametrized policies in the mean-field regime:\n$\\begin{aligned} &\\underset{\\mu \\in \\mathcal{P}(X)}{\\text{maximize}}~\\mathbb{E}_{\\pi_{\\mu}}\\left[\\sum_{t=0}^{\\infty} \\beta^t (u(s_t, a_t) - \\epsilon F(\\pi_{\\mu}(a_t|s_t))) + \\kappa H(\\mu) \\right] \\\\ &\\text{s.t.}~\\mathbb{E}_{\\pi_{\\mu}}\\left[ \\sum_{t=0}^{\\infty} \\beta_k^t g_k(s_t, a_t) \\right] \\leq b_k, ~\\forall k \\in \\{1, ..., K\\},\\end{aligned}$\nwhere $F$ is a reward-regularization function and $H$ is a convex functional on the space of parameter distributions $\\mathcal{P}(X)$. By applying recent developments in mean-field theory and"}, {"title": "Wasserstein Spaces; Gradient Flow; Reinforcement Learning; Continuous Spaces; Safety Constraints; Regularization.", "content": "Wasserstein gradient flows [8, 19], we model policies as elements on an infinite-dimensional statistical manifold $\\{\\pi_{\\mu} : \\mu \\in \\mathcal{P}(X) \\}$ and consider policy updates in terms of gradient flows $(\\rho_t)_{t\\geq 0}$ on the space of parameter measures $\\mathcal{P}(X)$. Similar policy space has been previously studied by [3, 58]. This approach leverages convex regularization to not only smooth the reward landscape but also directly influence the distribution of policy parameters, facilitating convergence even under stringent safety constraints. Regularization can act as a \"convexifier,\" transforming inherently non-convex problems into tractable forms where gradient-based methods can be effectively applied.\nOur main contribution is to establish the conditions for local and global convergence of policy gradient methods for safety-constrained decision processes. For this purpose we start by proving solvability of safety constraint augmented problem (without regularization). We then prove that one can approximate the problem by using smooth and bounded approxima- tions for various objects, including regularization functions $F$ and $H$. This allows to define gradient flows that then can be translated into particle method and corresponding RL algo- rithm to find the solution. In the process, we study the roles of both regularizations and, in particular, show that under sufficient amount of regularization, the problem becomes convex and one obtains convergence towards a global solution with exponential rate of convergence. It is worth to note that we consider general functions $F$ and $H$, from which some studied special cases including entropy regularization follows as a special case.\nThe remainder of the paper is organized as follows. In Section 2, we present more detailed discussion on our general setting and results, accompanied with a related literature review. Our technical presentation is postponed into Sections 3\u20138. In Section 3 we formulate the solvability result for the problem with safety constraints. After that in Section 4, we provide precise details on our double regularization framework. Smooth approximations are then considered in Section 5, and gradient flows and their convergence is detailed in Section 6. The associated particle method is discussed in Section 7, while all the technical proofs are postponed to Section 8. Some essential preliminaries are presented in the appendix."}, {"title": "2. OVERVIEW OF THE RESULTS AND RELATED LITERATURE", "content": "Let us now formulate our safety-constrained problem (without regularization),\n$\\begin{aligned} &\\underset{\\pi \\in \\Pi}{\\text{maximize}}~E_{\\pi}\\left[\\sum_{t=0}^{\\infty} \\beta^t u(s_t, a_t) | s_0 \\sim \\rho_0 \\right] \\\\ &\\text{s.t.}~~\\sum_{t=0}^{\\infty} \\beta_k g_k(s_t, a_t) \\leq b_k~~P_{\\pi}\\text{-a.s.}, \\forall k \\in \\{1, ..., K\\},\\end{aligned}$\nwith more details. Throughout we assume that the state space $S$ and action space $A$ are Polish spaces (i.e., separable, completely metrizable topological spaces), with possibly infinite cardinality, $p: S \\times A \\rightarrow \\mathcal{P}(S)$ is a weakly continuous transition probability kernel (see Assumption 1), with $\\mathcal{P}(S)$ denoting the set of probability measures on $S$, $\\rho_0 \\in \\mathcal{P}(S)$ is an initial state distribution, $u : S \\times A \\rightarrow [-\\infty, +\\infty]$ is a bounded, upper semicontinuous utility (reward) function with discount factor $\\beta$, and $g_k : S \\times A \\rightarrow [-\\infty, +\\infty]$ are single-period safety cost functions with corresponding discount factors $\\beta_k$ and budgets $b_k \\geq 0$. To ensure that the problem is well-posed, we further require the safety cost functions to be lower semicontinuous and satisfy an inf-compactness condition (see Assumption 2). Here, the expected discounted utility is maximized over a policy space $\\Pi$, where a policy can be interpreted as a sequence"}, {"title": "CONVEX REGULARIZATION OF PARAMETRIZED POLICY GRADIENT", "content": "$\\pi = (\\pi_t)$ of transition probabilities such that $\\pi_t(\\cdot|h_t)$ is a conditional probability measure on A for every history $h_t = (s_0, a_0, ..., s_t)$. By Ionescu-Tulcea theorem, the probability measure $P_\\pi$, which is induced by $\\pi$ on the product space of histories, is uniquely defined given the initial state distribution $p_0$ and the transition kernel $p$. We use $\\mathbb{E}_{\\pi}[\\cdot]$ to denote the expectation taken with respect to the measure $P_\\pi$. Throughout, we assume that the safety constraint is satisfied almost surely.\nWe note that (1) can be usually expressed in an unconstrained form\n$\\displaystyle \\underset{\\pi \\in \\Pi}{\\text{maximize}}~\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty} \\beta^t u(s_t, a_t) - \\delta_{\\geq 0}(z_t) ~|~ s_0 \\sim p_0 \\right],$ \nwhere $(z_t)_{t\\geq 0}$ is a sequence of state variables $z_t = (z_{t,k})_{k=1}^K$ representing the remaining safety budgets for each of the constraints $k = 1,...,K$. Given the initial budgets $z_{0,k} = b_k$, we define\n$z_{t+1,k} = \\mathcal{O}_{z_{t,k}}(s_t, a_t) := (z_{t,k} - g_k(s_t, a_t))/\\beta_k,$\nwhere $\\mathcal{O}_{z_{t,k}}$ is known as a safety index for constraint k parametrized by the remaining budget, $z_{t,k}$, before paying the cost for choosing an action $a_t$ in the current state $s_t$. The budget process monitors constraint violations and records the value of the remaining safety budget. The indicator function $\\delta_{\\geq 0}(z_t)$ is 0, if $z_{t,k} \\geq 0$ for all $k = 1, . . ., K$, and $+\\infty$ otherwise. The representation (2) is valid, when the instantaneous safety cost functions $g_k$ are assumed to be non-negative. Since the budget process is Markov, it can be considered to be a part of the environment state. The key benefit is that we can use additional state variables to represent the constraints instead of a Lagrangian with state-dependent multipliers [85].\nAlthough the formulation (2) appears promising due to its convenient way to handle the state-wise safety constraints, we can see that the sharpness of the indicator function is prob- lematic when constraint violations are encountered during the learning process. By consid- ering a smooth barrier function B, we obtain the following unconstrained reformulation\n$\\displaystyle \\underset{\\pi \\in \\Pi}{\\text{maximize}}~\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty} \\beta^t u_B(s_t, a_t) ~|~ s_0 \\sim p_0 \\right],$ \nwhere the single-period utility u is replaced with the safety-augmented utility function\n$u_B(s_t, a_t) = u(s_t, a_t) - B(z_t (s_t, a_t)).$\nHere, $\\mathcal{O}_{z_t} = (\\mathcal{O}_{z_{t,k}})_{k=1}^K$ is a vector of constraint-wise safety indices, and $B_{\\circ}\\mathcal{O}_{z_t}$ is a state-action safety barrier function such that for every history $(s_t, a_t)_{t>0}$, where $(s_t, a_t) \\in \\text{lev}_{>0} z_t$ and $\\lim_{t\\rightarrow \\infty} z_t (s_t, a_t) = 0$, we have $\\lim_{t\\rightarrow \\infty} B (\\mathcal{O}_{z_t} (s_t, a_t)) = +\\infty$.\nIn general, there may not always exist an optimal policy, let alone a Markov policy or a stationary policy. Most approximate dynamic programming and reinforcement learning methods are designed for problems where the agent collects data from the environment in short time steps. In natural resource management problems, the outcomes of policies are observed only after several years, making a fully data-driven approach with trial-and-error learning impractical, especially with safety constraints limiting admissible policies. Hence, before considering numerical solutions for infinite horizon problems, it is sensible to establish the conditions under which the safety constrained problem (1) admits an optimal solution"}, {"title": "CONVEX REGULARIZATION OF PARAMETRIZED POLICY GRADIENT", "content": "that can be represented by a stationary Markov policy. To ensure that the model is well- defined, we will make use of the regularity assumptions introduced by [81] to limit the class of admissible problem formulations.\nIn the case of finite $S$ and $A$, policies are typically modeled using either direct or tabular softmax parametrization. How- ever, when $S$ and $A$ are large or infinite, the tabular representation becomes computationally intractable. For continuous or mixed state-action spaces, we can model the policy space using a class of parametrized policies, where the policies are commonly of form\n$\\pi_x(da|s) \\propto \\exp (\\psi(s, a, x)) d\\rho(a),$\nwhere $\\psi : S \\times A \\times X \\rightarrow \\mathbb{R}$ is a nonlinear function (e.g., neural network), $X$ denotes the parameter space, and $\\rho$ is a finite reference measure. For a finite-dimensional $X \\subset \\mathbb{R}^d$, the parametrized policy $\\pi_x$ can then be interpreted as a point on a finite-dimensional statistical manifold (or a parameter submanifold), $\\Pi:= {\\pi_x : x \\in X } \\subset \\mathcal{P}(A|S)$, where $\\mathcal{P}(A|S)$ denotes the space of conditional probability measures. As a convex subspace of the space of bounded signed kernels, $\\mathcal{P}(A|S)$ is a Banach space when endowed with the total variation norm [60]; see Appendix B.\nExample 2.1 (Diagonal Gaussian Policy). For continuous actions, we can take the diago- nal Gaussian policies as an example. A diagonal Gaussian distribution is a special case of multivariate Gaussians where the covariance matrix only has entries on the diagonal, which can be represented as a vector. Let $m : S \\times X \\rightarrow \\mathbb{R}^{\\text{dim}(A)}$ and $\\sigma: S \\times X \\rightarrow \\mathbb{R}^{\\text{dim}(A)}$ denote the mean actions and their standard deviations. We can then define the function approximator for a Gaussian policy as\n$\\psi(s, a, x) = - \\frac{1}{2} ((a - m(s,x))^T \\Sigma(s,x)^{-1} (a - m(s,x)))$\nwhere $\\Sigma(s, x) = \\sigma(s,x)\\sigma(s,x)^T$ is a diagonal matrix. The policy is admissible as long as m and $\\sigma$ are sufficiently smooth.\nTo allow for more general policies, with infinite-dimensional parametrization, we will next consider another formulation, where policies are parametrized by measures. We use $\\mathcal{P}_p(X)$ to denote the space of Borel probability measures on $X$ with a finite p-moments, which becomes a metric space when endowed with the $L^p$-Wasserstein distance. For $\\mu,\\nu \\in \\mathcal{P}_p(X)$, their $L^p$-Wasserstein distance is defined by\n$\\displaystyle W_p(\\mu, \\nu) = \\left\\{ \\inf_{\\gamma \\in \\Gamma(\\mu,\\nu)} \\left( \\int_{X^2} |x - x'|^p d\\gamma(x, x') \\right)^{1/p} : \\gamma \\in \\Gamma(\\mu, \\nu) \\right\\}.$\nwhere the infimum is taken over the a set of transfer plans or couplings between \u03bc and \u03bc'. In this paper, we mainly consider the case of p = 2. Letting $pr^1, pr^2 : X \\times X \\rightarrow X$ be the projections of $X \\times X$ onto the first and second copy of $X$, respectively, we define the set of couplings as probability measures on $X \\times X$ with marginals $\\mu$ and $\\nu$; that is,\n$\\Gamma(\\mu, \\nu) := \\{\\gamma \\in \\mathcal{P}(X \\times X) ~|~ pr^1_{\\#} \\gamma = \\mu, ~pr^2_{\\#} \\gamma = \\nu\\}$\nwhere the push-forwards of \u03b3 along projections are defined as $pr^1_{\\#} \\gamma(A) := \\gamma(A \\times X) = \\mu(A)$ and $pr^2_{\\#} \\gamma(B) := \\gamma(X \\times B) = \\nu(B)$ for any Borel sets $A, B \\subset X$. The set of optimal transfer plans, $\\Gamma_o(\\mu, \\nu) \\subset \\Gamma(\\mu, \\nu)$ is the convex and narrowly compact set of plans that attain the minimum Wasserstein distance between the given marginals $\\nu$ and $\\mu$."}, {"title": "CONVEX REGULARIZATION OF PARAMETRIZED POLICY GRADIENT", "content": "Let us now consider the Hilbertian case $\\mathcal{P}_2(X)$ with $X = \\mathbb{R}^d$. By modeling the distributions of policy parameters as elements of $\\mathcal{P}_2(X)$, we can use the recent developments in mean-field theory to extend our results to policies parametrized by measures [3, 58]. In the mean-field approach, the policy space is understood as an infinite-dimensional statististical manifold $\\Pi_2 := {\\pi_{\\mu} : \\mu \\in \\mathcal{P}_2(X) }$, where the policies are themselves functionals on the space of parameter measures. For a given $\\mu$, the parametrized policy is then given by\n$\\pi_{\\mu}(da|s) := \\pi(\\mu)(da|s) \\propto \\exp \\left( \\int_X \\psi(s, a, x) d\\mu(x) \\right) d\\rho(a),$\nwhere the approximation function $\\psi : S \\times A \\times X \\rightarrow \\mathbb{R}$ is a bounded and jointly measurable function on the product space $S \\times A \\times X$ equipped with product measure $\\mathcal{L}^n_S \\otimes \\rho \\otimes \\mathcal{L}^d$. Here $\\mathcal{L}^n_S$ and $\\mathcal{L}^d$ denote Lebesgue measures on $S$ and $X$, respectively, and $\\rho$ is a fixed reference measure on $A$. In this paper, we assume that the approximator $\\psi$ is at least once differentiable with respect to $x$, with bounded derivatives for all $s \\in S$, $\\rho$-a.e. $a \\in A$.\nRemark 2.2 (Deep ensemble models as mean-field approximations). The mean-field frame- work can be motivated by establishing a connection to training of deep horizontal ensemble models. Let $(x_i)_{i=1}^K \\in X^K$ be a parameter vector separated into K components $x_i \\in X$. When the model $\\psi$ is allowed to be of arbitrary complexity, we can see the mean-field policy $\\pi_{\\mu}$ as an infinite-width limit of policies $\\pi_{\\mu_K}$ defined by an ensemble of K neural networks ${\\psi(\\cdot; x_i)}_{i=1}^K$:\n$\\pi_{\\mu_K}(da|s) \\propto \\exp \\left( \\frac{1}{K} \\sum_{i=1}^K \\psi(s, a; x_i) \\right) d\\rho(a),$\nwhere $\\mu_K = \\frac{1}{K}\\sum_{i=1}^K \\delta_{x_i}$ denotes an empirical measure defined by the parameters of the K ensemble models.\nBy considering discrete measures $\\mu = \\delta_x \\in \\mathcal{P}_2(X)$, where $\\delta_x$ denotes the Dirac's measure at $x$, we can always recover the policies $\\Pi_{\\delta}$ as a subset of $\\Pi_2$ by setting $\\pi(\\mu) = \\pi(\\delta_x) = \\pi_x$. When the parameter space $X$ has a dense countable subset $D$, Hahn-Banach theorem implies that the subset of all the convex combinations with rational coefficients of $\\delta$-measures concentrated in $D$ is narrowly dense in $\\mathcal{P}_2(X)$, i.e. we have that $\\mathcal{P}_2(X) = \\overline{Conv(C)}$, where $\\overline{Conv(C)}$ is the closed convex hull of $C = {\\delta_x : x \\in D }$. Then, we can also note that the empirical measures $\\mu_K \\rightarrow \\mu \\in \\mathcal{P}_2(X)$ converge to a limiting measure as $K \\rightarrow \\infty$. where the ensemble policies converge to a mean-field policy on $\\Pi_2$ as $\\mu_K = \\frac{1}{K}\\sum_{i=1}^K \\delta_{x_i} \\rightharpoonup \\mu$ on $\\mathcal{P}_2(X)$ when $K \\rightarrow \\infty$.\nThe most commonly applied form of regularization for MDPs is to include entropy as part of the reward (or cost) function (see, e.g., [24, 43, 47]). However, we can also consider a more direct form of regularization of parameter distributions as demonstrated by [58] in the case of mean-field policies. In this paper, we extend the pioneering work by [58] to safety-constrained problems by consider- ing the following generalized framework for convex regularization of rewards and parameter distributions:\n$\\displaystyle \\underset{\\mu \\in \\mathcal{P}_2(X)}{\\text{minimize}}~J(\\mu) = -V_{\\epsilon}(\\pi_{\\mu}) + \\kappa H(\\mu),~~~~H(\\mu) = \\sum_{j=1}^J k_j H_j(\\mu|\\gamma_j),~k_j \\geq 0 ~\\forall j,$\nwhere $V_{\\epsilon}$ is the function with regularized rewards, and H is a $\\lambda$-convex regularization func- tional acting on parameter distributions with strength parameter $\\kappa$."}, {"title": "CONVEX REGULARIZATION OF PARAMETRIZED POLICY GRADIENT", "content": "To ensure that H is a convex and lower semicontinuous functional on $\\mathcal{P}_2(X)$, we assume that Assumptions 6 and 7 hold. Then, the proof of lower semicontinuity and convexity of $H(\\gamma)$ stated in Lemma 4.6 is readily obtained by combining [8, Lemma 9.4.3 and Theorem 9.4.12].\nLet $H : [0, +\\infty) \\rightarrow [0, +\\infty]$ be a proper, convex and lower semicontinuous function with superlinear growth at infinity such that\nthe map $z \\mapsto H(e^{-z})e^z$ is convex and non-increasing in $(0, +\\infty)$.\nFurthermore, we assume that $H \\in C^2(0,+\\infty)$ has a second derivative $H''$ that is at most of linear growth.\nWe say that a reference measure $\\gamma$ is defined by a normalized potential, if there exists a convex and a differentiable function $U : X \\rightarrow (-\\infty,+\\infty]$ such that $\\gamma = e^{-U} \\cdot \\mathcal{L}^d$ is a Borel probability measure, and $\\nabla U$ is Lipschitz- continuous and at most of linear growth in $x$.\nSuppose $H : [0,+\\infty) \\rightarrow [0,+\\infty]$ satisfies Assumption 6. If $\\gamma \\in \\mathcal{P}_2(X)$ is a probability measure defined by a normalized potential such that Assumption 7 holds, then H given by\n$\\displaystyle H(\\mu) := H(\\mu|\\gamma) = \\begin{cases} \\int_X H(\\frac{d\\mu}{d\\gamma}) d\\gamma &\\text{if}~\\mu \\ll \\gamma \\\\ +\\infty &\\text{otherwise, }\\end{cases}$\nis lower semicontinuous and convex in $\\mathcal{P}_2(X)$.\nTo define the gradient flow in the Wasserstein metric for the safety constrained RL prob- lem, we can translate the original maximization problem into a corresponding minimization problem, where the solution can be approximated using a sequence of smooth and bounded functionals. Let $\\text{argmin} \\{J\\}$ denote the set of (exact) minimizers of $J$, and let $\\epsilon-\\text{argmin} \\{J^n\\}$ denote the set of $\\epsilon$-optimal (approximate) minimizers of $J^n$:\n$\\text{argmin} \\{J\\} := \\{ \\mu^* \\in \\mathcal{P}_2(X) ~|~ J(\\pi_{\\mu^*}) = \\inf_{\\mu \\in \\mathcal{P}_2(X)} J(\\pi_{\\mu}) \\};$\n$\\epsilon-\\text{argmin} \\{J^n\\} := \\{ \\mu \\in \\mathcal{P}_2(X) ~|~ J^n(\\pi_{\\mu}) \\leq \\inf_{\\mu \\in \\mathcal{P}_2(X)} J^n(\\mu) + \\epsilon \\};$\nSpecifically, we need to construct a sequence of bounded, smooth approximations ${J^n}_{n=1}^\\infty$ such that\n$\\displaystyle \\lim_{n\\rightarrow +\\infty} \\underset{\\mu \\in \\mathcal{P}_2(X)}{\\text{inf}}~J^n(\\mu) = \\underset{\\mu \\in \\mathcal{P}_2(X)}{\\text{inf}}~J(\\mu),$\nand, for any choice of $\\epsilon_n \\searrow 0$ and regularization parameters $\\epsilon > 0$ and $\\kappa > 0$, the cluster points for a sequence $(\\mu_n)_{n\\in \\mathbb{N}}$, where $\\mu_n \\in \\epsilon_n-\\text{argmin}\\{J^n\\}$, belong to $\\text{argmin}\\{J\\}$.\nWe will now present epiconvergent approximations $V^n$ and $H^n$ for both value function (see Section 5.2) and the regularizer (see Section 5.3), respectively, where each approximator is a smooth and bounded functional. Then, having the components defined, we can show in Section 5.4 that the smooth approximation $J^n = V^n + \\kappa H^n$ satisfies the above requirements."}, {"title": "5. WASSERSTEIN DIFFERENTIABLE APPROXIMATION", "content": "In the sequel", "phi": "mathcal{P"}, 2, "X) \\rightarrow (-\\infty, +\\infty"], "mu}": "mathcal{P"}, {"holds": "n$\\phi(\\mu') - \\phi(\\mu) = \\int_X \\left( \\int_0^1 \\frac{\\delta \\phi"}, {"phi": "mathcal{P"}, {"hold": "n(i) For any $\\mu \\in \\mathcal{P"}, {"varphi": "varphi \\in C^1(X)"}, {"by\n$\\text{dom}(\\phi)": {}}]