{"title": "Out-of-Distribution Recovery with Object-Centric Keypoint Inverse Policy For Visuomotor Imitation Learning", "authors": ["George Jiayuan Gao", "Tianyu Li", "Nadia Figueroa"], "abstract": "We propose an object-centric recovery policy framework to address the challenges of out-of-distribution (OOD) scenarios in visuomotor policy learning. Previous behavior cloning (BC) methods rely heavily on a large amount of labeled data coverage, failing in unfamiliar spatial states. Without relying on extra data collection, our approach learns a recovery policy constructed by an inverse pol- icy inferred from object keypoint manifold gradient in the original training data. The recovery policy serves as a simple add-on to any base visuomotor BC pol- icy, agnostic to a specific method, guiding the system back towards the training distribution to ensure task success even in OOD situations. We demonstrate the effectiveness of our object-centric framework in both simulation and real robot experiments, achieving an improvement of 77.7% over the base policy in OOD.", "sections": [{"title": "1 Introduction", "content": "Robot learning has achieved significant success in deploying Imitation Learning (IL) methods on real-world robotic systems [1]. One widely studied approach within IL is Behavior Cloning (BC), which has been explored extensively in recent work [2, 3, 4, 5, 6, 7]. BC methods enable learning control policies directly from demonstrations without the need for explicit environmental modeling, making the process relatively straightforward. However, despite producing promising results, BC is well-known for its susceptibility to the covariate shift problem [1]. This issue arises because tradi- tional BC approaches depend heavily on large quantities of labeled data, which are often obtained through labor-intensive methods such as teleoperation or kinesthetic teaching. Consequently, BC may struggle to perform reliably in out-of-distribution (OOD) scenarios, where data is sparse or noisy-reflecting a broader challenge faced in supervised learning. Addressing this issue typically requires either returning to laborious data collection or utilizing corrective mechanisms, such as guidance from human operators or reinforcement learning (RL) agents [8, 9, 10, 11], both of which impose additional deployment efforts on robotic systems.\nTo enjoy the benefits of strong performing BC policies in distribution (ID) settings while not re- quiring the human effort of collecting more data or the compute effort of running an RL step when OOD, in this work, we propose a recovery policy framework that brings the system back to the training distribution to ensure task success even when OOD. In particular, we focus on the key challenges of visuomotor policy learning by integrating a recovery policy constructed from the gra- dient of the training data manifold with a base visuomotor BC policy (e.g., a diffusion policy [2]). Inspired by the \"Back to the Manifold\" approach [12] the recovery policy guides the system back to- wards the training manifold, at which point the base policy resumes control. However, unlike [12], which focuses on recovering from OOD scenarios related to the robot's state, our approach takes an object-centric perspective, specifically addressing OOD situations for task-relevant object states. We believe this object-centric approach significantly enhances the OOD recovery capabilities of vi- suomotor policies, leading to more robust learning for object manipulation tasks. Furthermore, our recovery framework is designed to be agnostic to the choice of base policy, allowing it to be seam- lessly integrated with various BC implementations. This flexibility makes our method adaptable for future developments in imitation learning (IL). In this paper, we make the assumption that we have access to relevant object models. Also, we mainly consider the OOD cases in which the relevant object enters unfamiliar spatial regions.\nThe rest of the paper is organized as follows: Section 2 presents an overview of the existing works. Section 3 describes the problem formulation. Section 4 presents the object-centric recovery policy framework in detail, including its construction of the training data manifold and the keypoint in- verse policy. In section 5, we demonstrate the effectiveness of our approach on several benchmarks, including both simulation and real robot experiments, showing that our recovery policy improves the performance when entering unfamiliar states. We also show that our method has the desired property for lifelong learning of visuomotor policies, improving the performance of OOD while not diminishing the in-distribution performance. Section 6 discusses the limitations and future direc- tions."}, {"title": "2 Related Work", "content": "When deploying to the real world, vision-based IL could easily be initialized or moved to OOD situations, possibly due to bias in data collection and compounding errors. Deploying BC methods OOD could lead to unknown behavior in the low-data region. To address this, a well-known family of approaches is Data Aggregation, which gathers extra data from expert policies (usually provided by humans) through online interaction [13, 8, 10, 11]. However, performing such an online data collection procedure is an additional burden to the human when building a system. Our method tries to avoid additional online interaction and cumbersome data collection by squeezing as much information from the existing training data. The OOD problem has received much attention from the offline RL community. Offline RL suffers less compounding error than BC methods as it optimizes for long-term outcomes [14]. However, it could still struggle with distribution shifts like extrapola- tion errors due to limited data. To tackle the OOD problem, methods like [15, 16, 17] try to penalize actions that are far from the data. Moreover, several works [18, 19] also propose to recover back to the training data region, which indirectly shares a similar idea as our work. The paradigm of BC+RL has also been a popular choice for addressing the OOD problem [20, 9, 21]. Our approach takes on a similar direction for training a recovery policy, but instead, we use object-centric BC as the add-on for the base BC policy. Closely related to our work is [12], which introduces a vision-based OOD re- covery policy by 1) learning an equivariant map that encodes visual observations into a latent space whose gradient corresponds to robot end-effector positions, and 2) following a gradient learned by a Mixture Density Network (MDN) [22]. Rather than recovering the robot action, our work focuses on recovering task-relevant objects and inducing the robot action. When dealing with objects, it could"}, {"title": "3 Recovery Problem Formulation", "content": "Distribution shift for learning models is commonly quantified by measuring the Kullback-Leibler (KL) divergence between the distribution of observations during training and the distribution of observations encountered at test time [32, 33, 34]. This divergence reflects how much the test-time observations deviate from those seen during training, providing a metric to measure if a scenario we encountered is out-of-distribution. Formally, if the probability distribution of the training and testing observations is $P(O)$ and $Q(O)$ respectively, with $O$ representing the set of observations, we say that the testing observation will be considered out-of-distribution (OOD) if,\n$D_{KL}(P(O)||Q(O)) > \\epsilon$\nis asserted to be true with some threshold $\\epsilon > 0$.\nWe formulate our visuomotor policy interaction with the environment as a Partially Observable Markov Decision Model (POMDP) [35]. We describe this POMDP by the tuple $(S, A, O, T, E)$, where $s\\in S$ is the set of environmental states, which are directly observable, $a \\in A$ is the set of robot actions, and $o\\in O$ is the set of visual observations. The transition function $T: S\\times A \\rightarrow S$ dictates how the unobservable state changes when robot actions are performed, and the emission function $E: S \\rightarrow O$ is a surjective function that determines the visual observations given states.\nGiven this formulation, we can reformulate the KL Divergence OOD metric as follows,\n$D_{KL}(P(E(S))||Q(E(S))) > \\epsilon$.\nHence, fundamentally, given an observation-level out-of-distribution scenario, if the environmental state variables are recovered back into the training distribution, the observations will also be re- covered back into distribution. However, the recovery of all state variables is difficult to tackle all at once under the imitation learning framework, which typically has access to only task-relevant demonstrations. Therefore, for this work, we specifically focus on the recovery of task-relevant ob- jects in manipulation tasks. Unlike previous data aggregation or reinforcement learning approaches, we aim for our recovery framework to exclusively leverage the training demonstrations of the base policy and not require any additional policy-related data collection."}, {"title": "4 Method", "content": "We present our approach in augmenting a base policy trained via Behavior Cloning (BC) by in- corporating an object-centric recovery strategy, which enables task-relevant objects to return to its Euclidean training manifold where the base BC policy functions at its best. For our work, we will assume task-relevant objects in the scene are rigid and non-deformable.\nTo achieve this, we introduce the Object-Centric Recovery (OCR) framework, as illustrated in Fig- ure 2. We first explicitly model the distribution of objects keypoints in the training dataset with a Gaussian Mixture Model (GMM) [36] $p(p_k^{(i)}) = \\sum_{m=1}^M \\lambda_{k,m} \\mathcal{N}_{\\theta_k}(p_k^{(i)}| \\mu_{k,m}, \\Sigma_{k,m})$ where $p_k^{(i)}$ are keypoints in the dataset and $\\theta_k = {(\\lambda_{k,m}, \\mu_{k,m}, \\Sigma_{k,m})}_{m=1}^M$ parameters of the GMM (Sec- tion 4.2.1, 4.2.2). At test time, we evaluate the gradient $\\nabla p(p_{test}|\\theta_k)$ to obtain the object-recovery vectors, which we use to plan for an object-recovery trajectory $\\mathcal{L}_{rec}$ (Section 4.2.4). We then trans- late this trajectory into robot actions via a Keypoint Inverse Policy $\\pi_{inv}$ that is trained using the base dataset (Section 4.2.3). Lastly, Section 4.2.5 describes how the base policy interacts with the recovery policy to become the OCR joint policy."}, {"title": "4.1 Base BC Policy", "content": "Our formulation considers a generic visuomotor policy that outputs future actions based on past vi- sual observations as the base BC policy. We consider such a liberal formulation to demonstrate that our framework can work alongside any variations of BC policy. Formally, we define a typical visuo- motor policy training dataset as $\\mathcal{D}_b = \\{d^{(i)}\\}_{i=1}^N$, where each episode $d^{(i)} = \\{(o_t^{(i)}, a_t^{(i)}, p_t^{(i)})\\}_{t=1}^{T^{(i)}}$ consists of the observations $o_t^{(i)}$, robot actions $a_t^{(i)}$, and robot proprioception $p_t^{(i)}$ at time step $t$. Then, under the imitation learning framework, a base visuomotor policy $\\pi_\\theta$ that is parameterized by $\\theta_b$ is learning by optimizing the following behavior cloning objective:\n$\\pi_{\\theta_b}^* = \\arg \\min_{\\theta_b} \\mathbb{E}_{(o,a,p) \\sim \\mathcal{D}_b} [\\mathcal{L}(\\pi_{\\theta_b}(o, p), a)]$\nWhere the loss function $\\mathcal{L}$ is typically Cross-Entropy Loss or Mean-Squared Error."}, {"title": "4.2 Object-Centric Recovery Policy", "content": "We choose to use artificial object keypoints to represent object poses for studying object-centric recovery, as keypoints allow us to tightly couple the position and orientation of the object, facilitating a more accurate estimation of its distribution during training.\nWe consider the same visuomotor policy training dataset formulation $\\mathcal{D}_b = \\{d^{(i)}\\}_{i=1}^N$, where each episode $d^{(i)} = \\{(o_t^{(i)}, a_t^{(i)}, p_t^{(i)})\\}_{t=1}^{T^{(i)}}$ consists of the observations $o_t^{(i)}$, robot actions $a_t^{(i)}$, and robot proprioception $p_t^{(i)}$ at time step $t$. To extract object poses from these visuomotor datasets, we em- ploy off-the-shelf object pose estimators [37, 38, 27] to transform each observation frame $o_t^{(i)}$ into the object pose $T_{obj,t}^{(i)}$. Next, we define an arbitrary set of keypoints $\\mathcal{P} = \\{p_k\\}_{k=1}^n$, where each $p_k \\in \\mathbb{R}^d$. For each keypoint $k$ at time step $t$ in demonstration $i$, we compute the transformed keypoints $p_{kt}^{(i)} = h^{-1}(T_{obj,t}^{(i)} h(p_k))$, where $h$ represents the function that converts points into ho- mogeneous coordinates. The transformed keypoint $p_{kt}^{(i)} = h^{-1}(T_{obj,t}^{(i)} h(p_k)) \\{p_{kt}^{(i)}\\}_{k=1}^n$ then serves as the keypoint representation of the object's current pose. Thus, using $\\mathcal{D}_b$, we create a new dataset that will be used for recovery $\\mathcal{D}_{rec} = \\{d^{(i)}\\}_{i=1}^N$, where each episode $d_{rec}^{(i)} = \\{(e_t^{(i)}, T_{obj,t}^{(i)}, a_t^{(i)}, p_t^{(i)})\\}_{t=1}^{T^{(i)}}$ consists of the keypoints, object poses, robot actions, and robot proprioception at each time step."}, {"title": "4.2.2 Object Manifold Estimation", "content": "To estimate the manifold of the object distribution in the training dataset, we fit a Gaussian Mixture Model (GMM) [36] on each keypoint using its positions across every time step in every demon- stration. Specifically, given dataset $\\mathcal{D}_{kp,k} = \\{\\{(p_{k,t}^{(i)})\\}_{t=1}^{T^{(i)}}\\}_{i=1}^N$ consisting of one object keypoint $k$ across all time step $t$ in every demonstration $i$, we model the probability of each $p_{kt}^{(i)}$ as a weighted sum of $M$ Gaussian distributions:\n$p(p_{k,t}^{(i)}|\\theta_k) = \\sum_{m=1}^M \\lambda_{k,m} \\mathcal{N}_{\\theta_k}(p_{kt}^{(i)}| \\mu_{k,m}, \\Sigma_{k,m}),$\nwhere $\\lambda_{k,m}$ is the mixing coefficient of keypoint $k$ for the $m$-th Gaussian, $\\mathcal{N}(p_{kt}^{(i)}| \\mu_{k,m}, \\Sigma_{k,m})$ is the Gaussian probability density function of keypoint $k$ for the $m$-th component with mean $\\mu_{k,m}$ and covariance $\\Sigma_{k,m}$, and $\\theta_k = {(\\lambda_{k,m}, \\mu_{k,m}, \\Sigma_{k,m})}_{m=1}^M$ are the parameters of the model that estimates the distribution of keypoint $k$. To fit this GMM, we used Expectation-Maximization [36] to maximize the likelihood estimation of the model on the data. Computing a GMM for all $n$ keypoints would result in parameters $\\Theta = {\\theta_k\\}_{k=1}^n$ that collectively estimate the probability distribution of the keypoints of the object."}, {"title": "4.2.3 Keypoint Inverse Policy", "content": "To facilitate object manipulation for recovery, we propose the use of an Keypoint Inverse Pol- icy $\\pi_{inv}$, which is designed to translate a sequence of object-keypoint trajectory along with the robot's current state, into the corresponding robot actions necessary to execute those object mo- tions effectively. Formally, if we define $\\mathcal{K}$ to be the set of object keypoints observed, and $\\mathcal{P} \\subseteq S$ to be the set of robot proprioception states, $\\pi_{inv} : \\mathcal{K}^L \\times \\mathcal{P} \\rightarrow A^L$, where $L$ is the observa- tion length. We already have access to the dataset of object keypoints, pose, and action tuples $\\mathcal{D}_{rec} = \\{\\{(e_t^{(i)}, T_{obj,t}^{(i)}, a_t^{(i)}, p_t^{(i)})\\}_{t=1}^{T^{(i)}}\\}_{i=1}^N$ that we obtained in Section 4.2.1, which we could uti- lize to directly train $\\pi_{inv}$ with the learning objective:\n$\\pi_{inv}^* = \\arg \\min_{\\theta_{inv}} \\mathbb{E}_{(\\{e_t^{(i)}\\}, \\{a_t\\}, p_t) \\sim \\mathcal{D}_{rec}} [\\mathcal{L} (\\pi_{inv} (\\{e_{t+j}^{(i)}\\}_{t=j}^{t+L}, p^{(i)}), \\{a_{t+j}^{(i)}\\}_{t=j}^{t+L})]$ \nwhere sequences of length $L$ are pulled from the keypoint and action datasets to form $\\{e_{t+j}^{(i)}\\}_{t=j}^{t+L}$ and $\\{a_{t+j}^{(i)}\\}_{t=j}^{t+L}$, and the initial proprioception of the sequence $p_t^{(i)}$. However, by training this way directly, we will still run into the same issue of distribution shift, having no keypoints-to-action coverage on the OOD regions to generate properly useful manipulation outputs. To alleviate this, we propose the use of the initial object pose $T_{obj,t}^{(i)}$ to \"zero-out\" the data sequence. Specifically, instead of using the original sequence, we use the initial object pose of each sequence $T_{obj,t}^{(i)}$ to modify the sequence into $\\{((T_{obj,t}^{(i)})^{-1} e_{t+j}^{(i)})\\}_{t=j}^{t+L}$, $a_t^{(i)}$, and $((T_{obj,t}^{(i)})^{-1} p_t^{(i)})$. For simplicity, we will name these quantities $e^{zero}, a^{zero}, p^{zero}$ respectively. Hence, the learning objective becomes:\n$\\pi_{inv}^* = \\arg \\min_{\\theta_{inv}} \\mathbb{E}_{ (e^{zero}, a^{zero}, p^{zero}) \\sim \\mathcal{D}_{rec} } [\\mathcal{L} (\\pi_{inv} (e^{zero}, p^{zero}), a^{zero})]$ \nIn other words, the keypoint inverse policy only needs to learn to output robot actions from object keypoint trajectories that initialize from the identity frame. At test time, to carry out an object motion, we input the desired keypoint trajectory in the current object frame, and the policy outputs the corresponding robot action in that same frame. To execute the robot action, we then transform the action from the object frame to the robot frame. This way, regardless of the object and robot end-effector's true pose in Euclidean space, OOD or ID, as long as we have access to the current object pose, we can output robot actions that are useful for manipulation. In addition, we can think of this as a way of \"compressing\" the input domain of the keypoint inverse policy based on the information available to us (object pose), making the learning problem extremely data efficient."}, {"title": "4.2.4 Desired Object-Recovery Motion", "content": "Object-Recovery Vectors. At test time, we obtain the explicit current pose of the recovery object via pose estimation and generate keypoints using the same methodology as described in Section"}, {"title": "4.2.5 Final Joint Policy", "content": "We join our base policy and recovery action as a Joint Policy through a density activation switch. Specifically, after computing the mean keypoint density $\\Gamma_{rec}$, we use a tunable hyperparameter $\\epsilon_{rec}$ to define the threshold for distinguishing between out-of-distribution and in-distribution scenarios. If the scenario is classified as OOD, the recovery pipeline is activated; otherwise, if classified as ID, the base policy will proceed with the standard BC process. Formally, our joint policy is:\n$\\pi_{joint} = \\mathbb{I} \\{\\Gamma_{rec} \\geq \\epsilon_{rec}\\} \\pi_b(O_t, p_t) + \\mathbb{I} \\{\\Gamma_{rec} < \\epsilon_{rec}\\} \\pi_{inv} (\\mathcal{S}_{rec}, p_t)$\nWe also describe our Joint Policy algorithmically in Algorithm 1 in Appendix A."}, {"title": "5 Evaluation", "content": "We first evaluated the Object-Centric Recovery framework's performance using two simulation tasks, covering both prehensile and non-prehensile manipulation. These tasks were selected to demonstrate the framework's versatility and robustness in handling a range of manipulation sce- narios. Additionally, we evaluated a prehensile task in a real-world robot setting to showcase the framework's effectiveness in practical applications. Given the absence of existing methods for object-centric recovery in visuomotor policies, to the best of our knowledge, we benchmarked our results against the out-of-distribution performance of the base BC policy. To ensure consistency across all tasks, we employed the vision-input U-Net-based diffusion policy [2] as our base policy,"}, {"title": "5.2 Experimental Results & Analysis", "content": "1) Push-T. Shown in Table 1, the Push-T base policy generalized to the OOD region object initial- ization in only 10% of cases, mainly due to random initialization placing the end-effector on the right, allowing it to accidentally push the block back into the in-distribution region. In contrast, with the OCR framework, recovery actions guide the system to a feasible interaction region (e.g., positioning on the right to push the block left) and gently return the block to the ID region, where the base policy completes the task. Across 30 random OOD initializations, the OCR framework achieved a 93% task success rate, significantly outperforming the base policy.\n2) Square. In OOD object initialization scenarios, the Square base policy consistently attempted to move the end-effector toward the direction of the object but failed to reach it in all cases. In contrast, as shown in Table 1, the OCR framework successfully manipulated the square object for recovery, achieving an 80% task success rate in OOD scenarios, a substantial improvement over the base policy.\n3) Bottle. In OOD object initialization scenarios, the Bottle base policy failed similarly to the Square base policy, as both tasks involve prehensile manipulation. However, as shown in Table 2, the OCR framework effectively handled the recovery for both tasks, achieving an 70% task success rate in OOD scenarios, significantly outperforming the base policy. Interestingly, we observed that the OCR joint policy's OOD success rate is significantly higher than the base policy's ID success rate for the bottle task. We hypothesize that this can be attributed to the OCR framework's ability to move objects toward regions of high training density regardless of where the objects are initialized.\n4) Continuous Learning on Push-T. By resuming training on the base policy using both the original dataset $D_b$ and the augmented dataset $D_{aug}$ - autonomously collected via the OCR framework - we enabled the base policy to recover independently, achieving 80% task completion in the original OOD regions, as shown in Table 3,. This improvement did not come at the cost of performance in the original ID scenarios; in fact, the augmented policy showed enhanced performance in ID as well, improving from 90% to 97%. We hypothesize this is due to the OCR's augmented dataset consistently providing robot actions that move the object toward regions of higher density, even on the ID side. In other words, OCR demonstrations likely offer actions that converge the object to the base policy, complementing it rather than replacing it. We believe this showcases the OCR framework's ability to provide valuable data for continuous learning."}, {"title": "6 Conclusion", "content": "In this work, we proposed the Object-Centric Recovery policy framework designed to address out- of-distribution challenges in visuomotor policy learning, by recovering task-relevant objects into distribution without requiring additional data collection. When our framework was tested against various manipulation tasks, it demonstrated considerable improvement in performance in OOD re- gions. Furthermore, the framework's capacity for continuous learning highlights its potential to autonomously enhance policy behavior over time. However, a key limitation of our approach is the reliance on explicit object poses, which restricts its applicability to articulated and deformable objects. We aim to extend our framework by incorporating more flexible scene representations to recover from a broader range of OOD scenarios. Despite this limitation, we believe our framework represents a step towards improving the robustness of visuomotor policies in real-world settings."}]}