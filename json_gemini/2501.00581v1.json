{"title": "Causal Graph Guided Steering of LLM Values via Prompts and Sparse Autoencoders", "authors": ["Yipeng Kang", "Junqi Wang", "Yexin Li", "Fangwei Zhong", "Xue Feng", "Mengmeng Wang", "Wenming Tu", "Quansen Wang", "Hengli Li", "Zilong Zheng"], "abstract": "As large language models (LLMs) become increasingly integrated into critical applications, aligning their behavior with human values presents significant challenges. Current methods, such as Reinforcement Learning from Human Feedback (RLHF), often focus on a limited set of values and can be resource-intensive. Furthermore, the correlation between values has been largely overlooked and remains under-utilized. Our framework addresses this limitation by mining a causal graph that elucidates the implicit relationships among various values within the LLMs. Leveraging the causal graph, we implement two lightweight mechanisms for value steering: prompt template steering and Sparse Autoencoder feature steering, and analyze the effects of altering one value dimension on others. Extensive experiments conducted on Gemma-2B-IT and Llama3-8B-IT demonstrate the effectiveness and controllability of our steering methods.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement and widespread deployment of large language models (LLMs) have revolutionized a range of fields, from natural language processing to decision-making systems. These models, powered by vast amounts of data and sophisticated algorithms, have demonstrated remarkable abilities in various domains. However, as LLMs are increasingly deployed in critical applications, ensuring their alignment with human values and societal norms has become a pressing concern. Misalignment between LLM behaviors and ethical standards can lead to unintended, or even harmful consequences. As a result, value alignment, which aims to ensure that the actions and outputs of these models are consistent with human values has emerged as a pivotal challenge.\nCurrent approaches to value alignment typically concentrate on a few core values, such as the 3H: helpfulness, harmlessness, and honesty. While this focus has proven effective in guiding models toward certain desirable behaviors, human values encompass a much broader spectrum, often spanning hundreds of distinct dimensions with intricate and interconnected substructures (Schwartz and Boehnke, 2004). However, when LLMs are deployed, these value systems frequently remain implicit, with their underlying structures and causal relationships poorly understood. To effectively align the values of LLMs, it is crucial to develop a comprehensive understanding of these value systems, including the spectrum of values and their interconnections, and causal dynamics.\nMeanwhile, there are established methodologies aimed at guiding LLMs toward predefined value dimensions. Notably, Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) and constitutional learning (Bai et al., 2022) stand out. RLHF utilizes human-generated feedback to reward or penalize specific behaviors, while constitutional learning incorporates a set of guiding principles or rules that the model must adhere to in its decision-making processes. Although these approaches mark significant advancements in aligning LLMs with human values, they come with limitations. A primary constraint is that the alignment process can be resource-intensive, necessitating considerable computational power, extensive human feedback, and thorough fine-tuning. As a result, it is impractical to guide LLMs toward every one of the hundreds of value dimensions. Additionally, these methods often lack flexibility; once a model has been deployed, modifying its value preferences becomes challenging without retraining.\nTo address these gaps, this paper introduces a novel framework designed to unveil and steer the value systems inherent in LLMs. Our approach begins by mining the causal graph of values within the LLMs, which maps out the implicit relationships between various internal values. This graph provides a structure of how different values interact and influence the model's decisions. Building on this causal graph, we introduce two mechanisms for value steering. The first mechanism involves setting the agent's role via prompting. Compared to traditional prompting engineering paradigms, our causal graph offers prior knowledge about the consequences of prompting, indicating which value dimensions will be jointly affected when attempting to change one specific value dimension. The second mechanism leverages Sparse Auto Encoder (SAE) features from the internal representations of the transformer layers. By manipulating a single dimension of the SAE feature with a minimal number of tokens, we can effectively steer specific value dimensions of the LLM agent. We conduct experiments on Gemma-2B-IT and Llama3-8B-1T to demonstrate the effectiveness of our steering methods.\nIn summary, the contribution of this paper is threefold. First, we uncover the causal graphs of values within LLMs, revealing the relationships among different values and providing a foundation for guiding value steering methods. Second, we introduce a novel lightweight steering method: SAE feature steering, which leverages the causal graph to ensure that the effects of altering one value dimension on others are controllable. Lastly, we conduct extensive experiments on Gemma-2B-IT and Llama3-8B-IT to validate the effectiveness of the mined causal graphs and to demonstrate both the effectiveness and controllability of our steering methods."}, {"title": "2 Related Work", "content": "Graph Mining in Social Science. Relationship analysis has been extensively applied in social science to investigate complex interdependencies among variables, including research on personality psychology (Cramer et al., 2012; Costantini et al., 2020; Marcus et al., 2018), political beliefs (Boutyline and Vaisey, 2017; Brandt et al., 2019), attitudes (Dalege et al., 2016), self-concept (Elder et al., 2023), and mental disorders (Boschloo et al., 2015). In particular, Schwartz's theory posits that human values form a quasi-circumplex structure, where adjacent values share highly consistent underlying motivations, while opposing values tend to conflict with one another (Schwartz and Boehnke, 2004). This structure was developed using data derived from extensive questionnaire results (Schwartz et al., 2012; Schwartz, 1992, 2012). However, these studies provide limited insight into causal relationships (Rohrer, 2018; Borsboom et al., 2021; Ryan et al., 2022; Imai, 2022). In contrast, our work utilizes directed graphs to represent causal relationships among values. While some studies (Russo et al., 2022) leverage Schwartz's value structure to predict human behaviors, none have explored using it to steer human values. In comparison, our work leverages causal graphs to steer the values of LLMs.\nValue Systems within LLMs. Previous research has explored the value systems of LLMs. ValueBench provides the first comprehensive psychometric benchmark for evaluating value orientations and value understanding in LLMs (Ren et al., 2024). ValueCompass (Shen et al., 2024) introduces a framework of fundamental values, grounded in psychological theory and a systematic review, to identify and evaluate human-AI alignment. UniVaR uses the responses of different LLMs to the same set of value-eliciting questions to explore how LLMs prioritize different values in various languages and cultures (Cahyawijaya et al., 2024). ValueLex reveals both the similarities and differences between the value systems of LLMs and that of humans (Biedma et al., 2024). FULCRA (Yao et al., 2023) proposes a basic value alignment paradigm and introduces a value space spanned by basic value dimensions. These works have studied the underlying value dimensions of LLMs, but have neglected the deeper causal relationships between value dimensions, which improve the controllability of LLMs. This is precisely the issue that our paper aims to address.\nSparse Autoencoder (SAE). The SAE learning algorithm is a widely used approach for automatically learning features and has proven effective in interpreting the internal representations of LLMs. Elhage et al. (2022) explore how neural networks encode features within high-dimensional representations, providing insights into mechanisms like feature superposition and sparsity. Cunningham et al. (2023) train a number of SAEs on models such as Pythia-70M and Pythia-140M, showing that it is possible to extract human interpretable features from neurons using SAEs and translate using LLMs. Gao et al. (2024) leverage k-sparse autoencoders to enhance sparsity control, streamline tuning, and improve the reconstruction-sparsity trade-off. Marks et al. (2024) introduce sparse feature circuits as human-interpretable subnetworks to explain language model behaviors, focusing on the causal relationships between SAE features and model outputs. In contrast, our work investigates the causal relationships among value dimensions. Changing the values of SAEs in a model is often used as a method to modify the model's output, also known as SAE steering. For instance, ActAdd (Turner et al., 2024) use vectors of specific prompts to steer the input vector at a specific layer to modify the output. Li et al. (2023) rank the attention heads and adds the top-K activation vectors to the output of the layer to shift the final output in the direction of more truthful answers. Many works have proven that modifying certain vectors in the trained SAEs can change the output consistency with the explanation of the steered vector (Bricken et al., 2023; Cunningham et al., 2023)."}, {"title": "3 Value Causal Graph", "content": "Human value is complex in structure. A single-dimensional modeling is far from sufficient to explain the various decision styles. However, when considering value space as multidimensional, finding decoupled dimensions via math techniques such as Gram-Schmidt orthogonalization will cause losses in clarity of semantics: a massive linear combination of word-based meanings may not remain simply explainable by language. Therefore, living together with the complexity of human values, causal structures amongst the \u201cdimensions\u201d is inevitable. In this section, we set up language to discuss 1) how to find causal graph from text-based surveys, 2) prompt / SAE feature steerings, and 3) the steering effect along causal relations.\n3.1 Preliminaries\nFor any set A, $a \\in A$ denotes an element a in A. For a set X and a map $T: X \\rightarrow Y$, let $S \\subseteq X$, write $S_T$ (for prompt steering) or $S_T$ (for SAE steering) as the images of S, and we abuse the notation $T: 2^X \\rightarrow 2^Y$ where $T(S)=S_T$.\n3.2 Causal Graphs from Survey\nWe start from a set of values V, the final goal is collecting data and discovering the causal relation in V from data. For data collection, a text-based questionnaire consisting of yes or no questions is provided. Each question is related to a single value $v \\in V$ and bears an agreement metrics, i.e., whether it supports or opposes v when the answer is \"yes\". Thus, once the questions are provided individually to an LLM, the corresponding set of answers maps to scores $S \\in R^{|V|}$ by taking the average score of the answer to each question with agreement metric as weight. And with set ${S}$ of scores each corresponding to a distinguished questionnaire result, passive causal discovery algorithms, such as Peter-Clark algorithm (Spirtes et al., 2001), can be applied to build a causal graph (V, E) on V.\n3.3 Steering Methods\nPrompt Template Steering. When posing a question to a large language model (LLM), we use a \"template\" T that incorporates the question before it is submitted to the LLM. When T changes, the model's output is subsequently changed, resulting in variations in the score $S_T \\in R^{|V|}$ even within the same LLM. In our experiments, unrestricted changes to prompt templates encounter the inherent flexibility of language, allowing for many equivalent expressions of the same meaning. Consequently, we limit modifications of prompt templates to two specific categories. The first category is role playing, where only the role settings change. This method is selected for three reasons: 1. Role-playing templates are consistent with standard psychological survey methods, which collect data from a wide range of human subjects. 2. The structured nature of role-playing allows for effective control and meaningful cross-template comparisons. 3. By varying roles in terms of occupation, personality, etc., it still provides sufficient data diversity for analysis. Role playing helps establish a foundational set of questionnaire responses ${S_R}$. The second category includes \"hard prompts\" H, which explicitly instruct the language model to enhance or diminish certain value dimensions, generating ${S_{H \\cup R}}$ for a fixed H and various roles R.\nSAE Feature Steering. In addition to prompt template steering, another method to influence the output of an LLM involves directly This is achieved by \"pushing\" the activation state along the directions of features learned by SAE from the activation state data of the LLM. SAE feature steering is compatible with prompt template steering. Precisely, for a given feature f and strength $\\sigma$, steering LLM by $(f, \\sigma)$ while applying the questionnaire with template T results in a scoring $S_T^{(f,\\sigma)}$ on V different from $S_T$. In practice, features are usually layer-specific for training convenience.\nAs mentioned above, it is possible to apply SAE steering to the model independent of a role playing prompt template R. The resulting score data ${S_R^{(f,\\sigma)}}$ reflects the influence of $(f, \\sigma)$ generally on the value dimensions and eventually filters out value-related roles.\n3.4 Steering Effect along Causal Relations\nThe causal graph offers a straightforward representation that illustrates the expected outcomes when a particular variable is altered. This characteristic can be used to assess the quality of the graph. As our SAE features are trained in an unsupervised manner, each feature may have an effect on multiple value dimensions. The value causal graph could also help to understand the side effects when the effect of a steering on only one value item is evaluated, usually by asking part of the questionnaire. In contrast, since each value dimension represents a semantic meaning, language models such as GPT-4 could be able to annotate the causal relations between such word pairs according to their understanding of the words. With such a reference \"causal\" graph build, we may validate the steering effect of each method on them, evaluating whether some deeper causal relation is lying beneath the semantic meaning of the values."}, {"title": "4 Experiments", "content": "4.1 Settings\nWe conduct experiments using the Gemma-2B-IT and Llama3-8B-IT models. For the dataset, we select 10 representative values from ValueBench (Ren et al., 2024), each containing more than 20 QA pairs. We utilize the dataset's questions and agreement metrics to assess the value of LLM agents. For the SAE models, we employ pre-trained models of the two LLMs provided by the SAElens (Bloom and Chanin, 2024) third-party tool. Specifically, we extract the 25 significant SAE features from the token sequence \u201cyour values\" within the system prompt and apply a 100-fold increase as strength to each feature individually for SAE value steering. We assume that features selected in this way are more related to \"value\", thus more likely to affect concrete values.\nWe generate 125 virtual roles with diverse background settings using GPT-4, partitioning them into a training set of 100 roles and a test set of 25 roles. The training and test roles evaluate their values using different splits of each value's QA pairs. The test roles use 30% of each value's QA pairs, while the training roles use the remaining 70%. To minimize potential bias from any specific question, we randomly sample 40% of the training data for each role-SAE setting combination. This sampling rate ensures that the probability of any two questions occurring under a given setting remains relatively low, thereby preventing the effect of SAE value steering from being skewed by a few special cases influencing all roles.\nTo evaluate the agents' responses, we primarily use a binary (yes/no) classification based on the provided instructions. In cases of ambiguity, we leverage GPT-4 to assess the agent's inclination towards a given value based on its response.\n4.2 Value Causal Graph of LLMs\nFor both LLMs (Gemma-2B-IT and Llama3-8B-IT), we use the value scores from all 101 training roles (including the empty role) under 25 SAE steering features, resulting in a total of 2,525 rows of data. This data is used to discover the causal relationships among the value dimensions via PC algorithm, yielding two causal graphs presented in Figure 2 and Figure 3. Additionally, we generate a reference causal graph using GPT-4, guided by upper-dimensional information from ValueBench and Schwartz's Theory of Basic Values.\n4.2.1 Predicting the Effects of Steering via Causal Graphs\nWhen steering a target value, particularly when using role-setting prompts, the subsequent effects on other value dimensions are often unpredictable. By constructing the causal graph, we can analyze the successors of each value node to anticipate these side effects. As shown in Figure 4 and Figure 5, for both Gemma-2B-IT and Llama-3B-IT, our causal graph provides an effective prediction of the subsequent effects of role-setting prompts and SAE steering. Each time a value node changes its score, we expect its subsequent nodes on the causal graph also change scores while the non-subsequent nodes stay the unchanged. We also present results of the same experiments using the reference causal graph. Note that all tests are conducted on the test set, which uses completely different roles and value questions than those used to build the causal graph. The following observations can be made:\n\u2022 The value dimensions expected to change after steering, according to our discovered graph, are more likely to change in real cases than those predicted by the reference graphs, for both prompt and SAE steering across all LLMs. For example, in the case of prompt steering methods, the frequency of changes in the expected values from our graph is higher than 0.5, while the frequency in the reference graph is lower than 0.5. Conversely, unexpected value changes are more common in the reference graphs in real cases. This suggests that the internal mechanisms of LLM value systems are fundamentally different from a semantic graph, which only captures the lexical meanings of value-related terms, or from a human value structure (e.g., Schwartz's value theory). This underscores the significance of introducing a causal graph for guiding value steering, rather than relying solely on a few steering methods.\n\u2022 Unexpected changes are fewer or similar for SAE steering than for prompts on our graphs. This suggests that SAE steering has a more precise effect. One explanation is that, in the causal graph, an SAE feature has a precise effect on only a few source nodes, whereas a prompt may influence more source nodes.\n\u2022 In some cases, the frequency of expected value changes of SAE steering is lower than with prompts. This may be because when fewer source value nodes are changed, the strength of change in subsequent causal nodes are smaller, sometimes making them harder to detect. This explains why we still need prompts alongside more precise methods like SAE-the effect of prompts is more comprehensive, and they can address steering targets beyond the scope of individual SAE features.\n4.3 Steering Values via SAE Features\nFor each SAE feature, we observed that it could either stimulate, suppress, or maintain specific value dimensions depending on the context. Some outcomes are relatively consistent, enabling us to controllably alter the value orientation of the LLM agent using SAE features. However, there are also instances where the outcomes are less predictable. In Table 1, we present the expected effects of each SAE feature on various value dimensions, along with the extent to which these effects are replicated during actual testing under different role settings and value questions.\nFor both LLM models, in most cases, values could be steered as expected. Each SAE feature exhibits distinct effects on certain value dimensions, and for the majority of dimensions, it is possible to identify SAE features that support steering in desired directions. However, a few values remain challenging to steer effectively.\nTo further demonstrate that SAE is effectively steering the LLM values, rather than randomly altering the output for specific questions, we examine multiple levels of consistency in the answers to value-related questions.\nConsistency within a QA. One key indicator that the SAE steering method is genuinely influencing the LLMs is the alignment between the answers and the corresponding thought processes. As shown in Table 2, we find that the answers remain largely consistent with the thought processes, both before and after steering.\nConsistency within a Value. One of the strongest indicators of SAE's effectiveness in steering a value is its ability to consistently alter the answers to different questions related to that value in the same direction. For each value-SAE pair, we select the questions with changed scores after steering and calculate the standard deviation (std) of the score changes. The score change can be 2, -2, 1, or -1, as the possible scores are 1, -1, and 0. For Gemma-2B-IT, the average std across values, SAE features, and roles is 1.01, while for Llama3-8B-IT, it is 0.95. This roughly corresponds to one inverse change for every five score alterations.\nComparing SAE with Direct Value Instructions. To further manifest the impact of SAE feature steering, we compare it with an ideally effective steering method for a single value, namely, directly informing the LLMs of the definition of the value and their intended inclinations. For each value, we apply its most effective positive and negative SAE features, along with the direct value instruction, to the test roles. We then analyze the steering results on these roles. From Table 3, it is evident that each method has its own advantages. For Gemma-2B-IT, SAE is more effective in positive steering but less effective in negative steering. Conversely, for Llama3-8-IT, SAE performs less effectively in positive steering but better in negative steering. These results suggest that LLMs do not always follow direct instructions as effectively as expected. This discrepancy may arise from the LLM's imprecise understanding of certain values during its pre-training. Taking into account the advantages of side-effect control, SAE generally has its advantage over direct value instructions."}, {"title": "5 Conclusion", "content": "In this paper, we address the challenge of aligning LLMs with the broad and intricate spectrum of human values. We present a novel framework that begins by mining a causal graph to uncover and analyze the implicit value relationships embedded within LLMs. Building on this graph, our framework introduces two lightweight value-steering mechanisms: (1) manipulating sparse autoencoder (SAE) features and (2) role-setting via prompt engineering. By integrating these mechanisms with the causal graph, our approach offers a more granular and systematic method for steering LLMs across multiple value dimensions, providing enhanced flexibility and control over their behaviors. We evaluate our framework using Gemma-2B-IT and Llama3-8B-IT, demonstrating that: (1) the causal graph accurately predicts the ripple effects of adjusting one value dimension on others, out-performing traditional semantic graphs in practical utility; (2) the two proposed steering mechanisms are both effective and controllable. These findings underscore the potential of our framework for advancing value alignment in LLMs."}]}