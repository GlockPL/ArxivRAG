{"title": "SIM: SURFACE-BASED FMRI ANALYSIS FOR INTER-SUBJECT MULTIMODAL DECODING FROM MOVIE-WATCHING EXPERIMENTS", "authors": ["Simon Dahan", "Gabriel B\u00e9n\u00e9dict", "Logan Z. J. Williams", "Yourong Guo", "Daniel Rueckert", "Robert Leech", "Emma C. Robinson"], "abstract": "Current AI frameworks for brain decoding and encoding, typically train and test models within the same datasets. This limits their utility for brain computer interfaces (BCI) or neurofeedback, for which it would be useful to pool experiences across individuals to better simulate stimuli not sampled during training. A key obstacle to model generalisation is the degree of variability of inter-subject cortical organisation, which makes it difficult to align or compare cortical signals across participants. In this paper we address this through use of surface vision transformers, which build a generalisable model of cortical functional dynamics, through encoding the topography of cortical networks and their interactions as a moving image across a surface. This is then combined with tri-modal self-supervised contrastive (CLIP) alignment of audio, video, and fMRI modalities to enable the retrieval of visual and auditory stimuli from patterns of cortical activity (and vice-versa). We validate our approach on 7T task-fMRI data from 174 healthy participants engaged in the movie-watching experiment from the Human Connectome Project (HCP). Results show that it is possible to detect which movie clips an individual is watching purely from their brain activity, even for individuals and movies not seen during training. Further analysis of attention maps reveals that our model captures individual patterns of brain activity that reflect semantic and visual systems. This opens the door to future personalised simulations of brain function. Code & pre-trained models will be made available at https://github.com/metrics-lab/sim, processed data for training will be available upon request at https://gin.g-node.org/Sdahan30/sim.", "sections": [{"title": "1 INTRODUCTION", "content": "Over recent years, there has been growing interest in the extent to which machine learning frameworks, such as convolutional neural networks (CNNs) and transformers, can model neurological processes: from spatial encoding of the hippocampus (Ellwood, 2024; Whittington et al., 2021; Kim et al., 2024) to replicating semantic (Antonello & Huth, 2024; Caucheteux et al., 2023; Huth et al., 2016; Millet et al., 2022) and visual (Benchetrit et al., 2023; Ozcelik & VanRullen, 2023; Tang et al., 2024; Wen et al., 2018) representations within the cortex. These approaches support the testing of new theories of human cognition (Millet et al., 2022; Antonello & Huth, 2024; Caucheteux et al., 2023; Ellwood, 2024; Kriegeskorte, 2015; Whittington et al., 2021; Kim et al., 2024), and allow for the encoding or decoding of stimuli (Benchetrit et al., 2023; D\u00e9fossez et al., 2023; Gu et al., 2022; Lindsay, 2021; Kriegeskorte, 2015; Ozcelik & VanRullen, 2023; Scotti et al., 2024; Thomas et al., 2022; Thual et al., 2023; Wen et al., 2018; Yamins & DiCarlo, 2016) from non-invasive human brain recordings such as functional magnetic resonance imaging (fMRI), magneto-encephalography (MEG) or electro-encephalography (EEG). However, such comparative models are subject-specific, typically require large amounts of imaging data per subject, and do not generalise to unseen individuals. This limits the extent to which they can be used to probe individual sources of variation, to ultimately build precision models of human cognition and behaviour.\nOne contributing factor to the lack of generalisation of current models, is arguably that most models treat signals from adjacent locations in the brain as spatially independent - encoding patterns of brain activity using linear, or non-linear regression modules (Huth et al., 2016; Millet et al., 2022; Caucheteux et al., 2021; Scotti et al., 2024; Ozcelik & VanRullen, 2023). This ignores the well-documented spatial auto-correlation of signals across the cortex that is known to be behaviourally meaningful (Bijsterbosch et al., 2018; Kong et al., 2019; Shinn et al., 2023; Leech et al., 2023; Margulies et al., 2016).\nAt a high level, cortical activity may be decomposed into signals from a relatively small number of functionally specialised areas (Glasser et al., 2016a; Smith et al., 2013). These regions communicate through neuronal connections, resulting in coordinated patterns (or networks) of activity. Previous studies have shown that human brain function may be modelled from temporal sequences of these networks (Vidaurre et al., 2017; Pervaiz et al., 2022; Smith et al., 2013). This suggests that if we can build an encoding model that generalises these spatial patterns across individuals, then we can predict what any individual's cortical functional dynamics should be in response to a given stimulus; or predict a novel stimulus from the spatial topography of their brain activity.\nWith this in mind, recent efforts to generalise vision transformers (ViTs) (Dosovitskiy et al., 2020) to the cortical surface present significant opportunity as they allow for the modelling long-range spatio-temporal interactions through the encoding of space-time self-attention (Dahan et al., 2022; 2024). Relative to a 3D network of equivalent resolution, surface models are much more compact and require less memory; this creates scope for building more sophisticated network architectures. Moreover, surface modelling more faithfully represents the true geometry of the convoluted cortical surface, and for this reason has long been favoured for analysis of cortical signals (Glasser et al., 2016b; Margulies et al., 2016; Gordon et al., 2017b;a).\nContributions: In this paper, we introduce SIM (Fig 1), a novel framework that combines self-supervised contrastive learning with surface vision transformers (SiT) (Dahan et al., 2022; 2024), to support multimodal decoding of audio+video (movie) stimuli from 7T cortical fMRI. Importantly, this generalises to predicting new movie clips from new individuals (not used during training); using only a few seconds of fMRI; and is trained and tested using the HCP 7T movie-watching dataset (Van Essen et al., 2013; Finn & Bandettini, 2021a), which collects many hours less data per individual, than most recent decoding frameworks (Benchetrit et al., 2023; D\u00e9fossez et al., 2023; Scotti et al., 2024; 2023; Ozcelik & VanRullen, 2023). Achieving this involved engineering contributions for tri-modal CLIP alignment of audio, video stimuli with fMRI. Benefits extend to reconstruction, where our embeddings may be used to reconstruct movie frames, from the brain activity of unseen subjects, that reliably decode the semantic content of scenes. Results show that joint encoding of audio/video stimuli and fMRI inserts complementary information that allows better decoding between any pair of these modalities. Furthermore, visualisation of self-attention suggests the model attends to functional brain networks in audio-visual information. This opens the door to the future precision simulation (Digital Twins) of human brain function in response to unseen stimuli and tasks."}, {"title": "2 RELATED WORKS", "content": "Classic approaches to fMRI decoding, classify stimuli from the temporal dynamics of vox-els/vertices extracted from visual regions of the human brain. Most often linear or non-linear re-gression is used; with recent models leveraging the power of generative or foundational AI to learn rich encodings of stimuli, which are subsequently matched to patterns of brain activity through use of CLIP contrastive learning (Benchetrit et al., 2023; D\u00e9fossez et al., 2023; Scotti et al., 2024; 2023; Ozcelik & VanRullen, 2023). Often methods are complemented with \"hyper-alignment\u201d techniques that seek to map all data to a space in which brain activations overlap across individuals (and so may compared) (Haxby et al., 2020; Thual et al., 2023); however, in practice, this is ill-posed\u00b9 which has meant that most decoding frameworks (Benchetrit et al., 2023; D\u00e9fossez et al., 2023; Scotti et al., 2024; 2023; Ozcelik & VanRullen, 2023) are trained and tested within the same brain, using densely sampled datasets (Allen et al., 2021; Hebart et al., 2023) that collect tens of hours of recordings for each subject. Similarly, approaches to inter-subject generalisation (Scotti et al., 2024; Thual et al., 2023) introduce costly alignment steps that require \u2265 1 hour of recording for each test subject.\nLearning-based frameworks support transformation equivariant modelling of images and, as such, offer improved potential for generalisation. However, the cortex is a highly curved manifold with activations best modelled as patterns across a surface (Glasser et al., 2016b;a; Coalson et al., 2018). This presents challenges due to the difficulty in translating convolutional operations to non-Euclidean domains that lack a global coordinate system (Bronstein et al., 2021); resulting in frameworks that perform sub-optimally on cortical phenotyping and fMRI decoding tasks (Fawaz et al., 2021; Gu et al., 2022).\nRecent work on SiTs (Dahan et al., 2022) has indicated that vision transformers robustly outperform surface convolutional neural networks (CNNs) across a range of cortical phenotyping tasks, while offering inherent interpretability through visualisation of self-attention. Moreover, careful adaption of the video masked autoencoder pre-training (Tong et al., 2022; Feichtenhofer et al., 2022) to sur-"}, {"title": "3 METHODS", "content": "In what follows, we consider cortical fMRI signals as functions in space and time $S(v, t)$ defined on a spherical mesh ($v \\in V6$) corresponding to a 6th-order icosahedron: $I6 = (V6, F6)$, with $V6 = 40962$ vertices and $F6 = 81920$ faces, see Figure 1.A. The objective of our framework is to use SiTs to encode the spatio-temporal dynamics of the signal as it evolves. This is imple-mented through video surface-masked autoencoder (vsMAE) self-supervision. Decoding is then implemented through aligning representations learnt from the vsMAE (Fig 1.C) with audio and visual representations (learnt from wav2vec (Baevski et al., 2020) and videoMAE (Tong et al., 2022)) using CLIP contrastive learning (Fig 1.D). At inference time, this supports the retrieval of any one modality from each of the others."}, {"title": "3.1 BASE ARCHITECTURES", "content": "SiT: Cortical surface analysis Fischl (2012), is now a very common processing stage for most neuroimaging data collections, involving the fitting of tessellated meshes to the inner and outer boundaries of the cortex, followed by inflation and projection to a sphere (Figure 1.A). Cortical imaging features, such as fMRI are projected onto the surface through ribbon-constrained weighted averaging (Glasser et al., 2013). The SiT (Dahan et al., 2022) leverages this simplified spherical domain to patch cortical imaging data using regularly tessellated icosahedral meshes. First imaging features are resampled to a high-resolution $I6 = (V6, F6)$ mesh (with $|V6| = 40962$ vertices and $|F6 = 81920$ faces; these features are then patched with the faces of a low-resolution icosphere (typically I3, with $F3 = 1280$) (Fig 1.B) - partitioning the cortical features into a sequence of $N = |F3|$ non-overlapping triangular patches: $P = {p1, p2, ..p|N|}$ (with $p\u00b2C V6, |p\u00b2 = 45$). Features within each patch are then concatenated across channels (C) flattened and projected, with a trainable linear layer, into a set of D-dimensional input tokens to produce an initial sequence: $X\u00ba = [X],..., X] \\in RN\u00d7D$. Sine-cosine positional embeddings, $Epos = {Ei}_{i=1}^{N}$ are then added to each of the tokens to encode patch location within the sequence: $X(0) = [X\u00b0 + E1, ..., X% + En]$. This initial sequence $X(0) is then processed by L consecutive transformer encoder blocks of H Multi-Head Self-Attention (MHSA) and Feed Forward Network (FFN) layers, with residual layers in-between, resulting in an output sequence of fMRI token embeddings ($XfMRI \\in RN\u00d7D$). As with classic vision transformers (Dosovitskiy et al., 2020) the objective is to model long-range co-occurrences of spatial structure in the surface imaging data (as self-attention between tokens), which should confer sufficient image understanding to perform any given supervised learning task."}, {"title": "vsMAE:", "content": "Transformers are powerful learning models but their lack of inductive biases present challenges when training on limited data. In Dahan et al. (2024), a self-supervision pre-training task was proposed that extends the concept of video-masked autoencoders (Tong et al., 2022; Feicht-enhofer et al., 2022) to the surface. This frames self-supervision as an auto-encoding task, where unmasked tokens, from T input fMRI frames, are first randomly selected from the set of all available patches according to a masking ratio p . Each token then represents a 'tube' of cortical data patched in space and time. These are passed to a SiT encoder (MRI), which compresses each spatio-temporal token through its linear layer, and then passes the resulting sequence of tokens through each transformer encoder block. Next, random embeddings are added into the encoder's latent embeddings sequence - in place of the masked tokens - restoring the sequence to its original length. Positional embeddings are added to encode spatial information and the resulting sequence is fed into the SiT decoder (fMRI). The last layer performs a linear projection to restore the input patch resolution (T \u00d7 p\u00b2). Following He et al. (2021), the vsMAE is optimised by calculating the mean square error (MSE) between the masked input feature patches and their reconstructed versions only. The pre-trained vsMAE encoder (fMRI) then forms the basis of the proposed decoding framework (Fig 1)."}, {"title": "3.2 DECODING NETWORK", "content": "Taking learnt representations of the fMRI from the pre-trained vsMAE encoder (XfMRI), the next objective is to align these to video (V) and audio (A) representations, respectively noted $Xy \\in RNv\u00d7Dv$ and $XA \\in RNA\u00d7DA$, learnt from pre-trained videoMAE (Tong et al., 2022) and wav2vec2.0 models (Baevski et al., 2020) (\u03a6enc and A) in Fig 1.E & F), using CLIP contrastive learning (Radford et al., 2021). Details about audio-visual stimuli processing and embeddings extraction are provided in Appendix B.3.\nMultimodal mappers: Since each unimodal model outputs a sequence with different token length, it is necessary to first compress all representations to vectors of the same length such that they can be directly compared (Fig 1.D). This is performed with multimodal mappers $f^{mod}$, similar to (Najdenkoska et al., 2023), that project each unimodal sequence of tokens through two linear layers, with GeLU activation, dropout and residual connections, before averaging the sequence to output a vector of equivalent resolution (DCLIP) for each modality: such that $YfMRI = ffMRI (XfMRI)$, $YA = f(XA)$ and $yv = fy (Xv)$, and $YfMRI, YV, YA \\in RDCLIP$.\nAlignment: Tri-modal CLIP alignment is then achieved by sampling, for each batch, exactly one positive triplet and M 1 negative triplets. A positive triplet consists of fMRI, audio, and video data from the same 3s movie clip, while negative triplets are sampled from different 3s movie clips. Next, cosine similarities $(za,b(i, j) = (ya, y))$ are calculated between pairs of modalities (a and b), which are then converted into probabilities through applying a softmax function: $Pa,b(i, j) = \\frac{exp(za,b(i,j)/\u0442)}{\\Sigma_{k=1}^{M} exp(za,b(i,k)/\u03c4)}$ (here, \u03c4 is a temperature hyperparameter that scales the logits). The CLIP loss from modality a to b (noted Lab) is then calculated using cross-entropy to push together the embeddings of positive samples and push apart the embeddings of negative samples, such that: $Lab = -\\Sigma_{i=1}^{M} log Pa,b(i, j)$. To perform alignment across three modalities simultaneously, we average the losses calculated between all pairs of audio/video/fMRI in the batch:\n$L = (LfMRI\u2192V + LV\u2192fMRI + LfMRI\u2192A+LA\u2192fMRI + LA\u2192v + Lv\u2192A)/6$\nThen, at inference time, and for each given modality a, the model can be used to rank the samples that are most likely to be aligned with modality b (highest probability) from a list of available stimuli."}, {"title": "4 EXPERIMENTAL METHODS", "content": "In this paper, stimuli and accompanying brain recordings were taken from 174 participants, aged 29.4\u00b13.3 years (68 male and 106 female) who were scanned as part of the HCP 7T movie-watching experiment (Van Essen et al., 2013; Finn & Bandettini, 2021a). Participants underwent 4 recording sessions, each lasting ~ 15 minutes, during which time they were shown a series of movie scenes from independent/Hollywood movies (1-4.3 mins in length). These movie scenes were interleaved with rest periods of 20 seconds, where participants were told to fixate on a cross on a blank screen (Fig 2). For each session, audio was delivered via earbuds, and movie files were cropped and zoomed from their original 16:9 aspect ratio (AR) to a 1024 \u00d7 720, 14.22:10 AR to fit the projector screen. All Hollywood movie scenes were prepared and published by (Cutting et al., 2012)."}, {"title": "4.2 TRAINING", "content": "Subjects were partitioned into train/validation/test splits of size 124/25/25, while stratifying sex and age distribution across splits, with fMRI from left and right hemispheres treated as independent samples but placed in the same split. This corresponds to training, 200 validation and 200 testing samples. We then divide the movie data into a series of non-overlapping 3s movie clips: corresponding to 16 frames of movie stimuli and 3 frames from the cortical fMRI, where this was sampled with a temporal lag of 6 seconds to account for the haemodynamic response (Huth et al., 2016) (AppendixC.8).\nenc\nThe SiT backbone, forming OfMRI, follows the standard structure of a DeiT-small (Touvron et al., 2020), which extends (Dosovitskiy et al., 2020) to more efficient ViT architectures. For all training phases (vsMAE pre-training and tri-modal CLIP alignment), the AdamW (Loshchilov & Hutter, 2019) optimisation was used with LR = 3e-4 and cosine decay. Distributed training was implemented in all experiments with a batch size of 64 (per GPU) for the vsMAE pre-training task. For the tri-modal CLIP training, batch size was maximised across all GPU instances to 256 by implementing aggregation across instances\u00b2. All experiments were run on an internal cluster of 4 NVIDIA V100 GPUs (32 GB of memory). Video and audio encoders (\u03a6\u03c2, \u03a6.) were kept frozen for all experiments. Multimodal mappers (ffMRI, fy, foA) were trained in all experiments. During the tri-modal CLIP training, we investigated various training regimes for the pre-trained vsMAE en-coder: (1) training from scratch, (2) keeping the SiT encoder frozen after vsMAE pre-training or (3) fine-tuning from pre-trained weights. To evaluate the impact of different modalities during the CLIP-alignment training, we also investigated the training of two modalities only (e.g. fMRI, V) or the three modalities altogether (fMRI, V, A). Ablation experiments for (1) & (2) are now in Table C.1."}, {"title": "4.3 INFERENCE", "content": "At test time, models were evaluated based on their retrieval and reconstruction abilities, where inference of modality a from modality b is defined as a \u2192 b (e.g. fMRI \u2192 V). In practice, this is calculated from sampling M test samples, including one positive (correct) pair and M 1 negative (mismatched) pairs. These are passed to the CLIP model, which outputs a (softmax) probability that each of these pairs is a match. Results are then ranked and performance is reported from top-K accuracy i.e. whether the true pair ranks within the top K samples, as per recent works (Thual et al., 2023; Ozcelik & VanRullen, 2023; Scotti et al., 2023; 2024; Benchetrit et al., 2023; D\u00e9fossez et al., 2023). The number of clips tested is adapted for each pair of modalities to account for different noise levels - applying M = 64 for video and M = 32 for audio testing (audio samples being noisier). Two types of negative sampling procedures were used: soft-negative - where negative pairs are sampled only from different movies to the positive sample; and hard-negative sampling - where"}, {"title": "4.4 EVALUATION", "content": "The objective of this paper is to demonstrate that it is possible to train encoding and decoding frameworks that generalise to (i) new subjects (Experiment 1); (ii) new movie scenes (Experiment 2); and (iii) new movie scenes within new subjects (Experiment 3). The experimental setup is described in Figure 2.\nBeyond evaluation of retrieval performance, we also consider: (1) how well self-attention maps, encoded by the vsMAE, reflect the current understanding of the underlying cognitive processes; (2) if tri-modal alignment allows stimuli reconstructions from fMRI signals. We, therefore, visualise attention maps by projecting them back to the cortical surface (Fig4). This is achieved by extracting the self-attention-weights matrix from all layers, passing these through softmax operations; then slicing to retain only weights associated with the output [CLS] token (following Caron et al. (2021)). These weights are then interpolated back to 16 resolution by assigning all vertices, for each cortical patch, the attention weight of the corresponding token in the sequence. For the video-frame reconstruction, we adapt the code from Ozcelik & VanRullen (2023) to the CLIP/latent encoding resolutions of our architecture; applying 50 DDIM steps with a strength of 0.75.\nBaseline: To evaluate the contribution of the SiT over and above the impact of contrastive learning, we compare against Ridge regression models, taking inspiration from Ozcelik & VanRullen (2023)."}, {"title": "5 RESULTS", "content": "Since all data has been MSMall (functionally) aligned across subjects, this represents an extremely robust baseline."}, {"title": "Generalisation to new subjects (i)", "content": "Results in Table 1 report the top-1 and top-10 performance for Experiment 1 (described in Fig 2) in two inference directions fMRI \u2192 V, fMRI \u2192 A. In both cases, the best retrieval performance is achieved when training on all three modalities, demonstrating the complementary contributions of video and audio for decoding (see also Appendix: Table C.1 for the inverse retrieval results). In all cases, performance increases considerably over random and ridge regression baselines, even for CLIP alignment of only two modalities. Table 1 summarises results for hard-negative sampling and decoding only - but comparable improvements are seen for soft-negative sampling, and inverse retrieval (V \u2192 fMRI and A \u2192 fMRI), which may be seen as encoding brain activations from audio/video stimuli. For more results, please refer to Appendix C: Table C.1, Figure C.6 and Figure C.7."}, {"title": "Generalising to new movie scenes and new subjects (ii) & (iii)", "content": "Results for Experiment 2 and Experiment 3 for soft-negative sampling are reported in Figure 5; and demonstrates that the SIM frameworks clearly generalises to new movie scenes and new subjects, compared to baselines. Visualisation of retrieved clips (Figure 3), for two test subjects, and for both hard-negative and soft-negative sampling, shows that our model meaningfully ranks movie clips with similar contents together. The full tables of results are available in Appendix C (Table C.2 and C.3); in all cases the proposed model (fMRI \u2192 V) strongly outperforms baselines."}, {"title": "Interpretation of self-attention maps", "content": "Visualisation of attention maps, corre-sponding to cortical fMRI, averaged across all test subjects, from a 3s movie clip involving dia-logue, are shown in Figure 4. Results are shown averaged within the individual HCP multimodal areal parcellation (Glasser et al., 2016b). This is done to support comparisons between cortical areas and their reported functions (see Tables 1-3 supplementary anatomical results (Glasser et al., 2016b)). Results are shown for each attention head separately, we also display the mean and variance calculated across all heads, for all test subjects.\nSystematic comparison of the activation patterns from each attention head, against well-validated topographic maps of functional networks (Yeo et al., 2011; Margulies et al., 2016) reveals their specialisation into sensorimotor, visual, and auditory cortices. A correlation analysis against Margulies' gradient-based maps (Margulies et al., 2016) shows that Gradient 2 is the highest correlated with all attention heads, pointing to dominance of attention within visual and sensorimotor/auditory cortices. Closer examination shows that attention heads 1 to 4 specialise in sensorimotor/auditory processing, while heads 5 and 6 specialise in visual processing. Similarly, a comparison with the functional networks identified by Yeo et al. (Yeo et al., 2009) shows that attention heads 1 to 4 highly correlate with Network 2 (sensorimotor and auditory), whereas heads 5 and 6 correlate with Network 1 (visual). Margulies's gradient maps (Margulies et al., 2016) and Yeo's functional networks (Yeo et al., 2009) are provided in Tables C.2 & C.3 for qualitative comparison."}, {"title": "6 DISCUSSION", "content": "Previous studies have shown that modern AI frameworks, such as CNNs and transformers, model natural stimuli in ways that parallel human cognitive processing (Millet et al., 2022; Antonello & Huth, 2024; Caucheteux et al., 2023; Kriegeskorte, 2015) and that this can be harnessed to decode auditory and visual stimuli from human brain recordings (Benchetrit et al., 2023; D\u00e9fossez et al., 2023; Scotti et al., 2024; 2023; Ozcelik & VanRullen, 2023; Gu et al., 2022; Lindsay, 2021; Thomas et al., 2022; Thual et al., 2023; Wen et al., 2018; Yamins & DiCarlo, 2016). While powerful, these approaches lack any model of individual cortical areal topography from which to simulate unseen stimuli. In this paper we take an important step in this direction by showing that SiT encoding of fMRI spatial-autocorrelations can allow for CLIP decoding of new movie scenes, from subjects that the model was not trained on. Moreover, vision transformers are inherently interpretable allowing us to visualise the patterns of self-attention encoded by the model. Our results suggest that each attention head may be modelling different visual and semantic concepts. Thus far, this has only been assessed at a global level by comparing average patterns of attention against state-of-the-art models of functional organisation including gradient maps (Margulies et al., 2016), functional connectivity networks (Yeo et al., 2009) and the HCP multimodal parcellation (Glasser et al., 2016a). We are yet to explore whether self-attention varies meaningfully across individuals, including whether maps are predictive of behavioural or cognitive traits (Finn & Bandettini, 2021a).\nOne notable current limitation with the current SiT architecture is how its computations scale with resolution. Increasing sampling of cortical patching by a single resolution level (from 13 to 14) increases the complexity of self-attention operations 16\u00d7. As this trade-off is critical for contrastive learning, exploring lighter forms of attention computation could help with scaling up the model. Another important consideration is the current lack of any model of sub-cortical fMRI, despite the known involvement of deep grey structures such as the Lateral Geniculate Nucleus (LGN) in vision Ghodrati et al. (2017). In future this could be addressed by adding tokens for sub-cortical structures. Similarly, our study focuses on matching modalities based solely on 3s movie clips. While this represents an attempt at modelling temporal dynamics, increasing the length of samples might allow for modelling of more complex brain processes, such as memory and attention.\nIn doing so it may be important to consider expanding the dataset to include a wider range of audio-visual stimuli. While the HCP 7T movie-watching dataset is an incredible resource, collected across a large number of subjects and crucial to evaluate generalisation properties, it comprises a restricted set of audio-visual stimuli, collected from very different styles of movies. This constrains the amount of information that can be extracted and learnt by our model, meaning that we were unable to effectively test whether the model would generalise to completely different movies. In particular, this can also limits the potential for stimuli reconstruction (Figure 6) as the variety of visual signals is limited. Alternative datasets, which are richer in the amount of data collected per subject, include the 'Friends' fMRI dataset (Boyle J.A., 2020) and the 'Narratives' dataset (Nastase et al., 2021).\nWhen considering the broader impact of such technologies, our interest is in tailoring brain-computer interfaces and neurofeedback therapies to individual brains, in which the needs and abil-ities of individual patients must be taken into account. Under these conditions, development of decoding models that generalise across brains are of vital importance, as they would allow for the sampling different experimental conditions across participants, requiring far less data collection for each new individual, while allowing models to simulate outside their training regime i.e. respond to novel situations. While, practical deployment of such healthcare models remains a long way off, it is important to be mindful of longterm potential ethical impacts of models that might generalise the de-coding of human thought from brain activations; not least because all freely available open datasets are derived from healthy (largely Caucasian) controls, which risk model bias and poor generalisation to patient and minority groups."}, {"title": "A APPENDIX - DATA PROCESSING", "content": ""}, {"title": "A.1 7T TASK-FMRI HCP DATA", "content": "Functional MRI data were downloaded from the Movie Task fMRI 1.6mm/59k FIX-Denoised pack-age available for download at https://db.humanconnectome.org/. Cifti files were sep-arated into left and right hemispheres; then resampled from native resolution (59292 vertices) to 16 resolution (40962 vertices). This resampling is necessary to integrate with the SiT framework, which utilises regular icosahedral grids (e.g. 13) to patch the input surface data (at 16)."}, {"title": "A.2 VIDEO & AUDIO DATA PROCESSING", "content": "All fMRI sessions (MOVIE1-4) were divided into non-overlapping 3s .mp4v movie-clips us-ing opencv. Audio files were extracted to .wav format at 16kHz from all movie clips with torchaudio library.\nChoice of temporal length of stimuli The duration of 3s for movie clips was chosen for two reasons. First, it corresponds to the average \u2018movie-shot' duration for most movies shown during the different fMRI sesions as detailed in Table 2. Second, a three-frame reconstruction yielded the best results during the vsMAE pre-training, which motivated the use of 3s movie clips (as TR = 1). Qualitative and quantitative assessments of frame reconstruction are provided in Appendix C.7."}, {"title": "B APPENDIX - METHODS", "content": ""}, {"title": "B.1 VSMAE PRE-TRAINING - ARCHITECTURE DETAILS", "content": "enc\ndec\nIn the Table B.2, we summarise the vsMAE architecture used for pre-training the SiT encoder (fMRI) on video-frame reconstruction. Both encoder (fMRI) and decoder (fMRI) are based on the SiT-small architectures (Dahan et al., 2022). A classification token [CLS] is appended and used alongside the token sequence during the pre-training. It is used for the visualisation of attention maps."}, {"title": "B.2 VSMAE PRE-TRAINING - TRAINING DETAILS", "content": "Here, we summarise some essential training details for the vsMAE pre-training task.\nMasking ratio While the temporal redundancy of pixels in natural videos, allows for effective agnostic masking with high masking ratios (of up to 90%) (Feichtenhofer et al., 2022; He et al., 2021), fMRI activations are less structured and much more noisy. Following results in Appendix C.7, we use a masking ratio of \u03c1 = 50% in all vsMAE pre-training.\nMasking strategy Similarly, the lack of structure and low temporal resolution in the spatio-temporal dynamics of brain activity is not suitable for agnostic masking strategy as in (Feichtenhofer et al., 2022). Therefore"}]}