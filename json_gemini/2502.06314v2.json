{"title": "From Pixels to Components:\nEigenvector Masking for Visual Representation Learning", "authors": ["Alice Bizeul", "Thomas M. Sutter", "Alain Ryser", "Bernhard Sch\u00f6lkopf", "Julius von K\u00fcgelgen", "Julia F. Vogt"], "abstract": "Predicting masked from visible parts of an image is a powerful self-supervised approach for visual representation learning. However, the common practice of masking random patches of pixels exhibits certain failure modes, which can prevent learning meaningful high-level features, as required for downstream tasks. We propose an alternative masking strategy that operates on a suitable transformation of the data rather than on the raw pixels. Specifically, we perform principal component analysis and then randomly mask a subset of components, which accounts for a fixed ratio of the data variance. The learning task then amounts to reconstructing the masked components from the visible ones. Compared to local patches of pixels, the principal components of images carry more global information. We thus posit that predicting masked from visible components involves more high-level features, allowing our masking strategy to extract more useful representations. This is corroborated by our empirical findings which demonstrate improved image classification performance for component over pixel masking. Our method thus constitutes a simple and robust data-driven alternative to traditional masked image modeling approaches.", "sections": [{"title": "1. Introduction", "content": "In masked image modeling (MIM; Pathak et al., 2016), parts of an image are masked, and a model has to reconstruct the missing parts from the visible ones\u2014analogous to predicting hidden words in masked language modeling (Devlin, 2019). To succeed at this task, it is thought that the model must learn a meaningful representation of the visual content (Kong et al., 2023). Empirically, this approach indeed produces representations that perform well when fine-tuned on downstream tasks, such as image classification and semantic segmentation (Zhou et al., 2021; Bao et al., 2022; Xie et al., 2022; Baevski et al., 2022; Dong et al., 2023).\nMIM has been particularly effective when combined with vision transformers (ViT; Dosovitskiy et al., 2021). A prominent example is the masked autoencoder (MAE; He et al., 2021), consisting of a ViT encoder-decoder architecture and a masking strategy that randomly selects a fixed ratio of square image patches, see Figure 1 (top). The encoder processes the visible patches and the decoder aims to reconstruct the masked content from the inferred representation.\nWhile the inner workings of MAEs remain poorly understood (Zhang et al., 2022; Yue et al., 2023), Kong et al. (2023) suggested an explanation from a latent variable model perspective. By splitting an image into two separate views and asking the model to predict one from the other, MAEs are compelled to pick up any information that is shared across views. If the partition into views (i.e., the masking strategy) is chosen carefully, this shared information will include high-level latent variables, such as object class. Since solving common downstream tasks with a simple (e.g., linear) predictor precisely requires identifying such high-level features, this offers a possible explanation for the observed effectiveness of MAE representations.\nWith this in mind, we ask: Is masking random patches in pixel space the optimal strategy for MIM? In natural language, each word in a sentence tends to carry semantic information, and the information shared between sets of words often conveys the general message of the sentence. However, this does not necessarily apply to the visual domain, where individual pixels or entire patches commonly contain redundant information (e.g., background). Moreover, (small) objects can be masked out completely such that any information about them is lost and reconstruction becomes impossible, see Figure 3 (right). Some works have thus sought to devise more elaborate masking strategies which leverage auxiliary information such as learned or given image segmentations (Li et al., 2021; Kakogeorgiou et al., 2022; Shi et al., 2022). Without such prior knowledge or complex training pipelines to identify the structure of an image, randomly masking a fixed ratio of patches of pixels remains the default practice. Yet, relying on this masking strategy assumes-rather unrealistically-that the information shared between any random partition of pixel patches naturally aligns with high-level variables of interest (Kong et al., 2023).\nIn this work, we introduce an alternative data-driven masking strategy for MIM. Rather than working directly in pixel space, we propose to first project images into a latent space and then perform random masking on the transformed data. Among the infinitely many candidate transformations, we opt for simplicity and choose principal component analysis (PCA), a well-established linear pre-processing technique that is deterministic and (hyper)parameter-free. Following the PCA transformation, some principal components are masked-out and need to be reconstructed from the remaining visible components, see Figure 1 (bottom) for an example. Further, we leverage the fact that each principal component captures a known fraction of the data variance to inform our masking strategy. Specifically, we mask a random subset of principal components that accounts for a fixed ratio of the variance (Figure 4). The ratio of masked variance serves as a proxy for the complexity of the modeling task making it a more interpretable and more easily tunable hyperparameter than the ratio of masked patches (Figure 5, left). We combine this masking strategy with the MAE architecture and refer to the resulting method as principal masked autoencoder (PMAE), see Figure 2 for an overview.\nRelying on a transformation like PCA allows for partitioning the information in an image into a set of global features rather than into local patches of pixels, see Figure 1. This can help overcome the aforementioned failure modes of spatial masking where the input and target share too much (redundancy) or too little (impossibility) information. The space of principal components may, therefore, constitute a more meaningful domain for masking, resulting in more useful high-level representations.\nThis is consistent with recent work, highlighting the beneficial partitioning of image information by PCA. Balestriero &\nLeCun (2024) demonstrate that low-eigenvalue components capture features crucial for common downstream tasks (see also Figure 7), and Chen et al. (2024b) highlight the importance of the space in which image distortions are applied, referring to PCA as a valuable transformation to consider. To the best of our knowledge, our work is the first to leverage such insights to devise a simple, robust, and effective data-driven alternative to pixel-space masking in MIM.\nIn experiments on the CIFAR10, TinyImageNet and three medical MedMNIST datasets, we observe our approach (PMAE) to yield superior performance to the default strategy of spatial masking (MAE), while being less sensitive to the choice of masking ratio hyperparameter. These empirical findings support our claim that masking principal components instead of pixel patches can facilitate the learning of more meaningful high-level representations."}, {"title": "2. Background", "content": "Principal Component Analysis.\nPrincipal component analysis (PCA; Pearson, 1901; Hotelling, 1933) identifies components in data that exhibit the highest variance. Given a centered data matrix $X \\in \\mathbb{R}^{N \\times D}$ with $N$ observations of dimension $D$, PCA seeks weight vectors $v_l \\in \\mathbb{R}^D$ for $l = 1, ..., L < D$ (the principal components or PCs) that maximize the variance of the projections $Xv_l$, subject to orthogonality with previously found PCs and unit-length constraints. The solution to this problem is given by the eigenvalue decomposition of the empirical covariance matrix $\\Sigma = X^T X$, i.e., $\\Sigma = V\\Lambda V^T$, where $\\Lambda = \\text{diag}(\\lambda_1,...,\\lambda_D)$ contains the eigenvalues $\\lambda_1 > ... > \\lambda_D$, and $V \\in \\mathbb{R}^{D \\times D}$ contains the corresponding eigenvectors. The first $L$ PCs correspond to the eigenvectors of $\\Sigma$ with the largest eigenvalues, capturing the dominant modes of variation in the data, with the variance explained by each PC proportional to its eigenvalue $\\lambda_l$. While PCA is often used with $L < D$ for dimensionality reduction, we focus on the lossless case where $L = D$. The projection of $X$ onto its principal components (\u201cinto PC space\") is given by $X_{PC} = XV$, and the inverse transformation by $X = X_{PC}V^T$. By construction, PCA produces features which are uncorrelated ($X_{PC}^T X_{PC} = \\Lambda$). However, we emphasize that-except for special cases such as when the data is multivariate Gaussian-this does not imply independence. In general, the values of a subset of PCs are therefore (non-linearly) predictive of the values of other PCs. Our results in Section 5 and examples of reconstructions of masked principal components in Appendix A.2.1 demonstrate that this is the case for the natural and medical images we consider.\nRepresentation Learning. Representation learning (Bengio et al., 2013) aims at learning an embedding function or encoder $f: x \\rightarrow z$, which maps observations $x \\in \\mathbb{R}^D$ to representations $z \\in \\mathbb{R}^K$. These representations are meant to capture some of the explanatory factors underlying the data, thus making them well-suited for use in downstream tasks such as predicting a target variable $y$ (e.g., the class or location of objects), often thought of as simple functions of the high-level explanatory factors."}, {"title": "Masked Image Modeling.", "content": "Prominent approaches to representation learning in the image domain rely on the masked image modeling paradigm (Pathak et al., 2016; Zhou et al., 2021; He et al., 2021; Bao et al., 2022). Here we focus on the widely adopted MAE (He et al., 2021) as a representative of MIM. A MAE consists of: an encoder $f_\\phi$, parametrized by $\\phi$, which maps the visible portions of the input together with their positional embeddings to a representation; and a decoder $g_\\theta$, parametrized by $\\theta$, which reconstructs the missing parts from their positional embeddings and the representation produced by the encoder.\nGiven an observation $x \\in \\mathbb{R}^D$ and complementary binary masks $m$, $(1 \u2013 m) \\in \\{0, 1\\}^D$, which extract the visible and masked parts, respectively, the MAE objective is given by\n$\\mathcal{L}_{\\text{MAE}} (x, m; \\theta, \\phi) = \\\\||(1 - m) \\odot [g_\\theta \\circ f_\\phi (m \\odot x) - x] \\\\|_2^2$, (2.1)\nwhere $\\odot$ denotes element-wise multiplication and $\\circ$ denotes function composition. The parameters $(\\phi, \\theta)$ are optimized via stochastic gradient descent on Equation (2.1).\nThe mask $m$ partitions the $D$ pixels into two disjoint sets of $(1 - r)D$ visible and $rD$ masked out pixels, where $r$ is referred to as the masking ratio. Typically, $m$ is chosen by partitioning the image into square patches (thus introducing patch size as an additional hyperparameter) and then randomly selecting a ratio of $r$ patches to be masked.\nPrior work has relied on hyperparameter sweeps to identify the masking ratio and patch size that optimize downstream performance (He et al., 2021; Zhang et al., 2022). These efforts have led to the widely adopted approach of masking out 75% ($r = 0.75$) of patches of size 16 \u00d7 16 pixels.\nChallenges Resulting from Spatial Masking. Even though MIM with random spatial masking produces strong results on representation learning benchmarks (Dong et al., 2023), it is based on the rather unrealistic assumption that, given any partition of image patches into two disjoint sets, the information shared between views contains variables that are linearly predictive of downstream targets $y$ (Kong et al., 2023). As visualized in Figure 3, for some masks (such as the one in the middle) shared information indeed includes features such as object type. However, for other masks (such as the one on the right) predicting the correct class label from the visible patches is almost impossible. The latter therefore yields input-target pairs which do not contribute a useful self-supervised learning signal. Moreover, even for well-designed masks, many masked-out and visible patches contain redundant information such as background. This leads us to conjecture that spatial masking is a suboptimal approach to MIM that is not always well-aligned with common downstream tasks and may suffer from slow convergence and high sensitivity to hyperparameters, as suggested by prior work (He et al., 2021; Balestriero & LeCun, 2024) and confirmed in Figure 5."}, {"title": "3. Principal Masked Autoencoders", "content": "To address the challenges arising from spatial masking, we propose an alternative masking strategy and a corresponding MAE-variant which we refer to as principal masked autoencoder (PMAE). PMAE builds on the MIM learning paradigm but differs from prior approaches by performing the masking operation on a latent space. Specifically, we first apply an invertible transformation $t: \\mathbb{R}^D \\rightarrow \\mathbb{R}^D$ to an image $x$ and then mask and reconstruct at the level of $t(x)$, resulting in the following objective:\n$\\mathcal{L}_{\\text{PMAE}} (x, m; \\theta, \\phi) = \\\\|(1 - m) \\odot [t \\circ g_\\theta \\circ f_\\phi \\circ t^{-1} (m \\odot t(x)) - t(x)] \\\\|_2^2$ (3.1)\nSimilar to Equation (2.1), $f_\\phi$ is an embedding function which encodes visible parts of the image and $g_\\theta$ is a decoder which reconstructs the missing parts. Note that Equation (2.1) is recovered as a special case of Equation (3.1) if $t$ is chosen to be the identity mapping.\nDesign Choices. While Equation (3.1) can generally accommodate any invertible mapping $t$, this work specifically explores the use of PCA as a data transformation, i.e., we choose $t(x) = X_{PC} = xV$. Figure 2 provides a visual summary of PMAE based on PCA. We leave an investigation of alternative image transformations (e.g., the Fourier transform) for future work, see Section 8 for further discussion.\nFollowing MAEs, we use ViTs (Dosovitskiy et al., 2021) for both $f_\\phi$ and $g_\\theta$. Since these architectures are optimized for processing images, we project the visible components back into image space using the inverse PCA transformation $t^{-1}$ before encoding. Similarly, to reconstruct the missing components, the decoder output is projected back into PC space using $t$. ViTs are a common choice of architecture for MIM due to the use of binary masks in pixel space. These masks produce images with black-patch occlusions (see Figure 3), which are poorly aligned with the inductive biases of other architectures. By masking principal components instead of patches of pixels, the encoder inputs and reconstruction targets in PMAEs display smoother activation patterns (see Figure 1, bottom). PMAEs are therefore, in principle, also compatible with, e.g., convolutional networks. An exploration of other architectural choices for PMAE is left for future work.\nAnalogous to the MAE objective in Equation (2.1), the PMAE objective only penalizes reconstruction errors on the part masked out by $(1 \u2013 m)$. In Equation (3.1), the model output is projected into PC space, and the $l_2$ norm is computed between the predicted and ground truth masked-out principal components. In Appendix A.2.4, we explore an alternative objective for which the reconstruction error is calculated directly in the image space. Specifically, Equation (A.1) computes the $l_2$ norm between the decoder output and the masked-out principal components projected back into the image space. However, this alternative approach results in lower downstream performance, likely due to the stricter constraints placed on the decoder which, in this case, also needs to predict the effect of the visible components-akin to penalizing an MAE decoder for in-painting, rather than predicting perfectly black patches for the visible parts.\nMask design is also a crucial aspect of our approach. In MAEs, a fixed percentage of pixel patches is masked out. The masking ratio is usually chosen based on a hyperparameter tuning conducted for each individual dataset (see Figure 5, top). Instead in PMAE, we choose to systematically mask components that collectively account for a fixed percentage of the variance of the dataset. Figure 4 illustrates this process. After applying PCA, we randomly shuffle the order of principal components within each batch and divide"}, {"title": "4. Experimental Setup", "content": "We now outline the setup used to validate PMAE. Our experiments adapt the implementation proposed by He et al. (2021). Further experimental details and information about computational costs can be found in Appendix A.1.\nMask Design. Following MAE (He et al., 2021), we take random image patch masking as our baseline. Based on ablation studies from He et al. (2021), the standard practice involves masking out 75% of image patches (denoted as MAEstd). We also examine an oracle-based masking strategy (denoted as MAEocl), where the masking ratio is tuned to optimize linear probing downstream performance. This setting serves as an upper bound to the downstream performance. Additionally, we introduce a randomized masking approach, MAErd, in which the masking ratio is independently sampled within a range of 0.1 to 0.9 for each batch. This strategy is exempt from any hyperparameter tuning and offers insights into the robustness of methods under suboptimal hyperparameters.\nA similar approach is applied to PMAE, where we consider both oracle (PMAEocl) and randomized (PMAErd) masking strategies. In the oracle approach, we define the optimal percentage of variance to be masked based on linear probe downstream performance on a held-out dataset. In the randomized strategy, we ensure at least 10% and at most 90% of the data variance is masked out. This percentage is independently sampled for each batch. Figure 9 provides examples of the images obtained from these masking strategies.\nTraining & Evaluation. We train a ViT-Tiny (Touvron et al., 2021; Dosovitskiy et al., 2021) encoder and decoder backbones. Following He et al. (2021), we use image flipping and random image cropping as data augmentations. The decoder's output is normalized (He et al., 2021) for MAEs only as this did not result in a consistent performance gain for PMAE. We train representations for 800 epochs"}, {"title": "5. Results", "content": "In this section, we will outline and analyze the empirical advantages of PMAE compared to standard MAEs in image classification tasks. Specifically, we provide evidence that masking within the space of principal components facilitates the learning of discriminative features, resulting in improved classification performance. Our findings are supported by empirical evidence across five datasets, including two natural image datasets of 32 \u00d7 32 and 64\u00d764 resolutions, and three medical datasets taken from MedMNIST (Yang et al., 2023) of 64 \u00d7 64 resolution (see Figure 8 for examples of MedMNIST images).\nTable 1 presents the classification accuracy using both a linear probe and a MLP classifier. Across datasets, we observe substantial improvements with PMAEocl in linear probing compared to the standard MAEstd, with an average increase of 38% (+14 percentage points). Additionally, PMAEocl outperforms MAEocl by 20.3% (+9.5 percentage points). We observe similar trends with the randomized hyperparameter strategy: PMAE consistently outperforms MAE across all datasets, yielding an average performance increase of 29.9% (+5.4 percentage points), even when sub-optimal hyperparameters are used. These findings also extend to the non-linear evaluation setting, (see the middle part of Table 1). The bottom part of Table 1 presents the downstream performance in the fine-tuning setting. We continue to observe a performance gain brought by PMAE over MAE of 2.3% (+8.8 percentage points) on average. For two of the MedMNIST datasets we observe an equal and high (> 98%) performance for both methods, showcasing that both approaches easily complete these tasks.\nThese empirical findings lead to several conclusions. We observe that the recommended masking of 75% of image patches is largely sub-optimal across datasets. Figure 5 reports an ablation study of the masking ratios for MAE and PMAE. Figure 5 (top) shows that, across all five datasets, a 75% masking ratio is sub-optimal. For PMAE, the masking ratio is a more stable hyperparameter. Figure 5 (bottom) shows that across all evaluated datasets we observe the best or near-optimal performance for PMAE at 20% of the variance masked. We also validate the empirical benefits brought by PMAE. Interestingly, we notice that PMAE without any hyperparameter tuning (PMAErd) outperforms MAE with optimum masking ratio in all but one case (i.e., CIFAR10). Finally, masking 20% of the variance in PMAE leads to substantial performance gains over the standard approach of using MAEs with a 75% masking ratio.\nFigure 5 (right) shows the downstream performance over training epochs, where PMAE surpasses MAE as early as 200 epochs. Figures for other datasets are provided in Appendix A.2. Moreover, Figure 6 examines how downstream accuracy varies with different masking ratios. Our findings indicate that PMAE demonstrates similar or lower standard errors across masking ratios compared to MAE. Overall, these results highlight that the masking strategy of PMAE better aligns image reconstruction with image classification tasks compared to the MAE objective."}, {"title": "6. Understanding PMAE", "content": "In this section, we aim to provide more intuition as to why masking components rather than image patches leads to a more robust objective. In Section 2, we discuss the hypothesis under which MIM operates (Kong et al., 2023) and present an example failure case of spatial masking in Figure 3. We highlight how masking pixels can lead to a misalignment between the MIM objective and the learning of meaningful representations. If all patches covering an object are masked out, it is uncertain whether the remaining patches share any information with the object. On the contrary, if the masked-out information is redundant with the information carried by visible patches, it is likely that the information shared does not contain the object class but rather perceptual features (e.g., colors or textures).\nDifferent from spatial masking, masking principal components leads to the removal of global image features, instead of only acting locally as in spatial masking. Figure 7 serves as an example highlighting the correspondence between principal component and perceptual features. In this example, the principal components with the highest eigenvalues capture the colors within the image while the bottom PCs highlight the edges. Early work in image processing (Turk & Pentland, 1991) has demonstrated this connection between an image's dominant modes of variation and its low spatial frequency components, providing further intuition for how information is partitioned in the space of PCs of natural images.\nBy removing a subset of principal components, PMAE prevents the removal of all the information characterizing an object and prevents redundant information to remain after masking. Instead, PMAE drops a set of unique image components. By taking advantage of the information partitioning in PCA, PMAE thereby mitigates MAE's failure cases, ultimately leading to increased accuracy. Although the potential of the principal component space for MIM (Balestriero & LeCun, 2024) or Image Denoising (Chen et al., 2024b) has been recently explored, our work is the first to propose an effective masking strategy that directly leverages PCA."}, {"title": "7. Related Work", "content": "Self-supervised learning. Self-supervised learning (SSL) leverages auxiliary tasks to learn from unlabeled data, often outperforming supervised methods on downstream tasks. SSL can be divided into two categories: discriminative and generative (Liu et al., 2021). Discriminative methods (Chen et al., 2020a; Caron et al., 2021) focus on enforcing invariance or equivariance between data views in the representation space, while generative methods (He et al., 2021; Bizeul et al., 2024) rely on data reconstruction from, often, corrupted observations. Though generative SSL historically lagged in performance, recent work has bridged the gap by integrating strengths from both paradigms (Assran et al., 2022; Dong et al., 2023; Oquab et al., 2023; Chen et al., 2024a; Lehner et al., 2023). Interestingly, recent discriminative methods employ cropping strategies to create distinct data views (Oquab et al., 2023; Assran et al., 2023), which is reminiscent of image masking. Balestriero & LeCun (2024) point out the misalignment between auxiliary and downstream tasks in reconstruction-based SSL and suggest novel masking strategies to help realign these objectives.\nMasked Image Modelling. MIM extends the successful masked language modeling paradigm to vision tasks. Early methods, such as Context Encoder (Pathak et al., 2016), used a convolutional autoencoder to inpaint a central region of the image. The rise of Vision Transformers (ViTs) (Dosovitskiy et al., 2021) has driven significant advancements in MIM. BEIT (Zhou et al., 2021; Bao et al., 2022) combines a ViT encoder with image tokenizers (Ramesh et al., 2021) to predict discrete tokens for masked patches. SimMIM (Xie et al., 2022) simplifies the task by pairing a ViT encoder with a regression head to directly predict raw pixel values for the masked regions. MAE (He et al., 2021) introduces a more efficient encoder-decoder architecture, with a shallow decoder. MIM's domain-agnostic masking strategies have also proven effective in multi-modal tasks (Baevski et al., 2022; Bachmann et al., 2022).\nMask Design Strategies. A critical component of the masked image modeling paradigm is the design of effective masking strategies. Early MIM approaches have relied on random spatial masking techniques, such as masking out the central region of an image (Pathak et al., 2016), image patches (He et al., 2021; Xie et al., 2022), and blocks of patches (Bao et al., 2022). Inspired by advances in language modeling, recent efforts have explored semantically guided mask design. Li et al. (2021) use self-attention maps to mask irrelevant regions, while Kakogeorgiou et al. (2022) focus on masking semantically rich areas. Shi et al. (2022) design masks through adversarial learning, where the resulting masks resemble semantic maps, a concept extended by Li et al. (2022a) through progressive semantic region masking. Further advancing this direction, Wang et al. (2023) and Madan et al. (2024) introduce curriculum learning-inspired mask design methods. These methods often require additional training steps, components, or more complex objectives. More closely related to our work, Chang et al. (2022); Chen et al. (2024b) explore the use of pre-existing image representations for asked Image Modeling and image denoising. Chen et al. (2024b) introduce additive Gaussian noise to principal components as an alternative to the traditional Denoising Autoencoders. Chang et al. (2022) utilize masked token modeling by leveraging the discrete latent space of a pre-trained VQVAE to develop an image generation model."}, {"title": "8. Discussion", "content": "In this work, we have investigated different masking strategies for Masked Image Modelling (MIM). To this end, we have introduced the Principal Masked Autoencoder (PMAE) as an alternative to spatial masking. PMAE is rooted in principal component analysis (PCA; Pearson, 1901; Hotelling, 1933), which is a widely used data-driven, linear and deterministic transformation. Unlike recent alternatives that require additional supervision, learnable components, or complex training pipelines (Li et al., 2021; 2022a; Kakogeorgiou et al., 2022; Li et al., 2022b), PMAE stays close to the core principles of MIM: the combination of a randomized masking strategy and an encoder-decoder architecture. Despite its simplicity, we demonstrate that PMAE yields substantial performance improvements over spatial masking on image classification tasks for CIFAR10, TinyImageNet and MedMNIST datasets. Further, in a PMAE, the masking ratio\u2014typically a sensitive and difficult-to-tune hyperparameter in MIM-appears more robust and has a natural interpretation as the ratio of variance explained by the masked input.\nSince PCA, or its generalisation kernel PCA (Sch\u00f6lkopf et al., 1997), is easily applicable to any data modality, our proposal of masking principal components is not specific to MIM. Instead, it can be viewed as a general strategy that should also be applicable to other types of modalities beyond images, as well as to other self-supervised learning (SSL) approaches. Indeed, data masking is commonly adopted in discriminative SSL methods. Whereas early approaches, such as SimCLR (Chen et al., 2020a) or MoCo (Chen et al., 2020b), relied on combinations of image transformations (e.g., color jitter, flips, crops, etc.) as data augmentation strategies, more recent state-of-the-art methods like DINO (Caron et al., 2021; Oquab et al., 2023) and I-JEPA (Assran et al., 2023) have shifted to relying solely on image cropping, which can be considered a type of masking. The integration of principal component masking instead of image cropping into such SSL pipelines constitutes a promising future direction of research.\nIn the present work, we have focused on PCA subspaces as meaningful masking spaces. However, our core idea of masking a transformed version of the data (rather than the raw data) can be viewed as laying the groundwork for other, more generic approaches to information masking for self-supervised representation learning. Moving beyond PCA, a natural extension would be to learn a suitable latent space in which the masking is performed. This route can potentially leverage recent theoretical insights (Kong et al., 2023) by more explicitly enforcing that the shared information between visible and masked-out latent components contains high-level latent variables that are most useful for the downstream tasks of interest. Other off-the-shelf transformations, such as the Fourier transform (Bracewell & Kahn, 1966), Wavelet transform (Daubechies, 1992), Kernel principal component analysis (Sch\u00f6lkopf et al., 1997), or Diffusion Maps (Coifman & Lafon, 2006), represent alternative candidate transformations. Future research should explore whether the properties of these spaces provide comparable or additional advantages over PCA. Preliminary results with non-linear transformations, namely Kernel PCA, presented in Appendix A.2.6, demonstrate performance gains over PMAE, motivating further exploration. A particularly appealing aspect of some of these methods (e.g., Fourier & Wavelet transforms and Diffusion Maps) is the use of fixed bases, which could eliminate the computational overhead of PCA-whose cost scales cubically with the data dimensionality\u2014and improve scalability to larger datasets.\nOur work demonstrates the potential of using PCA subspaces to guide image representation learning. Exploring alternative masking spaces with similar properties presents an exciting avenue for future research, offering the possibility of developing unsupervised tasks that encourage the learning of features more closely aligned with human perception."}, {"title": "A. Appendix", "content": "A.1. Experimental Setup\nA.1.1. Datasets\nCIFAR-10 is a widely used benchmark dataset containing 50,000 training and 10,000 validation 32 \u00d7 32 RGB images depicting 10 object classes, such as airplanes, cars, and animals.\nTinyImageNet is a subset of the ImageNet dataset, containing 200 classes of 64 \u00d7 64 RGB images. It consists of 100,000 training images and 10,000 validation images, making it a challenging benchmark for classification tasks with more fine-grained object categories compared to CIFAR-10. Examples images are depicted in Figure 8\nThe MedMNIST (Yang et al., 2023) dataset is a collection of medical imaging datasets, each focusing on different types of biomedical data. In this work, three MedMNIST datasets are used:\nBloodMNIST consists of 12,000 training and 1,700 validation 64\u00d764 RGB images across 8 classes and represents microscopic images of blood cells, used for hematology classification tasks.\nDermaMNIST contains 7,000 training and 1,000 validation 64\u00d764 RGB images of skin samples, each part of one out of 7 types of skin diseases.\nPathMNIST comprises 90,000 training and 10,000 validation 64 \u00d7 64 RGB images across 9 classes and depicts histopathological images of colorectal cancer tissue, aiding in classification tasks relevant to pathology.\nWe apply an equivalent data augmentation strategy to all datasets and for all learning objectives during training; Following He et al. (2021), our augmentation strategy consists of a random cropping followed by image resizing using bicubic interpolation. The scale of the random cropping is fixed to [0.2, 1.0]. We add horizontal flipping and we normalize images using each dataset's training mean and standard deviation; We keep these data augmentations during training and evaluation. We find that the use of data augmentations during the evaluation leads to a substantial performance drop for PMAE but we do keep these augmentations for fair comparisons. For all datasets and methods, we define image patches as patches of 8\u00d78 pixels."}, {"title": "A.1.2. Mask Design", "content": "Figure 9 depicts examples of the different masking strategies explored in Section 5. For MAE with std masking, the masking ratio is fixed to 75-75% of patches are masked out. For MAE and PMAE with ocl masking, we tune the masking ratio to find the optimal ratio for each dataset and method. For rd masking, the masking ratio is sampled uniformly from the range [10,90] for each new batch of data during training."}, {"title": "A.1.3. Model Architecture", "content": "We train a tiny Vision Transformer encoder architecture (ViT-T) with image patch size 8\u00d78 for all datasets (ViT-T/8). The specifics of this architecture can be found in Figure 10. Note that while for MAE, we keep the normalized pixel values of each masked patch as reconstruction targets (He et al., 2021), we find it to not have a clear positive impact for PMAE and hence enforce the reconstruction of the raw target values for PMAE."}, {"title": "A.1.4. Training Hyperparameters", "content": "We train the ViT-T encoder-decoder architecture for 800 epochs with the hyperparameters found in Table 2. These hyperparameters are taken from (He et al., 2021). We use the linear lr scaling rule: lr = base lrxbatchsize / 256 (Goyal et al., 2017). Note that for our oracle masking settings, we conduct ablation studies across a masking ratio range of [10, 90]."}, {"title": "A.1.5. Evaluation Hyperparameters", "content": "We evaluate the learned representation (i.e., [CLS"}]}