{"title": "LENGTH-INDUCED EMBEDDING COLLAPSE IN TRANSFORMER-BASED MODELS", "authors": ["Yuqi Zhou", "Sunhao Dai", "Zhanshuo Cao", "Xiao Zhang", "Jun Xu"], "abstract": "Text embeddings enable various applications, but their performance deteriorates on longer texts. In this paper, we find that the performance degradation is due to a phenomenon called Length Collapse, where longer text embeddings collapse into a narrow space. This collapse results in a distributional inconsistency between embeddings of different text lengths, ultimately hurting the performance of downstream tasks. Theoretically, by considering the self-attention mechanism inherently functions as a low-pass filter, we prove that long sequences increase the attenuation rate of the low-pass filter effect of the self-attention mechanism. With layers going deeper, excessive low-pass filtering causes the token signals to retain only their Direct-Current (DC) component, which means the input token feature maps will collapse into a narrow space, especially in long texts. Based on the above analysis, we propose to mitigate the undesirable length collapse limitation by introducing a temperature in softmax(\u00b7), which achieves a higher low-filter attenuation rate. The tuning-free method, called TempScale, can be plugged into multiple transformer-based embedding models. Empirically, we demonstrate that TempScale can improve existing embedding models especially on long text inputs, bringing up to 0.53% performance gains on 40 datasets from Massive Text Embedding Benchmark (MTEB) and 0.82% performance gains on 4 datasets from LongEmbed, which specifically focuses on long context retrieval.", "sections": [{"title": "INTRODUCTION", "content": "Text embeddings-dense vectors that preserve the semantic information of given texts-have become fundamental to many downstream natural language processing (NLP) applications, including text analysis (Aggarwal & Zhai, 2012; Angelov, 2020), question answering (Tan et al., 2023; Xu et al., 2024), web search (Zhao et al., 2023; Yates et al., 2021), and retrieval-augmented generation (Gao et al., 2023; Fan et al., 2024). Typically, embeddings are generated by pre-trained language models (PLMs), which produce fixed-dimensional embeddings regardless of the input text length. In practice, we expect PLMs to perform consistently on texts of varying lengths in any downstream applications.\nUnfortunately, we observe that popular transformer-based embedding models perform poorly on longer texts. As shown in Figure 1a, using the classification task on the IMDB dataset from the Massive Text Embedding Benchmark (MTEB) (Muennighoff et al., 2023) leaderboard as an example, we evaluate the performance of mainstream embedding models on test sets grouped by different text lengths. The experimental results reveal that models of different capabilities and context window sizes consistently exhibit performance degradation as text length increases. For instance, the BGE (Xiao et al., 2023) model's classification accuracy drops significantly from 75.6% in the length range [0, 100) tokens to 59.0% in the range [400, 500) tokens, indicating a substantial decline of 16.6% points.\nWe attribute this performance degradation to a biased behavior of embedding models: embeddings of longer texts tend to cluster together, a phenomenon we term as length collapse. To verify this, we conduct controlled experiments depicted in Figures 1b and 1c. Figure 1b shows that embeddings of longer texts are more densely clustered near the origin in the dimensionally reduced embedding space, indicating a collapse that reduces variance among embeddings. Details of the rewriting process can be found in Appendix E.1. Figure 1c further demonstrates that embeddings of longer texts exhibit higher cosine similarity to each other, leading to smaller differences between them. This collapse results in a"}, {"title": "2 LONG INPUTS LEAD TO HOMOGENEOUS EMBEDDINGS", "content": "In this section, we first present our notations and define the problem. Then, we introduce the Transformer structure in the mainstream embedding model and briefly explain the Fourier transform used in (Wang et al., 2022b). Based on the Fourier transform, we show that the attention mechanism acts as a low-pass filter, and longer input sequences strengthen the filtering effect, leading to increasingly similar representations. This results in cosine similarity increasing with the length of the text.\n2.1 PRELIMINARIES AND BACKGROUND\nNotations. Let $X \\in \\mathbb{R}^{n \\times d}$ denote the input feature matrix, where $n$ is the number of input tokens, and $d$ is the embedding dimension. Let $x_i \\in \\mathbb{R}^{d}$ represent the vector corresponding to the $i$-th token and $z_j \\in \\mathbb{R}^{n}$ represent the token sequence corresponding to the $j$-th dimension, where $i \\in \\{1, ..., n\\}$ denotes the $i$-th row, and $j \\in \\{1, ..., d\\}$ denotes the $j$-th column.\nTransformer Architecture. In most modern embedding models (Chen et al., 2024; Xiong et al., 2021), a bidirectional transformer architecture based on attention mechanisms is widely used. These models generally consist of three key components: the embedding layer, a stack of transformer encoder blocks incorporating Multi-Head Self-Attention (MSA) and Feed-Forward Networks (FFN), and a pooling layer at the end to generate the final embedding representation of the input sequence. The Self-Attention(SA) module is the fundamental part of MSA, which takes inputs consisting of the token representations $X$ from the previous layer, and it encodes each token by aggregating information from other tokens based on the attention scores, formulated as below (Vaswani, 2017):\n$SA(X) = softmax(\\frac{(XW_Q(XW_K)^T)}{\\sqrt{d}})XW_V$, (1)\nwhere $W_K \\in \\mathbb{R}^{d \\times d_k}, W_Q \\in \\mathbb{R}^{d \\times d_q}, W_V \\in \\mathbb{R}^{d \\times d_v}$ are the key, query and value weight matrices, respectively. The dimensions of the query and key vectors are denoted by $d_q$ and $d_k$, while $\\sqrt{d}$ serves as a scaling factor to adjust the magnitude of the dot product. The function $softmax(\\cdot)$ normalizes the attention scores row-wisely. Multi-Head Self-Attention (MSA) consists of SA heads, with their outputs combined through a linear projection:\n$MSA(X) = [SA_1(X) \\dots SA_H(X)] W_O$,\nwhere the subscripts indicate the number of self-attention (SA) heads, $H$ represents the total number of heads, and $W_O \\in \\mathbb{R}^{Hd \\times d}$ projects the combined multi-head outputs back to the hidden dimension.\nFourier Analysis. We use Fourier transform as the main analytic tool in this paper as used in (Wang et al., 2022b). Let $F : \\mathbb{R}^{n} \\rightarrow \\mathbb{C}^{n}$ represent the Discrete Fourier Transform (DFT), with its inverse, the Inverse Discrete Fourier Transform (IDFT), denoted by $F^{-1} : \\mathbb{C}^{n} \\rightarrow \\mathbb{R}^{n}$. Applying $F$ to a token sequence $z$ is equivalent to left multiplying a DFT matrix, where $k$-th row of DFT matrix denotes the Fourier basis corresponding to a certain frequency $f_k = [ \\frac{e^{2\\pi j(k-1)\\cdot 0}}{\\sqrt{n}} \\dots \\frac{e^{2\\pi j(k-1)\\cdot (n-1)}}{\\sqrt{n}} ]^T \\in \\mathbb{R}^{n}$, and $j$ is the imaginary unit. Let $\\tilde{z} = Fz$ represent the spectrum of $z$, where $\\tilde{z}_{dc} \\in \\mathbb{C}$ and $\\tilde{z}_{hc} \\in \\mathbb{C}^{n-1}$ correspond to the first element and the remaining elements of $\\tilde{z}$, respectively. We define the Direct-Current (DC) component of the input sequence $z$ as $DC[z] = \\tilde{z}_{dc}f_1 \\in \\mathbb{C}^{n}$, and the complementary high-frequency component as $HC[z] = [ f_2 \\dots f_n ] \\tilde{z}_{hc} \\in \\mathbb{C}^{n}$, consistent with the definition in (Wang et al., 2022b).\nIn signal processing, a low-pass filter is a system that attenuates the high-frequency components of a signal while retaining the low-frequency components. In this paper, we specifically define a low-pass filter as one that preserves only the DC component $DC[z]$, while diminishing all other high-frequency components $HC[z]$. A more precise definition is provided in Definition 1.\nDefinition 1. Let $f : \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}$ be an endomorphism with $f^t$ obtained by applying $f$ for $t$ times. The function $f$ acts as a low-pass filter if and only if $\\lim_{t \\rightarrow \\infty} \\frac{||HC[f^t(z)]||_2}{||DC[f^t(z)]||_2} = 0$ for all $z \\in \\mathbb{R}^{n}$.\nFor additional background information, please refer to Appendix A.\n2.2 THEORETICAL ANALYSIS ON LENGTH COLLAPSE\nOverview. In this subsection, we aim to demonstrate that increasing the sequence length $n$ accelerates the rate of low-pass filtering in the attention matrix, leading to greater similarity in text embeddings"}, {"title": "3 MITIGATING LENGTH COLLAPSE VIA TEMPERATURE SCALING", "content": "As discussed in Section 2.2, self-attention matrix perform low-pass filtering, which narrows the filter space embedding model can express. Furthermore, a longer sentence length will cause a more narrow filter space, which results in the embedding model's failure in long texts. To address the problem, an intuitive idea is to increase the diversity of embeddings for long texts, making them more distinguishable within the space. Inspired by Eqn. 5, we propose a scaling technique, called Temperature Scaling (TempScale), which directly manipulates the attention map by multiplying $T_s$ by a constant temperature $\\tau$ less than 1. This slows down the filtering process by increasing $\\sigma_a$.\nBased on Eqn. 4, a smaller $\\tau$, results in a larger $\\sigma_s$ which will further result in a larger $\\sigma_a$ based on Eqn. 3. In other words, the low-pass filtering rate of the attention matrix decreases as $T_s$ decreases. Inspired by this, TempScale introduces a temperature $\\tau$ to re-scale the self-attention matrix $A$. Formally, let $A = softmax(\\frac{XW_Q(XW_K)^T}{\\sqrt{d}})$ denote a self-attention matrix. To decrease the low-pass filtering rate of $A$, we apply a temperature coefficient $\\tau$ to the logits before performing the softmax operation. Specifically, for each row $p_i$ in the attention score matrix $\\frac{XW_Q(XW_K)^T}{\\sqrt{d}}$, we compute the scaled logits by dividing by a temperature $\\tau \\in (0, 1]$, and then apply the softmax function to obtain the attention weights:\n$A = softmax(\\frac{XW_Q(XW_K)^T}{\\tau\\sqrt{d}})$. (6)\nwhere a lower $\\tau$ results a smaller $T_s$ and furthur a smaller rate of low pass filtering.\nIntuitive Explanation. We present the effects of temperature scaling on two extreme cases to illustrate how TempScale works. As shown in Figure 4, when scaling matrix $A$ with a relatively large $\\tau$, the elements in the final matrix $A$ can be approximated as nearly equal. In this scenario, matrix"}, {"title": "4 EXPERIMENTS", "content": "In this section, we first conduct experiments to validate the effectiveness of our TempScale on MTEB (Muennighoff et al., 2023) and LongEmbed. Then we analyze how different tasks can benefit from TempScale and how different $\\tau$ affect performance on datasets from LongEmbed (Zhu et al., 2024). Finally, we adaptively set $\\tau$ based on text length $n$ to validate our theoretical analysis.\n4.1 CAN TEMPSCALE BENEFIT EMBEDDING MODELS?\nExperiment Settings. For long context retrieval, we use 4 real-world tasks curated from long-form QA and summarization in LongEmbed (Zhu et al., 2024) to access embedding models' factuality in short-query and long-document settings. For other embedding tasks, we consider six other tasks, including classification, clustering, summarization, semantic textual similarity (STS), retrieval, and reranking, comprising 36 datasets from MTEB (Muennighoff et al., 2023). To comprehensively evaluate TempScale, we select several representative Transformer-based embedding models, including: (1) ANCE (Xiong et al., 2021); (2) GTR (Ni et al., 2022); (3) GIST (Solatorio, 2024); (4) BGE (Xiao et al., 2023); (5) E5 (Zhu et al., 2024). These models are fine-tuned from various pretrain language models, including BERT (Kenton & Toutanova, 2019), RoBERTa (Liu et al., 2019), and T5 (Chung et al., 2024). More descriptions of the datasets and models can be found in Appendix D. When evaluating the embedding models, we set the same $\\tau$ on the softmax function for the attention modules across all layers within the range of {0.1, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. Unless otherwise specified, all temperatures used for TempScale are set to 0.8. The metrics used for different tasks are consistent with MTEB and can be found in Appendix C.\nResults. We select the optimal temperature $\\tau$ for each model based on their performance across all tasks and organize the experimental results as shown in Table 1. The results show that these embedding"}, {"title": "4.2 FURTHER ANALYSIS", "content": "models can benefit from our proposed method TempScale across various general tasks with an average improvement of 0.53% and across long context retrieval datasets with an average improvement of 0.82%. In these tasks, some datasets, such as those for STS, have an average text length of only around 10 tokens, whereas the texts in LongEmbed generally exceed 1000 tokens. Our method proves effective across both, demonstrating that using TempScale not only prevents long text collapse but also enhances the embeddings for short texts, leading to improved performance in downstream tasks. In addition, larger context window sizes lead to greater performance improvements, with E5 showing the highest improvement of 1.07%. This could be attributed to larger windows providing more and longer data for adjustment, which allows for greater performance improvement. Overall, our method, TempScale, is designed specifically for long texts but is also capable of improving model performance in general embedding tasks.\nHow do the classification and clustering tasks benefit from TempScale? For data grouping tasks like classification and clustering, in the case of $N$-class tasks, we can think of the model as learning $N$ classification boundaries. The farther the text embedding is from the boundary, the closer the output probability approaches 1 or 0. As shown in Figure 6 (left), we plot the entropy of the model output probabilities across different length intervals. The model outputs higher entropy for longer texts, which may be because the embeddings of longer texts are positioned closer to the center of the space as described in Figure 1b, resulting in a shorter distance to various classification boundaries. Meanwhile, in Figure 6 (right), we can also observe that accuracy and entropy follow the same trend: the model achieves higher accuracy when it has lower entropy. In other words, the model performs better if the text embeddings are farther from the boundary. After applying TempScale, a decreased entropy will result in increased accuracy. This further supports the relationship between entropy and accuracy in classification tasks. Moreover, if the model exhibits a more severe length collapse phenomenon, meaning a greater performance drop on longer texts, the more performance improvement it experiences after applying TempScale. This suggests that one possible reason TempScale is effective on shorter text datasets is that it adjusts the distribution differences between texts of varying lengths. As shown in Figure 1c, after applying TempScale, the distribution of text embeddings across different length intervals becomes more uniform, which facilitates the model in learning length-agnostic parameters."}, {"title": "5 RELATED WORK", "content": "Text Embedding Models. Embeddings are generated by pre-trained language models (PLMs), which produce fixed-dimensional embeddings regardless of the input text length, laying the foundation of numerous NLP applications. Early works on text embeddings lack context awareness and are"}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "In this paper, we identify the phenomenon of length collapse, where embeddings of longer texts tend to cluster together and propose practical solutions through Fourier domain analysis. Our theoretical findings suggest that Multi-Head Self-Attention inherently performs stronger low-pass filtering as the token sequence length increases, leading to patch uniformity issues in longer sentences. To this end, we propose a technique called TempScale, which effectively reduces the low-pass filtering effect by introducing a temperature parameter when applying softmax in the self-attention matrix. Our extensive experiments validate the effectiveness of our methods and enhance the performance of general embedding models on the MTEB and LongEmbed benchmarks. Overall, TempScale is a crucial advancement in enhancing the performance of embedding models on longer texts.\nFuture work includes: 1) LLM-based embedding model: Current work focuses on embedding models with bidirectional attention mechanisms. In the future, we plan to investigate length collapse in LLM-based embedding models that utilize unidirectional attention mechanisms; 2) Tuning method: The work in this paper relies on existing models and pre-trained parameters without using a training dataset. In future work, we will focus on tuning the temperature for additional improvements. 3) Analysis on more modules: We primarily investigate the impact of the self-attention module on length collapse in this paper. Moving forward, we plan to explore the effects of additional modules in transformers such as LayerNorm and FFN."}]}