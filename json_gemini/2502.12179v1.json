{"title": "Identifiable Steering via Sparse Autoencoding of Multi-Concept Shifts", "authors": ["Shruti Joshi", "Andrea Dittadi", "S\u00e9bastien Lachapelle", "Dhanya Sridhar"], "abstract": "Steering methods manipulate the representations of large language models (LLMs) to induce responses that have desired properties, e.g., truthfulness, offering a promising approach for LLM alignment without the need for fine-tuning. Traditionally, steering has relied on supervision, such as from contrastive pairs of prompts that vary in a single target concept, which is costly to obtain and limits the speed of steering research. An appealing alternative is to use unsupervised approaches such as sparse autoencoders (SAEs) to map LLM embeddings to sparse representations that capture human-interpretable concepts. However, without further assumptions, SAEs may not be identifiable: they could learn latent dimensions that entangle multiple concepts, leading to unintentional steering of unrelated properties. We introduce Sparse Shift Autoencoders (SSAEs) that instead map the differences between embeddings to sparse representations. Crucially, we show that SSAEs are identifiable from paired observations that vary in multiple unknown concepts, leading to accurate steering of single concepts without the need for supervision. We empirically demonstrate accurate steering across semi-synthetic and real-world language datasets using Llama-3.1 (Llama Team et al., 2024) embeddings.", "sections": [{"title": "1. Introduction", "content": "As increasingly powerful large language models (LLMs) are deployed and widely used, researchers seek to steer their behaviour at inference time, without the need to perform expensive model updates, as with fine-tuning. Steering refers to perturbing an LLM's representation of an input prompt (potentially at different layers in the LLM) so that the LLM responds with some desired property. For example, recent work has estimated steering vectors for sycophancy (Rimsky et al., 2024), sentiment (Subramani et al., 2022), machine unlearning (Li et al., 2024b), and even exploration in the case of LLM agents (Rahn et al., 2024). As such, research that improves the ease of steering has the potential to impact the alignment of LLMs to human values, such as harmlessness and helpfulness. However, a crucial assumption made by related work is access to supervision, either from target responses to prompts (c.f., Subramani et al. (2022)), or from contrastive pairs of prompts that differ by a single target concept like truthfulness (c.f., Turner et al. (2024); Rimsky et al. (2024)). Since supervision is costly, designing steering methods that can learn with limited supervision that is easily accessible, could speed up steering research. To this end, this paper focuses on learning to steer from paired samples that vary in multiple unknown concepts.\nTo better understand the challenges with unsupervised learning, consider a naive approach to learn to steer: obtain a dataset of LLM embeddings of text and to this data, fit an autoencoder that learns to map LLM embeddings to some latent space and decode the original vectors back. Assuming the learned latent space captures human-interpretable concepts, steering involves encoding an embedding, perturbing a single dimension in the latent space, and decoding to obtain the steered embedding. This example of an unsupervised approach is a simplified version of existing approaches to interpreting LLMs using sparse autoencoders (Cunningham et al., 2023). Why might this unsupervised approach fail to output accurate steered embeddings? The problem is that by fitting observations alone, autoencoders are generally not identifiable: infinitely many encoders and decoders can fit the observations equally well. In the context of steering, this means that in general the learned latent space does not capture individual concepts but rather some nonlinear entangling thereof so that sparse latent perturbations steer multiple concepts at once and not the target concept alone.\nIn contrast to the naive unsupervised learning approach, the field of causal representation learning (CRL) proposes numerous approaches to identifiable generative models (see Section 4 for related work). However, these approaches focus on generative models of low-level data such as im-"}, {"title": "2. Identifiable learning of steering vectors", "content": "We begin by formalizing steering before developing a theory about learning steering vectors from multi-concept changes. Lowercase bold letters (x) denote vectors, uppercase bold letters (A) denote matrices, sets are denoted using uppercase letters (S) or calligraphic letters S when they refer to domains of random variables. Please refer to Apx. A.1 for notation followed in the paper."}, {"title": "2.1. Preliminaries", "content": "We assume that the observed data consists of vectors $\\mathbf{x} \\in \\mathcal{X} \\subset \\mathbb{R}^{d_x}$ generated from underlying concept representations $\\mathbf{c} \\in \\mathcal{C} \\subseteq \\mathbb{R}^{d_c}$ through an unknown generative process $g : \\mathcal{C} \\rightarrow \\mathcal{X}$. Each data point x is produced by a corresponding ground-truth concept representation c such that $\\mathbf{x} = g(\\mathbf{c})$, where each component $c_k$ corresponds to some abstract concept such as the language a text might be written in or its overall sentiment. While we cannot observe the concept representation c of an observation x, we have access to learned representations $\\mathbf{z} = f(\\mathbf{x})$, where the function $f : \\mathcal{X} \\rightarrow \\mathcal{Z} \\subseteq \\mathbb{R}^{d_z}$ maps observations x to $d_z$-dimensional real vectors $\\mathbf{z} \\in \\mathcal{Z}$, known as their embeddings. For example, f(x) can be learned via next-token prediction as with LLMs. However, there is no guarantee that $f = g^{-1}$ (the function g might not even be invertible). Consequently, true concept representations c are encoded in the representations z through the unknown composite function $\\mathbf{z} = f(g(\\mathbf{c}))$ ."}, {"title": "2.2. Problem formulation", "content": "In this paper, we are interested in learning steering functions that ideally allow us to manipulate the concepts that are represented in an observation such as a text. To directly manipulate concept k in the space $\\mathcal{C}$, we define the per-"}, {"title": "2.3. Learning steering vectors via multi-concept shifts", "content": "In practice, a steering function $\\Phi_{\\lambda, k}$ can be learned via supervised learning given a dataset comprising of carefully designed paired observations (x, xk), in which a single concept changes between x and $\\mathbf{x}_k$ (Shen et al., 2017; Turner et al., 2024; Rimsky et al., 2024). However, such a dataset might be difficult to acquire. This raises the following question, at the heart of our contribution:\nHow can we learn a steering function $\\Phi_{\\lambda, k}$, with a dataset of paired observations (x, x) in which multiple concepts vary?\nData-generating process. Following Locatello et al. (2020b), we consider paired observations (x, x) assumed to be sampled from the following generative process:\n$\\begin{aligned}\n    \\mathcal{S} &\\sim p(\\mathcal{S}), \\\\\n    (\\mathbf{c}, \\tilde{\\mathbf{c}}) &\\sim p(\\mathbf{c}, \\tilde{\\mathbf{c}} \\mid \\mathcal{S}), \\\\\n    \\mathbf{x} &= g(\\mathbf{c}),  \\\\\n    \\tilde{\\mathbf{x}} &= g(\\tilde{\\mathbf{c}}), \\\\\n\\end{aligned}$\nwhere $\\mathcal{S} \\subseteq \\{1, ..., d_c\\}$ denotes the subset of concepts that vary between x and $\\tilde{\\mathbf{x}}$. More precisely, $p(\\mathbf{c}, \\tilde{\\mathbf{c}} \\mid \\mathcal{S})$ is such that, with probability one, $c_k = \\tilde{c}_k$ for all $k \\notin \\mathcal{S}$. We also define the difference vectors $\\delta^\\dagger := f(\\mathbf{x}) - f(\\tilde{\\mathbf{x}}) = \\mathbf{z} - \\tilde{\\mathbf{z}}$ and $\\delta^\\circ := \\tilde{\\mathbf{c}} - \\mathbf{c}$. To rephrase what we just said differently, we have that $\\delta^\\circ_k = 0$ for all $k \\notin \\mathcal{S}$. Crucially, across each"}, {"title": "Varying concepts.", "content": "In what follows, it will be useful to define $\\mathcal{V} \\subset \\{1, . . ., d_c \\}$ to be the set of varying concepts:\n$\\mathcal{V} := \\bigcup_{\\mathcal{S}: p(\\mathcal{S})>0} \\mathcal{S}$.\nThe set $\\mathcal{V}$ thus contains the concepts that can change in a pair (x, x). Even though concepts outside $\\mathcal{V}$ are assumed to remain fixed within a pair (x, x), they can still vary across pairs. Without loss of generality, assume that $\\mathcal{V} := \\{1, ...., |\\mathcal{V}|\\}$.\nMethod. We propose Sparse Shift Autoencoders (SSAEs), models of difference vectors $\\delta^\\dagger$ consisting of an affine encoder $r : \\mathbb{R}^{d_z} \\rightarrow \\mathbb{R}^{|\\mathcal{V}|}$ and an affine decoder $q : \\mathbb{R}^{|\\mathcal{V}|} \\rightarrow \\mathbb{R}^{d_z}$. The representation $r(\\delta^\\dagger)$ predicts $\\delta^\\circ$, i.e., the concept shifts corresponding to $\\delta^\\dagger$, with $\\delta^\\circ = (\\delta^\\circ_i)_{i \\in \\mathcal{V}}$ the subvector of $\\delta^\\circ$ corresponding to the index set $\\mathcal{V}$. The decoder q then maps these shifts back to embedding differences $\\delta^\\dagger$. We train SSAEs to solve the following constrained problem:\n$\\begin{aligned}\n    (\\hat{r}, \\hat{q}) \\in \\underset{r, q}{\\arg \\min} \\mathbb{E}_{x, \\tilde{x}} [\\lVert \\delta^\\dagger - q(r(\\delta^\\dagger)) \\rVert_2^2] \\\\\n    \\text{s.t.} \\ \\mathbb{E}_{x, \\tilde{x}} [ \\lVert r(\\delta^\\dagger) \\rVert_0] \\leq \\beta,\n\\end{aligned}$\nwhere Eqn. (4) is the standard auto-encoding loss that encourages good reconstruction and Eqn. (5) is a regularizer that encourages the predicted concept shift vector $\\hat{\\delta}^\\circ := r(\\delta^\\dagger)$ to be sparse. Then, we use $\\hat{q}$ to decode single concept shifts into steering vectors. Formally, we end up with a steering function of the form\n$\\hat{\\Phi}_k(z) := z + \\hat{q}(e_k)$,\nwhere $\\hat{q}$ is a solution to the above constrained problem.\nTo summarize, our method consists of two steps:\n1. Solving the sparsity regularized problem of Eqns. (4) and (5) to get an encoder-decoder pair (r, q).\n2. Use the vectors $\\hat{q}(e_k)$ for all $k \\in \\mathcal{V}$ as steering vectors for all concepts (these are the columns of the matrix representing $\\hat{q}$).\nTheoretical justification. In Section 2.4 we will show that, under suitable assumptions on the data-generating process and a suitable choice of $\\beta$, the $l_0$-regularized problem of Eqns. (4) and (5) is guaranteed to learn a $(\\hat{r}, \\hat{q})$ such that $\\hat{\\delta}^\\circ = PD\\delta^\\circ$, where D is an invertible diagonal matrix, P is a permutation matrix. In other words, the learned representation $\\hat{r}(\\delta^\\dagger)$ can be related to the ground-truth concept shift vector $\\delta^\\circ$ (considering only the varying concepts $\\mathcal{V}$) via a permutation-scaling matrix. We will later see how sparsity regularization is crucial for this to happen. Although our theoretical analysis assumes the learned representation has"}, {"title": "2.4. Identifiability analysis", "content": "This section explains why we expect the representation learned in Eqn. (4) to identify the ground-truth concept shift vector $\\delta^\\circ$ up to permutation and rescaling. To do so, we first demonstrate that, under suitable assumptions, the learned representation $\\hat{r}(\\delta^\\dagger)$ identifies the ground-truth concept shift $\\delta^\\circ$ up to an invertible linear transformation when we do not use sparsity regularization. Second, we show that by adding sparsity regularization, the learned representation identifies $\\delta^\\circ$ up to permutation and element-wise rescaling.\nFirst, we provide justification for assuming an affine encoder-decoder pair in Eqn. (4):\nAssumption 1 (Linear representation hypothesis). The generative process $g : \\mathcal{C} \\rightarrow \\mathcal{X}$ and the learned encoding function $f : \\mathcal{X} \\rightarrow \\mathcal{Z}$ are such that $f \\circ g : \\mathcal{C} \\rightarrow \\mathcal{Z}$ is linear, implying there exists a $d_z \\times d_c$ real matrix A such that:\n$\\mathbf{z} = f(g(\\mathbf{c})) = A\\mathbf{c}$.\nThe linear representation hypothesis (LRH) implies that the learned representation z linearly encodes concepts. A long line of work provides evidence for this hypothesis (c.f."}, {"title": "2.5. Extracting steering vectors", "content": "In Section 2.3, we proposed using steering functions of the form $\\Phi_k(z) := z + \\hat{q}(e_k)$ to perturb concepts, where $\\hat{q}$ is learned via the constrained problem of Eqns. (4) and (5). Here, we show that these steering functions are valid. Our argument relies on the identifiability guarantees presented in the previous section. Under Asm. 1 to 4, Prop. 3 shows that $\\hat{q} = A_{\\mathcal{V}}DP$, from which we can verify that $\\hat{q}(e_k) = D_{\\pi(k), \\pi(k)} A_{\\epsilon_{\\pi(k)}}$ for all $k \\in \\mathcal{V}$, where $\\pi : \\mathcal{V} \\rightarrow \\mathcal{V}$ is the permutation that P represents. From this, we can show that $\\Phi_k$ is (almost) a steering function as per Defn. 1:\n$\\begin{aligned}\n    \\Phi_k(f(g(\\mathbf{c}))) &= f(g(\\mathbf{c})) + \\hat{q}(e_k)  \\\\\n    &= A\\mathbf{c} + D_{\\pi(k), \\pi(k)} A e_{\\pi(k)} \\\\\n    &= A(\\mathbf{c} + D_{\\pi(k), \\pi(k)} e_{\\pi(k)}) \\\\\n    &= f(g(\\mathbf{c}+D_{\\pi(k), \\pi(k)} e_{\\pi(k)})) . \\\\\n\\end{aligned}$\nThus, $\\Phi_k$ sends the representations $\\mathbf{z} = f(g(\\mathbf{c}))$ to their counterfactual representations where only concept $\\pi(k)$ changed. We note that, strictly speaking, this does not correspond exactly to the definition of steering function introduced in Section 2.2, since both the scale of the shift $D_{\\pi(k), \\pi(k)}$ and the concept being manipulated $\\pi(k)$ are unknown. Although the scaling and permutation indeterminacies are unavoidable with unsupervised learning, a practitioner can choose a steering vector $\\hat{q}(e_k)$, add different scalings of this vector to an embedding z and, e.g., use it to generate the next tokens with an LLM to infer which concept the vector $\\hat{q}(e_k)$ affects.\nLinear identifiability is insufficient. In contrast, the above strategy fails when concept shifts are only linearly identified, i.e., $\\hat{q} := A_{\\mathcal{V}}L$. In this case, we see that\n$\\begin{aligned}\n\\hat{q}(e_k) = A_{\\mathcal{V}}Le_k = \\sum_{j=1}^{|\\mathcal{V}|} L_{j,k}Ae_j = A \\sum_{j=1}^{|\\mathcal{V}|} L_{j,k}e_j,  \n\\end{aligned}$\nwhich itself implies that\n$\\begin{aligned}\n\\Phi_k(f(g(\\mathbf{c}))) &= A (\\mathbf{c} + \\sum_{j=1}^{|\\mathcal{V}|} L_{j,k}e_j) =  f(g(\\mathbf{c}+ \\sum_{j=1}^{|\\mathcal{V}|} L_{j,k}e_j)). \\\\\n\\end{aligned}$"}, {"title": "3. Empirical Studies", "content": "In this section, we investigate two main questions: (i) how well can SSAE identify concept shifts in practice, and (ii) do the steering vectors recovered by SSAE accurately steer target concepts?\nImplementation details. As mentioned in Section 2.4, key to identifying steering vectors is the sparsity constraint from Eqn. (9). Two key aspects of enforcing sparsity of the learnt representation are: (i) using hard constraints rather than penalty tuning, which helps address concerns with $\\ell_1$-based regularization (e.g., feature suppression (Anders et al., 2024)) and (ii) appropriate normalisation. For the former, we use the cooper library (Gallego-Posada & Ramirez, 2022). For the latter, we implement batch normalization (Ioffe & Szegedy, 2015) after the encoder and column normalisation in the decoder at each step (Bricken et al., 2023; Gao et al., 2024). To tune the model's hyperparameters in an unsupervised way, we use the Unsupervised Diversity Ranking (UDR) score (Duan et al., 2019), and test the model's sensitivity on key parameters (such as the sparsity level e and learning rate). For details, refer to Apx. B.\nBaselines. We consider two baselines: an affine Autoencoder (aff), with identical architecture but no sparsity constraint, and Mean Difference (MD) vectors, which use paired observations differing in a single concept to compute Z = 1\u03a3, Ae as the steering vector for concept k. We denote the concept-steered embeddings produced by each method as \u017e\u014b (SSAE), \u017daff (aff), and ZMD (MD).\nEvaluation criteria. We measure the degree of identifiability via the Mean Correlation Coefficient (MCC) (Hyvarinen & Morioka, 2016; Khemakhem et al., 2020b), which computes the highest average correlation between each learned latent dimension and the true latent dimension and equals 1.0 when they are aligned perfectly up to permutation and scaling. On synthetic data, we compute MCC against known ground-truth concept representations. When ground-truth is not known, such as in experiments with"}, {"title": "4. Related work", "content": "Linear representation hypothesis. This paper builds on the linear representation hypothesis that language models encode concepts linearly. Several papers provide empirical evidence for this hypothesis (Mikolov et al., 2013; Gittens et al., 2017; Ethayarajh et al., 2019; Allen & Hospedales, 2019; Seonwoo et al., 2019; Burns et al., 2024; Li et al., 2024a; Moschella et al., 2023; Tigges et al., 2023; Nanda et al., 2023; Nissim et al., 2020; Ravfogel et al., 2020a; Park et al., 2023). Recent work also provides theoretical justification for why linear properties might consistently emerge across models that perform next-token prediction (Roeder et al., 2021; Jiang et al., 2024; Marconato et al., 2024; Park et al., 2024).\nInterpretability of LLMs. This paper contributes to the literature on interpretability and steering of LLMs. Much of the work on finding concepts in LLM representations for steering relies on supervision, either from paired observations with a single-concept shift (Panickssery et al., 2024; Turner et al., 2024; Rimsky et al., 2024; Li et al., 2024a) or from examples of target LLM completions to prompts (Subramani et al., 2022). This prior work also focuses on applying the same steering vector to all examples, implicitly relying on the linear representation hypothesis as justification. In contrast, we make the assumption precise, and show how it leads to steering vectors. This paper also departs from supervised learning and focuses on learning with limited"}, {"title": "5. Conclusion", "content": "We propose Sparse Shift Autoencoders (SSAEs) for discovering accurate steering vectors from multi-concept paired observations as an alternative to both SAEs and approaches relying on supervised data. Key to this result are the identifiability guarantees that the SSAE enjoys as a consequence of considering sparse concept shifts. We study the SSAE empirically, using Llama3.1 embeddings on several real language tasks, and find evidence that the method facilitates accurate steering learned via limited supervision. However, we stress that these experiments are intended to validate the identifiability results in Section 2 and their implications for accurate steering. Although we include real-world data (TruthfulQA), to fully understand the impacts of the SSAE on steering research, especially LLM alignment, more evaluation is needed on embeddings from more complex datasets, on more challenging tasks (e.g., MTEB (Muennighoff et al., 2023)), and by generating text with steered embeddings to assess model behavior. Rigorous large-scale evaluations are a promising avenue for future work. Such evaluations would benefit from more expansive real-world benchmarks and a more nuanced approach to categorical concepts\u2014one that moves beyond reducing them to binary contrasts."}, {"title": "A. Theory", "content": "A.1. Notation and Glossary"}, {"title": "B Implementation and experimental details", "content": "B.1 SSAE Architecture"}]}