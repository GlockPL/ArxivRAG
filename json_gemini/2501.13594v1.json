{"title": "Text-to-SQL based on Large Language Models and Database Keyword Search", "authors": ["Eduardo R. Nascimento", "Caio Viktor S. Avila", "Yenier T. Izquierdo", "Grettel M. Garc\u00eda", "Lucas Feij\u00f3 L. Andrade", "Michelle S.P. Facina", "Melissa Lemos", "Marco A. Casanova"], "abstract": "Text-to-SQL prompt strategies based on Large Language Models (LLMs) achieve remarkable performance on well-known benchmarks. However, when applied to real-world databases, their performance is significantly less than for these benchmarks, especially for Natural Language (NL) questions requiring complex filters and joins to be processed. This paper then proposes a strategy to compile NL questions into SQL queries that incorporates a dynamic few-shot examples strategy and leverages the services provided by a database keyword search (KwS) platform. The paper details how the precision and recall of the schema-linking process are improved with the help of the examples provided and the keyword-matching service that the KwS platform offers. Then, it shows how the KwS platform can be used to synthesize a view that captures the joins required to process an input NL question and thereby simplify the SQL query compilation step. The paper includes experiments with a real-world relational database to assess the performance of the proposed strategy. The experiments suggest that the strategy achieves an accuracy on the real-world relational database that surpasses state-of-the-art approaches. The paper concludes by discussing the results obtained.", "sections": [{"title": "1 INTRODUCTION", "content": "The Text-to-SQL task is defined as \"given a relational database D and a natural language (NL) sentence QN that describes a question on D, generate an SQL query QSOL over D that expresses QN\" (Katsogiannis-Meimarakis and Koutrika, 2023; Kim et al., 2020). Numerous tools have addressed this task with relative success (Affolter et al., 2019; Katsogiannis-Meimarakis and Koutrika, 2023; Kim et al., 2020; Shi et al., 2024) over well-known benchmarks, such as Spider - Yale Semantic Parsing and Text-to-SQL Challenge (Yu et al., 2018) and BIRD \u2013 BIg Bench for Large-scale Database Grounded Text-to-SQL Evaluation (Li et al., 2024). The leaderboards of these benchmarks point to a firm trend: the best text-to-SQL tools are all based on Large Language Models (LLMs) (Shi et al., 2024). Text-to-SQL tools must face several challenges. To begin with, they must be able to process NL questions that require multiple SQL constructs (Yu et al., 2018). For example, processing the NL question: \"Which has more open orders, P-X or P-Y?\" requires: Recognizing that P-X and P-Y are industrial installations; Joining installations and orders; Understanding what is an open order; Computing the number of open orders for each of the installations. Returning the installation with the largest number of open orders. Omitting the details, the following SQL query would answer the above NL question:\nSELECT t.name,\nCOUNT(*) AS number_open_orders\nFROM Installation t JOIN Order o\nON t.code = o.installation_code\nWHERE (t.name = 'P-X' OR t.name = 'P-Y')\nAND LOWER (o.status) LIKE LOWER ('% Open%')\nGROUP BY t.code\nORDER BY number_open_orders DESC\nFETCH 1\nThis is an example of a challenging NL question that the strategy proposed in this paper can compile into a correct SQL query. In addition, real-world databases raise a different set of challenges for several reasons, among which: The relational schema is often large, in the number of tables, columns per table, and foreign keys which may lead to queries with many joins, which are difficult to synthesize. The relational schema is often an inappropriate specification of the database from the point of view of the LLM - the table and column names are often different from the terms the users adopt to formulate their NL questions. The data semantics are often complex; for example, some data values may encode enumerated domains, which implies that the terms the users adopt to formulate their NL questions must be mapped to this internal semantics. Metadata and data are often ambiguous, which influences the behavior of an LLM-based text-to-SQL tool, leading to unexpected results. Indeed, the performance of some of the best LLM-based text-to-SQL tools on real-world databases is significantly less than that observed for the Spider and BIRD benchmarks (Nascimento et al., 2024a; Lei et al., 2024). This paper then addresses the real-world text-to-SQL problem, which is the version of the text-to-SQL problem for real-world databases. Albeit the original problem has been investigated for some time, this version is considered far from solved, as argued in (Floratou et al., 2024; Lei et al., 2024). The first contribution of the paper is a novel strategy to compile NL questions into SQL queries that leverages the services provided by a database keyword search (KwS) platform, called DANKE (Izquierdo et al., 2021; Izquierdo et al., 2024). The proposed strategy is the first one to explore a symbiotic combination of a KwS platform and a prompt strategy to process NL questions. Briefly, Section 4.3 details how the combination of DANKE's data dictionary with a dynamic few-shot examples strategy improves the precision and recall of the schema-linking process, that is, the process of finding a set of tables that suffice to compile an input NL question. Then, Section 4.4 shows how the SQL query compilation step is also improved by calling DANKE to synthesize a view V that captures the required joins to answer the input NL question QN, and then calling an LLM to compile QN into an SQL query QSOL Over V, which can be remapped to the database schema with the help of the definition of V. The second contribution of the paper is a set of experiments with a real-world benchmark to assess the performance of the proposed strategy. The benchmark is built upon a relational database with a challenging schema, which is in production at an energy company, and a set of 100 NL questions carefully defined to reflect the NL questions users submit and to cover a wide range of SQL constructs (Spider and BIRD, two of the familiar text-to-SQL benchmarks, were not adopted for the reasons explained in Section 2.1). These new results, combined with results from (Nascimento et al., 2024a), indicate that the proposed strategy performs significantly better than LangChain SQL-QueryChain, SQLCoder\u00b9, \u201cC3 + ChatGPT + Zero-Shot\" (Dong et al., 2023), and \u201cDIN-SQL + GPT-4\" (Pourreza and Rafiei, 2024). This paper is an extended version of (Nascimento et al., 2025). The paper is organized as follows. Section 2 covers related work. Section 3 describes the database keyword search platform adopted in the paper. Section 4 details the proposed text-to-SQL strategy. Section 5 presents the experiments, including the real-world benchmark used. Finally, Section 6 contains the conclusions."}, {"title": "2 RELATED WORK", "content": "2.1 Text-to-SQL Datasets\nThe Spider - Yale Semantic Parsing and Text-to-SQL Challenge (Yu et al., 2018) defines 200 datasets, covering 138 different domains, for training and testing text-to-SQL tools. For each database, Spider lists 20-50 hand-written NL questions and their SQL translations. An NL question S, with an SQL translation QN, is classified as easy, medium, hard, and extra-hard, where the difficulty is based on the number of SQL constructs of QN - GROUP BY, ORDER BY, INTERSECT, nested sub-queries, column selections, and aggregators - so that an NL query whose translation QN contains more SQL constructs is considered more complex. The set of NL questions introduced in Section 5.1.2 follows this classification, but does not consider extra-hard NL questions. Spider proposes three evaluation metrics: component matching checks whether the components of the prediction and the ground-truth SQL queries match exactly; exact matching measures whether the predicted SQL query as a whole is equivalent to the ground-truth SQL query; execution accuracy requires that the predicted SQL query selects a list of gold values and fills them into the correct slots. Section 5.2 describes the metric used in the experiments of this paper, which is a variation of execution accuracy. Most databases in Spider have very small schemas the largest five databases have between 16 and 25 tables, and about half have schemas with five tables or fewer. Furthermore, all Spider NL questions are phrased in terms used in the database schemas. These two limitations considerably reduce the difficulty of the text-to-SQL task. Therefore, the results reported in the Spider leaderboard are biased toward databases with small schemas and NL questions written in the schema vocabulary, which is not what one finds in real-world databases. Spider has two interesting variations. Spider-Syn (Gan et al., 2021a) is used to test how well text-to-SQL tools handle synonym substitution, and Spider-DK (Gan et al., 2021b) addressed testing how well text-to-SQL tools deal with domain knowledge. BIRD - BIg Bench for LaRge-scale Database Grounded Text-to-SQL Evaluation (Li et al., 2024) is a large-scale, cross-domain text-to-SQL benchmark in English. The dataset contains 12,751 text-to-SQL data pairs and 95 databases with a total size of 33.4 GB across 37 domains. However, BIRD still does not have many databases with large schemas - of the 73 databases in the training dataset, only two have more than 25 tables, and, of the 11 databases used for development, the largest one has only 13 tables. Again, all NL questions are phrased in the terms used in the database schemas. Finally, the sql-create-context\u00b2 dataset also addresses the text-to-SQL task, and was built from WikiSQL and Spider. It contains 78,577 examples of NL questions, SQL CREATE TABLE statements, and SQL queries answering the questions. The CREATE TABLE statement provides context for the LLMs, without having to provide actual rows of data. Despite the availability of these benchmark datasets for the text-to-SQL task, and inspired by them, Section 5.1 describes a benchmark dataset constructed specifically to test strategies designed for the real-world text-to-SQL task. The benchmark dataset consists of a relational database, three sets of LLM-friendly views, specified as described in Section 5.1.3,"}, {"title": "2.2 Text-to-SQL Tools", "content": "A comprehensive survey of text-to-SQL strategies can be found in (Shi et al., 2024), including a discussion of benchmark datasets, prompt engineering, and fine-tuning methods, partly covered in what follows. The Spider Web site\u00b3 publishes a leaderboard with the best-performing text-to-SQL tools. At the time of this writing, the top 5 tools achieved an accuracy that ranged from an impressive 85.3% to 91.2% (two of the tools are not openly documented). Four tools use GPT-4, as their names imply. The three tools that provide detailed documentation have an elaborate first prompt that tries to select the tables and columns that best match the NL question. Therefore, this first prompt is prone to failure if the database schema induces a vocabulary disconnected from the NL question terms. This failure cannot be fixed by even more elaborate prompts that try to match the schema and the NL question vocabularies, but it should be addressed as proposed in this paper. The BIRD Web site4 also publishes a leaderboard with the best-performing tools. At the time of this writing, out of the top 5 tools, two use GPT-4, one uses CodeS-15B, one CodeS-7B, and one is not documented. The sixth and seventh tools also use GPT-4, appear in the Spider leaderboard, and are well-documented. The Awesome Text2SQL Web sites lists the best-performing text-to-SQL tools on WikiSQL, Spider (Exact Match and Exact Execution) and BIRD (Valid Efficiency Score and Execution Accuracy). The DB-GPT-Hub is a project exploring how to use LLMs for text-to-SQL. It contains data collection, data preprocessing, model selection and building, and fine-tuning weights, including LLaMA-2, and evaluating several LLMs fine-tuned for text-to-SQL. Several text-to-SQL tools were tested in (Nascimento et al., 2024a) against the benchmark used in this paper - SQLCoder, LangChain SQLQueryChain, C3, and DIN+SQL. SQLCoder7 is a specialized text-to-SQL model, open-sourced under the Apache-2 license. The sqlcoder-34b-alpha model features 34B parameters and was fine-tuned on a base CodeLlama model, on more than 20,000 human-curated questions, classified as in Spider, based on ten different schemas. LangChain is a generic framework that offers several pre-defined strategies to build and run SQL queries based on NL prompts. \"C3 + ChatGPT + Zero-Shot\" (Dong et al., 2023) (or briefly C3) is a prompt-based strategy, originally defined for ChatGPT, that uses only approximately 1,000 tokens per query and achieves a better performance than fine-tuning-based methods. C3 has three key components: Clear Prompting (CP); Calibration with Hints (CH); Consistent Output (CO). At the time of this writing, C3 was the sixth strategy listed in the Spider leaderboard, achieving 82.3% in terms of execution accuracy on the test set. It outperformed state-of-the-art fine-tuning-based approaches in execution accuracy on the test set. \"DIN-SQL + GPT-4\" (Pourreza and Rafiei, 2024) (or briefly DIN) uses only prompting techniques and decomposes the text-to-SQL task into four steps: schema linking; query classification and decomposition; SQL generation; and self-correction. When released, DIN was the top-performing tool listed in the Spider leaderboard, achieving 85.3% in terms of execution accuracy. Despite the impressive results of C3 and DIN on Spider, and of SQLCoder on a specific benchmark, the performance of these tools on the benchmark used in this paper was significantly lower (Nascimento et al., 2024a), and much less than that of the strategy described in Section 4. A similar remark applies to LangChain SQLQueryChain, whose results are shown in Line 1 of Table 3."}, {"title": "2.3 Retrieval-Augmented and Dynamic Few-shot Examples Prompting", "content": "Retrieval-Augmented Generation (RAG), introduced in (Lewis et al., 2020), is a strategy to incorporate data from external sources. This process ensures that the responses are grounded in retrieved evidence, thereby significantly enhancing the accuracy and relevance of the output. There is an extensive literature on RAG. A recent survey (Gao et al., 2024) classified RAG strategies into naive, advanced, and modular RAG. Naive RAG follows the traditional process that includes indexing, retrieval, and generation of document \"chunks\". Advanced RAG introduces various methods to optimize retrieval. Modular RAG integrates strategies to enhance functional modules, such as incorporating a search module for similarity retrieval and applying a fine-tuning approach in the retriever. As for text-to-SQL, recent references include a RAG technique (Panda and Gozluklu, 2024) to retrieve table and column descriptions from a metadata store that are related to the NL question, based on similarity search. LangChain offers a dynamic few-shot examples prompting technique also based on similarity search. Given an NL question QN, the prompting strategy includes the examples most relevant to QN, retrieved by a similarity search between QN and a set of examples previously stored in a vector database. A retrieval-augmented prompting method for a LLM-based text-to-SQL framework is proposed in (Guo et al., 2024), involving sample-aware prompting and a dynamic revision chain. The method uses two strategies to retrieve questions sharing similar intents with input questions. Firstly, using LLMs, the method simplifies the original questions, unifying the syntax and thereby clarifying the users' intentions. To generate executable and accurate SQL queries without human intervention, the method incorporates a dynamic revision chain, which iteratively adapts fine-grained feedback from the previously generated SQL queries. A similar strategy is proposed in (Coelho et al., 2024), that also describes a technique to create synthetic datasets with sets of examples (QN,S) where QN is an NL question and S is its SQL translation."}, {"title": "3 A DATABASE KEYWORD QUERY PROCESSING TOOL", "content": "DANKE is the keyword search platform currently deployed for the industrial database described in Section 5.1 and used for the experiments. The reader is referred to (Izquierdo et al., 2021; Izquierdo et al., 2024) for the details of the platform. DANKE operates over both relational databases and RDF datasets, and is designed to compile a keyword query into an SQL or SPARQL query that returns the best data matches. For simplicity, the description that follows uses the relational terminology. DANKE's architecture comprises three main components: (1) Storage Module; (2) Preparation Module; and (3) Data and Knowledge Extraction Module. The Storage Module houses a centralized relational database, constructed from various data sources. The database is described by a conceptual schema, treated in what follows as a relational schema, again for simplicity. The Storage Module also holds the data indices required to support the keyword search service. The indexing process is enriched to create a keyword dictionary containing: for each table T, an entry of the form (T,Ts), where Ts is a list of terms users adopt to refer to table T. for each column A of a table T, an entry of the form (A,T,As), where As is a list of terms that users adopt to refer to column A in the context of table T. for each indexed value v, an entry of the form (v,T,A), where T[A] is the table/column where v occurs. The Preparation Module has tools for creating the conceptual schema and for constructing and updating the centralized database through a pipeline typical of a data integration process. The conceptual schema is defined by de-normalizing the relational schemas of the underlying databases and indicating which columns will have their values indexed. The Data and Knowledge Extraction Module has two main sub-modules, Query Compilation and Query Processing. Given a keyword query, represented by a list of keywords, the Query Compilation Module has three major steps: (Matching Discovery) Match each keyword in the keyword query with table and column names or data values in the keyword dictionary. (Matching Optimization) Select the most relevant matches. (Conceptual Query Compilation) Compile a conceptual query over the conceptual schema from the most relevant matches. The Query Processing Module, in turn, has two major steps: (Query Compilation) Compile the conceptual query into an SQL query. (Query Execution) Submit the SQL query for execution, collect the results, and display them to the user. Let R be the referential dependencies diagram of the database schema in question, where the nodes of R are the tables and there is an edge between nodes t and u iff there is a foreign key from t to u or vice-versa. Given a set of keywords K, let Tk be a set of table schemes whose instances match the largest set of keywords in K. The conceptual query compilation step first constructs a Steiner tree Sk of R whose end nodes are the set Tk. This is the central point since it guarantees that the final SQL query will not return unconnected data, as explored in detail in (Garc\u00eda et al., 2017). If R is connected, then it is always possible to construct one such Steiner tree; overwise, one would have to find a Steiner forest to cover all tables in Tk. Using the Steiner tree, the Query Compilation step compiles the keyword query into an SQL query that includes restriction clauses representing the keyword matches and join clauses connecting the restriction clauses. Without such join clauses, an answer would be a disconnected set of tuples, which hardly makes sense. The generation of the join clauses uses the Steiner tree edges. Lastly, DANKE's internal API was expanded to support the text-to-SQL strategy described in Section 4. Briefly, it now offers the following services: Keyword Match Service: receives a set K of keywords and returns the set KM of pairs (k, dk) such that k \u2208 K and dk is the dictionary entry that best matches k, using the matching optimization heuristic mentioned above. The dictionary entry dk will be called the data associated with k. View Synthesis Service: receives a set S' of tables and returns a view V that best joins all tables in S', and be faster than other optimization heuristic mentioned above."}, {"title": "4 A STRATEGY FOR THE TEXT-TO-SQL TASK", "content": "4.1 Outline of the Proposed Strategy\nBriefly, the proposed strategy comprises two modules, schema linking and SQL query compilation, as typical of text-to-SQL prompt strategies. Figure 1 summarizes the proposed strategy, leaving the details to the next sections. The two modules run under LangChain. They use a dynamic few-shot examples strategy that retrieves a set of samples from a synthetic dataset D, indexed with the help of the FAISS similarity search library10. The key point is the use of services provided by DANKE to enhance schema linking and simplify SQL compilation, as explained in the following sections. In particular, DANKE will generate a single SQL view containing all data and encapsulating all joins necessary to answer the input NL question. The current implementation runs in-house: LangChain, FAISS, DANKE, and Oracle. The experiments used the OpenAI GPT-4 and its variations, as detailed in Section 5.\n4.2 Synthetic Dataset Construction\nLet DB be a relational database with schema S. A synthetic dataset D for DB contains pairs (QN, QSQL), where QN is an NL question and QSOL is its SQL translation. Such pairs should provide examples that help the LLM understand how the database schema is structured, how the user's terms map to terms of the database schema, and how NL language constructions map to data values. The synthetic dataset construction process repeatedly calls Algorithm 1 to generate as many pairs (QN, QSQL) as desired. The parameter n is set in each call to determine how many tables the SQL query should involve. Step 1 (on Line 2) selects a set T of n tables from the database schema S. The selection process employs a weighted random distribution, which reflects the likelihood of each table being chosen by an average user. Note that users may choose some tables more often than others, which justifies employing a weighted random distribution, obtained from the users' access log. Step 2 (on Line 3) selects column pairs for each table chosen in Step 1. The first column selected is always the primary key of the table, and the second column is chosen based on the weighted random distribution of each column in the database schema S. Step 3 (on Line 4) creates a simplified Data Definition Language (DDL) statement L, encompassing only the columns and tables involved. Column and table names are renamed to their respective names in the conceptual schema views (see Section 5.1.3). Step 4 (on Line 5) creates an NL question QN' by prompting GPT-4 with the simplified DDL statement L and sample values of each column from the database DB. In addition, the prompt includes the type of restriction to be incorporated into the NL question, which depends on the data type of each column. For example, numeric-type columns can be used to create queries with aggregations. Finally, the prompt includes instructions that indicate that QN' must be generated in the database vocabulary; that is, the table and column names must be kept to facilitate the generation of the SQL corresponding to the NL question. Step 5 (on Line 6) calls GPT-4 to translate QN' into an SQL query that responds to the NL question by providing QN' and L. This process is facilitated by the fact that the SQL query should use the database schema vocabulary; that is, the prompt provides clues about which tables, columns, and values are involved. The key point is to explore the column type to decide which SQL construct must be used to express a restriction for the column. For example, given a string B, if column INSTALLATION_NAME were of type STRING and not a key, the prompt would guide GPT-4 to create a restriction of the form\nINSTALLATION_NAME LIKE '%B%'\nHowever, if column INSTALLATION_NAME were a key, then the prompt would guide GPT-4 to create a restriction of the form\nINSTALLATION_NAME = 'B'\nFinally, Step 6 (on Line 7) calls GPT-4 to translate QN' into an improved NL question QN, using the database documentation DBdoc, which includes the description of each column and table, along with synonyms. During this step, the LLM is instructed to rephrase the NL question by translating from the database schema vocabulary to the user's vocabulary, preserving the original NL question intent.\n4.3 Schema Linking\nLet DB be a relational database with schema S and D be the synthetic dataset created for DB. Let QN be an NL question over S. The schema linking module primarily finds a minimal set S'CS such that S' has all tables in S required to answer QN. It has the following major components (see Figure 2):\nKeyword Extraction and Matching\nReceives as input an NL question QN.\nCalls the LLM to extract a set K of keywords from QN.\nCalls the DANKE Keyword Matching service to match K with the dictionary, creating a final set KM of keywords and associated data.\nReturns KM.\nDynamic Few-shot Examples Retrieval (DFE)\nReceives as input an NL question QN.\nRetrieves from the synthetic dataset D a set of k examples whose NL questions are most similar to QN, generating\nL= [(Q1, SQL1), \u2026\u2026\u2026, (Qk, SQLk)]\nCreates a list T of pairs by retaining only the table names in the FROM clauses, that is,\nT = [(Q1, F1), ..., (Qk, Fk)]\nwhere Fi is the set of tables in the FROM clause of SQLi.\nReturns T.\nSchema Linking\nReceives as input an NL question QN, a set KM of keywords and associated data, and a list T as above.\nRetrieves the set of tables in S and their columns.\nCalls the LLM to create S' prompted by QN, KM, S, and T.\nReturns S' and KM.\n4.4 SQL Query Compilation\nThe SQL query compilation module receives as input the NL question QN, the set of tables S', and the set KM of keywords and associated data, and returns an SQL query QSOL. It has the following major components (see Figure 3):\nView Synthesis\nReceives as input a set of tables S'.\nCalls the DANKE View Synthesis service to synthesize a view V that joins the tables in S'.\nReturns V.\nQuestion Decomposition\nReceives as input an NL question QN.\nDecomposes QN into sub-questions Q1,..., Qm.\nReturns Q1,..., Qm.\nDynamic Few-shot Examples Retrieval (DFE)\nReceives as input a list of NL questions Q1,..., Qm.\nLet p = [k/m]. For each i \u2208 [1,m], retrieves from the synthetic dataset D a set of p examples whose NL questions are most similar to Qi and whose SQL queries are over S', generating a list\nLi = [(Qi\u2081, SQLi\u2081), ..., (Qip, SQLip)]\nin decreasing order of similarity of Qi; to Qi.\nCreates the final list L, with k elements, by intercalating the lists Li and retaining the top-k pairs.\nReturns L.\nSQL Compilation\nReceives as input a view V, a set KM of keywords and associated data, an NL question QN, and a list L as above."}, {"title": "4.5 Limitations and Examples", "content": "If the proposed text-to-SQL strategy can compile an NL question QN into an SQL query QSOL, then QSOL is such that: QSOL is defined over a single table V. V is a view defined by an SQL query over a single table or a block of equijoin clauses, with no WHERE clause and no optional clauses (GROUP BY, HAVING, ORDER BY, and LIMIT clauses). These conditions reflect the way the text-to-SQL strategy is structured. Indeed, first, observe that, in the SQL Query Compilation module, the View Synthesis step calls DANKE's View Synthesis Service to create a view V, defined by a set of equijoin clauses over the set of tables S' passed by the schema linking module. This step improves the (manual) approach proposed in (Nascimento et al., 2024b). Then, the SQL Compilation step prompts the LLM with view V to generate the SQL query. View V then facilitates the translation of an NL question into an SQL query since the LLM no longer needs to discover which joins to include in the SQL query. The predicted SQL queries and the views in Tables 5 and 6 at the end of the paper provide examples of such NL questions, SQL queries, and views, two of which are discussed in more detail in what follows. As a very simple example, consider Question 24 in Table 5, which requires joining tables Recommendation and Installation, as its ground-truth SQL query indicates. The schema linking module can compute that Question 24 requires these two tables. Then, the View Synthesis step calls DANKE, which receives the relational schema (see Table 4) and these two tables, finds the required join, and synthesizes the following view (some details of the view definition are omitted here for brevity):\nCREATE VIEW Recommendation_Installation AS\nSELECT r.id AS Recommendation_id,\nr.situation AS Recommendation_situation,\nFROM Recommendation r\nJOIN Installation p\nON r.installation_name = p.name\nAs a result, the SQL compilation step generates an SQL query without explicitly including this join (see the predicted SQL query for Question 24 in Table 5). The SQL compilation step prompts this view to the LLM as if it were a table in DDL format:\nCREATE TABLE Recommendation_Installation\n(Recommendation_id,\nRecommendation_situation,...)\nHowever, this very simple example does not fully illustrate the power of using DANKE to join any number of tables. A slightly more complex example goes as follows. Consider Question 93 in Table 6, which requires joining tables Maintenance_request, Maintenance_recommendation, and Maintenance_order, as its ground-truth SQL query indicates. Again, the schema linking module can compute that Question 93 requires these three tables. Then, the View Synthesis step calls DANKE, which receives the relational schema and these three tables, finds the required two joins, and synthesizes the following view (again, some details of the view definition are omitted here for brevity):\nCREATE VIEW Request_Recommendation_Order AS\nSELECT m.id AS Request_id,\nr.id AS Recommendation_id,\no.id AS Order_id,\nFROM Maintenance_request m\nJOIN Maintenance_recommendation r\nON m.id = r.note_id\nJOIN Maintenance_order o\nON r.order_id = o.id\nAs a result, the SQL compilation step generates an SQL query without explicitly including these joins (see the predicted SQL query for Question 93 in Table 6). The SQL compilation step prompts this view to the LLM as if it were a table in DDL format:\nCREATE TABLE Request_Recommendation_Order\n(Request_id,\nRecommendation_id,\nOrder_id,...)"}, {"title": "5 EXPERIMENTS", "content": "5.1 A Benchmark Dataset\nThis section describes a benchmark to help investigate the real-world text-to-SQL task. The benchmark consists of a relational database, a set of 100 test NL questions and their SQL ground-truth translations, and a set of partially extended views.\n5.1.1 The Relational Database\nThe selected database is a real-world relational database (in Oracle) that stores data related to the integrity management of an energy company's industrial assets. The relational schema of the adopted database contains 27 relational tables with, in total, 585 columns and 30 foreign keys (some multi-column), where the largest table has 81 columns. Figure 4 shows the referential dependencies diagram of a much-simplified and anonymized version of the relational schema of the real-world database, where an arrow represents a foreign key and points to the referenced table, as usual. Note that the diagram is a connected graph, which implies that, given any set of tables T of the relational schema, it is always possible to create a Steiner tree of the diagram that covers all tables in T. This implies that Step 1 of the SQL Query compilation process will always succeed in creating the required view.\n5.1.2 The Set of Test Questions and their Ground-Truth SQL Translations\nThe benchmark contains a set of 100 NL questions that consider the terms and questions experts use when requesting information related to the maintenance and integrity processes. The ground-truth SQL queries were manually defined over the conceptual schema views so that the execution of a ground-truth SQL query returns the expected answer to the corresponding NL question. An NL question is classified into simple, medium, and complex, based on the complexity of its corresponding ground-truth SQL query, as in the Spider benchmark (extra-hard questions were not considered). The set L contains 33 simple, 33 medium, and 34 complex NL questions, with the basic statistics shown in Table 1. Tables 4, 5, and 6 at the end of the paper show three examples of each of these classes.\n5.1.3 The Set of Partially Extended Views\nThe benchmark also includes a set of partially extended views (Nascimento et al., 2024b) that rename table and column names of the relational schema to end users' terms. Such views also have new columns that pre-define joins that follow foreign keys and import selected columns from the referenced tables to facilitate SQL query compilation. They are maintained in the benchmark since they were used in the earlier experiments reported in Lines 1 to 3 of Table 3.\n5.2 Evaluation Procedure\nThe experiments used an automated procedure to compare the predicted and the ground-truth SQL queries, entirely based on column and table values, and not just column and table names. Therefore, a text-to-SQL tool may generate SQL queries over the relational schema or any set of views, and the resulting SQL queries may be compared with the ground-truth SQL queries based on the results returned. The results of the automated procedure were manually checked to eliminate false positives and false negatives."}, {"title": "5.3 Experiments with Schema Linking", "content": "5.3.1 Experimental Setup\nThe first set of experiments evaluated several alternatives for performing the schema linking task. The experiments adopted the benchmark described in Section 5.1. For each NL question", "alternatives": "LLM): A strategy that prompts an LLM with QN and S to find the set of tables S'. (DANKE): A strategy that", "LLM+DFE)": "A strategy that", "LLM+DANKE)": "A strategy that", "LLM+DANKE+DFE)": "A strategy that", "Complete)": "The entire Schema Linking process. 5.3.2 Results\\"}]}