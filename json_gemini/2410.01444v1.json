{"title": "GEOMETRIC SIGNATURES OF COMPOSITIONALITY\nACROSS A LANGUAGE MODEL'S LIFETIME", "authors": ["Jin Hwa Lee", "Thomas Jiralerspong", "Lei Yu", "Yoshua Bengio", "Emily Cheng"], "abstract": "Compositionality, the notion that the meaning of an expression is constructed from\nthe meaning of its parts and syntactic rules, permits the infinite productivity of\nhuman language. For the first time, artificial language models (LMs) are able\nto match human performance in a number of compositional generalization tasks.\nHowever, much remains to be understood about the representational mechanisms\nunderlying these abilities. We take a high-level geometric approach to this problem\nby relating the degree of compositionality in a dataset to the intrinsic dimensionality\nof its representations under an LM, a measure of feature complexity. We find not\nonly that the degree of dataset compositionality is reflected in representations'\nintrinsic dimensionality, but that the relationship between compositionality and\ngeometric complexity arises due to learned linguistic features over training. Finally,\nour analyses reveal a striking contrast between linear and nonlinear dimensionality,\nshowing that they respectively encode formal and semantic aspects of linguistic\ncomposition.", "sections": [{"title": "INTRODUCTION", "content": "By virtue of linguistic compositionality, few syntactic rules and a finite lexicon can generate an\nunbounded number of sentences (Chomsky, 1957). That is, language, though seemingly high-\ndimensional, can be explained using relatively few degrees of freedom. A great deal of effort has been\nmade to test whether neural language models (LMs) exhibit human-like compositionality (Hupkes\net al., 2019; Baroni, 2019; McCoy, 2022). We take a geometric view of this question, asking how an\nLM's representational structure reflects and supports compositional understanding over training.\nIf a language model is a good model of language, we expect its internal representations to reflect\nthe relatively few variables underlying the latter. That is, representations should reflect the manifold\nhypothesis, or the notion that real-life, high-dimensional data lie on a low-dimensional manifold\n(Goodfellow et al., 2016). The dimension of this manifold, or intrinsic dimension (ID), is then the\nminimal number of degrees of freedom required to describe it without suffering from information loss"}, {"title": "BACKGROUND", "content": "Compositionality It has long been a topic of debate whether neural networks also exhibit human-\nlike compositionality when processing natural language (Fodor & Pylyshyn, 1988; Smolensky, 1990;\nMarcus, 2003). This debate has fueled an extensive line of empirical exploration that assesses\nthe compositionality of neural networks in language modeling via synthetic data (Bentivogli et al.,\n2016; Lake & Baroni, 2018; Bahdanau et al., 2018) and natural language stimuli (Sathe et al., 2023;\nDankers et al., 2022; Press et al., 2023). After the recent introduction of large language models\nwith human-level linguistic capabilities (Wei et al., 2022), researchers have shown via mechanistic\ninterpretability analyses that LMs often extract individual word meanings in early layers, and compose\nthem via later-layer attention heads to construct semantic representations for multi-word expressions\n(Haviv et al., 2023; Geva et al., 2023). We use complementary tools to understand compositionality:\nrather than neurons and circuits, we link compositionality to the geometric properties of a model's\nembedding space which describe its learned feature complexity.\nLanguage defines a mapping from form to meaning (de Saussure, 1916). Form is the physical shape\nof an utterance, for example, the sequence of letters or morphemes when written, or sounds when\nspoken. Broadly, meaning is the concepts or entities to which the forms refer. Unlike prior work, we\nmake a distinction between form and meaning composition, where the formal composition relates to\nthe combinatorial complexity of the data, and semantic composition relates to the ability to construct\nsentence-level meaning from word meaning. While, in grammatical sentences, meaning composition\noften inherits from form composition, we disentangle them by creating agrammatical versions of the\ndataset, further described in the Methods.\nThe manifold hypothesis and low-dimensional geometry Deep learning problems are often\nconsidered high-dimensional, but research suggests that they have low-dimensional intrinsic structure.\nIn computer vision, studies have shown that common learning objectives and natural image data reside\non low-dimensional manifolds (Li et al., 2018; Pope et al., 2021; Valeriani et al., 2023; Psenka et al.,"}, {"title": "SETUP", "content": "We consider the relationship between a dataset's degree of compositionality and its representational\ncomplexity under an LM. Here, we describe the models, dataset generation, and compositionality\nquantification and feature complexity estimation."}, {"title": "MODELS", "content": "We evaluate Transformer-based causal language models from the Pythia family (Biderman et al.,\n2023), as Pythia is one of the only model suites to release intermediate training checkpoints. Models\nare trained on The Pile, a large natural language corpus encompassing encyclopedic text, books,\nsocial media, code, and reviews (Gao et al., 2020). Over training, models are tasked to predict the\nnext token given context, subject to a negative log-likelihood loss. Experiments are performed on all\nmodels in sizes \u2208 {14m, 70m, 160m, 410m, 1.4b, 6.9b, 12b}.\nPre-training analysis For the three intermediate sizes 410m, 1.4b, and 6.9b, we report model\nperformance throughout the pre-training phase on the set of evaluation suites provided by (Biderman\net al., 2023; Gao et al., 2024), further described in Appendix F. This encompasses a range of higher-\nlevel linguistic and reasoning tasks, spanning from long-range text comprehension (Paperno et al.,\n2016) to commonsense reasoning (Bisk et al., 2019). The evolution of task performance provides a\ncue for the type of linguistic knowledge learned by the model by a certain training checkpoint."}, {"title": "DATASETS", "content": "As we consider the relationship between the degree of compositionality and geometric feature\ncomplexity, we create a custom grammar whose compositional structure we can control. In addition,\nwe replicate experiments on The Pile in order to compare results to a general slice of natural language."}, {"title": "CONTROLLED GRAMMAR", "content": "Our stimulus dataset consists of grammatical nonce sentences from the grammar illustrated in Figure 1.\nTo create the grammar, we set 12 semantic categories and randomly sample a vocabulary of 50 words\nfor each category, where the categories' vocabularies are disjoint. The categories include 5 adjective\ntypes (quality, nationality, size, color, texture), 2 noun types (job, animal) and 1 verb type. We use a\nsimple, fixed syntactic structure by ordering the word categories:\nThe [quality1.ADJ][nationality1.ADJ][job1.N] [action1.V] the [size1.ADJ][texture.ADJ]\n[color.ADJ][animal.N] then [action2.V] the [size2.ADJ][quality2.ADJ][nationality2.ADJ]\n[job2.N].\nThis produces sentences that are 16 words long. The order is chosen so that the generated sentences\nare grammatical and that the adjective order complies with the accepted ordering for English (Dixon,\n1976). Vocabularies are chosen such that the sentences are semantically coherent. For example, for\nthe first verb, the agent is a person and patient is an animal, so the possible verbs are constrained to\npermit \"walks\", but not \"types\". The vocabularies for each category may be found in Appendix E.\nAlthough the syntactic structure and individual vocabulary items are likely seen during training, words\nare sampled independently for each category without considering their probability in relationship to\nother words in the sentence. Therefore, generated sentences are highly unlikely to be in the training\ndata. Then, when encountering these sentences for the first time, a frozen LM must successfully\nconstruct their meanings from the meanings of their parts, or compositionally generalize."}, {"title": "Controlling compositionality", "content": "We modify the grammar in order to vary the dataset's degree of\ncompositionality. While linguistic compositionality spans many interpretations (Hupkes et al., 2019),\nwe are interested in two specific types: (1) composition of forms, or combinatorial complexity of the\ndataset, where a dataset is more compositional if it contains more unique word combinations; (2)\ncomposition of meanings, or sentence-level compositional semantics, where sentence meaning is\ncomposed, via syntax, from word meanings.\nFirst, to control for dataset combinatorial complexity, we couple the values of k contiguous word\npositions for k = 1\u00b7\u00b7\u00b74. That is, the sequence's atomic units are sets of k adjacent words, or k-grams,\nsampled independently. This constrains the degrees of freedom in sampling to $l/k$ where $l = 12$ is\nthe number of categories: for instance, in the 1-coupled setting, each word is sampled independently,\nhence 12 degrees of freedom; in the 2-coupled setting, each bigram is sampled independently, hence\n6 degrees of freedom. Varying k maintains the dataset's unigram distribution by design, but constrains\nthe dataset's k-gram distributions, or combinatorial complexity.\nTo investigate compositional semantics, we randomly shuffle the words in each sequence. This\ndestroys syntactic coherence, and in turn, the overall meaning of the sentence. It instead preserves\nsuperficial distributional properties like word count and word co-occurrences at the sentence level,\nas well as unigram frequencies (see Figure 1 right). Then, LM behavior on grammatically sane\nvs. shuffled sequences proxies compositional vs. lexical-only semantics.\nFor each setting in k \u2208 {1.4} \u00d7 {sane, shuffled}, we sampled a dataset of N = 50000 sequences,\nthen randomly split into 5 disjoint sets of 10000 sequences. Results are reported across data splits."}, {"title": "Measuring formal and semantic compositionality", "content": "Form compositionality is controlled by the\ndataset combinatorial complexity. We quantify form compositionality of the controlled dataset by\nits Kolmogorov complexity, estimated using gzip, a popular lossless compression algorithm. We\nestimate the Kolmogorov complexity for k \u2208 {1.4} \u00d7 {sane, shuffled} by the gzip-compressed\ndataset size in kilobytes (kb), and then compute the Spearman correlation to Ia and d for each layer.\nMeaning complexity differs from form complexity. For example, the data [cat, lion, puma] are related\nsemantically but not formally. As there is no unified definition for semantic complexity (Pollard &\nBiermann, 2000; Chersoni et al., 2016), we do not attempt to quantify it. But, as sane sequences are\ngrammatical and semantically coherent, it is guaranteed for sane datasets that meaning complexity is\nmonotonic in form complexity. In addition, as shuffling removes sequence-level semantics, meaning\ncomplexity is guaranteed to be lower on shuffled compared to sane text, by definition."}, {"title": "THE PILE", "content": "Although we focus on the controlled grammar in order to vary compositionality, to ensure that\nresults are not an artifact of our prompts, we replicate experiments on The Pile, a general slice of\nnatural language consisting of encyclopedic text, social media, reviews, news articles, and books. We\nuniformly sample N = 50000 sequences in The Pile, each consisting of 16 words, the same length\nas sequences in the controlled grammar, and report results over 5 random data splits."}, {"title": "MEASURING FEATURE COMPLEXITY VIA DIMENSIONALITY ESTIMATION", "content": "We are interested in how the geometric complexity of representations reflects the inputs' degree of\ncompositionality. In particular, we consider representations in the Transformer's residual stream (El-\nhage et al., 2021). Because sequence lengths may slightly vary due to the tokenization scheme, in line\nwith prior work (Cheng et al., 2023; Doimo et al., 2024), we aggregate over the sequence by taking\nthe last token representation, as, due to causal attention, it is the only to attend to the entire context.\nFor each layer and dataset, we compute both a nonlinear and a linear measure of dimensionality.\nNonlinear and linear dimensionality have key conceptual differences. The nonlinear Ia is the number\nof degrees of freedom, or latent features, needed to describe the underlying manifold (Campadelli"}, {"title": "RESULTS", "content": "We find that representational dimensionality reflects compositionality in ways that are predictable\nover pre-training and model scale. First, we show that language models represent linguistic data on\nlow-dimensional, nonlinear manifolds, but in high-dimensional linear subspaces that scale linearly\nwith the hidden dimension. Then, we show that, over training, geometric feature complexity is\ninformative of an LM's linguistic competence, such that both exhibit a nontrivial phase transition"}, {"title": "NONLINEAR AND LINEAR FEATURE COMPLEXITY SCALE DIFFERENTLY WITH MODEL SIZE", "content": "Like in previous work (Cai et al., 2021; Valeriani et al., 2023; Cheng et al., 2024), we confirm that\ninput data are represented on a nonlinear manifold with orders-of-magnitude lower dimension than\nthe embedding dimension. In particular, for both the controlled dataset, see Figure 2, and for The\nPile, see Figure H.1, we find that Ia ~ O(10) while linear d, D ~ O(103).\nOur novel finding is that nonlinear and linear dimensionality measures scale differently with model\nsize. We fit linear regressions $D ~\\langle d\\rangle_{layer}$ and $D ~ \\langle Id\\rangle_{layer}$ for each setting in k \u2208 {1.4} \u00d7\n[sane, shuffled], as well as for The Pile. Linear effect sizes a, correlation coefficients R, and p-values\nfor each setting are reported in Tables G.1 and H.1 (Pile), and the curves themselves found in\nFigures 2 and H.1 (Pile). For the controlled dataset, d scales linearly with hidden dimension D,\nshown in Figure 2 (right); all cases show a highly significant linear fit with R > 0.99 and p < 0.005\n(Table G.1). Meanwhile, Ia stabilizes to a low range ~ O(10) regardless of D, see Figure 2 (left):\nhere, in all cases, the effect sizes a \u2248 0 and fits are not statistically significant (Table G.1). On The\nPile, Figure H.1 and Table H.1 similarly show that d \u221d D, where the linear relationship is highly\nsignificant; the high effect size a = 0.81, in this case, indicates that the model tends to fill the ambient\nspace such that d \u2248 D. While for The Pile, Id \u221d D (R = 0.95, p < 0.001) as well, the tiny effect\nsize a = 0.002 shows that Id changes negligibly with respect to D, seen in Figure H.1 (left).\nThese results highlight key differences in how linear and nonlinear dimensions are recruited: LMs\nglobally distribute representations to occupy d \u221d D dimensions of the space, but their shape is locally\nconstrained to a low-dimensional (Ia) manifold. Robustness of Ia to scaling the hidden dimension\nsuggests that LMs, once sufficiently performant, recover the degrees of freedom underlying the data."}, {"title": "EVOLUTION OF REPRESENTATIONAL GEOMETRY TRACKS EMERGENT LINGUISTIC\nABILITIES OVER TRAINING", "content": "We just saw how dimensionality scales with size, and now we investigate its change over time. We\nfind that feature complexity is highly related to the LM's linguistic capabilities, assessed using the\neval-harness benchmark performance, over training. Figure 3 shows the evolution of Id on the k = 1\ndataset (top), where each curve is one layer, with the evolution of LM performance on the benchmark\ntasks (bottom), where each curve plots performance on an individual task."}, {"title": "REPRESENTATIONAL COMPLEXITY REFLECTS INPUT COMPOSITIONALITY", "content": "We just established that feature complexity is informative of when models gain complex linguistic\ncapabilities that, by definition, require compositional understanding. Now, we establish our key\nresult, which is that feature complexity encodes input compositionality, both when considering\nformal compositionality, or data combinatorial complexity, as well as meaning compositionality, or\nsentence-level semantics. We first show that this holds for fully-trained models that have reached\nfinal linguistic competency. Then, using evidence from the training phase of the model, we show\nthat the correspondence between feature complexity and input compositionality is present first as an\ninductive bias of the model that encodes formal complexity; but then, that it persists due to learned\nsyntactic and semantic features that encode meaning complexity. Lastly, we further develop the\ncoding differences between d and Ia, confirming an existing hypothesis in the literature (Recanatesi\net al., 2021) that they respectively encode formal and semantic complexity of inputs.\nData combinatorial complexity On fully-trained models, representational dimensionality pre-\nserves relative dataset compositionality. Figure 4 shows for fully-trained Pythia 410m, 1.4b, and\n6.9b that Id and d increase with the degree of formal compositionality within both sane (solid) and"}, {"title": "DISCUSSION", "content": "We have studied language model compositionality from a geometric and dynamic perspective. Using\na carefully curated synthetic dataset, we found strong relationships between the compositionality of\nlinguistic expressions and the dimensionality of their representations. On one hand, representational\ndimensionality is positively correlated to datasets' formal, or combinatorial, complexity. On the other\nhand, grammatical sequences, whose semantics are composed via syntax, tend to exhibit a higher\nnon-linear dimensionality, but a lower linear dimensionality, than agrammatical shuffled sequences.\nWe showed that the positive relationship between compositionality and dimensionality is an inductive"}]}