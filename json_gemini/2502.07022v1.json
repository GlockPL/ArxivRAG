{"title": "AIMS.AU: A DATASET FOR THE ANALYSIS OF\nMODERN SLAVERY COUNTERMEASURES IN\nCORPORATE STATEMENTS", "authors": ["Adriana Eufrosiana Bora", "Pierre-Luc St-Charles", "Mirko Bronzi", "Ars\u00e8ne Fansi Tchango", "Bruno Rousseau", "Kerrie Mengersen"], "abstract": "Despite over a decade of legislative efforts to address modern slavery in the supply\nchains of large corporations, the effectiveness of government oversight remains\nhampered by the challenge of scrutinizing thousands of statements annually. While\nLarge Language Models (LLMs) can be considered a well established solution\nfor the automatic analysis and summarization of documents, recognizing concrete\nmodern slavery countermeasures taken by companies and differentiating those\nfrom vague claims remains a challenging task. To help evaluate and fine-tune\nLLMs for the assessment of corporate statements, we introduce a dataset composed\nof 5,731 modern slavery statements taken from the Australian Modern Slavery\nRegister and annotated at the sentence level. This paper details the construction\nsteps for the dataset that include the careful design of annotation specifications,\nthe selection and preprocessing of statements, and the creation of high-quality\nannotation subsets for effective model evaluations. To demonstrate our dataset's\nutility, we propose a machine learning methodology for the detection of sentences\nrelevant to mandatory reporting requirements set by the Australian Modern Slavery\nAct. We then follow this methodology to benchmark modern language models\nunder zero-shot and supervised learning settings.", "sections": [{"title": "INTRODUCTION", "content": "The proliferation of legal mandates requiring corporations to disclose specific information regarding\ntheir human rights and environmental actions has necessitated the development of robust platforms\nand tools to facilitate compliance analysis. In line with other countries, the Australian Modern\nSlavery Act of 2018 (the AU MSA, or the \"Act\u201d, Australian Government, Act No. 153, 2018) requires\nover 3000 corporations to detail their efforts to combat modern slavery within their operations and\nsupply chains (Australian Government, Attorney-General's Department, Modern Slavery Business\nEngagement Unit, 2023). The resulting number of freeform, annually-published statements worldwide\nexceeds the resources allocated by supervisory bodies to monitor modern slavery compliance. While\nnumerous datasets have been created to support the development of automated approaches for text\nsummarization and understanding such as in the medical and legal domains (Zambrano Chaves et al.,\n2023; Guha et al., 2023), there exists a gap in large-scale datasets that help detect and extract relevant\ninformation explicitly mandated by this type of legislation from corporate statements. We address\nthis gap by introducing a novel dataset tailored to the analysis of modern slavery statements, focusing\non the extraction of pertinent information as specified by the Act.\nTraditional approaches in machine learning for legal and declarative text understanding have primarily\ncentered on summarization and synthesis (Abdallah et al., 2023; Niklaus et al., 2024; Martinez-Gil,\n2023). These methodologies aim to condense lengthy documents into concise summaries or to\ninterpret their key points and link them with a given query. The introduction of legislation that"}, {"title": "BACKGROUND", "content": "Modern slavery describes situations where coercion, threats, or deception are used to exploit victims\nand deprive them of their freedom. It encompasses any situation of exploitation that a person cannot\nrefuse or leave due to threats, violence, coercion, deception, or abuse of power (Walk Free, 2022a).\nIn 2021, an estimated 50 million people were subject to modern slavery, with 28 million in forced\nlabor. This issue is believed to affect all industries worldwide, with industries such as agriculture,\nmanufacturing, and construction being at higher risk.\nA critical impediment to eradicating modern slavery is the lack of transparency and accountability\nin corporate efforts to eliminate it from their supply chains. Without clear due diligence, reporting\nrequirements and oversight, it is difficult to hold companies responsible for unethical practices\nand recognize those that adhere to ethical standards. To address this issue, many governments\nhave enacted legislation mandating companies to increase transparency in their supply chains. The\nmovement began with the California Transparency in Supply Chains Act of 2010, which required\nlarge retailers and manufacturers doing business in California to disclose their efforts to eradicate\nslavery and human trafficking from their supply chains. This was followed by the UK's Modern\nSlavery Act of 2015, the first national law of its kind, mandating companies to publish a slavery and\nhuman trafficking statement approved by their governing body and posted on their website. However,\nthese early laws primarily focused on disclosure without specifying mandatory reporting criteria or\nrobust enforcement mechanisms (McCorquodale, 2022)."}, {"title": "DATASET DESCRIPTION", "content": "Our proposed dataset, AIMS.au, is a combination of modern slavery statements published in PDF\nformat by corporate entities and of sentence-level labels provided by human annotators and domain\nexpert analysts. As shown Figure 2, a total of 5,670 statements were processed by hired annotators\nwith respect to the three basic reporting criteria of the Act to determine whether each statement is\napproved, signed, and has a clearly-identified reporting entity. The other more advanced reporting\ncriteria (previously shown in Figure 1) involve nuanced interpretations and required higher levels of\nscrutiny; for these, a subset of 4,657 statements that were found to be of a reasonable length were\ndouble annotated by hired annotators. Lastly, two specialized \"gold\" subsets with each 50 unique\nstatements were created by experts to allow for evaluations with higher reliability across all criteria.\nThe first gold subset was annotated by a single expert and validated through team discussions, while\nthe second gold subset underwent a collaborative annotation process involving three experts. In\nall cases, disagreements were discussed until the experts achieved consensus. Given all these data\nsubsets, we propose that future research utilizes statements annotated by hired workers for model\ntraining, statements in the first \u201cgold\u201d subset for model validation, and statements in the second gold\nsubset for model testing; this should provide optimal trust in model performance assessments.\nThe final result is over 800,000 labeled sentences across 5,731 unique modern slavery statements\ncovering 7,270 Australian entities between 2019 and 2023. As outlined in the following section and\nin Appendix E, the annotation process was highly complex and resource-intensive, far from being\na low-cost crowdsourced task. This process took over a year and a half to complete and required a\nlarge team of highly skilled annotators, working under the close supervision of experts. Below, we\ndetail the steps involved in the collection and preprocessing of statements, we discuss the choices\nthat were made before and during the annotation process, and we provide summary statistics of our\nresulting dataset."}, {"title": "BENCHMARK EXPERIMENTS", "content": "Splitting training and evaluation data. For training and evaluation purposes, we cluster statements\nbased on their associated entities and trademarks. We then assign each statement cluster to either\nthe training set, validation set, or test set. This method ensures that similar statements made by\nrelated entities or by the same entity across different years are assigned to the same set, effectively\npreventing data leakage. For validation and testing, we created \"gold\" sets of statements that were\nannotated exclusively by extensively trained members of our team based on multiple rounds of review\nand discussion. Each of these sets contains 50 statements: the validation set was annotated by a\nsingle analyst, while the test set was annotated collaboratively by three analysts. These gold sets\naim to minimize label noise, which is more prevalent in annotations provided by external annotators.\nBased on our observations, this noise primarily consists of omissions, such as missed relevant text.\nWe emphasize that omissions are less problematic in the gold set annotations, where we use the\nunion of multi-labeled sentences from multiple annotators; indeed, the likelihood of all annotators\nomitting exactly the same text is low. The statements in both gold sets were randomly selected based\non clustering results while ensuring they were not used elsewhere, such as in the examples for the\nannotation specifications. We handled the statements and annotations with care (particularly those in\nthe gold sets) to prevent indirect leakage to future generations of language models (Balloccu et al.,\n2024).\nWe detail limitations of our dataset in Section 6 and in Appendix F. For more specific details on the\npreparation of our dataset and on its contents, we refer the reader to Appendix D.\nIn this section, we outline our experimental setup and present the results of benchmarking various\nmodels for detecting sentences relevant to the mandatory reporting requirements of the Act. We\nevaluate the performance of these models under both zero-shot and fine-tuning settings to assess their\neffectiveness in extracting mandated information from statements. We then analyze the results to\nidentify key insights and potential areas for improvement.\nTask definition. Our proposed dataset includes a variety of labels that models could predict; these\nlabels are detailed in Appendix D. For conciseness and clarity, we focus on a task that we believe\nwill be of greatest interest to the machine learning community: predicting relevant or irrelevant\nlabels according to our eleven questions. We frame this task as a sentence-level binary classification\nproblem which we evaluate across the eleven questions using the F1 metric. We selected this metric\nover accuracy because it allows us to identify cases where models simply learn to predict all sentences\nas irrelevant, since those are over-represented in our dataset (see Figure 4).\nFor the statements that are double annotated by hired workers, we adopt a \u201cunion\" label combination\nstrategy, where a sentence is considered relevant if any annotator marks it as such. This approach\naddresses the possibility that individual annotators may have missed relevant text in some statements.\nWe suggest that future works explore more sophisticated methods for leveraging annotator disagree-\nments as a supervision signal. For our current experiments, models are evaluated exclusively using\nthe subsets of \"gold\" annotated statements. Since these gold sets contain high-quality annotations,\ntheir smaller size (roughly 7000 sentences each) with respect to the overall dataset size should not\nsignificantly impact the reliability of model evaluations. Furthermore, this approach helps us, as\nwell as future researchers, avoid incurring significant API usage costs when using state-of-the-art,\nclosed-source language models for large-scale evaluations.\nEvaluated models. We conduct our experiments using a range of language models that includes four\nopen models DistilBERT (Sanh et al., 2020), BERT (Devlin et al., 2019), Llama2 (7B) (Touvron\net al., 2023) and Llama3.2 (3B) (Dubey et al., 2024) and two closed models, namely OpenAI's\nGPT3.5 Turbo and GPT40 (see Appendix G for more details). We use the OpenAI and Llama3.2 (3B)\nmodels to evaluate zero-shot (prompt-based) approaches, and we compare them with DistilBERT,\nBERT, Llama2 (7B) and Llama3.2 (3B) models fine-tuned directly on statements annotated by hired\nworkers. Our experiments are structured based on two input data setups: in the first (\"No context\"\nsetup), models only have access to the target sentence being classified; in the second (\"With context\"\nsetup), we provide additional context by including up to 100 words balanced before and after the\""}, {"title": "RELATED WORKS", "content": "AI for analyzing supply chain disclosures under the California Transparency Act. A few\ninitiatives have considered machine learning to analyze statements in response to modern slavery\nlegislation in the literature. For instance, LegalBench (Guha et al., 2023) proposed a benchmark for\nevaluating legal reasoning capabilities in language models. It consists of 162 tasks crafted by legal\nexperts, and one of these is related to supply chain disclosures under the California Transparency in\nSupply Chains Act. The analysis of roughly 400 statements with one or two pages each using modern\nlanguage models reveals only an accuracy of around 75%. Similar to the high-level decision process\nused by analysts, the proposed classification approach for this task relies on statement-level decision\nmaking for a limited set of questions. The researchers discuss in their report how model performance\ndiminishes in tasks involving longer text or more numerous questions, which suggests that scaling\nthis statement-level decision making strategy to much larger statements is probably not ideal.\nAI for the analysis of UK modern slavery statements. Despite numerous studies analyzing a\nhandful of modern slavery statements manually (details in Appendix A), only a few have investigated\nthe use of machine learning to date. For instance, modern slavery statements from the UK are\nanalyzed without supervision using topic modeling (Nersessian & Pachamanova, 2022; Bora, 2019).\nWhile this approach allows the authors to monitor disclosure trends and correlate them across different\nstatements, it is unable to analyze each statement and differentiate vague claims and promises from\nsubstantive actions. Consequently, this approach cannot adequately verify compliance with respect to\na specific legislation. Based on their analysis, the authors highlight that many companies \u201canchor\" \ntheir disclosures in broader human rights language and that they emphasize their engagement in\nsocial causes in an effort to bolster their company's social reputation. This underlines the challenge of\ncarefully avoiding distractions while assessing whether a statement contains mandated information.\""}, {"title": "CONCLUSION", "content": "Our work presents a significant contribution to the field of machine learning and natural language pro-\ncessing by introducing a manually annotated dataset of modern slavery statements that is specifically\ncurated to determine whether companies meet the mandatory reporting requirements outlined by the\nAustralian Modern Slavery Act. This dataset is particularly valuable due to the unique and challenging\nnature of the sentence relevance classification task, characterized by vague and distracting text, as\nwell as by the large amount of context required to understand the most complicated statements.\nWhile this dataset provides a broad collection of annotated statements for future machine learning\nexperiments, several limitations should be acknowledged. First, the reliance on external annotation\nservices, despite extensive training and oversight, may introduce inconsistencies and biases in the\nlabeled data. Annotators' varying interpretations of vague language and subjective judgment in\nidentifying relevant information could affect the overall quality and consistency of the annotations.\nAnother limitation involves figures and tables within statements, which cannot be easily analyzed\nwithout OCR or without a vision model. Although we can limit the scope of models to only focus on\nthe extraction of relevant text that is not embedded inside figures or tables, some necessary context\nmight sometimes be missing in order to understand a human annotator's decision. Lastly, we chose\nnot to differentiate past and future information based on reporting periods to simplify the annotation\nprocess. In other words, corporations often detail past actions or future plans within their statements,\nand we consider all such disclosures relevant. This approach may complicate the assessment of\nwhether a reporting entity meets the Act's requirements for a specific period, as it necessitates\nclassifying relevant text according to each reporting period. We discuss potential solutions to these\nlimitations in Appendix F.\nWe have conducted evaluations on modern language models, establishing performance benchmarks\nusing both zero-shot and fine-tuning approaches. These benchmarks will serve as comparison\nbaselines for future research in this domain. Our findings underscore the necessity of high-quality,\ncurated datasets to reliably train and evaluate language models, especially in tasks that demand\nnuanced understanding and contextual analysis. Despite the promising results, there is significant\nroom for future improvements, including the exploration of noisy label classification and more\nsophisticated context-handling techniques. Future research could also investigate the potential\nof integrating Vision-Language Models (VLMs, Bordes et al., 2024) to enhance the accuracy of\ninformation extraction in complex documents. Lastly, as we highlighted in Appendix J, this dataset\ncan be considered a key resource for other relevant studies and tools tackling mandatory reporting\nlegislation on business and human rights, such as the UK Modern Slavery Act UK Government (2015)\nand the Canadian Fighting Against Forced Labour and Child Labour in Supply Chains Act Canadian\nGovernment (2023)."}, {"title": "DATA AVAILABILITY AND MAINTENANCE STRATEGY", "content": "For reviewing purposes, a data sample that is representative of the final dataset is available via THIS\nLINK, and the complete dataset will be made available online upon acceptance with official links\nadded directly to the paper. At that point, download links for the dataset along with evaluation scripts,\nPython classes for data loading, and baseline experiment configuration files will be available in a\ndedicated GitHub repository. This repository will also be linked to a Digital Object Identifier (DOI)\nto ensure easy reference and citation.\nWe will make the dataset available in two formats: HDF5 (The HDF Group) and Activeloop DeepLake\n(Hambardzumyan et al., 2022). The HDF5 format is widely used across various domains and pro-"}, {"title": "EXAMPLES OF DISCLOSURES", "content": "In developing the annotation guidelines, our goal was to assist annotators in identifying concrete\nsupporting evidence in statements. This was necessary as despite legislative mandates for specific\ndisclosures, companies often provide vague, ambiguous, or distracting information that obstructs\neffective monitoring and progress. Table 4 provides, for all our questions related to the Mandatory\nCriteria of the Act, fictitious examples of: 1) relevant information; 2) irrelevant information due to\nambiguity (i.e. due to a lack of context); 3) irrelevant information due to vagueness (i.e. unacceptable\nno matter the context); and 4) distracting information. These examples are inspired by the contents\nof real statements and highlight the significant challenge of distinguishing between relevant and\nirrelevant information."}, {"title": "AIMS.AU DATA CARD", "content": "Dataset summary. See Section 4 of the paper.\nLanguages. The dataset contains English text only.\nDomain. Long, freeform statements made by corporate entities.\nAdditional details. The dataset contains modern slavery statements originally published in PDF\nformat by Australian corporate entities between 2019 and 2023, metadata for those statements, and\nannotations (labels) provided by hired workers and ourselves. Additional unannotated statements\npublished over the same period and beyond are also packaged in the dataset as supplementary data\nfor unsupervised learning experiments.\nMotivation. We publish this dataset to support the development and evaluation of machine learning\nmodels for extracting mandated information from corporate modern slavery statements. Our aim is to\nfacilitate research in this domain and foster future efforts to assess companies' compliance with the\nAustralian Modern Slavery Act and other similar legislation."}, {"title": "META INFORMATION", "content": "Dataset curators. Withheld for anonymity; will be specified here at the camera-ready deadline.\nPoint of contact. Withheld for anonymity; will be specified here at the camera-ready deadline.\nLicensing. The dataset is released under the Creative Commons Attribution 4.0 International (CC\nBY 4.0) license.\nFunding sources. Withheld for anonymity; will be specified in the paper's acknowledgments at the\ncamera-ready deadline."}, {"title": "DATASET STRUCTURE", "content": "Data format and structure. We structure our dataset so that one \"instance\" corresponds to a single\nstatement. Each statement is associated with a unique identifier, a PDF file, and a set of twelve\nmetadata fields, all provided by the Australian Modern Slavery Register. These metadata fields are:\n\u2022 Annual revenue;\n\u2022 Countries where headquartered;\n\u2022 Covered entities;\n\u2022 Industry sectors;\n\u2022 Overseas obligations;\n\u2022 Reporting period end date;\n\u2022 Reporting period start date;\n\u2022 Publication date;\n\u2022 Publication year in the register;\n\u2022 Submission date;\n\u2022 Associated trademarks;\n\u2022 Statement type (normal or joint).\nThe PDFs are freeform, allowing reporting entities the flexibility to choose their format; some use\na brochure-style layout, while others incorporate extensive background images or unique design\nelements. In addition to the provided metadata, we enhance these statements with several annotated\nfields, filled by our hired annotators or ourselves. These fields capture critical information such\nas compliance with reporting requirements and supporting content, as detailed in the next few\nparagraphs."}, {"title": "ANNOTATION PROCESS", "content": "\u0391\u039d\u039dOTATION GUIDELINES\nText extraction and labeling workflow for C2 (\u201csupply chains\u201d)\nDoes the reporting entity describe its supply chains?\n\u2192 Yes, the statement describes the supply chains of the reporting entity:\n\u2022 Copy-paste the text passages from the statement that justify that the reporting entity\ndescribed its supply chains.\n\u2022 If any relevant information comes in other formats than text, fill in the required\ninformation in the \"Visual Element\u201d fields: note the page where the information is\nfound, and extract any relevant text (if possible).\n\u2192 No, the statement does not describe the reporting entity's supply chains:\n\u2022 Copy-paste the exact text passages from the statement that justifies that the entity\ndoes not meet this criterion, OR\n\u2022 If no information is found about this criterion, set the \"No relevant information\nfound\" flag.\n\u2192 Unclear, in any other case:\n\u2022 Select this label if the information found is unclear or there are other concerns.\n\u2022 If you decide to select this label, you have to provide an explanation that justifies\nyour decision as supporting text.\nWe provide a copy of our annotation specifications document as supplementary material with this\nappendix. This document contains guidelines for hired workers to annotate statements according\nto our eleven questions on the Mandatory Criteria of the Act (listed in Section 2 of the paper). It\nincludes detailed instructions on handling non-contiguous text, intricate formatting, sections with\nembedded text, headings, and dates. Following the general guidelines, we outline the eleven questions\nrelated to the Mandatory Criteria and how to address them. Each of the first six Mandatory Criteria\nis associated with a question; for example, for C1, we ask which entities covered by the statement\nare the \"reporting entities\". Exceptions were made for C2 and C4, as these criteria encompass\nmultiple disclosure topics. Specifically, C2 is divided into three questions covering the descriptions of\noperations, governance structure, and supply chains, while C4 is split into two questions addressing\nthe descriptions of remediation actions and risk mitigation actions. We did not include a direct\nquestion for C7 (\"any other relevant information\") due to its subjective nature. Instead, we request\nthat any relevant information be extracted in response to the appropriate questions. We note that\nthis criterion was also omitted in the Australian Government's annual analysis report (Australian\nGovernment, 2022). Besides, all instructions and questions are accompanied by numerous examples\nbased on real statements.\nFor each question, the annotators are presented with a labeling workflow; an example is given in\nFigure 5 for C2 (\"supply chains\"). Recognizing that ambiguous, vague, and distracting sentences can\nsometimes be challenging to assess, we provide annotators with the option to answer a question with\nan \"unclear\" label. This helped us understand confusing cases and improve our instructions during\nearly iterations on the guidelines. Ultimately, only a very limited number of \"unclear\" labels were\nobtained in the final annotated dataset, and these are not considered in our experiments.\nIn Figure 6 we present a highly simplified fictitious example of an annotated statement for the\nproposed tasks and labels, offering readers a clearer high-level overview. However, we strongly"}, {"title": "CONTRACTING AND QUALITY ASSURANCE DETAILS", "content": "We contacted and evaluated several companies offering professional annotation services, and short-"}]}