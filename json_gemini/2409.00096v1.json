{"title": "Non-instructional Fine-tuning: Enabling Instruction-Following Capabilities in Pre-trained Language Models without Instruction-Following Data", "authors": ["Juncheng Xie", "Shensian Syu", "Hung-yi Lee"], "abstract": "Instruction fine-tuning is crucial for today's large language models (LLMs) to learn to follow instructions and align with human preferences. Conventionally, supervised data, including the instruction and the correct response, is required for instruction fine-tuning. To obtain such data, some researchers prompted well-trained models like GPT-4 to generate instructions and correct responses. In this paper, we propose a novel approach that uses the first half of a random text from OpenWebText as the instruction and GPT-3.5-turbo or GPT-4-turbo to complete the text as the response. Despite the data being \"non-instructional\", we found that pre-trained LLMs fine-tuned on this data can gain instruction-following capabilities. This observation is verified by fine-tuning several well-known pre-trained LLMs (e.g., LLaMA-2-7B, LLaMA-3-8B, LLaMA-3-70B, Mistral-7B-v0.1). The \"non-instructional data\" also improved some models that underwent supervised fine-tuning and human preference alignment. Our LLaMA-3-70B-Instruct fine-tuned through \"non-instructional data\" is comparable with LLaMA-3.1-70B-Instruct on the Arena Hard leaderboard. We analyzed the \"non-instructional data\" and ensured it is devoid of content related to instruction fine-tuning. Our findings will inspire further investigation into how to develop instruction-following capabilities without explicit instruction-related data.", "sections": [{"title": "1 Introduction", "content": "In recent years, large language models (LLMs) like GPT-3 (Bai et al. 2022) (Bai et al. 2022) and LLAMA (Touvron et al. 2023) have showcased remarkable natural language processing capabilities across diverse domains (Zhao et al. 2021; Wei et al. 2023; Wan et al. 2023; Sun et al. 2023; Li et al. 2023a; Gao et al. 2023). Previous studies have introduced instruction fine-tuning to align LLM training objectives with user goals. These methods involve either fine-tuning the model on various tasks using human-annotated prompts and feedback (Ouyang et al. 2022), or supervised fine-tuning utilizing public benchmarks and datasets augmented with manually or automatically generated instructions(Wang et al. 2022). Among these approaches, Self-Instruct tuning (Wang et al. 2023) stands out as a simple and effective method of aligning LLMs with human intent. This is achieved by learning from instruction-following data generated by state-of-the-art instruction-tuned teacher LLMs.\nThis paper finds that LLMs with instruction-following capabilities can be learned from \"non-instructional data.\" In this context, \"non-instructional data\" refers to content that does not contain any explicit instructions. We employed publicly available datasets, such as OpenWebText (Radford et al. 2019), for ChatGPT(OpenAI et al. 2024) to continue writing. We demonstrate that data generated through distillation with continuous writing, even without explicit instructions, can enhance the capacity of LLMs to understand and execute tasks. This paper investigates novel methodologies that empower LLMs to learn human instructions from a wider range of data, thus eliminating the need for manually annotated or explicitly generated instructional data. Our contributions are summarized as follows:\n1. Introduce a simple framework for generating non-instructional datasets to finetune LLMs, enabling them to more effectively follow human instructions.\n2. Propose a methods for generating non-instructional data: conditional distillation and knowledge distillation with continuous writing.\n3. Propose a method of fine-tuning various LLMs using datasets generated by a novel approach. This method retains pre-fine-tuning scores on the Open LLM Leaderboard and significantly improves performance on the Arena Hard and MT Bench benchmark. Notably, our fine-tuned Meta-Llama-3-8b model demonstrated substantial gains on Arena Hard, compared to other strong SFT dataset, and the fine-tuned Meta-Llama-3-70b-Instruct model achieved the highest recorded score of 57.0, surpassing even the more advanced Meta-Llama-3.1-70b-Instruct. These results underscore the effectiveness of our fine-tuning approach in enhancing the instruction-following capabilities of large language models.\n4. Introduce the use of lora-base for model enhancement, demonstrating its effectiveness in improving performance. This technique involves merging the LoRA module fine-tuned on the foundation (base) model with the Instruct model, showcasing improvements across various benchmarks without additional training overhead."}, {"title": "2 Related Work", "content": "In the landscape of LLM distillation and fine-tuning, Stanford Alpaca (Taori et al. 2023) and Code Alpaca (Chaudhary 2023) underscore the utility of leveraging the LLaMA model and GPT variants to generate instructional content, demonstrating the power of distillation for tailored instruction following. Extending this concept, Alpaca-CoT (Si et al. 2023), with its focus on instruction tuning, and Baize (Xu et al. 2023), showcasing a novel self-chat method with ChatGPT for dialogue improvement, represent innovative strides in model enhancement. Vicuna (Chiang et al. 2023) and Koala (Geng et al. 2023), through user-shared conversations and web-collected dialogues, respectively, imply the indirect but significant influence from the distillation of high-caliber models like ChatGPT. (Peng et al. 2023) elucidates the direct application of GPT-4 for knowledge distillation, enriching the field with nuanced methodologies. UltraChat (Ding et al. 2023) further contributes to this narrative by presenting a large-scale, diverse dataset of instructional conversations, aiming to elevate the performance of open-source models like UltraLLaMA beyond existing benchmarks such as Vicuna.\nMAGPIE (Xu et al. 2024) introduces a scalable method to synthesize high-quality instruction data by leveraging the auto-regressive nature of aligned LLMs, such as LLaMA-3-Instruct, to self-generate instructions and responses without the need for human seed data or prompt engineering. By using this method, MAGPIE efficiently generates large-scale datasets that significantly enhance the performance of models fine-tuned on them, surpassing even officially aligned models like LLaMA-3-8B-Instruct on various alignment benchmarks. This positions MAGPIE as a critical advancement in the creation of alignment datasets, offering a robust alternative to traditional human-involved methods or synthetic data generation approaches that rely on extensive prompt engineering.\nThe methods mentioned above rely on instructional data to train LLMs. This paper represents the first attempt to explore the potential of acquiring instruction-following capabilities from non-instructional data."}, {"title": "3 Non-instructional Dataset Generation", "content": "In contrast to the intricate process traditionally involved in creating instruction-following datasets, which includes amassing extensive texts, categorizing them, and further formatting these texts into instruction-based dialogues, our methodology offers a streamlined alternative. We bypass the multifaceted stages of traditional data preparation, including the generation of instruction-formatted content, by directly distilling knowledge from substantial models like ChatGPT. Our framework is shown in Figure 1. In the following sections, we refer to our datasets as non-instructional datasets.\nOur approach employs 'halving and completion' on 80,000 pieces of data uniformly sampled from the OpenWebText(Radford et al. 2019) corpus sourced from Reddit. This method involves selecting a midpoint for halving that is uniformly sampled from between the first and the last quarter of the article (measured by word count), and then using an instruction-following LLM as the teacher model to (almost) deterministically generate the concluding half with a temperature of 0.0. Such a process leverages the inherent capabilities of LLMs to produce diverse and contextually rich content. Employing this kind of fine-tuning data mirrors continued unsupervised pretraining. We selected the latest and most cost-effective versions from OpenAI: gpt-3.5-turbo-0125 and gpt-4-0125-preview, as our teacher models for continuous writing. The outcomes of training on these distilled datasets across various models are elaborated in Section 5.\nWe will release the data under the terms and conditions of OpenAI's Terms of Use 1 and Usage Policies 2."}, {"title": "Distillation from Anthropic AI LLMs", "content": "As discussed in Section 5, some of the benchmarks we used employ GPT-4 as the judge. However, based on validation and discussions in (Zheng et al. 2023; Panickssery, Bowman, and Feng 2024; Li et al. 2023b), it has been noted that GPT-4 not only tends to favor its own generations but may also favor the generations of models fine-tuned on data distilled from itself. To demonstrate the robustness of our method under different strong LLMs as teacher models, we also performed distillation using Anthropic Al's Claude-3-Haiku, Claude-3-Sonnet, and Claude-3-Opus.\nDue to time and budget constraints, we limited our distillation to 10,000 pieces of OpenWebText data. Apart from adding a \"system prompt\": \"Please continue directly from the end of the given sentence without repeating it,\" the remaining settings were identical to those described in Section 3. This adjustment ensures that the process maintains consistency and leverages the same systematic approach used with OpenAI models, allowing for a fair comparison of the efficacy of distillation across different LLMs."}, {"title": "4 Experiment Setup", "content": "In our experiments, we utilize a diverse set of Large Language Models (LLMs) for fine-tuning with our dataset, alongside others for comparative analysis. The fine-tuning models include LLaMA-2-7B(Touvron et al. 2023), Meta-Llama-3-8B and Meta-Llama-3-70B (AI@Meta 2024), which come in both foundation and Instruct (chat) variants. We also employ the Mistral-7B-v0.1 series(Jiang et al. 2023), which includes Mistral-7B-v0.1, Mistral-7B-Instruct-v0.1, and Mistral-7B-Instruct-v0.2.\nThe finetuning procedure adopted in our study is relatively straightforward, utilizing the codebase provided by LLaMA-Factory\u00b3(hiyouga 2023), which is tailored for the efficient"}, {"title": "Finetuning Details", "content": "The finetuning procedure adopted in our study is relatively straightforward, utilizing the codebase provided by LLaMA-Factory\u00b3(hiyouga 2023), which is tailored for the efficient"}, {"title": "5 Evaluation", "content": "To ensure a fair comparison of model capabilities, we select three benchmarks: MT-Bench (Zheng et al. 2023), Open LLM Leaderboard (Beeching et al. 2023), and Arena Hard (Li et al. 2024).\nMT-Bench MT-Bench evaluates LLMs' dialogue and instruction-following capabilities using 80 multi-turn questions. Each response is scored by GPT-4 (gpt-4-0613) on a scale from 1 to 10, with an average score calculated over three rounds. It uses FastChat\u00b9 for model inference, ensuring efficient analysis. The scores for subsequent experiments are averaged over three inference runs.\nOpen LLM Leaderboard The Open LLM Leaderboards uses the Eleuther AI Language Model Evaluation Harness to evaluate models across six benchmarks. This comprehensive assessment covers reasoning, knowledge, and truthfulness. Scores are benchmarked against those on the leaderboard website for consistency.\nArena Hard Arena Hard evaluates LLMs with 500 complex, real-world questions from Chatbot Arena. Pairwise comparisons against a strong baseline (GPT-4-0314) using GPT-4-Turbo ensure robust assessment. This benchmark emphasizes real-world application, with high separability (87.4%) and agreement with human preference rankings (89.1%)."}, {"title": "IFEval Benchmark", "content": "To further evaluate the instruction-following capabilities of our models, we include the IFEval dataset (Zhou et al. 2023), specifically tests models on their ability to follow pattern-level or syntax-level instructions. This benchmark focuses on verifiable instructions\u2014clear, objective directives such as \"write in more than 400 words\" or \"mention the keyword of AI at least 3 times.\u201d Comprising 25 types of instructions and around 500 prompts, IFEval is designed to verify whether a model can adhere to precise instruction formats, including specific structural requirements. We include this benchmark in our evaluation to highlight the models' ability to comply with detailed instructional patterns."}, {"title": "Results", "content": "In Table 1, the performance of LLMs on MT-Bench, aiming to align more closely with human preferences on open-ended questions, is evaluated through a series of experiments using various fine-tuning datasets. We explore the impact of different data sources, including: original 80k data from OpenWebText (no continuous writing), data continuous writing using the llama-2-7b-chat model, gpt-3.5-turbo-0125 and gpt-4-0125-preview. The 'Template' refers to the template used during MT-Bench inference, as detailed in the documentation found here. The 'Fine-tuned Modules' denote the specific modules that were fine-tuned. The term 'lora' signifies using the corresponding backbone model to fine-tune LoRA adapters which are then merged with the same backbone model for enhanced performance. 'lora-base' is unique to Instruct/chat models, indicating their merging with adapters fine-tuned on their corresponding foundation model.\nThe impact of different data sources on LLaMA-2-7B can be observed by comparing ID 3, 4, 5 and 8. First, fine-tuning with OpenWebText data cannot improve the performance on the MT-Bench (ID 3 vs 1), and fine-tuning with data continuous writing by LLMs all improve the performance (ID 4, 5, 8 vs 1). The comparison clearly demonstrates that the LLaMA-2-7B model fine-tuned with data writing by the gpt-4-turbo model showcases superior performance on the MT-Bench (ID 8 vs 4, 5). This highlights the significance of selecting high-quality data writing by advanced models"}, {"title": "for fine-tuning purposes, leading us to primarily utilize the dataset from gpt-4-0125-preview as the cornerstone of this research.", "content": "for fine-tuning purposes, leading us to primarily utilize the dataset from gpt-4-0125-preview as the cornerstone of this research.\nThe effectiveness of non-instructional data not just for enhancing the foundation models but also for Instruct/chat variants. Additionally, LLaMA-2-7B-chat, Meta-Llama-3-8b-Instruct and Meta-Llama-3-70b-Instruct show improved performance upon fine-tuning with the non-instructional datasets (ID 9, 10 vs 2 and ID 23, 24 vs 21 and ID 28, 29 vs 26). Notably, even when chat/Instruct models are merged with LoRA modules fine-tuned on corresponding foundation models, an enhancement in performance is observed. This suggests that our LoRA modules possess a function akin to chat vectors(Huang et al. 2024).\nThis indicates a significant improvement in dialogue capabilities across both model types.\nReflecting on the nuances of our findings, it becomes apparent that the three foundation models, which have only undergone pretraining, and Instruct/chat models exhibit significant performance improvements on the MT-Bench when fine-tuned with our non-instructional data. This observation underscores the efficacy of our fine-tuning approach, not just for the foundation models but also for those specifically designed for Instruct/chat interactions.\nMoreover, an intriguing aspect of our analysis highlights that Instruct/chat models, even when merged with LoRA modules fine-tuned on corresponding foundation models,"}, {"title": "demonstrate enhanced performance. This outcome suggests that the LoRA modules can carry functionalities and add to other models, contributing to improved dialogue capabilities. Such advancements indicate a promising direction for refining the conversational abilities of language models, pointing towards the potential of targeted fine-tuning strategies to elevate the sophistication of conversational AI systems further.", "content": "demonstrate enhanced performance. This outcome suggests that the LoRA modules can carry functionalities and add to other models, contributing to improved dialogue capabilities. Such advancements indicate a promising direction for refining the conversational abilities of language models, pointing towards the potential of targeted fine-tuning strategies to elevate the sophistication of conversational AI systems further."}, {"title": "Claude distillation data", "content": "Our experiments involving Claude-3 distillation data reveal significant improvements across various models, supporting our hypothesis that non-instructional data from powerful LLMs can be highly beneficial. The results in Table 2 indicate that even smaller datasets distilled from Claude-3-Haiku, Claude-3-Sonnet, and Claude-3-Opus can effectively enhance the performance of models such as Mistral-7B-v0.1, Meta-Llama-3-8b, and Meta-Llama-3-70b.\nWhen comparing these results to our previous evaluations using gpt-4-0125-preview. data, it is evident that the Claude-3 distillation data is equally impactful. For instance, the Meta-Llama-3-70b-Instruct model fine-tuned on Claude-3 datasets consistently achieved high MT Bench scores, with a peak score of 9.00 using Claude-3-Opus. This underscores the robustness and utility of leveraging high-quality"}, {"title": "non-instructional data from strong LLMs for fine-tuning, aligning with our findings that such data can significantly boost model performance without relying on traditional instruction-following datasets.", "content": "non-instructional data from strong LLMs for fine-tuning, aligning with our findings that such data can significantly boost model performance without relying on traditional instruction-following datasets."}, {"title": "Comparison with alpaca data", "content": "Alpaca (Taori et al. 2023), developed by Stanford, is a well-known dataset for instruction fine-tuning based on the LLaMA model. It comprises 52,000 instruction-following demonstrations generated using OpenAI's text-davinci-003 model. In table 3, we compare the performance of Mistral-7B-v0.1 fine-tuned on the original Alpaca data, the GPT-4 distilled Alpaca data (Peng et al. 2023) 10 and a smaller, non-instructional dataset generated from GPT-4-Turbo and Claude-3-Haiku. Our results show that models fine-tuned on just 10,000 examples from GPT-4-Turbo and Claude-3-Haiku outperform those trained on the larger Alpaca dataset, including the GPT-4 distilled version. Specifically, the MT Bench scores for the GPT-4-Turbo and Claude-3-Haiku datasets are 6.75 and 6.87, respectively, compared to 5.67 for the original Alpaca and 6.56 for the GPT-4 distilled Alpaca. This demonstrates that a smallernon-instructional dataset can achieve superior performance, highlighting the efficiency and potential of our approach in instruction-following tasks."}, {"title": "Open LLM Leaderboard Results", "content": "Table 4 shows the results on the Open LLM Leaderboard. Here, the utilization of distilled data for fine-tuning exhibits a consistent trend across various models. Task-specific performance shifts due to non-instructional data offer insightful"}, {"title": "observations. Increases in scores on tasks like TruthfulQA and GSM-8K suggest that distilled data specifically bolsters models' capabilities in generating accurate, nuanced responses and understanding complex queries. Conversely, varied performance across tasks like ARC and HellaSwag indicates that the impact of non-instructional data can be multifaceted, enhancing certain model capabilities while not universally boosting performance across all tasks.", "content": "observations. Increases in scores on tasks like TruthfulQA and GSM-8K suggest that distilled data specifically bolsters models' capabilities in generating accurate, nuanced responses and understanding complex queries. Conversely, varied performance across tasks like ARC and HellaSwag indicates that the impact of non-instructional data can be multifaceted, enhancing certain model capabilities while not universally boosting performance across all tasks."}, {"title": "Non-instructional fine-tuning does not result in any decline in average scores. It underscores the efficacy of non-instructional datasets in maintaining or enhancing the performance of LLMs across a spectrum of benchmark tasks.", "content": "Non-instructional fine-tuning does not result in any decline in average scores. It underscores the efficacy of non-instructional datasets in maintaining or enhancing the performance of LLMs across a spectrum of benchmark tasks."}, {"title": "Results on Arena Hard", "content": "Performance of Models on Arena Hard Benchmark We compare the performance of fine-tuned models against their instruct counterparts in Table 5. The fine-tuned Mistral-7B-v0.1 shows a modest improvement over its instruct version, Mistral-7B-Instruct-v0.2.\nFor the Meta-Llama-3-8b series, while the fine-tuned base model gains instruction-following ability, it does not exceed the original instruct model. However, the lora-base version demonstrates a clear advantage, outperforming both the fine-tuned and original instruct models.\nThe Meta-Llama-3-70b models show a similar trend, where the fine-tuned base model improves but still lags behind its instruct counterpart. However, the fine-tuned Meta-Llama-3-70b-Instruct version achieves a remarkable score of 57.0, which not only surpasses its original instruct counterpart but also exceeds the performance of the more advanced llama-3.1-70b-Instruct (which achieved 55.7). This result represents the highest recorded score on Arena Hard in this paper, highlighting the efficacy of our fine-tuning approach with the 80k gpt-4-0125-preview dataset."}, {"title": "Impact of Teacher Model and Data Size on Fine-tuning Performance", "content": "Initially, our goal was to explore the impact of increasing data size from 80k to 300k on fine-tuning performance, based on the observation of the Data Size subsection of the next section that larger datasets generally lead to better results. However, due to budget constraints, we opted to use gpt-40-mini, a stronger variant in the same series as gpt-4-0125-preview, to generate the 300k dataset for distillation.\nAs shown in Table 6, the increase in data size from 80k to 300k significantly improves the model's Win Rate (WR), with the Meta-Llama-3-8b model achieving a WR of 33.28 when fine-tuned with the larger dataset. Additionally, even with the same 80k data size, using gpt-40-mini as the teacher"}, {"title": "model results in a higher WR compared to gpt-4-0125-preview, indicating the advantages of using a more powerful teacher model.", "content": "model results in a higher WR compared to gpt-4-0125-preview, indicating the advantages of using a more powerful teacher model."}, {"title": "Comparison with MAGPIE models", "content": "In this subsection, we compare our fine-tuning results with those achieved using the MAGPIE datasets, focusing on the Arena Hard and IFEval benchmarks. MAGPIE demonstrates strong performance, particularly with multi-turn dialog data distilled from LLaMA-3-70B-Instruct (MAGPIE-Pro-MT-300K-v0.111) and further improved with DPO alignment on"}, {"title": "The IFEval results show that while our LORA fine-tuned models are competitive with those using MAGPIE data, full-model fine-tuning with MAGPIE data and UltraFeedback alignment produces slightly higher scores.", "content": "The IFEval results show that while our LORA fine-tuned models are competitive with those using MAGPIE data, full-model fine-tuning with MAGPIE data and UltraFeedback alignment produces slightly higher scores."}, {"title": "Data Size", "content": "Figure 2 studies the impact of varying amounts of uniformly sampled non-instructional data by GPT-4-turbo for fine-tuning. This study is base on ID 13 of Table 1. The results of Mistral-7B-v0.1 on MT-Bench are reported. Notably, as the amount of distilled data increases, there is a general trend of improved average scores, although not strictly linear. The performance initially sees a significant rise when data size increases from 1k to 10k, suggesting that even"}, {"title": "a small amount of high-quality distilled data can substantially enhance model capabilities. However, between 10k and 20k data points, the performance slightly dips and then stabilizes.", "content": "a small amount of high-quality distilled data can substantially enhance model capabilities. However, between 10k and 20k data points, the performance slightly dips and then stabilizes."}, {"title": "Analyzing the Impact of Filtering Possible Instructional and Conversational Content on Model Performance", "content": "Since we cannot fully control the generation process, some instructional or conversational data may be generated during continuation and thus hidden in the non-instructional datasets. Readers may challenge that the improved performance on benchmarks such as MT-Bench and Arena Hard stems from these latent instructional or conversational data. In this subsection, by rigorously filtering out potential instructional or conversational content in non-instructional datasets, we rule out this possibility.\nWe used gpt-40 to detect instructional and conversational content, with the detailed prompt in Appendix B. Tables 8 (each with 2000 samples) shows minimal instructional (0.7%) and varying conversational content in non-instructional data.\nFor \"Random Article,\" we prompted gpt-40 with \"Randomly generate an article.\" Since these articles are not expected to contain instructional content, any detected instructional instances (0.5%) are likely false positives from GPT-4's detection.\nFor \"Random Conversation,\" we prompted gpt-40 with \"Randomly generate a dialog.\u201d These conversations are entirely dialog-based, hence the expected 100% conversational content, serving as a baseline to ensure our detection methods accurately identify conversational instances.\nTable 9 (1000 samples each) compares MT-Bench scores before and after filtering instructional and conversational content. The slight differences (e.g., GPT-4 continuous: 6.28 to 6.26) suggest that filtering does not significantly impact model performance. This indicates that the model's instruction-following capabilities are not derived from learning possible instructional or conversational content. Instead, it suggests that LLMs have an intrinsic ability to perform well even without explicit instruction-following data."}, {"title": "Despite feeding the first half of articles to GPT-4-Turbo without any explicit prompts (e.g., \"please continue the article\"), a few instances where the models do not continue but instead summarize or guess the user's intent (starting with uppercase letters) are identified.", "content": "Despite feeding the first half of articles to GPT-4-Turbo without any explicit prompts (e.g., \"please continue the article\"), a few instances where the models do not continue but instead summarize or guess the user's intent (starting with uppercase letters) are identified."}, {"title": "Changes in Responses of LLaMA-3-8B to an Arena Hard Question Before and After Fine-tuning", "content": "In this section, we analyze the responses of the LLaMA-3-8B model to an Arena Hard question before and after fine-tuning.\nThe example provided in Table 10 illustrates the response to an Arena Hard task before and after fine-tuning. To highlight the differences, we use color to indicate repeated sentences in the pre-fine-tuned response."}, {"title": "Question:", "content": "Question:\nFind root cause for this error: sshd[54785]: error:\nkex_exchange_identification: Connection closed by remote\nhost\nResponse Before Fine-tuning:\nI have a server running Ubuntu 16.04.4 LTS. I have a user that is\ntrying to ssh into the server and getting the above error. I have\ntried to ssh into the server from another server and it works\nfine....\nResponse After Fine-tuning:\nThis error indicates that the SSH connection was closed by the\nremote host during the key exchange process. This can occur\nfor several reasons, including: 1. **Network Issues**: If there\nare issues with the network connection between the client and\nthe server, such as packet loss or congestion, it can lead to the\nconnection being dropped.\n2. **Remote Host Configuration**: The remote host may have\na configuration issue that prevents it from accepting the SSH\nconnection. This could be due to firewall rules, incorrect SSH\nconfiguration, or a misconfigured SSH daemon.\n3. **Client Configuration**: The client may have a configura-\ntion issue, such as an incorrect key, an invalid configuration file,\nor a problem with the SSH client itself.\n4. **Timeouts**: If the remote host has a timeout set for the\nSSH connection, it may close the connection if it does not re-\nceive a response within a specified time.\n5. **Authentication Failure**: If the client attempts to authenti-\ncate with an invalid key or password, the remote host may close\nthe connection.\nTo troubleshoot this issue, you can try the following steps:\n1. **Check Network Connectivity**: Ensure that the client\ncan reach the remote host using other protocols (e.g., ping or\nHTTP).\n2. **Check SSH Configuration**: Verify that the SSH daemon\nis running and configured correctly on the remote host. Check\nthe SSH configuration file (/etc/ssh/sshd_config) for\nany relevant settings.\n3. **Check Client Configuration**: Ensure that the SSH\nclient is configured correctly. Check the SSH configuration file\n(~/.ssh/config or /etc/ssh/ssh_config) for any\nsettings that may be causing issues.\n4. **Check for Firewall Rules**: Ensure that the firewall on the\nremote host is not blocking SSH traffic.\n5. **Check for Timeout Settings**: If the remote host has a\ntimeout set, adjust it to a longer duration to see if that resolves\nthe issue.\n6. **Check for Authentication Issues**: If you are using a key,\nensure that the key is valid and that the correct key is being used\nfor authentication.\n7. **Check for Logs**: Review the SSH logs on the remote host\nfor any additional information about the connection attempt.\nIf the issue persists, consider checking for any recent changes\nto the configuration or environment that may have caused this\nbehavior."}, {"title": "The pre-fine-tuned response includes a repeated sentence highlighted in red, indicating that the model continued generating text without stopping, eventually reaching the token limit.", "content": "The pre-fine-tuned response includes a repeated sentence highlighted in red, indicating that the model continued generating text without stopping, eventually reaching the token limit."}, {"title": "This work introduces a novel approach for enabling instruction-following capabilities in pre-trained language models without relying on \"non-instructional data\".", "content": "This work introduces a novel approach for enabling instruction-following capabilities in pre-trained language models without relying on \"non-instructional data\"."}, {"title": "7 Conclusion", "content": "This work introduces a novel approach for enabling instruction-following capabilities in pre-trained language models without relying on \"non-instructional data\". Comprehensive experiments with various well-known pre-trained LLMs, including LLaMA and Mistral series models on several benchmarks, validate the effectiveness of our approach, with performance even surpassing models tuned on traditional instruction data. Further analysis reveals that the enhanced instruction-following capabilities do not stem from latent instructional content in the non-instructional datasets. This work may open up new avenues for training instruction-following LLMs because, compared to typical instruction-following datasets, which are usually generated in a supervised manner, the generation of non-instructional data is more scalable and less labor-intensive. For future work, we will further investigate how LLMs develop instruction-following abilities from non-instructional data."}, {"title": "8 Limitations", "content": "Our study reveals several limitations. Firstly, the mechanisms through which non-instructional data confers instruction-following abilities remain unclear, necessitating further research. Secondly, more comprehensive comparisons with GPT-4 and GPT-4-Turbo distilled Alpaca data are required. The impact of increasing data volume on model performance also needs investigation.\nAdditionally, expert evaluations are necessary to confirm whether the improvements on MT-Bench and Arena Hard reflect genuine advances or merely mimic the stylistic tendencies of GPT-4 and Claude-3. Lastly, the generalizability of our findings to broader real-world tasks remains uncertain, warranting further exploration."}, {"title": "A Finetuning Details", "content": "This appendix provides a comprehensive overview of the technical details concerning our finetuning process. Our choice to deploy the LLaMA-Factory repository for finetuning operations aimed to leverage its optimised environment for LLMs.\nThe LLaMa-Factory, accessible at https://github.com/hiyouga/LLaMA-Factory, is a resource specifically developed for the community to enable streamlined finetuning and experimentation with LLMs. It provides guidelines and pre-set configurations that significantly reduce the complexity involved in model optimization."}, {"title": "LLaMA-Factory Usage", "content": "The LLaMa-Factory, accessible at https://github.com/hiyouga/LLaMA-Factory, is a resource specifically developed for the community to enable streamlined finetuning and experimentation with LLMs. It provides guidelines and pre-set configurations that significantly reduce the complexity involved in model optimization."}, {"title": "Finetuning Specifications", "content": "\u2022 Finetuning Mode: Supervised Finetuning (sft)\n\u2022 Finetuning Module: LORA\n\u2022 Epochs: 3"}, {"title": "Computational Environment", "content": "Due to the computational demands of LLMs, finetuning operations were conducted using specialized hardware. Details of the computational environment, includ ing the specific versions of software and models used, are listed below:\n\u2022 GPU Model: A node with 8 V-100\n\u2022 LLaMA-Factory Version: 0.5.2"}, {"title": "Finetuning Command", "content": "The command used to initiate the finetuning process is detailed here for replication and validation purposes. Minor adjustments may be necessary based on the specific computational environment and model specifications."}, {"title": "B Prompt for Instructional Content Analysis", "content": "The prompt used for identifying potential instructional content within the datasets is as follows:\n1\n2\n3"}, {"title": "C Open LLM Leaderboard Tasks", "content": "Below are individual introductions to"}]}