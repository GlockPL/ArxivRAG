{"title": "Missing Modality Prediction for Unpaired Multimodal Learning via Joint Embedding of Unimodal Models", "authors": ["Donggeun Kim", "Taesup Kim"], "abstract": "Multimodal learning typically relies on the assumption that all modalities are fully available during both the training and inference phases. However, in real-world scenarios, consistently acquiring complete multimodal data presents significant challenges due to various factors. This often leads to the issue of missing modalities, where data for certain modalities are absent, posing considerable obstacles not only for the availability of multimodal pretrained models but also for their fine-tuning and the preservation of robustness in downstream tasks. To address these challenges, we propose a novel framework integrating parameter-efficient fine-tuning of unimodal pretrained models with a self-supervised joint-embedding learning method. This framework enables the model to predict the embedding of a missing modality in the representation space during inference. Our method effectively predicts the missing embedding through prompt tuning, leveraging information from available modalities. We evaluate our approach on several multimodal benchmark datasets and demonstrate its effectiveness and robustness across various scenarios of missing modalities.", "sections": [{"title": "1 Introduction", "content": "Humans perceive the world through various sensory modalities, such as seeing images and hearing voices, integrating these diverse sources to enhance comprehension. Similarly, a fundamental goal in artificial intelligence is to equip computers with the capability to effectively learn from multi-sensory data. Multimodal learning emerges as a promising approach to improve our understanding of complex data by leveraging multiple communicative modalities. In this context, significant advancements have been made in multimodal learning, particularly through self-supervised learning within the vision-language domain [7,32,35,36]. Furthermore, remarkable progress has been achieved in audio-visual learning [10], as well as in other multiple modalities."}, {"title": "2 Related Work", "content": "Missing Modality in Multimodal Learning Specific modalities available during training may be unavailable at inference, posing challenges in multimodal learning. [28] investigated the robustness of pretrained multimodal transformers when encountering incomplete modalities during inference. For addressing a more general scenario where the absence of modality may occur during either the training or testing phase, [24] leveraged missing-aware-prompts according to the missing case. However, both works rely on multimodal joint encoder pretrained with extensive image-text pairs, assuming the availability of large paired multimodal datasets. This assumption limits applicability in scenarios lacking such paired data. In contrast, our method uses unimodal models trained on unpaired data, respectively, eliminating the need for extensive paired datasets. Therefore, we offer a more adaptable approach to handling missing modalities, enhancing flexibility in multimodal learning without reliance on paired data.\nThere has also been some progress in addressing missing modalities by inferring them by modeling the probabilistic relationships between modalities. [29] proposes a method of Bayesian Meta-Learning to estimate the latent feature of the modality-incomplete data and reconstruct the features of the missing modality data. [39] proposes a strategy using shared encoder features from available modalities to generate modality-specific features of missed modality. These approaches are similar to ours in utilizing modality-specific encoders; however, our method focuses explicitly on the effective use of unimodal pretrained models. By efficiently fine-tuning unimodal models widely available and pretrained on extensive unlabeled datasets we ensure the maximal preservation of knowledge from pretraining. This distinction underscores our method's versatility and adaptability, suggesting its potential effectiveness in low-resource scenarios, such as handling multilingual text-image data.\nParameter-Efficient Transfer Learning As the field advances with substantial pretrained models based on transformer [38] architecture, various parameter-efficient adaptation approaches [12,13,44] have emerged, approximating the performance of full fine-tuning by updating only a subset of parameters. Concurrently, prompt-based learning [25, 26], initially successful in natural language processing, has shown promising results in computer vision tasks [15,41-43] as well, notably with vision transformer [8]. Inspired by these approaches, many recent works [17, 18, 46, 47] have been explored for adapting large pretrained vision-language models (e.g., CLIP [32]) without re-training the entire model."}, {"title": "3 Preliminaries", "content": "To maintain simplicity without losing generality, we consider a multimodal problem setting consisting of two modalities (M = 2), namely $m_1$ and $m_2$ (e.g., image and text). We further assume that these modalities do not always coexist, indicating the presence of missing modalities throughout both training and testing phases. Therefore, given a multimodal dataset $D = D^C \\cup D^{m_1} \\cup D^{m_2}$, it can be divided into three subsets: the modality-complete subset $D^C = \\{(x^{m_1}, x^{m_2},y_i)\\}$ and two modality-incomplete subsets $D^{m_1} = \\{(x^{m_1},y_j)\\}$ and $D^{m_2} = \\{(x^{m_2}, Y_k)\\}$ (e.g., image-only and text-only)."}, {"title": "4 Proposed Method", "content": "Although pretrained unimodal encoders with a late-fusion strategy generally perform well without fine-tuning, they may not be sufficient to attain optimal performance in certain multimodal downstream tasks. However, full fine-tuning, while potentially improving performance, is less favorable due to its significant memory and resource demands. Therefore, we employ BitFit [44] as a PEFT approach, which freezes all parameters of the entire model and updates only the bias terms during fine-tuning. Based on this setting, we define the multimodal classification loss $L_{cls}$ as the summation of standard cross-entropy classification losses over multiple modalities as follows:\n$L_{cls} = L_{m_1}(D^{m_1}; \\theta_{enc}^{m_1}, \\theta_{cls}^{m_1})+L_{m_2}(D^{m_2}; \\theta_{enc}^{m_2}, \\theta_{cls}^{m_2}) + L_c(D^{c}; \\theta_{enc}^{m_1}, \\theta_{cls}^{m_1}, \\theta_{enc}^{m_2}, \\theta_{cls}^{m_2})$\nwhere $L_{m_1}$, $L_{m_2}$ are the loss for modality-incomplete subsets, and $L_c$ is the loss for a modality-complete subset. As our framework is based on a late-fusion strategy, it enables our approach to be compatible with any other PEFT methods, such as adapter-based tuning [12] or reparametrization-based method [13]."}, {"title": "4.2 Missing Modality Feature Prediction with Prompt-Tuning", "content": "In the presence of a missing modality, instead of using only the features of the existing modalities, we posit that the predicted features of the missing modality can be integrated with those of the available modalities during inference to enhance prediction performance. Consequently, we introduce a feature predictor $f_{\\theta_{prd}}^{m'\\leftarrow m}$ using a set of trainable prompts $\\O^{m}$ to address the issue of missing modalities effectively. To facilitate this, we utilize read-only prompts described in 3.2 that are concatenated to the unimodal input data and then processed through an unimodal encoder based on transformers with specially designed masked attention. This makes our feature predictor only read the internal representation of the encoder $f_{\\theta_{enc}}^m$, which is fine-tuned for the downstream task, and to learn"}, {"title": "5 Experiment", "content": "Dataset We evaluate our proposed method using three multimodal classification datasets following prior works [24, 28]. MM-IMDb [1] consists of 25,956 image-text pairs with movie plot outlines and poster images. This encompasses 23 different genre classes, and the objective is to predict the genres of movies. As movies are frequently associated with multiple genres, the task is multimodal multi-label classification. UPMC Food-101 [40] is a multimodal classification dataset that includes images obtained by Google Image Search and corresponding textual descriptions for 101 food types. Comprising 90,840 image-text pairs, the dataset captures real-world challenges due to the noisy nature of the image and text pairs. Hateful Memes [19] is a challenging dataset designed to identify hate speech in memes using both image and text modalities. The selection of memes is structured to challenge strictly unimodal classifiers, making them struggle to classify correctly, while multimodal models are more likely to perform better. Hateful Memes emphasizes the importance of multimodal approaches in mitigating the limitations of unimodal signals.\nMetric For MM-IMDb, we measure multi-label classification performance using the F1-Macro score; for UPMC Food-101, we compute the classification accuracy; and for Hateful Memes, we assess performance using the Area Under the Receiver Operating Characteristic Curve (AUROC)."}, {"title": "5.1 Experiment Setting", "content": "This paper explores two training settings involving missing modalities: complete training setting and missing training setting. Throughout our experiments, we compare our method with previous works [24,28] based on a multimodal encoder (ViLT [20]) and an unimodal baseline. The unimodal baseline employs encoders identical to ours but only leverages an unimodal classifier from the available data when a modality drops. It should be noted that our framework is adaptable to any training setting, while prior works lack the flexibility to apply to both settings effectively. This highlights the distinct advantage of our approach, which can handle various missing scenarios.\nComplete training setting involves training on modality-complete data DC, and evaluating on modality-incomplete data $D^{m_1}$ (i.e., image-only (text-missing) data). This setup is designed to measure the model's robustness in the absence of the dominant modality. We compare our method with a previous work [28] with a multimodal pretrained transformer. We have replicated the results from [28] as no official code is available. Additionally, we compare unimodal methods against a vision-only encoder trained and tested solely on image modality data to measure the missing modality's robustness. For a fair comparison, results are averaged over five different random seeds, thereby enhancing the validity of the results by accounting for variability in the absence of text during testing.\nMissing training setting is a more general and challenging scenario where modality is absent in both the training and testing phases. We set a 70% missing"}, {"title": "5.2 Main Result", "content": "Fig. 2 shows performance under the complete training setting, highlighting declines when text is missing at inference. Although all methods perform similarly well when all modalities are present, they struggle when a dominant modality is absent during testing. This result aligns with a prior study [28] that the performance of multimodal models trained on complete data degrades when faced with incomplete data at inference. Our findings indicate that using separate unimodal encoders for multimodal learning is also susceptible to missing modalities. Specifically, on MM-IMDb and Food-101, the performance of the unimodal baseline is even worse than that of training solely with the image encoder when less than 10% of the test data are paired. Our approach, however, stands out by significantly outperforming others, especially when the text is severely missing. Moreover, on the Hateful Memes, the performance gap between the unimodal baseline and the image-encoder-only approach is slight, indicating that it does not rely on a single modality. As shown, our model is always superior to others."}, {"title": "5.3 Ablation Study", "content": "Effect of Prompts-based Feature Prediction We investigate the effects of prompts for feature prediction on the MM-IMDb, as shown in Fig. 3 and Table 2. We compare our method with a predictor leveraging the CLS token, which captures the aggregated information of input sequences. Our prompts-based method only requires an additional 0.005% parameters of backbone but outperforms the method leveraging CLS token for feature prediction in both scenarios. It reveals that the CLS token is specialized for the target task and less suitable for predicting the embedding of other modalities. In the next step, we present the t-SNE [30] visualizations of the embeddings in Fig. 4. Each data point in the figure represents ground truth embeddings, and the prediction leveraging read-only prompts and CLS token for each test sample. The figure illustrates that our method produces embeddings more closely aligned with the original features than using the CLS token. Finally, we examine the cosine similarity of our feature predictions for quantitative confirmation. Surprisingly, the prompts-based approach increased the similarity of text prediction from 0.54 to 0.57 and improved image feature prediction from 0.4 to 0.71."}, {"title": "6 Conclusion", "content": "This paper addresses the practical challenges in multimodal learning associated with acquiring complete multimodal data. In real-world scenarios, using a pretrained joint encoder on a large paired dataset may not always be feasible. Furthermore, the issue missing modalities for downstream tasks presents potential challenges during both training (fine-tuning) and testing phases. We introduce a simple yet effective framework designed to tackle missing modalities by employing PEFT on separate pretrained unimodal models. Our approach utilizes VICReg to effectively predict the embeddings of other modality within the representation space, leveraging read-only prompts. Our method exhibits superior performance across different multimodal datasets in various scenarios for missing modality that occurs during both the training and testing phases."}, {"title": "A Implementation Details", "content": "We conduct overall experiments using a single RTX 3090 GPU. Similar to [16], we employ DeiT III [37] as an image encoder and SimCSE [9] as a text encoder throughout the overall experiment, which model parameters are initialized using the pretrained weights. The structure of the feature predictor is modified from [3], which consists of two fully-connected layers with layer normalization layer [2] and activation function, and a third linear layer. The dimensions of all three layers are set to equal to the output dimension of the encoders. The length of learnable read-only prompts for feature prediction is set to 6 in MM-IMDb, 20 in UPMC Food-101, and 2 in Hateful Memes in both training settings. We conduct all experiments with batch size 12 of 20 epochs, using the AdamW [27] optimizer with a weight decay of 5 \u00d7 10-2. We initiate the warm-up steps for the learning rate at 0, with a base learning rate set to 1 \u00d7 10-2. The warm-up phase linearly progresses from 0 for the first 10% of the total training steps before decay. The variance, invariance, and covariance coefficients \u03bb, \u03bcand \u03bd for VICReg loss in Eq 1 are set to 50, 50, and 1.\nFor the ablation study on section 5.3, we add 36 learnable tokens to the input data specifically for the target task in prefix tuning, separate from utilizing read-only prompts for feature prediction. For adapter tuning, the adapter is inserted before each layer normalization layer in transformer-based pretrained models. It consists of a weight matrix that downsamples the input, followed by an activation function, and another weight matrix that restores the input to its original dimension. Finally, it includes a residual connection. We set the reduction factor to 4. For a fair comparison that considers trainable parameters, adding an adapter is applied to the first and last layers, and separately to all layers for further analysis."}, {"title": "B Read Only Prompts Attention Mask", "content": "As illustrated in Fig. 7, we applied an attention masking mechanism for our encoder's architecture, following the approach outlined in [23]. This structure is essential to maintain the representations of the input tokens for the target task while training the learnable tokens specialized for feature prediction."}, {"title": "C Additional Results", "content": "We provide a result of an image missing case in the Fig 8 under the complete training with the Food-101, which is the most susceptible among the three datasets. Due to the dominance of the text modality, the impact of the image modality on target task is minimal. However, the unimodal baseline suffers in severely image-missing cases. Conversely, our method, which demonstrates superior performance in the text missing case, also shows robustness to severely image missing cases"}]}