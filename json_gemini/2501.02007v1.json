{"title": "TART: Token-based Architecture Transformer for Neural Network Performance Prediction", "authors": ["Yannis Y. He"], "abstract": "In the realm of neural architecture design, achieving high performance is largely reliant on the manual expertise of researchers. Despite the emergence of Neural Architecture Search (NAS) as a promising technique for automating this process, current NAS methods still require human input to expand the search space and can-not generate new architectures. This paper explores the potential of Transformers in comprehending neural architectures and their performance, with the objective of establishing the foundation for utilizing Transformers to generate novel networks. We propose the Token-based Architecture Transformer (TART), which predicts neural network performance without the need to train candidate networks. TART attains state-of-the-art performance on the DeepNets-1M dataset for performance prediction tasks without edge information, indicating the potential of Transformers to aid in discovering novel and high-performing neural architectures.", "sections": [{"title": "Introduction", "content": "Manual expertise of researchers is often the key to achieving high performance in neural architecture design, encompassing metrics such as accuracy, fairness, robustness, calibration, interpretability, latency, and memory [1]. However, this approach is limited by the researcher's prior experience and involves manual trial-and-error.\nNeural Architecture Search (NAS) aims to automate the discovery of high-performing neural archi-tectures with minimal human intervention [2]. Conventional NAS methods often use directed acyclic graphs (DAG) [3] or a sequence of operators [4] to represent a neural architecture. However, the strong token-processing capability of Transformers [5] provides an incentive for representing neural architectures as tokens. Moreover, Transformers have illustrated promising results in graph learn-ing [6] as well as understanding token-based content, such as texts [7] and music notes [8]. Although prior research discovered that the token-based graph representation could achieve state-of-the-art performance in chemical compound structure generation [9], as far as our knowledge extends, no comprehensive research has been conducted on Transformers for understanding token-based neural architecture representation.\nMost of the NAS algorithms follow a typical search process (Fig. 1). Step 1) Define the search space: Given a task, researchers need to manually select candidate networks from all existing architectures to form a pool of models, also known as the search domain. This can include the type and number of"}, {"title": "Background", "content": "In this section, we briefly review research in NAS and parameter prediction. We also discuss representing neural architectures as graphs and representing graphs as tokens, which are critical intermediate steps to our work. Although the focus of the present study does not encompass the phase of architecture generation, a brief overview of Transformer-based generative models is also included as a means of contextualizing the incentives behind our exploration of Transformers."}, {"title": "Neural Architecture Search", "content": "NAS aims to create a neural architecture that maximizes performance with minimal human involve-ment in an automated way. Many architecture search algorithms are computationally demanding due to a large number of architecture evaluations. An inherent cause of inefficiency for those approaches is that architecture search is treated as a black-box optimization problem over a discrete domain. DARTS [11] was created to relax the search space to be continuous so that the architecture can be optimized with respect to its validation set performance by gradient descent. The data efficiency of gradient-based optimization, as opposed to inefficient black-box search, allows DARTS to achieve competitive performance using orders of magnitude fewer computation resources. NAO [12] uses a similar approach by converting neural networks into continuous embedding. Once the encoder maps the discrete neural architecture into the continuous representation, the performance predictor is taken as the optimization goal of the gradient ascent. The continuous representation of the best neural architecture can be obtained by maximizing the output of the performance predictor. Lastly, a decoder is applied to convert the continuous representation back to the final discrete architecture. While various research has been conducted to improve the architecture searching process, the computational bottleneck of NAS remains as training each candidate architecture to converge. Rather than discarding the weight of a child model after each performance measurement, ENAS [13] forces all child models to share weights to avoid training each child model from scratch. Although the weight sharing strategy was quickly recognized by many researchers [14, 15, 16] due to its efficiency, according to recent studies [17], this strategy may result in an incorrect evaluation of candidate architectures and difficulty in optimization."}, {"title": "Parameter and Performance Prediction", "content": "An alternative approach to avoid training every candidate architecture is utilizing parameter prediction. Knyazev et al. [10] proposed a framework that can predict parameters for diverse and large-scale architectures in a single forward pass in a fraction of a second via graph hyper network. Alongside parameter prediction, another approach to pass around the training bottleneck is utilizing perfor-mance prediction. White et al. [18] analyzed 31 performance predictors, ranging from learning curve extrapolation to weight-sharing, supervised learning, and zero-cost proxies on four different search spaces. While most predictors are domain-specific, the study provides recommendations for performance predictors under different run-time constraints. Wei et al. [19] proposed a Neural Predictor framework, which is capable of predicting the properties, such as accuracy, inference speed, and convergence speed, given an unseen architecture and dataset. These approaches have the potential to significantly decrease the computational cost of NAS and enable more creative search methods."}, {"title": "Graph Representation of Neural Architectures", "content": "A graph is a commonly used data structure across various fields. As deep learning continues to advance, a number of research has been devoted to representing neural networks using graph structures. Thost et al. [20] verified the efficiency of representing neural networks as directed acyclic graphs, which were widely used in many NAS frameworks [10, 11, 21]. There are two types of graph representation in NAS: node-based representation [10, 13, 21] and edge-based representation [22, 11]. The node-based representation is where nodes represent operations, such as convolution, pooling, and edges represent connectivity between operations, i.e. forward pass flow; whereas the edge-based representation is that each node is a latent representation (e.g. a feature map in convolutional networks) and each directed edge is associated with some operations.\nWe follow the node-based representation from [21] and define a directed acyclic computational graph as \\(A = (V, E)\\), where each node \\(v \\in V\\) has an associated computational operator \\(f_v\\) parameterized"}, {"title": "Token Representation of Graphs", "content": "In order to leverage the benefits of Transformers, which include their predictive and generative capabilities, as well as their potential to comprehend the connections between components, we studied the concept of representing a graph as a sequence of tokens. Krenn et al. [24] proposed a SELF-referencing embedded strings (SELFIES) representation of chemical molecular structures using deep learning, such as VAE [25] and GAN [26], and demonstrated the robustness of the token-based representation. In addition, Kim et al. [27] proposed a tokenized graph Transformer (TokenGT) that learns chemical structures as graphs in token representation via standard Transformers without graph-specific modification. In their studies, a graph's nodes and edges are treated as independent tokens. To avoid graph connectivity being discarded, they augment tokens with orthonormal node identifiers and trainable type identifiers and prove their approach is at least as expressive as an invariant graph network [28], which is better than Graph Neural Network [23]. In TokenGT, two specific tokenization methods are discussed: Orthogonal Random Features (ORF) and Laplacian Eigenvector (LAP). ORF initializes a random orthogonal matrix via QR decomposition of a random Gaussian matrix. This approach requires the Transformer to recognize graph structure from the incidence information provided by node identifiers, which was proven in TokenGT to be achievable. LAP uses the Laplacian eigenvectors by performing eigendecomposition on the adjacency matrix.\nOur study employs the LAP tokenization presented by TokenGT. Our decision is based on two primary considerations. Firstly, LAP demonstrated superior performance compared to the ORF tokenization in TokenGT's experimental analysis. Secondly, ORF's initialization process, which follows a normal distribution, encounters issues with sparse matrices as numerous features that could potentially be initialized as zeros. Consequently, convergence is more challenging to achieve when utilizing ORF as opposed to LAP."}, {"title": "Transformer-based Generative Model", "content": "Transformer [5] was known for its generative ability [29] in various domains [30, 31, 32]. In the graph generation domain, besides the studies mentioned in Section 2.4, Khajenezhad et al. developed Gransformer [33] that extends the basic autoregressive Transformer encoder to utilize the structural information of the given graph. The attention mechanism is adjusted to take into account the existence or absence of connections between each pair of nodes. In the token generation domain, Zeng et al. [34] offered a viewpoint for obtaining image synthesis by treating it as a visual token generation problem and proposed TokenGAN, which has the ability to control image synthesis by assigning styles to content tokens through attention mechanism via Transformer."}, {"title": "Method", "content": "Our proposed TART architecture (Fig. 2) is comprised of three fundamental stages: 1) tokenization, 2) transformer learning, and 3) prediction. Our primary aim in formulating this structure is not to optimize predictive capacity, but rather to demonstrate two principal objectives: 1) that Transformers can effectively learn a model's performance and 2) that tokenizers can enhance the learning capabili-ties of Transformers. As such, we have intentionally retained the design of each module consistent with that of related works, without any modification for the prediction task, in order to facilitate an equitable comparison.\nAs we discussed in the previous section, we construct our tokenizer using LAP method from TokenGT [27] to tokenize a neural architecture from a graph representation into a Laplacian token."}, {"title": "Experiments and Results", "content": "The experimental process entails the following steps: Firstly, a predictor is trained on the 500 samples of the training split from the DeepNets-1M dataset to learn the relationship between neural architectures and their performance. Secondly, the trained predictor takes in 500 unseen and untrained architectures from the test split and predicts their performance. Then, the predicted performance is compared against the ground truth performance, which is evaluated directly after training the architecture on the CIFAR-10 dataset. The Kendall-Tau correlation [37] is utilized as the metric to gauge the effectiveness of the predictor.\nWith our objective in mind, we design two sets of experiments, described in Section 4.3 and 4.4\nAll results can be found in table 1"}, {"title": "Experiment 1: Pure-Transformer Predictor", "content": "To explore whether Transformers can learn the performance of neural architectures, we conducted an experiment in which we trained a basic Transformer encoder. Since there is no obvious way to combine and encode node features and edge features without some form of tokenizers, the Transformer is designed to only take in node features and not use any information from the adjacency matrix."}, {"title": "Experiment 2: Token-based Transformer Predictor", "content": "To investigate whether tokenizers can improve the Transformer's performance, we trained a complete TART architecture and compared its performance to that of a pure-Transformer. Our empirical analysis (Figure 6) demonstrates the efficacy of incorporating a tokenizer in the training of Transformer-based predictors. Given our limited computational resources, we were only able to train the TART model for 30 epochs. To ensure a fair comparison, we evaluated its performance against that of the pure-Transformer, which was also trained for 30 epochs.\nBy comparing the performance of our TART architecture to that of a pure-Transformer model, we verify the positive impact of tokenization on enhancing the Transformer's ability to capture the relationship between neural architectures and their corresponding performance. This is somewhat expected because the tokenization process encodes the connections, which can be viewed as a generalization of sinusoidal positional embeddings of transformers."}, {"title": "Conclusions and Future Work", "content": "In this paper, we propose TART (Token-based Architecture Transformer), a novel approach to neural network performance prediction. To the best of our knowledge, we are the first to investigate the potential of Transformers in learning architecture performance by converting neural networks into tokens. Our approach achieves state-of-the-art performance on the DeepNets-1M dataset for performance prediction tasks without using edge information.\nOur results demonstrate that tokenization of neural architectures enhances the ability of Transformers to gain a deeper understanding of a model's performance. This lays the groundwork for future research that utilizes Transformers to generate new neural architectures for tasks that are yet to be encountered. Our approach offers an alternative to traditional methods of neural network performance prediction and opens up new avenues for future research.\nIn the future, we plan to explore several avenues to improve our TART approach. Firstly, we will investigate ways to speed up the training process of TART. The bottleneck is in the tokenization process. Converting edge features from adjacency matrix is currently done through a single-thread for-loop. One possible avenue is to preprocess the input data and vectorize the tokenization process,"}]}