{"title": "A Systematic Survey and Critical Review on Evaluating\nLarge Language Models: Challenges, Limitations, and Recommendations", "authors": ["Md Tahmid Rahman Laskar", "Sawsan Alqahtani", "M Saiful Bari", "Mizanur Rahman", "Mohammad Abdullah Matin Khan", "Haidar Khan", "Israt Jahan", "Md Amran Hossen Bhuiyan", "Chee Wei Tan", "Md Rizwan Parvez", "Enamul Hoque", "Shafiq Joty", "Jimmy Xiangji Huang"], "abstract": "Large Language Models (LLMs) have re-\ncently gained significant attention due to\ntheir remarkable capabilities in performing\ndiverse tasks across various domains. How-\never, a thorough evaluation of these mod-\nels is crucial before deploying them in real-\nworld applications to ensure they produce\nreliable performance. Despite the well-\nestablished importance of evaluating LLMs\nin the community, the complexity of the\nevaluation process has led to varied evalua-\ntion setups, causing inconsistencies in find-\nings and interpretations. To address this,\nwe systematically review the primary chal-\nlenges and limitations causing these incon-\nsistencies and unreliable evaluations in var-\nious steps of LLM evaluation. Based on our\ncritical review, we present our perspectives\nand recommendations to ensure LLM evalu-\nations are reproducible, reliable, and robust.", "sections": [{"title": "1 Introduction", "content": "The evolution of LLMs has transitioned from sim-\nple generative models predicting the next word to\nadvanced systems capable of following instruc-\ntions and solving complex problems (Zhao et al.,\n2023a). Early models like GPT (Radford et al.,\n2018) could generate coherent text but were lim-\nited to simple tasks, whereas instruction-tuned\nLLMs (Chung et al., 2022; Ouyang et al., 2022)\nlike ChatGPT\u00b9 greatly enhanced their versatility\nand ability to execute specific commands. This\nshift has revolutionized the development of real-\nworld applications powered by LLMs.\nWith the advancements and broad applicabil-\nity of LLMs, it is essential to properly evaluate\nthem to ensure they are safe to use. This is in-\ndeed important not only for academic benchmarks\nbut also for business use cases. Consequently,\nunderstanding the bottlenecks of current evalua-\ntion methods, and developing strategies to address\nthese challenges are crucial for standardizing eval-\nuations and enabling reliable use of LLMs in prac-\ntical applications. Nonetheless, evaluating LLMs\nis as complex and resource-intensive as their de-\nvelopment, involving multiple levels or aspects.\nExisting reviews (Chang et al., 2024; Guo et al.,\n2023b; Liang et al., 2022; Minaee et al., 2024;\nZhuang et al., 2023) related to the evaluation\nof LLMs often focus only on benchmark tasks,\ndatasets, and evaluation criteria, neglecting the\nbroader complexities. This oversight can under-\nmine the reliability of evaluation by ignoring is-\nsues like robustness and reproducibility. While\nsome recent studies (Balloccu et al., 2024; Mao\net al., 2023) have investigated data contamination\n(Ravaut et al., 2024) and evaluation malpractices\nin LLM evaluation, their focus is limited to only\nassessing ChatGPT, overlooking other LLMs, as\nwell as the entire evaluation pipeline.\nMore recently, Biderman et al. (2024) discussed\nthe reproducibility problem in existing evaluations\nof LLMs and introduced a library to address this.\nHowever, their work lacks comprehensive discus-\nsions on how aspects like reliability or robustness\nimpact LLM evaluation and how to address them.\nHence, existing LLM evaluation studies often fo-\ncus on individual aspects in a scattered manner,\nresulting in findings that are only sparsely useful.\nTo mitigate this gap, this paper brings together\nthe discussions to address the fundamental chal-\nlenges and limitations in LLM evaluations that\nemerge from diverse evaluation setups. First,\nwe craft a schematic workflow of the evaluation\npipeline in practical settings (presented in Sec-\ntion 2) for a systematic study. We then examine\neach step in the evaluation workflow, uncovering\nvarious inconsistencies and decision-making com-\nplexities affecting reproducibility, reliability, and\nrobustness (see Section 3). Based on our findings,"}, {"title": "2 Overview of LLM Evaluation Process", "content": "The following components are crucial for LLM\nevaluation: Evaluation Setup, Response Genera-\ntion, and Evaluation Methodology (Chang et al.,\n2024). Each component has its own challenges,\nwhich we discuss in Section 3. These components\nin an evaluation workflow are shown in Figure 1.\n2.1 Evaluation Setup\nBenchmark Selection: To initiate the evalua-\ntion process of LLMs, the first step is selecting ap-\npropriate benchmarks. We categorize the bench-\nmarking datasets into the following: general ca-\npability benchmarks, specialized benchmarks, and\nother diverse benchmarks. We refer to general ca-\npability benchmarks as the ones that are often used\nfor evaluation upon the release of an LLM (e.g.,\nMMLU (Hendrycks et al., 2020b), HumanEval\n(Chen et al., 2021)). In addition, there are special-\nized benchmarks that measure specific capabilities\nof LLMs (e.g., MT-Bench for chatting capabilities\n(Zheng et al., 2024)). There are also other bench-\nmarks that usually combine multiple benchmarks\nto evaluate LLMs on diverse task (e.g., HELM\n(Liang et al., 2022)). We provide more details on\neach category in Appendix A.1.\nModel Selection: Selecting the appropriate\nmodel from the numerous LLMs currently avail-\nable is crucial for ensuring a fair evaluation, as it\nhelps to avoid risks such as data contamination and\nunfair comparisons. For a detailed discussion on\nprominent LLMs, see Appendix A.2.\n2.2 Response Generation\nOnce the benchmarks and the models are selected,\nthe next step in the evaluation process is to design\nthe prompt and set up the decoding parameters for\nresponse generation. In the prompt design step,\ndecisions on what type of prompting (e.g., zero-\nshot or few-shot) would be used are taken. More-\nover, configuring the decoding parameters (e.g.,\ntemperature) is important to ensure optimal per-\nformance (Shi et al., 2024). More discussions on\nthis are provided in Appendix A.3 and A.4.\n2.3 Evaluation Methodology\nParsing Script Design: Evaluating LLM-\ngenerated responses is difficult because they often\nproduce verbose outputs (see Table 4 for some\nexamples). Therefore, parsing scripts are often\nnecessary (Jahan et al., 2024; Laskar et al., 2023a)\nto extract target labels before applying evaluation\nmetrics, ensuring alignment with evaluation\ncriteria to maintain reliability.\nEvaluation Approach: The evaluation ap-\nproach can be divided into the following:\nautomatic evaluation, human evaluation, LLMs\nas evaluators. In automatic evaluation, before\napplying task-specific metrics (e.g., F1, Exact\nMatch, Perplexity (Jelinek et al., 1977)), parsing"}, {"title": "3 Challenges in Evaluating LLMs", "content": "We examine challenges and limitations in the eval-\nuation process of LLMs based on three dimen-\nsions: reproducibility, reliability, and robustness.\n3.1 Reproducibility\nReproducibility, the ability to consistently repli-\ncate model results under the same conditions, is\na major challenge in generative models (Bider-\nman et al., 2024). The primary challenge is the\nlack of comprehensive documentation for each\npart of the evaluation cycle, including benchmark-\ning datasets, prompt construction, model details,\ndecoding strategy, response parsing, and evalua-\ntion methodology (Kosch and Feger, 2024; McIn-\ntosh et al., 2024). Table 1 presents an analysis by\nBalloccu et al. (2024), revealing that a relatively\nlow percentage of the analyzed papers shared their\nresources. Below, we discuss factors impacting re-\nproducibility in the evaluation step.\n3.1.1 Missing Details on Data & Models Used\nBenchmarking Data: One factor that can nega-\ntively impede the ability to reproduce results is not\nreleasing the exact data used for evaluation (Bal-\nloccu et al., 2024). Many studies evaluate LLMs\non only a subset of existing datasets (Bang et al.,\n2023; Koco\u0144 et al., 2023), while others use the ex-\nact benchmarking datasets (Laskar et al., 2023a;\nQin et al., 2023). Despite the expectation not to\ncompare results across studies using different sub-\nsets of the data, such comparisons often occur, as\ndiscussed by Balloccu et al. (2024). Nonetheless,\nwithout explaining the sampling strategy, or re-\nleasing the subsets used for evaluation (and possi-\nbly their responses), reproducing results using dif-\nferent data subsets of the same size is challenging.\nModel Versions: The information regarding the\nversion of a model being used is also missing\nin many studies (Balloccu et al., 2024; Biderman\net al., 2024), creating reproducibility concern (see\nTable 1). The continuous updates of the closed-\nsource models, often with undisclosed changes\ncan also impact reproducibility. With these up-\ndates, earlier versions are often deprecated, and re-\nsults from these versions may not apply to newer\nmodels (Chen et al., 2023b), making prior evalu-\nation results to be no longer reproducible (Bang\net al., 2023; Koco\u0144 et al., 2023; Laskar et al.,\n2023a; Qin et al., 2023). Therefore, it is crucial to\nspecify the model versions used (Balloccu et al.,\n2024; Biderman et al., 2024), while model owners\nshould keep earlier versions available.\n3.1.2 Lacking Response Generation Details\nPrompting: The lack of details behind how the\nprompts are designed may make the findings in\ndifferent literature inconsistent. For instace, vari-\nations in prompt design can lead to significantly\ndifferent results, as seen in various studies (Bang\net al., 2023; Jahan et al., 2024; Laskar et al.,\n2023a; Qin et al., 2023). While few-shot learn-\ning is found to outperform zero-shot in the orig-\ninal evaluation conducted by the authors of vari-\nous LLMs (Anil et al., 2023; OpenAI, 2023; Tou-\nvron et al., 2023b), many independent evaluations\ndemonstrate that adding few-shot examples does\nnot necessarily outperform zero-shot models in ev-\nery task (Jahan et al., 2024; Ye et al., 2023a). This\nraises the concern of whether certain prompt engi-\nneering techniques or optimizations to select few-\nshot samples were applied in the original evalu-\nations. Hence, not disclosing the details behind\nhow the prompt is designed or how the few-shot\nexamples are selected can hinder reproducibility.\nDecoding Strategy: LLMs are sensitive to de-\ncoding parameters, leading to significant perfor-\nmance variations based on the chosen settings\n(Roziere et al., 2023; Touvron et al., 2023b). How-\never, crucial details on their selection are excluded\nin existing literature (Bang et al., 2023; Koco\u0144\net al., 2023; Laskar et al., 2023a; OpenAI, 2023;"}, {"title": "Qin et al., 2023; Team et al., 2023). This lack\nof transparency raises reproducibility concerns,\nwhich could be responsible for inconsistent results\nacross studies even when similar prompts are used.\nFor instance, Qin et al. (2023) found that adding\noutput length restrictions in the prompt to gener-\nate summaries in no more than N words led to a\nperformance drop in the SAMSum dataset (Gliwa\net al., 2019). However, Laskar et al. (2023a) found\nthat such controlled experiments led to a gain in\nperformance in the SAMSum dataset.", "content": "3.1.3 Evaluation Methods Unavailable\nParsing Scripts: LLM-generated responses of-\nten require parsing scripts to extract desired in-\nformation. However, as demonstrated in Table 1,\nBalloccu et al. (2024) observed in their analysis\nthat almost half of the LLM evaluation papers do\nnot release any codes. We also observe that most\nstudies (these include both the LLM technical re-\nports, as well independent evaluations) do not re-\nlease their parsing scripts (Bang et al., 2023; Ko-\nco\u0144 et al., 2023; OpenAI, 2023; Qin et al., 2023;\nTeam et al., 2023, 2024). Nonetheless, inaccu-\nrate design of parsing scripts may lead to different\nevaluation results (Laskar et al., 2023a). Thus, the\nunavailability of parsing scripts would complicate\nresult comparisons while impacting reproducibil-\nity (Balloccu et al., 2024; Biderman et al., 2024).\nEvaluation Approach: LLMs are increasingly\nused to evaluate other LLMs in development\n(Zheng et al., 2024). Concerns arise due to the\nuse of closed-source LLMs as evaluators, as their\nfrequent updates can affect reproducibility (Chen\net al., 2023b; Verga et al., 2024). Moreover, Chen\net al. (2023b) observed significant behavioral\nchanges in closed-source LLMs over short peri-\nods. Such reproducibility concerns are also ob-\nserved in prior research that used LLMs as evalua-\ntors. For instance, Chiang and Lee (2023); Zheng\net al. (2024) found that using closed-source LLMs\nas the judge could collide with human evalua-\ntions, whereas Fu et al. (2023b) observed the op-\nposite. Since the recently proposed Prometheus-2\n(Kim et al., 2024a) model is an open-source alter-\nnative and demonstrates a strong correlation with\nhumans, utilizing open-source LLMs as the judge\ncan help mitigate the reproducibility issues preva-\nlent with closed-source LLMs."}, {"title": "3.2 Reliability", "content": "Reliability, the ability to trust that outcomes are as\nintended, is another challenge encountered during\nevaluation. Issues like contamination/inaccurate\nlabels in the data, irrelevant evaluation methods,\nand unfair comparisons may impact the reliability\nof the findings, which we discuss below.\n3.2.1 Data and Model Integrity Issues\nData Integrity: Errors in benchmarks under-\nmine accurate conclusions and model compar-\nisons, rendering evaluations of LLMs unreliable.\nAn integrity-compromising factor is the presence\nof incorrect gold labels. For instance, existing is-\nsues in the gold labels of the widely used MMLU\n(Hendrycks et al., 2020b) dataset have led to the\ndevelopment of MMLU-Pro (Wang et al., 2024b)\nand MMLU-Redux (Gema et al., 2024). Recently\nit was also found that the coding benchmarks, Hu-\nmanEval (Chen et al., 2021), lacked essential test\ncases, leading to the development of an advanced\nversion, HumanEvalPlus (Liu et al., 2024b).\nDespite these improvements, many recent\nstudies continue to use the older versions of\ndatasets. For instance, despite the release of Hu-\nmanEvalPlus, HumanEval is still used to bench-\nmark LLM coding performance (Gloeckle et al.,\n2024; Jiang et al., 2023; Li et al., 2023c; Roziere\net al., 2023; Team et al., 2023, 2024; Wong et al.,\n2023), potentially providing misleading insights.\nIn addition, outdated labels in existing bench-\nmarks undermine reliability of gold references.\nFor example, in tasks like open-domain question\nanswering, which demand real-world knowledge,\nmany gold labels become outdated over time, as\nnoted by Laskar et al. (2023a). Consequently,\neven if LLMs produce correct answers, compar-\ning them to obsolete gold labels can yield inaccu-\nrate results. Moreover, in tasks like summariza-\ntion, LLM-generated summaries are often favored\nover human-annotated gold references (Ding et al.,\n2022; Pu et al., 2023; Zhang et al., 2024b).\nContamination in Existing Models: Contamina-\ntion occurs when a benchmarking dataset is used\nin training, reducing result reliability and validity\n(Sainz et al., 2023a; Shi et al., 2023; Zhou et al.,\n2023b). Ensuring benchmarking examples are ex-\ncluded from training data is essential to maintain\nreliable results. Since LLMs are pre-trained on\nvast amounts of text data available on the internet,\nthis could lead to unfair evaluations if LLMs have"}, {"title": "already encountered these datasets during their\npre-training phase (Balloccu et al., 2024; Ravaut\net al., 2024; Xu et al., 2024).\nNonetheless, most prior LLM evaluation work\nfocusing on zero-shot evaluation did not con-\nduct any data contamination tests (Bang et al.,\n2023; Laskar et al., 2023a; OpenAI, 2023; Qin\net al., 2023; Team et al., 2023), raising concerns\nabout whether these evaluations truly represent\nthe zero-shot capabilities of LLMs. Recent re-\nsearch has also demonstrated a strong possibility\nof data contamination in many datasets used to\nevaluate different LLMs (Balloccu et al., 2024;\nGolchin and Surdeanu, 2023; Li and Flanigan,\n2023; Oren et al., 2023; Ravaut et al., 2024; Sainz\net al., 2023b; Xu et al., 2024; Zhang et al., 2024a).\nWith the current generation of LLMs being ex-\ntremely capable of learning new skills with min-\nimal amounts of data, exposing them to evalua-\ntion data may undermine the measurement of their\ntrue capabilities. Since the possibility of data con-\ntamination has led to the development of new ver-\nsions of existing datasets (e.g., utilizing GSM-8K\nto construct GSM-1K (Zhang et al., 2024a)), it is\ncrucial to use reliable and fair evaluation datasets.", "content": "3.2.2 Lack of Fairness by Manipulating\nResponse Generation\nPrompt Hacking: One major concern in terms\nof lack of fairness in LLM evaluation is the possi-\nbility of prompt hacking (Schulhoff et al., 2023),\nwhich involves manipulating input prompts to a\nlanguage model to elicit desired responses (e.g.,\nbiasing the outputs, or taking unfair advantages by\nusing specific few-shot examples). While the per-\nformance of LLMs depends on many factors rel-\nevant to how the prompt is structured, most work\n(Bang et al., 2023; Laskar et al., 2023a; Qin et al.,\n2023), even the official technical reports (An-\nthropic, 2024; OpenAI, 2023; Team et al., 2023)\nof different LLMs lack the necessary details be-\nhind prompt construction (e.g., missing scientific\nvalidity on why a certain prompt was preferred\nover others, how the few-shot examples are se-\nlected, etc.). This makes the claims regarding the\neffectiveness and limitations of certain LLMs in\ncomparison to others questionable\u00b2. Recogniz-\ning these parallels underscores the need for trans-\nparency and robust methodologies to ensure fair-\nness in Al research and development."}, {"title": "Lack of Transparency in Decoding Parameters:\nShi et al. (2024) demonstrated that extensive tun-\ning of decoding parameters could improve the per-\nformance during inference. However, how the dif-\nferent decoding parameters are selected is often\nunderexplored in existing evaluations (Bang et al.,\n2023; Laskar et al., 2023a,b; OpenAI, 2023; Qin\net al., 2023; Team et al., 2023), as discussed in\nSection 3.1. This poses the risk of optimizing the\nparameters on test sets to improve performance.", "content": "3.2.3 Inappropriate Evaluation Methodology\nInaccurate Design of Parsing Scripts: As Laskar\net al. (2023a) observed, evaluating LLMs entirely\nwith an automated approach based on the answer\nextracted using parsing scripts may lead to an er-\nror of up to more than 10% difference in many\ntasks. This raises questions about the reliability\nof LLM evaluations that solely depend on parsing\nscripts without validating the scripts' effectiveness\nfor the task. To tackle this, Laskar et al. (2023a)\nproposed a hybrid approach combining parsing\nscript-based automatic evaluation with human-in-\nthe-loop (Laskar et al., 2022a; Wu et al., 2022).\nInitially, the parsing script extracts answers from\nLLM-generated responses. If any issues arise, hu-\nmans resolve them, enhancing the reliability of\nparsing-based automatic evaluation.\nIn Figure 2, we demonstrate the differences\nbetween automatic and hybrid evaluation in\nOpen-Domain QA\u00b3 and reading comprehnesion\ndatasets. The figure highlights the influence\nof human intervention on results in open-domain\nQA, where LLMs may generate synonymous or\ntime-sensitive correct answers, potentially render-\ning gold answers outdated (Laskar et al., 2023a).\nParsing script-based automatic evaluation is found\nto be reliable in Race datasets for reading com-\nprehension, whereas notable discrepancies are ob-\nserved in the SQUAD-V2 dataset. Therefore,\nthere's a need for designing dependable parsing\nscripts and involving humans when appropriate.\nEvaluation Approaches Lacking Relevancy: In\ngenerative tasks, utilizing automatic string-based\nmatching techniques may not be reliable as well.\nFor instance, Laskar et al. (2023a) observed that\ndespite LLMs scoring quite poorly on the ROUGE\nmetric compared to SOTA summarization models,"}, {"title": "humans often prefer LLM-generated responses.\nMoreover, recent research observed potential bi-\nases while using LLMs as evaluators, such as\nLLMs preferring responses generated by LLMs of\nthe same series, positional bias (Bai et al., 2024;\nStureborg et al., 2024; Wang et al., 2023b; Wu and\nAji, 2023). To mitigate this, Verga et al. (2024)\nproposed a new technique that leveraged multiple\nLLMs as juries instead of using a single LLM as\nthe judge. This approach demonstrates higher cor-\nrelations with humans, while mitigating biases.", "content": "3.3 Robustness\nWhile there are many evaluation benchmarks cur-\nrently available, existing work mostly relies on\nevaluating LLMs on some common benchmarks,\nthis raises the question of whether the performance\nof LLMs in these common benchmarks in existing\nsettings reflects their true capabilities and limita-\ntions. In this section, we study the robustness of\nexisting LLM evaluations.\n3.3.1 Lacking Generalized Evaluation\nLimiting Evaluation to Certain Scenarios: In-\nterestingly, it has been observed in recent research\nthat certain performance gains in a specific dataset\nmay not necessarily imply that it would also im-\nprove the performance in other datasets for simi-\nlar tasks (Jahan et al., 2024; SambaNova, 2024).\nFor instance, Jahan et al. (2024) observes that not\na single LLM has superiority over other LLMs\nacross all biomedical datasets and tasks. This is\nalso evident if we compare the results between\nLLaMA-3 and Qwen2 reported in (Qwen2, 2024).\nAs shown in Figure 3, while the Qwen2 model out-\nperforms LLaMA-3 on most datasets, it falls short\non GPQA and MBPP. Interestingly, for coding"}, {"title": "tasks, Qwen2 significantly outperforms LLaMA-\n3 on the HumanEval dataset (Chen et al., 2021)\nbut not on the MBPP dataset (Austin et al., 2021).\nMeanwhile, existing common benchmarks also do\nnot take into account some specific settings, such\nas how LLMs perform in long context scenarios,\nas recent research demonstrated that LLMs often\nstruggle to generate the correct answer when rel-\nevant information does not appear at the begin-\nning or end of the input context (Liu et al., 2024c).\nThis highlights the importance of evaluating the\ngeneralized performance of LLMs across a set of\ndiverse benchmarks and settings,instead of limit-\ning evaluation to only common benchmarks like\nMMLU (Hendrycks et al., 2020b).\nDiversity and Coverage in Benchmarks: Al-\nthough benchmarking datasets are designed to ad-\ndress specific problems and objectives, the vari-\nation and complexity of language within these\ndatasets are often unclear. Liang et al. (2022)\nhighlighted that better coverage in benchmarking\ndatasets would enhance the comprehensiveness of\nthe model's evaluation. While different language\nmodels use different tokenizers to represent the\nbenchmarking dataset, it also leads to variations\nin what is evaluated across models.", "content": "As can be seen in Table 2, we conducted a\nsmall-scale analysis for LLaMA-2 (Touvron et al.,\n2023b), LLaMA-3,5 Mistral (Jiang et al., 2023),\nand Qwen26 on two benchmarking datasets with\nvarying complexities: MMLU (Hendrycks et al.,\n2020b) and its more challenging version, MMLU-\nPro (Wang et al., 2024b), as well as MixEval (Ni\net al., 2024) and its harder version, MixEval-Hard.\nOur findings indicate that these datasets cover a\nrelatively small portion of the model's capabilities.\nSpecifically, for MixEval, as the datasets became\nmore diverse and dynamic, the vocabulary cover-\nage for the tokenizer decreased. This trend con-\ntinued as the datasets increased in difficulty, with\nvocabulary coverage further declining.\n3.3.2 No Tuning of Prompt and Decoding\nParameters\nWhile various combinations of decoding parame-\nters may lead to differences in results (Shi et al.,\n2024), possibly due to high computing require-\nments, existing LLM evaluation work mostly un-\ndermines the necessity of evaluating how the\nmodel performance may vary depending on its\nvariations. Similar to the absence of decoder pa-\nrameter tuning, most prior work also evaluated\nLLMs using only a single prompt (Bang et al.,\n2023; Jahan et al., 2024; Koco\u0144 et al., 2023;\nLaskar et al., 2023a; Qin et al., 2023). However,\nin the real world, users express themselves with\ndiverse word choices, varying semantics and syn-\ntaxes, alongside minor discrepancies (e.g., mis-\nspellings or differing punctuation styles). To fur-\nther examine the effects of prompt variations, we\nconduct an experiment using GPT-4o (2024-04-\n09) and GPT-3.5-Turbo (0125) (OpenAI, 2023), as\nwell as Claude-3-Opus (2024-02-29) (Anthropic,\n2024) with the prompts used by (Laskar et al.,\n2023a) and (Qin et al., 2023) in the SAMSum\ndataset. For this experiment, the default param-\neters for respective LLMs are used.\nAs shown in Figure 4, the restricted prompt-\ning method by Laskar et al. (2023a) consistently\noutperforms the unrestricted approach across all\nthree models. Conversely, the restricted prompt-\ning method by Qin et al. (2023) fails to surpass\nthe unrestricted approach for GPT-3.5 and GPT-\n40. However, it surprisingly outperforms the unre-\nstricted method, indicating the significant impact\nof prompt tuning across models. Evaluating lan-\nguage models with a single prompt lacks fairness\n(Zhu et al., 2023b), yet it remains common prac-"}, {"title": "et al., 2023). Minor prompt variations can lead to\ndiverse outcomes for different models (Alzahrani\net al., 2024; An et al., 2023; Biderman et al., 2024;\nLanham et al., 2023; Sclar et al., 2023; Zhang\net al., 2024a), highlighting the need to compare\nbenchmarks across multiple prompts. Using auto-\nmated prompt tuning techniques like Meta Prob-\ning Agents (Zhu et al., 2024) can ensure robust-\nness to prompt variations.", "content": "3.3.3 Evaluation Method's Generalizability\nand Correlation Shortcomings\nWhile automatic evaluations are usually utilized\nin discriminative tasks, they may not be applica-\nble to every task, as demonstrated by Jahan et al.\n(2024) that parsing scripts are not usable in cer-\ntain discriminative tasks like relation extraction.\nJahan et al. (2024) also noted a significant per-\nformance gap between the string-matching-based\nROUGE metric (Lin, 2004) and the contextual\nsimilarity-based metric BERTScore (Zhang et al.,\n2019) in text summarization. While larger mod-\nels achieve better accuracy, they involve a speed-\naccuracy trade-off (Parvez et al., 2019), leading to\nhigher costs and latency (Fu et al., 2024b; Laskar\net al., 2023b). While metrics like perplexity are\nwidely used to evaluate language models (Chen\net al., 2023c), Huang et al. (2024b) found that\nquantized LLaMA-3 versions have lower output\nconfidence than the original. They noted simi-\nlar model rankings for perplexity and a common-\nsense QA dataset. However, Hu et al. (2024) found\nno correlation between perplexity and long context\nunderstanding tasks, highlighting the need for ro-\nbust evaluations with human-correlated metrics.\nThis raises another question, whether au-\ntomated evaluations and LLM-as-a-judge cor-"}, {"title": "relate with human evaluations (e.g., Elo rat-\nings). Zheng et al. (2024) demonstrated signif-\nicant correlations between Elo ratings, LLM-as-\na-judge, and automated evaluations. However,\nrecent research (Alzahrani et al., 2024) suggest\nthat automated evaluations, especially those us-\ning multiple-choice questions, can yield unstable\nrankings with minor changes in evaluation meth-\nods. Given this instability, it prompts us to ques-\ntion why these automated tests should align with\nhuman Elo ratings despite demonstrating such in-\nconsistencies. In our view, we should focus not\nonly on correlating scores but also on how well\na benchmark's rankings align with the gold stan-\ndards. Analysis in Table 3 for GPT-4 (OpenAI,\n2023), Gemini (Team et al., 2023), and Claude-\n3 (Anthropic, 2024) reveals two key observations:\n(i) MMLU rankings disagree with LMSys Chatbot\nArena and (ii) MMLU rankings vary among them-\nselves due to implementation differences.", "content": "4 Recommendations and Best Practices\nWe've outlined the primary challenges in evaluat-\ning LLMs thus far. In light of these challenges, a\ncrucial question arises: How can we enhance the\nevaluation process for LLMs? Crafting a struc-\ntured framework that's both practical and easy to\nimplement is daunting, given the complexities of\ngenerative LLM development. Previous studies\nhave tended to focus on specific evaluation as-\npects without offering comprehensive guidelines\nfor the entire evaluation cycle, leaving researchers\nwithout clear guidance. Before diving into rec-\nommendations for each evaluation stage, it's im-\nportant to acknowledge three key factors shaping\ncurrent LLM evaluation practices: inherent ran-\ndomness in generative models, significant compu-\ntational demands, and insufficient documentation\nacross stages.\nEvaluation Setup: Selecting benchmarks for\nmodel assessment is crucial. Rather than sim-"}, {"title": "ply replicating past choices"}, {"title": "A Systematic Survey and Critical Review on Evaluating\nLarge Language Models: Challenges, Limitations, and Recommendations", "authors": ["Md Tahmid Rahman Laskar", "Sawsan Alqahtani", "M Saiful Bari", "Mizanur Rahman", "Mohammad Abdullah Matin Khan", "Haidar Khan", "Israt Jahan", "Md Amran Hossen Bhuiyan", "Chee Wei Tan", "Md Rizwan Parvez", "Enamul Hoque", "Shafiq Joty", "Jimmy Xiangji Huang"], "abstract": "Large Language Models (LLMs) have recently gained significant attention due to their remarkable capabilities in performing diverse tasks across various domains. However, a thorough evaluation of these models is crucial before deploying them in real-world applications to ensure they produce reliable performance. Despite the well-established importance of evaluating LLMs in the community, the complexity of the evaluation process has led to varied evaluation setups, causing inconsistencies in findings and interpretations. To address this, we systematically review the primary challenges and limitations causing these inconsistencies and unreliable evaluations in various steps of LLM evaluation. Based on our critical review, we present our perspectives and recommendations to ensure LLM evaluations are reproducible, reliable, and robust.", "sections": [{"title": "1 Introduction", "content": "The evolution of LLMs has transitioned from simple generative models predicting the next word to advanced systems capable of following instructions and solving complex problems (Zhao et al., 2023a). Early models like GPT (Radford et al., 2018) could generate coherent text but were limited to simple tasks, whereas instruction-tuned LLMs (Chung et al., 2022; Ouyang et al., 2022) like ChatGPT\u00b9 greatly enhanced their versatility and ability to execute specific commands. This shift has revolutionized the development of real-world applications powered by LLMs.\nWith the advancements and broad applicability of LLMs, it is essential to properly evaluate them to ensure they are safe to use. This is indeed important not only for academic benchmarks but also for business use cases. Consequently, understanding the bottlenecks of current evaluation methods, and developing strategies to address these challenges are crucial for standardizing evaluations and enabling reliable use of LLMs in practical applications. Nonetheless, evaluating LLMs is as complex and resource-intensive as their development, involving multiple levels or aspects.\nExisting reviews (Chang et al., 2024; Guo et al., 2023b; Liang et al., 2022; Minaee et al., 2024; Zhuang et al., 2023) related to the evaluation of LLMs often focus only on benchmark tasks, datasets, and evaluation criteria, neglecting the broader complexities. This oversight can undermine the reliability of evaluation by ignoring issues like robustness and reproducibility. While some recent studies (Balloccu et al., 2024; Mao et al., 2023) have investigated data contamination (Ravaut et al., 2024) and evaluation malpractices in LLM evaluation, their focus is limited to only assessing ChatGPT, overlooking other LLMs, as well as the entire evaluation pipeline.\nMore recently, Biderman et al. (2024) discussed the reproducibility problem in existing evaluations of LLMs and introduced a library to address this. However, their work lacks comprehensive discussions on how aspects like reliability or robustness impact LLM evaluation and how to address them. Hence, existing LLM evaluation studies often focus on individual aspects in a scattered manner, resulting in findings that are only sparsely useful.\nTo mitigate this gap, this paper brings together the discussions to address the fundamental challenges and limitations in LLM evaluations that emerge from diverse evaluation setups. First, we craft a schematic workflow of the evaluation pipeline in practical settings (presented in Section 2) for a systematic study. We then examine each step in the evaluation workflow, uncovering various inconsistencies and decision-making complexities affecting reproducibility, reliability, and robustness (see Section 3). Based on our findings,"}, {"title": "2 Overview of LLM Evaluation Process", "content": "The following components are crucial for LLM evaluation: Evaluation Setup, Response Generation, and Evaluation Methodology (Chang et al., 2024). Each component has its own challenges, which we discuss in Section 3. These components in an evaluation workflow are shown in Figure 1.\n2.1 Evaluation Setup\nBenchmark Selection: To initiate the evaluation process of LLMs, the first step is selecting appropriate benchmarks. We categorize the benchmarking datasets into the following: general capability benchmarks, specialized benchmarks, and other diverse benchmarks. We refer to general capability benchmarks as the ones that are often used for evaluation upon the release of an LLM (e.g., MMLU (Hendrycks et al., 2020b), HumanEval (Chen et al., 2021)). In addition, there are specialized benchmarks that measure specific capabilities of LLMs (e.g., MT-Bench for chatting capabilities (Zheng et al., 2024)). There are also other benchmarks that usually combine multiple benchmarks to evaluate LLMs on diverse task (e.g., HELM (Liang et al., 2022)). We provide more details on each category in Appendix A.1.\nModel Selection: Selecting the appropriate model from the numerous LLMs currently available is crucial for ensuring a fair evaluation, as it helps to avoid risks such as data contamination and unfair comparisons. For a detailed discussion on prominent LLMs, see Appendix A.2.\n2.2 Response Generation\nOnce the benchmarks and the models are selected, the next step in the evaluation process is to design the prompt and set up the decoding parameters for response generation. In the prompt design step, decisions on what type of prompting (e.g., zero-shot or few-shot) would be used are taken. Moreover, configuring the decoding parameters (e.g., temperature) is important to ensure optimal performance (Shi et al., 2024). More discussions on this are provided in Appendix A.3 and A.4.\n2.3 Evaluation Methodology\nParsing Script Design: Evaluating LLM-generated responses is difficult because they often produce verbose outputs (see Table 4 for some examples). Therefore, parsing scripts are often necessary (Jahan et al., 2024; Laskar et al., 2023a) to extract target labels before applying evaluation metrics, ensuring alignment with evaluation criteria to maintain reliability.\nEvaluation Approach: The evaluation approach can be divided into the following: automatic evaluation, human evaluation, LLMs as evaluators. In automatic evaluation, before applying task-specific metrics (e.g., F1, Exact Match, Perplexity (Jelinek et al., 1977)), parsing"}, {"title": "3 Challenges in Evaluating LLMs", "content": "We examine challenges and limitations in the evaluation process of LLMs based on three dimensions: reproducibility, reliability, and robustness.\n3.1 Reproducibility\nReproducibility, the ability to consistently replicate model results under the same conditions, is a major challenge in generative models (Biderman et al., 2024). The primary challenge is the lack of comprehensive documentation for each part of the evaluation cycle, including benchmarking datasets, prompt construction, model details, decoding strategy, response parsing, and evaluation methodology (Kosch and Feger, 2024; McIntosh et al., 2024). Table 1 presents an analysis by Balloccu et al. (2024), revealing that a relatively low percentage of the analyzed papers shared their resources. Below, we discuss factors impacting reproducibility in the evaluation step.\n3.1.1 Missing Details on Data & Models Used\nBenchmarking Data: One factor that can negatively impede the ability to reproduce results is not releasing the exact data used for evaluation (Balloccu et al., 2024). Many studies evaluate LLMs on only a subset of existing datasets (Bang et al., 2023; Koco\u0144 et al., 2023), while others use the exact benchmarking datasets (Laskar et al., 2023a; Qin et al., 2023). Despite the expectation not to compare results across studies using different subsets of the data, such comparisons often occur, as discussed by Balloccu et al. (2024). Nonetheless, without explaining the sampling strategy, or releasing the subsets used for evaluation (and possibly their responses), reproducing results using different data subsets of the same size is challenging.\nModel Versions: The information regarding the version of a model being used is also missing in many studies (Balloccu et al., 2024; Biderman et al., 2024), creating reproducibility concern (see Table 1). The continuous updates of the closed-source models, often with undisclosed changes can also impact reproducibility. With these updates, earlier versions are often deprecated, and results from these versions may not apply to newer models (Chen et al., 2023b), making prior evaluation results to be no longer reproducible (Bang et al., 2023; Koco\u0144 et al., 2023; Laskar et al., 2023a; Qin et al., 2023). Therefore, it is crucial to specify the model versions used (Balloccu et al., 2024; Biderman et al., 2024), while model owners should keep earlier versions available.\n3.1.2 Lacking Response Generation Details\nPrompting: The lack of details behind how the prompts are designed may make the findings in different literature inconsistent. For instace, variations in prompt design can lead to significantly different results, as seen in various studies (Bang et al., 2023; Jahan et al., 2024; Laskar et al., 2023a; Qin et al., 2023). While few-shot learning is found to outperform zero-shot in the original evaluation conducted by the authors of various LLMs (Anil et al., 2023; OpenAI, 2023; Touvron et al., 2023b), many independent evaluations demonstrate that adding few-shot examples does not necessarily outperform zero-shot models in every task (Jahan et al., 2024; Ye et al., 2023a). This raises the concern of whether certain prompt engineering techniques or optimizations to select few-shot samples were applied in the original evaluations. Hence, not disclosing the details behind how the prompt is designed or how the few-shot examples are selected can hinder reproducibility.\nDecoding Strategy: LLMs are sensitive to decoding parameters, leading to significant performance variations based on the chosen settings (Roziere et al., 2023; Touvron et al., 2023b). However, crucial details on their selection are excluded in existing literature (Bang et al., 2023; Koco\u0144 et al., 2023; Laskar et al., 2023a; OpenAI, 2023;"}, {"title": "Qin et al., 2023; Team et al., 2023). This lack of transparency raises reproducibility concerns, which could be responsible for inconsistent results across studies even when similar prompts are used. For instance, Qin et al. (2023) found that adding output length restrictions in the prompt to generate summaries in no more than N words led to a performance drop in the SAMSum dataset (Gliwa et al., 2019). However, Laskar et al. (2023a) found that such controlled experiments led to a gain in performance in the SAMSum dataset.", "content": "3.1.3 Evaluation Methods Unavailable\nParsing Scripts: LLM-generated responses often require parsing scripts to extract desired information. However, as demonstrated in Table 1, Balloccu et al. (2024) observed in their analysis that almost half of the LLM evaluation papers do not release any codes. We also observe that most studies (these include both the LLM technical reports, as well independent evaluations) do not release their parsing scripts (Bang et al., 2023; Koco\u0144 et al., 2023; OpenAI, 2023; Qin et al., 2023; Team et al., 2023, 2024). Nonetheless, inaccurate design of parsing scripts may lead to different evaluation results (Laskar et al., 2023a). Thus, the unavailability of parsing scripts would complicate result comparisons while impacting reproducibility (Balloccu et al., 2024; Biderman et al., 2024).\nEvaluation Approach: LLMs are increasingly used to evaluate other LLMs in development (Zheng et al., 2024). Concerns arise due to the use of closed-source LLMs as evaluators, as their frequent updates can affect reproducibility (Chen et al., 2023b; Verga et al., 2024). Moreover, Chen et al. (2023b) observed significant behavioral changes in closed-source LLMs over short periods. Such reproducibility concerns are also observed in prior research that used LLMs as evaluators. For instance, Chiang and Lee (2023); Zheng et al. (2024) found that using closed-source LLMs as the judge could collide with human evaluations, whereas Fu et al. (2023b) observed the opposite. Since the recently proposed Prometheus-2 (Kim et al., 2024a) model is an open-source alternative and demonstrates a strong correlation with humans, utilizing open-source LLMs as the judge can help mitigate the reproducibility issues prevalent with closed-source LLMs."}, {"title": "3.2 Reliability", "content": "Reliability, the ability to trust that outcomes are as intended, is another challenge encountered during evaluation. Issues like contamination/inaccurate labels in the data, irrelevant evaluation methods, and unfair comparisons may impact the reliability of the findings, which we discuss below.\n3.2.1 Data and Model Integrity Issues\nData Integrity: Errors in benchmarks undermine accurate conclusions and model comparisons, rendering evaluations of LLMs unreliable. An integrity-compromising factor is the presence of incorrect gold labels. For instance, existing issues in the gold labels of the widely used MMLU (Hendrycks et al., 2020b) dataset have led to the development of MMLU-Pro (Wang et al., 2024b) and MMLU-Redux (Gema et al., 2024). Recently it was also found that the coding benchmarks, HumanEval (Chen et al., 2021), lacked essential test cases, leading to the development of an advanced version, HumanEvalPlus (Liu et al., 2024b).\nDespite these improvements, many recent studies continue to use the older versions of datasets. For instance, despite the release of HumanEvalPlus, HumanEval is still used to benchmark LLM coding performance (Gloeckle et al., 2024; Jiang et al., 2023; Li et al., 2023c; Roziere et al., 2023; Team et al., 2023, 2024; Wong et al., 2023), potentially providing misleading insights. In addition, outdated labels in existing benchmarks undermine reliability of gold references. For example, in tasks like open-domain question answering, which demand real-world knowledge, many gold labels become outdated over time, as noted by Laskar et al. (2023a). Consequently, even if LLMs produce correct answers, comparing them to obsolete gold labels can yield inaccurate results. Moreover, in tasks like summarization, LLM-generated summaries are often favored over human-annotated gold references (Ding et al., 2022; Pu et al., 2023; Zhang et al., 2024b).\nContamination in Existing Models: Contamination occurs when a benchmarking dataset is used in training, reducing result reliability and validity (Sainz et al., 2023a; Shi et al., 2023; Zhou et al., 2023b). Ensuring benchmarking examples are excluded from training data is essential to maintain reliable results. Since LLMs are pre-trained on vast amounts of text data available on the internet, this could lead to unfair evaluations if LLMs have"}, {"title": "already encountered these datasets during their pre-training phase (Balloccu et al., 2024; Ravaut et al., 2024; Xu et al., 2024).\nNonetheless, most prior LLM evaluation work focusing on zero-shot evaluation did not conduct any data contamination tests (Bang et al., 2023; Laskar et al., 2023a; OpenAI, 2023; Qin et al., 2023; Team et al., 2023), raising concerns about whether these evaluations truly represent the zero-shot capabilities of LLMs. Recent research has also demonstrated a strong possibility of data contamination in many datasets used to evaluate different LLMs (Balloccu et al., 2024; Golchin and Surdeanu, 2023; Li and Flanigan, 2023; Oren et al., 2023; Ravaut et al., 2024; Sainz et al., 2023b; Xu et al., 2024; Zhang et al., 2024a).\nWith the current generation of LLMs being extremely capable of learning new skills with minimal amounts of data, exposing them to evaluation data may undermine the measurement of their true capabilities. Since the possibility of data contamination has led to the development of new versions of existing datasets (e.g., utilizing GSM-8K to construct GSM-1K (Zhang et al., 2024a)), it is crucial to use reliable and fair evaluation datasets.", "content": "3.2.2 Lack of Fairness by Manipulating Response Generation\nPrompt Hacking: One major concern in terms of lack of fairness in LLM evaluation is the possibility of prompt hacking (Schulhoff et al., 2023), which involves manipulating input prompts to a language model to elicit desired responses (e.g., biasing the outputs, or taking unfair advantages by using specific few-shot examples). While the performance of LLMs depends on many factors relevant to how the prompt is structured, most work (Bang et al., 2023; Laskar et al., 2023a; Qin et al., 2023), even the official technical reports (Anthropic, 2024; OpenAI, 2023; Team et al., 2023) of different LLMs lack the necessary details behind prompt construction (e.g., missing scientific validity on why a certain prompt was preferred over others, how the few-shot examples are selected, etc.). This makes the claims regarding the effectiveness and limitations of certain LLMs in comparison to others questionable\u00b2. Recognizing these parallels underscores the need for transparency and robust methodologies to ensure fairness in Al research and development."}, {"title": "Lack of Transparency in Decoding Parameters: Shi et al. (2024) demonstrated that extensive tuning of decoding parameters could improve the performance during inference. However, how the different decoding parameters are selected is often underexplored in existing evaluations (Bang et al., 2023; Laskar et al., 2023a,b; OpenAI, 2023; Qin et al., 2023; Team et al., 2023), as discussed in Section 3.1. This poses the risk of optimizing the parameters on test sets to improve performance.", "content": "3.2.3 Inappropriate Evaluation Methodology\nInaccurate Design of Parsing Scripts: As Laskar et al. (2023a) observed, evaluating LLMs entirely with an automated approach based on the answer extracted using parsing scripts may lead to an error of up to more than 10% difference in many tasks. This raises questions about the reliability of LLM evaluations that solely depend on parsing scripts without validating the scripts' effectiveness for the task. To tackle this, Laskar et al. (2023a) proposed a hybrid approach combining parsing script-based automatic evaluation with human-in-the-loop (Laskar et al., 2022a; Wu et al., 2022). Initially, the parsing script extracts answers from LLM-generated responses. If any issues arise, humans resolve them, enhancing the reliability of parsing-based automatic evaluation.\nIn Figure 2, we demonstrate the differences between automatic and hybrid evaluation in Open-Domain QA\u00b3 and reading comprehnesion datasets. The figure highlights the influence of human intervention on results in open-domain QA, where LLMs may generate synonymous or time-sensitive correct answers, potentially rendering gold answers outdated (Laskar et al., 2023a). Parsing script-based automatic evaluation is found to be reliable in Race datasets for reading comprehension, whereas notable discrepancies are observed in the SQUAD-V2 dataset. Therefore, there's a need for designing dependable parsing scripts and involving humans when appropriate.\nEvaluation Approaches Lacking Relevancy: In generative tasks, utilizing automatic string-based matching techniques may not be reliable as well. For instance, Laskar et al. (2023a) observed that despite LLMs scoring quite poorly on the ROUGE metric compared to SOTA summarization models,"}, {"title": "humans often prefer LLM-generated responses. Moreover, recent research observed potential biases while using LLMs as evaluators, such as LLMs preferring responses generated by LLMs of the same series, positional bias (Bai et al., 2024; Stureborg et al., 2024; Wang et al., 2023b; Wu and Aji, 2023). To mitigate this, Verga et al. (2024) proposed a new technique that leveraged multiple LLMs as juries instead of using a single LLM as the judge. This approach demonstrates higher correlations with humans, while mitigating biases.", "content": "3.3 Robustness\nWhile there are many evaluation benchmarks currently available, existing work mostly relies on evaluating LLMs on some common benchmarks, this raises the question of whether the performance of LLMs in these common benchmarks in existing settings reflects their true capabilities and limitations. In this section, we study the robustness of existing LLM evaluations.\n3.3.1 Lacking Generalized Evaluation\nLimiting Evaluation to Certain Scenarios: Interestingly, it has been observed in recent research that certain performance gains in a specific dataset may not necessarily imply that it would also improve the performance in other datasets for similar tasks (Jahan et al., 2024; SambaNova, 2024). For instance, Jahan et al. (2024) observes that not a single LLM has superiority over other LLMs across all biomedical datasets and tasks. This is also evident if we compare the results between LLaMA-3 and Qwen2 reported in (Qwen2, 2024). As shown in Figure 3, while the Qwen2 model outperforms LLaMA-3 on most datasets, it falls short on GPQA and MBPP. Interestingly, for coding"}, {"title": "tasks, Qwen2 significantly outperforms LLaMA-3 on the HumanEval dataset (Chen et al., 2021) but not on the MBPP dataset (Austin et al., 2021). Meanwhile, existing common benchmarks also do not take into account some specific settings, such as how LLMs perform in long context scenarios, as recent research demonstrated that LLMs often struggle to generate the correct answer when relevant information does not appear at the beginning or end of the input context (Liu et al., 2024c). This highlights the importance of evaluating the generalized performance of LLMs across a set of diverse benchmarks and settings,instead of limiting evaluation to only common benchmarks like MMLU (Hendrycks et al., 2020b).\nDiversity and Coverage in Benchmarks: Although benchmarking datasets are designed to address specific problems and objectives, the variation and complexity of language within these datasets are often unclear. Liang et al. (2022) highlighted that better coverage in benchmarking datasets would enhance the comprehensiveness of the model's evaluation. While different language models use different tokenizers to represent the benchmarking dataset, it also leads to variations in what is evaluated across models.", "content": "As can be seen in Table 2, we conducted a small-scale analysis for LLaMA-2 (Touvron et al., 2023b), LLaMA-3,5 Mistral (Jiang et al., 2023), and Qwen26 on two benchmarking datasets with varying complexities: MMLU (Hendrycks et al., 2020b) and its more challenging version, MMLU-Pro (Wang et al., 2024b), as well as MixEval (Ni et al., 2024) and its harder version, MixEval-Hard. Our findings indicate that these datasets cover a relatively small portion of the model's capabilities. Specifically, for MixEval, as the datasets became more diverse and dynamic, the vocabulary coverage for the tokenizer decreased. This trend continued as the datasets increased in difficulty, with vocabulary coverage further declining.\n3.3.2 No Tuning of Prompt and Decoding Parameters\nWhile various combinations of decoding parameters may lead to differences in results (Shi et al., 2024), possibly due to high computing requirements, existing LLM evaluation work mostly undermines the necessity of evaluating how the model performance may vary depending on its variations. Similar to the absence of decoder parameter tuning, most prior work also evaluated LLMs using only a single prompt (Bang et al., 2023; Jahan et al., 2024; Koco\u0144 et al., 2023; Laskar et al., 2023a; Qin et al., 2023). However, in the real world, users express themselves with diverse word choices, varying semantics and syntaxes, alongside minor discrepancies (e.g., misspellings or differing punctuation styles). To further examine the effects of prompt variations, we conduct an experiment using GPT-4o (2024-04-09) and GPT-3.5-Turbo (0125) (OpenAI, 2023), as well as Claude-3-Opus (2024-02-29) (Anthropic, 2024) with the prompts used by (Laskar et al., 2023a) and (Qin et al., 2023) in the SAMSum dataset. For this experiment, the default parameters for respective LLMs are used.\nAs shown in Figure 4, the restricted prompting method by Laskar et al. (2023a) consistently outperforms the unrestricted approach across all three models. Conversely, the restricted prompting method by Qin et al. (2023) fails to surpass the unrestricted approach for GPT-3.5 and GPT-40. However, it surprisingly outperforms the unrestricted method, indicating the significant impact of prompt tuning across models. Evaluating language models with a single prompt lacks fairness (Zhu et al., 2023b), yet it remains common prac-"}, {"title": "et al., 2023). Minor prompt variations can lead to diverse outcomes for different models (Alzahrani et al., 2024; An et al., 2023; Biderman et al., 2024; Lanham et al., 2023; Sclar et al., 2023; Zhang et al., 2024a), highlighting the need to compare benchmarks across multiple prompts. Using automated prompt tuning techniques like Meta Probing Agents (Zhu et al., 2024) can ensure robustness to prompt variations.", "content": "3.3.3 Evaluation Method's Generalizability and Correlation Shortcomings\nWhile automatic evaluations are usually utilized in discriminative tasks, they may not be applicable to every task, as demonstrated by Jahan et al. (2024) that parsing scripts are not usable in certain discriminative tasks like relation extraction. Jahan et al. (2024) also noted a significant performance gap between the string-matching-based ROUGE metric (Lin, 2004) and the contextual similarity-based metric BERTScore (Zhang et al., 2019) in text summarization. While larger models achieve better accuracy, they involve a speed-accuracy trade-off (Parvez et al., 2019), leading to higher costs and latency (Fu et al., 2024b; Laskar et al., 2023b). While metrics like perplexity are widely used to evaluate language models (Chen et al., 2023c), Huang et al. (2024b) found that quantized LLaMA-3 versions have lower output confidence than the original. They noted similar model rankings for perplexity and a common-sense QA dataset. However, Hu et al. (2024) found no correlation between perplexity and long context understanding tasks, highlighting the need for robust evaluations with human-correlated metrics.\nThis raises another question, whether automated evaluations and LLM-as-a-judge cor-"}, {"title": "relate with human evaluations (e.g., Elo ratings). Zheng et al. (2024) demonstrated significant correlations between Elo ratings, LLM-as-a-judge, and automated evaluations. However, recent research (Alzahrani et al., 2024) suggest that automated evaluations, especially those using multiple-choice questions, can yield unstable rankings with minor changes in evaluation methods. Given this instability, it prompts us to question why these automated tests should align with human Elo ratings despite demonstrating such inconsistencies. In our view, we should focus not only on correlating scores but also on how well a benchmark's rankings align with the gold standards. Analysis in Table 3 for GPT-4 (OpenAI, 2023), Gemini (Team et al., 2023), and Claude-3 (Anthropic, 2024) reveals two key observations: (i) MMLU rankings disagree with LMSys Chatbot Arena and (ii) MMLU rankings vary among themselves due to implementation differences.", "content": "4 Recommendations and Best Practices\nWe've outlined the primary challenges in evaluating LLMs thus far. In light of these challenges, a crucial question arises: How can we enhance the evaluation process for LLMs? Crafting a structured framework that's both practical and easy to implement is daunting, given the complexities of generative LLM development. Previous studies have tended to focus on specific evaluation aspects without offering comprehensive guidelines for the entire evaluation cycle, leaving researchers without clear guidance. Before diving into recommendations for each evaluation stage, it's important to acknowledge three key factors shaping current LLM evaluation practices: inherent randomness in generative models, significant computational demands, and insufficient documentation across stages.\nEvaluation Setup: Selecting benchmarks for model assessment is crucial. Rather than sim-"}, {"title": "ply replicating past choices, researchers should align datasets with required capabilities. To ensure robustness, datasets should vary across expected LLM capabilities (e.g., long-context understanding), tasks (e.g., summarization), and language complexity (e.g., vocabulary coverage). Ideally, a metric should measure dataset diversity. For model selection, conduct contamination tests between the chosen model and benchmarks using relevant techniques (Ravaut et al., 2024). This acts as an additional filter for benchmarking datasets, ensuring selection of unseen ones measuring intended capabilities. Meanwhile, for reproducibility, document any subset use of benchmarking datasets, along with the selected model version. In addition, throughout scientific history, intelligence progress has evolved across generations. Tests from"}]}]}