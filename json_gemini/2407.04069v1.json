{"title": "A Systematic Survey and Critical Review on Evaluating\nLarge Language Models: Challenges, Limitations, and Recommendations", "authors": ["Md Tahmid Rahman Laskar", "Sawsan Alqahtani", "M Saiful Bari", "Mizanur Rahman", "Mohammad Abdullah Matin Khan", "Haidar Khan", "Israt Jahan", "Md Amran Hossen Bhuiyan", "Chee Wei Tan", "Md Rizwan Parvez", "Enamul Hoque", "Shafiq Joty", "Jimmy Xiangji Huang"], "abstract": "Large Language Models (LLMs) have re-\ncently gained significant attention due to\ntheir remarkable capabilities in performing\ndiverse tasks across various domains. How-\never, a thorough evaluation of these mod-\nels is crucial before deploying them in real-\nworld applications to ensure they produce\nreliable performance. Despite the well-\nestablished importance of evaluating LLMs\nin the community, the complexity of the\nevaluation process has led to varied evalua-\ntion setups, causing inconsistencies in find-\nings and interpretations. To address this,\nwe systematically review the primary chal-\nlenges and limitations causing these incon-\nsistencies and unreliable evaluations in var-\nious steps of LLM evaluation. Based on our\ncritical review, we present our perspectives\nand recommendations to ensure LLM evalu-\nations are reproducible, reliable, and robust.", "sections": [{"title": "1 Introduction", "content": "The evolution of LLMs has transitioned from sim-\nple generative models predicting the next word to\nadvanced systems capable of following instruc-\ntions and solving complex problems (Zhao et al.,\n2023a). Early models like GPT (Radford et al.,\n2018) could generate coherent text but were lim-\nited to simple tasks, whereas instruction-tuned\nLLMs (Chung et al., 2022; Ouyang et al., 2022)\nlike ChatGPT\u00b9 greatly enhanced their versatility\nand ability to execute specific commands. This\nshift has revolutionized the development of real-\nworld applications powered by LLMs.\nWith the advancements and broad applicabil-\nity of LLMs, it is essential to properly evaluate\nthem to ensure they are safe to use. This is in-\ndeed important not only for academic benchmarks\nbut also for business use cases. Consequently,\nunderstanding the bottlenecks of current evalua-\ntion methods, and developing strategies to address\nthese challenges are crucial for standardizing eval-\nuations and enabling reliable use of LLMs in prac-\ntical applications. Nonetheless, evaluating LLMs\nis as complex and resource-intensive as their de-\nvelopment, involving multiple levels or aspects.\nExisting reviews (Chang et al., 2024; Guo et al.,\n2023b; Liang et al., 2022; Minaee et al., 2024;\nZhuang et al., 2023) related to the evaluation\nof LLMs often focus only on benchmark tasks,\ndatasets, and evaluation criteria, neglecting the\nbroader complexities. This oversight can under-\nmine the reliability of evaluation by ignoring is-\nsues like robustness and reproducibility. While\nsome recent studies (Balloccu et al., 2024; Mao\net al., 2023) have investigated data contamination\n(Ravaut et al., 2024) and evaluation malpractices\nin LLM evaluation, their focus is limited to only\nassessing ChatGPT, overlooking other LLMs, as\nwell as the entire evaluation pipeline.\nMore recently, Biderman et al. (2024) discussed\nthe reproducibility problem in existing evaluations\nof LLMs and introduced a library to address this.\nHowever, their work lacks comprehensive discus-\nsions on how aspects like reliability or robustness\nimpact LLM evaluation and how to address them.\nHence, existing LLM evaluation studies often fo-\ncus on individual aspects in a scattered manner,\nresulting in findings that are only sparsely useful.\nTo mitigate this gap, this paper brings together\nthe discussions to address the fundamental chal-\nlenges and limitations in LLM evaluations that\nemerge from diverse evaluation setups. First,\nwe craft a schematic workflow of the evaluation\npipeline in practical settings (presented in Sec-\ntion 2) for a systematic study. We then examine\neach step in the evaluation workflow, uncovering\nvarious inconsistencies and decision-making com-\nplexities affecting reproducibility, reliability, and\nrobustness (see Section 3). Based on our findings,"}, {"title": "2 Overview of LLM Evaluation Process", "content": "The following components are crucial for LLM\nevaluation: Evaluation Setup, Response Genera-\ntion, and Evaluation Methodology (Chang et al.,\n2024). Each component has its own challenges,\nwhich we discuss in Section 3. These components\nin an evaluation workflow are shown in Figure 1.\n2.1 Evaluation Setup\nBenchmark Selection: To initiate the evalua-\ntion process of LLMs, the first step is selecting ap-\npropriate benchmarks. We categorize the bench-\nmarking datasets into the following: general ca-\npability benchmarks, specialized benchmarks, and\nother diverse benchmarks. We refer to general ca-\npability benchmarks as the ones that are often used\nfor evaluation upon the release of an LLM (e.g.,\nMMLU (Hendrycks et al., 2020b), HumanEval\n(Chen et al., 2021)). In addition, there are special-\nized benchmarks that measure specific capabilities\nof LLMs (e.g., MT-Bench for chatting capabilities\n(Zheng et al., 2024)). There are also other bench-\nmarks that usually combine multiple benchmarks\nto evaluate LLMs on diverse task (e.g., HELM\n(Liang et al., 2022)). We provide more details on\neach category in Appendix A.1.\nModel Selection: Selecting the appropriate\nmodel from the numerous LLMs currently avail-\nable is crucial for ensuring a fair evaluation, as it\nhelps to avoid risks such as data contamination and\nunfair comparisons. For a detailed discussion on\nprominent LLMs, see Appendix A.2.\n2.2 Response Generation\nOnce the benchmarks and the models are selected,\nthe next step in the evaluation process is to design\nthe prompt and set up the decoding parameters for\nresponse generation. In the prompt design step,\ndecisions on what type of prompting (e.g., zero-\nshot or few-shot) would be used are taken. More-\nover, configuring the decoding parameters (e.g.,\ntemperature) is important to ensure optimal per-\nformance (Shi et al., 2024). More discussions on\nthis are provided in Appendix A.3 and A.4.\n2.3 Evaluation Methodology\nParsing Script Design: Evaluating LLM-\ngenerated responses is difficult because they often\nproduce verbose outputs (see Table 4 for some\nexamples). Therefore, parsing scripts are often\nnecessary (Jahan et al., 2024; Laskar et al., 2023a)\nto extract target labels before applying evaluation\nmetrics, ensuring alignment with evaluation\ncriteria to maintain reliability.\nEvaluation Approach: The evaluation ap-\nproach can be divided into the following:\nautomatic evaluation, human evaluation, LLMs\nas evaluators. In automatic evaluation, before\napplying task-specific metrics (e.g., F1, Exact\nMatch, Perplexity (Jelinek et al., 1977)), parsing"}, {"title": "3 Challenges in Evaluating LLMs", "content": "We examine challenges and limitations in the eval-\nuation process of LLMs based on three dimen-\nsions: reproducibility, reliability, and robustness.\n3.1 Reproducibility\nReproducibility, the ability to consistently repli-\ncate model results under the same conditions, is\na major challenge in generative models (Bider-\nman et al., 2024). The primary challenge is the\nlack of comprehensive documentation for each\npart of the evaluation cycle, including benchmark-\ning datasets, prompt construction, model details,\ndecoding strategy, response parsing, and evalua-\ntion methodology (Kosch and Feger, 2024; McIn-\ntosh et al., 2024). Table 1 presents an analysis by\nBalloccu et al. (2024), revealing that a relatively\nlow percentage of the analyzed papers shared their\nresources. Below, we discuss factors impacting re-\nproducibility in the evaluation step.\n3.1.1 Missing Details on Data & Models Used\nBenchmarking Data: One factor that can nega-\ntively impede the ability to reproduce results is not\nreleasing the exact data used for evaluation (Bal-\nloccu et al., 2024). Many studies evaluate LLMs\non only a subset of existing datasets (Bang et al.,\n2023; Koco\u0144 et al., 2023), while others use the ex-\nact benchmarking datasets (Laskar et al., 2023a;\nQin et al., 2023). Despite the expectation not to\ncompare results across studies using different sub-\nsets of the data, such comparisons often occur, as\ndiscussed by Balloccu et al. (2024). Nonetheless,\nwithout explaining the sampling strategy, or re-\nleasing the subsets used for evaluation (and possi-\nbly their responses), reproducing results using dif-\nferent data subsets of the same size is challenging.\nModel Versions: The information regarding the\nversion of a model being used is also missing\nin many studies (Balloccu et al., 2024; Biderman\net al., 2024), creating reproducibility concern (see\nTable 1). The continuous updates of the closed-\nsource models, often with undisclosed changes\ncan also impact reproducibility. With these up-\ndates, earlier versions are often deprecated, and re-\nsults from these versions may not apply to newer\nmodels (Chen et al., 2023b), making prior evalu-\nation results to be no longer reproducible (Bang\net al., 2023; Koco\u0144 et al., 2023; Laskar et al.,\n2023a; Qin et al., 2023). Therefore, it is crucial to\nspecify the model versions used (Balloccu et al.,\n2024; Biderman et al., 2024), while model owners\nshould keep earlier versions available.\n3.1.2 Lacking Response Generation Details\nPrompting: The lack of details behind how the\nprompts are designed may make the findings in\ndifferent literature inconsistent. For instace, vari-\nations in prompt design can lead to significantly\ndifferent results, as seen in various studies (Bang\net al., 2023; Jahan et al., 2024; Laskar et al.,\n2023a; Qin et al., 2023). While few-shot learn-\ning is found to outperform zero-shot in the orig-\ninal evaluation conducted by the authors of vari-\nous LLMs (Anil et al., 2023; OpenAI, 2023; Tou-.\nvron et al., 2023b), many independent evaluations\ndemonstrate that adding few-shot examples does\nnot necessarily outperform zero-shot models in ev-\nery task (Jahan et al., 2024; Ye et al., 2023a). This\nraises the concern of whether certain prompt engi-\nneering techniques or optimizations to select few-\nshot samples were applied in the original evalu-\nations. Hence, not disclosing the details behind\nhow the prompt is designed or how the few-shot\nexamples are selected can hinder reproducibility.\nDecoding Strategy: LLMs are sensitive to de-\ncoding parameters, leading to significant perfor-\nmance variations based on the chosen settings\n(Roziere et al., 2023; Touvron et al., 2023b). How-\never, crucial details on their selection are excluded\nin existing literature (Bang et al., 2023; Koco\u0144\net al., 2023; Laskar et al., 2023a; OpenAI, 2023;"}, {"title": "3.1.3 Evaluation Methods Unavailable", "content": "Parsing Scripts: LLM-generated responses of-\nten require parsing scripts to extract desired in-\nformation. However, as demonstrated in Table 1,\nBalloccu et al. (2024) observed in their analysis\nthat almost half of the LLM evaluation papers do\nnot release any codes. We also observe that most\nstudies (these include both the LLM technical re-\nports, as well independent evaluations) do not re-\nlease their parsing scripts (Bang et al., 2023; Ko-\nc\u00f3n et al., 2023; OpenAI, 2023; Qin et al., 2023;\nTeam et al., 2023, 2024). Nonetheless, inaccu-\nrate design of parsing scripts may lead to different\nevaluation results (Laskar et al., 2023a). Thus, the\nunavailability of parsing scripts would complicate\nresult comparisons while impacting reproducibil-\nity (Balloccu et al., 2024; Biderman et al., 2024).\nEvaluation Approach: LLMs are increasingly\nused to evaluate other LLMs in development\n(Zheng et al., 2024). Concerns arise due to the\nuse of closed-source LLMs as evaluators, as their\nfrequent updates can affect reproducibility (Chen\net al., 2023b; Verga et al., 2024). Moreover, Chen\net al. (2023b) observed significant behavioral\nchanges in closed-source LLMs over short peri-\nods. Such reproducibility concerns are also ob-\nserved in prior research that used LLMs as evalua-\ntors. For instance, Chiang and Lee (2023); Zheng\net al. (2024) found that using closed-source LLMs\nas the judge could collide with human evalua-\ntions, whereas Fu et al. (2023b) observed the op-\nposite. Since the recently proposed Prometheus-2\n(Kim et al., 2024a) model is an open-source alter-\nnative and demonstrates a strong correlation with\nhumans, utilizing open-source LLMs as the judge\ncan help mitigate the reproducibility issues preva-\nlent with closed-source LLMs."}, {"title": "3.2 Reliability", "content": "Reliability, the ability to trust that outcomes are as\nintended, is another challenge encountered during\nevaluation. Issues like contamination/inaccurate\nlabels in the data, irrelevant evaluation methods,\nand unfair comparisons may impact the reliability\nof the findings, which we discuss below.\n3.2.1 Data and Model Integrity Issues\nData Integrity: Errors in benchmarks under-\nmine accurate conclusions and model compar-\nisons, rendering evaluations of LLMs unreliable.\nAn integrity-compromising factor is the presence\nof incorrect gold labels. For instance, existing is-\nsues in the gold labels of the widely used MMLU\n(Hendrycks et al., 2020b) dataset have led to the\ndevelopment of MMLU-Pro (Wang et al., 2024b)\nand MMLU-Redux (Gema et al., 2024). Recently\nit was also found that the coding benchmarks, Hu-\nmanEval (Chen et al., 2021), lacked essential test\ncases, leading to the development of an advanced\nversion, HumanEvalPlus (Liu et al., 2024b).\nDespite these improvements, many recent\nstudies continue to use the older versions of\ndatasets. For instance, despite the release of Hu-\nmanEvalPlus, HumanEval is still used to bench-\nmark LLM coding performance (Gloeckle et al.,\n2024; Jiang et al., 2023; Li et al., 2023c; Roziere\net al., 2023; Team et al., 2023, 2024; Wong et al.,\n2023), potentially providing misleading insights.\nIn addition, outdated labels in existing bench-\nmarks undermine reliability of gold references.\nFor example, in tasks like open-domain question\nanswering, which demand real-world knowledge,\nmany gold labels become outdated over time, as\nnoted by Laskar et al. (2023a). Consequently,\neven if LLMs produce correct answers, compar-\ning them to obsolete gold labels can yield inaccu-\nrate results. Moreover, in tasks like summariza-\ntion, LLM-generated summaries are often favored\nover human-annotated gold references (Ding et al.,\n2022; Pu et al., 2023; Zhang et al., 2024b).\nContamination in Existing Models: Contamina-\ntion occurs when a benchmarking dataset is used\nin training, reducing result reliability and validity\n(Sainz et al., 2023a; Shi et al., 2023; Zhou et al.,\n2023b). Ensuring benchmarking examples are ex-\ncluded from training data is essential to maintain\nreliable results. Since LLMs are pre-trained on\nvast amounts of text data available on the internet,\nthis could lead to unfair evaluations if LLMs have"}, {"title": "3.2.2 Lack of Fairness by Manipulating Response Generation", "content": "Prompt Hacking: One major concern in terms\nof lack of fairness in LLM evaluation is the possi-\nbility of prompt hacking (Schulhoff et al., 2023),\nwhich involves manipulating input prompts to a\nlanguage model to elicit desired responses (e.g.,\nbiasing the outputs, or taking unfair advantages by\nusing specific few-shot examples). While the per-\nformance of LLMs depends on many factors rel-\nevant to how the prompt is structured, most work\n(Bang et al., 2023; Laskar et al., 2023a; Qin et al.,\n2023), even the official technical reports (An-\nthropic, 2024; OpenAI, 2023; Team et al., 2023)\nof different LLMs lack the necessary details be-\nhind prompt construction (e.g., missing scientific\nvalidity on why a certain prompt was preferred\nover others, how the few-shot examples are se-\nlected, etc.). This makes the claims regarding the\neffectiveness and limitations of certain LLMs in\ncomparison to others questionable\u00b2. Recogniz-\ning these parallels underscores the need for trans-\nparency and robust methodologies to ensure fair-\nness in Al research and development."}, {"title": "3.2.3 Inappropriate Evaluation Methodology", "content": "Inaccurate Design of Parsing Scripts: As Laskar\net al. (2023a) observed, evaluating LLMs entirely\nwith an automated approach based on the answer\nextracted using parsing scripts may lead to an er-\nror of up to more than 10% difference in many\ntasks. This raises questions about the reliability\nof LLM evaluations that solely depend on parsing\nscripts without validating the scripts' effectiveness\nfor the task. To tackle this, Laskar et al. (2023a)\nproposed a hybrid approach combining parsing\nscript-based automatic evaluation with human-in-\nthe-loop (Laskar et al., 2022a; Wu et al., 2022).\nInitially, the parsing script extracts answers from\nLLM-generated responses. If any issues arise, hu-\nmans resolve them, enhancing the reliability of\nparsing-based automatic evaluation.\nIn Figure 2, we demonstrate the differences\nbetween automatic and hybrid evaluation in\nOpen-Domain QA\u00b3 and reading comprehnesion\ndatasets. The figure highlights the influence\nof human intervention on results in open-domain\nQA, where LLMs may generate synonymous or\ntime-sensitive correct answers, potentially render-\ning gold answers outdated (Laskar et al., 2023a).\nParsing script-based automatic evaluation is found\nto be reliable in Race datasets for reading com-\nprehension, whereas notable discrepancies are ob-\nserved in the SQUAD-V2 dataset. Therefore,\nthere's a need for designing dependable parsing\nscripts and involving humans when appropriate.\nEvaluation Approaches Lacking Relevancy: In\ngenerative tasks, utilizing automatic string-based\nmatching techniques may not be reliable as well.\nFor instance, Laskar et al. (2023a) observed that\ndespite LLMs scoring quite poorly on the ROUGE\nmetric compared to SOTA summarization models,"}, {"title": "3.3 Robustness", "content": "While there are many evaluation benchmarks cur-\nrently available, existing work mostly relies on\nevaluating LLMs on some common benchmarks,\nthis raises the question of whether the performance\nof LLMs in these common benchmarks in existing\nsettings reflects their true capabilities and limita-\ntions. In this section, we study the robustness of\nexisting LLM evaluations.\n3.3.1 Lacking Generalized Evaluation\nLimiting Evaluation to Certain Scenarios: In-\nterestingly, it has been observed in recent research\nthat certain performance gains in a specific dataset\nmay not necessarily imply that it would also im-\nprove the performance in other datasets for simi-\nlar tasks (Jahan et al., 2024; SambaNova, 2024).\nFor instance, Jahan et al. (2024) observes that not\na single LLM has superiority over other LLMs\nacross all biomedical datasets and tasks. This is\nalso evident if we compare the results between\nLLaMA-3 and Qwen2 reported in (Qwen2, 2024).\nAs shown in Figure 3, while the Qwen2 model out-\nperforms LLaMA-3 on most datasets, it falls short\non GPQA and MBPP. Interestingly, for coding"}, {"title": "3.3.2 No Tuning of Prompt and Decoding Parameters", "content": "While various combinations of decoding parame-\nters may lead to differences in results (Shi et al.,\n2024), possibly due to high computing require-\nments, existing LLM evaluation work mostly un-\ndermines the necessity of evaluating how the\nmodel performance may vary depending on its\nvariations. Similar to the absence of decoder pa-\nrameter tuning, most prior work also evaluated\nLLMs using only a single prompt (Bang et al.,\n2023; Jahan et al., 2024; Koco\u0144 et al., 2023;\nLaskar et al., 2023a; Qin et al., 2023). However,\nin the real world, users express themselves with\ndiverse word choices, varying semantics and syn-\ntaxes, alongside minor discrepancies (e.g., mis-\nspellings or differing punctuation styles). To fur-\nther examine the effects of prompt variations, we\nconduct an experiment using GPT-4o (2024-04-\n09) and GPT-3.5-Turbo (0125) (OpenAI, 2023), as\nwell as Claude-3-Opus (2024-02-29) (Anthropic,\n2024) with the prompts used by (Laskar et al.,\n2023a) and (Qin et al., 2023) in the SAMSum\ndataset. For this experiment, the default param-\neters for respective LLMs are used.\nAs shown in Figure 4, the restricted prompt-\ning method by Laskar et al. (2023a) consistently\noutperforms the unrestricted approach across all\nthree models. Conversely, the restricted prompt-\ning method by Qin et al. (2023) fails to surpass\nthe unrestricted approach for GPT-3.5 and GPT-\n40. However, it surprisingly outperforms the unre-\nstricted method, indicating the significant impact\nof prompt tuning across models. Evaluating lan-\nguage models with a single prompt lacks fairness\n(Zhu et al., 2023b), yet it remains common prac-\ntice (Bang et al., 2023; Laskar et al., 2023a; Qin"}, {"title": "3.3.3 Evaluation Method's Generalizability and Correlation Shortcomings", "content": "While automatic evaluations are usually utilized\nin discriminative tasks, they may not be applica-\nble to every task, as demonstrated by Jahan et al.\n(2024) that parsing scripts are not usable in cer-\ntain discriminative tasks like relation extraction.\nJahan et al. (2024) also noted a significant per-\nformance gap between the string-matching-based\nROUGE metric (Lin, 2004) and the contextual\nsimilarity-based metric BERTScore (Zhang et al.,\n2019) in text summarization. While larger mod-\nels achieve better accuracy, they involve a speed-\naccuracy trade-off (Parvez et al., 2019), leading to\nhigher costs and latency (Fu et al., 2024b; Laskar\net al., 2023b). While metrics like perplexity are\nwidely used to evaluate language models (Chen\net al., 2023c), Huang et al. (2024b) found that\nquantized LLaMA-3 versions have lower output\nconfidence than the original. They noted simi-\nlar model rankings for perplexity and a common-\nsense QA dataset. However, Hu et al. (2024) found\nno correlation between perplexity and long context\nunderstanding tasks, highlighting the need for ro-\nbust evaluations with human-correlated metrics.\nThis raises another question, whether au-"}, {"title": "4 Recommendations and Best Practices", "content": "We've outlined the primary challenges in evaluat-\ning LLMs thus far. In light of these challenges, a\ncrucial question arises: How can we enhance the\nevaluation process for LLMs? Crafting a struc-\ntured framework that's both practical and easy to\nimplement is daunting, given the complexities of\ngenerative LLM development. Previous studies\nhave tended to focus on specific evaluation as-\npects without offering comprehensive guidelines\nfor the entire evaluation cycle, leaving researchers\nwithout clear guidance. Before diving into rec-\nommendations for each evaluation stage, it's im-\nportant to acknowledge three key factors shaping\ncurrent LLM evaluation practices: inherent ran-\ndomness in generative models, significant compu-\ntational demands, and insufficient documentation\nacross stages.\nEvaluation Setup: Selecting benchmarks for\nmodel assessment is crucial. Rather than sim-\nply replicating past choices, researchers should\nalign datasets with required capabilities. To ensure\nrobustness, datasets should vary across expected\nLLM capabilities (e.g., long-context understand-\ning), tasks (e.g., summarization), and language\ncomplexity (e.g., vocabulary coverage). Ideally,\na metric should measure dataset diversity. For\nmodel selection, conduct contamination tests be-\ntween the chosen model and benchmarks using\nrelevant techniques (Ravaut et al., 2024). This acts\nas an additional filter for benchmarking datasets,\nensuring selection of unseen ones measuring in-\ntended capabilities. Meanwhile, for reproducibil-\nity, document any subset use of benchmarking\ndatasets, along with the selected model version.\nIn addition, throughout scientific history, intel-\nligence progress has evolved across generations.\nTests from a decade ago may appear simplis-\ntic compared to today's standards (e.g., Math\nOlympiads, ICPC programming contests). Re-\nfreshing LLM evaluations periodically can effec-\ntively communicate standard capabilities in both\nopen and closed-source LLM markets and ecosys-\ntems (e.g., chatbots, translation tools). Hence, to\nensure reliability, verify if the dataset has updated\nversions and incorporate them if available (e.g.,\nHumanEvalPlus (Liu et al., 2024b), MMLU-Pro\n(Wang et al., 2024b))\nResponse Generation: For reproducibility,\nthorough documentation of prompts and parame-\nter settings is essential (e.g., specifying how few-\nshot samples are selected). To ensure reliabil-\nity, it's crucial to justify why specific prompts\nand parameter settings are chosen over others,\nand provide comparisons with alternative options.\nAs for robustness, running experiments with di-\nverse prompts and parameters is key to showcas-\ning their effectiveness and limitations across dif-\nferent scenarios. In resource-constrained environ-\nments, conducting experiments with diverse eval-\nuation settings may pose challenges, yet it remains\nvital to perform robust evaluations on at least a\nsubset of samples.\nEvaluation Methodology: To ensure repro-\nducibility, the parsing scripts and the output\ndata used for evaluation should be published.\nMeanwhile, sanity-checking on the parsing script\nshould be done to ensure reliability and robustness\nof the designed parsing script. This can be done by\ncreating test cases for various response types, and\nthen verifying (with human intervention if possi-"}, {"title": "5 Conclusions and Future Work", "content": "In this paper, we systematically survey the chal-\nlenges and limitations in evaluating LLMs. We\nidentified significant inconsistencies and complex-\nities at various stages of the evaluation pipeline,\nimpacting the reproducibility, reliability, and ro-\nbustness of the results. These issues underline the\nnecessity for a standardized and systematic ap-\nproach to evaluating LLMs to ensure their reli-\nable usage in real-world applications. By compre-\nhensively reviewing the current evaluation prac-\ntices, we have provided a set of recommenda-\ntions aimed at enhancing the consistency and re-\nliability of LLM evaluations. Therefore, future\nwork should focus on developing and adopting\nstandardized evaluation protocols for LLMs to ad-\ndress the identified inconsistencies and complex-\nities. This includes creating benchmark datasets,\nevaluation metrics, and proper documentation of\nthe evaluation settings to ensure reproducibility,\nreliability, and robustness.\nLimitations\nOne limitation of this work is that it is focused\nonly on the evaluation phase of the LLM devel-\nopment cycle. Therefore, the challenges and lim-\nitations that happen during the training phase of\nLLMs are left out of the scope of this paper.\nNonetheless, with the rapid growth of LLM tech-\nnologies and huge financial incentives, it is es-\nsential to conduct a fair and reliable evaluation\nof LLM, alongside ensuring robustness and repro-\nducibility, which is the focus of this work.\nAnother limitation of this study is that it does\nnot study how to prevent closed-source LLMs\nfrom getting access to the online benchmarks. For\ninstance, assume we have two entities: model de-\nvelopers and evaluators. Evaluators do not want\nto expose their data to the modeling team. Con-\nversely, model developers do not want to release\ntheir model weights due to significant financial in-\ncentives. If evaluators use an API to get the re-\nsponses, there is a risk that the queries may get ex-\nposed to the model developers. Therefore, without\ngetting access to the weights, evaluators cannot re-\nliably assess the models on their queries. Mathe-\nmatically and technically, there is no fundamen-\ntal way to solve this problem without altering the\ntraining dynamics which may not be an option for\ntraining teams.\nFinally, the multimodal capability, in other\nwords, the ability to understand both language and\nvision is another interesting capability of recently\nproposed LLMs (Bai et al., 2023; Chen et al.,\n2023a; Dai et al., 2024; Liu et al., 2023b, 2024a;\nLuo et al., 2024; Ye et al., 2023b; Zhang et al.,\n2023; Zhu et al., 2023a). This has led to the devel-\nopment of many multi-modal benchmarks (Chen\net al., 2024b; Fu et al., 2023a, 2024a; Guan et al.,\n2023; Li et al., 2023a,b,d; Liu et al., 2024a, 2023e;\nLu et al., 2022; Qiu et al., 2024; Yu et al., 2023).\nHowever, this paper was mostly focused on text-\nbased NLP tasks and the evaluation of LLMs on\nmultimodal benchmarks is left out for future work."}, {"title": "6 Ethics Statement", "content": "This paper only reviews the existing challenges\nand limitations in LLM evaluations and provides\nan opinion piece and recommendation to ensure\nreliable, robust, and reproducible evaluations of\nLLMs. Thus, this review does not pose any eth-\nical concerns."}, {"title": "A Appendix", "content": "A.1 Benchmarking Datasets\nGeneral Capability Benchmarks: To bench-\nmark the performance of LLMs", "Benchmarks": "There are also spe-\ncialized benchmarks that measure specific capabil-\nities of LLMs. For instance, the MT-Bench (Zheng\net al., 2024)) evaluates whether LLMs can prop-\nerly engage in conversations, the RewardBench\n(Lambert et al., 2024) assesses the performance of\nreward models. Other specialized benchmarks like\nthe"}]}