{"title": "Deep-Bench: Deep Learning Benchmark Dataset for Code Generation", "authors": ["Alireza Daghighfarsoodeh", "Chung-Yu Wang", "Hamed Taherkhani", "Melika Sepidband", "Mohammad Abdollahi", "Hadi Hemmati", "Hung Viet Pham"], "abstract": "Deep learning (DL) has revolutionized areas such as computer vision, natural language processing, and more. However, developing DL systems is challenging due to the complexity of DL workflows. Large Language Models (LLMs), such as GPT, Claude, Llama, Mistral, etc., have emerged as promising tools to assist in DL code generation, offering potential solutions to these challenges. Despite this, existing benchmarks such as DS-1000 are limited, as they primarily focus on small DL code snippets related to pre/post-processing tasks and lack a comprehensive coverage of the full DL pipeline, including different DL phases and input data types.\nTo address this, we introduce Deep-Bench, a novel benchmark dataset designed for function-level DL code generation. Deep-Bench categorizes DL problems based on three key aspects: phases such as pre-processing, model construction, and training; tasks, including classification, regression, and recommendation; and input data types such as tabular, image, and text.\nGPT-40-the state-of-the-art LLM-achieved 31% accuracy on Deep-Bench, significantly lower than its 60% on DS-1000. We observed similar difficulty for other LLMs (e.g., 28% vs. 54% for Claude, 21% vs. 41% for LLaMA, and 15% vs. 20% for Mistral). This result underscores Deep-Bench's greater complexity. We also construct a taxonomy of issues and bugs found in LLM-generated DL code, which highlights the distinct challenges that LLMs face when generating DL code compared to general code.\nFurthermore, our analysis also reveals substantial performance variations across categories, with differences of up to 7% among phases and 37% among tasks. These disparities suggest that Deep-Bench offers valuable insights into the LLMs' performance and areas for potential improvement in the DL domain.", "sections": [{"title": "1. Introduction", "content": "In recent years, machine learning (ML) and deep learning (DL) have advanced significantly and have been integrated into various fields (Hordri et al., 2016; Kamilaris & Prenafeta-Bold\u00fa, 2018; Gamboa, 2017). DL coding has its challenges (Arpteg et al., 2018), and because of its widespread use, many DL systems are developed by domain experts who are often not software developers (Park et al., 2021; Singaravel et al., 2020), which amplifies the problems even more.\nRecently, with the rise of Large Language Models (LLMs) such as ChatGPT, LLMs are considered among the best solutions for coding tasks (Wang et al., 2021; Feng et al., 2020; Achiam et al., 2023). As shown in Table 1, numerous code generation datasets (Hendrycks et al.; Austin et al., 2021; Agashe et al., 2019; Lu et al., 2021; Yu et al., 2018; Du et al., 2024; Zhuo et al., 2024) benchmark LLMs' code generation capabilities. However, DS-1000 (Lai et al., 2023) is the only dataset offering a limited set of DL-specific code generation samples. Specifically, DS-1000 provides small code snippets, typically just a few lines, primarily focused on pre/post-processing tasks. Furthermore, they do not include categorizations such as DL phases (e.g., pre/post-processing, model construction, training, inference, evaluation) or input types (e.g., tabular, image, text), which could provide valuable insights for advancing DL code generation research.\nTo address this gap, we introduce Deep-Bench, a novel dataset designed to benchmark DL code generation at a functional level. Deep-Bench includes the code generation prompt, the ground-truth code at the function level, an extensive set of unit tests, and three categorizations (DL phases, ML tasks, and input data types). Unlike DS-1000, Deep-Bench provides a more comprehensive set of entries that cover the full DL pipeline, encompassing code examples for various machine learning tasks. Additionally, it includes DL functions with diverse input data types, including tabular, image, and text, making it a more diverse benchmark for DL code generation evaluation. Specifically, each entry of Deep-Bench dataset is categorized based on these three categories which allow a more in-depth evaluation of future DL code generation techniques: (1)The DL/ML pipeline stages (consist of pre/post-processing, model construction, training,"}, {"title": "2. Related Works", "content": "Code Generation for ML software: Shin et. al.(Shin et al., 2023) have explored the effectiveness of neural code generation models on ML tasks, which differ significantly from general programming tasks. A key study evaluated six state-of-the-art models across popular ML libraries, highlighting the need for domain-specific benchmarks and improvements tailored to the complexities of ML. Our work provides such benchmarks specifically for ML and DL software.\nRecently, DS-1000(Lai et al., 2023) is the only benchmark designed for data science code generation. It contains 1000 problems sourced from StackOverflow with diverse, practical tasks. Similarly, MLE-bench (Chan et al., 2024) targets evaluating ML engineering workflows in Kaggle competitions. However, Deep-Bench differs in key aspects: 1) it focuses on ML/DL tasks rather than general data science and MLE, 2) we categorize the data by ML phases, task types, and data types, and 3) our granularity is at the function level rather than script or workflow level. Additionally, unlike the other datasets, our dataset is based on GitHub repositories with real code.\nCode Generation Benchmarks: There have been multiple code generation benchmarks as shown in Table 1. Among them, HumanEval (Chen et al., 2021) is the most popular with 164 hand-crafted programming problems. Building on this foundation, AiXBench(Hao et al., 2022) extends the evaluation to Java, and MultiPL-E(Cassano et al., 2022) expands by supporting 18 different programming languages. Another prominent benchmark is MBPP (Austin et al., 2021) which offers 974 programming tasks solvable by entry-level programmers. Additionally, the Spider benchmark (Yu et al., 2018) provides 10,000 questions paired with 5,000 SQL queries. CoderEval (Yu et al., 2024) and APPS benchmark (Hendrycks et al.) assess code generation models in real-world programming scenarios in Python and Java with coding problems ranging from beginner to advanced competitive programming challenges. RepoEval (Zhang et al., 2023), meanwhile, assesses LLMs for repository-level code generation. All the above-mentioned benchmarks focus on general programming, unlike Deep-Bench, which focuses on DL/ML code generation problems."}, {"title": "3. Benchmark Construction", "content": "Deep-Bench consists of 520 instances of AI and DL data points (filtered from over 2,000 raw data points). The data is curated from 30 GitHub repositories (selected from an initial pool of 160 related repositories).\nThe construction process of Deep-Bench consists of two main phases: The Raw Data Extraction and the Label-ing Procedure. The raw data extraction involves six semi-automatic steps. Since Deep-Bench is designed to have diverse and realistic code samples, the first step (1) is to construct Deep-Bench from code crawled from highly rated GitHub repositories (i.e., with the most stars) filtered using 30 DL-related terms such as \u201cneural-networks\", \"pytorch\", \u201ccomputer-vision\u201d. We then manually select (step (2)) 160 high quality candidate DL projects (i.e., involve the integration of DL and AI-related frameworks, comprehensive test cases, clear and well-written docstrings, and detailed contri-\""}, {"title": "3.1. Raw Data Extraction", "content": "This phase consists of six semi-automatic steps that crawl data from GitHub repositories to generate a list of function definitions and their test cases.\nRepository Selection: We curated our data from the top 1000 starred DL-related GitHub repositories to include high-quality and widely used DL-related functions.\nIn step 1 we filtered GitHub projects with one of 30 DL-related tags such as \u201cneural-networks\u201d, \u201cpytorch\u201d, and \"computer-vision\u201d (we provided the complete list of tags in our repository). Specifically, we select the tags by collecting from DL and AI-related GitHub repositories and filtering the most relevant ones to get the final 30.\nIn step (2), we select 160 most relevant projects for Deep-Bench and retain only projects that: 1) are DL related (i.e., use DL libraries, or perform DL tasks like segmentation or detection), 2) have sufficient test cases (averaging at least three per function), and 3) include thorough documentation, such as source code docstrings or README files.\nFunction Extraction: One of the main design choices of Deep-Bench is to include a set of reliable and robust test cases for each benchmark entry. This is because programming languages are different from natural languages. Specifically, generated code can fulfill all of the functional requirements but could have a low BLEU score when compared with the ground truth code(Tran et al., 2019). This means that using text similarity metrics such as BLEU score as evaluation metrics is not the best method to evaluate code generation techniques. Instead, test cases (functional and non-functional) passing rate should be used to reliably access a new code generation approach.\nIn step (3 we crawled selected repositories for test files using standard test file name patterns such as tests/test_file_name.py (Madeja et al., 2021). In step (4), for each test file, we extract test cases using common patterns in Python test suites, such as the @pytest decorator.\nOnce we identified all test cases, in step (5), we performed"}, {"title": "3.2. Labeling Procedure", "content": "The labeling procedure involves three semi-automatic steps to generate and refine a prompt and assign categorizations for each entry in our Deep-Bench dataset. To determine the best procedure and criteria for our manual process, we perform a small trial run of the manual process on a small sample of the data points. In this trial run, we ask each reviewer to provide feedback on the labeling criteria so that when we start our full run we have the most comprehensive and accurate manual process possible.\nPrompt Generation: In step 7, we utilize two sources of data to create the code generation prompts: 1) the doc-strings provided by developers, which describe the functionality and parameters of the code, and 2) the function definitions themselves, which can be used to generate candidate prompts. Specifically, We take advantage of the function definitions to explain the code, and by combining them with their respective doc-strings (when available), we generate the initial candidate prompt by querying GPT-40 with the template as described in Fig. 2.\nHowever, generated prompts require manual validation to ensure accuracy and relevance. This review process is essential to refine prompts and guarantee quality for subsequent use (Shrivastava et al., 2023). We further refine prompts based on the following criteria: (1) contain clear, sufficient information for code generation, (2) specify input and output format, and (3) cover error handling and boundary conditions. More details are in the appendix.\nIf the prompt does not meet the mentioned criteria, the annotators propose and agree on changes that bring it up to the expected quality. This reviewing process produces prompts that are not only technically correct but also include details essential to code generation.\nData Filtering and Validation: After compiling all the data (i.e., the ground truth, test cases, and candidate prompts), in step (8 we manually evaluate each function meticulously, reading and modifying the prompts following a set of criteria. Specifically, we discard general codes (e.g., those for reading text files) that are not DL related. In this step, the annotators independently assess the prompt's clarity, relevance to DL-related tasks, and overall usability with the following criteria: (1) serving key DL tasks, (2) utilization of popular DL frameworks, and (3) algorithms' relevancy and clarity.\nLabeling: In step (9 we assign labels for each data point based on the role of the function in the ML pipeline (e.g.,"}, {"title": "4. Evaluation", "content": "To demonstrate the potential of Deep-Bench and the depth of analysis that can be done on Deep-Bench, we perform a preliminary study evaluating the performance of state-of-the-art LLM on the Deep-Bench benchmark. We choose the top LLMs: GPT-40, Claude3.5 sonnet, Llama 3.1 70B, and Mistral 7B. The LLMs are evaluated based on the commonly used pass@k, which measures the likelihood that at least one of the k-generated solutions passes all test cases (Lyu et al., 2024).\nTo minimize non-determinism and improve reproducibility, we set the temperature to zero for LLMs (Bommasani et al., 2021). This allowed us to accurately measure the quality of the generated code across various DL tasks, offering deeper insights into the LLM's strengths and weaknesses.\nFor each project, Deep-Bench provides a docker image that includes all required dependencies. These images ensure that test cases are runnable and the evaluations are easily replicable on other systems."}, {"title": "5. Quantitaive Analysis", "content": "What are the performances of SOTA LLMs on DL/ML code generation tasks? In this analysis, we investigate how the existing ML code generation benchmark (DS-1000) and Deep-Bench evaluate SOTA LLMs. Table 2 shows the pass@1 metric of SOTA LLMs on those benchmarks. To focus on demonstrating Deep-Bench's ability to evaluate existing LLMs, we intentionally avoided using specialized prompt strategies, opting instead for vanilla prompts to assess the model's baseline performance. However, the use of advanced prompt engineering strategies could yield different results and this will be interesting for future usage of Deep-Bench.\nOur analysis underscores that even the most advanced model such as GPT-40 struggles with ML/DL-specific code generation. Specifically, GPT-4o achieves only 60% and 31% Pass@1 scores on DS-1000 and Deep-Bench respectively. Also, when comparing between DS-1000 and Deep-Bench, our benchmark is more challenging as the SOTA LLM gets a much lower Pass@1 score, SOTA LLMs in other categories are behind GPT-40 with performance as low as 15% for the tiny Mistral 7B model on Deep-Bench. The overall weak performance of these models highlights the ongoing challenges in generating reliable, executable ML/DL code which supports the need for more benchmarks in this domain.\nOverall, GPT-40's pass@kis 31%, but to further assess its performance, we calculated the average passing rate across the functions, which was higher at 40%. This suggests that while only 31% of functions pass all test cases, many pass some. With additional insights, these partially valid cases could be improved.\nFinding 1: Our evaluation indicates that current LLMs struggle to generate correct, executable code for ML/DL tasks. Although GPT-40 is the strongest among the tested models, it still falls short of meeting practical standards (Pass@k score of only 31%). A deeper analysis is needed (which Deep-Bench can provide) to extract insight into improving prompting techniques.\nWhich stages in the ML pipeline pose a greater challenge for SOTA LLMS? We conduct an analysis of the challenges of generating DL code for specific stages in the pipeline (enabled by Deep-Bench provided categorization) Table 3 presents the Pass@1 scores that each LLM achieves for code in each ML/DL pipeline stage. Our result shows that the best SOTA LLM\u2014GPT-40\u2014-outperforms all of the other LLMs in all stages. Claude 3.5 sonnet is the closest second, where in the Inference category, it is level with GPT-40. Pre/post Processing stages often require small but important data transformation tasks, thus such codes are the most available. Hence, Deep-Bench collected the most data in this category (210/520). Code in this category prepares and cleans the input data for the model and formats the model's output. Our result shows that LLMs have the most success in generating code for the Pre/post Processing stages. One possible reason for the higher Pass@1 scores is that the models might have more changes to learn from a vast set of samples in this category.\nOn the other hand, LLMs struggle to generate code for the Model Construction stage with the lowest Pass@1 scores. This is because the code for this stage is more complex, very project-specific, and often longer.\nFinding 2: Our study shows that LLMs perform best (e.g., GPT-40's Pass@1 score is 33%) in Pre/post Processing stages because code in such stages involves common, repetitive tasks, making them easier to learn and generate. In contrast, the Model Construction stage has the lowest scores (e.g., GPT-40's Pass@1 is 26%) due to its complexity, variability across projects, and the need to integrate multiple libraries. Deep-Bench enables detailed analysis which helps future work develop prompting techniques to provide LLMs with the appropriate context and information, improving their performance.\nAre different ML task types easier or harder to generate code for? In this analysis, we demonstrate how Deep-Bench's categorization of ML task types can enable deeper analysis of LLMs code generation and may provide additional insight that can help research propose more accurate prompting techniques and models. Specifically, we made use of our categorization of ML/DL task types: Classification, Regression, Object Detection, Image Segmentation, Time Series Prediction, Recommendation, and General.\nThere is a significant disparity in the Pass@1 score of generated code across task types. Notably, scores for Recommendation tasks were the highest among all LLMs, with the best score of 58% with GPT-40. On the other end of the scale, Image Segmentation tasks' scores are the lowest"}, {"title": "6. Qualitative Analysis", "content": "Taxonomy of Bugs in Generated DL Code: GPT-40 achieves a pass@1 rate of only 31% hence in most cases, it failed to generate the correct code. In this section, we build a taxonomy of common bug patterns and issues that arise in DL code generated by GPT-40. This taxonomy is an expansion of Tambon et.al (Tambon et al., 2024)'s bug taxonomy for LLM-generated regular code.\nFollowing the same procedure as our labeling process, three authors manually investigate all GPT-40 failures and categorize them following Tambon et.al's taxonomy. At the same time, the annotators identify the DL-specific sub-categories for each failure. The result is the taxonomy presented in Fig 3. The appendix gives a detailed explanation of each bug type and sub-category.\nFailures in Generating the DL Code and General Code: Tambon et. al(Tambon et al., 2024) analyzed failures when CodeGen models generate code for the general tasks. Figures 4 show the distributions of the bug types when generating general code vs DL code. On the one hand, misinterpretation (purple) is a common bug when generating both general code and DL code, however, due to more complex logic and arithmetic requirements, LLMs more often make this mistake when generating DL code. An example of this type of bug can be seen in Figure 6. On the other hand, since GPT40 is much more capable compared to CodeGen models used by prior work, errors such as incomplete generation (green), silly mistake (dark gray), and syntax error (yellow) occur at a much lower rate.\nFurthermore, we have introduced several new categories of bugs that commonly arise in DL code generation. Firstly, errors in arithmetic and logical operations(light blue) occur when incorrect calculations or flawed logical code are generated. Secondly, performance(light brown) issues involve inefficient generated code with slow execution times, excessive memory consumption, or suboptimal utilization of resources. Lastly, prompt missing information(light purple) when the prompts are missing details to fully address the problem at hand, resulting in incomplete or partially implemented solutions. These new categories identify important challenges that are unique to DL code generation.\nObservation 1: Misinterpretation is a common issue in both generated general code and DL code, however, due to more complex logic and arithmetic requirements, LLMs are more likely to make this mistake when generating DL code. Errors in arithmetic and logical operations, performance, and prompt missing information emerged as new issues that are specific to DL code generation."}, {"title": "Bugs in Human-Written vs. LLM-Generated DL Code:", "content": "On one hand, prior study (Islam et al., 2019) identified the most common types of bugs in human-written DL code which include logic errors, API misuse, and data-related issues. Among these, API misuse is the most prevalent bug pattern in human-written DL code when using Tensor-Flow, whereas data flow bugs are more common when using PyTorch. On the other hand, according to our analysis of LLM-generated DL code, although API misuse remains a frequent issue, data structural problems, such as tensor mismatches and dimensional errors, occupy more frequently. Figure 5 highlights an instance of dimensional mismatches in LLM-generated DL code. In this case, GPT-40 incorrectly assumes that each shift value can be applied directly to all pixels in the image channel, causing a shape mismatch. Similar to prior findings (Islam et al., 2019) of human-written DL code, LLM-generated DL code often contains logic errors. This similarity may stem from the fact that LLMs are trained on human-written code, thereby inheriting logical structures and concepts from human programmers. An example of such logic-related bugs is shown in Figure 6, demonstrating how LLMs replicate logical reasoning errors that occur in human-written code. Here, GPT-40 applies scale_x only to the cosine whereas the scaling factors scale_x and scale_y should be applied uniformly to both the sine and cosine components of the rotation matrix. This results in improper scaling along the axes and triggers a test failure. Additionally, API misuse is a common bug pattern occurred in both human-written and LLM-generated DL code. Figure 7 provides an example of API misuse in LLM-generated code where GPT-40 attempts to call torch.idct, which is not implemented in PyTorch. One possible fix is to provide more context concerning third-party libraries. For example, one could hint to LLMs to use scipy instead, resulting in scipy.fftpack.idct(x.numpy(), norm=norm) instead.\nObservation 2: Unlike human-written DL code, LLM-generated DL code contains more data structural problems, such as tensor mismatches and dimensional errors. Similar to prior findings of human-written DL code, LLM-generated DL code often contains logic errors. Additionally, API misuse frequently occurs as a bug pattern in both human-written and LLM-generated DL code. These overlaps suggest that while LLMs exhibit unique weaknesses, their reliance on human-generated training data also leads to shared bug patterns, particularly in logic and API misuse errors."}, {"title": "7. Threats to validity", "content": "Even with the temperature parameter set to zero, our experiments still utilized non-deterministic models. While a lower temperature reduces randomness, it does not fully eliminate variability in the models' outputs (Ouyang et al., 2023; Song et al., 2024). Also, we sourced data from various repositories related to DL and AI but did not include all possible repositories or tags. Expanding the dataset could capture a wider range of use cases and code patterns.\nData labeling was performed by four independent annotators, achieving strong inter-rater reliability. Despite this, some labeling conflicts persisted and were addressed through discussions to reach a consensus. However, not all discrepancies could be fully resolved. Also, even if we used the commonly used pass@k metric to evaluate model performance, prior research (Shiri Harzevili et al.) shows that passing all test cases does not guarantee complete code correctness, especially in edge cases."}, {"title": "8. Conclusion", "content": "In this paper, we introduce Deep-Bench, a benchmark for deep learning tasks related to code generation. The dataset comprises 520 instances, gathered from the most starred and recently updated GitHub repositories. We categorize the data based on the pipeline stage, ML task, and input data type. Additionally, our quantitative analysis of the performance of four state-of-the-art LLMs on Deep-Bench reveals that DL code generation is challenging and Deep-Bench can provide more insight to help improve the generation process. Using our taxonomy of issues found in LLM-generated DL code, the qualitative analysis reveals the distinct challenges that LLMs face when generating DL code compared to general code as well as the similarities and differences between human-written and LLM-generated DL code."}, {"title": "Classification", "content": "Classification tasks involve assigning input data to categories or classes. For example, models using softmax activation in the final layer for outputs like \"dog\" or \"cat\" fall under this category. Categorical cross-entropy loss is a common indicator."}, {"title": "Regression", "content": "Regression tasks predict continuous values. Code indicating regression tasks often has linear activation functions in the final layer."}, {"title": "Object Detection", "content": "Detection tasks identify objects and their locations within images. Code that outputs bounding boxes and class labels (e.g., YOLO, Faster R-CNN) and employs anchor boxes or non-maximum suppression is indicative of detection tasks."}, {"title": "Image Segmentation", "content": "Segmentation tasks assign labels to each pixel in an image. Code involving semantic or instance segmentation (e.g., U-Net, Mask R-CNN) where the output is a mask with pixel-level classifications is a common example."}, {"title": "Time Series Prediction", "content": "These tasks forecast future values using historical data. Code involving recurrent neural networks (RNNs), LSTM, GRU models, and loss functions like mean absolute error (MAE) or MSE is typical."}, {"title": "Recommendation", "content": "Recommendation tasks suggest items or actions based on user data. Code implementing collaborative or content-based filtering algorithms, matrix factorization, or deep learning-based models for recommendations falls into this category."}, {"title": "General", "content": "Code that is versatile and applicable to multiple ML tasks without being exclusive to a specific one is labeled as General."}, {"title": "Incorrect DL Library or Framework Usage:", "content": "The generated code does not match the requested library or framework. For example, if the prompt asks for a TensorFlow implementation of a CNN, but the LLM generates the model using PyTorch instead, or if a user requests a NumPy-based neural network operation but the output code uses TensorFlow functions."}, {"title": "Shape and Dimension Mismatch:", "content": "The LLM produces code with incorrect tensor dimensions that do not follow the prompt specifications. For example, if the prompt requests a fully connected layer expecting an input of shape $(64, 128)$, but the generated code initializes it with an input shape of $(128, 64)$, leading to a mismatch in matrix operations."}, {"title": "Incorrect DL/ML Functionality:", "content": "The generated code does not implement the correct functionality as described in the prompt. For instance, if the prompt asks for a binary classification model using a sigmoid activation function, but the output code instead applies a softmax activation function intended for multi-class classification, altering the intended behavior."}, {"title": "Syntax Error:", "content": "Missing parenthesis, semicolon, or other syntax issues Straightforward syntactic mistakes such as un-closed quotes, unmatched braces, or misplaced punctuation prevent the code from compiling or running properly."}, {"title": "Silly Mistake:", "content": "Redundant conditions, unnecessary casting Simple but avoidable errors, such as repeating the same condition twice or performing extra type conversions with no purpose. While these do not always break the code, they reduce readability and hint at confusion in the model's reasoning."}, {"title": "Prompt-biased Code:", "content": "Code overly relies on examples from the prompt The LLM anchors too strongly to the examples provided in the prompt, resulting in a solution that works only for the specific inputs shown rather than generalizing the logic for broader applicability."}, {"title": "Missing Corner Cases:", "content": "Edge cases not handled The generated solution neglects special scenarios such as empty inputs, boundary values, or invalid parameters, leading to unreliable behavior outside of typical inputs."}, {"title": "Tensor Type and Value Edge Cases:", "content": "These bugs occur when operations fail due to unexpected tensor types or values. For example, using a tensor with $float32$ data type in a function that expects integers or encountering issues when dividing by zero in a tensor."}, {"title": "Shape and Dimension Edge Cases:", "content": "Bugs of this type happen when operations fail because of unexpected edge-case shapes. For example, trying to perform a convolution on a tensor with a batch size of 0 or a single dimension, such as $(1, 28, 28)$, when a shape like $(32, 28, 28)$ is expected."}, {"title": "Wrong Input Type:", "content": "Incorrect input type in function calls The code passes incompatible data types to functions or methods (e.g., providing a string instead of a list), which causes runtime failures or nonsensical outputs."}, {"title": "Tensor Shape Mismatch:", "content": "The generated code provides tensors with incorrect shapes to functions, leading to shape-related errors. For example, passing a 3D tensor of shape (batch, height, width) to a function that expects a 4D tensor of shape (batch, channels, height, width), causing a runtime error in deep learning frameworks like PyTorch or TensorFlow."}, {"title": "Incorrect ML/DL Function Library Arguments:", "content": "These occur when invalid arguments are passed to functions. For instance, using stride=-1 in a convolution function, which is not logically or mathematically valid."}, {"title": "Type Mismatch Problem:", "content": "The generated code uses tensors with incompatible data types in operations. For example, passing a tensor with data type $float32$ to a function that expects $int64$, or attempting to index a tensor with a floating-point value instead of an integer, leading to type-related execution failures."}, {"title": "Hallucinated Object:", "content": "Nonexistent or undefined objects used The LLM invents objects, classes, or modules that do not exist or have not been imported or defined. These errors result in runtime failures or developer confusion."}, {"title": "Missing or Undefined DL Modules:", "content": "This happens when a model, layer, or module that hasn't been properly defined or initialized is used. For example, attempting to forward-pass input through a neural network layer that hasn't been added to the model."}, {"title": "Incorrect Usage of DL Modules:", "content": "The generated code references deep learning modules, functions, or classes that do not exist or belong to the wrong framework. For example, calling $torch.nn.Dense()$ instead of $torch.nn.Linear()$, or attempting to use $tensorflow.layers.Conv2D$ instead of $tf.keras.layers.Conv2D$. These hallucinated module names cause import errors or incorrect function calls."}, {"title": "Wrong Attribute:", "content": "Incorrect/nonexistent attributes for objects or modules The LLM references valid objects but assigns them invalid or incorrect attributes. These subtle errors often result from misunderstandings of library APIs or typos in the generated code."}, {"title": "Wrong DL Module Import:", "content": "Bugs of this nature arise when modules are imported incorrectly. For example, importing jax functions when the rest of the code is written in PyTorch, leading to incompatibilities during execution."}, {"title": "Incorrect API Usage:", "content": "These bugs occur when a library API function is called incorrectly. For example, using the train() method instead of fit () for a Keras model or passing parameters in the wrong order to an optimizer."}, {"title": "Non-Prompted Consideration:", "content": "Non-requested features added The LLM includes functionality unrelated to the requirements, often due to extraneous training data or contextual noise. This bloats the code and complicates its scope."}, {"title": "Operation/Calculation Error:", "content": "Errors in arithmetic or logical operations The LLM makes errors in mathematical calculations or logical expressions, such as confusing addition with subtraction or mixing up operator precedence. These subtle mistakes produce incorrect results."}, {"title": "Data Type Casting Issues:", "content": "These bugs occur when tensors or variables are cast into incompatible data types. For instance, casting a $float32$ tensor into $int32$ without considering the loss of precision, which may disrupt training."}, {"title": "Shape and Dimension Error in Operations:", "content": "The generated code performs mathematical operations on tensors with incompatible shapes or dimensions, leading to incorrect computations or runtime failures. For example, attempting to add two tensors of shapes $(32, 64)$ and $(64, 32)$ without proper broadcasting, or performing a matrix multiplication between tensors with mismatched inner dimensions, such as $(4,3) \u00d7 (5, 4)$, causing a shape misalignment error."}, {"title": "Incorrect Algebraic Calculation:", "content": "These bugs refer to mathematical errors in computations. For instance, incorrectly normalizing data by dividing by the mean instead of the standard deviation, leading to improper scaling of input features."}, {"title": "Performance Issue:", "content": "This category includes inefficiencies in the generated code that impact runtime or resource usage. Examples include unnecessary nested loops, unoptimized algorithms, or excessive use of memory. While the code may produce correct results, its suboptimal implementation can make it impractical for large datasets or real-time applications. Performance issues often arise because the LLM generates a brute-force solution without understanding optimization principles."}, {"title": "DL Performance Issues:", "content": "These bugs refer to inefficiencies in implementation that degrade model performance. For instance, not using GPU acceleration for operations or improper batching strategies leads to high memory consumption and slow training."}, {"title": "Prompt Missing Information:", "content": "Incomplete or unclear prompts The bug arises due to insufficient detail or ambiguity in the input prompt, leading the LLM to make assumptions or guess certain details when generating the code. For example, if the prompt does not specify edge case handling or input constraints, the model may overlook these aspects entirely. This highlights the importance of crafting precise and comprehensive prompts when using LLMs for code generation."}, {"title": "Not Defining the Correct DL Library in the Prompt:", "content": "This occurs when the prompt or instructions fail to specify the appropriate library or framework. For example, a user asks a language model to generate PyTorch code but does not explicitly state this, leading to TensorFlow code generation instead."}, {"title": "Incorrect or Undefined Variable/Method References:"}]}