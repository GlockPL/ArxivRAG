{"title": "Shared Autonomy with IDA: Interventional Diffusion Assistance", "authors": ["Brandon J. McMahan", "Zhenghao Peng", "Bolei Zhou", "Jonathan C. Kao"], "abstract": "The rapid development of artificial intelligence (AI) has unearthed the potential to assist humans in controlling advanced technologies. Shared autonomy (SA) facilitates control by combining inputs from a human pilot and an AI copilot. In prior SA studies, the copilot is constantly active in determining the action played at each time step. This limits human autonomy and may have deleterious effects on performance. In general, the amount of helpful copilot assistance can vary greatly depending on the task dynamics. We therefore hypothesize that human autonomy and SA performance improve through dynamic and selective copilot intervention. To address this, we develop a goal-agnostic intervention assistance (IA) that dynamically shares control by having the copilot intervene only when the expected value of the copilot's action exceeds that of the human's action across all possible goals. We implement IA with a diffusion copilot (termed IDA) trained on expert demonstrations with goal masking. We prove a lower bound on the performance of IA that depends on pilot and copilot performance. Experiments with simulated human pilots show that IDA achieves higher performance than pilot-only and traditional SA control in variants of the Reacher environment and Lunar Lander. We then demonstrate that IDA achieves better control in Lunar Lander with human-in-the-loop experiments. Human participants report greater autonomy with IDA and prefer IDA over pilot-only and traditional SA control. We attribute the success of IDA to preserving human autonomy while simultaneously offering assistance to prevent the human pilot from entering universally bad states.", "sections": [{"title": "1 Introduction", "content": "As technology advances, humans continuously seek to operate more sophisticated and complex devices (Cascio and Montealegre, 2016). However, more sophisticated technologies typically involve complicated operational dynamics and high-dimensional control systems that restrict their use to narrowly defined environments and highly specialized operators (Schulman et al., 2018). While fully autonomous Al agents can be trained to perform these tasks, this approach has three key limitations. First, the user's goal is internalized and not easily deducible in most real-world environments. Second, removing the user from the control loop reduces their autonomy, potentially leading to poor performance and decreased engagement (Wilson and Daugherty, 2018a,b). Third, as the capabilities of AI advance, it is important to consider how to create technologies that assist and empower humans instead of replacing them (Wilson and Daugherty, 2018a,b).\nShared autonomy (SA) addresses these limitations by blending human (pilot) actions with assistive agent (copilot) actions in a closed-loop setting. Prior studies demonstrate SA can increase human task performance in robotic arm control (Laghi et al., 2018), drone control (Reddy et al., 2018), and navigation (Peng et al., 2023). A critical component of prior work is an empirically tuned control-sharing hyperparameter that trades off copilot assistance with human user autonomy (Reddy et al.,"}, {"title": "2 Related Work", "content": "We have a brief discussion on three related works (Du et al., 2021; Yoneda et al., 2023; Tan et al., 2022).\nAssistance via Empowerment (Du et al., 2021): It proposes a method that increases a human's ability to control the environment and mitigate the need to infer any goals. It defines an information"}, {"title": "3 Method", "content": "Our goal is to introduce a general-purpose intervention function that increases the performance and human autonomy of an SA system. The inputs to the intervention function are: (1) the goal-agnostic environment state, s, (2) a pilot action, ap, and (3) a copilot action, ac, illustrated in Figure 1b. The intervention function then plays either the pilot action (a\u2081 = ap) or copilot action (a1 = ac) in the environment. We define this intervention function and describe its implementation. We also prove a lower bound on the expected return associated with the policy using the intervention function proportional to the pilot and copilot performance.\nWe develop an intervention assistance called Interventional Diffusion Assistance (IDA). First, an expert policy is trained to maximize returns in the environment (Section 3.2). Second, the expert is used to perform policy rollouts and generate demonstrations. Goal information is removed from the demonstration data. We then train a diffusion copilot from these demonstrations (Section 3.3). Third, we define a trajectory-based intervention function that decides whether to play the human or copilot action (Section 3.4). All training was performed on a workstation with a single 3080Ti and took approximately 48 hours to complete all three steps for our tasks."}, {"title": "3.1 Notation and Problem Formulation.", "content": "We assume the environment can be modeled as an infinite-horizon Markov Decision Process (MDP) defined by the tuple M = (S, A, R, \u03b3, P, do). S is the space of all possible environment states, and A is the space of all possible actions. R : S\u00d7A \u2192 [Rmin, Rmax] is a scalar reward received for playing an action a \u2208 A in state s \u2208 S. P : S \u00d7 A \u00d7 S \u2192 [0, 1] are the transition dynamics for the environment, y is the discount factor, and do is the distribution of initial states. We define the state-action value function induced by policy \u03c0 to be $Q^\\pi (s,a) = E_{s_0=s,a_0=a,a_t\\sim\\pi(\\cdot|s_t),S_{t+1}\\sim p(\\cdot|s_t,a_t)} [\\sum_{t=0}^\\infty \\gamma^t r (s_t, a_t)]$, where \u03c0 : S \u00d7 A \u2192 [0, 1] is the action distribution conditioned on the state.\nWe additionally introduce the notion of a goal that encodes the task objective or intention of the human. We can decompose any state s = (s|\u011d) into a partial goal-agnostic state observation \u015d, which does not contain any goal specific information, and a goal \u011d\u2208 G*, where G* is the space of all possible goals. Then $Q^\\pi(s,a) = Q^\\pi((\\hat{s} | \\hat{g}), a) = Q^\\pi(\\hat{s}, a|\\hat{g})$ is the state-action value function under the goal-agnostic state s and goal \u011d.\nWe model the human behavior by a human pilot policy, \u03c0\u03c1, which generates human action ap ~ \u03c0\u03c1(8) according to the full state observation. The human observes and therefore has access to the goal, \u011d \u2208 G*. However, the goal is not assumed to be accessible to the copilot. In this paper, we assume the availability of an expert policy \u03c0e(ae|s), that observes the full state and solves the environment. We also assume that we can query the state-action value of the expert policy $Q_e (s, a)$, with which we use to evaluate the quality of an action a. We will train a copilot policy \u03c0c that generates actions based on the pilot action and the goal-agnostic state ac ~ \u03c0c(\u00b7|ap, 5). The ultimate goal of this paper is to derive a goal-agnostic intervention function T(\u0161, ac, ap) \u2208 {0,1} from the expert policy so that the SA system can achieve better performance than the pilot alone. The behavior policy that shares autonomy between the pilot and the copilot can be represented as $\u03c0_I = T\u03c0_e + (1 - T)\u03c0_\u03c1$."}, {"title": "3.2 Training an Expert Policy", "content": "We train a soft actor-critic (SAC) expert to solve the environment (Haarnoja et al., 2018) because it allows us to (1) later query $Q_e (s, a)$ for intervention and (2) generate demonstrations in the environment that can be used to train the copilot. In general, other methods of obtaining a Q-value estimator and for training a copilot are compatible with IA. We choose SAC for computational convenience. We parameterize our SAC model with a four-layer MLP with 256 units in each layer and the ReLU non-linearity. We use a learning rate of 3 \u00d7 10-4 and a replay buffer size of 106. The expert fully observes the environment including the goal, and is trained for 3 million time steps or until the environment is solved. We found that training with exploring starts (randomized initial state) produced a more robust Q function with better generalization abilities. Without exploring starts, the Qfunction performed poorly on unseen states, limiting the effectiveness of IDA."}, {"title": "3.3 Training a Diffusion Copilot", "content": "Following Yoneda et al. (2023), we trained a diffusion copilot $\u03c0_c(a_c|a_p, \\hat{s})$ using a denoising diffusion probabilistic model (DDPM) (Ho et al., 2020). For each environment, we collected 10 million state-action pairs from episodes using the SAC expert. All goal information was removed from this demonstration data. The copilot learned to denoise expert actions perturbed with Gaussian noise, conditioned on the goal-agnostic state s and the pilot's action ap.\nFormally, the forward diffusion process is a Markov chain that iteratively adds noise \u0454 ~ N(0, I) according to a noise schedule {A0, A1, ..., \u03b1\u03c4} to an expert action ao, via\n$a_t = \\sqrt{\\alpha_t} a_{t-1} + \\sqrt{1 - \\alpha_{t-1}}\\epsilon$.\nFollowing the forward diffusion process, the diffusion copilot is then trained to predict the noise added by the forward process by minimizing the following loss (Ho et al., 2020):\n$L_{DDPM} = E_{t,\\hat{s}\\sim\\tau,\\epsilon\\sim N(0,I)} [||\\epsilon - \\epsilon_\\theta(a_t, \\hat{s}, t)||^2]$,\nwhere e is a neural network parameterized by @ that approximates the noise e conditioned on the noisy action at, the goal-agnostic state \u0161, and the diffusion timestep t. 7 is the distribution of states in the demonstration data. The reverse diffusion process is modeled by a four-layer MLP to iteratively refine at toward ao."}, {"title": "3.4 Trajectory-based Goal-agnostic Value Intervention", "content": "IDA allows the copilot to intervene in pilot control when they take actions that are consistently bad for all possible goals. We therefore play the copilot's action ac instead of the pilot's action ap when the copilot's action has a higher expected return under the expert Q-value, that is,\n$Q^\\pi_e (s, a_c) > Q^\\pi_e (s, a_p)$.\nHowever, we can not directly assess Equation 3, since in a SA system the goal is internal to the pilot. Instead, we only have access to the goal agnostic state \u0161, such that, Equation 3 becomes,\n$Q^\\pi_e (\\hat{s}, a_c) > Q^\\pi_e (\\hat{s}, a_p)$.\nWe can define an intervention score $I(S_t, \\bar{a}_t|\\hat{g})$ which considers the value of $(s_t, a_t)$ under the assumption that \u011d \u2208 G* is the goal, where G* is the space of all possible goals,\n$I(S_t, a_t |\\hat{g}) = Q^\\pi_e (S_t, \\bar{a}_t|\\hat{g})$.\nBy marginalizing the difference in the intervention scores between the copilot and pilot over the entire goal space we can define a copilot advantage A(\u0161, ac, ap)\n$A(\\hat{s}, a_c, a_p) = \\frac{1}{\\mathcal{G}} \\int_{\\hat{g}\\in \\mathcal{G^*}} F (I(\\hat{s}, a_c |\\hat{g}) - I(\\hat{s}, a_p |\\hat{g})) d\\hat{g}$,\nwhere F is a function that maps the difference in intervention scores to {\u22121,+1} to ensure all possible goals are weighted equally. Here we choose $F(\\cdot) = sign(\\cdot)$. G is a normalization constant for the integral,\n$\\mathcal{G} = \\int_{\\hat{g}\\in \\mathcal{G^*}} max_{\\hat{s}, a_c, a_p} F (I(\\hat{s}, a_c |\\hat{g}) - I(\\hat{s}, a_p |\\hat{g})) d\\hat{g}$.\nWhen $F(\\cdot) = sign(\\cdot)$, then A(\u0161, ac, ap) \u2208 [\u22121, +1] is proportional to the fraction of the goal space over which the copilot action is superior to the pilot action. Also, when F is the sign function, the normalization constant reduces to $\\mathcal{G} = \\int_{\\hat{g}\\in \\mathcal{G^*}} d\\hat{g}$ and if the goal space is discrete then $\\mathcal{G} = |G^*|$ is the number of goals.\nWe adapt the value based intervention function proposed by Xue et al. (2023) to use for shared autonomy by allowing intervention to occur when A(\u0161, ac, ap) = 1. The copilot therefore intervenes when its action has a higher expected return compared to the pilot action for all possible goals. Formally, we let\n$T(\\hat{s}, a_c, a_p) = \\begin{cases} 1 & \\text{if } A(\\hat{s}, a_c, a_p) = 1 \\\\ 0 & \\text{otherwise} \\end{cases}$,\nwith intervention policy, $\u03c0_I = T\u03c0_e + (1 - T)\u03c0_\u03c1$. The process for performing Shared Autonomy (SA) with IA is highlighted in Algorithm 1. The copilot advantage is computed at every timestep. The behavioral (IA) policy is then determined by Equation 8."}, {"title": "3.5 Theoretical Guarantees on the Performance of IA", "content": "We prove that the return associated with IA is guaranteed to have the following safety and performance guarantees.\nTheorem 1. Let $J(\u03c0) = E_{s_0\\sim d_0, a_t\\sim \u03c0(\\cdot|s_t), S_{t+1}\\sim P(\\cdot|s_t,a_t)} [\\sum_{t=0}^\\infty \\gamma^t r (s_t, a_t)]$ be the expected discounted return of following a policy \u03c0. Then, the performance following the Interventional Assistance policy (or behavior policy) \u03c0\u2081 has the following guarantees:\n1. For a near-optimal pilot, ($Q^{\\pi_e}(s, a_p) \\approx max_{a^*} Q^{\\pi_e} (s, a^*)$), \u03c0\u2081 is lower bounded by \u03c0\u03c1:\n$J(\u03c0_I) \\geq J(\u03c0_\u03c1)$.\n2. For a low performing pilot, ($Q^\\pi_e(s, a_p) \\approx min_{a^*} Q^{\\pi_e} (s, a^*)$), \u03c0\u2081 is low bounded by \u03c0c:\n$J(\u03c0_I) \\geq J(\u03c0_c)$.\nThe proof of Theorem 1 is in Appendix A. Intuitively, the copilot will only intervene when the pilot attempts to play actions from the current state that have expected future returns less than that of the copilot's action across all possible goals. The IA policy therefore does not degrade performance of a high-performing pilot, and when the pilot is poor, guarantees performance no worse than the copilot."}, {"title": "4 Results", "content": "4.1 Experimental Setup\nBaselines. In the experiments that follow, we compared three different control methods. The first method is pilot-only control. The second method is copilot control, where the behavior policy is equal to the copilot policy \u03c0c(ac|ap, \u0161). Copilot control is the standard practice in SA (Reddy et al., 2018; Yoneda et al., 2023; Schaff and Walter, 2020; Jeon et al., 2020) as it allows a copilot to improve the human action before it is played in the environment. For copilot control, the action played is the action generated by the diffusion copilot using a forward diffusion ratio of y = 0.2, the setting that obtained the best control in (Yoneda et al., 2023). Our third method is IDA, which involves dynamically setting the behavior policy based on Equation 8.\nEnvironments. The first environment we use is Reacher, a 2D simulation environment that models a two-jointed robotic arm with inertial physics. In this environment, torques are applied to the two joints of the robotic arm to position the arm's fingertip at a randomly spawned goal position within the arm's plane of motion. The state of the environment is an 11 dimensional observation containing information about the position and velocities of the joints and goal location. Rewards are given for making smooth trajectories that move the fingertip close to the goal. In each episode, the arm's position is reset to a starting location and a new goal is sampled uniformly across the range of the arm's reach. Following previous works ((Reddy et al., 2018; Schaff and Walter, 2020; Yoneda et al., 2023; Tan et al., 2022)), we also use Lunar Lander, a 2D continuous control environment in which a rocket ship must be controlled with three thrusters to land at a desired goal location on the ground. We modify the environment as described in (Yoneda et al., 2023) to make the landing location spawn randomly at different locations along the ground. On each episode the landing zone is indicated by two flags. The states are 9 dimensional observations of the environment containing information about the rocket ship's position, angular velocity, and goal landing zone. We define the success rate as the fraction of episodes that ended with a successful landing between the landing zone flags. We define the crash rate as the fraction of episodes that terminated due to a crash or flying out of bounds.\nPilots. We use simulated surrogate pilots (Reacher, Lunar Lander) and eight human pilots (Lunar Lander) to benchmark the performance of pilot-only, copilot, and IDA control (see Appendix D for details about human participants). All human experiments were approved by the IRB and participants were compensated with a gift card. Surrogate control policies are designed to reflect some suboptimalities in human control policies. We consider noisy and laggy surrogate control policies. Surrogate policies are constructed by drawing actions from either an expert policy or a corrupted policy. We use a switch that controls if actions are drawn from the expert or corrupt policies. Actions are initially sampled from the expert policy. At every time step there is a probability of corruption being turned on. Once corruption is turned on, actions are sampled from the corrupt"}, {"title": "4.2 Reacher Experiments", "content": "We compared the performance of IDA to pilot-only and the copilot SA method of (Yoneda et al., 2023) in the Reacher environment with targets that could randomly appear anywhere (\u201cContinuous\" in Table 1). We introduce two additional goal spaces to probe the generalization abilities of IDA: \"Linear\" and \"Quadrant.\u201d In the linear goal space, goals spawned uniformly random along a horizontal line located 100cm in front of the arm. In the quadrant goal space, goals spawned uniformly random in the upper right quadrant of the workspace. To use IDA without making the goal space known to the advantage computation (Equation 6) we constructed a \u201cfaux goal\u201d space (FG) by assuming a uniform distribution over potential next positions as goals. We then estimated the copilot advantage through Monte-Carlo sampling. Furthermore, we examined IDA's performance when the goal space is unknown during Q function training by using a domain shift (DS) environment where goals appear randomly at one of five locations during training. In lieu of using humans, we employed laggy and noisy surrogate control policies to emulate imperfect pilots in the Reacher environment across these goal spaces.\nWe evaluated performance by quantifying hit rate, the number of targets acquired per minute. In the continuous goal space we found that IDA always achieved performance greater than or equal to pilot-only control and outperformed the copilot (Table 1). The expert control policy was optimal, and IDA therefore rarely intervened with a copilot action, leading to similar performance. We also found the laggy pilot performed relatively well because laggy actions do not significantly impair target reaches, although it may delay target acquisition. When the policy was noisy, IDA improved hit"}, {"title": "4.3 Lunar Lander", "content": "We next evaluated the performance of IDA in Lunar Lander with the noisy and laggy surrogate control policies. Consistent with Yoneda et al. (2023), we modified Lunar Lander so that the landing zone appears randomly in one of nine different locations. IDA always achieved performance greater than or equal to the performance of the pilot-only control policy (Table 2). Under the expert policy, which achieves 100% success rate and 0% crash rate, IDA does not degrade its performance, although copilot does. We observed that both copilot and IDA improved the performance of noisy and laggy surrogate control policies, with IDA consistently outperforming copilot in successful landings. Copilots and IDA also reduced crash rates for surrogate pilots. Additionally, we compared the performance of IDA to the penalty-based intervention approach proposed by Tan et al. (2022). Because the penalty-based intervention in Tan et al. (2022) used an MLP, we compare it to both IDA and IA with an MLP based copilot. We found that IA consistently achieved a higher rate of successful landings for both the noisy and laggy surrogate pilots than penalty based intervention for both copilot architectures. We further found that IA (MLP and IDA) yielded a lower crash rate than penalty-based intervention.\nNext we examined when and why copilot intervention occurred for surrogate pilots. Because these control policies were constructed by corrupting an expert's control policy, we were able to characterize intervention during periods of expert control versus corruption. We examined the distribution of copilot-human advantage scores, which measures the fraction of the goal space over which the copilot's action has a higher expected return than the pilot's action. For both the noisy and laggy pilots, we found the distribution of copilot advantage scores were different during expert actions vs corrupted actions (Figure 3a,b). When corrupted actions were played, there were a greater number of states where the copilot advantage was equal to 1, indicating the copilot's action had a greater expected return over the entire goal space. Consistent with this, we see that intervention was more common during periods of corruption."}, {"title": "4.4 Lunar Lander with Human-in-the-loop Control", "content": "Given IDA improved the performance of surrogate pilots in Lunar Lander, we performed experiments with eight human participants. Participants played Lunar Lander using pilot-only, copilot, or IDA. Participants used a Logitech game controller with two joysticks to control the rocket ship. The left joystick controlled the lateral thrusters and the right joystick controlled the main thruster. Each participant performed three sequences of 3 experimental blocks (pilot, copilot, IDA) for a total of 9 blocks (see Appendix D for experiment block details). Each block consisted of 30 trials (episodes). Participants were blind to what block they were playing. The game was rendered at 50 fps.\nHuman pilots experienced considerable difficulty playing Lunar Lander, successfully landing at the goal locations only 14% of the time (Table 3). Copilot control allowed humans participants to successfully land the rocket ship at the goal locations 68.2% of the time. However, the copilot also frequently prevented any landing, resulting in a timeout in 13.8% of trials (0% in pilot only and 0.1% in IDA). While this led to a lower crash rate (3.6%), it also reduced user autonomy and overall"}, {"title": "5 Conclusion and Discussion", "content": "Our primary contribution is Interventional Assistance (IA):\na hyperparameter-free and modular framework that plays a copilot action when it is better than the pilot action across all possible goals. We find that IA outperforms previous methods for intervention based shared autonomy proposed by Tan et al. (2022) as well as traditional copilot-only based methods for control sharing (Yoneda et al., 2023; Reddy et al., 2018). Furthermore, we empirically demonstrated IDA (IA with a Diffusion copilot) improves both objective task performance and subjective satisfaction with real human pilots in Lunar Lander (Figure 4). While prior SA systems may degrade pilot performance, particularly when copilots incorrectly infer the pilot's goal (Tan et al., 2022; Du et al., 2021), IA does not degrade human performance (Theorem 1) and often improves it.\nOne limitation of our approach is that we must train an autonomous agent in a simulated environment to obtain an expert Q function. However, this is not a fundamental requirement to learn an intervention function. A straightforward extension of the current work may use an offline dataset of expert demonstrations to train an ensemble of Q-networks (Chen et al., 2021). In general, while IA requires access to an expert Q function it makes no assumptions about how that Q function is obtained and we leave various methods of obtaining a Q function as directions for future work.\nAdditionaly, IDA demonstrated resilience across changes in goal space and can be easily adapted to real world settings where the goal space is unknown by construction of these faux goal spaces. Of course, in many settings task structure can be leveraged to further constrain the goal space and improve the assistance IA is able to provide. In these settings, another direction for future work is an implementation of IA that leverages a belief system to differentially weight candidate goals. Future work could potentially improve IA by removing unlikely goals from the advantage computation."}, {"title": "A Proof of Theorem 1", "content": "In this section, we will prove the theoretical guarantees on the performance of the interventional assistance (IA) in Theorem 1. We first introduce several useful lemmas.\nFirst, we introduce a lemma for later use by following the Theorem 3.2 in (Xue et al., 2023).\nLemma 1. For any behavior policy \u03c0\u2081 deduced by a copilot policy \u03c0c, a pilot policy \u03c0p, and an intervention function T(s, ap, ac), the state distribution discrepancy between \u03c0\u2081 and \u03c0\u0109 is bounded by the policy discrepancy and intervention rate:\n$|| T_{\u03c0_I} - T_{\u03c0_c} ||_1 \\le \\frac{\\beta}{(1-\\gamma)} + \\frac{\\gamma}{(1-\\gamma)^2} Es\\sim T_{\\pi_1} ||\\pi_c(\\cdot | s) - \\pi_p(\\cdot | s)||_1$,\nwhere $\\beta = \\frac{E_{s\\sim T_{\\pi_1}, a_c\\sim \\pi_c, a_p \\sim \\pi_p}||T(s, a_p, a_c)[\\pi_c(a_c|s) - \\pi_p(a_p|s)]||_1}{E_{s\\sim T_{\\pi_1}} ||\\pi_c(\\cdot | s) - \\pi_p(\\cdot | s)||_1}$ is the weighted expected intervention rate. $T_{\u03c0_I}$ and $T_{\u03c0_c}$ are the corresponding state visitation distributions following \u03c0\u2081 and \u03c0c, respectively.\nProof. We begin with the result of Theorem 3.2 in (Xue et al., 2023),\n$|| T_{\u03c0_I} - T_{\u03c0_c} ||_1 \\le \\frac{1}{(1-\\gamma)} E_{s\\sim T_{\\pi_1}} ||\\pi_I(\\cdot | s) - \\pi_c(\\cdot | s)||_1$ \n$=\\frac{1}{(1-\\gamma)} E_{s\\sim T_{\\pi_1}} ||T \\pi_e (a_c|s) + (1-T)\\pi_\u03c1(a_p|s) - \\pi_c (a_c|s)||_1 $\n$=\\frac{1}{(1-\\gamma)} E_{s\\sim T_{\\pi_1}, a_c\\sim \\pi_c, a_p\\sim \\pi_\u03c1} || (1 - T(s, a_p, a_c))[\u03c0_\u03c1(\\cdot |s) - \u03c0_c(\\cdot |s)] ||_1 $\n$\\le \\frac{1}{(1-\\gamma)} E_{s\\sim T_{\\pi_1}, a_c\\sim \\pi_c, a_p\\sim \\pi_\u03c1} (1 - \\beta) ||\u03c0_\u03c1(\\cdot |s) - \u03c0_c(\\cdot |s)] ||_1$.\nTo prove the theorem, the key lemma we use is the policy difference lemma in (Schulman et al., 2015) introduced below. It introduces one policy's advantage function computed on states and actions sampled from another policy's generated trajectory. Here, the advantage function is defined as $A^{\\pi'}(s, a) = Q^{\\pi'} (s, a) \u2013 V^{\\pi'} (s)$ and $V^{\\pi'} (s) = E_{a\\sim \u03c0'}Q(s, a)$ is the state value function. $J(\u03c0) = E_{s_0\\sim d_0, a_t\\sim \u03c0(\\cdot|s_t), S_{t+1}\\sim P(\\cdot|s_t,a_t)} [\\sum_{t=0}^\\infty \\gamma^t r (s_t, a_t)]$ is the expected return following policy \u03c0.\nLemma 2 (Policy difference lemma). Let \u03c0 and \u03c0' be two policies. The difference in expected returns can be represented as follows:\n$J(\u03c0) - J(\u03c0') = E_{s_t, a_t \\sim \u03c0} [ \\sum_{t=0}^{\\infty} \u03b3^t A^{\\pi'} (s_t, a_t)]$.\nWe introduce two lemmas that exploits the intervention function we proposed in Section 3.4.\nLemma 3. The Q value of the behavior action under the expert's Q estimate is greater or equal to the Q value of the pilot action.\n$E_{a\\sim \u03c0_I(\\cdot|s)} Q_e(s, a) \\ge E_{a_p\\sim \u03c0_\u03c1(\\cdot|s)}Q^\\pi_e(s, a_p)$\nProof. According to the intervention function T in Equation 8, intervention happens when the copilot advantage function $A(\\hat{s}, a_c, a_p) = 1$. If we consider F to be the sign function, then according to Equation 6, $A(\\hat{s}, a_c, a_p) = 1$ means for all goals we will always have $I(\\hat{s}, a_c|\\hat{g}) > I(\\hat{s}, a_p|\\hat{g}), \\forall \\hat{g}$. Recall $I(s_t, a_t |\\hat{g}) = Q^\\pi_e (\\hat{s}_t, \\bar{a}_t|\\hat{g})$, when intervention happens, we will have:\n$Q^\\pi_e (s_t, a_c |\\hat{g}) \\ge Q^\\pi_e (s_t, a_p|\\hat{g}), \\forall \\hat{g}$\nTherefore,\n$T(s, a_c, a_p) Q^\\pi_e (s_t, a_c |\\hat{g}) > T(s, a_c, a_p) Q^\\pi_e (s_t, a_p|\\hat{g}), \\forall \\hat{g}$,\nwhere the equality holds when intervention does not happen and T = 0.\nNow we introduce the expectation over the behavior policy.\n$E_{a\\sim \u03c0_I(\\cdot|s)} Q_e (s_t, a|\\hat{g}) = E_{a_p\\sim \u03c0_\u03c1} E_{a_o\\sim \\pi_e}TQ^\\pi_e(\\hat{s}_t, a_c|\\hat{g}) + (1 - T)Q^\\pi_e(\\hat{s}_t, a_p|\\hat{g})$\n$\\ge E_{a_p\\sim \u03c0_\u03c1} Q^\\pi_e (s_t, a_p|\\hat{g}), \\forall \\hat{g}$\nThe above equation holds for arbitrary \u011d. Therefore $E_{a\\sim \u03c0_I(\\cdot|s)} Q^{\\pi_e} (s, a) \\ge E_{a_p\\sim \u03c0_\u03c1} Q^{\\pi_e}(s, a_p)$."}, {"title": "A.1 The relationship between the behavior policy and the pilot policy", "content": "In this section, we will lower bound the performance of the IDA policy $J(\u03c0_I)$ by the performance of the pilot policy $J(\u03c0_\u03c1)$ under certain conditions.\nTheorem 2. Let $J(\u03c0) = E_{s_0\\sim d_0, a_t\\sim \u03c0(\\cdot|s_t), S_{t+1}\\sim P(\\cdot|s_t,a_t)} [\\sum_{t=0}^\\infty \\gamma^t r (s_t, a_t)", "frac{\\gamma}{(1-\\gamma)^2}": "frac{1}{2} E_{s\\sim d_{\\pi_I}} ||\u03c0_c(\\cdot | s) - \u03c0_\u03c1(\\cdot | s)||_1$,\nwherein $R = R_{max} - R_{min}$ is the range of the reward, $\\beta = \\frac{E_{s\\sim T_{\\pi_I}, a_c\\sim \\pi_c, a_p \\sim \\pi_p}||T(s, a_p, a_c)[\\pi_c(a_c|s) - \\pi_p(a_p|s)", "a)": "n$=\\sum_{t=0}^{\\infty} E_{s\\sim \u03c4_{\\pi_I}} (E_{a\\sim\\pi_I(\\cdot|s)} Q_e (s, a) \u2013 V_e(s)) $\n$=\\sum_{t=0}^{\\infty} E_{s\\sim \u03c4_{\\pi_I}} (E_{a\\sim\\pi_I(\\cdot"}]}