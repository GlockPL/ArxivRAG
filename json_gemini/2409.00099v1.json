{"title": "HARDWARE-EFFICIENT CUSTOMIZED KEYWORD SPOTTING WITH\nSPECTRAL-TEMPORAL GRAPH ATTENTIVE POOLING AND A HYBRID LOSS", "authors": ["Zhenyu Wang", "Shuyu Kong", "Li Wan", "Biqiao Zhang", "Yiteng Huang", "Mumin Jin", "Ming Sun", "Xin Lei", "Zhaojun Yang"], "abstract": "Most of the existing systems designed for keyword spotting\n(KWS) rely on a predefined set of keyword phrases. However,\nthe ability to recognize customized keywords is crucial for tai-\nloring interactions with intelligent devices. In this paper, we\npresent a novel framework for customized KWS. This frame-\nwork leverages the hardware-efficient LiCoNet architecture\nas the encoder, enhanced by a spectral-temporal pooling layer\nand a hybrid loss function to facilitate effective word embed-\nding learning. The experimental results on a substantial in-\nternal dataset have demonstrated the distinct advantages of\nthe proposed framework. LiCoNet performs at a similar level\n(1.98% FRR at 0.3 FAs/Hr) to the computationally intensive\nConformer, which requires 13x computational resources.", "sections": [{"title": "1. INTRODUCTION", "content": "A keyword spotting (KWS) system serves the purpose of de-\ntecting a predetermined keyword within a continuous real-\ntime audio stream. This capability is pivotal in facilitating\ninteractions between users and voice assistants. The intro-\nduction of a customized KWS system, which empowers users\nto define their own keywords, offers a substantial degree of\nflexibility and personalization in user experiences. However,\nthis customization also presents significant challenges, such\nas the need for a small KWS memory footprint, minimizing\nlatency, and handling user-defined keyword phrases that may\nnot align with the training data distribution.\nOne approach to address these challenges involves the use\nof Query-by-Example (QbyE) techniques [1]. In this context,\nthe KWS system utilizes audio samples of keywords provided\nby users to generate fixed-length embeddings. These em-\nbeddings are then employed to assess the similarity between\ntest samples and the enrolled keywords within the embedding\nspace, ultimately determining the presence of a keyword. The\nuppermost diagram in Figure 1 illustrates a broad framework\nfor QbyE KWS. Within this framework, there is a pooling\nlayer positioned after the encoder, which is responsible for\ncreating an information-rich embedding. This embedding is\nsubsequently supplied to a classifier to distinguish between\nsub-words or words.\nIn previous studies, transformers have found extensive use\nin encoder modeling because of their substantial modeling ca-\npabilities [2] [3]. Nevertheless, attention-based models are as-\nsociated with significant computational demands and impose\na high runtime memory burden when deployed on hardware.\nThis characteristic makes them unsuitable for an always-on\nKWS system. The linearized convolution network (LiCoNet)\nas introduced in [4] for KWS modeling, offers excellent hard-\nware efficiency while maintaining a high level of model ef-\nfectiveness.\nIn this study, we introduce a LiCoNet-based, hardware-\nefficient framework for customized KWS modeling. We em-\nploy spectro-temporal graph attentive pooling (GAP) [5] to\ngenerate informative embeddings. This pooling layer demon-\nstrates strong capability in comprehending the complex rela-\ntionships within spectral-temporal data. During the training\nphase, we formulate a hybrid loss function that combines el-\nements from the Additive Angular Margin (AAM) and Soft-\nTriplet losses, which are widely employed in tasks such as\nface recognition [6] and speaker recognition [7]. The hy-\nbrid loss is crafted to enhance the distinctiveness of words\nand phonemes while simultaneously reducing the variability\nin learned embeddings attributed to speakers. Our experimen-\ntal results, conducted on a substantial internal dataset, show-\ncase the advantages of our proposed framework, which fea-\ntures GAP and the hybrid loss, for customized KWS. No-\ntably, LiCoNet achieves performance levels similar to those\nof the computationally intensive Conformer, which requires\n13x computational resources."}, {"title": "2. METHODOLOGY", "content": "2.1. Encoder-decoder Architecture\nThe system architecture is illustrated in Figure 1. It adopts\nan encoder-decoder structure during the training phase. The\nencoder takes the acoustic feature of a word phrase as input\nand produces an embedding that is subsequently forwarded"}, {"title": "2.2. Feature Encoder", "content": "2.2.1. ECAPA_TDNN\nThe entire training and testing process shares similarities with\nthe speaker verification (SV) task. Consequently, we con-\nsider ECAPA_TDNN, a commonly used backbone model ar-\nchitecture for the SV task [8], as a potential choice for the\nacoustic feature encoder in this study. The ECAPA_TDNN\nmodel consists of a 1D convolution followed by three 1D SE-\nRes2Blocks, 1D convolution, attentive statistical pooling, and\na fully connected (FC) layer. After each layer within the SE-\nRes2Block, we apply non-linear ReLU activation and batch\nnormalization (BN). The embedding feature vectors are ex-\ntracted from the FC layer.\n2.2.2. Convolution-augmented Transformer (Conformer)\nThe Conformer architecture has proven its remarkable effec-\ntiveness within the sequence-to-sequence domain [9] and has\nachieved significant success in the realm of speech recogni-\ntion tasks [10] [11] [12]. This architecture seamlessly inte-\ngrates the capabilities of both convolutional and self-attention\nmechanisms, providing a flexible and exceptionally potent\nsolution for learning feature representations from sequential\ndata. Each Conformer block comprises four consecutive mod-\nules, including a feed-forward module, a self-attention mod-\nule, a convolution module, and a second feed-forward module\n[9]. This Conformer-based encoder demonstrates the ability\nto leverage position-specific local features, facilitated by the\nconvolution module, while simultaneously capturing content-\nbased global interactions through the self-attention module.\n2.2.3. Linearized Convolution Network (LicoNet)\nLiCoNet represents a hardware-efficient architecture specif-\nically designed for the KWS task, as detailed in [4]. This\narchitecture is carefully crafted as a streaming convolution"}, {"title": "2.3. Feature Aggregator", "content": "Pooling plays an essential role in neural architectures, serving\nthe purpose of distilling crucial insights from sequential data\nwhile preserving essential contextual details. In the context\nof our study, we investigate two distinct pooling strategies for\nword embedding learning.\nAttentive Statistic Pooling (ASP) ASP combines the strengths\nof both statistical pooling and attention mechanisms [13].\nAttention allows the model to dynamically weigh the impor-\ntance of different elements along the temporal dimension,\nenabling the extraction of salient features that are crucial for\nthe task at hand.\nSpectral-temporal Graph Attentive Pooling (GAP) GAP\nhas gained success in the field of speech and audio processing\n[5] [14]. It leverages the power of graph neural networks to\ncomprehend complex relationships within spectral-temporal\ndata. The spectral and temporal attention module comprises\nthree graph attention blocks, each housing the graph atten-\ntion network (GAT) and graph pooling. This configuration\nempowers the model to adapt the pooling procedure dynami-\ncally, facilitating the extraction of crucial features."}, {"title": "2.4. Loss Function", "content": "2.4.1. Additive Angular Margin (AAM)\nThe AAM loss is designed to enhance the discrimination\npower of neural networks by emphasizing the angular sep-\naration between class embeddings and is prevalent in the\ncontext of face recognition and feature embedding [6]. The\nloss function is defined as,\n$L_{aam} = -log{\\frac{e^{s \\cdot cos(\\theta_{y_i}+m)}}{\\sum_{j=1}^{C}e^{s \\cdot cos(\\theta_j)}}}$,\nwhere $\\theta_j$ is the angle between the feature $x \\in R^d$ and the\nweight $w_j \\in R^d$. $w_j$ denotes the j-th column of the weight\n$[W_1,\\dots, w_c] \\in R^{d \\times C}$ of the last fully-connected layer that\nmaps d-dimensional embeddings to the logits. C is the num-\nber of classes. s is a rescaling factor. An additive angular\nmargin m is applied for adjustment.\n2.4.2. SoftTriplet\nFor customized KWS tasks, it's important to note that the\ntraining and testing datasets have no overlap in data distribu-\ntion. This uniqueness necessitates that the model possesses\nstrong generalization capabilities. The QbyE system can"}, {"title": "2.4.3. Hybrid Loss", "content": "Phoneme Context Phonemes serve as the fundamental pho-\nnetic units that compose spoken words. Incorporating the\ncontext of phonemes into modeling offers a nuanced source of\ninformation for refining word embeddings. In our approach,\nas depicted in Fig. 1, we introduce a dedicated phoneme clas-\nsifier into the training framework. The phoneme loss is com-\nputed by aggregating the frame-level AAM loss, applied to\nphoneme labels, across all frames.\nSpeaker Variability Acoustic variations related to individ-\nual speakers, such as differences in pitch, tone, or pronunci-\nation, exert a significant influence on speech modeling. Ex-\nisting approaches in QbyE KWS often assume that the sys-\ntem user is the same as the enrolled speaker. To address and\ndisentangle speaker dependency within the application, we\nhave incorporated a reverse speaker loss into our methodol-\nogy, with the objective of learning speaker-independent em-\nbeddings (see Fig. 1). More specifically, we have devised an\nAAM-based reverse speaker loss, which is employed to maxi-\nmize the speaker classification loss through the application of\na gradient reversal layer (GRL) [16] during the training pro-\ncess. The parameter-free GRL functions as an identity trans-\nform during forward propagation but reverses gradients dur-\ning back-propagation, feeding them into the preceding layer.\nConsequently, our hybrid loss function is constructed as a\ncombination of word-level loss, phoneme-level loss, and the\nreverse speaker loss.\n$\\mathcal{L}(x, y) = L_{st}(x, y^w) - \\eta L_{aam}(x, y^p) + \\mu L_{aam}(x, y^s)$,\nwhere x is the acoustic feature vector, $y = (y^w, y^s, y^p)$ $y^w$\nis the word label, $y^s$ is the speaker label, $y^p \\in R^T$ is the\nphoneme label sequence, $\\eta$ and $\\mu$ are scaling factors."}, {"title": "3. EXPERIMENTS", "content": "3.1. Dataset\nWe use the Librispeech [17] dataset containing 960 hours of\nread English audiobooks sampled at 16 kHz along with tran-\nscriptions. We employ a pre-trained acoustic model for the\nforce-alignment to segment utterances into individual words.\nEach word-level segment is standardized to 2s long by clip-\nping or zero padding on both sides of the audio.\nWe use the internal aggregated and de-identified keyword\ndataset for evaluation. The positive data contains 275.7k ut-\ntterances from 629 speakers. The total duration of negative\ndata is up to 200 hours. We extract acoustic features using\n40-dimensional log Mel-filterbank energies computed over a\n25ms window every 10ms.\n3.2. Experimental Setup\nModel architecture We conduct the experiments on three\nmodel types: ECAPA_TDNN, Conformer, and LicoNet. We\nsetup ECAPA_TDNN with 128 channels in the convolution\nlayers and a 64 dimensional bottleneck in the SE-Block and\nattention module. The scale dimension s in the Res2Block is\n8. Conformer has two heads per multi-headed self-attention\nlayer with 128 input and output nodes [18]. The linear hid-\nden units have a dimensionality of 192, and the convolution\nmodule uses the kernel size of 7. We construct LiCoNet by\nstacking 5 LiCo-Blocks with the expansion factor of 6 and\nthe kernel size of 5 [4]. Table 2 presents the model size and"}, {"title": "4. RESULTS AND DISCUSSION", "content": "Model Performance Table 1 summarizes FRR of different\nmodels at 0.3 FAs/Hr. In the single word loss configuration,\nthe AAM loss significantly outperforms the CE loss across\ndifferent encoders. Specifically, AAM improves FRR by\n25.7% for ECAPA_TDNN, 31% for Conformer, and 26.1%\nfor LiCoNet. In the hybrid loss configuration featuring the\nword AAM loss, the inclusion of the reverse speaker loss\ngreatly decreases FRR, particularly for Conformer, result-\ning in a reduction of 40.9%. By incorporating the phoneme\nloss, we can notice additional enhancements. The efficacy\nof the hybrid loss underscores the value of using comple-\nmentary information from both auxiliary losses for KWS\nmodeling. As for the feature aggregator, GAP delivers fur-"}, {"title": "5. CONCLUSION", "content": "In this study, we introduce a hardware-efficient customized\nKWS system that is centered around the LiCoNet architecture\nand complemented by a spectral-temporal graph pooling layer\nand a hybrid loss function. The experimental results show-\ncase the advantages of our framework featuring GAP and the\nhybrid loss: LiCoNet achieves performance levels similar to\nthose of the computationally intensive Conformer."}]}