{"title": "Insuring Uninsurable Risks from AI: Government as Insurer of Last Resort", "authors": ["Cristian Trout"], "abstract": "Many experts believe that AI systems will sooner or later pose uninsurable risks, including existential risks. This creates an extreme judgment-proof problem: few if any parties can be held accountable ex post in the event of such a catastrophe. This paper proposes a novel solution: a government-provided, mandatory indemnification program for AI developers. The program uses risk-priced indemnity fees to induce socially optimal levels of care. Risk-estimates are determined by surveying experts, including indemnified developers. The Bayesian Truth Serum mechanism is employed to incent honest and effortful responses. Compared to alternatives, this approach arguably better leverages all private information, and provides a clearer signal to indemnified developers regarding what risks they must mitigate to lower their fees. It's recommended that collected fees be used to help fund the safety research developers need, employing a fund matching mechanism (Quadratic Financing) to induce an optimal supply of this public good. Under Quadratic Financing, safety research projects would compete for private contributions from developers, signaling how much each is to be supplemented with public funds.", "sections": [{"title": "1. Background", "content": "Many experts believe AI systems will, sooner or later, pose uninsurable risks, including existential risks (Grace et al., 2024; Bengio et al., 2024). If so, it will be impossible to hold accountable the parties liable for such harms (or their insurers).\nWeil (2024) proposes to solve this extreme judgment proof-problem by assigning punitive damages to harms that are correlated with uninsurable risks (where the correlation would be estimated by courts and juries). While of interest, this solution has several problems. First, is it's novelty: this would be an unprecedented application of punitive damages that may violate the Due Process Clause (2024, 40-44, 50-53), requiring a major doctrinal shift that would cut across all of tort law. Second, correlates of uninsurable risks might be difficult to find. Third, given the high uncertainty involved, correlation estimations by courts will likely be ad hoc, high variance, and fail to leverage all available information. Fourth and finally, punitive damages for correlated risks will send a very oblique and noisy signal to liable parties: its effectiveness at actually inducing greater care taken is doubtful. Liable parties might find powerful legal teams to be a safer investment than investments in safety.\nHistorically, the solution to uninsurable (albeit, non-existential risks) has been for government to step into its role as insurer of last resort (Moss, 2004), as seen in government provided reinsurance for"}, {"title": "2. A Risk-Priced Indemnification Program", "content": "An indemnification program is preferred over reinsurance as this removes the intermediary of insurers, allowing the government to directly manipulate incentives of risk-generating parties. Elsewhere I've argued that it's developers (e.g. OpenAI) who should be strictly and exclusively liable for said risks, largely based on their being least-cost avoiders (Trout, 2024).\nI'll call this proposed scheme the AI Disaster Insurance Program (AIDIP). Participation would be mandatory for developers of AI models trained over a certain effective compute\u00b2 threshold. The core of the program is a risk-priced indemnity fee that developers must pay per training run. It's recommended the fee be a function of effective compute.3"}, {"title": "Al Disaster Insurance\nProgram (AIDIP)", "content": "A government agency (such as the Department of Homeland Security) would estimate the disutility of various disaster scenarios, but risk-estimation would rely on a survey of public and private experts, including indemnified developers. The Bayesian Truth Serum (BTS) is employed to incentivize effortful and honest risk-estimations from respondents (Prelec, 2004). BTS rewards responses with high information scores \u2013 i.e. responses that are surprisingly common relative to respondents' predictions of how other respondents will respond. Scaling the BTS payout incents greater effort in information gathering. Honest reporting is a Bayes-Nash equilibrium under BTS \u2013 i.e. absent other incentives, a respondent will maximize their expected payout by reporting honestly if they believe a large enough majority of other respondents will also report honestly.\nA developer (who must pay the fee) obviously has a conflicting incentive to lie (underreport the risks), and can expect other developers to lie. This conflict of interest can be overcome by dramatically scaling BTS' payout or potentially removed entirely by silencing the developer's risk-estimation when their individualized fee is calculated. (This second option puts developers in a prisoner's dilemma: they could lower their fees by coordinating, but it's individually rational to defect, increasing fees for one's competitors.) An expectation of overwhelming honesty can be created by ensuring the vast majority of respondents are not developers but instead independent experts with no conflicts of interest. Where there are no conflicts of interest, relevant insurers and government agencies (such as the newly formed"}, {"title": "3. Quadratic Financing for Safety Research", "content": "It's recommended that revenue from the insurance program be used to fund AI related programs in the public's interest. One such program should aim to directly help developers shoulder the cost of the Safety Research (SR) they need to reduce their fees.\nBecause SR is a public good, developers face a coordination problem. Note that the coordination problem for supplying SR is greatly simplified by the liability and indemnification regime: instead of countless potential victims needing to coordinate, only developers need coordinate. To help them solve their coordination problem, ensure an optimal supply of SR, and contribute its own fair share of funds, it's recommended the government employ a fund-matching mechanism, Quadratic Finance (QF), designed to achieve all the above (Buterin et al., 2019).\nUnder QF, developers and partnered research institutions would propose various SR projects. Developers then choose to fund to whatever extent whichever projects it likes, knowing the government will top-up a project's total funds according to the QF formula. This top-up scales quadratically in the number of contributors to a project. As with BTS, QF will require basic defenses against collusion (multiple private contributors funding each other in order to receive a higher top-up) and fraud (one private contributor pretending to be multiple).\nProjects would essentially be competing for private contributions, signaling where to send public funds. Because of the agenda setting achieved by the liability and insurance program, projects would require minimal vetting. The market then determines which projects achieve that agenda most effectively."}, {"title": "4. Closing Remarks and Further Research", "content": "This paper proposes internalizing the negative externalities of uninsurable risks from AI through a risk-priced government insurance program (or from another angle, a Pigouvian tax). Revenue from this program then flows back to industry through an SR funding program.\nThe overall regime is market-based in that it has private actors compete to provide the most well- informed risk-estimations, and the most effective research projects to reduce said risks. This approach, it's argued, is cheaper, more responsive to new information, and more effective at protecting the public than alternative solutions to this extreme judgment-proof problem.\nWhile this paper focuses on elaborating the details of the market mechanisms core to this novel governance regime, it should be emphasized that the robust liability channeling, as discussed further in Trout (2024), is no less critical. Without channeling responsibility onto a few, well-resourced, and well-informed private actors these market mechanisms would likely be much less reliable (for appearing less legitimate, being costlier to administer and police, and returning a noisier signal).\nAs with quasi-regulation via insurance (2024), the goal here is not to fully substitute for regulation, but rather to produce effective risk-modeling and safety design for this emerging technology. Once available, well-calibrated regulation is much easier to develop.\nAs was done with the Terrorism Risk Insurance Program and Price-Anderson Act, this indemnification and funding program should include sunset mechanisms (an expiration date and or mechanisms for making private actors take ever greater responsibility for managing the risks of their activities, as this becomes possible). This would help ensure the program doesn't outlive its utility and is iterated on to meet changing needs.\nWhile confident in the theoretical soundness of its claims, the paper acknowledges the need for further empirical research into the effectiveness of BTS and QF. Available studies align with theoretical expectations (Weaver & Prelec, 2013; Pasquini, 2022), but more tests are needed."}]}