{"title": "LLM Embeddings Improve Test-time Adaptation to Tabular Y|X-Shifts", "authors": ["Yibo Zeng", "Jiashuo Liu", "Henry Lam", "Hongseok Namkoong"], "abstract": "For tabular datasets, the change in the relationship between the label and covariates (Y|X-shifts) is common due to missing variables (a.k.a. confounders). Since it is impossible to generalize to a completely new and unknown domain, we study models that are easy to adapt to the target domain even with few labeled examples. We focus on building more informative representations of tabular data that can mitigate Y|X-shifts, and propose to leverage the prior world knowledge in LLMs by serializing (write down) the tabular data to encode it. We find LLM embeddings alone provide inconsistent improvements in robustness, but models trained on them can be well adapted/finetuned to the target domain even using 32 labeled observations. Our finding is based on a comprehensive and systematic study consisting of 7650 source-target pairs and benchmark against 261,000 model configurations trained by 22 algorithms. Our observation holds when ablating the size of accessible target data and different adaptation strategies. The code is available at https://github.com/namkoong-lab/LLM-Tabular-Shifts.", "sections": [{"title": "1 Introduction", "content": "Predictive performance degrades when the distribution of target domain shifts from that of source (training) [4, 61, 26, 16, 1]. Distribution shifts can be categorized into shifts in the marginal distribution of covariates (X-shifts) or changes in the relationship between the label and covariates (Y|X-shifts). In computer vision, X-shifts are prevalent since high-quality human labels are consistent across different images [49, 43, 53]; in contrast, Y|X-shifts are prevalent in tabular data due to missing variables and hidden confounders. There is a large body of work addressing X-shifts due to its dominance in vision and language [37, 65, 64], yet the work on Y|X-shifts remain relatively limited [41].\nThe main challenge with addressing Y|X-shifts in tabular tasks is that the source data may provide little insight on the target distribution. Since it is impossible to generalize to a completely new and unknown domain [3, 51], we focus on leveraging few labeled target examples (on the order of 10 to 100) to address small Y|X-shifts that negatively impact model performance. Our goal is to build a feature representation (X) such that the difference between \\(E_{source}[Y|(X)]\\) and \\(E_{target}[Y|(X)]\\) are learnable even based on a few target data.\nUsing the wealth of world knowledge learned during pre-training, LLMs have the potential to build representations that mitigate the impact of confounders whose distribution changes across source and target. Specifically, we use a recent and advanced LLM encoder (e5-Mistral-7B-Instruct [58]) to featurize tabular data\u2014which we referred to as LLM embeddings\u2014and fit a shallow neural network (NN) on these embeddings for tabular prediction (Figure 1). In contrast to classical numerical"}, {"title": "2 Methods", "content": "In this section, we introduce a series of methods utilizing LLM embeddings for tabular prediction, as well as different choices to incorporate additional domain information, different model architectures, and target adaptation techniques using a small amount of labeled target samples. To the best of our knowledge, this work is the first to comprehensively explore the impact of LLM embeddings on tabular Y|X-shifts."}, {"title": "2.1 LLM Embeddings for Tabular Prediction", "content": "We first introduce how we transform tabular data into LLM embeddings, where the key idea is to serialize each sample into a natural language format that the LLM can process. There is a substantial body of research on serialization, including using another LLM to rewrite tabular data into natural language [27], adding descriptions of the classification task, training and test examples [27, 56], etc. Among these methods, Hegselmann et al. [27] demonstrate that using a straightforward text template with a task description consistently achieves the best empirical performance.\nUsing an income prediction problem to illustrate, consider a simple task description such as \"Classify whether US working adults' yearly income is above $50000 in 2018.\" along with a simple serialization template that enumerates all features in the format \"The [feature name] is [value]\". Adopting this serialization approach, we employ the encoder model e5-Mistral-7B-Instruct [58] to generate the LLM embedding. Formally, the encoder takes the serialization Serialize(X) of sample X as input and outputs its corresponding embedding \\(\\Phi(X)\\) as\n\\(X \\xrightarrow{\\text{serialization}} \\text{Serialize}(X) \\xrightarrow{e5\\text{-Mistral-7B-Instruct}} \\Phi(X)\\).\nSince e5-mistral-7b-instruct requires input data to be formatted in the following template:\nInstruct:\nQuery:\ndescription of the classification task n\ndescription of the data,\nwe provide task description in the \"Instruct\" part, and use the serilization template to format the tabular data in the \"Query\" part. An illustrative example is provided in Part A-D of Figure 1, with additional details available in Appendix A.1. Analyzing the impact of different LLM encoders, task descriptions, and serialization methods is left for future work."}, {"title": "2.2 Additional Domain Information", "content": "Another advantage of using LLM embeddings is their ability to incorporate additional domain information or prior knowledge, denoted by C. As demonstrated in Section 1, incorporating domain-specific information can help address Y|X-shifts and improve generalization performance in the target domain.\nIn this work, we propose a simple yet effective approach for integrating domain knowledge into tabular predictions. Rather than combining the domain information with serialized tabular data and generating a single LLM embedding, we generate separate LLM embeddings for the domain knowledge and the serialized tabular data, and then concatenate them together. The benefits of this approach are twofold: (a) although the domain information may contain significantly more words than the serialized tabular features, our concatenation method ensures a balanced 1:1 ratio"}, {"title": "2.3 Model Training and Target Adaptation", "content": "Model architecture For the backend model, we use a vanilla neural network (NN) classifier on both tabular features and LLM embeddings for tabular data classification. The NN is a simple feedforward neural network with several hidden layers, dropout layer, and ReLU activation functions.\nWhen adding additional domain information via an embedding layer, the same embedding is applied to all samples from the same domain. Since the output of e5-mistral-7b-instruct is a 4096-dimensional vector, we simply concatenate the LLM embeddings with the embeddings of the domain information. This concatenated vector is then passed through the hidden layers, dropout layer, and ReLU activation functions. For all NNs, the final linear layer an output dimension of 2, followed by a softmax layer for binary classification. During training, we use cross-entropy as the loss function, batch size as 128, and use the Adam optimizer. Detailed model architecture and hyper-parameters are provided in the Appendix A.3 and A.4, with a discussion on hyperparameter selection provided in Section 3.1.\nTarget Adaptation Even with the incorporation of LLM embeddings and domain information, our model may still experience Y|X-shifts. In practice, it is common to have a small set of samples from the target domain, which can be leveraged to better adapt the model to the target domain.\nFor each (source domain, target domain) pair, we begin by selecting the best training hyperparameter based on a validation criterion, which will be discussed in the Section 3.1. Using this model trained on the source domain, we explore four primary methods for target adaptation: in-context domain info, full-parameter fine-tuning, low-rank adaptation (LoRA), and prefix tuning for domain information.\nFor in-context domain info (F.1 of Figure 1), we keep the trained model frozen and only update"}, {"title": "3 Numerical Experiments", "content": "In this section, we conduct a thorough investigation of 7650 natural shift settings (source \u2192 target domain) in 3 tabular datasets over 261,000 model configurations and summarize the observations. Our findings highlight the potential of incorporating LLM embeddings to enhance the generalization ability in tabular data prediction tasks."}, {"title": "3.1 Testbed Setup", "content": "Dataset In this work, we use the ACS dataset [17] derived from the US-wide ACS PUMS data, where the goal is to predict various socioeconomic factors for individuals.\n\u2022 ACS Income: The goal is to predict whether an individual's income is above $50K based on individual features. We filter the dataset to only include individuals above 16 years old with usual working hours of at least 1 hour per week in the past year, and an income of at least $100.\n\u2022 ACS Mobility: The goal is to predict whether an individual has the same residential address as one year ago. We filter the dataset to only include individuals between the ages of 18 and 35, which increases the difficulty of the prediction task.\n\u2022 ACS Public Coverage (abbr. as ACS Pub. Cov): The goal is to predict whether an individual has public health insurance. We focus on low-income individuals who are not eligible for medicare by filtering the dataset to only include individuals under the age of 65 and with an income of less than $30,000.\nThe details of datasets are summarized in Table 1.\nShift Pattern Analysis Before benchmarking, we first analyze the shift patterns among the 2550 source target pairs in each dataset. Specifically, we utilize DISDE [13] (reference model as XGBoost) to decompose the performance degradation from the source domain to the target into two parts: (a) Y|X (concept)-shifts and (b) X (covariate)-shifts. By utilizing tailored shift patterns, we can conduct an in-depth analysis of where the strength of LLM embeddings lies. As shown in Figure 3, we sort all pairs according to the strength of Y|X-shifts, where we find that the natural spatial shifts are mainly comprised of Y|X-shifts. These findings broaden the scope of the analysis in WhyShift [41] by examining 7,650 shift pairs, a significant increase from the 169 pairs studied in the original work.\nAlgorithms As introduced in Section 2, we compare various methods that incorporate LLM embeddings into tabular data prediction, including different finetuning methods (no finetuning, finetuing on full parameters, and low rank adaptation (LoRA)) and different embeddings (w/ or w/o extra information). Besides, in order to fully compare the performances, we also include a wide range of learning strategies that perform on Tabular features, including basic models (LR, SVM, NN), tree ensembles (XGB, LGBM, GBM), robust methods (KL-DRO, x\u00b2-DRO, Wasserstein DRO, CVaR-DRO, and Unified-DRO), and in-context learning methods (TabPFN, GPT-40-mini). All methods are summarized in Table 2.\nExperiment Setup We conduct experiments with more than 261,000 model configurations on 2550 source target shift pairs in ACS Income, ACS Mobility, and ACS Pub. Cov datasets respectively (7650 settings in total). For each source\u2192target shift pairs, we randomly sample 20,000 labeled data from the source and target domain respectively, as the training and test dataset. We evaluate the model trained on the source domain, with or without target adaptation, and report the Macro F1 score on the testing dataset."}, {"title": "3.2 Primary Findings", "content": "We begin by presenting the key observations from our results. In addition to the metric (1.1) introduced in Section 1, we report performance metrics averaged over the source-target pairs.\nLLM embeddings improve performance, but when applied alone do not consistently outperform tree-ensembles. To better assess the generalization ability when incorporating LLM embeddings, we select the worst 500 settings (out of 2,550 total settings per dataset) based on the severity of Y|X-shifts and report the average Macro F1 Score in Figure 4. Each bar represents the average result across these worst 500 settings, characterized by the most severe YX-shifts. Thus, even a 1pp improvement is significant, as it implies consistent gains of about 1pp across each of the worst 500 settings.\nComparing \"NN on LLM embeddings\" to \"NN on tabular features\" (with the backbone model fixed as NN), we observe LLM embeddings significantly enhance generalization under distribution shifts on the ACS Income and ACS Mobility datasets, with average improvements of 2.4pp and 9.9pp. Notably, \u201cNN on LLM embeddings\" even outperforms XGBoost under strong distribution shifts on these datasets. This demonstrates the potential of LLM embeddings in tabular data prediction, where they can contribute to more generalizable models.\nA different trend is observed on the ACS Pub. Cov dataset, where the inclusion of LLM embeddings results in a performance drop for NN models. This suggests that simply incorporating LLM embeddings does not always resolve distribution shift issues; their effectiveness may vary across datasets, particularly depending on whether the LLM embeddings provide additional relevant information for the specific prediction task.\nA small number of target samples can make a big difference. While incorporating LLM embeddings doesn't always yield improvements, we find that even a small number of target samples can have a significant impact. As shown in Figure 4, finetuning the \"NN on LLM embeddings\" model with just 32 target samples significantly improves the average performance across the worst 500 settings for both ACS Mobility and ACS Pub. Cov. Similar trends are observed for other worst-K settings, as shown in Figure 6 (a)(b)(c) to come. Notably, for the ACS Pub. Cov dataset, where LLM embeddings alone provided no improvement, finetuning with only 32 target samples leads to a 5.4pp gain, even surpassing XGBoost by 2.2pp. This highlights the adaptability of LLM"}, {"title": "3.3 Auxiliary Findings", "content": "In addition to the primary findings, we have several other noteworthy observations.\n\"Right\" domain information matters. In Figure 6 (d)-(1), we study how additional domain information from Wikipedia, GPT-4, or 32 target samples (via in-context domain information, F.1 of Figure 1) impacts F1 scores in both non-adaptation and target adaptation scenario. We do NOT observe a consistent trend across all three datasets: additional domain information can either improve or reduce F1 scores. In the non-adaptation case, both LLM&wiki | NN and LLM&GPT | NN outperform LLM | NN in ACS Income (d)(g), but underperform in ACS Mobility (e)(h). When finetuning NNs, LLM&wiki | NN (finetuning) and LLM & GPT | NN (finetuning) beat LLM | NN (finetuning) in ACS Pub. Cov (f)(i), but underperform in ACSMobility. Moreover, LLM& Target Samples | NN shows comparable performance with LLM | NN (finetuning) in ACSIncome (j), but underperform LLM | NN (finetuning) in ACS Mobility and ACS Pub. Cov (k)(1), and even underperform LLM | NN that has no target adaptation in ACS Mobility (k). Our results indicates that \"right\" domain information can indeed improve tabular data classification under YX shifts, yet identifying the best domain information requires non-trivial engineering efforts, which we leave for future work.\nFor simplicity, we only compare LLM embeddings (without extra domain information) to LLM&Wiki in the sequel, as LLM&Wiki and LLM&GPT exhibit similar trends.\nSpecific finetuning approaches are less crucial than expected. Given the large number of model parameters and limited labeled target samples, one might expect parameter-efficient methods like Low-Rank Adaptation (LoRA) and Prefix Tuning (F.3 of Figure 1) to offer a clear advantage. However, as shown in 7, all methods significantly outperform the non-finetuned version when using"}, {"title": "4 Discussion based on theoretical insights", "content": "We take a brief examination of the theoretical insights that may lie behind our empirical findings. While standard generalization bounds are vacuous for neural networks, we nevertheless find that theoretical results from domain adaptation provide a useful starting point for understanding why finetuning LLM embeddings with small target samples can result in superior generalization performance under substantial Y|X shifts, particularly in comparison to tabular features.\nLet \\(\\Phi(X)\\) denote a feature map and Y a binary label. We let P and Q be the source and target distributions. We consider a model class H of VC dimension d. For any model \\(h \\in H\\), we use \\(\\epsilon_P(h) := E_P[I(h(\\Phi(X)) \\neq Y)]\\) to denote the expected 0-1 loss on the source domain and \\(\\epsilon_P(h)\\) its empirical counterpart based on m i.i.d. samples from P. We define \\(\\epsilon_Q(h)\\) and \\(\\hat{\\epsilon}_Q(h)\\) on the target.\nAlthough in practice we finetune on the target, we use a mixture problem as a rough approximation. Suppose that we have \\((1 - \\beta)m\\) i.i.d. samples from source domain P and \\((\\beta m)\\) i.i.d. samples from target domain Q. Let \\(h_{\\alpha,\\beta}\\) be the minimizer of the \\(\\alpha\\)-weighted empirical error\n\\(h_{\\alpha,\\beta} := \\underset{h \\in H}{\\operatorname{argmin}} \\{ \\alpha \\hat{\\epsilon}_{(1-\\beta)m}^P(h) + (1 - \\alpha) \\hat{\\epsilon}_{(\\beta)m}^Q(h) \\}.\\)\nThe following classical result from Ben-David et al. [5, Theorem 3] bounds the generalization error on the target.\nProposition 1. For any \\(\\delta \\in (0,1)\\), with probability at least 1 \u2212 \u03b4,\n\\(\\epsilon_Q(h_{\\alpha,\\beta}) - \\underset{h \\in H}{\\inf} \\epsilon_Q(h) \\leq 4 \\sqrt{\\frac{\\alpha^2}{1-\\beta} + \\frac{(1-\\alpha)^2}{\\beta}} + \\sqrt{\\frac{2 d \\log(2(m+1)) + 2\\log \\frac{8}{\\delta}}{m}}\n+ 2(1 - \\alpha) d_{\\triangle H}(P_X, Q_X)+2(1 - \\alpha) \\underset{h \\in H}{\\inf} \\{ \\epsilon_P(h) + \\epsilon_Q(h) \\},\\) (4.1)\n\\(\\text{Y|X-shifts}\\)\nwhere \\(d_{\\triangle H}(\\cdot,\\cdot)\\) denotes the H\u0394H-distance between two (marginal) distributions."}, {"title": "A Model Training Details", "content": "In this section, we outline the serialization scheme, the generation of additional domain information, our model architecture, and the hyperparameters used for training and target adaptation."}, {"title": "A.1 Serialization Scheme", "content": "As discussed in Section 2.1, serializing a row of tabular data, such as X, requires two components: a task description and a description of the data."}, {"title": "A.2 Additional domain information", "content": "As shown in Section 2.2, we study three sources of domain information: Wikipedia, GPT-4, and labeled target samples. As we use the e5-mistral-7b-instruct to generate an LLM embedding for the domain information, we need to specify both the \"Instruct\" and \"Query\" components.\nFor the \"Instruct\" part, we apply the same task description as outlined in Section A.1. For the \"Query\" part, we utilize various descriptions of the additional domain information:"}, {"title": "A.3 Model Architecture", "content": "We detail the baselines used in our paper.\nFully-connected Neural Networks (NN) Given the varying input dimensions for Tabular features, LLM embeddings, and LLM embeddings with additional domain information, we employ three neural networks with similar architectures. We then train these networks and conduct target adaptation using Empirical Risk Minimization (ERM).\nFor all three datasets using Tabular features, we use a hidden layer with output dimension being hidden layer dim as a hyperparameter, a dropuput layer with dropout ratio being a hyperparameter, ReLu activation. We then have another hidden layer with input and output dimension both being hidden layer dim, and then softmax layer with dimension 2 as the output.\nFor datasets using Tabular features, the network includes a hidden layer where the output dimension is set by the hyperparameter hidden layer dim. It is followed by a dropout layer (dropout ratio as a hyperparameter), ReLU activation, another hidden layer the input and output dimensions are both equal to hidden layer dim. The network concludes with a softmax layer with an output dimension of 2.\nFor datasets using only LLM embeddings, the input dimension is 4096, which is the output dimension of e5-mistral-7b-instruct. The network consists of three hidden layers with fixed dimensions, where the input and output dimensions are (4096, 1024), (1024, 256), and (256, 128), respectively. Each layer uses ReLU activation. Next, there's a hidden layer with an input dimension of 128 and an output dimension set by the hidden layer dim hyperparameter. This is followed by a dropout layer (with dropout ratio as a hyperparameter), ReLU activation, and another hidden layer where both the input and output dimensions are hidden layer dim. A ReLU activation follows this final hidden layer, and the network concludes with a softmax layer that outputs a dimension of 2.\nFor datasets using LLM embeddings concatenated with additional domain information, the input dimension is 8192. We slightly update the first three hidden layers, with input and out dimensions being (8192, 2048), (2048, 512), and (512, 128), respectively. All other neural network structure and hyperparameters are the same as NNs with LLM embeddings only.\nWhen applying low-rank adaptation (LoRA) for target adaptation, we introduce a low-rank adaptation layer to each linear layer by incorporating two smaller matrices, A and B, both with a rank of 16. Specifically, matrix A has dimensions corresponding to the input size and the rank, while matrix B has dimensions corresponding to the rank and the output size. Matrix A is initialized with a mean of 0 and a standard deviation of 0.02, whereas matrix B is initialized with zeros. These matrices are then multiplied together and added to the original weight matrix."}, {"title": "A.4 Hyperparameters for Training and Target Adaptation", "content": "For each algorithm, we maintain a grid of candidate hyperparameters as shown in Tables 3 and 4. and perform hyperparameter selection as described in Section 3.1. When the number of hyperparameter configurations exceeds 200, we randomly select 200 configurations to reduce computational cost and maintain fairness in the comparison across all algorithms."}, {"title": "B Additional Figures", "content": "Shift Patterns The shift patterns of all datasets are shown in Figure 9."}]}