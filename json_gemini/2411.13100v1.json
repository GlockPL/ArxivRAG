{"title": "Song Form-aware Full-Song Text-to-Lyrics Generation with Multi-Level Granularity Syllable Count Control", "authors": ["Yunkee Chae", "Eunsik Shin", "Hwang Suntae", "Seungryeol Paik", "Kyogu Lee"], "abstract": "Lyrics generation presents unique challenges, particularly in achieving precise syllable control while adhering to song form structures such as verses and choruses. Conventional line-by-line approaches often lead to unnatural phrasing, underscoring the need for more granular syllable management. We propose a framework for lyrics generation that enables multi-level syllable control at the word, phrase, line, and paragraph levels, aware of song form. Our approach generates complete lyrics conditioned on input text and song form, ensuring alignment with specified syllable constraints. Generated lyrics samples are available at: https://tinyurl.com/lyrics9999", "sections": [{"title": "1 Introduction", "content": "The field of lyrics information processing (Watanabe and Goto, 2020) presents unique challenges that extend beyond traditional text generation, as lyrics must align with both song form structures (such as verses, choruses, and bridges) and specific syllabic constraints. While natural language generation models have shown promise in generating coherent text, applying these models to lyrics introduces additional complexities due to the need for musical and rhythmic alignment.\nSeveral studies have made notable progress in aligning lyrics with melodies through melody-aligned or midi-aligned generation methods (Watanabe et al., 2018; Lu et al., 2019; Sheng et al., 2021). These efforts have been valuable in addressing melody-lyrics synchronization. However, the scarcity of large-scale audio-lyrics aligned datasets still poses a challenge for these approaches, limiting their generalizability across diverse musical forms (Meseguer-Brocal et al., 2019; Yu et al., 2021; Durand et al., 2023). Given this data limitation, we concentrate on a crucial aspect of melody-lyrics alignment: the syllable count, which directly"}, {"title": "2 Multi-Level Granularity Lyrics Generation", "content": "Our system supports flexible generation across different levels of textual granularity (words, phrases. lines, and paragraphs), ensuring precise syllable control at each level. The model is conditioned on input text, song structure (e.g., verses, choruses), and syllable constraints, generating lyrics that adhere to these requirements.\nToken Structure. To facilitate the generation of lyrics aligned with specific song forms, we define a structured token system. Song form tokens (e.g., <VERSE>, <CHORUS>) are used to signify transitions between different parts of the song. Following the song form token, we introduce a syllable count token, <SYL : s>, where s denotes the designated syllable count for the section, inspired by (Kim et al., 2023a) and (Guo et al., 2022). These tokens help the model generate text at varying levels of granularity while maintaining syllabic precision.\nInspired by the ideas presented in (Bai et al., 2021), we implement distinct end tokens for each level of granularity in our model. Our model utilizes a structured approach with specific tokens to generate lyrics at different levels of granularity. For generating a complete paragraph, the <GEN_P>\nfollowed by the syllable count token and paragraph, and concluded with an <END_P>. To simulate more detailed generation control, the granularity choice extends to generating entire lines or lines segmented into phrases and words. Deciding to generate an entire line involves adding a <GEN_L> token at the beginning and concluding with an <END_L> token, similar to paragraph generation.\nAlternatively, suppose a line is to be created with phrases and words. In that case, <GEN_L_NW> tokens are used along with respective syllable count tokens to indicate the start of such detailed line generation. Each segment, whether phrases or words, begins with <GEN_N> or <GEN_W> tokens, respectively, followed by the corresponding syllable count token and text, and concluded with <END_NW> tokens. Upon completing a line detailed with phrases and words, an <END_L> token signifies its end. An illustration of the generation plan is provided in Figure 2.\nSemantic Embedding for Input Text. Our model also incorporates semantic embeddings for the input text, allowing it to generate contextually relevant lyrics. Given the challenge of lacking datasets that pair text with lyrics, we devise an alternative training strategy that does not rely on such paired data. We employ the SentenceTransformer (Reimers and Gurevych, 2019) model to capture the semantic content of lyrics, a method inspired by the approach of Watanabe et al. (Watanabe and Goto, 2023), who used Stable Diffusion (Rombach et al., 2022) and pre-trained Vision Transformer (Dosovitskiy et al., 2020) for semantic extraction in lyrics. We use semantic embeddings of full lyrics as a substitute condition for input text during training, enabling the model to generate relevant lyrics by analyzing the semantic condition of any input text provided during inference.\nInference. During the inference, we initiate the process with the provided semantic content embedding from the input text and proceed to decode each token in an autoregressive manner. At each step of decoding we provide the model with special tokens that align with our predefined generation plan, rather than having the model predict these special tokens \u2013 such as song form, syllable counts, and generation directives tokens (<GEN_*>). We continue to generate lyrics corresponding to a"}, {"title": "3 Results", "content": "In this section, we present our experimental results, including preliminary findings from recent large LLM models, results from our generation model, and the song form consistency of lyrics generated by our model. Details of experimental setups and evaluation metrics are provided in Appendix C.", "sections": [{"title": "3.1 Preliminary Experiment", "content": "Large language models (LLMs) like ChatGPT (Ouyang et al., 2022) have shown remarkable performance in natural language generation recently. However, these models often struggle with accurately counting syllables (Sun et al., 2023). To evaluate the performance of LLMs in generating content conditioned on syllable count and structure, we conducted a simple experiment. In this test, we task an LLM to create a verse by providing input text along with the number of lines and the syllable count for each line. A trial is considered a failure if the model does not produce the correct number of paragraphs or lines within five attempts. The success rates are approximately 38% for ChatGPT 3.5 and 57% for ChatGPT 4.0. The results from Table 1 indicate that while LLMs are capable of generating lyrics that are contextually appropriate to the input text, they perform poorly in matching the required syllable count."}, {"title": "3.2 Lyrics Generation", "content": "In Table 2, we present the performance of our models across various settings. For each training method, we report on models trained from scratch as well as those initialized with large-scale pre-trained GPT-2 weights.\nFront models serve as the baselines, where all generation plans are established at the start, before the <LYR_START> token, allowing for the production of complete lyrics without the guidance of generation plans during the generation process. In other words, we concatenate the special tokens and use them as a prompt to generate the output lyrics. Back introduces the generation plan directly to the model during the actual generation process, as illustrated in Figure 2. Both merges the strategies of Front and Back. It begins by listing all generation plans before <LYR_START>, then proceeds to generate lyrics using the Back approach. -S and -P at the end of the name of each model refer to whether the models are trained from scratch or from a pre-trained model, respectively.\nAll models carry some risk of failure during the generation process. For instance, if a model continues generating text without producing the <END_*> token, it will run until reaching the maximum length limit, resulting in a failed attempt. Additionally, in the Front method, there is a risk that the generated output may not align with the planned structure. The failure rates for each model are detailed in the Appendix D.\nTable 2 presents the metric values calculated from the lyrics successfully generated by all models. Only samples that are successfully generated by all models within 10 attempts, for each input condition in the evaluation set, are included. One-tailed Wilcoxon signed-rank tests are performed on a per-generation plan basis to test directional hypotheses. The BERT-Score (BERT-S) (Zhang et al., 2019) is calculated between the input text and the generated lyrics. The average BERT-S between the input text and the original lyrics in the test set is 0.799, which serves as an upper bound.\nTable 2 shows that pre-trained models outperform those trained from scratch in terms of perplexity (PPL), with the Front and Both models showing"}, {"title": "3.3 Song Form Consistency Evaluation", "content": "To verify whether the model differently generates lyrics for different song form types, we measure BERT-S and normalized Levenshtein distance (NLD) between different paragraphs. Note that we do not measure metrics within the same paragraph. Specifically, for each song containing multiple paragraphs, we calculate metrics between distinct paragraphs, excluding comparisons with themselves (e.g., verse1&verse2, verse1&chorus1). This process is performed across all songs, and the average is taken for each case over the total occurrences.\nFigure 3 presents the results of generated lyrics by the Back-S model in a square matrix format. As these comparisons are between paragraphs within the same song, we can observe that the BERT-S generally exceeded 0.7. Notably, the BERT-S between the same song form types (diagonal values) are higher than those between different types. For NLD, We can observe results with a similar tendency to those of the BERT-S, with diagonal values being smaller than other values, which denotes the fewer steps to edit the string from one to another. Notably, verses show less similarity among the diagonal values for both metrics. This is expected as verses contain more varied lyrics contents compared to other song form types, but they still exhibit more similarity than the metrics between different song form types. The results suggest that the model tends to produce lyrics that are similar within the same song form and distinct across different forms."}]}, {"title": "4 Conclusion", "content": "Our study presents a sophisticated approach to lyrics generation and infilling, incorporating multi-level granularity for syllable control and effective song form management conditioned on an arbitrary input text. This offers enhanced flexibility in the lyrics generation process. For future work, we plan to extend the capabilities of our models by integrating more flexible control such as genre using genre tags or rhyme control, which we anticipate will further refine the adaptability and creative potential of automated lyrics composition in the music industry."}, {"title": "Limitations", "content": "Although our framework is capable of a comprehensive generation of lyrics under various conditions, there are some limitations:\nGeneration Failures. There is a small probability of generation failure, where the model may not produce the <END_*> token, resulting in an infinite sequence until it reaches the model's maximum length. While generating multiple times can mitigate this issue, it does not guarantee success.\nDependence on Song Form Tagged Datasets. Our framework requires a dataset with pre-labeled song forms (e.g., verse, chorus). As a result, the model cannot generate song forms that are not present in the training data, and it cannot be trained on unlabeled, lyrics-only datasets.\nContextual Understanding. Since our model is not based on large language models (LLMs) and relies solely on Sentence Transformer for capturing meaning, it may struggle to fully grasp the detailed context of the input text, potentially resulting in less nuanced lyric generation."}, {"title": "Ethical Considerations", "content": "While our model generates new lyrics based on patterns learned from training data, there is a risk of inadvertently generating content that closely resembles copyrighted lyrics, posing potential copyright infringement issues. Future work should incorporate mechanisms to detect and prevent the generation of lyrics that are too similar to existing copyrighted material.\nAdditionally, the training data used for our model may contain cultural, gender, or racial biases, which could be reflected in the generated lyrics. Without careful filtering and bias mitigation techniques, the model might inadvertently reinforce harmful stereotypes. It's important to use diverse, representative, and inclusive datasets, and to explore methods for detecting and correcting bias in lyrics generation."}, {"title": "A Related Work", "content": "Many studies on lyrics generation have attempted to use various features to generate lyrics that match naturally with the melody. Lyrics generation inherits intricate syntactic and semantic challenges from text generation. Notably, within this domain, there has been a significant increase in the utilization of neural networks. Recurrent Neural Networks (RNN) (Zaremba et al., 2015), in particular, have garnered acknowledgment for their effectiveness in language modeling and capturing sequential dependencies.\nThe procedure of converting melodies to expressive lyrics has also been studied, as seen in (Chen and Lerch, 2020; Ma et al., 2021; Sheng et al., 2021; Lu et al., 2019; Tian et al., 2023; Duan et al., 2024; Zhang et al., 2024). These models adeptly generate lyrics in coherence with the provided melody. For instance, (Chen and Lerch, 2020) used SeqGANs (Yu et al., 2017) for this task, and (Ma et al., 2021) proposed AI Lyricists, leveraging MIDI files for lyrics generation. Tailored specifically for melody-to-lyrics generation, models such as (Sheng et al., 2021) utilize architectures like MASS (Song et al., 2019). Though these models address the problem in various ways, they suffer from a lack of melody-lyrics-aligned dataset. Approaches used in (Tian et al., 2023) employ hierarchical frameworks successfully generating lyrics aligned with outlines, trained in an unsupervised manner to address this issue. However, they still do not consider song form or multi-granularity conditioned generation.\nResearches focused on syllable count control have employed rule-based and learning-based approaches to produce lyrics that adhere to constraints and convey meaning. Poetry generation is a notable task in this field. For example, studies such as (Zhang and Lapata, 2014; Yi et al., 2017, 2018) have employed RNN and LSTM for Chinese poetry generation. Additionally, (Lu et al., 2019) proposes a method that interprets the alignment between lyrics and melodies as representations of syllable structures. This method uses a multi-channel sequence-to-sequence model that considers both phrasal structures and semantics."}, {"title": "B Multi-Level Granularity Lyrics Infilling", "content": "During lyrics generation, users may need to adjust lyrics to ensure syllabic accuracy or to amend"}, {"title": "C Experiment Setup", "content": "For the training and evaluation of our models, we used the Genius Song Lyrics dataset\u00b2, which comprises approximately 5.1 million multilingual song lyrics, with around 3.3 million lyrics in English. From the dataset, we selectively extracted lyrics featuring explicit song form annotations with each paragraph, aligning with a pre-defined set of song forms: verse, chorus, pre-chorus, post-chorus, and bridge."}, {"title": "C.2 Data Preprocessing", "content": "In preprocessing, we implemented several steps to ensure data quality and relevance. This involved the elimination of special characters and the conversion of numerals to words using num2words library\u00b3. Additionally, we discarded non-lyrics content (e.g., 'guitar solo' or repetition notations like x2, x3) and lyrics exceeding 500 words. To ensure content suitability and avoid offensive language, we assessed the toxicity of the lyrics using the Detoxify\u2074 library (Hanu and Unitary team, 2020), excluding any lyrics with a toxicity score above 0.5 to create a more positive and inclusive dataset. The syllable counts for each word in the dataset were calculated using the Syllables library. The dataset was randomly divided into training, validation, and evaluation subsets. This partition resulted in approximately 340K for training, 18K for validation and 10K for evaluation."}, {"title": "C.3 Data Preparation", "content": "Since there is no plan-lyrics paired dataset suitable for our task, we synthesized a generation plan using lyrics samples to simulate the generation plan. In crafting our training samples for the generation task, as illustrated in Figure 2, we employ a strategy of dynamically generating random plans for data sample, guided by a pre-order tree traversal technique, inspired by (Donahue et al., 2020). Specifically, we begin by constructing a tree representing the hierarchical structure of the training example, encompassing paragraphs, lines, and words. Through pre-order tree traversal, each subtree is randomly selected with probability of p, ensuring"}, {"title": "C.4 Training and Inference Setup", "content": "In all our experiments, we train the GPT-2 model (Radford et al., 2019) for 10 epochs, with a batch size of 8 on a single GeForce 3090Ti GPU. We experiment with both training from scratch and using a pre-trained model. For the pre-trained model, we utilize the weights available from HuggingFace (Wolf et al., 2019). The learning rate is set to 5e-5, with 500 warm-up steps, and training is performed"}, {"title": "C.5 Evaluation Metrics", "content": "In assessing the performance of our models, we employ a suite of metrics designed to capture various dimensions of model effectiveness, from linguistic predictability to syllabic accuracy and semantic fidelity.\nTest set perplexity (PPL) measures the ability of model to predict the text within the test set, providing insights into the model's linguistic prediction capabilities. We calculated PPL specifically for text tokens, excluding special tokens. This approach ensures that the focus remains on the model's ability to predict natural language text, rather than the special tokens that are explicitly provided during the inference process. In all experimental tables, the PPL values represent the trimmed mean rather than the simple mean, to account for the influence of extreme outliers. Specifically, the PPL values shown in the tables are the averages calculated after excluding the top 1% of the highest values. However, the Wilcoxon test was conducted on the complete dataset.\nSyllable count distance (SCD) quantifies the dis-"}, {"title": "Results", "content": "crepancy between the expected set of syllable counts $S = \\{$s_1,..., s_n\\}$ and the syllable counts of the generated text $\\hat{S} = \\{\\hat{s_1}, ..., \\hat{s_n}\\}$. As defined in (Kim et al., 2023b), the calculation is given by:\n$$SCD(S, \\hat{S}) = \\frac{1}{2n} \\sum_{i=1}^{n} (\\frac{|s_i - \\hat{s_i}|}{s_i} + \\frac{|\\hat{s_i}-s_i|}{\\hat{s_i}}) \\text{(1)}$$\nSyllable count error rate (SCErr) measures how frequently the model generates text with incorrect syllable counts, representing a stricter assessment than SCD by penalizing all inaccuracies equally.\nBERT-Score (BERT-S) (Zhang et al., 2019) assess semantic coherence between a reference and generated lyrics, by using pre-trained BERT embedding. In the generation task, BERT-S is computed by comparing the input text with the lyrics produced by the model, whereas in the infilling task, it is calculated by comparing the masked segments with the lyrics that have been infilled in.\nNormalized Levenshtein Distance (NLD) is also adopted to assess the consistency of the song form. Levenshtein distance, also known as edit distance, measures how many character operations are needed to convert one string into another. These operations can be deleting, inserting, or replacing. Since the length of strings varies for each song and song form, we use normalized Levenshtein distance (NLD) (Tashima et al., 2018) defined as follows:\n$$NLD(p_1, p_2) = \\frac{LD(p_1, p_2)}{\\text{max}\\{\\Lambda(p_1), \\Lambda(p_2)\\}} \\text{(2)}$$\nwhere LD(p1, p2) is the Levenshtein distance and the function $\\Lambda$ calculates the length of the given string.\nOur evaluation extends across different levels of granularity, applying each metric to assess both the overall performance (Full) and the performance"}, {"title": "D Success Rate of Generation Model", "content": "As our models have to predict <END_*> token properly, there is an possibility of failure. To be specific, the Front-P model has a 37% chance of success on the first try, with the probability of success increasing to 90% within 10 attempts. The Front-S model performs better, with an 89% chance of success on the first attempt and a 99% chance within 10 tries. The remaining models perform well, achieving an average success rate of 97% on the first attempt and about 99.8% within 10 attempts. Overall, Table 2 presents generation results from 6,678 samples in total."}, {"title": "E Results: Lyrics Infilling", "content": "Table 3 presents the results for the infilling task. We used Back-S and Back-P as baselines, which are the same models trained in Table 2, referring to them as LM-S and LM-P respectively. These LM models consider only the past context and generate solely the masked lyrics. If tokens are not masked, they are directly provided to the model. Since LMs require semantic embedding as input, we provided the model with embeddings of the masked lyrics, replacing the masked segments with"}, {"title": "E.1 Infilling Model Variants", "content": "Table 4 explores the impact of changes in the infilling approach. We conducted the following ablation study in comparison with the ILM-S model.\nFirstly, we experimented with a model trained using a uniform <MASK> token for all conditioning, instead of using specific tokens like <INF_*><SYL:s> for different granularity levels and syllable count annotations (denoted as same mask in Table 4). In this case, syllable condition tokens are only given directly in ahead of each infilling step.\nThe results indicated a significant decline in the performance of syllable count generation at both paragraph and word levels, as well as a reduction in the overall BERT-S and PPL.\nNext, we trained a model without assigning song form tokens after the <START> token (denoted as no songform). While this change did not significantly affect syllable count performance, it also resulted in a notable decrease in the BERT-S and PPL, similar to the same mask case.\nLastly, we combined both conditions-using the same mask and omitting song form tokens\u2013in training a model. This configuration led to a deterioration across nearly all metrics, confirming the importance of specific masking and song form annotations in lyrics infilling tasks."}, {"title": "F Preliminary Experiment Setting", "content": "In this experiment, we used both gpt-3.5-turbo and gpt-4-turbo-preview to generate lyrics based on specific syllable constraints. We conducted simple task than main generation experiment, i.e., just generate verse paragraph, because full song template is so complex so chatgpt almost always failed to generate the lyrics within the form. Prompt for Generation. The following system role instruction was used to generate lyrics:\nYou are a lyricist, tasked with creating\na song verse. This template outlines the\ninput text and the lyric's structure, break-\ning it down into multiple lines. Each line\nis given with syllable count constraints.\nThe user input was provided in this format:\nInput text: \"A summary of the verse in\nlyrics from the dataset\""}, {"title": "Prompt for Re-Generation", "content": "If the model produced an incorrect number of lines, the following prompt was used to ask the model to regenerate the lyrics:\nGenerated lyrics do not match the line\ncounts in the given condition.\nGiven line count is:\n<expected_line_count>,\nbut generated line count is:\n<generated_line_count>.\nPlease regenerate the lyrics.\nThis allowed for refinement of the lyrics through\nup to five regeneration attempts."}]}