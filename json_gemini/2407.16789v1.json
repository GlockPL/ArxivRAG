{"title": "What Matters in Range View 3D Object Detection", "authors": ["Benjamin Wilson", "Nicholas Autio Mitchell", "Jhony Kaesemodel Pontes", "James Hays"], "abstract": "Lidar-based perception pipelines rely on 3D object detection models\nto interpret complex scenes. While multiple representations for lidar exist, the\nrange-view is enticing since it losslessly encodes the entire lidar sensor output.\nIn this work, we achieve state-of-the-art amongst range-view 3D object detection\nmodels without using multiple techniques proposed in past range-view literature.\nWe explore range-view 3D object detection across two modern datasets with\nsubstantially different properties: Argoverse 2 and Waymo Open. Our investigation\nreveals key insights: (1) input feature dimensionality significantly influences the\noverall performance, (2) surprisingly, employing a classification loss grounded in\n3D spatial proximity works as well or better compared to more elaborate IoU-based\nlosses, and (3) addressing non-uniform lidar density via a straightforward range\nsubsampling technique outperforms existing multi-resolution, range-conditioned\nnetworks. Our experiments reveal that techniques proposed in recent range-view\nliterature are not needed to achieve state-of-the-art performance. Combining the\nabove findings, we establish a new state-of-the-art model for range-view 3D object\ndetection - improving AP by 2.2% on the Waymo Open dataset while maintaining\na runtime of 10 Hz. We establish the first range-view model on the Argoverse 2\ndataset and outperform strong voxel-based baselines. All models are multi-class and\nopen-source. Code is available at https://github.com/benjaminrwilson/\nrange-view-3d-detection.", "sections": [{"title": "1 Introduction", "content": "Lidar-based 3D object detection enhances how machines perceive and navigate their environment\nenabling accurate tracking, motion forecasting, and planning. Lidar data can be represented in\nvarious forms such as unordered points, 3D voxel grids, bird's-eye view projections, and range-view representations. Each representation differs in terms of its sparsity and how it encodes spatial\nrelationships between points. Point-based representations preserve all information but compromise\non the efficient computation of spatial relationships between points. Voxel-based and bird's-eye view\nrepresentations suffer from information loss and sparsity, yet they maintain efficient neighborhood\ncomputation. The range-view representation preserves the data losslessly and densely in the \u201cnative\"\nview of the sensor, but 2D neighborhoods in such an encoding can span enormous 3D distances and\nobjects exhibit scale variance because of the perspective viewpoint.\nThe field of range-view-based 3D object detection is relatively less explored than alternative repre-\nsentations. Currently, the research community focuses on bird's-eye view or voxel-based methods.\nThis is partially due to the performance gap between these models and range-view-based models.\nHowever, we speculate that the lack of open-source range-view models prevents researchers from\neasily experimenting and innovating within this setting [1, 2, 3, 4]. Our research reveals several\nunexpected discoveries, including: (1) input feature dimensionality significantly influences overall\nperformance in 3D object detection in the range-view by increasing network expressivity to capture\nobject discontinuities and scale variance, (2) a straightforward classification loss based on 3D spatial\nproximity yields superior generalization across datasets compared to intricate 3D Intersection over\nUnion (IoU)-based losses, (3) simple range subsampling outperforms complex, range-specific net-\nwork designs, and (4) range-view 3D object detection can be competitive across multiple datasets.\nSurprisingly, we find without including certain contributions from prior work, we end up with a\nstraightforward, 3D object detection model that pushes state-of-the-art amongst range-view models\non both the Argoverse 2 and Waymo Open datasets.\nWhile the goal of this work is not to set a new state-of-the-art in 3D object detection, we show that\nrange-view methods are still competitive without the need for \u201cbells and whistles\u201d such as model\nensembling, time aggregation, or single-category models.\nOur contributions are outlined as follows:\n1. Analysis of What Matters. We provide a detailed analysis on design decisions in range-\nview 3D object detection. Our analysis shows that four key choices impact downstream\nperformance and runtime \u2013 input feature dimensionality, 3D input encoding, 3D classification\nsupervision, and range-based subsampling. When these design decisions are optimized, we\narrive at a relatively simple range view architecture that is surprisingly competitive with\nstrong baseline methods of any representation.\n2. Simple, Novel Modules. We propose a straightforward classification loss grounded in 3D\nspatial proximity yields superior generalization across datasets compared to more complex\nIoU-based losses [5] which generalizes surprisingly well across the Argoverse 2 and Waymo\nOpen datasets. We introduce a simple, range subsampling technique which outperforms\nmulti-resolution, range-conditioned network heads [5].\n3. High Performance without Complexity. Surprisingly, without range-specific network\ndesigns [5, 2], or IoU prediction [5], we demonstrate that range-view based models are\ncompetitive with strong voxel-based baselines on the Argoverse 2 dataset and establish a\nnew state-of-the-art amongst range-view based 3D object detection models on the Waymo\nOpen dataset - improving L1 mAP by 2.2% while running at 10 Hz.\n4. Open Source, Multi-class, Portable. Prior range-view methods have not provided open-\nsource implementations [1, 2], used single-class detection designs [5], or have been written\nin non-mainstream deep learning frameworks [5]. We provide multi-class, open-source\nmodels written in Pytorch on the Argoverse 2 [6] and Waymo Open [7] datasets with\nopen-source implementations to facilitate range-view-based, 3D object detection research at\nhttps://github.com/f64df84f/4f8d64d3-f087-40ff-a7a7-ae676787ebe5."}, {"title": "2 Related Work", "content": "Point-based Methods. Point-based methods aim to use the full point cloud without projecting\nto a lower dimensional space [8, 9, 10]. PointNet [8] and Deep Sets [11] introduced permutation\ninvariant networks which enabled direct point cloud processing, which eliminates reliance on the\nrelative ordering of points. PointNet++ [9] further improved on previous point-based methods, but\nstill remained restricted to the structured domain of indoor object detection. While PointRCNN [10]\nextends methods to the urban autonomous driving domain, point-based methods do not scale well\nwith the number of points and size of scenes, which makes them unsuitable for real-time, low-latency\nsafety critical applications.\nGrid-based Projections. Grid-based projection methods first discretize the world into either 2D\nBird's Eye View [12, 13, 14, 15] or 3D Voxel [16, 17, 18, 19] grid cells, subsequently placing 3D"}, {"title": "3 Range-view 3D Object Detection", "content": "We begin by describing the range view and its features, then we outline the task of 3D object detection\nand its specific formulation in the range view. Next, we describe a common architecture for a\nrange-view-based 3D object detection model.\nInputs: Range View Features. The range view is a dense grid of features captured by a lidar\nsensor. In Fig. 1, we illustrate the range view, its 3D representation along with an explicit geometric\ncorrespondence of a building between views and the location of the physical lidar sensor which\ncaptures the visualized 3D data. Fig. 2 shows the range-view features used for our Waymo Open\nexperiments."}, {"title": "3D Object Detection", "content": "Given range view features shown in Fig. 2, we seek to map over 50,000 3D\npoints to a much smaller set of objects in 3D and describe their location, size, and orientation. Given\nthe difficulty of this problem, approaches usually \u201canchor\u201d their predictions on a set of 3D \"query\"\npoints (3D points which are pixels in range view features). For each 3D point stored as features\nin a range image, we predict a 3D cuboid which describes a 3D object. Fig. 3 illustrates the range\nview input (top), the smaller set of object points (middle), and the regressed object cuboids prior\nto non-maximum suppression (bottom). Importantly, each object may contain thousands of salient\npoints which are transformed into 3D object proposals.\n3D Input Encoding. Feature extraction on 2D grids is a natural candidate for 2D-based convolu-\ntional architectures. However, unlike 2D architectures, a range-view architecture must reason in 3D\nspace. Prior literature has shown that learning this mapping is surprisingly difficult [25], which moti-\nvates the use of 3D encodings. 3D encodings incorporate explicit 3D information when processing\nfeatures in the 2D neighborhood of a range image. For example, the Meta-Kernel from RangeDet [5]\nweights range-view features by a relative Cartesian encoding. We include a cross-dataset analysis\nof two different methods from prior literature in our experiments. Surprisingly, we find that not all\nmethods yield improvement against our baseline encoding.\nScaling Input Feature Dimensionality. The backbone stage performs feature extraction for range-\nview features which have been processed by the 3D input encoding. We adopt a strong baseline\narchitecture, Deep Layer Aggregation (DLA), for all of our experiments following prior work [5].\nWe find that scaling feature dimensionality significantly impacts performance across datasets.\nDynamic 3D Centerness. We propose a dynamic 3D classification supervision method motivated\nby VarifocalNet [26]. During training, we compute classification targets by computing the spatial\nproximity between an object proposal and its assigned ground truth cuboid via a Gaussian likelihood:\n$C_{3D}(d_i, g_i) = exp(-\\frac{r_i^2}{\\sigma^2})$, where $r_i = ||d_{xyz}^i - g_{xyz}^i ||^2$,\nwhere $d_{xyz}^i$ and $g_{xyz}^i$ are the coordinates of the assigned object proposal and its corresponding ground\ntruth annotation, and $\\sigma$ controls the width of the Dynamic 3D Centerness. We adopt $\\sigma = 0.75$ for all"}, {"title": "Experiments", "content": "In this section, we present our experiments on\ntwo modern, challenging datasets for range-\nview-based 3D object detection. Our experi-\nments illustrate which decisions matter when designing a performant range-view-based detection\nmodel."}, {"title": "4.1 Datasets", "content": "Argoverse 2. The dataset contains 1,000 sequences of synchronized, multi-modal data. The\ndataset contains 750 training sequences, 150 validation sequences, and 150 testing sequences. In\nour experiments, we use the top lidar sensor to construct an 1800 \u00d7 32 range image. The official\nArgoverse 2 3D object detection evaluation contains 26 categories evaluated at a 150 m range with the\nfollowing metrics: average precision (AP), average translation (ATE), scaling (ASE), and orientation\n(AOE) errors, and a composite detection score (CDS). AP is a VOC-style computation with a true\npositive defined at 3D Euclidean distance averaged over 0.5 m, 1.0 m, 2.0 m, and 4.0 m. We outline\nadditional information in supplemental material and refer readers to Wilson et al. [6] for further\ndetails.\nWaymo Open. The Waymo Open dataset [7] contains three evaluation categories, Vehicle, Pedes-\ntrian, and Cyclist, evaluated at a maximum range of approximately 80 m. We use the training\nsplit (798 logs) and the validation split (202 logs) in our experiments. The dataset contains one\nmedium-range and four near-range lidar sensors. The medium range lidar sensor is distributed as a\nsingle, dense range image. We utilize the 2650 \u00d7 64 range image from the medium-range lidar for\nall experiments. We evaluate our Waymo experiments using 3D Average Precision (AP). Following\nRangeDet [5], we report Level-1 (L1) results. Additional details can be found in supplemental\nmaterial and the original paper [7]."}, {"title": "4.2 Experiments", "content": "In this section, we report our experimental results. Full details on our baseline model can be found in\nthe supplemental material.\nInput Feature Dimensionality. We find that\ninput feature dimensionality plays a large role\nin classification and localization performance.\nWe explore scaling feature dimensionality of\nthe high resolution pathway in both the back-\nbone, the classification, and regression heads.\nIn Table 1, we find that performance on Argov-\nerse 2 consistently improves when doubling the\nbackbone feature dimensionality. Additionally,\nthe error metrics continue to decrease despite\nincreasingly challenging true positives being de-\ntected. We report a similar trend in Table 2\non Waymo Open. We suspect that the perfor-\nmance improvements are largely due to learning\ndifficult variances found in the range-view (e.g.\nscale variance and large changes in depth). We\nchoose an input feature dimensionality of 256\nand 128 for our state-of-the-art comparison on\nArgoverse 2 and Waymo Open, respectively, to\nbalance performance and runtime.\n3D Input Encoding. Close proximity of pix-\nels in a range image does not guarantee that they\nare close in Euclidean distance. Previous literature has explored incorporating explicit 3D information\nto better retain geometric information into the input encodings. We re-implement two of these meth-\nods: the Meta-Kernel from RangeDet [5] and the Range-aware Kernel (RAK) from RangePerception\n[2]. We find that the Meta-Kernel outperforms our baseline by 2% mAP and 1.4% CDS. Additionally,\nthe average translation, scale, and orientation errors are reduced. Unexpectedly, we are unable to"}, {"title": "Dynamic 3D Classification Supervision", "content": "We compare two different strategies across the Argoverse\n2 and Waymo datasets. In Fig. 5, we illustrate the difference between the two different methods,\nDynamic IOUBEV and our proposed Dynamic 3D Centerness. On Argoverse 2, our Dynamic 3D\ncenterness outperforms IOUBEV by by 2.7% mAP and 1.9% CDS. We speculate that this performance\nimprovement occurs because Argoverse 2 contains many small objects e.g. bollards, construction\ncones, and construction barrels, which receive low classification scores due to translation error\nunder IoU-based metrics. Dynamic 3D Centerness also incurs less translation, scale, and orientation\nerrors than competing rankings. The optimal ranking strategy remains less evident for the Waymo\ndataset. The official Waymo evaluation uses vehicle, pedestrian, and cyclist as their object evaluation\ncategories, which are larger on average than many of the smaller categories in Argoverse 2. We find\nthat Dynamic IOUBEV and Dynamic 3D Centerness perform nearly identically at 59.9% AP; however,\nfor smaller objects such as pedestrian, Dynamic 3D Centerness outperforms IOUBEY by 0.95. Full\ntables are in the supplemental material. Our experiments suggest that IoU prediction is unnecessary\nfor strong performance on either dataset. We adopt Dynamic 3D Centerness for our state-of-the-art\ncomparison since it performs well on both datasets."}, {"title": "Range-based Sampling", "content": "We compare our baseline architecture (single resolution prediction head\nwith no sub-sampling) against the Range-conditioned Pyramid (RCP) [5] and our Range Subsampling\n(RSS) approach. In Table 3, we surprisingly find that RCP performs worse than our baseline model\nin overall performance; however, it yields a modest improvement in runtime by reducing the total\nnumber of object proposals processed via NMS. By sampling object proposals as a post-processing\nstep, our method, RSS, outperforms both RCP and the baseline with no additional parameters or\nnetwork complexity, and comparable runtime. Similarly, we examine the impact of range-based\nsampling across the Waymo Open dataset in Table 4. We find that the Range-conditioned pyramid\nyields marginal performance against our baseline despite having 2.8x the number of parameters in\nthe network heads. We speculate that feature-pyramid-network (FPN) approaches are not as effective\nin the range-view since objects cannot be normalized in the manner proposed by the original FPN\n[27]. We will adopt RSS in our state-of-the-art comparison."}, {"title": "Comparison against State-of-the-Art", "content": "Combining a scaled input feature dimensionality with\nDynamic 3D Centerness and Range-based sampling yields a model which is competitive with existing\nvoxel-based methods on the Argoverse 2 dataset, and state-of-the-art amongst Range-view models\non Waymo Open. In Table 5, we report our mAP over the 26 categories in Argoverse 2. We\noutperform VoxelNext [28], the strongest voxel-based model, by 0.9% mAP. In Table 6, we show our\nL1 AP against a variety of different models on Waymo Open. Our method outperforms all existing\nrange-view models while also being multi-class."}, {"title": "5 Conclusion", "content": "In this paper, we examine a diverse set of considerations when designing a range-view 3D object\ndetection model. Surprisingly, we find that not all contributions from past literature yield meaningful\nperformance improvements. We propose a straightforward dynamic 3D centerness technique which\nperforms well across datasets, and a simple sub-sampling technique to improve range-view model\nruntimes. These techniques allow us to establish the first range-view method on Argoverse 2, which is\ncompetitive with voxel-based methods, and a new state-of-the-art amongst range-view models on\nWaymo Open. Our results demonstrate that simple methods are at least as effective than recently-\nproposed techniques, and that range-view models are a promising avenue for future research."}, {"title": "6 Supplementary Material", "content": "Our supplementary materials covers the following: background on 3D object detection in the range\nview, additional quantitative results, qualitiative results, dataset details, and implementation details\nfor our models."}, {"title": "7 Range View Representation", "content": "The range view representation, also known as a range image, is a 2D grid containing the spherical\ncoordinates of an observed point with respect to the lidar laser's original reference frame. We define\na range image as:\nr\u2252 {(\u00c7ij, Oij, rij) : 1 \u2264 i \u2264 H; 1 \u2264 j \u2264 W},\nwhere (pij, dij, rij) are the inclination, azimuth, and range, and H, W are the height and width\nof the image. Importantly, the cells of a range image are not limited to containing only spherical\ncoordinates. They may also contain auxillary sensor information such as a lidar's intensity."}, {"title": "7.1 3D Object Detection", "content": "Given a range image r, we construct a set of 3D object proposals which are ranked by a confidence\nscore. Each proposal consists of a proposed location, size, orientation, and category. Let D represent\nare predictions from a network.\nD {di\u2208 R8}K=1, where K \u2282 N,\ndi {xego, yego, zego, li, Wi, hi, Oi, Ci}\nwhere xego, yego, zego are the coordinates of the object in the ego-vehicle reference frame, li, wi, hi\nare the length, width, and height of the object, \u03b8\u2081 is the counter-clockwise rotation about the vertical\naxis, and ci is the object likelihood. Similarly, we define the ground truth cuboids as:\nG{gi\u2208 R8}M=1, where M\u2282 N,\ngi {xego, yego, zego, li, Wi, hi, Oi, qi },\nwhere qi is a continuous value computed dynamically during training. For example, qi may be set to\nDynamic 3D Centerness or IoUBEV. The detected objects, D are decoded as the same parameterization\nas G.\nD\u2266 {dk\u2208 R8 : C1 \u2265 C2 \u2265 ... \u2265 Ck}K=1, where K \u2282 N,\ndk {xego, yego, zego, lk, wk, hk, 0k}.\nWe seek to predict a continuous representation of the ground truth targets as:\nD\u2266 {dk\u2208 R8 : C1 \u2265 C2 \u2265 ... \u2265 Ck}K=1, where K \u2282 N,\ngk {xego, yego, zego, lk, wk, hk,0k,Ck},\nwhere xk, Y, Zk are the coordinates of the object in the ego-vehicle reference frame, lk, wk, hk\nare the length, width, and height of the object, \u03b8k is the counter-clockwise rotation about the vertical\naxis, and ck is the object category likelihood."}, {"title": "Regression Targets", "content": "Following previous literature, we do not directly predict the object proposal\nrepresentation in Section 7.1. Instead, we define the regression targets as the following:\nT(P,G) = {ti(pi, gi) \u2208 R8}K1, where K \u2208 N,\nti(Pi, gi) = {\u2206xi, \u2206yi, Azi, log li, log wi, log hi, sin \u03b8i, cos \u03b8i},\nwhere P and G are the sets of points in the range image and the ground truth cuboids in the 3D\nscene, Axi, Ayi, Azi are the offsets from the point to the associated ground truth cuboid in the\npoint-azimuth reference frame, log li, log wi, log hi are the logarithmic length, width, and height of\nthe object, respectively, and sin \u03b8i, cos \u03b8i are continuous representations of the object's heading 0."}, {"title": "Classification Loss", "content": "Once all of the candidate foreground points have been ranked and assigned,\neach point needs to incur loss proportional to its regression quality. We use Varifocal loss [26] with a\nsigmoid-logit activation for our classification loss:\nVFL(Ci, qi) =\n{\nqi(-qi log(ci) + (1-qi) log(1 \u2013 ci)) if qi > 0\n-ac log(1 \u2013 ci) otherwise,\nwhere ci is classification likelihood and qi is 3D classification targets (e.g., Dynamic IOUBEY or\nDynamic 3D Centerness). Our final classification loss for an entire 3D scene is:\nLc =\n1\nM\nNPG\n\u03a3\u03a3 VFL(, ),\nj=1 i=1\nwhere M is the total number of foreground points, N is the total number of objects in a scene, P\nis the set of 3D points which fall inside the jth ground truth cuboid, is the likelihood from the\nnetwork classification head, and q is the 3D classification target."}, {"title": "Regression Loss", "content": "We use an l\u2081 regression loss to predict the regression residuals. The regression\nloss for an entire 3D scene is:\nLr =\n1\nN\n\u03a3\u03a3 LiLoss(),\nP\nj=1\ni=1\nwhere N is the total number of objects in a scene, P is the set of 3D points which fall inside the jth\nground truth cuboid, r is the predicted cuboid parameters from the network, and t are the target\nresiduals to be predicted."}, {"title": "Total Loss", "content": "Our final loss is written as:\nL = Lc + Lr"}, {"title": "7.2 Argoverse 2", "content": "Additional details on the evaluation metrics used in the Argoverse 2.\n\u2022 Average Precision (AP): VOC-style computation with a true positive defined at 3D Eu-\nclidean distance averaged over 0.5 m, 1.0 m, 2.0 m, and 4.0 m.\n\u2022 Average Translation Error (ATE): 3D Euclidean distance for true positives at 2 m.\n\u2022 Average Scale Error (ASE): Pose-aligned 3D IoU for true positives at 2 m.\n\u2022 Average Orientation Error (AOE): Smallest yaw angle between the ground truth and\nprediction for true positives at 2 m.\n\u2022 Composite Detection Score (CDS): Weighted average between AP and the normalized true\npositive scores:\nCDS = AP. 1 \u2212 x, where x \u2208 {ATEunit, ASEunit, AOEunit}.\nx\u0395\u03a7\nWe refer readers to Wilson et al. [6] for further details."}, {"title": "7.3 Waymo Open", "content": "Additional details on the evaluation metrics used in the Waymo Open are listed below.\n1. 3D Mean Average Precision (mAP): VOC-style computation with a true positive defined\nby 3D IoU. The gravity-aligned-axis is fixed.\n(a) Level 1 (L1): All ground truth cuboids with at least five lidar points within them.\n(b) Level 2 (L2): All ground cuboids with at least 1 point and additionally incorporates\nheading into its true positive criteria.\nFollowing RangeDet [5], we report L1 results."}, {"title": "8 Range-view 3D Object Detection", "content": "Baseline Model. Our baseline models are all multi-class and utilize the Deep Layer Aggregation\n(DLA) [32] architecture with an input feature dimensionality of 64. In our Argoverse 2 experiments,\nwe incorporate five input features: x, y, z, range, and intensity, while for our Waymo experiments, we\ninclude six input features: x, y, z, range, intensity, and elongation. These inputs are then transformed\nto the backbone feature dimensionality of 64 using a single basic block. For post-processing, we\nuse weighted non-maximum suppression (WNMS). All models are trained and evaluated using\nmixed-precision with BrainFloat16 [33]. Both models use a OneCycle scheduler with AdamW using\na learning rate of 0.03 across four A40 gpus. All models in the ablations are trained for 5 epochs on a\nuniformly sub-sampled fifth of the training set.\nState-of-the-art Comparison Model. We leverage the best performing and most general methods\nfrom our experiments for our state-of-the-art comparison for both the Argoverse 2 and Waymo Open\ndataset models. The Argoverse 2 and Waymo Open models use an input feature dimensionality of\n256 and 128, respectively. Both models uses the Meta-Kernel and a 3D input encoding, Dynamic 3D\nCenterness for their classification supervision, and we use our proposed Range-Subsampling with\nrange partitions of [0 - 30 m), [30m, 50m), [50m, \u221e) with subsampling rates of 8, 2, 1, respectively.\nFor both datasets, models are trained for 20 epochs."}, {"title": "8.1 Qualitative Results", "content": "We include qualitative results for both Argoverse 2 and Waymo Open shown in Figs. 6 and 7."}]}