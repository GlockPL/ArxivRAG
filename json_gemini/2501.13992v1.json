{"title": "DUAL-BRANCH HNSW APPROACH WITH SKIP BRIDGES AND LID-DRIVEN OPTIMIZATION", "authors": ["Hy Nguyen", "Nguyen Hung Nguyen", "Nguyen Linh Bao Nguyen", "Srikanth Thudumu", "Hung Du", "Rajesh Vasa", "Kon Mouzakis"], "abstract": "The Hierarchical Navigable Small World (HNSW) algorithm is widely used for approximate nearest neighbor (ANN) search, leveraging the principles of navigable small-world graphs. However, it faces some limitations. The first is the local optima problem, which arises from the algorithm's greedy search strategy, selecting neighbors based solely on proximity at each step. This often leads to cluster disconnections. The second limitation is that HNSW frequently fails to achieve logarithmic complexity, particularly in high-dimensional datasets, due to the exhaustive traversal through each layer. To address these limitations, we propose a novel algorithm that mitigates local optima and cluster disconnections while enhancing the construction speed, maintaining inference speed. The first component is a dual-branch HNSW structure with LID-based insertion mechanisms, enabling traversal from multiple directions. This improves outlier node capture, enhances cluster connectivity, accelerates construction speed and reduces the risk of local minima. The second component incorporates a bridge-building technique that bypasses redundant intermediate layers, maintaining inference and making up the additional computational overhead introduced by the dual-branch structure. Experiments on various benchmarks and datasets showed that our algorithm outperforms the original HNSW in both accuracy and speed. We evaluated six datasets across Computer Vision (CV), and Natural Language Processing (NLP), showing recall improvements of 18% in NLP, and up to 30% in CV tasks while reducing the construction time by up to 20% and maintaining the inference speed. We did not observe any trade-offs in our algorithm. Ablation studies revealed that LID-based insertion had the greatest impact on performance, followed by the dual-branch structure and bridge-building components.", "sections": [{"title": "1 INTRODUCTION", "content": "Hierarchical Navigable Small World (HNSW) graphs have become a state-of-the-art method for approximate nearest neighbor (ANN) search due to their efficiency and effectiveness in handling large-scale datasets (Malkov & Yashunin, 2020). HNSW constructs a multi-layer graph, where each layer provides a different level of abstraction of the data. The search process navigates these layers, starting from the top, to efficiently approximate the nearest neighbors of a query point.\nDespite its success, HNSW faces several limitations. The first drawback relates to local optimum during the search process. This issue arises from the node insertion mechanism, which inserts nodes into the HNSW graph randomly. Random insertion can result in disconnected regions and weaker inter-cluster connectivity, increasing the likelihood of the search process becoming trapped in local optimum. As shown in Figure 1, random insertion in the HNSW graph makes the greedy search process traverse to a local optimum (denoted by the red node), instead of reaching the global optimum (denoted by the green node). The second drawback is that the logarithmic complexity $O(n \\log n)$ proposed in the original HNSW paper (Malkov & Yashunin, 2020) is difficult to achieve consistently,"}, {"title": "2 RELATED WORK", "content": null}, {"title": "2.1 HIERARCHICAL NAVIGABLE SMALL WORLD", "content": "HNSW graphs are prominent due to their efficiency and scalability across various datasets. (Malkov & Yashunin, 2020) introduced HNSW as a multi-layered graph structure that allows efficient neighborhood exploration in high-dimensional spaces. The performance of HNSW is highly dependent on its parameters, such as the number of links per node (M) and search complexity ($ef-Search$), but its success also comes from the log distribution structure of the data being indexed.\nRecent advancements have significantly deepened the understanding and extended the capabilities of HNSW, driving state-of-the-art improvements in its performance and applications. (Zhang et al., 2022)) explored optimization techniques tailored for large-scale HNSW graphs, focusing on enhancing search efficiency through refined graph construction. Cole Foster (2023) further advanced"}, {"title": "2.2 HIGH LOCAL INTRINSIC DIMENSIONALITY IN HNSW", "content": "One of the key challenges in Approximate Nearest Neighbor (ANN) search is dealing with local minima, where the search process gets stuck in suboptimal regions of the data space. High Local Intrinsic Dimensionality (LID) values, as demonstrated by (Houle et al., 2018), indicates its role in capturing the density of data points around the query, serving as a measure of local intrinsic dimensionality. High-LID values often represent outliers or points in sparse, locally complex regions, which can disrupt the search process by anchoring the graph structure in unfavorable ways, mitigating the risk of local minima.\nThe importance of handling high-LID outliers in graph construction was also explored by (Amsaleg et al., 2015), who proposed several methods for estimating LID and demonstrated its usefulness in ANN search, classification, and outlier detection. Their findings support the idea that high-LID points should be treated differently during graph construction.\nRecent work by (Elliott & Clark, 2024) has highlighted the significant impact of data insertion order on the recall performance of HNSW graphs. Their study showed that inserting nodes with higher LID values into the upper layers of the HNSW graph can improve recall by up to 12.8 percentage points.\nWhile these works offer valuable insights into handling high-dimensional data and mitigating local minima, they leave open questions regarding the best methods for integrating LID considerations into HNSW graph construction. Furthermore, the impact of LID-based strategies on the broader structure of HNSW graphs remains an area for further exploration."}, {"title": "3 HNSW++: DUAL-BRANCH HNSW APPROACH WITH BRIDGES AND LID-DRIVEN OPTIMIZATION", "content": null}, {"title": "3.1 MOTIVATION", "content": "In HNSW, nodes are assigned to layers randomly, with the probability of a node being placed in a higher layer decreasing exponentially. In (Baranchuk et al., 2019), the authors pointed out that similarity graphs are vulnerable to local minima when the query is unable to escape suboptimal vertices. The combination of random insertion and greedy search in HNSW worsens this issue, as random factors can cause the search to deviate from the optimal path. Another challenge arising from the random insertion is the low inter-cluster connectivity. This problem can place different clusters on separate layers, limiting their connectivity. Consequently, the search process may terminate in a cluster different from the one containing the true nearest neighbors. (Lin & Zhao, 2019) also observed that the hierarchical structure of HNSW fails to achieve the expected logarithmic complexity. Instead, the exhaustive traversal of each layer becomes a bottleneck. As a result, HNSW encounters several inherent problems: (1) a high likelihood of local minima, which grows with the size of the data; (2) weak connectivity between clusters; and (3) slower search times, construction time, making it difficult to achieve logarithmic complexity in practice."}, {"title": "3.2 PROPOSED METHODOLOGY", "content": "To address these issues, we propose HNSW++, which partitions the dataset into two branches based on the index of the inserted nodes. By doing so, spatial regions are divided into different branches, allowing the algorithm to search in both simultaneously. The search process begins in the upper layers of both branches, navigating greedily until a local minimum is encountered. The algorithm then descends to the lower layers, where the two branches merge at layer 0, combining their results for the final output (see Fig. 3). This approach not only minimizes the influence of local minima and ensures a more accurate search for the true nearest neighbors, but it also reduces construction time, as each new node only needs to search through half the nodes already inserted. This dual-branch approach can be expressed as follows:\n$HNSW++(D) = Merge (S(q, L_1, exclude\\_set_1), S(q, L_2, exclude\\_set_2), k)$ (1)\nwhere D is the dataset containing n nodes, q is the query node. $S(x,l, exclude\\_set)$ is the search process for the nearest neighbor of node x, which begins at layer l, and exclude_set is the result from other branch if available. The exclude_set is passed to the Search algorithm in layer 0 of both branches to ensure that the neighbors returned by each branch are distinct, preventing overlap in the results between the two branches. $L_1$ and $L_2$ are the upper layers of branch 1 and branch 2, respectively. Among the neighbors returned by $S(x_q, L_1, exclude\\_set_1)$ and $S(q, L_2, exclude\\_set_2)$, Merge function selects the k neighbors that are closest to the query node q.\nAdditionally, to improve cluster connectivity, we utilize Local Intrinsic Dimensionality (LID) values during insertion into HNSW++. The Local Intrinsic Dimensionality (LID) of a data point can be estimated using Maximum Likelihood Estimation (MLE) combined with the k-Nearest Neighbors (kNN) algorithm (Hand et al., 2001). The Maximum Likelihood Estimate of the LID, $LID(x)$, is given by the following formula:\n$LID(x) = \\frac{1}{\\frac{1}{k-1} \\sum_{i=1}^{k-1} \\log \\frac{d_k}{d_i}}^{-1}$  (2)\nwhere $d_i$ is the Euclidean distance between the query point and its i-th nearest neighbor, while $d_k$ is the distance to the k-th nearest neighbor (Levina & Bickel, 2004). High LID scores indicate sparse regions, often found at the edges of clusters (see Fig. 2). By inserting nodes with high LID into upper layers, we facilitate connections between clusters. During the search, these nodes enable faster traversal between clusters in the upper layers, and a more precise search within the target cluster in the lower layers. This strategy reduces the chances of the search becoming stuck in a suboptimal cluster, guiding it more efficiently toward the true nearest cluster. This LID estimation plays a crucial role in guiding the construction and optimization of HNSW graphs by allowing us to distinguish between nodes that exist in regions of varying density, thus improving the efficiency and accuracy of search operations. This expanded version provides more context about LID and its importance while still preserving the meaning and structure of your original section.\nTo further accelerate the search and approximate logarithmic complexity in practice, we introduce a method for creating additional skip bridges between upper layers and bottom layer (layer 0) based"}, {"title": "3.3 ALGORITHM DESIGN", "content": "The network construction algorithm incrementally inserts nodes into the hierarchical structure based on their assigned layer and order. Starting at the top layer, the algorithm searches for the closest nodes to the query, then progresses to the next lower layer, using the previous layer's result as the entry point. This continues iteratively until reaching the query node's designated layer. From there, connections between the query node q and surrounding nodes are established, with further refinement through adding or dropping connections as limits are exceeded."}, {"title": "3.4 LID THRESHOLD INFLUENCE ON NUMBER OF SKIPS AND PERFORMANCE", "content": "During the inference stage, the LID threshold determines whether a search can directly jump to layer 0, bypassing redundant intermediate layers, thereby reducing inference time (Algorithm 2). Normalized LID values range from 0 to 1, where a value of 1 indicates that the neighborhood around a specific node is sparse, while a value of 0 reflects a dense neighborhood.\nTo achieve optimal performance, multiple experiments on LID threshold were conducted, including (1) Influence of threshold on number of skips, (2) Influence of threshold on accuracy and recall.\nThe impact of the LID threshold on the number of skips is illustrated in Figure 14, clearly showing that as the threshold increases, the number of skips decreases across all datasets. At lower thresholds, the algorithm performs more frequent skips, enhancing search efficiency in the denser regions of the graph.\nIn terms of accuracy and recall, as depicted in Figures 15a and 15b, most datasets have minimal changes given different LID threshold.\nIn conclusion, tuning the LID threshold primarily enhances computational efficiency by optimizing the number of skips, thereby reducing inference time, without affecting accuracy and recall."}, {"title": "3.5 COMPLEXITY ANALYSIS", "content": null}, {"title": "3.5.1 SEARCH COMPLEXITY", "content": "The search complexity of HNSW++ builds upon the original HNSW framework, integrating dual-branch navigation and LID-driven optimizations. By employing a dual-branch structure, the search process is divided between two branches, each exploring distinct graph regions. Starting at the top layers of both branches, the algorithm ensures robust exploration and minimizes the risk of search stagnation in local minima.\nIncorporating layer-skipping bridges, the complexity is further reduced by allowing direct transitions to lower layers when LID conditions are met. Let $P_{skip}$ represent the probability of a skip occurring based on a node's LID exceeding the threshold T, and $L_{total}$ represent the total number of layers. The expected number of layers traversed is reduced to $L_{total} \\cdot (1 \u2013 P_{skip})$, making the effective layer exploration more efficient than the standard HNSW.\nEach branch independently executes the search with complexity scaling as $O(log(N))$ due to the hierarchical structure. The merging of results at the base layer introduces a constant overhead,"}, {"title": "3.5.2 CONSTRUCTION COMPLEXITY", "content": "Construction in HNSW++ follows a layered insertion protocol, where nodes are assigned layers based on normalized LID values. The dual-branch structure halves the effective dataset size per branch, reducing the computational load for insertion operations. Each node is placed after executing nearest neighbor searches at each layer, with complexity per insertion scaling logarithmically as $O(log(N))$.\nThe layer-skipping mechanism also optimizes insertion by allowing high-LID nodes to directly establish connections in deeper layers, bypassing intermediate ones. With $M_{max}$ as the maximum number of connections per node and k as the neighbor set size, the average insertion time is proportional to $O(M_{max} \\cdot log(N))$ per node. Thus, the overall construction complexity for a dataset of size N is $O(N. log(N))$, consistent with the standard HNSW scaling.\nAssuming the LID values are provided beforehand, HNSW++ does not add significant computational overhead. The LID-based assignment process scales efficiently, as the majority of the computational load is absorbed during initial LID calculations. Consequently, HNSW++ retains the scalability of the original HNSW construction process."}, {"title": "4 EXPERIMENTS", "content": "The experiments were conducted on a system equipped with an AMD EPYC 7542 32-Core Processor (64 threads, 2 threads per core) and 80 GB of RAM, running a 64-bit Debian OS. To ensure a fair comparison, the best performance results for each algorithm were chosen based on recall across varying thresholds. The HNSW++ code were implemented in C++ (and Python as extension in Appendix).\nFor ground truth evaluation in Python verrsion, we employed Scikit-learn's NearestNeighbors function run by k-Nearest-Neighbours (kNN) algorithm to generate accurate nearest neighbor comparisons.\nDifferent hyperparameter sets are employed for each algorithm to ensure optimal performance and a fair comparison across all methods.\nIn this section, we provide (1) An overview of the datasets, (2) Main results - Performance comparison between the state-of-the-art approaches and HNSW++, (3) Ablation study - Performance comparison between original HNSW, LID-based HNSW, Multi-branch HNSW, and HNSW++."}, {"title": "4.1 DATASETS OVERVIEW", "content": "Our experiments leveraged six datasets spanning various domains, including Computer Vision (J\u00e9gou et al., 2011; Wolf et al., 2011; Babenko & Lempitsky, 2016), Natural Language Processing (Pennington et al., 2014), and randomly generated vectors. These datasets exhibited a range of Local Intrinsic Dimensionality (LID) values and dimensionalities. All distance computations were performed in L2 space for consistency. Due to resource limitations, the experiments were conducted using 10,000 data points for graph construction and 1,000 data points for inference (Table 1).\nThe LID of each data point was computed using Maximum Likelihood Estimation (MLE), considering the exact nearest neighbors defined by ef_construction (128). The distribution of LID values for each dataset is visualized in Figure 18."}, {"title": "4.2 EXPERIMENT RESULTS", "content": "To evaluate our algorithm, several methods were utilized for comparison. Since our code for HNSW++ was implemented in Python, and the other methods use different programming languages that offer faster runtimes than Python, we cannot fairly compare the construction time and inference time. Therefore, we focus on performance aspects only to ensure fairness. Further experiments including accuracy, recall, construction, and query time are conducted in Section 4.3. The algorithms we use to compare with HNSW++ are:\n\u2022 FAISS IVFPQ (Inverted File Index with Product Quantization)\u00b9 (Johnson et al., 2019) : Combines inverted file indexing with product quantization to perform efficient approximate nearest neighbor searches on large-scale datasets.\n\u2022 NMSLib (Non-Metric Space Library)\u00b2: A highly optimized library designed for approximate nearest neighbor search, built upon HNSW. :\n\u2022 PyNNDescent\u00b3: Implements an efficient, approximate nearest neighbor search using Nearest Neighbor Descent graphs.\n\u2022 Annoy (Approximate Nearest Neighbors Oh Yeah)4: Utilizes a forest of random projection trees to perform nearest neighbor searches. It follows a graph-based approach."}, {"title": "4.3 ABLATION STUDY", "content": "To gain a deeper understanding of the contribution of individual factors to recall, accuracy, construction time, and query time, we conducted a set of ablation experiments. Each experiment focused on a specific aspect of the algorithm by evaluating the following configurations:\n\u2022 Basic: The standard HNSW algorithm without any modifications.\n\u2022 Multi-Branch: This version retains only the two parallel branches of HNSW, excluding both the LID-based insertion mechanism and the skip-layer approach.\n\u2022 LID-Based: This variant utilizes only the LID-based insertion mechanism, removing the two-branch structure and the skip-layer mechanism.\n\u2022 HNSW++: The full version incorporating all three enhancements: two branches, the LID-based insertion mechanism, and the skip-layer mechanism."}, {"title": "5 CONCLUSION", "content": "In this work, we introduce HNSW++, a novel algorithm that addresses several key challenges in nearest neighbor search. HNSW++ is designed to reduce the probability of falling into local minima, improve the detection of outlier nodes, and enhance the connectivity of clusters. It also significantly accelerates the construction process while maintaining stable inference time. To achieve these advancements, we integrate innovative mechanisms such as a multi-branch structure, insertion based on Local Intrinsic Dimensionality (LID) values, and a skip-layer approach, ensuring the algorithm's efficiency and scalability.\nFurthermore, we perform an in-depth analysis of how each of these factors contributes to the overall performance of HNSW++, providing insights into their individual and combined effects. Through extensive experimentation on a wide range of tasks, datasets, and benchmarks, HNSW++ consistently achieves state-of-the-art performance, demonstrating both superior accuracy and faster execution times compared to existing methods. This highlights the potential of HNSW++ as a robust solution for large-scale nearest neighbor search problems."}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 LID THRESHOLD INFLUENCE ON NUMBER OF SKIPS AND PERFORMANCE", "content": "The provided figures illustrate the results of experiments analyzing the effect of varying thresholds on different aspects of the HNSW algorithm's performance.\n\u2022 Figure 14 shows the effect of different thresholds on the number of layers skipped during query searches. As the threshold increases, the number of layers bypassed by the search algorithms decreases significantly across all datasets.\n\u2022 Figure 15a and Figure 15b depict the impact of threshold values on accuracy and recall across six datasets. The two figures show that showing that most algorithms maintain stable accuracy across different threshold values, with minimal variance as the threshold increases."}, {"title": "A.2 PSEUDOCODES", "content": null}, {"title": "A.2.1 INSERT", "content": "The Insert algorithm constructs the hierarchical graph structure in HNSW++. Specifically, given the assigned layer and branch index for the new element q, the entry point ep starts traversing from the top layer of that branch and stops at the designated layer. At this point, q is added to the graph with edges connecting it to its neighbors, up to a maximum of maxk neighbors. During the insertion process, a dynamic candidate list of size $efConstruction$ is maintained. The node q is represented by its corresponding matrix."}, {"title": "A.2.2 SEARCH", "content": "The Search algorithm is complex due to the integration of Multi-branch and Skipping methods. Initially, for layers greater than 0, the search identifies one nearest neighbor for each branch, setting it as the entry point ep for the next layer of that branch. If a branch triggers a skip, the search for that branch will continue at layer 0, while it waits for the other branch to complete its search. If both branches reach layer 0 simultaneously, or if branch\u2081 reaches layer 0 first, brancho will initiate the search with $efsearch$, resulting in $W_1$, the set of k nearest neighbors from $branch_0$. When branch1 starts its search, it will use $W_1$ as an exclude_set to avoid returning the same neighbors as brancho (Fig.19). Conversely, if $branch_1$ reaches layer 0 first, the roles are reversed. Once the searches from both branches are completed, $W_1$ and $W_2$ are combined to retrieve the final set of k nearest neighbors."}, {"title": "A.2.3 ASSIGN LAYER", "content": "The Assign Layer algorithm is responsible for determining the layer placement of all nodes prior to graph construction. It begins by calculating the expected sizes of each layer and stores them in an array. Using the array of nodes sorted in descending order by their LID values, the algorithm assigns each node to a layer according to the corresponding expected layer sizes."}, {"title": "A.2.4 NORMALIZE LIDS", "content": "Given an array of LID values, this function performs normalization using min-max normalization, as defined by the following equation:\n$normalized\\_LID(x) = \\frac{x - min(LID)}{max(LID) - min(LID)}$ (5)\nThis ensures that the LID values are scaled to a range between 0 and 1, facilitating consistent comparisons across nodes."}, {"title": "A.3 HNSW++ IN PYTHON", "content": null}, {"title": "A.4 LID DISTRIBUTION", "content": "Please refer to Figure 18"}, {"title": "A.5 LID DISTRIBUTION", "content": "Please refer to Figure 19"}]}