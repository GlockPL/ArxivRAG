{"title": "Time Series Domain Adaptation via Latent Invariant Causal Mechanism", "authors": ["Ruichu Cai", "Junxian Huang", "Zhenhui Yang", "Zijian Li", "Emadeldeen Eldele", "Min Wu", "Fuchun Sun"], "abstract": "Time series domain adaptation aims to transfer the complex temporal dependence from the labeled source domain to the unlabeled target domain. Recent advances leverage the stable causal mechanism over observed variables to model the domain-invariant temporal dependence. However, modeling precise causal structures in high-dimensional data, such as videos, remains challenging. Additionally, direct causal edges may not exist among observed variables (e.g., pixels). These limitations hinder the applicability of existing approaches to real-world scenarios. To address these challenges, we find that the high-dimension time series data are generated from the low-dimension latent variables, which motivates us to model the causal mechanisms of the temporal latent process. Based on this intuition, we propose a latent causal mechanism identification framework that guarantees the uniqueness of the reconstructed latent causal structures. Specifically, we first identify latent variables by utilizing sufficient changes in historical information. Moreover, by enforcing the sparsity of the relationships of latent variables, we can achieve identifiable latent causal structures. Built on the theoretical results, we develop the Latent Causality Alignment (LCA) model that leverages variational inference, which incorporates an intra-domain latent sparsity constraint for latent structure reconstruction and an inter-domain latent sparsity constraint for domain-invariant structure reconstruction. Experiment results on eight benchmarks show a general improvement in the domain-adaptive time series classification and forecasting tasks, highlighting the effectiveness of our method in real-world scenarios.", "sections": [{"title": "1 INTRODUCTION", "content": "To overcome the challenges of distribution shift between\nthe training and the test time series datasets [1], time\nseries domain adaptation seeks to transfer the temporal\ndependence from labeled source data to unlabeled target\ndata. Mathematically, in the context of time series domain\nadaptation, we let \\(X = (x_1,\\cdots, x_\\tau,\\cdots, x_t)\\) be multivariate\ntime series with t timestamps and a channel size of n, where\n\\(x_\\tau \\in \\mathbb{R}^n\\) and Y is the corresponding label, such that Y\ncan be a time series or scalar, depending on the forecasting\nor classification tasks. By considering u as the domain label, we further assume that \\(P(X, Y|u_S)\\) and \\(P(X, Y|u_T)\\) are the source and target distributions, respectively. In the source domain, we can access \\(m_S\\) annotated X-Y pairs, represented as \\(\\{(X^S_i,Y^S_i)\\}_{i=1}^{m_S} = (X^S,Y^S)\\). While in the target domain, only \\(m_T\\) unannotated time series data can be observed, denoted by \\(\\{X^T_i\\}_{i=1}^{m_T} = (X^T)\\). The primary goal of unsupervised time series domain adaptation is to model the target joint distribution \\(P(X,Y|u_T)\\) by leveraging the labeled source and the unlabeled target data.\nSeveral methods have been proposed to address this problem. Previous works have extended the conventional assumptions of domain adaptation for static data [2], [3] to time series data, including covariate shift [4], label shift [5], and conditional shift [6]. For instance, leveraging the covariate shift assumption, i.e., \\(P(Y|X)\\) is stable, some researchers [7], [8] employ recursive neural network-based architecture as feature extractor and adopt adversarial training strategy to extract domain-invariant information. Others, e.g., Wilson et al. [9], utilize adversarial and contrastive learning to extract the domain-invariant representation for time series data. Moreover, other researchers assume that the distribution shift of Y varies across different domains, known as label shift. Specifically, He et al. [10] tackle the feature shift and label shift by aligning both temporal and frequency features. Conversely, Hoyez et al. [11] argue that the content shift and style shift in time series data belong to the conditional shift, where \\(P(X|Y)\\) varies with different domains in time series domain adaptation.\nInstead of introducing assumptions from a statistical perspective, another promising direction is to harness the invariant temporal dependencies for time series domain adaptation. Specifically, Cai et al. [12] address time series"}, {"title": "2 RELATED WORKS", "content": "2.1 Unsupervised Domain Adaptation\nUnsupervised domain adaptation [3], [6], [20], [21], [22], [23] aims to leverage the knowledge from a labeled source domain to an unlabeled target domain, by training a model to domain-invariant representations [24]. Researchers have adopted different directions to tackle this problem. For example, Long et al. [25] trained the model to minimize a similarity measure, i.e., maximum mean discrepancy (MMD), to guide learning domain-invariant representations. Tzeng et al. [26] used an adaptation layer and a domain confusion loss. Another direction is to assume the stability of conditional distributions across domains and extract the label-wise domain-invariant representation [27], [28], [29]. For instance, Xie et al. [30] constrained the label-wise domain discrepancy, and Shu et al. [31] considered that the decision boundaries should not cross high-density data regions, so they propose the virtual adversarial domain adaptation model. Another type of assumption is the target shift [5], [6], [32], [33], which assumes \\(p(Y|u)\\) varies across different domains.\nBesides, several methods address the domain adaptation problem from a causality perspective. Specifically, Zhang et al. [6] proposed the target shift, conditional shift, and generalized target shift assumptions, based on the premise that \\(p(Y)\\) and \\(P(X|Y)\\) vary independently. Cai et al. [3] leveraged the data generation process to extract the disentangled semantic representations. Building on causality analysis, Petar et al. [22] highlighted the significance of incorporating domain-specific knowledge for learning domain-invariant representation. Recently, Kong et al. [20] addressed the multi-source domain adaptation by identifying the latent variables, and Li et al. [34] further relaxed the identifiability assumptions.\n2.2 Domain Adaptation on Temporal Data\nIn recent years, domain adaptation for time series data has garnered significant attention. Da Costa et al. [7] is one of the earliest time series domain adaptation works, where authors adopted techniques originally designed for non-time series data to this domain, incorporating recurrent neural networks as feature extractors to capture domain-invariant representations. Purushotham et al. [8] further refined this approach by employing variational recurrent neural networks [35] to enhance extracting domain-invariant features. However, such methods face challenges in effectively capturing domain-invariant information due to the intricate dependencies between time points. Subsequently, Cai et al. [36] proposed the Sparse Associative Structure Alignment (SASA) method, based on the assumption that sparse associative structures among variables remain stable across domains. This method has been successfully applied to adaptive time series classification and regression tasks. Additionally, Jin et al. [37] introduced the Domain Adaptation Forecaster (DAF), which leverages statistical relationships from relevant source domains to improve performance in target domains. Li et al. [16] hypothesized that causal structures are consistent across domains, leading to the development of the Granger Causality Alignment (GCA) approach. This method uncovers underlying causal structures while modeling shifts in conditional distributions across domains.\nOur work also relates to domain adaptation for video data, which could be considered a form of high-dimensional time series data. Video data offers a robust benchmark for evaluating the performance of our method. Unsupervised domain adaptation for video data has recently attracted substantial interest. For instance, Chen et al. [38] proposed a Temporal Attentive Adversarial Adaptation Network (TA3N), which integrates a temporal relation module to enhance temporal alignment. Choi et al. [39] proposed the SAVA method, which leverages self-supervised clip order prediction and attention-based alignment across clips. In addition, Pan et al. proposed the Temporal Co-attention Network (TCON) [40], which employs a cross-domain co-attention mechanism to identify key frames shared across domains, thereby improving alignment.\nLuo et al. [41] focused on domain-agnostic classification using a bipartite graph network topology to model cross-domain correlations. Rather than relying on adversarial learning, Sahoo et al. [42] developed CoMix, an end-to-end temporal contrastive learning framework that employs background mixing and target pseudo-labels. More recently, Chen et al. [43] introduced multiple domain discriminators for multi-level temporal attentive features to achieve superior alignment, while Turrisi et al. [44] utilized a dual-headed deep architecture that combines cross-entropy and contrastive losses to learn a more robust target classifier. Additionally, Wei et al. [45] employed contrastive and adversarial learning to disentangle dynamic and static information in videos, leveraging shared dynamic information across domains for more accurate prediction.\n2.3 Granger Causal Discovery\nSeveral works have been raised to infer causal structures for time series data based on Granger causality [19], [46], [47], [48], [49], [50]. Previously, several researchers used the vector autoregressive (VAR) model [51], [52] with the sparsity constraint like Lasso or Group Lasso [53], [54] to learn Granger causality. Recently, several works have inferred Granger causality with the aid of neural networks. For instance, Tank et al. [55] developed a neural network-based autoregressive model with sparsity penalties applied to network weights. Inspired by the interpretability of self-explaining neural networks, Marcinkevivcs et al. [19] introduced a generalized vector autoregression model to learn Granger causality. Li et al. [16] considered the Granger causal structure as latent variables. Cheng et al. [56], [57] proposed a neural Granger causal discovery algorithm to discover Granger causality from irregular time series data. Lin et al. [49] used a neural architecture with contrastive learning to learn Granger causality. However, these methods usually consider the Granger causal structures over low-dimension observed time series data, which can hardly address the time series data with high dimension or latent causal relationships. To address this limitation, we identify the latent variables and infer the latent Granger causal structures behind high-dimensional time series data."}, {"title": "2.4 Identifiability of Generative Model", "content": "To achieve causal representation [58], [59], [60] for time series data, many researchers leverage Independent Component Analysis (ICA) to recover latent variables with identifiability guarantees [61], [62], [63], [64]. Conventional methods typically assume a linear mixing function from the latent variables to the observed variables [65], [66], [67], [68]. To relax the linear assumption, researchers achieve the identifiability of latent variables in nonlinear ICA by using different types of assumptions, such as auxiliary variables or sparse generation processes [69], [70], [71], [72], [73]. Aapo et al. [74] first achieved identifiability for methods employing auxiliary variables by assuming the latent sources follow an exponential family distribution and introducing auxiliary variables, such as domain indices, time indices, and class labels. To further relax the exponential family assumption, Zhang et al. [20], [75], [75], [76], [77] proposed component-wise identifiability results for nonlinear ICA, requiring \\(2n + 1\\) auxiliary variables for n latent variables. To seek identifiability in an unsupervised manner, researchers employed the assumption of structural sparsity to achieve identifiability [69], [78], [79], [80]. Recently, Zhang et al. [81] achieved identifiability under distribution shift by leveraging the sparse structures of latent variables. Li et al. [82] further employed sparse causal influence to achieve identifiability for time series data with instantaneous dependency."}, {"title": "3 PRELIMINARIES", "content": "In this section, we first describe the data generation process from multiple distributions under latent Granger Causality. Sequentially, we further provide the definition of generalized causal conditional shift as well as identifiability of latent causal process.\n3.1 Data Generation Process from Multiple Distributions under Latent Granger Causality\nTo illustrate how we address time series domain adaptation with latent causality alignment, we first consider the data generation process under latent Granger causality. Specifically, we first let \\(X = (x_1,\\ldots,\\ldots,x_\\tau,\\cdots,x_t)\\) be multivariate time series with t time steps. Each \\(x_\\tau \\in \\mathbb{R}^m\\) is generated from latent variables \\(Z_\\tau \\in \\mathbb{R}^n,m \\gg n\\) via an invertible nonlinear mixing function g as follows:\n\\[x_t = g(z_t).\\](1)\nMoreover, the i-th dimension of the latent variables \\(Z_{ti}\\) is generated through the latent causal process, which is assumed to be influenced by the time-delayed parent variables \\(Pa(z_{t,i})\\) and the different domains, simultaneously. Formally, this relationship can be expressed using a structural equation model as follows:\n\\[Z_{t,i}= f_i(Pa(z_{t,i}), u, \\epsilon_{t,i}) \\text{ with }\\epsilon_{t,i} \\sim P_{\\epsilon_i},\\](2)\nwhere \\(\\epsilon_{t,i}\\) denotes the temporally and spatially independent noise extracted from a distribution \\(p_{\\epsilon_i}\\), and u denotes the domain index that could be either the source (S) or target (T) domains."}, {"title": "3.2 Generalized Causal Conditional Shift Assumption", "content": "Inspired by the domain-invariant causal mechanism between the source and target domains, we generalize the causal conditional shift assumption [16] from observed variables to latent variables, which is discussed next.\nAssumption 1. (Generalized Causal Conditional Shift) Given latent causal structure A and latent variables \\(Z_1,..., Z_t, Z_{t+1}\\), we assume that the conditional distribution \\(P(Z_{t+1} | Z_1,...,Z_t)\\) varies with different domains, while the latent causal structures are stable across different domains. This can be formalized as follows:\n\\[P(Z_{t+1} | Z_1,..., Z_t, S) \\neq P(Z_{t+1} | Z_1,..., Z_t, T)\\]\\[A_S = A_T,\\](3)\nwhere \\(A_S\\) and \\(A_T\\) are the causal structures over latent variables from the source and target domains, respectively.\nBased on the aforementioned assumption, we consider both the forecasting and the classification time series domain adaptation tasks. Starting with the time series forecasting, the label \\(Y = (x_{t+1},...,x_{t+p})\\) denotes the values of the future p time steps. On the other hand, Y denotes the discrete class labels for the classification tasks, which are generated as follows:\n\\[Y = C(Z_1,...,Z_t),\\](4)\nwhere C is the labeling function. Our objective in this paper is to use the labeled source data and unlabeled target data to identify the target joint distribution.\n3.3 Identifiability of Target Joint Distribution\nIn this subsection, we discuss the mechanism of identifying the target joint distribution. By introducing the latent variables and combining the data generation process, we can factorize the target joint distribution as shown in Equation (5).\n\\[P(X,Y|T) = \\int P(X|Z)P(Y|Z)P(Z|T)dZ,\\](5)\nwhere \\(Z = (Z_1,...,Z_t)\\) denotes the latent causal process. According to the aforementioned equation, we can identify the target joint distribution by modeling the conditional distribution of observed data given latent variables and identifying the latent causal process under theoretical guarantees. Specifically, if estimated latent processes can be uniquely identified up to permutation and component-wise invertible transformations, then the latent causal relationships are also identifiable. This can be regarded to the conditional independence relations that fully characterize causal relationships within a causal system, assuming the absence of causal confounders within latent causal processes.\nDefinition 1 (Identifiable Latent Causal Process). Let \\(X = \\{x_1,...,x_t\\}\\) represent a sequence of observed variables generated according to the true latent causal processes characterized by \\((f_i, p(\\epsilon_i), g)\\), as described in Equations (1) and (2). A learned generative model \\((\\hat f_i,\\hat p(\\epsilon_i), \\hat g)\\) is said to be observationally equivalent to \\((f_i, p(\\epsilon_i), g)\\) if the distribution \\(P_{f_i,p(\\epsilon_i),g}(\\{x\\}_{t=1})\\) produced by the model matches the true data distribution"}, {"title": "4 IDENTIFYING LATENT CAUSAL PROCESS", "content": "Based on the definition of the identification of latent causal processes, we demonstrate how to identify the latent causal process within the context of a sparse latent process. Specifically, we first utilize the connection between conditional independence and cross derivatives [83], along with the sufficient variability of temporal data, to uncover the relationships between the estimated and true latent variables. Furthermore, we establish the identifiability of the latent variables by imposing constraints on sparse causal influences, as shown in Lemma 1.\nWe demonstrate that given certain assumptions, the latent variables are also identifiable. To integrate contextual information for identifying latent variables \\(z_t\\), we consider latent variables across L consecutive timestamps, including \\(z_t\\). For simplicity, we analyze the case where the sequence length is 2 (i.e., L = 2) and the time lag is 1.\nLemma 1. (Identifiability of Temporally Latent Process) [84] Suppose there exists invertible function \\(\\hat g\\) that maps \\(x_t\\) to \\(\\hat z_t\\), i.e., \\(\\hat z_t = \\hat g(x_t)\\), such that the components of \\(\\hat z_t\\) are mutually independent conditional on \\(z_{t-1}\\).Let\n\\[V_{t,k} = \\begin{pmatrix}\n    \\frac{\\partial^2 \\log p(z_{t,k}|z_{t-1})}{\\partial z_{t,k}\\partial z_{t-1,1}} & \\frac{\\partial^2 \\log p(z_{t,k}|z_{t-1})}{\\partial z_{t,k}\\partial z_{t-1,2}} & \\dots & \\frac{\\partial^2 \\log p(z_{t,k}|z_{t-1})}{\\partial z_{t,k}\\partial z_{t-1,n}} \\\\\n    \\frac{\\partial^3 \\log p(z_{t,k}|z_{t-1})}{\\partial z_{t,k}^2\\partial z_{t-1,1}} & \\frac{\\partial^3 \\log p(z_{t,k}|z_{t-1})}{\\partial z_{t,k}^2\\partial z_{t-1,2}} & \\dots & \\frac{\\partial^3 \\log p(z_{t,k}|z_{t-1})}{\\partial z_{t,k}^2\\partial z_{t-1,n}}\n\\end{pmatrix},\\](8)\nIf for each value of \\(z_t\\), \\(V_{t,1}, \\vee_{t,1}, V_{t,2}, \\vee_{t,2}, \\ldots,, V_{t,n}, \\vee_{t,n}\\), as 2n vector function in \\(z_{t-1,1}, z_{t-1,2}, \\ldots, z_{t-1,n}\\), are linearly independent, then \\(\\hat z_t\\) must be an invertible, component-wise transformation of a permuted version of \\(z_t\\)."}, {"title": "5 LATENT CAUSALITY ALIGNMENT MODEL", "content": "Based on the theoretical results, we propose the latent causality alignment (LCA), shown in Figure 2, for time series domain adaptation. This figure shows a variational-inference-based neural architecture to model time series data, a prior estimation network, and a downstream neural forecaster for different downstream tasks.\n5.1 Temporally Variational Inference Architecture\nWe first derive the evidence lower bound (ELBO) to model the time series data, as follows:\n\\[\\log P(X, Y) = \\log P(x_{1:t}, Y) = \\log \\int \\frac{P(x_{1:t}, Y, Z_{1:t})}{Q(Z_{1:t}|x_{1:t})} Q(Z_{1:t}|x_{1:t}) = \\log \\mathbb{E}_{Q(Z_{1:t}|x_{1:t})}\\left[\\frac{P(x_{1:t}, Y, Z_{1:t})}{Q(Z_{1:t}|X_{1:t})}\\right]\\]\\[\\geq \\mathbb{E}_{Q(Z_{1:t}|x_{1:t})} \\left[\\log P(x_{1:t}|Z_{1:t}) + \\log P(Y|Z_{1:t}) - D_{KL}(Q(Z_{1:t}|X_{1:t})||P(Z_{1:t}))\\right],\\](9)\nwhere \\(Q(Z_{1:t}|X_{1:t})\\) and \\(P(x_{1:t}|Z_{1:t})\\) are used to approximate the prior distribution of latent variables and reconstruct the observations, respectively. Technologically, we consider \\(Q(Z_{1:t}|x_{1:t})\\) and \\(P(x_{1:t}|Z_{1:t})\\) as the encoder and decoder networks, respectively, which can be formalized as follows:\n\\[\\hat z_{1:t} = \\epsilon(x_{1:t}) \\quad \\hat x_{1:t} = \\delta(\\hat z_{1:t}),\\](10)\nwhere \\(\\epsilon\\) and \\(\\delta\\) denote the encoder and decoder respectively.\n5.2 Prior Estimation Networks\nTo estimate the prior distribution \\(P(z_{1:t})\\), we propose the prior estimation networks. Specifically, we first let \\(r_i\\) be a set of learned inverse transition functions that receive the estimated latent variables with the superscript symbol as input, and use it to estimate the noise term \\(\\hat \\epsilon_i\\), i.e., \\(\\hat \\epsilon_{t,i} = r_i(\\hat z_{t,i}, \\hat z_{t-1})\\), where each \\(r_i\\) is implemented by Multi-layer Perceptron networks (MLPs). Sequentially, we devise a transformation \\(\\kappa := \\{\\hat z_{t-1}, \\hat z_{t}\\} \\rightarrow \\{\\hat z_{t-1}, \\hat \\epsilon\\}\\), whose Jacobian can can be formalized as \\(J_{\\kappa} = \\begin{pmatrix} J_a & J_b \\\\ 0 & J_e \\end{pmatrix}\\), where \\(J_a(i, j) = \\frac{\\partial \\hat z_{t-1,i}}{\\partial \\hat z_{t-1,j}}\\) and \\(J_e = diag(\\frac{\\partial \\hat r_i}{\\partial \\hat z_{t,i}})\\). Hence we have Equation (11) via the change of variables formula.\n\\[\\log p(\\hat z_t, \\hat z_{t-1}) = \\log p(\\hat z_{t-1}, \\hat \\epsilon_t) + \\log \\vert J_\\kappa \\vert = \\log p(\\hat z_{t-1}, \\hat \\epsilon_t) + \\log \\vert \\frac{\\partial \\hat r_i}{\\partial \\hat z_{t,i}} \\vert\\\\](11)\nAccording to the generation process, the noise term \\(\\epsilon_{t,i}\\) is independent of \\(z_{t-1}\\). Therefore, we can impose independence on the estimated noise term \\(\\hat \\epsilon_{t,i}\\). Consequently, Equation (11) can be further expressed as:\n\\[\\log p(\\hat z_t \\vert \\{\\hat z_{t-1}\\}) = \\log p(\\hat \\epsilon_t,\\mathcal{S}) + \\sum_{i=1}^n \\log \\frac{\\partial \\hat r_i}{\\partial \\hat z_{t,i}}\\](12)\nAssuming a \\(\\tau\\)-order Markov process, the prior \\(p(z_{1:T})\\) can be expressed as \\(p(z_{1:T}) \\triangleq \\prod_{t=\\tau+1}^T p(z_t \\vert \\{z_{t-\\tau}\\})\\). Hence, the log-likelihood \\(\\log p(z_{1:T})\\) can be estimated as:\n\\[\\log p(z_{1:T}) = \\log p(z_{1:T}) + \\sum_{t=\\tau+1}^T \\log p(\\hat \\epsilon_t,\\mathcal{S}) + \\sum_{i=1}^n \\log \\frac{\\partial \\hat r_i}{\\partial \\hat z_{t,i}}\\](13)\nHere, we assume that the noise term \\(p(\\hat \\epsilon_{\\tau,i})\\) and the initial latent variables \\(p(z_{1:\\tau})\\) follow Gaussian distributions. For \\(\\tau = 1\\), this simplifies to:\n\\[\\log p(z_{1:T}) = \\log p(z_{1}) + \\sum_{t=2}^T \\left( \\sum_{i=1}^n \\log p(\\hat \\epsilon_{t,i}) + \\sum_{i=1}^n \\log \\frac{\\partial \\hat r_i}{\\partial \\hat z_{t,i}} \\right)\\](14)\nSimilarly, the distribution \\(p(z_{t+1:T} \\vert z_{1:t})\\) can be estimated using analogous methods. For further details on the derivation of the prior, please refer to Appendix B.\n5.3 Sparsity Constraint for Latent Granger Causality\nBased on the theoretical results, we can identify the latent variables by using the variational-influenced-based architecture and prior estimation networks. However, without any further constraints, it is hard for us to infer the causal structures over latent variables. To address this problem, we propose the sparsity constraint on partial derivatives regarding independent noise and latent variables for the latent Granger Causality. To provide a clearer understanding of its implications, we use a straightforward example with ground truth and estimated generation processes as shown in Figure 3, where the boiled and dashed arrows denote the ground truth and spurious relationships. According to Figure 3, \\(\\hat z_{t,1}\\) and \\(z_{t,1}\\) are generated as follows:\n\\[\\begin{aligned}\\hat z_{t,1} &= f_1(z_{t-1,1}, \\epsilon_{t,1}) \\\\\\hat z_{t,1} &= \\hat f_1(\\hat z_{t-1,1}, \\hat z_{t-1,2}, \\hat \\epsilon_{t,1}),\\end{aligned}\\](15)\nwhere the estimated generation process of \\(\\hat z_{t,1}\\) includes the spurious dependence from \\(\\hat z_{t-1,2}\\). To remove these spurious dependencies, we find that the partial derivatives of ground truth generation process \\(\\frac{\\partial z_{t,1}}{\\partial z_{t-1,2}} = 0\\) and \\(\\frac{\\partial \\hat z_{t,1}}{\\partial \\hat z_{t-1,2}} \\neq 0\\), meaning that the J can provide an intuitive representation of the causal structures among the latent variables, as it"}, {"title": "5.4 Latent Causality Alignment Constraint", "content": "Since the latent structures are stable across different domains, we need to align the latent structures of the source domain and the target domain. However, unlike the structural alignment of the observed variables [16], the alignment of latent variables can be a more challenging task for two main reasons. First, the structure between latent variables is implicit, and the causal structure cannot be directly extracted for alignment as in GCA [16]. In addition, although the partial derivatives regarding estimated noise and latent variables can reflect the causal structure over latent variables, directly aligning the gradient can affect the correct gradient descent direction and result in suboptimal performance downstream tasks. To overcome these challenges, we propose the latent causality alignment constraint.\nSpecifically, we choose a threshold \\(\\mu\\) to determine the causal relations among the latent variables. For instance, if \\(\\vert J_{ij} \\vert > \\mu\\), then there is an edge from \\(\\hat z_{t-1,i}\\) to \\(\\hat z_{t,j}\\), otherwise, no edge is present. Formally, we can obtain the estimated causal relationships of the source or target domains (the superscript * shows the source or target domains) as follows:\n\\[J_{i,j}^* = \\begin{cases}\n1, \\quad \\text{if }\\vert J_{ij}^* \\vert > \\mu; \\\\\n0, \\quad \\text{otherwise.}\n\\end{cases}\\](17)\nA direct solution to align the latent causal structures is to restrict the discrepancy between the latent structures of source and target domains, following [16]. However, since these causal structures are represented using gradients with respect to latent variables, the direct alignment of the gradients can interfere with the model optimization, thereby increasing the difficulty of training. To overcome these issues, we find that it is sufficient to focus on reducing the differences in \\(J^*\\) between the source and target domains while ignoring the identical parts. This approach achieves causal structure alignment while minimizing the impact on the gradients. Based on this idea, we first obtain a masking matrix through an XOR operation. In this matrix, elements with a value of 0 represent identical structures between the source and target domains, while elements with a value of 1 represent differing structures. We then constrain only the differing parts, which are formalized as follows:\n\\[\\begin{aligned}M &= (J^S > \\mu) \\oplus (J^T > \\mu) \\\\\\mathcal{L}_A &= \\Vert C(J^S \\odot M) - J^T \\odot M \\Vert_1,\n\\end{aligned}\\](18)\nwhere \\(\\oplus\\) and \\(\\odot\\) denote the XOR and element-wise product operations, respectively, and \\(C(\\cdot)\\) denotes the gradient-stoping operation [16], which is used to enforce the target latent structures closer to the source latent structures."}, {"title": "6 EXPERIMENTS", "content": "6.1 Domain Adaptation for Time Series Forecasting\n6.1.1 Datasets\nIn this section, we provide an overview of the five real-world datasets used to evaluate the LCA model. PPG-DaLiA\u00b9 is a publicly available multimodal dataset, used for PPG-based heart rate estimation. It includes physiological and motion data collected from 15 volunteers using wrist and chest-worn devices during various activities. We categorize the data into four domains based on activities: Cycling (C), Sitting (S), Working (W), and Driving (D). Human Motion\u00b2 is a dataset for human motion prediction and cross-domain adaptation. We select four motion types as domains: Walking (W), Greeting (G), Eating (E), and Smoking (S). Electricity Load Diagrams\u00b3 is a dataset containing electricity consumption data from 370 substations in Portugal, recorded from January 2011 to December 2014. We account for seasonal domain shifts by dividing the data into four domains based on the months: Domain 1 (January, February, March), Domain 2 (April, May, June), Domain 3 (July, August, September), and Domain 4 (October, November, December). PEMS\u2074 dataset comprises traffic speed data collected by the California Transportation Agencies over a 6-month period from January 1st, 2017 to May 31st, 2017. To account for seasonal domain shifts, we divide the data into three domains based on months: Domain 1 (January), Domain 2 (February), and Domain 3 (March). ETT\u2075 dataset describes the electric power deployment. We use the ETT-small subset, which includes data from two stations, treating each station as a separate domain.\n6.1.2 Baselines\nHere, we introduce the benchmarks for unsupervised domain adaptation in time series forecasting, including SASA [12], AdvSKM [85], DAF [37], GCA [16], CLUDA [86], and Raincoat [10]. In addition, we include approaches that integrate the Gradient Reversal Layer (GRL) with state-of-the-art time series forecasting techniques such as SegRNN [87], TSLANet [88], TimeMixer [89], and iTransformer [90], which are based on RNN, CNN, MLP, and Transformer architectures, respectively. For all the above methods, we ensure consistency by employing the same experimental settings among them.\n6.1.3 Experimental Settings\nWe performed multivariate time series forecasting across all datasets, using an input length of 30 and an output length of 10. Each dataset is partitioned into train, validation, and"}, {"title": "6.2 Domain Adaptation for Time Series Classification", "content": "In classification tasks, we experimented on the UCIHAR and HHAR datasets, following AdaTime [92] framework, which is a benchmarking suite for domain adaptation on time series data. Furthermore, we validate the performance of our method on the high-dimensional video classification datasets.\n6.2.1 Datasets\nTime Series Datasets: We consider the UCIHAR [93] dataset, which comprises sensor data from accelerometers, gyroscopes, and body sensors collected from 30 subjects performing six activities: walking, walking upstairs, walking downstairs, standing, sitting, and lying down. Due to the variability between individuals, each subject is treated as a separate domain. We also consider the HHAR [94] dataset, which contains sensor readings from smartphones and smartwatches, collected from 9 subjects, with each subject similarly treated as an individual domain.\nVideo Datasets: We adopted two widely recognized and frequently used datasets, i.e., UCF101 and HMDB51. The UCF101 dataset, curated by the University of Central Florida, consists of videos primarily sourced from YouTube,"}, {"title": "7 CONCLUSION", "content": "In this paper, we propose a Latent Causality Alignment (LCA) model for time series domain adaptation. By identifying low-dimensional latent variables and reconstructing their causal structures with sparsity constraints, we effectively transfer domain knowledge through latent causal mechanism alignment. The proposed method not only addresses the challenges of modeling latent causal mechanisms in high-dimensional time series data but also guarantees the uniqueness of latent causal structures, providing improved performance on domain-adaptive time series classification and forecasting tasks. However, our method assumes sufficient changes in historical information for latent variable identification. Exploring how to relax this assumption and extend our framework to more complex scenarios would be a promising direction for future work."}]}