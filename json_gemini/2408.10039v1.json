{"title": "MSDiagnosis: An EMR-based Dataset for Clinical Multi-Step Diagnosis", "authors": ["Ruihui Hou", "Shencheng Chen", "Yongqi Fan", "Lifeng Zhu", "Jing Sun", "Jingping Liu", "Tong Ruan"], "abstract": "Clinical diagnosis is critical in medical practice, typically requiring a continuous and evolving process that includes primary diagnosis, differential diagnosis, and final diagnosis. However, most existing clinical diagnostic tasks are single-step processes, which does not align with the complex multi-step diagnostic procedures found in real-world clinical settings. In this paper, we propose a multi-step diagnostic task and annotate a clinical diagnostic dataset (MSDiagnosis). This dataset includes primary diagnosis, differential diagnosis, and final diagnosis questions. Additionally, we propose a novel and effective framework. This framework combines forward inference, backward inference, reflection, and refinement, enabling the LLM to self-evaluate and adjust its diagnostic results. To assess the effectiveness of our proposed method, we design and conduct extensive experiments. The experimental results demonstrate the effectiveness of the proposed method. We also provide a comprehensive experimental analysis and suggest future research directions for this task.", "sections": [{"title": "Introduction", "content": "Electronic Medical Records (EMRs) digitally document comprehensive information about the entire diagnostic and treatment process of a patient (Bates et al., 2003). This encompasses clinical data such as hospital courses, clinical notes, and diagnostic results (Keyhani et al., 2008). The clinical notes in the EMRs are medical texts written by doctors during consultations to address the patient's medical history, chief complaints, and examinations. Through these records, doctors can use their expertise to make a diagnosis for the patient (Toma\u0161ev et al., 2021). The clinical diagnosis process is a continuous and evolving process (Tiffen et al., 2014). Specifically, doctors make primary and differential diagnoses based on the patient's medical history, chief complaint, and laboratory and aided examination. The final diagnosis is then determined by considering clinical changes throughout the diagnosis and treatment process. However, a significant gap remains between the existing diagnostic dataset and actual clinical practice. Most existing diagnoses are single-step processes (Wang et al., 2024) (Lyu et al., 2023) (Liu et al., 2024a), where a diagnosis is made directly based on the patient's medical history, chief complaint, and examination results. This single-step approach does not align with the multi-step process typically used in clinical work. Hence, in this paper, we propose a multi-step diagnostic task and annotate a clinical diagnosis dataset called ReaDiagnosis. In the multi-step di-"}, {"title": "Problem Formulation", "content": "In this paper, we consider the multi-step diagnosis task as a multi-round dialogue problem. For each complex EMR, we simulate the interaction between an examiner and a candidate, where the candidate is required to provide an answer A to a specific diagnostic question Q. Specifically, given a patient's admission record E, which includes the chief complaint, present history, past history, physical examination, and laboratory and aided examination, the candidate answers two questions R1 = [Q1, Q2] in the first round of dialogue. These questions mainly involve the patient's primary diagnosis and its diagnostic criteria. As illustrated in Fig. 3, Q1 and Q2 are \"What is the patient's primary diagnosis?\u201d and \u201cWhat is the criterion for the primary diagnosis?\" respectively. In the second round of dialogue, the candidate needs to answer the question R2 = [Q3]. This question refers to asking the patient about their differential diagnosis. In the third round of dialogue, the candidate receives the patient's hospital course T, the dialogue history H, and two additional questions R3 = [Q4, Q5]. These questions focus on the pa-"}, {"title": "Data Construction", "content": "In this section, we detail the construction process of the ReaDiagnosis dataset, including the data collection, data pre-processing, and data annotation."}, {"title": "Data Collection and Pre-processing", "content": "In this study, we select a Chinese medical website\u00b9 as the source of EMRs. This website is an open-source medical platform that contains a large number of real EMRs. After de-identifying the data, we obtain a total of 11,900 EMRs. To ensure high-quality EMR, we preprocess the data through three steps. First, we remove redundant information unrelated to medical content, such as HTML web elements, copyright statements, and advertisements. Second, we eliminate EMRs with duplicate field values and missing fields, such as those lacking primary diagnosis, differential diagnosis, final diagnosis, or hospital course fields. Third, we deduplicate EMRs from the same department. If two records have identical chief complaints, present histories, and physical examinations, one of them is removed. After data preprocessing, we ultimately obtain 3,501 high-quality, complex, and authentic EMRs."}, {"title": "Data Annotation", "content": "We first manually construct five questions, including the patient's primary diagnosis, the criterion for the primary diagnosis, the differential diagnosis, the final diagnosis, and the criterion for the final diagnosis. The definition of each question is shown in Table 7 in Appendix D. Then, we use GPT-4 to expand each constructed question with ten similar questions. Finally, we manually check and filter out unreasonable question expressions. When constructing EMR questions, we randomly select from these similar questions to enhance the diversity of the questions."}, {"title": "Answer Annotation", "content": "To ensure the quality of the dataset construction, we form a professional team comprising three inspectors and one reviewer. All team members undergo specialized medical training to understand primary diagnosis, differential diagnosis, and final diagnosis. The construction process includes three stages: the first round of annotation, the second round of inspection, and the third round of review.\nThe first round of annotation. For the primary diagnosis, differential diagnosis, and final diagnosis, we directly take the values from the corresponding fields in the EMR as the standard answers for the respective diagnostic questions. For the criterion for the primary and final diagnosis, we use GPT-4 to generate these diagnostic criteria.\nThe second round of inspection. We engage three university students to simultaneously check the rationality of all question-answer pairs. Samples that all three inspectors considered unreasonable are directly discarded. If one or two inspectors find a sample unreasonable, it is manually reannotated and retained only if all three inspectors subsequently agree on its reasonableness.\nThe third round of review. A verified batch is given to a senior doctor for double review. Medical experts randomly check 20% of the batch. Unqualified annotations are returned to the inspection team, which can further refine the standards, thus standardizing the construction team's work. This process is repeated until the batch accuracy reaches 95%."}, {"title": "Scoring Point Annotation", "content": "To more accurately evaluate open-ended questions such as primary diagnostic criteria and final diagnostic criteria, we annotate the score point of the answers to these questions. Specifically, for each primary and final diagnostic criteria, we first construct score point extraction prompts and employ GPT-4 to extract the information from the standard answers. The score point for the diagnostic criteria mainly includes four categories: medical history, symptoms, physical signs, and test results. Then, we invite two students with specialized medical training to validate the extracted information. If any inconsistencies are found in the EMRs, the data is re-annotated. If a sample remains unreasonable after three attempts, it is discarded. Finally, we select the top 2,225 high-quality samples to form the ReaDiagnosis. We calculate the Cohen's Kappa (Banerjee et al., 1999) score for the key in-"}, {"title": "Dataset Analysis", "content": "In this section, we introduce the statistics of ReaDiagnosis and the characteristics of the ReaDiagnosis in detail."}, {"title": "Data statistics", "content": "As reported in Table 2, the ReaDiagnosis dataset contains a total of 2,225 EMRs covering 12 departments, which are divided into training, validation, and test sets according to a 7:1:2 ratio. Addition-"}, {"title": "Data Characteristics", "content": "The ReaDiagnosis dataset has several significant advantages compared to previous medical diagnostic datasets: 1) Reliability and Authenticity of Data: The ReaDiagnosis dataset is entirely based on real patient EMRs, including detailed treatment plans and course records. This real clinical data source ensures high reliability and authenticity, which helps simulate the actual medical environment more accurately. 2) Rich Variety of Departments and Diseases: The ReaDiagnosis dataset covers patient EMRs from 12 different departments and includes various types of diseases, such as common diseases, rare diseases, acute diseases, and chronic diseases. This wide coverage of departments and diverse disease types ensures the broad applicability of the data. 3) Inclusion of Multi-step Diagnostic Processes: This multi-step diagnostic process is more in line with actual clinical diagnostic scenarios. By recording diagnostic changes, the dataset better reflects the complex diagnostic paths and dynamic changes in real-world medical practice."}, {"title": "Methods", "content": "In this section, we first provide an overview of our framework. Then, we describe each module of the framework in detail."}, {"title": "Framework", "content": "As illustrated in Fig. 3, our proposed framework mainly consists of two stages. The first stage involves forward inference. In this stage, we retrieve similar EMRs to serve as in-context learning (ICL) for the diagnosis, allowing the LLM to diagnose the patient. The second stage is backward inference and reflection. In this stage, we first validate the diagnostic criteria against the facts derived from the diagnostic results. Then, the LLM uses designed reflection rules to reflect on the diagnostic outcomes. Finally, the diagnosis is optimized by integrating all the previous results."}, {"title": "Forward Inference", "content": "This part aims to make diagnoses for patients based on admission records. In this paper, we utilize an LLM (e.g., GPT40-mini) with the ICL method to achieve this purpose. Its core idea is to select similar EMRs from the training set, guiding the model in making accurate diagnoses for patients.\nSpecifically, given the admission record E and question Qo, we select ICL examples with similar semantics to E through the following two steps, where o is the total number of questions. First, we utilize the BGE (Xiao et al., 2023) model to obtain the representations of E and each sample Yi (1 \u2264 i \u2264 u) in the training set, where u is the total number of training set. Second, we calculate their cosine similarity and select the top K samples with the highest similarity as ICL examples. In addition to using E and the top K samples as input to the LLM, we introduce a role definition and a predefined rule. An illustrative example of this"}, {"title": "Backward Inference and Reflection", "content": "After obtaining the forward inference results, this part aims to conduct backward inference, reflection, and refine the patient's diagnosis. In this paper, we design specific rules to achieve this goal.\nSpecifically, given the forward inference diagnostic results ao, we first perform backward inference from the diagnosis to the diagnostic criteria. In this step, we define backward inference rules that aim to guide the LLM to generate output that complies with pre-defined content and format constraints, as shown in Fig. 7 in Appendix I. For content constraints, we have \u201cFor each diagnosis in the diagnostic result, recall the representative medical history, symptoms, physical signs, and auxiliary examination results for the disease\" (Rule 1). For format constraints, we consider \u201cThe recalled content should be in the following format: Medical History: <Recall the representative medical history for the disease; delete this item if not applicable>..."}, {"title": "Experiments", "content": "In this section, we first conduct extensive experiments to evaluate the effectiveness of our proposed framework. Next, we provide a detailed analysis to offer deeper insights into our framework."}, {"title": "Experimental Setup", "content": ""}, {"title": "Baseline", "content": "In this paper, we mainly describe several types of baseline methods, including open-source Medical LLMs, open-source General LLMs, closed-source LLMs, and other methods. The specific settings are shown in the Appendix A."}, {"title": "Evaluation Details", "content": "For different types of problems, we introduce various evaluation metrics. Specifically, for primary diagnosis, differential diagnosis, and final diagnosis, we introduce entity F1 (Liu et al., 2022) as the evaluation metric. For primary and final diagnosis criteria, we employ two types of evaluation metrics. First, we adapt Rouge - L (Lin, 2004) and BLEU-1 (Papineni et al., 2002) to measure the similarity between the generated text and the reference text. Second, we introduce the Macro - Recall metric, which is calculated based on the key information in the answers. More details are shown in Appendix B."}, {"title": "Implementation Details", "content": "For all the open-source models mentioned above, we use their default hyperparameters. The further implementation details are listed in Appendix C."}, {"title": "Main Results", "content": "To verify the effectiveness of our proposed method, we compare it with all baselines on the ReaDiagnosis test set. The results are reported in Table 3. From the table, we conclude that: 1) All LLMs perform poorly on ReaDiagnosis, with significant room for improvement compared to the human final F1 of 95.11%. For diagnostic-type questions, the best-performing model, GLM with LoRA, achieves a primary F1 score of 38.78% and a final F1 score of 35.00%. For diagnostic criteria questions, the best-performing model, Baichuan2 with LoRA, reaches a primary Macro - Recall of 51.15% and a final Macro - Recall of 52.58%. 2) Our method outperforms all baselines in the Macro \u2013 Recall, Rouge - L, and Blue-1 metrics, demonstrating the effectiveness of the proposed approach. 3) Among open-source General LLMs, the GLM and Baichuan models, after instruction fine-tuning, outperform those without instruction fine-tuning in diagnostic reasoning. Specifically, after instruction fine-tuning, the GLM shows improvements of 3.74% and 2.4% in Pre-F1 and Fin F1 metrics, respectively. However, the performance of the LLaMA model declines after instruction fine-tuning, with Pre \u2013 F1 and Fin - F1 metrics decreasing by 12.13% and 3.67%, respectively. This decline may be due to the relatively small proportion of Chinese in LLaMA's training corpus, which weakens the model's Chinese language capabilities, and direct instruction fine-tuning may further reduce its performance."}, {"title": "Detailed Analysis", "content": ""}, {"title": "Ablation Study", "content": "In this section, we design two ablation experiments to analyze the multi-stage diagnostic process of ReaDiagnosis and our framework respectively. Specifically, for the ablation of the multi-stage diagnostic process, we sequentially remove the primary diagnosis, differential diagnosis, and both to evaluate the effectiveness of the final diagnosis. It is important to note that in this experiment, we employ a method where the LLM performs direct reasoning. The experimental results are shown in Table 5. For the ablation of our framework, we sequentially remove backward inference, reflection, and refinement, and assess the effectiveness of the remaining combinations. The results are presented in Table 4.\nFrom the Table 5, we conclude that: 1) The primary diagnosis significantly influences the performance of the final diagnosis. This could be due to errors in the primary diagnosis propagating through the multi-step diagnostic process, leading to a reduction in the effectiveness of the final diagnosis. 2) The multi-step diagnostic process helps improve the interpretability of the final diagnosis. Specifically, by introducing a multi-step diagnosis, the final diagnosis is based on more criteria, thereby enhancing its interpretability. From the Table 4, we have the following observations: The removal of components backward inference, reflection, and refinement leads to a decline in performance, illus-"}, {"title": "Case Study", "content": "To further investigate the limitations of our method, we analyze 100 error samples generated in ReaDiagnosis. After manual classification, the errors are categorized into four types: (a) lack of domain knowledge (42%), (b) Symptoms or examination results are confused with the disease. (30%), (c) diagnostic basis inconsistent with facts (18%), and (d) other (10%). To illustrate these error types more intuitively, we provide examples as shown in Fig. 4 in appendix H."}, {"title": "Related Work", "content": "The related work in this paper can be categorized into two groups: clinical diagnosis on EMRs and prompting strategies of LLM."}, {"title": "Clinical Diagnosis on EMRs", "content": "Clinic diagnosis based on EMRs is crucial for improving healthcare quality and patient outcomes. However, existing datasets for EMR-based diagnosis primarily focus on single-step diagnosis, with"}, {"title": "Prompting Strategies of LLM", "content": "With the development of LLMs, many researchers have applied these models to medical downstream tasks. In these tasks, specific problems are typically addressed by designing prompts. These methods can be categorized into three types: Input-Output(IO) prompting, CoT prompting, and the DAC paradigm. IO prompting (Yao et al., 2024) is a standard prompting strategy where input is combined with instructions and/or a few in-context learning(ICL) examples to generate a response. CoT prompting (Wei et al., 2022) aims to emulate the step-by-step thought process humans use to tackle complex tasks, such as combinatorial reasoning and mathematical calculations. There are also various CoT variants, such as CoT with self-consistency (CoT-SC) prompting (Wang et al., 2022), designed to address the limitations of CoT in exploration. The DAC paradigm (Zhang et al., 2024) mainly refers to simply breaking down the input sequence into multiple sub-inputs to enhance LLM performance on certain specific tasks. Although these methods have shown promising results in some reasoning tasks, this reasoning approach is challenging to directly apply to medical diagnostic reasoning tasks."}, {"title": "Conclusion and Limitations", "content": "We propose ReaDiagnosis, a multi-step diagnostic reasoning dataset collected and annotated from medical websites. ReaDiagnosis addresses the weaknesses of existing datasets by constructing multi-stage diagnostic questions that better align with actual clinical diagnostic scenarios. On this dataset, we propose a simple and effective framework. We implement a series of closed-source and open-source LLMs and conduct extensive experimental analyses. The results demonstrate that ReaDiagnosis is a challenging dataset worthy of further exploration. Moreover, the method proposed in this paper shows effectiveness on this dataset. However, the dataset has a limitation: due to limited data sources, our medical records dataset exhibits an uneven distribution across different departments. This issue can be addressed through machine learning methods (Cao et al., 2019) and data sampling strategies (Chawla et al., 2002)."}, {"title": "Ethical Statement", "content": "Our data source adheres to ethical guidelines and respects copyright laws. The entire data collection process is free from copyright and privacy concerns. In addition, we have meticulously reviewed our dataset to ensure it does not contain"}, {"title": "Details of the Baseline", "content": "In the open-source Medical LLMs, we employ MMedLM (Qiu et al., 2024), PULSE (Xiaofan Zhang, 2023), and Lmama3-OpenBioLLM (Ankit Pal, 2024) for comparison. Based on these models, we manually construct an example to serve as ICL. In the open-source General LLMs, we use Llama (Touvron et al., 2023), glm (GLM et al., 2024), and Baichuan (Yang et al., 2023) for comparison. Based on these models, we design two settings: 1-shot reasoning and instruction tuning. In the first setting, we utilize the previously constructed example as ICL. In the second setting, we use parameter-efficient fine-tuning with LoRA (Hu et al., 2021). The instruction data is generated by transforming the input and output from the training data. In the closed-source LLMs, we employ two settings: 1-shot and prompting methods, In the first approach, we utilize the same example as previously used. In the second setting, we consider comparing method COT (Wei et al., 2022) and method DAC (Zhang et al., 2024). COT (Wei et al., 2022) uses \"Let's think step by step\" to enhance the model's reasoning ability for task-solving. DAC (Zhang et al., 2024) adopts a simple divide-and-conquer prompting strategy, where the input sequence is simply divided into multiple sub-inputs, which can enhance the reasoning performance of the LLM. In other methods, We mainly consider manual answering methods. Specifically, we randomly select 100 samples from ReaDiagnosis and invite one college student (different from the annotation team in Section 3.2.2) to answer the questions."}, {"title": "Details of the Evaluation Metrics", "content": "For the disease entities in the diagnosis results D and the reference results R in the medical records, we compute the edit distance to associate these entities with ICD-10 terms, thereby mapping D and R to two standardized disease sets, Sa and Sr, respectively. We then compute the entity F1 score based on Sa and Sr.\nFor primary and final diagnosis criteria, we employ two types of evaluation metrics. First, we adapt Rouge \u2013 L (Lin, 2004) and BLEU-1 (Papineni et al., 2002) to measure the longest matching sequence between the generated text and the reference text, capturing the similarity in the overall structure of the two sequences. Second, we introduce the Macro - Recall metric, which is calculated based on the key information in the answers. For our predefined N scoring point categories, the calculation method for key information Macro - Recall is as follows:\nMarco Recall = 1/N \u2211 (Recalli), \nwhere Recalli represents the recall rate of the i category."}, {"title": "Details of Implementation", "content": "To enhance the stability and reliability of the experimental results and reduce the impact of random factors, we conduct each experiment three times and then calculate the average of three results. For the backbone model, we utilize the OpenAI API, specifying the model as \u201cGPT4o-mini\". We set the top_p parameter to 0.01, and all other hyperparameters of the OpenAI API are maintained at default values. When selecting similar examples, we set K to 1.\""}, {"title": "Question Definition", "content": "In this section, we introduce in detail the definition of the diagnostic questions corresponding to each EMR. In ReaDiagnosis, the questions corresponding to each case are initially constructed manually and then expanded using GPT-4. The definitions of each question are shown in Table 7."}, {"title": "The Impact of Different Departments on the Framework", "content": "In this section, to evaluate the impact of different departments on our framework, we conduct an analysis of the department-specific results. The detailed experimental outcomes are presented in Table 6. From this analysis, we can draw the following conclusions: Different departments have different influences on the framework. The Oncology and Traditional Chinese Medicine departments have a significant impact on our framework. Specifically, the final F1 score in the Oncology department is only 5.89%."}, {"title": "The Impact of the Number of Diagnosed Diseases on the Framework", "content": "In this section, we primarily analyze the impact of the number of diseases on the framework. In this experiment, we categorize the number of diseases into three levels: 1 to 5 diseases, 5 to 10"}, {"title": "The Impact of the Number of ICL on the Framework", "content": "In this section, to analyze the impact of the number of ICL examples on the method's performance, we introduced varying numbers of ICL examples into the framework for comparison. In this experiment, when the number of ICL examples reaches 4, the context length exceeds the model's token limit, so we compared experiments with fewer than 4 ICL examples. The specific experimental results are shown in Table 9. As observed from the table, the performance of the framework gradually improves as the number of ICL examples increases."}, {"title": "Error Case", "content": "In this section, we present an error example in Fig. 4 to facilitate a better understanding of the error categories. The details are as follows: 1) Lack of domain knowledge (42%): For example, based on the patient's medical history and examination results, the model fails to accurately predict the patient's disease. This error is likely due to the model's limited domain knowledge, which leads to the model not accurately predicting the disease. 2) Symptoms or examination results are confused with the disease (30%): For example, \u201cIntracranial Space-Occupying Lesion\u201d and \u201cTinnitus\u201d are the patient's examination results and symptoms, respectively. However, the model predicts them as diseases. 3) Diagnostic basis inconsistent with facts (18%): For example, the model incorrectly extracts the patient's blood pressure as 140/80."}, {"title": "The Prompt used in our Framework", "content": "In this section, we primarily introduce the prompts corresponding to the four instances where the LLM is used within the framework. The prompt for forward reasoning is shown in Fig. 5. The prompt for backward reasoning is shown in Fig. 7. The prompt for reflecting on the diagnostic results is shown in Fig. 8. The prompt for optimizing the diagnostic results is shown in Fig. 6."}]}