{"title": "MSDiagnosis: An EMR-based Dataset for Clinical Multi-Step Diagnosis", "authors": ["Ruihui Hou", "Shencheng Chen", "Yongqi Fan", "Lifeng Zhu", "Jing Sun", "Jingping Liu", "Tong Ruan"], "abstract": "Clinical diagnosis is critical in medical practice, typically requiring a continuous and evolving process that includes primary diagnosis, differential diagnosis, and final diagnosis. However, most existing clinical diagnostic tasks are single-step processes, which does not align with the complex multi-step diagnostic procedures found in real-world clinical settings. In this paper, we propose a multi-step diagnostic task and annotate a clinical diagnostic dataset (MSDiagnosis). This dataset includes primary diagnosis, differential diagnosis, and final diagnosis questions. Additionally, we propose a novel and effective framework. This framework combines forward inference, backward inference, reflection, and refinement, enabling the LLM to self-evaluate and adjust its diagnostic results. To assess the effectiveness of our proposed method, we design and conduct extensive experiments. The experimental results demonstrate the effectiveness of the proposed method. We also provide a comprehensive experimental analysis and suggest future research directions for this task.", "sections": [{"title": "1 Introduction", "content": "Electronic Medical Records (EMRs) digitally document comprehensive information about the entire diagnostic and treatment process of a patient (Bates et al., 2003). This encompasses clinical data such as hospital courses, clinical notes, and diagnostic results (Keyhani et al., 2008). The clinical notes in the EMRs are medical texts written by doctors during consultations to address the patient's medical history, chief complaints, and examinations. Through these records, doctors can use their expertise to make a diagnosis for the patient (Toma\u0161ev et al., 2021).\nThe clinical diagnosis process is a continuous and evolving process (Tiffen et al., 2014). Specifically, doctors make primary and differential diagnoses based on the patient's medical history, chief complaint, and laboratory and aided examination. The final diagnosis is then determined by considering clinical changes throughout the diagnosis and treatment process. However, a significant gap remains between the existing diagnostic dataset and actual clinical practice. Most existing diagnoses are single-step processes (Wang et al., 2024) (Lyu et al., 2023) (Liu et al., 2024a), where a diagnosis is made directly based on the patient's medical history, chief complaint, and examination results. This single-step approach does not align with the multi-step process typically used in clinical work.\nHence, in this paper, we propose a multi-step diagnostic task and annotate a clinical diagnosis dataset called ReaDiagnosis. In the multi-step di-"}, {"title": "2 Problem Formulation", "content": "In this paper, we consider the multi-step diagnosis task as a multi-round dialogue problem. For each complex EMR, we simulate the interaction between an examiner and a candidate, where the candidate is required to provide an answer A to a specific diagnostic question Q. Specifically, given a patient's admission record E, which includes the chief complaint, present history, past history, physical examination, and laboratory and aided examination, the candidate answers two questions R1 = [Q1, Q2] in the first round of dialogue. These questions mainly involve the patient's primary diagnosis and its diagnostic criteria. As illustrated in Fig. 3, Q1 and Q2 are \"What is the patient's primary diagnosis?\u201d and \u201cWhat is the criterion for the primary diagnosis?\" respectively. In the second round of dialogue, the candidate needs to answer the question R2 = [Q3]. This question refers to asking the patient about their differential diagnosis. In the third round of dialogue, the candidate receives the patient's hospital course T, the dialogue history H, and two additional questions R3 = [Q4, Q5]. These questions focus on the pa-"}, {"title": "3 Data Construction", "content": "In this section, we detail the construction process of the ReaDiagnosis dataset, including the data collection, data pre-processing, and data annotation."}, {"title": "3.1 Data Collection and Pre-processing", "content": "In this study, we select a Chinese medical website\u00b9 as the source of EMRs. This website is an open-source medical platform that contains a large number of real EMRs. After de-identifying the data, we obtain a total of 11,900 EMRs. To ensure high-quality EMR, we preprocess the data through three steps. First, we remove redundant information unrelated to medical content, such as HTML web elements, copyright statements, and advertisements. Second, we eliminate EMRs with duplicate field values and missing fields, such as those lacking primary diagnosis, differential diagnosis, final diagnosis, or hospital course fields. Third, we deduplicate EMRs from the same department. If two records have identical chief complaints, present histories, and physical examinations, one of them is removed. After data preprocessing, we ultimately obtain 3,501 high-quality, complex, and authentic EMRs."}, {"title": "3.2 Data Annotation", "content": ""}, {"title": "3.2.1\nQuestion Construction", "content": "We first manually construct five questions, including the patient's primary diagnosis, the criterion for the primary diagnosis, the differential diagnosis, the final diagnosis, and the criterion for the final diagnosis. The definition of each question is shown in Table 7 in Appendix D. Then, we use GPT-4 to expand each constructed question with ten similar questions. Finally, we manually check and filter out unreasonable question expressions. When constructing EMR questions, we randomly select from these similar questions to enhance the diversity of the questions."}, {"title": "3.2.2\nAnswer Annotation", "content": "To ensure the quality of the dataset construction, we form a professional team comprising three inspectors and one reviewer. All team members undergo specialized medical training to understand primary diagnosis, differential diagnosis, and final diagnosis. The construction process includes three stages: the first round of annotation, the second round of inspection, and the third round of review.\nThe first round of annotation. For the primary diagnosis, differential diagnosis, and final diagnosis, we directly take the values from the corresponding fields in the EMR as the standard answers for the respective diagnostic questions. For the criterion for the primary and final diagnosis, we use GPT-4 to generate these diagnostic criteria.\nThe second round of inspection. We engage three university students to simultaneously check the rationality of all question-answer pairs. Samples that all three inspectors considered unreasonable are directly discarded. If one or two inspectors find a sample unreasonable, it is manually re-annotated and retained only if all three inspectors subsequently agree on its reasonableness.\nThe third round of review. A verified batch is given to a senior doctor for double review. Medical experts randomly check 20% of the batch. Unqualified annotations are returned to the inspection team, which can further refine the standards, thus standardizing the construction team's work. This process is repeated until the batch accuracy reaches 95%."}, {"title": "3.2.3 Scoring Point Annotation", "content": "To more accurately evaluate open-ended questions such as primary diagnostic criteria and final diagnostic criteria, we annotate the score point of the answers to these questions. Specifically, for each primary and final diagnostic criteria, we first construct score point extraction prompts and employ GPT-4 to extract the information from the standard answers. The score point for the diagnostic criteria mainly includes four categories: medical history, symptoms, physical signs, and test results. Then, we invite two students with specialized medical training to validate the extracted information. If any inconsistencies are found in the EMRs, the data is re-annotated. If a sample remains unreasonable after three attempts, it is discarded. Finally, we select the top 2,225 high-quality samples to form the ReaDiagnosis. We calculate the Cohen's Kappa (Banerjee et al., 1999) score for the key in-"}, {"title": "4 Dataset Analysis", "content": "In this section, we introduce the statistics of ReaDiagnosis and the characteristics of the ReaDiagnosis in detail."}, {"title": "4.1 Data statistics", "content": "As reported in Table 2, the ReaDiagnosis dataset contains a total of 2,225 EMRs covering 12 departments, which are divided into training, validation, and test sets according to a 7:1:2 ratio. Additionally, we conduct a detailed statistical analysis of patients' diagnoses from three main perspectives. 1) EMRs where the primary diagnosis matches the final diagnosis, and EMRs where the primary diagnosis differs from the final diagnosis. Fig. 2 shows a detailed visualization of the statistical data. In the category where the primary and final diagnoses are the same, surgery has the highest proportion (approximately 48.02%). In the category where the primary and final diagnoses are different, internal medicine has the highest proportion (approximately 31.64%). 2) Number of Diseases: The average number of diseases diagnosed at the primary diagnosis stage is 2, with a maximum of 21 diseases. At the final diagnosis stage, the average number of diseases is 3, with a maximum of 21 diseases. 3) Types of diagnostic changes: Through statistical analysis of diagnosis changes, we find that changes mainly fall into three categories: addition, deletion, and modification."}, {"title": "4.2 Data Characteristics", "content": "The ReaDiagnosis dataset has several significant advantages compared to previous medical diagnostic datasets: 1) Reliability and Authenticity of Data: The ReaDiagnosis dataset is entirely based on real patient EMRs, including detailed treatment plans and course records. This real clinical data source ensures high reliability and authenticity, which helps simulate the actual medical environment more accurately. 2) Rich Variety of Departments and Diseases: The ReaDiagnosis dataset covers patient EMRs from 12 different departments and includes various types of diseases, such as common diseases, rare diseases, acute diseases, and chronic diseases. This wide coverage of departments and diverse disease types ensures the broad applicability of the data. 3) Inclusion of Multi-step Diagnostic Processes: This multi-step diagnostic process is more in line with actual clinical diagnostic scenarios. By recording diagnostic changes, the dataset better reflects the complex diagnostic paths and dynamic changes in real-world medical practice."}, {"title": "5 Methods", "content": "In this section, we first provide an overview of our framework. Then, we describe each module of the framework in detail."}, {"title": "5.1 Framework", "content": "As illustrated in Fig. 3, our proposed framework mainly consists of two stages. The first stage involves forward inference. In this stage, we retrieve similar EMRs to serve as in-context learning (ICL) for the diagnosis, allowing the LLM to diagnose the patient. The second stage is backward inference and reflection. In this stage, we first validate the diagnostic criteria against the facts derived from the diagnostic results. Then, the LLM uses designed reflection rules to reflect on the diagnostic outcomes. Finally, the diagnosis is optimized by integrating all the previous results."}, {"title": "5.2 Forward Inference", "content": "This part aims to make diagnoses for patients based on admission records. In this paper, we utilize an LLM (e.g., GPT40-mini) with the ICL method to achieve this purpose. Its core idea is to select similar EMRs from the training set, guiding the model in making accurate diagnoses for patients.\nSpecifically, given the admission record E and question Qo, we select ICL examples with similar semantics to E through the following two steps, where o is the total number of questions. First, we utilize the BGE (Xiao et al., 2023) model to obtain the representations of E and each sample Y\u00bf (1 \u2264 i \u2264 u) in the training set, where u is the total number of training set. Second, we calculate their cosine similarity and select the top K samples with the highest similarity as ICL examples. In addition to using E and the top K samples as input to the LLM, we introduce a role definition and a predefined rule. An illustrative example of this process is presented in Fig. 5 in Appendix I. The role definition is phrased as \u201cYou are a professional doctor, and you need to complete a task related to diagnosis\". The rules for constraining the output format of the LLM refer to \"The diagnostic results should be in the following format, so they can be directly loaded using the JSON.load() function\". Finally, the LLM would generate the answer ao corresponding to question Qo."}, {"title": "5.3 Backward Inference and Reflection", "content": "After obtaining the forward inference results, this part aims to conduct backward inference, reflection, and refine the patient's diagnosis. In this paper, we design specific rules to achieve this goal.\nSpecifically, given the forward inference diagnostic results ao, we first perform backward inference from the diagnosis to the diagnostic criteria. In this step, we define backward inference rules that aim to guide the LLM to generate output that complies with pre-defined content and format constraints, as shown in Fig. 7 in Appendix I. For content constraints, we have \u201cFor each diagnosis in the diagnostic result, recall the representative medical history, symptoms, physical signs, and auxiliary examination results for the disease\" (Rule 1). For format constraints, we consider \"The recalled content should be in the following format: Medical History: <Recall the representative medical history for the disease; delete this item if not applicable>...\" (Rule 2). Then, based on the results of the backward inference, we design reflection rules to review the diagnostic results, as shown in Fig. 8 in Appendix I. Finally, we combine the aforementioned"}, {"title": "6 Experiments", "content": "In this section, we first conduct extensive experiments to evaluate the effectiveness of our proposed framework. Next, we provide a detailed analysis to offer deeper insights into our framework."}, {"title": "6.1 Experimental Setup", "content": ""}, {"title": "6.1.1 Baseline", "content": "In this paper, we mainly describe several types of baseline methods, including open-source Medical LLMs, open-source General LLMs, closed-source LLMs, and other methods. The specific settings are shown in the Appendix A."}, {"title": "6.1.2\nEvaluation Details", "content": "For different types of problems, we introduce various evaluation metrics. Specifically, for primary diagnosis, differential diagnosis, and final diagnosis, we introduce entity F1 (Liu et al., 2022) as the evaluation metric. For primary and final diagnosis criteria, we employ two types of evaluation metrics. First, we adapt Rouge - L (Lin, 2004) and BLEU-1 (Papineni et al., 2002) to measure the similarity between the generated text and the reference text. Second, we introduce the Macro - Recall metric, which is calculated based on the key information in the answers. More details are shown in Appendix B."}, {"title": "6.1.3 Implementation Details", "content": "For all the open-source models mentioned above, we use their default hyperparameters. The further implementation details are listed in Appendix C."}, {"title": "6.2 Main Results", "content": "To verify the effectiveness of our proposed method, we compare it with all baselines on the ReaDiagnosis test set. The results are reported in Table 3.\nFrom the table, we conclude that: 1) All LLMs perform poorly on ReaDiagnosis, with significant room for improvement compared to the human final F1 of 95.11%. For diagnostic-type questions, the best-performing model, GLM with LoRA, achieves a primary F1 score of 38.78% and a final F1 score of 35.00%. For diagnostic criteria questions, the best-performing model, Baichuan2 with LoRA, reaches a primary Macro - Recall of 51.15% and a final Macro - Recall of 52.58%. 2) Our method outperforms all baselines in the Macro \u2013 Recall, Rouge - L, and Blue-1 metrics, demonstrating the effectiveness of the proposed approach. 3) Among open-source General LLMs, the GLM and Baichuan models, after instruction fine-tuning, outperform those without instruction fine-tuning in diagnostic reasoning. Specifically, after instruction fine-tuning, the GLM shows improvements of 3.74% and 2.4% in Pre-F1 and Fin - F1 metrics, respectively. However, the performance of the LLaMA model declines after instruction fine-tuning, with Pre \u2013 F1 and Fin - F1 metrics decreasing by 12.13% and 3.67%, respectively. This decline may be due to the relatively small proportion of Chinese in LLaMA's training corpus, which weakens the model's Chinese language capabilities, and direct instruction fine-tuning may further reduce its performance."}, {"title": "6.3 Detailed Analysis", "content": ""}, {"title": "6.3.1 Ablation Study", "content": "In this section, we design two ablation experiments to analyze the multi-stage diagnostic process of ReaDiagnosis and our framework respectively. Specifically, for the ablation of the multi-stage diagnostic process, we sequentially remove the primary diagnosis, differential diagnosis, and both to evaluate the effectiveness of the final diagnosis. It is important to note that in this experiment, we employ a method where the LLM performs direct reasoning. The experimental results are shown in Table 5. For the ablation of our framework, we sequentially remove backward inference, reflection, and refinement, and assess the effectiveness of the remaining combinations. The results are presented in Table 4.\nFrom the Table 5, we conclude that: 1) The primary diagnosis significantly influences the performance of the final diagnosis. This could be due to errors in the primary diagnosis propagating through the multi-step diagnostic process, leading to a reduction in the effectiveness of the final diagnosis. 2) The multi-step diagnostic process helps improve the interpretability of the final diagnosis. Specifically, by introducing a multi-step diagnosis, the final diagnosis is based on more criteria, thereby enhancing its interpretability. From the Table 4, we have the following observations: The removal of components backward inference, reflection, and refinement leads to a decline in performance, illus-"}, {"title": "6.3.2 Case Study", "content": "To further investigate the limitations of our method, we analyze 100 error samples generated in ReaDiagnosis. After manual classification, the errors are categorized into four types: (a) lack of domain knowledge (42%), (b) Symptoms or examination results are confused with the disease. (30%), (c) diagnostic basis inconsistent with facts (18%), and (d) other (10%). To illustrate these error types more intuitively, we provide examples as shown in Fig. 4 in appendix H."}, {"title": "7 Related Work", "content": "The related work in this paper can be categorized into two groups: clinical diagnosis on EMRs and prompting strategies of LLM."}, {"title": "7.1 Clinical Diagnosis on EMRs", "content": "Clinic diagnosis based on EMRs is crucial for improving healthcare quality and patient outcomes. However, existing datasets for EMR-based diagnosis primarily focus on single-step diagnosis, with"}, {"title": "7.2 Prompting Strategies of LLM", "content": "With the development of LLMs, many researchers have applied these models to medical downstream tasks. In these tasks, specific problems are typically addressed by designing prompts. These methods can be categorized into three types: InputOutput(IO) prompting, CoT prompting, and the DAC paradigm. IO prompting (Yao et al., 2024) is a standard prompting strategy where input is combined with instructions and/or a few in-context learning(ICL) examples to generate a response. CoT prompting (Wei et al., 2022) aims to emulate the step-by-step thought process humans use to tackle complex tasks, such as combinatorial reasoning and mathematical calculations. There are also various CoT variants, such as CoT with self-consistency (CoT-SC) prompting (Wang et al., 2022), designed to address the limitations of CoT in exploration. The DAC paradigm (Zhang et al., 2024) mainly refers to simply breaking down the input sequence into multiple sub-inputs to enhance LLM performance on certain specific tasks. Although these methods have shown promising results in some reasoning tasks, this reasoning approach is challenging to directly apply to medical diagnostic reasoning tasks."}, {"title": "8 Conclusion and Limitations", "content": "We propose ReaDiagnosis, a multi-step diagnostic reasoning dataset collected and annotated from medical websites. ReaDiagnosis addresses the weaknesses of existing datasets by constructing multi-stage diagnostic questions that better align with actual clinical diagnostic scenarios. On this dataset, we propose a simple and effective framework. We implement a series of closed-source and open-source LLMs and conduct extensive experimental analyses. The results demonstrate that ReaDiagnosis is a challenging dataset worthy of further exploration. Moreover, the method proposed in this paper shows effectiveness on this dataset. However, the dataset has a limitation: due to limited data sources, our medical records dataset exhibits an uneven distribution across different departments. This issue can be addressed through machine learning methods (Cao et al., 2019) and data sampling strategies (Chawla et al., 2002)."}, {"title": "Ethical Statement", "content": "Our data source adheres to ethical guidelines and respects copyright laws. The entire data collection process is free from copyright and privacy concerns. In addition, we have meticulously reviewed our dataset to ensure it does not contain"}]}