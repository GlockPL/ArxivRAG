{"title": "SEMIEVOL: Semi-supervised Fine-tuning for LLM Adaptation", "authors": ["Junyu Luo", "Xiao Luo", "Xiusi Chen", "Zhiping Xiao", "Wei Ju", "Ming Zhang"], "abstract": "Supervised fine-tuning (SFT) is crucial in adapting large language models (LLMs) to a specific domain or task. However, only a limited amount of labeled data is available in practical applications, which poses a severe challenge for SFT in yielding satisfactory results. Therefore, a data-efficient framework that can fully exploit labeled and unlabeled data for LLM fine-tuning is highly anticipated. Towards this end, we introduce a semi-supervised fine-tuning framework named SEMIEVOL for LLM adaptation from a propagate-and-select manner. For knowledge propagation, SEMIEVOL adopts a bi-level approach, propagating knowledge from labeled data to unlabeled data through both in-weight and in-context methods. For knowledge selection, SEMIEVOL incorporates a collaborative learning mechanism, selecting higher-quality pseudo-response samples. We conducted experiments using GPT-40-mini and Llama-3.1 on seven general or domain-specific datasets, demonstrating significant improvements in model performance on target data. Furthermore, we compared SEMIEVOL with SFT and self-evolution methods, highlighting its practicality in hybrid data scenarios.", "sections": [{"title": "Introduction", "content": "Supervised fine-tuning (SFT) is a crucial method for enhancing large language models' (LLMs) performance on instructional or domain-specific tasks (Raffel et al., 2020; Chung et al., 2024), playing a vital role in adapting LLMs for specific scenarios. However, SFT relies on a substantial amount of annotated labeled data, which can be increasingly costly in real-world applications (Honovich et al., 2023; Kung et al., 2023). While existing LLMs often employ unsupervised pretraining methods (Devlin, 2018; Radford et al., 2019; Brown, 2020) to improve their capabilities, this approach typically requires vast datasets and substantial computational resources, making it impractical for scenarios with limited accessible samples.\nIn practice, however, it often presents a hybrid situation, where a small amount of labeled data coexists with a relatively larger volume of unlabeled data. On the one hand, when deploying LLMs to new target tasks, a limited amount of task-specific annotations can be valuable without incurring excessive costs (Perlitz et al., 2023; Kung et al., 2023). On the other hand, during the continuous inference process of LLMs, a substantial amount of unlabeled data accumulates (Tao et al., 2024; Honovich et al., 2023; Wang et al., 2023b). Effectively leveraging the labeled data to enhance model performance on unlabeled data, while simultaneously selecting high-quality unlabeled samples, can improve LLMs' performance in target scenarios, offering substantial practical utility. Therefore, we aim to address the following question:\nCan LLMs evolve in a real-world scenario of limited labeled data and abundant unlabeled data?\nDesigning an evolution framework for hybrid-data scenarios is non-trivial due to the following reasons: First, semi-supervised learning (Kipf and Welling, 2016; Shi et al., 2023), which has been widely studied in machine learning, primarily focuses on classification tasks. When considering generative tasks, the previous techniques such as pseudo-labeling (Sohn et al., 2020) and contrastive learning (He et al., 2020), cannot be directly applied to LLM use cases, like reasoning and planning (Chen et al., 2022; Hendrycks et al., 2020). Second, previous SFT and unsupervised pretraining methods typically deal with a single type of data (either labeled or unlabeled) (Zhang et al., 2023). Under hybrid-data circumstances, effectively maximizing their combined potential for model improvement becomes challenging.\nIn this work, we introduce SEMIEVOL for improving LLM reasoning in hybrid-data scenarios, as illustrated in Figure 1. SEMIEVOL employs a bi-level strategy for knowledge propagation-and-selection. For knowledge propagation, SEMIEVOL enhances LLMs' inference performance using labeled data through both in-weight and in-context scopes. During in-weight propagation, SEMIEVOL uses labeled data to adapt the model. During in-context propagation, SEMIEVOL employs k-nearest neighbor retrieval in latent space to assist prediction. Moreover, SEMIEVOL introduces a bi-level approach for data selection and generating pseudo-responses. First, it introduces a collaborative learning framework, utilizing multiple LLMs with different configurations for inference and self-justification of responses, yielding more accurate predictions. Second, SEMIEVOL adaptively selects unlabeled data by confidence based on response entropy. By mining on unlabeled data leveraging labeled data, we obtain high-quality pseudo-responses. Using these pseudo-response data, the model enhances its performance on target tasks. We conducted tests on seven general or domain-specific datasets (e.g., MMLU, MMLU-Pro and ConvFinQA), covering tasks such as question-answering, reasoning, and numerical computation. We compared SEMIEVOL with popular methods like retrieval augmented generation, self-evolution and SFT, demonstrating SEMIEVOL's consistent effectiveness across various scenarios.\nWe summarize the contributions as follows:\n\u2022 To the best of our knowledge, we are the first to study a practical problem of semi-supervised fine-tuning, aiming to adapt LLMs into different domains data-efficiently.\n\u2022 We introduce SEMIEVOL, a unified framework for knowledge propagation-and-selection that effectively combines labeled and unlabeled data for model evolution.\n\u2022 We demonstrate the consistent effectiveness of SEMIEVOL across seven widely used general or domain-specific generative tasks in comparison to extensive baseline models."}, {"title": "Challenges for Real-world LLM Fine-tuning", "content": ""}, {"title": "Supervised Fine-tuning", "content": "Supervised fine-tuning (SFT) aims to adapt Large Language Models (LLMs) to domain-specific scenarios. Given an LLM M and a dataset Dlabeled = {Ti, Yi}^N_{i=1}, where Ti represents the input task or context and Yi denotes the corresponding expected response. The model minimizes the loss function for each token of the anticipated output during the fine-tuning process FT.\nChallenge: Annotation Cost. Despite the effectiveness of supervised fine-tuning, it would require expensive labeling costs to access abundant labeled data. An economic solution is to utilize easily accessible unlabeled data without feedback as a supplement for fine-tuning."}, {"title": "Background and Problem Definition: Semi-supervised Fine-tuning", "content": "In real-world scenarios, it is more common to have access to both a small amount of labeled data Dlabeled and a larger volume of unlabeled data Dunlabeled = {Tj}^M_{j=1}. Labeled data offers higher confidence, while unlabeled data represents a broader sample distribution. In this paper, we propose SEMIEVOL approach, which primarily focuses on how to leverage both types of data Dsemi = Dlabeled \u222a Dunlabeled to optimize the LLM M. Our SEMIEVOL not only improves model performance but also offers greater practical value.\nChallenge: Generative Task. In fact, developing a semi-supervised fine-tuning framework is highly challenging. Tradition semi-supervised approaches usually focus on classification problems solved by pseudo-labeling while our problem is a generative task, which requires us to generate expected responses instead."}, {"title": "Methodology", "content": ""}, {"title": "Overview", "content": "In this paper, we develop SEMIEVOL to integrate labeled and unlabeled data for improving LLM performance in reasoning. The core idea of SEMIEVOL is to leverage labeled data through a bi-level propagation-and-select process. As illustrated in Figure 2, SEMIEVOL is featured by three key components: (1) Knowledge Propagation: We utilize labeled data to enhance model M's performance on unlabeled data. This process focuses on two aspects, i.e., model weights and context. The propagation process involves model adaptation using labeled data and providing the most relevant references from the latent space to assist model inference. (2) Collaborative Learning: We employ multiple LLMs with different configurations as mutual teachers to infer unlabeled data. We pay particular attention to inconsistent responses, using the models to self-justify these discrepancies. (3) Knowledge Self-selection: We design the adaptive selection for unlabeled data and pseudo-responses. Using labeled data as a guide, we identify the most valuable unlabeled data for learning. By optimizing LLMs on these selected data samples, the model achieves superior evolution performance.\nIn summary, SEMIEVOL addresses the prevalent real-world scenario where both labeled and unlabeled data coexist. By leveraging the labeled data and the capabilities of LLMs themselves, we perform knowledge propagation, mining, and selection on unlabeled data. This strategy improves model performance in the target scenarios."}, {"title": "Knowledge Propagation", "content": "Labeled data contain expected target responses, while unlabeled data represents a broader task distribution. To leverage this, we aim to propagate knowledge from labeled to unlabeled data, enabling the model to effectively utilize and learn from unlabeled instances. We design a bi-level knowledge propagation framework that operates simultaneously on two fronts: in-weight and in-context.\nFor in-weight propagation, we initially warm up the base model Mbase on labeled data Dlabeled to enhance its predictive capabilities for the target task. Specifically, we fine-tune the model, leveraging task data and target responses to obtain a preliminary adapted model (Mwarm). This process is formulated as:\nMwarm = FT (Mbase, Dlabeled), (1)\nwhere FT is the fine-tuning process.\nFor in-context propagation, we first embed labeled dataset into latent space using an embedding function \u20ac(\u00b7):\nElabeled = {\u20ac (ti) | (ti, Yi) \u2208 Dlabeled}. (2)\nDuring inference on unlabeled data, for each task tj\u2208 Dunlabeled, we retrieve the k nearest labeled instances in the embedding space:\nN (tj) = NN (Elabeled, \u2208 (tj), k), (3)\nwhere k is set to 3, NN is the nearest neighbors search. We use N (tj) as context to improve the inference on the unlabeled data.\nIn summary, labeled data facilitates knowledge propagation to unlabeled data through both in-weight and in-context manners. In practice, we first adapt the model to obtain the warm-up LLM Mwarm, then utilize labeled data as context to enhance inference on unlabeled instances."}, {"title": "Collaborative Learning", "content": "To further exploit unlabeled data, we designed a collaborative learning framework tailored for LLMs. This framework utilizes the inherent capabilities of LLMs for self-justify to obtain high-confidence pseudo-responses from unlabeled data. Some concurrent works also attempt to use LLMs for similar functionality (Wang et al., 2024a), while their focus differs from ours.\nInitially, we employ a set of n LLMs, denoted as M1, M2,\u2026, Mn to perform inference on the unlabeled dataset Dunlabeled, where n is 4 by default and will be discussed in Section 4.3.2. Each model is configured with different inference contexts and settings, providing diverse perspectives and yielding more comprehensive results. For each unlabeled sample tj \u2208 Dunlabeled, we obtain multiple predictions:\n{y_j^m } = {Mm (tj)}^n_{m=1}. (4)\nSubsequently, we implement a self-justification process using LLMs. This step synthesizes the inferences from various models to select and summarize the most accurate response \u0177j :\n\u0177j = Self-Justify ({y_j^m }^n_{m=1}). (5)\nwhere the Self-Justify operator is implemented via prompting Mwarm by natural language instructions. In summary, our LLM-specific collaborative learning framework harnesses multiple differently configured LLMs for multi-perspective inference. By utilizing the LLMs' inherent abilities to self-justify, we effectively mine unlabeled data, and generate high-confident pseudo-responses."}, {"title": "Knowledge Adaptive Selection", "content": "While the pseudo-responses \u1ef9j generated through the collaborative learning framework enrich the training data, they may still contain noise or low-quality information that could misguide the model's learning. To address this issue, we design an adaptive data selection approach within the SEMIEVOL framework. Specifically, we measure the confidence of the responses \u1ef9j for the unlabeled data selection.\nWe use the entropy of the LLM's responses to measure the model's confidence in the answers. Since LLMs generate responses token by token, we calculate the per-token negative log-likelihood, which serves as an approximation of the entropy. For each data sample tj \u2208 Dunlabeled, the entropy H (\u1ef9j) is computed on pseudo-response \u1ef9j after Eq. 5 as:\nH (\u1ef9j) = - \\frac{1}{Lj} \\sum_{k=1}^{Lj} log P(r_j^k|t_j, r_j^{<k}), (6)\nwhere Lj is the length of the response rj generated by Mwarm, r_j^k is the k-th token in the response, r_j^{<k} = {r_j^1, r_j^2,...,r_j^{k-1}} are the preceding tokens of yj, and P (r_j^k| t_j, k) is Mwarm's predicted probability of token r^k_j at position k.\nFor the unlabeled data, we compute the entropy H (\u1ef9j) for each pseudo-response \u1ef9j corresponding to task tj \u2208 Dunlabeled. We then use the \u03b8 percentile of the entropy values from the labeled data to establish a dynamic threshold T:\nT = Percentiles ({H (j)}^M_{j=1}, \u03b8), (7)\nwhere M is the amount of unlabeled samples, and \u03b8 is default to 50% and will be investigated in Section 4.3.2.\nUsing this dynamic threshold, we select confident samples from the unlabeled data. In formula,\nDselected = {(tj, \u1ef9j) | H (\u1ef9j) \u2264 T}. (8)\nWe filter the pseudo-responses obtained previously, resulting in the refined dataset Dselected.\nFinally, we use the high-quality pseudo-responses to fine-tune the model, which can enhance its performance and adaptability on the target task:\nMevol = FT (Mwarm, Dselected), (9)\nwhere Mwarm is the model obtained after initial fine-tuning in Eq. 1, and FT denotes the fine-tuning process.\nBy focusing on these high-quality assessed data, we enhance the model's performance and adaptability on the target task while reducing the influence of noisy or erroneous information."}, {"title": "Summary", "content": "SEMIEVOL enhances the performance and adaptability of LLMs in target tasks through a two-stage knowledge mining process, combining labeled and unlabeled data for model evolution. Firstly, we leverage a small amount of labeled data to enhance knowledge propagation across unlabeled data. Secondly, we employ knowledge mining and adaptive selection. This strategy effectively integrates both labeled and unlabeled data, culminating in the evolved model Mevol."}, {"title": "Experiment", "content": ""}, {"title": "Experiment Setup", "content": ""}, {"title": "Datasets", "content": "We employed both general-purpose and domain-specific evaluation datasets to provide a comprehensive assessment. These datasets encompass a variety of tasks, including multiple-choice questions, reasoning, numerical computations, etc.. Specifically, our general evaluation datasets include MMLU (Hendrycks et al., 2020), MMLU-Pro (Wang et al., 2024c), and ARC (Clark et al., 2018), while domain-specific datasets comprise FPB (Malo et al., 2014), USMLE (Jin et al., 2021), PubMedQA (Jin et al., 2019), and ConvFinQA (Chen et al., 2022), covering various fields such as finance and healthcare. This diverse selection enables a thorough evaluation of the model's performance across different task types and knowledge domains."}, {"title": "Backbones and Baselines", "content": "Base Models. To demonstrate the generalization capability of SEMIEVOL, we employed a diverse range of leading models, encompassing both commercial and open-source and LLMs, including GPT-40-mini and Llama-3.1-8B (Dubey et al., 2024).\nBaselines. We evaluated our method against baselines from several categories: (1) Vanilla, which involves testing solely through API calls or using the original model; (2) Supervised Fine-tuning (SFT) (Hu et al., 2021; Wei et al., 2021), which adapts the model to the target task using the labeled data; (3) Self-Evolution Methods (Self-Evol), which enhance LLM capabilities using additional unlabeled data. We compare with Reflection-Llama (Li et al., 2024)2 and Hermes-3 (Teknium et al., 2024)3, both of which evolve from the Llama-3.1-8B model; (4) Domain Adaptation Methods, including AdaptLLM (Cheng et al., 2024b) and InstructPT (Cheng et al., 2024a), utilize domain-specific data (e.g., finance and medical). We select models adapted to corresponding domains for testing, all with comparable parameter counts of 8B; (5) Inference-time enhancement methods, such as Retrieval Augmented Generation (RAG) (Lewis et al., 2020), including BM25 (Jones et al., 2000) and FAISS (Douze et al., 2024) algorithms. We also compare with MemoryLLM (Wang et al., 2024b), with the nearest labeled sample as memory;\nThis comprehensive comparison allows us to assess the effectiveness of our proposed method across various state-of-the-art approaches in LLM fine-tuning and adaptation."}, {"title": "Implementation Details", "content": "For the setting of semi-supervised fine-tuning of LLMs, we have Dlabeled, Dunlabeled and Dtest. The data proportion in our experiments is labeled : unlabeled : test = 2 : 6 : 2 and will be further discussed in Section 4.3.6. The answer information for Dunlabeled is inaccessible in our setting. We fine-tuned Llama-3.1-8B using Low-Rank Adaptation (LoRA) (Hu et al., 2021) and applied fine-tuning with the official API for GPT-40-mini4. All fine-tuning processes take 2 epochs. n is set to 4 and \u03b8 is set to 50%, with further investigation planned in subsequent experiments. Detailed training configurations are provided in the Appendix.\nWe evaluated all methods using the test sets Dtest. Model inference followed default settings for each approach. Codes are available in our GitHub repository5."}, {"title": "Main Result", "content": "We present the main results of SEMIEVOL in Table 1. We can draw the following insights. Firstly, the tasks are generally challenging. Off-the-shelf LLMs perform poorly on these tasks, highlighting the necessity of leveraging scenario data to enhance model performance. Secondly, SEMIEVOL consistently improves both commercial and open-source models. Notably, SEMIEvol is one of the few approaches that demonstrably enhances state-of-the-art commercial models, underscoring its practical value. Thirdly, SFT yield modest improvements, demonstrating the effectiveness of labeled data. Given the high cost of data labeling, SEMIEVOL effectively utilizes unlabeled data to complement this approach. Fourthly, the self-evolution method fails to achieve consistent improvements, showing limited improvement or even adverse effects on most datasets. Fifthly, adaptive fine-tuning methods can enhance performance only on specific tasks (e.g., ConvFinQA). Also, these methods may compromise the model's instruction-following ability, leading to significant performance drops in some tasks (e.g., USMLE and PubMedQA). Lastly, SEMIEVOL consistently outperforms SFT methods, which demonstrates the effectiveness of incorporating unsupervised data and leveraging labeled data to fully utilize unsupervised data. Even when base models perform poorly (e.g., MMLU-Pro and ConvFinQA), SEMIEVOL can still achieve substantial improvements in model performance."}, {"title": "Analysis and Discussions", "content": ""}, {"title": "Ablation Study", "content": "To evaluate the effectiveness of different components, we conducted an ablation analysis on SEMIEVOL, with results presented in Table 2. The findings reveal several key insights: (1) The full model consistently outperforms all other configurations across the three datasets, demonstrating its comprehensive effectiveness. (2) In terms of knowledge propagation, both In-weight Propagation (IWP) and In-context Propagation (ICP) contribute significantly to the transfer of knowledge from labeled to unlabeled data and subsequent model evolution. In-weight Propagation, in particular, shows a more pronounced impact. (3) Removing Collaborative Learning (CL) negatively affects model performance. This suggests that Collaborative Learning effectively leverages predictions from multiple LLMs to autonomously identify more accurate answers, thereby enhancing the prediction quality on unlabeled data. (4) The absence of Adaptive Selection (AS) also leads to decreased model performance. This indicates that AS successfully selects more confident samples, thus improving the accuracy of unlabeled data and enhancing the model's evolutionary process."}, {"title": "Sensitivity Analysis", "content": "We analyze the number of collaborating models (n) and the data selection ratio (\u03b8), with results illustrated in Figure 3. From the results, we have the following observations. (1) Our method demonstrates robust performance across various settings, indicating low sensitivity to these parameters. (2) Model accuracy generally increases with n, as more collaborating LLMs enhance prediction accuracy. However, this also introduces additional computational overhead. We chose n = 4 as the default. (3) Accuracy initially increases with \u03b8 but subsequently decreases, suggesting that introducing excessively noisy data is detrimental to model evolution. Consequently, we empirically set \u03b8 = 50% as the default value. It is noteworthy that we did not conduct extensive hyperparameter searches, as our primary focus was on validating the overall framework's effectiveness."}, {"title": "Response Entropy Analysis", "content": "We present the entropy distribution of different methods on the test set, as illustrated in Figure 4. Lower entropy indicates more confident responses. Compared to the Vanilla and SFT model, SEMIEVOL demonstrates a significant improvement in response confidence. This observation substantiates the effectiveness of SEMIEVOL in producing more decisive and assured outputs. This signifies that SEMIEVOL not only improves accuracy but also enhances the model's ability to generate more confident and reliable responses."}, {"title": "Category-wise Performance Analysis", "content": "We conducted an in-depth investigation into the differential impact of SEMIEVOL across various categories in MMLU-Pro, as illustrated in Figure 6. We find that (1) SEMIEVOL demonstrates enhanced performance across the majority of domains compared to both SFT and Vanilla approaches. This broad-spectrum improvement underscores the method's versatility and effectiveness across diverse subject areas. (2) SEMIEVOL achieves substantial gains in specific fields such as Law, Engineering, and Philosophy. This notable improvement suggests that knowledge in these domains is underrepresented in common knowledge bases, highlighting the necessity for targeted adaptation."}, {"title": "Stability Analysis", "content": "We evaluate the inference stability of different models by utilizing diverse prompts. Specifically, we employed GPT-40 to rephrase the instructions and conducted 5 tests on each model, reporting the average performance and standard deviation. As illustrated in Figure 5, changing the inference prompts had minimal impact on the various models. Notably, SEMIEVOL even demonstrated a slight improvement in model stability."}, {"title": "Discussion on Continuous Evolution", "content": "In real-world scenarios, unlabeled data often accumulates continuously, altering the ratio between labeled and unlabeled data. Table 3 illustrates the impact of various data proportions on SEMIEVOL's performance. As illustrated, model performance consistently improves with an increase in unsupervised data across different base models. This validates SEMIEVOL's effectiveness in addressing real-world scenarios, where model performance in specific domains can be progressively enhanced as more unsupervised data accumulates."}, {"title": "Discussion on Iterative Evolution", "content": "We verify the model's iterative evolution capability, as illustrated in Figure 7. After applying SEMIEVOL, we utilized the labeled data and pseudo-response data as new labeled data, initiating a fresh round of SEMIEVOL on the previously filtered unlabeled data. By the fourth iteration, we had utilized 94.75% of the unlabeled data, resulting in further performance improvements in the target scenario. The model's performance on MMLU-Pro exceeded 55%. This iterative evolution capability further demonstrates the practicality of SEMIEVOL."}, {"title": "Related Work", "content": ""}, {"title": "Data Engineering for SFT", "content": "With the rapid advancement of Large Language Models (LLMs) (Zhao et al., 2023), researchers have discovered that employing suitable data for Supervised Fine-Tuning (SFT) can enhance model performance on downstream tasks (Taori et al., 2023; Longpre et al., 2023). Some researchers focus on data selection (Bhatt et al., 2024; Zhou et al., 2024; Xia et al., 2024; Bukharin and Zhao, 2023), aiming to improve data quality to boost model effectiveness within limited training budgets. Others concentrate on data synthesis (Xu et al., 2023; Mukherjee et al., 2023; Chung et al., 2024; Honovich et al., 2022; Cheng et al., 2023), attempting to enhance models' instruction-following capabilities through synthesized instruction data. Complementary to these approaches, SEMIEVOL focuses on LLMs' ability to continuously evolve in real-world scenarios, relying solely on their inherent capabilities. It effectively utilizes small amounts of labeled data to improve model evolution performance."}, {"title": "Semi-supervised Learning", "content": "Semi-supervised learning aims to reduce the annotation cost during model training (Zhu, 2005; Tarvainen and Valpola, 2017), which has received increasing attention in various fields such as text classification (Duarte and Berton, 2023; Thangaraj and Sivakami, 2018; Linmei et al., 2019) and neural machine translation (Cheng et al., 2016; Pham et al., 2023). Current semi-supervised learning approaches can be mainly divided into two types, i.e., pseudo-labeling (Lee et al., 2013) and consistency regularization (Sohn et al., 2020; Berthelot et al., 2019). Pseudo-labeling approaches usually add extra unlabeled data into the labeled dataset by leveraging the labels predicted by the model. Recent studies attempt different techniques to enhance pseudo-labeling such as considering adaptive thresholds (Zhang et al., 2024; Rhee and Cho, 2019) and class imbalance (Wang et al., 2023a). In contrast, consistency regularization aims to encourage the consistency of predictions under different perturbations. However, these approaches focus on classification problems (Shi et al., 2023), which cannot be applied to LLM fine-tuning. To tackle this issue, we propose a new framework SEMIEVOL in a propagate-and-select manner for LLM adaptation."}, {"title": "Conclusion", "content": "We for the first time investigate the practical challenge of utilizing hybird-data (i.e., both labeled and unlabeled data) to enhance LLMs performance in specific scenarios. We designed a bi-level framework SEMIEVOL for knowledge propagation-and-selection. This framework leverages in-weight and in-context knowledge propagation from labeled data, while employing collaborative learning and adaptive selection to generate high-quality pseudo-responses. We validated SEMIEVOL's efficacy on both general and domain-specific datasets, conducting a detailed analysis of the improvements it yields. Furthermore, we demonstrated SEMIEVOL's capability for continuous iterative evolution, which plays a crucial role in enhancing LLMs' effectiveness in real-world applications."}, {"title": "Limitations", "content": "One limitation of our work is that due to the limit of computational resources, we do not evaluate our framework on more LLMs such as GPT-40 and Llama3.1 70B. In future work, we will attempt to incorporate our framework into these LLMs. Moreover, although our framework is evaluated on various benchmark datasets, we do not involve more complicated domains which require more scientific knowledge. To solve this, we will extend our framework to more advanced scientific domains such as genomics analysis."}, {"title": "Ethics Statement", "content": "Our research adheres to the ACL Code of Ethics. All datasets and language models used in this study are publicly available. The code and related materials will be appropriately released to ensure transparency and reproducibility of our work."}]}