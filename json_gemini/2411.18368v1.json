{"title": "AMPS: ASR with Multimodal Paraphrase Supervision", "authors": ["Amruta Parulekar", "Abhishek Gupta", "Sameep Chattopadhyay", "Preethi Jyothi"], "abstract": "Spontaneous or conversational multilingual speech presents many challenges for state-of-the-art automatic speech recognition (ASR) systems. In this work, we present a new technique AMPS that augments a multilingual multimodal ASR system with paraphrase-based supervision for improved conversational ASR in multiple languages, including Hindi, Marathi, Malayalam, Kannada, and Nyanja. We use paraphrases of the reference transcriptions as additional supervision while training the multi-modal ASR model and selectively invoke this paraphrase objective for utterances with poor ASR performance. Using AMPS with a state-of-the-art multimodal model SeamlessM4T, we obtain significant relative reductions in word error rates (WERs) of up to 5%. We present detailed analyses of our system using both objective and human evaluation metrics.", "sections": [{"title": "1 Introduction", "content": "Automatic speech recognition (ASR) systems have shown considerable progress in recent years but still falter when subjected to spontaneous conversational speech containing disfluencies, loosely articulated sounds, and other noise factors (Gabler et al., 2023). This degradation in ASR performance could be largely attributed to the unavailability of labeled spontaneous speech in most languages. How can we effectively utilize the limited quantities of existing labeled spontaneous speech? Towards this, we propose AMPS (ASR with Multimodal Paraphrase Supervision) that augments an existing multilingual multimodal ASR system with paraphrase-based supervision to improve ASR performance on spontaneous speech in multiple languages.\nUnlike standalone ASR models that are exclusively trained to perform ASR, multimodal models (such as SpeechT5 (Ao et al., 2022), MAESTRO (Chen et al., 2022), etc.) are trained on multiple tasks including ASR using speech and text data in various paired (and unpaired) forms. We focus on one such multilingual multimodal model, SeamlessM4T (Communication et al., 2023), that consists of dual encoders for speech and text and a shared text decoder, thus creating both speech-to-text and text-to-text pathways.\nAMPS leverages the multimodal nature of SeamlessM4T by introducing a paraphrasing objective jointly with ASR. Along with using spontaneous speech and its corresponding transcription to train the speech-to-text pathway in SeamlessM4T, AMPS also uses paraphrases of the reference transcriptions as additional supervision to train the text-to-text pathway. We selectively employ paraphrase-based augmentation during training when the ASR loss is high (as determined by a predetermined threshold); high ASR loss is typically triggered by noise or poorly enunciated words in spontaneous speech. This selective intervention offers the model an alternate path of opting for semantically close words and phrases when the audio is not very clear. It is important that the paraphrases should not significantly differ in word order from the original transcripts, thus enabling the model to easily align representations of speech, text, and its paraphrase. With AMPS, we derive significant improvements in ASR for spontaneous speech in Hindi, Marathi, Malayalam, Kannada, and Nyanja compared to strong ASR-only finetuned baselines. We report improvements not only in terms of word error rate (WER) reductions but also using semantic evaluation metrics. We also conduct a detailed human evaluation comparing the outputs of AMPS with the outputs from finetuning only with the ASR objective and show consistent improvements in human scores. We also present many ablations, including different paraphrasing techniques, the influence of varying thresholds on the performance of AMPS, and using varying amounts of training data. We envision that techniques like AMPS could be used to improve ASR of atypical speech for people with speech impairments where comprehensibility of the transcripts is critical (more than faithfulness of transcripts to the underlying speech, as highlighted in very recent work by Tomanek et al. (2024))."}, {"title": "2 Related Work", "content": "In recent years, multimodal models for speech recognition have gained significant recognition (Ao et al., 2022; Chen et al., 2022; Rubenstein et al., 2023; Zhang et al., 2023). These models are capable of processing both speech and text inputs and can be adapted for tasks such as translation and speech generation. A notable example is Meta AI's SeamlessM4T (Communication et al., 2023), which can support nearly 100 languages. One of the key advantages of such models is their ability to exploit text-only training to fine-tune shared parameters in the ASR pipeline. Some of the recent work on text-based adaptation for ASR models include Vuong et al. (2023); Bataev et al. (2023); Chen et al. (2023); Mittal et al. (2023). One potential approach for leveraging text-only data for ASR finetuning is through training the text decoder with a paraphrasing objective. Emerging research (Yu et al., 2023) has shown that text paraphrasing can be used to augment LLM performance but we are the first to show how paraphrases can be used to improve ASR. Tomanek et al. (2024) is a recent study focusing on meaning preservation in disordered speech transcription, but do not offer any technique to help improve meaning preservation in ASR outputs."}, {"title": "3 Methodology", "content": "AMPS scaffolds on a multimodal base model comprising a speech encoder, a text encoder, and a shared decoder that takes inputs from both encoders. SeamlessM4T is an example of such a model, capable of performing multiple tasks including text-to-text translation (T2T), and speech-to-text transcription/translation (S2T). We introduce a new auxiliary task of text-to-text paraphrasing. This allows the model to predict words that are semantically similar and fit within the context of the sentence, without significantly altering its word order. The shared decoder architecture of SeamlessM4T allows us to exploit common parameters of both S2T and T2T pipelines and enhance the ASR performance of the model.\nFormally, consider a speech utterance $X = {X_1, X_2, ..., X_L | x_i \\in R^d}$ with its corresponding transcript $Y = {Y_1, Y_2,\\dots,Y_N}$. For a transcript Y, we generate a paraphrase $Y' = {Y_1, Y_2,..., Y_M}$. Given a labeled instance {X, Y, Y'}, the ASR, paraphrase, and the AMPS loss functions are as follows.\n$L_{ASR} = \\sum_{t=1}^N log p_\\theta(y_t | Y_{<t}, X)$,\n$L_{PAR} = \\sum_{t=1}^M log p_\\theta(Y'_t | Y'_{<t}, Y)$,\n$L_{AMPS} = L_{ASR} + L_{PAR}$.\nFor each batch, we pass the audio through the S2T pathway and compute the ASR loss between the predicted and ground-truth transcriptions. We also pass the ground-truth transcriptions as input through the T2T pathway with paraphrase-based supervision to compute $L_{PAR}$.\nAMPST: Loss Function Thresholding. We aim at improving the model's performance in noisy regions where the ASR loss is high by selectively triggering the paraphrase objective only when the ASR loss exceeds a predefined threshold $\\tau$.\nThus, the loss for the system is given by\n$L_{AMPST} = \\begin{cases} L_{ASR} + L_{PAR} & \\text{if } L_{ASR} > \\tau, \\\\ L_{ASR} & \\text{otherwise,} \\end{cases}$  (1)\nwhere $\\tau$ is a hyperparameter chosen based on ASR validation losses. Henceforth, AMPS with the best threshold will be referred to as AMPST. $\\tau$ values for various experiments are in Appendix A."}, {"title": "5 Experiments and Results", "content": "Table 1 shows the main results for all the 50-hour Indian-language experiments. AMPS, consistently performs best compared to ASR, and the WER reductions are statistically significant (at p < 0.05 using the mapsswe test). Apart from the overall scores in All Data, we sorted the transcriptions in descending order of WER using pure ASR and averaged metrics were calculated for both pure ASR and AMPS for the first 100 (hardest) sentences. Improvements from ASR to AMPS, for these hardest 100 predictions are labeled \u2206Hard in Table 1. We see that \u2206Hard consistently exceeds \u2206All, indicating that the most improvement is observed in cases where pure ASR performs poorly. This supports the thresholding approach that triggers the paraphrase loss only when pure ASR predictions fall below a threshold."}, {"title": "5.1 Comparing Paraphrase Techniques", "content": "Table 2 shows results from training on 5 hrs of read/mixed Hindi speech and different paraphrasing techniques with mixed speech. Unsurprisingly, training on mixed speech yields significantly lower WERs compared to training on read speech. The highest performance gains were obtained using LLM paraphrasing for Hindi, suggesting that the LLM is a good option for medium-resource languages like Hindi. LLM outputs are subpar for low-resource languages like Kannada, and hence not an option. Comprehensive results comparing the paraphrase techniques for other languages are given in Appendix F and G."}, {"title": "5.2 Human Evaluation", "content": "The transcription capabilities of ASR, AMPS, and AMPST models were verified through extensive human evaluation of the utterances with differing model outputs. The annotators reviewed 172, 153, 216, and 229 instances for Hindi, Marathi, Kannada, and Malayalam, respectively, giving a max score of 5 for a perfect transcript and penalizing them for minor errors (spellings, etc.) and major errors (incorrect semantics). The annotators were asked not to penalize a semantically identical word that differs from the speech. More details and scoring guidelines are provided in Appendix H and qualitative examples are in Appendix D.1.Table 3 shows the averaged scores with AMPS consistently performing the best across all languages."}, {"title": "5.3 AMPS for Nyanja", "content": "Table 4 shows overall results* on Nyanja with 5 hours of training data and round-trip translated paraphrases. Again, AMPS+ performs the best, showing that AMPS could be applied to diverse languages across language families."}, {"title": "5.4 Conclusion", "content": "This work introduces a novel paraphrase-based supervision technique AMPS to improve the ASR performance of spontaneous speech in multimodal models. This auxiliary supervision makes the model more robust and helps the model generalize better, especially in utterances with large ASR errors. We show significant ASR improvements on multiple and diverse languages and further validate these improvements via a thorough human evaluation. Future work will investigate how techniques like AMPS could be used to improve ASR for atypical speech."}, {"title": "Limitations", "content": "The primary limitation of our study was the lack of any appropriate pre-existing evaluation metric for the task. When supervising with paraphrases, the model often predicts semantically similar words or phrases that do not exactly match the transcript, making traditional metrics like Word Error Rate (WER) overly harsh for such cases. While BERTScore addresses semantic similarity, recent research suggests using LLMs to directly assess whether sentence meaning is preserved (Tomanek et al., 2024). In the future, we plan to adopt LLM-based evaluation alongside human reviews to improve assessment.\nA second limitation was the occurrence of transliterated English words caused minor spelling errors in the model. We plan to mitigate this in the future by introducing code-switched words in our paraphrases to teach the model to associate the transliterated English words with their Latin script counterparts. Multilingual models like SeamlessM4T possess the unique ability to link semantically similar words across languages, thus comprehending code-switched speech easily and we aim to leverage this ability as future work.\nAdditionally, the threshold value $\\tau$ is manually defined and not a dynamic value that is learned across languages. In future work, we plan to make this threshold a learnable parameter."}, {"title": "B.1 Adapting SeamlessM4T", "content": "The SeamlessM4T (Medium) consists of 1.2B parameters. Full fine-tuning of these components using limited amounts of labeled data for low-resource languages may result in overfitting and degradation of ASR performance. To address these issues, parameter-efficient fine-tuning methods, such as the adapter framework, have become widely adopted in natural language processing tasks. Adapters have proven effective in low-resource ASR tasks, including accent and cross-lingual adaptation.\nFormally, the operations performed in the ith speech encoder layer can be described as follows:\nH = MHA(h^{i-1}, h^{i-1}, h^{i-1})\nC = Convolution(H)\n$\\hat{h^i}$ = FFN(C)\nh^i = Adapter($\\hat{h^i}$)\nSimilarly, the operations in the ith decoder layer can be summarized as:\nD = MHA(d^{i-1}, d^{i-1}, d^{i-1})\n$\\hat{O}_D^i$ = MHA(d^{i-1}, $h^l$, $h^l$)\n$\\hat{o^i}$ = FFN($\\hat{O}_D^i$)\nd^i = Adapter($\\hat{d^i}$)\nHere, l refers to the final encoder layer, and MHA(Q, K, V) denotes the standard multi-head attention mechanism (Vaswani, 2017), where Q, K, and V are the queries, keys, and values, respectively."}, {"title": "B.2 Implementation Details", "content": "The architecture of the Seamless M4T medium incorporates a speech encoder that has 12 conformer layers, while both the text encoder and text decoder consist of 12 Transformer blocks, with a model dimension of $D_1$ = 1024. In our experiments, adapters were introduced after each encoder conformer layer and the decoder Transformer layer. These adapters project the original $D_1$-dimensional features into a reduced intermediate space of dimension $D_2$, apply a GeLU non-linear activation function (Hendrycks and Gimpel, 2023), and then project the features back to $D_1$. The projected layer dimension on the adapters is $D_2$ = 2048. The value of $D_2$ controls the number of trainable parameters, with smaller values of $D_2$ reducing parameter count. With $D_2$ set to half of $D_1$, this setup introduced 100M trainable parameters while keeping the rest of the model frozen.\nAll the fine-tuning experiments were conducted using the SeamlessM4T codebase (Communication et al., 2023) released by Meta AI using NVIDIA RTX A6000 GPUs. The experiments were conducted over 20 epochs, utilizing a batch size of 8"}, {"title": "C LLM Prompts for Paraphrasing", "content": "The paraphrasing prompt given to the Aya model for our very specific paraphrasing task has been stated below:\nParaphrase the following sentence in lang, strictly adhering to these guidelines:\n1. Maintain the original sentence structure and word order as much as possible.\n2. Replace at least one word, and aim to replace as many words as feasible with Hindi synonyms or words with similar meanings.\n3. Do not add extra words or elaborate on the description.\n4. Preserve named entities (e.g., proper names, places) in their original form.\n5. Convert ALL numbers to their Hindi word equivalents. This includes dates, years, percentages, and any other numerical values.\n6. Ensure that all replacements are common Hindi words, avoiding obscure or highly technical terms.\n7. If a direct Hindi synonym is not available, use a phrase that conveys the same meaning.\n8. Maintain the original tense and grammatical structure of the sentence.\n9. If the original sentence contains English words commonly used in Hindi, you may keep them unchanged.\nIMPORTANT: Double-check that NO numerical digits remain in your paraphrase. All numbers must be written out in Hindi words.\nExamples: Some Hindi examples with the required paraphrases were provided"}, {"title": "E Paraphrase Evaluation Metrics", "content": "1. Word Error Rate (WER) measures the number of mistakes in transcription as a ratio of the number of words. These errors could be substitutions, insertions or deletions.\n$WER = \\frac{Substitutions + Insertions + Deletions}{Words in Reference Text}$ (2)\n2. METEOR (Banerjee and Lavie, 2005) is used for evaluating of machine translation quality. It has also previously been used for evaluating paraphrase quality(Shen et al., 2022b). It aligns words in the candidate and reference translations based on word level matches, including same meaning words and stemming.\n3. BERTScore (Zhang et al., 2020) evaluates the similarity between two texts by using BERT embeddings(Devlin et al., 2019) (Bidirectional Encoder Representations from Transformers). It captures contextual meaning and semantics by computing the cosine similarity between token embeddings from a reference sentence and a candidate sentence. We used AI4Bharat's IndicBERT (Kakwani et al., 2020)for our BERTScores."}, {"title": "I Paraphrase Supervision for Purely Speech-to-Text Models", "content": "To provide a comparison for our multimodal model technique, we propose an alternative approach involving pretraining and finetuning for purely speech-to-text ASR models. The hypothesis is that training an ASR model first on speech paired with paraphrased transcripts, followed by finetuning it on speech with original transcripts, will result in a model that is more robust to mispronunciations and noisy inputs. By learning to associate unclear or imprecise utterances with semantically similar phrases, this model should outperform one trained exclusively on ground-truth labels when evaluated on noisy test sets despite exposure to similar amounts of data. To support our hypothesis, we used the Whisper ASR model trained sequentially using paraphrased transcripts followed by the ground truth, with an ASR training objective."}, {"title": "I.1 Whisper", "content": "Whisper (Radford et al., 2022), developed by OpenAI, utilizes a transformer-based encoder-decoder framework suitable for a range of speech-related tasks. The model comprises an audio encoder that processes raw audio inputs, transforming them into log-mel spectrograms. This input is fed into multiple transformer layers designed to capture long-range dependencies within the audio data. The text decoder, operating autoregressively, generates transcriptions from the processed audio features while integrating task-specific tokens for seamless task-switching among any auxilliary tasks."}, {"title": "I.2 Experiment and Results", "content": "The Whisper model was trained sequentially with 5-hour round-trip translated read speech data in three different ways - training with ground truth training followed by paraphrased training, paraphrase training followed by ground truth training, and finally, ground truth training repeated twice. The WER (%) values for Hindi read speech were 87.68 for direct inference, 42.33 for ground truth ground truth training, 47.34 for paraphrase - ground truth training and 43.78 for ground truth - paraphrase training. Since pure ground truth training WER is the best, we chose not to proceed with this experiment as this strongly supports that multimodality of a model is essential for AMPS."}]}