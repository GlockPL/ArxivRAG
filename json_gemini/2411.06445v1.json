{"title": "Prompt-Efficient Fine-Tuning for GPT-like Deep Models to Reduce Hallucination and to Improve Reproducibility in Scientific Text Generation Using Stochastic Optimisation Techniques", "authors": ["Daniil Sulimov"], "abstract": "Large Language Models (LLMs) have demonstrated impressive performance in a variety of language-related tasks, including text generation, machine translation, text summarising. Sometimes the result produced by a LLM turns out to be inaccurate. This thesis aims to fine-tune the existing LLM, GPT-2 by OpenAI, to reduce model's hallucinations and increase the answers' reproducibility in mass spectrometry.\nThe research involved the application of the following scope of skills: data engineering, stochastic modelling, data science and statistics. I used two servers for all experiments: cHARISMa Higher School of Economics (HSE) server for fine-tuning and AI for Computational biology (AIC) server, where I run Docker images, necessary for the data preprocessing.\nOur fine-tuned model was named MassSpecGPT (MS-GPT). The thesis includes the novel approach of reproducibility score computations and calculation of Wilcoxon rank sum statistical test to compare the fine-tuned model MS-GPT against the base GPT-2 by OpenAI in reproducibility domain. The selection of optimal parameters (optimizer, learning rate) was based on several factors: validation error, run time, random-access memory (RAM) usage and Electricity usage. The fine-tuning of the model involved Low-Rank Adaptation of Large Language Models (LoRA) adapters, the state-of-the art (SOTA) method by now. I used common Natural Language Generation (NLG) evaluation metrics to compare the models' accuracies: Bilingual Evaluation Understudy (BLEU), Recall-Oriented Understudy for Gisting Evaluation (ROUGE) and Perplexity.\nAs the result of the research, the BLEU score increased from 0.33 to 0.34, ROUGE-1 - from 0.42 to 0.44, ROUGE-L - from 0.57 to 0.62, Perplexity reduced from 13586.37 to 10092.12 and reproducibility score went from 0.83 to 0.84. Statistically significant under 5% significance level turned out to be Perplexity score and reproducubility.", "sections": [{"title": "1 Introduction", "content": "A language model (LM) serves as a probabilistic framework for understanding natural language. It assesses the likelihood of various linguistic elements, including symbols, tokens, and sequences of tokens, within a given context [27]. Specifically, a large language model (LLM) like the one described here is capable of generating coherent sequences of words, forming sentences or paragraphs. Over time, the field has evolved through various approaches, among which are Recurrent Neural Networks (RNNs) [21, 22], Long-Short Term Memory Networks (LSTMs) [26, 8], and Generative Pre-Trained Transformers (GPT) [31]. These advancements have significantly enhanced the model's ability to comprehend and generate human-like text.\nConsiderable progress has been made in the improvement of text generating LLMs over past years, which reflects in reaching significant results on such benchmarks datasets for Language Modelling as WikiText-103[7], WikiText2, Text8, etc. Constant improvements of the text generated quality are presented almost monthly, showing how significant the growth of the sphere has become. These enhancements have been made by developing the LLM itself and then fine-tuning[24] it on the specific tasks. Apart from the achieved results, over the years, we observe how LLMs grow in terms of size of parameters. This can be seen in the Figure1.1.\nWithin 4 years from 2018, the size of models has increased for near 6000 times, and consequently, the amount of data needed either for training or fine-tuning LLMs has risen drastically. Basically, it reflects several points: the interest towards such models has grown exponentially, the usage of LLMs has spread all around, encompassing the majority of life spheres. Indeed, nowadays people rely increasingly on LLMs, especially after the release of Chat-GPT\u00b9 by OpenAI.\nHowever, LLMs are facing several challenges. First of all, while training, each LLM is being given huge amount of texts from the various Internet web pages and quite often these materials are biased in one or another manner. It means, that when a user wants, for example, generate some essay, he receives the text that is likely to reflect the point of view of some specific society group.\nAnother risk in LLMs is hallucinations[29]. Hallucinations are the piece of information, incorrectly generated by LLM but pretending to be true one. They have become one of the stand-out problems in LLM development[11] for a while. For example, the risk of hallucinations is crucial when a user tries to get some piece of knowledge in the medical field.\nOur approach aims to tackle these issues and change the behaviour of LLMs to more reproducible and accurate in computational mass spectrometry field."}, {"title": "1.2 Research Objectives", "content": "The training of LLMs with vast amounts of data from the Internet, e.g. the total amount of data used for GPT-3 models training is 300 billion words[4], inevitably leads to worse reproducibility and accuracy of the answers. This problem results in several issues:\n\u2022 Misinformation and disinformation that a user gets through relying on the LLMs.\n\u2022 Also, in such cases some bias can occur, where the model outputs the information that can be potentially discriminatory.\nEspecially, these problems turn out to be crucial when someone uses LLM in the context of educational source. Hence, getting a incorrect piece of information that, may outcome in severe consequences, such as utilising it in the further usage. Moreover, these wrong data can go public by spreading.\nIn order to prevent such situations and help researchers in the field of computational mass spectrometry to get the checked and verified data from the scientifically proven prospective, we set our objective to improve existing LLMs for more accurate and reproducible answers."}, {"title": "1.3 Improving the Accuracy of LLMs", "content": "In the up-to-date literature, we can encounter the studying of the LLM's tuning techniques from several points of view. First, in terms of LLM development one of the core places takes the optimisation technique, used during training process. Among the set of possible function to pick some of them are in the spotlight of almost every research, e.g. SGD (Stochastic Gradient Descent)[23], Adagrad (Adaptive Gradient)[6], RMSProp, Adam (Adagrad+RMSProp)[12].\nSecond, prompt engineering process itself encompasses several methods that could be split into two groups:\n\u2022 With modifying the parameters of initial model\n\u2022 Without modifying the initial weights\nThe latter includes such approaches like P-Tuning, Prefix-Tuning and the former includes Fine-Tuning, encompassing Parameter-Efficient Fine-Tuning (PEFT).\nP-Tuning is a method to tune the initial model with trainable embeddings represented as (xi, yi); - labelled dataset and conducting further back-propagation to minimise the loss function [16]. Prefix-Tuning prepends a sequence of continuous task-specific vectors to the input, which is called a prefix and only these vectors are optimised by the model while training[15].\nFine-tuning is a technique that is employed to either update the initial weights of the model with the new data or train some additional layers. This method requires updating and storing all the parameters of the LLM[15]. Laying on the up-to-date models' sizes, this approach could be computational inefficient. To mitigate this issue, LORA (Low-Rank Adaptation) adapters have been proposed. The main idea is presented in the Figure 1.2.\nLORA adapters represent two matrices A and B, which consist of trainable weights, whereas model's initial pretrained weights stay immutable during training. Once the weights in A and B are optimised, matrices are multiplied by each other to fit the matrix dimension of initial weights. Therefore, by using LoRA we only need to store and load a small number of task-specific parameters in addition to the pre-trained model for each task, greatly boosting the operational efficiency when deployed[9].\nSince the development of the first LLMs, they have diffused in various spheres, including computational biology as well. In particular, one of their applications could be decoding the language embedded in DNA, e.g. universal genetic code, elucidating the translation of DNA into proteins, has recently garnered attention for deciphering biological functions through models based on BERT or GPT architectures, producing such models as DNABERT[10]. Another model that has been developed recently is DNAGPT, a generalised pre-trained model for DNA sequences[32].\nTherefore, there are several ways how to adapt existing LLM to particular needs in different domains, including computational biology."}, {"title": "2 Methodology", "content": "In total we managed to download several thousands of open-access research articles related to computational mass spectrometry. This was facilitated by securing an API key from the Semantic Scholar research database\u00b9, which enabled us to comprehensively access and retrieve relevant papers. Subsequent to acquisition, we have developed an algorithm tailored to automate the downloading procedure. This script was executed on the AIC-lab (Laboratory on AI for Computational Biology) server, operating in the server's background. The code which downloaded the articles is available in the Appendix 4.1.\nThe next step was to convert the articles from PDF format to plain text format. We tested the following converting approaches: in-built Python libraries for preprocessing PDFs (PyPDF42, PyMuPDF3, PDFMiner\u2074), GROBID[1]. The first three programs have shown approximately the same accuracy, having the preprocessing of two-columned documents as the main drawback. The key disadvantage was that in-built libraries consider lines in two-columns as one and concatenate them. Hence, among the aforementioned methods the GROBID stood out to be the best open-source working software program. GROBID converts PDF documents into XML/TEI encoded documents that are structured and tailored for technical and scientific texts. GROBID is based on a deep learning model implemented through the DeLFT library[2] for processing text for tasks like sequence labeling and classification, implemented in Python environment through the Java Embedded Python (JEP) interface and run in a Docker environment on AIC server.\nI completed Python scripts to extract the scientific text from the XML/IEI files. The extraneous elements such as headers, footers, and references were removed. The code responsible for the mentioned is available in the Appendix 4.2. The resulting corpus of data was then critically reviewed to ensure its suitability for the subsequent stages of our computational analysis. The final total amount of the ready-to-train data exceeds 2.5 Giga Bytes and was uploaded to the HSE CHARISMa cluster[14].\nIn summary, the chosen methodology for PDF preprocessing was a careful blend of existing technology and modern deep learning innovations, crafted to meet the demands of our research goals efficiently."}, {"title": "2.2 Fine-tuning the LLM", "content": "After paper downloading and text extraction, the fine-tuning approach was designed in the following way:\n\u2022 Choosing the LLM to fine-tune;\n\u2022 Selecting the range of optimizers and learning rates - parameters that are responsible for training speed, for research;\n\u2022 Sampling the text corpus for the grid search needs;\n\u2022 Equip the initial model with LoRA adapters;\n\u2022 Conduct grid-search on the sampled data to pick the best performing optimiser and learning rate;\n\u2022 Launch the training process on the whole text corpus with the best model parameters\nAs the choice of the base model to fine-tune, I selected the most downloaded one (21 mln. of downloads) from the Open Hugging Face AI Community[28], i.e. GPT-2, developed by the OpenAI[19]. There are three open-source versions of GPT-2 model: small, medium and large. They differ from each other in their sizes, i.e. GPT-2-small contains 124 million of parameters, medium - 355 million, large - 774 million. For this project we selected small version of GPT-2 due to limitations in computational power. Afterwards, the initial model was equipped with LoRA adapters, using the PEFT library [17], it allowed to extend the GPT-2 model with more trainable weights for better performance. In terms of optimizers, the ones that were named in Section 1.3, were selected, i.e. SGD, AdaGrad, RMSProp, Adam with the following learning rates: 10\u20132, 5*10\u22123, 5 * 10\u20134. From the full text corpus, obtained after the preprocessing of PDF files, we picked 10% of data, that was further used in the finding the most advantageous optimization parameters. The code of the grid-search procedure is available in the Appendix 4.3.\nTherefore, the model training of the full text corpus was conducted after obtaining the optimal parameters, described above.\nAlso, let us state some points essential for the fine-tuning process:\n\u2022 Tokenization a process of breaking down the text into smaller manageable pieces, i.e. words. Also a tokenizer is responsible for converting text data into numerical representation.\n\u2022 TextDataset is used for loading text, its tokenization and breaking into parts with the parameter block_size, specifying the maximum length of the tokenized sequences.\n\u2022 Data Collator is responsible for preprocessing data in batches and padding the sequences. Padding is a technique used in natural language processing (NLP) and deep learning to ensure that input sequences are of uniform length. The essence of padding technique is in adding special characters to the shorter sequence.\n\u2022 Gradient accumulation steps - the number of steps being accumulated before the parameters update.\n\u2022 Weight decay - a regularisation technique used to penalise the large weights to prevent overfitting.\n\u2022 Batch size - the number of training examples utilised in one iteration of the training process.\nThe aforementioned terms define the training process."}, {"title": "2.2.1 Mathematical properties of the model training", "content": "Let us explain some mathematical properties regarding the optimizers we used during the grid-search.\nSGD:\n\\theta^{(t+1)} = \\theta^{(t)} \u2013 \\eta \\cdot \\text{SGD}(\\nabla_{\\theta} L(\\theta^{(t)})) (2.1)\n, where:\n\u2022 \\theta^{(t)} : Model parameters at iteration t\n\u2022 \\eta : Learning rate, controlling the step size in the parameter space.\n\u2022 \\nabla_{\\theta} L(\\theta^{(t)}): Gradient of the loss function with respect to the model parameters at iteration t.\n\u2022 \\text{SGD}\\nabla_{\\theta} L(\\theta^{(t)}): Stochastic Gradient Descent, using stochastic estimates of the gradient for updating \\theta.\nAdagrad:\nG^{(t+1)} = G^{(t)} + (\\nabla_{\\theta} L(\\theta^{(t)}))^2 (2.2)\n\\theta^{(t+1)} = \\theta^{(t)} \u2013 \\frac{\\eta}{\\sqrt{G^{(t+1)} + \\epsilon}} \\nabla_{\\theta} L(\\theta^{(t)}) (2.3)\n, where:\n\u2022 G^{(t)} : The sum of squared gradients up to iteration t.\n\u2022 \\epsilon: A small constant to prevent division by zero.\nRMSProp:\nG^{(t+1)} = \\beta \\cdot G^{(t)} + (1 \u2013 \\beta) \\cdot (\\nabla_{\\theta} L(\\theta^{(t)}))^2 (2.4)\n\\theta^{(t+1)} = \\theta^{(t)} \u2013 \\frac{\\eta}{\\sqrt{G^{(t+1)} + \\epsilon}} \\nabla_{\\theta} L(\\theta^{(t)}) (2.5)\n, where:\n\u2022 G^{(t)} : The moving average of squared gradients.\n\u2022 \\beta: A decay factor for the moving average.\n\u2022 \\epsilon: A small constant to prevent division by zero.\nAdam:\nm^{(t+1)} = \\beta_1 \\cdot m^{(t)} + (1 \u2013 \\beta_1) \\cdot \\nabla_{\\theta} L(\\theta^{(t)}) (2.6)\nv^{(t+1)} = \\beta_2 \\cdot v^{(t)} + (1 \u2013 \\beta_2) \\cdot (\\nabla_{\\theta} L(\\theta^{(t)}))^2 (2.7)\n\\theta^{(t+1)} = \\theta^{(t)} \u2013 \\eta \\cdot \\frac{m^{(t+1)}}{\\sqrt{v^{(t+1)} + \\epsilon}} (2.8)\n, where:\n\u2022 m^{(t)} : First moment estimate of the gradient.\n\u2022 v^{(t)} : Second moment estimate of the gradient.\n\u2022 \\beta_1: Exponential decay rate for the first moment estimate.\n\u2022 \\beta_2: Exponential decay rate for the second moment estimate.\n\u2022 \\epsilon: Small constant to prevent division by zero.\nThe GPT-2 model works on conditional generation. Let G represent the model. Given an input string I, the model generates an output 0:\nO = G(I) (2.9)\nThe generation process seeks to optimize the model parameters OG to maximize the probability of generating the correct query. This is expressed as follows:\n\\theta_{G} = \\underset{\\theta_{G}}{\\arg \\max} \\log P(Q | \\theta_{G}). (2.10)\n\u2022 \\theta_{G}: Model parameters of the GPT-2 model.\n\u2022 \\log P(Q | I; \\theta_{G}): The logarithm of the conditional probability of a correct generated text Q given input I with model parameters \\theta_{G}.\nOptimisers are responsible for iterative parameter update of the model to achieve the minimisation of the negative log likelihood loss function, which is set by default in GPT-2:\nNLL(y) = -\\log(p(y)) (2.11)\n\\min_{\\theta} -\\sum_y \\log(p(y;\\theta)) (2.12)\n\\max_{\\theta} \\prod_y p(y;\\theta) (2.13)\nMathematically, the aim of applied LoRA adapters is described the following way:\nO = G(I) (2.14)\nThe generation process seeks to optimise the adapter parameters \\theta_A to maximise the probability of generating the correct text. This is expressed as follows:\n\\theta_{A} = \\underset{(\\theta_A)}{\\arg \\max} \\log P(O | \\theta_{G}, \\theta_A). (2.15)\n\u2022 \\theta_{G}: Model parameters of the initial model.\n\u2022 \\theta_A: Adapter parameters of the LoRA adapter.\n\u2022 \\log P(O | I; \\theta_{G}, \\theta_A): The logarithm of the conditional probability of a correctly generated text O given input I with \\theta_G and \\Theta_A"}, {"title": "2.3 Model architecture", "content": "The architecture of the GPT-2 model is presented in the Figure 2.1.\nGPT-2 is a decoder-only model, developed for auto regressive tasks, e.g. text generation. Each decoder is presented as a transformer block. Since we use GPT-2-small version, in our case N = 12. The principle of a decoder block logic is based on masked self-attention. Attention technique itself is illustrated in the Figure 2.2 and the computations made during self-attention, presented in the Figure 2.3 are practically the same:\n\u2022 Each word x has its vector called \"Query\" - Qx;\n\u2022 It is multiplied by the \"Key\" (K\u2081) vector of every word in the sequence except x;\n\u2022 This score is used in softmax function: \\sigma(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}} for i = 1,2,..., K;\n\u2022 The probability, obtained after the softmax application, is multiplied by the corresponding \"Value\u201d (Vi) vector;\n\u2022 Attention(Qx, Ki, Vi) = softmax(\\frac{Q_x K_i^T}{\\sqrt{d_k}}) \\cdot V_i, where du is a vector of the dimensionality Ki, Vi, used as a normalisation.\nEach attention head, i.e. decoder block has its own query, key and value matrices(c_attn). Hence, after the last decoder layer, for each word in the sequence, its resulting vector is the concatenation of all (12 in our case) obtained vectors. The next step is the multiplication of the concatenated vector by the weight matrix (c_proj) that projects the results of the attention heads into the output vector of the self-attention sublayer. Having completed this step, the vector is sent to the multi-layer perceptron (mlp) and after proceeding throughout all 12 transformer blocks we get the output.\nFinally, this output is passed to lm_head after normalisation in order to get the next predicted word.\nLORA adapters, described above with r = 4, are applied to the following layers: c_attn, c_proj and Im_head. The overall number of parameters is 124.410.624, among them there are 4.876.832 trainable. i.e. 3.92%. Detailed presentation of the initial weights is presented in the Figure 2.4."}, {"title": "2.4 Results", "content": "As it was stated in the Section 2.2, we trained the model on the sample from the full text corpus to find out the best learning rate and the optimiser among selected. We employed Google Colab server with NVIDIA GPU (Graphics processing unit), that accelerates neural network training, provided by free with the limits. We chose several features as the criteria for all combinations learning rate - optimiser:\n\u2022 Validation error\n\u2022 Running time, sec\n\u2022 Memory usage, MB\n\u2022 Electricity usage, kWh (which can be interpreted as carbon emission and reflects environmental impact of the model)\nAccording to the learning rates, corresponding number of steps was chosen. For instance, if for learning rate = 0.01 100 steps are chosen, then for learning rate = 0.005 twice more steps are needed to converge. The final decision was also seriously influenced by the limits, set on the CHARISMA server[13], i.e. 4000 GPU hours per student. Having conducted the experiments, changing learning rates and optimizers of the model, we got that due to the trade-off between the validation error, running time, memory usage and electricity usage, the best optimiser stood out to be SGD with the learning rate = 0.01. Since for each learning rate we got practically the same validation errors with all optimizers, the key benchmarks for the choice were memory and electricity usage, keeping in mind the relation between running time and CHARISMA limits. In addition, lower electricity usage results in economics benefits, saving financial resources as well. For instance, reducing the training by one kilowatt-hour (kWh) saves 0.1$ in Russia. The detailed results of the research are stored in the Table 2.1."}, {"title": "2.5 Full Model Fine-Tuning", "content": "For the final GPT-2 fine-tuning we employed the parameters picked previously in the Section 2.4, i.e. learning_rate = 0.01 and SGD as an optimizer. Also the full text corpus of 2.5 GBs was divided into training and test parts.\nI launched the training process with 4 GPUs and the total training time was 400 hours (16.5 days). The code of the model training is presented in the Appendix 4.4. The final model was named MS-GPT."}, {"title": "2.5.1 Evaluation of the Full-Trained Model", "content": "To compare the final fine-tuned model with the initial GPT-2, we employed several metrics, used in the text generation validation:\n\u2022 BLEU (Bilingual Evaluation Understudy)\nBLEU scores are widely used to evaluate the quality of machine-generated information, especially in the case of machine translation. It measures the similarity between the generated response and one or more reference responses. BLEU's formula is explained as follows:\nBLEU = BP \\times exp (\\sum_{n=1}^N w_n \\log(precision_n)) (2.16)\n, where:\n(BP) is the brevity penalty to account for short generated responses, BP = min(1, \\frac{referencelength}{generatedlength}).\nTo illustrate, let's consider a toy example for BLEU computation. Let's assume that for unigrams, bigrams, and trigrams, the precision is 0.8, 0.6, and 0.4 respectively, and the weights assigned to these precisions are all 1/3. To compute unigram precision, we count the number of individual words that are present in both the generated response and the reference response. We then divide this count by the total number of words in the generated response The same logic holds for bigrams and trigrams also. Let's also assume the brevity penalty (BP) is 1.\nThen, substituting these values into the BLEU formula:\nBLEU = 1 x exp (\\frac{1}{3}.log(0.8) + \\frac{1}{3}.log(0.6) + \\frac{1}{3}.log(0.4)) (2.17)\nBLEU = 1 x exp ((\\frac{1}{3}.(-0.223) + (\\frac{1}{3}.(-0.511) + (\\frac{1}{3}.(-0.916)) (2.18)\nBLEU = 1 \\times exp(-0.55) (2.19)\nBLEU \\approx 0.576 (2.20)\nSo, in this example, the BLEU score for the generated translation would be approximately 0.576.\n\u2022 ROUGE is a set of metrics for evaluating the quality of data collected or generated. This includes measures such as ROUGE-N (n-gram overlap) and ROUGE-L (longest repetitive subsequence). The ROUGE-N metric is defined as:\nROUGE-N = \\frac{\\sum_{reference} \\sum_{n-grams} Count_{match}}{\\sum_{reference} \\sum_{n-grams} Count_{reference}} (2.21)\n, where:\n(Countmatch) is the count of matching n-grams in the generated response and reference.\n(Countreference) is the count of n-grams in the reference.\nLet's continue with an example for ROUGE-N:\nConsider the same example sentence \"The cat is on the mat.\" and its reference translation \"The cat is sitting on the mat.\" We want to compute the ROUGE-1 score for this translation.\nFirst, we count the number of unigrams (individual words) that appear in both the generated response and the reference:\n\"The\": Appears in both.\n\"cat\": Appears in both.\n\"is\": Appears in both.\n\"on\": Appears in both.\n\"the\": Appears in both.\n\"mat\": Appears in both.\nSo, there are 6 matching unigrams. The total number of unigrams in the reference is 7.\nTherefore, the ROUGE-1 score is:\nROUGE-1 = \\frac{6}{7} \\approx 0.857 (2.22)\nThis indicates that 85.7% of the unigrams in the generated response match with those in the reference.\n\u2022 Perplexity is a metric commonly used in language modelling to evaluate how well a probability distribution predicts a sample. For a language model, perplexity is calculated as:\nPerplexity = 2^{- \\frac{1}{N} \\sum log_2 P(w_i)} (2.23)\n, where:\n(N) is the number of words in the generated response.\n(P(w_i)) is the probability assigned to the word (w_i) by the language model.\nContinuing with our example, let's say we have a language model that assigns the following probabilities to each word in the generated response:\n(P(\"The\") = 0.8)\n(P(\"cat\") = 0.7)\n(P(\"is\") = 0.6)\n(P(\"on\") = 0.5)\n(P(\"the\") = 0.8)\n(P(\"mat\") = 0.9) Substituting these values into the perplexity formula:\nPerplexity = 2^{-(\\frac{1}{6}(log_2(0.8)+log_2(0.7)+log_2(0.6)+log_2(0.5)+log_2(0.8)+log_2(0.9))} (2.24)\nPerplexity = 2^{-(\\frac{1}{6}(-0.322-0.514-0.737-1.0-0.322-0.152)} (2.25)\nPerplexity = 2^{-0.509} (2.26)\nPerplexity \\approx 1.28 (2.27)\nSo, the perplexity of this language model for the given response would be approximately 1.28.\nThese examples illustrate how each metric works and how they can be applied to evaluate the quality of machine-generated responses. These measures provide valuable insights into various aspects of model performance, including fluency, coherence, and similarity in contextual responses The combination of these metrics can provide detailed analysis when comparing with GPT-LLMs. More specifically, BLEU and ROUGE measure response similarity, whereas perplexity measures the ability of the model to predict a given response distribution.\nTo measure and compare the models' (GPT-2 and MS-GPT) performance we required a certain amount of reference sentences to compute the aforepresented metrics between the generated lines and their respective sources. For this purpose, we needed to retrieve some ground truth in the field of mass spectrometry, not any private opinion, but clear facts, which can be established using more or less the same set of words and phrases. That is why we took a bunch of examples of random definitions from the mass spectrometry dictionary[18]. Definitions, used for models' evaluation are presented below:\n\u2022 Bottom-up proteomics is a method of protein identification that uses proteolytic digestion before analysis by liquid chromatography and mass spectrometry.\n\u2022 Imaging mass spectrometry is a procedure used to form chemically selective images of an object based on the mass spectrometric detection of ions desorbed from its surface.\n\u2022 Autodetachment is a process whereby a negative ion in a discrete state with energy greater than the detachment threshold loses an electron spontaneously without further interaction with an energy source.\n\u2022 Gas chromatography-mass spectrometry (GC-MS) is a technique by which a mixture is separated into individual components by gas chromatography, followed by detection with a mass spectrometer.\n\u2022 Mass spectrometry is study of matter through the formation of gas-phase ions that are characterized using mass spectrometers by their mass, charge, structure, and/or physico-chemical properties.\n\u2022 Peptide mass fingerprinting (PMF) is a method for protein analysis where an unknown protein is chemically or enzymatically cleaved into peptide fragments whose masses are determined by mass spectrometry.\n\u2022 Peptide sequence tag is sequence of a peptide ion fragment masses that can be used to aid in the identification of the amino acid sequence.\n\u2022 Photodissociation is a process wherein the reactant ion or molecule is dissociated as a result of absorption of one or more photons.\n\u2022 Pneumatically assisted electrospray ionization is electrospray ionization in which the nebulization of the liquid stream is assisted by a concentric stream of gas.\n\u2022 Post-acceleration detector (PAD) is a detector in which a high potential is applied after m/z separation to accelerate the ions and produce an improved signal.\n\u2022 Product ion spectrum is a mass spectrum in which the appropriate m/z separation analysis function is set to record the product ions of a selected precursor ion selected by m/z\".\n\u2022 Collision cell is a chamber in the ion path between m/z separation elements, or between ion source acceleration region and the first analyzer, in tandem mass spectrometry in space configurations.\n\u2022 Collision quadrupole is a transmission quadrupole to which an oscillating radio frequency potential is applied so as to focus a beam of ions through a collision gas or buffer gas with no m/z separation other than low m/z cut-off.\n\u2022 Continuous-flow fast atom bombardment (CF-FAB) is a variant of fast atom bombardment in which the mixture of analyte and liquid matrix is continuously supplied to the sample probe tip.\n\u2022 Deconvoluted mass spectrum is a mass spectrum processed with an algorithm designed to extract a desired signal or signals from raw experimental data in which the desired signals have been complicated (convolved) by some interferences or in some other way.\n\u2022 Direct infusion is a method of liquid sample introduction in which the sample is continuously flowed into a mass spectrometer ion source.\n\u2022 Dynamic exclusion is a software method used to minimize repeat selections of identical precursor ions for collision-induced dissociation in replicate chromatography-tandem mass spectrometry analyses of complex mixtures.\n\u2022 Hydrogen/deuterium exchange (HDX) is an exchange of hydrogen atoms with deuterium atoms in a chemical species in solution prior to introduction into a mass spectrometer, or by ion/molecule reaction with a neutral gas inside a mass spectrometer.\n\u2022 Hyphenated mass spectrometry technique is an analytical technique in which mass spectrometry is interfaced with a pretreatment step, most often chromatographic separation but many other combinations are possible.\n\u2022 Inductively coupled plasma-mass spectrometry (ICP-MS) is a mass spectrometry technique based on coupling a mass spectrometer with an inductively coupled plasma as an ion source that both atomizes samples into their constituent atoms and ionizes them to form atomic cations.\n\u2022 Ion funnel is series of stacked ring electrodes with progressively decreasing inner diameter to which a combined radio frequency and fixed potential is applied.\n\u2022 Ion gate is a set of plates or grid of wires in an ion mobility spectrometer, time-of-flight mass spectrometer, or other mass spectrometer that is used to apply a pulsed electric field with the purpose of selectively deflecting charged particles.\n\u2022 Ionic dissociation is a dissociation of an ion into another ion of lower mass and one or more neutral or ions with a lower charge.\n\u2022 Charge number, z is an absolute value of charge of an ion divided by the value of the elementary charge (e) rounded to the nearest integer.\n\u2022 Static secondary ion mass spectrometry (SSIMS) is a method of secondary ion mass spectrometry using low current densities for analysis of sample surface components, in contrast with dynamic secondary ion mass spectrometry.\n\u2022 Tandem mass spectrometer is a mass spectrometer designed for mass spectrometry.\n\u2022 Accelerating potential is an electrical potential difference used to impart translational energy to ions.\n\u2022 Accelerator mass spectrometry (AMS) is a mass spectrometry technique in which atoms and molecules from a sample are ionized, accelerated to MeV energies and separated according to their momentum, charge, and energy, allowing high discrimination for measurement of isotope abundances.\n\u2022 Auxiliary gas is a gas used in a spray ion source in addition to the nebulizing gas to aid in solvent removal.\n\u2022 Beam mass spectrometer is a mass spectrometer in which an ion beam accelerated from the ion source is transmitted through a m/z analyzer, or analyzers, to the detector.\nThe total number of instances is 30. Since we are dealing with GPT models, we need to pass some special prompt in order to give some instructions for the model to complete the task, e.g. for the definitions above, the prompts, passed to the models were the following:\n\u2022 Give the definition: Bottom-up proteomics is\n\u2022 Give the definition: Data-dependent acquisition is\n\u2022 Give the definition: Imaging mass spectrometry is\n\u2022 etc.\nThe detailed code is presented in the Appendix 4.7."}, {"title": "2.5.2 Statistical Testing", "content": "After passing all prompts through the competing models", "scores": "BLEU", "test[20": "due to the paired data with the following hypotheses:\nBLEU:\n\u2022 Ho: BLEUMS-GPT = BLEUGPT-2\n\u2022 H1: BLEUMS-GPT > BLEUGPT-2\nROUGE-1:\n\u2022 Ho : ROUGE \u2013 1MS-GPT = ROUGE \u2013 1GPT-2\n\u2022 H\u2081 : ROUGE - 1MS-GPT > ROUGE \u2013 1GPT-2\nROUGE-L:\n\u2022 Ho : ROUGE \u2013 LMS-GPT = ROUGE \u2013 LGPT-2\n\u2022 H\u2081:"}]}