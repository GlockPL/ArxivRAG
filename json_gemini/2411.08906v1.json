{"title": "Assessing the Auditability of AI-integrating Systems: A Framework and Learning Analytics Case Study", "authors": ["Linda Fernsel", "Yannick Kalff", "Katharina Simbeck"], "abstract": "Audits contribute to the trustworthiness of Learning Analytics (LA) systems that integrate Artificial Intelligence (AI) and may be legally required in the future. We argue that the efficacy of an audit depends on the auditability of the audited system. Therefore, systems need to be designed with auditability in mind. We present a framework for assessing the auditability of AI-integrating systems that consists of three parts: (1) Verifiable claims about the validity, utility and ethics of the system, (2) Evidence on subjects (data, models or the system) in different types (documentation, raw sources and logs) to back or refute claims, (3) Evidence must be accessible to auditors via technical means (APIs, monitoring tools, explainable AI, etc.). We apply the framework to assess the auditability of Moodle's dropout prediction system and a prototype AI-based LA. We find that Moodle's auditability is limited by incomplete documentation, insufficient monitoring capabilities and a lack of available test data. The framework supports assessing the auditability of AI-based LA systems in use and improves the design of auditable systems and thus of audits.", "sections": [{"title": "Introduction", "content": "Artificial Intelligence (AI) significantly impacts the field of Learning Analytics (LA). LA itself is gaining relevance in higher education [11], K-12 classes [58], and for virtual education [21]. AI adds to the utility of LA elements of educational data mining [11], deep learning capabilities, machine learning [57], or predictive and prescriptive analytics [64, 82]. AI in LA offers data-driven insights into learning processes and students' behavior to predict learning success, risks of failure or drop-out, and to prescribe proactive measures [4, 72]. Above that, AI technologies promise opportunities to improve learning situations and outcomes especially for disadvantaged and struggling students [40].\nHowever, AI in LA entail ethical issues [62, 63] and their utility or maturity in education are unclear [18]. The potential threat to equality and equity principles in education raised research and practitioner interest to mitigate discriminatory effects of AI models in LA [66, 63]. Above that, ethical concerns create the urgency for adequate legislation to counter negative effects or prevent biased systems. AI and AI-driven LA face regulation, such as the European AI Act [24], or frameworks to impose ethical requirements on AI products [75, 67], and to mitigate potential negative effects and discriminatory biases [12, 59]. For this case, the \"AI Act\" [24] aims to regulate \u201chigh-risk\u201d AI systems, like AI-based LA because of their impact on personal educational success.\nRegulatory frameworks, including the AI Act, require audits of AI systems that certify their legal and ethical compliance [14, 75]. The AI Act mandates two types of audits for high-risk AI systems: conformity assessments before deployment and post-market monitoring after system deployment [24]. Audits provide accountability and transparency, which is also necessary for establishing trust in AI systems [80, 16]. On a practical level, audits allow stakeholders, such as system providers or deploying institutions, regulators, and subjects of the systems' decisions, to understand how the system decides and to identify and correct biases or errors [69]. Audits of AI-based LA systems are effective in discovering and tackling algorithmic bias [63]. However, audits struggle with systems that are inaccessible, opaque, or proprietary [27]. We argue that AI-based LA systems must be auditable for any audit to achieve its results. Therefore, our research questions are:\nRQ1 How does a framework to assess the auditability of AI-based LA support auditors to judge a given system?\nRQ2 How does a framework to assess auditability improve the development of AI-based LA?\nWe define AI systems as software systems that implement methods of machine learning. AI-based LA systems implement machine learning methods to leverage learning data for analysis, predictions, and prescriptions in educational contexts [11, 57].\nIn section 3, we define auditability and discuss the limitations of auditability of AI-integrating LA systems. Auditability requires a system to collect evidence about specific claims and make it accessible for independent assessments [78, 81]. In section 4, we present our framework for assessing the auditability of AI-based LA systems. We apply our framework to Moodle's AI-based LA system (section 5) and a prototype LA (section 6). In section 7, we discuss the implications and limitations of our work and conclude in section 8."}, {"title": "Literature Review", "content": ""}, {"title": "Audits of AI systems and auditability", "content": "Technical, legal, and ethical reasons mandate audits of AI systems to ensure accountability for accurate, compliant, and fair systems [61, 25, 10]. For this purpose, auditing techniques, for example, from finance, are adapted for AI systems [51]. However, there are no standards for audit quality [3], and residual risks usually remain uncertain [42].\nAn audit analyses if an AI system complies with legal regulations, organiza- tional standards, or ethical values. It compares claims made by stakeholders, like developers or deployers of AI-based LA tools, to the system's actual behavior. Claims concern an AI-based LA system's validity, utility, and ethics: i.e., its models, components, data sets, or scopes accurately and reliably measure what they intend to measure, and are suitable for the intended purpose (validity); how useful and effective the system is in real-world fulfilling its application (utility); ethical, moral, or legal standards are considered and satisfied (ethics) [48, 10]. AI's functionality, application field, and associated risks require interdisciplinary competencies and skills in mandating and conducting audits [45]. Auditors re- cover \"auditable artifacts\" [10] to validate whether an AI system is implemented and operating as claimed. AI systems' design and implementation principles affect audits and processes of assessing claims, actual behavior, and auditable evidence [46, 10, 25].\nAuditability is given when a system is reviewable independently [80, 81]. [78] conceptualize auditability as a) the system provides information on how relevant values should be used or produced (claims), b) the system generates information on how relevant values are used or produced (evidence), and c) stakeholders can validate these claims based on the provided evidence. The complexity of AI systems creates special requirements for audits and, thus, for a system's auditability [47]. [46] propose a framework for auditability that focuses on the entire life cycle of AI systems, including training data, models, and organizational governance processes. [14] approach auditability of AI systems from a cyber security perspective where increased system complexity impedes system auditability. [61] propose a joint internal audit process of auditors and auditees, who provide claims and artifacts as evidence, and create a remediation plan to mitigate risks [61].\nClaims and suitable evidence to validate claims are cornerstones of auditable systems. In practice, these usually are not readily available. Various challenges and limitations exist, decreasing the auditability of AI-integrating LA systems at all stages [66, 27]."}, {"title": "Formulating claims", "content": "Claims are normative statements on a system's function- ality, scope, and purpose. System providers and system deployers define claims in system standards, targeted fields of application, scope and use cases, or as part of documentation [70]. Other claims stem from ethical or moral standpoints, laws, regulations, and standards that guide software implementation and use [17].\nThe first challenge for audits is defining verifiable claims about a system's validity, utility, and ethics. Ideally, these are quantifiable and measurable [26], and provided by the audited system provider [17, 61]. For example, an adequate utility claim for an AI-based LA system could be that it reduces the drop-out risk for students of a specific course by x%. Or that a specific ML-model based on student behavior data accurately predicts students at risk to fail. More difficult would be the claim that a system is robust to biased data from underrepresented groups. Quantifiable performance indicators are usually easier to assess than ethical or moral values. In practice, especially ethical claims remain vaguely defined by vendors or deploying organizations, making auditors responsible for their operationalization [43]. Although several auditing frameworks have been suggested (e.g., [61]), initiatives sought to standardize AI system audits [68, 2, 13], auditors still are missing harmonized standards for AI audits [22, 50]. Without guidelines, auditors must define and decide how to measure ethical values individually and subjectively [62, 67, 45]. Auditors are at risk of introducing their own biases into audits. For example, the definition of fairness may vary depending on context (i.e., equality or equity of the AI-based LA interventions). Fairness definitions also may be mutually exclusive [36]. Further, intersectional dimensions of inequality could neglected subsets of groups and introduce additional bias, e.g., when only gender and ethnicity are considered, but the social background is overlooked. For instance, auditors may assign test cases- -individuals to the wrong group or miss underrepresented groups [12, 71]."}, {"title": "Collecting evidence", "content": "For AI-based LA, evidence is \"relevant information about its execution\" [5] that allows us to analyze and trace errors. Auditees should enable evidence collection by organizational structures and processes that document a system's operation, while developers should provide accessible AI systems [9, 70].\nEvidence proves or rejects derived claims. However, sometimes, neither the system nor its raw sources (program code, model weights, data used for training and testing) are accessible to auditors, e.g., for proprietary HR software [60] or predictive policing tools [6]. In such cases, auditors must simulate algorithms and models to conduct data-based audit methods that assess the fair treatment of inputted data [6]. This limits the audit's effectiveness and is additionally aggravated by lacking balanced test data that represents marginalized groups [27]. Independent code-based audit methods are not feasible for proprietary systems, either. Lastly, the documentation of AI-integrating systems-proprietary or open-is often incomplete [6, 73, 60]."}, {"title": "Verifying claims", "content": "Auditors with system access and evidence can verify whether an AI-integrating system meets the derived claims. However, AI-based LA systems present challenges when designing test cases and selecting test data. AI can handle a broad range of input data and assume more possible states than common software [14, 47]. Therefore, a diverse and substantial amount of test data is required [74, 27]. Further, pre-deployment audits might not cover every possible use case and differ from de facto operational use [74]. After deployment, models can be updated by learning from new training data or through feedback loops, and introduce bias in the process [14, 26, 9]. Therefore, tests of an AI are not necessarily realistic and make continuous auditing necessary [19, 9, 50, 47].\nWhen verifying claims, auditors interpret the obtained output of a running AI-based LA system, which is challenging because AI-integrating systems have more complex and interrelated components than regular software [47]. Complex AI algorithms are often considered \u201cblack boxes\" that require explanations of the model output to users or auditors [17, 33], which must be actively enabled as well."}, {"title": "Enabling auditability of AI-based LA systems", "content": "Because of the limited auditability of AI-integrating systems, some audits rely on self-audits and require auditees to answer questions about the design principles and guarantees to functionality and compliance [61]. While this is a valid approach, we argue that AI-based LA systems must be designed with auditability in mind to enable independent audits. Even though such systems are complex, system providers and deploying institutions can take steps to enable independent auditability. Auditable AI-based LA systems require planning, documentation, the implementation of specific functionalities, such as logs, APIs, monitoring tools or explanations. Further, access to the system sources, such as program code, model configuration, or data facilitates realistic auditing under field conditions.\nThe following suggestions concern system development and responsible in- stitutions deploying AI-based LA. On the one hand, LA's auditibility requires systematically incorporated auditability principles early on in the development process; on the other hand, auditability requires institutional settings and infor- mation systems with well-documented organizational and technical processes, a clear scope of system utility, and openness to third-party auditors.\nPlanning for auditability Sufficient auditability will only be reached if it is planned for during the system design process. To help the completeness of evidence, \"accountability plans\" outline what and how information should be captured [56]. Workflows to increase auditability include logging model training and validation results, storing model metadata, and continuous monitoring [44, 47]. Plans should also determine applicable definitions of ethical standards and data for their evaluation to account for vague operationalizable claims [28, 67, 41]. Based on the accountability plan, institutions can adjust organizational processes for auditability (project and risk management, design and development processes).\nDocumentation Auditability is further influenced by the completeness of documentation. The AI Act requires documentation for high-risk AI systems on the system in general, the models, and the relevant data [24]."}, {"title": "Providing sources", "content": "AI-based LA results are often hard to reproduce [34]. Au- ditability can be increased by providing raw sources of the system for evaluation purposes, including its source code, models, model weights and the training and test data [73, 13]. Privacy issues may prohibit access to raw data. For such cases, auditors could collect or create (synthesized) test data [20]. Additionally, synthetic data can be helpful when data for underrepresented minorities is scarce [20]."}, {"title": "Implementing auditability", "content": "Enabling auditability of AI-based LA requires specific system functionalities for making system information accessible for auditors, such as secure system access [9], logging [19, 2, 16], monitoring tools [19, 5], and explanations for model behavior [65, 33].\nSecure system access for external auditors, e.g., via APIs, is a prerequisite for an audit [9, 79]. [69] show that APIs allow systematic tests of scenarios based on the system's claims. APIs can also enable secure third-party access to logs [7]. Auditors can use logs to understand data flows [25], or the recorded production process of predictions, data sets, and results [39]. Therefore, logs enhance the auditability of AI systems [17, 65]. Monitoring tools help to analyze performance, detect model behavior changes, and recognize violations of (ethical) constraints [19]. They track various aspects of a system, such as model input, the environment of use, internal model properties, and model output [8]. Post-market monitoring after deployment is a constant part of the AI life cycle [7, 47] and required under the AI Act [24]. Explainable AI assists in making sense of models and their output. Feature importance or counterfactual explanations are part of the user interface and can help users or auditors to understand and judge, for example, what features contribute to classifications of students as \"at risk to fail\" [65]."}, {"title": "A framework for assessing auditability of A\u0399 systems", "content": "Based on the discussion of audits and auditability of AI systems and methods to enhance the auditability of AI-integrating systems, we conceptualize a framework to assess auditability of AI-based LA systems and identify opportunities for their improvement. Figure 1 visualizes the framework. Any audit process has three steps displayed from the bottom to the top: first, auditors derive verifiable claims about the system, then identify, generate, and collect suitable evidence, and finally validate the claims based on the evidence.\nVerifyable claims Developers or deploying organizations ensure the properties of the AI-based LA system and the processes in which it is applied. Auditors can derive verifiable claims from such assurance statements, which form the benchmark for the actual functioning of AI-based LA [17]. Claims concern validity: are methods correctly applied in the system, and is the system output correct? Utility: is the system's functionality helpful in its use case? Finally, adherence to underlying ethical principles: does a system comply with applicable law (GDPR) or latent social, organizational, or societal norms, e.g., corporate culture, accessibility, or diversity, equity, and inclusion (DEI) goals? [45]\nEvidence Once auditors define claims, they identify, create, and collect evi- dence [61]. Evidence comprises different subjects: system, model and data; and can take various forms as documentations, raw sources like source code, model weights and raw data, or logs [61, 17, 73, 13, 24].\nDifferent evidence subjects assess specific aspects of AI-based LA systems. For the system, evidence should prove the system's functionality and its limitations; evidence must legitimize the underlying design and implementation choices and organizational processes [37, 13, 61, 24]. Evidence for the implemented models concerns algorithms in use, model parameters, and model performance indicators [13, 49, 24]. Evidence on data informs about structure, provenance, and quality of test, training, or production data [13, 8, 31, 75, 24].\nMeans of validation In the validation step, auditors access and assess evidence to validate the claims about the AI-based LA system's validity, utility, and ethics. Means to validate can be integrated into the system either as an interface to access raw data via APIs for further testing, in the form of monitoring tools to observe system output and parameters and deliver readily interpretable (real- time) results or as explainable AI principles on user interfaces [27, 79, 19, 15, 47].\nUtilizing the Framework The framework aims to assess the auditability of an AI-based LA system and facilitate the design of auditable systems. The identified claims and their specificity dictate what evidence subjects (system, model, data) and evidence types (sources, documentation, logs) are necessary to validate them. The evidence types, in turn, specify the technical means of validation. Auditors judge whether the evidence is sufficiently available and accessible.\nGathering claims requires a heuristic search, document analysis, and Q&A- interviews with responsible positions to determine the claims' relevance and hierarchy. Collecting evidence depends on subject and type, and is closely related to technical validation means. The most important type of evidence is documentation since it is the most accessible type to obtain and understand [13]. Arguably, the most challenging evidence could be source codes or logs of proprietary or security-sensitive systems [6, 60]. However, these forms of evidence may be necessary to complete information from the documentation or establish credibility [13].\nIn practice, the framework can be used to define ex-ante responsibilities in the audited organization for providing claims and evidence. It can be operationalized as a checklist to control the auditability of a system as an initial audit step or in the development cycle of a system. Since system development is an ongoing process, the framework assists in assessing the auditability on the developers' side and offers guardrails for quality assurance measures.\nIn the following two sections, we demonstrate our framework's practical application and utility to assess the auditability of AI-based LA systems. We assess the auditability of Moodle, a well-renowned tool with an integrated student dropout prediction system, and a research prototype tool that predicts student drop-out. The tools are AI-based LA that are potentially \"high-risk\" under the AI Act. Dropout prediction models have repeatedly been proven to work better for majority groups better represented in training data [29, 62]. Therefore, there is a risk that some groups of students benefit less from the AI-based LA module than others."}, {"title": "Case study I: Auditability of Moodle's student dropout prediction system", "content": "We chose Moodle because it is a commonly used open-source learning management system. Moodle's dropout prediction system aims to prevent students from dropping out of a course [52]. The software ships with an un-trained machine learning model (a model configuration) that, once trained on a particular Moodle platform, predicts whether a student is likely to drop out of a course [54, 52]. A model configuration can be tested in an \"evaluation mode\" before going live [54]. \nClaims\nThe first step when verifying auditability is determining which claims concerning validity, utility, and compliance to ethical norms are made [45]. Therefore, we consult the documentation of Moodle's student dropout prediction system and additional literature."}, {"title": "Required evidence", "content": "We established that evidence in the form of documentation, raw sources and logs is suitable to verify claims. Evidence can concern aspects of Moodle's dropout prediction system (the system), the dropout prediction model, and the underlying data. This subsection examines which evidence is required to validate which claim. The claims and evidence subjects are indicated in cursive.\nValidity To prove v1 (sufficiently good predictions), the most reliable way would be to reproduce the quality assessment conducted by [52]. To evaluate the dropout prediction configuration, an auditor requires access to a Moodle system with test data (data sources) to calculate the model performance. We call this type of system a \"test system\". In the absence of openly available test data, additional documentation on data quality requirements is helpful to acquire suitable test data. In this use case, data can only be acquired by exporting data from a Moodle platform, not through synthesis. This is because of the lack of seed data. Even if sufficient information on data properties was available, auditors cannot import data for model input, because the model requires meaningful related data, which cannot be synthesized [27].\nIf suitable test data cannot be obtained to reproduce MoodleHQ's quality assessment, the reliability of the evaluation conducted by [52] can at least be judged. The auditor needs to know the details of the quality evaluation (organizational processes) and the properties of the used training and test data (data structure, provenance, and quality). Additionally, information on model algorithms (training and testing, including feedback loops) and which model parameters were chosen and why (system design and implementation choices) could help to identify erroneous implementations that lead to biased results.\nTo prove v2 (cognitive depth and social breadth are valid indicators), auditors need to verify whether this claim is scientifically supported by studies. This information can be expected in the documentation on the application of the \"Community of Inquiry\" framework into cognitive depth and social breadth indicators (system design and implementation choices). Trust can be increased by examining the importance of each indicator on the predictions made on a \"production systems\u201d\u2014a Moodle instance that is using the dropout prediction model (data provenance)."}, {"title": "Means of validation", "content": "In this subsection, we assess to what extent Moodle's dropout prediction system implements interfaces to access and collect evidence for validating claims.\nAPI Moodle does not provide an API for secure third-party access to the dropout prediction system. However, the internal \"Analytics API\" may be used to access and extend the machine learning capabilities of Moodle with a plugin [52]. In a different publication, we successfully used this approach to increase the auditability of Moodle [27]."}, {"title": "Evidence accessibility", "content": "Finally, we analyze whether the required evidence is sufficiently available and accessible for validating the claims. The claim and evidence subject are indicated in cursive.\nValidity To prove v1 auditors should reproduce the dropout prediction model performance assessments from [52]. The performance assessment requires a test system to obtain logs of the model performance. However, as mentioned above, data sources for the test system are not publicly available. The auditor must acquire suitable test data to set up a test system. Quality requirements for the acquired data are available [54, 52] (data quality).\nIf no test system is available, the validity of the quality evaluation may be estimated by reviewing information on the properties and production of the system: organizational processes, data structure, data provenance, data quality, model algorithms, model parameters and system design and implementation choices. Basic information on the structure and provenance of the data used for MoodleHQ's quality assessment describe [52]. They also elaborate on the data quantity. The model training algorithms-logistic regression and a feed- forward neural network\u2014(model algorithm) and the model evaluation methods are documented as well (organizational processes). Furthermore, a possible feedback loop exists when prediction feedback is included in the training data for future models [54] (model algorithms). The configurable dropout prediction model parameters (analysis interval and context) are also documented [54, 52].\nHowever, auditors cannot rely on evidence from the documentation alone to validate MoodleHQ's quality evaluation. They will need to analyze relevant parts of the source code: the values for fixed model parameters (like the number of training epochs, learning rate, or batch size) are neither documented nor logged and can only be found in the source code. Also, a source code analysis by [73] found an undocumented 500MB limit for training data. Even by complementing information from the documentation with a source code analysis, some evidence that would help assess validity-related claims is missing. This concerns firstly, information on additional and more detailed data quality aspects, such as data completeness or data quantity per student or course; and secondly, information on whether model parameters, such as learning rate and batch size, were considered and if so why they were discarded (system design and implementation choices). To conclude, available and accessible evidence is insufficient to fully validate claim v1 that the dropout prediction model correctly predicts dropout risks.\nThe documentation on applying the \"Community of Inquiry\" framework into cognitive depth and social breadth indicators (system design and implementation choices) offers a starting point for auditors to validate v2 [54, 52]. MoodleHQ does not provide studies that support their indicator definitions. If an auditor can access a production system, she could review the explanations logged for the model's predictions (data provenance) and evaluate the soundness of the chosen indicators. Since this type of access could be challenging, we deem the available evidence insufficient to effectively assess claim v2 that cognitive depth and social breadth are valid indicators.\nUtility To assess the utility-related claim ul, the first step is to review the available documentation. The scientific theory behind the choice of model features is explained thoroughly [52], but no studies on the model's impact are documented (design and implementation choices). Project management, design, and development processes are not documented either (organizational processes). Production system-specific utility may be analyzed by viewing aggregated information about the feedback given by humans for predictions (model performance). In summary, the available evidence does not validate the claim ul that the dropout prediction system reduces dropout rates in online courses.\nEthics To validate ethics-based claim el, documentation on system function- ality and limitations can be reviewed. The screenshots in the Moodle docu- mentation show that users viewing dropout predictions are made aware of the uncertainty of predictions [54]. However, users do not appear explicitly informed that the AI-based LA system calculates the predictions. The available evidence allows us to reject claim el that AI-created predictions are marked as such.\nDocumentation on system functionality and limitations could also help vali- date claim e2. The documentation shows that teachers can decide how to use the predictions, and administrators can turn the dropout prediction system on or off [54]. Students do not appear able to opt in or out of being classified by the dropout prediction model. In conclusion, the available evidence indicates that claim e2-stakeholders can decide whether to use the dropout prediction system can only partly be confirmed.\nConcerning e3, the documentation on system functionality and limitations, design and implementation choices, and organizational processes does not ex- plicitly explain what has been done to comply with the GDPR, except that exportable data is anonymous and access to insights can be managed [54]. We conclude that only a source code analysis (algorithms, system functionality, and limitations) can assess the claim that the dropout prediction system is GDPR compliant.\nTo validate e4, documentation on the choice of model features and their underlying principles (design and implementation choices), as well as limiting technical factors (system functionality and limitations) hint at existing or absent bias in the dropout prediction system [52, 53, 54]. However, they do not mention any known issues. Source code analysis is required to complement the documentation. However, it cannot rule out any bias [73]. Evaluating the model performance per group could provide additional evidence on model fairness. Such a quality assessment is not documented and thus needs to be conducted by the auditor. Access to a production system (including the database) and data sources, including demographic data, is necessary. The \"evaluation mode\" cannot conduct such a performance evaluation because it does not return ethics- related metrics (e.g., group-based performance metrics). However, the auditor could calculate these metrics from predictions, truth values, and user data in a production system. This data can be collected from a log on what was predicted, when, and for which user [32]. While users can disclose information on their geographical location, Moodle does not collect information on their financial status [32]. The documentation on system functionality and limitations and design and implementation choices [52, 53, 54] does not mention model fairness. No evidence could be found that the risk of model bias was considered in the design and development (organizational processes). No information on the data provenance (e.g., information on data acquisition and pre-processing) or relevant data quality (e.g., information on representativeness) for MoodleHQ's quality assessment is available [52]. We conclude that insufficient evidence is available and accessible for an efficient audit of claim e4 that model performance is equally high across groups."}, {"title": "Case study II: Auditability of a research dropout-prediction system", "content": "In this second, shorter case study, we apply the auditability assessment framework to the extended approach published in [83] based on [76]. This research prototype represents an early stage of an AI-based LA system. The study implements a dropout prediction in an introductory computer science course using log data from the course and student's self-reported data. \nClaims\nWith regards to the validity of the system, the authors of the system claim v1 that their models \u201cprovide strong dropout predictions\" [83]. They also claim v2 that \"combining student behaviour and self-reported data early in a course enhances dropout prediction accuracy\u201d. Thus, they claim that self-reported student motivation and aptitude data are valid indicators.\nThe authors claim the utility (u1) of their approach is that \"dropout pre- diction in computer science education aids in detecting struggling students for early support\" [83].\nFor ethics, they state the \"framework and the models are transparent\" (e1); the system and its results are interpretable and accessible to human compre- hension. Further, since [76] is considered a privacy-first approach to student data in AI-based LA, their extended implementation shares the claim e2 of a privacy-friendly drop-out prediction system.\nIn contrast to our Moodle case study, [83] make all claims about a specific course at a particular university in a specific period. While they are reproducing prior work by [76] and are thus extending the approach's generalizability, they do not claim that their model works in all courses at all points in time, as is the case with Moodle.\nGiven the research nature of the project, the research article is an integral part of the system's documentation and explains the system's functionality and limitations. Discussing the limitations of their approach, the authors argue that the results in the form of feature importance are not \"explainable to educators\" (el) and not yet comparable to a \"simple-to-use dashboard\" [83]. With regards"}, {"title": "Means of validation", "content": "The system presented in [83] does not provide APIs or monitoring tools. Expert users can, however, consult feature weights of the logistic regression model, which is inherently explainable. The research prototype system uses the command line interface and Jupyter notebooks. Both user interfaces can be considered to address expert users and not educators in general. It is questionable if auditors are in the scope to assess such tools. However, given the prototype state, auditability can serve the developers to test and refine their tool to validate their claims about the system's functionality."}, {"title": "Auditability", "content": "The system documentation, as provided in the research article, and the open- source code are sufficient to ensure the auditability of the system concerning the claims made by authors. This underlines the importance of making reproducible code and data accessible in scientific papers. The documentation lacks a descrip- tion of the data structure outputted by the learning environment. This limits the auditability of the claims to the exact scenario described in the paper. To ensure auditability for broader, more generalized claims, more detailed documentation of implementation and data would be necessary."}, {"title": "Discussion", "content": "We have demonstrated that our auditability assessment framework is helpful for AI-based LA systems by successfully applying it to Moodle's dropout prediction feature and a prototype dropout prediction system. Through the structured approach of analysing claims, evidence, and means of validation, we support auditors to systematically approach complex AI-based systems (RQ1).\nAlthough Moodle is open source, well-documented, and includes a comprehen- sive logging system with explanations, only three of seven identified claims are effectively auditable. The assessment of Moodle shows that future LA systems need to provide system access to third-party auditors, e.g., by creating \"auditor\" roles, recording data (anonymized training data, predictions), and enabling auditors to control evaluation parameters. More documentation is needed on system design and implementation choices to justify the validity and ethical design of the system. When documentation is incomplete or not trustworthy enough, additional evidence for an audit of the dropout prediction system must be collected from the system, e.g., by monitoring. Two significant challenges hinder this approach. First, Moodle's model monitoring capabilities are inad- equate. Predictions are not preserved when evaluating a model configuration and are inaccessible to the auditor. Thus, the auditor cannot verify the model's performance and has to rely on minimal metrics returned by Moodle. Second, publicly available test data is lacking, which impedes data-based audits of Moo- dle's dropout prediction model. However, complex systems like Moodle can benefit from the supporting feature of the framework for auditability and inform the development of suitable fixes and features that retrofit auditability [27].\nThe assessment of the prototype LA shows that small-scale prototypes are less complex to audit, especially when their development and application are scientifically documented. This mainly boosts claims that are founded in scientific evidence. Potentially, their smaller scale makes claims and suitable evidence more manageable and may guide their development toward reflecting auditability early on (RQ2). Nonetheless, developing with auditability in mind increases complexity. A minimum set of claims could concern validity (accuracy, or performance) and ethical standards (privacy, GDPR-compliance, sensitivity for biased data) of an AI-based LA, while utility claims usually are tested in extensive piloting phases.\nHowever, limitations exist, some of which merit further research in the future to extend the auditability framework and add to its generalizability. The framework is only partly applicable to LA systems that don't utilize AI. In our discussion of Moodle and a prototype AI-based LA, we could not account for the organizational setting, i.e., the institution where a running instance would be embedded. These institutions can provide additional claims, derived, for example, from organizational culture or corporate social responsibility statements. Further, organizations and institutions offer additional evidence to validate claims that may emerge in documented decision processes, legal statements, or technical scopes. These additional spheres must be included in any assessment of the auditability of AI-based LA systems and highlight that AI audits are complex and multifaceted practices that scrutinize technical and organizational aspects of AI implementation [45]. Further, types of AI-based LA are not distinguished: analytical, predictive, or predictive LA systems place different demands on their AI components. In particular, systems that are specialized in individual prediction or that recommend concrete action for individuals must be audited particularly rigorously. The more aggregated data is, the more lax audits and therefore auditability could be.\nThe auditability assessment framework requires time and effort deriving claims and evidence, while technical means for validation might not be readily accessible. We are confident that development benefits from our auditability framework as will audits improve in quality. This justifies additional time and effort to procure more robust and ethically fair software for \"high-risk\" application fields."}, {"title": ""}]}