{"title": "Interpretable Triplet Importance for Personalized Ranking", "authors": ["Bowei He", "Chen Ma"], "abstract": "Personalized item ranking has been a crucial component contributing to the performance of recommender systems. As a representative approach, pairwise ranking directly optimizes the ranking with user implicit feedback by constructing (user, positive item, negative item) triplets. Several recent works have noticed that treating all triplets equally may hardly achieve the best effects. They assign different importance scores to negative items, user-item pairs, or triplets, respectively. However, almost all the generated importance scores are groundless and hard to interpret, thus far from trustworthy and transparent. To tackle these, we propose the Triplet Shapley-a Shapely value-based method to measure the triplet importance in an interpretable manner. Due to the huge number of triplets, we transform the original Shapley value calculation to the Monte Carlo (MC) approximation, where the guarantee for the approximation unbiasedness is also provided. To stabilize the MC approximation, we adopt a control covariates-based method. Finally, we utilize the triplet Shapley value to guide the resampling of important triplets for benefiting the model learning. Extensive experiments are conducted on six public datasets involving classical matrix factorization- and graph neural network-based recommendation models. Empirical results and subsequent analysis show that our model consistently outperforms the state-of-the-art methods.", "sections": [{"title": "1 INTRODUCTION", "content": "Personalized ranking is widely regarded as the essence of recommender systems (RS) [33]. It ranks the recommended items according to user preferences to make the most relevant items appear at the top of the recommendation list, thereby improving user satisfaction and the benefits of service providers. To enable personalized ranking, implicit feedback (e.g., check-ins and clicks) is more broadly utilized than explicit feedback (e.g., ratings), due to its accessibility. Hereby, many approaches have been proposed using implicit feedback. Among them, Bayesian Personalized Ranking (BPR) [33] is one of the most representative paradigms that can be incorporated in many different types of methods like matrix factorization (MF)-based models [1, 17, 26], and graph neural network (GNN)-based models [14, 16, 25, 38, 40, 42]. In detail, BPR constructs (user, positive item, negative item) triplets and optimizes the pairwise ranking score between positive and negative (randomly sampled) items for each triplet. Indeed, the quality of constructed triplets largely affects the ranking performance, especially when negative items are randomly sampled from non-interacted items. Because a user may have different preference levels for positive items, and it is hard to tell whether the randomly sampled items are truly negative and how negative they are. Therefore, it is highly necessary to further distinguish the importance of such triplets.\nTo discern the triplet importance, four main categories of methods have been proposed: negative item sampling, positive pair re-weighting, using auxiliary information, and triplet importance modeling. Among them, negative item sampling techniques [8, 29, 32] prioritize items differently during the training process, based on criteria such as item popularity or user engagement. While this approach can enhance the model's focus on more influential negative items, it can induce biases by over-representing popular items and under-representing less common but potentially significant items. Positive pair re-weighting methods [41] adjust the influence of user-item interactions within the model by assigning different weights to them. The approaches in such two categories have recognized the necessity of applying different weights to positive or negative items. However, they only focus on the part of the triplet and neglect the whole, as illustrated in Figure 1. For the methods of using auxiliary information [24, 30, 45, 47], they rely on the side information like dwell time that is often unavailable in practical settings. In addition to these three types of methods, triplet importance modeling [44] advances one step further and takes the whole triplet into consideration. By dispatching triplet-level importance, this method can distinguish the contribution of each triplet in the training. However, these works either use a heuristic approach or directly utilize a black-box neural network module [44] to generate the triplet importance, where such importance scores are groundless and hard to interpret. The lack of interpretability will make both the importance score and the recommendation model less trustworthy, and even less fair due to the potential unknown bias introduced in importance generation. Besides, they fail to bring insights on how to adjust the sampling strategy, which wastes massive high-importance triplets that can be easily accessed with resampling, thus restricting the further improvement of model performance.\nTo overcome aforementioned drawbacks, we propose a more interpretable approach to generate the importance of each triplet and"}, {"title": "2 RELATED WORK", "content": "Implicit Feedback in Recommendation. Due to the easier accessibility than explicit feedback like rating scores [35, 36] in most recommendation scenarios, the implicit feedback like historical clicking or browsing behaviors has become the main data source in personalized recommender system. Actually, there have been many recommendation models developed to utilize the implicit feedback for conducting personalized ranking, from the conventional MF [26] and KNN [6] to the recent deep neural network-based NeuMF [17], NGCF [42], and LightGCN [16]. BPR [33] first notices that few of previous methods are directly optimized for the personalized ranking and proposes to address this challenge by constructing (user, positive item, negative item) triplets and pairwise training. In detail, BPR proposes the maximum posterior estimator derived from the Bayesian analysis to the problem. Based on this, some improvement schemes have also been proposed, like negative item sampling [8, 29, 32], positive pair re-weighting [41], using auxiliary information [24, 30, 43, 45, 47]. Futhermore, many real-world observations and empirical results have demonstrated that different triplets contribute differently to the model training performance. To prevent the performance loss due to treating all triplets equally, TIL [44] leverages a neural network to learn the triplet importance end-to-end. However, previous methods either use a heuristic approach or an unexplainable blackbox model to generate the triplet importance. It is hard to understand why assigning such triplet"}, {"title": "3 PRELIMINARIES", "content": "In this section, we first formulate the top-K recommendation task. Then, we review the Bayesian Personalized Ranking paradigm which is the foundation of our method.\n3.1 Problem Formulation\nThe recommendation task in this paper leverages the user implicit feedback as input. Let U and V denote the set of all users and items in the system, respectively.  $\\mathbf{Y} \\in \\mathbb{R}^{|U|\\times|V|}$ is the binary implicit feedback rating matrix. For each user $u \\in U$, the user preference data is represented by the set of items in his/her interaction history as $I_u = \\{i \\in V|Y_{u,i} = 1\\}$. The top-K recommendation task is then formulated: for each user $u \\in U$, given the training item set $S_u$ and the non-empty test item set $T_u$ (requiring that $S_u \\cap T_u = I_u$ and $S_u \\cap T_u = \\emptyset$), the model needs to recommend an ordered item set $X_u$ such that $|X_u| = K$ and $X_u \\cap S_u = \\emptyset$. Then, the recommendation quality is evaluated with some matching scores between $X_u$ and $T_u$, like Recall@K and NDCG@K.\n3.2 Bayesian Personalized Ranking\nBayesian Personalized Ranking (BPR) has been widely integrated with many recommendation models and achieved great success in many real-world production scenarios [13, 16, 17, 26, 42]. The BPR method mainly consists of two parts: triplet construction and pairwise training. The details of them are as follows.\nTriplet Construction. Based on the assumption that the users prefer the interacted (positive) items over the uninteracted (negative) ones, the triplet construction space $D_s = U \\times V \\times V$ is first formalized by:\n$D_s := \\{(u, i, j)|i \\in I_u, j\\in V\\backslash I_u\\}, \\quad\\quad\\quad\\quad$(1)\nwhere for triplet $(u, i, j) \\in D_s$, i is the positive item, and j is the negative item. However, directly traversing all the triplets in $D_s$"}, {"title": "4 METHODOLOGY", "content": "In this section, we first follow the triplet importance model approach and introduce the triplet importance-based weighted pairwise learning. Then, we propose the triplet Shapley as the interpretable and equitable importance to tackle the transparency and fairness issues of the existing methods. It also serves as the more discriminative triplet weight to facilitate pairwise learning. Next, we re-examine the triplet Shapley formulation from the joining process perspective and utilize the descent-based Monte Carlo method to approximate the triplet Shapley. Besides, we provide a control covariates scheme to stabilize the triplet Shapley estimation. Finally, we build a triplet importance prediction model to guide the triplet resampling, so as to adaptively augment the original constructed triplets.\n4.1 Triplet Importance for Pairwise Learning\nAlthough Bayesian personalized ranking has been successfully integrated into many recommendation models, it equally treats all triplets which may not capture the fine-grained user preference, since users have different preference levels on different items. Moreover, a randomly sampled item does not necessarily mean that the user dislikes it, especially for those similar to the positive items. These motivate us to assign importance scores to different triplets to facilitate the personalized ranking. To incur the triplet importance into the original BPR loss, we adopt the formulation from the previous work [44] as follows,\n$L_{TI-BPR}(u, i, j; \\theta) = -W_{TI}^{(u,i,j)} ln \\sigma(\\hat{y}_{ui}(\\theta) - \\hat{y}_{uj}(\\theta)), \\quad\\quad\\quad$(3)\nwhere $W_{TI}^{(u,i,j)}$ is the triplet importance of the triplet (u, i, j) for the BPR loss.\n4.2 Triplet Shapley as Interpretable Importance\nAlthough previous approaches [8, 29, 32, 41, 44] have been proven to enhance Bayesian pairwise user preference learning, few of them pay attention to the interpretability and equitability of the modeled triplet importance. Moreover, due to the black-box property"}, {"title": "4.3 Approximating Triplet Shapley", "content": "4.3.1 Identical Transformation to Triplet Shapley. For more effectively computing the triplet Shapley defined in Eq. 4, we need to first conduct an identical transformation to it.\nTHEOREM 1. Triplet Shapley from Joining Process Perspective. Denote by $ \\Pi^T $ the set of all possible permutations of the triplets in DT, each of which representing a distinct joining order. Moreover, let $P_{\\pi}^{(u,i,j)}$ denote the set of triplets that precede (u, i, j) in the permutation $ \\pi \\in \\Pi^T$. Then, the triplet Shapley of (u,i,j) defined in Eq. 4 can be calculated as:\n$V_{TS}^{(u,i,j)} = \\frac{1}{|DT|!}\\sum_{\\pi \\in \\Pi^T} \\big[A(P_{\\pi}^{(u,i,j)} \\cup \\{(u, i, j)\\}) - A(P_{\\pi}^{(u,i,j)})\\big].\\quad\\quad$(5)\nThis theorem can be proved by setting $C = \\frac{1}{|DT|}$ in Eq. 4.\n4.3.2 Monte Carlo Approximation. Monte Carlo approximation has been widely used in mathematics and physics [23]. This kind of approach relies on the repeated random sampling to obtain the numerical results for some super complex or difficult problems. In this work, we also utilize the Monte Carlo approximation technique to estimate the triplet Shapley via randomly sampling possible permutations based on the above transformation in Sec 4.3.1. To elaborate on the theoretical rationality of Monte Carlo approximation, we provide the following theorem on the unbiasedness of Monte Carlo triplet Shapley approximation:\nTHEOREM 2. Unbiasedness of Monte Carlo Approximation. For $V_{TS}^{(u,i,j)}$, its average value $\\bar{V_{TS}^{(u,i,j)}}$ over the different permutations $\\pi_k$ randomly sampled from the uniform distribution $U(\\pi)$ is the unbiased estimator of the corresponding triplet Shapley $V_{TS}^{(u,i,j)}$.\n4.3.3 Efficient Triplet Shapley Approximation. However, directly applying Monte Carlo approximation will still face the challenges brought by the slow convergence of recommendation model training and the intrinsic noise as the permutation stretches. To address such challenges, we specially design the gradient descent-based truncated Monte Carlo approximation for triplet Shapley, illustrated in the Algorithm 1. The outer loop still follows the standard Monte Carlo approximation procedure. However, in the inner loop, we first use the one-epoch training with the studied triplet $\\pi_k[s]$ (Line 11) instead of the conventional recommendation model training, which conducts the multi-epoch gradient descent with the same data and leads to the slow convergence. To achieve this goal and guarantee that the model has approximately converged, we need to adopt a relatively bigger learning rate $\\alpha$ than that in the traditional multi-epoch training. In fact, this point has been proved to be feasible in [48]. Meanwhile, considering that the model performance change as $\\alpha_{s-1} - \\alpha_s$ becomes less and less obvious which can be easily drowned out by the noise as the the permutation stretches from 1 to $|DT |$ ($s \\to |DT|$), we design a truncation operation to the permutation scanning (Line 8, 9). In detail, we set a tolerance threshold and truncate the permutation scanning when the $\\alpha_s$ has been closed enough to the model performance with the whole triplet set $A(DT)$. Then, the marginal contribution $\\alpha_{s-1} - \\alpha_s$ of all the rest triplets that follow the s-th position in this permutation will be assigned to zero, in such a way to effectively avoid the huge amounts of the uninformative calculations.\n4.3.4 Stabilizing Triplet Shapley Approximation. Though the use of Monte Carlo Sampling Approximation brings significant computational cost reduction, an accompanying issue of value uncertainty has also been raised. The direct consequence is that the variance of each approximated triplet Shapley value may obscure its corresponding true value. Thus, it becomes infeasible to utilize these estimates to effectively differentiate the importance of different triplets. To stabilize the triplet Shapley approximation and provide reliable triplet importance scores, we adopt the control variate method, which has been explored in machine learning and statistics [18, 22, 31, 39] to reduce the variance of Monte Carlo estimators. In specific, it exploits information about the errors in known estimators to reduce the error of an unknown estimator."}, {"title": "4.4 Importance-aware Triplet Resampling", "content": "Previous works on distinguishing triplet-level importance [44] just focus on how to make use of the triplet set $D^T$ constructed at the beginning well, while ignoring exploiting the data potential in the whole triplet construction space ($|U|\\times |V|^2$). Actually, other potential triplets not sampled into the $D^T$ at first can also bring informative contributions to our recommendation model training. Besides, the triplet Shapley values for elements in $D^T$ approximated in Sec. 4.3 can exactly inform us which kind of triplets are high-importance and which are low-importance. To utilize this information well and effectively estimate the triplet Shapley for other potential triplets not in $D^T$, we first fit a neural network model to learn predicting their triplet Shapley values, which is named as Triplet Importance Prediction (TIP) model. Without sophisticated design, we just use a simple two-layer MLP structure here:\n$h(u,i,j) = ReLU(W_1 \\cdot [p_u; q_i; q_j] + b_1),$\\newline\n$\\hat{V_{TS}^{(u,i,j)}} = Tanh(W_2\\cdot h(u,i,j) + b_2).\\quad\\quad$(12)\nNext, to fully unleash the potential of the whole triplet construction space $D_s$, we rely on such kind of triplet importance information to guide the resampling process. In detail, when resampling triplets, given user u and positive item i, we model the sampling probability of the unsampled negative item $j((u, i, j) \\notin D^T)$ as follows:\n$p(j|u, i) = \\frac{e^{TIP(u,i,j)}}{\\sum_{j' \\in V \\backslash I_u,\\quad (u,i,j') \\notin D^T}e^{TIP(u,i,j')}}.\\quad\\quad\\quad\\quad\\quad$(13)\n4.5 Overall Learning Procedure\nTo better illustrate the workflow and facilitate the reproducibility, we summarize the overall learning procedure of our proposed method in Algorithm 2. It can be generally divided into three parts: triplet importance approximation, importance-aware triplet resampling, and triplet importance-weighted pairwise learning. Note that before training recommendation model $M(\\theta)$ with the augmented triplet set $D^{aug}$ we need to reestimate the triplet Shapley $V_{TS}^{(u,i,j)}$ for each triplet in $D^{aug}$, in this way to take the influence brought by the newly resampled triplets into consideration. In addition, we perform a normalization operation on the reestimated $V_{TS}^{(u,i,j)}$ to rescale them into a reasonable value range, which are then taken as weights for corresponding BPR losses (Eq. 3) during training."}, {"title": "5 EXPERIMENTS", "content": "In this section, we conduct experiments on several public datasets to demonstrate the effectiveness, applicability, interpretability, and time efficiency of our method. In the experiments, we mainly focus on answering the following questions:\n*   RQ1: Whether our method can achieve better recommendation performance than other baselines utilizing implicit feedback?\n*   RQ2: Whether the superiority of our method can be consistently performed when selecting different MF-based and GNN-based recommendation models as the backbone?\n*   RQ3: Does each component of our method contribute to the performance improvement (Ablation study)?\n*   RQ4: Can our method provide a reasonable and intuitive explanation to the importance score assigned to each triplet?\n*   RQ5: Whether our control covariate-based method can effectively stabilize the triplet Shapley approximation, thus leading to more reliable triplet Shapley values?\n*   RQ6: Whether the time complexity of our method is still acceptable considering the substantial computation cost brought by the triplet Shapley? In other words, Can our approximation approach effectively reduce the training time cost?\n5.1 Experiment Setup\n5.1.1 Datasets. We evaluate our method on six public datasets: Amazon-Books [15], Amazon-CDs [15], Yelp [2], Gowalla [3], ML-20M [12], and ML-Latest [12]. To accommodate the implicit feedback setting, for the datasets with explicit ratings, we preserve the ratings more than three (out of five) as positive feedback and treat all other ratings as missing entries.\n5.1.2 Baselines. Generally, previous works on utilizing implicit feedback for personalized ranking can be divided into four categories. Note that we do not compare with works on using auxiliary information considering this kind of information is often inaccessible. In experiments, we use representative works in left three categories as our baselines:\nNegative item sampling:\n*   WBPR [8]: This method assigns more sampling probabilities to the negative items with higher global popularity.\n*   AOBPR [32]: It adopts a context-specific adaptive sampler to oversample popular negative items.\n*   PRIS [29]: It utilizes the importance sampling to obtain negative samples according to their informativeness.\nPositive pair re-weighting:\n*   TCE and RCE [41]: They adaptively prune noisy positive interactions with large loss values in the training.\n*   TCE-BPR and RCE-BPR: The modified versions of the above two methods in which the point-wise loss is replaced with a pair-wise ranking loss following BPR's scheme.\nTriplet importance modeling:\n*   TIL-UI and TIL-MI [44]: They design a neural network-based weight generation module to learn triplet importance."}, {"title": "6 CONCLUSION AND FUTURE WORKS", "content": "In this paper, we introduce the triplet Shapley for interpretable triplet importance measurement in Bayesian personalized ranking and improve assessment through a Monte Carlo method with control covariates to reduce variance. Our method, combined with importance-aware resampling, enhances recommendation model training. Experiments on six datasets confirm our approach's effectiveness and efficiency. Future work will focus on efficient computation for large-scale data in practical applications."}, {"title": "A SUPPLEMENTARY PRELIMINARIES", "content": "A.1 Main Notations\nThe main notations in this paper are summarized in Table 4.\nA.2 Base Recommendation Models\nMatrix factorization (MF)-based models [1, 17, 26] and Graph Neural Network (GNN)-based models [16, 25, 38, 40, 42] are two categories of most representative and commonly used recommendation models. In this paper, we adopt some popular and effective instances of them to demonstrate the applicability and robustness of our proposed framework. We provide the brief introduction as follows.\nMF-based Models: MF [26] is a pioneer and fundamental method in this line of research. In MF, the user-item interaction matrix is factorized into two matrices that represent the embeddings of users and items. The inner product of user u's embedding $p_u$ and item i's embedding $q_i$ is used to predict the user-item interaction between them:\n$\\hat{Y}_{u,i} = p_u^T q_i.\\quad\\quad\\quad\\quad\\quad\\quad\\quad$(14)\nThe goal of MF is to minimize the difference between the predicted and actual ratings. Neural Matrix Factorization (NeuMF) [17] is an extension of MF that uses neural networks to learn embeddings of users and items, which allows more complex interactions between them. The output of the neural network which utilizes the u's embedding $p_u$ and item i's embedding $q_i$ as the input is used to predict the user-item interaction between them:\n$\\hat{Y}_{u,i} = MLP(p_u q_i, MLP(p_u, q_i)),\\quad\\quad$(15)\nwhere $MLP$ represents the multi-layer perceptron. NMF has been shown to outperform traditional MF in terms of prediction accuracy and scalability.\nGNN-based Models: Neural Graph Collaborative Filtering (NGCF) [42] is a graph-based recommender system model that leverages the power of GNN to learn user and item embeddings. NGCF considers the user-item interactions as a bipartite graph and uses GNN to propagate information between connected nodes in the graph. The"}, {"title": "B SUPPLEMENTARY METHODOLOGY", "content": "B.1 Proof for Theorem 1\nWe provide the following proof for the Theorem 1.\nPROOF. First, recall Eq. 4 and set $C = \\frac{1}{|DT|}$. Then, the following transformation holds:"}, {"title": "B.2 Proof for Theorem 2", "content": "We provide the proof for Theorem 2 as follows:\nPROOF. Let $U (\\pi)$ be the uniform distribution of $\\pi$ over all $|DT|!$ permutations of $I(D^T)$. Each permutation has the probability of $\\frac{1}{|DT|!}$ to be sampled. The permutation samples $\\pi_1, \\pi_2, ..., \\pi_n$ for conducting the Monte Carlo approximation are sampled from this"}]}