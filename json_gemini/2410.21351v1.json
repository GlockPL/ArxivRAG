{"title": "LinFormer: A Linear-based Lightweight Transformer Architecture For Time-Aware MIMO Channel Prediction", "authors": ["Yanliang Jin", "Yifan Wu", "Yuan Gao", "Shunqing Zhang", "Shugong Xu", "Cheng-Xiang Wang"], "abstract": "The emergence of 6th generation (6G) mobile networks brings new challenges in supporting high-mobility communications, particularly in addressing the issue of channel aging. While existing channel prediction methods offer improved accuracy at the expense of increased computational complexity, limiting their practical application in mobile networks. To address these challenges, we present LinFormer, an innovative channel prediction framework based on a scalable, all-linear, encoder-only Transformer model. Our approach, inspired by natural language processing (NLP) models such as BERT, adapts an encoder-only architecture specifically for channel prediction tasks. We propose replacing the computationally intensive attention mechanism commonly used in Transformers with a time-aware multi-layer perceptron (TMLP), significantly reducing computational demands. The inherent time awareness of TMLP module makes it particularly suitable for channel prediction tasks. We enhance LinFormer's training process by employing a weighted mean squared error loss (WMSELoss) function and data augmentation techniques, leveraging larger, readily available communication datasets. Our approach achieves a substantial reduction in computational complexity while maintaining high prediction accuracy, making it more suitable for deployment in cost-effective base stations (BS). Comprehensive experiments using both simulated and measured data demonstrate that LinFormer outperforms existing methods across various mobility scenarios, offering a promising solution for future wireless communication systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Multiple-input multiple-output (MIMO) technology plays a pivotal role in enhancing spectrum efficiency for fifth-generation (5G) and beyond 5G (B5G) mobile networks [1]\u2013[3]. Accurate channel state information (CSI) is crucial for BS to optimize beamforming and precoding, thereby maximizing data transmission rates in MIMO systems.\nConventional CSI acquisition methods, which rely on transmitting known pilot signals, face significant challenges in 5G and B5G networks. The increasing number of antennas in modern systems necessitates more pilot signals, leading to substantial overhead. Moreover, the demand for higher mobility causes rapid CSI variations, exacerbating the channel aging problem [4]. The use of outdated CSI in precoding can severely degrade system performance [5], [6].\nChannel prediction has emerged as a promising solution to these challenges, allowing for the estimation of future CSI based on historical data without additional overhead. While traditional mathematical approaches such as linear extrapolation [7], sum-of-sinusoids models [8], Kalman filtering [9], and autoregressive (AR) models [10] have been employed, they often struggle with the complexities of real-world scenarios, such as multi-path propagation and Doppler effects.\nRecent advancements in artificial intelligence have led to the application of machine learning techniques in channel prediction [11]. Deep learning approaches, including physics-inspired neural networks [12], have shown promise in predicting static channel impulse responses. However, their performance in dynamic scenarios remains limited. To address this, some researchers have proposed hybrid approaches that combine model-driven and data-driven techniques in the angular delay domain, aiming to improve prediction robustness in high-mobility massive MIMO environments [13].\nIn time-domain channel prediction, sequential frameworks such as Recurrent Neural Networks (RNNs) [14], [15], Gated Recurrent Units (GRUs) [16], [17], and Long Short-Term Memory (LSTM) networks [18]\u2013[20] have demonstrated superior performance, even with limited data. These models benefit from their Markov inductive bias, which allows them to capture temporal dependencies effectively. However, they are susceptible to cumulative errors, particularly in long-term predictions. While RNNs inherently incorporate positional information, they struggle with long-term dependencies due to the vanishing gradient problem [16], [18]. This limitation can lead to errors in multi-step predictions. GRUs have shown promise in tapped delay line (TDL) channel prediction, but their performance is often constrained by the size of available training datasets [17].\nTransformer models [21], which have revolutionized NLP, have recently been applied to temporal channel prediction with promising results [22]. The attention mechanism employed by Transformers enables effective modeling of long-range dependencies and parallel sequence processing, potentially mitigating the cumulative error issues observed in RNN-based approaches.\nDespite the advantages of the Transformer model, its complexity can lead to longer inference times for channel prediction, particularly when dealing with long input sequences and when base station devices have limited computing resources. Insufficient inference speed may render predicted channels applicable only for precoding the final few time intervals\u2014or possibly none at all\u2014as earlier intervals might elapse before the inference process concludes.\nMoreover, the extensive parameter volume in Transformer models elevates the risk of overfitting, potentially compromising their generalization capabilities. The attention mechanism, a key component of Transformers, is also vulnerable to the inherent noise and redundancy typically found in time series data [23].\nThe inherent permutation-invariance of the attention mechanism in Transformers poses an additional challenge. Even with position encoding, there remains a risk of losing vital temporal information. Recent research [24], [25] employing straightforward linear models has cast doubt on the efficacy of the original Transformer architecture for time series prediction. These studies highlight how the permutation-invariance of self-attention in Transformers may compromise the preservation of temporal information, which is crucial for tasks such as channel prediction.\nIntriguingly, these studies demonstrated that a single linear layer can occasionally surpass sophisticated Transformer architectures in time series forecasting tasks. However, it\u2019s important to note that while these linear models may perform well with small, clean datasets, they often encounter difficulties when dealing with more complex and noisy temporal channel data.\nFurthermore, the majority of existing models have been predominantly trained using simulated CSI data. This reliance on simulated data raises significant concerns regarding the practical applicability of these models in real-world mobile network environments."}, {"title": "A. Our Contributions", "content": "This paper addresses the aforementioned challenges in channel prediction by proposing a novel deep learning-based framework that achieves low prediction error, high inference speed, and improved generality in practical mobile networks. Our approach, named LinFormer, combines the strengths of linear layers and the Transformer encoder-only architecture. By replacing the attention layer with a linear-based layer while retaining the Transformer encoder\u2019s multi-block design, LinFormer offers a scalable solution for handling complex temporal channel series with enhanced inference speed and prediction accuracy. The key contributions of this work are as follows:\n\u2022 We propose LinFormer, a scalable model structured as an encoder with trainable parameters exclusively derived from linear layers. This innovative design preserves the Transformer\u2019s architectural benefits while significantly reducing parameter count and overall complexity. We develop the TMLP module that directly models long-range dependencies within channel sequences by learning time-step-dependent weights. This approach is particularly effective for time-varying channels affected by Doppler frequency shifts and multipath propagation.\n\u2022 We explore the potential of the proposed LinFormer comprehensively. Specifically, we investigate the impact of model parameter quantity and training data size on channel prediction error. Our analysis includes scaling the model by increasing the number of layers to expand parameter capacity, as well as examining the effects of varying training sample sizes and channel sequence lengths. Additionally, we enhance prediction accuracy through the implementation of WMSELoss and novel data augmentation techniques.\n\u2022 We perform extensive simulations to demonstrate Lin-Former\u2019s superiority in channel prediction accuracy and inference speed across various scenarios, including different speeds and Signal-to-Noise Ratios (SNRs), using simulated CSI aligned with 3GPP standards. Our results show that LinFormer reduces channel prediction error by over 60% compared to GRU at comparable inference speeds, while achieving marginally better prediction performance at six times the speed. Furthermore, we validate the generalization ability of the LinFormer in practical mobile networks using measured CSI data."}, {"title": "B. Organizations and Notations", "content": "The structure of this paper is as follows. Section II presents the system model for massive MIMO. Section III defines the channel prediction problem. Section IV introduces our proposed LinFormer model, including the rationale for using WMSELoss over traditional MSE, and a comprehensive description of the TMLP. Section V details our experimental results using simulated and measurement CSI data with in-depth discussion. Section VI offers concluding remarks and future directions.\nThroughout this paper, we employ the following notation. Boldface lower-case letters represent column vectors, while boldface upper-case letters denote matrices. $(\\cdot)^T$, $(\\cdot)^H$, and $(\\cdot)^{-1}$ signify the transpose, conjugate transpose, and inverse of a matrix, respectively. $\\mathbb{C}$ and $\\mathbb{R}$ represent the sets of complex and real numbers, respectively. $\\mathbb{E}\\{\\cdot\\}$ indicates the expectation operator. $||\\cdot||_2$ denotes the Frobenius norm of a vector or matrix."}, {"title": "II. SYSTEM MODEL", "content": "This section presents the system model for our wireless communication study.\nWithout loss of generality, we consider a multi-antenna wireless system with T transmit and R receive antennas operating in time division duplex (TDD) mode. In this setup, time is segmented into frames, which are further divided into slots. The BS acquires CSI via uplink pilot transmissions from the UE. The channel matrix at the BS, denoted as $H \\in \\mathbb{C}^{R \\times T}$, is a complex matrix with statistically independent coefficients:\n$H = \\begin{bmatrix}\nh_{11} & \\cdots & h_{1T} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nh_{R1} & \\cdots & h_{RT}\n\\end{bmatrix}$\nwhere $h_{ij}$ represents the channel coefficient between the i-th receive antenna and the j-th transmit antenna.\nThe BS employs beamforming to transmit signals to the UE. A symbol c is weighted with a transmit vector v to form the transmitted signal vector s. The received signal vector y is expressed as:\n$y = Hs + n$,\nwhere the transmitted signals $s = cv = c \\begin{bmatrix} v_1 \\\\ \\vdots \\\\ v_T \\end{bmatrix}$. The additive white Gaussian noise (AWGN) vector $n = \\begin{bmatrix} n_1 \\\\ \\vdots \\\\ n_R \\end{bmatrix}$. The received signal y is combined by the receiving weighting vector $w \\in \\mathbb{C}^{1 \\times R}$ to obtain the estimated symbol $\\hat{c}$:\n$\\hat{c} = w y = cwHv + wn$.\nWe primarily targets communication scenarios in 5G and beyond, where rapid user mobility leads to fast time-varying channels. Modern statistical wireless channel modeling stems from the Saleh-Valenzuela clustered channel model [26], is which is particularly valuable for understanding how different factors, such as multipath propagation and Doppler shifts, can influence signal strength and transmission performance over time. It effectively simulates the random and unpredictable variations in channel quality that occur in real-world wireless communication environments. The channel matrix for the n-th frame in TDD mode is calculated as\n$H(n) = \\sum_{l=1}^{L} \\alpha_l(n)e^{-j2\\pi f_l n T_s} A(\\theta_l) A^H(\\phi_l)$,\nwhere $\\alpha_l(n)$ denotes the complex gain of the l-th propagation path, $T_s$ denotes the period of the frame, $f_l$ denotes Doppler shift, $A(\\theta_l)$ denotes the receive steering vector, $A(\\phi_l)$ is the transmit steering vector. $\\theta_l$ and $\\phi_l$ are the angles of departure (AoA) and angles of arrival (AoD), respectively. It is worth noting that the AoA and AoD are time-invariant, because the time scale of the frames in the 3GPP standard is in the order of tens of milliseconds [27]\u2013[29]. Consequently, the time-varying nature of the channel is primarily attributed to the Doppler effect [30], [31].\nTime-domain channel prediction exploits the temporal correlation of time-varying channels. This correlation is quantified by the autocorrelation [32] of $H(t)$, expressed as\n$R_H(\\tau) = \\mathbb{E}\\{H(n) (H(n+\\tau))^H\\} = \\sigma^2 J_0(2\\pi f_d \\tau T_s)A$,\nwhere\n$\\sigma^2 = \\sum_{l} \\mathbb{E}\\{\\alpha_l^2\\}$,\n$A = \\sum_{l} A(\\theta_l) A^H(\\phi_l) A(\\phi_l)A^H(\\theta_l)$,\n$f_d$ is the maximum Doppler frequency, and $J_0(\\cdot)$ denotes the zero-order Bessel function of the first kind. Conventionally, it is assumed that $\\sum_{l} \\mathbb{E}\\{\\alpha_l^2\\} = 1$ to ensure that the received signal power is equal to the transmitted signal power which results in $R_H(\\tau) = J_0(2\\pi f_d \\tau T_s)A$."}, {"title": "III. PROBLEM FORMULATION OF CHANNEL PREDICTION", "content": "In this section, we formulate the channel prediction as a sequence-to-sequence problem.\nThe channels at the BS is time-varying. The channel over a period of time can be represented as a sequence of matrices as $H_{past} = \\{H_{(n-N_p+1)}, \\dots, H_{(n)}\\}$ $\\in \\mathbb{C}^{N_P \\times R \\times T}$, where $N_p$ is the number of past channels. The BS needs to predict the future channels $H_{future} = \\{H_{(n+1)}, \\dots, H_{(n+N_L)}\\}$ $\\in \\mathbb{C}^{N_L \\times R \\times T}$, where $N_L$ is the number of future channels. The goal of the channel prediction is to predict the future channel $H_{future}$ based on the past estimated channel $H_{past}$.\nAccording to Eq. (2), the channel matrix $H_{past}$ can be recovered from the received signal y and the transmitted signal s. The BS can obtain the estimated past channel $H_{past}$ by receiving the pilot signal from the UE by Minimum Mean-Square Error (MMSE) estimation [33].\nFirst, we vectorize the channel matrix H to transmit pilot signal $p \\in \\mathbb{C}^{RT \\times RT}$ which is a diagonal matrix.\nThe least square (LS) estimation $H_{LS}$ of the channel matrix is\n$vec(H_{LS}) = p^{-1}y$.\nThe MMSE estimation $\\hat{H}_{MMSE}$ of the channel matrix is\n$vec(\\hat{H}_{MMSE}) = R_{HH}(R_{HH} + \\frac{1}{\\gamma_0}I_{RT})^{-1}vec(\\hat{H}_{LS})$,\nwhere $R_{HH} = \\mathbb{E}\\{HH^H\\}$ is the autocorrelation matrix of the channel matrix H, which can be obtained by statistics. $\\frac{1}{\\gamma_0} = \\frac{\\sigma_n^2}{\\gamma_0}$ is the SNR, $\\sigma_s^2$ is the power of the pilot signal, $\\sigma_n^2$ is the power of the noise.\nThus, the channel prediction problem is formulated as\n$H_{future} = f_\\theta(H_{past})$,\nwhere $f_\\theta(\\cdot)$ is the channel prediction neural network, which is learned from the training data, $\\theta$ is the learnable parameters of the neural network.\nThe objective is to find $\\theta$ such that the MSE between the predicted CSI and the ideal CSI is minimized.\n$\\theta = \\underset{\\theta}{\\text{arg min}}(\\|f_\\theta (H_{past}) - H_{future}\\|_2)$,\nThe maximum ratio transmission (MRT) beamforming is adopted in the BS to maximize the SNR of the received signal. According to Eq. (3), if $w = \\begin{bmatrix} w_1 \\\\ \\vdots \\\\ w_R \\end{bmatrix}$, the transmit weighting vector v can be obtained given the predicted future channel $H_{future}$.\nv = \\frac{(w^H H_{future})^H}{||(w^H H_{future})^H ||_2},\nTherefore, the estimated signal vector $\\hat{c}$ can be rewritten as\n$\\hat{c} = c \\alpha + wn$,"}, {"title": "IV. ALL-LINEAR BASED CHANNEL PREDICTION FRAMEWORK", "content": "LinFormer adapts the encoder-only model architecture, which is inspired by the BERT model [34]. Here, the encoder maps an input embedding sequence of estimated past channels $H_{past} = \\{H_{past}^{(1)}, H_{past}^{(2)}, \\dots, H_{past}^{(N_P)}\\}$ to a sequence of continuous representations $F = \\{F^{(1)}, F^{(2)}, \\dots, F^{(N_L)}\\}$. Given the continuous representations F, the prediction head then generates the future channel $H_{future} = \\{H_{future}^{(1)}, H_{future}^{(2)}, \\dots, H_{future}^{(N_L)}\\}$ in parallel."}, {"title": "A. Overall architecture", "content": "Our model integrates two novel components, i.e., the TMLP module and the dimension-wise separable linear head (DSLH), as depicted in Fig. 1. The TMLP is a lightweight module that replaces the self-attention mechanism in the original Transformer model, in which the weights are fixed time-step-dependent. The encoder consists of a stack of N = 6 identical layers, an architecture inherited from the original Transformer that makes LinFormer scalable. The DSLH is a simple linear layer that decomposes the weights of a fully connected matrix to reduce the number of parameters."}, {"title": "B. Principles and Flaws of Attention", "content": "Fig. 2a illustrates the self-attention layer, a key component of Transformer-based models. This layer transforms an input embedding matrix X into an output matrix O. The output is essentially a weighted sum of value matrix V, where the weights are determined by the scaled dot product of query and key vectors. The self-attention mechanism can be expressed in a general form as:\n$O = A X B$,\nwhere X is the input matrix and $B = W_v$. The attention matrix A is\n$A = f(X) = SoftMax(\\frac{X W_q (X W_k)^T}{\\sqrt{d_k}})$,\nwhere $W_q \\in \\mathbb{R}^{d \\times d_k}$, $W_k \\in \\mathbb{R}^{d \\times d_k}$, and $W_v \\in \\mathbb{R}^{d \\times d_v}$ are learnable weight matrices. The dimensions d, $d_k$, and $d_v$ represent the dimensions of the input vector, key, and value, respectively. The weight A used to weight the input X is mapped from X through the function f(.). So it is data-dependent.\nWhile this mechanism is powerful in the field of NLP, it is important to note that its permutation-invariant nature can potentially compromise the preservation of temporal information [21]. In NLP tasks, each word in a sentence has rich semantic information. Even when the word order is disrupted or the letters within a word are rearranged, humans or large language models (LLMs) can still comprehend the content [35]. This is because, the information conveyed by the text is of primary importance and relatively sparse, making the attention mechanism all that a Transformer needs.\nHowever, in the task of CSI prediction, the channel matrix at each time step does not carry strong semantics, and the information is evenly and densely distributed in a channel sequence. And temporal information becomes more critical. Simply using a Transformer may not yield satisfactory results. We derive the channel autocorrelation function as shown in Eq. (5). Assuming the neural network can statistically obtain the autocorrelation function from a substantial number of training samples, the maximum Doppler frequency deviation $f_a$ can be determined given $\\tau$. Once $f_a$ is identified, it becomes feasible to infer speed information, indicating that a channel sequence can represent the relative velocity between the receiver and transmitter. This capability exceeds what can be achieved by analyzing a single channel at an instantaneous time t."}, {"title": "C. Time-Aware Multilayer Perceptron", "content": "In order to make the model aware of time information, the weights in Eq. (16) are designed to be time-step-dependent. The TMLP module is obtained by applying MLP on the time dimension:\n$O = (ReLU(X^T W_1)W_2)^T$,\nwhere $ReLU(\\cdot)$ is a element-wise activation function, $W_1$ and $W_2$ are the weight matrices. So the weights used to weight the input vector X is time-step-dependent. TMLP is relatively simple but very effective in making our model LinFormer time-step-dependent. The TMLP can directly model the long-range dependency of the sequence and avoid cumulative errors. And thanks to the multiplication operator, it is lightweight and computationally efficient, which is suitable for channel prediction tasks."}, {"title": "D. Encoder", "content": "The LinFormer architecture builds upon the multi-block design of the Transformer [21], enhancing its scalability. At its core, the encoder comprises a stack of $N_{enc} = 6$ identical layers, each containing two sub-layers: a TMLP and a feed-forward network (FFN). To maintain the integrity of information flow, each sub-layer incorporates a residual connection, followed by layer normalization [36].\nAs illustrated in step two of Algorithm 3, we propose to substitute the TMLP module for the multi-head self-attention mechanism in the encoder of Transformer. This substitution proves crucial, as our subsequent experiments demonstrate a significant improvement in channel prediction accuracy. The remaining components of the encoder layer remain consistent with the original Transformer design [21].\nThe synergy between these architectural elements creates a robust framework that effectively balances the extraction of local and global temporal features while capturing cross-channel relationships. This unique combination positions the LinFormer as a powerful tool for temporal channel prediction analysis in wireless communication systems."}, {"title": "E. Dimension-wise Separable Linear Head", "content": "As shown in Fig. 4, the prediction head consists of two linear layers. Decomposing the weights of a fully connected matrix from $W_{FC} \\in \\mathbb{R}^{dN_P \\times 2N_LRT}$ to $W_{time} \\in \\mathbb{R}^{N_P \\times N_L}$ and $W_{channels} \\in \\mathbb{R}^{d \\times 2RT}$ can significantly reduce the number of parameters.\nGiven the continuous representations $F \\in \\mathbb{R}^{N_P \\times d}$, the DSLH can reduce the number of parameters to $N_PN_L+2dRT$, which is much smaller than the fully connected matrix $W_{FC}$.\n$\\hat{H}_{future} = (F^T W_{time})^T W_{channels}$,\nwhere $W_{time}$ is a fully connected matrix in the time dimension and $W_{channels}$ is a fully connected matrix in the channels dimension."}, {"title": "F. Loss Function", "content": "To enhance the prediction accuracy of future channels, our approach incorporates a WMSELoss function. The MSE, a standard metric for regression tasks, quantifies the disparity between predicted and actual values, and is defined as\n$MSE = \\frac{1}{RTN_L} \\sum_{n=1}^{N_L} ||\\hat{H}_{future}^{(n)} - H_{future}^{(n)}||_2$,\nwhere $H_{future}$, $\\hat{H}_{future}$ and n denote the actual ideal future channels, the predicted future channels and the time step, respectively. The MSE weights errors at different time steps equally, which can be suboptimal for prediction tasks.\nThis limitation is addressed by considering the data processing inequality [37], which suggests that data loses mutual information as it undergoes more processing. In the context of channel prediction, this implies that near-future channels are generally easier to predict due to their stronger correlation with historical channels compared to far-future channels.\nTo account for this temporal correlation, we derive weights for the proposed WMSELoss function based on the time-domain autocorrelation function of the channel information. We begin with the asymptotic expression of the zero-order Bessel function $J_0(n)$ for large n [38]:\n$J_0(n) \\approx \\sqrt{\\frac{2}{\\pi n}} cos(n - \\frac{\\pi}{4})$."}, {"title": "G. Complexity Analysis", "content": "This section presents a comparative analysis of the inference complexity between our proposed LinFormer model and the standard Transformer model. Our analysis primarily focuses on multiplication operations, as they are computationally more intensive than additions.\nWe begin by establishing that the complexity of multiplying matrices $A \\in \\mathbb{R}^{a \\times b}$ and $B \\in \\mathbb{R}^{b \\times c}$ is denoted as abc. This forms the foundation for our complexity estimations.\nIn the standard Transformer, the attention module\u2019s complexity is influenced by the input sequence length N, input sequence dimension d, and the dimensions of the key and value matrices ($d_k$ and $d_v$, respectively). Typically, $d_k$ and $d_v$ are set equal to d. Referring to Algorithm 1, the complexity of matrix multiplications in steps 1-3 is $3Nd^2$. Step 4 involves two matrix multiplications with complexities $N^2d$ each. The multi-head attention mechanism introduces an additional output mapping $W_o \\in \\mathbb{R}^{d \\times d}$ [21], contributing $Nd^2$ to the complexity. Thus, the total computational complexity of the standard self-attention module is $4Nd^2 + 2N^2d$.\nThe standard Transformer\u2019s decoder and output head contribute an additional complexity of $N_{dec}(4N_Ld^2 + 2N^2d + 2N_pd^2 +2N_Ld^2+2N_pN_Ld+8N_Ld^2)+2N_LdRT$, where $N_L$ and $N_{dec}$ represent the predicted sequence length and number of decoder layers, respectively.\nIn contrast, our proposed TMLP module, as outlined in Algorithm 2, has a computational complexity of $2N^2d$, derived from the matrix multiplications in steps 1 and 3.\nWe now present the computational complexity of our proposed LinFormer model:\nProposition 1. The computational complexity of the proposed LinFormer is\n$2N_{enc}Nd + 2N_{enc}Npd^2 + 2N_LRTd + NpN_Ld$,\nwhere $N_{enc}$, $N_P$, $N_L$, and d denote the number of encoder iterations, input sequence length, output sequence length, and model dimension, respectively.\nProof. The complexity of the feed-forward network remains $2Nd^2$, consistent with the Transformer. For an input sequence of length Np, the encoder\u2019s complexity is $2N_{enc}Nd + 2N_{enc}Npd^2$, where $N_{enc}$ represents the number of encoder iterations. The channel prediction head, following DSLH, contributes $N_pN_Ld + 2N_LRTd$ to the complexity. Summing these components yields the total computational complexity of LinFormer as expressed in Eq. (24).\nThis analysis demonstrates that the LinFormer model achieves significantly lower complexity compared to the standard Transformer. This reduction is achieved by replacing the attention module with the TMLP module and adopting an encoder-only architecture. The resulting low complexity makes the LinFormer model well-suited for practical wireless communication systems."}, {"title": "V. EXPERIMENTS AND DISCUSSIONS", "content": "A. Simulation Settings\nWe consider a MIMO system with R = 2 receive antennas and T = 4 transmit antennas. The prediction model utilizes Np frames of historical CSI $H_{past}$ to predict NL frames of future CSI $H_{future}$. We examine various combinations of past and future frame numbers, with Np taking values of 30, 60, or 90, and NL being either 10 or 30. Consequently, each data sample spans a total of NP + NL frames. In alignment with the 3GPP standard [27], we maintain a consistent time interval of 0.625 ms between consecutive frames. This interval corresponds to the duration of a single Sounding Reference Signal (SRS) transmission, ensuring our simulation adheres to practical timing constraints in wireless communications."}, {"title": "B. Performance Analysis Using Simulated CSI", "content": "1) The length of input and output: Fig. 7 illustrates a comparative analysis of the proposed LinFormer, Transformer, GRU and LSTM models in terms of channel prediction MSE for each frame, considering various input and output sequence lengths. As illustrated in Fig. 7a, both the Transformer and LinFormer models demonstrate significant performance improvements with longer input sequences. In contrast, the GRU and LSTM models show only marginal improvements under similar conditions. These results suggest that Transformer and LinFormer architectures are more capable at leveraging extended input sequences, effectively modeling long-term dependencies and extracting relevant features. Conversely, the inherent limitations of GRU and LSTM models in retaining long-term information hinder their ability to fully utilize extended input sequences.\nFig. 7b reveals that LinFormer and Transformer models exhibit a slight advantage over GRU when using shorter output sequences. A notable finding is the substantial increase in channel prediction error for GRU and LSTM models between the first and tenth frame, attributable to cumulative error propagation. In contrast, LinFormer and Transformer models, leveraging parallel computing capabilities, demonstrate a considerably smaller increase in channel prediction error between the first and tenth frame.\n2) WMSELoss over Predicting Frames: Analysis of Fig. 7b reveals that the initial future frames exhibit the highest prediction accuracy, making them particularly valuable for base station operations such as precoding or beamforming. In light of this observation, we propose the implementation of WMSELoss function for model training. The resultant channel prediction error is depicted in Fig. 8. The results demonstrate notable performance improvements for both LinFormer and Transformer models between the first and fourth frames. In contrast, the GRU model and LSTM do not exhibit similar enhancements. Interestingly, the MSE performance for frames five through ten remains relatively stable, showing only minimal degradation. This stability can be attributed to the weighting scheme applied to the MSE at each time step, which is derived from the autocorrelation of the time channel sequence as defined in Eq. (23).\n3) Robustness to Low SNR: To improve the generalization of our model under various SNRs, we propose a data augmentation methoddetailed in Section V.A. This method involves applying MMSE channel estimation to the input historical channels over an SNR range of 0 to 20 dB, yielding the model input $H_{past}$.\nWithout data augmentation, all training samples are estimated at an SNR of 15 dB. Fig. 9 illustrates that models employing data augmentation demonstrate enhanced MSE performance, particularly at low SNRs. Notably, the Transformer model without data augmentation achieves optimal MSE performance near 15 dB but exhibits performance degradation at higher SNRs, suggesting overfitting to the training conditions. Fig. 10 presents a comprehensive evaluation of all models at 0 dB and 15 dB, showcasing MSE performance across the entire prediction horizon. The results reveal that our proposed LinFormer model, when trained with data augmentation, achieves superior performance at SNR = 0 dB. It not only outperforms the Transformer model but also surpasses the immediate MMSE channel estimation for future time steps. This outcome underscores the generalization ability of LinFormer in low SNR environments.\n4) Model Analysis: Fig. 11 presents a comparative analysis between the TMLP module and the standard attention module. In this experiment, we replace the standard Transformer's decoder with the DSLH, as detailed in Section IV.E. The results indicate that the MSE performance deteriorates after the removal of the Transformer's decoder component. Nonetheless, within the encoder-only architecture, LinFormer demonstrates a significant improvement in performance by simply replacing the attention mechanism with the proposed TMLP module. To assess the sensitivity of the channel prediction models to temporal information in the input sequence, we conducted an experiment illustrated in Fig. 12. This involved shuffling the 90 time steps of each sample input using a consistent permutation across all samples.\nThis result aligns with expectations: although the Transformer employs positional encoding, it still suffers from a loss of positional information. Consequently, the Transformer model faces challenges in accurately capturing the maximum Doppler frequency shift characteristics and differentiating between varying user terminal speeds, resulting in diminished prediction accuracy. Regarding RNN-like models, such as LSTM and GRU, their performance degradation can be attributed to the constraints of their hidden state size, which may lead to the loss of long-range dependencies in the input sequence.\nAs illustrated in Fig. 12, we shuffled the 90 time steps of each sample input according to the same permutation to investigate the model\u2019s sensitivity to the time information of the input sequence. The \u201c+\u201d symbol indicates that the model is trained on this shuffled input sequence across all samples. In contrast, our proposed LinFormer model, featuring time-step-dependent weights, demonstrates remarkable resilience, maintaining consistent performance even when subjected to identical input sequence shuffling.\n5) Inference Speed: Fig. 13 illustrated the inference speed and MSE of LSTM, GRU, Transformer and LinFormer in case of using 90 frames to predict 10 frames on a single NVIDIA GTX 1050 Ti GPU. The results demonstrate that LinFormer outperforms other models, achieving both the lowest MSE and the highest inference speed. The inference times for Transformer, LSTM, and GRU models range from approximately 4 to 10 ms. This duration is comparable to 10 SRS periods, as defined by industry standards. This means that these models are not fast enough to be applied in practical systems. In contrast, the proposed LinFormer model completes its predictions within 2 ms, demonstrating significant potential for practical applications in time-sensitive wireless communication scenarios.\n6) Scaling Efficiency: Fig. 14 illustrates a comparative analysis of the MSE performance between the Transformer and LinFormer architectures across various parameter configurations. The MSE here is the average of all time steps and test samples. We compared the performance of Transformer and LinFormer architectures with varying parameter volumes. Specifically, we increased the number of encoder layers in LinFormer to raise its parameter count, while for Transformer, we increased the number of both encoder and decoder layers. Interestingly, we observed that beyond a certain threshold of parameter count, the MSE performance on the test set began to deteriorate for both models. This phenomenon, indicative of overfitting, may be attributed to the limited size of our dataset. This finding underscores the importance of balancing model complexity with the available training data to achieve optimal performance in channel prediction tasks.\n7) Data Volume: As shown in Fig. 15a, the MSE performance of all models decreases as the amount of training data increases. The LinFormer model has the best performance. With a limited 10% amount of training data, we achieved results similar to those in [17], with GRU performing the best in the nearest future time steps. However, as the amount of training data increased, both Transformer and LinFormer showed significant improvements, whereas GRU and LSTM demonstrate the much smaller improvement.\nAs shown in Fig. 15b, the MRT channel capacity of all models increases as the amount of training data increases. We also observed that when the MSE is relatively high, the channel capacity with MRT beamforming is not inversely correlated with MSE. This occurs because, with the MRT beamforming method, the predicted channel matrix is summed over the transmit antenna dimension T.\n8) Performance Vs. Speed: As illustrated in Fig. 16, and 16b, we tested all models at SNR = 0 dB and 15 dB across speeds ranging from 30 to 60 km/h in Fig. 16a and 0 to 300 km/h in Fig. 16b, respectively. Additionally, we observe that under speed settings ranging from 0 to 300 km/h, the overall MSE performance surpasses that of speeds from 30 to 60 km/h. This improvement can be attributed to the training dataset containing approximately 6 million samples at speeds from 0 to 300 km/h, which is ten times greater than those available for speeds from 30 to 60 km/h. The richer training dataset enables the model to learn more generalized features, thereby achieving enhanced performance."}]}