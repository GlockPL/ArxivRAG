{"title": "ILDiff: Generate Transparent Animated Stickers by Implicit Layout Distillation", "authors": ["Ting Zhang", "Zhiqiang Yuan", "Yeshuang Zhu", "Jie Zhou", "Jinchao Zhang"], "abstract": "High-quality animated stickers usually contain transparent channels, which are often ignored by current video generation models. To generate fine-grained animated transparency channels, existing methods can be roughly divided into video matting algorithms and diffusion-based algorithms. The methods based on video matting have poor performance in dealing with semi-open areas in stickers, while diffusion-based methods are often used to model a single image, which will lead to local flicker when modeling animated stickers. In this paper, we firstly propose an ILDiff method to generate animated transparent channels through implicit layout distillation, which solves the problems of semi-open area collapse and no consideration of temporal information in existing methods. Secondly, we create the Transparent Animated Sticker Dataset (TASD), which contains 0.32M high-quality samples with transparent channel, to provide data support for related fields. Extensive experiments demonstrate that ILDiff can produce finer and smoother transparent channels compared to other methods such as Matting Anything and Layer Diffusion. Our code and dataset will be released at link https://xiaoyuan1996.github.io.", "sections": [{"title": "I. INTRODUCTION", "content": "As a commonly used medium on social platforms, animated stickers play an important role in conversation and communication [1] [2]. To obtain the available animated stickers, existing video generation algorithms can be directly applied [3] [4]. However, most of the results generated by these models are in the RGB color space and lack the transparent channel [5] [6], which plays a crucial role in high-quality stickers.\nTo predict a transparent channel based on an RGB image, the cutting-edge algorithms can be roughly divided into video matting-based algorithms [7] [8] and diffusion-based algorithms [9] [10]. The video matting method usually trains the neural network based on a large amount of data to directly predict the transparent channel. Diffusion method, such as layer diffusion [7], encodes transparent channel into the potential manifold of the pre-trained diffusion model, and transform the model into a transparent image generator by slightly changing the pre-trained distribution.\nAlthough the above two methods have made some progress in natural scenes, they inevitably encounter some problems when directly applied to the modeling of transparent animated stickers. Due to the singleness of the background color in stickers, the video matting-based method does not work well when processing semi-open areas, where the foreground and background colors are consistent. While the current diffusion-based method is used to model a single image without considering the temporal information [9] [10], which will cause local flicker when modeling animated stickers.\nIn this paper, an ILDiff method is proposed to generate transparent animated channels through implicit layout distillation, which solves the problems of semi-open area collapse and no consideration of temporal information in existing methods. Specifically, we manage to add implicit layout information via distill SAM to the current diffusion-based method, thus to characterize the semi-open area in the case of consistent foreground and background colors, which is difficult to be processed by the classical segmentation model. Next, we design a temporal modeling branch to endow ILDiff with the ability of temporal processing, thereby improving the local flickering problem encountered by traditional diffusion-based methods. Lastly, we construct a transparent animated sticker dataset TASD, which contains 0.32M high-quality samples with transparent channel, to provide data support for this field. The experiments qualitatively and quantitatively show that ILDiff can produce more refined and smoother transparent channels compared with other methods. The main contributions of our work are as follows:\n\u2022\tWe propose ILDiff, a method that leverages implicit layout distillation to generate animated transparent channels, which achieves a more refined and smoother result compared to other state-of-the-art methods.\n\u2022\tA high-quality transparent animated sticker dataset TASD with corresponding benchmark is contributed, aiming to provide more data support for intelligent creation."}, {"title": "II. METHODS", "content": "Diffusion models [11] [12] are a notable class of generative models that create new samples by gradually introducing noise to an initial data point and subsequently learning to reverse this process. Given a paired dataset $(x, y) \\sim P_{train}(x, y)$, where $x$ represents a video clip consisting of $N$ frames, $x = \\{x_i | i = 1, 2, ..., N\\}$, and $y$ corresponds to the associated caption, these models aim to transform samples from a Gaussian distribution into the target distribution through iterative processes. In the forward diffusion process, at each step $t$, a noisy image $x_t$ is generated using the formula $x_t = \\sqrt{\\alpha_t}x + \\sqrt{1 - \\alpha_t}\\epsilon$, where $\\epsilon \\sim N(0, I_d)$ denotes Gaussian noise, $I_d$ is the identity matrix of dimension $d$, and $\\alpha_t$ determines the noise ratio at step $t$. The reverse process employs a learnable network $G_{\\epsilon_t}(x, t)$ to predict the noise and recover the clean image from the noisy input $x_t$. After training, the model begins with pure Gaussian noise $x_T \\sim N(0, I_d)$ and iteratively refines this noise over $T$ steps to sample a clean image.\nLatent diffusion models [13] introduce the latent space, making the diffusion process more efficient and computationally less intensive. Among these, the classic Stable Diffusion (SD) combines a Variational Autoencoder (VAE) [17] for diverse and stable image generation. VAE is trained by maximizing the likelihood function and its variational inference. Structurally, it includes an encoder $E_{sd}$ that maps RGB images to a latent space, and a decoder $D_{sd}$ that reconstructs images from these latent representations. Based on the latent diffusion, Layer Diffusion [9] extends the capabilities of large-scale pre-trained latent diffusion models to directly generate images with transparency. It trains two independent neural network models: a latent transparency encoder $E_{tr}$ and a decoder $D_{tr}$. During training, the encoder $E_{tr}$ is responsible for transforming the RGB channels $I_r \\in R^{h \\times w \\times 3}$ and the transparent channel $I_{\\alpha} \\in R^{h \\times w \\times 1}$ into an offset $\\Delta z \\in Z$ in the latent space. This offset is added to the original latent vector $z \\in Z$ to form an adjusted latent vector $\\tilde{z}_{adj} = z + \\Delta z$. The decoder $D_{tr}$ then reconstructs the transparent image, yielding reconstructed RGB $\\hat{I}_r$, and transparent channels $\\hat{I}_\\alpha$. Layer Diffusion trains $E_{tr}$ and $D_{tr}$ using reconstruction losses $||I_r - \\hat{I}_r||_2^2$ and $||I_\\alpha - \\hat{I}_\\alpha||_2^2$ to ensure the quality of the decoded transparent channel. Additionally, a harmlessness measure is designed by comparing the image differences between the inputs and outputs of frozen encoder $E_{sd}$ and decoder $D_{sd}$, making the latent distribution of the pre-trained model not be disrupted by introduced transparency channel."}, {"title": "B. Implicit Layout Distill for Animated Transparent Channels", "content": "Although layer diffusion can generate transparent channels for single images, there are two problems when dealing with animated stickers: (a) Failure to consider timing information, which will cause local flicker when modeling animated stickers. (b) As with the video matting method, the prior perception of semi-open areas needs to be injected. To improve the above problems, we come up with a layout adapter module, which attempts to introduce layout priors while performing temporal modeling. The layout adapter module consists of an image encoder and a temporal modeling layer. The image encoder captures the layout information of each component, while the temporal modeling layer integrates the inter-frame temporal information. The output feature of the layout adapter module is then incorporated into the latent space to guide the training process.\nFor layout prior, what we need is an implicit feature, which can represent the semi-open area and is different from the strict segmentation information. For this reason, we distill the Segment Anything Model (SAM) [18] to train the image encoder, making it better suited to the lightweight pipeline of the layout adapter. Followed by EfficientSAM [19], we use mask image modeling to learn the high-level feature embeddings generated by the ViT-H [20] encoder of SAM, achieving effective feature representation. Correspondingly, the reconstruction distillation loss function can be expressed as $L_R = ||f_{sam}(x) - f_h(x)||_2^2$, where $||\\cdot||_2^2$ denotes the L2 norm distance, $f_{sam}(x)$ and $f_h(x)$ represent the features output by the SAM image encoder and the features of the distilled lightweight image encoder in specific frame after MAE [21] reconstruction and linear projection, respectively.\nFurthermore, the temporal modeling layer effectively captures the temporal dependencies of each frame feature output by the distilled image encoder through a series of 3D convolutional layers. Each layer employs Group Normalization and ReLU activation functions to ensure stable and efficient training. We found that simple 3D convolutional layers can effectively achieve smooth transitions between frames of animated sticker component elements. The final output features are dimensionally transformed using a 2D convolution and adaptive average pooling to map to the same dimension in the latent space."}, {"title": "III. TRANSPARENT ANIMATED STICKER DATASET", "content": "We collected 0.32M high-quality animated sticker data with transparent channels from private sources by retrieving 115 crawled words. To obtain relevant text descriptions, we first manually labeled part of the data for the annotation task, which enabled us to fine-tune the caption model. Next, the manually annotated data is used to perform supervised fine-tuning on VideoLlama to improve the accuracy of automatic annotation, so that the trained model can be directly used for visual caption generation. In addition to the English version, we also provide a Chinese version to facilitate the needs of researchers. Crawled words related to the visual samples are aggregated and used as trigger words."}, {"title": "IV. RELATED WORKS", "content": "Animated Sticker Generation Stickers are a common medium for communication on social platforms, introducing a dynamic and engaging element to multimedia interactions. Knn-Diffusion [22] investigated static sticker generation, achieving the creation of out-of-distribution stickers using extensive retrieval methods. Text-to-Sticker [23] initially fine-tuned Emu [24] with millions of weakly supervised sticker images to promote diversity. Subsequently, they introduced a style-tailoring fine-tuning approach, enhancing the visual quality of stickers by simultaneously fitting content and style distributions. For multi-frame sticker generation, Animated Stickers [25] utilized a two-phase fine-tuning process: leveraging weak in-domain data at first, and then employing a human-machine loop strategy, which significantly improved motion quality. This paper improves animated sticker generation by generating transparent channel, achieving better visual effects across different application scenarios.\nTransparent channel generation For transparent sticker generation, current research can be categorized into post-processing methods [26] [27] and generative approaches [9] [10]. Post-processing methods primarily focus on image segmentation and matting. These methods divide RGB images into different regions based on color, texture, and other visual features. Kirillov et al. introduced the Segment Anything Model (SAM) [18], a foundational model in computer vision capable of segmenting any object based on user prompts. Further, SAM was extended to SAM-2 [8], which incorporates a Transformer structure with streaming storage and memory mechanisms to achieve coherent segmentation predictions in video sequences. Matting Anything [7] builds on SAM's feature maps and mask outputs, adding a lightweight Mask-to-Matte module to predict the transparent channel. On the other hand, generative methods such as layer diffusion [9] present the concept of \"latent transparency\u201d. This method encodes transparent channel transparency within the latent space of pre-trained latent diffusion models, such as Stable Diffusion, using separate encoder and decoder networks to transform and reconstruct transparency information. Building on layer diffusion, we have extended its capability to handle animated images, and integrated the advantages of SAM's post-processing methods to inject implicit layout information."}, {"title": "V. EXPERIMENTS", "content": "We use layer diffusion as the backbone and adds the layout adapter for training. During training, we fix the weights of VAE and SAM encoder and only fine-tune the layout adapter. The depth of temporal layer is 5, which is selected through ablation experiments in Section V-B."}, {"title": "B. Quantitative & Qualitative Results", "content": "Automatic Metrics. For method comparison, since SAM and Matting Anything are designed for single image processing, we segmented each frame of the expressions individually. We marked the center point and the outer rectangle of the ground truth alpha image to simulate user input of point prompts and box prompts. For SAM2, which has the capability of video segmentation, we provided prompts only for a single frame containing the subject. Our method is significantly ahead of other methods in both two indicators, which verifies that ILDiff can better model transparent channels with the help of implicit layout. Among other comparison methods, Layer Diffusion ranks second in both indicators, followed by Matting Anything."}, {"title": "User Preference", "content": "In addition to automatic metrics, we also introduce manual evaluation to overcome the limitation of existing metrics. We request ten labelers to select the best generated stickers in terms of frame smooth and hole residue."}, {"title": "Ablative Study", "content": "As shown in , we ablated the depth of the temporal layer in the layout adapter to study the ILDiff setting that achieves the best generation performance. When the depth of temporal layers is 0, ILDiff only adds implicit layout information compared to layer diffusion, which achieves suboptimal PSNR indicator. When the layer depth is gradually increased to 5, the PSNR and SSIM indicators of the ILDiff model reach the optimal. As the layer depth increases further, the quantitative performance of ILDiff begins to decay."}, {"title": "VI. CONCLUSION", "content": "In this paper, we propose an ILDiff method to generate transparent animated stickers, which achieves a smoother transparent channel with less hole residue than other methods. In addition, we also provide a high-quality transparent animated sticker dataset TASD, which aims to provide richer data resources for the field of intelligent creation."}]}