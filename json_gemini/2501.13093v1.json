{"title": "Guaranteed Recovery of Unambiguous Clusters", "authors": ["Kayvon Mazooji", "Ilan Shomorony"], "abstract": "Clustering is often a challenging problem because of the inherent ambiguity in what the \"correct\" clustering should be. Even when the number of clusters K is known, this ambiguity often still exists, particularly when there is variation in density among different clusters, and clusters have multiple relatively separated regions of high density. In this paper we propose an information-theoretic characterization of when a K-clustering is ambiguous, and design an algorithm that recovers the clustering whenever it is unambiguous. This characterization formalizes the situation when two high density regions within a cluster are separable enough that they look more like two distinct clusters than two truly distinct clusters in the clustering. The algorithm first identifies K partial clusters (or \"seeds\") using a density-based approach, and then adds unclustered points to the initial K partial clusters in a greedy manner to form a complete clustering. We implement and test a version of the algorithm that is modified to effectively handle overlapping clusters well, and observe that it requires little parameter selection, and delivers improved performance on artificial and benchmark datasets compared to widely used algorithms for non-convex cluster recovery.", "sections": [{"title": "I. INTRODUCTION", "content": "There is often significant ambiguity in what the \u201ccorrect\" clustering is for a dataset. Even in the case when the number of clusters K is known, this ambiguity often still exists because a cluster can have multiple relatively separated regions of high density, and clusters can have very different densities. These difficulties cause many algorithms to incorrectly separate a true cluster into multiple clusters, while merging true clusters or failing to detect sufficiently prominent true clusters of low density. These issues are compounded by the well-known fact that many clustering algorithms are ineffective at identifying non-convex clusters, which are present in many applications including image segmentation [1], geospatial data [2, 3], and time series data [4].\nIn this paper, we try to overcome these difficulties from an information theoretic perspective by proposing a necessary condition for a K-clustering C to be unambiguous, along with an algorithm that recovers C whenever this condition holds. This condition characterizes the situation where two high density regions within the same cluster in C look more like two distinct clusters than two truly distinct clusters in C. Our framework yields the provable recovery of unambiguous K-clusterings that can have\n\u2022 clusters with arbitrarily many relatively separated regions of high density\n\u2022 arbitrary variation in density among different clusters\n\u2022 arbitrary variation in density within clusters\n\u2022 arbitrarily shaped clusters.\nOur approach is information theoretic in the sense that our algorithm guarantees recovery of a K-clustering whenever the dataset warrants an unambiguous K-clustering. This is in contrast to algorithms like K-means that are motivated by the optimization of a clustering quality measure.\nThe clustering algorithm we propose is somewhat complex, but has a computationally efficient runtime. It works by first finding a small subset of points called a \"seed\" from each cluster, and then expanding the seeds to form clusters. We implement and test a version of the algorithm that is modified to handle overlapping clusters well, and observe that it requires little parameter selection, and delivers improved performance on artificial and benchmark datasets compared to widely used algorithms for non-convex cluster recovery.\nWe begin by discussing related work and introducing background information and notation. We then present our main theoretical results, followed by experiments comparing our algorithm to existing algorithms."}, {"title": "II. RELATED WORK", "content": "The most widely used paradigms for finding non-convex clusters are spectral clustering, and density-based clustering. Spectral clustering algorithms perform dimensionality reduction on the data points, and run a simple clustering algorithm such as K-means to cluster the low dimensional data [5]. Density-based clustering algorithms find regions of high point density, and then output a set of high density clusters according to some criterion [6\u20139]. While spectral clustering is very well studied from a theoretical standpoint, most widely used density-based algorithms do not come with theoretical guarantees on when a clustering is recoverable.\nThe first widely-used density-based clustering algorithm was DBSCAN [6]. Since then, many algorithms have been designed to improve upon various aspects of DBSCAN [7\u201314]. In addition to DBSCAN, the other most widely used density-based clustering algorithms are OPTICS [7] and HDBSCAN [8], which were both designed to improve upon DBSCAN's ability to output clusters of varying density. On the theoretical side, there has been a recent line of work studying \u03bb-density level set estimation using DBSCAN [14\u201316].\nOur algorithm works by first identifying K partial clusters (or \"seeds\") using a density-based approach, and then adding unclustered points to the initial K partial clusters in a greedy manner to form a complete clustering. To extract these seeds, we sequentially find seeds of decreasing density. The intuitive idea of sequentially finding disjoint clusters of decreasing density has been employed in various work [11, 12]. However, to the best of our knowledge, our algorithm for finding K seeds and our algorithm for expanding the seeds to form complete clusters have not previously appeared in the literature. Furthermore, we are not aware of any existing mathematical analysis of density-based clustering algorithms that give guarantees similar to those presented in this paper."}, {"title": "III. PRELIMINARIES", "content": "We use X to denote a set of data points. The distance between points x and y is denoted by d(x,y). As in many density-based clustering papers, the measure of density at a point $x \\in X$ is determined by an integer $N_p$, and is defined as 1/$\\epsilon_{N_p}(x)$ where $\\epsilon_{N_p}(x)$ is the minimum distance $\\epsilon$ such that there are $N_p$ points in X at distance at most $\\epsilon$ from x (including x itself). In other words, $\\epsilon_{N_p}(x)$ is the distance from x to its ($N_p$-1)th nearest neighbor in X. We call $\\epsilon_{N_p}(x)$ the sparsity at x.\nA cluster is simply a set of points in X. A clustering C of X is a set of disjoint clusters, where every point in X belongs to exactly one cluster (i.e. a partitioning of X into clusters). A K-clustering is a clustering with K clusters. A partial clustering of X is a set of disjoint clusters whose union does not necessarily include all points in X. We say that a clustering C extends (or is an extension of) a partial clustering C' if there exists a bijective function f that maps the clusters in C' to the clusters in C such that for each cluster c' \u2208 C"}, {"content": "c' is a non-empty subset of the cluster $f(c') \\in C$.\nWe say a point $x_1$ is $\\epsilon$-connected to a point $x_t$ if there exists some sequence of points $x_1, x_2, ..., x_t$ such that $x_{i+1}$ is distance at most $\\epsilon$ from $x_i$ for $1 \\le i \\le t \u2212 1$ and $\\epsilon_{N_p}(x_i) \\le \\epsilon$ for $1 \\le i \\le t$. A set of points c is called $\\epsilon$-connected if every pair of points in c is $\\epsilon$-connected.\nFor a given $\\epsilon$, a set of points e is called an $\\epsilon$-cluster if it is a maximal $\\epsilon$-connected set of points. In other words, any point that is $\\epsilon$-connected to a point in an $\\epsilon$-cluster c is included in c. A set of points c is called a maximal cluster if it is an $\\epsilon$-cluster for some $\\epsilon$. For a given $\\epsilon$, it is helpful to consider the graph that is formed where each node corresponds to a data point, and an edge is drawn between two nodes if the corresponding points $x_1, x_2$ are such that $\\epsilon_{N_p}(x_1) \\le \\epsilon$, $\\epsilon_{N_p}(x_2) \\le \\epsilon$, and $d(x_1,x_2) \\le \\epsilon$. A set of points is an $\\epsilon$-cluster if and only if it corresponds to a connected component in this graph.\nThe $\\epsilon$-cluster centered at a point x is defined as the set of points that are $\\epsilon$-connected to x, and is denoted by $c^*(x, \\epsilon)$. If $\\epsilon < \\epsilon_{N_p}(x)$, then $c^*(x, \\epsilon) = {\\}$. The sparsity of a set of points c is defined as the minimum $\\epsilon$ such that c is $\\epsilon$-connected, and is denoted by $\\epsilon^*(c)$. The $\\epsilon$-distance between points x and y is defined as the minimum $\\epsilon$ such that x is $\\epsilon$-connected to y, and is denoted by $\\epsilon(x, y)$. The minimum $\\epsilon$-distance from a point x to a cluster c is defined as $\\epsilon(x, c) = \\min_{y \\in c} \\epsilon(x, y)$. The minimum $\\epsilon$-distance from cluster $c_1$ to cluster $c_2$ is defined as $\\epsilon(c_1, c_2) = \\min_{x \\in c_1, y \\in c_2} \\epsilon(x, y)$.\nFor a dataset X and density parameter $N_p$, the dendrogram G = (V, E) is a tree structure that gives all $\\epsilon$-clusters in X for each $\\epsilon > 0$. V and E are the sets of nodes and edges in G respectively. Each node v \u2208 V corresponds to an $\\epsilon$-cluster for some $\\epsilon$, and the clusters corresponding to the children of v form the smallest possible partition of the maximal cluster corresponding to v into maximal clusters. Each value of $\\epsilon$ specifies a clustering given by all nodes in V corresponding to $\\epsilon$-clusters. For a given x and $N_p$, the dendrogram is unique, and yields a hierarchy of possible clusterings, making it the foundational structure in hierarchical density-based clustering (HDBSCAN) [8].\nFor a node v \u2208 V, c(v) denotes the cluster corresponding to v. Each node v has a real number $\\epsilon(v)$ associated with it where $\\epsilon(v)$ is the smallest $\\epsilon$ such that c(v) is a $\\epsilon$-cluster. For a leaf node v corresponding to the cluster {x}, $\\epsilon(v) = \\epsilon_{N_p}(x)$. A node $v_0$ has children $v_1, v_2, ..., v_i$ if $c(v_1), c(v_2), ..., c(v_i)$ form the smallest possible partition of $c(v_0)$ composed of maximal clusters, which is guaranteed to be unique. The root node $v_r$ of the resulting tree is such that $c(v_r) = X$.\nAny S \u2286 V such that no node in S is a descendant of another node in S induces a (partial) clustering of X given by {c(v) for v \u2208 S}. Any such clustering is called a dendrogram clustering. In fact, any (partial) clustering that consists of maximal clusters is a dendrogram (partial) clustering since every maximal cluster corresponds to a node in the dendrogram. The $\\epsilon$-cut of a dendrogram is the (partial) clustering that includes all clusters corresponding to nodes v such that $\\epsilon(v) \\le \\epsilon$, and v has no parent $v'$ with $\\epsilon(v') \\le \\epsilon$. For any integer K, there exists at most one partial clustering given by an $\\epsilon$-cut of G that contains K clusters. The (partial) clustering output by DBSCAN for a given $N_p$ and $\\epsilon$ can be formed by taking the $\\epsilon$-cut of G, and adding any unclustered point x to the cluster c that minimizes d(x,c) if d(x, c) < $\\epsilon$ where $d(x, c) = \\min_{y \\in c} d(x, y)$."}, {"title": "IV. RESULTS", "content": "We first define two conditions that should be satisfied for a K-clustering C of X to be unambiguous. The first condition is called weak separability, and the second is called local maximum separability. We then design an algorithm that is guaranteed to recover C if these two conditions are satisfied.\nA. Weak Separability\nThe simplest density-based notion of cluster separability is what we call weak separability.\nDefinition 1 For a given $N_p$, C is called weakly separable if for each c \u2208 C, c is $\\epsilon$-connected where $\\epsilon < \\min_{c' \\in C, c' \\neq c} \\epsilon(c, c')$.\nIf a clustering C is not weakly separable, there is ambiguity in what the correct clustering should be in the sense that there must exist a cluster c \u2208 C such that any $\\epsilon$-cluster containing c must include at least one point from a distinct cluster c' \u2208 C. Figures 1 and 2 illustrate weak separability."}, {"title": "Interestingly, a dataset X often has more than one weakly separable K-clustering. For example, let $N_p = 2$ and $C = [\\{1, 3, 5, 7.02, 9.02, 11.02\\}, \\{17, 18, 19, 20\\}, \\{22.01, 23.01, 24.01, 25.01\\}]$. Clearly, C is weakly separable. However, $C' = [\\{1,3,5\\}, \\{7.02, 9.02, 11.02\\}, \\{17, 18, 19, 20, 22.01, 23.01, 24.01, 25.01\\}]$ is also a weakly separable 3-clustering. Intuitively, C is the correct clustering because the spacing between points is nearly uniform within each cluster in C, while there is a large relative gap in the middle of the third cluster in C'. It is indeed very common that the intuitively correct clustering is weakly separable, but is not the unique weakly separable clustering. In fact, it follows from Lemma 1 that any set of maximal clusters that partition X (a dendrogram clustering) is weakly separable.", "content": "Lemma 1 For a given $N_p$, C is weakly separable if and only if it is a dendrogram clustering.\nProof: Suppose C is a dendrogram clustering, but is not weakly separable. Let G = (V, E) be the dendrogram. Then there exists some v, v' \u2208 V such that c(v), c(v') \u2208 C, we have that $\\epsilon(v) \\ge \\epsilon(c(v), c(v'))$, then c(v) would include points from c(v') since c(v) is maximal. This contradicts the definition of a clustering.\nIf C is weakly separable, then each cluster in C is maximal. This follows because if a cluster c is not maximal, there exists a point x \u2209 c that is $\\epsilon$-connected to c such that $\\epsilon < \\epsilon^*(c)$, which contradicts weak separability. Every maximal cluster has a corresponding node in G, so C is a dendrogram clustering.\nCorollary 1 For a given $N_p$, X has a unique weakly- separable K-clustering if and only if exactly one dendrogram K-clustering exists.\nIf there are two weakly separable clusterings for X, it is impossible to guarantee that we recover one of them if our only criteria is to find a weakly separable clustering. Because there usually does not exist a unique weakly separable K-clustering even when an intuitively correct K-clustering exists, weak separability alone as a sufficient condition for clustering recoverability is not adequate."}, {"title": "B. Local Maximum Separability", "content": "To define local maximum separability, several new definitions are required. We call a point x \u2208 X a local maximum if $\\epsilon_{N_p}(y) \\ge \\epsilon_{N_p}(x)$ for y \u2208 X such that $d(x, y) \\le \\epsilon_{N_p}(x)$. For a cluster c \u2208 C, let $X^\u2217_c$ denote the set of all points x in c such that $\\epsilon_{N_p}(x) = \\min_{y \\in c} \\epsilon_{N_p}(y)$. In other words, $X^\u2217_c$ denotes the set of all points x in c that have highest density among points in c.\nFor a given density parameter $N_p$, the relative separability of a point x \u2208 X to a point y \u2208 X is the value of A such that $A \u00b7 \\epsilon_{N_p}(x) = \\epsilon(x, y)$, and is denoted by A(x, y). Similarly, the relative separability of a point x \u2208 X to a cluster c is given by $\\min_{y \\in c} A(x, y)$, and is denoted by A(x, c)."}, {"title": "Definition 2 For a given $N_p$ and C, we use $A^0(C)$ to denote the minimum $A \\in \\mathbb{R}$ such that", "content": "\\min_{\\substack{y \\in c(x) \\\\ \\epsilon_{N_p}(y) < \\epsilon_{N_p}(x)}} A(x, y) \\le A,\nfor every local maximum x \u2208 $X^\u2217_c$ where there exists a y \u2208 c(x) such that $\\epsilon_{N_p}(y) < \\epsilon_{N_p}(x)$. C is called local maximum separable (LM-separable) if\n$A^0(C) < \\min_{c, c' \\in C} \\min_{z \\in X^*_c} A(z, c')$.\nIn specific, if C has only one local maximum per cluster, then C is trivially LM-separable.\nLM-separability specifies that for every local maximum x \u2208 $X^*_c$ where there exists a y \u2208 c(x) whose density is at least as high as that of x, the minimum relative separability of x to such a y is smaller than the relative separability of the highest density point in any cluster to another cluster. This is a condition that should hold for a clustering to unambiguous because if there exists a local maximum x where the minimum relative separability to such a point y \u2208 c(x) is higher than the relative separability of a highest density point z \u2208 $X^*_c$ for some c \u2208 C to another c' \u2208 C (and therefore to a local maximum in c'), then in a sense, x looks more like it belongs to a separate cluster from y than z looks like it belongs to a separate cluster from c', and the correct K-clustering of X is ambiguous. Figures 1 and 2 illustrate LM-separability.\nNote that LM-separability does not imply weak separability. Let $N_p = 2$, and consider the clustering C = [{\\{7, 8, 10, 13, 21\\}, \\{17, 25, 27\\}]. C is clearly not weakly separable because the clusters overlap, but is LM-separable, as 7, 8, 25, 27 are the only local maximums.\nOur main result states that if C is weakly separable and LM-separable for a given density parameter $N_p$, then it is the unique weakly separable, LM-separable clustering for $N_p$, and can be reconstructed efficiently."}, {"title": "Theorem 1 If C is weakly separable and LM-separable for $N_p$, then C is the unique weakly separable, LM-separable clustering for $N_p$, and can be found in O(|X|\u00b3 log(|X|)) time.", "content": "For a given $N_p$, it is in general not possible to recover the unique weakly separable, LM-separable clustering by simply finding an extension of the (partial) clustering given by the $\\epsilon$-cut of the dendrogram that contains K clusters (if there exists such an $\\epsilon$) as proved in Lemma 2. Since DBSCAN follows this approach, it is not sufficient for finding the unique weakly separable, LM separable clustering.\nLemma 2 There exists a weakly separable, LM-separable clustering for some $N_p$ that does not extend any (partial) clustering given by an $\\epsilon$-cut of the dendrogram.\nProof: Recall that there can only be one $\\epsilon$-cut of the dendrogram that gives a (partial) K-clustering. Let $N_p = 3$, and suppose that C = [{\\{1, 3, 5, 7.02, 9.02, 11.02\\}, \\{17, 18, 19, 20\\}, \\{22.01, 23.01, 24.01, 25.01\\}]. This is clearly weakly separable and LM-separable. The only $\\epsilon$-cut that gives a (partial) clustering with three clusters is chosen by setting $\\epsilon = 2.01$, and is given by $C' = [{\\{3\\}, \\{9.02\\}, \\{17, 18, 19, 20, 22.01, 23.01, 24.01, 25.01\\}]$. The only weakly separable clustering that is an extension of $C'$ is [{\\{1,3,5\\}, \\{7.02, 9.02, 11.02\\}, \\{17, 18, 19, 20, 22.01, 23.01, 24.01, 25.01\\}].\nIn the clustering C used to prove Lemma 2, the distance separating the points 1,3,5 from the points 7.02, 9.02, 11.02 within the first cluster is 2.02, which is larger than the distance of 2.01 separating the clusters \\{17, 18, 19, 20\\} and \\{22.01, 23.01, 24.01, 25.01\\}. However, 2.02 is very similar to the distance of 2 separating the other pairs of adjacent points in the first cluster, while 2.01 is very large compared to the distance of 1 separating the adjacent points in the second cluster. C is therefore a more natural clustering than C' in a sense, but an $\\epsilon$-cut is unable to capture C because it only considers absolute distances when separating clusters, without taking cluster density into account.\nIn the appendix we discuss a stronger notion of separability called strong separability which implies weak separability and LM-separability. Lemma 2 holds for strong separability as well."}, {"title": "V. ALGORITHM AND PROOF OF THEOREM 1", "content": "For density parameter $N_p$, we will prove that if C is weakly separable and LM-separable, then Algorithm 1 returns C, thus proving that C is the unique weakly separable and LM-separable |C|-clustering for $N_p$. We then prove that the algorithm can be implemented to run in O(|X|\u00b3 log(|X|)) time.\nAlgorithm 2 is an auxiliary algorithm used to define Algorithm 1. Algorithm 2 works by outputting a partial K-clustering of X that can then be postprocessed to form a K-clustering. For Algorithm 2, A governs the maximum variation in density within a partial cluster, M governs the minimum size of a partial cluster, and D governs the maximum variation in density among different partial clusters. We use $C_g(X, N_p, A, M, D)$ to denote the partial clustering of X that is output by Algorithm 2 with parameters $N_p$, A, M, D."}, {"title": "Algorithm 1 Minimal Seed Expansion", "content": "Input: X, $N_p$, M, D, K\nOutput: $\\hat{C}$\n1: $C' \\leftarrow$ output of Algorithm 2 with parameters $N_p$, M, D, K\n2: $\\hat{C} \\leftarrow$ output of Algorithm 3 with parameters X, C"}, {"content": "$N_p$\nAt a high level, Algorithm 2 forms a partial clustering of X by greedily selecting the highest density unclustered point in X, and creating a maximal cluster centered at the point that satisfies the partial cluster constraints set by the parameters A, M, D. Note that if there are multiple candidate points for x* in a greedy step of Algorithm 2, the final output is not affected by which candidate is assigned to x*. Thus, for parameters X, $N_p$, A, M, D, the clustering $C_g(X, N_p, A, M, D)$ output by Algorithm 2 is unique."}, {"title": "Definition 3 The min-A clustering of X with parameters $N_p$, M, D, K is given by $C_m(X, N_p, A_{min}, M, D)$ where", "content": "$A_{min} = \\min A such that |C_g (X, N_p, A, M, D)| = K$."}, {"title": "For parameters $N_p$, M, D, K, the min-A clustering is denoted by $C_m(X, N_p, M, D, K)$. $A_{min} \\ge 1$ since $c^*(x, \\epsilon) = {\\}$ if $\\epsilon < \\epsilon_{N_p}(x)$.\nAlgorithm 1 accepts $N_p$, M, D along with a number of clusters Kas input, and begins by finding the min-A partial clustering with parameters M, D, K. The min-A partial clustering is then passed to Algorithm 3 to produce the final output clustering.", "content": "Algorithm 2 Greedy algorithm to find partial clusters\nInput: X, $N_p$, A, M, D\nOutput: C'\n$C' \\leftarrow \\emptyset$\nMinExtracted $\\leftarrow \\infty$\nTried $\\leftarrow \\emptyset$\nwhile X \\\\ Tried $\\neq \\emptyset$ do\n$x^* \\leftarrow arg \\min_{x \\in X \\\\ Tried} \\epsilon_{N_p} (x)$\nif $\\epsilon_{N_p} (x^*) > D \\cdot MinExtracted$ then\nbreak loop\n$c' \\leftarrow c^* (x^*, A \\cdot \\epsilon_{N_p} (x^*))$\nif $|c'| > M$ and $c' \\cap c = \\emptyset \\forall c \\in C'$ then\nadd c' to C'\nif MinExtracted == $\\infty$ then\n$MinExtracted \\leftarrow \\epsilon_{N_p} (x^*)$\nX $\\leftarrow X\\\\c'$\nelse\nTried $\\leftarrow Tried \\cup x^*$\nAlgorithm 3 Greedy algorithm to expand partial clusters\nInput: X, C'\nOutput: $\\hat{C}$\n$\\hat{C} \\leftarrow C'$\nY $\\leftarrow$ X \\\\ ($\\cup_{c \\in C'} c$)\nwhile $\\cup_{c \\in \\hat{C}} c \\neq$ X do\n$(x^*, c^*) \\leftarrow arg \\min_{z \\in Y, c \\in \\hat{C}} \\epsilon(x, c)$\n$c^* \\leftarrow c^* \\cup \\{x^*\\}$\nY$\\leftarrow$Y\\\\x^*$\nLemma 3 For a given $N_p$, if C is LM-separable, then $|C_g(X, N_p, A^0(C),1,\\infty)| = K$.\nProof: We will prove that all partial clusters output by Algorithm 2 are subsets of distinct clusters in C. Consider the ith partial cluster output by Algorithm 2. The highest density point x that the algorithm uses to build the ith cluster is clearly a local maximum, and can either be from a previously partially reconstructed cluster in C, or can be from a new cluster in C.\nIf x is from a previously partially reconstructed cluster c \u2208 C, then it must include some z \u2208 $X^*_c$, by the definitions of LM- separability and $A^0(C)$, and the fact that $\\epsilon_{N_p}(x) \\ge \\epsilon_{N_p}(z)$. This is a contradiction. This along with the fact that M = 1 < $N_p$ guarantees that x must be from a new cluster c' \u2208 C and x \u2208 $X^*_{c''}$. The ith partial cluster $c^*(x, A^0(C) \u00b7 \\epsilon_{N_p}(x))$ must"}, {"title": "only include points from c"}]}