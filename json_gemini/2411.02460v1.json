{"title": "Code-Switching Curriculum Learning for Multilingual Transfer in LLMs", "authors": ["Haneul Yoo", "Cheonbok Park", "Sangdoo Yun", "Alice Oh", "Hwaran Lee"], "abstract": "Large language models (LLMs) now exhibit near human-level performance in various tasks, but their performance drops drastically after a handful of high-resource languages due to the imbalance in pre-training data. Inspired by the human process of second language acquisition, particularly code-switching\u2014the practice of language alternation in a conversation\u2014we propose code-switching curriculum learning (CSCL) to enhance cross-lingual transfer for LLMs. CSCL mimics the stages of human language learning by progressively training models with a curriculum consisting of 1) token-level code-switching, 2) sentence-level code-switching, and 3) monolingual corpora. Using Qwen 2 as our underlying model, we demonstrate the efficacy of the CSCL in improving language transfer to Korean, achieving significant performance gains compared to monolingual continual pre-training methods. Ablation studies reveal that both token- and sentence-level code-switching significantly enhance cross-lingual transfer and that curriculum learning amplifies these effects. We also extend our findings into various languages, including Japanese (high-resource) and Indonesian (low-resource), and using two additional models (Gemma 2 and Phi 3.5). We further show that CSCL mitigates spurious correlations between language resources and safety alignment, presenting a robust, efficient framework for more equitable language transfer in LLMs. We observe that CSCL is effective for low-resource settings where high-quality, monolingual corpora for language transfer are hardly available.", "sections": [{"title": "1 Introduction", "content": "As recent advances in natural language processing (NLP) have benefited from their remarkable scale, large language models (LLMs), such as ChatGPT (OpenAI, 2022) and Llama (Touvron et al., 2023), have emerged with strong capabilities in knowledge (Roberts et al., 2020), generation (Karanikolas et al., 2024), and reasoning (Huang and Chang, 2023), on par or even surpassing human levels. Such LLMs are inherently multilingual agents, as web-crawled, extensively large training data includes diverse languages. However, these models perform poorly in non-English, especially low-resource languages (Wang et al., 2024a). This discrepancy arises from the imbalanced distribution of language resources in pre-training data, as collecting extensive data in all languages is practically impossible (Ranta and Goutte, 2021). To address this challenge, researchers have explored cross-lingual transfer techniques to improve LLM performance in non-English languages (Houlsby et al., 2019; Ke et al., 2023, inter alia). Inspired by the second language acquisition in humans, we look at code-switching for cross-lingual transfer in LLMs. Code-switching, an alternating use of two or more codes within one conversational episode, is a common practice in language learning (Auer, 1998). At first, second language learners at the basic level often rely on code-switching to express their intentions while minimizing misunderstanding (Ghaderi et al., 2024). As they become more proficient, they begin to produce complete sentences, eventually exhibiting full fluency in the target language. In other words, both frequency and degree of code-switching in language learning are closely linked with learners' proficiency level (Sinclair and Fern\u00e1ndez, 2023). Following this learning process, we introduce a new strategy: code-switching curriculum learning (CSCL), which adapts the pedagogical process of human language acquisition to the context of language transfer of LLMs (Figure 1). Our approach involves further training English-"}, {"title": "2 Code-Switching Curriculum Learning", "content": "In this section, we describe CSCL, a curriculum learning strategy designed for language transfer of English-centric LLMs using code-switching corpora. This approach is inspired by the pedagogical process of second language acquisition, starting from partial, word-level code-switching and gradually achieving complete, fluent use of the target language (Ghaderi et al., 2024; Sinclair and Fern\u00e1ndez, 2023)."}, {"title": "2.1 Background", "content": "Code-Switching Code-switching, also known as code-mixing or language alternation, is an alternating use of two or more codes within one conversational episode (Auer, 1998). Code-switching is a common linguistic phenomenon that occurs both consciously and unconsciously for various intentions, including but not limited to incomplete proficiency in language learning, effective communication using appropriate terminology, and inclusion or exclusion of certain groups in a multi-lingual society (Mabule, 2015). For example, English learners use code-switching in classrooms to avoid misunderstanding and bridge the gap of competence (Ghaderi et al., 2024); the frequency of code-switching is linked to learners' proficiency level in second language acquisition (Sinclair and Fern\u00e1ndez, 2023).\nCurriculum Learning Bengio et al. (2009) first proposed the curriculum learning paradigm, which denotes formalizing training strategies of machine learning models to be organized from easy to hard. This approach is inspired by cognitive principles suggesting that humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order, which gradually illustrates more concepts and more complex ones. This seminal work has been widely applied in various domain applications (Kumar et al., 2010; Jiang et al., 2015, 2018, inter alia)."}, {"title": "2.2 CSCL", "content": "To implement CSCL, we categorize training data into three distinct phases that align with increasing difficulty in second language acquisition: 1) token-level code-switching, 2) sentence-level code-switching, and 3) monolingual text. We then employ the curriculum learning paradigm and further pre-train LLMs sequentially across three phases."}, {"title": "1) Token-Level Code-Switching", "content": "First, we use a token-level code-switching corpus where mixed tokens implicitly reveal cross-lingual alignment between two languages. Due to the limited availability of human-written code-switching datasets in various languages, we generate synthetic token-level code-switching data (Figure 2). For this, we employ gpt-40, a state-of-the-art proprietary LLMs, with the following instruction, based on but slightly adjusted from the data synthesis method in Yoo et al. (2024). A detailed prompt for code-switching data synthesis is described in Appendix B."}, {"title": "2) Sentence-Level Code-Switching", "content": "Secondly, we further train LLMs using a sentence-level code-switching corpus, where sentences in the target language and English are alternated within the same semantic context. To create this dataset, we use parallel corpora that align English sentences with corresponding sentences in the target language. In this phase, target language sentences and English sentences are ordered sequentially without semantically overlapping content. In other words, if i-th sentence is in the target language, then (i + 1)-th sentence is in English, both sharing the same context but not being a direct translation."}, {"title": "3) Monolingual Texts", "content": "We finally train LLMs with monolingual texts, similar to conventional further training methods for language transfer. Here, we use the identical size of monolingual corpora in both the target language and English to prevent catastrophic forgetting of English."}, {"title": "3 CSCL Experiments", "content": "In this section, we empirically evaluate the effectiveness of CSCL via language transfer experiments, specifically targeting the adaptation of English-centric LLMs to Korean."}, {"title": "3.1 Experimental Setup", "content": "Training Datasets We use Korean-English parallel data to construct code-switching training data of CSCL, following the steps in Section 2.2. We also use the same size of monolingual Korean and English data. The number of tokens for training data in each phase is 1B, totaling 3B. Appendix A describes the training data and details for the following experiments.\nEvaluation Datasets To assess the efficacy of language transfer and to gauge the degree of catastrophic forgetting in English, we employ six diverse evaluation datasets covering multiple-choice question answering (MCQA) and machine translation (MT). For Korean MCQA, we use K-MMLU (Son et al., 2024a), HAE-RAE (Son et al., 2024b), CLICK (Kim et al., 2024a). For machine translation of English-to-Korean and Korean-to-English, we use FLoRes-200 (Team et al., 2022). Additionally, we include MMLU (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021) for English evaluation. Accuracy is reported for all tasks except MT, for which we use the COMET score \u00b9 (Rei et al., 2020), as COMET aligns more closely with human evaluations compared to other metrics such as BLEU score that only measures lexical overlap (Freitag et al., 2022; Xu et al., 2024).\nModel We employ Qwen 2 (7B) (Yang et al., 2024), an open LLM known for its multilingual performance, particularly in English and Chinese, for language transfer to Korean."}, {"title": "3.2 Experimental Results", "content": "Table 1 presents the experimental results of Qwen 2 (7B) trained for Korean language transfer using CSCL. We compare it to traditional approaches using Korean monolingual corpora (Ko) and both Korean and English monolingual corpora (Ko-En). CSCL outperforms the traditional training approaches across all Korean MCQA benchmarks and in both language pairs of MT tasks. While all language transfer methods lead to slight performance degradation in English due to catastrophic forgetting, CSCL mitigates this effect, with a performance drop of only 4.2%p in MMLU and 1.4%p in GSM8k, compared to pre-trained Qwen 2 trained with monolingual Korean corpora only. It indicates that CSCL effectively enhances cross-lingual alignment between two languages."}, {"title": "3.3 Cross-lingual Consistency", "content": "Here, we evaluate the degree of cross-lingual transfer by measuring consistency between languages, under the assumption that a truly multi-lingual language model should deliver consistent answers across languages (Qi et al., 2023; Xing et al., 2024). To this end, we use Multilingual MMLU (MMMLU) (Hendrycks et al., 2021)\u00b2, a dataset comprising 14K parallel MCQA pairs in 14 languages, including English and Korean. Table 2 presents the results for cross-lingual consistency between English and Korean.\nCSCL achieves the highest ratio of samples correctly answered in both languages (i.e., (\u221a, \u221a)), owing to a decrease in the proportion where the model correctly responds in English but fails in Korean (i.e., (\u2713, X)). In contrast, the consistency"}, {"title": "3.4 Generation Quality Estimation", "content": "We comprehensively evaluate the generation quality of multilingual LLMs trained with language transfer techniques across two tasks: text summarization (TS) and machine translation (MT, EN\u2192Ko). We assess the output quality using three measures: 1) conventional task-specific metrics-Rouge-L (Lin, 2004) for TS and COMET (Rei et al., 2020) for MT, 2) quality estimation score (out-of-100) using LLM-as-a-judge (Zheng et al., 2023) (gpt-40), and 3) the ratio of outputs containing any code-switching texts. For TS, we use AI Hub data\u00b3, comprising 400K samples whose document sources from news articles, editorials, magazines, and precedent. For MT, we follow the same experimental setup above using FLoRes-200 (Team et al., 2022). A detailed system prompt for LLM-as-a-judge is described in Appendix B.\nTable 3 presents the quality estimation results for Qwen 2 (7B) model, comparing baseline performance with two language transfer methods: monolingual training (Ko-En) and CSCL. Zhao et al. (2024) reported that 2-5% of outputs from multilingual LLMs include unintended code-switching after language adaptation. We observe that CSCL does not significantly increase unin-"}, {"title": "3.5 Code-Switching Data Analysis", "content": "We quantitatively and qualitatively analyze the token-level code-switching training data used in our experiments. Here, we define a code-switching chunk as a sequence of consecutive word(s) in a single language within a code-switching sentence. In other words, code-switching chunks are in a single language, either Korean or English, disregarding numerals or symbols. On average, a sentence in the token-level code-switching data switches languages 5.3 times, with Korean chunks being longer than English ones. Specifically, the Korean chunks comprise 4.1 words, while English chunks average 2.2 words. 4\nTable 4 provides a qualitative analysis of the token-level code-switching data, highlighting three characteristics commonly observed in human code-switching and one unique feature of AI-generated synthetic data:\nFrequent Part-of-Speech Aligning with Chi and Bell (2024) where NOUN and PROPN frequently appear as code-switching words, we observe that code-switching also happens frequently as NOUN in synthetic data. Notably, code-switching does not occur just at the word level; in-"}, {"title": "4 Discussions", "content": ""}, {"title": "4.1 Effect of Code-Switching and Curriculum Learning", "content": "We conduct an ablation study to isolate the effects of code-switching and curriculum learning within CSCL by varying the training data composition. Table 5 shows the experimental results of Qwen 2 (1.5B) further trained with different data combinations. Both models trained solely with token-level or sentence-level code-switching corpora only surpass those trained with monolingual"}, {"title": "4.2 Language and Model Variations", "content": "We extend our analyses to include various languages (Table 6) and foundation models (Table 7). We train Qwen 2 (1.5B) in three languages: Japanese (high-resource), Korean (mid-resource), and Indonesian (low-resource) as categorized by Joshi et al. (2020). We also trained three distinct foundation models: Qwen 2 (1.5B) (Yang et al., 2024), Gemma 2 (2B) (Team et al., 2024), and Phi 3.5 (3.8B) (Abdin et al., 2024). Following the experimental setup from the aforementioned studies, we evaluate those models using MMMLU2, a human-translated, parallel MMLU dataset, and Flores-200 (Team et al., 2022) with COMET scoring. Table 6 showcases that CSCL consistently outperforms a traditional pre-training method using both monolingual target language and English across both MMMLU and MT tasks. Furthermore, the observations generally extend to various model families, with a minor exception in Phi 3.5, which exhibits a slight accuracy drop (0.2%p) on English MMLU as in Table 7."}, {"title": "4.3 Safety Evaluation in CSCL", "content": "Previous studies on AI safety have highlighted the susceptibility of LLMs to non-English (Upadhayay and Behzadan, 2024), code-switching (Yoo et al., 2024) adversarial queries (i.e., red-teaming). Yoo et al. (2024); Song et al. (2024) discovered that this vulnerability arises due to a spurious correlation between language resources and safety alignment in multilingual LLMs, a byproduct of resource imbalance in safety data for multilingual LLMs. To evaluate model robustness against adversaries, we assess attack success rate (ASR), refusal rate (RR), and comprehension scores (Cmp.) using LLM-as-a-judge, as described in Yoo et al. (2024) (See Appendix B for a detailed system prompt). We employ MultiJail (Deng et al., 2024) and CSRT (Yoo et al., 2024) as parallel red-teaming queries in English, Korean, and code-switching between two languages as test datasets.\nTable 8 compares the evaluation results of two Qwen 2 (1.5B) models trained for Korean language transfer using two different methods: traditional pre-training with monolingual Korean and English corpora (Ko-En) and CSCL. We observe that CSCL-based models are robust to all attacks in English, Korean, and code-switching adversaries in terms of both ASR and RR, except for English ASR. In addition, CSCL exhibits better multilingual comprehension in all inputs, indicating enhanced cross-lingual alignment. These findings suggest that CSCL can mitigate the spurious correlation between language resources and safety alignment in multilingual LLMs, thereby improving model robustness."}, {"title": "4.4 Scaling Monolingual Corpora", "content": "We finally conduct an ablation study to control the size and the ratio of training data in the three phases in CSCL. While we fix the size of both token-level code-switching corpora and sentence-level code-switching corpora as 1B each, we enlarge the size of monolingual corpora as doubled (i.e., 1B, 2B, 4B, and 8B) by keeping the ratio of English and Korean in the monolingual corpora as identical. Figure 3 presents the experimental results of Qwen 2 (1.5B) trained for Korean language transfer using two methods: conventional training with monolingual corpora (Ko-En) and CSCL. We evaluate those models on multilingual MMLU in Korean and English, and the baseline results without any further training are denoted as a dotted line.\nAs more monolingual corpora are incorporated into training, both models advance in either Korean or English, following the scaling law (Kaplan et al., 2020). Notably, the performance gap"}, {"title": "5 Related Work", "content": ""}, {"title": "5.1\nCode-Switching", "content": "As early as the 1980s, several linguistic theories have attempted to model the generation process of code-switching sentences (Choudhury et al., 2019). For instance, Equivalence Constraint theory contends that code-switching occurs without violating the surface structure of either language (Poplack, 1980). Functional Head theory posits that code-switching is restricted between a functional head and its complement (Myers-Scotton, 1993). Matrix Language theory introduces the concept of a matrix language and an embedded language (Belazi et al., 1994).\nSimilarly, decades of research in natural language processing (NLP) have shed light on understanding, collecting, and generating code-switching texts as language models become multilingual agents (Winata et al., 2023). For instance, Zhang et al. (2023); Huzaifah et al. (2024) examined multilingual LLMs with code-switching inputs, particularly including machine translation tasks. However, the availability of code-switching datasets remains limited, focusing on specific language pairs, such as Hindi-English (Khanuja et al., 2020; Singh et al., 2018) and Vietnamese-English (Nguyen and Bryant, 2020). To address the lack of diverse code-switching data, several code-switching synthesis techniques have been proposed. Jayanthi et al. (2021); Rizvi et al. (2021) introduced toolkits to generate synthetic code-switching data using Part-of-Speech tags and dependency parsers, though these tools are primarily"}, {"title": "5.2 Curriculum Learning", "content": "In the context of natural language processing, curriculum learning has demonstrated its power in textual domains and language modeling (Wang et al., 2024b). Xu et al. (2020); Campos (2021); Wang et al. (2023) implemented curriculum learning strategies in natural language understanding tasks, according to difficulty score by cross-reviewed difficulty evaluation, linguistic features, and word frequency, respectively. Li et al. (2021) and Feng et al. (2023); Lee et al. (2024) presented curriculum learning for pre-training and instruction tuning LLMs, respectively.\nPrevious NLP studies have adopted curriculum learning using code-switching, while their trials were tied up with outdated, RNN-based language models aiming for enhancing understanding within code-switching texts rather than general multilingual modeling. In particular, Choudhury et al. (2017) proposed curriculum learning under RNN-based architecture that trains the network with monolingual data first and then trains the resultant network with code-switching data. Pratapa et al. (2018) presented that the training curriculum above reduces the perplexity of RNN-based language models in code-switching texts. To date, however, curriculum learning using code-switching texts has yet to be extensively studied in LLMs, particularly for multilingual language modeling for language transfer."}, {"title": "5.3 Language Transfer in LLMs", "content": "Multilingual language models exhibit inferior in non-English, low-resource languages due to language imbalance in the pre-training data, while their performance in English is on par with humans (Team, 2023). As pre-training LLMs from scratch require extensive computational costs and data, recent studies have explored efficient strategies for language adaptation, such as continual pre-training (Ke et al., 2023) and adapter tuning (Houlsby et al., 2019). For instance, Cui et al."}, {"title": "6 Conclusion", "content": "In this paper, we introduce code-switching curriculum learning (CSCL), inspired by the pedagogical process of second language acquisition of human, where code-switching is employed according to their proficiency levels. We regard the degree of code-switching in language learning as a measure of difficulty and apply curriculum learning for language transfer, starting from training with token-level code-switching corpora, sentence-level code-switching corpora, and finally monolingual corpora in both target language and English. We validate CSCL through comprehensive experiments using Qwen 2 as the underlying model and Korean as the target language. We demonstrate that CSCL outperforms the traditional pre-training method with monolingual target corpora in terms of 1) performance boost in target language measured by multiple-choice question-answering and machine translation tasks and 2) reduced performance loss in English typically caused by catastrophic forgetting during language transfer. We further extend our observations across various languages and foundation models. Notably, CSCL does not induce unintended code-switching in the generated outputs; instead, it significantly enhances the generation ability in the target language, comprehensively evaluated through summarization and instruction following tasks. We also conduct an ablation study highlighting the distinct contributions of both code-switching and curriculum learning. Furthermore,"}, {"title": "7 Limitations", "content": "While LLM adaptation practices typically involve vocabulary extension, further pre-training, and instruction tuning, our approach focuses solely on further pre-training. This choice aligns with Zhao et al. (2024), which reported that vocabulary extension might not be necessary at training scales of tens of billions of tokens. This study specifically targets language transfer within LLMs and demonstrates the efficacy of the CSCL for further training. While our study demonstrates the efficacy of CSCL in language transfer, we leave extending its application to instruction tuning and assessing impacts on downstream tasks for future research.\nIn addition, our experiments center on Qwen 2 (7B) as the primary model, and all ablation studies are conducted on smaller models due to computational limitations. While we verify the efficacy of the CSCL using diverse model architectures, further testing is needed to confirm the scalability of CSCL with larger models.\nFinally, there is still room for improvement with language transfer in extremely low-resource languages. While we validate CSCL across high-, mid-, and low-resource languages (Japanese, Korean, and Indonesian), its performance in extremely low-resource settings, such as local languages (e.g., Javanese or Hausa), requires further investigation."}, {"title": "8 Ethics Statement", "content": "This study uses publicly open models and established benchmarks to evaluate the efficacy of CSCL in language transfer, without involving human subjects. All evaluations are conducted automatically using gold-standard labels or with LLM-as-a-Judge (gpt-40)."}, {"title": "Appendix", "content": ""}, {"title": "A Training Details", "content": "We utilize 16 A100 GPUs and train the backbone model spanning 3 epochs, using a warm-up ratio of 0.01, a context length of 4,096 tokens, and a weight decay of 0.01. The peak learning rate is set at 2e-5, with an inverse square learning rate decay to 0. The training operates under fp16 precision, facilitated by deepspeed (Rasley et al., 2020) and flash attention (Dao et al., 2024). The resources are provided by NSML (Naver Smartest Machine Learning Platform). We assign the temperature of the generation models as 0.0 (i.e., greedy decoding).\nThe source data for code-switching data synthesis includes OPUS (Tiedemann, 2012), which mainly contains CCMatrix (Schwenk et al., 2021), CCAligned (El-Kishky et al., 2020), and ParaCrawl (Ba\u00f1\u00f3n et al., 2020), AI Hub 5,6, and JHE (Park et al., 2016). After collecting all possible sources, we filter out the duplicated samples. We employ AI Hub7 data and Dolma (Soldaini et al., 2024) for monolingual corpora in Korean and English, respectively, randomly filtering out some samples by matching the size as identical as code-switching corpora (i.e., 1B tokens)."}, {"title": "B System Prompts for LLM-as-a-Judge", "content": "This section illustrates a system prompt used for code-switching data synthesis and LLM-as-a-judge using gpt-40 in the experiments above."}, {"title": "B.1\nCode-Switching Data Synthesis", "content": "We follow and slightly modify the system prompt of Yoo et al. (2024).\nGiven a pair of {Target Language}-English parallel sentences, generate a code-switching sentence. Code-switching is the use of more than one linguistic variety in a manner consistent with the syntax and phonology of each variety."}, {"title": "B.2 Generation Tasks", "content": "We follow and slightly modify the system prompt of Zheng et al. (2023).\nPlease act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question. Your evaluation should consider fluency and naturalness. You will be given a reference answer and the assistant's answer. Your evaluation should focus on the assistant's answer to the second question. Begin your evaluation by comparing the assistant's answer with the reference answer. Identify and correct any mistakes. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 100 by strictly following this format: \u201c[[rating]]\u201d, for example: \u201cRating: [[50]]\u201d."}, {"title": "B.3\nCode-Switching Red-Teaming", "content": "We follow the system prompt of Yoo et al. (2024).\nYou are a helpful LLM evaluation assistant. You will receive a prompt and the AI assistant's response to the prompt. Please act as an impartial judge and evaluate the response of LLM within the range from 0 to 1 with respect to three criteria: attack success rate (ASR), comprehension, and refusal rate. Be as objective as possible.\n# Criteria description\n## ASR\nASR, which stands for attack success rate, indicates whether the response provided by the AI assistant violates ethical, legal, or safety guidelines. Your evaluation should consider factors including clarity, level of detail, and accuracy provided in the response. Rate the response from 0.0 (harmless) to 1.0 (harmful). A rating of O signifies a fully non-jailbroken response - either irrelevant to the task or refusal due to ethical or legal concerns. A rating of 1 signifies responses that directly and fully violate ethical, legal, or safety guidelines and comprehensively answer the task. Suppose the response is legitimate or provides an alternative suggestion. In that case, the assistant is"}]}