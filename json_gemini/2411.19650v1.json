{"title": "CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation", "authors": ["Qixiu Li", "Yaobo Liang", "Zeyu Wang", "Lin Luo", "Xi Chen", "Mozheng Liao", "Fangyun Wei", "Yu Deng", "Sicheng Xu", "Yizhong Zhang", "Xiaofan Wang", "Bei Liu", "Jianlong Fu", "Jianmin Bao", "Dong Chen", "Yuanchun Shi", "Jiaolong Yang", "Baining Guo"], "abstract": "The advancement of large Vision-Language-Action (VLA) models has significantly improved robotic manipulation in terms of language-guided task execution and generalization to unseen scenarios. While existing VLAs adapted from pre-trained large Vision-Language-Models (VLM) have demonstrated promising generalizability, their task performance is still unsatisfactory as indicated by the low tasks success rates in different environments. In this paper, we present a new advanced VLA architecture derived from VLM. Unlike previous works that directly repurpose VLM for action prediction by simple action quantization, we propose a componentized VLA architecture that has a specialized action module conditioned on VLM output. We systematically study the design of the action module and demonstrate the strong performance enhancement with diffusion action transformers for action sequence modeling, as well as their favorable scaling behaviors. We also conduct comprehensive experiments and ablation studies to evaluate the efficacy of our models with varied designs. The evaluation on five robot embodiments in simulation and real work shows that our model not only significantly surpasses existing VLAs in task performance but also exhibits remarkable adaptation to new robots and generalization to unseen objects and backgrounds. It exceeds the average success rates of OpenVLA which has similar model size (7B) with ours by over 35% in simulated evaluation and 55% in real robot experiments. It also outperforms the large RT-2-X model (55B) by 18% absolute success rates in simulation. Code and models can be found on our project page.", "sections": [{"title": "1. Introduction", "content": "In recent years, there has been a surge of interest in robotic control models equipped with visual capabilities [7, 8, 15, 30, 34, 45, 48, 58, 60, 62, 67, 69]. Among them, the development of large-scale Vision-Language-Action (VLA) models [8, 30, 32] are particularly promising, which empowers robots to perform complex tasks guided by natural language instructions and potentially manage objects or environments that deviate from the training distribution. Additionally, they exhibit rapid adaptability to new tasks and embodiments through finetuning.\nThe notable generalization capability of large VLAs can be attributed to both their substantial model size and the potent Vision-Language-Models (VLM) [13, 28, 35] that serve as their foundation. These VLMs are typically pretrained on massive, Internet-scale image-text pairs, which play a crucial role in enhancing VLA generalization to novel objects and semantically diverse instructions [8].\nExisting large VLAs often adapt VLMs for action prediction in simple ways, leading to several issues that hinder task performance. For instance, works like [8, 30] directly quantize the continuous spectrum of robot actions into discrete bins in accordance to the next token prediction scheme of VLMs. However, such a simple quantization, unlike sophisticated tokenizers such as those designed for images [65, 72] and audio [19, 73], poses difficulties in action learning and limits action precision. [32] introduces additional action heads, such as LSTMs, to transform VLM output into actions. The shift to a regression-based learning scheme, however, overlooks the probabilistic and multimodal\u00b9 nature of actions.\nIn this paper, we propose a new VLA model architecture derived from VLM. Instead of repurposing pretrained VLMs for action prediction, we use the cognitive information extracted by VLM to guide the action prediction process of a specialized action module. To handle the inherent characteristics of action signals \u2013 continuous, multimodal, temporally correlated, and requiring high precision \u2013 we employ advanced diffusion-based transformers (DiT) [51] as our action modules, preconditioned on VLM output via the attention mechanism.\nThe intuition behind our design is the decoupling of \u201ccognition\u201d and \u201caction\u201d capabilities. While the large VLMs amass broad visual and semantic knowledge learned from vast amounts of text and images, the cognitive capability and the output language modality have fundamental gaps to dense robot actions. Rather than directly repurposing the VLMs, we advocate the design of componentized VLAs with a dedicated action module.2 This action mod-"}, {"title": "2. Related Works", "content": "Vision-Language-Action Models. The success of Large Language Models (LLMs) [2, 9, 63, 64] and Vision-Language Models (VLMs) [1, 14, 28, 37, 61] has inspired the development of Vision-Language-Action (VLA) models, which extend the capabilities of VLMs by integrating action generation. For instance, RoboFlamingo [32] extends OpenFlamingo [3] by incorporating a head network to predict actions and optimizing with MSE loss. RT-2 [8]"}, {"title": "3. Method", "content": "Problem Formulation. Our goal is to develop a VLA model that enables different robots to physically execute diverse tasks while receiving visual observations and language instructions. Formally, given the language instruction I and visual observation \\(o_t\\) at time t, a model \\(\\pi\\) predicts a temporal action sequence \\((a_t, a_{t+1}, ..., a_{t+N})\\) for execut-\ning the desired task:\n\\[\\pi: (l, o_t) \\rightarrow (a_t, a_{t+1}, ..., a_{t+N}).\\]\nWhile in general, \\(a_t\\) can describe various robot actions with different control modes and end-effectors, we consider the action space of a gripper with 7 degrees of freedom (DoF) in this work:\n\\[a_t = [\\Delta x, \\Delta y, \\Delta z, \\Delta\\varphi, \\Delta\\theta, \\Delta\\psi, g],\\]\nwhere \\(\\Delta x, \\Delta y, \\Delta z\\) are the relative translation offsets of the end effector, \\(\\Delta\\varphi, \\Delta\\theta, \\Delta\\psi\\) denote the rotation changes, and \\(g \\in \\{0,1\\}\\) indicates the gripper's open/close state.\nOverall architecture. To effectively handle complex visual observations and language instructions and collaboratively transform them into precise actions, we componentize the model into three parts: a vision module, a language module, and an action module, as shown in Fig. 2. We describe each part in details below.\n3.1. Vision and Language Modules\nOur vision and language modules are adapted from an existing VLM from [28] that has about 7B parameters in total, similar to [30]. We briefly describe them below for completeness.\nVision Module. The vision module processes raw images input into a set of perceptual tokens. It consists of powerful vision transformers, DINOv2 [49] and SigLIP [74], pretrained on Internet-scale image data, to capture rich visual features and a comprehensive semantic understanding of the observations. At each timestep t, the image observation \\(o_t\\) is fed into the two models, producing two downsampled feature maps \\(f^{DINO}_{t}\\) and \\(f^{Sig}_{t}\\), respectively. These feature maps are then concatenated along the channel dimension, passed through a linear projection layer, and serialized into a set of visual perceptual tokens, \\(V = \\{V_1, V_2, ..., V_{N_v} \\}\\) with a length \\(N_v\\) (we use 256 by default).\nLanguage Module. The language module is responsible for integrating visual information and language instructions and conducting cognitive reasoning. Here, a LLAMA-2 model [64] is applied as the backbone. The language instruction l is converted into a set of linguistic tokens, \\(T = \\{l_1, l_2, ..., l_{N_l} \\}\\), using LLAMA-2's tokenizer. These tokens are then concatenated with the visual tokens V and an additional learnable cognition token c, and processed by the model using a causal attention mechanism. The resulting output feature \\(f_c\\), corresponding to the cognition token, encodes integrated information that determines the action to be executed for the current task. This serves as a condition for the subsequent action module to interpret and derive the desired actions."}, {"title": "3.2. Diffusion Action Module", "content": "The action module receives the cognition feature as an input condition to generate a series of actions, as defined in Eq. (1) and (2). Given that real-world physical actions are continuous and often multi-modal, we predict them using a diffusion modeling process [47]. To model complex and temporally-correlated actions, we apply a diffusion transformer (DiT) [51] as a powerful backbone for the action decoding process.\nSpecifically, our action module takes the cognition feature \\(f_c\\) along with a series of noisy actions \\((\\hat{a}^i_t, \\hat{a}^i_{t+1},..., \\hat{a}^i_{t+N})\\) as input, where i denotes the current denoising step. It predicts the final actions \\((a_t, a_{t+1}, ..., a_{t+N})\\) through multiple denoising steps. The cognition feature and the noisy actions serve as input tokens to the transformer blocks, while the step information i is added to the cognition feature with a sinusoidal positional encoding. We enforce the action model to predict not only the current action \\(a_t\\) but also multiple future actions \\((a_{t+1},..., a_{t+N})\\). This approach enhances the overall smoothness of the predicted actions at each time step and increases the final success rates for task execution, as observed similarly in previous studies [15, 75]. In practice, the number of predicted future actions is set to a small value (N = 15 by default), leading to a context length of N+2 = 17 for the action module. This makes the diffusion process highly efficient and does not introduce much computational cost to the overall framework."}, {"title": "3.3. Training Objective", "content": "Our vision module, language module, and action module are trained/finetuned end-to-end by minimizing the mean-squared error (MSE) between the predicted noises from the action module and the ground truth noises. The loss function is defined as:\n\\[L_{MSE} = E_{\\epsilon \\sim N(0,1), i} || \\hat{\\epsilon}^i - \\epsilon ||_2,\\]\nwhere \\(\\hat{\\epsilon}^i\\) is the predicted noise for the noisy action sequence \\((\\hat{a}^i_t, \\hat{a}^i_{t+1},..., \\hat{a}^i_{t+N})\\) at the i's denoising step, and \\(\\epsilon\\) is the corresponding ground truth."}, {"title": "3.4. Adaptive Action Ensemble", "content": "During inference, our model predicts actions for multiple time steps. One straightforward strategy is to execute these actions consecutively (Action Chunking [75]) based on the current observation, \\(o_t\\). However, this does not fully leverage the visual information available at each time step and may result in jerky motion, as discussed in [75]. Alternatively, executing only the action for the current time step (i.e., \\(a_t\\)) also leads to a less smooth trajectory and diminished performance.\nTo alleviate these, [75] introduced a temporal ensemble strategy that combines actions predicted for the current time step from both present and past predictions using preset aggregation weights. However, feasible actions for task execution can belong to different modes [15], and simply aggregating them could result in an action that does not align with any mode, which is suboptimal.\nWe propose an adaptive ensemble strategy which considers similarities between actions to be aggregated, as shown in Fig. 3. This approach avoid unreasonable aggregation of actions from different modes. Specifically, let \\(a_t|o_t\\) represent the action prediction for the current time step t given the observation \\(o_t\\), and \\(\\{a_t|o_{t-K},......, a_t|o_{t-1}\\} \\) denote the corresponding action predictions based on historical observations \\(\\{o_{t-K},......, o_{t-1}\\} \\). We derive the final action \\(a_t\\) to be executed at time step t as:\n\\[a_t = \\sum_{k=0}^{K} w_{t|o_{t-k}}. a_{t|o_{t-k}}.\\]\nHere, \\(w_{t|o_{t-k}}\\) is an adaptive weighting scalar that assigns greater importance to past predictions that are more similar to the current prediction \\(a_t|o_t\\):\n\\[w_{t|o_{t-k}} = exp(\\alpha < a_{t|O_t}, a_{t|O_{t-k}} >),\\]\nwhere \\(< , >\\) calculates the cosine similarity between two actions, and \\(\\alpha\\) is a hyperparameter set to 0.1 in practice.\nOur empirical results show that this adaptive action ensemble strategy effectively boosts the success rate of task executions while adding minimal extra cost to inference, since past predictions can be readily cached."}, {"title": "4. Experiment", "content": "Training Dataset. We use the Open X-Embodiment (OXE) [48] dataset as our primary training dataset. It includes over 1 million real-world robotic trajectories collected from 60 datasets", "settings": "Visual Matching", "tasks": 1}, {"tasks": 1, "Pick": "Stack", "Place": "n\u2022 Pick up the Object and place it onto the Color plate, where Object \u2208 {Banana, Lemon, Avocado},"}, {"title": "5. Conclusion", "content": "We have presented a large VLA model designed with a specialized focus on action modeling. Unlike previous VLAs that employ simple adaptations of vision-language models for action prediction, CogACT separates cognitive and action capabilities, using advanced diffusion transformers as a dedicated action module. This approach effectively addresses the continuous, multimodal, and temporally correlated nature of robot actions, leading to substantial improvements in performance and generalization. Our findings highlight the advantages of a componentized VLA model, where a large VLM serves as the cognitive foundation while the DiT action module handles precise, sequential action prediction. Favorable scaling behaviors of the action module have been observed, where modest parameter increases yield significant performance gains. The extensive experiments show that our model not only significantly surpasses existing VLAs in task performance and but also exhibits remarkable generation capability to unseen objects and backgrounds."}]}