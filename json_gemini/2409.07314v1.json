{"title": "MEDIC: TOWARDS A COMPREHENSIVE FRAMEWORK FOR EVALUATING LLMS IN CLINICAL APPLICATIONS", "authors": ["Praveen K Kanithi", "Cl\u00e9ment Christophe", "Marco AF Pimentel", "Tathagata Raha", "Nada Saadi", "Hamza Javed", "Svetlana Maslenkova", "Nasir Hayat", "Ronnie Rajan", "Shadab Khan"], "abstract": "The rapid development of Large Language Models (LLMs) for healthcare applica- tions has spurred calls for holistic evaluation beyond frequently-cited benchmarks like USMLE, to better reflect real-world performance. While real-world assess- ments are valuable indicators of utility, they often lag behind the pace of LLM evolution, likely rendering findings obsolete upon deployment. This temporal dis- connect necessitates a comprehensive upfront evaluation that can guide model selection for specific clinical applications. We introduce MEDIC, a framework assessing LLMs across five critical dimensions of clinical competence: medical reasoning, ethics and bias, data and language understanding, in-context learning, and clinical safety. MEDIC features a novel cross-examination framework quan- tifying LLM performance across areas like coverage and hallucination detection, without requiring reference outputs. We apply MEDIC to evaluate LLMs on med- ical question-answering, safety, summarization, note generation, and other tasks. Our results show performance disparities across model sizes, baseline vs medi- cally finetuned models, and have implications on model selection for applications requiring specific model strengths, such as low hallucination or lower cost of in- ference. MEDIC's multifaceted evaluation reveals these performance trade-offs, bridging the gap between theoretical capabilities and practical implementation in healthcare settings, ensuring that the most promising models are identified and adapted for diverse healthcare applications.", "sections": [{"title": "1 INTRODUCTION", "content": "The advent of advanced large language models (LLMs), both general-purpose (ChatGPT (Achiam et al., 2023), Gemini (Gemini et al., 2023), Claude (Anthropic, 2024), Llama3 (Dubey et al., 2024)) and the ones optimized for medical use (NYUTron (Jiang et al., 2023), Med-PaLM 2 (Singhal et al., 2023b), Meditron (Chen et al., 2023), Med42 (Christophe et al., 2024a;b)), have demonstrated im- pressive performance in medical knowledge tasks and shown significant promise in healthcare appli- cations. Generative AI, distinct from predictive AI, creates novel content across various modalities, including audio, visual, and textual formats (Nah et al., 2023). LLMs, a subset of generative AI, spe- cialize in producing coherent and structured text responses to textual inputs or instructions, offering wide-ranging applications within healthcare system operations (Thirunavukarasu et al., 2023).\nThe integration of LLMs into healthcare has begun to transform various aspects of clinical practice, from assisting with medical documentation to supporting diagnostic processes and treatment plan- ning (Gottlieb & Silvis, 2023; Clusmann et al., 2023). As these AI systems demonstrate increasingly sophisticated capabilities in medical contexts, there is a growing need for comprehensive, automated evaluation approaches that can keep pace with rapid LLM development and technological advance- ments. While human evaluations remain crucial and provide valuable insights into real-world per- formance, they are often costly, time-consuming, and may lag behind the pace of LLM development, potentially rendering findings obsolete upon deployment. This temporal disconnect necessitates a comprehensive, efficient evaluation framework that can guide model selection for specific clinical applications and serve as an initial testing suite."}, {"title": "2 MEDIC EVALUATION FRAMEWORK", "content": "The integration of language models into healthcare applications presents both immense potential and significant challenges. To rigorously evaluate these models for clinical use, we propose MEDIC (Medical reasoning, Ethical and bias concerns, Data and language understanding, In-context learn- ing, and Clinical safety and risk assessment), a novel framework designed to assess LLMs across multiple dimensions critical to healthcare (Figure 1). MEDIC aims to provide a more holistic as- sessment that transcends traditional benchmarks (like answering medical exam questions), which, while valuable, often fall short in predicting real-world performance across diverse clinical scenar- ios. By offering a comprehensive \"unit-testing\" approach, MEDIC attempts to enable stakeholders to conduct initial, in-depth evaluations of LLMs for specific clinical applications, complementing and informing subsequent human-led assessments. This framework bridges the divide between various stakeholders care providers, patients, clinical researchers, scribes and the practical application of LLMs in clinical settings, addressing the complex, high-stakes nature of medical practice that demands evaluation beyond mere accuracy and linguistic fluency.\nMEDIC encompasses five interconnected dimensions that collectively address the multifaceted na- ture of healthcare AI evaluation.\nMedical reasoning: This vertex focuses on the LLM's ability to engage in clinical decision- making processes. It encompasses the model's capacity to interpret medical data, formulate dif- ferential diagnoses (McDuff et al., 2023), recommend appropriate tests or treatments (Sandmann et al., 2024), and provide evidence-based justifications for its conclusions (Kresevic et al., 2024). It also evaluates the LLM's understanding of medical concepts, its ability to apply clinical guidelines, and its capacity to integrate complex medical information from various sources and modalities to ar- rive at sound clinical judgements and/or recommendations (Han et al., 2024b; Kanjee et al., 2023b; Levine et al., 2024).\nEthical and bias concerns: This aspect addresses the critical issues of fairness, equity, and ethical considerations in healthcare AI. It examines the LLM's performance across diverse patient popula- tions, assessing for potential biases related to race, gender, age, socioeconomic status, or other demographic factors (Zack et al., 2024). This category also considers the model's ability to handle sensitive medical information, respect patient privacy, and adhere to medical ethics principles (Ong et al., 2024; Haltaufderheide & Ranisch, 2024). Additionally, it considers the LLM's transparency and explainability in its decision-making processes.\nData and language understanding: This vertex evaluates the LLM's proficiency in interpreting and processing various types of medical data and language. It includes the model's ability to com- prehend medical terminologies (Soroush et al., 2024), understand clinical jargon, interpret clinical notes, laboratory test reports, and imaging results Li et al. (2024). This category assesses the LLM's capacity to handle structure and unstructured medical data, recognize patterns in health records, and accurately extract relevant information from diverse medical sources (Veen et al., 2024).\nIn-context learning: This component examines the model's adaptability and ability to learn and apply new information within the context of a given clinical scenario. It evaluates how well the model can incorporate new guidelines (Ferber et al., 2024; Luo et al., 2024), recent research findings (via RAG, Zakka et al. (2024)), or patient-specific information (Hager et al., 2024) into its reasoning process. This also involves the assessment of the LLM's capacity to recognize the limits of its knowledge and appropriately seek additional information when needed.\nClinical safety and risk assessment: This final vertex focuses on the LLM's ability to priori- tize patient safety and manage potential risks in clinical settings (Lee et al., 2023). It evaluates the model's capacity to identify and flag potential medical errors, drug interactions, or contraindications (Pais et al., 2024). This category also assesses the LLM's ability to provide appropriate cautionary advice, recognize emergency situations, and in its ability to decline responding to attempts to gen- erate medical misinformation (Menz et al., 2024). Additionally, it examines the model's reliability and consistency in providing safe recommendations across various clinical scenarios."}, {"title": "3 EVALUATION TASKS", "content": "Building upon the five dimensions discussed previously, the MEDIC framework incorporates a di- verse set of evaluation tasks designed to assess LLMs across various clinical competencies and scenarios (Table 1). These tasks are selected to provide a well-rounded assessment of the model's capabilities on the five dimensions. The evaluation methodology includes closed-ended questions, which test the LLM's ability to provide specific, accurate answers to clinical queries; open-ended questions, which assess the model's capacity for medical reasoning and explanations; summarization tasks, which evaluate the LLM's ability to distill complex medical information into concise, relevant summaries; and note creation exercises, which test the model's abilities in generating coherent and accurate clinical documentation.\nThe evaluation tasks in MEDIC intersect with multiple dimensions of the framework, depending on the specific datasets and contexts in which they are applied. A summarization task, for example, primarily assesses in-context learning while also engaging data understanding and medical reason- ing capabilities, dependent on the content. Open-ended questions can evaluate medical reasoning, data comprehension, and the model's ability to navigate ethical and clinical safety concerns. This multifaceted approach ensures a comprehensive assessment of LLM capabilities across various clin- ical scenarios. Table 1 summarizes the relationships between these evaluation tasks and the MEDIC dimensions.\nThis multifaceted nature of the tasks reflects the interconnections that exist in clinical practice and allows for a more balanced evaluation of LLMs across the MEDIC dimensions. With this approach, researchers can design an evaluation process for specific applications, while still assessing the mod- els in a comprehensive manner."}, {"title": "3.1 EVALUATING MEDICAL KNOWLEDGE THROUGH CLOSED-ENDED QUESTIONS", "content": "Closed-ended question evaluation for LLMs provides insights into their medical knowledge breadth and accuracy. With this approach, we aim to quantify an LLM's comprehension of medical con- cepts across various specialties, ranging from basic to advanced professional levels. It assesses the model's ability to arrive at appropriate answers in specific clinical contexts and tests its capacity to understand and reason with biomedical literature. These assessments serve as standardized bench- marks, enabling direct comparison of LLM performance.\nTo achieve these evaluation objectives, we use the following selected datasets:\n\u2022 MedQA: A dataset containing multiple-choice questions similar to those on the United States Medical Licensing Examination (USMLE), covering a wide range of medical topics across various specialties (Jin et al., 2020). We have also included two sets of official USMLE's practice materials (Nori et al., 2023a; Han et al., 2023).\n\u2022 MMLU: This dataset includes only the medical-related subsets from the broader MMLU\u00b9 benchmark (Hendrycks et al., 2021). The subsets included (clinical knowledge, college biology, college medicine, medical genetics, professional medicine and anatomy) range in difficulty from elementary to advanced professional levels, testing both medical lan- guage understanding abilities. We also included the same subsets of the more challenging MMLU-Pro dataset (Wang et al., 2024), which integrates more difficult, reasoning-focused questions.\n\u2022 MedMCQA: A large-scale, multiple-choice question answering dataset specifically de- signed for medical entrance exams, covering a wide range of medical topics (Pal et al., 2022).\n\u2022 PubMedQA: Derived from PubMed abstracts, this dataset tests the model's ability to com- prehend and answer questions based on biomedical literature Jin et al. (2019).\n\u2022 ToxiGen: This dataset assesses the model's ability to avoid harmful content, which is important for patient safety in healthcare (Hartvigsen et al., 2022).\nWe benchmarked the LLMs across these datasets, using the Eleuther AI's Evaluation Harness frame- work (Gao et al., 2023a), which focuses on the likelihood of a model generating each proposed answer rather than directly evaluating the generated text itself. We modified the framework's code- base to provide more detailed and relevant results. Rather than just calculating the probability of generating answer choice labels (e.g., a, b, c, or d), we calculate the probability of generating the full answer text. This modification provides a more detailed understanding of the model's perfor- mance by taking into account the entire answer generation process, including the ability to articulate reasoning and justify the selected answer choice."}, {"title": "3.2 EVALUATING OPEN-ENDED RESPONSES", "content": "While closed-ended questions are useful for evaluating specific medical knowledge, they do not fully capture the reasoning and explanatory skills needed in real world scenarios. A more complete assessment of LLMs can be achieved by complimenting closed-ended questions with open-ended questions (Hager et al., 2024). This evaluation method goes beyond providing options to the LLM and is therefore fit for evaluating the model's ability to identify key concepts, synthesize information, and generate appropriate responses. Open-ended evaluation is also fit for assessing model's ability to not provide answers to the questions when such responses can negatively impact patients.\nOur open-ended evaluation includes clinical Q&A to assess domain knowledge across medical spe- cialties, and a benchmark to evaluate ethical considerations and safety."}, {"title": "3.2.1 CLINICAL QUESTION-ANSWERING IN OPEN-ENDED SETTING", "content": "We use three open-source medical question datasets (MedicationQA, HealthSearchQA, and Ex- pertQA) to evaluate LLMs' clinical question-answering abilities across various medical topics:"}, {"title": "3.2.2 GENERATING SAFE RESPONSES", "content": "We assess the safety and ability to comply with ethics in medical considerations, we utilize the med-safety benchmark (Han et al., 2024a). This benchmark consists of a dataset comprising 900 distinct scenarios, each presenting a harmful medical request. These scenarios are designed to cover all nine principles of medical ethics as defined by the American Medical Association (AMA, 2001), providing an evaluation of an LLM's ability to navigate complex ethical situations in healthcare. This benchmark, while effective for assessing behaviour of physicians towards the patients, has a few limitations which we discuss in the section 5.\nFor evaluation, we employ an LLM-as-a-judge approach. However, in this case, each response from the evaluated model is assigned a harmfulness score on a predefined scale, where a low score indicates full refusal of the harmful request, and a high score signifies complete compliance (see Appendix C.1.3 for further details about the score rubric used)."}, {"title": "3.3 EVALUATING THE SUMMARIZATION CAPABILITIES", "content": "Clinical summarization is a crucial task in healthcare that involves condensing and re-synthesizing complex medical information into concise, accurate, and actionable summaries. Evaluating lan- guage models on this task provides insights into their ability to process large volumes of medical data and support healthcare providers in interpreting patient information efficiently.\nWe utilize two benchmark datasets to evaluate the clinical summarization capabilities of language models:\n\u2022 Clinical Trial is a dataset compiled from ClinicalTrials.gov, a publicly accessible registry of clinical trial protocols from around the world. This registry offers detailed descriptions of study designs, recruitment statuses, and eligibility criteria, making it a valuable resource for medical and natural language processing research (Roberts et al., 2022). For our anal- ysis, we randomly sampled from all publicly available protocols as of June 2024. We then applied pre-processing steps, including the requirement that each protocol's description section was sufficiently detailed (between 3000 to 8000 word-level tokens long), resulting in a final dataset of 1629 clinical trial protocols. The primary task with this dataset is to generate concise and accurate summaries from the study descriptions.\n\u2022 Problem Summarization is a dataset that was originally generated by attending internal medicine physicians during the course of routine clinical practice (Gao et al., 2022; 2023b). In this task, the goal is to generate a \"problem list\", or condensed list of diagnoses and medical problems using the provider's progress notes during hospitalization. The dataset includes human-generated (reference) summaries in the form of problem lists.\nVarious metrics have been proposed and developed to evaluate the quality of text summarization tasks. Traditional evaluation metrics like ROUGE, BLEU and BERTScore offer quantitative assess- ments of lexical and semantic similarity between generated and reference summaries. However, these methods have well documented limitations in capturing the full range of acceptable summa- rizations (Akter et al., 2022; Fabbri et al., 2021). To address these limitations and provide a more comprehensive evaluation approach, which crucially does not require human-annotated reference summaries, we introduce a novel \"Cross-Examination\" framework. Depicted in Figure 2, this ap- proach assesses text generation tasks, including summarization, in three key steps. First, through the generation of close-ended question-answer pairs from both the original document and (generated) summary. To better ground the question-answer pairs in facts from the respective sources, the gen- erated questions are constrained to have \"YES\" only answers. Second, a 'cross-examining' step is performed in which the document/summary derived questions are posed to the summary/document texts with answers predicted from the set \"YES\", \"NO\", \"IDK\" for each question. That is, by pre- dicting answers to questions derived from the document based only on the content of the summary, and vice versa. Third, the predicted answers from the cross-examination step are compared with the ground truth-answers associated with the questions, and from this four key scores are calculated: Consistency, Coverage, Conformity, and Conciseness. We formally define the scores below.\n\u2022 Coverage score: this score measures how comprehensively the summary covers the con- tent of the original document. It is calculated as 100 X, where X is the percentage of document generated questions that receive an \"IDK\" (I Don't Know) response based on the summary. A higher coverage score indicates that the summary captures more of the original details and is less generic.\n\u2022 Conformity score: also known as the non-contradiction score, this metric evaluates whether the summary avoids contradicting the document. It is derived by identifying the percentage of questions for which the summary's answer is \"NO\" and the document's is \"YES\", or vice versa, and computing 100 X. A higher conformity score signifies a greater alignment between the summary and the document.\n\u2022 Consistency score: this score, which measures the level of non-hallucination, is based on the accuracy of factual information in the summary as compared to the document. It is calculated as 100 \u2013 X, where X is the percentage of summary derived questions that are answered with an \"IDK\" based on the document, indicating factual discrepancies. A higher consistency score suggests that the summary is more factual and contains fewer inaccuracies or fabrications.\n\u2022 Conciseness score: reflecting the summary's briefness, this score is computed by the re- duction in word-level token count from the original document to the summary. A higher conciseness score indicates a more brief summary, efficiently capturing the essence of the original content without redundancy.\nIn order to ensure a fair comparison between the different models used for the text and questions gen- eration, we make use of basic prompt engineering. The prompts used for generating the summary, generating the questions from the document and the summary and the prompt for cross examining is provided in Appendix C.2. Whenever possible, we utilize ground-truth or reference summaries and compute traditional metrics for comparative purposes.\nBy employing this model- and data-agnostic framework alongside traditional metrics, we aim to offer a more nuanced and thorough evaluation of LLMs' clinical summarization capabilities, better reflecting their potential to enhance workflow efficiency and improve information transfer in health- care settings."}, {"title": "3.4 EVALUATING STRUCTURED RESPONSES", "content": "Clinical note generation is also a critical task in healthcare that involves synthesizing information from either doctor-patient interactions, or a physician's dictation of a case, into concise and struc- tured documentation. The goal of this task is to summarize these inputs into SOAP (Subjective, Ob- jective, Assessment, and Plan) note sections, or simply an \"assessment and plan\" paragraph, which outlines the physician's evaluation of the patient's condition and the proposed course of treatment. This process is essential for maintaining accurate medical records, ensuring continuity of care, and facilitating communication among healthcare providers."}, {"title": "4 RESULTS", "content": "4.1 CLOSED-ENDED EVALUATION - LARGER MODELS PERFORM BETTER\nOur evaluation of language models on closed-ended questions encompasses a range of medical knowledge benchmarks, designed to assess models' understanding of clinical concepts, diagnostic reasoning, and medical decision-making. These benchmarks include standardized medical exams, clinical vignettes, and specialized medical knowledge tests. The results provide insights into the performance of various competitive models across different closed-ended question formats, offering a comprehensive view of their capabilities in structured medical knowledge assessment as part of the MEDIC framework.\nTable 2 presents the performance of various clinical LLMs across multiple medical benchmarks for close-ended question answering. The models are categorized based on their (active) parameter sizes and evaluated on a range of MCQ benchmark tasks, including MMLU and MMLU-Pro for data understanding, MedMCQA, MedQA, and USMLE for medical reasoning, PubMedQA for in- context learning, and ToxiGen for ethics and safety. These benchmarks target various dimensions of the MEDIC framework and are designed to assess different facets of LLMs' close-ended Q&A performance.\nOverall, the results consistently show that larger models outperform smaller ones in these tasks, aligning with general trends in language model scaling. However, the performance increase is less pronounced on the safety-specific benchmark dataset (ToxiGen) as shown in Figure 3. Additionally, we observe that domain-specific instructed models, such as Med42-Llama3.1-70b and OpenBioLLM-70b, show improved performance on these benchmarks compared to their general counterparts. This demonstrates the benefits of explicit medical instruction and alignment in en- hancing the models' clinical knowledge base and reasoning capabilities. It is also important to note that these results reflect zero-shot performance. Previous studies have shown that prompting strate- gies, such as Medprompt (Nori et al., 2023b), or integration with search capabilities can achieve even higher performances, with Med-Gemini, for example, reaching a 91.2% accuracy on bench- marks such as MedQA (Saab et al., 2024).\nInterestingly, we observe near-perfect results on certain benchmarks (e.g., USMLE), particularly those focusing on foundational medical knowledge. While impressive, these scores indicate a de- gree of saturation in these specific evaluation frameworks. This observation shows the need for more comprehensive and challenging evaluation methods to continue pushing the boundaries of medical AI capabilities and to better differentiate between top-performing models. Moreover, we note that using MEDIC we can easily identify critical gaps in the current evaluation landscape for LLMs, particularly in the context of ethics, bias concerns, and clinical safety. While closed-ended Q&A benchmarks are prevalent, those specifically designed to assess these vital aspects are exceedingly rare. By highlighting this deficiency, our framework underscores the need for more targeted bench- marks that can rigorously evaluate and address ethical and safety concerns, thereby contributing to the responsible development and deployment of LLMs in healthcare."}, {"title": "4.2 OPEN-ENDED EVALUATION - LARGER MODELS DO NOT ALWAYS PERFORM BETTER", "content": "In this section, we present the results of our evaluation of open-ended questions, conducted using our developed framework to systematically assess the quality of responses generated by LLMs. To ensure consistency and comparability, we employed a standardized approach across all models. Each model was presented with open-ended questions and prompted with a simple instruction: \"Answer the question truthfully\". This approach deliberately avoided more complex prompting techniques, allowing us to evaluate the inherent capabilities of each model in generating accurate responses.\nFor the direct assessment and pairwise comparison of models' responses, we utilized Prometheus- 2-8x7b (Kim et al., 2024) as the Judge. This model has been specifically trained to evaluate other models' responses according to predefined scoring rubrics, ensuring a consistent assessment process. In Appendix D, we further analyze the selection of judges, presenting an agreement analysis that demonstrates a high level of concordance among the (model) judges used in our evaluations. This consistency underscores the reliability of the framework in assessing the quality of open-ended responses.\nFirst, we evaluate each model's response independently by analyzing the judge-assigned scores across multiple quality axes, and then we present pairwise comparisons between models' responses using win-rates and Elo scores.\nAbsolute scoring Figure 4 illustrates the performance of each model's generated responses Med42-Llama3.1-70b (Christophe et al., 2024b), Llama3.1-Instruct (70 and 405 billion parameters versions (AI@Meta, 2024)), GPT-40 (Achiam et al., 2023), OpenBioLLM-70b (Ankit Pal, 2024) and Mistral-Large-Instruct-2407 on judge-assigned scores across multiple quality assessment axes. It provides an aggregated overview of their performance in key categories such as Relevance and Completeness, Clarity and Communication, and Safety and Ethical considerations, as well as a detailed radar plot that highlights specific strengths and weaknesses.\nThese results highlight that while all models generally perform well, certain distinctions can be ob- served. Notably, the performance of all models in the category of safety and ethical considerations is comparatively lower than in the other two evaluation categories (Figure 4 top). Specifically, the areas of Risk Mitigation, Language Clarity, Patient Safety, and Comprehensive Coverage show the most variation among the responses' quality. The high scores in areas like Confidentiality, Bias, Contextual Relevance, and Professional Tone suggest that, in general, models are well-aligned with clinical communication standards. For example, Med42-Llama3.1-70b, fine-tuned from Llama3.1- 70b-Instruct, excels in Risk Mitigation and practical application, although it may occasionally com- promise on Language Clarity and Comprehensive Coverage.\nIt is interesting to observe that GPT-40, despite its strong performance on numerous benchmarks, falls short in key areas like Risk Mitigation and Comprehensive Coverage. This highlights the potential need for domain-specific fine-tuning/alignment, and rigorous evaluation, particularly in sensitive fields like healthcare, where subtle understanding of the subject and risk assessment are paramount.\nIn our evaluation framework, we employ nine assessment axes, three of which are particularly focused on clinically relevant criteria, while the others emphasize linguistic attributes. To assess the reliability of the model-judge in scoring LLM-generated responses according to our predefined rubrics, we conducted a small, focused study involving direct clinician input. This study involved a detailed evaluation of a subset (50) of the questions, each of which was scored independently by"}, {"title": "4.3 MEDICAL SAFETY EVALUATION - PREFERENCE ALIGNMENT IS KEY", "content": "In this section, we present the findings of our evaluation of LLMs with respect to medical safety using Med-Safety benchmark. Our analysis focuses on the model's responses to potentially harm- ful requests, utilizing a dataset of 900 questions spanning 9 categories that violate medical safety principles, as described in Section 3.2.2.\nWe evaluated a range of LLMs, including both general-purpose models and those fine-tuned specif- ically for medical applications, as illustrated in Figure 7. To ensure a fair comparison, all models were prompted with a simple system instruction: \"Answer the question truthfully\". The full prompt template is given in Appendix C.1.3. By not allowing complex prompting techniques, we were able to evaluate inherent capabilities of each model.\nOur evaluation methodology centered on a harmfulness score, which quantifies an LLM's willing- ness to comply with harmful requests. We employed Meta-Llama3-70b-Instruct as an independent judge to rate the responses generated by the other LLMs. The scoring scale ranges from 1 to 5, where a score of 1 indicates that the model fully refused to comply with the harmful request, while a score of 5 represents full compliance. It's worth noting that declining to answer or expressing unwillingness to respond is considered the least likely to cause harm in this context. The detailed scoring rubric and evaluation template used in this study are provided in Appendix C.1.3.\nIt is important to note that some of these models were not aligned for safety preferences. Based on our scoring rubric, we have delineated a \"safety threshold\" in Figure 7, below which model responses are considered safe and ethical. Within this zone, models exhibit one of two desirable behaviors: (1) they proactively decline to act on the user's harmful instruction (score 1), or (2) they opt for a restrained approach without directly addressing or endorsing the user's inappropriate request (score 2). These responses indicate that the model has successfully recognized the potential harm in the prompt and chosen to prioritize safety over blind compliance. Any score above these two would indicate some level of compliance with the request. The majority of clinical LLMs evaluated fall within the safety threshold we have defined based on this rubric. This may be an indication that the pre-training data for the underlying base-models may inherently induce safety behaviour.\nDespite the low harmfulness scores present in Figure 7, our findings align with the analysis presented by Han et al. (2024a), demonstrating that preference-aligned models such as Med42-Llama3.1-70b (Christophe et al., 2024b), Mixtral-Instruct (Jiang et al., 2024), Llama3-Instruct (AI@Meta, 2024), and OpenBioLLM (Ankit Pal, 2024) and others consistently achieve lower harmfulness scores com- pared to other fine-tuned models like Med42 (Christophe et al., 2024a), ClinicalCamel (Toma et al., 2023), BioMistral (Labrak et al., 2024), and JSL-8b-v2, etc. This highlights the necessity of imple- menting explicit safety measures and alignment techniques in the development of clinical LLMs.\nIt is important to recognize the limitations of various scenarios present in the med-safety benchmark, which are primarily focused on the ethical behaviour of physicians towards patients. This physician- centric view does not fully capture the diverse ways in which LLMs might be used by different stakeholders (or end-users), including care providers, and patients. As such, future iterations of the"}, {"title": "4.4 CROSS-EXAMINATION FRAMEWORK", "content": "To assess the summaries and clinical notes generated by various language models, we used the cross- examination framework detailed in Section 3.3 (and Figure 2). Within this evaluation framework, we used Llama3.1-70b-Instruct to generate questions and answers based on both the original documents and the generated texts. We set the number of questions N to 10, with a temperature of 0. These questions and answers were then utilized to compute coverage, conformity, and consistency scores. The conciseness score was calculated as the reduction in word-level token count (Bird et al., 2009) from the original document to the generated text. More details with respect to specific prompts used within the framework can be found in Appendix C.2. Additionally, we also reported conventional scores like BLEU and ROUGE, whenever ground truth reports are available."}, {"title": "4.4.1 CLINICAL TEXT SUMMARIZATION", "content": "In this section, we discuss clinical text summarization capabilities of different LLMs on the clinical trial and problem summarization datasets. Figure 8 shows some of the top performing LLMs evalu- ated using our framework (for more and detailed results, refer to Table 9 and Table 10 in Appendix B.2.1).\nOverall, the models evaluated in this study exhibit competitive performance relative to one another, with particularly very high values and low variation observed in the conformity and consistency metrics across all models. This indicates that the generated summaries are generally free from hallucinations and remain consistent with the input text. Conversely, a strong correlation is noted between the conciseness and coverage metrics. Models that produce longer summaries tend to"}, {"title": "4.4.2 CLINICAL NOTE GENERATION", "content": "We evaluated the capabilities of different models to generate structured medical outputs in the form of clinical or SOAP notes. Figure 11 shows some of the top performing LLMs evaluated using the cross-examination method (more detailed results are available in Table 7 for the ACI bench dataset and in Table 8 for the SOAP notes dataset in Appendix B.2.1).\nAt a high level, the models evaluated for clinical note generation tasks show trends similar to those observed in the summarization tasks. However, we note lower consistency scores for the SOAP Note dataset and higher coverage scores across both datasets compared to the summarization tasks. These differences can be attributed to the nature of clinical note generation, which, unlike summarization, involves structuring content from patient-doctor dialogues. This process helps to ensure that most"}, {"title": "5 DISCUSSION & CONCLUSION", "content": "The integration of LLMs into healthcare presents a complex landscape of opportunities and chal- lenges. While these models show potential to transform various aspects of clinical practice, their evaluation in such a critical domain necessitates a thorough and multifaceted approach. This need is particularly pressing given the rapid evolution of LLMs and the limitations of existing bench- marks in reflecting real-world performance. Traditional MCQ benchmarks like MedQA or USMLE, while valuable, often fall short in capturing the full spectrum of competencies required for safe and effective clinical application. Moreover, real-world assessments, though crucial, frequently lag be- hind the pace of LLM development, potentially rendering findings obsolete upon deployment. This temporal disconnect highlights the need for a comprehensive upfront evaluation framework that can guide model selection for specific clinical applications.\nThis paper introduces the MEDIC framework as a response to this need. MEDIC encompasses five key dimensions: Medical reasoning, Ethical and bias concerns, Data and language understanding, In-context learning, and Clinical safety and risk assessment. This comprehensive structure acknowl- edges the diverse facets of clinical competence and the varied requirements of healthcare applica- tions. By addressing these critical dimensions, MEDIC aims to bridge the gap between benchmark performance and real-world clinical utility, providing a more robust prediction of an LLM's potential effectiveness and safety in actual healthcare settings.\nOne of MEDIC's strengths lies in its modular design, allowing for customization to specific med- ical domains and beyond. This flexibility enables researchers to select relevant evaluation tasks and datasets, and facilitates the incorporation of new elements as the field evolves. By assessing LLM performance across a wide range of clinical competencies, MEDIC provides a more holistic assessment than traditional metrics alone.\nOur comprehensive evaluation using MEDIC has yielded several important insights into the perfor- mance of LLMs in healthcare applications. In closed-ended Q&A evaluations, we observed that fine-tuned models and larger models consistently outperformed others. However, this trend did not hold true for open-ended clinical Q&A tasks, where models were required to generate free-form responses. Interestingly, some domain-specific fine-tuned models performed worse than their base"}, {"title": "A RELATED WORK", "content": "LLM evaluation frameworks have rapidly evolved to address the growing capabilities and applica- tions of these models in various domains. Notable frameworks include HELM (Holistic Evaluation of Language Models) (Liang et al., 2022), which assesses models across multiple dimensions such as accuracy, calibration, robustness, fairness, and efficiency. EleutherAI's Language Model Evaluation Harness (Gao et al., 2023a) offers an open-source approach to evaluating models on a wide range of natural language processing tasks. However, the choice of evaluation framework can significantly impact reported performance metrics. For instance, Pimentel et al. (2024) demonstrated that per- formance variations of up to 26% can occur when evaluating the same LLMs on identical datasets but using different evaluation frameworks. BIG-bench (Beyond the Imitation Game Benchmark) presents a diverse set of tasks designed to probe the capabilities of language models beyond tradi- tional benchmarks, including novel tasks that test reasoning, multilingual abilities, and multi-step problem-solving (BIG-Bench, 2023).\nThe evaluation of LLMs in clinical settings is focused on medical knowledge benchmarks such as MedQA (Jin et al., 2020) and the medical subset of the Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021) dataset assess models' ability to handle complex medical queries across various specialties. These benchmarks often include questions from medical licensing exams, providing a standardized measure of medical knowledge. Clinical LLMs, trained on extensive med"}]}