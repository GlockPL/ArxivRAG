{"title": "Internal Activation Revision: Safeguarding Vision Language Models Without Parameter Update", "authors": ["Qing Li", "Jiahui Geng", "Zongxiong Chen", "Kun Song", "Lei Ma", "Fakhri Karray", "Mohamed bin Zayed University of Artificial Intelligence", "Fraunhofer FOKUS", "The University of Tokyo", "University of Alberta"], "abstract": "Warning: This paper contains offensive content that may disturb some readers. Vision-language models (VLMs) demonstrate strong multimodal capabilities but have been found to be more susceptible to generating harmful content compared to their backbone large language models (LLMs). Our investigation reveals that the integration of images significantly shifts the model's internal activations during the forward pass, diverging from those triggered by textual input. Moreover, the safety alignments of LLMs embedded within VLMs are not sufficiently robust to handle the activations discrepancies, making the models vulnerable to even the simplest jailbreaking attacks. To address this issue, we propose an internal activation revision approach that efficiently revises activations during generation, steering the model toward safer outputs. Our framework incorporates revisions at both the layer and head levels, offering control over the model's generation at varying levels of granularity. In addition, we explore three strategies for constructing positive and negative samples and two approaches for extracting revision vectors, resulting in different variants of our method. Comprehensive experiments demonstrate that the internal activation revision method significantly improves the safety of widely used VLMs, reducing attack success rates by an average of 48.94%, 34.34%, 43.92%, and 52.98% on SafeBench, Safe-Unsafe, Unsafe, and MM-SafetyBench, respectively, while minimally impacting model helpfulness.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have been further enhanced by adopting visual instruction tuning to develop vision language models (VLMs), enabling more accurate and contextually relevant responses across multimodal tasks. However, recent studies show that VLMs are more vulnerable than LLMs, with their safety alignments more easily bypassed, leading them to easily follow malicious instructions. Furthermore, Liu et al. illustrates that VLMs are prone to producing harmful content when prompted with a contextually relevant image.\nCurrently, some efforts in safety alignment research have made strides to ensure these models adhere to human ethical standards. The earlier work, AdaShield employs adaptive shield prompting to enhance the robustness of MLLMs, focusing specifically on structure-based jailbreak attacks. Subsequently, Pi et al. introduces MLLM-Protector, which addresses safety challenges by integrating a harm detector to identify potentially harmful outputs and a detoxifier to modify them. However, both components require training, and if the output is harmful, the detoxifier introduces additional computational overhead to rewrite the response. Recently, Zong et al. introduces VLGuard, a dataset specifically designed for the safety fine-tuning of VLMs. Also, Zhang et al. creates the SPA-VL dataset, which combines textual and visual data to enhance safety performance through RLHF. However, these efforts rely on extensive training data to update the models, requiring significant human labor to obtain high-quality annotated data. When new attack methods are introduced, further model adjustments and data collection are necessary. Therefore, there is an urgent need for more efficient and flexible safety measures.\nTo investigate this area, we examine the vulnerabilities of VLMs by analyzing the differences in internal activations between textual and textual-visual inputs. Visualizations with t-SNE reveal significant differences in activation distributions between unimodal and multimodal inputs. We also train a classifier on textual datasets to distinguish between safe and unsafe instructions. However, a notable performance decline of about 35% can be observed on multimodal instructions. These observations indicate that the safety alignments in VLMs are not robust to handle activation discrepancies, potentially leading to model fragility.\nWe further propose an activation revision framework, internal activation revision, that enhances model safety by shifting the model's activations using revision vectors extracted from positive and negative samples. We develop two revision schemes, coarse-grained layer-level, and fine-grained head-level revisions, and evaluate three different methods of constructing positive and negative samples: Multi-Instruction, Text-Response, and Multi-Response. Additionally, we test two vector extraction methods: probe weight direction (PWD) and mass mean shift (MMS). Head-level revision with Multi-Response samples and MMS"}, {"title": "Related Work", "content": "Jailbreak Multimodal Large Language Models Studies have demonstrated visual instruction tuning may escalate the likelihood of LLMs responding to harmful commands. Furthermore, VLMS are particularly vulnerable to images that are related to the text queries. Gong et al. proposed FigStep, which converts textual instructions into embedded text within images, prompting VLMs to execute tasks depicted in those images, which significantly amplifies the vulnerability of VLMs. Despite growing observations into the potential for images to circumvent AI alignment protocols by embedding malicious content, a significant gap persists in the mechanisms driving these vulnerabilities.\nLLMs' Internal State Analysis and Intervention Several approaches have been proposed to analyze the model's internal states for a deeper understanding of its behavior. Probing is a standard tool for identifying a network's internal representations, involving training a simple classifier (probe) on intermediate activations to predict specific linguistic attributes or task-related information. Furthermore, Concept Activation Vectors (CAVs) offer a framework for interpreting deep neural networks by encoding high-level concepts within a model's layer activation spaces. This paper employs related approaches to explore why VLMs are more vulnerable by analyzing the internal states. Based on the analysis of the internal states of models, several recent works have explored steering LLMs during inference to achieve desired outputs without fine-tuning. Inference-Time Intervention (ITI) focuses on eliciting truthful answers by modifying internal activations of the model based on causal relationships identified through intervention. Activation addition involves adding a fixed vector to the activations of specific layers to influence the model's behavior. Building on this concept, researchers have applied contrastive activation addition to steer Llama-2, demonstrating improved performance on various tasks. These techniques collectively represent a growing trend in the LLMs domain aimed at enhancing model controllability and output customization without extensive training or fine-tuning."}, {"title": "Why VLMs Are More Vulnerable?", "content": "Preliminary Before explaining the methodology, we briefly introduce the architecture of decoder-only language models, which serve as the backbone for many VLMs. This architecture stacks multiple transformer layers, indexed by the variable $l$. Tokens are initially embedded into a high-dimensional space $x_0 \\in \\mathbb{R}^{D \\times H}$ where $D$ represents the embedding dimension and $H$ hidden dimension, which starts off the residual stream. The first transformer layer processes the value of $x_0$, performs computations, and produces the next vector $x_1$ in the sequence. This"}, {"title": "Activation Visualization with t-SNE", "content": "Figure 2 illustrates the distribution of activations of the last token in LLaVA-V1.5-7B across the shallow (5th), middle (15th), and final (31st) layers. The visualization result reveals a clear pattern: the activation distributions for textual and textual-visual inputs consistently exhibit significant differences across the shallow, middle, and final layers. Notably, the activations associated with TextSeta and TextSet\u00df are interwoven and significantly distant from those of MultiSet. More results are shown in the Appendix. We hypothesize that differences in internal state distributions may lead to weaker robustness to safety alignment and potentially make the model more susceptible to such attacks. Therefore, we conduct the following experiments to explore how these distribution differences in internal states affect alignment."}, {"title": "The Robustness of Safety Alignment", "content": "We utilize probes specifically engineered to distinguish between safe and unsafe instructions by analyzing individual attention heads across various layers. The internal states captured between head attention and concatenation, shown in Figure 1, form"}, {"title": "Activation Revision for Safety Enhancement", "content": "Based on the above observations, we propose an activation revision framework that efficiently revises activations during generation to enhance VLMs safety. As shown in Figure 1, our framework consists of two revision options based on the transformer layer structure. The first method revises the final activation output of a specific layer, called layer-level revision. The second, more granular method, targets the activations after head attention and before concatenation, termed head-level revision. They not only differ in computational complexity but also strike a critical balance between safety and helpfulness."}, {"title": "Layer and Head Level Revision", "content": "Layer-level revision selects a single layer $l$ for modification. Specifically, Equation (2) in the pre-revision architecture can be modified by:\n$x_{l+1} = x'_l = MLP(x_l) + x_l + \\alpha \\cdot r_{\\text{layer}}$,\nwhere $r_{\\text{layer}}$ is the revision vector at the layer $l$ to guide the model towards the safety-enhanced direction, and $\\alpha$ is the strength of intervention. A higher $\\alpha$ value corresponds to stronger perturbations.\nIn head-level revision, we only intervene on some of the heads in one layer $l$ so as to be minimally invasive. Equation (1) in the pre-revision model can be modified by:\n$x^\\prime_l = x_l + \\sum_{h=1}^{H} (Att(x_l))_{l, h} = x_l + \\sum_{h=1}^{H} ((x^\\prime)_{l, h} + \\alpha \\theta_{l, h}r_{\\text{head}}),$\nwhere $r_{\\text{head}}$ is the revision vector at the head $h$ of layer $l$, and $\\theta_{l, h} \\in \\{0, 1\\}$. If head $h$ at layer $l$ no revision, then let $\\theta_{l, h} = 0$; otherwise, $\\theta_{l, h} = 1$."}, {"title": "Revision Vectors Extraction", "content": "As illustrated in Equations (3) and (4), we need to calculate the revision vectors, denoted as $r_{\\text{layer}}$ and $r_{\\text{head}}$. A straightforward approach is to utilize the contrastive information between positive and negative samples to obtain them. Specifically, for the layer-level revision, we extract the activations at layer $l$ at the last token of the samples. This process generates a dataset $\\{((x^\\prime)_l, y)\\}_{i=1}^{N}$, where $N$ is the number of positive or negative samples. We set $y = 1$ for positive samples and $y = 0$ for negative samples to further obtain the distribution of clusters for both types, as shown in Figure 4. Mathematically, the clusters are defined as follows:\n$(A_l)^+ = \\{((x^\\prime)_l, y = 1)_i\\}_{i=1}^{N},$\n$(A_l)^- = \\{((x^\\prime)_l, y = 0)_i\\}_{i=1}^{N}.$\nSimilarly, we collect head activations at the last token to collect a dataset $\\{((x^\\prime)_{l, h}, y)_i\\}_{i=1}^{N_l}$ for layer $l$ and head $h$. The clusters are defined as follows:\n$(A_{l, h})^+ = \\{((x^\\prime)_{l, h}, y = 1)_i\\}_{i=1}^{N_l},$\n$(A_{l, h})^- = \\{((x^\\prime)_{l, h}, y = 0)_i\\}_{i=1}^{N_l}.$\nThere are two methods to determine the revision vectors based on the activation distribution: probe weight direction (PWD) and mass mean Shift (MMS). PWD is defined as the vector orthogonal to the hyperplane that separates positive"}, {"title": "Construction of contrastive samples", "content": "Our method relies on a small set of positive and negative samples to capture contrastive information and extract the revision vectors. To achieve this, we propose three approaches for constructing these samples: \u2460 Multi-Instruction, which utilizes safe and unsafe image-text instructions. We randomly sample 100 harmful instructions from the Unsafe and 100 benign instructions from the Safe-Safe of the VLGuard training dataset. \u2461 Text-Response, which focuses on safe and unsafe responses corresponding to unsafe text-only instructions. We randomly select 200 entries from the Refusal, each containing a text prompt with a decline and a response answer. \u2462 Multi-Response, which leverages safe and unsafe responses associated with multimodal unsafe instructions. We sample 200 representative harmful instructions from the VLGuard training dataset, including 100 from the Safe-Unsafe and 100 from the Unsafe. Figure 5 illustrates an example of Multi-Response."}, {"title": "Evaluation", "content": "We evaluate our proposed method from two perspectives. For safety, we measure the attack success rate (ASR) on SafeBench, MM-SafetyBench, Safe-Unsafe, and Unsafe subsets from VLGuard. We use the Perspective API to evaluate whether the responses are safe. For helpfulness, we assess the model's accuracy (ACC) in ScienceQA, a multiple-choice question-"}, {"title": "Analysis and Discussion", "content": "Defense Effectiveness\nTable 2 presents a comprehensive comparison of our framework against other advanced defense methods, including Adashield, MLLM-Protector and Fine-tuning. The evaluation covers widely used VLMs with diverse sizes and architectures, including LLaVA-V1.5-7B, LLaVA-V1.5-13B and Qwen2-VL-7B-Instruct. We have provided the results of our framework under different configurations, including various revision strategies, positive and negative sample construction methods, and vector extraction techniques. Note that the effectiveness of our method is influenced by the revision strength $\\alpha$, as well as the specific revision layers and heads utilized. Table 2 highlights the optimal results across various settings determined through hyperparameter search, with a detailed analysis provided in the following subsection. All experiments are conducted on NVIDIA A100 GPUs. More experimental setups are in the Appendix. We have the following observations:\n(1) The head-level activation revision method using Multi-Response with MMS achieves the best performance across all models. Compared to the vanilla model, all methods can noticeably enhance the safety of the model. However, our method using Multi-Response at head level with MMS performs the best and achieves the highest composite scores of 34.35, 29.98, and 30.81 on LLaVA-V1.5-7B, LLaVA-V1.5-13B, and Qwen2-VL-7B-Instruct, respectively. The ASR on SafeBench, Safe-Unsafe, Unsafe, and MM-SafetyBench decreases by an average of 48.94%, 34.34%, 43.92%, and 52.98%, respectively. Accuracy on ScienceQA and GQA decreases by only 7.72% and 1.17%, respectively. In addition, MLLM-Protector outperforms both AdaShield and Fine-tuning methods overall. Notably, on the SafeBench dataset, the MLLM-Protector achieves performance very close to our optimal method.\n(2) Head-level revision achieves a better balance between safety and helpfulness than layer-level revision. Specifically, under Multi-Response and MMS settings, head-level revision outperforms layer-level revision on LLaVA-V1.5-7B by reducing ASR by 5.40%, 2.33%, 7.14%, and 2.43% on SafeBench, Safe-Unsafe, Unsafe, and MM-SafetyBench, respectively, while improving ACC by 2.93% on Sci-"}, {"title": "Impact of Layer, Head, and Strength a", "content": "We conduct extensive experiments to explore how different layers, heads, and revision strengths \u03b1 affect the performance. To streamline the search space, we focus on seven specific layers and four different strength values. For head-level revision, we select the optimal proportion of modified heads per layer, which we empirically set at 70%. Figure 6(a) and Table 3(a) present the results of layer-level revision, whereas Figure 6(b) and Table 3(b) are the results of head-level revision on LLaVA-V1.5-7B. A detailed analysis of the choice to use 70% heads and results on other datasets is provided in the Appendix.\n(1) With the same \u03b1, deeper layers lead to a greater impact across various tasks. ASR on SafeBench of Figure 6 remains high in the initial layer (the 4th layer) and significantly drops at deeper layers (24th, 29th, and 31st). A similar pattern is observed in ScienceQA: revisions in the initial layers only decrease model accuracy by about 3%, but after the 24th layer, the accuracy is noticeably compromised.\n(2) The revision layer and strength \u03b1 balance the trade-off between safety and helpfulness. The composite score with respect to \u03b1 follows an upside-down U curve at each revision layer. The model's safety improves while helpfulness gradually diminishes. We empirically find that greater strength, \u03b1 = 2.0, at the 31st layer, causes the model to be overly defensive. Commonly, it responds with \"Sorry, I can't fulfill your request\" to even safety questions. Appendix shows multiple answers with different values of \u03b1.\n(3) Determination of optimal parameters. From Figure 6, it is evident that revising activations at the initial layer (4th), whether at the layer-level or head-level, provides only limited improvements to safety. In contrast, interventions at deeper layers (24th, 29th, and 31st) significantly compromise the model's accuracy. Therefore, revising the middle layers (9th, 14th, and 19th) emerges as a better option. This"}, {"title": "Ethical Statement", "content": "This work is dedicated to exploring why VLMs are more vulnerable than LLMs and then proposing the internal activation revision method to safeguard VLMs. We firmly"}, {"title": "Appendix", "content": "Visulization of layer activations\nWe use t-SNE to visualize the extracted layer activations $\\{((x^\\prime)_l)\\}$, as shown in Figure 7. Although we only display activations at the 0th, 4th, 9th, 14th, 19th, and 24th layers, a clear trend appears: activation values of positive and negative samples become increasingly distinguishable with more layers. Notably, at the 9th layer, the distinction between positive and negative samples is particularly clear, suggesting that safety might be a simpler or more shallow concept.\nHead-level Revision\nSelecting heads for intervention. We initiate our investigation by assessing the capacity of various heads across different layers to detect safe responses in image-text input scenarios. Utilizing probing techniques, we trained probes on all heads using the pre-constructed dataset containing safe and unsafe responses, which was randomly divided into a 4:1 ratio. Figure 8 displays the statistical results of probe training accuracy across different layers and heads, including minimum, lower quartile, median, upper quartile, and maximum values. The results indicate that for lower layers, the accuracy of probes is lower and exhibits greater fluctuation. However, starting from the sixth layer, fluctuations in accuracy decrease, and the mean remains above 90%, suggesting minimal differences among the heads in perceiving safe responses. Consequently, consistent with prior experiments, we selected some heads from the ninth layer for head revision.\nThe impact of head utilization ratio on model performance. Based on the observations in Figure 8, we focus on selecting the proportion of heads in a certain layer rather than which head. We evaluate the impact of varying head ratios on the experimental outcomes with $\\alpha$ = 2.0. Figure 9 illustrates when the head ratio is increased from 70% to 80%, the accuracy of ScienceQA declines significantly without much gain in safety performance. Conversely, increasing"}]}