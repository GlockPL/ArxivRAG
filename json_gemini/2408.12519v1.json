{"title": "ADVANCED ATOM-LEVEL REPRESENTATIONS FOR PROTEIN\nFLEXIBILITY PREDICTION UTILIZING GRAPH NEURAL\nNETWORKS", "authors": ["Sina Sarparast", "Aldo Zaimi", "Maximilian Ebert", "Michael-Rock Goldsmith"], "abstract": "Protein dynamics play a crucial role in many biological processes and drug interactions. However, measuring, and\nsimulating protein dynamics is challenging and time-consuming. While machine learning holds promise in deciphering\nthe determinants of protein dynamics from structural information, most existing methods for protein representation\nlearning operate at the residue level, ignoring the finer details of atomic interactions. In this work, we propose for the\nfirst time to use graph neural networks (GNNs) to learn protein representations at the atomic level and predict B-factors\nfrom protein 3D structures. The B-factor reflects the atomic displacement of atoms in proteins, and can serve as a\nsurrogate for protein flexibility. We compared different GNN architectures to assess their performance. The Meta-GNN\nmodel achieves a correlation coefficient of 0.71 on a large and diverse test set of over 4k proteins (17M atoms) from the\nProtein Data Bank (PDB), outperforming previous methods by a large margin. Our work demonstrates the potential of\nrepresentations learned by GNNs for protein flexibility prediction and other related tasks.", "sections": [{"title": "Introduction", "content": "The B-factor is an atomic displacement parameter (expressed in square angstroms, \u00c5\u00b2) that is typically measured\nexperimentally in proteins by X-ray crystallography (Caldararu et al. (2019)). The growing interest in B-factor\nmeasurement has been motivated by a number of early studies suggesting that regions with higher B-factors could\ncorrelate with higher mobility (Sun et al. (2019)). For instance, high mobility regions are characterized by, higher\nthan average flexibility, higher than average hydrophilicity, and higher net charge (Radivojac et al. (2004)). Therefore,\nB-factors can be used in protein science for identifying flexible regions that can potentially be targeted for future drug\ndiscovery applications (Sun et al. (2019)). Publicly available X-ray protein structures are deposited in the Protein Data\nBank (PDB) (Rose et al. (2016)), which include B-factor measurements along with other properties.\nAccurate methods to predict protein B-factors in the absence of experimental structures remain elusive. Due to the\nlack of experimental protein structures for many relevant targets (full protein, pathogenic mutations, ligand complexes,\netc.) structural models, for example generated using AlphaFold (Jumper et al. (2021)), are missing B-factors. Accurate\nprediction of B-factors from structure alone could serve as a fast method to evaluate protein dynamics without running\nan all atomistic protein dynamics simulations.\nIn this work, we present a deep learning framework for predicting protein B-factors at the atomic level based on\n3D protein structure. A set of diverse graph neural network (GNN) architectures were implemented and adapted to\nthis use case. The architecture of Graph Neural Networks (GNNs) demonstrates promising performance in learning"}, {"title": "Protein representation learning", "content": "With the success of deep learning, significant advances have been made in the last few years in the field of protein\nrepresentation learning. Graphs are particularly well suited for encoding protein tertiary structures in the form of a\nnetwork of residues (the amino acids) represented by graph nodes and bonds represented by graph edges. Protein\nrepresentation learning models based on GNNs have been able to tackle a great number of challenging tasks, including\nprotein structure prediction from residue sequence (Jumper et al. (2021)), protein function prediction (Kulmanov and\nHoehndorf (2022), You et al. (2021)), Gligorijevi\u0107 et al. (2021)), protein fold classification (Roethel et al. (2022)),\nprotein model quality assessment (Sanyal et al. (2020)), protein interface prediction (Fout et al. (2017)), protein-ligand\nbinding affinity prediction (Jiang et al. (2022), Jim\u00e9nez et al. (2017)) and protein-protein binding prediction (Pancino\net al. (2020)). A few approaches have successfully combined the protein sequence with the protein 3D structure in order\nto obtain more expressive multi-level representations (Chen et al. (2022), Zhang et al. (2022)). Although numerous\napproaches and tasks have been explored in the literature, there is a scarcity of studies focusing on the atomic-level\nrepresentation for protein-related tasks. There are two primary reasons that account for this: first, the significant\ncomputational resources necessary to encode proteins, which are typically comprised of thousands of atoms each; and\nsecond, the majority of protein-related tasks, such as predicting protein properties and binding affinity, operate at a\nhigh level (i.e. residue level) and do not necessitate encoding at the atomic level as required for tasks involving small\nmolecules."}, {"title": "Small molecule representation learning", "content": "A series of approaches mainly inspired by quantum chemistry have been proposed for prediction and classification\ntasks on small molecules at the atomic level. For example, Duvenaud et al. (2015) proposes the use of Convolutional\nNeural Networks (CNNs) to learn molecular fingerprints on graphs for predictive tasks such as solubility, drug efficacy,\nand organic photovoltaic efficiency. Gilmer et al. (2017) propose the Message Passing Neural Network (MPN) for\nmolecular property prediction tasks on the quantum chemistry QM9 benchmark dataset. Atomic features such as the\natom type, the atomic number, the hybridization and the number of hydrogens are encoded in the input node embeddings.\nSimilarly, Sch\u00fctt et al. (2017) offers a way to model interactions of atoms in molecules, using interaction maps, while\nrespecting quantum-chemical constraints related to energy conservation. Despite success on small molecules, these\napproaches do not necessarily scale well for large macromolecules such as proteins, where the number of atoms can\neasily exceed a few thousand and building the interaction maps can become prohibitively expensive. Moreover, in\nproteins, sequences of residues combine to form secondary structure components (e.g. alpha helices and beta sheets) via\nhydrogen bonds, which will then fold to form the tertiary 3D structure via non-covalent bonds (e.g. polar hydrophilic\nhydrogen interactions, ionic interactions, and internal hydrophobic interactions). Capturing these interactions that are\nonly present at the higher protein structure levels is challenging."}, {"title": "Protein B-factor prediction", "content": "Most of the existing work on protein B-factor prediction has been done with standard machine learning algorithms.\n(Bramer and Wei (2018)) generated 2D images of protein features/encodings via a Multiscale Weighted Colored Graph\n(MWCG). Random forests, gradient-boosted trees and CNNs were then compared for a blind B-factor prediction task.\nThey made use of local features such as the atom type and amino-acid type for every heavy atom, but also of global\nfeatures such as the number of heavy atoms in the protein and the resolution of the crystallographic data. (Jing et al.\n(2014)) considered protein features such as disorder, mutation information, secondary structure and physicochemical\nand biochemical properties. The B-factor prediction was performed with four different machine learning algorithms:\nlinear regression, REP (Reduced Error Pruning) tree, Gaussian process regression and random forest regression. Finally,\n(Yuan et al. (2005)) trained a support vector regressor to predict a B-factor distribution profile from a protein sequence."}, {"title": "Methods", "content": null}, {"title": "Data Description", "content": "All protein structures used in this work were obtained from the Protein Data Bank (PDB) (https://www.rcsb.org)\nin .pdb format. PDB files were parsed using TorchDrug's (Zhu et al., 2022) data parsing tool, and RDKit (https:\n//www.rdkit.org). Typically, each PDB file contains a protein, which is presented as a 3D atom cloud with x, y, z\ncoordinates, and information about chemical bonds between atoms, and other information such as crystallographic\nresolution (in the case of X-ray and Cryo-EM structures), the author(s) of the file, etc. Hydrogen atoms are typically not\nresolved by x-ray or Cryo-EM explicitly and their approximate location can often be calculated. We have a general set\nof Apo and Holo proteins. The Apo proteins are a set of proteins that do not have their necessary co-factors or ligands\nbound to it. Conversely, The Holo state consists of proteins that contain ligands. In parallel, Kinase proteins of different\nsubfamilies and groups were diversely sampled from the PDB, thus making a test dataset of diverse proteins that are\nrepresentative of known structures of the kinome (labeled as the Kinase dataset in this work). The Kinases are essential\nfor a wide range of cellular functions, including growth, differentiation, metabolism, and apoptosis (programmed\ncell death), and hence of prime importance for the drug discovery process. They are particularly important in signal\ntransduction pathways, where they transmit signals from the cell surface to the nucleus, ultimately influencing gene\nexpression and cellular responses. Due to their central role in cell regulation, Kinases have been a focus of extensive\nresearch, and they are also important targets for drug development, which is the reason we are interested in evaluating\nperformance on them.\nA quantitative comparison between the two sets can be found in Table 1. For both datasets, an initial filtering was\nperformed to remove proteins with erroneous B-factor values (e.g. B-factor of 0, and constant B-factor value across all\nprotein atoms, neither are valid proteins for the purpose of this work). A second filtering involved only keeping proteins\nwith an X-ray crystallographic resolution < 2.5A.\nIt is generally agreed that deposited proteins with resolutions larger than 2.5\u00c5 are considered unreliable (Carugo\n(2018)). This is primarily due to the complex interplay of dynamics and experimental errors present in structures with\nresolutions beyond this threshold, making it challenging to disentangle these factors."}, {"title": "Graph Representation", "content": "Figure 1 illustrates the conversion from a protein 3D structure to a graph representation, where each node represents an\natom and each edge represents a bond. We adopt the graph representation used in Pytorch Geometric Fey and Lenssen\n(2019). Each protein is represented as a graph object that contains the following matrices:\n\u2022 The matrix of the nodes features, with dimension [number of nodes, number of node features];\n\u2022 The matrix of the 3D coordinates of the nodes, with dimension [number of nodes, 3];\n\u2022 The matrix of the edge features, with dimension [number of edges, number of edge features];\n\u2022 The target node feature vector to predict, with dimension [number of nodes]."}, {"title": "Node Features", "content": "Several node features of the protein atoms are included in the graph object. The categorical node features are represented\nas one-hot encoders, while the numerical features are normalized and scaled.\nAtom type The atom element (here referred as atom type) is included in the set of node features as a one-hot encoder.\nThe majority of heavy atoms in the protein structure are carbon (C), oxygen (O), and nitrogen (N). The hydrogen atoms\nare very light compared to other atoms and are rarely modeled by crystallographers.\nAtom relative location Every atom from the PDB also contains information about its relative location (also called\nlocant) in the residue. Typically, atoms closer to the backbone will present less mobility than the ones further in the\nside chain. We include this information as a one-hot encoder."}, {"title": "Edge Features", "content": "Two edge feature categories were included in the graph object. The categorical features are incorporated as one-hot\nencoders while the numerical features are normalized and scaled.\nCovalent bond type For the covalent bond between two atoms (nodes), we incorporate the type of bond (i.e. single,\ndouble, triple, delocalized) as a one-hot encoder.\nDistance: Given two connected nodes $v_i$ and $v_j$ with 3D coordinates $p_i$ and $p_j$, we compute the Euclidean distance\nas $\\sqrt{(P_i \u2013 P_j)^2}$ and incorporate it into the edge features."}, {"title": "General Notation", "content": "For each protein of the dataset, we consider its graph representation G = (V, E), where V are the nodes representing\nits atoms and E are the edges representing the covalent bonds between atoms. We define $X \\in R^{N \\times C}$ as the node\nfeature matrix, where N is the number of nodes and C is the number of features for each node. Similarly, we can\nalso define $E \\in R^{M \\times F}$ as the edge feature matrix, where M is the number of edges and F is the number of features\nfor each edge. The general framework of GNN layers consists of two main operations: (i) aggregate the information\nof neighboring nodes via a function A and (ii) update the node embeddings by combining the incoming aggregated"}, {"title": "Models", "content": "In computational chemistry, it is not established which architectures are more suited for predicting B-factors. Similar\nto the use of GNNs to predict force fields for small molecules in Park et al. (2021), certain architectures excel at the\natomic prediction task for proteins. Therefore, we have selected a diverse set of GNN architectures to explore their\ncapabilities and to build a model that can incorporate valuable elements from the explored architectures.\nDifferent GNN architectures from the literature were selected and adapted to the task of protein B-factor prediction.\nThe diversity of the chosen architectures results in the exploration of different types of aggregation and update\nmechanisms. Each architecture design is based on different inductive biases of the graph data and can thus potentially\nlearn and encode different embeddings. The seven models presented in this report fall into the following GNN\narchitectures:\n\u2022 Convolutional graph networks: the GCN architecture from Kipf and Welling (2017);\n\u2022 Attentional graph networks: the GAT architecture from Veli\u010dkovi\u0107 et al. (2017) and the Transformer Convolu-\ntional layer from Shi et al. (2020);\n\u2022 Message passing graph networks: the MPN formulation from Gilmer et al. (2017);\n\u2022 Encoder-decoder graph networks: the Graph-UNet architecture from Gao and Ji (2019);\n\u2022 Equivariant graph networks: the EGNN architecture from Satorras et al. (2021);\n\u2022 General multipurpose graph networks: the general graph network formulation from Battaglia et al. (2018).\nOur implementations are based on PyTorch and the PyTorch Geometric package Fey and Lenssen (2019), unless\notherwise stated. For all implementations, regularization (dropout and/or batch normalization) and residual connections\nbetween consecutive layers are implemented when suitable. A general overview of the models presented is illustrated in\nFigure 2. A brief description and key differences of each architecture can be found in Table 2, and a more detailed\ndescription for all models except the Meta GNN model is given in Supplementary Equation 12, we describe the general\nnotation of GNNs and the Meta model in the next section."}, {"title": "Meta GNN - General Graph Networks", "content": "The general meta-layer proposed by Battaglia et al. (2018) generalizes the concept of graph networks with three\nlevels of features: node features, edge features and global graph features. Each graph network layer takes input node,\nedge and global features V, E and u, and outputs a graph with updated features V', E' and u'. In the case where we\nonly make use of nodes and edges features, three operations are needed to go from layer k to layer k + 1: compute\nupdated edge attributes $e_{i}^{k+1}$ using edge attributes $e_{i}^{k}$ and its receiver and sender node attributes $v_i$ and $v_j$ (equation 4),\naggregate edge attributes per node to obtain $\\hat{e}_{j}^{k+1}$ (equation 5), compute updated node attributes $v_{j}^{k+1}$ using aggregated\nedge attributes $\\hat{e}_{j}^{k+1}$ and node attributes $v_{j}^{k}$ (equation 6). Figure 2 (c) illustrates the update mechanisms of the Meta\nlayer. These steps can be formalized with the following equations for a given edge i and a given node j, where $N_e$ is\nthe number of edges, $N_n$ is the number of nodes and $E_j^{k+1}$ is the collection of edge vectors with receiver node j:\n$e_{i}^{k+1} = \\phi_{e}^{k}(e_{i}^{k}, v_i, v_j), i \\in \\{1...N_e\\}$ (4)\n$\\hat{e}_{j}^{k+1} = \\rho(E_j^{k+1}), j \\in \\{1...N_n\\}$ (5)\n$v_{j}^{k+1} = \\phi_{u}^{k}(\\hat{e}_{j}^{k+1}, v_j^{k}), j \\in \\{1...N_n\\}$ (6)\nIn their simplest case, functions $\\phi_e$ and $\\phi_u$ can be implemented as learnable Multi-Layer Perceptrons (MLPs). In our\nimplementation, we provide an edge update module $\\phi_e$ that is a stack of MLP layers and 2 options of node update\nmodules: a standard MLP $\\rho$ and a $\\phi_u$ that follows the node update mechanism of the GAT architecture (Veli\u010dkovi\u0107\net al. (2017)). $\\rho$ is an aggregation mechanism used to aggregate edge updates and is shared between updates. This\narchitecture, unlike previous GNN layers presented (e.g. GCN, GAT, MPN), naturally enables the updating of the edge\nattributes after each layer. Moreover, it opens the door for the potential use of global features (e.g. protein family,"}, {"title": "Experiments", "content": "Dataset splits The training was done on the Apo/Holo dataset for all models, using a random split of 85/15% for the\ntraining and validation sets based on the same seed. The Kinase set is a test set to evaluate the performance of the best\ntrained models. Apo/Holo are more general than the Kinase, so we expect to generalize to the Kinase when testing on\nthem.\nEvaluation Metrics Based on the literature (Jing et al. (2014), Bramer and Wei (2018)), we consider the following\nmetrics for the evaluation of the performance on the node regression task: the Pearson correlation coefficient (CC), the\nmean absolute error (MAE), the mean absolute percentage error (MAPE) and the root relative squared error (RRSE).\nThe CC measures the correlation between the target and predicted atomic B-factors, while the MAE, MAPE and RRSE\nmeasure the difference between the target and prediction values. Given a set of n nodes, a set of targets y and the\ncorresponding set of predictions \u0177, the performance metrics are defined as follows:\nCC = $\\frac{\\sum_{i=1}^{n}(y_i \u2013 \\bar{y}) (\\hat{y}_i \u2013 \\bar{\\hat{y}})}{\\sqrt{\\sum_{i=1}^{n}(y_i \u2013 \\bar{y})^2 \\sum_{i=1}^{n}(\\hat{y}_i \u2013 \\bar{\\hat{y}})^2}}$ (7)\nMAE = $\\frac{1}{n}\\sum_{i=1}^{n} |y_i - \\hat{y}_i|$ (8)\nMAPE = $\\frac{1}{n}\\sum_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{\\max(\\epsilon, |y_i|)}$ (9)\nRRSE = $\\sqrt{\\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}}$ (10)\nThe best predictive models are expected to have a CC close to 1, and MAE, MAPE and RRSE values close to 0.\nTraining setup Hyper-parameter search was performed on all 7 architectures for 10 epochs and the best models were\nselected based on the average CC metric over the validation data. Among the runs with train and validation CCs of\ngreater than 0.6 and validation a MAPE lower than 0.5, we select the runs with the highest validation CC, since what\nis most important is to capture the trends of B-factor values within each protein. More details on the hyperparameter\ntuning process and a list of hyperparameters of each model can be found in Supplementary Section 14. All final models\nwere trained for a maximum of 50 epochs, using the MAE loss (see equation 8) and the weighted Adam optimizer. A\nbatch size of one was used (i.e. one protein) since some proteins are very large and can cause GPU memory issues. For\neach model, 3 different runs were executed, and the average metrics over these runs were reported. All models were\ntrained on NVIDIA GPUs (models A100, V100 and/or RTX8000), using one GPU node per training session.\nTraining and Inference Time Our models typically take around 2 minutes per epoch (using a batch size of one\nprotein and a training set of almost 3k proteins) to train on a GPU. The inference speed is ~37 proteins/s on a GPU and\n~15 proteins/s on a CPU. This allows us to potentially run predictions on more than 100k proteins per hour."}, {"title": "Results", "content": "Performance metrics The performance metrics of each model after hyper-parameter tuning on the training and\nvalidation sets are presented in Table 3. The models' performance metrics on the Kinase test dataset are presented in\nTable 4. As it can be observed in Table 4, the Meta-GNN implementation is the best-performing model on all metrics,\nobtaining a CC of 0.71 as well as the lowest MAE, MAPE and RRSE values. Interestingly, the Meta-GNN is also the\nmodel with the second-lowest number of trainable parameters (145k). It is followed by the GCN, GAT and TConv\nimplementations (CC of 0.67). For a more detailed visualization of the protein CC distributions on the test set, see\nFigure 3.\nTo illustrate the predictions of the Meta model, we present three proteins from the test set and their projected predictions\ninto the 3D structure in Figure 4. In addition, a comparison between target and prediction B-factor distributions of a few\nselected proteins from the test set is presented in Supplementary Figure 5."}, {"title": "Discussion", "content": "The GCN model is used as a baseline in this work since it is typically viewed as the simplest form of GNNs that can be\napplied to our task. The GCN model updates node features and considers a scalar constant like distance as an edge\nweight (Supplementary Section 12.0.1).\nTherefore, to include more information for the edges, models such as GAT, TConv, MPN, which aggregate edge\nfeatures and use them to update node features, were also considered, since we have edge types too. We included the\nEGNN model, which includes encoded atom coordinates along with edge embeddings when updating node embeddings,\nbecause other models do not use this information.\nWe included UNet as it has a different aggregation mechanism compared to the other models, and operates based on\npooling and unpooling node features (for more details refer to Supplementary Section 12.0.6). Finally, we combined\nfeatures of the GAT architecture into the Meta model for updating both node embeddings and edge embeddings, making\nthe Meta model the only model updating edge embeddings in this work. The performance of the UNet model indicates\nthat this architecture, which uses pooling layers, may not be well-suited for node regression tasks, as it was used\nfor graph and node classification in the Gao and Ji (2019). This could be related to the lack of structural and edge\ninformation in the unpooling layers of the decoder (Supplementary Section 12.0.6), and the loss of finer-grained details\nin the encoder, due to the pooling operation.\nThe EGNN model was selected because it is among the few models that embed node coordinates as features, and\nupdating these coordinate embeddings at every layer is possible. Layers of the EGNN model can embed this information\nin a relative manner that is translation, rotation, and permutation equivariant, as proven in Satorras et al. (2021). The\nEGNN's performance on the test set is similar to UNet (Table 4). This model encodes more information compared\nto the others and considers symmetric features in the structures, which may not be helpful in the case of proteins, as\nstructural symmetry does not necessarily result in similar B-factors. Note that the best configuration of hyperparameters\nfor the EGNN model, based on the criteria specified in Supplementary Section 14, does not update nodes. It only\nupdates the embeddings of the coordinates (Supplementary Table 6). It seems that the network learns to rely solely on\nthe coordinates and propagates and updates the coordinate embeddings. This is similar to what UNet does, as distances\nto connected neighbors are added to node embeddings and propagated."}, {"title": "Limitations", "content": "When using the models for protein B-factor prediction in this work, one should consider the following limitations:\n\u2022 The models in the Protein Data Bank (PDB) can have problematic B-factor values, such as missing values, very\nhigh values (above 80), uniform values for all atoms, or significant variation between bonded atoms. These\nissues can be due to over-fitting in crystallography models or may accurately represent disorder caused by a\ncomplex mix of protein dynamics and static disorder (e.g., from crystal imperfections). When lower-resolution\nstructures have multiple conformations, B-factors may increase to account for uncertainty.\nTo study the impact of protein dynamics on B-factors, it's crucial to avoid models with such artifacts as they\ncan obscure useful information. Instead, training models on B-factors primarily reflecting thermal motion\nmight be a viable approach. Then, comparing predicted and modeled B-factors in problematic structures can\nhelp identify the root cause of B-factor issues. Analyzing trends in B-factors from high-resolution data to\nmedium-high resolution data may provide insights into the relationship between B-factor analysis and static\nand dynamic aspects of protein structure.\n\u2022 Challenges can arise due to the experimental nature of the measurements: (i) many additional factors such\nas crystal defects, large-scale disorder, and diffraction data quality can deteriorate the overall data quality\n(Carugo (2022)) and (ii) the values can vary significantly depending on the crystallography resolution (Carugo\n(2018)) and between protein structures. A more in-depth curation of the PDB data/proteins used for training\nthe models may be necessary to avoid learning from biased experimental data."}, {"title": "Conclusion and Future Work", "content": "We presented here for the first time a deep learning framework predicting atomic protein B-factors based on their 3D\nstructure using graph neural network architectures. Past work done on protein B-factor prediction has the following\nlimitations: (i) they are trained and/or tested on very small datasets, (ii) they rarely reach CC values larger than 0.6\nand (iii) they use traditional machine learning approaches that do not make use of the rich 3D graph structure of\nproteins. Our methodology addresses these limitations by providing a GNN model (Meta-GNN) that reaches an average\ncorrelation coefficient of 0.71 on an unseen test set of more than 4k proteins (17M atoms). We believe that this work\ncan be a promising step towards atomic property prediction tasks on proteins, which can contribute to drug discovery\nresearch and applications such as protein misfolding.\nThis work also opens the door for many future ideas. We list a few of them here in an effort to motivate new discussions\nand increase the reach of the framework implemented:\n\u2022 Processing Anisotropic B-factors: Anisotropic B-factors Yang L. (2009) take the refinement process a step\nfurther by acknowledging that atoms can vibrate differently along different axes. Instead of assuming an equal\nlevel of vibration in all directions, anisotropic B-factors use a more sophisticated model that considers distinct\nvibrational behavior along the principal axes (x, y, and z) for each atom. It is interesting to see how the models\ndeveloped and trained in this work will perform on predicting the anisotropic B-factors.\n\u2022 Novel GNN design based on the Meta-GNN formulation: The promising performance of the modular\nMeta-GNN model leads us to believe that the next iteration of this work could consist of designing a novel\narchitecture more adapted to the specific task of atomic property prediction on proteins. Interesting design\nideas from various architectures could then be incorporated into a novel GNN design: positional encodings\n(EGNN), attention formulations (GAT, TConv), aggregation, and update mechanisms of the nodes, edges and\nglobal embeddings.\n\u2022 Hierarchical GNN approaches: Hierarchical approaches that exchange information between multiple levels\nof protein structure (e.g. between atomic level and residue level) in order to capture these interactions would\nbe interesting to explore (Fey et al. (2020), Somnath et al. (2021)). These multi-level approaches would be\nrelevant for large macromolecules such as proteins that have primary, secondary, tertiary, and even quaternary\nstructures.\n\u2022 Larger input features space: Due to the limited time-frame of this work, a broader exploration of node and/or\nedge features to incorporate into our data pipeline was not possible. More information on the crystal lattice\ncan be added. Examples of potential features that could be added by using computational chemistry tools are\nangles, secondary structure information (e.g. alpha helices, beta sheets), and non-covalent interactions (e.g.\nhydrogen bonds).\n\u2022 Other losses and/or performance metrics: It would be relevant to use other loss formulations that could\ncapture the distribution differences between predictions and targets better. Metrics such as the Wasserstein\ndistance could potentially help in that regard.\n\u2022 RNA and DNA representation: It would be interesting to apply the GNN architectures in this work on tasks\non RNAs and DNAs, to see how expressive they are on these types of data."}, {"title": "Data Availability", "content": "The proteins are retrieved from the PDB (https://www.rcsb.org/). The code names are listed for the training\nset at https://gitlab.com/congruencetx/pfp/-/blob/main/data/trainval_codes.txt and for test set at\nhttps://gitlab.com/congruencetx/pfp/-/blob/main/data/test_codes.txt."}, {"title": "Code Availability", "content": "All the code developed in this project has been made freely available at https://gitlab.com/congruencetx/pfp."}, {"title": "Competing interests", "content": "The authors declare no competing interests."}, {"title": "Supplementary Information", "content": null}, {"title": "GCN - Graph Convolutional Networks", "content": "Graph Convolutional Networks are one of the simplest architectures for node classification tasks. For a GCN model\nwith K layers, the intermediate layers perform the following operations to update node embeddings:\n$x_{v}^{k+1} = \\phi_{u}^{k}(x_{v}^{k}, \\sum C_{vw}x_{w}^{k}), k = 1,..., K - 1$ (11)\n$w \\in N(v)$\nwhere $x_{v}^{k+1}$ is the node embedding for node v at step k + 1 that has a set of neighbors called N(v). The function $\\phi_{u}$\ncombines the neighboring features of node v with its own features $x_{v}^{k}$ to compute the updated feature embeddings $x_{v}^{k+1}$.\nA set of constants $C_{vw}$ can be used to weigh every neighbor differently during the aggregation operation.\nA GCN layer does not make use of possible edge features, but it can be seen as a simple and efficient way to learn node\nrepresentations based on its local neighbors. For instance, after 3 GCN layers, a given node v will have a new hidden\nrepresentation that captures all the previous hidden representations up to its 3-hop neighbors (i.e. nodes that it can\nreach with 3 edges or fewer) (see Sanchez-Lengeling et al. (2021)). Our model implementation offers two options for\nregression heads: a standard MLP or a GCN layer that directly maps to the final node prediction value."}, {"title": "GAT - Graph Attention Networks", "content": "In Graph Attention Networks (Veli\u010dkovi\u0107 et al. (2017)), the representation of a given node is updated based on\nthe linear combination of its neighbors' representations. The shared weights @ in the linear combination are a set of\nlearnable parameters known as the attention coefficients.\nIn the GAT formulation, the attention mechanisms called $a_{vw}$ for a given node v with neighbors w are computed as a\nsoftmax over the concatenation of its features $x_v$, its neighbors' features $x_w$ and the edge features $e_{vw}$ between them\ntransformed by the aggregation layer $\\theta^k$, which is a linear layer with LeakyReLU in our work.\nA GAT layer k + 1 will update the embeddings $x_v^{k+1}$ of node v by: (i) aggregating the neighbors' features and their edge\nfeatures via the attention mechanism and (ii) combining them with the previous embeddings $x_v^{k}$ of node v:\n$\\alpha_{vw} = softmax(\\phi^k(\\theta^{k}x_{v}^{k}, \\theta^{k}x_{w}^{k}, \\theta^{k}e_{vw}))$ (12)\n$x_v^{k+1} = \\phi_{u}^{k}(\\alpha_{v \\omega}\\theta^{k}x_v^{k}, \\sum \\alpha_{v \\omega}\\theta^{k}x_w^{k})$ (13)\n$\\omega \\in N(v)$"}, {"title": "TConv - Graph Transformer Networks", "content": "The Unified Message Passing (UniMP) model presented in Shi et al. (2020) proposes a graph transformer convolu-\ntional layer (which we will name TConv here for simplicity) that is another flavor of attention-based GNNs. The general\nformulation is very similar to the GAT layer, but the computation of the attention in the TConv layer is done similar to\nthe Vaswani et al. (2017). A TConv layer k +"}]}