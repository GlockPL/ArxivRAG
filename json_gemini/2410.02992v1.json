{"title": "GUIDED STREAM OF SEARCH: LEARNING TO\nBETTER SEARCH WITH LANGUAGE MODELS\nVIA OPTIMAL PATH GUIDANCE", "authors": ["Seungyong Moon", "Bumsoo Park", "Hyun Oh Song"], "abstract": "While language models have demonstrated impressive capabilities across a range\nof tasks, they still struggle with tasks that require complex planning and reasoning.\nRecent studies have proposed training language models on search processes rather\nthan optimal solutions, resulting in better generalization performance even though\nsearch processes are noisy and even suboptimal. However, these studies overlook\nthe value of optimal solutions, which can serve as step-by-step landmarks to guide\nmore effective search. In this work, we explore how to leverage optimal solutions\nto enhance the search and planning abilities of language models. To this end, we\npropose guided stream of search (GSoS), which seamlessly incorporates optimal\nsolutions into the self-generation process in a progressive manner, producing high-\nquality search trajectories. These trajectories are then distilled into the pre-trained\nmodel via supervised fine-tuning. Our approach significantly enhances the search\nand planning abilities of language models on Countdown, a simple yet challenging\nmathematical reasoning task. Notably, combining our method with RL fine-tuning\nyields further improvements, whereas previous supervised fine-tuning methods do\nnot benefit from RL. Furthermore, our approach exhibits greater effectiveness than\nleveraging optimal solutions in the form of subgoal rewards.", "sections": [{"title": "1 INTRODUCTION", "content": "Transformer-based language models have achieved remarkable success, demonstrating human-level\nperformance across a wide range of natural language tasks, including conversation, code generation,\nand mathematical problem-solving (Achiam et al., 2023; Touvron et al., 2023; Roziere et al., 2023;\nLi et al., 2023; Lightman et al., 2023; Shao et al., 2024). Their impressive performance is primarily\nattributed to auto-regressive training on high-quality, internet-scale data. However, language models\nstill face challenges with tasks that require complex planning and reasoning (Pallagani et al., 2023;\nValmeekam et al., 2023). Models trained using next-token prediction often result in the snowballing\nof errors over long sequences, making it difficult for them to maintain consistent plans over multiple\nsteps. Furthermore, teacher-forcing, where models are given the correct sequence of previous tokens\nfor each prediction, exacerbates this problem by encouraging them to learn shortcuts rather than truly\nunderstanding the underlying structure of the task (Bachmann & Nagarajan, 2024).\nA growing body of literature attempts to improve the planning and reasoning capabilities of language\nmodels through prompt-based strategies, allowing them to perform chain-of-thought reasoning, self-\ncorrection, and planning with symbolic search algorithms (Wei et al., 2022; Wang et al., 2023; Shinn\net al., 2023; Yao et al., 2023). While these methods are successful in certain tasks, they have inherent\nlimitations. They only assist the model during inference without updating its internal weights, which\nsignificantly constrains performance to that of the base model. Moreover, their success heavily relies\non the quality of prompt design, and a poorly constructed prompt sometimes degrades performance\n(Huang et al., 2024).\nTo address these limitations, recent studies have shifted toward directly improving the planning and\nreasoning abilities of language models during the training phase (Lehnert et al., 2024; Gandhi et al.,\n2024). This approach, known as stream of search (SoS), involves training a model to predict search\ntrajectories that encompass the entire decision-making process of finding solutions through trial and"}, {"title": "2 PRELIMINARIES", "content": null}, {"title": "2.1 PROBLEM SETUP", "content": "In this paper, we consider a sequential decision-making problem derived from the program synthesis\nliterature, which requires strong search and planning abilities (Devlin et al., 2017; Shi et al., 2024)."}, {"title": "2.2 STREAM OF SEARCH", "content": "One straightforward approach to this problem is to apply imitation learning through BC (Ross et al.,\n2011), which trains the model to directly predict optimal solutions from input-output pairs. However,\nprior studies have demonstrated that BC struggles to generalize to unseen test examples (Yang et al.,\n2022; Lehnert et al., 2024; Gandhi et al., 2024).\nTo address this, SoS introduces an approach that leverages the search process rather than the optimal\nsolution (Gandhi et al., 2024). This method reformulates the problem as a tree search, navigating the\nsearch tree through trial and error with operations until the solution is reached. Each node in the tree\nrepresents a state, and each edge represents an operation between two states. SoS expresses primitive\noperations for the tree search in language, including node generation, exploration, backtracking, and\nverification. It then generates search trajectories using symbolic search algorithms, including depth-\nfirst search (DFS) and breadth-first search (BFS), representing them as sequences of tokens. Finally,\nit trains a language model to predict these trajectories from input-output pairs used as prompts:\n$\\max_{\\theta} \\mathbb{E}_{x \\sim D, y \\sim \\text{symbolic}(x)}[\\log f_{\\theta}(y | x)]$.\nNote that the SoS model is not specifically designed to find solutions, as symbolic search algorithms\nmay produces suboptimal trajectories with a limited search budget. See Appendix A for more details."}, {"title": "2.3 SUPERVISED FINE-TUNING", "content": "The pre-trained language model, not originally designed for particular tasks, requires alignment with\na downstream task to enhance its performance. One approach to achieving this alignment is to apply\nsupervised fine-tuning with self-generated data, referred to as self-taught reasoner (STaR) (Zelikman\net al., 2022; Gulcehre et al., 2023). This method generates trajectories using the model from prompts\nand filters them based on a task-specific metric $M$ that evaluates quality. It then fine-tunes the model\nto predict these filtered trajectories:\n$\\max_{\\theta} \\mathbb{E}_{x \\sim D, y \\sim f_{\\theta}(\\cdot | x)}[\\mathbb{1} [M(y | x) > T] \\cdot \\log f_{\\theta}(y | x)],$ (1)\nwhere $T$ is the threshold that controls the ratio of filtered trajectories. This process can be performed\niteratively to further refine the model."}, {"title": "2.4 REINFORCEMENT LEARNING FINE-TUNING", "content": "Another approach to aligning the pre-trained language model is to perform RL fine-tuning (Stiennon\net al., 2020; Ouyang et al., 2022). This method uses the model as a policy $\\pi_{\\theta}$ and fine-tunes the policy\nto maximize a task-specific reward $R$, while minimizing the KL divergence from the reference policy\n$\\pi_{\\text{ref}}$. The problem is formulated as a token-level finite-horizon Markov decision process (MDP). The\nstate space $\\mathcal{S}$ is the set of all possible token sequences, and the action space $\\mathcal{A}$ is the set of all tokens.\nThe initial state $s_0 \\in D$ is a randomly sampled prompt. Each state $s_h \\in \\mathcal{S}$ is the concatenation of the\nprompts and the previously generated tokens, with the transition function $p : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathcal{S}$ appending\nthe action $a_h \\in \\mathcal{A}$ to the state. Given a trajectory generated by the policy with horizon $H \\in \\mathbb{Z}^{+}$, the\npolicy is trained to optimize the following objective:\n$\\max_{\\theta} \\mathbb{E}_{s_0 \\sim D, (s_h, a_h) \\sim \\pi_{\\theta}} \\Big[\\sum_{h=0}^{H} R(s_h, a_h) - \\beta \\cdot (\\log \\pi_{\\theta}(a_h | s_h) - \\log \\pi_{\\text{ref}}(a_h | s_h))\\Big]$\nwhere $\\beta > 0$ is the coefficient that controls the influence of the KL divergence. Typically, the reward\nis provided at the end of the trajectory.\nA common algorithm for training this policy is proximal policy optimization (PPO) (Schulman et al.,\n2017). This method optimizes the policy using the clipped surrogate objective, which is designed to\nlimit policy changes. Simultaneously, it trains a value function $V_{\\Phi}: \\mathcal{S} \\rightarrow \\mathbb{R}$ to predict the multi-step\nreturns computed by the generalized advantage estimator (GAE) (Schulman et al., 2016)."}, {"title": "2.5 COUNTDOWN BENCHMARK", "content": "To evaluate the search and planning abilities of language models, we use Countdown as a benchmark\n(Gandhi et al., 2024). Each problem consists of input numbers and a target number, where the goal\nis to transform the inputs into the target using the four basic arithmetic operations. This problem has\na high branching factor of $O(k^2)$ in the search tree, where $k$ is the number of remaining inputs. To\nensure a tractable level of difficulty, we set the number of initial inputs to 4, following the setup in\nGandhi et al. (2024). See Appendix B for more details on Countdown."}, {"title": "3 METHODS", "content": null}, {"title": "3.1 MOTIVATION", "content": "Gandhi et al. (2024) show that training a language model using noisy, suboptimal search trajectories\nleads to better generalization compared to clean, optimal solutions. That said, optimal solutions can\nstill offer valuable guidance during generation. If the SoS model has the ability to continue the search\nfrom arbitrary incomplete trajectories, we can leverage this to generate high-quality trajectories. By\nproviding the model with partial optimal solutions alongside the prompts as hints, we can guide it to\ncontinue the search within a reduced search space, increasing the probability of finding a solution.\nTo examine whether the SoS model possesses this ability, we conduct an experiment on Countdown.\nFor each problem with an optimal solution of $N$ operations, we define the partial optimal path as the\ntrajectory generated by applying the first $n$ operations in the search tree. We then append this path to\nthe initial prompt. Finally, we generate search trajectories using the SoS model from these modified\nprompts and evaluate correctness. Table 1 shows the ratio of successful trajectories with varying the\nlength of partial optimal solutions for 200,000 training problems. The model successfully discovers\nsolutions starting from these paths, despite not having encountered them during training. Moreover,\nincreasing their length greatly improves the correctness of the resulting trajectories. This encourages\nus to use these high-quality, self-generated data for supervised fine-tuning, distilling the knowledge\nof optimal solutions into the model.\nHowever, this guidance results in low likelihood under the model. Table 1 presents the cross-entropy\nloss of the trajectories, showing that using longer partial optimal paths results in higher loss values.\nFine-tuning on these trajectories may lead to significant changes in the model's weights, potentially\ndegrading its search and planning abilities. Therefore, it is crucial to explore methods for effectively\nintegrating optimal solutions to produce trajectories that maintain both high likelihood and quality."}, {"title": "3.2 GUIDED STREAM OF SEARCH", "content": "In this subsection, we introduce guided stream of search (GSoS), a supervised fine-tuning approach\nthat seamlessly incorporates optimal solutions into the self-generation process and effectively distills\nthem into the model. The key idea of GSoS is to leverage an unsuccessful search trajectory as context\nfor each intermediate step of the optimal solution. This approach effectively mimics how the model\ndiscovers the solution through the search procedure, producing a trajectory that has a high likelihood\nunder the SoS model. Moreover, by providing an exploratory context for reaching each intermediate\nstep, it facilitates distilling the optimal solutions into the model (Yang et al., 2022).\nBefore delving into our approach, we establish some notations. We define subgoal nodes as the non-\nleaf nodes along the optimal path in the search tree. An optimal solution consisting of $N$ operations\ncontains $N$-1 subgoal nodes. We define the generation and exploration lines of a node as sequences\nof tokens that represent the primitive operations for generating and exploring the node, respectively."}, {"title": "4 EXPERIMENTS", "content": null}, {"title": "4.1 EXPERIMENT SETUP", "content": "Throughout all experiments, we use a GPT-2 architecture with 250M parameters and a context length\nof 4096 as the base language model (Radford et al., 2019). Additionally, we utilize FlashAttention-2\nfor faster training and inference (Dao, 2024). Although Gandhi et al. (2024) employ GPT-Neo as the\nbase architecture (Gao et al., 2020), we find that using GPT-2 instead results in better performance.\nSee Appendix E.2 for a more detailed comparison."}, {"title": "4.2 RESULTS", "content": "Figure 4 presents the test accuracy of each method for both seen and unseen targets. Our approaches\noutperform the symbolic method and SoS by a substantial margin. Specifically, GSOS+PPO achieves\n75% accuracy on seen targets and 73% accuracy on unseen targets, yielding an average gain of over\n16% compared to both methods. Furthermore, our approaches significantly outperform the baseline\nfine-tuning methods. For supervised fine-tuning, GSoS shows a gain of 7% compared to SoS+STaR.\nFor RL fine-tuning, GSOS+PPO exhibits a gain of 9% compared to SoS+PPO. It is worth noting that\nRL fine-tuning further enhances the performance of GSoS by an additional 6%."}, {"title": "4.3 ANALYSIS OF GSOS NODE SELECTION STRATEGIES", "content": "Figure 5 shows the test accuracy of GSoS with different node selection strategies. The random node\nselection strategy achieves the highest accuracy for both target types, outperforming the second-best\nstrategy by 2%. To investigate the effectiveness of random node selection, we analyze the statistics\nof trajectories generated by each strategy. Figure 7 displays the distributions of cross-entropy losses\nfor successful GSoS trajectories at the final iteration, where the losses are calculated on the base SoS\nmodel. Strategies with longer contexts generate trajectories with higher likelihoods under the model\ncompared to shorter contexts. However, this benefit comes at the cost of lower quality. As shown in\nFigure 6, the ratio of successful trajectories decreases as the context length increases. One potential\nreason is that longer contexts reduce the number of tokens available for the model to generate within\na given context limit, thereby restricting its opportunities to search. This explanation is supported by\nthe fact that many of the successful trajectories generated by the last node selection strategy already\napproach the context length limit, as presented in Figure 7. In summary, there is an inherent trade-off"}, {"title": "4.4 ANALYSIS OF OPERATION-LEVEL PPO", "content": "To examine the effectiveness of the operation-level action space for RL fine-tuning, we fine-tune the\nGSOS model using PPO in the token-level MDP and compare their performance. Figure 8 shows the\ntest accuracy and value predictions for initial states of GSOS+PPO in the token-level and operation-\nlevel MDPs. Notably, the operation-level approach achieves an accuracy gain of over 2% compared\nto the token-level approach for both target types. Moreover, the operation-level value function learns\nat a significantly faster rate than the token-level value-function, highlighting the benefits of reducing\nthe effective horizon for training the value function."}, {"title": "4.5 ANALYSIS OF RL FINE-TUNING ON STAR", "content": "In Section 4.2, we find that RL fine-tuning further enhances the performance of GSOS. This raises the\nquestion of whether RL fine-tuning might also benefit SoS+STaR. To explore this, we fine-tune the\nSoS+STaR model using PPO and compare their performance. Figure 9 shows the test accuracy and\nKL divergence from the reference policy of SoS+PPO with and without STaR. While applying STaR\nprior to RL fine-tuning leads to lower KL divergence and improved training stability, its performance\nremains similar. In summary, STaR does not provide additional benefits beyond RL fine-tuning. This\nhighlights the importance of optimal solutions in enhancing performance."}, {"title": "4.6 ANALYSIS OF RL FINE-TUNING WITH SUBGOAL REWARD", "content": "Another approach to distilling the information from optimal solutions into the SoS model is to apply\nRL fine-tuning with a subgoal reward. Specifically, we train the model using PPO with a new reward\nfunction that combines the original reward and a subgoal reward, which is defined as:\n$R_{\\text{new}}(s_t, a_t) = R(s_t, a_t) + \\eta \\cdot R_{\\text{sub}} (s_t, a_t), \\quad R_{\\text{sub}}(s_t, a_t) = \\begin{cases}\n1 & \\text{if } (s_t, a_t) \\text{ explores a subgoal}\n\\\\ 0 & \\text{otherwise},\n\\end{cases}$\nwhere $\\eta > 0$ controls the influence of the subgoal reward. We set $\\eta$ to 0.2 to ensure that the subgoal\nreward does not dominate the original reward. To avoid exploitation, the subgoal reward is provided"}, {"title": "5 RELATED WORKS", "content": "Searching with language models Yang et al. (2022) introduce the idea of using search procedure\nfor sequential decision making. They train a neural network to imitate not only actions generated by\nan expert MCTS policy but also the search procedures involved in determining the actions. However,\ntheir approach primarily focuses on utilizing search procedures to enhance imitation learning rather\nthan improving the search and planning capabilities of the model. Recent studies focus on enhancing\nthese capabilities using language models. Lehnert et al. (2024) train a T5 model to imitate A* search\ntraces and optimize it to generate shorter traces through supervised fine-tuning (Raffel et al., 2020).\nGandhi et al. (2024) train a GPT-Neo model to imitate DFS and BFS search trajectories and improve\nits performance through supervised or RL fine-tuning. Our work differs from these prior studies in\nthat it utilizes optimal solutions throughout the self-data generation process rather than relying solely\non fully self-generated data.\nFine-tuning on self-generated data Anthony et al. (2017); Silver et al. (2017) introduce the idea\nof iteratively distilling self-generated data into neural networks to enhance their performance. They\ngenerate high-quality trajectories using an expert MCTS policy and imitate them to improve a neural\nnetwork-based policy. However, their work is restricted to small-scale convolutional neural networks\nand the narrow domain of gaming. Recent studies extend this idea to fine-tune language models for\nvarious downstream tasks, including theorem proving, question answering, and machine translation\n(Polu et al., 2022; Zelikman et al., 2022; Gulcehre et al., 2023). While Zelikman et al. (2022) utilize\noptimal solutions as prompts to generate higher-quality data, our approach differs by first generating\ntrajectories without any prior knowledge of optimal solutions and then integrating optimal solutions\ninto them. Furthermore, we incorporate each intermediate subgoal of optimal solutions step by step,\nin the same spirit as Lightman et al. (2023).\nRL fine-tuning with higher-level MDP Verma et al. (2022); Zhou et al. (2024) address the long-\nhorizon problem in multi-turn conversation tasks by defining each single-turn utterance as an action.\nHowever, their work focuses exclusively on off-policy RL algorithms. Furthermore, their approaches\nrequire an additional value network for baseline estimation and a target network to improve training\nstability. Our work differs from these prior studies by successfully extending the idea of higher-level\nMDP to on-policy RL algorithms without introducing any additional networks. Although Ahmadian\net al. (2024) explore sequence-level optimization using REINFORCE (Williams, 1992), their focus\nis limited to tasks with short horizons, where the average generation length is under 100."}, {"title": "6 CONCLUSION", "content": "In this work, we explore the role of optimal solutions in enhancing the search and planning abilities\nof language models. We identify that optimal solutions can serve as step-by-step subgoals, providing\nvaluable guidance to models pre-trained on search trajectories. We propose guided stream of search\n(GSOS), which seamlessly integrates optimal solutions into the self-generation process step by step,\nproducing high-quality trajectories for supervised fine-tuning. Our method achieves state-of-the-art\nperformance on the challenging Countdown benchmark. Furthermore, we find that GSoS possesses\nthe intriguing property of working in tandem with RL fine-tuning, which is not observed in standard\nsupervised fine-tuning.\nOur work is not without limitations. Transformer models trained on search trajectories face inherent\nchallenges as they require long context lengths. Given their quadratic memory and time complexity,\nthis significantly raises the computational demands for both training and inference. This issue can be\nfurther exacerbated as tasks grow more complex and require even longer context lengths. Exploring\nthe search and planning capabilities of more efficient language models, such as Mamba (Gu & Dao,\n2023), is a promising direction for future work. Alternatively, training a model to implicitly perform\nsearch through knowledge distillation also presents a valuable approach (Deng et al., 2023)."}, {"title": "A STREAM OF SEARCH", "content": "SoS trains a language model to predict search trajectories generated by symbolic search algorithms\n(Gandhi et al., 2024). For each problem with the input state I and the output state O, its search tree is\nstructured with the root node representing the input state, non-root nodes representing intermediate\nstates, and edges representing operations between nodes. A leaf node is reached when no additional\noperations can be applied. Formally, each nodes and edge in the tree are defined as follows:\n*   **Node:** Each node $v_a$ at depth d, where $a = (i_1, i_2, ..., i_d)$, represents a sequence of $d - 1$\n    operations and the resulting intermediate states, along with the output state.\n*   **Edge:** Each edge $e_{v,v'}$ represents an operation that maps the node v to the node $v'$.\nSoS defines the following primitive operations for symbolic search algorithms in langauge:\n*   **Generation:** Given a node $v_a$ at depth $d$, this action generates the $i_{d+1}$-th child node $v_{a'}$\n    at depth $d + 1$ by applying an operation, where $a' = (a, i_{d+1})$.\n*   **Exploration:** Given a node $v_a$ at depth $d$, this action transitions to the previously generated\n    $i_{d+1}$-th child node $v_{a'}$ at depth $d + 1$, where $a' = (a, i_{d+1})$.\n*   **Backtracking:** Given a node $v_a$ at depth $d$, if exploring its child nodes is unlikely to yield\n    a solution, this action transitions to a previously generated node $v_{a'}$ at depth $d'$.\n*   **Verification:** Given a leaf node $v_a$ at depth $D$, where no further operations can be applied,\n    this action verifies whether the node represents the solution.\nFinally, it generates search trajectories using diverse symbolic search algorithms. Since all primitive\noperations are expressed in language, search trajectories are also represented in language. Figure 10\nillustrates an example of a search tree and its corresponding search trajectory. Note that an optimal\nsolution can be represented as a path in a search tree.\nOne major challenge in training language models on these search trajectories is their context length.\nSymbolic search algorithms can generate excessively long search trajectories via exhaustive search.\nHowever, language models, such as Transformers (Vaswani et al., 2017), have a fixed context length,\npreventing them from capturing or learning from sequences beyond this length. To address this, SoS\nuses heuristic-guided search algorithms to generate shorter search trajectories, albeit with reduction\nin performance. See Appendix E.1 for more details on these algorithms."}, {"title": "B COUNTDOWN BENCHMARK", "content": "Each problem begins with $K$ input numbers and a target number, all of which are integers. The input\nnumbers are either 1-digit or 2-digit, whereas the target number is 2-digit. Arithmetic operations are"}, {"title": "C GUIDED STREAM OF SEARCH", "content": "Figure 12 illustrates a trajectory generated by subgoal augmentation in GSOS on Countdown. Given\nthe root node, the child node of index (0, 1) is selected and replaced with the second subgoal node.\nThis involves modifying the operation and resulting numbers within the generation and exploration\nlines of the child node to match those of the subgoal node. Finally, all lines beyond the exploration\nline are truncated. The resulting trajectory is fed into the model to restart the search from the subgoal\nnode. This process is repeated if the model fails to explore the next subgoal."}, {"title": "D OPERATION-LEVEL RL FINE-TUNING", "content": "Before delving into operation-level RL fine-tuning, we first explain how RL fine-tuning is performed\nusing PPO in the token-level MDP. Starting from the initial state $s_0$, the policy generates a trajectory\n$(s_0, a_0, s_1, a_1, ..., s_H)$ up to the horizon H by sampling a token as an action and appending it to the\ncurrent state at each timestep. The policy $\\pi_\\theta$ and value function $V_\\phi$ are then trained on this trajectory\nusing the following objectives:\n$\\max_\\theta \\mathbb{E}_{s_0 \\sim D, (s_h, a_h) \\sim \\pi_\\theta} \\min \\left( \\frac{\\pi_\\theta(a_h | s_h)}{\\pi_{\\theta_{\\text{old}}}(a_h | s_h)}, \\text{clip} \\left( \\frac{\\pi_\\theta(a_h | s_h)}{\\pi_{\\theta_{\\text{old}}}(a_h | s_h)}, 1-\\epsilon, 1+\\epsilon \\right) \\hat{A}_h \\right) (2)$\n$\\min_\\phi \\mathbb{E}_{s_0 \\sim D, (s_h, a_h) \\sim \\pi_\\theta} \\frac{1}{2} \\left( V_\\phi(s_h) - (A_h + V_{\\phi_{\\text{old}}}(s_h)) \\right)^2 (3)$\nHere, $\\pi_{\\theta_{\\text{old}}}$ and $V_{\\phi_{\\text{old}}}$ are the policy and value function immediately prior to the update, and $\\hat{A}$ is the\nadvantage compute by GAE, which is defined as\n$\\hat{A}_h = \\sum_{h'=h}^{H} (\\gamma \\lambda)^{h'-h} \\delta_{h'}, \\quad \\delta_h = r_h + V_\\phi(s_{h+1}) - V_\\phi(s_h). (4)$\nIn the operation-level MDP, each action $a_h$ is defined as a sequence of tokens $(a_{h,1}, a_{h,2}, ..., a_{h,T})$,\nwith the last token being a newline token. Consequently, each state $s_h$ is defined as the concatenation\nof the initial state and previously generated actions, with the last token being a newline token. In this\nMDP, calculating the advantage and multi-step return is straightforward: simply forward the state to\nthe value function and apply Equation (4). The action probability is calculated by factorizing it over\nthe sequence of tokens:\n$\\pi_\\theta(a_h | s_h) = \\prod_{t=1}^{T} \\pi_\\theta(a_{h,t} | a_{h,t-1}, ..., a_{h,1}, s_h).$\nFinally, the policy and value function are trained to optimize Equations (2) and (3)."}, {"title": "E IMPLEMENTATION DETAILS", "content": null}, {"title": "E.1 DATASET", "content": "We construct the dataset following the procedure outlined in Gandhi et al. (2024). First, we split the\nset of target numbers into 90% for training and 10% for testing. We create 500,000 training problems"}, {"title": "E.2 UNSUPERVISED PRE-TRAINING", "content": "For unsupervised pre-training, we use the official code provided by Gandhi et al. (2024). However,\nwe find that replacing the architecture from GPT-Neo to GPT-2, while maintaining the same number\nof parameters, improves performance in both validation loss and test accuracy, as shown in Figure 15\nand Table 3. Therefore, we choose GPT-2 over GPT-Neo as the base architecture. The configuration\nfor GPT-2 is provided in Table 4. We use the same hyperparameter settings as in the original paper,\nwith the specific values provided in Table 5."}, {"title": "E.3 SUPERVISED FINE-TUNING", "content": "For supervised fine-tuning, we also use the official code provided by Gandhi et al. (2024). We keep\nthe same hyperparameter settings for data generation and training as in the original paper, with the"}, {"title": "E.4 RL FINE-TUNING", "content": "For RL fine-tuning, we implement PPO based on TRIL (Chang et al., 2023), a library that supports\ndistributed RL training with transformers using Hugging Face Accelerate and Microsoft Deepspeed\n(Gugger et al., 2022; Rasley et al., 2020). However, we identify several critical issues in the original\nimplementation and make the following modifications:\n*   The original code always passes a gradient accumulation step of 1 to the Deepspeed plugin,\n    ignoring the intended setting. We modify the code to pass the correct value.\n*   The original code uses separate networks for the policy and value function but updates both\n    networks simultaneously using a single optimizer. This hinders the gradient clipping, as the\n    larger gradient overshadows the smaller gradient. We modify the code to employ separate\n    optimizers, preventing the interference between the networks during the gradient clipping."}, {"title": "E.5 COMPUTATIONAL RESOURCES", "content": "We conduct all experiments on an internal HPC cluster, with each node consisting of 2 AMD EPYC\n7402 CPUs and 750GB of RAM, using the PyTorch deep learning framework (Paszke et al., 2019).\nFor supervised pre-training and fine-tuning, we utilize 4 NVIDIA A100 GPUs (80GB VRAM each).\nFor RL fine-tuning, we utilize 4 NVIDIA RTX 3090 GPUs (24GB VRAM each). For inference, we\nutilize a single NVIDIA RTX 3090 GPU."}, {"title": "F COMPARISON OF ITERATIVE APA WITH PPO", "content": "Gandhi et al. (2024) adopt iterative APA for RL fine-tuning instead of PPO. In this approach, a policy\nis fine-tuned using APA over multiple iterations, with the best-performing policy from each iteration\nserving as the reference for the subsequent iteration. However, this method can be cumbersome, as\nit requires periodically saving and evaluating checkpoints to determine the best-performing policy.\nWe observe that a single iteration of PPO achieves better performance than iterative APA with fewer\nrollouts, as shown in Table 9. Notably, SoS+PPO in the operation-level MDP outperforms iterative\nAPA, with an accuracy gain of 4% for both seen and unseen targets. For a fair comparison, we train\nthe SoS model using the same dataset and architecture as in the original paper. Therefore, we choose\noperation-level PPO over iterative APA as the base RL algorithm."}]}