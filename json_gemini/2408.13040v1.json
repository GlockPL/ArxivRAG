{"title": "SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks", "authors": ["Kai-Wei Chang", "Haibin Wu", "Yu-Kai Wang", "Yuan-Kuei Wu", "Hua Shen", "Wei-Cheng Tseng", "Iu-thing Kang", "Shang-Wen Li", "Hung-yi Lee"], "abstract": "Prompting has become a practical method for uti- lizing pre-trained language models (LMs). This approach offers several advantages. It allows an LM to adapt to new tasks with minimal training and parameter updates, thus achieving efficiency in both storage and computation. Additionally, prompt- ing modifies only the LM's inputs and harnesses the generative capabilities of language models to address various downstream tasks in a unified manner. This significantly reduces the need for human labor in designing task-specific models. These advantages become even more evident as the number of tasks served by the LM scales up. Motivated by the strengths of prompting, we are the first to explore the potential of prompting speech LMs in the domain of speech processing. Recently, there has been a growing interest in converting speech into discrete units for language modeling. Our pioneer research demonstrates that these quantized speech units are highly versatile within our unified prompting framework. Not only can they serve as class labels, but they also contain rich phonetic information that can be re- synthesized back into speech signals for speech generation tasks. Specifically, we reformulate speech processing tasks into speech- to-unit generation tasks. As a result, we can seamlessly integrate tasks such as speech classification, sequence generation, and speech generation within a single, unified prompting framework. The experiment results show that the prompting method can achieve competitive performance compared to the strong fine- tuning method based on self-supervised learning models with a similar number of trainable parameters. The prompting method also shows promising results in the few-shot setting. Moreover, with the advanced speech LMs coming into the stage, the proposed prompting framework attains great potential.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently, self-supervised representation learning has be- come an essential component in the speech processing field [1]. The speech representation model is trained on a large- scale unlabeled corpus in a self-supervised learning (SSL) manner. The learned representation has been demonstrated to be informative and can benefit a wide range of speech processing tasks [2]\u2013[4]. When leveraging these speech representation models for a downstream task of interest, a typical approach is to follow the \"pre-train, fine-tune\" paradigm [1], [5]. Under this paradigm, the representation models serve as feature extractors. The models encode speech into informative representations, which are subsequently fed into a task-specific model. This model, referred to as the expert downstream model, specializes in solving a specific speech processing task. While fine-tuning often yields optimal performance, this paradigm, as depicted in Fig. 1, requires delicately designing a task-specific down- stream model and loss function for each task. This complexity significantly causes an increasing burden of human labor. Furthermore, the requirement to train the expert downstream model alongside the optionally fine-tuned speech represen- tation model leads to substantial computational and storage demands. This is especially challenging as the number of downstream tasks grows due to the necessity to store separate model parameters for each task. On the other hand, researchers have explored the \"prompt- ing paradigm\" [5] as an alternative method to leverage pre- trained language models (LMs) to solve downstream tasks in an efficient manner. Originating from the Natural Language Processing (NLP) field, prompting refers to the technique that finds a task-specific template or instruction, which is called prompt, to steer a pre-trained LM without modifying"}, {"title": "II. RELATED WORKS", "content": "A. Self-supervised Speech Representation and Discretization The exploration of speech representations through Self- Supervised Learning (SSL) objectives has evolved into a crucial research topic within the speech research area in recent years. By utilizing different SSL pre-training tasks, the repre- sentation models can mainly be grouped into three categories: predictive models [29], [30], contrastive models [31]\u2013[33], and generative models [34]\u2013[36]. To leverage SSL representations, a common way is to build specialized downstream models on top of SSL representations and fine-tune the entire model or only the downstream models for supervised downstream tasks. Based on this, SUPERB [2] benchmarks SSL speech models with a wide variety of downstream tasks. Although using continuous SSL representations as features for downstream tasks can yield stronger performance [37], there's a growing trend of adopting discrete speech units derived by quantizing the SSL representations [22], [38]. A common approach involves applying the K-means algorithm to the SSL representations, quantizing them into clusters. Dis- crete units significantly reduce storage space and transmission bandwidth compared to raw waveforms and SSL features [22], [37]. For instance, as discussed in [22] and shown in Table II, a T-second 16kHz waveform in 16-bit format requires 16 \u00d7 16, 000 \u00d7 T bits for storage and transmission. In contrast,"}, {"title": "B. Textless Speech Language Models", "content": "Textless speech LMs regard discrete speech units as pseudo- text and adopt them as LM's vocabulary. Leveraging these discrete units, speech LMs are trained to perform language modeling tasks that mirror those in the NLP field. As shown in Fig. 2, in the textless speech language model, there are three components: (1) speech-to-unit encoder, (2) unit language model, and (3) unit-to-speech decoder. Speech- to-unit encoder comprises an SSL representation model, such as HuBERT [29], paired with a quantizer, like K-means. The continuous representation extracted by the SSL model is clustered into discrete units. These discrete units have shown to encapsulate rich phonetic and linguistic information, thereby effectively representing speech [18], [21]. In conven- tional speech language models, these discrete units undergo a deduplication process, which removes consecutive repeated units to form a more compact sequence of tokens for language modeling. The unit language model is an LM that performs generative language modeling based on the discrete units. For instance, in GSLM [18], the unit language model conducts the next-token-prediction task akin to GPTs [24], [39]. Unit mBART performs the denoising sequence reconstruction task similar to the BART model [25]. The unit-to-speech decoder"}, {"title": "C. Prompting and Reprogramming in Speech Processing", "content": "This journal paper is an extension of our previous work [16], where we explored the concept of prompting on speech LM, particularly GSLM. Previous work [16] showed promising results in speech classification tasks such as spoken com- mand recognition and intent classification and demonstrated better parameter efficiency compared to the \"pre-train, fine- tune\" paradigm. However, despite achieving notable results in sequence generation tasks like ASR and slot filling, its performance still lags behind the fine-tuning method. In this paper, we further explore an advanced encoder-decoder speech LM, Unit mBART, across a broader range of speech processing tasks. This includes a more diverse set of speech classifica- tion tasks, as well as speech generation tasks. The results are more promising: (1) Prompting Unit mBART achieves competitive performance in sequence generation tasks and (2) Prompting Unit mBART is well-suited for speech generation tasks, thereby establishing a unified prompting framework for various speech processing tasks. Additionally, compared to our previous work, we introduce a learnable verbalizer in this paper to bridge the gap between discrete units and downstream task labels, enhancing both explainability and performance. WavPrompt [45] is also a pioneer in studying the prompting paradigm in speech processing. WavPrompt consists of a text LM, GPT-2 [39], and an audio encoder, wav2vec 2.0 [32]. The text LM is prompted with audio embeddings and text questions to perform few-shot speech understanding tasks. In contrast to SpeechPrompt, which uses textless speech LM for various speech processing tasks, WavPrompt employs a text LM and performs limited speech understanding tasks. On the other hand, the work [46] studies hand-crafted prompts for a speech recognition model, Whisper [47], for var- ious speech recognition tasks. The backbone model, Whisper, is trained using large-scale speech-text paired data. In contrast, our work prompts a textless speech LM, and we not only focus on speech recognition, a type of sequence generation task, but also explore speech generation tasks. Another branch of utilizing a pre-trained model's capability for different tasks is model reprogramming [48], [49]. In [50], [51], the input data (target domain) are first transformed with a task-specific function to become the reprogrammed data. The pre-trained acoustic model is then capable of generating labels for this reprogrammed data. These labels (source domain) are then mapped to the classes of downstream tasks (target domain) by a mapping function. This mapping function serves the same role as the verbalizer in the prompting method and is usually a random mapping in the reprogramming literature. We also adopt the idea of reprogramming a foundation model for solving various tasks. For example, in speech classifica- tion tasks and sequence generation tasks, the speech LM is prompted/reprogrammed to adapt to the distribution of the target domain (the class label and the transcription)."}, {"title": "III. METHOD", "content": "The overview of the proposed framework is depicted in Fig. 3. The input speech waveform is encoded into a se- quence of discrete units using an SSL speech model and a quantizer. The unit LM (Section III-A) then takes this unit sequence and performs conditional generation based on the task-specific prompts. The design of task-specific prompts will be illustrated in Section III-B. The prompts steer the unit LM to solve the downstream speech processing task, which is reformulated into a speech-to-unit generation task as discussed in Section III-C. The resulting unit sequence is transformed into the downstream task's target through a verbalizer (for speech classification and sequence generation tasks) or through a pre-trained speech decoder (for speech generation tasks) as discussed in Section III-D. The notations used in the section are listed in Table III."}, {"title": "A. Unit Language Models", "content": "This subsection explains the backbone unit language model in our prompt framework. As shown in Fig. 4, these unit LMs receive discretized speech units sequence $u^x$ and trainable prompts $p$ as inputs, subsequently using them to generate target unit sequence $u^y$ for downstream speech processing tasks. Without loss of generality, in this paper, we investigate two variants of widely-adopted unit LMs based on Transform- ers [52]: (1) The decoder-only unit LM that mimics the GPT architecture [24], and (2) The encoder-decoder unit LM that mirrors the BART language model [25]. Both model types em- ploy a causal decoder and are characterized as autoregressive LMs, enabling the capability to generate outputs of varying lengths. Specifically, the probability of each unit $u_j \\in \\mathcal{V}$ generated by the model at the timestep t is conditioned on the preceding context, denoted by $C_t$. The context $C_t$ includes the input discretized source speech $u^x_t$, the task prompts $p$, and the units $u^y_t$ generated preceding the timestep $t$ in the autoregressive process. Formally, the autoregressive model generates the probability of a unit $u_j$ within a vocabulary $\\mathcal{V} = \\{u_1, u_2, ..., u_{|\\mathcal{V}|}\\}$ at timestep $t$ given the context $C_t$ as:\n$P(u_j | C_t) = \\frac{e^{z_{tj}}}{\\sum_{k=1}^{|\\mathcal{V}|} e^{z_{tk}}}$\nwhere $z_{tj} \\in \\mathbb{R}^{|\\mathcal{V}| \\times 1}$ is the logit for the j-th unit at timestep t, and the denominator is the sum of exponentiated logits for all units at that timestep."}, {"title": "1) Encoder-Decoder Unit LM", "content": "The encoder-decoder unit LM includes the encoder $\\mathcal{E}$ and decoder $\\mathcal{D}$ based on Trans- former. The discretized speech is first processed by the encoder $\\mathcal{E}$ to form part of the enriched context that the decoder $\\mathcal{D}$ performs cross-attention on to guide the generation of the discrete units. The encoder $\\mathcal{E}$ is composed of multiple layers that process the input unit sequence:\n$g^{(1)} = [e(u_1^x), e(u_2^x), ..., e(u_T^x)],$\nwhere T is the sequence length and $e(\\cdot): \\mathbb{Z} \\rightarrow \\mathbb{R}^d$ denotes the vocabulary embedding table, which transforms a discrete unit $u \\in \\mathbb{Z}$ into its corresponding embedding vector $e(u) \\in \\mathbb{R}^d$, and d is the embedding dimension. In the encoder, the i-th layer receives hidden representation $g^{(i)} = [g^{(i)}_1, g^{(i)}_2, ..., g^{(i)}_T] \\in \\mathbb{R}^{T \\times d}$ as input and outputs $g^{(i+1)}$. The decoder layers operate similarly, with each taking input $h^{(i)} = [h^{(i)}_1, h^{(i)}_2, ..., h^{(i)}_{T'}] \\in \\mathbb{R}^{T' \\times d}$, and outputs $h^{(i+1)}$, where T' represents the decoder sequence length, which increases incrementally during the autoregres- sive process."}, {"title": "2) Decoder-only Unit LM", "content": "In the decoder-only LM, the model lacks the encoder and relies solely on the decoder $\\mathcal{D}$, which functions in an analogous fashion to the encoder- decoder setup but without the encoder's guidance. Without the encoder, the discretized source speech $u^x_t$ is integrated at the beginning of the sequence, serving as the initial context for the decoder to predict the subsequent units. A separation token $\\langle sep \\rangle$ is inserted in between the source unit sequence $u^x$ and the generated units $u^y$. Therefore, for each timestep $t$ in the autoregressive process, the input to the decoder $\\mathcal{D}$ is:\n$h^{(1)} = [e(u^x), e(\\langle sep \\rangle), e(u_1^y), ..., e(u_t^y)].$"}, {"title": "B. Prompt Tuning", "content": "As depicted in Fig. 3, the speech LM is capable of per- forming predefined speech tasks when provided with various types of prompts. In this subsection, we will elaborate on the process of prompt design. Prompting employs task-specific templates, known as prompts, to steer the generation process of the LM. This tech- nique involves freezing the LM's parameters while integrating"}, {"title": "1) Input Prompt Tuning", "content": "Inspired by the method in [14], input prompt tuning prepends continuous prompt vectors at the LM's input. Specifically, the prompts are prepended at the embedding sequence of the first layer's input $h^{(1)}$ (and $g^{(1)}$ for Encodoer-Decoder model):\n$h^{(1)} \\leftarrow Concat(p^1, h^{(1)}),$\n$g^{(1)} \\leftarrow Concat(p^1, g^{(1)}),$\nwhere $p^1 = [p_1, p_2, ..., p_l]$ represents a series of prompt vectors $p_i \\in \\mathbb{R}^d$ at the input of the unit LM, with l indicating the prompt length."}, {"title": "2) Deep Prompt Tuning", "content": "Inspired by prefix-tuning [12], deep prompt tuning involves concatenating prompt vectors at the input of the Transformer layer. Specifically, it modifies the input of the attention modules to guide the forward process of the LM. The self-attention module at the beginning of each transformer layer takes the Query (Q), Key (K), and Value (V) as input:\n$Attn(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V,$\nwhere $\\sqrt{d_k}$, the square root of the dimensionality of the key vectors, scales the dot product to ensure normalization of the attention weights by the softmax function. For self-attention, the matrices Q, K, and V are projections of the same input $g$ or h transformed by the weight matrices $W_Q$, $W_K$, and $W_V$, respectively. Trainable prompt vectors are prepended to the input of each transformer layer, affecting both Key (K) and Value (V) matrices in the attention mechanism:\n$K \\leftarrow Concat(p^K, h)W_K,$\n$V \\leftarrow Concat(p^V, h)W_V,$\nwhere $p^K = [p^K_1, p^K_2, ..., p^K_l]$ and $p^V = [p^V_1, p^V_2, ..., p^V_l]$ are series of trainable prompt vectors for key and value, respectively, and has the same prompt length l as $p^1$. Similar adjustments are applied to the encoder's represen- tation g for encoder-decoder unit LM. It is crucial to note that throughout the prompt tuning process, only the prompt vectors are trainable. The embedding table and the unit LM remain fixed."}, {"title": "C. Speech-to-Unit Generation", "content": "In this paper, we focus on leveraging the generative ca- pabilities of autoregressive speech LMs to handle various downstream tasks. Specifically, we recast speech processing tasks, including speech classification, sequence generation, and speech generation, into a unified speech-to-unit generation task. In this approach, speech LM takes discretized speech as input and generates a sequence of discrete units corresponding to the intended output for the task at hand. In sequence generation tasks, like automatic speech recog- nition (ASR), the model generates a unit sequence $u^y$ ="}, {"title": "D. Verbalizer and Speech Decoder", "content": "Within the prompting paradigm, the verbalizer [8], [9] $v(\\cdot)$ is a label-mapping module, which establishes the connection between the downstream task labels and the LM's vocabu- lary. For speech LM, the vocabulary is the discrete units. The verbalizer can adopt various forms, including random mapping [51], [53], [54] and heuristic methods [16], we refer to this as \"fixed verbalizer\" since the mapping is pre-defined and does not include updates. On the other hand, to generate speech signal, a speech decoder is employed to synthesize waveform from discrete unit sequence."}, {"title": "1) Fixed Verbalizer", "content": "The fixed verbalizer establishes a static mapping between the downstream task label and a unique unit. For example, in the ASR task, it might map the character \u201ca\u201d to \u201cunit 28\u201d and \u201cb\u201d to \u201cunit 72.\u201d In spoken command recognition (SCR), it could map the command \u201c[UP]\u201d to \u201cunit 65,\u201d following either a random mapping or a frequency-based approach [16]. Once established, this mapping remains static without further learning or adaptation. In practice, with a fixed verbalizer, the most probable unit at each timestep t is selected and directly converted to the downstream task's label $y_t$."}, {"title": "2) Speech Decoder", "content": "For speech generation tasks, where the target output is a speech signal rather than a sequence of labels, the discrete units can be synthesized back into speech signals using a pre-trained, off-the-shelf unit-to-speech decoder. This speech decoder is self-supervised and trained with pairs of discrete units and their corresponding speech. In this work, we employ a speech decoder that corresponds to the given unit LM, as illustrated in Fig. 2."}, {"title": "E. Learnable Verbalizer", "content": "Fixed verbalizers can lead to subpar performance in speech processing tasks because, unlike the distinct semantic meaning present in NLP vocabulary, the vocabulary of discrete speech units lacks clear semantic meanings. To address this, we"}, {"title": "IV. EXPERIMENTAL SETUP", "content": "In this work, we compare the \"pre-train, fine-tune\" paradigm with the prompting paradigm for speech processing across three types of tasks: (1) speech classification tasks, (2) se- quence generation tasks, and (3) speech generation tasks. The used dataset and the basic statistics are presented in Table IV."}, {"title": "A. Tasks and Datasets", "content": "1) Speech Classification Tasks: Speech Command Recognition (SCR): The task is to rec- ognize which keyword is presented in a given utterance. We"}, {"title": "B. Model and Training Setup", "content": "We compare the \"pre-train, fine-tune\" paradigm with the prompting paradigm to assess whether prompting can achieve competitive performance while also providing parameter effi- ciency and other associated benefits as discussed in Table. I. 1) Prompting Paradigm: We explore two types of speech LMs within the prompting paradigm: the decoder-only Gener- ative Spoken Language Model (GSLM) [18] and the encoder- decoder model Unit mBART [20]. GSLM is pre-trained using a next token prediction task on discrete units obtained by quantizing the 6-th layer of HuBERT representations into 100 clusters. The GSLM paper [18] considered different settings, including various SSL models and cluster numbers. This set- ting is selected for its superior performance. GSLM consists of 12 Transformer-decoder layers, each with 16 attention heads, an embedding size of 1024, and a feedforward network (FFN) size of 4096, totaling 150 million parameters. It is pre-trained on HUBERT discrete units derived from a clean 6k-hour sub- sample of the Libri-Light dataset [72]. On the other hand, Unit mBART is pre-trained on a multilingual denoising task using discrete units derived from quantizing the 11-th layer of mHuBERT representations into 1,000 clusters. Unit mBART includes 12 Transformer-encoder layers and 12 Transformer- decoder layers, each with an embedding size of 1024, an FFN dimension of 4096, and 16 attention heads, totaling 353 million parameters. The embedding tables of the encoder and decoder share the same parameters. Unit mBART is pre- trained on mHuBERT discrete units obtained from VoxPopuli with 16k hours for Spanish, 14k hours for English, and Libri- Light with 60k hours for English."}, {"title": "V. RESULTS", "content": "A. Main Results 1) Speech Classification Tasks: The comparison of the prompting paradigm (PT) and the \u201cpre-train, fine-tune\" paradigm (FT) for the speech classification tasks are shown in Table V. Our results indicate that the prompting method generally delivers competitive performance and often outper- forms the fine-tuning approach. Specifically, for HuBERT and GSLM models, prompting outperforms fine-tuning in 6 out of 10 datasets (AR-SC, LT-SC, DM-SC, Grabo-SC, Fluent- SC, and Mustard++). For mHuBERT and mBART, prompting excels in 8 out of 10 datasets, including all datasets under SCR (with LT-SC achieving identical performance), IC, SD, and LID. For the few tasks where prompting is slightly outperformed by fine-tuning in HuBERT and GSLM settings (Google-SC, and GFSound), the performance gap is minimal, within a"}, {"title": "2) Sequence Generation Tasks", "content": "The experiment results of sequence generation tasks are shown in Table VI. In sequence generation tasks, we observe that although prompting the decoder-only model GSLM can yield non-trivial results, it still underperforms compared to the fine-tuning paradigm by a substantial margin. The reasons are discussed in previous work [16], including that quantizing speech into discrete units results in longer sequences, which might be difficult for a decoder model to handle. In our preliminary study, even utilizing a learnable verbalizer for prompting GSLM does not show performance improvement. On the other hand, surprisingly, prompting an encoder- decoder model like Unit mBART can achieve competitive performance, outperforming the fine-tuning paradigm in most scenarios, except for the metric CER in ASR, which falls behind by 0.7. Furthermore, we can observe the effectiveness of introducing a learnable verbalizer in Unit mBART. For every metric other than the F1 score in Slot Filling (SF), there is a substantial improvement when comparing Unit mBARTlearn with Unit mBART fixed. The analysis of the learn- able verbalizer will be discussed in Section V-C. From GSLM to Unit mBART, the speech LM becomes better, and tasks that previously yielded poor results with GSLM can now yield favorable outcomes with Unit mBART. We anticipate that in the future, with more advanced speech LMs emerging, further performance improvement can be seen with the proposed prompting framework."}, {"title": "3) Speech Generation Tasks", "content": "In speech generation tasks, we focus on two tasks: Speech Translation (ST) and Speech Continuation (SC). Our experiments show the effectiveness of prompting Unit mBART for speech translation, as detailed in Table VIII. Speech-to-speech translation poses significant challenges, often requiring incorporating auxiliary tasks [73], [74] or adopting an advanced speech LM [20]. Our results also support this observation: neither the prompting GSLM baseline nor the fine-tuning baseline with an expertly built mHuBERT model yielded reasonable results. We experimented with var- ious learning rates and downstream expert models; however, the fine-tuning baseline still yielded unsatisfactory outcomes."}, {"title": "B. Few-shot Learning", "content": "The prompting method has demonstrated its few-shot learn- ing capabilities in the NLP field [5], [9] because of the inher- ent rich prior knowledge within language models. Similarly, speech LMs have already learned to comprehend discretized speech, that is, the discrete speech units. This study extends the investigation into the few-shot learning abilities of the prompt- ing method for speech LMs. We conduct 10-shot learning experiments. For both PT and FT, the trainable parameters are updated with the provided few-shot training data. Specifically, 10 samples per class were used as training data. The models are evaluated using the same testing set as the full dataset setting. Table IX illustrates the performance of the prompting method in comparison to the \u201cpre-train, fine-tune\" paradigm in a 10-shot learning scenario. The experiment shows that the prompting paradigm (PT) possesses robust few-shot learn- ing capabilities and generally outperforms the fine-tuning"}, {"title": "C. Verbalizer Analysis", "content": "In this study, we introduce an optional learnable verbalizer that bridges the gap between discrete units and the downstream tasks' labels. Prior research has shown that discrete units encapsulate acoustic and phonetic information [18], [75]. Thus, rather than employing a random mapping of the heuristic method in ASR and PR, it is more reasonable to employ a learnable verbalizer that discerns which discrete units correlate with specific labels, such as characters or phonemes. The efficacy of the learnable verbalizer is presented in Fig. 6. This figure demonstrates the capability of the learnable verbalizer in linking discrete units with characters for the ASR task, as displayed in the figure's first row, and with phonemes for the PR task, as illustrated in the second row. The heatmaps display the weights $W$ from the learnable verbalizer in Euqation 9, with each map's right side indicating the connected discrete unit. Besides the units, the top three phonemes with the highest correlation to the discrete units are listed, as determined by forced alignment. We observe that for a particular character, such as \u201cB,\u201d the verbalizer prefers discrete units with a strong association with the phoneme \u201cB.\u201d This pattern is consistent in the second row, which pertains to phoneme recognition tasks. Here, labels are connected to units with a high correspondence to the relevant phonemes."}, {"title": "VI. DISCUSSION", "content": "We list observations, limitations, and future directions:"}, {"title": "Architecture and Pre-training Task of Speech Language Models", "content": "In the field of NLP, there is a growing trend to- wards employing decoder-only language models, particularly GPT variants, for a broad range of text generation tasks. However, based on the experimental results, we suggest that encoder-decoder models may offer distinct advantages for speech processing. We hypothesize that it is because many speech processing tasks require handling different modalities, especially the speech signal and text. The unique continuous characteristics of speech signals may be more effectively processed by an encoder. Therefore, encoder-decoder models are likely better suited for the first encoding of the speech signal into a compact representation, after which the decoder generates the desired output, whether it is a class label, text, or another speech signal. This observation aligns with recent work [76] comparing GSLM and Wav2Seq [77] models of similar sizes and datasets. The encoder-decoder model Wav2Seq demonstrates an advantage. However, it is important to note that the pre-training tasks of these models may also play a significant role in their perfor- mance. For instance, GSLM, which performs the next-token prediction task during pre-training, achieves promising results for the speech continuation task. Conversely, Unit mBART'S pre-training task focuses on denoising, which may contribute to its superior performance in other speech processing tasks. The exploration of model architectures and their respective pre-training tasks remains an interesting and valuable direction for future research in the prompting paradigm."}, {"title": "Performance of Prompting Speech Language Models", "content": "We have observed the competitive performance of the prompt- ing Unit mBART model in both speech classification and generation tasks. Notably, in speech generation tasks, relying solely on an SSL speech model does not yield satisfactory performance. However, a discernible performance gap still exists between the prompting and fine-tuning paradigms, es- pecially the sequence generation task. Taking SUPERB as an"}, {"title": "Develop Advanced Speech Language models", "content": "Speech lan- guage models are currently in their nascent stage of develop- ment compared to text-based language models. The proposed prompt framework, although effective in motivating speech LMs, may not achieve exceptional performance. However, with advancements in speech LMs, such as the transition from GSLM to Unit mBART, there has been a significant improvement in prompt performance. Particularly, tasks that were previously challenging for GSLM now exhibit improved performance with Unit mBART. We anticipate the emergence of even more promising speech LMs in the future. Moreover, this paper primarily focuses on textless speech LMs, where the model adapts to various tasks through prompt optimization. Achieving true 0-shot inference remains a chal- lenging but compelling goal within the field of speech pro-"}, {"title": "Beyond Content Information", "content": "Current speech LMs do not fully capture speaker and emotion information, posing a challenge for tasks beyond content-related aspects. In sce- narios where preserving speaker and emotion information is possible, we plan to explore the integration of plug-and- play modules specifically designed to incorporate speaker and emotion details into the framework. Looking ahead, we anticipate that future speech LMs will incorporate and leverage these additional factors and better handle speaker and emotion- related aspects in speech generation tasks. Google's latest speech LM [44] tries to include such information."}, {"title": "VII. CONCLUSION", "content": "In this paper, we investigate how prompting can leverage the generative capabilities of speech language models (speech LMs) for solving a wide range of speech processing tasks. Our approach includes minimal trainable parameters to guide the speech LMs within a unified framework, achieving competitive performance compared to the fine-tuning paradigm while keeping the benefits of the prompting paradigm. The proposed framework exhibits several desirable characteristics, including its textless nature, versatility, efficiency, transferability, and affordability. To demonstrate our framework's capabilities, we study the decoder-only GSLM and encoder-decoder Unit mBART as case studies. We conduct experiments on three distinct types of speech processing tasks: speech classifica- tion, sequence generation, and speech generation. Also, the proposed framework shows promising results in the few- shot scenario. We observe a trend that as more advanced speech LMs are developed, the performance of prompting will significantly improve. We also discuss the limitations and future directions of prompting speech LMs. With the imminent arrival of advanced speech LMs, our unified framework holds immense potential in terms of efficiency and effectiveness, standing on the shoulders of giants."}]}