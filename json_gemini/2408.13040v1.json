{"title": "SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks", "authors": ["Kai-Wei Chang", "Haibin Wu", "Yu-Kai Wang", "Yuan-Kuei Wu", "Hua Shen", "Wei-Cheng Tseng", "Iu-thing Kang", "Shang-Wen Li", "Hung-yi Lee"], "abstract": "Prompting has become a practical method for utilizing pre-trained language models (LMs). This approach offers several advantages. It allows an LM to adapt to new tasks with minimal training and parameter updates, thus achieving efficiency in both storage and computation. Additionally, prompting modifies only the LM's inputs and harnesses the generative capabilities of language models to address various downstream tasks in a unified manner. This significantly reduces the need for human labor in designing task-specific models. These advantages become even more evident as the number of tasks served by the LM scales up. Motivated by the strengths of prompting, we are the first to explore the potential of prompting speech LMs in the domain of speech processing. Recently, there has been a growing interest in converting speech into discrete units for language modeling. Our pioneer research demonstrates that these quantized speech units are highly versatile within our unified prompting framework. Not only can they serve as class labels, but they also contain rich phonetic information that can be re-synthesized back into speech signals for speech generation tasks. Specifically, we reformulate speech processing tasks into speech-to-unit generation tasks. As a result, we can seamlessly integrate tasks such as speech classification, sequence generation, and speech generation within a single, unified prompting framework. The experiment results show that the prompting method can achieve competitive performance compared to the strong fine-tuning method based on self-supervised learning models with a similar number of trainable parameters. The prompting method also shows promising results in the few-shot setting. Moreover, with the advanced speech LMs coming into the stage, the proposed prompting framework attains great potential.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently, self-supervised representation learning has become an essential component in the speech processing field [1]. The speech representation model is trained on a large-scale unlabeled corpus in a self-supervised learning (SSL) manner. The learned representation has been demonstrated to be informative and can benefit a wide range of speech processing tasks [2]\u2013[4].\nWhen leveraging these speech representation models for a downstream task of interest, a typical approach is to follow the \"pre-train, fine-tune\" paradigm [1], [5]. Under this paradigm, the representation models serve as feature extractors. The models encode speech into informative representations, which are subsequently fed into a task-specific model. This model, referred to as the expert downstream model, specializes in solving a specific speech processing task. While fine-tuning often yields optimal performance, this paradigm, as depicted in Fig. 1, requires delicately designing a task-specific downstream model and loss function for each task. This complexity significantly causes an increasing burden of human labor. Furthermore, the requirement to train the expert downstream model alongside the optionally fine-tuned speech representation model leads to substantial computational and storage demands. This is especially challenging as the number of downstream tasks grows due to the necessity to store separate model parameters for each task.\nOn the other hand, researchers have explored the \"prompting paradigm\" [5] as an alternative method to leverage pre-trained language models (LMs) to solve downstream tasks in an efficient manner. Originating from the Natural Language Processing (NLP) field, prompting refers to the technique that finds a task-specific template or instruction, which is called prompt, to steer a pre-trained LM without modifying"}, {"title": "II. RELATED WORKS", "content": "A. Self-supervised Speech Representation and Discretization\nThe exploration of speech representations through Self-Supervised Learning (SSL) objectives has evolved into a crucial research topic within the speech research area in recent years. By utilizing different SSL pre-training tasks, the representation models can mainly be grouped into three categories: predictive models [29], [30], contrastive models [31]\u2013[33], and generative models [34]\u2013[36]. To leverage SSL representations, a common way is to build specialized downstream models on top of SSL representations and fine-tune the entire model or only the downstream models for supervised downstream tasks. Based on this, SUPERB [2] benchmarks SSL speech models with a wide variety of downstream tasks.\nAlthough using continuous SSL representations as features for downstream tasks can yield stronger performance [37], there's a growing trend of adopting discrete speech units derived by quantizing the SSL representations [22], [38]. A common approach involves applying the K-means algorithm to the SSL representations, quantizing them into clusters. Discrete units significantly reduce storage space and transmission bandwidth compared to raw waveforms and SSL features [22], [37]. For instance, as discussed in [22] and shown in Table II, a T-second 16kHz waveform in 16-bit format requires 16 \u00d7 16,000 \u00d7 T bits for storage and transmission. In contrast,"}, {"title": "B. Textless Speech Language Models", "content": "Textless speech LMs regard discrete speech units as pseudo-text and adopt them as LM's vocabulary. Leveraging these discrete units, speech LMs are trained to perform language modeling tasks that mirror those in the NLP field.\nAs shown in Fig. 2, in the textless speech language model, there are three components: (1) speech-to-unit encoder, (2) unit language model, and (3) unit-to-speech decoder. Speech-to-unit encoder comprises an SSL representation model, such as HuBERT [29], paired with a quantizer, like K-means. The continuous representation extracted by the SSL model is clustered into discrete units. These discrete units have shown to encapsulate rich phonetic and linguistic information, thereby effectively representing speech [18], [21]. In conventional speech language models, these discrete units undergo a deduplication process, which removes consecutive repeated units to form a more compact sequence of tokens for language modeling. The unit language model is an LM that performs generative language modeling based on the discrete units. For instance, in GSLM [18], the unit language model conducts the next-token-prediction task akin to GPTs [24], [39]. Unit mBART performs the denoising sequence reconstruction task similar to the BART model [25]. The unit-to-speech decoder"}, {"title": "C. Prompting and Reprogramming in Speech Processing", "content": "This journal paper is an extension of our previous work [16], where we explored the concept of prompting on speech LM, particularly GSLM. Previous work [16] showed promising results in speech classification tasks such as spoken command recognition and intent classification and demonstrated better parameter efficiency compared to the \"pre-train, fine-tune\" paradigm. However, despite achieving notable results in sequence generation tasks like ASR and slot filling, its performance still lags behind the fine-tuning method. In this paper, we further explore an advanced encoder-decoder speech LM, Unit mBART, across a broader range of speech processing tasks. This includes a more diverse set of speech classification tasks, as well as speech generation tasks. The results are more promising: (1) Prompting Unit mBART achieves competitive performance in sequence generation tasks and (2) Prompting Unit mBART is well-suited for speech generation tasks, thereby establishing a unified prompting framework for various speech processing tasks. Additionally, compared to our previous work, we introduce a learnable verbalizer in this"}, {"title": "III. METHOD", "content": "The overview of the proposed framework is depicted in Fig. 3. The input speech waveform is encoded into a sequence of discrete units using an SSL speech model and a quantizer. The unit LM (Section III-A) then takes this unit sequence and performs conditional generation based on the"}, {"title": "A. Unit Language Models", "content": "This subsection explains the backbone unit language model in our prompt framework. As shown in Fig. 4, these unit LMs receive discretized speech units sequence $u^x$ and trainable prompts $p$ as inputs, subsequently using them to generate target unit sequence $u^y$ for downstream speech processing tasks.\nWithout loss of generality, in this paper, we investigate two variants of widely-adopted unit LMs based on Transformers [52]: (1) The decoder-only unit LM that mimics the GPT architecture [24], and (2) The encoder-decoder unit LM that mirrors the BART language model [25]. Both model types employ a causal decoder and are characterized as autoregressive LMs, enabling the capability to generate outputs of varying lengths. Specifically, the probability of each unit $u_j \\in V$ generated by the model at the timestep $t$ is conditioned on the preceding context, denoted by $C_t$. The context $C_t$ includes the input discretized source speech $u^x_t$, the task prompts $p$, and the units $u^y_t$ generated preceding the timestep $t$ in the autoregressive process. Formally, the autoregressive model generates the probability of a unit $u_j$ within a vocabulary $V$ = {$u_1,u_2,..., u_|V|$} at timestep $t$ given the context $C_t$ as:\n$$P(u_j|C_t) = \\frac{e^{z_{tj}}}{\\sum_{k=1}^{|V|} e^{z_{tk}}}$$\nwhere $z_{tj} \\in R^{|V|\\times1}$ is the logit for the j-th unit at timestep t, and the denominator is the sum of exponentiated logits for all units at that timestep."}, {"title": "1) Encoder-Decoder Unit LM:", "content": "The encoder-decoder unit LM includes the encoder $\\mathcal{E}$ and decoder $\\mathcal{D}$ based on Transformer. The discretized speech is first processed by the encoder $\\mathcal{E}$ to form part of the enriched context that the decoder $\\mathcal{D}$ performs cross-attention on to guide the generation of the discrete units. The encoder $\\mathcal{E}$ is composed of multiple layers that process the input unit sequence:\n$$g^{(1)} = [e(u^x_1), e(u^x_2), ..., e(u^x_T)],$$\nwhere $T$ is the sequence length and $e(.) : Z \\rightarrow R^d$ denotes the vocabulary embedding table, which transforms a discrete unit $u \\in Z$ into its corresponding embedding vector $e(u) \\in R^d$, and $d$ is the embedding dimension. In the encoder, the i-th layer receives hidden representation $g^{(i)} = [g^{(i)}_1, g^{(i)}_2, ..., g^{(i)}_T]$ as input and outputs $g^{(i+1)}$. The decoder layers operate similarly, with each taking input $h^{(i)} = [h^{(i)}_1, h^{(i)}_2, ..., h^{(i)}_{T'}]$ and outputs $h^{(i+1)}$, where $T'$ represents the decoder sequence length, which increases incrementally during the autoregressive process."}, {"title": "2) Decoder-only Unit LM:", "content": "In the decoder-only LM, the model lacks the encoder and relies solely on the decoder $\\mathcal{D}$, which functions in an analogous fashion to the encoder-decoder setup but without the encoder's guidance. Without the encoder, the discretized source speech $u^x_t$ is integrated at the beginning of the sequence, serving as the initial context for the decoder to predict the subsequent units. A separation token ($sep$) is inserted in between the source unit sequence $u^x$ and the generated units $u^y$. Therefore, for each timestep $t$ in the autoregressive process, the input to the decoder $\\mathcal{D}$ is:\n$$h^{(1)} = [e(u^x), e((sep)), e(u^y_1), ..., e(u^y_t)].$$"}, {"title": "B. Prompt Tuning", "content": "As depicted in Fig. 3, the speech LM is capable of performing predefined speech tasks when provided with various types of prompts. In this subsection, we will elaborate on the process of prompt design.\nPrompting employs task-specific templates, known as prompts, to steer the generation process of the LM. This tech-nique involves freezing the LM's parameters while integrating"}, {"title": "1) Input Prompt Tuning:", "content": "Inspired by the method in [14], input prompt tuning prepends continuous prompt vectors at the LM's input. Specifically, the prompts are prepended at the embedding sequence of the first layer's input $h^{(1)}$ (and $g^{(1)}$ for Encodoer-Decoder model):\n$$h^{(1)} \\leftarrow Concat(p^1, h^{(1)}),$$\n$$g^{(1)} \\leftarrow Concat(p^1, g^{(1)}),$$\nwhere $p^1 = [p^1_1, p^1_2, ..., p^1_l]$ represents a series of prompt vectors $p^1_i \\in R^d$ at the input of the unit LM, with $l$ indicating the prompt length."}, {"title": "2) Deep Prompt Tuning:", "content": "Inspired by prefix-tuning [12], deep prompt tuning involves concatenating prompt vectors at the input of the Transformer layer. Specifically, it modifies the input of the attention modules to guide the forward process of the LM. The self-attention module at the beginning of each transformer layer takes the Query (Q), Key (K), and Value (V) as input:\n$$Attn(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V,$$\nwhere $\\sqrt{d_k}$, the square root of the dimensionality of the key vectors, scales the dot product to ensure normalization of the attention weights by the softmax function. For self-attention, the matrices Q, K, and V are projections of the same input $g$ or $h$ transformed by the weight matrices $W_Q$, $W_K$, and $W_V$, respectively. Trainable prompt vectors are prepended to the input of each transformer layer, affecting both Key (K) and Value (V) matrices in the attention mechanism:\n$$K \\leftarrow Concat(p^K, h)W_K,$$\n$$V \\leftarrow Concat(p^V, h)W_V,$$\nwhere $p^K = [p^K_1, p^K_2, ..., p^K_l]$ and $p^V = [p^V_1, p^V_2, ..., p^V_l]$ are series of trainable prompt vectors for key and value, respectively, and has the same prompt length $l$ as $p^1$.\nSimilar adjustments are applied to the encoder's representation $g$ for encoder-decoder unit LM. It is crucial to note that throughout the prompt tuning process, only the prompt vectors are trainable. The embedding table and the unit LM remain fixed."}, {"title": "C. Speech-to-Unit Generation", "content": "In this paper, we focus on leveraging the generative capabilities of autoregressive speech LMs to handle various downstream tasks. Specifically, we recast speech processing tasks, including speech classification, sequence generation, and speech generation, into a unified speech-to-unit generation task. In this approach, speech LM takes discretized speech as input and generates a sequence of discrete units corresponding to the intended output for the task at hand.\nIn sequence generation tasks, like automatic speech recognition (ASR), the model generates a unit sequence $u^y$ ="}, {"title": "D. Verbalizer and Speech Decoder", "content": "Within the prompting paradigm, the verbalizer [8], [9] $v(\u00b7)$ is a label-mapping module, which establishes the connection between the downstream task labels and the LM's vocabulary. For speech LM, the vocabulary is the discrete units. The verbalizer can adopt various forms, including random mapping [51], [53], [54] and heuristic methods [16], we refer to this as \"fixed verbalizer\" since the mapping is pre-defined and does not include updates. On the other hand, to generate speech signal, a speech decoder is employed to synthesize waveform from discrete unit sequence."}, {"title": "1) Fixed Verbalizer:", "content": "The fixed verbalizer establishes a static mapping between the downstream task label and a unique unit. For example, in the ASR task, it might map the character \u201ca\u201d to \u201cunit 28\u201d and \u201cb\u201d to \u201cunit 72.\u201d In spoken command recognition (SCR), it could map the command \u201c[UP]\u201d to \u201cunit 65,\u201d following either a random mapping or a frequency-based approach [16]. Once established, this mapping remains static without further learning or adaptation. In practice, with a fixed verbalizer, the most probable unit at each timestep t is selected and directly converted to the downstream task's label yt."}, {"title": "2) Speech Decoder:", "content": "For speech generation tasks, where the target output is a speech signal rather than a sequence of labels, the discrete units can be synthesized back into speech signals using a pre-trained, off-the-shelf unit-to-speech decoder. This speech decoder is self-supervised and trained with pairs of discrete units and their corresponding speech. In this work, we employ a speech decoder that corresponds to the given unit LM, as illustrated in Fig. 2."}, {"title": "E. Learnable Verbalizer", "content": "Fixed verbalizers can lead to subpar performance in speech processing tasks because, unlike the distinct semantic meaning present in NLP vocabulary, the vocabulary of discrete speech units lacks clear semantic meanings. To address this, we"}, {"title": "IV. EXPERIMENTAL SETUP", "content": "In this work, we compare the \"pre-train, fine-tune\" paradigm with the prompting paradigm for speech processing across three types of tasks: (1) speech classification tasks, (2) sequence generation tasks, and (3) speech generation tasks. The used dataset and the basic statistics are presented in Table IV."}, {"title": "A. Tasks and Datasets", "content": "1) Speech Classification Tasks:\nSpeech Command Recognition (SCR): The task is to recognize which keyword is presented in a given utterance. We"}, {"title": "2) Sequence Generation Tasks:", "content": "Automatic Speech Recognition (ASR): The task is to transcribe an utterance into text (character sequence). We utilized the LibriSpeech [67] train-clean-100 dataset for training and the test-clean dataset for testing. The evaluation metrics are word error rate (WER) and character error rate (CER).\nPhoneme Recognition (PR): The task involves transcribing an utterance into a phoneme sequence. We utilized LibriSpeech train-clean-100 and test-clean datasets for training and testing. The evaluation metric is phoneme error rate (PER).\nSlot Filling (SF): In the slot filling task, models are expected not only to recognize the spoken content but also to decode the associated slot type. Specifically, the slot type is decoded in conjunction with the transcription in a sequence generation approach. We adopted AudioSNIPS dataset [68] and the evaluation metrics are character error rate (CER) and F1 score."}, {"title": "3) Speech Generation Tasks:", "content": "Speech Translation (ST): ST is the process of converting speech signals from the source language into speech in the target language, enabling communication between individuals who speak different languages. We utilize the CoVoST2 [69] Es-En dataset. This dataset comprises parallel text data for Spanish (Es) and English (En) translations. Following [20], we utilize a single-speaker TTS system to synthesize the speech of the target language. We utilize an off-the-shelf ASR system to transcribe the generated speech and calculate the"}, {"title": "B. Model and Training Setup", "content": "We compare the \"pre-train, fine-tune\" paradigm with the prompting paradigm to assess whether prompting can achieve competitive performance while also providing parameter efficiency and other associated benefits as discussed in Table. I.\n1) Prompting Paradigm: We explore two types of speech LMs within the prompting paradigm: the decoder-only Generative Spoken Language Model (GSLM) [18] and the encoder-decoder model Unit mBART [20]. GSLM is pre-trained using a next token prediction task on discrete units obtained by quantizing the 6-th layer of HuBERT representations into 100 clusters. The GSLM paper [18] considered different settings, including various SSL models and cluster numbers. This setting is selected for its superior performance. GSLM consists of 12 Transformer-decoder layers, each with 16 attention heads, an embedding size of 1024, and a feedforward network (FFN) size of 4096, totaling 150 million parameters. It is pre-trained on HUBERT discrete units derived from a clean 6k-hour subsample of the Libri-Light dataset [72]. On the other hand, Unit mBART is pre-trained on a multilingual denoising task using discrete units derived from quantizing the 11-th layer of mHuBERT representations into 1,000 clusters. Unit mBART includes 12 Transformer-encoder layers and 12 Transformer-decoder layers, each with an embedding size of 1024, an FFN dimension of 4096, and 16 attention heads, totaling 353 million parameters. The embedding tables of the encoder and decoder share the same parameters. Unit mBART is pre-trained on mHuBERT discrete units obtained from VoxPopuli with 16k hours for Spanish, 14k hours for English, and Libri-Light with 60k hours for English."}, {"title": "V. RESULTS", "content": "A. Main Results\n1) Speech Classification Tasks: The comparison of the prompting paradigm (PT) and the \u201cpre-train, fine-tune\" paradigm (FT) for the speech classification tasks are shown in Table V. Our results indicate that the prompting method generally delivers competitive performance and often outperforms the fine-tuning approach. Specifically, for HuBERT and GSLM models, prompting outperforms fine-tuning in 6 out of 10 datasets (AR-SC, LT-SC, DM-SC, Grabo-SC, Fluent-SC, and Mustard++). For mHuBERT and mBART, prompting excels in 8 out of 10 datasets, including all datasets under SCR (with LT-SC achieving identical performance), IC, SD, and LID.\nFor the few tasks where prompting is slightly outperformed by fine-tuning in HuBERT and GSLM settings (Google-SC, and GFSound), the performance gap is minimal, within a"}, {"title": "2) Sequence Generation Tasks:", "content": "The experiment results of sequence generation tasks are shown in Table VI. In sequence generation tasks, we observe that although prompting the decoder-only model GSLM can yield non-trivial results, it"}, {"title": "3) Speech Generation Tasks:", "content": "In speech generation tasks, we focus on two tasks: Speech Translation (ST) and Speech Continuation (SC). Our experiments show the effectiveness of prompting Unit mBART for speech translation, as detailed in Table VIII. Speech-to-speech translation poses significant challenges, often requiring incorporating auxiliary tasks [73], [74] or adopting an advanced speech LM [20]. Our results also support this observation: neither the prompting GSLM baseline nor the fine-tuning baseline with an expertly built mHuBERT model yielded reasonable results. We experimented with various learning rates and downstream expert models; however, the fine-tuning baseline still yielded unsatisfactory outcomes."}, {"title": "B. Few-shot Learning", "content": "The prompting method has demonstrated its few-shot learning capabilities in the NLP field [5], [9] because of the inherent rich prior knowledge within language models. Similarly, speech LMs have already learned to comprehend discretized speech, that is, the discrete speech units. This study extends the investigation into the few-shot learning abilities of the prompting method for speech LMs. We conduct 10-shot learning experiments. For both PT and FT, the trainable parameters are updated with the provided few-shot training data. Specifically, 10 samples per class were used as training data. The models are evaluated using the same testing set as the full dataset setting.\nTable IX illustrates the performance of the prompting method in comparison to the \u201cpre-train, fine-tune\" paradigm in a 10-shot learning scenario. The experiment shows that the prompting paradigm (PT) possesses robust few-shot learning capabilities and generally outperforms the fine-tuning"}, {"title": "C. Verbalizer Analysis", "content": "In this study, we introduce an optional learnable verbalizer that bridges the gap between discrete units and the downstream tasks' labels. Prior research has shown that discrete units encapsulate acoustic and phonetic information [18], [75]. Thus, rather than employing a random mapping of the heuristic method in ASR and PR, it is more reasonable to employ a learnable verbalizer that discerns which discrete units correlate with specific labels, such as characters or phonemes. The efficacy of the learnable verbalizer is presented in Fig. 6. This figure demonstrates the capability of the learnable verbalizer in linking discrete units with characters for the ASR task, as displayed in the figure's first row, and with phonemes for the PR task, as illustrated in the second row. The heatmaps display the weights W from the learnable verbalizer in Euqation 9, with each map's right side indicating the connected discrete unit. Besides the units, the top three phonemes with the highest correlation to the discrete units are listed, as determined by forced alignment. We observe that for a particular character, such as \"B,\" the verbalizer prefers discrete units with a strong association with the phoneme \"B.\u201d This pattern is consistent in the second row, which pertains to phoneme recognition tasks. Here, labels are connected to units with a high correspondence to the relevant phonemes."}, {"title": "VI. DISCUSSION", "content": "We list observations, limitations, and future directions:"}, {"title": "Architecture and Pre-training Task of Speech Language Models:", "content": "In the field of NLP, there is a growing trend towards employing decoder-only language models, particularly GPT variants, for a broad range of text generation tasks. However, based on the experimental results, we suggest that encoder-decoder models may offer distinct advantages for speech processing. We hypothesize that it is because many speech processing tasks require handling different modalities, especially the speech signal and text. The unique continuous characteristics of speech signals may be more effectively processed by an encoder. Therefore, encoder-decoder models are likely better suited for the first encoding of the speech signal into a compact representation, after which the decoder generates the desired output, whether it is a class label, text, or another speech signal. This observation aligns with recent work [76] comparing GSLM and Wav2Seq [77] models of similar sizes and datasets. The encoder-decoder model Wav2Seq demonstrates an advantage.\nHowever, it is important to note that the pre-training tasks of these models may also play a significant role in their performance. For instance, GSLM, which performs the next-token prediction task during pre-training, achieves promising results for the speech continuation task. Conversely, Unit mBART'S pre-training task focuses on denoising, which may contribute to its superior performance in other speech processing tasks. The exploration of model architectures and their respective pre-training tasks remains an interesting and valuable direction for future research in the prompting paradigm."}, {"title": "Performance of Prompting Speech Language Models:", "content": "We have observed the competitive performance of the prompting Unit mBART model in both speech classification and generation tasks. Notably, in speech generation tasks, relying solely on an SSL speech model does not yield satisfactory performance. However, a discernible performance gap still exists between the prompting and fine-tuning paradigms, especially the sequence generation task. Taking SUPERB as an"}, {"title": "VII. CONCLUSION", "content": "In this paper, we investigate how prompting can leverage the generative capabilities of speech language models (speech LMs) for solving a wide range of speech processing tasks. Our approach includes minimal trainable parameters to guide the speech LMs within a unified framework, achieving competitive performance compared to the fine-tuning paradigm while keeping the benefits of the prompting paradigm. The proposed framework exhibits several desirable characteristics, including its textless nature, versatility, efficiency, transferability, and affordability. To demonstrate our framework's capabilities, we study the decoder-only GSLM and encoder-decoder Unit mBART as case studies. We conduct experiments on three distinct types of speech processing tasks: speech classification, sequence generation, and speech generation. Also, the proposed framework shows promising results in the few-shot scenario. We observe a trend that as more advanced speech LMs are developed, the performance of prompting will significantly improve. We also discuss the limitations and future directions of prompting speech LMs. With the imminent arrival of advanced speech LMs, our unified framework holds immense potential in terms of efficiency and effectiveness, standing on the shoulders of giants."}]}