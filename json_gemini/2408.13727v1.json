{"title": "LogParser-LLM: Advancing Efficient Log Parsing with Large Language Models", "authors": ["Aoxiao Zhong", "Dengyao Mo", "Guiyang Liu", "Jinbu Liu", "Qi Zhou", "Jiesheng Wu", "Quanzheng Li", "Qingsong Wen"], "abstract": "Logs are ubiquitous digital footprints, playing an indispensable role in system diagnostics, security analysis, and performance optimization. The extraction of actionable insights from logs is critically dependent on the log parsing process, which converts raw logs into structured formats for downstream analysis. Yet, the complexities of contemporary systems and the dynamic nature of logs pose significant challenges to existing automatic parsing techniques. The emergence of Large Language Models (LLM) offers new horizons. With their expansive knowledge and contextual prowess, LLMs have been transformative across diverse applications. Building on this, we introduce LogParser-LLM, a novel log parser integrated with LLM capabilities. This union seamlessly blends semantic insights with statistical nuances, obviating the need for hyper-parameter tuning and labeled training data, while ensuring rapid adaptability through online parsing. Further deepening our exploration, we address the intricate challenge of parsing granularity, proposing a new metric and integrating human interactions to allow users to calibrate granularity to their specific needs. Our method's efficacy is empirically demonstrated through evaluations on the Loghub-2k and the large-scale LogPub benchmark. In evaluations on the Log-Pub benchmark, involving an average of 3.6 million logs per dataset across 14 datasets, our LogParser-LLM requires only 272.5 LLM invocations on average, achieving a 90.6% F1 score for grouping accuracy and an 81.1% for parsing accuracy. These results demonstrate the method's high efficiency and accuracy, outperforming current state-of-the-art log parsers, including pattern-based, neural network-based, and existing LLM-enhanced approaches.", "sections": [{"title": "1 Introduction", "content": "Logs are pervasive records in the digital realm, vital for system di-agnostics, security analysis, and performance optimization. As we navigate the complexities of contemporary digital environments, our systems, applications, and networks consistently generate vast amounts of logs. These abundant logs serve as an invaluable re-source for understanding system behaviors, tracking activities, and uncovering hidden patterns. Their importance cannot be overstated, especially given the sophisticated nature of present-day systems and the crucial need for maintaining robust and efficient operations. This rich information source of cloud computing aids in tasks such as anomaly detection [6, 48, 51], failure prediction [26, 50], and"}, {"title": "2 Related Work and Motivation", "content": "Log parsing, extensively explored in research [19, 53], identifies static templates and dynamic parameters within log entries. As shown in Figure 1, the template \"Successfully connected to <*> for <*>\" includes dynamic elements like \"/10.190.173.170:50010\" and \"blk_1073742826_2022\" as parameters. We categorize log pars-ing techniques into syntax-based, semantic-based, interactive, and LLM-based methods. We assess their pros and cons and identify opportunities for innovation, particularly in leveraging Large Lan-guage Models to improve log parsing capabilities."}, {"title": "2.1 Syntax-based Log Parsers", "content": "Syntax-based parsers detect templates by identifying repeating pat-terns as static and others as parameters. Frequency-based parsers like SLCT [41], LFA [32], LogCluster [42], and Logram [2], build on token recurrence. Similarity-based parsers, including LKE [7], LogSig [39], LogMine [12], SHISO [30], and LenMa [38] cluster logs by similarity. Heuristics-based parsers such as AEL [18], IPLOM [27], Drain [13], Spell [5], Brain [49], and MoLFI [28], apply specific strategies including the longest common subsequence-based ap-proach, iterative partitioning, prefix trees, and evolutionary algo-rithms for template extraction. These methods are fast and cost-efficient but may miss semantic details and require domain-specific tuning."}, {"title": "2.2 Semantic-based Log Parsers", "content": "Semantic parsers have evolved with neural networks like bidirec-tional LSTM, as seen in Semparser [16] and Uniparser [25], and pre-trained language models such as LogPPT [21]. VALB [22] fur-ther enhances the model's semantic understanding by classifying specific parameter categories. These models require labeled data for training and classify tokens into templates or parameters. They offer semantic understanding and can generalize across log types, but also demand resource-intensive training and periodic updates, presenting significant operational challenges."}, {"title": "2.3 Interactive Log Parsing", "content": "Recent studies [43, 44] have incorporated user feedback into log parsers, facilitating human-in-the-loop log parsing. This approach not only enables the parser to swiftly adapt to evolving logs but also enhances the accuracy of template mining."}, {"title": "2.4 LLMs-based Log Parsing", "content": "Large Language Models (LLMs) have emerged as transformative tools in numerous domains, demonstrating their prowess and ver-satility. Their pre-training on vast datasets, which include diverse content such as code and log data, makes them particularly adept for specialized tasks like log parsing. Studies like [20, 24, 31, 47] have begun to tap into this potential, primarily focusing on prompt engineering to improve template extraction efficiency. While these advancements highlight the promise of LLMs in log parsing, they predominantly utilize a line-by-line parsing approach. This method, although innovative, leads to high computational demands due to LLMs' extensive parameter spaces, making these approaches impractical for real-world applications due to the significant computational overhead.\nThe benefits of LLMs extend beyond their raw computational ability, offering deep semantic understanding and the capacity to generalize across different log formats, adapting seamlessly to new data types. This adaptability is crucial, as it reduces the need for extensive preprocessing, hyper-parameter tuning, and manual labeling, streamlining the deployment process.\nDespite these advantages, the practical deployment of LLMs in log parsing is hindered by their high operational costs. Effec-tive utilization requires careful prompt tuning, a process that can be as resource-intensive as the computational demands of the models themselves. This challenge underscores the need for more efficient approaches that can leverage the strengths of LLMs with-out incurring prohibitive costs, ensuring their viability for broader, real-world application."}, {"title": "3 Granularity of Log Parsing", "content": "In this section, we delve into the granularity of log parsing. Starting with Section 3.1, we characterize its two primary facets: Specificity and Applicability, elucidating them through an illustrative exam-ple. In Section 3.2, we first highlight the shortcomings of existing metrics, emphasizing their inability to capture granularity nuances. Concluding the section, we introduce the granularity distance, a novel metric adept at gauging granularity discrepancies on two distinct levels, effectively addressing the gaps in prior metrics."}, {"title": "3.1 Characterization of Granularity", "content": "The granularity of log parsing is pivotal for how the parsing re-sult looks like. We primarily characterized the granularity by two dimensions: specificity and applicability."}, {"title": "3.1.1 Specificity", "content": "Specificity in log parsing indicates the depth of detail within a template. It is primarily driven by the information and content of templates. The more detailed they are, the higher the specificity.\nHigh Specificity (High Granularity): Such templates have fewer, more detailed variable parts, aligning with a narrower set of logs due to their intricacy.\nLow Specificity (Low Granularity): These are more general, with numerous variable components, catering to a broader log range.\nThe desired level of specificity often varies based on the log analysis context and user needs."}, {"title": "3.1.2 Applicability", "content": "Applicability in log parsing gauges a template's adaptability across varied log entries, primarily based on the struc-ture of its placeholders. The more structurally generic they are, the broader their reach, translating to higher applicability.\nHigh Applicability (Low Granularity): Templates here have a wide-reaching, generic structure, suitable for numerous logs.\nLow Applicability (High Granularity): These are designed for specific log subsets, with unique structural placeholders.\nBoth specificity and applicability play crucial roles in determin-ing the outcome of log templates, subsequently affecting metrics that measure grouping and parsing accuracy of log parsing. To-gether, they delineate the granularity of log parsing. The ideal granularity often finds a midpoint between these two dimensions and is shaped by user preferences and the nuances of individual use cases. Notably, even a minor discrepancy in granularity can result in substantially different groupings, a difference that can be exaggerated when using inappropriate metrics. This highlights the pressing need for a well-conceived metric. It is essential to rec-ognize the inherently subjective nature of granularity. As such, it is inappropriate to strictly label a particular granularity as domi-nant or to view benchmark dataset labels as definitive standards."}, {"title": "3.2 Measuring Granularity Discrepancy", "content": "Existing evaluation metrics, while versatile, emphasize either the accuracy of grouping logs or the fidelity in extracting templates and parameters. Both dimensions are indispensable, especially con-sidering their implications for downstream tasks like log anomaly detection. However, these metrics often overlook the subtle gran-ularity differences inherent in log parsing. Existing benchmark datasets [17, 53] are anchored to the annotators' subjective inter-pretations, suggesting that multiple valid granular interpretations can exist for a single log. Such diversity challenges the conven-tional wisdom of treating annotated labels as an unequivocal gold standard. Instead of a myopic focus on exact matches, a more encom-passing metric that can quantify and understand this granularity discrepancy is imperative."}, {"title": "3.2.1 Existing metrics", "content": "We examine four prevalent metrics in this section. The widely recognized message-level metrics, Grouping Accuracy (GA) [53] and Parsing Accuracy (PA) [2], focus on the volume of messages associated with each template, often prioritiz-ing templates with a larger number of log messages. To address this bias, template-level metrics like F1-score of Group Accuracy (FGA) [17] and F1-score of Template Accuracy (FTA) [19] have been introduced, ensuring an equitable evaluation of each template. The detailed definitions can be found in Appendix B.\nGA and PA primarily evaluate based on the volume of log mes-sages, making them susceptible to biases from imbalanced templates. In real-world scenarios, less frequent templates, such as error mes-sages, might be of paramount importance. Their misinterpretation could be detrimental, yet this might not be reflected effectively using these metrics. Template-level metrics ensures a holistic evaluation of log parsers, giving equal importance to each template. However, while these metrics minimize biases from frequent templates, they still present challenges. If a token is interpreted differently based on granularity nuances, whether designated as a static part or a parameter, it might result in considerable variances in template counts. Additionally, such metrics don't provide a clear insight into granularity differences."}, {"title": "3.2.2 Granularity Distance (GD)", "content": "In light of the discussions above and the sensitivity of existing metrics to subtle granularity discrep-ancies, we introduce the Granularity Distance metric. Inspired by the traditional edit distance, this metric calculates the minimum operations necessary to transform one parsing result into another. It serves as a quantitative reflection of the least human interven-tion needed to attain the desired granularity. This metric can be dissected into two main components:\nGrouping Granularity Distance(GGD): This aspect empha-sizes the grouping of log messages. The aim is to match the expected grouping of log messages without mandating identical templates within those groups.\nParsing Granularity Distance(PGD): This is a more rigorous metric requiring an exact match for each log template. Disparities in the parsed templates increment the distance.\nFor the operations contributing to this distance:\nOperations on GGD: 1) Merge: Combine groups by changing one static section to variable. 2) Split: Separate groups by switching one variable to static section.\nOperations on PGD: 1) Static to Variable: Convert a static sec-tion of the template to a variable. 2) Variable to Static: Revert a variable within the template to a static section.\nSimilar to the edit distance, granularity distance possesses sym-metrical properties, meaning the distance from one log template to another is the same as the distance from the second to the first."}, {"title": "4 Methodology", "content": "In this section, we introduce LogParser-LLM tailored to tackle the challenges previously highlighted. Our approach is built upon four key pillars: 1) Enhanced Template Extraction: Leveraging the prowess of LLMs, we aim to boost the accuracy of template extraction. 2) Efficient LLM Use: We design an algorithm that har-nesses the advanced capabilities of LLMs while optimizing resource consumption. 3) Reduced Human Effort with Broad Applica-bility: Our method minimizes human intervention, especially in label annotation and hyper-parameter tuning, yet remains versatile across various domains and log formats. 4) Interactive Feedback Integration: Our method is integrated with human feedback for parsing granularity calibration. The following sections delve deeper into these principles, elucidating the techniques and decisions un-derpinning our approach."}, {"title": "4.1 Preprocessing", "content": "Our method hinges on minimal preprocessing, using only a basic regular expression to extract log content. While many approaches demand greater domain knowledge, often employing regular ex-pressions to substitute common variables like IP addresses and block IDs [13], we retain the original message, ensuring the LLM grasps the log's full context. Unlike other strategies that use distinct separators for log tokenization [9, 25, 49], we consistently tokenize using spaces. Hence, unless otherwise specified, tokens in follow-ing sections are space-separated, capitalizing on the LLM's native tokenizer. This streamlined preprocessing minimizes the need for specialized expertise, yet upholds strong log parsing efficacy."}, {"title": "4.2 Base Algorithm with Prefix Parse Tree", "content": "Central to our methodology is a base algorithm employing a prefix parse tree, inspired by the efficiency demonstrated in Drain [13]. This section elaborates on the data structures integral to the al-gorithm, detailing their design and their roles in addressing the aforementioned principles. Specifically, we'll elucidate how incom-ing logs are matched with existing clusters during tree traversal, how and when LLMs are invoked for template extraction, and the dynamics of updating the tree with new templates obtained from the LLM extractor."}, {"title": "4.2.1 Data Structures", "content": "Three primary data structures form the backbone of our approach: a set of log clusters, a template pool, and a prefix parse tree. Figure 3 offers a visual representation of this organizational structure. The subsequent discussion delineates their respective functionalities:"}, {"title": "4.2.2 cluster matching with tree search", "content": "Upon receiving a new log, our first step is tokenization. Tokens are then processed sequentially, with each token checked against nodes in the prefix tree. After matching the initial token, we proceed to the subsequent token, considering only the children of the previously matched node. This progression continues either until all tokens are matched or when no further matching tokens exist. Throughout this traversal, log clusters referenced by the encountered nodes are shortlisted as potential candidates for a thorough match evaluation.\nAt this juncture, we have pinpointed a subset of log clusters consistent with the rules encoded in the tree path. Our task now is to determine the genuine match from these candidates. Contrary to existing methodologies such as those in [13, 49], which deploy similarity metrics and predefined, dataset-specific thresholds, our approach crystallizes outcomes into three distinct categories: i) Strict match, ii) Loose match, and iii) No match. For each prospec-tive cluster, an initial check compares the token count between the incoming log and the cluster's syntax templates. Discrepant token counts immediately exclude the possibility of a match. Following this, a 'loose match' is attempted, aligning tokens from the syntax template and the log. Here, any token within the syntax template containing the \"<*>\" wildcard can align with any log token. To illustrate, a token such as \"prefetching...<*>\" can loosely align with any log entry with a singular token. After achieving a loose match, regular expressions ensure a rigorous alignment with elements outside the \"<*>\" in the syntax template. A complete token align-ment signifies a strict match. It is worth noting that the matching process stops upon achieving a strict match. In scenarios where a strict match is identified, the log is straightforwardly added to the matched cluster. Conversely, in the absence of a strict match, the LLM template extractor is invoked for template extraction, followed by the necessary updates to the data structures.\nOur method's precision, rooted in the capabilities of LLMs, elim-inates the need for meticulous hyperparameter tuning across log"}, {"title": "4.2.3 Parse tree update", "content": "The comprehensive update rule is elu-cidated in Algorithm 1. As outlined in lines 8-9, for scenarios of either a loose match or no match, the LLM is invoked to derive a log template. If this extracted template already resides in the template pool, it suggests that the current log pertains to an existing cluster but with an alternative syntax template variant. In such cases, the associated cluster can be swiftly identified via the template pool mapping. It then becomes essential to integrate the novel syntax template into the cluster and adjust the tree to accommodate nodes that align with this new syntax template.\nConversely, if the template isn't found in the template pool yet a loose match has been identified, the LLM is once more consulted. Its task here is to determine if the loosely matched cluster can integrate this new log. A positive outcome leads to the generation of a merged template. Subsequently, both the syntax and log templates of the cluster undergo an update, with the merged template being added to the template pool.\nIf a log, after undergoing the entire aforementioned process, still has not been allocated to an existing cluster, it is indicative of a unique log template. Such instances mandate the creation of a new cluster, with corresponding updates made to the tree."}, {"title": "4.3 Enhancing LLM Template Extraction", "content": "While the base algorithm already paves the way for efficient and precise log cluster matching, there remains room to refine the accuracy of the LLM template extractor. To this end, we introduce variable-aware prompting, amalgamating it with in-context learn-ing. This fusion not only amplifies the LLM's task comprehension but also augments its overall performance."}, {"title": "4.3.1 Variable-Aware Prompting", "content": "Past research [22] has highlighted the benefits of identifying and classifying specific variables within logs. By categorizing these variables, not only is the accuracy of template extraction enhanced, but it also proves advantageous for subsequent tasks. Drawing inspiration from this research and the concept of chain-of-thought prompting [45], we restructure our prompts. These prompts now serve dual purposes: they identify variables and categorize them into one of the ten classifications as outlined in [22]. This refined approach prompts the model to understand and determine which components should be classified as variables and the reasoning behind such categorization."}, {"title": "4.3.2 In-Context Learning with K-Shot Demonstrations", "content": "In-context learning (ICL) has become a favored approach when using LLMs for downstream tasks without the need for finetuning [4]. \u0422\u0443\u0440-ically, ICL-based prompts contain three elements: Instruction: A task-specific description. Demonstrations: A set of examples, es-sentially pairs of queries coupled with their ground truth answers. Query: The direct question to which the LLM provides a response. Each time the LLM is called upon for template extraction, we draw a sample of k = 3 examples from our existing pool of log template pairs. Incorporating the principles of Variable-Aware Prompting, we include ten examples, each representing a distinct type of log pa-rameter, as seed examples. Subsequent template extraction results expand this pool. To obtain these samples, we calculate cosine sim-ilarity between LLM embedding of query log and all embeddings present in the example pool. The top-k samples are then chosen as k-shot demonstrations within the prompt."}, {"title": "4.4 Optimal Granularity via Human-in-Loop", "content": "Integrating human expertise into the automated log parsing pro-cess is key to achieving the right granularity. Human input can be seamlessly incorporated at various stages of the parsing pipeline to enhance accuracy and maintain consistency:\n1) Pre-Processing Intervention: Experts annotate a sample of logs before parsing begins. These annotations serve dual purposes: they can be used as seed examples for In-Context Learning (ICL) or to fine-tune LLMs, ensuring the model's output aligns more closely with specific parsing needs.\n2) Real-Time Calibration: During the parsing process, human judgment can be applied to guide decisions on template merging, ensuring the parsing maintains the desired level of granularity throughout.\n3) Post-Processing Refinement: After parsing, the system identifies potential merges or splits based on semantic similarity or template variability. Experts review these suggestions, making adjustments to achieve the optimal granularity."}, {"title": "5 Experiments", "content": "We assess the effectiveness of our method using two datasets: loghub-2k [53] and logPub [17]. First, we detail the experimental settings. Subsequently, we outline the evaluation metrics employed, highlighting a novel metric we introduce to gauge the granularity distance of parsing outcomes. In examining results from the loghub-2k dataset, our primary objective is to elucidate the contribution of each design component of our method. With the logPub benchmark, our intent is to demonstrate both the effectiveness and efficiency of our approach when handling large-scale datasets in practice."}, {"title": "5.1 Experimental Settings", "content": "Datasets. Loghub-2k is a widely recognized benchmark in the field of log parsing. It encompasses logs from 16 diverse systems, including distributed systems, supercomputers, operating systems, mobile platforms, server applications, and individual software pack-ages. For every system source, 2,000 log messages are meticulously annotated. Complementing this, LogPub is a more recent, expansive iteration of Loghub-2k. It features 14 systems, with each averaging a substantial 3.6 million log lines, and showcases a pronounced increase in the number of log templates. This dataset offers a real-istic, large-scale environment, paving the way for comprehensive evaluations of log parsing methodologies."}, {"title": "5.1.2 Implementation Details", "content": "Our experimental setup involves a server powered by Ubuntu 20.04.3 LTS with 512GB of RAM. We use both ChatGPT (version gpt-3.5-turbo-0301) and GPT-4 (version gpt-4-0613) for template extraction. For embedding the logs, the text-embedding-ada-002 method is adopted. All interactions with these models are facilitated through the official OpenAI API. To guarantee consistency in our findings and support reproducibility, we maintain the temperature parameter at 0 to minimize variability. For fine-tuning our LLM, the Llama-2-13b model [40] serves as the foundation. Comprehensive details regarding this fine-tuning process can be found in Appendix B.2. For in-context learning, we uniformly sample 32 log-template pairs from the first 10% of each dataset based on token length as candidate logs. The same samples are employed for fine-tuning."}, {"title": "5.2 Evaluation Metrics", "content": "In alignment with prevailing methods outlined in [17, 19, 25], we utilize the GA, PA, FGA, PTA, RTA and FTA metrics delineated in Appendix B for evaluation. Furthermore, we use the Grouping Granularity Distance (GGD) proposed in Section 3.2.2 as a more intuitive metric to gauge the granularity discrepancies in parsing outcomes."}, {"title": "5.3 Evaluation on Loghub-2k", "content": "Our primary objective in conducting experiments with the smaller-scale Loghub-2k dataset is to assess the efficacy of our method's key components. Additionally, we employ this dataset as a devel-opment set, refining our prompts for LLM template extraction, merging check and verification. The final prompts we adopted are"}, {"title": "5.4 Evaluation on LogPub", "content": "Accuracy and Generalizability Results from the expansive log-Pub dataset are shown in Table 1. We use LogParser-LLM-C to denote calibrated variants of our method. It is clear that our model, LogParser-LLM, even without granularity calibration, significantly surpasses all baseline methods in GA, FGA, and PTA, marking im-provements of 7.8%, 48.3%, and 32.0% compared to the best baseline results. However, PA performance lags, mostly due to granularity nuances complicating the LLM's ability to generate templates that perfectly match annotated labels. A standout point is the consistent performance of our method across the 14 datasets, achieved without domain-specific tweaks, maintaining uniform settings throughout.\nUpon introducing domain-specific granularity calibration with ICL in LogParser-LLM-C, there is a noticeable boost, especially in tem-plate parsing metrics such as PA and FTA. This highlights the reduced discrepancy in the applicability of log parsing achieved through ICL.\nGranularity Discrepancy Evaluation Both Grouping Granu-larity Distance (GGD) and Parsing Granularity Distance (PGD) are calculated and shown in Table 1. PGD is computed using spaces as delimiters for tokenization, representing a lower bound since pre-cise tokenization isn't feasible for such large datasets. This approxi-mation remains valuable for consistent cross-method comparisons.\nUnlike message-level GA and PA metrics, which depend on log volume, the proposed metrics avoid template imbalance and provide a clearer performance indicator. For example, in the Linux dataset, Drain's GA is 68.6 compared to our 53.4. However, Drain's GGD is 30 versus our 10, indicating significantly more effort needed to align Drain's results with the ground truth.\nCompared to template-level metrics, GGD and PGD show that smaller GD correlates with higher FGA and FTA. However, FGA and FTA can overly penalize repetitive differences. For example, if a ground truth template \"instance: <*>\" has many instance IDs not correctly identified as variables, it increases the number of identified templates, skewing precision calculations. GGD and PGD count such differences only once, offering a fairer measurement. For instance, GA and PA for Uniparser on OpenSSH are 0.9 and 0.5, respectively-values that indicate a significant gap compared to other methods and are not informative. Conversely, GGD and PGD for Uniparser on OpenSSH are 15 and 26, respectively, providing an informative and intuitive comparison. This robustness is also observed in HealthApp, OpenStack, and Thunderbird datasets.\nEvaluation with Different LLMs By design, our framework is versatile enough to be compatible with any language model that can process individual log messages and accordingly generate log tem-plates. This evaluation's primary objective is to assess the impact of different LLMs on the efficacy and efficiency of our approach. Our results, as presented in Table 2, demonstrate that using GPT-4 as the LLM template extractor paired with ICL yields op-timal performance. However, this comes at the cost of increased"}, {"title": "6 Conclusion", "content": "In this study, we introduce LogParser-LLM, a novel approach to log parsing that seamlessly integrates the strengths of Large Language Models (LLMs). Centralizing around a prefix tree and an LLM-based template extractor, LogParser-LLM not only streamlines the extrac-tion of semantically rich log templates but also ensures efficiency through strategic LLM call reductions. While demonstrating com-pelling results, we also uncovered nuances in parsing granularity, prompting the creation of the Granularity Distance metric. Our rigorous tests on benchmark datasets reveal that LogParser-LLM significantly outshines existing parsers in accuracy and efficiency, demonstrating its potential as a valuable tool for both researchers and practitioners in the field of log analysis."}, {"title": "A Additional Discussion", "content": "The challenges associated with log parsing encompass several key aspects."}, {"title": "A.1 Challenges of Log Parsing in Practice", "content": "Huge Volume. Modern systems generate vast amounts of log data, which are difficult to manage, store, and analyze. For instance, services like Amazon, Alibaba, and Facebook generate billions of visits per day, each creating multiple log entries [10, 29]. Log parsing, along with tasks like anomaly detection and root cause analysis, is crucial for minimizing system downtime and financial loss [8, 11, 34]. The requirement for real-time, streaming log parsing makes handling such vast volumes challenging.\nConstantly Evolving. Systems and technologies continuously evolve, leading to changes in log entry types, formats, structures, and content. New features and components introduce novel log formats, necessitating updates to log templates for accurate parsing. Without timely template updates, parsing algorithms may fail to extract relevant information, leading to inaccuracies and incom-plete analysis. Proactively updating log templates ensures effective parsing and adaptation to dynamic log generation.\nDiverse Sources. Logs from different systems often have diverse formats, posing a challenge for log parsing algorithms. Each sys-tem's unique log format can vary significantly in structure, syntax, and content. Effective log parsing algorithms must generalize to handle various formats without relying on system-specific rules or assumptions."}, {"title": "A.2 Insights and Opportunities of Log Parsing with LLMs", "content": "The relentless growth in log volumes, the ever-evolving nature of logs, and the vast diversity in log sources have presented daunting challenges in the realm of log parsing. Syntax-based parsers, while efficient, often grapple with the dynamic nuances introduced by log evolution and diverse sources. LLMs, with their deep semantic understanding and adaptability, are poised as a promising solution but need prompt tuning and optimization to handle vast volumes.\nMoreover, the vital role of log data in modern systems under-scores the need for log parsing tools that embody certain foun-dational principles. In practice, a log parser must be Accurate,"}, {"title": "B Existing metrics", "content": "Grouping Accuracy (GA) GA measures the ratio of correctly grouped log messages. A message is considered correctly grouped if and only if its template group is exactly aligned with ground truth grouping.\nParsing Accuracy (PA) PA assesses the ability to extract tem-plates accurately, critical for tasks like anomaly detection. it is the fraction of messages parsed correctly, meaning all template and variable tokens are identified accurately.\nF1 score of Grouping Accuracy (FGA) FGA is a template-level metric that evaluates the fraction of correctly grouped templates. Using the true number of templates ($N_g$), parsed templates ($N_p$), and correctly parsed templates ($N_c$), we calculate the Precision (PGA = $\\frac{N_c}{N_p}$) and Recall (RGA = $\\frac{N_c}{N_g}$) of Grouping Accuracy. FGA is their harmonic mean.\nF1 score of Template Accuracy (FTA) FTA is the harmonic mean of Recall of Template Accuracy (RTA) and Precision of Template Accuracy (PTA). Like FGA, FTA evaluates correct template identification at the template level. A template is correct if log messages with the same parsed template share the same ground-truth template and the parsed template matches the ground-truth template exactly. Using $N_e$ to denote the number of templates identified accurately by a parser, PTA is then given by $\\frac{N_e}{N_p}$ and RTA by $\\frac{N_e}{N_g}$, allowing us to compute FTA as $\\frac{2 \\times PTA + RTA}{PTA \\times RTA}$"}, {"title": "C Additional Implementation Details", "content": "Fine-tuning Settings The Ilmama-2-13b model was finetuned on a server equipped with 8 Tesla A100 80GB GPUs using the Hugging Face Transformers package. The model was finetuned for 50 epochs with 32 samples for each dataset. During inference, we utilized DeepSpeed [35] with 8-bit quantization to expedite the inference process on a single Tesla A100 80GB GPU. Additionally, the model was fine-tuned using LoRA [15] with rank r set to 64. For"}]}