{"title": "LLaRA: Supercharging Robot Learning Data for Vision-Language Policy", "authors": ["Xiang Li", "Yoo Sung Jang", "Cristina Mata", "Jongwoo Park", "Jinghuan Shang", "Kanchana Ranasinghe", "Mu Cai", "Yong Jae Lee", "Michael S. Ryoo", "Ryan Burgert"], "abstract": "Large Language Models (LLMs) equipped with extensive world knowledge and strong reasoning skills can tackle diverse tasks across domains, often by posing them as conversation-style instruction-response pairs. In this paper, we propose LLaRA: Large Language and Robotics Assistant, a framework which formulates robot action policy as conversations, and provides improved responses when trained with auxiliary data that complements policy learning. LLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity to process state information as visual-textual prompts and generate optimal policy decisions in text. To train such action policy VLMs, we first introduce an automated pipeline to generate diverse high-quality robotics instruction data from existing behavior cloning data. A VLM finetuned with the resulting collection of datasets based on a conversation-style formulation tailored for robotics tasks, can generate meaningful robot action policy decisions. Our experiments across multiple simulated and real-world environments demonstrate the state-of-the-art performance of the proposed LLaRA framework. The code, datasets, and pretrained models are available at https://github.com/LostXine/LLaRA.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) such as GPT-4 [1], Llama [2, 3], and Gemini [4] exhibit unprecedented abilities across diverse language tasks. Such LLMs are often trained together with visual"}, {"title": "2 Related Work", "content": "Spatial Reasoning in VLMs: Several recent work investigate how VLMs can be modified for spatial awareness within images [28, 29, 30, 31, 9, 10, 32, 33, 34], which is a critical ability of a visuomotor policy. One line of work explores prompting using textual or specialized tokens to encode locations, followed by instruction tuning on localization-specific datasets [31, 9, 10, 32]. In fact, several works [9, 10, 32] combine LLMs / VLMs with templating operations to generate datasets in formats adhering to human conversation, using existing annotated datasets that only contain per-object bounding box annotations. In contrast, our framework extends beyond localization to directly predict robot actions. An alternate line of work explores visual prompting but without any focus on robot learning (e.g., red circles [34] or free-form visual scribbles [33] overlaid on images images to focus a VLM on specific regions). An extension of such ideas to robotics is explored"}, {"title": "3 Preliminary: Visual Instruction Tuning", "content": "Visual Instruction Tuning is a framework to finetune a large Vision Language Model (VLM) using language as task instructions. The objective is to develop a general-purpose multimodal model capable of learning from diverse vision tasks described by language [63]. A typical example of Visual Instruction Tuning is LLaVA [7], which consists of three neural networks as its model: a pretrained LLM \\(\\Theta_{LLM}\\), a pretrained visual encoder \\(\\theta_\\gamma\\) and an adapter layer \\(\\theta_{MLP}\\).\nConsider a conversation data sample that contains a single image \\(x_v\\) and user instruction text \\(x_i\\) as inputs. During inference, the visual encoder \\(\\theta_\\gamma\\) and adapter layer \\(\\theta_{MLP}\\) process \\(x_v\\) successively to produce a set of visual tokens that can be directly concatenated with the textual tokens of \\(x_i\\) in language embedding space of LLM \\(\\Theta_{LLM}\\). Next, \\(\\Theta_{LLm}\\) autoregressively generates new tokens conditioned on both the visual tokens from \\(x_v\\) and text tokens from \\(\\chi_\\iota\\). These new tokens will be decoded into natural language as the output of the model.\nA two-phase training process is executed in Liu et al. [7] with a next-token prediction objective over the \\(\\Theta_{LLM}\\) output in each phase. First, only \\(\\Theta_{MLP}\\) is trained using image-caption pairs (similar to datasets for CLIP [64] training). Then, both \\(\\Theta_{MLP}\\) and \\(\\Theta_{LLM}\\) are finetuned using a specialized instruc-tion tuning dataset termed LLAVA_Instruct_150k (see Fig. 2). LLaVA_Instruct_150k contains human-style instruction-response conversations, automatically created by a powerful LLM [1] from a generic object detection dataset (see Liu et al. [7] for details). In our paper, we start from a pretrained LLaVA model and only perform a single-stage finetune, updating \\(\\Theta_{LLM}\\) and \\(@MLP\\)."}, {"title": "4 Visuomotor Instruction Tuning", "content": "We leverage large Vision Language Models (VLMs) as a generalist to address diverse visuomotor control challenges, a process we call Visuomotor Instruction Tuning. Specifically, we transform a typical behavior cloning dataset into an instruction tuning dataset and subsequently finetune a pretrained VLM on this tailored dataset. The resulting LLaRA framework benefits from the broad, inherent knowledge embedded within the VLM, enabling better visuomotor task learning.\nIn this section, we present our LLaRA framework to a) convert a set of robot manipulation expert trajectories into a visuomotor instruction tuning dataset, and b) turn a VLM finetuned on such a dataset into a competitive robot policy. In the next section, we show that such a VLM can be further improved for robot control in a self-supervised manner using auxiliary instruction tuning datasets, which are also extracted from the same existing expert demonstrations."}, {"title": "4.1 Problem Formulation", "content": "We consider a behavioral cloning (BC) setting over a Markov Decision Process (MDP), described by the tuple (S, A, P). Here, \\(s_t \\in S\\) denotes the state at timestamp t, \\(a_t \\in A\\) represents the action"}, {"title": "4.2 Instruction Tuning Data from Trajectories", "content": "Next, we introduce our method to turn a set of expert trajectories D into a visuomotor instruction tuning dataset. We focus on the stationary robot manipulation scenario featuring a robotic manipulator fixed above a flat table. The visual observation for this setup is captured by a fixed third-person view camera positioned to observe the table surface.\nThe policy closely follows the design of the original LLaVA [7]. The model consumes a single image and language instruction, generating a language output. For each state transition, we convert the state action pair (st, at) into a single-round conversation. The current visual observation and textual task description, forming st, can be directly assimilated by a VLM as the user instruction. However, the numerical action a needs conversion into textual format to be generated by a VLM. In contrast to the approach employed by RT-2 [21], which utilizes special tokens to directly encode numerical action values, our methodology adopts 2D Image Coordinates-normalized relative to the image size-to represent positional actions. We establish a mapping between the numerical action values and their corresponding 2D positions within the image. Additionally, any rotational component of the action, denoted as ar (such as the angle of a joint), is expressed textually as \"rotate <r>ar</r> degrees\". More details are covered in Sec. A.1.\nBy implementing these strategies, we effectively convert a trajectory into a format comprising both image and text, which is readily processed by the VLM. This approach also facilitates the transfer of the model across different robotic embodiments. We name the dataset converted by this method as Instruct-BC (inBC) and one example is presented in Fig. 3 and more detailed examples are at Tab. 1 and Tab. 5. We use the same dataset name as a prefix to refer to our LLaRA framework trained with inBC data. In scenarios where a single observation st comprises multiple images, systems such as LLaVA suffer from low performance because it was never trained on multiple images (see Tab. 10). To accommodate this, one possible solution is to switch to a VLM setup which can consume multiple interleaved images like Qwen-VL [65] and LLaVA-NeXT [66]. Instead, we convert each additional image into a language description with the help of object detection. Compared to inBC, we name this method that takes additional object detection results in the text as Description-Instruct-BC (D-inBC, and our model trained on this data is referred to with the same name as earlier. More examples are also available in Tab. 1 and Tab. 5."}, {"title": "4.3 Inference Pipeline", "content": "During inference, the prompt for the Vision-Language Model (VLM) is prepared using the same template as either inBC or D-inBC. As illustrated in Fig. 3, for a model trained on inBC, each conversation turn encapsulates the current visual observation, the task description, and the previous actions described in the text. For a model trained on D-inBC, the process starts with object detection to convert any extra images from the task description into text. The task description, now including the detection outcomes, is then combined with visual observation and the action history to complete instruction.\nThen LLaRA generates the text output that contains numerical values (e.g., 2D image coordinates) for a robot action. These numbers can be easily extracted from the text output by following the decorators in the template (e.g., <b>, <r>). The extracted rotation angle can be directly mapped to the rotation of the end effector. The 2D image coordinates will be further converted to robot action space via predefined mapping, which can be estimated using visual calibration in both simulated and real-world environments.\nAfter that, the robot executes the action, and a new observation can be captured from the environment. This initiates another round of interaction, setting the stage for the subsequent action."}, {"title": "5 Supercharging Visuomotor Instruction Dataset", "content": "Motivated by the success of LLaVA [7] instruction finetuning data, and subsequent domain-specific instruction datasets (e.g., for reasoning, grounding, or referring) [9, 10, 31], LLaRA creates auxiliary robotics instruction tuning datasets that enhance a VLM-based policy. The idea is that the auxiliary datasets will drive VLMs to learn a better spatio-temporal understanding of the scene and eventually benefit robot learning.\nMore specifically, given a robot trajectory, we generate expert question-answer pairs formulating conversations that reveal useful information for learning a policy indirectly. In addition to visual observations, we make use of object labels, locations, and geometries (e.g., rotation) together with task descriptions as expert information. We primarily introduce 6 dataset variants, augmenting existing expert information, such as auxiliary semantics, to better finetune VLMs. In each dataset, we consider a standard question, which is further rephrased multiple times using GPT-4 to infuse a reasonable diversity. Answers follow simple rule-based templates that format expert information as expected responses for each conversation style. Refer to Fig. 4 for formatting details and the appendix for qualitative samples of each dataset. The following paragraphs describe each auxiliary dataset."}, {"title": "6 Experiments", "content": "We conduct experiments in both simulated environments and the real world. For the simulated tasks, we turn a VLM into a generalist for diverse tasks. Furthermore, we conduct real-world robot experiments using three protocols: zero-shot generalization, finetuning, and joint training."}, {"title": "6.1 Simulation Experiments", "content": "Settings. We employ VIMA-Bench [43], a simulated table-top robot manipulation environment to evaluate VLMs trained by our robotics instruction tuning dataset. The environment contains 17 tasks and each task is associated with a multi-modal instruction, including text instructions and images that refer to objects of interest or a particular scene arrangement. The robot action space is two 2D coordinates for pick and place positions and a rotation. We first uniformly subsample the VIMA dataset [43] to form three subsets with different sizes: VIMA-0.8k, VIMA-8k, and VIMA-80k where the number indicates the number of expert trajectories in the dataset.\nMethods. We compare three variants of our method: inBC, inBC + Aux, and D-inBC + Aux, with baselines that follows the recipe of RT-2 [21], RT-2 Style and D-RT-2 Style. As introduced in Sec. 4, inBC is the dataset converted from the expert trajectories, D- means the images in the task description are described by text and we will perform the object detection first during inference, and Aux means the auxiliary datasets introduced in Sec. 5 are included in the training set, the letter after it (e.g., (D)) indicates the detailed configuration of the auxiliary dataset (please refer to Tab. 2). Oracle means that the groundtruth bounding box of objects is used. RT-2 Style is similar to inBC"}, {"title": "6.2 Real-world Robot Experiments", "content": "We further conduct zero-shot generalization, finetuning, and joint training experiments in a novel real-world environment. For the zero-shot generalization and finetuning, we use the pretrained mod-"}, {"title": "7 Conclusion", "content": "We present LLaRA, a framework that turns an instruction-tuned vision language model (VLM) into a robot policy using curated instruction tuning datasets. We first construct a conversation-style instruction dataset using robot trajectory data. A VLM instruction tuned on this data confirms the viability of our framework to tackle robot manipulation tasks. We also construct auxiliary datasets using the same robot trajectory data through self-supervision. Experiments across several synthetic environments and robot manipulation tasks in the real world validate the effectiveness of our proposed LLaRA framework."}, {"title": "C Limitations", "content": "While our model has demonstrated significant achievements and holds considerable potential, it is important to acknowledge that there remains space for further optimization.\nFirst, some concepts in images still can not be easily and precisely described by language, while in many cases these features are critical for robot control.\nSecond, we still rely on the object detector trained on a limited number of classes which limit the generalization ability of this method.\nThird, the information extracted from the dataset can be noisy, which may lead to model confusion. In Tab. 1, there are discrepancies such as the location of an object in a reference image differing from its actual location in the current image observation. At the same time, in VIMA dataset, when referring to the shape of an object without a color, the object in the reference image will appear in gray, which is a totally different color than the object with the same shape in the current observations.\nFinally, our method relies on 2D to 2D mapping to convert image coordinates into actual actions, which may face challenges when the complex 3D movement of the robot is required.\nWe believe enhancements in the mentioned aspects could further improve performance and broader applicability in complex, real-world applications."}, {"title": "A Implementation Details", "content": "In this section, we provide more implementation details regarding preparing the datasets, training the model, and the real-world robot experiment setting."}, {"title": "A.1 Dataset Preparation", "content": "In this subsection, we give more details and examples on the major datasets used in this paper."}, {"title": "A.1.1 Build inBC Dataset from Expert Trajectories", "content": "We formulate the dataset in a single-image single-turn conversation setting to emulate a policy where the user queries the Vision Language Model (VLM) with the current observation, and the VLM generates a response that can be directly translated into a concrete action, as introduced in Sec. 4.2. The image and text from the current observation can be directly processed by the VLM using a fixed template, while the conversion of numerical actions into text is facilitated through the use of normalized 2D image coordinates. Tab. 1 contains an example using a VIMA-Bench sample.\nGiven the current limitations of LLaVA [7], to optimize performance, we propose two techniques:\n\u2022 Action History in Query Each query to the VLM includes a history of actions previously executed in the episode. This approach enhances the VLM's understanding of task context within the constraints of a single-turn conversation. (See the orange text in Tab. 1)\n\u2022 Plan 'Twice' Act Once - Multi-step Planning In action generation, the dataset is prepared such that the VLM is designed to generate all successive actions in the response, literally performing multi-step action planning. However, only the first action is executed. For the next state, we query the VLM again with a new observation and a question and only take the first action generated by the VLM.\nThese designs proved beneficial in our ablation study, as detailed in Tab. 9.\nThe rest of this section details the process on VIMA-Bench [43]. VIMA-Bench introduces 17 sim-ulated robot manipulation tasks using multimodal prompts that combine text and reference images. The full accompanying dataset includes 660k expert trajectories, covering 13 of the 17 tasks.\nAll tasks occur in an environment where a robot arm, equipped with either a spatula or a suction cup, is positioned alongside a flat table. Multiple objects with diverse textures are randomly initialized on the table according to the specific task setting.\nEach episode in the dataset features a multimodal task description that clarifies the episode's goal, incorporating images referred to as 'reference images.' Two third-person view cameras capture the scene, providing a top-down view and a front view looking down at the table. The dataset includes RGB, instance segmentation masks and meta information (e.g., textures and shapes) of the objects in images for the reference images and the observations captured by these cameras. We first extract the bounding boxes of each object from the instance-level segmentation mask, as an object detection oracle. Additionally, the dataset contains the expert action sequence required to complete the episode. The action space consists of two poses: for the robot equipped with a spatula, these poses indicate the start and end points of a push; for the robot with a suction cup, they specify where the robot picks up and places an object.\nDue to the constraints of LLaVA [7] and our findings presented in Tab. 10, in each conversation, only the current image from the front view camera is retained as the visual input to the VLM. Other refer-ence images are substituted with short captions that describe the texture (color) and shape of the re-ferred object, which are details extracted from the meta information (See inBC in Tab. 1). However,"}, {"title": "A.1.2 Build D-inBC from inBC", "content": "While inBC in Tab. 1 has shown its great power in many aspects when the reference image contains a scene that has multiple objects instead of one, the inBC will fail to deliver any useful information (e.g., Tab. 5). To address this issue, we utilize an object detector to parse an image of a scene into a list of objects with its corresponding bounding box. The new dataset is named D-inBC because a reference image will be 'described' as a list of objects now. Tab. 5 shows an example where D-inBC delivers more critical information than inBC."}, {"title": "A.1.3 Auxiliary Datasets", "content": "The auxiliary datasets are created using the template outlined in Fig. 4. During dataset generation, for each sample, one template is randomly selected from a pool of 15 templates. These templates were initially rephrased from a single sentence using GPT-4 [1]. The full list of the pools are listed in Tab. 20, Tab. 19, Tab. 21, Tab. 22, Tab. 23, Tab. 24, Tab. 25, Tab. 26, Tab. 27, and Tab. 28. The qualitative examples are available in Tab. 6 and Tab. 7."}, {"title": "A.2 Training", "content": "We initiate training using a pretrained LLaVA-1.5-7B [8] model and finetune all parameters, in-cluding the language model and the projection layer, with the exception of the vision encoder. The training settings closely align with those of the original LLaVA stage 2. However, for VIMA-0.8k and VIMA-8k, we employ a batch size of 32, whereas for VIMA-80k, we restore the batch size to 128."}, {"title": "A.3 Inference", "content": "During inference, for the models trained on inBC, a new conversation is initiated at each timestamp. The model is queried with a current image observation and an instruction that mirrors those in the dataset. Additionally, a prompt randomly selected from Tab. 18 is added to the instruction during inference only, although it appears to have limited impact in retrospect.\nModel sampling is disabled during training, ensuring that the model consistently outputs the token with the highest probability. This approach is designed to maximize the consistency and reliability of the model's responses.\nFor the models trained on D-inBC, before we query the VLM, we would first run object detection on all reference images and take the detection results to fill the instruction template of D-inBC. Besides this, the settings are identical to the models introduced ahead.\nIn this paper, we explore three approaches to object detection. The first method involves using a single query from Tab. 20 and employing the same VLM as the policy to perform object detection. The second approach utilizes a Mask-RCNN [68] model, which features a ResNet50 [69] backbone pretrained on the COCO dataset [70] and subsequently finetuned using the VIMA dataset. A suffix 'OD' will be added to the model name if the model uses this method.\nFinally, we test the versions that use the groundtruth detection results. A suffix \u2018Oracle\u2019 will be added to the model name if the model uses this information."}, {"title": "A.4 RT-2-Style baseline", "content": "For the RT-2-Style baseline, the procedure begins by normalizing each element of the action vector to a range from 0 to 1. We then quantize these values into 256 bins and map the resulting discrete values to tokens indexed from 31000 to 31255 in the tokenizer. The dataset employed is similar to"}, {"title": "A.5 Real-world robot experiments", "content": "In this section, we introduce the implementation details of our real-world robot experiments.\nThe object detector for all real-world experiments is an OWLv2 [67] (ViT-base, patch size 16) running in one-shot detection mode. The object detector takes a prompt image from the same domain and detects the objects in the observation. The list of plastic toys we used as {object} (and detected) is: {duck, croissant, cupcake, blueberries, carrot, banana, grapes, donut, tomato, corn, pepper}\nIn the GPT-4o baseline, the model is fed the same prompt as in RT-2, with an additional prompt describing the robot arm: \u201cYou are operating a robot arm at a table to complete a task specified between <task></task>. Given the current image, you need to generate a series of actions to control the robot arm to accomplish the task.\". The model's answer is parsed to extract the pick and place points."}, {"title": "B Expanded Experiments", "content": "In this section, we include more experiment results. All the results are collected in Tab. 12 and Tab. 13 (VIMA-0.8k), Tab. 14 and Tab. 15(VIMA-8k) and Tab. 16 and Tab. 17(VIMA-80k).\nClarification on L4 results We adopted the testing protocol from VIMA-Bench [43], conducting evaluations across four levels of difficulty, from L1 to L4. However, during our analysis, we dis-covered an inconsistency in the training set: the rotation information for the robot end effector was recorded as zero for all sweeping tasks where the end effector is a spatula. Given the importance of the spatula orientation in a sweeping task and the fact that sweeping tasks constitute 25% of the evaluations at the L4 difficulty level, we concluded that our ability to accurately evaluate our method at L4 was compromised. Considering that the original VIMA model released by the authors appears to include this rotation information, we have chosen not to report the results for L4 in our study.\nAblation on action history and multi-step planning As described in Sec. A.1.1, we enable action history and multi-step planning when generating inBC and D-inBC. Tab. 9 shows that these designs are helpful.\nAblation on multiple image inputs This ablation studies multiple image inputs within a single conversation. Each image is processed by the vision encoder and the projection layer to generate a series of tokens. These image tokens are then integrated into the textual conversation at the points where the corresponding images are referenced.\nHowever, because LLaVA is trained with one image per conversation instead of an interleaving style. The performance drops significantly when applying multiple image inputs, listed in Tab. 10.\nAblation on object detector We study three types of object detectors in this paper: the VLM model itself, an external object detector separately trained on the training set (the methods with a suffix OD), and an oracle from the groundtruth dataset (Oracle). Fig. 13 shows that a reliable object detector is highly beneficial, enhancing the accuracy of image-based inputs and consequently improving model performance."}]}