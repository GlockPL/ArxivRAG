{"title": "Towards a Scalable Reference-Free Evaluation of Generative Models", "authors": ["Azim Ospanov", "Jingwei Zhang", "Mohammad Jalali", "Xuenan Cao", "Andrej Bogdanov", "Farzan Farnia"], "abstract": "While standard evaluation scores for generative models are mostly reference-based, a reference-dependent assessment of generative models could be generally difficult due to the unavailability of applicable reference datasets. Recently, the reference-free entropy scores, VENDI [1] and RKE [2], have been proposed to evaluate the diversity of generated data. However, estimating these scores from data leads to significant computational costs for large-scale generative models. In this work, we leverage the random Fourier features framework to reduce the computational price and propose the Fourier-based Kernel Entropy Approximation (FKEA) method. We utilize FKEA's approximated eigenspectrum of the kernel matrix to efficiently estimate the mentioned entropy scores. Furthermore, we show the application of FKEA's proxy eigenvectors to reveal the method's identified modes in evaluating the diversity of produced samples. We provide a stochastic implementation of the FKEA assessment algorithm with a complexity O(n) linearly growing with sample size n. We extensively evaluate FKEA's numerical performance in application to standard image, text, and video datasets. Our empirical results indicate the method's scalability and interpretability applied to large-scale generative models. The codebase is available at https://github.com/aziksh-ospanov/FKEA.", "sections": [{"title": "1 Introduction", "content": "A quantitative comparison of generative models requires evaluation metrics to measure the quality and diversity of the models' produced data. Since the introduction of variational autoencoders (VAEs) [3], generative adversarial networks (GANs) [4], and diffusion models [5] that led to impressive empirical results in the last decade, several evaluation scores have been proposed to assess generative models learned by different training methods and architectures. Due to the key role of evaluation criteria in comparing generative models, they have been extensively studied in the literature.\nWhile various statistical methods have been applied to measure the fidelity and variety of a generative model's produced data, the standard scores commonly perform a reference-based evaluation of generative models, i.e., they quantify the characteristics of generated samples in comparison to a reference distribution. The reference distribution is usually chosen to be either the distribution of samples in the test data partition or a comprehensive dataset containing a significant fraction of real-world sample types, e.g. ImageNet [6] for evaluating image-based generative models.\nTo provide well-known examples of reference-dependent metrics, note that the distance scores, Fr\u00e9chet Inception Distance (FID) [7] and Kernel Inception Distance (KID) [8], are explicitly reference-based, measuring the distance between the generative and reference distributions. Similarly, the standard quality/diversity score pairs, Precision/Recall [9, 10] and Density/Coverage [11], perform the evaluation in comparison to a reference dataset. Even the seemingly reference-free Inception Score (IS) [12] can be viewed as implicitly reference-based, since it quantifies the variety and fidelity of data based on the labels and confidence scores assigned by an ImageNet pre-trained neural net, where ImageNet implicitly plays the role of the reference dataset. The reference-based nature of these evaluation scores is desired in many instances including standard image-based generative models, where either a sufficiently large test set or a comprehensive reference dataset such as ImageNet is available for the reference-based evaluation.\nOn the other hand, a reference-based assessment of generative models may not always be feasible, because the selection of a reference distribution may be challenging in a general learning scenario. For example, in prompt-based generative models where the data are created in response to a user's input text prompts, the generated data could follow an a priori unknown distribution depending on the specific distribution of the user's input prompts. Figure 1 illustrates text prompt generated colorful elephant images. Each elephant image corresponds to an unusual color choice, such as luminous yellow or neon pink. Even though reference dataset, such as ImageNet [6] contains elephants, additional elements such as neon colors are not attributed to those images. A proper reference-based evaluation of every user's generated data would require a distinct reference dataset, which may not be available to the user during the assessment time. Moreover, finding a comprehensive text or video dataset to choose as the reference set would be more difficult compared to image data, because the higher length of text and video samples could significantly contribute to their variety, requiring an inefficiently large reference set to cover all text or video sample types.\nThe discussed challenging scenarios of conducting a reference-based evaluation highlight the need for reference-free assessment methods that remain functional in the absence of a reference dataset. Recently, entropy-based diversity evaluation scores, the VENDI metric family [1,14] and RKE score [2], have been proposed to address the need for reference-free assessment metrics. These scores calculate the entropy of the eigenvalues of a kernel similarity matrix for the generated data. Based on the theoretical results in [2], the evaluation process of these scores can be interpreted as an unsupervised identification of the generative model's produced sample clusters, followed by the entropy calculation for the frequencies of the detected clusters.\nWhile the VENDI and RKE entropy scores provide reference-free assessments of generative models, estimating these scores from generated data could incur significant computational costs. In this work, we show that computing the precise RKE and VENDI scores would require at least $\\Omega(n^2)$ and $\\Omega(n^{2.376})^1$ computations for a sample size n, respectively. While the randomized projection methods in [1,15] can reduce the computational costs to O(n\u00b2) for a general VENDI score, the quadratic growth would be a barrier to the method's application to large n values. Although the computational expenses could be reduced by limiting the sample size n, an insufficient sample size would lead to significant error in estimating the entropy scores."}, {"title": "2 Related Work", "content": "Evaluation of deep generative models. The assessment of generative models has been widely studied in the literature. The existing scores either quantify a distance between the distributions of real and generated data, as in FID [7] and KID [8] scores, or attempt to measure the quality and diversity of the trained generative models, including the Inception Score [12], quality/diversity metric pairs Precision/Recall [9, 10] and Density/Coverage [11]. The mentioned scores are reference-based, while in this work we focus on reference-free metrics. Also, we note that the evaluation of memorization and novelty has received great attention, and several scores including the authenticity score [17], the feature likelihood divergence [18], and the rarity score [19] have been proposed to quantify the generalizability and novelty of generated samples. Note that the evaluation of novelty and generalization is, by nature, reference-based. On the other hand, our study focuses on the diversity of data which can be evaluated in a reference-free way as discussed in [1,2].\nRole of embedding in quantitative evaluation. Following the discussion in [20], we utilize DinoV2 [21] image embeddings in most of our image experiments, as [20]'s results indicate DinoV2 can yield scores more aligned with the human notion of diversity. As noted in [22], it is possible to utilize other non-ImageNet feature spaces such as CLIP [23] and SwAV [24] as opposed to InceptionV3 [25] to further improve metrics such as FID. In this work, we mainly focus on DinoV2 feature space, while we note that other feature spaces are also compatible with entropy-based diversity evaluation.\nDiversity assessment for text-based models. To quantify the diversity of text data, the n-gram-based methods are commonly used in the literature. A well-known metric is the BLEU score [26], which is based on the geometric average of n-gram precision scores times the Brevity Penalty. To adapt BLEU score to measure text diversity, [27] proposes the Self-BLEU score, calculating the average BLEU score of various generated samples. To further isolate and measure diversity, N-Gram Diversity scores [28-30] were proposed and defined by a ratio between the number of unique n-grams and overall number of n-grams in the text. Other prominent metrics include Homogenization (ROUGE-L) [31], FBD [32] and Compression Ratios [33]."}, {"title": "3 Preliminaries", "content": "Consider a generative model G generating random samples $x_1, ..., x_n \\in \\mathbb{R}^d$ following the model's probability distribution $P_g$. In our analysis, we assume the n generated samples are independently drawn from $P_g$. Note that in VAEs [3] and GANs [4], the generative model G is a deterministic function $G : \\mathbb{R}^r \\rightarrow \\mathbb{R}^d$ mapping an r-dimensional latent random vector $Z \\sim P_z$ from a known distribution $P_z$ to G(Z) distributed according to $P_g$. On the other hand, in diffusion models, G represents an iterative random process that generates a sample from $P_g$. The goal of a sample-based diversity evaluation of generative model G is to quantify the variety of its generated data $X_1,..., X_n$.\n3.1 Kernel Function, Kernel Covariance Matrix, and Matrix-based R\u00e9nyi Entropy\nFollowing standard definitions, $k : \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is called a kernel function if for every integer $n \\in \\mathbb{N}$ and set of inputs $x_1,...,x_n \\in \\mathbb{R}^d$, the kernel similarity matrix $K = [k(x_i, x_j)]_{n \\times n}$ is positive semi-definite. We call a kernel function k normalized if for every input x we have k(x,x) = 1. A well-known example of a normalized kernel function is the Gaussian kernel $k_{Gaussian(\\sigma^2)}$ with bandwidth parameter $\\sigma^2$ defined as\n$$k_{Gaussian(\\sigma^2)}(x, x') := exp(-\\frac{||x - x'||^2}{2\\sigma^2})$$\nFor every kernel function k, there exists a feature map $\\phi : \\mathbb{R}^d \\rightarrow \\mathbb{R}^m$ such that $k(x,x') = <\\phi(x), \\phi(x')>$ is the inner product of the m-dimensional feature maps $\\phi(x)$ and $\\phi(x')$. Given a kernel k with feature map $\\phi$, we define the kernel covariance matrix $C_x \\in \\mathbb{R}^{m \\times m}$ of a distribution $P_x$ as\n$$C_x := E_{x \\sim P_x}[\\phi(x)\\phi(x)^T] = \\int p_x(x) \\phi(x)\\phi(x)^T dx$$\nThe above matrix $C_x$ is positive semi-definite with non-negative values. Furthermore, assuming a normalized kernel k, it can be seen that the eigenvalues of $C_x$ will add up to 1 (i.e., it has unit trace $Tr(C_x) = 1$), providing a probability model. Therefore, one can consider the entropy of $C_x$'s eigenvalues as a quantification of the diversity of distribution $P_x$ based on the kernel similarity score k. Here, we review the general family of R\u00e9nyi entropy used to define VENDI and RKE scores.\nDefinition 1. For a positive semi-definite matrix $C_x \\in \\mathbb{R}^{m \\times m}$ with eigenvalues $\\lambda_1,..., \\lambda_m$, the order-$\\alpha$ R\u00e9nyi entropy $H_{\\alpha}(C_x)$ for $\\alpha > 0$ is defined as\n$$H_{\\alpha}(C_x) := \\frac{1}{1-\\alpha} log(\\sum_{i=1}^m \\lambda_i^{\\alpha})$$\nTo estimate the entropy scores from finite empirical samples $x_1,..., x_n$, we consider the empirical kernel covariance matrix $\\hat{C}_x$ defined as $\\hat{C}_x := \\frac{1}{n} \\sum_{i=1}^n \\phi(x_i)\\phi(x_i)^T$. This matrix provides an empirical estimation of the population kernel covariance matrix $C_x."}, {"title": "3.2 Shift-Invariant Kernels and Random Fourier Features", "content": "A kernel function k is called shift-invariant, if there exists a function $\\kappa : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ such that $k(x,x') = \\kappa(x - x')$ for every $x,x' \\in \\mathbb{R}^d$. Bochner's theorem proves that a function $\\kappa : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ will lead to a shift-invariant kernel similarity score $\\kappa(x - x')$ between x, x' if and only if its Fourier transform $\\hat{\\kappa} : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is non-negative everywhere (i.e, $\\hat{\\kappa}(\\omega) \\geq 0$ for every w). Note that the Fourier transform $\\hat{\\kappa}$ is defined as\n$$\\hat{\\kappa}(\\omega) := \\frac{1}{(2\\pi)^d} \\int \\kappa(x) exp(-i\\omega^T x) dx$$\nSpecifically, Bochner's theorem shows the Fourier transform of a normalized shift-invariant kernel $k(x, x') = \\kappa(x - x')$, where $\\kappa(0) = 1$, will be a probability density function (PDF). The framework of random Fourier features (RFFs) [16] utilizes independent samples drawn from PDF $\\hat{\\kappa}$ to approximate the kernel function. Here, given independent samples $\\omega_1,...,\\omega_r \\sim \\hat{\\kappa}$, we form the following proxy feature map $\\phi_r : \\mathbb{R}^d \\rightarrow \\mathbb{R}^{2r}$\n$$\\phi_r(x) = \\frac{1}{\\sqrt{r}}[cos(\\omega_1^T x), ..., cos(\\omega_r^T x), sin(\\omega_1^T x), ..., sin(\\omega_r^T x)] \\qquad (1)$$\nAs demonstrated in [16,42], the 2r-dimensional proxy map $\\phi_r$ can approximate the kernel function as $k(x, x') = E_{\\omega \\sim \\hat{\\kappa}} [cos(\\omega^T x) cos(\\omega^T x') + sin(\\omega^T x) sin(\\omega^T x')] \\approx <\\phi_r(x), \\phi_r(x')>$."}, {"title": "4 Computational Complexity of VENDI & RKE Scores", "content": "As discussed, computing RKE and general VENDI scores requires computing the order-$\\alpha$ entropy of kernel matrix K. Using the standard definition of $\\alpha$-norm $||v||_\\alpha = (\\sum_{i=1}^n |v_i|^\\alpha)^{1/\\alpha}$, we observe that the computation of $VENDI_\\alpha$ score is equivalent to computing the $\\alpha$-norm $||\\Lambda||_\\alpha$ of the n-dimensional eigenvalue vector $\\Lambda = [\\lambda_1,..., \\lambda_n]$ where $\\lambda_1 > ... > \\lambda_n$ are the sorted eigenvalues of the normalized kernel matrix K.\nIn the following theorem, we prove that except order $\\alpha = 2$, which is the RKE score, computing any other VENDI score is at least as expensive as computing the product of two n \u00d7 n matrices. Therefore, the theorem suggests that the computational complexity of every member of the VENDI family is lower-bounded by $O(n^{2.367})$ which is the least known cost of multiplying n \u00d7 n matrices.\nIn the theorem, we suppose B is any fixed set of \"basis\" functions. A circuit C is a directed acyclic graph each of whose internal nodes is labeled by a gate coming from a set B. A subset of gates are designated as outputs of C. A circuit with n source nodes and m outputs computes a function from $ \\mathbb{R}^n$ to $\\mathbb{R}^m$ by evaluating the gate at each internal gate in topological order. The size of a circuit is the number of gates. Also, $\\nabla B$ is the basis consisting of the gradients of all functions in B. We will provide the proof of the theorems in the Appendix."}, {"title": "5 A Scalable Fourier-based Method for Computing Kernel Entropy Scores", "content": "As we showed earlier, the complexity of computing RKE and VENDI scores are at least quadratically growing with the sample size n. The super-linear growth of the scores' complexity with sample size n can hinder their application to large-scale datasets and generative models with potentially hundreds of sample types. In such cases, a proper entropy estimation should be performed over potentially hundreds of thousands of data, where the quadratic complexity of the scores would be a significant barrier toward their accurate estimation.\nHere, we conisider a shift-invariant kernel matrix $k(x, x') = \\kappa(x - x')$ where $\\kappa(0) = 1$ and propose applying the random Fourier features (RFF) framework [16] to perform an efficient approximation of the RKE and VENDI scores. To do this, we utilize the Fourier transform $\\hat{\\kappa}$ that, according to Bochner's theorem, is a valid PDF, and we independently generate $\\omega_1,..., \\omega_r \\sim \\hat{\\kappa}$ iid. Note that in the case of the Gaussian kernel $k_{Gaussian(\\sigma^2)}$, the corresponding PDF will be an isotropic Gaussian $\\mathcal{N}(0, \\frac{I}{d})$ with zero mean and covariance matrix $I_d$. Then, we consider the RFF proxy feature map $\\phi_r : \\mathbb{R}^d \\rightarrow \\mathbb{R}^{2r}$ as defined in (1) and define the proxy kernel covariance matrix $C_{x,r} \\in \\mathbb{R}^{2r \\times 2r}$:\n$$C_{x,r} = \\frac{1}{n} \\sum_{i=1}^n \\phi_r(x_i)\\phi_r(x_i)^T \\qquad (2)$$\nNote that the $2r \\times 2r$ matrix $\\hat{C}_{x,r}$ has the same non-zero eigenvalues as the $n \\times n$ RFF proxy kernel matrix $K_r$, and therefore can be utilized to approximate the eigenvalues of the original $n \\times n$ kernel matrix K. Therefore, we propose the Fourier-based Kernel Entropy Approximation (FKEA) method to approximate the RKE and VENDI scores as follows:\n$$FKEA-RKE(x_1,...,x_n) = exp(H_2(\\hat{C}_{x,r})) = ||\\hat{C}_{x,r}||_F^2,$$\n$$FKEA-VENDI(x_1,...,x_n) = exp(H_{\\alpha}(\\hat{C}_{x,r})) = (\\sum_{i=1}^{2r} \\hat{\\lambda}_i^{\\alpha})^{\\frac{1}{\\alpha}} \\qquad (3)$$\nNote that in the above, $\\hat{\\lambda}_i$ denotes the ith eigenvalue of the $2r \\times 2r$ matrix $\\hat{C}_{x,r}$. We remark that the computation of both FKEA-RKE and FKEA-VENDI can be done by a stochastic algorithm which computes the proxy covariance matrix (2) by summing the sample-based $2r \\times 2r$ matrix terms, and then computing the resulting matrix's Frobenius norm for RKE score or all the 2r matrix's eigenvalues for a general VENDI with $\\alpha \\neq 2$."}, {"title": "6 Numerical Results", "content": "We evaluated the FKEA method on several image, text, and video datasets to assess its performance in quantifying diversity in different domains. In the experiments, we computed the empirical covariance matrix of 2r-dimensional Fourier features with a Gaussian kernel with bandwidth parameter $\\sigma$ tuned for each dataset, and then applied FKEA approximation for the $VENDI_1$, $VENDI_{1.5}$, and the RKE (same as $VENDI_2$) scores. Experiments were conducted on 8 RTX3090 GPUs. We interpreted the modes identified by FKEA entropy-based diversity evaluation using the eigenvectors of the proxy covariance matrix as discussed in Remark 2. For each eigenvector, we presented the training data with maximum eigenfunction values corresponding to the eigenvector.\n6.1 Experimental Results on Image Data\nTo investigate the FKEA method's diversity evaluation in settings where we know the ground-truth clusters and their quantity, we simulated an experiment on the colored MNIST [46] data with the images of 10 colored digits as shown in Figure 2. We evaluated the FKEA approximations of the diversity scores while including samples from t digits for t \u2208 {1, ...,10}. The plots in Figure 2 indicate the increasing trend of the scores and FKEA's tight approximations of the scores. Also, we show the top-25 training samples with the highest scores according to the top-10 FKEA eigenvectors, showing the method captured the ground-truth clusters.\nWe conducted an experiment on the ImageNet data to monitor the scores' behavior evaluated for 50k samples from an increasing number of ImageNet labels. Figure 3 shows the increasing trend of the scores as well as the top-9 samples representing the top-4 identified clusters used for the entropy calculation. Figures 3, 5 and 4 illustrate the top FKEA modes with corresponding parameters. The modes represent a specific characteristic common between the top images. Table 1 evaluates the diversity scores of various ImageNet GAN models using the FKEA method applied to VENDI-1 and RKE, with potential extension to the entire VENDI family. The comparison includes baseline diversity metrics such as Inception Score [12], FID [7], Improved Precision/Recall [10], and Density/Coverage [11]. Also, Figure 7 presents the FKEA approximated entropy scores with different truncation factors in StyleGAN3 [47] on 30k generated data for each truncation factor, showing the increasing diversity scores with the truncation factor.\nWe defer discussing the results on generated synthetic images to the Appendix.\n6.2 Experimental Results on Text and Video Data\nTo perform experiments on the text data with known clustering ground-truth, we generated 500,000 paragraphs using GPT-3.5 [59] about 100 randomly selected countries (5k samples per country). In the experiments, the text embedding used was text-embedding-3-large [59-61]. We evaluated the diversity scores"}, {"title": "7 Limitations", "content": "Incompatibility with non shift-invariant kernels. Our analysis targets a shift-invariant kernel, which does not apply to a general kernel function, such as polynomial kernels. In practice, many ML algorithms rely on simpler kernels that may not have the shift-invariant property. Due to to specifics of FKEA framework, we cannot directly extend the work to such kernels. We leave the framework's extension to other kernel functions for future studies."}, {"title": "8 Conclusion", "content": "In this work, we proposed the Fourier-based FKEA method to efficiently approximate the kernel-based entropy scores VENDI and RKE scores. The application of FKEA results in a scalable reference-free evaluation of generative models, which could be utilized in applications where no reference data is available for evaluation. A future direction to our work is to study the sample complexity of the matrix-based entropy scores and the FKEA's approximation under high-dimensional kernel feature maps, e.g. the Gaussian kernel. Also, analyzing the role of feature embedding in the method's application to text and video data would be interesting for future exploration."}, {"title": "A.1 Proof of Theorem 1", "content": "The proof of Theorem 1 combines third ingredients. The first is the relation between the circuit size of a function C and of its partial derivatives $\\nabla C = ((\\frac{\\partial C}{\\partial x})_1 , ..., (\\frac{\\partial C}{\\partial x})_n)$.\nLemma 1. The function $\\nabla C$ has a circuit over basis $\\nabla B \\cup {+,\\times}$ whose size is within a constant factor of the size of C.\nLemma 1 is a feature of the backpropagation algorithm [71,72]. This is a linear-time algorithm for constructing a circuit for $\\nabla C$ given the circuit C as input. In contrast, the forward propagation algorithm allows efficient computation of a single (partial) derivative even for circuits with multivalued outputs, giving the second ingredient:\nLemma 2. Let C be a circuit over basis B and t be an input to C. There exists a circuit that computes the derivative $\\frac{\\partial (g)}{\\partial (t)}$ for every gate g of C over basis $\\nabla B \\cup {+, \\times}$ whose size is within a constant factor of the size of C.\nThe last ingredient is the following identity. For a scalar function f over the complex numbers and matrix X diagonalizable as $UA U^T$, we define f(X) to be the function $Uf(A)U^T$ where f is applied entry-wise to the diagonal matrix A.\nLemma 3. For every f that is analytic over an open domain $\\Omega$ containing all sufficiently large complex numbers and every matrix X whose spectrum is contained in $\\Omega$, $\\nabla Tr(f(X)) = f'(X)."}, {"title": "A.2 Proof of Theorem 2", "content": "Assuming that the shift-invariant kernel $k(x, x') = \\kappa(x - x')$ is normalized (i.\u0435. $\\kappa(0) = 1$), then the Fourier transform $\\hat{\\kappa}$ is a valid PDF according to Bochner's theorem and also an even function because k takes real values. Then, we have\n$$k(x,x') = \\kappa(x - x')$$\n$$\\overset{(a)}{=} \\int \\hat{\\kappa} (\\omega) exp (i\\omega^T (x - x')) d\\omega$$\n$$\\overset{(b)}{=} \\int \\hat{\\kappa} (\\omega) cos(\\omega^T (x - x')) d\\omega$$\n$$= E_{\\omega \\sim \\hat{\\kappa}} [cos(\\omega^T (x - x'))]$$\n$$= E_{\\omega \\sim \\hat{\\kappa}} [cos(\\omega^T x) cos(\\omega^T x') + sin(\\omega^T x) sin(\\omega^T x')]$$\nHere, (a) comes from the synthesis property of the Fourier transform. (b) holds since $\\hat{\\kappa}$ is an even function, resulting in a zero imaginary term in the Fourier synthesis.\nTherefore, since |cos(\u03c9\u00b7y)| \u2264 1 for all \u03c9 and y, one can apply Hoeffding's inequality to show for\n$\\omega_1, ..., \\omega_r \\overset{iid}{\\sim} \\hat{\\kappa}$, the following probably correct bound holds:\n$$P(\\left |\\frac{1}{r} \\sum_{i=1}^r cos (\\omega_i(x - x')) - E_{\\omega \\sim \\hat{\\kappa}} [cos (\\omega (x - x'))]\\right | \\geq \\epsilon) \\leq 2 exp(\\frac{-r \\epsilon^2}{2})$$\nTherefore, as the identity $cos(a \u2212 b) = cos(a) cos(b) + sin(a) sin(b)$ reveals $\\frac{1}{r} \\sum_{i=1}^r cos(\\omega_i (x \u2212 x')) = <\\phi_r(x), \\phi_r(x')>$, the above bound can be rewritten as\n$$P(|<\\phi_r(x), \\phi_r(x')> - k(x,x')| \\geq \\epsilon) \\leq 2 exp(\\frac{-r \\epsilon^2}{2})$$\nAlso, $k_r(x, x') = <\\phi_r(x), \\phi_r(x')>$ is by definition a normalized kernel, implying that\n$$\\forall x \\in \\mathbb{R}^d : <\\phi_r(x), \\phi_r(x)> - k(x,x) = 0.$$\nAs a result, one can apply the union bound to combine the above inequalities and show for every sample set $x_1,..., x_n$:\n$$P(\\mathop{max}\\limits_{1 \\leq i,j \\leq n} |<\\phi_r(x_i), \\phi_r(x_j)> - k_{Gaussian}(\\sigma^2)(x_i, x_j)| \\geq \\epsilon) \\leq \\frac{n^2}{2} exp(\\frac{-r \\epsilon^2}{2}).$$\nConsidering the normalized kernel matrix $K = [k(x_i, x_j)]_{1<i,j<n}$ and the proxy normalized kernel matrix $\\hat{K} = [<\\phi_r(x_i), \\phi_r(x_j)>]_{1<i,j<n}$, the above inequality implies that\n$$P(\\frac{1}{n^2}| |\\hat{K} - K||_F^2 \\geq \\epsilon^2) \\leq \\frac{n^2}{2} exp(\\frac{-r \\epsilon^2}{2})$$\n$$= P(\\frac{1}{n^2}||\\hat{K} - K||_F^2 > \\epsilon) \\leq \\frac{n^2}{2} exp(\\frac{-r \\epsilon^2}{2}) \\qquad (7)$$\nLeveraging the eigenvalue-perturbation bound in [73], we can translate the above bound to the following for the sorted eigenvalues $\\lambda_1 \\geq ... \\geq \\lambda_n$ of K and the sorted eigenvalues $\\hat{\\lambda}_1 \\geq ... \\geq \\hat{\\lambda}_n$ of $\\hat{K}$:\n$$\\sum_{i=1}^n \\sqrt{(\\hat{\\lambda}_i - \\lambda_i)^2} \\leq ||\\hat{K} - K||_F$$\nwhich shows\n$$P(\\sqrt{\\sum_{i=1}^n (\\hat{\\lambda}_i - \\lambda_i)^2} \\geq \\epsilon) \\leq \\frac{n^2}{2} exp(\\frac{-r \\epsilon^2}{2}) \\qquad (8)$$\nDefining $\\delta = \\frac{n^2}{2} exp(\\frac{-r \\epsilon^2}{2})$, i.e., $\\epsilon = \\sqrt{\\frac{8log(n/2 \\delta)}{r}}$, leads to\n$$P(\\sqrt{\\sum_{i=1}^n (\\hat{\\lambda}_i - \\lambda_i)^2} \\leq \\epsilon) \\geq 1 - \\delta. \\qquad (9)$$"}, {"title": "B.1 Synthetic Image Dataset Modes", "content": "In addition to running clustering on real image datasets, we also applied FKEA with varying Gaussian Kernel bandwidth parameter \u03c3 to generated synthetic datasets. The results are presented for LDM, VDVAE, StyleGAN-XL and InsGen models trained on FFHQ samples."}, {"title": "B.2 Effect of other embeddings on FKEA clustering", "content": "Even though DinoV2 is a primary embedding in our experimental settings, we acknowldge the use of other embedding models such as SwAV [24] and CLIP [23]. The resulting clusters differ from original DinoV2 clusters and require separate bandwidth parameter finetuning. In our experiments, SwAV embedding emphasizes object placement, such as animal in grass or white backgrounds, as seen in figure 14. CLIP on the other hand clusters by objects, such as birds/dogs/bugs, as seen in figure 15. These results indicate that FKEA powered by other embeddings will slightly change the clustering features; however, it does not hinder the clustering performance of RFF based clustering with FKEA method."}, {"title": "B.3 Text Dataset Modes", "content": "To understand the applicability and effectiveness of the FKEA method beyond images, we extended our study to text datasets. We observed that clustering text data poses a more challenging task compared to image data. This increased difficulty arises from the ambiguity in defining clear separability factors within text, a contrast to the more visually distinguishable criteria in images. The process of evaluating text clusters is not straightforward and often varies significantly based on human judgment and perception.\nTo visualise the results, we use YAKE [65] algorithm to extract the keywords in each text mode and present the identified unigram and bigram keywords. We demonstrate that the results hold for text datasets and identified clusters are meaningful.\nTable 4 outlines the largest modes identified within a news dataset analyzed using the FKEA method, with a detailed focus on the content themes of each mode. The most dominant mode is associated with topics related to crime and police activities, indicating a frequent coverage area in the dataset. Mode 2 is closely correlated with President Obama, reflecting a significant focus on political coverage. Mode 3 pertains to"}, {"title": "B.4 Video Dataset Modes", "content": "In this section, we present additional experiments on the Kinetics-400 [67] video dataset. This dataset comprises 400 human action categories, each with a minimum of 400 video clips depicting the action. Similar"}]}