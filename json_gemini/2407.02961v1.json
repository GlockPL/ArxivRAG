{"title": "Towards a Scalable Reference-Free Evaluation of Generative Models", "authors": ["Azim Ospanov", "Jingwei Zhang", "Mohammad Jalali", "Xuenan Cao", "Andrej Bogdanov", "Farzan Farnia"], "abstract": "While standard evaluation scores for generative models are mostly reference-based, a reference- dependent assessment of generative models could be generally difficult due to the unavailability of applicable reference datasets. Recently, the reference-free entropy scores, VENDI [1] and RKE [2], have been proposed to evaluate the diversity of generated data. However, estimating these scores from data leads to significant computational costs for large-scale generative models. In this work, we leverage the random Fourier features framework to reduce the computational price and propose the Fourier-based Kernel Entropy Approximation (FKEA) method. We utilize FKEA's approximated eigenspectrum of the kernel matrix to efficiently estimate the mentioned entropy scores. Furthermore, we show the application of FKEA's proxy eigenvectors to reveal the method's identified modes in evaluating the diversity of produced samples. We provide a stochastic implementation of the FKEA assessment algorithm with a complexity O(n) linearly growing with sample size n. We extensively evaluate FKEA's numerical performance in application to standard image, text, and video datasets. Our empirical results indicate the method's scalability and interpretability applied to large-scale generative models. The codebase is available at https://github.com/aziksh-ospanov/FKEA.", "sections": [{"title": "1 Introduction", "content": "A quantitative comparison of generative models requires evaluation metrics to measure the quality and diversity of the models' produced data. Since the introduction of variational autoencoders (VAEs) [3], generative adversarial networks (GANs) [4], and diffusion models [5] that led to impressive empirical results in the last decade, several evaluation scores have been proposed to assess generative models learned by different training methods and architectures. Due to the key role of evaluation criteria in comparing generative models, they have been extensively studied in the literature.\nWhile various statistical methods have been applied to measure the fidelity and variety of a generative model's produced data, the standard scores commonly perform a reference-based evaluation of generative models, i.e., they quantify the characteristics of generated samples in comparison to a reference distribution. The reference distribution is usually chosen to be either the distribution of samples in the test data partition or a comprehensive dataset containing a significant fraction of real-world sample types, e.g. ImageNet [6] for evaluating image-based generative models.\nTo provide well-known examples of reference-dependent metrics, note that the distance scores, Fr\u00e9chet Inception Distance (FID) [7] and Kernel Inception Distance (KID) [8], are explicitly reference-based, measuring the distance between the generative and reference distributions. Similarly, the standard quality/diversity score pairs, Precision/Recall [9, 10] and Density/Coverage [11], perform the evaluation in comparison to a"}, {"title": "2 Related Work", "content": "Evaluation of deep generative models. The assessment of generative models has been widely studied in the literature. The existing scores either quantify a distance between the distributions of real and generated data, as in FID [7] and KID [8] scores, or attempt to measure the quality and diversity of the trained generative models, including the Inception Score [12], quality/diversity metric pairs Precision/Recall [9, 10] and Density/Coverage [11]. The mentioned scores are reference-based, while in this work we focus on reference-free metrics. Also, we note that the evaluation of memorization and novelty has received great attention, and several scores including the authenticity score [17], the feature likelihood divergence [18], and the rarity score [19] have been proposed to quantify the generalizability and novelty of generated samples. Note that the evaluation of novelty and generalization is, by nature, reference-based. On the other hand, our study focuses on the diversity of data which can be evaluated in a reference-free way as discussed in [1,2].\nRole of embedding in quantitative evaluation. Following the discussion in [20], we utilize DinoV2 [21] image embeddings in most of our image experiments, as [20]'s results indicate DinoV2 can yield scores more aligned with the human notion of diversity. As noted in [22], it is possible to utilize other non-ImageNet feature spaces such as CLIP [23] and SwAV [24] as opposed to InceptionV3 [25] to further improve metrics such as FID. In this work, we mainly focus on DinoV2 feature space, while we note that other feature spaces are also compatible with entropy-based diversity evaluation.\nDiversity assessment for text-based models. To quantify the diversity of text data, the n-gram-based methods are commonly used in the literature. A well-known metric is the BLEU score [26], which is based on the geometric average of n-gram precision scores times the Brevity Penalty. To adapt BLEU score to measure text diversity, [27] proposes the Self-BLEU score, calculating the average BLEU score of various generated samples. To further isolate and measure diversity, N-Gram Diversity scores [28-30] were proposed and defined by a ratio between the number of unique n-grams and overall number of n-grams in the text. Other prominent metrics include Homogenization (ROUGE-L) [31], FBD [32] and Compression Ratios [33]."}, {"title": "3 Preliminaries", "content": "Consider a generative model G generating random samples x1, ..., xn \u2208 Rd following the model's probability distribution Pg. In our analysis, we assume the n generated samples are independently drawn from Pg. Note that in VAEs [3] and GANs [4], the generative model G is a deterministic function G : R\" \u2192 Rd mapping an r-dimensional latent random vector Z ~ Pz from a known distribution Pz to G(Z) distributed according to Pg. On the other hand, in diffusion models, G represents an iterative random process that generates a sample from Pg. The goal of a sample-based diversity evaluation of generative model G is to quantify the variety of its generated data X1,..., Xn."}, {"title": "3.1 Kernel Function, Kernel Covariance Matrix, and Matrix-based R\u00e9nyi Entropy", "content": "Following standard definitions, k : Rd \u00d7 Rd \u2192 R is called a kernel function if for every integer n \u2208 N and set of inputs x1,...,xn \u2208 Rd, the kernel similarity matrix K = [k(xi, xj)]nxn is positive semi-definite. We call a kernel function k normalized if for every input x we have k(x,x) = 1. A well-known example of a normalized kernel function is the Gaussian kernel $k_{Gaussian(\\sigma^2)}$ with bandwidth parameter \u03c3\u00b2 defined as\n$k_{Gaussian(\\sigma^2)}(x, x') := exp(-\\frac{||x - x'||_2^2}{2\\sigma^2})$\nFor every kernel function k, there exists a feature map 6 : Rd \u2192 Rm such that k(x,x') = ($(x), (x')) is the inner product of the m-dimensional feature maps (x) and (x'). Given a kernel k with feature map 6, we define the kernel covariance matrix Cx \u2208 Rmxm of a distribution Px as\n$C_x := E_{x \\sim P_x} [\\phi(X) \\phi(X)^T] = \\int p_x(x) \\phi(x) \\phi(x)^T dx$\nThe above matrix Cx is positive semi-definite with non-negative values. Furthermore, assuming a normalized kernel k, it can be seen that the eigenvalues of Cx will add up to 1 (i.e., it has unit trace Tr(Cx) = 1), providing a probability model. Therefore, one can consider the entropy of Cx's eigenvalues as a quantification of the diversity of distribution Px based on the kernel similarity score k. Here, we review the general family of R\u00e9nyi entropy used to define VENDI and RKE scores.\nDefinition 1. For a positive semi-definite matrix Cx \u2208 Rmxm with eigenvalues \u03bb1,..., \u03bbm, the order-\u03b1 R\u00e9nyi entropy Ha(Cx) for \u03b1 > 0 is defined as\n$H_{\\alpha}(C_x) := \\frac{1}{1-\\alpha} log(\\sum_{i=1}^m \\lambda_i^{\\alpha})$\nTo estimate the entropy scores from finite empirical samples x1,..., xn, we consider the empirical kernel covariance matrix \u0108x defined as $\\hat{C_x} := \\frac{1}{n} \\sum_{i=1}^n \\phi(x_i) \\phi(x_i)^T$. This matrix provides an empirical estimation of the population kernel covariance matrix Cx."}, {"title": "3.2 Shift-Invariant Kernels and Random Fourier Features", "content": "A kernel function k is called shift-invariant, if there exists a function \u03ba : Rd \u2192 R such that k(x,x') =\n\u03ba(x \u2013 x') for every x,x' \u2208 Rd. Bochner's theorem proves that a function K : Rd \u2192 R will lead to a shift-invariant kernel similarity score \u03ba(x \u2013 x') between x, x' if and only if its Fourier transform : Rd \u2192 R is non-negative everywhere (i.e, (\u03c9) \u2265 0 for every w). Note that the Fourier transform is defined as\n$\\hat{\\kappa}(\\omega) := \\frac{1}{(2\\pi)^d} \\int \\kappa(x) exp(-i\\omega^T x) dx$\nSpecifically, Bochner's theorem shows the Fourier transform of a normalized shift-invariant kernel k(x, x') =\n\u03ba(x \u2013 x'), where \u03ba(0) = 1, will be a probability density function (PDF). The framework of random Fourier features (RFFs) [16] utilizes independent samples drawn from PDF to approximate the kernel function. Here, given independent samples w\u2081,...,\u03c9 ~ k, we form the following proxy feature map : Rd \u2192 R2r\n$\\phi_r(x) = \\frac{1}{\\sqrt{r}} [cos(\\omega_1^T x), ..., cos(\\omega_r^T x), sin(\\omega_1^T x), ..., sin(\\omega_r^T x)]$\nAs demonstrated in [16,42], the 2r-dimensional proxy map or can approximate the kernel function as\nk(x, x') = Ew~ [cos(x) cos(w\u00afx') + sin(w\u00afx) sin(w\u00afx')] \u2248 \u00a7,(x), (x')."}, {"title": "4 Computational Complexity of VENDI & RKE Scores", "content": "As discussed, computing RKE and general VENDI scores requires computing the order-\u03b1 entropy of kernel matrix K. Using the standard definition of a-norm $||v||_\\alpha = (\\sum_{i=1}^n|v_i|^\\alpha)^{1/\\alpha}$, we observe that the computation of VENDI\u0105 score is equivalent to computing the a-norm ||A||a of the n-dimensional eigenvalue vector x = [1,..., \u03bb\u03b7] where \u03bb\u2081 > \u2026 > An are the sorted eigenvalues of the normalized kernel matrix K.\nIn the following theorem, we prove that except order a = 2, which is the RKE score, computing any other VENDI score is at least as expensive as computing the product of two n\u00d7n matrices. Therefore, the theorem suggests that the computational complexity of every member of the VENDI family is lower-bounded by O(n2.367) which is the least known cost of multiplying n \u00d7 n matrices.\nIn the theorem, we suppose B is any fixed set of \"basis\" functions. A circuit C is a directed acyclic graph each of whose internal nodes is labeled by a gate coming from a set B. A subset of gates are designated as outputs of C. A circuit with n source nodes and m outputs computes a function from Rn to Rm by evaluating the gate at each internal gate in topological order. The size of a circuit is the number of gates. Also, VB is the basis consisting of the gradients of all functions in B. We will provide the proof of the theorems in the Appendix."}, {"title": "5 A Scalable Fourier-based Method for Computing Kernel Entropy Scores", "content": "As we showed earlier, the complexity of computing RKE and VENDI scores are at least quadratically growing with the sample size n. The super-linear growth of the scores' complexity with sample size n can hinder their application to large-scale datasets and generative models with potentially hundreds of sample types. In such cases, a proper entropy estimation should be performed over potentially hundreds of thousands of data, where the quadratic complexity of the scores would be a significant barrier toward their accurate estimation.\nHere, we conisider a shift-invariant kernel matrix k(x, x') = x(x-x') where \u03ba(0) = 1 and propose applying the random Fourier features (RFF) framework [16] to perform an efficient approximation of the RKE and VENDI scores. To do this, we utilize the Fourier transform that, according to Bochner's theorem, is a valid PDF, and we independently generate w1,..., wr iid. Note that in the case of the Gaussian kernel kGaussian(02), the corresponding PDF will be an isotropic Gaussian N(0, Ia) with zero mean and covariance matrix Id. Then, we consider the RFF proxy feature map 6 : Rd \u2192 R2r as defined in (1) and define the proxy kernel covariance matrix Cx,r \u2208 R2rx2r:\n$\\hat{C}_{x,r} = \\frac{1}{n} \\sum_{i=1}^n \\phi_r(x_i) \\phi_r(x_i)^T$\nNote that the 2r \u00d7 2r matrix \u0108x,r has the same non-zero eigenvalues as the n \u00d7 n RFF proxy kernel matrix Kr, and therefore can be utilized to approximate the eigenvalues of the original n \u00d7 n kernel matrix K. Therefore, we propose the Fourier-based Kernel Entropy Approximation (FKEA) method to approximate the RKE and VENDI scores as follows:\n$FKEA-RKE(x_1,...,x_n) = exp(H_2(\\hat{C}_{x,r})) = ||\\hat{C}_{x,r}||_F^2,$\n$FKEA-VENDI_{\\alpha}(x_1,...,x_n) = exp(H_{\\alpha}(\\hat{C}_{x,r})) = (\\sum_{i=1}^{2r} \\hat{\\lambda}_i^{\\alpha})^{\\frac{1}{\\alpha}}$\nNote that in the above, a denotes the ith eigenvalue of the 2r \u00d7 2r matrix \u0108x,r. We remark that the computation of both FKEA-RKE and FKEA-VENDI can be done by a stochastic algorithm which computes the proxy covariance matrix (2) by summing the sample-based 2r \u00d7 2r matrix terms, and then computing the resulting matrix's Frobenius norm for RKE score or all the 2r matrix's eigenvalues for a general VENDI with a \u2260 2."}, {"title": "6 Numerical Results", "content": "We evaluated the FKEA method on several image, text, and video datasets to assess its performance in quantifying diversity in different domains. In the experiments, we computed the empirical covariance matrix of 2r-dimensional Fourier features with a Gaussian kernel with bandwidth parameter o tuned for each dataset, and then applied FKEA approximation for the VENDI1, VENDI1.5, and the RKE (same as VENDI2) scores. Experiments were conducted on 8 RTX3090 GPUs. We interpreted the modes identified by FKEA entropy-based diversity evaluation using the eigenvectors of the proxy covariance matrix as discussed in Remark 2. For each eigenvector, we presented the training data with maximum eigenfunction values corresponding to the eigenvector."}, {"title": "6.1 Experimental Results on Image Data", "content": "To investigate the FKEA method's diversity evaluation in settings where we know the ground-truth clusters and their quantity, we simulated an experiment on the colored MNIST [46] data with the images of 10 colored digits as shown in Figure 2. We evaluated the FKEA approximations of the diversity scores while including samples from t digits for t \u2208 {1, ...,10}. The plots in Figure 2 indicate the increasing trend of the scores and FKEA's tight approximations of the scores. Also, we show the top-25 training samples with the highest scores according to the top-10 FKEA eigenvectors, showing the method captured the ground-truth clusters.\nWe conducted an experiment on the ImageNet data to monitor the scores' behavior evaluated for 50k samples from an increasing number of ImageNet labels. Figure 3 shows the increasing trend of the scores as well as the top-9 samples representing the top-4 identified clusters used for the entropy calculation. Figures 3, 5 and 4 illustrate the top FKEA modes with corresponding parameters. The modes represent a specific characteristic common between the top images. Table 1 evaluates the diversity scores of various ImageNet GAN models using the FKEA method applied to VENDI-1 and RKE, with potential extension to the entire VENDI family. The comparison includes baseline diversity metrics such as Inception Score [12], FID [7], Improved Precision/Recall [10], and Density/Coverage [11]. Also, Figure 7 presents the FKEA approximated entropy scores with different truncation factors in StyleGAN3 [47] on 30k generated data for each truncation factor, showing the increasing diversity scores with the truncation factor.\nWe defer discussing the results on generated synthetic images to the Appendix."}, {"title": "6.2 Experimental Results on Text and Video Data", "content": "To perform experiments on the text data with known clustering ground-truth, we generated 500,000 paragraphs using GPT-3.5 [59] about 100 randomly selected countries (5k samples per country). In the experiments, the text embedding used was text-embedding-3-large [59-61]. We evaluated the diversity scores"}, {"title": "7 Limitations", "content": "Incompatibility with non shift-invariant kernels. Our analysis targets a shift-invariant kernel, which does not apply to a general kernel function, such as polynomial kernels. In practice, many ML algorithms rely on simpler kernels that may not have the shift-invariant property. Due to to specifics of FKEA framework, we cannot directly extend the work to such kernels. We leave the framework's extension to other kernel functions for future studies."}, {"title": "8 Conclusion", "content": "In this work, we proposed the Fourier-based FKEA method to efficiently approximate the kernel-based entropy scores VENDI and RKE scores. The application of FKEA results in a scalable reference-free evaluation of generative models, which could be utilized in applications where no reference data is available for evaluation. A future direction to our work is to study the sample complexity of the matrix-based entropy scores and the FKEA's approximation under high-dimensional kernel feature maps, e.g. the Gaussian kernel. Also, analyzing the role of feature embedding in the method's application to text and video data would be interesting for future exploration."}, {"title": "Appendix A Proofs", "content": null}, {"title": "A.1 Proof of Theorem 1", "content": "The proof of Theorem 1 combines third ingredients. The first is the relation between the circuit size of a function C and of its partial derivatives \u2207C = ((C) / (x)\u2081 , ..., (C) / (x)n).\nLemma 1. The function \u2207C has a circuit over basis \u2207BU{+,\u00d7} whose size is within a constant factor of the size of C.\nLemma 1 is a feature of the backpropagation algorithm [71,72]. This is a linear-time algorithm for constructing a circuit for VC given the circuit C as input. In contrast, the forward propagation algorithm allows efficient computation of a single (partial) derivative even for circuits with multivalued outputs, giving the second ingredient:\nLemma 2. Let C be a circuit over basis B and t be an input to C. There exists a circuit that computes the derivative (g) / (t) for every gate g of C over basis \u2207BU {+, \u00d7} whose size is within a constant factor of the size of C.\nThe last ingredient is the following identity. For a scalar function f over the complex numbers and matrix X diagonalizable as UAUT, we define f(X) to be the function Uf(A)UT where f is applied entry-wise to the diagonal matrix A.\nLemma 3. For every f that is analytic over an open domain \u03a9 containing all sufficiently large complex numbers and every matrix X whose spectrum is contained in \u03a9, \u2207Tr(f(X)) = f'(X)."}, {"title": "A.2 Proof of Theorem 2", "content": "Assuming that the shift-invariant kernel k(x, x') = \u03ba(x \u2013 x') is normalized (i.\u0435. \u03ba(0) = 1), then the Fourier transform is a valid PDF according to Bochner's theorem and also an even function because k takes real values. Then, we have\n$k(x,x') = \\sigma (x - x')\\\\\n= \\int \\hat{\\kappa_\\sigma} (\\omega) exp (i\\omega^T (x - x')) d\\omega\\\\\n= \\int \\hat{\\kappa_\\sigma} (\\omega) cos(\\omega^T (x - x')) d\\omega\\\\\n= E_{\\omega \\sim \\hat{\\kappa_\\sigma}}[cos(\\omega^T (x - x'))]$\n= Ew~ [cos(x) cos(x) + sin(x) sin(Tx)]\nHere, (a) comes from the synthesis property of the Fourier transform. (b) holds since is an even function, resulting in a zero imaginary term in the Fourier synthesis.\nTherefore, since | cos(wy)| \u2264 1 for all w and y, one can apply Hoeffding's inequality to show for independently drawn w\u2081, ..., \u1ff3, the following probably correct bound holds:\n$P(\\frac{1}{r} \\sum_{i=1}^{r} cos(\\omega (x-x')) - E_{\\omega \\sim \\hat{\\kappa_\\sigma}} [cos(\\omega (x - x'))] | \\geq \\epsilon) \\leq 2 exp(-\\frac{r \\epsilon^2}{2})$\nTherefore, as the identity cos(a \u2013 b) = cos(a) cos(b) + sin(a) sin(b) reveals $\\frac{1}{r} \\sum_{i=1}^{r} cos(\\omega (x - x')) = \\phi_r(x) \\phi_r(x')$, the above bound can be rewritten as\n$P(|\\phi_r(x)^T \\phi_r(x') - k(x,x) | \\geq \\epsilon) \\leq 2 exp(-\\frac{r \\epsilon^2}{2})$\nAlso, kr(x, x') = 6,(x),(x') is by definition a normalized kernel, implying that\n$\\forall x \\in R^d: \\ \\ \\ \\ \\phi_r(x)^T \\phi_r(x) - k(x,x) = 0$\nAs a result, one can apply the union bound to combine the above inequalities and show for every sample set X1,..., Xn:\n$P(\\underset{1 \\leq i,j \\leq n}{max} ((\\phi_r(x_i)^T \\phi_r(x_j) - k_{Gaussian(\\sigma^2)} (x_i, x_j))^2) \\geq \\epsilon^2) \\leq \\frac{n^2}{2} exp(-\\frac{r \\epsilon^2}{2})$\nConsidering the normalized kernel matrix K = [k(xi, xj)]1<i,j<n and the proxy normalized kernel matrix K = [(xi) Tor(x)]1[i,jn, the above inequality implies that\n$P(||\\frac{1}{n^2}K - \\frac{1}{n^2} \\hat{K}||_F^2 \\geq \\epsilon^2) \\leq \\frac{n^2}{2} exp(-\\frac{r \\epsilon^2}{2})$\n$\\implies P(||\\frac{1}{n^2}K - \\frac{1}{n^2} \\hat{K}||_F \\geq \\epsilon) \\leq \\frac{n^2}{2} exp(-\\frac{r \\epsilon^2}{2})$\nLeveraging the eigenvalue-perturbation bound in [73], we can translate the above bound to the following for the sorted eigenvalues \u03bb\u2081 > \u2026 > \u03bb\u03b7 of K and the sorted eigenvalues X1 >> An of K\nn$\n\\frac{1}{n^2} \\sum_{i=1}^{n} (\\lambda_i - \\hat{\\lambda}_i)^2 \\leq || \\frac{1}{n^2}K - \\frac{1}{n^2} \\hat{K}||_F$\nwhich shows\n$P(\\sqrt{\\frac{1}{n^2} \\sum_{i=1}^{n} (\\lambda_i - \\hat{\\lambda}_i)^2} \\geq \\epsilon) \\leq \\frac{n^2}{2} exp(-\\frac{r \\epsilon^2}{2})$\nDefining \u03b4 = $\\frac{n^2}{2} exp(-\\frac{r \\epsilon^2}{2})$, i.e., $\\epsilon = \\sqrt{\\frac{8 log(n/2 \\delta)}{r}}$, leads to\n$P(\\sqrt{\\frac{1}{n^2} \\sum_{i=1}^{n} (\\lambda_i - \\hat{\\lambda}_i)^2} \\leq \\epsilon) \\geq 1 - \\delta$"}]}