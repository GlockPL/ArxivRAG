{"title": "DPVS-Shapley:Faster and Universal Contribution Evaluation Component in Federated Learning", "authors": ["Keting Yin", "Zonghao Guo", "Zhenghan Qin"], "abstract": "In the current era of artificial intelligence, federated learning has emerged as a novel approach to addressing data privacy concerns inherent in centralized learning paradigms. This decentralized learning model not only mitigates the risk of data breaches but also enhances the system's scalability and robustness. However, this approach introduces a new challenge: how to fairly and accurately assess the contribution of each participant. Developing an effective contribution evaluation mechanism is crucial for federated learning. Such a mechanism incentivizes participants to actively contribute their data and computational resources, thereby improving the overall performance of the federated learning system. By allocating resources and rewards based on the size of the contributions, it ensures that each participant receives fair treatment, fostering sustained engagement.Currently, Shapley value-based methods are widely used to evaluate participants' contributions, with many researchers proposing modifications to adapt these methods to real-world scenarios. In this paper, we introduce a component called Dynamic Pruning Validation Set Shapley (DPVS-Shapley). This method accelerates the contribution assessment process by dynamically pruning the original dataset without compromising the evaluation's accuracy. Furthermore, this component can assign different weights to various samples, thereby allowing clients capable of distinguishing difficult examples to receive higher contribution scores.", "sections": [{"title": "1 INTRODUCTION", "content": "Federated learning [20] has emerged as a popular machine learn-ing paradigm that allows for local data training, transmitting only model updates rather than raw data to a central server for aggre-gation. This approach ensures sensitive data remains within its domain, minimizing the risk of data breaches. While federated learning addresses issues such as data silos and privacy protection to some extent, a new challenge has arisen: how to fairly evaluate and compare the contributions of each client in a training session. Consequently, establishing a fair and effective incentive and reward allocation mechanism to encourage participants to provide higher quality data for circulation has become an urgent issue to resolve.\nThe Shapley value[21] method has garnered significant attention from researchers due to its properties of group rationality, symme-try, null player, and additivity. However, it is also known for its high computational complexity and low efficiency. The evaluation process typically requires multiple rounds of validation on a valida-tion set to accurately assess metrics such as accuracy. The original Shapley value approach, which required combining multi-party data and retraining the model, was extremely time-consuming. The introduction of MR[22] technology and gradient aggregation-based methods eliminated the need for iterative model retraining, significantly improving the time required for measuring contribu-tions. Nevertheless, these methods still face the issue of exponential growth in validation iterations due to combinatorial calculations. Existing work[8][16][12] primarily focuses on reducing the number of validation rounds through sampling and truncation techniques, thereby shortening the overall validation time. Additionally, gradi-ent aggregation-based Shapley value methods assign equal weight to each sample in the validation set, which is evidently unreasonable as the value of test samples often varies.\nTo further enhance the efficiency of participant contribution assessment in Shapley value-based federated learning (FL), this paper proposes a method to accelerate the contribution evaluation process by reducing the time spent on each validation round, based on gradient aggregation techniques. We introduce the Dynamic Pruning Validation Set Shapley (DPVS-Shapley) method, which implements a dynamic pruning strategy on the validation set. By recording the results of the validation process, the validation set is divided into difficult and easy subsets. In each validation, extracting a certain proportion of cases from the easy subset and combining them with the difficult subset to form the validation set can achieve comparable accuracy to the full validation set while expediting the contribution assessment process. Furthermore, this method can assign different weights to the two types of samples, making the evaluation process more aligned with expectations and the assessment results more distinctive.\nOur main contributions are as follows: 1) We have discovered that by eliminating a subset of samples from the validation set, we can still achieve performance approximating that of the original val-idation set. 2) We conducted ablation experiments to demonstrate the effectiveness of our regression strategy and its variations. 3) Extensive experiments under various FL data distribution settings (i.i.d. and non-i.i.d. data distributions) show that DPVS can effec-tively improve the efficiency of contribution calculations without compromising the contribution assessment."}, {"title": "2 RELATED WORK", "content": "In recent years, the Shapley Value has also been widely applied in federated learning, primarily to assess the contribution of each participant to the training of the federated learning (FL) model.\nThe Shapley Value (SV) is a classical concept from game theory, introduced by Lloyd S. Shapley [21] in 1953, to measure the contribu-tion of each player to the total payoff in cooperative games. However, calculating the Shapley Value involves considering various possible coalition combinations and permutations. As the number of par-ticipants increases, the computational burden grows significantly. For a game with n participants, the computational complexity can reach O(2\"). To address this issue, Castro et al. [1] proposed a Monte Carlo (MC) sampling method to approximate the Shapley Value with fewer utility evaluations, achieving favorable results. Following this, Maleki et al. [19] explored the complexity of approximating the Shap-ley Value (SV) using independent Monte Carlo methods, providing theoretical insights and proofs.Van Campen et al. [23] introduced the SMC-Shapley method, which first applied the Shapley Value frame-work to federated learning. They proposed a structured Monte Carlo sampling estimation method to effectively approximate the Shapley Value. Given the irregular utility values of different coali-tions, their method approximates the Shapley Value by swapping participants' positions within coalition sequences.Ghorbani et al. [8] proposed the TMC-Shapley method, which calculates the Shap-ley Value for each data point by sequentially adding data points. At each addition, the algorithm computes the performance difference between the current and previous subsets. If the difference is be-low a pre-set performance tolerance threshold, the algorithm halts, thus achieving early termination and further reducing the compu-tational load.Jia et al. [12] proposed the GTB-Shapley method. This method employs the concept of \"group testing,\" where participants are divided into multiple groups for testing, thereby reducing the number of utility function evaluations.\nSampling-based methods effectively reduce the cost of the valida-tion process. However, earlier approaches often require retraining the model, leading to significant time spent on repeated model training. Song et al. [22] proposed reconstructing the model based on gradients provided by participants, thereby avoiding model re-training and reducing the exponential cost.Jia et al. [11] introduced KNN-Shapley from a data pruning perspective, which employs K-nearest neighbors in federated tasks to perform data sample pruning for contribution assessment. However, this method exhibits consid-erable deviation compared to the true Shapley Value.Ghorbani et al. [7] explored scenarios with a large number of participants, where their method samples only a few participants to optimize the effi-ciency of contribution evaluation. The contributions of unsampled participants are then assessed using regression fitting.Liu et al. [16] accelerated convergence and reduced computational costs by using guided permutation sampling combined with both inter-round and intra-round truncation.\nFurthermore, some researchers have utilized utility function ap-proximation methods to calculate contributions. Wang et al. [25] proposed FedSV, which approximates the utility function by ran-domly selecting a group of clients, setting the contributions of other clients to zero in the current round. Fan et al. [4] introduced a novel contribution evaluation algorithm, ComFedSV, employing a low-rank matrix completion model to assess Shapley value-based contri-butions. This method records the improvement in the loss function between updated model parameters and those from the previous round. However, it is only applicable to horizontal federated learn-ing. To address this limitation, Fan et al. subsequently proposed VerFedSV[5] for vertical federated learning. This approach is suit-able for both synchronous and asynchronous settings, evaluating data contributions while measuring communication and computa-tional performance. Nevertheless, calculating Shapley values still requires considerable time. Other researchers have combined dis-tributed computing, edge computing, homomorphic encryption, and other technologies with Shapley values. Ma et al. [17] proposed a blockchain-based federated learning framework and a protocol for transparently evaluating each participant's contribution. Liu et al.[15] introduced a peer-to-peer payment system called Fed-Coin, utilizing a \"Proof of Shapley\" (PoSap) consensus protocol to compute the Shapley value for each data owner, as opposed to traditional proof-of-work methods. Dong et al. [3] presented an effi-cient Shapley value estimation method, leveraging the advantages of edge computing to achieve an affordable federated edge learn-ing framework. Zheng et al.[28] proposed HESV, a homomorphic encryption-based single-server solution, and SecSV, a dual-server solution, to address security issues in federated learning systems.\""}, {"title": "3 PRELIMINARIES", "content": "3.1 Shapley Value In Federated Learning\nThe Shapley Value (SV) is a classical concept from game theory, introduced by Lloyd S. Shapley [21] in 1953, to measure the contri-bution of each player to the total payoff in cooperative games.\nIn federated learning, the Shapley Value is defined as follows: Consider n participants with datasets $D_1, D_2, ..., D_{n-1}, D_n$,a model M,and a standard test set T.Let $D_S$ denote a combined dataset where $S \\subseteq N$.The model M trained on the dataset $D_S$ is denoted as $M_S$.The performance of the model on the standard test set is denoted by U (M, T), abbreviated as U (M). The Shapley Value $(D_N, T, D_i)$,abbreviated as $d_i$, is used to compute the contribution of each FL par-ticipant i, and is defined by the formula:\n$\\Phi_i = C \\sum_{S \\subseteq N \\{i\\}} \\frac{U(M_{S\\cup\\{i\\}}) - U(M_S)}{{n-1}\\{|S|}}$\nWhere C is a constant. Obviously, calculating Shapley requires a lot of retraining of the model, which creates a huge overhead, which is unacceptable for individual clients.\n3.2 Optimization Strategies\nBefore computing Shapley values based on model aggregation, obtaining model performance metrics often requires retraining the models. This can lead to significant time spent on training models based on different data combinations, which is clearly unacceptable for participants, as they do not gain additional rewards from the evaluation process. The proposals of MR[22] address this issue by accumulating the contribution of each new round of evaluation and using the model, which aggregates client gradients, as the new base model, thus solving the problem of model retraining. This shift moves the primary time-consuming part of contribution evaluation from model training to model evaluation.\nCurrently, most methods rely on sampling[24] and early stop-ping to achieve the convergence of the convergence function and ultimately fit the true Shapley values. These methods primarily aim to accelerate by reducing the number of validations on the validation set. From another perspective, acceleration can also be achieved by attempting to reduce the time of each individual vali-dation, which aligns with the concept of data pruning. By reducing the amount of data that needs to be validated, corresponding speed improvements can be achieved. Based on local correlation charac-teristics, federated learning uses K-nearest neighbor task models to achieve data sample pruning for contribution evaluation [11]."}, {"title": "3.3 Model Stability", "content": "Support Vector Machine (SVM)[10][27] is a machine learning al-gorithm used for classification and regression analysis, with its primary application in classification problems. The core concept of SVM is to identify an optimal decision boundary (hyperplane) that separates data points into different categories while maximizing the margin between classes. As the learning process progresses, changes in the classification boundary become increasingly subtle, with most samples consistently remaining on one side of the hyperplane. Points closer to the decision boundary have a higher likelihood of being classified to the opposite side in the next round of model training. During the validation process, we tend to focus more on the points that are misclassified in the current round and those that are likely to be misclassified in the next, while paying relatively less attention to the points that are stably classified on either side.\nThe universal approximation capability of neural networks was demonstrated by Cybenko et al.[2], showing that single-hidden-layer neural networks can approximate any continuous function. Hornik et al. [9] further explored the universal approximation capabilities of multi-layer feedforward neural networks, explaining why neural networks can learn complex decision boundaries. Neu-ral networks leverage their hierarchical structure and non-linear activation functions to learn highly complex decision boundaries, enabling sophisticated mappings from input to output.\nAlthough the decision boundaries of neural networks are often highly non-linear, sample classification tends to stabilize as the model is trained. We have adapted our SVM-inspired approach to neural networks, considering samples that are consistently clas-sified correctly and far from the decision boundary as candidates for pruning. This approach allows us to progressively reduce the number of samples being tested."}, {"title": "4 THE PROPOSED APPROACH", "content": "4.1 Main Process\nBased on the existing problems and related explorations, we pro-pose a method for validation set pruning aimed at accelerating the single-round validation process by reducing the number of validation samples, thereby speeding up the entire contribution assessment process. Figure 1 illustrates the confidence regression-based flowchart. In federated learning, the server distributes the model, and clients upload gradients to the server after local training. Contribution assessment methods based on gradient combination using Shapley value require repeatedly combining gradients and evaluating on a validation set. The final contribution proportions are then calculated based on marginal benefits derived from all validation results.\nOur method comprises two phases. The first phase is the experi-ence accumulation phase, during which no pruning is performed on the validation set. This is because the model is still relatively unsta-ble at this stage and unable to effectively distinguish between simple and difficult examples. By evaluating the model on the complete validation set, we accumulate multiple rounds of sample decision results and decision confidence scores. Once the accumulated quan-tity meets the requirements, we can classify the samples into simple and difficult examples, allowing us to enter the dynamic pruning phase.\nThe second phase is the dynamic pruning phase. In this stage, we first categorize the samples in the validation set into difficult and simple examples based on previous sample judgment results. According to our extraction strategy, we select a corresponding proportion of simple examples and combine them with all diffi-cult examples to obtain the pruned test set. If a confidence-based extraction strategy is chosen, the probability of selecting samples with lower confidence increases during the extraction process. We believe that these samples are closer to the decision boundary, and their judgment results are more likely to change in the model's next training process. The pruning algorithm is shown in Algorithm 1.\nWe run the aggregated model on this dynamically pruned vali-dation set to obtain the model's accuracy on the pruned test set and collect intermediate results. Since our accuracy does not include the pruned samples, we need to adjust this accuracy accordingly. The final accuracy formula 1 is shown as follows.\n$Acc_{final} = (100 * \\frac{N_{prune}}{N_{sum}}) + Acc_{dynamic} * \\frac{N_{dynamic}}{N_{sum}}$\nAs the results collected during the validation process only in-clude unpruned test samples, we need to supplement the validation results and confidence scores of the pruned data using historical records. For the judgment results of pruned samples, we assume all are correct by default. However, for confidence scores, we apply a certain confidence decay, because as the model training rounds accumulate, doubts arise about previous classification results. Re-ducing confidence increases the probability of the sample being selected in the next extraction, thereby enabling timely and effec-tive regression. The regression algorithm is shown in Algorithm 2."}, {"title": "4.2 Why and how to prune simple sample", "content": "To further reduce the time spent on contribution evaluation, we propose the concept of Dynamic Pruned Validation Set (DPVS) from the perspective of decreasing the validation time per round. The question of which examples to include in the pruned dataset arises. We posit that samples that do not affect accuracy should be eliminated, specifically those consistently classified correctly during the model's decision process. Such samples have negligible impact on accuracy, as we can largely anticipate their correct clas-sification in the subsequent validation round. To further reduce the time consumed by contribution assessment, we propose the concept of Dynamic Pruning Validation Set (DPVS) from the perspective of decreasing validation time per round. The question of which examples in the pruned dataset to retain becomes a new challenge. We argue that samples that do not affect accuracy should be re-moved, specifically those consistently classified correctly during the model's discrimination process. Such examples have minimal impact on accuracy, as we can largely anticipate their correct classi-fication in subsequent validation rounds. We define these as simple samples. To this end, we collect the classification results for each round, $R_{sample} = [0, 1, 1, ...., 1, 0, 1]$, which indicates whether each sample was correctly classified. After n rounds, we obtain a two-dimensional matrix containing the validation results. This matrix allows us to categorize the validation set samples into simple and difficult samples. We define simple samples as follows: if a sample is correctly classified in all of the most recent n rounds, it is considered an simple sample; otherwise, it is deemed a difficult sample. In our subsequent experiment, \"Simple sample scale in different dataset\", we found that the proportion of simple samples varies from approximately 30%+ to 90%+ across different datasets. This finding suggests that our categorization is meaningful and can potentially achieve corresponding proportions of validation accel-eration. By focusing solely on predicting difficult samples, we can achieve faster validation speeds. However, considering that models undergo continuous changes during the training process, which may lead to previously simple samples becoming misclassified, it is necessary to consider how to timely and effectively reintegrate these samples into the validation set."}, {"title": "4.3 Strategies for Updating Dynamically Pruned Validation Sets", "content": "To address this issue, we propose two methods: random regression and confidence-based regression. Random regression involves ran-domly selecting a certain proportion of samples from the simple samples to combine with the difficult examples, forming the validation set. Each simple sample has an equal probability of being selected, while the remaining simple samples constitute the pruned portion. The confidence-based regression method requires collect-ing not only the classification results but also the confidence levels associated with these results. We apply a function transformation to the confidence levels, ensuring that simple samples with higher confidence have a lower probability of being selected during the sampling process.\nTo address different objectives, we propose two update timing strategies. The EE strategy aims to update the validation set af-ter each validation round, ensuring that the accuracy obtained from this set more closely approximates the true accuracy. However, in our subsequent experiment \"Effect of aggregated gradient number on the accuracy of pruning data set\", we discovered that when the number of combined gradients is low, the resulting ag-gregated model becomes too similar to a single client's model. This leads to significant discrepancies in accuracy. Consequently, we found it necessary to increase the sampling ratio when dealing with a smaller number of combined gradients. To assign greater importance to difficult samples and further accelerate contribution measurement, we propose the ET strategy. This strategy updates the validation set only before contribution calculation, ensuring that all gradient-combined models are validated on the same set. By adopting this strategy, we no longer focus on fitting the contribu-tion of the original Multi-Round (MR) approach, allowing us to use a smaller regression ratio. The purpose of this strategy is to widen the gap between different parties' contributions, enabling clients capable of identifying difficult samples to have a larger contribu-tion share. This approach increases the disparity between parties, enhancing differentiation and facilitating effective contribution ranking."}, {"title": "5 EXPERIMENTAL EVALUATION", "content": "5.1 Settings\nThe hardware and software environments for the experiment are as follows:\n\u2022 Operating system: Ubuntu 16\n\u2022 Development language: Python 3.9\n\u2022 CPU: Intel Xeon Gold 5118\n\u2022 GPU: NVIDIA RTX A4000, 16GB of video memory.\n\u2022 Memory: 96GB\n\u2022 Hard disk: 3.6TB capacity\nThis experiment employs the FedAvg (Federated Averaging) strategy for federated aggregation. The dataset used in the experi-ments is CIFAR-10, with five primary data configurations:\n\u2022 SDSS: Clients have the same distribution of the same size data.\n\u2022 DDSS: Clients with different distributions of data of the same size.\n\u2022 SDDS: Clients with the same distribution of data of different size.\n\u2022 NFSS: Clients have data of the same size from the same distribution, but there is some noise in the client's features.\n\u2022 NLSS: Clients have data of the same size from the same distribution, but there is some noise in the client's labeling.\n5.2 Simple sample scale in different dataset\nTo validate the proportional scale of simple examples across various types of datasets, we trained corresponding models on numerical, image, and text datasets, recording the results of each validation round. In this experiment, we employed a single-machine training mode and split the dataset into training and testing sets with an 8:2 ratio. The criterion for identifying simple examples was as follows: samples correctly classified for 30 consecutive training epochs were designated as simple examples. The remaining experimental settings are detailed in table1"}, {"title": "5.3 Time comparison of different strategies", "content": "We have performed model training on the cifar-10 dataset after putting our strategy on the cifar-10 dataset with different strategies. The experimental results are shown in the following figure3.The time per round of validation for our method in the early time is higher than the original full time, but as the accuracy of the model increases, the number of pruned samples increases, which offsets this consumption and saves more time. It can be seen that the overall picture is that the Ignore strategy drops the fastest in the time dimension. Since randomChoose will have a portion of regression, resulting in the consumption of time to process the random strategy and the testing of this portion of regression samples, randomChoose will be higher than the Ignore strategy. Since weightChoose has to perform weight randomization compared to randomChoose, this operation will consume more time compared to randomChoose.Concurrently, under the same number of rounds, the weightChoose pruning method eliminates fewer data points compared to randomChoose. This is because weightChoose demonstrates a superior ability to accurately identify samples that transition from being simple to challenging examples due to model modifications."}, {"title": "5.4 Effect of aggregated gradient number on the accuracy of pruning data set", "content": "To validate the accuracy of our method on federated learning, we average the cifar-10 data across 5 clients and validate it on the server. Since gradient combinations are needed to compute the sum of marginal effects across clients, we perform a difference measure between real and simulated accuracy for various length combinations. For this purpose, we set up controlled experiments to demonstrate the effectiveness of weighted sampling (WS) and the sampling proportion determined by the number of clients (CNDSR) in our method. For the RS and WS experiments we chose a regression proportion of 0.05 from the pruned sample. for CNDSR the aggregated number of clients 1,2,3,4,5, the regression proportions were taken to be 0.5,0.4,0.3,0.2,0.1 respectively."}, {"title": "5.5 Tested on gradient aggregation-based MR", "content": "Finally we applied our components on the Shapley value method based on gradient aggregation and simulated five data distribution conditions to verify the effectiveness of our method, and the five data settings are as above."}, {"title": "6 CONCLUSION", "content": "This paper addresses the challenge of enhancing contribution as-sessment efficiency in federated learning (FL) systems. We intro-duce the Dynamic Validation Pruning Set Shapley (DPVS-Shapley) method, which employs a dynamic pruning strategy to categorize the validation set into simple and difficult samples. By selectively regressing different proportions of simple samples, we achieve two objectives: reducing overall validation time without compromising accuracy, and amplifying the contribution disparities among parties by assigning higher weights to difficult samples while significantly shortening validation time."}]}