{"title": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's Impact on Spatio-Temporal Cross-Attentions", "authors": ["Ashkan Taghipour", "Morteza Ghahremani", "Mohammed Bennamoun", "Aref Miri Rekavandi", "Zinuo Li", "Hamid Laga", "Farid Boussaid"], "abstract": "This paper investigates the role of CLIP image embeddings within the Stable Video Diffusion (SVD) framework, focusing on their impact on video generation quality and computational efficiency. Our findings indicate that CLIP embeddings, while crucial for aesthetic quality, do not significantly contribute towards the subject and background consistency of video outputs. Moreover, the computationally expensive cross-attention mechanism can be effectively replaced by a simpler linear layer. This layer is computed only once at the first diffusion inference step, and its output is then cached and reused throughout the inference process, thereby enhancing efficiency while maintaining high-quality outputs. Building on these insights, we introduce the VCUT, a training-free approach optimized for efficiency within the SVD architecture. VCUT eliminates temporal cross-attention and replaces spatial cross-attention with a one-time computed linear layer, significantly reducing computational load. The implementation of VCUT leads to a reduction of up to 322T Multiple-Accumulate Operations (MACs) per video and a decrease in model parameters by up to 50M, achieving a 20% reduction in latency compared to the baseline. Our approach demonstrates that conditioning during the Semantic Binding stage is sufficient, eliminating the need for continuous computation across all inference steps and setting a new standard for efficient video generation.", "sections": [{"title": "I. INTRODUCTION", "content": "Advancements in generative AI have boosted the evolution of innovative technology, initially sparked by generative adversarial networks (GANs) [1], [2], [3], across various fields [4] such as image generation [5], [6], [7], motion generation [8], robotics [9], and video generation [10]. Although there is a growing interest in video modeling, progress is not keeping pace with advancements in image generation and editing. This slower progress can be attributed to several factors, including the high computational demands associated with training on video data [11], [12], the scarcity of comprehensive and publicly available video datasets [13], and the complexity inherent in video generation architectures [14], [15], [16]. Recent developments in diffusion models have garnered considerable attention for their potential in video generation. Building on extensive research in text-to-image (T2I) generation, researchers initially started to explore text-to-video (T2V) models [17]. These models align video content with corresponding text inputs, producing videos that reflect the provided textual descriptions. However, text-conditioned video generation imposes significant computational burdens, and the alignment between text and video is not optimal [18]. Consequently, recent research has shifted towards image-conditioned video generation. In this approach, a model uses a given initial image, which can be obtained using an off-the-shelf T2I generative model, to generate subsequent video frames. This significantly reduces computational demands and increases generation speed compared to text-based methods. Similar to the T2I generative model, Stability AI [19] has open-sourced the Stable Video Diffusion (SVD) model [20] for the image-to-video (I2V) task, positioning it as a pioneering development.\nA pivotal component in diffusion architecture that enables the conditioning of generation is the Cross-Attention (CA) mechanism [21]. The CA mechanism plays a crucial role in aligning different modalities, such as text and images, for tasks involving image and video generation. Several studies have explored the importance of CA for spatial control in image generation tasks [22], [23], [7]. However, few, if any, have analyzed the role of CA in video generation during the denoising process, particularly from spatial and temporal perspectives.\nIn this study, we address three new questions regarding the role of CA in the SVD architecture of diffusion models:\n\u2022\n\u2022\n\u2022\n\"Is the CLIP image embedding an effective choice for aligning spatial and temporal features in the I2V generation task?\"\n\"If necessary, can other computationally efficient architectures achieve the same results as the costly Cross-Attention for conditioning the video generation process?\u201d\n\"Regardless of the architectural selection, be it Cross-Attention or another, is it essential to condition the video generation process at every inference step?\"\nTo investigate these queries, we examine the effectiveness"}, {"title": "II. RELATED WORKS", "content": "Following the success of diffusion models in the T2I generation, research in the field of T2V has gained more attention. A pioneering work in T2V, LVDM [25], adapted image diffusion models by transforming its 2D UNet architecture into a 3D UNet and trained it on a vast dataset. ModelScope [26], inspired by LVDM's approach, utilized a more diverse dataset but struggled with handling compositional text descriptions. T2V-Zero [17], employing a training-free approach, converted the image diffusion self-attention mechanisms into cross-frame attention; however, its generated motion lacked realism due to the warping method used for frame generation. VideoFactory [27] introduced a swapped cross-attention mechanism in 3D windows to address temporal distortions, though its performance heavily depends on the distribution of the training data. AnimateDiff [28] incorporated a trained plug-in motion module within the existing T2I architecture, achieving high-quality video generation but failing to optimally align complex texts with the generated videos. Text2Performer [29] focused on the appearance and motion of a human performer to tackle text-video alignment, yet it predominantly generated videos with clean backgrounds and struggled with more complex environments. Lumier [15] introduced a space-time model capable of generating high-quality videos through a multi-diffusion framework; however, its network parameters are not yet open-sourced, hindering community evaluation of its text-video alignment capabilities. VSTAR [30] introduced the concept of temporal nursing in T2V to address the shortcomings of previous models in generating longer videos. Despite tackling longer video generation, it still falls short in optimally aligning videos with complex and lengthy prompts. Following this, Mora [31] proposed to use multiple visual agents for generating longer videos, up to 10 seconds, but it still struggles to encompass all objects mentioned in the text. Consequently, given the existing shortcomings in aligning text with generated videos in T2V models, researchers have explored and tackled similar issues in T2I models [32], [33]. A new direction suggests using powerful T2I generative models [34] that can optimally align generated images with the provided text. These images could then be used as a basis for video generation, suggesting a shift towards an image-to-video generation approach."}, {"title": "B. Image-to-Video Generation", "content": "VideoCrafter-I2V [35] is a pioneering model in image-to-video (I2V) generation capable of producing videos that adhere to the style, content, and structure of a given reference image. Despite its success, it faces challenges such as unsatisfactory facial representations and inconsistent subject portrayal. ConsistI2V [36] proposes a method that enhances subject consistency by utilizing the low-frequency band of the first frame for the noise initialization process. However, it struggles to provide realistic motion, and the videos often exhibit limited motion magnitude, restricting subject movement. I2V-Adapter [37] preserves the identity of the reference image using cross-frame attention in its architecture. As of this writing, the model's code and checkpoints have not been released, preventing further community assessment of its consistency and motion realism. SIENE [38] attempts to generate long videos at the story level, featuring smooth motion transitions between frames, using an auto-regressive video prediction approach. Although successful in generating story-level videos, it falls short in maintaining background consistency and the aesthetic quality of the frames. Following this, I2VGen-XL [39] proposes a two-stage video generation process. The first stage, called the base stage, ensures semantic coherency and preserves content, while the second stage, called the refinement stage, enhances details and improves video quality. Despite its high performance, it tends to produce more static frames compared to its competitors, thus limiting the range of subject motions. DynamicCrafter [40] addresses the domain limitations of existing video generation models with a dual stream image injection approach that leverages motion priors for open-domain video generation. Despite its success in generating open-domain videos, it faces challenges in maintaining subject consistency when generating videos from complex and highly detailed reference images.\nStable Video Diffusion (SVD) [20] is an advanced latent video diffusion model designed for high-quality I2V generation. It employs a structured three-phase training process: firstly, image pre-training on the well-known model SD-21 to develop robust visual representations; secondly, video pre-training using a large, specially curated video dataset influenced by human preferences; and thirdly, fine-tuning on a select group of high-resolution videos for enhanced quality. This comprehensive approach, enhanced by selective data curation and the integration of temporal layers into the image model, enables SVD to effectively capture dynamic motion and surpass other video generation models in performance. Despite SVD's success in generating high-quality videos with consistent subjects and backgrounds, it is relatively computationally costly. Consequently, in this paper, we introduce VCUT, a method that enhances efficiency. VCUT achieves this by eliminating Temporal Cross Attention and replacing Spatial Cross-Attention with a simple linear layer that is computed once, cached, and reused in semantic binding stages, maintaining the aesthetics and quality of the videos. This integration of VCUT into the SVD framework reduces the computational load by up to 322T Multiple-Accumulate Operations (MACs) per video and cuts down up to 50M parameters, which results in a 20% reduction in latency compared to the baseline model, without the need for additional training costs."}, {"title": "III. PROPOSED METHOD", "content": "VCUT is a training-free video generation approach optimized to increase the efficiency of the SVD-based video generation architectures. Let $(x_{t,t,y}), t \\in \\{1,\\dots,T\\}$, represent a sequence of spatial-temporal denoising UNets with gradients $\\mathcal{V}_{\\theta}$ over a batch. Sampling in the Latent Diffusion Model (LDM) of video generation employs a decoder to generate a series of RGB frames $x \\in \\mathbb{R}^{b \\times f \\times H \\times W \\times 3}$ from the latent space $z \\in \\mathbb{R}^{b \\times f \\times c \\times h \\times w}$ that is conditioned on an input image $y$. Here, $b$ represents the batch size, $f$ denotes the number of frames with height $H$ and width $W$; likewise, $h$, $w$, and $c$ denote the height, width, and the number of channels of the frames in the latent code. The conditional LDM is trained through T steps:\n$\\mathcal{L}_{LDM} = \\mathbb{E}_{z, y, \\epsilon \\sim \\mathcal{N}(0, 1)} [\\| \\epsilon - \\epsilon_{\\theta}(z_t, t, \\tau(y)) \\|_2^2 ] .$ (1)\nIn this equation, $\\tau(y)$ is the a pre-trained image embedding that projects the input image $y$ into an intermediate representation, and $\\theta$ represents the learnable spatial and temporal parameters of the network.\nAs indicated by Eq. 1, a key step in the I2V generation process is embedding the given image $y$ into a space that guides video generation through spatial and temporal cross-attention. The predominant method for image embedding, as used in [41], [20], [42], [43], [39], is CLIP image embedding [44].\nIn this paper, we examine the critical role of this image embedding within the SVD architecture [20], [14] and its impact on network design. Building on the selected CLIP image embedding used in the SVD family, we propose an enhanced and optimized architecture. This new design significantly improves computational efficiency in video diffusion (VD) without compromising video generation quality."}, {"title": "A. CLIP Image Embedding for Temporal Feature Representation", "content": "One of the main differences between T2I and I2V generation is the temporal dimension, which plays a crucial role in subject and background consistency. In the SVD framework [20], the temporal dimension is addressed by two key blocks:\n\u2022 TemporalResnetBlock: Consist of residual blocks based on 3D convolutions [14].\n\u2022 TemporalBasicTransformerBlock: It is designed to ensure subject and background consistency. This block comprises:\nTemporal Self-Attention (TSA): Includes 16 attention mechanisms, including 6 in the encoder, 1 in the middle, and 9 in the decoder section of the denoising Unet. It manages intra-frame dependencies to maintain temporal continuity within the video.\nTemporal Cross-Attention (TCA): Includes 16 attention mechanisms, with 6 in the encoder, 1 in the middle, and 9 in the decoder section of the denoising Unet. It is designed to direct the temporal aspects of generation based on the guided signals (embeddings).\nTo dig deeper into how these temporal attentions perform, consider a 5-D video tensor z, which has dimensions for batch size b, number of latent channels c, number of frames f, height h, and width w. During temporal attention processing, the spatial dimensions (height and width) are merged with the batch dimension. Consequently, the tensor is reshaped into new sequences formatted as $[b \\times h \\times w, f, c]$. This sequence is then utilized as the query Q, key K, and value V in the self-attention mechanism is computed as:\n$TSA(z) = attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$ (2)"}, {"title": "B. CLIP Image Embedding for Spatial Feature Representation", "content": "In addition to the temporal dimension, the spatial dimension also plays a crucial role in video generation, ensuring the quality of the generated video [47]. In SVD architecture [20], spatial dimension is addressed by two important blocks:\nResnetBlock2D: Primarily applies 2D convolutions [14] in its ResNet in a way that interprets the video as a batch of independent images.\nBasicTransformerBlock: Designed to capture the most relevant information in the spatial dimension and guide the video generation in the spatial perspective process.\nThis block comprises:\nSpatial Self-Attention (SSA): which consists of 16 attention mechanisms, including 6 in the encoder, 1 in the middle, and 9 in the decoder section of denoising Unet.\nSpatial Cross-Attention (SCA): which consists of 16 attention mechanisms applied after SSA, including 6 in the encoder, 1 in the middle, and 9 in decoder section of denoising Unet.\nTo further explore the attention mechanism from the spatial perspective, consider the previously defined 5D video tensor z with shape [b, c, f, h, w]. Following the method described by SVD[20], the temporal axis is shifted into the batch dimension, interpreting frames as independent images. Consequently, the tensor is reshaped into new sequences formatted as $[b \\times f, h \\times w, c]$. This sequence is then used as Q, K, and V for calculating the SSA according to Equation 2, resulting in an output tensor of SSA $\\in \\mathbb{R}^{(b \\times f) \\times (h \\times w) \\times c}$.\nIn SCA, Q is the spatial sequence with shape $[b \\times f, hxw, c]$. K and V are obtained from the CLIP image embedding of the initial image. The shape of this embedding is [b, 1, 1024]. To match dimensions, embeddings are broadcast across the batch dimension, resulting in the shape $[b \\times f, 1, 1024]$. After passing through a linear layer, K and V attain the shape [b $\\times$ f, 1, c]. Consequently, the attention score $QK^T$ has the shape [b \u00d7 f, h \u00d7 w, 1], implying that there is only one key for each query to interact with, which makes the attention score trivial because all pixels in Q attend to only one feature vector in K. Thus, similar to TCA, regardless of Q, the output of the spatial attention score will always be one, suggesting that the choice of CLIP image embedding makes the cross-attention mechanism trivial and ineffective. This leads us to the second insight of this paper:"}, {"title": "C. Inference Stages In Stable Video Diffusion", "content": "Based on the insights given in III-B and III-A, the computation of the attention score ($QK^T$) is no longer needed because its output invariably consists of a tensor where every element is one. Consequently, the question arises: Can we completely discard the cross-attention mechanisms? To address this, one might consider that the outputs of both TCA and SCA could be replicated by applying a linear layer to the value V. In other words, the functionality achieved by SVD through its cross-attentions in every inference step can be replicated more simply. This is done by applying a linear layer to the CLIP image embedding of the initial frame only once in the first step. The result is then cached and reused in subsequent inference steps. This approach results in significantly fewer parameters and a lower computational cost.\nHowever, a pertinent question arises: Can CLIP image embedding capture temporal information, although they were not originally designed for this purpose but based on textual-visual alignment through contrastive loss?\nTo answer this question, we conducted a toy experiment by selecting two videos: one with minimal motion (see Fig.1(b)) and another with considerable camera motion, resulting in significant viewpoint changes (see Fig.1(a)). We computed the cosine similarity distance between the embeddings of the first and last frames of each video using two different embedding techniques, CLIP [44] and DINO [48], as shown in Fig. 1.\nThe experiment revealed very similar cosine similarity scores between the first and last frame embeddings for both videos using CLIP image embedding, as depicted in Fig.1. This outcome suggests that CLIP image embeddings are not very effective at extracting temporal features. Notably, in Fig.1(a), there are objects in the first frame (indicated by green boxes) that do not exist in the last frame. Despite these differences, the cosine similarity score between the first and last frame embeddings of this video is very similar to that of another video that shows negligible motion and has a very similar start and ending frames. However, the cosine similarity distance obtained from another image embedding method DINO [46] shows a considerable difference. Specifically, the cosine similarity distance between the first and last frame embeddings of the first video is much greater than that of"}, {"title": "D. Two Stages of Inference in Stable Video Diffusion", "content": "So far, we have demonstrated that we can discard the TCA and replace the SCA with a simple linear layer, leading to faster video generation with fewer parameters required. Now, the question arises: Do we need to apply the classifier-free guidance (CFG) at all diffusion steps, as we do in the SVD architecture and in text-to-image (T2I) generation, considering that we have replaced the cross-attention with a simple linear layer?\nCFG [24] is a method that improves the alignment of generated videos with the initial reference image. It integrates conditional and unconditional generation in all inference steps, enabling improved control over the final output while preserving the quality and diversity of the video [20]. This process is described by the equation below:\n$\\epsilon_{\\theta}(z_t, t) = \\epsilon_{\\theta}(z_t, t, \\oslash) + \\lambda (\\epsilon_{\\theta}(z_t, t, \\tau(y)) - \\epsilon_{\\theta}(z_t, t, \\oslash))$ (3)\nwhere $\\oslash$ denotes unconditional generation, a process that generates videos without considering any reference image and its CLIP image embedding. In the SVD architecture family, the guidance scale, denoted by $\\lambda$, varies linearly with respect to the number of frames. It starts at 1 and increases linearly to a maximum of 3. Inspired by [23], [50], [51] which suggested that guiding the diffusion process with directive signals, such as CLIP text embeddings in image generation, is most effective in the early inference stages and less so in later stages, our analysis in the I2V generation task reveals a similar pattern. As illustrated in Fig. 2, applying CLIP image embedding guidance early on produces the most effective results, whereas its impact diminishes in later stages.\nBased on these observations and drawing from practices in image generation, we have divided the video generation inference process into two stages: the Semantic Binding stage and the Quality Improvement stage. The first stage directs the video generation process to align semantically with the reference image, leveraging the effectiveness of CLIP image embeddings. The second stage, termed the Quality Improvement stage, primarily focuses on the denoising process of diffusion models, where CLIP image embeddings prove to be less effective.\nConsequently, within this two-stage framework, we propose replacing the output of two cross-attention maps, $SCA_T^c$ and $SCA_m^c$, with outputs from two simpler linear layers, $L_{em}$ and $L_{om}$, starting from a specific step $c$ onward. This is defined mathematically as:\n$M = \\frac{1}{l} (L_{em}^{T(\\gamma)} + L_{om}) \\mid m \\in [c, l]$ (4)\nwhere $L_{em}^{T(\\gamma)}$ denotes the linear layer applied to the CLIP image embedding of the reference image, $L_{om}^T$ represents the output from the linear layer for unconditional generation (similar to CFG), and $l$ is the total number of 16 linear layers (previously SCA) applied to the CLIP image embedding of the reference image. Applying VCUT from the specific step c onward means that guidance is only applied during the"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we evaluate the performance of the proposed method from two distinct perspectives to comprehensively assess its efficacy. Firstly, we consider the computational burden of the method, focusing on its efficiency in reducing computation operations. Secondly, we examine the quality of the generated videos. For each of these perspectives, specific metrics are employed to quantitatively measure performance. We will explore the detailed evaluation metrics used for each perspective, providing a structured framework for our analysis."}, {"title": "A. Evaluation of Computational Efficiency", "content": "To assess the computational efficiency of our proposed method, we utilize several key metrics. Firstly, we count the Multiple-Accumulate Operations (MACs) [52], which are crucial for understanding computational complexity. Additionally, we evaluate the total number of parameters (Params.) in our model to determine the impact of proposed changes in terms of a reduction in the number of parameters which is one of the vital criteria in modern computer vision systems [53], [54]. We measure the latency per sample to gauge the time savings achieved by integrating our method into the network. This latency measurement is conducted on an NVIDIA A10 graphics card with 24GB VRAM."}, {"title": "B. Evaluation of Generated Videos", "content": "To precisely assess the effectiveness of the proposed approach, we use the VBench [55] video generation assessment suite, which focuses on various temporal and spatial dimensions,\n1) Temporal Analysis: For the temporal assessment, we use the bellow metrics:\nSubject Consistency: It measures the identity variations of the subject within the video. It employs DINO [46], [48] feature extraction method to compute distances between features extracted from the subject in the generated frames, as described by the following equation:\n$S_{subject} = \\frac{1}{T} \\sum_{t=2}^{T} \\frac{1}{2} ((d_1, d_t) + (d_{t-1}, d_t))$ (5)\nwhere $d_t$ denotes the normalized DINO image feature of the t-th frame, T is the number of frames, and $(.,.)$ represents the dot product for cosine similarity calculations.\nVideo-Image Subject Consistency: It assesses the identity consistency between the subject in the reference image and subsequent frames, using cosine similarity calculations of DINO features as specified:\n$SVI\\text{-Subj-cons} = \\frac{1}{T} \\sum_{t=2}^{T} \\frac{1}{2} ((d_r, d_t) + (d_{t-1}, d_t))$ (6)\nwhere $d_r$ is the normalized DINO image feature of the reference image, aligning with the computation and definitions in Eq. 5.\nBackground Consistency: This metric evaluates the uniformity of the background across different frames to ensure visual continuity. It is calculated using the CLIP image encoder [44] to extract feature vectors:\n$S_{BG\\text{-Consist}} = \\frac{1}{T-1} \\sum_{t=2}^{T} \\frac{1}{2} ((C_1, C_t) + (C_{t-1}, C_t))$ (7)\nwhere $c_i$ represents the normalized CLIP image feature of the i-th frame.\nVideo-Image Background Consistency: It assesses the background consistency between the reference image and subsequent frames using cosine similarity calculations:\n$SVI\\text{-BG-Consist} = \\frac{1}{T-1} \\sum_{t=2}^{T} \\frac{1}{2} ((C_r, C_t) + (C_{t-1}, C_t))$ (8)\nwhere $c_r$ is the normalized CLIP image feature of the reference image, consistent with the parameters in Eq. 7.\nMotion Smoothness: It measures whether the motion in the generated video is smooth and follows the physical law. Specifically, for a video with frames $[f_0, f_1, f_2, f_3, ..., f_{2n-2}, f_{2n-1}, f_{2n}]$, the odd-numbered frames are removed and video frame interpolation [56] is applied to estimate the missing frames $[f_1, f_3, ..., f_{2n-1}]$. The MAE is then calculated between these interpolated frames and the original odd-numbered frames. This MAE is normalized resulting in a final score ranging from [0, 1], where a higher score denotes greater smoothness of motion.\nDynamic Degree: It measures how a model tends to generate static videos that result in a lack of motion. To calculate this, the average of the largest 5% of RAFT [57] optical flows is considered as the basis. The final Dynamic Degree score is calculated by measuring the proportion of non-static videos generated by the model."}, {"title": "E. Impact of Removing TCA and Replacing SCA with Linear Layer", "content": "As demonstrated in Fig. 1 through a toy experiment, CLIP Image embeddings are not ideal for preserving temporal information in video frames. To quantitatively assess the impact of removing the TCA and replacing the SCA with a linear layer on the video generation process, we modified SVD family models accordingly and evaluated the generated videos using the Vbench image benchmark suite [55] from various aspects, including temporal consistency, motion capability, and imaging quality. The results, presented in Table I, show that the performance metrics do not significantly decline, indicating the ineffectiveness of TCA for temporal consistency (both subject and background). Furthermore, this suggests that SCA can be replaced with a simple linear layer without compromising the aesthetics or image quality of the generated videos.\nMoreover, the results show that the proposed changes lead to an increase in the dynamic degree metric, which we believe allows the generated video to exhibit more motion than the original architecture. This enhancement occurs because the modified approach removes the temporal restrictions imposed by the TCA on each frame's adherence to the CLIP image embedding of the reference image. Originally, this adherence did not improve the consistency metric and actually diminished the dynamic degree."}, {"title": "F. Computation Complexity Improvement over Base Models", "content": "We implemented the proposed modifications in cross-attention across all three models of the SVD family. As demonstrated in Table II, discarding TCA and replacing SCA with a simpler linear layer leads to significant reductions at each diffusion step-up to 1.5 trillion MACs and millions of parameters. This reduction is crucial, as the diffusion process involves multiple denoising steps, cumulatively enhancing computational efficiency throughout the video generation process."}, {"title": "G. Quality preservation of the Base Models", "content": "In Table IV, we present a quantitative analysis of integrating the proposed VCUT method into the SVD family at various empirical cut steps c = 10, 17, and 20. The results show that applying the VCUT technique at early steps, such as c = 10, significantly decreases the computation burden (Latency) up to 31%, 30%, and 30% for SVD, SVD-XT, and SVD-XT.1 correspondingly, but at the cost of compromising the consistency-related metrics (subject and background consistency) and video quality (Aesthetics and Imaging Quality). This shows that in the Semantic Binding stages of the video generation process, utilizing the CLIP image embedding and CFG is necessary to ensure the quality of generated videos.\nHowever, integrating the VCUT technique at later steps (c = 17 or 20) ensures that neither the consistency metrics (subject and background consistency) nor the quality metrics (Aesthetics and Imaging Quality) of the generated videos are compromised, and simultaneously boosts the generation speed up to 20%, 19%, and 19% for SVD, SVD-XT, and SVD-XT.1, respectively.\nIt is important to note that integrating the VCUT at c = 17 and c = 20 steps also leads to higher Dynamic Degree metrics, indicating the production of more dynamic videos. This is particularly relevant as many existing video generation frameworks primarily produce highly static frames [39], [38], which can misleadingly inflate consistency-related scores. These frameworks often fail to achieve acceptable motion in frames, a challenge that the VCUT integration addresses by enhancing video dynamism.\nIn Fig. 3, we present samples from videos generated using the standard SVD and the VCUT-integrated versions. The first two rows show a shark example where VCUT integration enhances motion towards the camera, illustrating increased dynamism and justifying the improved dynamic degree metric shown in Table IV. The next two rows, highlighted by red boxes, demonstrate that VCUT does not compromise but in some cases rather enhances the consistency of the generated teapot video. This suggests that removing TCA does not negatively impact, and can even improve, consistency by reducing the restrictions imposed by alignment with CLIP image embeddings, which predominantly preserve global rather than local fine-grained features.\nThe final two rows of Fig. 3 show that integrating VCUT with the base SVD network does not degrade, and can enhance, the spatial quality of the generated frames. In some cases, indicated by red boxes, the butterfly appears less blurry, demonstrating an improvement in frame quality."}, {"title": "V. CONCLUSION", "content": "In this paper, we explore the role of the Cross-Attention mechanism within the Stable Video Diffusion (SVD) family and its application in guiding the video generation process based on CLIP image embeddings. We demonstrate that neither temporal nor spatial cross-attention is essential within the SVD framework. Temporal cross-attention can be entirely discarded, and spatial cross-attention can be replaced with a simple linear layer without sacrificing the consistency and quality metrics of the generated videos. Based on these insights and an analysis of how granularly CLIP image embeddings preserve features of the input image, we propose the VCUT method. VCUT is a training-free approach that can be integrated into the SVD family during inference to accelerate video generation without compromising quality."}]}