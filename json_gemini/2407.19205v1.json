{"title": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's Impact on Spatio-Temporal Cross-Attentions", "authors": ["Ashkan Taghipour", "Morteza Ghahremani", "Mohammed Bennamoun", "Aref Miri Rekavandi", "Zinuo Li", "Hamid Laga", "Farid Boussaid"], "abstract": "This paper investigates the role of CLIP image embeddings within the Stable Video Diffusion (SVD) framework, focusing on their impact on video generation quality and computational efficiency. Our findings indicate that CLIP embeddings, while crucial for aesthetic quality, do not significantly contribute towards the subject and background consistency of video outputs. Moreover, the computationally expensive cross-attention mechanism can be effectively replaced by a simpler linear layer. This layer is computed only once at the first diffusion inference step, and its output is then cached and reused throughout the inference process, thereby enhancing efficiency while maintaining high-quality outputs. Building on these insights, we introduce the VCUT, a training-free approach optimized for efficiency within the SVD architecture. VCUT eliminates temporal cross-attention and replaces spatial cross-attention with a one-time computed linear layer, significantly reducing computational load. The implementation of VCUT leads to a reduction of up to 322T Multiple-Accumulate Operations (MACs) per video and a decrease in model parameters by up to 50M, achieving a 20% reduction in latency compared to the baseline. Our approach demonstrates that conditioning during the Semantic Binding stage is sufficient, eliminating the need for continuous computation across all inference steps and setting a new standard for efficient video generation.", "sections": [{"title": "I. INTRODUCTION", "content": "Advancements in generative AI have boosted the evolution of innovative technology, initially sparked by generative adversarial networks (GANs) [1], [2], [3], across various fields [4] such as image generation [5], [6], [7], motion generation [8], robotics [9], and video generation [10]. Although there is a growing interest in video modeling, progress is not keeping pace with advancements in image generation and editing. This slower progress can be attributed to several factors, including the high computational demands associated with training on video data [11], [12], the scarcity of comprehensive and publicly available video datasets [13], and the complexity inherent in video generation architectures [14], [15], [16]. Recent developments in diffusion models have garnered considerable attention for their potential in video generation. Building on extensive research in text-to-image (T2I) generation, researchers initially started to explore text-to-video (T2V) models [17]. These models align video content with corresponding text inputs, producing videos that reflect the provided textual descriptions. However, text-conditioned video generation imposes significant computational burdens, and the alignment between text and video is not optimal [18]. Consequently, recent research has shifted towards image-conditioned video generation. In this approach, a model uses a given initial image, which can be obtained using an off-the-shelf T2I generative model, to generate subsequent video frames. This significantly reduces computational demands and increases generation speed compared to text-based methods. Similar to the T2I generative model, Stability AI [19] has open-sourced the Stable Video Diffusion (SVD) model [20] for the image-to-video (I2V) task, positioning it as a pioneering development.\nA pivotal component in diffusion architecture that enables the conditioning of generation is the Cross-Attention (CA) mechanism [21]. The CA mechanism plays a crucial role in aligning different modalities, such as text and images, for tasks involving image and video generation. Several studies have explored the importance of CA for spatial control in image generation tasks [22], [23], [7]. However, few, if any, have analyzed the role of CA in video generation during the denoising process, particularly from spatial and temporal perspectives.\nIn this study, we address three new questions regarding the role of CA in the SVD architecture of diffusion models:\n\u2022\n\u2022\n\u2022\n\"Is the CLIP image embedding an effective choice for aligning spatial and temporal features in the I2V generation task?\"\n\"If necessary, can other computationally efficient architectures achieve the same results as the costly Cross-Attention for conditioning the video generation process?\u201d\n\"Regardless of the architectural selection, be it Cross-Attention or another, is it essential to condition the video generation process at every inference step?\"\nTo investigate these queries, we examine the effectiveness"}, {"title": "II. RELATED WORKS", "content": "Following the success of diffusion models in the T2I generation, research in the field of T2V has gained more attention. A pioneering work in T2V, LVDM [25], adapted image diffusion models by transforming its 2D UNet architecture into a 3D UNet and trained it on a vast dataset. ModelScope [26], inspired by LVDM's approach, utilized a more diverse dataset but struggled with handling compositional text descriptions. T2V-Zero [17], employing a training-free approach, converted the image diffusion self-attention mechanisms into cross-frame attention; however, its generated motion lacked realism due to the warping method used for frame generation. VideoFactory [27] introduced a swapped cross-attention mechanism in 3D windows to address temporal distortions, though its performance heavily depends on the distribution of the training data. AnimateDiff [28] incorporated a trained plug-in motion module within the existing T2I architecture, achieving high-quality video generation but failing to optimally align complex texts with the generated videos. Text2Performer [29] focused on the appearance and motion of a human performer to tackle text-video alignment, yet it predominantly generated videos with clean backgrounds and struggled with more complex environments. Lumier [15] introduced a space-time model capable of generating high-quality videos through a multi-diffusion framework; however, its network parameters are not yet open-sourced, hindering community evaluation of its text-video alignment capabilities. VSTAR [30] introduced the concept of temporal nursing in T2V to address the shortcomings of previous models in generating longer videos. Despite tackling longer video generation, it still falls short in optimally aligning videos with complex and lengthy prompts. Following this, Mora [31] proposed to use multiple visual agents for generating longer videos, up to 10 seconds, but it still struggles to encompass all objects mentioned in the text. Consequently, given the existing shortcomings in aligning text with generated videos in T2V models, researchers have explored and tackled similar issues in T2I models [32], [33]. A new direction suggests using powerful T2I generative models [34] that can optimally align generated images with the provided text. These images could then be used as a basis for video generation, suggesting a shift towards an image-to-video generation approach."}, {"title": "III. PROPOSED METHOD", "content": "VCUT is a training-free video generation approach optimized to increase the efficiency of the SVD-based video generation architectures. Let $(x_{t,t,y}), t \\in {1,\u2026\u2026,T}\\$, represent a sequence of spatial-temporal denoising UNets with gradients Ve over a batch. Sampling in the Latent Diffusion Model (LDM) of video generation employs a decoder to generate a series of RGB frames x \u2208 Rbxf\u00d7H\u00d7W\u00d73 from the latent space z \u2208 Rbxfxcxhxw that is conditioned on an input image y. Here, b represents the batch size, f denotes the number of frames with height H and width W; likewise, h, w, and c denote the height, width, and the number of channels of the frames in the latent code. The conditional LDM is trained through T steps:\n$L_{LDM} = E_{z,y,\\epsilon~N(0,1)} [||\\epsilon - \\epsilon_{\\theta} (Z_t, t, \\tau(y))||^2]\\$ . (1)\nIn this equation, \u03c4(y) is the a pre-trained image embedding that projects the input image y into an intermediate representation, and 0 represents the learnable spatial and temporal parameters of the network.\nAs indicated by Eq. 1, a key step in the I2V generation process is embedding the given image y into a space that guides video generation through spatial and temporal cross-attention. The predominant method for image embedding, as used in [41], [20], [42], [43], [39], is CLIP image embedding [44].\nIn this paper, we examine the critical role of this image embedding within the SVD architecture [20], [14] and its impact on network design. Building on the selected CLIP image embedding used in the SVD family, we propose an enhanced and optimized architecture. This new design significantly improves computational efficiency in video diffusion (VD) without compromising video generation quality."}, {"title": "A. CLIP Image Embedding for Temporal Feature Representation", "content": "One of the main differences between T2I and I2V generation is the temporal dimension, which plays a crucial role in subject and background consistency. In the SVD framework [20], the temporal dimension is addressed by two key blocks:\n\u2022 TemporalResnetBlock: Consist of residual blocks based on 3D convolutions [14].\n\u2022 TemporalBasicTransformerBlock: It is designed to ensure subject and background consistency. This block comprises:\nTemporal Self-Attention (TSA): Includes 16 attention mechanisms, including 6 in the encoder, 1 in the middle, and 9 in the decoder section of the denoising Unet. It manages intra-frame dependencies to maintain temporal continuity within the video.\nTemporal Cross-Attention (TCA): Includes 16 attention mechanisms, with 6 in the encoder, 1 in the middle, and 9 in the decoder section of the denoising Unet. It is designed to direct the temporal aspects of generation based on the guided signals (embeddings).\nTo dig deeper into how these temporal attentions perform, consider a 5-D video tensor z, which has dimensions for batch size b, number of latent channels c, number of frames f, height h, and width w. During temporal attention processing, the spatial dimensions (height and width) are merged with the batch dimension. Consequently, the tensor is reshaped into new sequences formatted as [b \u00d7 h \u00d7 w, f, c]. This sequence is then utilized as the query Q, key K, and value V in the self-attention mechanism is computed as:\n$TSA(z) = attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}}) V\\$ (2)"}, {"title": "B. CLIP Image Embedding for Spatial Feature Representation", "content": "In addition to the temporal dimension, the spatial dimension also plays a crucial role in video generation, ensuring the quality of the generated video [47]. In SVD architecture [20], spatial dimension is addressed by two important blocks:\n\u2022 ResnetBlock2D: Primarily applies 2D convolutions [14] in its ResNet in a way that interprets the video as a batch of independent images.\n\u2022 BasicTransformerBlock: Designed to capture the most relevant information in the spatial dimension and guide the video generation in the spatial perspective process. This block comprises:\nSpatial Self-Attention (SSA): which consists of 16 attention mechanisms, including 6 in the encoder, 1 in the middle, and 9 in the decoder section of denoising Unet.\nSpatial Cross-Attention (SCA): which consists of 16 attention mechanisms applied after SSA, including 6 in the encoder, 1 in the middle, and 9 in decoder section of denoising Unet.\nTo further explore the attention mechanism from the spatial perspective, consider the previously defined 5D video tensor z with shape [b, c, f, h, w]. Following the method described by SVD[20], the temporal axis is shifted into the batch dimension, interpreting frames as independent images. Consequently, the tensor is reshaped into new sequences formatted as [b \u00d7 f, h \u00d7 w, c]. This sequence is then used as Q, K, and V for calculating the SSA according to Equation 2, resulting in an output tensor of SSA \u2208 R(b\u00d7f)\u00d7(hxw)\u00d7c.\nIn SCA, Q is the spatial sequence with shape [b\u00d7f, hxw,c]. K and V are obtained from the CLIP image embedding of the initial image. The shape of this embedding is [b, 1, 1024]. To match dimensions, embeddings are broadcast across the batch dimension, resulting in the shape [b \u00d7 f, 1, 1024]. After passing through a linear layer, K and V attain the shape [b \u00d7 f, 1, c]. Consequently, the attention score QKT has the shape [b \u00d7 f, h \u00d7 w, 1], implying that there is only one key for each query to interact with, which makes the attention score trivial because all pixels in Q attend to only one feature vector in K. Thus, similar to TCA, regardless of Q, the output of the spatial attention score will always be one, suggesting that the choice of CLIP image embedding makes the cross-attention mechanism trivial and ineffective."}, {"title": "C. Inference Stages In Stable Video Diffusion", "content": "Based on the insights given in III-B and III-A, the computation of the attention score (QKT) is no longer needed because its output invariably consists of a tensor where every element is one. Consequently, the question arises: Can we completely discard the cross-attention mechanisms? To address this, one might consider that the outputs of both TCA and SCA could be replicated by applying a linear layer to the value V. In other words, the functionality achieved by SVD through its cross-attentions in every inference step can be replicated more simply. This is done by applying a linear layer to the CLIP image embedding of the initial frame only once in the first step. The result is then cached and reused in subsequent inference steps. This approach results in significantly fewer parameters and a lower computational cost.\nHowever, a pertinent question arises: Can CLIP image embedding capture temporal information, although they were not originally designed for this purpose but based on textual-visual alignment through contrastive loss?\nTo answer this question, we conducted a toy experiment by selecting two videos: one with minimal motion (see Fig.1(b)) and another with considerable camera motion, resulting in significant viewpoint changes (see Fig.1(a)). We computed the cosine similarity distance between the embeddings of the first and last frames of each video using two different embedding techniques, CLIP [44] and DINO [48], as shown in Fig. 1. The experiment revealed very similar cosine similarity scores between the first and last frame embeddings for both videos using CLIP image embedding, as depicted in Fig.1. This outcome suggests that CLIP image embeddings are not very effective at extracting temporal features."}, {"title": "D. Two Stages of Inference in Stable Video Diffusion", "content": "So far, we have demonstrated that we can discard the TCA and replace the SCA with a simple linear layer, leading to faster video generation with fewer parameters required. Now, the question arises: Do we need to apply the classifier-free guidance (CFG) at all diffusion steps, as we do in the SVD architecture and in text-to-image (T2I) generation, considering that we have replaced the cross-attention with a simple linear layer?\nCFG [24] is a method that improves the alignment of generated videos with the initial reference image. It integrates conditional and unconditional generation in all inference steps, enabling improved control over the final output while preserving the quality and diversity of the video [20]. This process is described by the equation below:\n$\\epsilon_{\\theta}(Z_t, t) = \\epsilon_{\\theta}(Z_t, t, \u00d8) + \\lambda (e_{\\theta} (z_t, t, \\tau(y)) \u2013 e_{\\theta}(z_t, t, \u00d8))\\$ (3)"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we evaluate the performance of the proposed method from two distinct perspectives to comprehensively assess its efficacy. Firstly, we consider the computational burden of the method, focusing on its efficiency in reducing computation operations. Secondly, we examine the quality of the generated videos. For each of these perspectives, specific metrics are employed to quantitatively measure performance. We will explore the detailed evaluation metrics used for each perspective, providing a structured framework for our analysis."}, {"title": "A. Evaluation of Computational Efficiency", "content": "To assess the computational efficiency of our proposed method, we utilize several key metrics. Firstly, we count the Multiple-Accumulate Operations (MACs) [52], which are crucial for understanding computational complexity. Additionally, we evaluate the total number of parameters (Params.) in our model to determine the impact of proposed changes in terms of a reduction in the number of parameters which is one of the vital criteria in modern computer vision systems [53], [54]. We measure the latency per sample to gauge the time savings achieved by integrating our method into the network. This latency measurement is conducted on an NVIDIA A10 graphics card with 24GB VRAM."}, {"title": "B. Evaluation of Generated Videos", "content": "To precisely assess the effectiveness of the proposed approach, we use the VBench [55] video generation assessment suite, which focuses on various temporal and spatial dimensions,\n1) Temporal Analysis: For the temporal assessment, we use the bellow metrics:\n\u2022 Subject Consistency: It measures the identity variations of the subject within the video. It employs DINO [46], [48]"}, {"title": "E. Impact of Removing TCA and Replacing SCA with Linear Layer", "content": "As demonstrated in Fig. 1 through a toy experiment, CLIP Image embeddings are not ideal for preserving temporal information in video frames. To quantitatively assess the impact of removing the TCA and replacing the SCA with a linear layer on the video generation process, we modified SVD family models accordingly and evaluated the generated videos using the Vbench image benchmark suite [55] from various aspects, including temporal consistency, motion capability, and imaging quality. The results, presented in Table I, show that the performance metrics do not significantly decline, indicating the ineffectiveness of TCA for temporal consistency (both subject and background). Furthermore, this suggests that SCA can be replaced with a simple linear layer without compromising the aesthetics or image quality of the generated videos.\nMoreover, the results show that the proposed changes lead to an increase in the dynamic degree metric, which we believe allows the generated video to exhibit more motion than the original architecture. This enhancement occurs because the modified approach removes the temporal restrictions imposed by the TCA on each frame's adherence to the CLIP image embedding of the reference image. Originally, this adherence did not improve the consistency metric and actually diminished the dynamic degree."}, {"title": "F. Computation Complexity Improvement over Base Models", "content": "We implemented the proposed modifications in cross-attention across all three models of the SVD family. As demonstrated in Table II, discarding TCA and replacing SCA with a simpler linear layer leads to significant reductions at each diffusion step-up to 1.5 trillion MACs and millions of parameters. This reduction is crucial, as the diffusion process involves multiple denoising steps, cumulatively enhancing computational efficiency throughout the video generation process."}, {"title": "G. Quality preservation of the Base Models", "content": "In Table IV, we present a quantitative analysis of integrating the proposed VCUT method into the SVD family at various empirical cut steps c = 10, 17, and 20. The results show that applying the VCUT technique at early steps, such as c = 10, significantly decreases the computation burden (Latency) up to 31%, 30%, and 30% for SVD, SVD-XT, and SVD-XT.1 correspondingly, but at the cost of compromising the consistency-related metrics (subject and background consistency) and video quality (Aesthetics and Imaging Quality). This shows that in the Semantic Binding stages of the video generation process, utilizing the CLIP image embedding and CFG is necessary to ensure the quality of generated videos.\nHowever, integrating the VCUT technique at later steps (c = 17 or 20) ensures that neither the consistency metrics (subject and background consistency) nor the quality metrics (Aesthetics and Imaging Quality) of the generated videos are compromised, and simultaneously boosts the generation speed up to 20%, 19%, and 19% for SVD, SVD-XT, and SVD-XT.1, respectively.\nIt is important to note that integrating the VCUT at c = 17 and c = 20 steps also leads to higher Dynamic Degree metrics, indicating the production of more dynamic videos. This is particularly relevant as many existing video generation frameworks primarily produce highly static frames [39], [38], which can misleadingly inflate consistency-related scores. These frameworks often fail to achieve acceptable motion in frames, a challenge that the VCUT integration addresses by enhancing video dynamism."}, {"title": "V. CONCLUSION", "content": "In this paper, we explore the role of the Cross-Attention mechanism within the Stable Video Diffusion (SVD) family and its application in guiding the video generation process based on CLIP image embeddings. We demonstrate that neither temporal nor spatial cross-attention is essential within the SVD framework. Temporal cross-attention can be entirely discarded, and spatial cross-attention can be replaced with a simple linear layer without sacrificing the consistency and quality metrics of the generated videos. Based on these insights and an analysis of how granularly CLIP image embeddings preserve features of the input image, we propose the VCUT method. VCUT is a training-free approach that can be integrated into the SVD family during inference to accelerate video generation without compromising quality."}]}