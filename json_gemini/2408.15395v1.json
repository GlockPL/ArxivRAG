{"title": "SCAN-Edge: Finding MobileNet-speed Hybrid Networks for Diverse Edge Devices via Hardware-Aware Evolutionary Search", "authors": ["Hung-Yueh Chiang", "Diana Marculescu"], "abstract": "Designing low-latency and high-efficiency hybrid networks for a variety of low-cost commodity edge devices is both costly and tedious, leading to the adoption of hardware-aware neural architecture search (NAS) for finding optimal architectures. However, unifying NAS for a wide range of edge devices presents challenges due to the variety of hardware designs, supported operations, and compilation optimizations. Existing methods often fix the search space of architecture choices (e.g., activation, convolution, or self-attention) and estimate latency using hardware-agnostic proxies (e.g., FLOPs), which fail to achieve proclaimed latency across various edge devices. To address this issue, we propose SCAN-Edge, a unified NAS framework that jointly searches for Self-attention, Convolution, and Activation to accommodate the wide variety of edge devices, including CPU-, GPU-, and hardware accelerator-based systems. To handle the large search space, SCAN-Edge relies on with a hardware-aware evolutionary algorithm that improves the quality of the search space to accelerate the sampling process. Experiments on large-scale datasets demonstrate that our hybrid networks match the actual MobileNetV2 latency for 224 \u00d7 224 input resolution on various commodity edge devices.", "sections": [{"title": "1. Introduction", "content": "Automatically designing deep learning architectures for specific hardware has become a compelling research topic. Both hardware vendors and software companies are developing accelerators to meet the operational budgets (e.g., hardware and energy costs) and serving constraints (e.g., latency and memory) of machine learning applications. Off-the-shelf models often fail to satisfy these constraints across a wide range of accelerators, necessitating the design of custom model architectures tailored to specific hardware platforms. This design process is costly, as it requires machine learning experts to validate the performance of carefully tuned models on large-scale datasets and evaluate their efficiency on actual hardware. Consequently, this issue significantly increases costs, prolongs the design cycle, and delays the deployment of machine learning services.\nA significant body of prior work addresses this issue by proposing frameworks to automatically design optimal model architectures for different hardware or applications. Cai et al. [3] and Yu et al. [45] train a one-shot supernet that searches for MobileNet-like subnets (convolution only) across various devices. Recent approaches, such as those by Gong and Wang [13] and Tang et al. [41], extend these methods to hybrid networks combining convolution and transformers. However, these methods often fix the search space of architectural choices (e.g., GELU activation, depth-wise convolution, or self-attention layers) for network stages. This can be suboptimal for certain edge devices, as the optimal search space largely depends on both hardware implementation and compiler optimization. For instance, a memory-bound operator like depth-wise convolution may not be efficient or optimal for a highly parallelized device [27, 47]. Additionally, prior works like Gong and Wang [13] estimate model complexity using zero-cost proxies [43] such as floating-point operations (FLOPs) and the number of parameters, which do not accurately reflect the actual latency on target edge devices. To illustrate this point, Figure 1 and Section 3 demonstrate that different edge devices favor different operations when actual hardware metrics are considered.\nTo address this issue, we propose a unified NAS framework, SCAN-Edge, which utilizes a weight-sharing supernet to search for a variety of hybrid networks optimized for edge devices. Our search space includes unified feedforward networks (unified FFNs, memory-bound), fused feedforward networks (fused FFNs, compute-bound), and multihead self-attention layers (MHSAs), along with two activation functions, GELU and ReLU, to support a wide variety of commodity devices. During the search process, we incorporate calibrated latency lookup tables (LUTs) pro-"}, {"title": "2. Related Work", "content": "Efficient vision transformers. Though Dosovitskiy et al. [11], Touvron et al. [42], and Liu et al. [25] show the competitive performance of vision transformers (ViTs) against convolutional neural networks (CNNs), deploying ViTs on edge devices is challenging due to the O(n\u00b2) complexity of attention layers. Many works [4, 7, 21, 22, 28, 29, 33] address this issue and propose lightweight attention operators. Li et al. [21] replace self-attention with pooling layers for early-stage high-resolution inputs. Li et al. [22] and Pan et al. [33] downsample the attention resolution in exchange for model efficiency. However, the lightweight operators with fewer floating-point operations (FLOPS) are not necessarily friendly to edge devices. For instance, MobileViT [29] includes fold and unfold mobile-unfriendly operations to reorder the data memory before processing, which incurs additional latency during inference. We design our supernet with a search space based on EfficientFormerV2 [22] with MobilenetV2-like [38] feedforward layers (FFNs) and global-local attention layers, which are friendly to hardware platforms.\nNeural architecture search. NAS has achieved huge success in CNNs [3, 39, 45]. Recent approaches [5, 6, 24, 40] apply NAS to automatically design and improve ViTs architectures. Chen et al. [5] propose a weight-entangled ViT supernet where all subnets can be trained at once and without further fine-tuning before deployment. Chen et al. [6] use a high-quality search space before identifying the ViTs. However, the patch embedding layer with large kernel sizes and non-overlapping stride sizes are ill-supported by edge devices [21]. Hybrid networks (CNN-ViT) [14, 21] replace the patch embedding layer with a sequence of CNN stages to reduce feature resolution before passing to transformer stages. Some works [13, 41] focus on searching for efficient hybrid networks for mobile devices. However, Gong and Wang [13] only use FLOPS as the complexity metric during the search process, which does not truly reflect on-device latency. Instead of using FLOPS, we build block-wise latency lookup tables (LUTs) for a wide variety of devices and calibrate the LUT estimations by linear regression from 10 subnet latencies obtained from real measurements. Therefore, our method generalizes to and searches for optimal architectures across various commodity edge devices, characterized by actual device latencies."}, {"title": "3. Preliminary Study", "content": "In our preliminary study, we use EfficientFormerV2 S0 [22] as our base architecture and experiment with different FFNs, activations, and self-attention ratios (expansion of V dimension). We follow EfficientFormerV2 and use the term, feedforward network, for convolution layers.\nZero-cost proxies vs. Latency profiling. Zero-cost proxies such as the number of parameters and floating-point operations (FLOPS) are widely used in NAS for estimating the model complexity and latency due to their immediate availability. However, FLOPS do not accurately capture the actual edge device latency. In Figure 1, we show that estimating model complexity with zero-cost proxies fails to generalize to a wide range of edge devices. For example, fused FFNs, despite having more FLOPS and parameters, have lower latency than MHSA layers on Nano and NCS2."}, {"title": "4. One-shot Supernet", "content": "Search space. We construct our supernet based on the EfficientFormer V2 backbone. The supernet consists of four stages and three embedding layers. The embedding layers adjust the model width, i.e., the channel dimension $C_i$, for the next stage. Each block in the first two stages, $S_1$ and $S_2$, only consists of FFNs. Every block in the last two stages, $S_3$ and $S_4$, consists of a MHSA followed by a FFN. Our search algorithm determines the width $C_i$, the number of FFN blocks $N_{ffn}$ in every stage $S_i$, the number of MHSA $N_{mhsa}$ in the last two stages for the network, and the expansion ratios $E$ (FFN and MHSA) and kernel size $K$ (FFN only) for every block $j$ in stage $i$. The full search space is shown in Table 2.\nDual feedforward network. We design our supernet with dual FFN to best accommodate various edge devices. As shown in Figure 2, in each FFN block, we provide two choices for searching: unified FFN and fused FFN with different kernel sizes (e.g., 3, 5) and expansion ratios (e.g., 2, 3, 4). Only one set of weight matrices will be activated (unified or fused) in a block during training. We empirically find dual FFNs converge slightly better than entangled FFNs (cf. Suppl. S2, and Figure S1). During training, we sample $N_{ffn}$ blocks for every stage $S_i$ and randomly switch between unified FFN and fused FFN. For each FFN block, we sample the kernel size $K$, and expansion ratio $E$. In the search stage, we search the number of FFNs $N_{ffn}$ for every stage $S_i$, and the kernel size $K$, as well as the expansion ratios $E$ for every block $j$ in stage $i$.\nSearching for multi-head self-attention. We design our MHSA with weight entanglement [5]. We allow the V matrices in each MHSA to have larger dimensions [14, 41]. More specifically, we search the expansion ratios $E$ for the value matrix in every MHSA block, such that dim(V) = N_{head} \u00d7 dim(Q-K-V) \u00d7 E, where N_{head} and dim(Q-K-V) are fixed. The dimension of the Q and K matrices are fixed to N_{head} \u00d7 dim(Q-K-V) such that the attention matrices $A= QK^T$ are shared with all subnets. During subnet search, the algorithm finds the optimal subnet by searching the number and the position of MHSA $N_{mhsa}$ and deciding their expansion ratios $E$ for the V matrices in the last two stages (i.e., $i \\in \\{3,4\\}$).Dynamic activation layers. In Section 3, the advanced activation layers such as GELU [15] are not well-supported by edge devices, thereby incurring a latency overhead during inference. We designed our supernet to support ReLU [1], one of the basic activation layers that is hardware-friendly. During training, we randomly switch between two activation layers for FFN and MHSA. Our search algorithm searches for the best activation combinations to optimize latency and accuracy.\nSupernet training algorithm. We apply the sandwich rule [44] to train our supernet which samples the smallest subnet, the biggest (full) subnet, and M randomly sampled subnets (M = 2 in our experiments). To accommodate dual FFN, we first sample the FFN structure (i.e., fused or unified) and apply it to the largest and smallest subnets. For the M randomly sampled subnets, we randomize the FFN selection in every block. The smallest subnet has minimal width, depth, and kernel size in FFNs, with no MHSAs in the last two layers, using only MHSA downsampling in the third embedding layer. Conversely, the largest subnet maximizes width, depth, and kernel size, applying MHSAs before all FFNs in layers S3 and S4. The FNNs and MHSAs are dropped by drop path [16] with probabilities. We scale the FFN dropping probabilities according to the stage depth so that the last FFN in the stage has the highest probability of being dropped. All MHSAs are dropped with a constant probability so that the number as well as the position of MHSAs in the stage can be searched. The full supernet training algorithm is shown in Suppl. 1."}, {"title": "5. Searching Subnets for Edge Devices", "content": "5.1. Search Objective\nGiven a supernet architecture $A = \\{a_1, ..., a_n\\}$ and its trained weight $W = \\{W_1, ..., W_n\\}$, we denote the supernet as $f(A, W)$ and a sampled subnet as $f(a_i, w_i)$, where the architecture $a_i \\in A$ and the weights $w_i \\in W$ are sampled from the supernet. Our search objective is to find an optimal architecture $a^*$ and $w^*$ that maximizes the accuracy while satisfying a set of constraints $C = \\{C_1, ..., C_n\\}$ on a given device, such that\n$\u03b1^*, \u03c9^* = \\underset{\u03b1\u2208A, \u03c9\u2208W}{argmax}Acc(f(\u03b1, \u03c9)) \\ \\ s.t. \u03b6_i(\u03b1, \u03c9) < c_i, i = 1, 2, ..., n.$\nwhere $\u03b6_i$ is the predictor function for the latency or the memory footprint of the subnet.\n5.2. Accuracy Predictor\nSince evaluating thousands of subnets on the validation set during the search process is not practical, we train a neural network $\u03c7$ to predict the accuracy. Every block is encoded with a 24-bit length binary string where stages (4-bit), in/output width (8-bit), expansion ratios (6-bit), FFN types (2-bit), kernel sizes (2-bit), and activation functions (2-bit) are one-hot encoded. The binary string is stacked as a matrix n \u00d7 24 and padded to 44 rows for a n-block subnet resulting in a 44 \u00d7 24 matrix. The network $\u03c7$ is built"}, {"title": "5.3. Latency Lookup Table with Calibration", "content": "The latency predictor is device-dependent. Therefore, we build latency lookup tables (LUTs) for every device and profile all possible blocks in every stage on the device. The total number of blocks is 568 with 4 additional output linear layers for different width choices. However, simply estimating the end-to-end latency by summing up the latencies of all blocks in the LUT will overestimate the actual latency, since the intermediate feature maps are usually cached in the memory instead of loaded from scratch. Therefore, for each device, we additionally profile the end-to-end latency"}, {"title": "5.4. Hardware-aware Search Space Evolution", "content": "Search space quality. We adopt an evolutionary search algorithm [37] to find the optimal subnet for each device. However, sampling subnets that satisfy the constraints (e.g., latency and memory) from a very large search space is difficult (cf. Suppl. S7). The sampling time increases exponentially when we reduce the latency constraint, as shown by the blue line in Figure 4 (a). To address the issue, we update the search space during the search. The search space evolves if the quality of the search space $Q(A)$ defined by the current top k subnets $A = \\{a_1, ..., a_k\\}$ is better than the previous one, such that\n$Q(A) - Q(A^{t-1}) > \u03b4$\nThe quality of the search space is evaluated by the average predicted accuracy of the top k subnets, i.e.,\n$Q(A) = E_{\u03b1 \u2208 A} [\u03a7(\u03b1)]$We maintain separate probability distributions for all stages and update the distribution of each stage at the end of each search episode. As shown in Figure 4 (b), our search space evolves based on the given constraints and remains constant time subnet sampling. The detailed algorithm is shown in the Suppl. S8.\n6. Main Results\n6.1. Experimental Setup\nWe follow the setup of Li et al. [22]. Our models are implemented with PyTorch [35] framework and Timm library [17]. We use 24 A5000 GPUs to train the supernet for 300 epochs with a total batch size of 3072 and use 8 A5000 GPUs to fine-tune the searched subnets for 150 epochs with a total batch size of 2048 on ImageNet 1k training set [10]. Models are validated on the ImageNet validation set. Both training and validation are using the standard resolution 224 x 224. We also use AdamW optimizer [26], set the initial learning rate to $10^{-3} \u00d7 batch \\ size/1024$ to train the supernet, and use $10^{-4} \u00d7 batch \\ size/1024$ to fine-tune the subnets. The cosine decay is applied in both training and fine-tuning. RegNetY-16GF [36] with 82.9% top-1 accuracy are used in supernet training and subnet fine-tuning as the teacher model for hard distillation, as Li et al. [22] and Touvron et al. [42].\nCommodity devices. We profile model latency on three different commodity hardware with their compilers using official benchmark tools. All models are compiled and profiled with batch size 1."}, {"title": "6.2. Image Classification with MobileNet Speed", "content": "The experiments include three different edge devices (Cortex CPU, Nano GPU, and NCS2), four compilers (Default ONNX Execution Provider, Nvidia Cuda and TensorRT, OpenVINO), and two precisions (FP32 and FP16). In this experiment, we optimize the latency with 224 \u00d7 224"}, {"title": "6.3. Joint Optimization of Latency and Model Size", "content": "The smaller models benefit from using the limited memory on edge devices and being easily downloaded from cloud to edge devices. This motivates us to jointly optimize the latency and model size. We experiment with joint optimization on ARM Cortex-A57 and Nvidia Jetson Nano platforms by constraining the search algorithm with both latency and number of parameters. In the experiment, we use"}, {"title": "6.4. Downstream Tasks via Transfer Learning", "content": "We perform transfer learning from the ImageNet pretrained weight to various downstream tasks: CIFAR10, CIFAR100 [19], Food [2], and Pets [34]. All models are trained for 50 epochs with downstream datasets on an A500 GPU and set the batch size to 256 with a $10^{-3}$ base learning rate scaled by the batch size. The results are shown in Table 5. In general, our models outperform in accuracy their counterparts with similar latency in the downstream classification tasks on Cortex-A57. Moreover, our models match the MobileNetV2 latency on Cortex-A57 CPU (cf. Table 3)."}, {"title": "7. Object Detection", "content": "We integrate our searched subnets as the backbone to SSDLite [38]. We train the SSDLite with different backbones on COCO2017 dataset [23] by using the MMDetection library [32]. We load the ImageNet pre-trained weights of each backbone, and train the detection models with a resolution of 320 \u00d7 320. The learning rates are tuned for each model. We deploy models on the Nvidia Jetson Nano 4G by using the MMDeploy library [31] and profile the latency with Nvidia TensorRT profiling tools. As shown in Table 6, our model outperforms MobileViT XXS and EdgeNeXt XXS in both mAP and latency."}, {"title": "8. Conclusion", "content": "We propose a unified NAS framework that searches for hybrid networks with MobileNetV2-speed yet superior accuracy for low-cost commodity edge devices. Our framework incorporates different device-friendly operations for diverse edge devices with different hardware designs. The proposed search algorithm relies on real latency instead of zero-cost proxies (e.g., FLOPs, number of parameters) and reduces the sampling time by search space evolution for a wide variety of edge devices. Our experiments show our hybrid models match MobileNetV2-speed on Edge CPUs, Edge GPUs, and USB accelerators with better accuracy than MobileNet V2."}]}