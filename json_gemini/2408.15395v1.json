{"title": "SCAN-Edge: Finding MobileNet-speed Hybrid Networks for Diverse Edge Devices via Hardware-Aware Evolutionary Search", "authors": ["Hung-Yueh Chiang", "Diana Marculescu"], "abstract": "Designing low-latency and high-efficiency hybrid networks for a variety of low-cost commodity edge devices is both costly and tedious, leading to the adoption of hardware-aware neural architecture search (NAS) for finding optimal architectures. However, unifying NAS for a wide range of edge devices presents challenges due to the variety of hardware designs, supported operations, and compilation optimizations. Existing methods often fix the search space of architecture choices (e.g., activation, convolution, or self-attention) and estimate latency using hardware-agnostic proxies (e.g., FLOPs), which fail to achieve proclaimed latency across various edge devices. To address this issue, we propose SCAN-Edge, a unified NAS framework that jointly searches for Self-attention, Convolution, and Activation to accommodate the wide variety of edge devices, including CPU-, GPU-, and hardware accelerator-based systems. To handle the large search space, SCAN-Edge relies on with a hardware-aware evolutionary algorithm that improves the quality of the search space to accelerate the sampling process. Experiments on large-scale datasets demonstrate that our hybrid networks match the actual MobileNetV2 latency for 224 \u00d7 224 input resolution on various commodity edge devices.", "sections": [{"title": "1. Introduction", "content": "Automatically designing deep learning architectures for specific hardware has become a compelling research topic. Both hardware vendors and software companies are developing accelerators to meet the operational budgets (e.g., hardware and energy costs) and serving constraints (e.g., latency and memory) of machine learning applications. Off-the-shelf models often fail to satisfy these constraints across a wide range of accelerators, necessitating the design of custom model architectures tailored to specific hardware platforms. This design process is costly, as it requires machine learning experts to validate the performance of carefully tuned models on large-scale datasets and evaluate their efficiency on actual hardware. Consequently, this issue significantly increases costs, prolongs the design cycle, and delays the deployment of machine learning services.\nA significant body of prior work addresses this issue by proposing frameworks to automatically design optimal model architectures for different hardware or applications. Cai et al. [3] and Yu et al. [45] train a one-shot supernet that searches for MobileNet-like subnets (convolution only) across various devices. Recent approaches, such as those by Gong and Wang [13] and Tang et al. [41], extend these methods to hybrid networks combining convolution and transformers. However, these methods often fix the search space of architectural choices (e.g., GELU activation, depth-wise convolution, or self-attention layers) for network stages. This can be suboptimal for certain edge devices, as the optimal search space largely depends on both hardware implementation and compiler optimization. For instance, a memory-bound operator like depth-wise convolution may not be efficient or optimal for a highly parallelized device [27, 47]. Additionally, prior works like Gong and Wang [13] estimate model complexity using zero-cost proxies [43] such as floating-point operations (FLOPs) and the number of parameters, which do not accurately reflect the actual latency on target edge devices. To illustrate this point, Figure 1 and Section 3 demonstrate that different edge devices favor different operations when actual hardware metrics are considered.\nTo address this issue, we propose a unified NAS framework, SCAN-Edge, which utilizes a weight-sharing supernet to search for a variety of hybrid networks optimized for edge devices. Our search space includes unified feedforward networks (unified FFNs, memory-bound), fused feedforward networks (fused FFNs, compute-bound), and multi-head self-attention layers (MHSAs), along with two activation functions, GELU and ReLU, to support a wide variety of commodity devices. During the search process, we incorporate calibrated latency lookup tables (LUTs) profiled on target devices and a learning-based accuracy predictor. These LUTs accurately estimate the end-to-end latency of the subnets. To find the optimal subnet from this huge search space, our search algorithm optimizes the search space based on the target hardware. Our experiments demonstrate that hybrid networks discovered by SCAN-Edge match the actual MobileNetV2 latency while providing better accuracy across a wide range of edge devices compared to prior approaches.\nOur contributions are as follows:\n\u2022 We perform a comprehensive analysis of various hardware platforms focusing on self-attention, convolution, and activation operations.\n\u2022 We created a unified framework that optimizes convolution and activation types, and the number and placement of MHSA layers for subnets that meet the latency constraints on a variety of devices.\n\u2022 To handle the large search space, we introduce a hardware- and compiler-aware evolutionary algorithm that enhances search quality and speeds up the sampling process."}, {"title": "2. Related Work", "content": "Efficient vision transformers. Though Dosovitskiy et al. [11], Touvron et al. [42], and Liu et al. [25] show the competitive performance of vision transformers (ViTs) against convolutional neural networks (CNNs), deploying ViTs on edge devices is challenging due to the O(n\u00b2) complexity of attention layers. Many works [4, 7, 21, 22, 28, 29, 33] address this issue and propose lightweight attention operators. Li et al. [21] replace self-attention with pooling layers for early-stage high-resolution inputs. Li et al. [22] and Pan et al. [33] downsample the attention resolution in exchange for model efficiency. However, the lightweight operators with fewer floating-point operations (FLOPS) are not necessarily friendly to edge devices. For instance, MobileViT [29] includes fold and unfold mobile-unfriendly operations to reorder the data memory before processing, which incurs additional latency during inference. We design our supernet with a search space based on EfficientFormerV2 [22] with MobilenetV2-like [38] feedforward layers (FFNs) and global-local attention layers, which are friendly to hardware platforms.\nNeural architecture search. NAS has achieved huge success in CNNs [3, 39, 45]. Recent approaches [5, 6, 24, 40] apply NAS to automatically design and improve ViTs architectures. Chen et al. [5] propose a weight-entangled ViT supernet where all subnets can be trained at once and without further fine-tuning before deployment. Chen et al. [6] use a high-quality search space before identifying the ViTs. However, the patch embedding layer with large kernel sizes and non-overlapping stride sizes are ill-supported by edge devices [21]. Hybrid networks (CNN-ViT) [14, 21] replace the patch embedding layer with a sequence of CNN stages to reduce feature resolution before passing to transformer stages. Some works [13, 41] focus on searching for efficient hybrid networks for mobile devices. However, Gong and Wang [13] only use FLOPS as the complexity metric during the search process, which does not truly reflect on-device latency. Instead of using FLOPS, we build block-wise latency lookup tables (LUTs) for a wide variety of devices and calibrate the LUT estimations by linear regression from 10 subnet latencies obtained from real measurements. Therefore, our method generalizes to and searches for optimal architectures across various commodity edge devices, characterized by actual device latencies."}, {"title": "3. Preliminary Study", "content": "In our preliminary study, we use EfficientFormerV2 S0 [22] as our base architecture and experiment with different FFNs, activations, and self-attention ratios (expansion of V dimension). We follow EfficientFormerV2 and use the term, feedforward network, for convolution layers.\nZero-cost proxies vs. Latency profiling. Zero-cost proxies such as the number of parameters and floating-point operations (FLOPS) are widely used in NAS for estimating the model complexity and latency due to their immediate availability. However, FLOPS do not accurately capture the actual edge device latency. In Figure 1, we show that estimating model complexity with zero-cost proxies fails to generalize to a wide range of edge devices. For example, fused FFNs, despite having more FLOPS and parameters, have lower latency than MHSA layers on Nano and NCS2."}, {"title": "Unified FFN vs. Fused FFN.", "content": "Although unified FFNs learn spatial relationships in the feature maps by a depthwise convolution with fewer parameters and fewer theoretical FLOPS, they are highly memory-bound with low arithmetic intensity. In Table 1 and Figure 1, fused FFNs with vanilla convolutions improve accuracy by 1.4% are not only well-supported by NCS2 but also better suited for early network stages among all devices. We include two operators in our supernet to support more subnet choices for different edge devices, as shown in Figure 2."}, {"title": "Feedforward network vs. Multi-head self-attention layer.", "content": "MHSAs learn the long-range dependencies in the feature map which greatly boosts performance. However, they are the major bottleneck in latency (cf. Figure 1). As shown in Table 1, reducing the value ratio from 4 to 2 in the self-attention layers (dim(V) = dim(Input) \u00d7 ratio) hurts the accuracy by 2.4%. Our framework searches the number and the placement of MHSA layers along with their expansion ratios for the value matrices. For devices where MHSA is less suitable, our framework minimizes the number of MHSA layers within the architecture. Conversely, if a compiler or implementation enhances MHSA support on a device, as seen in [8, 9], our search algorithm adaptively increases the number of MHSA layers to enhance accuracy while maintaining the same latency constraints."}, {"title": "GELU vs. ReLU.", "content": "While GELU activation [15] improves accuracy, it is often not well supported by low-cost edge devices. In contrast, most devices and compilers support conv-relu fusion which provides optimal latency [18, 30]. As shown in Table 1, the latency is greatly improved by 3.1ms (14.4%) when replacing GELU with ReLU [1] in the EfficientFormerv2 SO. We include GELU and ReLU as the activation choices for different layers in our supernet.\nWe conclude the preliminary study, and based on it, propose a unified NAS framework, named SCAN-Edge, for general edge devices. Our search space includes device-friendly operators: two feedforward networks, the number and the placement of self-attention layers, and two activation functions."}, {"title": "4. One-shot Supernet", "content": "Search space. We construct our supernet based on the EfficientFormer V2 backbone. The supernet consists of four stages and three embedding layers. The embedding layers adjust the model width, i.e., the channel dimension $C_i$, for the next stage. Each block in the first two stages, $S_1$ and $S_2$, only consists of FFNs. Every block in the last two stages, $S_3$ and $S_4$, consists of a MHSA followed by a FFN. Our search algorithm determines the width $C_i$, the number of FFN blocks $N_{ffn}$ in every stage $S_i$, the number of MHSA $N_{mhsa}$ in the last two stages for the network, and the expansion ratios $E$ (FFN and MHSA) and kernel size $K$ (FFN only) for every block $j$ in stage $i$. The full search space is shown in Table 2.\nDual feedforward network. We design our supernet with dual FFN to best accommodate various edge devices. As shown in Figure 2, in each FFN block, we provide two choices for searching: unified FFN and fused FFN with different kernel sizes (e.g., 3, 5) and expansion ratios (e.g., 2, 3, 4). Only one set of weight matrices will be activated (unified or fused) in a block during training. We empirically find dual FFNs converge slightly better than entangled FFNs (cf. Suppl. S2, and Figure S1). During training, we sample $N_{ffn}$ blocks for every stage $S_i$ and randomly switch between unified FFN and fused FFN. For each FFN block, we sample the kernel size $K$, and expansion ratio $E$. In the search stage, we search the number of FFNs $N_{ffn}$ for every stage $S_i$, and the kernel size $K$, as well as the expansion ratios $E$ for every block $j$ in stage $i$.\nSearching for multi-head self-attention. We design our MHSA with weight entanglement [5]. We allow the $V$ matrices in each MHSA to have larger dimensions [14, 41]. More specifically, we search the expansion ratios $E$ for the value matrix in every MHSA block, such that $dim(V) = N_{head} \\times dim(Q-K-V) \\times E$, where $N_{head}$ and $dim(Q-K-V)$ are fixed. The dimension of the $Q$ and $K$ matrices are fixed to $N_{head} \\times dim(Q-K-V)$ such that the attention matrices $A = Q K^T$ are shared with all subnets. During subnet search, the algorithm finds the optimal subnet by searching the number and the position of MHSA $N_{mhsa}$ and deciding their expansion ratios $E$ for the $V$ matrices in the last two stages (i.e., $i \\in \\{3,4\\}$)."}, {"title": "Dynamic activation layers.", "content": "In Section 3, the advanced activation layers such as GELU [15] are not well-supported by edge devices, thereby incurring a latency overhead during inference. We designed our supernet to support ReLU [1], one of the basic activation layers that is hardware-friendly. During training, we randomly switch between two activation layers for FFN and MHSA. Our search algorithm searches for the best activation combinations to optimize latency and accuracy.\nSupernet training algorithm. We apply the sandwich rule [44] to train our supernet which samples the smallest subnet, the biggest (full) subnet, and M randomly sampled subnets (M = 2 in our experiments). To accommodate dual FFN, we first sample the FFN structure (i.e., fused or unified) and apply it to the largest and smallest subnets. For the M randomly sampled subnets, we randomize the FFN selection in every block. The smallest subnet has minimal width, depth, and kernel size in FFNs, with no MHSAs in the last two layers, using only MHSA downsampling in the third embedding layer. Conversely, the largest subnet maximizes width, depth, and kernel size, applying MHSAs before all FFNs in layers S3 and S4. The FNNs and MHSAS are dropped by drop path [16] with probabilities. We scale the FFN dropping probabilities according to the stage depth so that the last FFN in the stage has the highest probability of being dropped. All MHSAs are dropped with a constant probability so that the number as well as the position of MHSAs in the stage can be searched. The full supernet training algorithm is shown in Suppl. 1."}, {"title": "5. Searching Subnets for Edge Devices", "content": "5.1. Search Objective\nGiven a supernet architecture $A = \\{a_1, ..., a_n\\}$ and its trained weight $W = \\{W_1, ..., W_n\\}$, we denote the supernet as $f(A, W)$ and a sampled subnet as $f(a_i, w_i)$, where the architecture $a_i \\in A$ and the weights $w_i \\in W$ are sampled from the supernet. Our search objective is to find an optimal architecture $a^*$ and $w^*$ that maximizes the accuracy while satisfying a set of constraints $C = \\{C_1, ..., C_n\\}$ on a given device, such that\n$a^*, w^* = \\underset{a \\in A, w \\in W}{\\text{arg max}} Acc(f(a, w)) \\newline s.t. \\\\ \\zeta_i(a, w) < c_i, i = 1, 2, ..., n.$ (1)\nwhere $\\zeta_i$ is the predictor function for the latency or the memory footprint of the subnet.\n5.2. Accuracy Predictor\nSince evaluating thousands of subnets on the validation set during the search process is not practical, we train a neural network $\\chi$ to predict the accuracy. Every block is encoded with a 24-bit length binary string where stages (4-bit), in/output width (8-bit), expansion ratios (6-bit), FFN types (2-bit), kernel sizes (2-bit), and activation functions (2-bit) are one-hot encoded. The binary string is stacked as a matrix n \u00d7 24 and padded to 44 rows for a n-block subnet resulting in a 44 \u00d7 24 matrix. The network $\\chi$ is built"}, {"title": "5.3. Latency Lookup Table with Calibration", "content": "The latency predictor is device-dependent. Therefore, we build latency lookup tables (LUTs) for every device and profile all possible blocks in every stage on the device. The total number of blocks is 568 with 4 additional output linear layers for different width choices. However, simply estimating the end-to-end latency by summing up the latencies of all blocks in the LUT will overestimate the actual latency, since the intermediate feature maps are usually cached in the memory instead of loaded from scratch. Therefore, for each device, we additionally profile the end-to-end latency of 10 subnets to calibrate the estimation with linear regression, as shown in Figure 3. During the search process, we estimate the subnet latency by summing the latency of every block in the lookup table, such that\n$\\zeta_{lat.} (a_i, w_i) = \\kappa \\sum_{j=1}^{n} LUT(a_j) + \\epsilon$\nwhere $\\kappa$ and $\\epsilon$ are the parameters obtained from the linear regression algorithm. The $\\zeta_{lat.}$ is the latency estimation for the n-block subnet $a_i$ from the LUT, which should satisfy the latency constraint $\\mathcal{C}_{lat.}$ during the search such that $\\zeta_{lat.} < \\mathcal{C}_{lat.}$. As shown in Figure 3, the calibrated block-wise LUTs not only preserve the latency order with a high Spearman's rank correlation but also have accurate estimations with low error for all devices. We compare our method with zero-cost proxies in Suppl. S4."}, {"title": "5.4. Hardware-aware Search Space Evolution", "content": "Search space quality. We adopt an evolutionary search algorithm [37] to find the optimal subnet for each device. However, sampling subnets that satisfy the constraints (e.g., latency and memory) from a very large search space is difficult (cf. Suppl. S7). The sampling time increases exponentially when we reduce the latency constraint, as shown by the blue line in Figure 4 (a). To address the issue, we update the search space during the search. The search space evolves if the quality of the search space $Q(A)$ defined by the current top k subnets $A = \\{a_1, ..., a_k\\}$ is better than the previous one, such that\n$Q(A) - Q(A^{t-1}) > \\delta$\nThe quality of the search space is evaluated by the average predicted accuracy of the top k subnets, i.e.\n$Q(A) = \\mathbb{E}_{a \\in A} [\\chi(a)]$"}, {"title": "Theorem 1.", "content": "Given $\\lambda \\in [0,1]$, the weighted mean of two probability distributions $\\Psi$ and $\\Phi$ that are defined in the same sample space $\\Omega$ such that $\\Theta = \\lambda \\Psi + (1 - \\lambda) \\Phi$ is a probability distribution defined in $\\Omega$."}, {"title": "Search space evolution.", "content": "Based on Theorem 1, we update the sampling space by the moving average of the probability:\n$p_{t+1}(X = x) = \\lambda p_t(X = x) + (1 - \\lambda) p^{*}(X = x)$\nwhere $\\lambda \\in [0,1]$ is a scalar, $X$ is a random variable for a set of architecture choices of block $j$ in stage $i$, $x$ is a specific architecture, and $p_t(X = x)$ is the current probability of selecting architecture x for the block. The $p^{*}$ is a probability defined as\n$p^{*}(X=x)=\\frac{\\sum_{s=1}^{k}\\sigma_{s}(X=x)}{k}$\nwhere $\\sigma_{s}$ is an indicator function for subnet s in top k population such that\n$\\sigma_s = {\\begin{cases}1, & \\text{if } X^s = x \\\\ 0, & \\text{otherwise}\\end{cases}}$\nWe maintain separate probability distributions for all stages and update the distribution of each stage at the end of each search episode. As shown in Figure 4 (b), our search space evolves based on the given constraints and remains constant time subnet sampling. The detailed algorithm is shown in the Suppl. S8."}, {"title": "6. Main Results", "content": "6.1. Experimental Setup\nWe follow the setup of Li et al. [22]. Our models are implemented with PyTorch [35] framework and Timm library [17]. We use 24 A5000 GPUs to train the supernet for 300 epochs with a total batch size of 3072 and use 8 A5000 GPUs to fine-tune the searched subnets for 150 epochs with a total batch size of 2048 on ImageNet 1k training set [10]. Models are validated on the ImageNet validation set. Both training and validation are using the standard resolution 224 \u00d7 224. We also use AdamW optimizer [26], set the initial learning rate to $10^{-3} \\times$ batch size/1024 to train the supernet, and use $10^{-4} \\times$ batch size/1024 to fine-tune the subnets. The cosine decay is applied in both training and fine-tuning. RegNetY-16GF [36] with 82.9% top-1 accuracy are used in supernet training and subnet fine-tuning as the teacher model for hard distillation, as Li et al. [22] and Touvron et al. [42].\nCommodity devices. We profile model latency on three different commodity hardware with their compilers using official benchmark tools. All models are compiled and profiled with batch size 1.\n\u2022 Edge CPU. We get the model latency on ARM Quad-core Cortex-A57 (Cortex). Models are converted to ONNX [30] format and run with the default compiler and execution provider in full precision (FP32).\n\u2022 Edge GPU. We obtain the latency on Jetson Jetson Nano 4G (Nano). Models are converted to ONNX format and compiled by the Cuda / TensorRT (TRT) in full precision (FP32).\n\u2022 USB accelerator. We get the latency on Intel Neural Compute Stick 2 (NCS2). Models are converted to OpenVINO IR (Intermediate Representation) and run with OpenVINO [18] in half precision (FP16).\n6.2. Image Classification with MobileNet Speed\nThe experiments include three different edge devices (Cortex CPU, Nano GPU, and NCS2), four compilers (Default ONNX Execution Provider, Nvidia Cuda and TensorRT, OpenVINO), and two precisions (FP32 and FP16). In this experiment, we optimize the latency with 224 \u00d7 224"}, {"title": "6.3. Joint Optimization of Latency and Model Size", "content": "The smaller models benefit from using the limited memory on edge devices and being easily downloaded from cloud to edge devices. This motivates us to jointly optimize the latency and model size. We experiment with joint optimization on ARM Cortex-A57 and Nvidia Jetson Nano platforms by constraining the search algorithm with both latency and number of parameters. In the experiment, we use"}, {"title": "6.4. Downstream Tasks via Transfer Learning", "content": "We perform transfer learning from the ImageNet pre-trained weight to various downstream tasks: CIFAR10, CIFAR100 [19], Food [2], and Pets [34]. All models are trained for 50 epochs with downstream datasets on an A500 GPU and set the batch size to 256 with a $10^{-3}$ base learning rate scaled by the batch size. The results are shown in Table 5. In general, our models outperform in accuracy their counterparts with similar latency in the downstream classification tasks on Cortex-A57. Moreover, our models match the MobileNetV2 latency on Cortex-A57 CPU (cf. Table 3)."}, {"title": "7. Object Detection", "content": "We integrate our searched subnets as the backbone to SSDLite [38]. We train the SSDLite with different backbones on COCO2017 dataset [23] by using the MMDetection library [32]. We load the ImageNet pre-trained weights of each backbone, and train the detection models with a resolution of 320 \u00d7 320. The learning rates are tuned for each model. We deploy models on the Nvidia Jetson Nano 4G by using the MMDeploy library [31] and profile the latency with Nvidia TensorRT profiling tools. As shown in Table 6, our model outperforms MobileViT XXS and EdgeNeXt XXS in both mAP and latency."}, {"title": "8. Conclusion", "content": "We propose a unified NAS framework that searches for hybrid networks with MobileNetV2-speed yet superior accuracy for low-cost commodity edge devices. Our framework incorporates different device-friendly operations for diverse edge devices with different hardware designs. The proposed search algorithm relies on real latency instead of zero-cost proxies (e.g., FLOPs, number of parameters) and reduces the sampling time by search space evolution for a wide variety of edge devices. Our experiments show our hybrid models match MobileNetV2-speed on Edge CPUs, Edge GPUs, and USB accelerators with better accuracy than MobileNetV2."}, {"title": "S1. Proof of Theorem 1", "content": "Theorem 2. Given $\\lambda \\in [0,1]$, the weighted mean of two probability distributions $\\Psi$ and $\\Phi$ that are defined in the same sample space $\\Omega$ such that $\\Theta = \\lambda \\Psi + (1 - \\lambda) \\Phi$ is also a probability distribution defined in $\\Omega$.\nProof. Since $\\Psi$ and $\\Phi$ are defined in the same sample space $\\Omega$, we define their associated density distribution functions are $p_{\\Psi}$ and $p_{\\Phi}$ such that.\n$\\int_{\\Omega} p_{\\Psi}(X=x) = q^{\\Psi}$, and $\\int_{\\Omega} p_{\\Phi}(X=x) = q^{\\Phi}$, and\n$\\int_{\\Omega} p_{\\Psi}(X=x) = 1$\n$\\int_{\\Omega} p_{\\Phi}(X=x) = 1$\nThe X is the random variable of the sample space $\\Omega$, and $q^\\Psi$ is the probability that associated with $X = x, x \\in \\Omega$. Given $\\lambda \\in [0, 1]$, we have\n$\\int_{\\Omega} p_{\\Theta}(X = x) = \\lambda p_{\\Psi}(X = x) + (1 - \\lambda) p_{\\Phi}(X = x)$\n$\\int_{\\Omega} p_{\\Theta}(X = x) = \\lambda \\int_{\\Omega} p_{\\Psi}(X = x) + (1 - \\lambda) \\int_{\\Omega} p_{\\Phi}(X = x)$\n$\\int_{\\Omega} p_{\\Theta}(X = x) = \\lambda + (1 - \\lambda) = 1$\nThis concludes the proof that $\\Theta$ is also a probability distribution defined in the sample space $\\Omega$."}, {"title": "S2. Dual vs. Entangled FFNS", "content": "We show two different designs of the supernet to accommodate two types of FFN in Figure S1. The dual feedforward networks are composed of two sets of separated weight matrices, while entangled feedforward networks share the expansion and projection weight matrices. Only one type of FFN will be activated (unified or fused) in a block during the training time. We empirically find the dual FFN converges slightly better than entangled FFN and has higher accuracy of the subnets, as shown in Figure S2. We use dual FFN in the supernet for all experiments."}, {"title": "S3. Supernet Training Algorithm", "content": "We show the pseudo-code of our supernet training algorithm in Algorithm 1."}, {"title": "S8. The search algorithm", "content": "We show the pseudo-code of our search algorithm in Algorithm 2. The top-k subnets perform mutation and crossover in the search. Our search space evolves based on the current top-ranking subnets. After some generations, the algorithm returns the top1 subnet architecture."}, {"title": "S9. Searched Architecture Details", "content": "We show the architecture details of searched subnets in Table S3 and Table S4. Our search algorithm tends to adopt depthwise convolutions (unified FFNs) with large-size kernels (e.g., 5) and MHSA for ARM Cortex-A57. In contrast, our search algorithm is in favor of vanilla convolutions (fused FFNs) with small-size kernels (e.g., 3) over depthwise convolutions (unified FFNs) and MHSA for Nvidia Jetson Nano 4G with TensorRT compiler. GELU and MHSA which boost accuracy are placed in the model closer to the output with minimal latency impact. The result shows that our search optimizes the model architecture for different hardware implementations and characteristics. Therefore, we expect that our search algorithm can propose more competitive model architectures once the MHSA and GELU are optimized for the target hardware."}, {"title": "5.5. The Quality of the Accuracy Predictor", "content": "The accuracy predictor is trained with 6000 subnet-accuracy pairs, where the accuracy is evaluated with the weights directly inherited from the supernet on the ImageNet validation set. We show the correlations between ground truth accuracy (y-axis) and predicted accuracy (x-axis) in Figure S4. Our accuracy predictor preserves the accuracy rank of the subnets. We use the value predicted from our accuracy predictor as a proxy instead of exhaustively evaluating subnets on the dataset during the search."}, {"title": "S6. Analysis of Subnet Accuracy", "content": "We show the predicted accuracy of the subnets from our accuracy predictor, the accuracy of the subnets inherited from the supernet, and fine-tuned accuracy of the subnets in Table S2 for reference. We note that our accuracy predictor is trained with 6000 subnet-accuracy pairs, where the accuracies are evaluated with the weights directly inherited from the supernet."}, {"title": "S7. Analysis of Search Space", "content": "Our search space contains roughly $10^{45}$ subnets, which is much larger than previous work [3]. The details of the search space are shown in Table 2 in the paper. We show the mathematical analysis of our search space in the section. Our supernest consists of a stem conv followed by 4 stages with 3 embeddings in between.\n\u2022 Each stage has 4 choices of width values resulting in $4^4$ width combinations.\n\u2022 We search the activation layers for stem conv resulting in 2 architecture choices.\n\u2022 The embedding layers only consist of one layer of convolution and do not contain activation layers, except for the MHSA downsampling layer, which has 6 architecture choices.\n\u2022 In the first and second stages, we have 24 choices for each FFN block and 2 depth choices (2 or 3 blocks) for each stage and therefore the total number of choices of the architecture is $\\sum_{n=2}^{N=3} 24^n$.\n\u2022 In the third and fourth stages, we have 24 choices for each FFN block for each stage. We search for the number m and the position of MHSA among n FFN blocks each of which has 6 choices resulting in $6^m {\\binom{n}{m}}$.\nSummarizing the above items together in an equation, we obtain\n$4^4 \\times 2 \\times \\sum_{n=2}^{3} 24^n \\times \\sum_{n=6}^{9} 24^n \\times \\sum_{n=2}^{3} {\\binom{n}{m}} 6^m \\times 6 \\times\\sum_{n=4}^{6} {\\binom{n}{m}} 6^m \\approx 10^{45}$"}]}