{"title": "Simple but Effective Compound Geometric Operations for Temporal Knowledge Graph Completion", "authors": ["Rui Ying", "Mengting Hu", "Jianfeng Wu", "Yalan Xie", "Xiaoyi Liu", "Zhunheng Wang", "Ming Jiang", "Hang Gao", "Linlin Zhang", "Renhong Cheng"], "abstract": "Temporal knowledge graph completion aims to infer the missing facts in temporal knowledge graphs. Current approaches usually embed factual knowledge into continuous vector space and apply geometric operations to learn potential patterns in temporal knowledge graphs. However, these methods only adopt a single operation, which may have limitations in capturing the complex temporal dynamics present in temporal knowledge graphs. Therefore, we propose a simple but effective method, i.e. TCompoundE, which is specially designed with two geometric operations, including time-specific and relation-specific operations. We provide mathematical proofs to demonstrate the ability of TCompoundE to encode various relation patterns. Experimental results show that our proposed model significantly outperforms existing temporal knowledge graph embedding models.", "sections": [{"title": "1 Introduction", "content": "A knowledge graph (KG) comprises a collection of structured knowledge presented in triples, offering a simple and effective means of describing factual information. In a KG, a triplet is conventionally represented as (s, r, o), where s, o and \u0155 correspond to the head entity, tail entity and the relation linking between the head and tail entities, respectively. However, knowledge is not static in the real world. Temporal knowledge graph (TKG) represents knowledge (s, r, o) occurring at timestamp \u0442, denoted as a quadruple (s, \u00ee, o, t), thereby adding a temporal dimension to knowledge graphs. The introduction of the timestamp 7 enables TKG to delineate the temporal scope of knowledge more accurately and helps us better uncover the potential information within it. Therefore, TKG is widely applied in downstream tasks such as question answering, information retrieval, and recommendation systems due to its temporal characteristics.\nHowever, TKG usually does not cover all the facts. The incompleteness of TKG hinders the performance of its downstream tasks. To enhance the overall completeness of TKG, the temporal knowledge graph embedding (TKGE) model utilizes existing knowledge to predict and estimate missing facts. Specifically, the TKGE model employs distinct score functions to acquire effective vector space representations for entities, relations and timestamps, thus utilizing these representations to predict missing facts in TKG. Furthermore, how to enhance the expressive capabilities of the TKGE model is also an important issue that has received widespread attention.\nGeometric operations such as translation and scaling are widely used operations in the field of graphics. These operations help to distinguish between these different classes of entities and to model different relational patterns. Models like TTransE, TComplEx"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Knowledge Graph Embedding", "content": "Alternatively referred to as static knowledge graph embedding, knowledge graph embedding commonly involves embedding entities and relations into low-dimensional vector spaces. This low-dimensional embedding is employed to enhance the representation of entities and relations through score functions. The model can be categorized into a translation model and a bilinear model based on the distinct score functions employed. In translation model, TransE originally introduces a straightforward and efficient score function $(s,r, 0) = ||es+er-eo||2, where es, er, eo represent the lower dimensional embedding of head entity, relation and tail entity respectively, utilizing relations to denote the translation distance from head entities to tail entities. Subsequent models, including TransH , TransR  and TransD  adopt distinct projection strategies to implement a relation-specific representation of entities. RotatE introduces the concept of relations into knowledge graph embedding through a rotation operation in complex space. The PairRE model suggests implementing separate relation-specific scaling operations on the head and tail entities. HAKE operates by mapping the embeddings into polar coordinate space. CompoudE  employs a combination of translation, scaling and rotation to form a low-dimensional representation of the relation, addressing the limitations associated with individual operations. In bilinear model, DisMult represents relationships through a symmetric matrix, deriving a scoring function $(s, r, 0) =< es, W\u00ee, eo > by evaluating the semantic similarity between head and tail entities, where W represents the symmetric matrix of relation. ComplEx  operates within complex spaces. TuckER employs Tucker decomposition to assess the plausibility of fact triples."}, {"title": "2.2 Temporal Knowledge Graph Embedding", "content": "Temporal knowledge graph embedding closely resembles static knowledge graph embedding. Therefore, the majority of TKGE are adapted from knowledge graph embeddings to accommodate dynamic changes in facts. For instance, TTransE represents timestamp through a translation operation, following the concept introduced by TransE. Its score function is denoted by $(s,r,0,T) = ||es + er + ET eo||2, where e represents the lower-dimensional embedding of timestamp. TA-"}, {"title": "3 Background and Notation", "content": "In subsequent sections of this article, the CompoundE methodology is employed to elucidate translation, scaling, and rotation operations. The ensuing discussion provides a comprehensive account of the representation of these operations, followed by an introduction to the CompoundE model."}, {"title": "3.1 Translation, Rotation, and Scaling", "content": "Translation, rotation and scaling transformations are fundamental operations in graphics, frequently employed in various engineering applications. In the processing of robot motion, the cascade application of translation, rotation and scaling operations constitutes a method for precise position determination. The translation, rotation and scaling operations are illustrated in Fig. 2. Expressing these operations in matrix form proves to be both more efficient and straightforward. The translation operation in Fig. 2 is represented in 2D as follows:\nT=\n10 tx\n01 ty\n001\n(1)\nwhile 2D rotation matrix can be written as:\nR=\ncos 0 - sin 0 0\nsin\ncos \u03b8 0\n0\n0\n1\n(2)\nAnd 2D scaling matrix can be expressed as:\nS=\nSx\n00\n0\nSy 0\n0\n01\n(3)"}, {"title": "3.2 CompoundE", "content": "CompoundE encompasses a range of models that integrate various translation, rotation and scaling operations. In this discussion, we focus on a specific combination approximating our model. This involves applying a blend of translation, rotation and scaling to the head entity. The ultimate score is determined by computing the distance between the transformed head and the initial tail entity embeddings. The corresponding score function is formally defined as follows:\n$(s, r, 0) = ||T\u00f4\u00b7R\u00b7S\u00ee\u00b7es - eo || (4)\nwhere S, R, T denote the translation, rotation and scaling operations for the head entity embedding. These constituent operators are specific to relations. It is essential to highlight that the scaling, rotation and translation operations employed here are sequential, and altering their order yields distinct outcomes."}, {"title": "3.3 Problem formulation", "content": "For a temporal knowledge graph G, we use & to denote the set of entities. R and T represent the set of relations and the set of timestamps in the temporal knowledge graph respectively. A fact in the temporal knowledge graph is represented by the quadruple (s,\u00f4, o, t), where s, o \u2208 E, \u00ce \u2208 R and \u03c4\u2208\u03a4. The task of knowledge graph completion is to predict the missing facts through the existing facts in the knowledge graph. We train the data in the training set through the score function $(s, r, o, T) in the model. When predicting a fact, we give either the head entity, relation and timestamp or the tail entity, relation and timestamp in the quadruple. We input these missing quadruples (s,r,?, T) or (?, \u00ee, \u03bf, \u03c4) and candidate entities into the score function, taking the highest score to form a new fact quadruple for the entity."}, {"title": "4 Methodology", "content": ""}, {"title": "4.1 TCompoundE Model", "content": "In this section, we present our model, TCompoundE, which employs compound geometric operations on both relations and timestamps. For a quadruple (s, r, \u03bf, \u03c4) in TKG. We utilize the notations es, eo to represent the embeddings of the head entity s and tail entity o. We utilize S and T to represent relation-specific scaling, translation operations. Integrate temporal information into relation-specific operations before applying them to entity embeddings. This merging involves time-specific translation T, and scaling S operations in a relationship-specific process. In our model, we employ translation and scaling operations to represent relation-specific operations and time-specific operations. To facilitate a comprehensive introduction to our model, we categorize it into two distinct sections: Time-Specific Operation and Relation-Specific Operation; The initial section elucidates the utilization of time-specific operations in conjunction with relation-specific operations within a quadruple. In the subsequent section, we elaborate on the impact of relation-specific operations on the embedding of the head entity.\nTime-Specific Operation. In our model, we employ time-specific translation T, and scaling S operations to imbue temporal information into the relation. We exclusively apply these operations to the relation-specific scaling operation S. It is crucial to highlight that we scale the relation-specific operation by first applying translation and then scal-ing. This sequencing is intentional, as the order of operations can influence the outcome. However, for the relation-specific translation operation, we refrain from integrating time information. This approach aims to capture features of relations that remain constant over time. Subsequently, we obtain relation-specific operations that integrate temporal information. Herein, ST and T denote the relation-specific scaling and translation operations, respectively, after incorporating time information. These operations can be precisely described by the following formula:\nST = S\u2533\u00b7T\u00b7S (5)\nT\u4ee4T = T\u00ea (6)\nRelation-Specific Operation. We denote the relation-specific translation and relation-specific scaling operations for the head entity as T and S respectively. To capture temporal information within the TKG, we refrain from applying these operations directly to the head entity embeddings. Instead, we execute relation-specific operations subsequent to the time-specific operations. Specifically, we utilize S and Try to conduct relation-specific operations incorporating time information on the head entity embedding. This operation is formally represented as:\ne's T = STT T\u00caTes (7)\nWe obtain the head entity representation er incorporating fused time and relation information using Formula 7. Unlike CompoundE our chosen score function is not a distance metric; instead, it is determined by the semantic similarity between er and the tail entity eo. This decision is grounded in our belief that semantic similarity offers more advantages than distance metrics in the context of TKG . The score function for TCompoundE is expressed as:\n$(s, r, o,t) =< er, eo > (8)"}, {"title": "4.2 Loss Function", "content": "Building upon TNTComplEx  and TeAST , we adopt reciprocal learning for training our model, with the loss func-"}, {"title": "4.3 Modeling Various Relation Patterns", "content": "TCompoundE can model crucial relation patterns, encompassing symmetric, asymmetric, inverse and temporal evolution patterns (detail in Appendix A). We enumerate all the propositions in this section, with corresponding proofs provided in the Appendix.\nProposition 1 TCompoundE can model the symmetric relation pattern. (proof in Appendix B)\nProposition 2 TCompoundE can model the asymmetric relation pattern. (proof in Appendix C)\nProposition 3 TCompoundE can model the inverse relation pattern. (proof in Appendix D)\nProposition 4 TCompoundE can model the temporal evolution relation pattern. (proof in Appendix E)"}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Datasets", "content": "We assess the performance of TCompoundE on three benchmark datasets for TKGE. The datasets include ICEWS14 and ICEWS05-15 , both derived from the Integrated Crisis Early Warning System (ICEWS)"}, {"title": "5.2 Baselines", "content": "We conduct a comprehensive comparison of our model with state-of-the-art Temporal Knowledge Graph Embedding (TKGE) models, which include TTransE , DE-SimplE , TA-DisMult , ChronoR , TComplEx , TNTComplEx , BoxTE , RotateQVS , and TeAST .\nAmong these existing TKGE methods, TeAST achieves state-of-the-art results on the ICEWS14, ICEWS05-15, and GDELT datasets. Consequently, we consider TeAST  as the primary baseline for our comparative analysis. In addition, we also used a variant of the TCompoundE model for ablation experiments. A detailed description of the variant model can be found in Appendix F."}, {"title": "5.3 Evaluation Protocol", "content": "This paper evaluates our temporal knowledge graph embedding (TKGE) model using the aforementioned benchmarks. Following established base-"}, {"title": "5.4 Experimental Setup", "content": "We implement our proposed model TCompoundE via pytorch based on the TeAST training framework. All experiments are trained on a single NVIDIA RTX A6000 with 48GB memory. We use the Adagrad optimizer and conduct a lot of experiments to find the optimal parameter configuration on each dataset. The learning rate is set to 0.01 and the embedding dimension d is set to 6000 and the batch size is set to 4000 in the ICEWS14 dataset and the max epoch is set to 400. In the ICEWS05-15 dataset, we set the learning rate to 0.08, d to 8000, the batch size to 6000 and the max epoch to 100. In GDELT, the learning rate, d, the batch size and the max epoch are set to 0.35, 6000, 2000 and 50 respectively. The optimal hyperparameters for TCompound are as follows:\n\u2022 ICEWS14: \u03bb\u03b9 = 0.0025, \u03bb\u03c4 0.01\n\u2022 ICEWS05-15:\u03bb\u03b9 = 0.002, \u03bb = 0.1\n\u2022 GDELT: = 0.001, \u03bb\u30f6 = 0.001\nWe report the average results on the test set for five runs."}, {"title": "6 Results and Analysis", "content": ""}, {"title": "6.1 Main Results", "content": "Table 2 presents the results of knowledge graph completion for ICEWS14, ICEWS05-15 and GDELT datasets. The best-performing results are highlighted in bold font. Our observations indicate that TCompoundE outperforms all baseline models across all metrics for t datasets. As TCompoundE exclusively applies the time-specific operation to the relation-specific scaling operation, this enables relations occurring simultaneously to utilize the same time-specific operation, facilitating the evolution of all relations over time. For the relation-specific translation operation, we maintain the timestamps unchanged to preserve the features of relations that remain constant over time. This demonstrates the significance of applying the time-specific operation exclusively to the relation-specific scaling operation. Additionally, Table 2 reveals that TCompoundE has achieved significant improvements over TKGE models utilizing a single operation for both relation-specific and time-specific operations, such as translation (TTransE) and scaling (TComplEx). This confirms that employing compound geometric operations for both relation-specific and time-specific operations is an effective strategy for designing TKG embeddings. Furthermore, BoxTE  highlights that GDELT necessitates a considerable level of temporal inductive capacity to achieve effective encoding. This is because GDELT displays a notable degree of temporal variability, wherein certain facts endure across multiple consecutive timestamps, while others are momentary and sparse. The"}, {"title": "6.2 Different Combinations", "content": "We investigate the impact of various combinations on ICEWS14 and ICEWS05-15. And we also compare the performance of TCompoundE with its variants. More comprehensive experimental results can be found in Appendix H. We classify these variants into two groups: the first group involves maintaining the time-specific operation unchanged while modifying the relation-specific operation, and the second group involves maintaining the relation-specific operation unchanged while modifying the time-specific operation. The results of the ICEWS14 and ICEWS05-15 variants from the aforementioned two groups are presented in Table 3 and Table 4. The best results are highlighted in bold font, while the second best are underlined. In Table 3, we can observe that TCompoundE outperforms its variants across all metrics on ICEWS14 and ICEWS05-15. As TCompoundE utilizes translation and scaling operations as relation-specific operations, this enables TCompoundE to model critical relation patterns. We can observe from Table 3 that TCompoundE has significantly outperformed its variants when employing a single operation for relation-specific operations, including translation (V1), scaling (V4) and rotation (V6). It confirms that utilizing a single operation as relation-specific has drawbacks in TKGs. We also observe that TCompoundE outperforms its variants across all geometric operations, including V3, V4 and V7. These variant models employ translation, scaling, and rotation as relation-specific operations. The distinction among these models lies in the usage of time-specific operations in different relation-specific operations, such as translation (V3), scaling (V5) and rotation (V7). This underscores that incorporating too many geometric operations as relation-specific operations is not optimal in TKGs. Conversely, V2 utilizes the same relation-specific operation as TCompoundE, while applying the time-specific operation to the relation-specific translation operation. This indicates that more features that remain constant over time can be learned from relation-specific translation operations.\nTable 4 displays the results of the second group of TCompoundE variants. In Table 4, V8, V9, and V10 employ single translation, scaling and rotation as time-specific operations. V12, V13 and V14 are"}, {"title": "6.3 Effects of Operation", "content": "We randomly select 100 quadruples from ICEWS14 and employ t-SNE to visualize the head and tail entity embeddings of TCompoundE. The visualization results are depicted in Fig. 3. We note that the initial state of the head entity and tail entity embeddings appears chaotic and irregular. However, when the head entity embeddings undergo TT and ST transformations, the distribution of the head and tail entity embeddings becomes more regular. The reason for this phenomenon, we think head entity is mapped to a space close to tail entity by relation-specific operation merging the temporal information. This is due to the design of the scoring function and illustrates the effectiveness of the composite operation. Specifically, the head and tail entities in the same quadruple exhibit a high degree of proximity. This confirms the effectiveness of employing both relation-specific and time-specific operations. In addition, we have a case study of the quadruple we use. Details can be found in the appendix I."}, {"title": "6.4 Effects of Embedding Dimension", "content": "We investigate the effect of embedding dimension on TCompoundE. In Fig. 4, we compare the MRR scores of TCompoundE and previous state-of-the-art (SOTA) embedding models on the ICEWS14 dataset across different dimension settings d \u2208 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000. According to the experimental results, TCompoundE is suboptimal in low dimensions, but it outperforms benchmarking methods when d \u2265 2000."}, {"title": "7 Conclusion", "content": "This paper introduces a novel method, TCompoundE, designed to address the challenge of"}, {"title": "Limitations", "content": "Similar to many temporal knowledge graph embedding models, our proposed method, TCompoundE, faces limitations during the training phase as it cannot learn about invisible entities and time. Consequently, TCompoundE cannot directly perform temporal knowledge graph extrapolation tasks. Additionally, our model excels at high embedding dimensions, leading to its larger size compared to others."}, {"title": "Appendix", "content": ""}, {"title": "A Definition of Relation Patterns", "content": "Definition 1 A relation is symmetric, if\n\u2200s, o, T, (s, \u00ee, 0, \u0442) \u2227 (o, r, s, t) \u2208 G\nDefinition 2 A relation is asymmetric, if\n\u2200s, o, T, (s, \u00ee, 0, \u0442) \u2208 G \u2227 (o, r, s, T) \u2209 G\nDefinition 3 Relation \u00ee\u2081 is the inverse of 2, if\n\u2200s, o, T, (S, \u00ce 1, 0, \u0442) \u2227 (0, 12, s, T) \u2208 G\nDefinition 4 Relation 1 and 2 are evolving over time from timestamp T\u2081 to timestamp T2, if\nds, o, (s, 1, 0, T\u2081) \u2227 (s, r2, 0, T2) E G.\nThe above describes various relational patterns from a mathematical point of view. We will il-lustrate various relational patterns next. For sym-metric patterns, (Canada, Consult, France) and (France, Consult, Canada) show that Consult is symmetric. For asymmetric patterns, is father of is an asymmetric relation, because (personA, is father of, personB) and (personB, is father of, personA) can't both be true. is father of and is son of are in-verse relations. Temporal evolution patterns detail can be found at Figure 1."}, {"title": "B Proof of Propositions 1", "content": "Let M denote the compound operation for head entity. Following ComplEx, we employ the stan-dard dot product < a, b >= a \u2022 b = \u03a3\u03ba\u03b1\u03babk. For< a, b >=< c, d >, there are special cases that the previous equation holds true when aibi == cidi, where i \u2208 [0, k]. For symmetric pattern, we canget $(s, r, o, t) = $(s, r, o, t) based on definitionof symmetric pattern. According to score functionof TCompoundE, we get:\n< Mes, eo >=< Meo, es >\u21d2\nMeses = Me, oes (12)"}, {"title": "CProof of Propositions 2", "content": "By definition of asymmetric pattern, we can get$(s,r,o,t) \u2260 $(s,\u00f4, o,\u03c4). By similar proof forPropositions 1, TCompoundE can model asymmet-ric pattern when matrix is not invertible."}, {"title": "D Proof of Propositions 3", "content": "Based on definition of inverse pattern, we have\u03c6(s,r1, 0, \u03c4) = $(o, r2, s, T). Hence, we get\nMies o eo = M2e, o es (14)\nIf the matrix M\u2081 or M2 is invertible, the we canget:\nes o eo = M\u012b\u00b9M2e, o es\nM\u012b\u00af\u00b9M2 = I or M\u2082\u00b9M\u2081 = I (15)\nTherefore, TCompoundE can model symmetric pat-tern when M\u2081 and M2 are inverse matrices."}, {"title": "E Proof of Propositions 4", "content": "For temporal evolution pattern, we can get$(s, 1, 0, T\u2081) = $(s, r2, 0, T2) based on definitonof temporal evolution pattern. Then through thescore function, we can get:\nMrities Co = M2T2lo es (16)\nwhere M1T1 = STTT1 ST1Tr1 and Mr2T2 =ST2TT2 ST2 T2. Then we can get:\nlso lo =\nMM2T2 = I\nMr171\nor\nMr171\nMM2T2Oes\n2T2\n= I (17)\nWe can observe from the above formula: TCom-poundE can model temporal evolution pattern whenMr171 and Mr272 are inverse matrices."}, {"title": "F Introduction of Variant Models", "content": "We introduce R and R in the variant model torepresent the relation-specific and time-specific ro-tation operation. To better explain the differencesbetween variant models, we introduce a more gen-eral formula instead of Formula 7:\nesT = RTS TTSes (18)\nwhere RT represents relation-specific rotationoperation that incorporates temporal information.Next we will look at the formula differences be-tween TCompoundE and its variant models in ob-taining the RT, ST and T\u4ee4T"}, {"title": "G Different Scoring Function", "content": "The semantic similarity scoring function is shownin function 8. The distance scoring function isdefined as follows:\n$(s, r, o, t) = ||et - eo||2 (19)\nFrom the definition of the distance scoring func-"}, {"title": "H More result of variant of TCompound", "content": "In the experimental analysis section, we exam-ined the utilization of single operations as well as"}, {"title": "I Case Study", "content": "We have selected some of the quadruples used inSection 6.3 experiments as examples in the casestudy. The experimental results are shown in thetable 10, where es, T\u00eeres, and STT\u00eetes respec-tively represent the three stages in Section 6.3."}]}