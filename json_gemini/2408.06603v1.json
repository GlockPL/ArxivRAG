{"title": "Simple but Effective Compound Geometric Operations for Temporal Knowledge Graph Completion", "authors": ["Rui Ying", "Mengting Hu", "Jianfeng Wu", "Yalan Xie", "Xiaoyi Liu", "Zhunheng Wang", "Ming Jiang", "Hang Gao", "Linlin Zhang", "Renhong Cheng"], "abstract": "Temporal knowledge graph completion aims to infer the missing facts in temporal knowledge graphs. Current approaches usually embed factual knowledge into continuous vector space and apply geometric operations to learn potential patterns in temporal knowledge graphs. However, these methods only adopt a single operation, which may have limitations in capturing the complex temporal dynamics present in temporal knowledge graphs. Therefore, we propose a simple but effective method, i.e. TCompoundE, which is specially designed with two geometric operations, including time-specific and relation-specific operations. We provide mathematical proofs to demonstrate the ability of TCompoundE to encode various relation patterns. Experimental results show that our proposed model significantly outperforms existing temporal knowledge graph embedding models.", "sections": [{"title": "1 Introduction", "content": "A knowledge graph (KG) comprises a collection of structured knowledge presented in triples, offering a simple and effective means of describing factual information. In a KG, a triplet is conventionally represented as (s, r, o), where s, o and \u0155 correspond to the head entity, tail entity and the relation linking between the head and tail entities, respectively. However, knowledge is not static in the real world. Temporal knowledge graph (TKG) represents knowledge (s, r, o) occurring at timestamp \u0442, denoted as a quadruple (s, \u00ee, o, t), thereby adding a temporal dimension to knowledge graphs. The introduction of the timestamp 7 enables TKG to delineate the temporal scope of knowledge more accurately and helps us better uncover the potential information within it. Therefore, TKG is widely applied in downstream tasks such as question answering, information retrieval, and recommendation systems due to its temporal characteristics.\nHowever, TKG usually does not cover all the facts. The incompleteness of TKG hinders the performance of its downstream tasks. To enhance the overall completeness of TKG, the temporal knowledge graph embedding (TKGE) model utilizes existing knowledge to predict and estimate missing facts. Specifically, the TKGE model employs distinct score functions to acquire effective vector space representations for entities, relations and timestamps, thus utilizing these representations to predict missing facts in TKG. Furthermore, how to enhance the expressive capabilities of the TKGE model is also an important issue that has received widespread attention.\nGeometric operations such as translation and scaling are widely used operations in the field of graphics. These operations help to distinguish between these different classes of entities and to model different relational patterns. Models like TTransE, TComplEx"}, {"title": "2 Related Work", "content": "Alternatively referred to as static knowledge graph embedding, knowledge graph embedding commonly involves embedding entities and relations into low-dimensional vector spaces. This low-dimensional embedding is employed to enhance the representation of entities and relations through score functions. The model can be categorized into a translation model and a bilinear model based on the distinct score functions employed. In translation model, TransE originally introduces a straightforward and efficient score function $(s,r, 0) = ||es+er-eo||2$, where es, er, eo represent the lower dimensional embedding of head entity, relation and tail entity respectively, utilizing relations to denote the translation distance from head entities to tail entities. Subsequent models, including TransH , TransR and TransD adopt distinct projection strategies to implement a relation-specific representation of entities. RotatE introduces the concept of relations into knowledge graph embedding through a rotation operation in complex space. The PairRE model suggests implementing separate relation-specific scaling operations on the head and tail entities. HAKE operates by mapping the embeddings into polar coordinate space. CompoudE employs a combination of translation, scaling and rotation to form a low-dimensional representation of the relation, addressing the limitations associated with individual operations. In bilinear model, DisMult represents relationships through a symmetric matrix, deriving a scoring function $(s, r, 0) =< es, W\u00ee, eo > by evaluating the semantic similarity between head and tail entities, where W represents the symmetric matrix of relation. ComplEx operates within complex spaces. TuckER employs Tucker decomposition to assess the plausibility of fact triples.\nTemporal knowledge graph embedding closely resembles static knowledge graph embedding. Therefore, the majority of TKGE are adapted from knowledge graph embeddings to accommodate dynamic changes in facts. For instance, TTransE represents timestamp through a translation operation, following the concept introduced by TransE. Its score function is denoted by $(s,r,0,T) = ||es + er + ET - eo||2$, where e represents the lower-dimensional embedding of timestamp. TA-"}, {"title": "3 Background and Notation", "content": "In subsequent sections of this article, the CompoundE methodology is employed to elucidate translation, scaling, and rotation operations. The ensuing discussion provides a comprehensive account of the representation of these operations, followed by an introduction to the CompoundE model."}, {"title": "3.1 Translation, Rotation, and Scaling", "content": "Translation, rotation and scaling transformations are fundamental operations in graphics, frequently employed in various engineering applications. In the processing of robot motion , the cascade application of translation, rotation and scaling operations constitutes a method for precise position determination. The translation, rotation and scaling operations are illustrated in Fig. 2. Expressing these operations in matrix form proves to be both more efficient and straightforward. The translation operation in Fig. 2 is represented in 2D as follows:\nT=\n10 tx\n01 ty\n001\n(1)\nwhile 2D rotation matrix can be written as:\nR=\ncos 0 - sin 0 0\nsin\ncos \u03b8 0\n0\n0\n1\n(2)\nAnd 2D scaling matrix can be expressed as:\nS=\nSx\n00\n0\nSy\n0\n0\n0\n1\n(3)"}, {"title": "3.2 CompoundE", "content": "CompoundE encompasses a range of models that integrate various translation, rotation and scaling operations. In this discussion, we focus on a specific combination approximating our model. This involves applying a blend of translation, rotation and scaling to the head entity. The ultimate score is determined by computing the distance between the transformed head and the initial tail entity embeddings. The corresponding score function is formally defined as follows:\n$(s, r, 0) = ||T\u00f4\u00b7R\u00b7S\u00ee\u00b7es - eo || (4)\nwhere S, R, T denote the translation, rotation and scaling operations for the head entity embedding. These constituent operators are specific to relations. It is essential to highlight that the scaling, rotation and translation operations employed here are sequential, and altering their order yields distinct outcomes (Ge et al., 2022)."}, {"title": "3.3 Problem formulation", "content": "For a temporal knowledge graph G, we use & to denote the set of entities. R and T represent the set of relations and the set of timestamps in the temporal knowledge graph respectively. A fact in the temporal knowledge graph is represented by the quadruple (s,\u00f4, o, t), where s, o \u2208 E, \u00ce \u2208 R and \u03c4\u2208\u03a4. The task of knowledge graph completion is to predict the missing facts through the existing facts in the knowledge graph. We train the data in the training set through the score function $(s, r, o, T) in the model. When predicting a fact, we give either the head entity, relation and timestamp or the tail entity, relation and timestamp in the quadruple. We input these missing quadruples (s,r,?, T) or (?, \u00ee, \u03bf, \u03c4) and candidate entities into the score function, taking the highest score to form a new fact quadruple for the entity."}, {"title": "4 Methodology", "content": "In this section, we present our model, TCom-poundE, which employs compound geometric operations on both relations and timestamps. For a quadruple (s, r, \u03bf, \u03c4) in TKG. We utilize the notations es, eo to represent the embeddings of the head entity s and tail entity o. We utilize S and T to represent relation-specific scaling, translation operations. Integrate temporal information into relation-specific operations before applying them to entity embeddings. This merging involves time-specific translation T, and scaling S operations in a relationship-specific process. In our model, we employ translation and scaling operations to represent relation-specific operations and time-specific operations. To facilitate a comprehensive introduction to our model, we categorize it into two distinct sections: Time-Specific Operation and Relation-Specific Operation; The initial section elucidates the utilization of time-specific operations in conjunction with relation-specific operations within a quadruple. In the subsequent section, we elaborate on the impact of relation-specific operations on the embedding of the head entity.\nTime-Specific Operation. In our model, we employ time-specific translation T, and scaling S operations to imbue temporal information into the relation. We exclusively apply these operations to the relation-specific scaling operation S. It is crucial to highlight that we scale the relation-specific operation by first applying translation and then scaling. This sequencing is intentional, as the order of operations can influence the outcome. However, for the relation-specific translation operation, we refrain from integrating time information. This approach aims to capture features of relations that remain constant over time. Subsequently, we obtain relation-specific operations that integrate temporal information. Herein, ST and T denote the relation-specific scaling and translation operations, respectively, after incorporating time information. These operations can be precisely described by the following formula:\nST = S\u2533\u00b7T\u00b7S (5)\nT\u4ee4T = T\u00ea (6)\nRelation-Specific Operation.\nWe denote the relation-specific translation and relation-specific scaling operations for the head entity as T and S respectively. To capture temporal information within the TKG, we refrain from applying these operations directly to the head entity embeddings. Instead, we execute relation-specific operations subsequent to the time-specific operations. Specifically, we utilize S and Try to conduct relation-specific operations incorporating time information on the head entity embedding. This operation is formally represented as:\ne's T = STT T\u00caTes (7)\nWe obtain the head entity representation er incorporating fused time and relation information using Formula 7. Unlike CompoundE , our chosen score function is not a distance metric; instead, it is determined by the semantic similarity between er and the tail entity eo. This decision is grounded in our belief that semantic similarity offers more advantages than distance metrics in the context of TKG (proof in Appendix G). The score function for TCompoundE is expressed as:\n$(s, r, o,t) =< er, eo > (8)"}, {"title": "4.2 Loss Function", "content": "Building upon TNTComplEx and TeAST , we adopt reciprocal learning for training our model, with the loss func-"}, {"title": "4.3 Modeling Various Relation Patterns", "content": "TCompoundE can model crucial relation patterns, encompassing symmetric, asymmetric, inverse and temporal evolution patterns (detail in Appendix A). We enumerate all the propositions in this section, with corresponding proofs provided in the Appendix.\nProposition 1 TCompoundE can model the symmetric relation pattern. (proof in Appendix B)\nProposition 2 TCompoundE can model the asymmetric relation pattern. (proof in Appendix C)\nProposition 3 TCompoundE can model the inverse relation pattern. (proof in Appendix D)\nProposition 4 TCompoundE can model the temporal evolution relation pattern. (proof in Appendix E)"}, {"title": "5 Experiments", "content": "We assess the performance of TCompoundE on three benchmark datasets for TKGE. The datasets include ICEWS14 and ICEWS05-15 , both derived from the Integrated Crisis Early Warning System (ICEWS)"}, {"title": "6 Results and Analysis", "content": "Table 2 presents the results of knowledge graph completion for ICEWS14, ICEWS05-15 and GDELT datasets. The best-performing results are highlighted in bold font. Our observations indicate that TCompoundE outperforms all baseline models across all metrics for t datasets. As TCompoundE exclusively applies the time-specific operation to the relation-specific scaling operation, this enables relations occurring simultaneously to utilize the same time-specific operation, facilitating the evolution of all relations over time. For the relation-specific translation operation, we maintain the timestamps unchanged to preserve the features of relations that remain constant over time. This demonstrates the significance of applying the time-specific operation exclusively to the relation-specific scaling operation. Additionally, Table 2 reveals that TCompoundE has achieved significant improvements over TKGE models utilizing a single operation for both relation-specific and time-specific operations, such as translation (TTransE) and scaling (TComplEx). This confirms that employing compound geometric operations for both relation-specific and time-specific operations is an effective strategy for designing TKG embeddings. Furthermore, BoxTE highlights that GDELT necessitates a considerable level of temporal inductive capacity to achieve effective encoding. This is because GDELT displays a notable degree of temporal variability, wherein certain facts endure across multiple consecutive timestamps, while others are momentary and sparse. The performance of our model demonstrates it can capture the dynamic evolution of relations. The results across all three datasets indicate the effectiveness of our combined approach in addressing the temporal knowledge graph completion problem."}, {"title": "6.2 Different Combinations", "content": "We investigate the impact of various combinations on ICEWS14 and ICEWS05-15. And we also compare the performance of TCompoundE with its variants. We classify these variants into two groups: the first group involves maintaining the time-specific operation unchanged while modifying the relation-specific operation, and the second group involves maintaining the relation-specific operation unchanged while modifying the time-specific operation. The results of the ICEWS14 and ICEWS05-15 variants from the aforementioned two groups are presented in Table 3 and Table 4. The best results are highlighted in bold font, while the second best are underlined. In Table 3, we can observe that TCompoundE outperforms its variants across all metrics on ICEWS14 and ICEWS05-15. As TCompoundE utilizes translation and scaling operations as relation-specific operations, this enables TCompoundE to model critical relation patterns. We can observe from Table 3 that TCompoundE has significantly outperformed its variants when employing a single operation for relation-specific operations, including translation (V1), scaling (V4) and rotation (V6). It confirms that utilizing a single operation as relation-specific has drawbacks in TKGs. We also observe that TCompoundE outperforms its variants across all geometric operations, including V3, V4 and V7. These variant models employ translation, scaling, and rotation as relation-specific operations. The distinction among these models lies in the usage of time-specific operations in different relation-specific operations, such as translation (V3), scaling (V5) and rotation (V7). This underscores that incorporating too many geometric operations as relation-specific operations is not optimal in TKGs. Conversely, V2 utilizes the same relation-specific operation as TCompoundE, while applying the time-specific operation to the relation-specific translation operation. This indicates that more features that remain constant over time can be learned from relation-specific translation operations.\nTable 4 displays the results of the second group of TCompoundE variants. In Table 4, V8, V9, and V10 employ single translation, scaling and rotation as time-specific operations. V12, V13 and V14 are"}, {"title": "6.3 Effects of Operation", "content": "We randomly select 100 quadruples from ICEWS14 and employ t-SNE to visualize the head and tail entity embeddings of TCompoundE. The visualization results are depicted in Fig. 3. We note that the initial state of the head entity and tail entity embeddings appears chaotic and irregular. However, when the head entity embeddings undergo TT and ST transformations, the distribution of the head and tail entity embeddings becomes more regular. The reason for this phenomenon, we think head entity is mapped to a space close to tail entity by relation-specific operation merging the temporal information. This is due to the design of the scoring function and illustrates the effectiveness of the composite operation. Specifically, the head and tail entities in the same quadruple exhibit a high degree of proximity. This confirms the effectiveness of employing both relation-specific and time-specific operations. In addition, we have a case study of the quadruple we use. Details can be found in the appendix I."}, {"title": "6.4 Effects of Embedding Dimension", "content": "We investigate the effect of embedding dimension on TCompoundE. In Fig. 4, we compare the MRR scores of TCompoundE and previous state-of-the-art (SOTA) embedding models on the ICEWS14 dataset across different dimension settings d \u2208 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000. According to the experimental results, TCompoundE is suboptimal in low dimensions, but it outperforms benchmarking methods when d \u2265 2000."}, {"title": "7 Conclusion", "content": "This paper introduces a novel method, TCompoundE, designed to address the challenge of knowledge graph completion in TKGs. TCompoundE applies a compound of translation and scaling as relation-specific and time-specific operations. Our experimental results demonstrate that TCompoundE effectively manages both relation and time information in TKGs. Furthermore, we provide mathematical evidence supporting TCompoundE's capability to handle various relational patterns. Additionally, we explore the effectiveness of different combinations of relation-specific and time-specific operations."}, {"title": "Limitations", "content": "Similar to many temporal knowledge graph embedding models, our proposed method, TCompoundE, faces limitations during the training phase as it cannot learn about invisible entities and time. Consequently, TCompoundE cannot directly perform temporal knowledge graph extrapolation tasks. Additionally, our model excels at high embedding dimensions, leading to its larger size compared to others."}, {"title": "A Definition of Relation Patterns", "content": "Definition 1 A relation is symmetric, if\n\u2200s, o, T, (s, \u00ee, 0, \u0442) \u2227 (o, r, s, t) \u2208 G\nDefinition 2 A relation is asymmetric, if\n\u2200s, o, T, (s, \u00ee, 0, \u0442) \u2208 G \u2227 (o, r, s, T) \u2209 G\nDefinition 3 Relation \u00ee\u2081 is the inverse of 2, if\n\u2200s, o, T, (S, \u00ce 1, 0, \u0442) \u2227 (0, 12, s, T) \u2208 G\nDefinition 4 Relation 1 and 2 are evolving\nover time from timestamp T\u2081 to timestamp T2, if\nds, o, (s, 1, 0, T\u2081) \u2227 (s, r2, 0, T2) E G.\nThe above describes various relational patterns\nfrom a mathematical point of view. We will il-\nlustrate various relational patterns next. For sym-\nmetric patterns, (Canada, Consult, France) and\n(France, Consult, Canada) show that Consult is\nsymmetric. For asymmetric patterns, is father of is\nan asymmetric relation, because (personA, is father\nof, personB) and (personB, is father of, personA)\ncan't both be true. is father of and is son of are in\nverse relations. Temporal evolution patterns detail\ncan be found at Figure 1."}, {"title": "B Proof of Propositions 1", "content": "Let M denote the compound operation for head\nentity. Following ComplEx, we employ the stan-\ndard dot product < a, b >= a \u2022 b = \u03a3\u03ba\u03b1\u03babk. For\n< a, b >=< c, d >, there are special cases that the\nprevious equation holds true when aibi == cidi,\nwhere i \u2208 [0, k]. For symmetric pattern, we can\nget (s, r, o, t) = $(s, r, o, t) based on definition\nof symmetric pattern. According to score function\nof TCompoundE, we get:\n< Mes, eo >=< Meo, es >\u21d2\nMese = Me, oes (12)"}, {"title": "C Proof of Propositions 2", "content": "By definition of asymmetric pattern, we can get\n$(s,r,o,t) \u2260 $(s,\u00f4, o,\u03c4). By similar proof for\nPropositions 1, TCompoundE can model asymmet-\nric pattern when matrix is not invertible."}, {"title": "D Proof of Propositions 3", "content": "Based on definition of inverse pattern, we have\n\u03c6(s,r1, 0, \u03c4) = $(o, r2, s, T). Hence, we get\nMies o eo = M2e, o es (14)\nIf the matrix M\u2081 or M2 is invertible, the we can\nget:\n-1\nes o eo = M\u012b\u00b9M2e, o es\nM\u012b\u00af\u00b9M2 = I or M\u2082\u00b9M\u2081 = I (15)\nTherefore, TCompoundE can model symmetric pattern when M\u2081 and M2 are inverse matrices."}, {"title": "E Proof of Propositions 4", "content": "For temporal evolution pattern, we can get\n$(s, 1, 0, T\u2081) = $(s, r2, 0, T2) based on definiton\nof temporal evolution pattern. Then through the\nscore function, we can get:\nMM272Oes (16)\nwhere M1T1\nST2TT2 ST2 T2. Then we can get:\nes o lo =\nM\n1\n+1\nM1\n== I\nloses\nlses\nM\n12\nor\n12MM+13.4\n(17)\nWe can observe from the above formula: TCom-\npoundE can model temporal evolution pattern when\nMr171 and Mr272 are inverse matrices."}, {"title": "F Introduction of Variant Models", "content": "We introduce R and R in the variant model to\nrepresent the relation-specific and time-specific ro-\ntation operation. To better explain the differences\nbetween variant models, we introduce a more gen-\neral formula instead of Formula 7:\nes\nsT = SRTT\u00caTes (18)\nwhere RT represents relation-specific rotation\noperation that incorporates temporal information.\nNext we will look at the formula differences be-\ntween TCompoundE and its variant models in ob-\ntaining the RT, ST and T\u4ee4T\nIn keeping with section 6.2, the variant model\nis also divided into two groups, where the variant\nmodel in the first group keeps the time-specific\noperations unchanged, and changes the relation-\nspecific operations; The second group keeps the\nrelation-specific operations of the same and change\nthe time-specific operations."}, {"title": "G Different Scoring Function", "content": "The semantic similarity scoring function is shown\nin function 8. The distance scoring function is\ndefined as follows:\n$(s, r, o, t) = ||et - eo||2 (19)\nFrom the definition of the distance scoring func-"}, {"title": "H More result of variant of TCompound", "content": "In the experimental analysis section, we exam-\nined the utilization of single operations as well as"}]}