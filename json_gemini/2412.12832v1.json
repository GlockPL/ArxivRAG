{"title": "DSGram: Dynamic Weighting Sub-Metrics for Grammatical Error Correction in the Era of Large Language Models", "authors": ["Jinxiang Xie", "Yilin Li", "Xunjian Yin", "Xiaojun Wan"], "abstract": "Evaluating the performance of Grammatical Error Correction (GEC) models has become increasingly challenging, as large language model (LLM)-based GEC systems often produce corrections that diverge from provided gold references. This discrepancy undermines the reliability of traditional reference-based evaluation metrics. In this study, we propose a novel evaluation framework for GEC models, DSGram, integrating Semantic Coherence, Edit Level, and Fluency, and utilizing a dynamic weighting mechanism. Our framework employs the Analytic Hierarchy Process (AHP) in conjunction with large language models to ascertain the relative importance of various evaluation criteria. Additionally, we develop a dataset incorporating human annotations and LLM-simulated sentences to validate our algorithms and fine-tune more cost-effective models. Experimental results indicate that our proposed approach enhances the effectiveness of GEC model evaluations.", "sections": [{"title": "1 Introduction", "content": "Grammatical Error Correction (GEC) models aims to automatically correct grammatical errors in natural language texts, enhancing the quality and accuracy of written content. Traditionally, the evaluation of GEC models has employed a variety of metrics, categorized into those requiring a reference (reference-based evaluation) and those that do not (reference-free evaluation).\nReference-based metrics such as BLEU (Papineni et al. 2002), ERRANT (Bryant et al. 2017), and M\u00b2 (Dahlmeier et al. 2012) compare the model-generated text with a correct reference text to evaluate the accuracy of grammatical corrections, and they are widely used in this area. Despite the usefulness of these metrics, they possess inherent limitations. For example, the golden reference may not encompass all potential corrections (Choshen et al. 2018), and the alignment of existing automatic evaluation metrics with human judgment is often weak (Coyne et al. 2023). Additionally, LLMs-based GEC models may excessively correct sentences, resulting in unnecessary editing not captured by traditional metrics (Fang et al. 2023b).\nConversely, reference-free metrics like Perplexity, GLEU (Napoles et al. 2015), and SOME (Yoshimura et al. 2020) assess the quality of generated text directly, proving beneficial when a correct reference text is unavailable, or when a single (or several) reference is insufficient for GEC evaluation in the era of LLMs. While existing reference-free evaluation metrics like SOME offer an analytical foundation, they no longer fully encompass the scope of GEC evaluation. It is necessary to design new sub-metrics based on the original"}, {"title": "2 Related Work", "content": "Numerous studies have focused on evaluating GEC models. This section provides an overview of key research utilizing LLMs for GEC assessment."}, {"title": "2.1 Model-Based Evaluation Metrics", "content": "Model-based evaluation metrics have garnered significant attention, especially in the domain of GEC. BLEURT (Sellam et al. 2020) is a versatile metric that evaluates based on (prediction, reference) pairs. It employs a scalable pretraining phase where it learns to predict automatically generated signals of supervision from semantically comparable synthetic pairs.\nA burgeoning trend in automatic evaluation involves the direct application of LLMs for assessment purposes. Liu et al. (2023) have LLMs generate numerical ratings by interpreting descriptions of evaluation criteria through a Chain-of-Thought method (Wei et al. 2023). Sottana et al. (2023) demonstrate the viability of GPT-4 in GEC assessment, highlighting an approach that uses natural language instructions to define evaluation criteria."}, {"title": "2.2 Reference-Free Evaluation", "content": "Reference-free evaluation is a method of assessing the performance of models without relying on reference. Asano et al. (2017) integrate three sub-metrics, which are Grammaticality, Fluency, and Meaning Preservation, to surpass reference-based metrics. They employ a language model and edit-level metrics as Fluency and Meaning Preservation sub-metrics, respectively, although these sub-metrics are not tailored for manual evaluation. The final score is determined through a weighted linear summation of each individual evaluation score.\nSOME is a reference-free GEC metric that follows the approach of Asano et al. (2017). It utilizes three distinct BERT models, each dedicates to one scoring aspect. The researchers construct a novel dataset for training these BERT models by annotating the outputs from various GEC systems on the CoNLL-2013 test set across the three scoring dimensions.\nHowever, in real-world applications, the use of just three evaluation metrics presents a limitation, as it hinders the generation of a holistic score in an inherently intuitive way. Consequently, this constraint diminishes its effectiveness in directing the training of GEC models. In the sphere of GEC evaluation, Wu et al. (2023) assessed ChatGPT and observed a tendency towards over-correction, which might be attributed to its extensive generative capacities as a large language model. Similarly, Fang et al. (2023b) noted that ChatGPT makes over-corrections, leading to revised sentences with high fluency. These findings align with the outcomes of our own experiments, which indicate that existing reference-free metrics do not adequately reflect these issues."}, {"title": "2.3 GEC Dataset with Human Scoring", "content": "There are very few GEC datasets with human evaluation scores. The dataset annotated by Yoshimura et al. (2020) includes individual scores for Grammaticality, Fluency, and Meaning Preservation, but lacks an overall score. GJG15 (Grundkiewicz et al. 2015) and SEEDA (Kobayashi et al. 2024) are manually annotated but provide ranking information rather than scores. Sottana et al. (2023) conducted ratings based on the gold standard, which presents certain limitations.\nSottana et al. (2023) suggested the potential of GPT-4 as a reviewer, prompting us to consider using GPT-4 to annotate a dataset that simulates human scoring for GEC model evaluation."}, {"title": "3 Dynamic Weighting Sub-Metrics for GEC", "content": "DSGram comprises two main components: score generation and weight generation. By applying specific weights to the generated scores, an overall score is obtained."}, {"title": "3.1 Generating Scores", "content": "Sub-Metrics Definition Upon our analysis, the three sub-metrics introduced by Asano et al. (2017), exhibit redundancy and insufficiency. We compute the correlation of these metrics using the SOME's dataset\nThe heatmap reveals a high correlation between Grammaticality and Fluency. Hence, we have combined these two metrics into a single \"Fluency\u201d measure. Furthermore, to address the issue of over-corrections present in LLM-based GEC models, we have incorporated a new sub-metric called \"Edit Level\" to evaluate concerns associated with excessive corrections.  It can be observed that our classification criteria have improved the sub-metrics' distribution.\nIn our definition, the meanings of the three sub-metrics are as follows:\nSemantic Coherence: The degree to which the meaning of the original sentence is preserved in the corrected sentence. It evaluates whether the corrected sentence conveys the same intended meaning as the original, without introducing semantic errors or altering the core message.\nEdit Level: The extent to which the GEC model has modified the sentence. It assesses whether the corrections made are necessary and appropriate, or if the sentence has been unnecessarily or excessively altered, deviating from the original prose more than required.\nFluency: The grammatical correctness and the natural flow of the corrected sentence. It evaluates whether the sentence adheres to proper grammar rules, has a coherent structure, and reads smoothly without awkward phrasing or unnatural constructions.\nThese three sub-metrics comprehensively cover the key aspects that GEC models need to consider in the era of large language models.\nSub-Metrics Score Generation We employ prompt engineering techniques such as Chain-of-Thought (Wei et al. 2023), few-shots (Brown et al. 2020) and output reason before scoring (Chu et al. 2024) to craft the prompts, enabling LLMs to generate scores based on the aforementioned three sub-metrics. The specific prompts can be found in Appendix.\nThe method is applicable to a variety of different LLMs. Our experiments reveal that the GPT-4 and GPT-40 exhibit greater scoring consistency and alignment with human judgments. Models such as LLaMA2 and LLaMA3 struggle to adhere to structured prompts and do not effectively grasp the scoring tasks. To reduce evaluation costs and enhance model usability, we annotate a dataset simulating human scoring with GPT-4 to fine-tune open-source LLMs, exploring their feasibility for scoring. We utilize GPT-4 to annotate DSGram-LLMs and subsequently fine-tune the LLaMA2-13B and LLaMA3-8B models on this dataset, thereby confirming their consistency with human scores.\nIn summary, we achieve the automatic generation of evaluation scores with prompting GPT-4 and open-source LLMS like LLaMA."}, {"title": "3.2 Generating Dynamic Weights", "content": "The prevalent approach using sub-metrics methods typically involves assigning specific weights to compute an overall score. However, we contend that in the context of GEC evaluation tasks, human judges may have varying considerations for different sentences. It is essential to adjust the weights according to the evaluation scenario; for formal documents such as legal texts and medical instructions, a stricter standard is required for Semantic Coherence and Edit Level. In contrast, for more relaxed contexts like dialogues, Fluency should be given priority in the assessment.\nWe employ LLMs in conjunction with the Analytic Hierarchy Process (AHP) to generate varying weights for different sentences, thereby making the weighted scores more aligned with human scoring. AHP is a decision-making framework that decomposes complex decisions into a hierarchy of simpler, more manageable elements through pairwise comparisons. It synthesizes subjective judgments into objective priorities that are appropriately weighted based on a structured evaluation of criteria and objectives.\nThe weight generation algorithm involves the following key steps:\nConstructing the Judgement Matrix: For each criterion, utilize LLMs to construct a pairwise comparison matrix $A = (a_{ij})$, where $a_{ij}$ represents the importance of criterion i relative to criterion j. The specific prompt for this process is documented in the Appendix.\nConsistency Check: Compute the consistency index (CI) and consistency ratio (CR) to check the consistency of the pairwise comparison matrix.\n$CI = \\frac{A_{max} - n}{n-1}$, $CR = \\frac{CI}{RI}$,\nwhere RI is the Random Index for a matrix of order n. For instance, an RI value of 0.58 is used for a 3\u00d73 matrix, while for a 4x4 matrix, RI is 0.90, and this pattern continues. A CR below 0.1 indicates that the pairwise comparison matrix possesses sufficient consistency.\nCalculating the Eigenvector and Eigenvalue: Solve the eigenvalue problem $Aw = \\lambda_{max}w$ to obtain the maximum eigenvalue $\\lambda_{max}$ and the corresponding eigenvector w, which is then normalized.\nCalculating the Composite Weight: Compute the composite weights by multiplying the weights at each level.\nThe procedure for this approach is outlined in Algorithm 1. Figure 5 depicts an illustration of this method for two distinct sentence pairs. Sentence (a) represents a casual daily"}, {"title": "4 Experiments and Analysis", "content": "4.1 Datasets Preparation\nWe mainly evaluate our metrics using the SEEDA dataset (Kobayashi et al. 2024), and compare them with existing metrics. The SEEDA corpus comprises human-rated annotations at two distinct evaluation levels: edit-based (SEEDA-E) and sentence-based (SEEDA-S), and encompasses corrections from 12 cutting-edge neural systems, including large language models, as well as corrections made by humans.\nIn addition to SEEDA, we also manually annotate an evaluation dataset with human scores for validation purposes. We name this human-annotated dataset DSGram-Eval. For details on annotating this dataset, please refer to the Appendix.\nFurthermore, we construct an additional training set by utilizing system outputs from the BEA-2019 shared task official website\u00b9. After removing biased output groups, a sample of approximately 2500 entries is randomly selected. Then GPT-4 is employed to generate the sub-metrics scores, which we refer to as DSGram-LLMs. This set is subsequently used for model training.\n4.2 Meta-Evaluation of Our Metrics\nWe evaluate the performance of various GEC models using the DSGram metric on the SEEDA dataset. GPT-3.5 secures the highest score in our DSGram metric, aligning with the findings in Fang et al. (2023b), which report ChatGPT as having the highest human score in fluency. Despite it exhibiting substandard scores in metrics like M\u00b2, we still regard it as an outstanding GEC system. Our metric demonstrates equally outstanding performance on models like T5 (Rothe et al. 2021) and TransGEC (Fang et al. 2023a) that excel in GLEU and ERRANT. However, the REF-F model, despite boasting the highest SOME score, underperforms significantly when assessed using our metric.\nWe calculate the correlation between the ranking according to DSGram and human ranking on the SEEDA dataset and compare it with existing metrics.  The results indicate that the correlation of our metric with human feedback surpasses that of all conventional reference-based metrics, as well as reference-free metrics like GLEU, Scribendi Score and DSGram weighted by 0.33.\n4.3 Validation of LLMs Scoring\nTo verify the broad applicability of our metrics, we conduct tests on DSGram-Eval dataset by using various LLMs (including few-shot and finetuned LLaMA models) to generate the sub-metric scores and then calculate the correlation between the sub-metric/overall scores with human scores. The correlation results indicate that GPT-4 consistently shows a high correlation with human scores across three sub-metrics. In contrast, LLaMA3-70B is found to be highly correlated in Semantic Coherence and Fluency but less so in Edit Level. Since the few-shot LLAMA performs poorly, we fine-tune LLaMA on the DSGram-LLMs dataset. The fine-tuned LLaMA3-8B and LLaMA2-13B models out-perform their original few-shot outcomes, demonstrating that LLMs can achieve higher human-aligned scores for GEC tasks, while fine-tuning smaller LLMs serves to reduce evaluation costs and enhance model usability.\n4.4 Validation of Dynamic Weights\nTo ascertain the efficacy of the dynamic weights in DSGram, we conduct a comparison between the average overall score assigned by human annotators and the score weighted by generated dynamic weights derived from human-labeled sub-metrics.\nWe calculate the AHP Human Score by applying dynamic weights to the human-annotated Semantic Coherence, Edit Level, and Fluency scores. The Pearson correlation coefficient between the AHP Human Score and the human-annotated overall score is 0.8764. Subsequently, we compute the overall score derived from the average weighting method and correlated it with the human scoring overall score. By assigning an equal weight of 0.33 to each metric, we obtain the AVG Human Score. The calculation reveal a Pearson correlation coefficient of 0.8544 between the AVG Human Score and the human scoring overall score.\nOur experimental findings indicate that when the overall score is adjusted using dynamic weights, it significantly corresponds with the evaluations provided by human annotators, and the dynamic weighting approach outperforms the average weighting method at a relaxed significance level.\nIt is suggested that, although human annotators do not consciously consider the specific weights of each metric while annotating, they implicitly gauge their relative importance. This intuitive evaluation process resembles the creation of a judgment matrix, and our method effectively mimics the human scoring procedure.\nTo further verify the effectiveness, we select three distinctly different text datasets: OpenSubtitles2, Justia (CaseLaw)\u00b3, and Wikipedia Corpus, to represent everyday conversations, legal documents, and technical explanations, respectively. For each dataset, we generate weights for three sub-metrics and evaluate the internal consistency of these weights using Cronbach's Alpha coefficient (Cronbach 1951). Specifically, we preprocess each dataset to extract sub-metric scores and then calculate Cronbach's Alpha based on the generated weights to assess the consistency and reliability of the sub-metrics across different datasets.\nThe experimental results demonstrate that the Cronbach's"}, {"title": "5 Discussion", "content": "Human Feedback For reference-free evaluation metrics, their effectiveness is often gauged by how closely they mirror human judgments. Although ChatGPT is rated as the best model in terms of grammar by human reviewers, it is rated as the worst or second-worst model in terms of semantics and over-correction (Sottana et al. 2023). This suggests that human feedback also seems to have its limitations.\nLLMs as Reviewers The utilization of large language models to construct evaluation metrics has been found to closely approximate human feedback, thereby enhancing the accuracy of assessment. Additionally, the dynamic evaluation approach demonstrates a degree of rationality, which can potentially be transferred to other forms of assessment.\nApplications The metrics in question may prove applicable in guiding the training of GEC models. For large-scale models, the rate of gradient descent during the training process may vary across different tasks. For instance, the model's ability to correct semantic errors may have already saturated, whereas there is still room for improvement in terms of fluency correction. Employing this metric to construct the loss function could potentially enhance the training of GEC models."}, {"title": "6 Conclusions", "content": "This study presents an evaluation framework for Grammatical Error Correction models that integrates Semantic Coherence, Edit Level, and Fluency through a dynamic weighting system. By leveraging the AHP in conjunction with LLMs, we have developed a method that dynamically adjusts the importance of different evaluation criteria based on the context, leading to a more nuanced and accurate assessment.\nThrough extensive experiments, our methodology has demonstrated strong alignment with human judgments, particularly when utilizing advanced LLMs like GPT-4. The dynamic weighting system has shown considerable promise in mirroring the intuitive scoring processes of human annotators, thereby validating its application in various contexts.\nLooking forward, several avenues for future research are evident. First, it is imperative to conduct more extensive tests of this method across a broader range of LLMs, including Claude, GLM and Qwen. Moreover, a more comprehensive validation of the dynamic evaluation approach is required, to explore its applicability in diverse evaluation contexts."}, {"title": "A Implement Details", "content": "The configuration for all models is standardized as follows:"}, {"title": "A.1 Prompt for Dataset Annotation", "content": "# Character\nYou are an expert in sentence revision quality assessment, specializing in evaluating the accuracy of grammatical error corrections. Your task is to assess the quality of these corrections based on three key metrics: semantic coherence, edit level, and fluency.\n# Input Format\nYou will be provided with a valid JSON string containing the following keys:\noriginal sentence: This sentence may contain grammatical errors.\ncorrected sentence: This is the sentence after it has been corrected by a grammatical error correction model.\nthe input format is as follows:\n```json\n[\n  {\n    \"original sentence\": \"...\",\n    \"corrected sentence\": \"...\"\n  }\n]\n```\n% Task Instruction\n# 1. Understand the Sentences\nEnsure you comprehend both the original and corrected sentences before proceeding with the analysis.\n# 2. Apply Evaluation Criteria\nEvaluate each sentence pair using the three sub-metrics below, scoring them independently.\n# 2.1 Semantic Coherence\nEvaluate how well the corrected sentence preserves the meaning of the original sentence.\n9 to 10 points: The context and meaning remain perfectly intact.\n7 to 8 points: Minor changes noted, yet the intended meaning stays preserved.\n5 to 6 points: Some loss of purpose, but comprehensible.\n3 to 4 points: Meaning notably altered leading to possible confusion.\n1 to 2 points: Severely changed meaning, straying from the original intention.\n# 2.2 Edit Level\nAssess whether the corrections were necessary or if the sentence was over-edited.\n9 to10 points: Purely essential edits made, free of needless corrections or no editing.\n7 to 8 points: Slight and unimportant modifications that don't majorly affect overall quality.\n5 to 8 points: Some avoidable changes made that nonetheless don't hinder overall clarity.\n3 to 4 points: Numerous unneeded corrections that potentially degrade sentence quality.\n1 to 2 points: Sentence quality seriously compromised due to unnecessary editing.\n# 2.3 Fluency\nEvaluate the grammatical accuracy and natural flow of the corrected sentence.\n9 to 10 points: Grammatically flawless and naturally flowing like a native speaker.\n7 to 8 points: Few minor grammatical misalignments resulting in slight awkwardness.\n5 to 6 points: Some grammar inaccuracies persist, leading to noticeable clumsiness.\n3 to 4 points: Several grammar mistakes unamended, destroying the flow.\n1 to 2 points: The sentence is littered with uncorrected major grammar errors, rendering it unreadable.\n# Step 4: Assign an Overall score\nRender an encompassing score reflecting the overall effectiveness of the correction. Taking all factors into consideration, check that all grammatical errors have been corrected, that there are no unnecessary corrections, or that there are no semantic inconsistencies etc. The goal is to provide a score that reflects the most accurate and natural feedback a human would give.\n# Output format\nOutput the result in JSON format. The output format is as follows:\n```json\n[\n  {\n    \"original sentence\": \"original sentence 1\",\n    \"corrected sentence\": \"corrected sentence 1\",\n    \"score\": {\n      \"semantic coherence\": X,\n      \"edit level\": X,\n      \"fluency\": X,\n      \"overall\": X\n    },\n    \"reason\": \"Explanation for the assigned scores.\",\n    \"id\": 1\n  },\n  {\n    \"original sentence\": \"original sentence 2\",\n    \"corrected sentence\": \"corrected sentence 2\",\n    \"score\": {\n      \"semantic coherence\": X,\n      \"edit level\": X,\n      \"fluency\": X,\n      \"overall\": X\n    },\n    \"reason\": \"Explanation for the assigned scores.\",\n    \"id\": 2\n  }\n]\n```"}, {"title": "A.2 Prompt for Dynamic Weights Generation", "content": "# Task Description:\nAs an expert evaluator, you will utilize the Analytic Hierarchy Process (AHP) to determine the weightage of three key dimensions in an evaluation model for grammar correction algorithms. You will evaluate and compare the relative importance of the following key dimensions by constructing judgment matrices.\nYou will be provided with a pair of sentences. Based on the sentence pair you can judge the importance weight of the following three evaluation dimensions.\n# Evaluation Criteria:\n1. Semantic Coherence Weight: To what extent the semantic similarity of the two sentences needs to be emphasized\n2. Edit Level Weight: To what extent the sentences have been overly modified and unnecessarily change should be avoided.\n3. Fluency Weight: To what extent the corrected sentence's fluidity and the natural flow of the sentence is important\n1 indicates equal importance between two dimensions.\n3 indicates a slightly more important dimension.\n5 indicates a strongly more important dimension.\n7 indicates a very strongly more important dimension.\n9 indicates an extremely more important dimension.\nIntermediate values (2, 4, 6, 8) represent varying degrees of importance.\n# Scoring Process:\n1. Construct a Judgment Matrix: For each pair of evaluation criteria (e.g., Semantic Coherence vs. Fluency), construct a judgment matrix and rate each pair based on their relative importance in the context of grammar correction.\n2. Output matrix in the following format\n{\n  \"Semantic Coherence\": [1,0.333,0.5],\n  \"Edit Level\": [1,1,1],\n  \"Fluency\":[1,0.7,1]\n}\nIf there are scores, please replace them with decimals\nPlease do not output any other information except for the matrix\n# Notes:\nEnsure that your evaluations and weight allocations are based on professional knowledge and expert judgment.\nMaintain objectivity and consistency when making pairwise comparisons.\nBased on these guidelines, please rate the following candidate equipment, and provide your recommended selection."}, {"title": "A.3 Human Annotation Details", "content": "We randomly select 200 sentences from the CoNLL-2014 and recruit three human annotators to provide detailed scoring of these entries, offering them an average reward of"}]}