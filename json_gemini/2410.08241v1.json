{"title": "LecPrompt: A Prompt-based Approach for Logical Error Correction with CodeBERT", "authors": ["Zhenyu Xu", "Victor S. Sheng"], "abstract": "Logical errors in programming don't raise compiler alerts, making them hard to detect. These silent errors can disrupt a program's function or cause run-time issues. Their correction requires deep insight into the program's logic, highlighting the importance of automated detection and repair. In this paper, we introduce LecPrompt to localize and repair logical errors, an prompt-based approach that harnesses the capabilities of CodeBERT, a transformer-based large language model trained on code. First, LecPrompt leverages a large language model to calculate perplexity and log probability metrics, pinpointing logical errors at both token and line levels. Through statistical analysis, it identifies tokens and lines that deviate significantly from the expected patterns recognized by large language models, marking them as potential error sources. Second, by framing the logical error correction challenge as a Masked Language Modeling (MLM) task, LecPrompt employs CodeBERT to autoregressively repair the identified error tokens. Finally, the soft-prompt method provides a novel solution in low-cost scenarios, ensuring that the model can be fine-tuned to the specific nuances of the logical error correction task without incurring high computational costs. To evaluate LecPrompt's performance, we created a method to introduce logical errors into correct code and applying this on QuixBugs to produce the QuixBugs-LE dataset. Our evaluations on the QuixBugs-LE dataset for both Python and Java highlight the impressive capabilities of our method, LecPrompt. For Python, LecPrompt achieves a noteworthy 74.58% top-1 token-level repair accuracy and 27.4% program-level repair accuracy. In Java, LecPrompt delivers a 69.23% top-1 token-level repair accuracy and 24.7% full program-level repair accuracy. Especially when compared against other state-of-the-art models such as CoCoNuT, CodeBERTa, and ROBERTa, our method consistently showcases superior performance across multiple key metrics. These results illuminate the potential of LecPrompt in the domain of automated logical error correction in programming.", "sections": [{"title": "I. INTRODUCTION", "content": "logical errors, which are also known as semantic errors, can be a significant problem in programming. Unlike syntax errors, which produce compiler errors, logical errors occur when the code is written correctly but produces unintended or undesired results due to a mistake in the program's logic [1]. These errors can result from a programmer writing code that is not logily correct or when the program's execution flow is not as intended. Examples of logical errors in programming include using incorrect iteration numbers, using incorrect logical operators or comparisons, such as using \"==\" instead of \"=\", and working with incorrect data types, among other things. These errors can cause the program to function improperly, even if it is written correctly from a syntax perspective. For instance, using the wrong data type or an incorrect number of iterations can lead to unexpected results, making it difficult to identify and correct the problem [2].\nSyntax errors are typically flagged by compilers or interpreters when the code is compiled or run. In contrast, logical errors are not detected by the compiler and can be more challenging to fix and localize. These errors often arise from incorrect assumptions or misunderstandings about how the program should behave. For instance, a logical error is when a statement lacks a self-consistent logical structure after the condition. This can cause unexpected results and can make it difficult for the programmer to identify and correct the error. In such a case, the programmer must carefully analyze the logic of the program to determine the source of the error and how to fix it. Additionally, logical errors can manifest in unexpected ways, producing incorrect results for some inputs but not others. This makes it difficult for the programmer to identify the error and determine the most effective way to address it. To correct a logical error in programming, the programmer needs to carefully examine the code and identify the mistake. This often involves debugging the code, testing with different input values, or adding additional debugging statements. Once the error has been pinpointed, the programmer can fix the code and retest to verify that the issue has been resolved. However, this entire process can be very time-consuming and require a lot of effort.\nIt's important to note that repairing errors in a program does not always involve reasoning. Some errors can be easily detected and fixed by the compiler, such as syntax errors that violate the rules of the programming language. However, logical errors are more challenging to identify and repair. They occur when the code produces unintended or incorrect results due to a mistake in the program's logic, and require a more in-depth analysis and reasoning to fix. This may involve examining the code's algorithm, data, or other components to pinpoint the source of the error and make changes to the code accordingly. It's this process of programmatic understanding and reasoning that makes logical errors very difficult to fix.\nOver the years, researchers and practitioners have proposed various approaches to tackle the problem of logical errors. One traditional solution is static analysis [3], which involves analyzing the program's code without actually executing it. This approach can be used to identify potential errors before the code is run, and can also be used to identify coding practices that are generally considered poor or unsafe. Static analysis tools can detect coding issues such as incorrect variable usage, incorrect output format, or data-type mismatches"}, {"title": "II. BACKGROUND", "content": "Fine-tuning pre-trained language models involves adapting a model, which has already been trained on a large dataset, to a specific task or dataset. This adaptation allows the model to perform better on the target task by leveraging the knowledge it has acquired from its initial, extensive training. During this process, parameters of the model are adjusted based on the new dataset, optimizing its performance for the specific task.\nPrompt fine-tune in natural language processing supplies additional information to a language model, helping it generate more precise outputs. This extra context, often used in in-context learning, enhances the model's understanding of tasks. Prompt fine-tune effectively harnesses pre-trained language models' capabilities for various tasks. For instance, when evaluating the sentiment of \"Best pizza ever!\", a template like \"Best pizza ever! It was _.\" transforms the sentiment analysis into a cloze task [22]. By creating suitable templates, pre-trained language models' potential can be effectively extracted. In addition to the hard prompt template approach, soft templates serve as an alternative method for fine-tuning processes, enabling the effective utilization of pre-learned knowledge from language models. As shown in Figure 1, we list the main idea of two different prompt methods:\nHard Prompt: A hard prompt is essentially a discrete, pre-determined input structure. Crafting these prompts mandates both domain expertise and an intimate understanding of both the language model and the targeted task. Due to their specific and manual design, they often excel in specific tasks for which they were intended but might lack the versatility required for broader applications or newly introduced tasks.\nSoft Prompt: Distinct from hard prompts, the soft prompt utilizes a continuous, tunable embedding structure. Rather than being manually designed, this vector is learned throughout the fine-tuning phase to adapt downstream tasks. A significant upside of this approach lies in its inherent flexibility and potential to discern intricate data patterns. However, a potential drawback is the resulting prompt might not provide the same interpretability as hard prompt.\nIn this paper, we adapt soft prompt tuning as an alternative to traditional fine-tuning methods in language modeling for tackling the logical error correction task. Instead of designing specific templates, our approach involves inserting a prompt embedding layer into the input embeddings and fine-tuning only the added parameters, which is akin to prefix tuning."}, {"title": "III. APPROACH", "content": "In the LecPrompt framework, the process is divided into two stages: localization and correction. In the localization stage, we use a log probability-driven approach to identify tokens in the program that likely contain logical errors. Once identified, these tokens are replaced with the placeholder <mask>. The correction stage employs CodeBERT. Having been trained on a vast dataset of program codes, CodeBERT can generate the appropriate token to replace each <mask>. However, it's designed to handle one <mask> at a time. To address multiple <mask> tokens, we use an autoregressive approach: after replacing one <mask>, the updated code is input back into CodeBERT for the next replacement. This process repeats until all placeholders are filled, producing the corrected program. To assess LecPrompt's effectiveness, we also develop a method that introduces logical errors into correct programs, simulating prevalent logical errors.\nTo localize logical errors in code, we first need to understand two key concepts: perplexity and log probability. Perplexity (PPL) measures a model's uncertainty about a sequence, with higher values denoting more surprise or deviation from expected sequences. On the other hand, the log probability reflects a token's likelihood in its context, where values closer to 0 indicate a more common or expected token. LLMs tends to show a high perplexity about parts of the code where logical errors occur. The primary objective of our approach is to localize logical errors at the token level. Through a probabilistic lens, we aim to localize exact tokens in code sequences that are probable candidates for logical errors. This localization procedure has two layers of granularity: token-level and line-level.\nFor example, given a program below designed to compute the number of set bits in an integer. However, due to logical errors at two parts (i.e. and -=), it falls into infinite iterations:\ndef bitcount(n):\n^\ncount = 0\nwhile n:\nn ^= n\ncount -= 1\nreturn count\nTo systematically diagnose logical errors within code, we utilize the statistical might of mean and standard deviation applied to the log probabilities of code tokens. Starting with a given set of log probabilities, {$P_1, P_2, ..., P_n$}, we calculate the mean, \u03bc, and standard deviation, \u03c3, using the formulas:\n$\u03bc = \\frac{1}{n} \\sum_{i=1}^{n} P_i$\n$\u03c3 = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (P_i - \u03bc)^2}$\nFrom here, we establish a threshold, 7, defined as \u03bc-k\u00d7\u03c3. The factor k dictates the sensitivity of our anomaly detection, it's adjustable based on specific contexts. Tokens falling below this threshold are anomalies and potential sources of logical errors. This method's strength lies in its granularity: from analyzing individual tokens to entire code lines. Using this method on our data, we can effectively predict token and line indices potentially cause logical errors.\nAfter identifying the predicted logical error token, our subsequent objective is to rectify the masked tokens within the program. To cast the logical error correction task into a masked language modeling (MLM) task, we initiate the fine-tuning of the soft prompt embedding. Given an input text sequence x = ($x_1,x_2,...,x_n$), we employ a soft prompt embedding layer \u00a7 = ($s_1, s_2, ..., s_m$), where m is the soft prompt length, and a text embedding layer $E_{text}(x)$. These embeddings are then concatenated in the combined embedding layer\n$E_{comb} (x, s) = [E_{text}(x); \u00a7]$.\nThe resulting combined embedding is passed to the frozen CodeBERT model\n$h_1,..., h_{|x,\u00a7|} = f_\u03b8(E_{comb}(x,\u00a7))$,\nwhich outputs contextualized representations of the input tokens, where @ denotes the CodeBERT parameterized by 0, and its parameters are frozen. In the soft prompt finetune,"}, {"title": "D. Model Architecture", "content": "illustrate the overall architecture of LecPrompt. The input to the model is a code snippet represented as text. This text is transformed into a word vector representation through an Embedding layer, which consists of two parts: a Tunable soft prompt and a Text embedding. The Tunable soft prompt is a learnable token sequence that is used to fine-tune the model for specific tasks. The soft prompt's weights are updated during training to better adapt to the task. The Text Embedding, on the other hand, transforms the input text into word vector representations. These representations form the original text embedding. The Tunable soft prompt and the Text Embedding are combined in the Combined Embedding layer to form a unified representation. This representation contains both the information from the input text and the task-related information provided by the soft prompt. The Combined Embedding is then passed as input to the CodeBERT model, where the parameters are frozen during fine-tuning. The Code-BERT model uses its Transformer [28] structure to learn the representation of the input code and complete the downstream task. Finally, the Combined Embedding, processed through the CodeBERT parameters, is passed to a feedforward neural network (FFNN) [29]. The FFNN transforms the embedding into prediction scores for each word in the vocabulary. The Softmax function is then applied to these scores to transform them into a probability distribution. The output is a probability distribution representing the model's prediction for each possible token. During prediction, the token with the highest probability is selected as the model's predicted output."}, {"title": "E. Iterative Masking for Logical Error Introduction", "content": "We introduce a novel iterative method for inducing logical errors into a codebase. Our approach integrates the concepts of replacing, deleting, and inserting random tokens with the '<mask>' token, drawing inspiration from the \"fill-mask\" procedures in prominent language models. The specifics of the algorithm are delineated in Algorithm 1. With a correct piece of code as the base, the method operates as follows:\n1) Initialization: Treat the initial code as the primary candidate.\n2) Masking: For each iteration, perform random operations: replace a token with '<mask>', delete a token, or insert a '<mask>' token.\n3) Filling the Mask: Leveraging a predictive model, predict and replace each \u2018<mask>' token, leading to an assortment of code variants.\n4) Logical Error Checking: Execute each variant and check for logical discrepancies. Only variants that run"}, {"title": "IV. DATASET AND EVALUATION", "content": "We aim to answer following research questions through experiments:\nHow effective is LecPrompt in localizing and repairing logical errors compared to other state-of-the-art method?\nHow does the design of the soft prompt and the mask percentage affect LecPrompt's performance?\nWhat advantages does the soft prompt method offer, and what are its limitations?\nWhat implications arise if the localization process misidentifies the location of a logical error?\nWe designed RQ1 to evaluate the performance of LecPrompt against various repair methods using the QuixBugs-LE dataset. RQ2 investigates the influence of the soft prompt's initialization, placement, and length, as well as varying mask percentages on LecPrompt's performance. Through RQ3, we aim to highlight the space-efficiency and few-shot scenario of the soft prompt approach and delve into its potential constraints. Lastly, RQ4 focuses on analyzing the potential ramifications of incorrect identifications during the logical error localization step.\nTo train and evaluate LecPrompt, we use two datasets: Codeparrot and QuixBugs-LE.\nCodeparrot: Codeparrot, sourced from GitHub [31], is a vast collection of code spanning multiple programming languages. For LecPrompt's training, we employed a specific subset of Codeparrot: the GitHub-code-clean subset. We further refined the dataset, aiming for an average line length below 60, a maximum line length under 100, and ensuring that the fraction of alphanumeric characters exceeded 0.45. The resulting Codeparrot-filtered subset contains about 745,000 Python and 884,000 Java files. During the fine-tuning process, the CodeBERT model is trained on the Codeparrot-filtered subset using the masked language modeling task, where certain tokens are masked and the model is tasked with predicting the missing token.\nThe LecPrompt utilizes the QuixBugs-LE dataset to assess its performance. The original QuixBugs dataset, sourced from the Quixey Challenge, comprises 50 Python and 40 Java programs [30]. For our dataset modification, we introduced logical errors into the correct Python and"}, {"title": "V. EXPERIMENT RESULTS", "content": "We commenced by employing the text-davinci-003 model to identify logical errors within the QuixBugs-LE dataset, both at the token and line levels, utilizing log probability metrics. This approach is a zero-shot method without fine-tuning. The outcomes of our localization efforts compared with state-of-the-art method are presented in Table I."}, {"title": "VI. DISCUSSION AND FUTURE WORK", "content": "Through our studies, we've identified several key areas that could pose threats to the validity of our current approach. These will serve as focal points for our future direction.\nThe dataset we utilized was iteratively generated through the fill-mask task of CodeBERT. The masked language model (MLM) pre-training of CodeBERT restricts it to operate on a single token at a time. Consequently, our method only attempts to introduce errors into one token at a time. However, semantic changes caused by successive multiple tokens may be closer to real-world logical errors. Such error types are challenging for CodeBERT to produce. As a future direction, we plan to employ CodeT5 [13] instead of CodeBERT. Unlike CodeBERT, CodeT5 facilitates setting a mask span, enabling it to mask multiple tokens simultaneously. This would lead to a more realistic spectrum of logical errors.\nWe currently employ a zero-shot approach using OpenAI's text-davinci-003 to compute log probabilities and perplexities. While this model performs exceptionally on mainstream languages like Python, Java, and JavaScript, it may not be as competitive for low-resource languages, such as Solidity. To address this, there is a need to leverage open-source large language models (LLMs) and fine-tune them for such low-resource codebases. Future works will explore fine-tuning on models like GPT-J [43], GPT-Neo [44], PolyCoder [45], and GPT-NeoX [46] to achieve a more adaptable localization mechanism.\nProgram repair at the token level can be categorized into three distinct operations: (1) insertion of correct token(s), (2) deletion of error token(s), and (3) substitution of error token(s) with the correct ones. While LecPrompt excels at fixing logical errors that require replacements, it falters when confronted with more intricate logical anomalies. However, the high accuracy of line prediction presents a novel avenue to explore. We aim to harness LLMs to regenerate an entire line based on predicted lines, transforming the logical error correction challenge into a line-level code completion task. Moreover, our observations from the top 1/5/10 token candidates indicate that lots of correct tokens are present within the top 5/10 candidates produced by LecPrompt. Yet, our current strategy solely relies on the top 1 candidate. Optimizing the fill-mask process to effectively harness all available candidates will be our subsequent research focus."}, {"title": "VII. RELATED WORK", "content": "Over the years, various methodologies have emerged to tackle logical errors in programming. AVATAR [36], introduced as an automated program repair technique, employs machine learning to generate and rank candidate patches for Java programs, showcasing its efficacy on the Defects4J benchmark. Around the same time, FixML emerged [37], adeptly diagnosing and rectifying errors in functional programming assignments by melding statistical error-localization with type-directed program synthesis. Neural Attribution for Semantic Bug-Localization (NBL) then came into the spotlight, leveraging deep learning to guide students in mending their C programs [38]. Song et al. further enriched the field by devising a technique that automatically detects discrepancies between reference functional programming assignments and student submissions [39]. CoCoNuT is an automated program repair technique that uses ensemble learning and context-aware neural machine translation to fix errors in multiple programming languages [40]. It has been evaluated against state-of-the-art APR techniques and approved to be able to repair logical errors. More recently, Yoshizawa et al. brought forth an innovative iterative model, addressing the longstanding challenges compilers and IDEs face with logical errors in source code [41].\nIn recent years, pre-trained language models have gained significant attention in the field of natural language processing (NLP) for their impressive performance on various NLP tasks, including language translation, text summarization, and sentiment analysis. As a result, there has been growing interest in applying these models to programming languages. By training on code or documentation and fine-tuning for tasks such as code completion, bug detection, and code generation, pre-trained language models can potentially improve software development processes. Several studies have investigated the use of these models for programming languages, showing"}, {"title": "VIII. CONCLUSION", "content": "In this paper, we introduced LecPrompt, a prompt-based approach for automatically correcting logical errors in programming languages. LecPrompt leverages the power of pre-trained large language models, specifically CodeBERT, to provide efficient and accurate logical error localization and repair at the token level. By mapping the downstream task of logical error correction to Masked Language Modeling (MLM), LecPrompt can effectively utilize pre-learned context understanding of programming languages. The soft-prompt embeddings in LecPrompt's architecture enable the model to adapt to the specific task of correcting logical"}]}