{"title": "Multimodal Inconsistency Reasoning (MMIR):\nA New Benchmark for Multimodal Reasoning Models", "authors": ["Qianqi Yan", "Yue Fan", "Hongquan Li", "Shan Jiang", "Yang Zhao", "Xinze Guan", "Ching-Chen Kuo", "Xin Eric Wang"], "abstract": "Existing Multimodal Large Language Mod-\nels (MLLMs) are predominantly trained and\ntested on consistent visual-textual inputs, leav-\ning open the question of whether they can han-\ndle inconsistencies in real-world, layout-rich\ncontent. To bridge this gap, we propose the\nMultimodal Inconsistency Reasoning (MMIR)\nbenchmark to assess MLLMs' ability to detect\nand reason about semantic mismatches in ar-\ntifacts such as webpages, presentation slides,\nand posters. MMIR comprises 534 challenging\nsamples, each containing synthetically injected\nerrors across five reasoning-heavy categories:\nFactual Contradiction, Identity Misattribution,\nContextual Mismatch, Quantitative Discrep-\nancy, and Temporal/Spatial Incoherence. We\nevaluate six state-of-the-art MLLMs, showing\nthat models with dedicated multimodal reason-\ning capabilities, such as o1, substantially out-\nperform their counterparts while open-source\nmodels remain particularly vulnerable to incon-\nsistency errors. Detailed error analyses further\nshow that models excel in detecting inconsis-\ntencies confined to a single modality, partic-\\ularly in text, but struggle with cross-modal\nconflicts and complex layouts. Probing exper-\niments reveal that single-modality prompting,\nincluding Chain-of-Thought (CoT) and Set-of-\nMark (SoM) methods, yields marginal gains,\nrevealing a key bottleneck in cross-modal rea-\nsoning. Our findings highlight the need for\nadvanced multimodal reasoning and point to\nfuture research on multimodal inconsistency.", "sections": [{"title": "Introduction", "content": "Recent advances in Large Language Models\n(LLMs) have demonstrated impressive reasoning\nabilities across a variety of tasks (OpenAI, 2024b;\nGuo et al., 2025; Kojima et al., 2022; Wei et al.,\n2022). Building on pre-trained LLMs, Multimodal\nLarge Language Models (MLLMs) are fast evolv-\ning. However, they usually face greater challenges\nas they need to reason across different modalities,\nespecially when inconsistencies (i.e., mismatched\nor contradictory contents) exist. We find that, be-\ning primarily trained and evaluated on consistent\nvisual-textual inputs, existing MLLMs are largely\nuntested in scenarios where the input contains mis-\naligned or contradictory information\u2014a situation\nthat is common in real-world scenarios. For ex-\nample, in Figure 1, a user presents a web page"}, {"title": "Related Work", "content": "Multimodal Understanding and Reasoning\nMultimodal Large Language Models (MLLMs)\nprocess multimodal inputs by first processing vi-\nsual inputs with pre-trained vision encoders such\nas CLIP (Radford et al., 2021) to extract features,\nand then projecting them into the textual repre-\nsentation space with adapters (Liu et al., 2024a; Li\net al., 2023a). Significant efforts have been made to\nbridge the gap between vision and text modalities\nvia integrating more cross-modality data such as in-\nterleaved image-text sequences and visual ground-\ning data (Alayrac et al., 2022; Chen et al., 2023;\nPeng et al., 2023). Also, some recent works de-\nvelop MLLMs with improved nuanced multimodal\nabilities, such as Optical Character Recognition\n(OCR) (Bai et al., 2023; Liu et al., 2024b), layout"}, {"title": "MMIR", "content": "The MMIR benchmark is designed to assess how\neffectively MLLMs can detect and localize seman-\ntic mismatches within complex, layout-rich arti-\nfacts. Unlike conventional benchmarks that assume\ncoherent visual-textual inputs, MMIR challenges\nmodels with realistic errors that require deep, cross-\nmodal reasoning. In MMIR, errors are defined and\ncategorized along five semantic dimensions:\nA. Factual Contradiction: Direct conflict be-\ntween two elements (text\u2013text, text-image, or im-\nage-image) within the modified content.\nB. Identity Misattribution: Mislabeling of enti-\nties (objects, locations, brands, people) that conflict\nwith other elements.\nC. Contextual Mismatch: Tonal, thematic, or\nsituational incompatibility between elements.\nD. Quantitative Discrepancy: Numerical or sta-\ntistical inconsistencies between elements.\nE. Temporal/Spatial Incoherence: Implied time-\nlines, dates, or spatial relationships that are impos-\nsible or conflicting."}, {"title": "Data Curation", "content": "MMIR's data is curated through a four-stage\npipeline (Figure 3), ensuring high-quality, diverse,\nand challenging test cases.\nArtifact Collection and Parsing We begin by\nmanually selecting a total of 521 original arti-\nfacts from two domains: 349 webpages (sub-"}, {"title": "Evaluation", "content": "MMIR assesses a model's ability to detect incon-\nsistency, i.e., identifying and localizing seman-\ntic mismatches where elements deviate from their\nexpected roles within an artifact. To assess the\nmodel's performance comprehensively, each of the\n534 test samples is provided to models under two\ndistinct settings:\nOpen-Ended Setting Models receive the artifact\nA with a fixed prompt Qopen_ended and generate\na free-form response that identifies the semantic\nmismatch. This formulation evaluates the model's\nability to detect inconsistencies without relying\non predefined answer options, thereby testing its\nunsupervised perception and reasoning.\nMultiple-Choice Setting Models receive the arti-\nfact A, but now with a combined prompt QMCQ =\n(Qopen_ended, Ci). Each candidate in C\u00bf is a textual\ndescription of an element. The model must select,\nfrom these options, the element(s) corresponding\nto the introduced inconsistency."}, {"title": "Experiments and Analysis", "content": "We first evaluate the advanced multimodal rea-\nsoning model o1 (OpenAI, 2024b) and five other\nstate-of-the-art MLLMs: GPT-40 (OpenAI, 2024a),\nQwen2.5-VL (Team, 2025), LLaVA-NeXT (Liu\net al., 2024b), InternVL2.5 (Chen et al., 2024) and\nPhi-3.5-Vision (Abdin et al., 2024) on the MMIR\nbenchmark. We implement open-source models us-\ning their default settings and select the 1217 version\nof ol and the 1120 version of GPT-40 for evalua-\ntion. Model implementation details are provided in\nAppendix B. We then examine error patterns across\ndifferent inconsistency types and layout complexi-\nties and finally explore how prompting strategies\naffect multimodal reasoning under the open-ended\nsetting."}, {"title": "Main Results", "content": "As shown in Table 2, proprietary models (01 and\nGPT-40) significantly outperform open-source al-\nternatives, though all models exhibit substantial\nroom for improvement. Appendix A.4 shows\na qualitative example with question-answer and\nmodel response.\nPerformance Gap Between Reasoning, Propri-\netary and Open-Source Models. In both open-\nended and MCQ settings, the reasoning o1 model\nsubstantially outperforms the rest, surpassing all\nopen-source models by over 30%. The other propri-\netary model GPT-40, although missing the explicit\nreasoning ability of 01, outperforms open-source\nalternatives, reflecting stronger multimodal align-\nment and reasoning capabilities.\nImpact of Semantic Cues. GPT-4o sees a 14.61%\naccuracy boost in the MCQ setting with additional\nelement descriptions as options, narrowing its gap\nwith o1 from 18.26% to just 4.4%. This indicates\nthat GPT-40 relies heavily on semantic context\nwhen available.\nInconsistent Gains for Open-Source Models.\nMost open-source models gain moderate or little\naccuracy when provided with MCQ-style prompts.\nPhi-3.5-Vision-4B experiences a 9.93% drop, sug-\ngesting weaker reasoning capacity and less effec-\ntive use of textual cues. The gap between pro-\nprietary and open-source models widens further\nin MCQ (from 27.08% to 35.21%), highlighting\nthe persistent challenge of integrating perceptual\ngrounding with logical inference."}, {"title": "Error Analysis", "content": "To investigate how different types of inconsisten-\ncies affect model performance, we show the results"}, {"title": "Impact of Layout Complexity", "content": "We further examine the relationship between model\naccuracy and the number of elements in an arti-\nfact. To ensure statistical significance, we only\ninclude data points where at least 10 samples share"}, {"title": "Probing on Prompting Methods", "content": "We further investigate whether textual or visual\nprompts can alleviate the reasoning bottleneck. Ta-\nble 3 compares Chain-of-Thought (CoT) prompt-\ning (Wei et al., 2022) and Set-of-Mark (SoM) vi-\nsual augmentation (Yang et al., 2023), as well as\ntheir combination. We also explored an interleaved\nmultimodal reasoning strategy, which we term Mul-\ntimodal Interleaved CoT (MM-CoT) to further in-"}, {"title": "Chain-of-Thought (CoT) Prompting", "content": "To assess whether explicit reasoning instructions\ncan enhance performance, we apply CoT prompt-\ning (Wei et al., 2022) to the four open-sourced\nmodels (benchmarked proprietary models have API\nguides to not include additional CoT prompting).\nAs shown in Table 3, CoT prompting yields neg-\nligible or even negative effects on accuracy. This\nsuggests that simply injecting explicit reasoning\nsteps is insufficient when the underlying model\nlacks strong cross-modal alignment or robust logi-\ncal inference mechanisms."}, {"title": "Set-of-Mark (SoM) Prompting", "content": "We next examine the effect of SoM visual prompt-\ning (Yang et al., 2023). By overlaying bounding\nboxes onto the artifact screenshots (example in Fig-\nure 6), we aim to enhance the models' ability to\nperceive and localize elements."}, {"title": "Multimodal Interleaved CoT (MM-CoT)", "content": "Our previous analyses indicate that single-modality\nprompts (CoT or SoM) often yield minimal or even\ndetrimental gains in the open-ended setting when\nmodels receive no textual hints about which ele-\nments might be inconsistent. We hypothesize that\nMMIR tasks demand iterative reasoning that tightly\nintegrates both visual and textual modalities. To\naddress this, we propose Multimodal Interleaved\nCoT (MM-CoT), a two-stage approach explicitly\ndesigned to weave visual cues into a step-by-step\nreasoning process:\nStage 1: Initial Candidate Generation The\nmodel receives the same input in Stage 1 as in\nthe open-ended setting, generating its top five pre-\ndictions (along with associated reasoning). Using\n01-mini (0912) to interpret these responses, we\nmap each prediction back to one or a pair of ele-"}, {"title": "Discussion and Conclusion", "content": "In this work, we introduce the Multimodal Incon-\nsistency Reasoning Benchmark (MMIR) to evalu-\nate how well MLLMs detect and localize seman-\ntic mismatches in complex real-world artifacts.\nMMIR challenges models across five error cate-\ngories and two reasoning settings for a detailed\nassessment of multimodal reasoning. Our experi-\nments show that even advanced proprietary mod-\nels struggle with open-ended inconsistency detec-\ntion. Although providing natural-language descrip-\ntions in a multiple-choice format offers modest\ngains, standard prompting techniques (e.g., Chain-\nof-Thought and Set-of-Mark) yield inconsistent or\nnegative effects, while a proposed Multimodal In-\nterleaved CoT (MM-CoT) method that iteratively\nrefines reasoning by integrating visual and textual\nmodalities, yielding greater performance improve-\nments. Despite these advances, significant chal-\nlenges remain, motivating further research on ro-\nbust multimodal reasoning for real-world inconsis-\ntency detection."}, {"title": "Limitations", "content": "While MMIR provides a rigorous framework for\nevaluating multimodal inconsistency reasoning, it\nis not without its limitations. Annotating and ver-\nfying inconsistencies in layout-rich artifacts re-\nmains a labor-intensive process. Although MMIR's\npipeline integrates automated editing and verifica-\ntion, the overall scale is still limited by the need for\ncareful human review. Although these domains cap-\nture a range of layouts and content types, they do\nnot encompass the full variety of real-world multi-\nmodal artifacts (e.g., multi-page documents, social\nmedia feeds, or mobile application interfaces). On\nthe other hand, synthetic error generation\u2014while\neffective for systematically introducing controlled\ninconsistencies-may not perfectly mirror the nu-\nanced mistakes that occur in human-generated con-\ntent. This could lead to discrepancies between\nmodel performance on MMIR and in truly open-\nended, real-world scenarios. Scaling up the dataset\nto cover broader domains, more intricate layouts,\nand diverse error types would strengthen its ability\nto serve as a comprehensive benchmark for real-\nworld multimodal inconsistency detection."}]}